Experiments with Corpus-based LFG Specialization 
Nico la  Cancedda and  Chr i s te r  Samuelsson  
Xerox Research Centre Europe, 
6, chemin de Maupertuis  
38240 Meylan, France 
{cancedda.lsamuelsson}@xrce.xerox.com 
Abst rac t  
Sophisticated grammar formalisms, uch as LFG, al- 
low concisely capturing complex linguistic phenom- 
ena. The powerful operators provided by such for- 
malisms can however introduce spurious ambigu- 
ity, making parsing inefficient. A simple form of 
corpus-based grammar pruning is evaluated experi- 
mentally on two wide-coverage grammars, one En- 
giish and one French. Speedups of up to a factor 6 
were obtained, at a cost in grammatical coverage of 
about 13%. A two-stage architecture allows achiev- 
ing significant speedups without introducing addi- 
tional parse failures. 
1 In t roduct ion  
Expressive grammar formalisms allow grammar de- 
velopers to capture complex linguistic generaliza- 
tions concisely and elegantly, thus greatly facilitat- 
ing grammar development and maintenance. (Car- 
rol, 1994) found that the empirical performance 
when parsing with unification-based grammars is 
nowhere near the theoretical worst-case complexity. 
Nonetheless, directly parsing with such grammars, 
in the form they were developed, can be very ineffi- 
cient. For this reason, grammars are typically com- 
piled into representations that allow faster parsing. 
This does however not solve the potential problem 
of the grammars overgenerating considerably, thus 
allowing large amounts of spurious ambiguity. In- 
deed, a current rend in high-coverage parsing, es- 
pecially when employing a statistical model of lan- 
guage, see, e.g., (Collins 97), is to allow the grammar 
to massively overgenerate and instead isambiguate 
by statistical means during or after parsing. If the 
benefits resulting from more concise grammatical de- 
scriptions are to outweigh the costs of spurious am- 
biguity, the latter must be brought down. 
In such a situation, corpus-based compilation 
techniques can drastically improve parsing perfor- 
mance without burdening the grammar developer. 
The initial, and much seminal work in this area 
was been carried out by Rayner and coworkers, see 
(Rayner 1988), (Samuelsson and Rayner 91) and 
(Rayner and Carter 1996). In the current article, 
we apply similar ideas to Lexical Functional Gram- 
mar (LFG) in the incarnation of the Xerox Linguis- 
tic Environment (XLE). The goal is to investigate 
to what extent corpus-based compilation techniques 
can reduce overgeneration a d spurious ambiguity, 
and increase parsing efficiency, without jeopardiz- 
ing coverage. The rest of the article is organized 
as follows: Section 2 presents the relevant aspects 
of the LFG formalism and the pruning strategy em- 
ployed, Section 3 describes the experimental setup, 
Section 4 reports the experimental results and Sec- 
tion 5 relates this to other work. 
2 LFG and  Grammar  Prun ing  
The LFG formalism (Kaplan and Bresnan, 1982) al- 
lows the right-hand sides (RHS) of grammar rules to 
consist of a regular expression over grammar sym- 
bols. This makes it more appropriate to refer to 
the grammar rules as rule schemata, since each RHS 
can potentially be expanded into a (possibly infinite) 
number of distinct sequences of grammar symbols, 
each corresponding to a traditional phrase-structure 
rule. As can easily be imagined, the use of regular- 
expression operators uch as Kleene-star and com- 
plementation may introduce a considerable amount 
of spurious ambiguity. Moreover, the LFG formal- 
ism provides operators which - -  although not in- 
creasing its theoretical expressive power - -  allow 
rules to be written more concisely. Examples of such 
operators are the ignore operator, which allows skip- 
ping any sequence of grammar symbols that matches 
a given pattern; the shuffle operator, which allows 
a set of grammar symbols to occur in any order; 
and the linear precedence operator, which allows par- 
tially specifying the order of grammar symbols. 
The pruning method we propose consists in elim- 
inating complex operators from the grammar de- 
scription by considering how they were actually in- 
stantiated when parsing a corpus. In LFGs, each 
rule scheme corresponds to a particular grammar 
symbol, since different expansions of the same sym- 
bol are expressed as alternatives in the regular ex- 
pression on its RHS. We can define a specific path 
through the RHS of a rule scheme by the choices 
~tf~ 211 204
made when matching it against some sequence of 
grammar symbols. Our training data allows us to 
derive, for each training example, the choices made 
at each rule expansion. By applying these choices to 
the rule scheme in isolation, we can derive a phrase- 
structure rule from it,. 
The grammar is specialized, or pruned, by retain- 
ing all and only those phrase-structure ules that 
correspond to a path taken through a rule scheme 
when expanding some node in some training exam- 
ple. Since the grammar formalism requires that each 
LHS occur only in one rule scheme in the gram- 
mar, extracted rules with the same LHS symbol are 
merged into a single rule scheme with a disjunction 
operator at its top level. For instance, if a rule 
scheme with the structure 
A ~ B*{CI D} 
is expanded in the training data only in the following 
ways 
A -> C 
A --+ BC 
A -+ BD 
then it will be replaced by a rule scheme with the 
following structure 
A --+ {C IBC\ ]BD} 
The same approach is taken to replace all regular- 
expression operators, other than concatenation, with 
the actual sequences of grammar symbols that are 
matched against them. A more realistic example, 
taken from the actual data, is shown in Figure 1: 
none of the optional alternative portions following 
the V is ever used in any correct parse in the corpus. 
Moreover, the ADVP preceding the V occurs only 
0 or 1 times in correct parses. 
Like other unification-based formalisms, lexical 
functional grammars allow grammar ules to be an- 
notated with sets of feature-based constraints, here 
called "functional descriptions", whose purpose is 
both to enforce additional constraints on rule appli- 
cability and to build an enriched predicate-argument 
structure called "f-structure", which, together with 
the parse tree, constitutes the output of the parsing 
process. As these constraints are maintained verba- 
tim in the specialized version of the rule scheme, this 
poses no problem for this form of grammar pruning. 
3 Exper imenta l  Setup  
The experiments carried out to determine the ef- 
fectiveness of corpus-based specialization were per- 
formed as illustrated in Figure 2. Two broad- 
coverage LFG grammars were used, one for French 
and one for English, both of which were developed 
within the Pargram project (Butt et al, 1999) dur- 
ing several years time. The French grammar consists 
of 133 rule schemata, the English grammar of 8.5 rule 
schemata. 
Each gralmnar is equipped with a treebank, which 
was developed for other purposes than grammar spe- 
cialization. Each treebank was produced by letting 
the system parse a corpus of technical documenta- 
tion. Any sentence that did not obtain any parse 
was discarded. At this point, the French corpus 
was reduced to 960 sentences, and the English cor- 
pus to 970. The average sentence length was 9 for 
French and 8 for English. For each sentence, a hu- 
man expert then selected the most appropriate anal- 
ysis among those returned by the parser. 
In the current experiments, each treebank was 
used to specialize the grammar it had been devel- 
oped with. A set of 10-fold cross-validation experi- 
ments was carried out to measure several interesting 
quantities under different conditions. This means 
that, for each language, the corpus was randomly 
split into ten equal parts, and one tenth at a time 
was held out for testing while the remaining nine 
tenths were used to specialize the grammar, and the 
results were averaged over the ten runs.. For each 
grammar the average number of parses per sentence, 
the fraction of sentences which still received at least 
one parse (angparse) and the fraction of sentences for 
which the parse selected by the expert was still de- 
rived (coverage) were measured 1. The average CPU 
time required by parsing was also measured, and this 
was used to compute the speedup with respect o the 
original grammar. 
The thus established results constitute one data 
point in the trade-off between ambiguity reduction 
on one side, which is in turn related to parsing speed, 
and loss in coverage on the other. In order to deter- 
mine other points of this trade-off, the same set. of 
experiments was performed where speciMization was 
inhibited for certain rule schemata. In particular, for 
each grammar, the two rule schemata that received 
the largest number of distinct expansions in the cor- 
pora were determined. These proved to be those 
associated with the LHS symbols 'VPverb\[main\]' 
and 'NP'  for the French grammar, and 'VPv'  and 
'NPadj'  for the English one. 2 The experiments were 
repeated while inhibiting specialization of first the 
scheme with the most expansions, and then the two 
most expanded schemata. 
Measures of coverage and speedup are important 
1 As long  as we are  in teres ted  in preserv ing  the  f - s t ructure  
ass igned to  sentences ,  th i s  not ion  of  coverage  is s t r i c te r  than  
necessary .  The  same f - s t ructure  can  in fac t  be  ass igned by  
more  than  one  parse ,  so that  in  some cases  a sentence  is con-  
s idered  out  of  coverage  ven if  the  spec ia l i zed  grammar  ass igns  
to  it  the  cor rect  f - s t ruc ture .  
2 'VPv '  and  'VPverb \ [main \ ] '  cover  VPs  headed by  a main  
verb .  'NPad j '  covers  NPs  w i th  ad jec t ives  a t tached.  
205
The original rule: 
l/Pperfp --+ 
ADVP* 
SE (t ADJUNCT) 
($ ADV_TYPE) = t,padv 
~/r 
{ @M_Head_Perfp I@M_Head_Passp } 
@( Anaph_Ctrl $) 
{ AD VP+ 
SE ('~ ADJUNCT) 
($ ADV_TYPE) = vpadv 
is replaced by the following: 
ADVP 
,\[.E (~ ADJUNCT) 
(.l. ADV_TYPE)  = vpadv 
l/'Pperfp --+ 
@PPadjunct @PPcase_obl 
{@M.Head_Pevfp \[@M..Head_Passp} 
@( Anaph_Ctrl ~ )
V 
{ @M_Head_Perfp I@M_Head_Passp } 
@( Anaph_Ctrl ~) 
Figure 1: The pruning of a rule from the actual French grammar. The "*" and the "+" signs have the usual 
interpretation as in regular expressions. A sub-expression enclosed in parenthesis optional. Alternative 
sub-expressions are enclosed in curly brackets and separated by the "\[" sign. An "@" followed by an identifier 
is a macro expansion operator, and is eventually replaced by further functional descriptions. 
Corpus 
--..,, 
0.1\[ 
Disambiguated 
Treebank treebank 
Human 
expert 
Grammar 
specialization 
Specialized 
grammar 
Figure 2: The setting for our experiments on grammar specialization. 
indicators of what can be achieved with this form of 
grammar pruning. However, they could potentially 
be misleading, since failure times for uncovered sen- 
tences might be considerably ower than their pars- 
ing times, had they not been out of coverage. If 
the pruned grammar fails more frequently on sen- 
tences which take longer to parse, the measured 
speedup might be artificiMly high. This is easily 
realized, as simply removing the hardest sentences 
froln the corpus would cause a decrease ill the av- 
erage parsing time, and thus result in a speedup, 
without any pruning at all. To factor out the contri- 
bution of uncovered sentences fi'om the results, the 
performance of a two-stage architecture analogous 
to that of (Samuelsson and Rayner, 1991) was siln- 
ulated, in which the pruned grammar is attempted 
206
"A Sentence" 
Parser with specialized 
grammar 
Fails 
1 
Succeeds 
L_ 
Time = Timespecialize d 
Parser with original 
grammar 
Time = Timespecialize d + Time original 
Figure 3: A schematic representation of the simu- 
lated two-stage coverage-preserving architecture. 
first, and the sentence is passed on to the original 
unpruned grammar whenever the pruned grammar 
fails to return a parse (see Figure 3). The mea- 
sured speedup of this simulated architecture, which 
preserves the anyparse measure of the original gram- 
mar, takes into account the contribution of uncov- 
ered sentences, as it penalizes weeping difficult sen- 
tences under the carpet. 
4 Experimental Results 
The results of the experiments described in the sec- 
tion above are summarized in the table in Figure 4. 
The upper part of the table refers to experiments 
with the French grammar, the lower part to exper- 
iments with the English grammar. For each lan- 
guage, the first line presents data gathered for the 
original grammar for comparison with the pruned 
grammars. The figures in the second line were col- 
lected by pruning the grammar based on the whole 
corpus, and then testing on the corpus itself. The 
grammars obtained in this way contain 516 and 388 
disjuncts - -  corresponding to purely concatenative 
rules - -  for French and English respectively. Any- 
parse and coverage are not, of course, relevant in 
this case, but the statistics on parsing time are, es- 
pecially the one on the maximum parsing time. For 
each iteration in the 10-fold cross-validation experi- 
ment, the maximum parsing time was retained, and 
those ten times were eventually averaged. If pruning 
tended to leave sentences which take long to parse 
uncovered, then we would observe a significant dif- 
ference between the average over ma.ximum times on 
the grammar trained and tested on the same corpus 
(which parses all sentences, including the hardest), 
and the average over maximum times for grammars 
trained and tested on different sets. The fact that 
this does not seem to be the case indicates that prun- 
ing does not penalize difficult sentences. Note also 
that the average number of parses per sentence is 
significantly smaller than with the full grammar, of 
almost a factor of 9 in the case of the French gram- 
inar. 
The third line contains results for the fully pruned 
grammar .  In the case of the French grammar  a 
speedup of about 6 is obtained with a loss in cov- 
erage of 13%. The smaller speedup gained with 
the English grammar can be explained by the fact 
that here, the parsing times are lower in general, 
and that a non-negligible part of this time, espe- 
cially that needed for morphological nalysis, is un- 
affected by pruning. Even in the case of the English 
grammar, though, speedup is substantial (2.67). For 
both grammars, the reduction in the average max- 
inmm parsing time is particularly good, confirming 
our hypothesis that tr imming the grammar by re- 
moving heavy constructs makes it considerably more 
efficient. A partially negative note comes from the 
average number of disjuncts in the prun.ed grain- 
mars, which is 501 for French and 374 for English. 
Comparing this figures to the number of disjuncts in 
grammars pruned on the full corpus (516 and 388), 
we find that after training on nine tenths of the cor- 
pus, adding the last tenth still leads to an increase 
of 3-4% in the size of the resulting grammars. In 
other words, the marginal gain of further training 
examples is still significant after considering about 
900 sentences, indicating that the training corpora 
are somewhat too small. 
The last two lines for each language show figures 
for grammars with pruning inhibited on the most 
variable and the two most variable symbols respec- 
tively. For both languages, inhibiting pruning on the 
most variable symbol has the expected effect of in- 
creasing both parsing time and coverage. Inhibiting 
pruning also on the second most variable symbol has 
ahnost no effect for French, and only a small effect 
for English. 
The table in Figure 5 summarizes the measures 
on the simulated two-stage architecture. For both 
languages the best trade-off, once the distribution 
of uncovered sentences has been taken into account, 
is achieved by the fully pruned grammars. 
5 Re la ted  Work  
The work presented in the current article is related 
to previous work on corpus-based grammar spe- 
cialization as presented in (Rayner, 1988; Salnuels- 
son and Rayner, 1991; Rayner and Carter, 1996; 
Samuelsson, 1994; Srinivas a.nd Joshi, 1995; Neu- 
mann, 1997). 
207 
Parses/sentence 
French  
original grammar 1941 
test = training 219 
Anyparse Coverage Avg. time 
(secs.) 
Max. time 
(secs.) 
Speedup 
1.00 1.OO 1.52 78.5 1 
1.00 1.00 0.28 5.62 5.43 
0.91 0.25 First-order pruning 164 0.87 5.69 6.08 
no pruning on 
'VPverb\[main\]' 1000 0.94 0.91 0.42 8.70 3.62 
no pruning on 
'Vpverb\[main\]' and 'NP'  
First-order pruning 
0.94 0.92 0.42 8.42 1279 3.62 
0.88 
1.00 1.00 0.56 31.73 1 
1.00 1.00 0.23 3.92 2.43 
0.94 0.21 
0.91 
Eng l i sh  
original grammar 58 
test = training 24 
21 
0.96 0.32 
0.35 
no pruning on 
'VPv'  
0.96 
25 
no pruning on 
'Vpv' and 'NPadj '  31 0.93 
3.92 
11.06 
11.16 
Figure 4: The results of the experiments on LFG specialization. 
Avg. CPU time (secs.) Speedup 
French  
0.570 2.67 
0.616 2.47 
0.614 2.48 
First-order pruning 
no pruning on 'VPverb\[main\]' 
no pruning on 'VPverb\[main\]' and 'NP'  
Eng l i sh  
First-order pruning 0.311 1.81 
no pruning on 'VPv'  0.380 1.47 
no pruning on 'VPv'  and 'NPadj'  0.397 1.40 
2.67 
1.75 
1.60 
Figure 5: Results for the simulated two-stage architecture. 
The line of work described in (Rayner, 1988; 
Samuelsson and Rayner, 1991; Rayner and Carter, 
1996; Samuelsson, 1994) deals with unification- 
based grammars that already have a purely- 
concatenative context-fi'ee backbone, and is more 
concerned with a different t~orm of specialization, 
consisting in the application of explanation-based 
learning (EBL). Here, the central idea is to collect 
the most frequently occurring subtrees in a treebank 
and use them as atomic units for parsing. The cited 
works differ mainly in the criteria adopted for select- 
ing subtrees fi'om the treebank. In (Rayner, 1988; 
Samuelsson and Rayner, 1991; Rayner and Carter, 
1996) these criteria are handcoded: all subtrees at- 
isfying some properties are selected, and a new gram- 
mar rule is created by flattening each such subtree, 
i.e., by taking the root as lefl.-hand side and the yield 
as right-hand side, and in the process performing all 
unifications corresponding to the thus removed in- 
ternal nodes. Experiments carried out on a corpus 
of 15,000 trees from the ATIS domain using a ver- 
sion of the SRI Core Language Engine resulted in a 
speedup of about 3.4 at a cost of 5% in gralmnati- 
cal coverage, which however was compensated by an 
increase in parsing accuracy. 
Finding suitable tree-cutting criteria requires a 
considerable amount of work, and must be repeated 
for each new grammar and for each new domain to 
which the grammar is to be specialized. Samuelsson 
(Samuelsson, 1994) proposes a technique to auto- 
matically selects what subtrees to retain. The se- 
lection of appropriate subtrees is done by choosing 
a subset of nodes at which to cut trees. Cutnodes 
are determined by computing the entropy of each 
node, and selecting only those nodes whose entropy 
exceeds a given threshold. Intuitively, nodes with 
low entropy indicate locations in the trees where a 
given symbol was expanded using a predictable set 
of rules, at least most of the times, so that the loss 
of coverage that derives from ignoring the remain- 
ing cases is low. Nodes with high entropy, on the 
other hand, indicate positions in which there is a 
high uncertainty in what rule was used to expand 
the symbol, so that it is better to preserve all alter- 
natives. Several schemas are proposed to compute 
entropies, each leading to a different trade-off be- 
~fllR 
tween coverage reduction and speedup. In general, 
results are not quite as good as those obtained using 
handcoded criteria, though of course the specialized 
grammar is obtained fully automatically, and thus 
with much less effort. 
When ignoring issues related to the elimination of 
complex operators t"1"o111 the RHS of rule schemata, 
the grammar-pruning strategy described in the cur- 
rent article is equivalent to explanation-based l arn- 
ing where all nodes have been selected,as eutnodes. 
Conversely, EBL can be viewed as higher-order 
grammar pruning, removing not grammar ules, but 
gramlnar-rule combinations. 
Some of the work done on data-oriented parsing 
(DOP) (Bod, 1993; Bod and Scha, 1996; Bod and 
Kaplan, 1998; Sima'an, 1999) can also be considered 
related to our work, as it can be seen as a way to 
specialize in an gBL-like way the (initially unknown) 
grammar implicitly underlying a treebank. 
(Srinivas and aoshi, 1995) and (Neumann, 1997) 
apply EBL to speed up parsing with tree-adjoining 
grammars and sentence generation with HPSGs re- 
spectively, though they do so by introducing new 
components in their systems rather then by modify- 
ing the grammars they use. 
6 Conclus ions 
Sophisticated grammar formalisms are very useful 
and convenient when designing high-coverage ram- 
mars for natural languages. Very expressive gram- 
matical constructs can make the task of develop- 
ing and maintaining such a large resource consid- 
erably easier. On the other hand, their use can re- 
sult in a considerable increase in grammatical am- 
biguity. Gramnaar-compilation techniques based on 
grammar structure alone are insufficient remedies in 
those cases, as they cannot access the information 
required to determine which alternatives to retain 
and which alternatives to discard. 
The current article demonstrates that a relatively 
simple pruning technique, employing the kind of ref- 
erence corpus that is typically used for grammar de- 
velopment and thus often already available, can sig- 
nificantly improve parsing performance. On large 
lexical functional grammars, speedups of up to a 
factor 6 were observed, at the price of a. reduction 
in grammatical coverage of about 13%. A simple 
two-stage architecture was also proposed that pre- 
serves the anyparse measure of the original gram- 
mar, demonstrating that significant speedups can be 
obtained without increasing the number of parsing 
failures. 
Future work includes extending the study of 
corpus-based grammar specialization from first- 
order grammar pruning to higher-order gram- 
mar pruning, thus extending previous work on 
explanation-based learning for parsing, aad apply- 
ing it to the LFG fornaalism. 
References  
Rens Bod and Ronald Kaplan. 1998. A probabilistic 
corpus-driven model for lexical-functional naly- 
sis. In Proceedings of Coling-ACL-98, Montreal, 
Canada. 
R. Bod and R. Scha. 1996. Data-oriented lan- 
guage processing: An overview. Technical report, 
ILLC, University of Amsterdam, Alnsterdam, The 
Netherlands. 
Rens Bod. 1993. Using an annotated corpus as a 
stochastic grammar. In Proceedings of EACL-93, 
Utrecht, The Netherlands. 
M. Butt, T.H. King, M.E. Nifio, and F. Segond. 
1999. A Grammar Writer's Cookbook. CSLI Pub- 
lications, Stanford, CA. 
John Carrol. 1994. Relating complexity to practical 
performance in parsing with wide-coverage uni- 
fication grammars. In Proceedings of (ACL '94), 
Las Cruces, New Mexico, June. 
Ronald Kaplan and Joan Bresnan. 1982. Lexical- 
functional grammar: A formal system for gram- 
matical representation. In Joan Bresnan, editor, 
The Mental Representation f Grammatical Rela- 
tions, pages 173-281. MIT Press. 
G/inter Neumann. 1997. Applying explanation- 
based learning to control and speeding-up natu- 
ral language generation. In Proceedings of A CL- 
EACL-97, Madrid, Spain. 
Manny Rayner and David Carter. 1996. Fast pars- 
ing using pruning and grammar specialization. In 
Proceedings of the ACL-96, Santa. Cruz, CA. 
Manny Rayner. 1988. Applying explanation-based 
generalization to natural-language processing. 
In Proceedings of the International Conference 
on Fifth Generation Computer Systems, Tokyo, 
Japan. 
Christer Samuelsson and Manny Rayner. 1991. 
Quantitative evaluation of explanation-based 
learning as an optimization tool for a large-scale 
natural language system. In Proceedings of the 
IJCAI-91, Sydney, Oz. 
Christer Samuelsson. 1994. Grammar specialization 
through entropy thresholds. In Proceedings of the 
ACL-94, Las Cruces, New Mexico. Available as 
cmp-lg/9405022. 
Khalil Sima'an. 1999. Learning Efficient Dis- 
ambiguation. Ph.D. thesis, Institute for Logic, 
Language and Computation, Amsterdam, The 
Netherlands. 
B. Srinivas and A. Joshi. 1995. Some novel appli- 
cations of explanation-based learning to parsing 
lexicalized tree-adjoining ramlnars. In Proceed- 
ings of the ACL-95, Cambridge, MA. 
209 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 755?762, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Translating with non-contiguous phrases
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman,
Eric Gaussier, Cyril Goutte, Kenji Yamada
Xerox Research Centre Europe
FirstName.FamilyName@xrce.xerox.com
Philippe Langlais
RALI/DIRO Universite? de Montre?al
felipe@iro.umontreal.ca
Arne Mauser
RWTH Aachen University
arne.mauser@rwth-aachen.de
Abstract
This paper presents a phrase-based statis-
tical machine translation method, based
on non-contiguous phrases, i.e. phrases
with gaps. A method for producing such
phrases from a word-aligned corpora is
proposed. A statistical translation model
is also presented that deals such phrases,
as well as a training method based on the
maximization of translation accuracy, as
measured with the NIST evaluation met-
ric. Translations are produced by means of
a beam-search decoder. Experimental re-
sults are presented, that demonstrate how
the proposed method allows to better gen-
eralize from the training data.
1 Introduction
Possibly the most remarkable evolution of recent
years in statistical machine translation is the step
from word-based models to phrase-based models
(Och et al, 1999; Marcu and Wong, 2002; Yamada
and Knight, 2002; Tillmann and Xia, 2003). While
in traditional word-based statistical models (Brown
et al, 1993) the atomic unit that translation operates
on is the word, phrase-based methods acknowledge
the significant role played in language by multi-
word expressions, thus incorporating in a statistical
framework the insight behind Example-Based Ma-
chine Translation (Somers, 1999).
However, Phrase-based models proposed so far
only deal with multi-word units that are sequences
of contiguous words on both the source and the tar-
get side. We propose here a model designed to deal
with multi-word expressions that need not be con-
tiguous in either or both the source and the target
side.
The rest of this paper is organised as follows. Sec-
tion 2 provides motivations, definition and extrac-
tion procedure for non-contiguous phrases. The log-
linear conditional translation model we adopted is
the object of Section 3; the method used to train
its parameters is described in Section 4. Section 5
briefly describes the decoder. The experiments we
conducted to asses the effectiveness of using non-
contiguous phrases are presented in Section 6.
2 Non-contiguous phrases
Why should it be a good thing to use phrases
composed of possibly non-contiguous sequences of
words? In doing so we expect to improve trans-
lation quality by better accounting for additional
linguistic phenomena as well as by extending the
effect of contextual semantic disambiguation and
example-based translation inherent in phrase-based
MT. An example of a phenomenon best described
using non-contiguous units is provided by English
phrasal verbs. Consider the sentence ?Mary switches
her table lamp off?. Word-based statistical mod-
els would be at odds when selecting the appropri-
ate translation of the verb. If French were the target
language, for instance, corpus evidence would come
from both examples in which ?switch? is translated
as ?allumer? (to switch on) and as ?e?teindre? (to
switch off). If many-to-one word alignments are not
allowed from English to French, as it is usually the
755
2 31
Pierre
Pierre
ne mange pas
does not eat
Figure 1: An example of a complex alignment asso-
ciated with different syntax for negation in English
and French.
case, then the best thing a word-based model could
do in this case would be to align ?off? to the empty
word and hope to select the correct translation from
?switch? only, basically a 50-50 bet. While han-
dling inseparable phrasal verbs such as ?to run out?
correctly, previously proposed phrase-based models
would be helpless in this case. A comparable behav-
ior is displayed by German separable verbs. More-
over, non-contiguous linguistic units are not limited
to verbs. Negation is formed, in French, by inserting
the words ?ne? and ?pas? before and after a verb re-
spectively. So, the sentence ?Pierre ne mange pas?
and its English translation display a complex word-
level alignment (Figure 1) current models cannot ac-
count for.
Flexible idioms, allowing for the insertion of lin-
guistic material, are other phenomena best modeled
with non-contiguous units.
2.1 Definition and library construction
We define a bi-phrase as a pair comprising a source
phrase and a target phrase: b = ?s?, t??. Each of the
source and target phrases is a sequence of words and
gaps (indicated by the symbol ?); each gap acts as
a placeholder for exactly one unspecified word. For
example, w? = w1w2?w3?? w4 is a phrase of length
7, made up of two contiguous words w1 and w2, a
first gap, a third word w3, two consecutive gaps and
a final word w4. To avoid redundancy, phrases may
not begin or end with a gap. If a phrase does not
contain any gaps, we say it is contiguous; otherwise
it is non-contiguous. Likewise, a bi-phrase is said to
be contiguous if both its phrases are contiguous.
The translation of a source sentence s is produced
by combining together bi-phrases so as to cover the
source sentence, and produce a well-formed target-
language sentence (i.e. without gaps). A complete
translation for s can be described as an ordered se-
quence of bi-phrases b1...bK . When piecing together
the final translation, the target-language portion t?1
of the first bi-phrase b1 is first layed down, then each
subsequent t?k is positioned on the first ?free? posi-
tion in the target language sentence, i.e. either the
leftmost gap, or the right end of the sequence. Fig-
ure 2 illustrates this process with an example.
To produce translations, our approach therefore
relies on a collection of bi-phrases, what we call a
bi-phrase library. Such a library is constructed from
a corpus of existing translations, aligned at the word
level.
Two strategies come to mind to produce non-
contiguous bi-phrases for these libraries. The first is
to align the words using a ?standard? word aligne-
ment technique, such as the Refined Method de-
scribed in (Och and Ney, 2003) (the intersection of
two IBM Viterbi alignments, forward and reverse,
enriched with alignments from the union) and then
generate bi-phrases by combining together individ-
ual alignments that co-occur in the same pair of sen-
tences. This is the strategy that is usually adopted in
other phrase-based MT approaches (Zens and Ney,
2003; Och and Ney, 2004). Here, the difference is
that we are not restricted to combinations that pro-
duce strictly contiguous bi-phrases.
The second strategy is to rely on a word-
alignment method that naturally produces many-to-
many alignments between non-contiguous words,
such as the method described in (Goutte et al,
2004). By means of a matrix factorization, this
method produces a parallel partition of the two texts,
seen as sets of word tokens. Each token therefore
belongs to one, and only one, subset within this par-
tition, and corresponding subsets in the source and
target make up what are called cepts. For example,
in Figure 1, these cepts are represented by the circles
numbered 1, 2 and 3; each cept thus connects word
tokens in the source and the target, regardless of po-
sition or contiguity. These cepts naturally constitute
bi-phrases, and can be used directly to produce a bi-
phrase library.
Obviously, the two strategies can be combined,
and it is always possible to produce increasingly
large and complex bi-phrases by combining together
co-occurring bi-phrases, contiguous or not. One
problem with this approach, however, is that the re-
sulting libraries can become very large. With con-
756
danser le tango
to tango
I do not want to tango anymore
I do not want anymore
doI want
Je ne veux plus danser le tango
Je
I
ne plus
veux
wantdo
not anymore
I
source =
bi?phrase 1 =
bi?phrase 2 =
bi?phrase 3 =
bi?phrase 4 =
target =
Figure 2: Combining bi-phrases to produce a translation.
tiguous phrases, the number of bi-phrases that can
be extracted from a single pair of sentences typically
grows quadratically with the size of the sentences;
with non-contiguous phrases, however, this growth
is exponential. As it turns out, the number of avail-
able bi-phrases for the translation of a sentence has
a direct impact on the time required to compute the
translation; we will therefore typically rely on vari-
ous filtering techniques, aimed at keeping only those
bi-phrases that are more likely to be useful. For ex-
ample, we may retain only the most frequently ob-
served bi-phrases, or impose limits on the number of
cepts, the size of gaps, etc.
3 The Model
In statistical machine translation, we are given a
source language input sJ1 = s1...sJ , and seek the
target-language sentence tI1 = t1...tI that is its most
likely translation:
t?I1 = argmaxtI1Pr(t
I
1|s
J
1 ) (1)
Our approach is based on a direct approximation
of the posterior probability Pr(tI1|sJ1 ), using a log-
linear model:
Pr(tI1|s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 )
)
In such a model, the contribution of each feature
function hm is determined by the corresponding
model parameter ?m; ZsJ1 denotes a normalization
constant. This type of model is now quite widely
used for machine translation (Tillmann and Xia,
2003; Zens and Ney, 2003)1.
Additional variables can be introduced in such a
model, so as to account for hidden characteristics,
and the feature functions can be extended accord-
ingly. For example, our model must take into ac-
count the actual set of bi-phrases that was used to
produce this translation:
Pr(tI1, b
K
1 |s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 , b
K
1 )
)
Our model currently relies on seven feature func-
tions, which we describe here.
? The bi-phrase feature function hbp: it rep-
resents the probability of producing tI1 using
some set of bi-phrases, under the assump-
tion that each source phrase produces a target
phrase independently of the others:
hbp(t
I
1, s
J
1 , b
K
1 ) =
K?
k=1
logPr(t?k|s?k) (2)
Individual bi-phrase probabilities Pr(t?k|s?k)
are estimated based on occurrence counts in the
word-aligned training corpus.
? The compositional bi-phrase feature function
hcomp: this is introduced to compensate for
1Recent work from Chiang (Chiang, 2005) addresses simi-
lar concerns to those motivating our work by introducing a Syn-
chronous CFG for bi-phrases. If on one hand SCFGs allow to
better control the order of the material inserted in the gaps, on
the other gap size does not seem to be taken into account, and
phrase dovetailing such as the one involving ?do ?want? and
?not ???anymore? in Fig. 2 is disallowed.
757
hbp?s strong tendency to overestimate the prob-
ability of rare bi-phrases; it is computed as in
equation (2), except that bi-phrase probabilities
are computed based on individual word transla-
tion probabilities, somewhat as in IBM model
1 (Brown et al, 1993):
Pr(t?|s?) =
1
|s?||t?|
?
t?t?
?
s?s?
Pr(t|s)
? The target language feature function htl: this
is based on a N -gram language model of the
target language. As such, it ignores the source
language sentence and the decomposition of
the target into bi-phrases, to focus on the actual
sequence of target-language words produced
by the combination of bi-phrases:
htl(t
I
1, s
J
1 , b
K
1 ) =
I?
i=1
logPr(ti|t
i?1
i?N+1)
? The word-count and bi-phrase count feature
functions hwc and hbc: these control the length
of the translation and the number of bi-phrases
used to produce it:
hwc(tI1, s
J
1 , b
K
1 ) = I hbc(t
I
1, s
J
1 , b
K
1 ) = K
? The reordering feature function
hreord(tI1, s
J
1 , b
K
1 ): it measures the amount of
reordering between bi-phrases of the source
and target sentences.
? the gap count feature function hgc: It takes as
value the total number of gaps (source and tar-
get) within the bi-phrases of bK1 , thus allowing
the model some control over the nature of the
bi-phrases it uses, in terms of the discontigui-
ties they contain.
4 Parameter Estimation
The values of the ? parameters of the log-linear
model can be set so as to optimize a given crite-
rion. For instance, one can maximize the likely-
hood of some set of training sentences. Instead, and
as suggested by Och (2003), we chose to maximize
directly the quality of the translations produced by
the system, as measured with a machine translation
evaluation metric.
Say we have a set of source-language sentences
S. For a given value of ?, we can compute the set of
corresponding target-language translations T . Given
a set of reference (?gold-standard?) translations R
for S and a function E(T,R) which measures the
?error? in T relative to R, then we can formulate the
parameter estimation problem as2:
?? = argmin?E(T,R)
As pointed out by Och, one notable difficulty with
this approach is that, because the computation of T
is based on an argmax operation (see eq. 1), it is not
continuous with regard to ?, and standard gradient-
descent methods cannot be used to solve the opti-
mization. Och proposes two workarounds to this
problem: the first one relies on a direct optimiza-
tion method derived from Powell?s algorithm; the
second introduces a smoothed (continuous) version
of the error function E(T,R) and then relies on a
gradient-based optimization method.
We have opted for this last approach. Och shows
how to implement it when the error function can be
computed as the sum of errors on individual sen-
tences. Unfortunately, this is not the case for such
widely used MT evaluation metrics as BLEU (Pa-
pineni et al, 2002) and NIST (Doddington, 2002).
We show here how it can be done for NIST; a simi-
lar derivation is possible for BLEU.
The NIST evaluation metric computes a weighted
n-gram precision between T and R, multiplied by
a factor B(S, T,R) that penalizes short translations.
It can be formulated as:
B(S, T,R) ?
N?
n=1
?
s?S In(ts, rs)
?
s?S Cn(ts)
(3)
where N is the largest n-gram considered (usually
N = 4), In(ts, rs) is a weighted count of common
n-grams between the target (ts) and reference (rs)
translations of sentence s, and Cn(ts) is the total
number of n-grams in ts.
To derive a version of this formula that is a con-
tinuous function of ?, we will need multiple trans-
lations ts,1, ..., ts,K for each source sentence s. The
general idea is to weight each of these translations
2For the sake of simplicity, we consider a single reference
translation per source sentence, but the argument can easily be
extended to multiple references.
758
by a factor w(?, s, k), proportional to the score
m?(ts,k|s) that ts,k is assigned by the log-linear
model for a given ?:
w(?, s, k) =
[
m?(ts,k|s)
?
k? m?(ts,k? |s)
]?
where ? is the smoothing factor. Thus, in
the smoothed version of the NIST function, the
term In(ts, rs) in equation (3) is replaced by?
k w(?, s, k)In(ts,k, rs), and the term Cn(ts) is
replaced by
?
k w(?, s, k)Cn(ts,k). As for the
brevity penalty factor B(S, T,R), it depends on
the total length of translation T , i.e.
?
s |ts|. In
the smoothed version, this term is replaced by
?
s
?
k w(?, s, k)|ts,k|. Note that, when ? ? ?,
then w(?, s, k) ? 0 for all translations of s, except
the one for which the model gives the highest score,
and so the smooth and normal NIST functions pro-
duce the same value. In practice, we determine some
?good? value for ? by trial and error (5 works fine).
We thus obtain a scoring function for which we
can compute a derivative relative to ?, and which can
be optimized using gradient-based methods. In prac-
tice, we use the OPT++ implementation of a quasi-
Newton optimization (Meza, 1994). As observed by
Och, the smoothed error function is not convex, and
therefore this sort of minimum-error rate training is
quite sensitive to the initialization values for the ?
parameters. Our approach is to use a random set of
initializations for the parameters, perform the opti-
mization for each initialization, and select the model
which gives the overall best performance.
Globally, parameter estimation proceeds along
these steps:
1. Initialize the training set: using random pa-
rameter values ?0, for each source sentence of
some given set of sentences S, we compute
multiple translations. (In practice, we use the
M -best translations produced by our decoder;
see Section 5).
2. Optimize the parameters: using the method de-
scribed above, we find ? that produces the best
smoothed NIST score on the training set.
3. Iterate: we then re-translate the sentences of S
with this new ?, combine the resulting multiple
translations with those already in the training
set, and go back to step 2.
Steps 2 and 3 can be repeated until the smooothed
NIST score does not increase anymore3.
5 Decoder
We implemented a version of the beam-search stack
decoder described in (Koehn, 2003), extended to
cope with non-contiguous phrases. Each transla-
tion is the result of a sequence of decisions, each of
which involves the selection of a bi-phrase and of a
target position. The final result is obtained by com-
bining decisions, as in Figure 2. Hypotheses, cor-
responding to partial translations, are organised in a
sequence of priority stacks, one for each number of
source words covered. Hypotheses are extended by
filling the first available uncovered position in the
target sentence; each extended hypotheses is then
inserted in the stack corresponding to the updated
number of covered source words. Each hypothesis is
assigned a score which is obtained as a combination
of the actual feature function values and of admissi-
ble heuristics, adapted to deal with gaps in phrases,
estimating the future cost for completing a transla-
tion. Each stack undergoes both threshold and his-
togram pruning. Whenever two hypotheses are in-
distinguishable as far as the potential for further ex-
tension is concerned, they are merged and only the
highest-scoring is further extended. Complete trans-
lations are eventually recovered in the ?last? priority
stack, i.e. the one corresponding to the total num-
ber of source words: the best translation is the one
with the highest score, and that does not have any
remaining gaps in the target.
6 Evaluation
We have conducted a number of experiments to eval-
uate the potential of our approach. We were par-
ticularly interested in assessing the impact of non-
contiguous bi-phrases on translation quality, as well
as comparing the different bi-phrase library contruc-
tion strategies evoked in Section 2.1.
3It can be seen that, as the set of possible translations for
S stabilizes, we eventually reach a point where the procedure
converges to a maximum. In practice, however, we can usually
stop much earlier.
759
6.1 Experimental Setting
All our experiments focused exclusively on French
to English translation, and were conducted using the
Aligned Hansards of the 36th Parliament of Canada,
provided by the Natural Language Group of the USC
Information Sciences Institute, and edited by Ulrich
Germann. From this data, we extracted three dis-
tinct subcorpora, which we refer to as the bi-phrase-
building set, the training set and the test set. These
were extracted from the so-called training, test-1
and test-2 portions of the Aligned Hansard, respec-
tively. Because of efficiency issues, we limited our-
selves to source-language sentences of 30 words or
less. More details on the evaluation data is presented
in Table 14.
6.2 Bi-phrase Libraries
From the bi-phrase-building set, we built a number
of libraries. A first family of libraries was based on
a word alignment ?A?, produced using the Refined
method described in (Och and Ney, 2003) (com-
bination of two IBM-Viterbi alignments): we call
these the A libraries. A second family of libraries
was built using alignments ?B? produced with the
method in (Goutte et al, 2004): these are the B li-
braries. The most notable difference between these
two alignments is that B contains ?native? non-
contiguous bi-phrases, while A doesn?t.
Some libraries were built by simply extracting the
cepts from the alignments of the bi-phrase-building
corpus: these are the A1 and B1 libraries, and vari-
ants. Other libraries were obtained by combining
cepts that co-occur within the same pair of sen-
tences, to produce ?composite? bi-phrases. For in-
stance, the A2 libraries contain combinations of 1
or 2 cepts from alignment A; B3 contains combina-
tions of 1, 2 or 3 cepts, etc.
Some libraries were built using a ?gap-size? filter.
For instance library A2-g3 contains those bi-phrases
obtained by combining 1 or 2 cepts from alignment
A, and in which neither the source nor the target
phrase contains more than 3 gaps. In particular, li-
brary B1-g0 does not contain any non-contiguous
bi-phrases.
4Preliminary experiments on different data sets allowed us
to establish that 800 sentences constituted an acceptable size
for estimating model parameters. With such a corpus, the esti-
mation procedure converges after just 2 or 3 iterations.
Finally, all libraries were subjected to the same
two filtering procedures: the first excludes all bi-
phrases that occur only once in the training corpus;
the second, for any given source-language phrase,
retains only the 20 most frequent target-language
equivalents. While the first of these filters typically
eliminates a large number of entries, the second only
affects the most frequent source phrases, as most
phrases have less than 20 translations.
6.3 Experiments
The parameters of the model were optimized inde-
pendantly for each bi-phrase library. In all cases,
we performed only 2 iterations of the training proce-
dure, then measured the performance of the system
on the test set in terms of the NIST and BLEU scores
against one reference translation. As a point of com-
parison, we also trained an IBM-4 translation model
with the GIZA++ toolkit (Och and Ney, 2000), using
the combined bi-phrase building and training sets,
and translated the test set using the ReWrite decoder
(Germann et al, 2001)5.
Table 2 describes the various libraries that were
used for our experiments, and the results obtained
for each.
System/library bi-phrases NIST BLEU
ReWrite 6.6838 0.3324
A1 238 K 6.6695 0.3310
A2-g0 642 K 6.7675 0.3363
A2-g3 4.1 M 6.7068 0.3283
B1-g0 193 K 6.7898 0.3369
B1 267 K 6.9172 0.3407
B2-g0 499 K 6.7290 0.3391
B2-g3 3.3 M 6.9707 0.3552
B1-g1 206 K 6.8979 0.3441
B1-g2 213 K 6.9406 0.3454
B1-g3 218 K 6.9546 0.3518
B1-g4 222 K 6.9527 0.3423
Table 2: Bi-phrase libraries and results
The top part of the table presents the results for
the A libraries. As can be seen, library A1 achieves
approximately the same score as the baseline sys-
tem; this is expected, since this library is essentially
5Both the ReWrite and our own system relied on a trigram
language model trained on the English half of the bi-phrase
building set.
760
Subset sentences source words target words
bi-phrase-building set 931,000 17.2M 15.2M
training set 800 11,667 10,601
test set 500 6726 6041
Table 1: Data sets.
made up of one-to-one alignments computed using
IBM-4 translation models. Adding contiguous bi-
phrases obtained by combining pairs of alignments
does gain us some mileage (+0.1 NIST)6. Again, this
is consistent with results observed with other sys-
tems (Tillmann and Xia, 2003). However, the addi-
tion of non-contiguous bi-phrases (A2-g3) does not
seem to help.
The middle part of Table 2 presents analogous re-
sults for the corresponding B libraries, plus the B1-
g0 library, which contains only those cepts from the
B alignment that are contiguous. Interestingly, in
the experiments reported in (Goutte et al, 2004),
alignment method B did not compare favorably to A
under the widely used Alignment Error Rate (AER)
metric. Yet, the B1-g0 library performs better than
the analogous A1 library on the translation task.
This suggests that AER may not be an appropriate
metric to measure the potential of an alignment for
phrase-based translation.
Adding non-contiguous bi-phrases allows another
small gain. Again, this is interesting, as it sug-
gests that ?native? non-contiguous bi-phrases are in-
deed useful for the translation task, i.e. those non-
contiguous bi-phrases obtained directly as cepts in
the B alignment.
Surprisingly, however, combining cepts from the
B alignment to produce contiguous bi-phrases (B2-
G0) does not turn out to be fruitful. Why this
is so is not obvious and, certainly, more experi-
ments would be required to establish whether this
tendency continues with larger combinations (B3-
g0, B4-g0...). Composite non-contiguous bi-phrases
produced with the B alignments (B2-g3) seem
to bring improvements with regard to ?basic? bi-
phrases (B1), but it is not clear whether these are
significant.
6While the differences in scores in these and other experi-
ments are relatively small, we believe them to be significant, as
they have been confirmed systematically in other experiments
and, in our experience, by visual inspection of the translations.
Visual examination of the B1 library reveals
that many non-contiguous bi-phrases contain long-
spanning phrases (i.e. phrases containing long se-
quences of gaps). To verify whether or not these
were really useful, we tested a series of B1 libraries
with different gap-size filters. It must be noted that,
because of the final histogram filtering we apply on
libraries (retain only the 20 most frequent transla-
tions of any source phrase), library B1-g1 is not
a strict subset of B1-g2. Therefore, filtering on
gap-size usually represents a tradeoff between more
frequent long-spanning bi-phrases and less frequent
short-spanning ones.
The results of these experiments appear in the
lower part of Table 2. While the differences in score
are small, it seems that concentrating on bi-phrases
with 3 gaps or less affords the best compromise.
For small libraries such as those under consideration
here, this sort of filtering may not be very important.
However, for higher-order libraries (B2, B3, etc.) it
becomes crucial, because it allows to control the ex-
ponential growth of the libraries.
7 Conclusions
In this paper, we have proposed a phrase-based sta-
tistical machine translation method based on non-
contiguous phrases. We have also presented a esti-
mation procedure for the parameters of a log-linear
translation model, that maximizes a smooth version
of the NIST scoring function, and therefore lends
itself to standard gradient-based optimization tech-
niques.
From our experiments with these new methods,
we essentially draw two conclusions. The first and
most obvious is that non-contiguous bi-phrases can
indeed be fruitful in phrase-based statistical machine
translation. While we are not yet able to character-
ize which bi-phrases are most helpful, some of those
that we are currently capable of extracting are well
suited to cover some short-distance phenomena.
761
The second conclusion is that alignment quality is
crucial in producing good translations with phrase-
based methods. While this may sound obvious, our
experiments shed some light on two specific aspects
of this question. The first is that the alignment
method that produces the most useful bi-phrases
need not be the one with the best alignment error
rate (AER). The second is that, depending on the
alignments one starts with, constructing increasingly
large bi-phrases does not necessarily lead to better
translations. Some of our best results were obtained
with relatively small libraries (just over 200,000 en-
tries) of short bi-phrases. In other words, it?s not
how many bi-phrases you have, it?s how good they
are. This is the line of research that we intend to
pursue in the near future.
Acknowledgments
The authors are grateful to the anonymous reviewers
for their useful suggestions. 7
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast Decoding and Optimal Decoding
for Machine Translation. In Proceedings of ACL 2001,
Toulouse, France.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Proc.
ACL?04, pages 503?510.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
7This work was supported in part by the IST Programme
of the European Community, under the PASCAL Network of
Excellence, IST-2002-506778. This publication only reflects
the authors? views.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP 02), Philadel-
phia, PA.
J. C. Meza. 1994. OPT++: An Object-Oriented Class
Library for Nonlinear Optimization. Technical Report
SAND94-8225, Sandia National Laboratories, Albu-
querque, USA, March.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of ACL 2000, pages
440?447, Hongkong, China, October.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VCL 99), College Park,
MD.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL?03: 41st Ann. Meet.
of the Assoc. for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 311?318, Philadel-
phia, USA.
Harold Somers. 1999. Review Article: Example-based
Machine Translation. Machine Translation, 14:113?
157.
Christoph Tillmann and Fei Xia. 2003. A phrase-based
unigram model for statistical machine translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), Philadelphia, PA.
Richard Zens and Hermann Ney. 2003. Improvements
in Phrase-Based Statistical Machine Translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
762
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 333?341,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Phrase-Based Statistical Machine Translation as a Traveling Salesman
Problem
Mikhail Zaslavskiy? Marc Dymetman Nicola Cancedda
Mines ParisTech, Institut Curie Xerox Research Centre Europe
77305 Fontainebleau, France 38240 Meylan, France
mikhail.zaslavskiy@ensmp.fr {marc.dymetman,nicola.cancedda}@xrce.xerox.com
Abstract
An efficient decoding algorithm is a cru-
cial element of any statistical machine
translation system. Some researchers have
noted certain similarities between SMT
decoding and the famous Traveling Sales-
man Problem; in particular (Knight, 1999)
has shown that any TSP instance can be
mapped to a sub-case of a word-based
SMT model, demonstrating NP-hardness
of the decoding task. In this paper, we fo-
cus on the reverse mapping, showing that
any phrase-based SMT decoding problem
can be directly reformulated as a TSP. The
transformation is very natural, deepens our
understanding of the decoding problem,
and allows direct use of any of the pow-
erful existing TSP solvers for SMT de-
coding. We test our approach on three
datasets, and compare a TSP-based de-
coder to the popular beam-search algo-
rithm. In all cases, our method provides
competitive or better performance.
1 Introduction
Phrase-based systems (Koehn et al, 2003) are
probably the most widespread class of Statistical
Machine Translation systems, and arguably one of
the most successful. They use aligned sequences
of words, called biphrases, as building blocks for
translations, and score alternative candidate trans-
lations for the same source sentence based on a
log-linear model of the conditional probability of
target sentences given the source sentence:
p(T, a|S) = 1ZS
exp
?
k
?khk(S, a, T ) (1)
where the hk are features, that is, functions of the
source string S, of the target string T , and of the
? This work was conducted during an internship at
XRCE.
alignment a, where the alignment is a representa-
tion of the sequence of biphrases that where used
in order to build T from S; The ?k?s are weights
and ZS is a normalization factor that guarantees
that p is a proper conditional probability distri-
bution over the pairs (T,A). Some features are
local, i.e. decompose over biphrases and can be
precomputed and stored in advance. These typ-
ically include forward and reverse phrase condi-
tional probability features log p(t?|s?) as well as
log p(s?|t?), where s? is the source side of the
biphrase and t? the target side, and the so-called
?phrase penalty? and ?word penalty? features,
which count the number of phrases and words in
the alignment. Other features are non-local, i.e.
depend on the order in which biphrases appear in
the alignment. Typical non-local features include
one or more n-gram language models as well as
a distortion feature, measuring by how much the
order of biphrases in the candidate translation de-
viates from their order in the source sentence.
Given such a model, where the ?i?s have been
tuned on a development set in order to minimize
some error rate (see e.g. (Lopez, 2008)), together
with a library of biphrases extracted from some
large training corpus, a decoder implements the
actual search among alternative translations:
(a?, T ?) = arg max
(a,T )
P (T, a|S). (2)
The decoding problem (2) is a discrete optimiza-
tion problem. Usually, it is very hard to find the
exact optimum and, therefore, an approximate so-
lution is used. Currently, most decoders are based
on some variant of a heuristic left-to-right search,
that is, they attempt to build a candidate translation
(a, T ) incrementally, from left to right, extending
the current partial translation at each step with a
new biphrase, and computing a score composed of
two contributions: one for the known elements of
the partial translation so far, and one a heuristic
333
estimate of the remaining cost for completing the
translation. The variant which is mostly used is
a form of beam-search, where several partial can-
didates are maintained in parallel, and candidates
for which the current score is too low are pruned
in favor of candidates that are more promising.
We will see in the next section that some char-
acteristics of beam-search make it a suboptimal
choice for phrase-based decoding, and we will
propose an alternative. This alternative is based on
the observation that phrase-based decoding can be
very naturally cast as a Traveling Salesman Prob-
lem (TSP), one of the best studied problems in
combinatorial optimization. We will show that this
formulation is not only a powerful conceptual de-
vice for reasoning on decoding, but is also prac-
tically convenient: in the same amount of time,
off-the-shelf TSP solvers can find higher scoring
solutions than the state-of-the art beam-search de-
coder implemented in Moses (Hoang and Koehn,
2008).
2 Related work
Beam-search decoding
In beam-search decoding, candidate translation
prefixes are iteratively extended with new phrases.
In its most widespread variant, stack decoding,
prefixes obtained by consuming the same number
of source words, no matter which, are grouped to-
gether in the same stack1 and compete against one
another. Threshold and histogram pruning are ap-
plied: the former consists in dropping all prefixes
having a score lesser than the best score by more
than some fixed amount (a parameter of the algo-
rithm), the latter consists in dropping all prefixes
below a certain rank.
While quite successful in practice, stack decod-
ing presents some shortcomings. A first one is that
prefixes obtained by translating different subsets
of source words compete against one another. In
one early formulation of stack decoding for SMT
(Germann et al, 2001), the authors indeed pro-
posed to lazily create one stack for each subset
of source words, but acknowledged issues with
the potential combinatorial explosion in the num-
ber of stacks. This problem is reduced by the use
of heuristics for estimating the cost of translating
the remaining part of the source sentence. How-
1While commonly adopted in the speech and SMT com-
munities, this is a bit of a misnomer, since the used data struc-
tures are priority queues, not stacks.
ever, this solution is only partially satisfactory. On
the one hand, heuristics should be computationally
light, much lighter than computing the actual best
score itself, while, on the other hand, the heuris-
tics should be tight, as otherwise pruning errors
will ensue. There is no clear criterion to guide
in this trade-off. Even when good heuristics are
available, the decoder will show a bias towards
putting at the beginning the translation of a certain
portion of the source, either because this portion
is less ambiguous (i.e. its translation has larger
conditional probability) or because the associated
heuristics is less tight, hence more optimistic. Fi-
nally, since the translation is built left-to-right the
decoder cannot optimize the search by taking ad-
vantage of highly unambiguous and informative
portions that should be best translated far from the
beginning. All these reasons motivate considering
alternative decoding strategies.
Word-based SMT and the TSP
As already mentioned, the similarity between
SMT decoding and TSP was recognized in
(Knight, 1999), who focussed on showing that
any TSP can be reformulated as a sub-class of the
SMT decoding problem, proving that SMT decod-
ing is NP-hard. Following this work, the exis-
tence of many efficient TSP algorithms then in-
spired certain adaptations of the underlying tech-
niques to SMT decoding for word-based models.
Thus, (Germann et al, 2001) adapt a TSP sub-
tour elimination strategy to an IBM-4 model, us-
ing generic Integer Programming techniques. The
paper comes close to a TSP formulation of de-
coding with IBM-4 models, but does not pursue
this route to the end, stating that ?It is difficult
to convert decoding into straight TSP, but a wide
range of combinatorial optimization problems (in-
cluding TSP) can be expressed in the more gen-
eral framework of linear integer programming?.
By employing generic IP techniques, it is how-
ever impossible to rely on the variety of more
efficient both exact and approximate approaches
which have been designed specifically for the TSP.
In (Tillmann and Ney, 2003) and (Tillmann, 2006),
the authors modify a certain Dynamic Program-
ming technique used for TSP for use with an IBM-
4 word-based model and a phrase-based model re-
spectively. However, to our knowledge, none of
these works has proposed a direct reformulation
of these SMT models as TSP instances. We be-
lieve we are the first to do so, working in our case
334
with the mainstream phrase-based SMT models,
and therefore making it possible to directly apply
existing TSP solvers to SMT.
3 The Traveling Salesman Problem and
its variants
In this paper the Traveling Salesman Problem ap-
pears in four variants:
STSP. The most standard, and most studied,
variant is the Symmetric TSP: we are given a non-
directed graph G on N nodes, where the edges
carry real-valued costs. The STSP problem con-
sists in finding a tour of minimal total cost, where
a tour (also called Hamiltonian Circuit) is a ?cir-
cular? sequence of nodes visiting each node of the
graph exactly once;
ATSP. The Asymmetric TSP, or ATSP, is a vari-
ant where the underlying graph G is directed and
where, for i and j two nodes of the graph, the
edges (i,j) and (j,i) may carry different costs.
SGTSP. The Symmetric Generalized TSP, or
SGTSP: given a non-oriented graph G of |G|
nodes with edges carrying real-valued costs, given
a partition of these |G| nodes into m non-empty,
disjoint, subsets (called clusters), find a circular
sequence of m nodes of minimal total cost, where
each cluster is visited exactly once.
AGTSP. The Asymmetric Generalized TSP, or
AGTSP: similar to the SGTSP, but G is now a di-
rected graph.
The STSP is often simply denoted TSP in the
literature, and is known to be NP-hard (Applegate
et al, 2007); however there has been enormous
interest in developing efficient solvers for it, both
exact and approximate.
Most of existing algorithms are designed for
STSP, but ATSP, SGTSP and AGTSP may be re-
duced to STSP, and therefore solved by STSP al-
gorithms.
3.1 Reductions AGTSP?ATSP?STSP
The transformation of the AGTSP into the ATSP,
introduced by (Noon and Bean, 1993)), is illus-
trated in Figure (1). In this diagram, we assume
that Y1, . . . , YK are the nodes of a given cluster,
while X and Z are arbitrary nodes belonging to
other clusters. In the transformed graph, we in-
troduce edges between the Yi?s in order to form a
cycle as shown in the figure, where each edge has
a large negative cost ?K. We leave alone the in-
coming edge to Yi from X , but the outgoing edge
Figure 1: AGTSP?ATSP.
from Yi to X has its origin changed to Yi?1. A
feasible tour in the original AGTSP problem pass-
ing through X,Yi, Z will then be ?encoded? as a
tour of the transformed graph that first traverses
X , then traverses Yi, . . . , YK , . . . , Yi?1, then tra-
verses Z (this encoding will have the same cost as
the original cost, minus (k ? 1)K). Crucially, if
K is large enough, then the solver for the trans-
formed ATSP graph will tend to traverse as many
K edges as possible, meaning that it will traverse
exactly k ? 1 such edges in the cluster, that is, it
will produce an encoding of some feasible tour of
the AGTSP problem.
As for the transformation ATSP?STSP, several
variants are described in the literature, e.g. (Ap-
plegate et al, 2007, p. 126); the one we use is from
(Wikipedia, 2009) (not illustrated here for lack of
space).
3.2 TSP algorithms
TSP is one of the most studied problems in com-
binatorial optimization, and even a brief review of
existing approaches would take too much place.
Interested readers may consult (Applegate et al,
2007; Gutin, 2003) for good introductions.
One of the best existing TSP solvers is imple-
mented in the open source Concorde package (Ap-
plegate et al, 2005). Concorde includes the fastest
exact algorithm and one of the most efficient im-
plementations of the Lin-Kernighan (LK) heuris-
tic for finding an approximate solution. LK works
by generating an initial random feasible solution
for the TSP problem, and then repeatedly identi-
fying an ordered subset of k edges in the current
tour and an ordered subset of k edges not included
in the tour such that when they are swapped the
objective function is improved. This is somewhat
335
reminiscent of the Greedy decoding of (Germann
et al, 2001), but in LK several transformations can
be applied simultaneously, so that the risk of being
stuck in a local optimum is reduced (Applegate et
al., 2007, chapter 15).
As will be shown in the next section, phrase-
based SMT decoding can be directly reformulated
as an AGTSP. Here we use Concorde through
first transforming AGTSP into STSP, but it might
also be interesting in the future to use algorithms
specifically designed for AGTSP, which could im-
prove efficiency further (see Conclusion).
4 Phrase-based Decoding as TSP
In this section we reformulate the SMT decoding
problem as an AGTSP. We will illustrate the ap-
proach through a simple example: translating the
French sentence ?cette traduction automatique est
curieuse? into English. We assume that the rele-
vant biphrases for translating the sentence are as
follows:
ID source target
h cette this
t traduction translation
ht cette traduction this translation
mt traduction automatique machine translation
a automatique automatic
m automatique machine
i est is
s curieuse strange
c curieuse curious
Under this model, we can produce, among others,
the following translations:
h ? mt ? i ? s this machine translation is strange
h ? c ? t ? i ? a this curious translation is automatic
ht ? s ? i ? a this translation strange is automatic
where we have indicated on the left the ordered se-
quence of biphrases that leads to each translation.
We now formulate decoding as an AGTSP, in
the following way. The graph nodes are all the
possible pairs (w, b), where w is a source word in
the source sentence s and b is a biphrase contain-
ing this source word. The graph clusters are the
subsets of the graph nodes that share a common
source word w.
The costs of a transition between nodes M and
N of the graph are defined as follows:
(a) If M is of the form (w, b) and N of the form
(w?, b), in which b is a single biphrase, and w and
w? are consecutive words in b, then the transition
cost is 0: once we commit to using the first word
of b, there is no additional cost for traversing the
other source words covered by b.
(b) If M = (w, b), where w is the rightmost
source word in the biphrase b, and N = (w?, b?),
where w? 6= w is the leftmost source word in b?,
then the transition cost corresponds to the cost
of selecting b? just after b; this will correspond
to ?consuming? the source side of b? after having
consumed the source side of b (whatever their rel-
ative positions in the source sentence), and to pro-
ducing the target side of b? directly after the target
side of b; the transition cost is then the addition of
several contributions (weighted by their respective
? (not shown), as in equation 1):
? The cost associated with the features local to
b in the biphrase library;
? The ?distortion? cost of consuming the
source word w? just after the source word w:
|pos(w?) ? pos(w) ? 1|, where pos(w) and
pos(w?) are the positions of w and w? in the
source sentence.
? The language model cost of producing the
target words of b? right after the target words
of b; with a bigram language model, this cost
can be precomputed directly from b and b?.
This restriction to bigram models will be re-
moved in Section 4.1.
(c) In all other cases, the transition cost is infinite,
or, in other words, there is no edge in the graph
between M and N .
A special cluster containing a single node (de-
noted by $-$$ in the figures), and corresponding to
special beginning-of-sentence symbols must also
be included: the corresponding edges and weights
can be worked out easily. Figures 2 and 3 give
some illustrations of what we have just described.
4.1 From Bigram to N-gram LM
Successful phrase-based systems typically employ
language models of order higher than two. How-
ever, our models so far have the following impor-
tant ?Markovian? property: the cost of a path is
additive relative to the costs of transitions. For
example, in the example of Figure 3, the cost of
this ? machine translation ? is ? strange, can only
take into account the conditional probability of the
word strange relative to the word is, but not rela-
tive to the words translation and is. If we want to
extend the power of the model to general n-gram
language models, and in particular to the 3-gram
336
Figure 2: Transition graph for the source sentence
cette traduction automatique est curieuse. Only
edges entering or exiting the node traduction ? mt
are shown. The only successor to [traduction ?
mt] is [automatique ? mt], and [cette ? ht] is not a
predecessor of [traduction ? mt].
Figure 3: A GTSP tours is illustrated, correspond-
ing to the displayed output.
case (on which we concentrate here, but the tech-
niques can be easily extended to the general case),
the following approach can be applied.
Compiling Out for Trigram models
This approach consists in ?compiling out? all
biphrases with a target side of only one word.
We replace each biphrase b with single-word tar-
get side by ?extended? biphrases b1, . . . , br, which
are ?concatenations? of b and some other biphrase
b? in the library.2 To give an example, consider
that we: (1) remove from the biphrase library the
biphrase i, which has a single word target, and (2)
add to the library the extended biphrases mti, ti,
si, . . ., that is, all the extended biphrases consist-
ing of the concatenation of a biphrase in the library
with i, then it is clear that these extended biphrases
will provide enough context to compute a trigram
probability for the target word produced immedi-
ately next (in the examples, for the words strange,
2In the figures, such ?concatenations? are denoted by
[b? ? b] ; they are interpreted as encapsulations of first con-
suming the source side of b?, whether or not this source side
precedes the source side of b in the source sentence, produc-
ing the target side of b?, consuming the source side of b, and
producing the target side of b immediately after that of b?.
Figure 4: Compiling-out of biphrase i: (est,is).
automatic and automatic respectively). If we do
that exhaustively for all biphrases (relevant for the
source sentence at hand) that, like i, have a single-
word target, we will obtain a representation that
allows a trigram language model to be computed
at each point.
The situation becomes clearer by looking at Fig-
ure 4, where we have only eliminated the biphrase
i, and only shown some of the extended biphrases
that now encapsulate i, and where we show one
valid circuit. Note that we are now able to as-
sociate with the edge connecting the two nodes
(est,mti) and (curieuse, s) a trigram cost because
mti provides a large enough target context.
While this exhaustive ?compiling out? method
works in principle, it has a serious defect: if for
the sentence to be translated, there are m relevant
biphrases, among which k have single-word tar-
gets, then we will create on the order of km ex-
tended biphrases, which may represent a signif-
icant overhead for the TSP solver, as soon as k
is large relative to m, which is typically the case.
The problem becomes even worse if we extend the
compiling-out method to n-gram language models
with n > 3. In the Future Work section below,
we describe a powerful approach for circumvent-
ing this problem, but with which we have not ex-
perimented yet.
5 Experiments
5.1 Monolingual word re-ordering
In the first series of experiments we consider the
artificial task of reconstructing the original word
order of a given English sentence. First, we ran-
domly permute words in the sentence, and then
we try to reconstruct the original order by max-
337
100 102 104
?0.8
?0.6
?0.4
?0.2
0
0.2
Time (sec)
D
ec
od
er
 s
co
re
BEAM?SEARCH
TSP
100 102 104
?0.4
?0.3
?0.2
?0.1
0
0.1
Time (sec)
D
ec
od
er
 s
co
re
BEAM?SEARCH
TSP
(a) (b) (c) (d)
Figure 5: (a), (b): LM and BLEU scores as functions of time for a bigram LM; (c), (d): the same for
a trigram LM. The x axis corresponds to the cumulative time for processing the test set; for (a) and (c),
the y axis corresponds to the mean difference (over all sentences) between the lm score of the output
and the lm score of the reference normalized by the sentence length N: (LM(ref)-LM(true))/N. The solid
line with star marks corresponds to using beam-search with different pruning thresholds, which result in
different processing times and performances. The cross corresponds to using the exact-TSP decoder (in
this case the time to the optimal solution is not under the user?s control).
imizing the LM score over all possible permuta-
tions. The reconstruction procedure may be seen
as a translation problem from ?Bad English? to
?Good English?. Usually the LM score is used
as one component of a more complex decoder
score which also includes biphrase and distortion
scores. But in this particular ?translation task?
from bad to good English, we consider that all
?biphrases? are of the form e ? e, where e is an
English word, and we do not take into account
any distortion: we only consider the quality of
the permutation as it is measured by the LM com-
ponent. Since for each ?source word? e, there is
exactly one possible ?biphrase? e ? e each clus-
ter of the Generalized TSP representation of the
decoding problem contains exactly one node; in
other terms, the Generalized TSP in this situation
is simply a standard TSP. Since the decoding phase
is then equivalent to a word reordering, the LM
score may be used to compare the performance
of different decoding algorithms. Here, we com-
pare three different algorithms: classical beam-
search (Moses); a decoder based on an exact TSP
solver (Concorde); a decoder based on an approx-
imate TSP solver (Lin-Kernighan as implemented
in the Concorde solver) 3. In the Beam-search
and the LK-based TSP solver we can control the
trade-off between approximation quality and run-
ning time. To measure re-ordering quality, we use
two scores. The first one is just the ?internal? LM
score; since all three algorithms attempt to maxi-
mize this score, a natural evaluation procedure is
to plot its value versus the elapsed time. The sec-
3Both TSP decoders may be used with/or without a distor-
tion limit; in our experiments we do not use this parameter.
ond score is BLEU (Papineni et al, 2001), com-
puted between the reconstructed and the original
sentences, which allows us to check how well the
quality of reconstruction correlates with the inter-
nal score. The training dataset for learning the LM
consists of 50000 sentences from NewsCommen-
tary corpus (Callison-Burch et al, 2008), the test
dataset for word reordering consists of 170 sen-
tences, the average length of test sentences is equal
to 17 words.
Bigram based reordering. First we consider
a bigram Language Model and the algorithms try
to find the re-ordering that maximizes the LM
score. The TSP solver used here is exact, that is,
it actually finds the optimal tour. Figures 5(a,b)
present the performance of the TSP and Beam-
search based methods.
Trigram based reordering. Then we consider
a trigram based Language Model and the algo-
rithms again try to maximize the LM score. The
trigram model used is a variant of the exhaustive
compiling-out procedure described in Section 4.1.
Again, we use an exact TSP solver.
Looking at Figure 5a, we see a somewhat sur-
prising fact: the cross and some star points have
positive y coordinates! This means that, when us-
ing a bigram language model, it is often possible
to reorder the words of a randomly permuted ref-
erence sentence in such a way that the LM score
of the reordered sentence is larger than the LM of
the reference. A second notable point is that the
increase in the LM-score of the beam-search with
time is steady but very slow, and never reaches the
level of performance obtained with the exact-TSP
procedure, even when increasing the time by sev-
338
eral orders of magnitude. Also to be noted is that
the solution obtained by the exact-TSP is provably
the optimum, which is almost never the case of
the beam-search procedure. In Figure 5b, we re-
port the BLEU score of the reordered sentences
in the test set relative to the original reference
sentences. Here we see that the exact-TSP out-
puts are closer to the references in terms of BLEU
than the beam-search solutions. Although the TSP
output does not recover the reference sentences
(it produces sentences with a slightly higher LM
score than the references), it does reconstruct the
references better than the beam-search. The ex-
periments with trigram language models (Figures
5(c,d)) show similar trends to those with bigrams.
5.2 Translation experiments with a bigram
language model
In this section we consider two real translation
tasks, namely, translation from English to French,
trained on Europarl (Koehn et al, 2003) and trans-
lation from German to Spanish training on the
NewsCommentary corpus. For Europarl, the train-
ing set includes 2.81 million sentences, and the
test set 500. For NewsCommentary the training
set is smaller: around 63k sentences, with a test
set of 500 sentences. Figure 6 presents Decoder
and Bleu scores as functions of time for the two
corpuses.
Since in the real translation task, the size of the
TSP graph is much larger than in the artificial re-
ordering task (in our experiments the median size
of the TSP graph was around 400 nodes, some-
times growing up to 2000 nodes), directly apply-
ing the exact TSP solver would take too long; in-
stead we use the approximate LK algorithm and
compare it to Beam-Search. The efficiency of the
LK algorithm can be significantly increased by us-
ing a good initialization. To compare the quality of
the LK and Beam-Search methods we take a rough
initial solution produced by the Beam-Search al-
gorithm using a small value for the stack size and
then use it as initial point, both for the LK algo-
rithm and for further Beam-Search optimization
(where as before we vary the Beam-Search thresh-
olds in order to trade quality for time).
In the case of the Europarl corpus, we observe
that LK outperforms Beam-Search in terms of the
Decoder score as well as in terms of the BLEU
score. Note that the difference between the two al-
gorithms increases steeply at the beginning, which
means that we can significantly increase the qual-
ity of the Beam-Search solution by using the LK
algorithm at a very small price. In addition, it is
important to note that the BLEU scores obtained in
these experiments correspond to feature weights,
in the log-linear model (1), that have been opti-
mized for the Moses decoder, but not for the TSP
decoder: optimizing these parameters relatively to
the TSP decoder could improve its BLEU scores
still further.
On the News corpus, again, LK outperforms
Beam-Search in terms of the Decoder score. The
situation with the BLEU score is more confuse.
Both algorithms do not show any clear score im-
provement with increasing running time which
suggests that the decoder?s objective function is
not very well correlated with the BLEU score on
this corpus.
6 Future Work
In section 4.1, we described a general ?compiling
out? method for extending our TSP representation
to handling trigram and N-gram language models,
but we noted that the method may lead to combi-
natorial explosion of the TSP graph. While this
problem was manageable for the artificial mono-
lingual word re-ordering (which had only one pos-
sible translation for each source word), it be-
comes unwieldy for the real translation experi-
ments, which is why in this paper we only consid-
ered bigram LMs for these experiments. However,
we know how to handle this problem in principle,
and we now describe a method that we plan to ex-
periment with in the future.
To avoid the large number of artificial biphrases
as in 4.1, we perform an adaptive selection. Let us
suppose that (w, b) is a SMT decoding graph node,
where b is a biphrase containing only one word on
the target side. On the first step, when we evaluate
the traveling cost from (w, b) to (w?, b?), we take
the language model component equal to
min
b?? 6=b?,b
? log p(b?.v|b.e, b??.e),
where b?.v represents the first word of the b? tar-
get side, b.e is the only word of the b target
side, and b??.e is the last word of the b?? tar-
get size. This procedure underestimates the total
cost of tour passing through biphrases that have a
single-word target. Therefore if the optimal tour
passes only through biphrases with more than one
339
103 104 105
?273
?272.5
?272
?271.5
?271
Time (sec)
D
ec
od
er
 s
co
re
BEAM?SEARCH
TSP (LK)
103 104 105
0.18
0.185
0.19
Time (sec)
BL
EU
 s
co
re
BEAM?SEARCH
TSP (LK)
103 104
?414
?413.8
?413.6
?413.4
?413.2
?413
Time (sec)
D
ec
od
er
 s
co
re
TSP (LK)
BEAM?SEARCH
103 104
0.242
0.243
0.244
0.245
Time (sec)
BL
EU
 s
co
re
TSP (LK)
BEAM?SEARCH
(a) (b) (c) (d)
Figure 6: (a), (b): Europarl corpus, translation from English to French; (c),(d): NewsCommentary cor-
pus, translation from German to Spanish. Average value of the decoder and the BLEU scores (over 500
test sentences) as a function of time. The trade-off quality/time in the case of LK is controlled by the
number of iterations, and each point corresponds to a particular number of iterations, in our experiments
LK was run with a number of iterations varying between 2k and 170k. The same trade-off in the case of
Beam-Search is controlled by varying the beam thresholds.
word on their target side, then we are sure that
this tour is also optimal in terms of the tri-gram
language model. Otherwise, if the optimal tour
passes through (w, b), where b is a biphrase hav-
ing a single-word target, we add only the extended
biphrases related to b as we described in section
4.1, and then we recompute the optimal tour. Iter-
ating this procedure provably converges to an op-
timal solution.
This powerful method, which was proposed in
(Kam and Kopec, 1996; Popat et al, 2001) in the
context of a finite-state model (but not of TSP),
can be easily extended to N-gram situations, and
typically converges in a small number of itera-
tions.
7 Conclusion
The main contribution of this paper has been to
propose a transformation for an arbitrary phrase-
based SMT decoding instance into a TSP instance.
While certain similarities of SMT decoding and
TSP were already pointed out in (Knight, 1999),
where it was shown that any Traveling Salesman
Problem may be reformulated as an instance of
a (simplistic) SMT decoding task, and while cer-
tain techniques used for TSP were then adapted to
word-based SMT decoding (Germann et al, 2001;
Tillmann and Ney, 2003; Tillmann, 2006), we are
not aware of any previous work that shows that
SMT decoding can be directly reformulated as a
TSP. Beside the general interest of this transfor-
mation for understanding decoding, it also opens
the door to direct application of the variety of ex-
isting TSP algorithms to SMT. Our experiments
on synthetic and real data show that fast TSP al-
gorithms can handle selection and reordering in
SMT comparably or better than the state-of-the-
art beam-search strategy, converging on solutions
with higher objective function in a shorter time.
The proposed method proceeds by first con-
structing an AGTSP instance from the decoding
problem, and then converting this instance first
into ATSP and finally into STSP. At this point, a
direct application of the well known STSP solver
Concorde (with Lin-Kernighan heuristic) already
gives good results. We believe however that there
might exist even more efficient alternatives. In-
stead of converting the AGTSP instance into a
STSP instance, it might prove better to use di-
rectly algorithms expressly designed for ATSP
or AGTSP. For instance, some of the algorithms
tested in the context of the DIMACS implemen-
tation challenge for ATSP (Johnson et al, 2002)
might well prove superior. There is also active re-
search around AGTSP algorithms. Recently new
effective methods based on a ?memetic? strategy
(Buriol et al, 2004; Gutin et al, 2008) have been
put forward. These methods combined with our
proposed formulation provide ready-to-use SMT
decoders, which it will be interesting to compare.
Acknowledgments
Thanks to Vassilina Nikoulina for her advice about
running Moses on the test datasets.
340
References
David L. Applegate, Robert E. Bixby, Vasek Chvatal,
and William J. Cook. 2005. Concorde
tsp solver. http://www.tsp.gatech.edu/
concorde.html.
David L. Applegate, Robert E. Bixby, Vasek Chvatal,
and William J. Cook. 2007. The Traveling Sales-
man Problem: A Computational Study (Princeton
Series in Applied Mathematics). Princeton Univer-
sity Press, January.
Luciana Buriol, Paulo M. Franc?a, and Pablo Moscato.
2004. A new memetic algorithm for the asymmetric
traveling salesman problem. Journal of Heuristics,
10(5):483?506.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Josh Schroeder, and Cameron Shaw Fordyce, edi-
tors. 2008. Proceedings of the Third Workshop on
SMT. ACL, Columbus, Ohio, June.
Ulrich Germann, Michael Jahr, Kevin Knight, and
Daniel Marcu. 2001. Fast decoding and optimal
decoding for machine translation. In In Proceedings
of ACL 39, pages 228?235.
Gregory Gutin, Daniel Karapetyan, and Krasnogor Na-
talio. 2008. Memetic algorithm for the generalized
asymmetric traveling salesman problem. In NICSO
2007, pages 199?210. Springer Berlin.
G. Gutin. 2003. Travelling salesman and related prob-
lems. In Handbook of Graph Theory.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses decoder for statistical machine translation. In
ACL 2008 Software workshop, pages 58?65, Colum-
bus, Ohio, June. ACL.
D.S. Johnson, G. Gutin, L.A. McGeoch, A. Yeo,
W. Zhang, and A. Zverovich. 2002. Experimen-
tal analysis of heuristics for the atsp. In The Trav-
elling Salesman Problem and Its Variations, pages
445?487.
Anthony C. Kam and Gary E. Kopec. 1996. Document
image decoding by heuristic search. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
18:945?950.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25:607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL 2003, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Adam Lopez. 2008. Statistical machine translation.
ACM Comput. Surv., 40(3):1?49.
C. Noon and J.C. Bean. 1993. An efficient transforma-
tion of the generalized traveling salesman problem.
INFOR, pages 39?44.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei J. Zhu. 2001. BLEU: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report, RC22176.
Kris Popat, Daniel H. Greene, Justin K. Romberg, and
Dan S. Bloomberg. 2001. Adding linguistic con-
straints to document image decoding: Comparing
the iterated complete path and stack algorithms.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search
algorithm for statistical machine translation. Com-
put. Linguist., 29(1):97?133.
Christoph Tillmann. 2006. Efficient Dynamic Pro-
gramming Search Algorithms For Phrase-Based
SMT. In Workshop On Computationally Hard Prob-
lems And Joint Inference In Speech And Language
Processing.
Wikipedia. 2009. Travelling Salesman Problem ?
Wikipedia, The Free Encyclopedia. [Online; ac-
cessed 5-May-2009].
341
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 791?799,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Source-Language Entailment Modeling for Translating Unknown Terms
Shachar Mirkin?, Lucia Specia?, Nicola Cancedda?, Ido Dagan?, Marc Dymetman?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Xerox Research Centre Europe
{mirkins,dagan,szpekti}@cs.biu.ac.il
{lucia.specia,nicola.cancedda,marc.dymetman}@xrce.xerox.com
Abstract
This paper addresses the task of handling
unknown terms in SMT. We propose us-
ing source-language monolingual models
and resources to paraphrase the source text
prior to translation. We further present a
conceptual extension to prior work by al-
lowing translations of entailed texts rather
than paraphrases only. A method for
performing this process efficiently is pre-
sented and applied to some 2500 sentences
with unknown terms. Our experiments
show that the proposed approach substan-
tially increases the number of properly
translated texts.
1 Introduction
Machine Translation systems frequently encounter
terms they are not able to translate due to some
missing knowledge. For instance, a Statistical Ma-
chine Translation (SMT) system translating the
sentence ?Cisco filed a lawsuit against Apple for
patent violation? may lack words like filed and
lawsuit in its phrase table. The problem is espe-
cially severe for languages for which parallel cor-
pora are scarce, or in the common scenario when
the SMT system is used to translate texts of a do-
main different from the one it was trained on.
A previously suggested solution (Callison-
Burch et al, 2006) is to learn paraphrases of
source terms from multilingual (parallel) corpora,
and expand the phrase table with these para-
phrases1. Such solutions could potentially yield a
paraphrased sentence like ?Cisco sued Apple for
patent violation?, although their dependence on
bilingual resources limits their utility.
In this paper we propose an approach that con-
sists in directly replacing unknown source terms,
1As common in the literature, we use the term para-
phrases to refer to texts of equivalent meaning, of any length
from single words (synonyms) up to complete sentences.
using source-language resources and models in or-
der to achieve two goals.
The first goal is coverage increase. The avail-
ability of bilingual corpora, from which para-
phrases can be learnt, is in many cases limited.
On the other hand, monolingual resources and
methods for extracting paraphrases from monolin-
gual corpora are more readily available. These
include manually constructed resources, such as
WordNet (Fellbaum, 1998), and automatic meth-
ods for paraphrases acquisition, such as DIRT (Lin
and Pantel, 2001). However, such resources have
not been applied yet to the problem of substitut-
ing unknown terms in SMT. We suggest that by
using such monolingual resources we could pro-
vide paraphrases for a larger number of texts with
unknown terms, thus increasing the overall cover-
age of the SMT system, i.e. the number of texts it
properly translates.
Even with larger paraphrase resources, we may
encounter texts in which not all unknown terms are
successfully handled through paraphrasing, which
often results in poor translations (see Section 2.1).
To further increase coverage, we therefore pro-
pose to generate and translate texts that convey a
somewhat more general meaning than the original
source text. For example, using such approach,
the following text could be generated: ?Cisco ac-
cused Apple of patent violation?. Although less in-
formative than the original, a translation for such
texts may be useful. Such non-symmetric relation-
ships (as between filed a lawsuit and accused) are
difficult to learn from parallel corpora and there-
fore monolingual resources are more appropriate
for this purpose.
The second goal we wish to accomplish by
employing source-language resources is to rank
the alternative generated texts. This goal can be
achieved by using context-models on the source
language prior to translation. This has two advan-
tages. First, the ranking allows us to prune some
791
candidates before supplying them to the transla-
tion engine, thus improving translation efficiency.
Second, the ranking may be combined with target
language information in order to choose the best
translation, thus improving translation quality.
We position the problem of generating alterna-
tive texts for translation within the Textual Entail-
ment (TE) framework (Giampiccolo et al, 2007).
TE provides a generic way for handling language
variability, identifying when the meaning of one
text is entailed by the other (i.e. the meaning of
the entailed text can be inferred from the mean-
ing of the entailing one). When the meanings of
two texts are equivalent (paraphrase), entailment
is mutual. Typically, a more general version of
a certain text is entailed by it. Hence, through TE
we can formalize the generation of both equivalent
and more general texts for the source text. When
possible, a paraphrase is used. Otherwise, an alter-
native text whose meaning is entailed by the orig-
inal source is generated and translated.
We assess our approach by applying an SMT
system to a text domain that is different from the
one used to train the system. We use WordNet
as a source language resource for entailment rela-
tionships and several common statistical context-
models for selecting the best generated texts to be
sent to translation. We show that the use of source
language resources, and in particular the extension
to non-symmetric textual entailment relationships,
is useful for substantially increasing the amount of
texts that are properly translated. This increase is
observed relative to both using paraphrases pro-
duced by the same resource (WordNet) and us-
ing paraphrases produced from multilingual paral-
lel corpora. We demonstrate that by using simple
context-models on the source, efficiency can be
improved, while translation quality is maintained.
We believe that with the use of more sophisticated
context-models further quality improvement can
be achieved.
2 Background
2.1 Unknown Terms
A very common problem faced by machine trans-
lation systems is the need to translate terms (words
or multi-word expressions) that are not found in
the system?s lexicon or phrase table. The reasons
for such unknown terms in SMT systems include
scarcity of training material and the application
of the system to text domains that differ from the
ones used for training.
In SMT, when unknown terms are found in the
source text, the systems usually omit or copy them
literally into the target. Though copying the source
words can be of some help to the reader if the
unknown word has a cognate in the target lan-
guage, this will not happen in the most general
scenario where, for instance, languages use dif-
ferent scripts. In addition, the presence of a sin-
gle unknown term often affects the translation of
wider portions of text, inducing errors in both lex-
ical selection and ordering. This phenomenon is
demonstrated in the following sentences, where
the translation of the English sentence (1) is ac-
ceptable only when the unknown word (in bold) is
replaced with a translatable paraphrase (3):
1. ?. . . , despite bearing the heavy burden of the
unemployed 10% or more of the labor force.?
2. ?. . . , malgre? la lourde charge de compte le
10% ou plus de cho?meurs labor la force .?
3. ?. . . , malgre? la lourde charge des cho?meurs
de 10% ou plus de la force du travail.?
Several approaches have been proposed to deal
with unknown terms in SMT systems, rather than
omitting or copying the terms. For example, (Eck
et al, 2008) replace the unknown terms in the
source text by their definition in a monolingual
dictionary, which can be useful for gisting. To
translate across languages with different alpha-
bets approaches such as (Knight and Graehl, 1997;
Habash, 2008) use transliteration techniques to
tackle proper nouns and technical terms. For trans-
lation from highly inflected languages, certain ap-
proaches rely on some form of lexical approx-
imation or morphological analysis (Koehn and
Knight, 2003; Yang and Kirchhoff, 2006; Langlais
and Patry, 2007; Arora et al, 2008). Although
these strategies yield gain in coverage and transla-
tion quality, they only account for unknown terms
that should be transliterated or are variations of
known ones.
2.2 Paraphrasing in MT
A recent strategy to broadly deal with the prob-
lem of unknown terms is to paraphrase the source
text with terms whose translation is known to
the system, using paraphrases learnt from multi-
lingual corpora, typically involving at least one
?pivot? language different from the target lan-
guage of immediate interest (Callison-Burch et
792
al., 2006; Cohn and Lapata, 2007; Zhao et al,
2008; Callison-Burch, 2008; Guzma?n and Gar-
rido, 2008). The procedure to extract paraphrases
in these approaches is similar to standard phrase
extraction in SMT systems, and therefore a large
amount of additional parallel corpus is required.
Moreover, as discussed in Section 5, when un-
known texts are not from the same domain as the
SMT training corpus, it is likely that paraphrases
found through such methods will yield misleading
translations.
Bond et al (2008) use grammars to paraphrase
the whole source sentence, covering aspects like
word order and minor lexical variations (tenses
etc.), but not content words. The paraphrases are
added to the source side of the corpus and the cor-
responding target sentences are duplicated. This,
however, may yield distorted probability estimates
in the phrase table, since these were not computed
from parallel data.
The main use of monolingual paraphrases in
MT to date has been for evaluation. For exam-
ple, (Kauchak and Barzilay, 2006) paraphrase ref-
erences to make them closer to the system transla-
tion in order to obtain more reliable results when
using automatic evaluation metrics like BLEU
(Papineni et al, 2002).
2.3 Textual Entailment and Entailment Rules
Textual Entailment (TE) has recently become a
prominent paradigm for modeling semantic infer-
ence, capturing the needs of a broad range of
text understanding applications (Giampiccolo et
al., 2007). Yet, its application to SMT has been so
far limited to MT evaluation (Pado et al, 2009).
TE defines a directional relation between two
texts, where the meaning of the entailed text (hy-
pothesis, h) can be inferred from the meaning of
the entailing text, t. Under this paradigm, para-
phrases are a special case of the entailment rela-
tion, when the relation is symmetric (the texts en-
tail each other). Otherwise, we say that one text
directionally entails the other.
A common practice for proving (or generating)
h from t is to apply entailment rules to t. An
entailment rule, denoted LHS ? RHS, specifies
an entailment relation between two text fragments
(the Left- and Right- Hand Sides), possibly with
variables (e.g. build X in Y ? X is completed
in Y ). A paraphrasing rule is denoted with ?.
When a rule is applied to a text, a new text is in-
ferred, where the matched LHS is replaced with the
RHS. For example, the rule skyscraper? building
is applied to ?The world?s tallest skyscraper was
completed in Taiwan? to infer ?The world?s tallest
building was completed in Taiwan?. In this work,
we employ lexical entailment rules, i.e. rules with-
out variables. Various resources for lexical rules
are available, and the prominent one is WordNet
(Fellbaum, 1998), which has been used in virtu-
ally all TE systems (Giampiccolo et al, 2007).
Typically, a rule application is valid only under
specific contexts. For example, mouse ? rodent
should not be applied to ?Use the mouse to mark
your answers?. Context-models can be exploited
to validate the application of a rule to a text. In
such models, an explicit Word Sense Disambigua-
tion (WSD) is not necessarily required; rather, an
implicit sense-match is sought after (Dagan et al,
2006). Within the scope of our paper, rule ap-
plication is handled similarly to Lexical Substitu-
tion (McCarthy and Navigli, 2007), considering
the contextual relationship between the text and
the rule. However, in general, entailment rule ap-
plication addresses other aspects of context match-
ing as well (Szpektor et al, 2008).
3 Textual Entailment for Statistical
Machine Translation
Previous solutions for handling unknown terms in
a source text s augment the SMT system?s phrase
table based on multilingual corpora. This allows
indirectly paraphrasing s, when the SMT system
chooses to use a paraphrase included in the table
and produces a translation with the corresponding
target phrase for the unknown term.
We propose using monolingual paraphrasing
methods and resources for this task to obtain a
more extensive set of rules for paraphrasing the
source. These rules are then applied to s directly
to produce alternative versions of the source text
prior to the translation step. Moreover, further
coverage increase can be achieved by employing
directional entailment rules, when paraphrasing is
not possible, to generate more general texts for
translation.
Our approach, based on the textual entailment
framework, considers the newly generated texts as
entailed from the original one. Monolingual se-
mantic resources such as WordNet can provide en-
tailment rules required for both these symmetric
and asymmetric entailment relations.
793
Input: A text t with one or more unknown terms;
a monolingual resource of entailment rules;
k - maximal number of source alternatives to produce
Output: A translation of either (in order of preference):
a paraphrase of t OR a text entailed by t OR t itself
1. For each unknown term - fetch entailment rules:
(a) Fetch rules for paraphrasing; disregard rules
whose RHS is not in the phrase table
(b) If the set of rules is empty: fetch directional en-
tailment rules; disregard rules whose RHS is not
in the phrase table
2. Apply a context-model to compute a score for each rule
application
3. Compute total source score for each entailed text as a
combination of individual rule scores
4. Generate and translate the top-k entailed texts
5. If k > 1
(a) Apply target model to score the translation
(b) Compute final source-target score
6. Pick highest scoring translation
Figure 1: Scheme for handling unknown terms by using
monolingual resources through textual entailment
Through the process of applying entailment
rules to the source text, multiple alternatives of
entailed texts are generated. To rank the candi-
date texts we employ monolingual context-models
to provide scores for rule applications over the
source sentence. This can be used to (a) directly
select the text with the highest score, which can
then be translated, or (b) to select a subset of top
candidates to be translated, which will then be
ranked using the target language information as
well. This pruning reduces the load of the SMT
system, and allows for potential improvements in
translation quality by considering both source- and
target-language information.
The general scheme through which we achieve
these goals, which can be implemented using dif-
ferent context-models and scoring techniques, is
detailed in Figure 1. Details of our concrete im-
plementation are given in Section 4.
Preliminary analysis confirmed (as expected)
that readers prefer translations of paraphrases,
when available, over translations of directional en-
tailments. This consideration is therefore taken
into account in the proposed method.
The input is a text unit to be translated, such as a
sentence or paragraph, with one or more unknown
terms. For each unknown term we first fetch a
list of candidate rules for paraphrasing (e.g. syn-
onyms), where the unknown term is the LHS. For
example, if our unknown term is dodge, a possi-
ble candidate might be dodge ? circumvent. We
inflect the RHS to keep the original morphologi-
cal information of the unknown term and filter out
rules where the inflected RHS does not appear in
the phrase table (step 1a in Figure 1).
When no applicable rules for paraphrasing are
available (1b), we fetch directional entailment
rules (e.g. hypernymy rules such as dodge ?
avoid), and filter them in the same way as for para-
phrasing rules. To each set of rules for a given un-
known term we add the ?identity-rule?, to allow
leaving the unknown term unchanged, the correct
choice in cases of proper names, for example.
Next, we apply a context-model to compute an
applicability score of each rule to the source text
(step 2). An entailed text?s total score is the com-
bination (e.g. product, see Section 4) of the scores
of the rules used to produce it (3). A set of the
top-k entailed texts is then generated and sent for
translation (4).
If more than one alternative is produced by the
source model (and k > 1), a target model is ap-
plied on the selected set of translated texts (5a).
The combined source-target model score is a com-
bination of the scores of the source and target
models (5b). The final translation is selected to be
the one that yields the highest combined source-
target score (6). Note that setting k = 1 is equiva-
lent to using the source-language model alone.
Our algorithm validates the application of the
entailment rules at two stages ? before and af-
ter translation, through context-models applied at
each end. As the experiments will show in Sec-
tion 4, a large number of possible combinations of
entailment rules is a common scenario, and there-
fore using the source context models to reduce this
number plays an important role.
4 Experimental Setting
To assess our approach, we conducted a series of
experiments; in each experiment we applied the
scheme described in 3, changing only the mod-
els being used for scoring the generated and trans-
lated texts. The setting of these experiments is de-
scribed in what follows.
SMT data To produce sentences for our experi-
ments, we use Matrax (Simard et al, 2005), a stan-
dard phrase-based SMT system, with the excep-
tion that it allows gaps in phrases. We use approxi-
mately 1M sentence pairs from the English-French
794
Europarl corpus for training, and then translate a
test set of 5,859 English sentences from the News
corpus into French. Both resources are taken
from the shared translation task in WMT-2008
(Callison-Burch et al, 2008). Hence, we compare
our method in a setting where the training and test
data are from different domains, a common sce-
nario in the practical use of MT systems.
Of the 5,859 translated sentences, 2,494 contain
unknown terms (considering only sequences with
alphabetic symbols), summing up to 4,255 occur-
rences of unknown terms. 39% of the 2,494 sen-
tences contain more than a single unknown term.
Entailment resource We use WordNet 3.0 as
a resource for entailment rules. Paraphrases are
generated using synonyms. Directionally entailed
texts are created using hypernyms, which typically
conform with entailment. We do not rely on sense
information in WordNet. Hence, any other seman-
tic resource for entailment rules can be utilized.
Each sentence is tagged using the OpenNLP
POS tagger2. Entailment rules are applied for un-
known terms tagged as nouns, verbs, adjectives
and adverbs. The use of relations from WordNet
results in 1,071 sentences with applicable rules
(with phrase table entries) for the unknown terms
when using synonyms, and 1,643 when using both
synonyms and hypernyms, accounting for 43%
and 66% of the test sentences, respectively.
The number of alternative sentences generated
for each source text varies from 1 to 960 when
paraphrasing rules were applied, and reaches very
large numbers, up to 89,700 at the ?worst case?,
when all TE rules are employed, an average of 456
alternatives per sentence.
Scoring source texts We test our proposed
method using several context-models shown to
perform reasonably well in previous work:
? FREQ: The first model we use is a context-
independent baseline. A common useful
heuristic to pick an entailment rule is to se-
lect the candidate with the highest frequency
in the corpus (Mccarthy et al, 2004). In this
model, a rule?s score is the normalized num-
ber of occurrences of its RHS in the training
corpus, ignoring the context of the LHS.
? LSA: Latent Semantic Analysis (Deerwester
et al, 1990) is a well-known method for rep-
2http://opennlp.sourceforge.net
resenting the contextual usage of words based
on corpus statistics. We represented each
term by a normalized vector of the top 100
SVD dimensions, as described in (Gliozzo,
2005). This model measures the similarity
between the sentence words and the RHS in
the LSA space.
? NB: We implemented the unsupervised
Na??ve Bayes model described in (Glickman
et al, 2006) to estimate the probability that
the unknown term entails the RHS in the
given context. The estimation is based on
corpus co-occurrence statistics of the context
words with the RHS.
? LMS: This model generates the Language
Model probability of the RHS in the source.
We use 3-grams probabilities as produced by
the SRILM toolkit (Stolcke, 2002).
Finally, as a simple baseline, we generated a ran-
dom score for each rule application, RAND.
The score of each rule application by any of
the above models is normalized to the range (0,1].
To combine individual rule applications in a given
sentence, we use the product of their scores. The
monolingual data used for the models above is the
source side of the training parallel corpus.
Target-language scores On the target side we
used either a standard 3-gram language-model, de-
noted LMT, or the score assigned by the com-
plete SMT log-linear model, which includes the
language model as one of its components (SMT).
A pair of a source:target models comprises a
complete model for selecting the best translated
sentence, where the overall score is the product of
the scores of the two models.
We also applied several combinations of source
models, such as LSA combined with LMS, to take
advantage of their complementary strengths. Ad-
ditionally, we assessed our method with source-
only models, by setting the number of sentences to
be selected by the source model to one (k = 1).
5 Results
5.1 Manual Evaluation
To evaluate the translations produced using the
various source and target models and the different
rule-sets, we rely mostly on manual assessment,
since automatic MT evaluation metrics like BLEU
do not capture well the type of semantic variations
795
Model
Precision (%) Coverage (%)
PARAPH. TE PARAPH. TE
1 ?:SMT 75.8 73.1 32.5 48.1
2 NB:SMT 75.2 71.5 32.3 47.1
3 LSA:SMT 74.9 72.4 32.1 47.7
4 NB:? 74.7 71.1 32.1 46.8
5 LMS:LMT 73.8 70.2 31.7 46.3
6 FREQ:? 72.5 68.0 31.2 44.8
7 RAND 57.2 63.4 24.6 41.8
Table 1: Translation acceptance when using only para-
phrases and when using all entailment rules. ?:? indicates
which model is applied to the source (left side) and which to
the target language (right side).
generated in our experiments, particularly at the
sentence level.
In the manual evaluation, two native speakers
of the target language judged whether each trans-
lation preserves the meaning of its reference sen-
tence, marking it as acceptable or unacceptable.
From the sentences for which rules were applica-
ble, we randomly selected a sample of sentences
for each annotator, allowing for some overlap-
ping for agreement analysis. In total, the transla-
tions of 1,014 unique source sentences were man-
ually annotated, of which 453 were produced us-
ing only hypernyms (no paraphrases were appli-
cable). When a sentence was annotated by both
annotators, one annotation was picked randomly.
Inter-annotator agreement was measured by the
percentage of sentences the annotators agreed on,
as well as via the Kappa measure (Cohen, 1960).
For different models, the agreement rate varied
from 67% to 78% (72% overall), and the Kappa
value ranged from 0.34 to 0.55, which is compa-
rable to figures reported for other standard SMT
evaluation metrics (Callison-Burch et al, 2008).
Translation with TE For each model m, we
measured Precisionm, the percentage of accept-
able translations out of all sampled translations.
Precisionm was measured both when using only
paraphrases (PARAPH.) and when using all entail-
ment rules (TE). We also measured Coveragem,
the percentage of sentences with acceptable trans-
lations, Am, out of all sentences (2,494). As
our annotators evaluated only a sample of sen-
tences, Am is estimated as the model?s total num-
ber of sentences with applicable rules, Sm, mul-
tiplied by the model?s Precision (Sm was 1,071
for paraphrases and 1,643 for entailment rules):
Coveragem = Sm?Precisionm2,494 .
Table 1 presents the results of several source-
target combinations when using only paraphrases
and when also using directional entailment rules.
When all rules are used, a substantial improve-
ment in coverage is consistently obtained across
all models, reaching a relative increase of 50%
over paraphrases only, while just a slight decrease
in precision is observed (see Section 5.3 for some
error analysis). This confirms our hypothesis that
directional entailment rules can be very useful for
replacing unknown terms.
For the combination of source-target models,
the value of k is set depending on which rule-set
is used. Preliminary analysis showed that k = 5
is sufficient when only paraphrases are used and
k = 20 when directional entailment rules are also
considered.
We measured statistical significance between
different models for precision of the TE re-
sults according to the Wilcoxon signed ranks test
(Wilcoxon, 1945). Models 1-6 in Table 1 are sig-
nificantly better than the RAND baseline (p <
0.03), and models 1-3 are significantly better than
model 6 (p < 0.05). The difference between
?:SMT and NB:SMT or LSA:SMT is not statisti-
cally significant.
The results in Table 1 therefore suggest that
taking a source model into account preserves the
quality of translation. Furthermore, the quality is
maintained even when source models? selections
are restricted to a rather small top-k ranks, at a
lower computational cost (for the models combin-
ing source and target, like NB:SMT or LSA:SMT).
This is particularly relevant for on-demand MT
systems, where time is an issue. For such systems,
using this source-language based pruning method-
ology will yield significant performance gains as
compared to target-only models.
We also evaluated the baseline strategy where
unknown terms are omitted from the translation,
resulting in 25% precision. Leaving unknown
words untranslated also yielded very poor transla-
tion quality in an analysis performed on a similar
dataset.
Comparison to related work We compared our
algorithm with an implementation of the algo-
rithm proposed by (Callison-Burch et al, 2006)
(see Section 2.2), henceforth CB, using the Span-
ish side of Europarl as the pivot language.
Out of the tested 2,494 sentences with unknown
terms, CB found paraphrases for 706 sentences
(28.3%), while with any of our models, including
796
Model Precision (%) Coverage (%) Better (%)
NB:SMT (TE) 85.3 56.2 72.7
CB 85.3 24.2 12.7
Table 2: Comparison between our top model and the
method by Callison-Burch et al (2006), showing the per-
centage of times translations were considered acceptable, the
model?s coverage and the percentage of times each model
scored better than the other (in the 14% remaining cases, both
models produced unacceptable translations).
NB:SMT , our algorithm found applicable entail-
ment rules for 1,643 sentences (66%).
The quality of the CB translations was manually
assessed for a sample of 150 sentences. Table 2
presents the precision and coverage on this sample
for both CB and NB:SMT , as well as the number
of times each model?s translation was preferred by
the annotators. While both models achieve equally
high precision scores on this sample, the NB:SMT
model?s translations were undoubtedly preferred
by the annotators, with a considerably higher cov-
erage.
With the CB method, given that many of the
phrases added to the phrase table are noisy, the
global quality of the sentences seem to have been
affected, explaining why the judges preferred the
NB:SMT translations. One reason for the lower
coverage of CB is the fact that paraphrases were
acquired from a corpus whose domain is differ-
ent from that of the test sentences. The entail-
ment rules in our models are not limited to para-
phrases and are derived from WordNet, which has
broader applicability. Hence, utilizing monolin-
gual resources has proven beneficial for the task.
5.2 Automatic MT Evaluation
Although automatic MT evaluation metrics are
less appropriate for capturing the variations gen-
erated by our method, to ensure that there was no
degradation in the system-level scores according
to such metrics we also measured the models? per-
formance using BLEU and METEOR (Agarwal
and Lavie, 2007). The version of METEOR we
used on the target language (French) considers the
stems of the words, instead of surface forms only,
but does not make use of WordNet synonyms.
We evaluated the performance of the top mod-
els of Table 1, as well as of a baseline SMT sys-
tem that left unknown terms untranslated, on the
sample of 1,014 manually annotated sentences. As
shown in Table 3, all models resulted in improve-
ment with respect to the original sentences (base-
Model BLEU (TE) METEOR (TE)
?:SMT 15.50 0.1325
NB:SMT 15.37 0.1316
LSA:SMT 15.51 0.1318
NB:? 15.37 0.1311
CB 15.33 0.1299
Baseline SMT 15.29 0.1294
Table 3: Performance of the best models according to auto-
matic MT evaluation metrics at the corpus level. The baseline
refers to translation of the text without applying any entail-
ment rules.
line). The difference in METEOR scores is statis-
tically significant (p < 0.05) for the three top mod-
els against the baseline. The generally low scores
may be attributed to the fact that training and test
sentences are from different domains.
5.3 Discussion
The use of entailed texts produced using our ap-
proach clearly improves the quality of translations,
as compared to leaving unknown terms untrans-
lated or omitting them altogether. While it is clear
that textual entailment is useful for increasing cov-
erage in translation, further research is required to
identify the amount of information loss incurred
when non-symmetric entailment relations are be-
ing used, and thus to identify the cases where such
relations are detrimental to translation.
Consider, for example, the sentence: ?Conven-
tional military models are geared to decapitate
something that, in this case, has no head.?. In this
sentence, the unknown term was replaced by kill,
which results in missing the point originally con-
veyed in the text. Accordingly, the produced trans-
lation does not preserve the meaning of the source,
and was considered unacceptable: ?Les mode`les
militaires visent a` faire quelque chose que, dans
ce cas, n?est pas responsable.?.
In other cases, the selected hypernyms were too
generic words, such as entity or attribute, which
also fail to preserve the sentence?s meaning. On
the other hand, when the unknown term was a
very specific word, hypernyms played an impor-
tant role. For example, ?Bulgaria is the most
sought-after east European real estate target, with
its low-cost ski chalets and oceanfront homes?.
Here, chalets are replaced by houses or units (de-
pending on the model), providing a translation that
would be acceptable by most readers.
Other incorrect translations occurred when the
unknown term was part of a phrase, for exam-
ple, troughs replaced with depressions in peaks
797
and troughs, a problem that also strongly affects
paraphrasing. In another case, movement was the
hypernym chosen to replace labor in labor move-
ment, yielding an awkward text for translation.
Many of the cases which involved ambiguity
were resolved by the applied context-models, and
can be further addressed, together with the above
mentioned problems, with better source-language
context models.
We suggest that other types of entailment rules
could be useful for the task beyond the straight-
forward generalization using hypernyms, which
was demonstrated in this work. This includes
other types of lexical entailment relations, such as
holonymy (e.g. Singapore ? Southeast Asia) as
well as lexical syntactic rules (X cure Y ? treat
Y with X). Even syntactic rules, such as clause re-
moval, can be recruited for the task: ?Obama, the
44th president, declared Monday . . . ?? ?Obama
declared Monday . . . ?. When the system is un-
able to translate a term found in the embedded
clause, the translation of the less informative sen-
tence may still be acceptable by readers.
6 Conclusions and Future Work
In this paper we propose a new entailment-based
approach for addressing the problem of unknown
terms in machine translation. Applying this ap-
proach with lexical entailment rules from Word-
Net, we show that using monolingual resources
and textual entailment relationships allows sub-
stantially increasing the quality of translations
produced by an SMT system. Our experiments
also show that it is possible to perform the process
efficiently by relying on source language context-
models as a filter prior to translation. This pipeline
maintains translation quality, as assessed by both
human annotators and standard automatic mea-
sures.
For future work we suggest generating entailed
texts with a more extensive set of rules, in particu-
lar lexical-syntactic ones. Combining rules from
monolingual and bilingual resources seems ap-
pealing as well. Developing better context-models
to be applied on the source is expected to further
improve our method?s performance. Specifically,
we suggest taking into account the prior likelihood
that a rule is correct as part of the model score.
Finally, some researchers have advocated re-
cently the use of shared structures such as parse
forests (Mi and Huang, 2008) or word lattices
(Dyer et al, 2008) in order to allow a compact rep-
resentation of alternative inputs to an SMT system.
This is an approach that we intend to explore in
future work, as a way to efficiently handle the dif-
ferent source language alternatives generated by
entailment rules. However, since most current MT
systems do not accept such type of inputs, we con-
sider the results on pruning by source-side context
models as broadly relevant.
Acknowledgments
This work was supported in part by the ICT Pro-
gramme of the European Community, under the
PASCAL 2 Network of Excellence, ICT-216886
and The Israel Science Foundation (grant No.
1112/08). We wish to thank Roy Bar-Haim and
the anonymous reviewers of this paper for their
useful feedback. This publication only reflects the
authors? views.
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of WMT-08.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of Unknown Words in Phrase-
Based Statistical Machine Translation for Lan-
guages of Rich Morphology. In Proceedings of
SLTU.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of IWSLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of WMT.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of ACL.
798
Ido Dagan, Oren Glickman, Alfio Massimiliano
Gliozzo, Efrat Marmorshtein, and Carlo Strappar-
ava. 2006. Direct Word Sense Matching for Lexical
Substitution. In Proceedings of ACL.
Scott Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R.A. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the American So-
ciety for Information Science, 41.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Trans-
lation. In Proceedings of ACL-HLT.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2008.
Communicating Unknown Words in Machine Trans-
lation. In Proceedings of LREC.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of ACL-WTEP Workshop.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy
Bengio, and Walter Daelemans. 2006. Investigat-
ing Lexical Substitution Scoring for Subtitle Gener-
ation. In Proceedings of CoNLL.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis, Univer-
sity of Trento.
Francisco Guzma?n and Leonardo Garrido. 2008.
Translation Paraphrases in Phrase-Based Machine
Translation. In Proceedings of CICLing.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-HLT.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating Unknown Words by Analogical Learning. In
Proceedings of EMNLP-CoNLL.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
SIGKDD.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Substitu-
tion Task. In Proceedings of SemEval.
Diana Mccarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding Predominant Word Senses
in Untagged Text. In Proceedings of ACL.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of
EMNLP.
Sebastian Pado, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of WMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL.
M. Simard, N. Cancedda, B. Cavestro, M. Dymet-
man, E. Gaussier, C. Goutte, and K. Yamada. 2005.
Translating with Non-contiguous Phrases. In Pro-
ceedings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual Preferences. In Pro-
ceedings of ACL-HLT.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-Based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of EACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-HLT.
799
In: Proceedings of CoNLL-2000 and LLL-2000, pages 7-12, Lisbon, Portugal, 2000. 
Corpus-Based Grammar Specialization 
Nico la  Cancedda and Chr i s te r  Samuelsson  
Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240, Meylan, France 
{Nicola. Cancedda, Christer. Samuelsson}@xrce. xerox, com 
Abst rac t  
Broad-coverage rammars tend to be highly am- 
biguous. When such grammars are used in a 
restricted omain, it may be desirable to spe- 
cialize them, in effect trading some coverage for 
a reduction in ambiguity. Grammar specializa- 
tion is here given a novel formulation as an opti- 
mization problem, in which the search is guided 
by a global measure combining coverage, ambi- 
guity and grammar size. The method, applica- 
ble to any unification grammar with a phrase- 
structure backbone, is shown to be effective in 
specializing a broad-coverage LFG for French. 
1 In t roduct ion  
Expressive grammar formalisms allow grammar 
developers to capture complex linguistic gener- 
alizations concisely and elegantly, thus greatly 
facilitating grammar development and main- 
tenance. Broad-coverage grammars, however, 
tend to overgenerate considerably, thus allowing 
large amounts of spurious ambiguity. If the ben- 
efits resulting from more concise grammatical 
descriptions are to outweigh the costs of spuri- 
ous ambiguity, the latter must be brought down. 
We here investigate a corpus-based compilation 
technique that reduces overgeneration a d spu- 
rious ambiguity without jeopardizing coverage 
or burdening the grammar developer. 
The current work extends previous work 
on corpus-based grammar specialization, which 
applies variants of explanation-based learning 
(EBL) to grammars of natural anguages. The 
earliest work (Rayner, 1988; Samuelsson and 
Rayner, 1991) builds a specialized grammar by 
chunking together grammar ule combinations 
while parsing training examples. What rules to 
combine is specified by hand-coded criteria. 
Subsequent work (Rayner and Carter, 1996; 
Samuelsson, 1994) views the problem as that 
of cutting up each tree in a treebank of cor- 
rect parse trees into subtrees, after which the 
rule combinations corresponding to the subtrees 
determine the rules of the specialized gram- 
mar. This approach reports experimental re- 
sults, using the SRI Core Language Engine, 
(Alshawi, 1992), in the ATIS domain, of more 
than a 3-fold speedup at a cost of 5% in gram- 
matical coverage, the latter which is compen- 
sated by an increase in parsing accuracy. Later 
work (Samuelsson, 1994; Sima'an, 1999) at- 
tempts to automatically determine appropriate 
tree-cutting criteria, the former using local mea- 
sures, the latter using global ones. 
The current work reverts to the view of EBL 
as chunking grammar rules. It extends the 
latter work by formulating rammar special- 
ization as a global optimization problem over 
the space of all possible specialized grammars 
with an objective function based on the cover- 
age, ambiguity and size of the resulting gram- 
mar. The method was evaluated on the LFG 
grammar for French developed within the PAR- 
GRAM project (Butt et al, 1999), but it is 
applicable to any unification grammar with a 
phrase-structure backbone where the reference 
treebank contains all possible analyses for each 
training example, along with an indication of 
which one is the correct one. 
To explore the space of possible grammars, a 
special treebank representation was developed, 
called a \]folded treebank, which allows the ob- 
jective function to be computed very efficiently 
for each candidate grammar. This representa- 
tion relies on the fact that all possible parses 
returned by the original grammar for each train- 
ing sentence axe available and the fact that the 
grammar specialization ever introduces new 
parses; it only removes existing ones. 
The rest of this paper is organized as follows: 
Section 2 describes the initial candidate gram- 
mar and the operators used to generate new 
candidate grammars from any given one. The 
function to be maximized is introduced and mo- 
tivated in Section 3. The folded treebank repre- 
sentation is described in Section 4, while Sec- 
tion 5 presents the experimental results. 
2 Unfo ld ing  and  Spec ia l i za t ion  
The initial grammar is the grammar underly- 
ing the subset of correct parses in the training 
set. This is in itself a specialization of the gram- 
mar which was used to parse the treebank, since 
some rules may not show up in any correct parse 
in the training set; experimental results for this 
first-order specialization are reported in (Can- 
cedda and Samuelsson, 2000). This grammar 
is further specialized by inhibiting rule combi- 
nations that show up in incorrect parses much 
more often than in correct parses. 
In more detail, we considered ownward un- 
folding of grammar ules (see Fig. l)3 A gram- 
mar rule is unfolded downwards on one of the 
symbols in its right-hand side if it is replaced 
by a set of rules, each corresponding to the ex- 
pansion of the chosen symbol by means of an- 
other grammar ule. More formally, let G = 
(E, EN, S, R) be a context-free grammar, and 
let r , r '  C R, k E .M + such that rhs(r) = aAfl, 
lal = k - 1, lhs(r') = A, rhs(r') = V. The rule 
adjunction of r I in the k th position of r is defined 
as a new rule RA(r, k, r ~) = r ' ,  such that: 
lhs(r") = lhs(r) 
rhs(r") = aVfl 
For unification grammars, we instead require 
lhs(r') U rhs(r)(k) 
lhs(r 1') = O(lhs(r)) 
rhs(r") = O(oLTfl )
where rhs(r)(k) is the kth symbol of rhs(r), 
where X t3 Y indicates that X and Y unify, and 
where 0 is the most general unifier of lhs(r ~) and 
rhs(r)(k). 
The downward rule unfolding of rule r on its 
k th position is then defined as: 
DRU(r, k) = 
1The converse operation, upward unfolding, was not 
used in the current experiments. 
{r'\[3r"\[r' = RA(r, k, r")\]} if ? 0 
= {r} otherwise 
It is easy to see that if all r I E DRU(r, k) are 
retained then the new grammar has exactly the 
same coverage as the old one. Once the rule 
has been unfolded, however, the grammar can 
be specialized. This involves inhibiting some 
rule combinations by simply removing the cor- 
responding newly created rules. Any subset 
X C_ DRU(r,k) is called a downward special- 
ization of rule r on the k th element of its rhs. 
Given a grammar, all possible (downward) 
unfoldings of its rules are considered and, for 
each unfolding, the specialization leading to the 
best increase in the objective function is deter- 
mined. The set of all such best specializations 
defines the set of candidate successor grammars. 
In the experiments, a simple hill-climbing algo- 
r ithm was adopted. Other iterative-refinement 
schemes, such as simulated annealing, could eas- 
ily be implemented. 
3 The  Ob ject ive  Funct ion  
Previous research approached the task of de- 
termining which rule combinations to allow ei- 
ther by a process of manual trial and error or 
by statistical measures based on a collection of 
positive examples only: if the original grammar 
produces more than a single parse of a sentence, 
only the "correct" parse was stored in the tree- 
bank. However, we here also have access to all 
incorrect parses assigned by the original gram- 
mar. This in turn means that we do not need 
to estimate ambiguity through some correlated 
statistical indicator, since we can measure it di- 
rectly simply by checking which parse trees are 
licensed by every new candidate grammar G. 
There are many possible ways of combining the 
counts of correct and incorrect parses in a suit- 
able objective function. For the sake of sire- 
plicity we opted for a linear combination. How- 
ever, simply maximizing correct parses and min- 
imizing incorrect ones would most likely lead to 
overfitting. In fact, a grammar with one large 
flat rule for each correct parse in the treebank 
would achieve a very high score during training, 
but most likely perform poorly on unseen data. 
A way to avoid overfitting consists in penalizing 
large grammars by introducing an appropriate 
term in the linear combination. The objective 
8 
A C  A-oc  A_oc 
B "-'~" D A "-'P" E F C A ---~" E F C 
B-----P- E F A ~  
B ~ G Specialization 
Downward unfolding 
of A -> B C on "B" 
A  C  j 
B -----~ A D B " '~"B  C C " 'D"E  B C 
B ' ' '~  A C " " -~E B C 
C ' ' '~  E A Specialization 
Upward unfolding 
of  A -> B C 
Figure 1: Schematic examples of upward and downward unfolding of rules. 
function Score was thus formulated as follows: 
Scorea = Acorr Corra -- Aine InCa - ~size S i zea  
where Corr and Inc are the number of correct 
and incorrect parses allowed by the grammar, 
and Size is the size of the grammar measured 
as the total number of symbol occurrences in the 
right-hand sides of its rules. Acorr and Ainc are 
weights controlling the pruning aggressiveness: 
their ratio Acorr/Ainc intuitively corresponds to 
the number of incorrect trees a specialization 
must disallow for each disallowed correct tree, 
if the specialization is to lead to an improve- 
ment over the current grammar. The lower this 
ratio is, the more aggressive the pruning is. The 
relative value of ;~size with respect o the other 
As also controls the depth to which the search is 
conducted: most specializations result in an in- 
crease in grammar size, which tends to be more 
and more significant as the number and the size 
of rules grows; a larger Asize thus has the effect 
of stopping the search earlier. Note that only 
two of the three weights are independent. 
4 T reebank  Representat ion  
A folded treebank is a representation of a set 
of parse trees which allows an immediate as- 
sessment of the effects of inhibiting specific rule 
combinations. It is based on the idea of "fold- 
ing" each tree onto a representation f the gram- 
mar itself. Any phrase-structure grammar can 
be represented as a concatenation/or g aph - -  
a directed bipartite multigraph with an or-node 
for each symbol and a concatenation-node for
each rule in the grammar. The present de- 
scription covers context-free grammars, but the 
scheme can easily be extended to any unifica- 
tion grammar with a context-free backbone by 
replacing symbol eqality with unification. 
Given a grammar G = (E, EN, S,R), we can 
define a relation ~ and a (partial) function ~?n: 
? ~/~ C EN ? R s.t. (A , r  / E r/~ iffA = lhs(r) 
? r~R : R ? Af + ~ E s.t. r lR ( r  , i )  = X iff 
rhs(r) = f iX% \]/3\[ = i - 1 
Figure 2 shows the correspondence between a 
simple grammar fragment and its concatena- 
t ion/or graph. 
Each tree can be represented by folding it 
onto the concatenation/or graph representing 
the grammar it was derived with, or, in other 
words, by appropriately annotating the graph 
itself. If N is the set of nodes of a parse tree 
obtained using grammar G, the corresponding 
folded tree is a partial function f
f :NxN- - -~RxAf  + 
such that f (n ,n ' )  = (r, k) implies that node n 
was expanded using rule r, and that node n' is 
its k th daughter in the tree (Fig.3). In the fol- 
lowing, we will use the inverse image of (r, k) un- 
der f ,  which we denote ?(r, k) : f - l ( ( r ,  k)) : 
{(n ,n ' ) l f (n ,n '  ) : ( r ,k)} C g ? N. This can in 
turn be seen as a partial function 
? : R ? Jkf+ ~ 2 N?N 
Disallowing the expansion of the k th element 
in the right-hand side of rule r by means of rule 
r' (assuming symbols match, i.e., (r/R(r, k), r') E 
rl2 ) results in suppressing a tree where: 
3n, n', n" E N, k' E .hf + 
\[<n, e ?(r, k) A <n', n"> ? ?(r', k')\] 
This check can be performed very efficiently 
once the tree is represented by the ? function, 
i.e., once it is folded, as all this requires is to 
compare the entries for (r, k) and (r', k') 2 with a 
procedure linear in the size of the entries them- 
selves. If we used a more traditional represen- 
tation, the same check would require traversing 
2In fact, it suffices to check the entries for (r', 1). 
A 
? nO 
nl ~ B 
? n2 ? 
c ~ n4 
? 
f n3 
c On5 
? n7 ? n8 f e 
?(rl ,  1) = {<no,n1), <rt4,rt5>} 
?(r l ,  2) = {<no, n2), (n4, n6)} 
?(r2, 1) = 0 
?(r2, 2) = 0 
?(r3, 1) = {(n2, n3)} 
?(r3,  2) = (<n2, n4>} 
?(r4, I) = {<n6, nT>} 
?(r4,  2) -~ {(n6, n8) } 
A 
r 1 --% ~ r2 
O^'~ " O 
C x \  
e .  ~ ~;, .e  
r ~ ~o" 
~ ; !  '~r4 
Figure 3: A tree and its folded representation. 
the whole tree. The worst-case complexity is 
still linear in the size of the tree, but in prac- 
tice, the number of nodes expanded using any 
given rule is much smaller than the total num- 
ber of nodes. 
Whenever a specialization is performed, all 
folded trees that are no longer licensed are 
removed; the concatenation/or graph for the 
grammar is updated to reflect the introduction 
and the elimination of rules; and the annota- 
tions on the affected edges are appropriately re- 
combined and distributed. If the performed spe- 
cialization is X C_ DRU(r, k) ~ {r}, 3 then the 
concatenation/or g aph is updated as follows 
= nux\{~) 
~r = ~r U {(lhs(r),~)l~ E X} \ {(lhs(r),r)} 
~(r" ,  i) = 
{ VR(r",i), r" ? X 
~\]R(r,i), r u E X , i  < k 
= VR(r' , i  -- k + 1), r" = aA(r ,  k, r') E X, 
k<i<k+m-1,  
~R(r, i -- m + 1), r" = RA(r, k, r') E X, 
i>k+m-1,  
where m = arity(r') = Irhs(r')l is the number 
of right-hand-side symbols of rule r ~. For each 
tree that is not eliminated as a consequence of
3If X = DRU(r, k) = {r}, then no update is needed. 
the specialization we have 
~(~",~) =
?(r",i), 
if r" ?~ X ,  r" ? r' 
?(~",/) k(<n',n">13n\[<n,n'> e ?(~, k)\]), 
if r" = r ~ 
{(n, n">13n', n", k'\[<n, n'> ~ ?(~, k) 
A(n ' ,n" )  E ?(r ' ,k ' )  A (n,n'")  E ?(r,i)\]} 
if r" = RA( r ,k , r  ~) E X , i  < k 
{(n, n")13n'\[(n, n') E ?(r, k)A 
(n', n") E ?(r', i -- k + 1)\]} 
i f r "=RA(r ,k , r  ~) EX ,  k<i<k+m-1,  
{ (n ,n" )13n ' ,n" ,k ' \ [ (n ,n '  ) E ?(r,k)A 
<n', n"> e ?(~', k')A 
(n, n'") E ?(r, i - m + 1)\]} 
if r" = RA(r ,  k, r ~) E X ,  i > k + m - 1, 
where again m = arity(r'). These updates can 
be implemented efficiently, requiring neither a 
traversal of the tree nor of the grammar. 
5 Exper imenta l  Resu l t s  
We specialized a broad-coverage LFG grammar 
for French on a corpus of technical documen- 
tation using the method described above. The 
treebank consisted of 960 sentences which were 
all known to be covered by the original gram- 
mar. For each sentence, all the trees returned by 
10 
R --  { r l ,  r2, r3, r4} s.t.  
rl: A -~cB 
r2: A --+ e A 
ra: B -+fA  
r4: B -arE  
~2 = {(A, r l) ,  (A, r2}, {B, r3), (B, r4)} 
C 
fiR: (rl, 1) ~ c 
(rl, 2) ~ B 
(r2,1) -+ e 
(r2, 2) ~ A 
(r3, 1) ~ f 
(r3, 2) ~ A 
(r4,1) I 
@4, 2) --~ e 
A 
r l  r 2 
? ? 
? ? / ? e / 
r 3 \ / r4  
f 
Figure 2: A tiny grammar and the correspond- 
ing concatenation/or graph. 
the original grammar were available, together 
with a manually assigned indication of which 
was the correct one. The environment used, 
the Xerox Linguistic Environment (Kaplan and 
Maxwell, 1996) implements a version of opti- 
mality theory where parses are assigned "opti- 
mality marks" based on a number of criteria, 
and are ranked according to these marks. The 
set of parses with the best marks are called the 
optimal parses for a sentence. The correct parse 
was also an optimal parse for 913 out of 960 sen- 
tences. Given this, the specialization was aimed 
at reducing the number of optimal parses per 
sentence. 
We ran a series of ten-fold cross-validation ex- 
periments; the results are summarized in the ta- 
ble in Fig.4. The first line contains values for 
the original grammar. The second line contains 
measures for the first-order pruning grammar, 
i.e., the grammar with all and only those rules 
actually used in correct parses in the training 
set, with no combination inhibited. Lines 3 and 
4 list results for fully specialized grammars. Re- 
sults in the third line were obtained with a value 
for ~corr equal to 15 times the value of Ainc in 
the objective function: in other words, during 
training we were willing to lose a correct parse 
only if at least 15 incorrect parses were canceled 
as well. Results in the fourth line were obtained 
when this ratio was reduced to 10. The average 
number of parses per sentence is reported in the 
first column, whereas the second lists the av- 
erage number of optimal parses. Coverage was 
measured as the fraction of sentences which still 
receive the correct parse with the specialized 
grammar. To assess the trade off between cover- 
age and ambiguity reduction, we computed the 
F-score 4 considering only optimal parses when 
computing precision. This measure should not 
be confused with the F-score on labelled brack- 
eting reported for many stochastic parsers; here 
precision and recall concern perfect matching of 
whole trees. Recall is the same as coverage: the 
ratio between the number of correct parses pro- 
duced by the specialized grammar and the to- 
tal number of correct parses (equalling the total 
number of sentences in the test set). Precision is 
the ratio between the number of correct parses 
produced by the specialized grammar and the 
total number of parses produced by the same 
grammar. The fourth column lists values for the 
F-score when equal weight is given to precision 
and recall. Intuitively, however, in many cases 
missing the correct parse is more of a problem 
than returning spurious parses, so we also com- 
puted the F-score with a much larger emphasis 
on recall, i.e., with a = 0.1. The corresponding 
values are listed in the last column. 
The average number of parses per sentence, 
both optimal and non-optimal, decreases signif- 
icantly as more and more aggressive specializa- 
tion.is carried out, and consequently, more cov- 
erage is lost. The most aggressive form of spe- 
4The F-score is the harmonic mean of recall and pre- 
cision, where precision is weighted a and recall 1 - a. 
11 
Avg.p/s Avg. o.p./s. Coverage (%) F(a-- 0.5) F(c~-- 0.1) 
orig. 1941 4.69 100 35.15 73.05 
f.o.pruning 184 3.38 89 40.64 71.89 
Acorr=lh)~inc 82 2.23 86 53.25 76.58 
~corr----lO)kin c 63 2.03 82.5 54.46 74.80 
Figure 4: Results of the 
cialization gives the highest F-score for c~ = 0.5, 
whereas omewhat more conservative parame- 
ter settings lead to a better F-score when re- 
call is valued more. A speedup of a factor 4 
is achieved already by first-order pruning and 
remains approximately the same after further 
specialization. 
6 Conclusions 
Broad-coverage rammars tend to be highly am- 
biguous, which may constitute a serious prob- 
lem when using them for natural-language pro- 
cessing. Corpus-independent compilation tech- 
niques, although useful for increasing efficiency, 
do little in terms of reducing ambiguity. 
In this paper we proposed a corpus-based 
technique for specializing a grammar on a do- 
main for which a treebank exists containing all 
trees returned for each sentence. This tech- 
nique, which builds extensively on previous 
work on explanation-based learning for NLP, 
consists in casting the problem as an optimiza- 
tion problem in the space of all possible spe- 
cializations of the original grammar. As initial 
candidate grammar, the first-order pruning of 
the original grammar is considered. Candidate 
successor grammars are obtained through the 
downward rule unfolding and specialization op- 
erator, that has the desirable property of never 
causing previously unseen parses to become 
available for sentences in the training set. Can- 
didate grammars are then assessed according to 
an objecting function combining rammar am- 
biguity and coverage, adapted to avoid overfit- 
ting. In order to ensure efficient computability 
of the objective function, the treebank is pre- 
viously folded onto the grammar itself. Exper- 
imental results using a broad-coverage lexical- 
functional grammar of French show that the 
technique allows effectively trading coverage for 
ambiguity reduction. Moreover, the parameters 
of the objective function can be used to control 
the trade off. 
specialization experiments. 
Acknowledgements  
We would like to thank the members of the 
MLTT group at the Xerox Research Centre Eu- 
rope in Grenoble, France, and the three anony- 
mous reviewers for valuable discussions and 
comments. This research was funded by the Eu- 
ropean TMR network Learning Computational 
Grammars. 
Re ferences  
Hiyan Alshawi, editor. 1992. The Core Language 
Engine. MIT Press. 
M. Butt, T.H. King, M.E. Nifio, and F. Segond. 
1999. A Grammar Writer's Cookbook. CSLI Pub- 
lications, Stanford, CA. 
Nicola Cancedda nd Christer Samuelsson. 2000. 
Experiments with corpus-based lfgspecialization. 
In Proceedings o/ the NAACL-ANLP 2000 Con- 
ference, Seattle, WA. 
Ronald Kaplan and John T. Maxwell. 1996. 
LFG grammar writer's workbench. Technical 
report, Xerox PARC. Available on-line as 
ftp://ftp.parc.xerox.com/pub/lfg/lfgmanual.ps. 
Manny Rayner and David Carter. 1996. Fast pars- 
ing using pruning and grammar specialization. In 
Proceedings of the ACL-96, Santa Cruz, CA. 
Manny Rayner. 1988. Applying explanation-based 
generalization to natural-language processing. 
In Proceedings o/ the International Con/erence 
on Fifth Generation Computer Systems, Tokyo, 
Japan. 
Christer Samuelsson and Manny Rayner. 1991. 
Quantitative evaluation of explanation-based 
learning as an optimization tool for a large-scale 
natural anguage system. In Proceedings o/the 
IJCAI-91, Sydney, Australia. 
Christer Samuelsson. 1994. Grammar specialization 
through entropy thresholds. In Proceedings ofthe 
ACL-94, Las Cruces, New Mexico. Available as 
cmp-lg/9405022. 
Khalil Sima'an. 1999. Learning Efficient Dis- 
ambiguation. Ph.D. thesis, Institute for Logic, 
Language and Computation, Amsterdam, The 
Netherlands. 
12 
Probabilistic Models for PP-attachment Resolution and NP Analysis
Eric Gaussier
XRCE
6, Chemin de Maupertuis
38240 Meylan, France
fname.lname@xrce.xerox.com
Nicola Cancedda
XRCE
6, Chemin de Maupertuis
38240 Meylan, France
fname.lname@xrce.xerox.com
Abstract
We present a general model for PP attach-
ment resolution and NP analysis in French.
We make explicit the different assumptions
our model relies on, and show how it gener-
alizes previously proposed models. We then
present a series of experiments conducted
on a corpus of newspaper articles, and as-
sess the various components of the model,
as well as the different information sources
used.
1 Introduction
Prepositional phrase attachment resolution and noun
phrase analysis are known to be two difficult tasks.
Traditional context-free rules for example do not help
at all in selecting the good parse for a noun phrase,
since all valid parses are a priori correct. Subcatego-
rization information can help solve the problem, but
the amount of information necessary to be encoded
in such lexicons is huge, since in addition to subcat-
egorization frames, one should encode the different
senses of words and rules on how these senses can
be combined, as well as the different units (single or
multi word expressions) a language makes use of. It
has been recognized in different works that part of
this knowledge can be (semi-)automatically acquired
from corpora and used in the decision process. Several
models have been proposed for PP-attachment resolu-
tion and NP analysis, built on various building blocks
and making use of diverse information. Most of these
models fit within a probabilistic framework. Such
a framework allows one to estimate various quanti-
ties and to perform inference with incomplete infor-
mation, two necessary steps for the problem at hand.
We present in this paper a model generalizing several
models already proposed and integrating several infor-
mation sources.
We focus here on analysing strings corresponding
to a combination of traditional VERB NOUN PREP
sequences and noun phrases. More precisely, the NPs
we consider are arbitrarily complex noun phrases with
no embedded subordinate clause. The PP attachment
problems we tackle are those involved in analysing
such NPs for languages relying on composition of Ro-
mance type (as French or Italian), as well as those
present in verbal configurations for languages relying
on composition of Germanic type (as German or En-
glish). However, our model can easily be extended to
deal with other cases as well. The problem raised in
analysing such sequences is of the utmost relevance
for at least two reasons: 1) NPs constitute the vast ma-
jority of terms. Correctly identifying the boundaries
and the internal structure of noun phrases is crucial to
the automatic discovery of domain-specific termino-
logical databases; 2) PP-attachment ambiguity is one
of the main sources of syntactic ambiguity in general.
In the present work, we focus on the French lan-
guage since we have various lexical information at our
disposal for this language that we would like to as-
sess in the given context. Furthermore, French dis-
plays interesting propoerties (like gender and num-
ber agreement for adjectives) which makes it an in-
teresting language to test on. The remainder of the
paper is organized as follows: we first describe how
we preprocessed our corpus, and which part we re-
tained for probability estimation. We then present the
general model we designed, and show how it com-
pares with previous ones. We then describe the exper-
iments we performed and discuss the results obtained.
We also describe, in the experiments section, how
we integrated different types of information sources
(e.g. prior knowledge encoded as subcategorization
frames), and how we weighted multiple sources of ev-
idence according to their reliability.
2 Nuclei and sequences of nuclei
We first take the general view that our problem can
be formulated as one of finding dependency relations
between nuclei. Without loss of generality, we define
a nucleus to be a unit that contains both the syntactic
and semantic head and that exhibits only unambiguous
internal syntactic structure. For example, the base NP
?the white horse? is a nucleus, since the attachments
of both the determiner and the adjective to the noun
are straightforward. The segmentation into nuclei re-
lies on a manually built chunker, similar to the one
described in (Ait-Mokhtar and Chanod, 1997), and re-
sembles the one proposed in (Samuelsson, 2000). The
motivation for this assumption is twofold. First, the
amount of grammatical information carried by indi-
vidual words varies greatly across language families.
Grammatical information carried by function words
in non-agglutinative languages, for instance, is real-
ized morphologically in agglutinative languages. A
model manipulating dependencies at the word level
only would be constrained to the specific amount of
grammatical and lexical information associated with
words in a given language. Nuclei, on the other hand,
tend to correspond to phrases of the same type across
languages, so that relying on the notion of nucleus
makes the approach more portable. A second moti-
vation for considering nuclei as elementary unit is that
their internal structure is by definition unambiguous,
so that there is no point in applying any algorithm
whatsoever to disambiguate them.
We view each nucleus as being composed of several
linguistic layers with different information, namely
a semantic layer comprising the possible semantic
classes for the word under consideration, a syntactic
layer made of the POS category of the word and its
gender and number information, and a lexical layer
consisting of the word itself (referred to as the lexeme
in the following), and the preposition, for prepositional
phrases. For nuclei comprising more than two non-
empty words (as ?the white horse?), we retain only one
lexeme, the one associated with the last word which is
considered to be the head word in the sequence. Ex-
cept for the semantic information, all the necessary in-
formation is present in the output of the chunker. The
semantic lexicon we used was encoded as a finite-state
transducer, which was looked up for injecting seman-
tic classes in each nucleus. When no semantic infor-
mation is available for a given word, we use its part-of-
speech category as its semantic class1. For example,
starting with the sentence:
1The semantic resource we used can be purchased from
www.lexiquest.com. This resource contains approximately
90 different sementic classes organized into a hierarchy. We
have not made use of this hierarchy in our experiments.
Il doit rencontrer le pre?sident de la fe?de?ration franc?aise.
(He has to meet the president of the French federation.)
we obtain the following sequence of nuclei:
il
CAT=?PRON?, GN=?Masc-Sg?, PREP=??,
SEM=?PRON?
rencontrer
CAT=?VERB?, GN=??, PREP=??, SEM=?VERB?
pre?sident
CAT=?NOUN?, GN=?Masc-Sg?, PREP=??,
SEM=?FONCTION?
fe?de?ration
CAT=?NOUN?, GN=?Fem-Sg?, PREP=?de?,
SEM=?HUMAIN?
franc?aise
CAT=?ADJ?, GN=?Fem-Sg?, PREP=??, SEM=?GEO?
As we see in this example, the semantic resource we
use is incomplete and partly questionable. The at-
tribute HUMAN for federation can be understood if
one views a federation as a collection of human be-
ings, which we believe is the rationale behind this an-
notation. However, a federation also is an institution,
a sense which is missing in the resource we use.
In the preceding example, the preposition de can
be attached to the verb rencontrer or to the noun
pre?sident. It cannot be attached to the pronoun il.
As far as terminology extraction is our final objective,
pre?sident de la fe?de?ration franc?aise can be deemed
a good candidate term. However, in order to accu-
rately identify this unit, a high confidence in the fact
that the preposition de attaches to the noun pre?sident
must be achieved. Sentences can be conveniently seg-
mented into smaller self-contained units according to
some heuristics to reduce the combinatorics of attach-
ments ambiguities. We define safe chains as being
sequences of nuclei in which all the items but the first
are attached to other nuclei within the chain itself. In
the preceding example, for instance, only the nucleus
associated with rencontrer is not attached to a nucleus
within the chain rencontrer ... franc?aise. This chain
is thus a safe chain. To keep the number of alterna-
tive (combinations of) attachments as low as possible,
we are interested in isolating as short safe chains as
possible given the information available at this point,
i.e. words and their parts-of-speech (the knowledge of
semantic classes is of little help in this task).
In French, and except for few cases involving em-
bedded clauses and coordination, the following heuris-
tics can be used to identify ?minimal? safe chains: ex-
tract the longest sequences beginning with a nominal,
verbal, prepositional or adjectival nucleus, containing
only nominal, prepositional, adjectival, adverbial or
verbal nuclei in indefinite moods.
There is a tension in parameter estimation of prob-
abilistic models between relying on accurate infor-
mation and relying on enough data. In an unsuper-
vised approach to PP-attachment resolution and NP
analysis, accurate information in the form of depen-
dency relations between words is not directly acces-
sible. However, specific configurations can be identi-
fied from which accurate information can be extracted.
Safe chains provide such configurations. Indeed if
there is only one possible attachment site to the left
of a nucleus, then its attachment is unambiguous. Due
to the possible ambiguities the French language dis-
plays (e.g. a preposition can be attached to a noun,
a verb or an adjective), only the first two nuclei of a
safe chain provide reliable information (we skip ad-
verbs, the attachment of which obeys specific and sim-
ple rules). From the preceding example, for instance,
we can infer a direct relation between rencontrer and
pre?sident, but this is the only attachment we can be
sure of. The use of less reliable information sources
for model parameters whose estimation would other-
wise require manual supervision is the object of an ex-
periment described in Section 6.
3 Attachment Model
Let us denote the   nucleus in a chain by    , and the
the nucleus to which it is attached by 	   (for each
chain, we introduce an additional empty nucleus to
which the head of the chain is attached). Given a chain
of nuclei 
 , we denote by  the set of dependency re-
lations covering the chain of nuclei 
   . We
are interested in the set  such that Learning Computational Grammars
John Nerbonne   , Anja Belz  , Nicola Cancedda  , Herve? De?jean  ,
James Hammerton

, Rob Koeling

, Stasinos Konstantopoulos   ,
Miles Osborne   , Franck Thollard

and Erik Tjong Kim Sang 
Abstract
This paper reports on the LEARNING
COMPUTATIONAL GRAMMARS (LCG)
project, a postdoc network devoted to
studying the application of machine
learning techniques to grammars suit-
able for computational use. We were in-
terested in a more systematic survey to
understand the relevance of many fac-
tors to the success of learning, esp. the
availability of annotated data, the kind
of dependencies in the data, and the
availability of knowledge bases (gram-
mars). We focused on syntax, esp. noun
phrase (NP) syntax.
1 Introduction
This paper reports on the still preliminary, but al-
ready satisfying results of the LEARNING COM-
PUTATIONAL GRAMMARS (LCG) project, a post-
doc network devoted to studying the application
of machine learning techniques to grammars suit-
able for computational use. The member insti-
tutes are listed with the authors and also included
ISSCO at the University of Geneva. We were im-
pressed by early experiments applying learning
to natural language, but dissatisfied with the con-
centration on a few techniques from the very rich
area of machine learning. We were interested in

University of Groningen,  nerbonne,konstant  @let.
rug.nl, osborne@cogsci.ed.ac.uk
	
SRI Cambridge, anja.belz@cam.sri.com, Rob.Koe-
ling@netdecisions.co.uk

XRCE Grenoble, nicola.cancedda@xrce.xerox.com

University of Tu?bingen, Herve.Dejean@xrce.xerox.
com, thollard@sfs.nphil.uni-tuebingen.de

University College Dublin, james.hammerton@ucd.ie

University of Antwerp, erikt@uia.ua.ac.be
a more systematic survey to understand the rele-
vance of many factors to the success of learning,
esp. the availability of annotated data, the kind
of dependencies in the data, and the availability
of knowledge bases (grammars). We focused on
syntax, esp. noun phrase (NP) syntax from the
beginning. The industrial partner, Xerox, focused
on more immediate applications (Cancedda and
Samuelsson, 2000).
The network was focused not only by its sci-
entific goal, the application and evaluation of
machine-learning techniques as used to learn nat-
ural language syntax, and by the subarea of syn-
tax chosen, NP syntax, but also by the use of
shared training and test material, in this case ma-
terial drawn from the Penn Treebank. Finally, we
were curious about the possibility of combining
different techniques, including those from statisti-
cal and symbolic machine learning. The network
members played an important role in the organi-
sation of three open workshops in which several
external groups participated, sharing data and test
materials.
2 Method
This section starts with a description of the three
tasks that we have worked on in the framework of
this project. After this we will describe the ma-
chine learning algorithms applied to this data and
conclude with some notes about combining dif-
ferent system results.
2.1 Task descriptions
In the framework of this project, we have worked
on the following three tasks:
1. base phrase (chunk) identification
2. base noun phrase recognition
3. finding arbitrary noun phrases
Text chunks are non-overlapping phrases which
contain syntactically related words. For example,
the sentence:
 
He 
 
reckons 
 
the current
account deficit 
 
will narrow 
 
to 
 
only  1.8 billion 
 
in 
 
September  .
contains eight chunks, four NP chunks, two VP
chunks and two PP chunks. The latter only con-
tain prepositions rather than prepositions plus the
noun phrase material because that has already
been included in NP chunks. The process of
finding these phrases is called CHUNKING. The
project provided a data set for this task at the
CoNLL-2000 workshop (Tjong Kim Sang and
Buchholz, 2000)1. It consists of sections 15-18 of
the Wall Street Journal part of the Penn Treebank
II (Marcus et al, 1993) as training data (211727
tokens) and section 20 as test data (47377 tokens).
A specialised version of the chunking task is
NP CHUNKING or baseNP identification in which
the goal is to identify the base noun phrases. The
first work on this topic was done back in the
eighties (Church, 1988). The data set that has
become standard for evaluation machine learn-
ing approaches is the one first used by Ramshaw
and Marcus (1995). It consists of the same train-
ing and test data segments of the Penn Treebank
as the chunking task (respectively sections 15-18
and section 20). However, since the data sets
have been generated with different software, the
NP boundaries in the NP chunking data sets are
slightly different from the NP boundaries in the
general chunking data.
Noun phrases are not restricted to the base lev-
els of parse trees. For example, in the sentence In
early trading in Hong Kong Monday , gold was
quoted at $ 366.50 an ounce ., the noun phrase
  $ 366.50 an ounce  contains two embedded
noun phrases
  $ 366.50  and   an ounce  .
In the NP BRACKETING task, the goal is to find
all noun phrases in a sentence. Data sets for this
task were defined for CoNLL-992. The data con-
sist of the same segments of the Penn Treebank as
1Detailed information about chunking, the CoNLL-
2000 shared task, is also available at http://lcg-
www.uia.ac.be/conll2000/chunking/
2Information about NP bracketing can be found at
http://lcg-www.uia.ac.be/conll99/npb/
the previous two tasks (sections 15-18) as train-
ing material and section 20 as test material. This
material was extracted directly from the Treebank
and therefore the NP boundaries at base levels are
different from those in the previous two tasks.
In the evaluation of all three tasks, the accu-
racy of the learners is measured with three rates.
We compare the constituents postulated by the
learners with those marked as correct by experts
(gold standard). First, the percentage of detected
constituents that are correct (precision). Second,
the percentage of correct constituents that are de-
tected (recall). And third, a combination of pre-
cision and recall, the F ffCombining labelled and unlabelled data: a case study on Fisher
kernels and transductive inference for biological entity recognition
Cyril Goutte, Herve Dejean, Eric Gaussier,
Nicola Cancedda and Jean-Michel Renders
Xerox Research Center Europe
6, chemin de Maupertuis
38240 Meylan, France
Abstract
We address the problem of using partially la-
belled data, eg large collections were only little
data is annotated, for extracting biological en-
tities. Our approach relies on a combination of
probabilistic models, which we use to model the
generation of entities and their context, and ker-
nel machines, which implement powerful cate-
gorisers based on a similarity measure and some
labelled data. This combination takes the form
of the so-called Fisher kernels which implement
a similarity based on an underlying probabilistic
model. Such kernels are compared with trans-
ductive inference, an alternative approach to
combining labelled and unlabelled data, again
coupled with Support Vector Machines. Exper-
iments are performed on a database of abstracts
extracted from Medline.
1 Introduction
The availability of electronic databases of ra-
pidly increasing sizes has encouraged the de-
velopment of methods that can tap into these
databases to automatically generate knowledge,
for example by retrieving relevant information
or extracting entities and their relationships.
Machine learning seems especially relevant in
this context, because it helps performing these
tasks with a minimum of user interaction.
A number of problems like entity extraction
or ltering can be mapped to supervised tech-
niques like categorisation. In addition, modern
supervised classication methods like Support
Vector Machines have proven to be ecient and
versatile. They do, however, rely on the avail-
ability of labelled data, where labels indicate
eg whether a document is relevant or whether
a candidate expression is an interesting entity.
This causes two important problems that mo-
tivate our work: 1) annotating data is often a
dicult and costly task involving a lot of hu-
man work
1
, such that large collections of la-
belled data are dicult to obtain, and 2) inter-
annotator agreement tends to be low in eg ge-
nomics collections (Krauthammer et al, 2000),
thus calling for methods that are able to deal
with noise and incomplete data.
On the other hand, unsupervised techniques
do not require labelled data and can thus be
applied regardless of the annotation problems.
Unsupervised learning, however, tend to be less
data-ecient than its supervised counterpart,
requiring many more examples to discover sig-
nicant features in the data, and is incapable
of solving the same kinds of problems. For ex-
ample, an ecient clustering technique may be
able to distribute documents in a number of
well-dened clusters. However, it will be unable
to decide which clusters are relevant without a
minimum of supervision.
This motivates our study of techniques that
rely on a combination of supervised and unsu-
pervised learning, in order to leverage the avail-
ability of large collections of unlabelled data and
use a limited amount of labelled documents.
The focus of this study is on a particular
application to the genomics literature. In ge-
nomics, a vast amount of knowledge still resides
in large collections of scientic papers such as
Medline, and several approaches have been pro-
posed to extract, (semi-)automatically, informa-
tion from such papers. These approaches range
from purely statistical ones to symbolic ones
relying on linguistic and knowledge processing
tools (Ohta et al, 1997; Thomas et al, 2000;
Proux et al, 2000, for example). Furthermore,
due to the nature of the problem at hand, meth-
1
If automatic annotation was available, we would ba-
sically have solved our Machine Learning problem
ods derived from machine learning are called
for, (Craven and Kumlien, 1999), whether su-
pervised, unsupervised or relying on a combi-
nation of both.
Let us insist on the fact that our work is pri-
marily concerned with combining labelled and
unlabelled data, and entity extraction is used
as an application in this context. As a conse-
quence, it is not our purpose at this point to
compare our experimental results to those ob-
tained by specic machine learning techniques
applied to entity extraction (Cali, 1999). Al-
though we certainly hope that our work can be
useful for entity extraction, we rather think of
it as a methodological study which can hope-
fully be applied to dierent applications where
unlabelled data may be used to improve the re-
sults of supervised learning algorithms. In addi-
tion, performing a fair comparison of our work
on standard information extraction benchmarks
is not straightforward: either we would need to
obtain a large amount of unlabelled data that is
comparable to the benchmark, or we would need
to \un-label" a portion of the data. In both
cases, comparing to existing results is dicult
as the amount of information used is dierent.
2 Classication for entity extraction
We formulate the following (binary) classica-
tion problem: given an input space X , and from
a dataset of N input-output pairs (x
k
; y
k
) 2
X  f 1; +1g, we want to learn a classier
h : X ! f 1; +1g so as to maximise the proba-
bility P (h(x) = y) over the xed but unknown
joint input-output distribution of (x; y) pairs.
In this setting, binary classication is essentially
a supervised learning problem.
In order to map this to the biological en-
tity recognition problem, we consider for each
candidate term, the following binary decision
problem: is the candidate a biological entity
2
(y = 1) or not (y =  1). The input space is a
high dimensional feature space containing lexi-
cal, morpho-syntactic and contextual features.
In order to assess the validity of combining
labelled and unlabelled data for the particular
task of biological entity extraction, we use the
following tools. First we rely on Suport Vec-
tor Machines together with transductive infer-
2
In our case, biological entities are proteins, genes
and RNA, cf. section 6.
ence (Vapnik, 1998; Joachims, 1999), a train-
ing technique that takes both labelled and unla-
belled data into account. Secondly, we develop
a Fisher kernel (Jaakkola and Haussler, 1999),
which derives the similarity from an underlying
(unsupervised) model of the data, used as a sim-
ilarity measure (aka kernel) within SVMs. The
learning process involves the following steps:
 Transductive inference: learn a SVM classi-
er h(x) using the combined (labelled and
unlabelled) dataset, using traditional ker-
nels.
 Fisher kernels:
1. Learn a probabilistic model of the data
P (xj) using combined unlabelled and
labelled data;
2. Derive the Fisher kernel K(x; z) ex-
pressing the similarity in X -space;
3. Learn a SVM classier h(x) using this
Fisher kernel and inductive inference.
3 Probabilistic models for
co-occurence data
In (Gaussier et al, 2002) we presented a gen-
eral hierarchical probabilistic model which gen-
eralises several established models like Nave
Bayes (Yang and Liu, 1999), probabilistic latent
semantic analysis (PLSA) (Hofmann, 1999) or
hierarchical mixtures (Toutanova et al, 2001).
In this model, data result from the observation
of co-occuring objects. For example, a docu-
ment collection is expressed as co-occurences
between documents and words; in entity extrac-
tion, co-occuring objects may be potential en-
tities and their context, for example. For co-
occuring objects i and j, the model is expressed
as follows:
P (i; j) =
X

P ()P (ij)
X

P (j)P (jj)
(1)
where  are latent classes for co-occurrences
(i; j) and  are latent nodes in a hierarchy gener-
ating objects j. In the case where no hierarchy
is needed (ie P (j) = ( = )), the model
reduces to PLSA:
P (i; j) =
X

P ()P (ij)P (jj) (2)
where  are now latent concepts over both i and
j. Parameters of the model (class probabilities
P () and class-conditional P (ij) and P (jj))
are learned using a deterministic annealing ver-
sion of the expectation-maximisation (EM) al-
gorithm (Hofmann, 1999; Gaussier et al, 2002).
4 Fisher kernels
Probabilistic generative models like PLSA and
hierarchical extensions (Gaussier et al, 2002)
provide a natural way to model the generation
of the data, and allow the use of well-founded
statistical tools to learn and use the model.
In addition, they may be used to derive a
model-based measure of similarity between ex-
amples, using the so-called Fisher kernels pro-
posed by Jaakkola and Haussler (1999). The
idea behind this kernel is that using the struc-
ture implied by the generative model will give
a more relevant similarity estimate, and allow
kernel methods like the support vector machines
or nearest neighbours to leverage the probabilis-
tic model and yield improved performance (Hof-
mann, 2000).
The Fisher kernel is obtained using the log-
likelihood of the model and the Fisher informa-
tion matrix. Let us consider our collection of
documents fx
k
g
k=1:::N
, and denote by `(x) =
logP (xj) the log-likelihood of the model for
data x. The expression of the Fisher kernel
(Jaakkola and Haussler, 1999) is then:
K(x
1
; x
2
) = r`(x
1
)
>
I
F
 1
r`(x
2
) (3)
The Fisher information matrix I
F
can be seen
as a way to keep the kernel expression inde-
pendent of parameterisation and is dened as
I
F
= E

r`(x)r`(x)
>

, where the gradient
is w.r.t.  and the expectation is taken over
P (xj). With a suitable parameterization, the
information matrix I is usually approximated by
the identity matrix (Hofmann, 2000), leading
to the simpler kernel expression: K(x
1
; x
2
) =
r`(x
1
)
>
r`(x
2
).
Depending on the model, the various log-
likelihoods and their derivatives will yield dif-
ferent Fisher kernel expressions. For PLSA (2),
the parameters are  = [P (); P (ij); P (jj)].
From the derivatives of the likelihood `(x) =
P
(i;j)2x
log P (i; j), we derive the following sim-
ilarity (Hofmann, 2000):
K(x
1
; x
2
) =
X

P (jd
i
)P (jd
j
)
P ()
(4)
+
X
w
b
P
wd
i
b
P
wd
j
X

P (jd
i
; w)P (jd
j
; w)
P (wj)
with
b
P
wd
i
,
b
P
wd
j
the empirical word distributions
in documents d
i
, d
j
.
5 Transductive inference
In standard, inductive SVM inference, the an-
notated data is used to infer a model, which is
then applied to unannotated test data. The in-
ference consists in a trade-o between the size
of the margin (linked to generalisation abilities)
and the number of training errors. Transductive
inference (Gammerman et al, 1998; Joachims,
1999) aims at maximising the margin between
positives and negatives, while minimising not
only the actual number of incorrect predictions
on labelled examples, but also the expected
number of incorrect predictions on the set of
unannotated examples.
This is done by including the unknown la-
bels as extra variables in the original optimisa-
tion problem. In the linearly separable case, the
new optimisation problem amounts now to nd
a labelling of the unannotated examples and a
hyperplane which separates all examples (anno-
tated and unannotated) with maximum margin.
In the non-separable case, slack variables are
also associated to unannotated examples and
the optimisation problem is now to nd a la-
belling and a hyperplane which optimally solves
the trade-o between maximising the margin
and minimising the number of misclassied ex-
amples (annotated and unannotated).
With the introduction of unknown labels as
supplementary optimisation variables, the con-
straints of the quadratic optimisation problem
are now nonlinear, which makes solving more
dicult. However, approximated iterative algo-
rithms exist which can eciently train Trans-
ductive SVMs. They are based on the principle
of gradually improving the solution by switching
the labels of unnannotated examples which are
misclassied at the current iteration, starting
from an initial labelling given by the standard
(inductive) SVM.
WUp Is the word capitalized?
WAllUp Is the word alls capitals?
WNum Does the word contain digits?
Table 1: Spelling features
6 Experiments
For our experiments, we used 184 abstracts from
the Medline site. In these articles, genes, pro-
teins and RNAs were manually annotated by a
biologist as part of the BioMIRE project. These
articles contain 1405 occurrences of gene names,
792 of protein names and 81 of RNA names. All
these entities are considered relevant biological
entities. We focus here on the task of identify-
ing names corresponding to such entities in run-
ning texts, without dierentiating genes from
proteins or RNAs. Once candidates for bio-
logical entity names have been identied, this
task amounts to a binary categorisation, rele-
vant candidates corresponding to biological en-
tity names. We divided these abstracts in a
training and development set (122 abstracts),
and a test set (62 abstracts). We then retained
dierent portions of the training labels, to be
used as labelled data, whereas the rest of the
data is considered unlabelled.
6.1 Denition of features
First of all, the abstracts are tokenised, tagged
and lemmatized. Candidates for biological en-
tity names are then selected on the basis of the
following heuristics: a token is considered a can-
didate if it appears in one of the biological lexicons
we have at our diposal, or if it does not belong to
our general English lexicon. This simple heuris-
tics allows us to retain 93% (1521 out of 1642)
of biological names in the training set (90% in
the test set), while considering only 21% of all
possible candidates (5845 out of 27350 tokens).
It thus provides a good pre-lter which signif-
icantly improves the performance, in terms of
speed, of our system. The biological lexicons
we use were provided by the BioMIRE project,
and were derived from the resources available
at: http://iubio.bio.indiana.edu/.
For each candidate, three types of features
were considered. We rst retained the part-of-
speech and some spelling information (table 1).
These features were chosen based on the inspec-
tion of gene and protein names in our lexicons.
LexPROTEIN Protein lexicon
LexGENE Gene lexicon
LexSPECIES Biological species lexicon
LEXENGLISH General English lexicon
Table 2: Features provided by lexicons.
The second type of features relates to the pres-
ence of the candidate in our lexical resources
3
(table 2). Lastly, the third type of features de-
scribes contextual information. The context we
consider contains the four preceding and the
four following words. However, we did not take
into account the position of the words in the
context, but only their presence in the right or
left context, and in addition we replaced, when-
ever possible, each word by a feature indicating
(a) whether the word was part of the gene lex-
icon, (b) if not whether it was part of the pro-
tein lexicon, (c) if not whether it was part of
the species lexicon, (d) and if not, whenever the
candidate was neither a noun, an adjective nor
a verb, we replaced it by its part-of-speech.
For example, the word hairless is associated
with the features given in Table 3, when en-
countered in the following sentence: Inhibition
of the DNA-binding activity of Drosophila sup-
pressor of hairless and of its human homolog,
KBF2/RBP-J kappa, by direct protein{protein
interaction with Drosophila hairless. The word
hairless appears in the gene lexicon and is
wrongly recognized as an adjective by our tag-
ger.
4
The word human, the fourth word of
the right context of hairless, belongs to the
species lexicon, ans is thus replaced by the fea-
ture RC SPECIES. Neither Drosophila nor sup-
pressor belong to the specialized lexicons we
use, and, since they are both tagged as nouns,
they are left unchanged. Prepositions and con-
junctions are replaced by their part-of-speech,
and prexes LC and RC indicate whether they
were found in left or right context. Note that
since two prepositions appear in the left context
of hairless, the value of the LC PREP feature
is 2.
Altogether, this amounts to a total of 3690
possible features in the input space X .
3
Using these lexicons alone, the same task with the
same test data, yields: precision = 22%, recall = 76%.
4
Note that no adaptaion work has been conducted on
our tagger, which explains this error.
Feature Value
LexGENE 1
ADJ 1
LC drosophila 1
LC suppressor 1
LC PREP 2
RC CONJ 1
RC SPECIES 1
RC PRON 1
RC PREP 1
Table 3: Features of hairless in \...of Drosophila
suppressor of hairless and of its human...".
6.2 Results
In our experiments, we have used the following
methods:
 SVM trained with inductive inference, and
using a linear kernel, a polynomial kernel of
degree d = 2 and the so-called \radial ba-
sis function" kernel (Scholkopf and Smola,
2002).
 SVM trained with transductive inference,
and using a linear kernel or a polynomial
kernel of degree d = 2.
 SVM trained with inductive inference us-
ing Fisher kernels estimated from the whole
training data (without using labels), with
dierent number of classes c in the PLSA
model (4).
The proportion of labelled data is indicated
in the tables of results. For SVM with induc-
tive inference, only the labelled portion is used.
For transductive SVM (TSVM), the remaining,
unlabelled portion is used (without the labels).
For the Fisher kernels (FK), an unsupervised
model is estimated on the full dataset using
PLSA, and a SVM is trained with inductive
inference on the labelled data only, using the
Fisher kernel as similarity measure.
6.3 Transductive inference
Table 4 gives interesting insight into the ef-
fect of transductive inference. As expected, in
the limit where little unannotated data is used
(100% in the table), there is little to gain from
using transductive inference. Accordingly, per-
formance is roughly equivalent
5
for SVM and
% annotated: 1.5% 6% 24% 100%
SVM (lin) 41.22 45.34 49.67 62.97
SVM (d=2) 40.97 46.78 52.12 62.69
SVM (rbf) 42.51 49.53 51.11 63.96
TSVM (lin) 38.63 51.64 61.84 62.91
TSVM (d=2) 43.88 52.38 55.36 62.72
Table 4: F
1
scores(in %) using dierent propor-
tions of annotated data for the following models:
SVM with inductive inference (SVM) and lin-
ear (lin) kernel, second degree polynomial ker-
nel (d=2), and RBF kernel (rbf); SVM with
transductive inference (TSVM) and linear (lin)
kernel or second degree polynomial (d=2) ker-
nel.
TSVM, with a slight advantage for RBF kernel
trained with inductive inference. Interestingly,
in the other limit, ie when very little annotated
data is used, transductive inference does not
seem to yield a marked improvement over in-
ductive learning. This nding seems somehow
at odds with the results reported by Joachims
(1999) on a dierent task (text categorisation).
We interpret this result as a side-eect of the
search strategy, where one tries to optimise
both the size of the margin and the labelling
of the unannotated examples. In practice, an
exact optimisation over this labelling is imprac-
tical, and when a large amount of unlabelled
data is used, there is a risk that the approxi-
mate, sub-optimal search strategy described by
Joachims (1999) may fail to yield a solution that
is markedly better that the result of inductive
inference.
For the two intermediate situation, however,
transductive inference seems to provide a size-
able performance improvement. Using only 24%
of annotated data, transductive learning is able
to train a linear kernel SVM that yields approxi-
mately the same performance as inductive infer-
ence on the full annotated dataset. This means
that we get comparable performance using only
what corresponds to about 30 abstracts, com-
pared to the 122 of the full training set.
6.4 Fisher kernels
The situation is somewhat dierent for SVM
trained with inductive inference, but using
5
Performance is not strictly equivalent because SVM
and TSVM use the data dierently when optimising the
trade-o parameter C over a validation set.
% annotated: 1.5% 6% 24% 100%
SVM (lin) 41.22 45.34 49.67 62.97
SVM (d=2) 40.97 46.78 52.12 62.69
lin+FK8 46.08 42.83 54.59 63.92
lin+FK16 44.43 40.92 55.70 63.76
lin+combi 46.38 38.10 52.74 63.08
Table 5: F
1
scores(in %) using dierent propor-
tions of annotated data for the following mod-
els: standard SVM with linear (lin) and second
degree polynomial kernel (d=2); Combination
of linear kernel and Fisher kernel obtained from
a PLSA with 4 classes (lin+FK4) or 8 classes
(lin+FK8), and combination of linear and all
Fisher kernels obtained from PLSA using 4, 8,
12 and 16 classes (lin+combi).
Fisher kernels obtained from a model of the
entire (non-annotated) dataset. As the use
of Fisher kernels alone was unable to consis-
tently achieve acceptable results, the similarity
we used is a combination of the standard lin-
ear kernel and the Fisher kernel (a similar solu-
tion was advocate by Hofmann (2000)). Table 5
summarises the results obtained using several
types of Fisher kernels, depending on how many
classes were used in PLSA. FK8 (resp. FK16)
indicates the model using 8 (resp. 16) classes,
while combi is a combination of the Fisher ker-
nels obtained using 4, 8, 12 and 16 classes.
The eect of Fisher kernels is not as clear-cut
as that of transductive inference. For fully an-
notated data, we obtain results that are similar
to the standard kernels, although often better
than the linear kernel. Results obtained using
1.5% and 6% annotated data seem somewhat in-
consistent, whith a large improvement for 1.5%,
but a marked degradation for 6%, suggesting
that in that case, adding labels actually hurts
performance. We conjecture that this may be
an artifact of the specic annotated set we se-
lected. For 24% annotated data, the Fisher ker-
nel provides results that are inbetween induc-
tive and transductive inference using standard
kernels.
7 Discussion
The results of our experiments are encouraging
in that they suggest that both transductive in-
ference and the use of Fisher kernels are poten-
tially eective way of taking unannotated data
into account to improve performance.
These experimental results suggest the follow-
ing remark. Note that Fisher kernels can be
implemented by a simple scalar product (lin-
ear kernel) between Fisher scores r`(x) (equa-
tion 3). The question arises naturally as to
whether using non-linear kernels may improve
results. One one hand, Fisher kernels are
derived from information-geometric arguments
(Jaakkola and Haussler, 1999) which require
that the kernel reduces to an inner-product of
Fisher scores. On the other hand, polynomial
and RBF kernels often display better perfor-
mance than a simple dot-product. In order to
test this, we have performed experiments using
the same features as in section 6.4, but with a
second degree polynomial kernel. Overall, re-
sults are consistently worse than before, which
suggest that the expression of the Fisher kernel
as the inner product of Fisher scores is theoret-
ically well-founded and empirically justied.
Among possible future work, let us mention
the following technical points:
1. Optimising the weight of the contributions
of the linear kernel and Fisher kernel, eg
as K(x; y) =  hx; yi + (1   )FK(x; y),
 2 [0; 1].
2. Understanding why the Fisher kernel alone
(ie without interpolation with the linear
kernel) is unable to provide a performance
boost, despite attractive theoretical prop-
erties.
In addition, the performance improvement
obtained by both transductive inference and
Fisher kernels suggest to use both in cunjunc-
tion. To our knowledge, the question of whether
this would allow to \bootstrap" the unlabelled
data by using them twice (once for estimating
the kernel, once in transductive learning) is still
an open research question.
Finally, regarding the application that we
have targeted, namely entity recognition, the
use of additional unlabelled data may help us
to overcome the current performance limit on
our database. None of the additional experi-
ments conducted internally using probabilisitc
models and symbolic, rule-based methods have
been able to yield F
1
scores higher than 63-64%
on the same data. In order to improve on this,
we have collected several hundred additional
abstracts by querying the MedLine database.
After pre-processing, this yields more than a
hundred thousand (unlabelled) candidates that
we may use with transductive inference and/or
Fisher kernels.
8 Conclusion
In this paper, we presented a comparison be-
tween two state-of-the-art methods to combine
labelled and unlabelled data: Fisher kernels and
transductive inference. Our experimental re-
sults suggest that both method are able to yield
a sizeable improvement in performance. For ex-
ample transductive learning yields performance
similar to inductive learning with only about a
quarter of the data. These results are very en-
couraging for tasks where annotation is costly
while unannotated data is easy to obtain, like
our task of biological entity recognition. In ad-
dition, it provides a way to benet from the
availability of large electronic databases in or-
der to automatically extract knowledge.
9 Acknowledgement
We thank Anne Schiller,

Agnes Sandor and Vi-
olaine Pillet for help with the data and re-
lated experimental results. This research was
supported by the European Commission un-
der the KerMIT project no. IST-2001-25431
and the French Ministry of Research under the
BioMIRE project, grant 00S0356.
References
M. E. Cali, editor. 1999. Proc. AAAI Work-
shop on Machine Learning for Information
Extraction. AAAI Press.
M. Craven and J. Kumlien. 1999. Construct-
ing biological knowledge bases by extract-
ing information from text sources. In Proc.
ISMB'99.
A. Gammerman, V. Vovk, and V. Vapnik. 1998.
Learning by transduction. In Cooper and
Morla, eds, Proc. Uncertainty in Articial In-
telligence, pages 145{155.Morgan Kaufmann.
Eric Gaussier, Cyril Goutte, Kris Popat, and
Francine Chen. 2002. A hierarchical model
for clustering and categorising documents.
In Crestani, Girolami, and van Rijsbergen,
eds, Advances in Information Retrieval|
Proc. ECIR'02, pages 229{247. Springer.
Thomas Hofmann. 1999. Probabilistic latent
semantic analysis. In Proc. Uncertainty in
Articial Intelligence, pages 289{296.Morgan
Kaufmann.
Thomas Hofmann. 2000. Learning the similar-
ity of documents: An information-geometric
approach to document retrieval and catego-
rization. In NIPS*12, page 914. MIT Press.
Tommi S. Jaakkola and David Haussler. 1999.
Exploiting generative models in discrimina-
tive classiers. In NIPS*11, pages 487{493.
MIT Press.
Thorsten Joachims. 1999. Transductive in-
ference for text classication using support
vector machine. In Bratko and Dzeroski,
eds, Proc. ICML'99, pages 200{209. Morgan
Kaufmann.
M. Krauthammer, A. Rzhetsky, P. Morozov,
and C. Friedman. 2000. Using blast for iden-
tifying gene and protein names in journal ar-
ticles. Gene.
Y. Ohta, Y. Yamamoto, T. Okazaki,
I. Uchiyama, and T. Takagi. 1997. Au-
tomatic constructing of knowledge base from
biological papers. In Proc. ISMB'97.
D. Proux, F. Reichemann, and L. Julliard.
2000. A pragmatic information extraction
strategy for gathering data on genetic inter-
actions. In Proc. ISMB'00.
Bernhard Scholkopf and Alexander J. Smola.
2002. Learning with Kernels. MIT Press.
J. Thomas, D. Milward, C. Ouzounis, S. Pul-
man, and M. Caroll. 2000. Automatic ex-
traction of protein interactions from scientic
abstracts. In Proc. PSB 2000.
Kristina Toutanova, Francine Chen, Kris Popat,
and Thomas Hofmann. 2001. Text classica-
tion in a hierarchical mixture model for small
training sets. In Proc. ACM Conf. Informa-
tion and Knowledge Management.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley.
Yiming Yang and Xin Liu. 1999. A re-
examination of text categorization methods.
In Proc. 22nd ACM SIGIR, pages 42{49.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1114?1123, Dublin, Ireland, August 23-29 2014.
Fast Domain Adaptation of SMT models without in-Domain Parallel Data
Prashant Mathur
?
Fondazione Bruno Keseler
Povo - 38100 (IT)
first@fbk.eu
Sriram Venkatapathy
Xerox Research Center Europe
Meylan (FR)
first.last@xrce.xerox.com
Nicola Cancedda
?
Microsoft, London (UK)
first.last@gmail.com
Abstract
We address a challenging problem frequently faced by MT service providers: creating a domain-
specific system based on a purely source-monolingual sample of text from the domain. We solve
this problem by introducing methods for domain adaptation requiring no in-domain parallel data.
Our approach yields results comparable to state-of-the-art systems optimized on an in-domain
parallel set with a drop of as little as 0.5 BLEU points across 4 domains.
1 Introduction
We consider the problem of creating the best possible statistical machine translation (SMT) system for
a specific domain when no parallel sample or training data from such domain is available. We assume
that we have access to a collection of phrase tables (PT) and other models independently created from
now unavailable corpora, and we receive a monolingual source language sample from a text source we
would like to optimize for.
For a MT provider to deliver a SMT system tailored to a customer?s domain, a sample dataset is
requested. In most cases, the customer is able to provide an in-domain mono-lingual sample from his
operations. However, it is generally not feasible for the customer to provide the translations as well
because the customer has to hire professional translators to do that. In such a scenario, the translations has
to be generated by MT service provider itself by hiring human translators thus requiring an investment
upfront. The methods proposed in this paper aim to avoid that by building a good quality pilot SMT
system leveraging only sample mono-lingual source corpus, and previously trained library of models.
This in turn postpones the task of generating in-domain parallel data to a later date when there is a
commitment by the customer.
Unavailability of the raw parallel data could derive from a trading model where data owners share
intermediate-level resources like PTs, Reordering Models (RM) and Language Models (LM), but can
not, or do not want to, share the textual data such resources were derived from. This particular scenario
has been explained in (Cancedda, 2012).
This scenario is similar to the multi-model framework studied in (Sennrich et al., 2013), with the
additional challenge that no parallel development set is available. We build on the linear mixture model
combination of the cited work, extending it to our more challenging environment:
1. We propose a new measure derived from the popular BLEU score (Papineni et al., 2002) to assess
the fitness of a PT to cope with a given monolingual sample S. This measure is computed from
n-gram statistics that can be easily extracted from a PT.
2. We propose a new method for tuning the parameters of a log linear model that does not require
an in-domain parallel development set, and yet achieves results very close to traditional tuning on
parallel in-domain data.
We present our proposed metric BLEU-PT and computation of multi-model in Section 2. The pa-
rameter estimation of log-linear parameters of the SMT system is described in Section 3. We present
experiments and results in Sections 4 and 6 respectively.
?
Major part of the work was performed when the authors were in Xerox Research Center Europe.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1114
2 Building Multi-Model
Given a library of phrase tables, the goal of this step is to generate a domain adapted multi-model. The
challenging aspect in our scenario is the lack of in-domain parallel data, as well as absence of original
parallel corpora corresponding to the library of models. This rules out the possibility of using metrics
such as cross-entropy (Sennrich, 2012b) or LM-perplexity for computing the mixing coefficients. We
present our proposed metric in section 2.1, and interpolation of the phrase tables in section 2.2.
2.1 BLEU-PT
Given a source corpus s, and a set of phrase tables {pt
1
, pt
2
,. . . ,pt
n
}, the goal is to measure the similarity
of each of these tables with s. For measuring the similarity, we use BLEU-PT which is an adaptation of
the popular BLEU score for measuring the similarity between a corpus and a phrase table. The metric
BLEU-PT is measured as described in Equation 1.
BLEU-PT(PT, S) =
(
4
?
n=1
match(n|pt, s)
total(n|s)
)
1/4
(1)
where match(n|pt, s) is the count of n-grams of order n in the source corpus s that exist in the source
side of the phrase table pt. total(n|s) is the number of n-grams of order n in the source corpus.
2.2 Interpolating Models
A state-of-the-art approach for building multi-models is through linear interpolation of component mod-
els, exemplified in Equation 2 for the case of the forward conditional phrase translation model.
h
phr
(s, t) = log
N
?
j=1
?
j
P
phr,j
(t|s) (2)
Various approaches have been suggested for computing the coefficients ? of the interpolated model, the
most recent being perplexity minimization described in (Sennrich, 2012b), where each translation model
feature is optimized separately on the parallel development set. Our work is set in a scenario where no
parallel development set is available for optimizing the interpolation coefficients. We have also observed
that perplexity minimization is computationally intensive, requires aligned parallel development set, and
the optimization time increases rapidly with increasing number of component models (for details, see
Section 4.2).
We propose a simple approach for computation of the mixing coefficients that relies on the similarity
of each model with respect to the test set. The mixing coefficients are obtained by normalizing similarity
values. The similarity between a model (phrase table) and a corpus is computed using the BLEU-PT
metric proposed in the previous section. Another similarity metric that can be used is LM Perplexity.
However, in the current scenario we do not have resources (training data) to build a source side LM for
computing the perplexity.
We empirically compare our method for computing mixing coefficients with the the perplexity min-
imization method. We also experiment with applying the mixing coefficients obtained by using our
method for mixing features of a reordering and language model.
3 Parameter Estimation
The overall quality of translation is strongly impacted by how optimized the weights of the log-linear
combination of various translation features are for a domain of interest. MERT (Och, 2003) and MIRA
(Watanabe et al., 2007) are popular solution to compute an optimal weight vector by minimizing the error
on a held-out parallel development set. BLEU and its approximations are commonly used error metrics.
In this paper we assume lack of a parallel development set, therefore the above methods cannot be used.
Pecina et. al. (2012) showed that the optimized log-linear weight vector
1
of a SMT system does not
depend as much on the actual domain of the development set (on which the system was optimized), as
1
Not to be confused with the mixing coefficients in a linear combination of model components.
1115
on how ?distant? the relevant domain is from the domain of the training corpus used to build the SMT
models. This is an important finding. It means that the weight vector can be modeled as a function of the
distance/similarity between the in-domain development set and the model built from the training set. In
this work, we learn this function from examples of previous parameter optimizations, using our BLEU-
PT as a similarity metric. Once we have retrieved the most relevant PTs (translation and reordering
models) from our library, and we have linearly interpolated them using normalized BLEU-PT, we use
the learned model to estimate the optimal value of the log-linear weights, instead of optimizing them.
In order to learn this mapping, we create a dataset of examples (pairs of the form <BLEU-PT, log-
linear weight vector>, where weight vectors are normalized to ensure comparability across models) by
performing repeated optimizations for out-domain models on a number of parallel development sets (see
section 4 for more details of this data) using a traditional optimization method (MIRA in this work).
Based on this dataset, the function of our interest can therefore be learnt using a supervised approach.
We explore two parametric methods and a non-parametric method. We present these in Section 3.1, and
3.2 respectively. For a mono-lingual source in a new domain, the BLEU-PT can be computed, and then
mapped to the appropriate weight vector using the methods presented below.
3.1 Parametric Methods
We considered two distinct parametric methods for estimating the mapping from model/corpus similarity
into weight vectors. The first one makes the assumption that parameters can be estimated independently
of one another, given the similarity, whereas the second tries to leverage known covariance between
distinct parameters in the vector.
3.1.1 Linear Regression
Motivated by initial experiments highlighting strong correlation between BLEU-PT and optimal feature
weights (see Section 5.1 below), we assumed here a simple linear relation of the form:
?
?
i
= W
i
X + b
i
(3)
where ?
?
i
is the optimal log-linear weight for feature i, X is the feature vector (BLEU-PT vector), W
i
and b
i
are coefficients to be estimated. While a drastic assumption, this has the advantage of limiting
the risk of overfitting in a situation like ours where there is only relatively few datapoints to learn from.
We estimate a
i
and b
i
by simple least squares regression. Once these are available for all features, we
can predict the log linear weights of any model given its BLEU-PT similarity to a monolingual source
sample using Eq. 3.
3.1.2 Multi-Task learning
Optimal log-linear parameters might not be fully independent given BLEU-PT, especially since it is
known that model features can be highly correlated. To account for correlation between parameter
weights, we explore the use of multi-task lasso
2
(Caruana, 1997) where several functions corresponding
to each parameter are learned jointly considering the correlation between their values observed in the
training data. Multi-task lasso consists of a least square loss model trained along with a regularizer and
the objective is to minimize the following:
argmin
w
1
2N
||X ?W ? ?||
2
2
+ ?||W ||
21
where; ||W ||
21
=
M
?
j
?
?
i
w
2
ij
(4)
Here, N is the number of training samples, X is the feature vector(BLEU-PT score vector) ? is the label
vector(log linear weights). ||W ||
21
is the l
21
regularizer (Yang et al., 2011). The problem of prediction
of log linear weights is reduced to prediction of i interlinked tasks where each task has M features
3
.
Coefficients are calculated using coordinate descent algorithm in Multi-Task lasso. Once the coefficients
are calculated we use Eq. 3 to predict the log linear weights.
2
http://scikit-learn.org/
3
In our case we only have 1 feature i.e. BLEU-PT score.
1116
3.2 Non Parametric: Nearest Neighbor
Finally, instead of building a parametric predictor for log linear weights, we experimented with a simple
nearest-neighbor approach:
?
?
i
= ?
i
(M
j
?
) (5)
whereM
j
ranges over the linearly interpolated phrase tables, and ?
i
(M) returns the stored optimal value
for the i
th
log-linear weight, and:
j
?
= argmin
j
min
s
?
(|BLEU-PT(M, s)? BLEU-PT
?
(M
j
, s
?
)|) (6)
where s is the monolingual sample on which we want to calculate the BLEU-PT and s
?
ranges over
the source sides of our available parallel development sets. In other words, a BLEU-PT of a model is
calculated on the source sample to be translated and the log-linear weight is chosen which corresponds
to BLEU-PT
?
, where BLEU-PT
?
is a training data point closest to BLEU-PT. This approach is close to
the cross-domain tuning of Pecina et. al. (2012).
4 Experimental Program
We conducted a number of experiments for English-French language pair, comparing the methods pro-
posed in the previous sections among one another and against state-of-the-art baselines and oracles.
4.1 Datasets
In this section, we present the datasets (EN-FR) that we have used for our experiments and the training
data that was created for the purpose of supervised learning. We collected a set of 12 publicly available
corpora and 1 proprietary corpus, statistics of datasets are provided in Table 1.
Corpus Train Development Test
Commoncrawl 78M 12.4K 12.6K
ECB 4.7M 13.9K 14K
EMEA 13.8M 14K 15.7K
EUconst 133K 8K 8.4K
Europarl 52.8M 13.5K 13.5K
P1 5M 35K 14.5K
KDE4 1.5M 12.8K 5.8K
News Comm. 4M 12.7K 65K
OpenOffice 400K 5.4K 5.6K
OpenSubs 156M 16K 15.7K
PHP 314K 3.5K 4K
TED 2.65M 21K 14.6K
UN 1.92M 21K 21K
Table 1: Statistics of parallel sets (# of source tokens)
500 1000 1500 2000 2500 3000 3500 4000 4500Phrase Table size (MB)0
500
1000
1500
2000
2500
Search
 Time 
(secon
ds)
Calculating time of Metrics v/s Size of Phrase Tablebleu-ptcross entropy
Figure 1: BLEU-PT v/s Cross-Entropy
Commoncrawl (CC) (Smith et al., 2013) and News Commentary (Bojar et al., 2013) corpora were
provided in the 2013 shared translation task organized with workshop on machine translation. TED talks
data was released as a part of IWSLT evaluation task (Cettolo et al., 2012). ECB, EMEA, EUconst,
OpenOffice, OpenSubs 2011, PHP and UN corpora are provided as a part of OPUS parallel corpora
(Tiedemann, 2012). The parallel corpora from OPUS were randomly split into training, development
and testsets. Commoncrawl, News Commentary and TED datasets were used as they were provided in
the evaluation task.
Out of 13 different domain datasets we selected 4 datasets randomly: Commoncrawl, KDE4, TED and
UN (in bold in Table 1), to test our methods.
4.2 BLEU-PT v/s Cross-Entropy
We compared the overheads of calculating BLEU-PT and Cross-Entropy
4
. We are interested in estimat-
ing whether with increasing number of phrase tables the computation of both measures becomes slow or
memory intensive.
4
We used tmcombine.py script that comes along with the moses package to calculate the mixing coefficients.
1117
Another advantage of using BLEU-PT apart from fast retrieval is that we can index the phrase tables
using wFSA based indexing (explanation of indexing the phrase tables is not in the scope of this paper)
and store the FSTs in binarised format on disk. When a source sample comes, we just load the indexed
binaries and calculate the BLEU-PT while this cannot be achieved when we want to calculate cross
entropy because we have to do one pass over all the phrase tables in question.
Experimental results depicted in Figure 1 shows that computation of BLEU-PT is fast (160 seconds)
while computation of cross-entropy is slow (42 minutes) when we combine 12 phrase tables with total
size of 4.2GB.
4.3 Training data for supervised learning and testing
As mentioned earlier, for estimating the parameters we require a training data containing the tuples of
<BLEU-PT, log-linear-weight>. We perform parameter estimation on four of our datasets: Common-
crawl, KDE4, TED and UN. So, for obtaining evaluation results on say, UN, the rest of the resources
are used for generating the training data. Our experimental setup can be explained well using the Venn
diagram shown in Figure 2.
We set one of four domains as the test domain (in this case, UN) whose parallel set is not available to us
and call it setup-UN. The training data tuples obtained from the rest of the 12 datasets are used to estimate
parameters for the UN domain. From these 12 datasets we perform a round-robin experiment where one
by one each dataset is considered as in-domain and the rest as out-domain. In-domain dataset provides the
development set and the rest 11 out-domain models are linearly combined to build translation models.
In figure 2, for example, the development set from the TED domain is taken as the development set
of the multi-model build using the rest (i.e. excluding TED and UN). This multi-model is built by a
weighted linear combination of the out-domain models (11 models). The parameters of this multi-model
are tuned on the in-domain development set using MIRA. Simultaneously, we also calculate the BLEU-
PT of the linear interpolated model on the source side of the in-domain development set (i.e. TED).
This provides us the tuples of BLEU-PT and the log linear weights, which is our training data. So, four
sets of experiments are conducted (one each for four datasets considered for testing), and for each set
of experiments, there are 12 training data points. The final evaluation is done by measuring the BLEU
score obtained on each test set using the predicted parameter estimates.
Reiterating, our optimizing method is fast, and hence, we are not not looking to learn the parameters
apriori for all the domains based on a source side of the development set. The goal is to do a fast
adaptation by predicting the parameters using statistical models for every new test in a particular domain
even in the absence of a parallel development set.
4.4 Prediction
For prediction of parameters for a new domain, the BLEU-PT of the sample source corpus (UN in our
example) is measured with the multi-model built on all the models (all the rest of 12 datasets including
the TED model) and then the supervised predictor is applied. In our experiments, we test both parametric
and non-parametric methods to estimate the parameters based on the training data obtained using the 12
domains.
TEST
In-domain
UN
...
...
EMEA
ECB
KDE
PHP
DEV
In-domain
TED
Figure 2: Cross domain tuning setup
Figure 3: Correlation of log linear weights with BLEU-PT
when indomain sets set to UN and TED
1118
Domain Linear Interpolation
System Train Dev Param. Est. TM(coeff.) RM(coeff.) LM(coeff.)
in-dom-train In In mira N.A N.A N.A
mira-bleupt-tm-rm Out In mira 3 3 7
mira-perp-tm-bleupt-rm Out In mira 3(Perp. Min) 3 7
mira-bleupt-tm-rm-perp-lm Out In mira 3 3 3(LM Perp. Min.)
mira-bleupt-all Out In mira 3 3 3
def-bleupt-all Out 7 def 3 3 3
gen-reg-bleupt-all Out 7 regression 3 3 3
gen-mtl-bleupt-all Out 7 multi-task 3 3 3
gen-nn-bleupt-all Out 7 Near.Neigh. 3 3 3
top5-reg-bleupt-all Out 7 regression 3 3 3
top5-mtl-bleupt-all Out 7 multi-task 3 3 3
top5-nn-bleupt-all Out 7 Near.Neigh. 3 3 3
Table 2: System Description: Each system?s training domain and development set domain along with the optimizer/predictor
is mentioned. def-bleupt-all uses default weights from Moses decoder. Near.Neigh. shows that we used Nearest Neighbor
predictor for optimizing weights. 7 represent log linear interpolation of models while 3 represents linear interpolation. The
mixing coefficients for linear interpolation are calculated by normalizing bleu-pt scores unless mentioned otherwise.
5 Experiments and Results
5.1 Correlation analysis
Before embarking in the actual regression task, we examined the correlation between the similarity values
(BLEU-PT) and the various weights in the training data. If there is good correlation between BLEU-PT
and a particular parameter, then the linear regressor is expected to fit well and then predict an accurate
parameter value for a new domain. For computing the correlation, we use Pearson correlation coefficient
(PCC). Figure 3 shows the PCC between the feature weights and the BLEU-PT scores. The tm?s are the
translation model features, and rm?s are the reordering model features.
We see that there is either a strong positive correlation or a strong negative correlation for most fea-
tures in both the experimental setups shown in the figure 3. This validates our hypothesis that optimal
parameters for a new test domain can indeed be estimated with good reliability. One can also observe
that the correlation level also varies based on the mixture of training models. For example, the correla-
tion is much higher in the training data that excluded UN (setup-UN) than the one that excluded TED
(setup-TED).
In figure 3, one can also see that tm0 (forward phrase conditional probability) and tm2 (backward
phrase conditional probability) which are shown in previous work to be the two most important features
amongst all SMT features (Lopez and Resnik, 2006) in terms of their impact on translation quality, have
a high correlation in setup-UN.
5.2 Systems
All SMT systems were built using the Moses toolkit (Koehn et al., 2007). To automatically align the
parallel corpora we used MGIZA (Gao and Vogel, 2008). Aligned training data in each domain was
then used to create the corresponding component translation models and lexical reordering models. We
created 5-gram language models for every domain using SRILM (Stolcke, 2002) with improved Kneser-
Ney smoothing (Chen and Goodman, 1999) on the target side of the training parallel corpora. Log linear
weights for the systems were optimized using MIRA (Watanabe et al., 2007; Hasler et al., 2011) which
is provided in the Moses toolkit. Performance of the systems are measured in terms of BLEU computed
using the MultEval script (mteval-v13.pl).
We built one in-dom-train system where only in-domain training data is taken into account. This
system shows the importance of in-domain training data in SMT (Haddow and Koehn, 2012). Three
oracle systems are trained on out-domain training corpus and tuned on in-domain development data (in
this case there are four domains we chose to test on: UN, TED, CommonCrawl and KDE4), thus 4
systems for each of the in-domain test sets.
We build another set of SMT systems in which language models are combined by linear interpolation
5
.
5
Linear interpolation of 12 LMs result in one single large LM, thus, one weight. So, a total of 14 weights have to be
optimized or predicted
1119
The systems using linear interpolated LM (mixing coefficients are normalized BLEU-PT scores) are def-
bleupt-all, mira-bleupt-all, gen-reg-bleupt-all, gen-mtl-bleupt-all and gen-nn-bleupt-all. We compare
mira-bleupt-all with mira-bleupt-tm-rm-perp-lm where mixing coefficients for LM interpolation are cal-
culated by standard LM perplexity minimization method over target side of development set.
As mentioned earlier, ideally only a subset of all the models closer to the source sample should be
taken into account for quick adaptation, so we select the top five domains related to the source sample
and interpolate the respective models and address them as top5-* systems. Adding more domains would
unnecesary increase the size of the model and add more noise. Table 2 shows the configuration of
different systems. In the next section we compare the performances of these systems and report the
findings.
6 Results and Discussion
Table 3 presents results of the systems that use an in-domain parallel data. As expected, when an in-
domain corpus is used both for training as well as for optimizing the log-linear parameters, the pefor-
mance is much higher than those systems that do not use in-domain parallel corpus for training (Koehn
and Schroeder, 2007). We also observe that the use of normalized BLEU-PT for computing mixing
coefficients gives comparable performance to using Cross-Entropy. The primary advantage in using
BLEU-PT is that it can be compute much faster than Cross-Entropy (as shown in Figure 1). Evidently,
normalized BLEU-PT scores as mixing coefficients performs at par with mixing coefficients retrieved by
standard perplexity minimization method (Bertoldi and Federico, 2009). One can also use BLEU-PT for
LM interpolation in cases where target side in-domain text is not available.
System UN TED CC KDE
in-dom-train 67.87 29.98 26.62 35.82
mira-bleupt-tm-rm 44.14 31.20 17.43 24.25
mira-perp-tm-bleupt-rm 43.56 31.36 17.54 24.72
mira-bleupt-tm-rm-perp-lm 43.96 31.85 18.45 23.39
mira-bleupt-all 43.66 32.04 18.44 23.09
Table 3: Comparison of In-Domain system versus the estab-
lished Oracles in different setups.
System UN TED CC KDE
gen-reg-bleupt-all 43.27 32.18 17.95 21.05
gen-mtl-bleupt-all 43.35 32.61 18.26 20.67
gen-nn-bleupt-all 42.73 31.04 18.24 21.85
Table 4: Performance of generic systems (gen-*) in all se-
tups.
Table 4 illustrates the impact of phrase table retrieval on the performance of multi-model. All the
systems presented in this table use BLEU-PT for computing mixing coefficients, while the weights
are computed using the three techniques that we explored in this paper. We see that in case of re-
gression, the phrase table retrieval also results in a better MT performance. In the other two cases,
the results are comparable. It shows that retrieval helps in building smaller sized multi-models while
being more accurate on an average. Phrase table retrieval, thus, becomes particularly useful when a
multi-model needs to be built from a library of dozens of pre-trained phrase tables of various domains.
 15.5
 16
 16.5
 17
 17.5
 18
 18.5
 2  4  6  8  10  12
BLEU
 sco
re
Number of Models
Nearest NeighborRegressionMulti-Task Learning
Figure 4: BLEU scores when top k models were
used to evaluate commoncrawl test set where
k ? 1..12.
System UN TED CC KDE
def-bleupt-all 42.03 30.82 17.97 19.66
mira-bleupt-all 43.66 32.04 18.44 23.09
top5-reg-bleupt-all 43.39
N
32.31
N
18.10 21.54
N
top5-mtl-bleupt-all 43.56
N
32.60
N
18.14 20.91
N
top5-nn-bleupt-all 42.96
N
30.89
M
17.79 22.24
N
Table 5: Comparing the baseline system (def-bleupt-all)
and Oracle (mira-bleupt-all) with domain specific multi-model
systems trained on top5 domains.
N
and
M
denotes significantly
better results in comparison with def-bleupt-all system with
p-value < 0.0001 and < 0.05 respectively.
Table 5 compares our approach of computing log-linear weights (in the absence of in-domain develop-
ment set) to the state-of-art weight optimization technique MIRA (which requires an in-domain devel-
opment set). As a baseline, we set default weights to all the parameters, which was shown to a strong
1120
baseline in (Pecina et al., 2012). We see that the methods proposed by us perform significantly bet-
ter than the default weights baseline (improvement of more than 1.5 BLEU score on an average across 4
domains). Among the three approaches for computing weights, the method that uses multi-task lasso per-
forms best (except in setup-KDE where the non-parametric method performs best), along the expected
lines as multi-task lasso considers the correlation between various features. In comparison to MIRA, our
methods result in an average drop of as little as 0.5 BLEU points across 4 domains (see Table 5).
Figure 4 shows BLEU score curve when we vary the k in top-k systems. BLEU score curve is almost
tangential zero when k is between 5 and 6 which essentially means that selection of k = 5 is a good
choice. For CommonCrawl test set, the top five domains used were Europarl, OpenSubs, NewsCom-
mentary, TED and ECB. This is a significant result which indicates that one can build a good system for
a domain even in the absence of the parallel data in the domain of interest.
7 Related Work
Domain adaptation in statistical machine translation has been widely studied and leveraged through
adding more training data (Koehn and Knight, 2001), filtering of out of domain training data (Axelrod
et al., 2011; Koehn and Haddow, 2012), fillup technique (Bisazza et al., 2011), language model adap-
tation by perplexity minimization over in-domain data (Bertoldi and Federico, 2009) and various other
approaches. However, all the above adaptation approaches require either parallel in-domain corpus or
monolingual in-domain target side corpus, thus, not applicable in our scenario.
In this paper we studied mixture modelling of heterogeneous translation models which was first pro-
posed in Foster et. al. (2007). They showed various ways of computing mixing coefficients for linear
interpolation using several distance based metrics borrowed from information theory. However, to cal-
culate any such metrics it was required that one has an access to the source/target training corpus and
source/target development corpus. Other noteable works in mixture modelling in SMT are (Civera and
Juan, 2007; Razmara et al., 2012; Duan et al., 2010).
More recently, Sennrich (2012b) designed an approach to calculate mixing coefficients by minimizing
the perplexity of translation models over an aligned development set for mixture modelling via linear
interpolation or by weighting the corpora. Sennrich et. al. (2012a) clustered of a large heterogeneous
development corpus and tuned a translation system on different clusters. In the decoding phase each
sentence was assigned to a cluster and the translation system tuned on that cluster was used to translate
that sentence.
(Banerjee et al., 2010) build several domain specific translation systems, and trained a classifier to
assign each incoming sentence to a domain and use the domain specific system to translate the corre-
sponding sentence. They assume that each sentence in test set belongs to one of the already existing
domains which means it would fail in the case where the sentence doesn?t belong to any of the existing
domains. In our case we do not make any such assumptions.
Academically, above approaches are well suited for solving the problem of domain adaptation, but
during the deployment of SMT systems in industrial scenario where the client is unable to deliver the
parallel in-domain data these approaches fail to provide a quick solution.
8 Conclusion
We present an approach to multi-model domain adaptation in a particularly challenging setting where
there is no parallel in-domain data. Parameter estimation without in-domain development set is a problem
that, to the best of our knowledge, has not been addressed before. We designed a method for tuning model
parameters without parallel development set and validated it through an experimental program for which
we compared performances against an array of Oracles and Baselines. The effectiveness of the proposed
method empirically supports the findings of (Pecina et al., 2012), who discovered that the log linear
weights largely depend on the distance of training domain from the domain on which the models are
being optimized on. As a side result, we designed in the process a novel similarity metric between a
phrase table and a source sample and implemented it effectively using wFSAs. We empirically showed
the excellent computation speed of BLEU-PT scores as compared to standard Cross-Entropy measure
using standard toolkits.
1121
Acknowledgement
The authors thank the three anonymous reviewers for their comments and suggestions.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
355?362, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Kumar Naskar, Andy Way, and Josef Van Genabith. 2010. Com-
bining multi-domain statistical machine translation models using automatic classifiers. In Proceedings of 9th
Conference of the Association for Machine Translation in the Americas.
Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolin-
gual resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ?09, pages
182?189, Stroudsburg, PA, USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In International Workshop on Spoken Language Translation (IWSLT), pages 136?143, San
Francisco, CA.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Nicola Cancedda. 2012. Private access to phrase tables for statistical machine translation. In ACL (2), pages
23?27.
Rich Caruana. 1997. Multitask learning. Mach. Learn., 28(1):41?75, July.
Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit
3
: Web inventory of transcribed and translated
talks. In Proceedings of the 16
th
Conference of the European Association for Machine Translation (EAMT),
pages 261?268, Trento, Italy, May.
Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 4(13):359?393.
Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 177?180, Prague, Czech
Republic, June. Association for Computational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 2010. Mixture model-based minimum bayes risk de-
coding using multiple machine translation systems. In Proceedings of the 23rd International Conference on
Computational Linguistics, pages 313?321. Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ?08, pages 49?57, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on smt systems. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 422?432, Montreal, Canada, June.
Association for Computational Linguistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2011. Margin Infused Relaxed Algorithm for Moses. The Prague
Bulletin of Mathematical Linguistics, 96:69?78.
Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 317?321,
Stroudsburg, PA, USA. Association for Computational Linguistics.
1122
Philipp Koehn and Kevin Knight. 2001. Knowledge sources for word-level translation models. In In Proceedings
of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 27?35.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statisti-
cal Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech
Republic.
Adam Lopez and Philipp Resnik. 2006. Word-based alignment, phrase-based translation: What?s the link? In In
Proceedings of AMTA, pages 90?99.
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu. 2002. BLEU : a Method for Automatic Evalua-
tion of Machine Translation. In Computational Linguistics, volume pages, pages 311?318.
Pavel Pecina, Antonio Toral, and Josef van Genabith. 2012. Simple and effective parameter tuning for domain
adaptation of statistical machine translation. In COLING, pages 2209?2224.
Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models
in statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages 940?949. Association for Computational Linguistics.
Rico Sennrich, Holger Schwenk, and Walid Aransa. 2013. A multi-domain translation model framework for
statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 832?840, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Rico Sennrich. 2012a. Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine
translation. In Proceedings of the 16th Annual Conference of the European Association of Machine Translation
(EAMT).
Rico Sennrich. 2012b. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez.
2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1374?1383, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, Denver,
Colorado.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical
machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech
Republic, June. Association for Computational Linguistics.
Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. 2011. l 2, 1-norm regularized discriminative
feature selection for unsupervised learning. In Proceedings of the Twenty-Second international joint conference
on Artificial Intelligence-Volume Volume Two, pages 1589?1594. AAAI Press.
1123
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 606?615,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Minimum Error Rate Training by Sampling the Translation Lattice
Samidh Chatterjee?
Department of Computer Science
Florida State University, USA
chatterj@cs.fsu.edu
Nicola Cancedda
Xerox Research Centre Europe
6 Chemin de Maupertuis, 38240 Meylan, France
nicola.cancedda@xrce.xerox.com
Abstract
Minimum Error Rate Training is the algo-
rithm for log-linear model parameter train-
ing most used in state-of-the-art Statistical
Machine Translation systems. In its original
formulation, the algorithm uses N-best lists
output by the decoder to grow the Transla-
tion Pool that shapes the surface on which
the actual optimization is performed. Recent
work has been done to extend the algorithm
to use the entire translation lattice built by
the decoder, instead of N-best lists. We pro-
pose here a third, intermediate way, consist-
ing in growing the translation pool using sam-
ples randomly drawn from the translation lat-
tice. We empirically measure a systematic im-
provement in the BLEU scores compared to
training using N-best lists, without suffering
the increase in computational complexity as-
sociated with operating with the whole lattice.
1 Introduction
Most state-of-the-art Statistical Machine Translation
(SMT) systems are based on a log-linear model of
the conditional probability of generating a certain
translation given a specific source sentence. More
specifically, the conditional probability of a transla-
tion e and a word alignment a given a source sen-
tence f is modeled as:
?The work behind this paper was done during an intern-
ship at the Xerox Research Centre Europe. The author was par-
tially supported by NSF through Grant CCF-0643593 and the
AFOSR Young Investigator Research Program.
P (e, a|f) ? exp
(
K
?
k=1
?khk (e,a,f)
)
(1)
where the hk(e,a,f) are feature functions provid-
ing complementary sources of information on the
quality of the produced translation (and alignment).
Once such a model is known: the decoder (i.e. the
actual translation program), which builds a transla-
tion by searching in the space of all possible transla-
tions the one that maximizes the conditional proba-
bility:
(e?, a?) = arg max
e,a
K
?
k=1
?khK(e,a,f) (2)
where we have taken into account that the exponen-
tial is monotonic.
The parameters ?k determine the relative impor-
tance of the different feature functions in the global
score. Best results are typically obtained by search-
ing in the space of all possible parameter vectors ??
for the one that minimizes the error on a held-out
development dataset for which one or more refer-
ence human translations are available, as measured
by some automatic measure. This procedure is re-
ferred to as Minimum Error Rate Training (MERT).
1.1 Minimum Error Rate Training on N-best
Lists
The most widespread MERT algorithm is the one
described in (Och, 2003). This algorithm starts
by initializing the parameter vector ??. For each
source sentence in the development set, the decoder
606
is used to initialize a translation pool with a list of N-
best scoring candidate translations according to the
model. Using this pool and the corresponding refer-
ence translations then, an optimization procedure is
run to update the parameter vector to a ??? with re-
duced error. The decoder is then invoked again, the
new output N-best list is merged into the translation
pool, and the procedure is iterated. The algorithm
stops either after a predefined number of iterations
or upon convergence, which is reached when no new
element is added to the translation pool of any sen-
tence, or when the size of the update in the parameter
vector is below a threshold.
The error measure minimized at each iteration is
usually BLEU (Papineni et al, 2002). BLEU essen-
tially measures the precision with which the trans-
lation produced by a system recovers n-grams of
different orders from the available reference trans-
lation(s), used as a gold standard.
The optimization procedure that is run within
each iteration on the growing translation pools is
based on the key observation that BLEU only de-
pends on the single translation receiving the highest
score by the translation model (which would be the
one shown to the receipient) in the translation pool.
This in turn means that, for any given sentence, its
contribution to BLEU changes only when the value
of the parameters change in such a way that the sen-
tence ranking first according to the model switches
from one to another. This situation does not change
when one considers all the sentences in a develop-
ment set instead of just one: while varying the ??
vector, the BLEU score changes only when there is
a change at the top of the ranking of the alternatives
for at least one sentence in the set. In other words,
BLEU is piece-wise constant in ??. MERT then pro-
ceeds by performing an iterative line search by fix-
ing each time the value of all components of ?? but
one1: for such a free parameter a global optimum
can be identified by enumerating all the points that
cause a change in BLEU. The value of the compo-
nent is then fixed at the middle of an interval with
maximum BLEU, and the procedure is iterated un-
til convergence. Since the error function is highly
irregular, and the iterative line search is not guaran-
1More generally, one can select each time a combination of
coordinates identifying a line in the parameter space, and is not
restricted to a coordinate direction.
teed to converge to a global optimum, the procedure
is repeated many times with different initializations,
and the best convergence point is retained.
The MERT algorithm suffers from the following
problem: it assumes at each iteration that the set
of candidates with a chance to make it to the top
(for some value of the parameter vector) is well
represented in the translation pool. If the transla-
tion pool is formed in the standard way by merg-
ing N-best lists, this assumption is easily violated in
practice. Indeed, the N-best list often contains only
candidates displaying minor differences, and repre-
sents only a very small sample of alternative possi-
ble translations, strongly biased by the current pa-
rameter setting.
Recognizing this shortcoming, Macherey et al
(2008) extended the MERT algorithm so as to use
the whole set of candidate translations compactly
represented in the search lattice produced by the de-
coder, instead of only a N-best list of candidates
extracted from it. This is achieved via an elegant
but relatively heavy dynamic programming algo-
rithm that propagates sufficient statistics (called en-
velopes) throughout the whole search graph. The re-
ported theoretical worst-case complexity of this al-
gorithm is O(|V ||E| log |E|), where V and E are the
vertex set and the edge set of the lattice respectively.
We propose here an alternative method consist-
ing in sampling a list of candidate translations from
the probability distribution induced by the transla-
tion lattice. This simple method produces a list of
candidates more representative of the complete dis-
tribution than an N-best list, side-stepping the in-
tricacies of propagating envelopes throughout the
lattice. Computational complexity increases only
marginally over the N-best list approach, while still
yielding significant improvements in final transla-
tion quality.
1.2 The translation lattice
Finding the optimal translation according to Equa-
tion 1 is NP-complete (Knight, 1999). Most phrase-
based SMT systems resort then to beam-search
heuristic algorithms for solving the problem approx-
imately. In their most widespread version, PBSMT
decoders proceed by progressively extending trans-
lation prefixes by adding one new phrase at a time,
and correspondingly ?consuming? portions of the
607
source sentence. Each prefix is associated with a
node in a graph, and receives a score according to
the model. Whenever two prefixes having exactly
the same possible extensions are detected, the lower-
scoring one is merged into the other, thus creating a
re-entrancy in the directed graph, which has then the
characteristics of a lattice (Figure 1). Edges in the
lattice are labelled with the phrase-pair that was used
to perform the corresponding extension, the source
word positions that were covered in doing the ex-
tension, and the corresponding increment in model
score.
0
F
I have a
J?ai une
(1,2,3)
?12.24
bleue
(4)
...
blue
voiture
(5)
...
car
J?ai une bleue
I have a blue
(1,2,3,4)
...
2 3
1
a blue car
une voiture bleue
(3,4,5)
...
I have
J?ai
(1,2)
...
Figure 1: A lattice showing some possible translations of
the English sentence: I have a blue car. The state with
ID 0 is the start state and the one with F is the final state.
2 Related Work
Since its introduction, (Och, 2003) there has been
various suggestions for optimizing the MERT cri-
terion. Zens et al (2007) use the MERT criterion
to optimize the N-best lists using the Downhill
Simplex Algorithm (Press, 2007). But the Down-
hill Simplex Algorithm loses its robustness as the
dimension goes up by more than 10 (Macherey
et al, 2008). Deterministic Annealing was sug-
gested by Smith and Eisner (2006) where the au-
thors propose to minimize the expected loss or
risk. They define the expectation using a proba-
bility distribution over hypotheses that they gradu-
ally anneal to focus on the 1-best hypothesis. Dif-
ferent search strategies were investigated by Cer
et al (2008). Work has been done to investigate a
perceptron-like online margin training for statisit-
ical machine translation (Watanabe et al, 2007).
Building on this paper, the most recent work to
our knowledge has been done by Chiang et al
(2008). They explore the use of the Margin Infused
Relaxed Algorithm (MIRA) (Crammer and Singer,
2003; Crammer et al, 2006) algorithm instead of
MERT. Macherey et al (2008) propose a new varia-
tion of MERT where the algorithm is tuned to work
on the whole phrase lattice instead of N-best list
only. The new algorithm constructs the error surface
of all translations that are encoded in the phrase lat-
tice. They report significant convergence improve-
ments and BLEU score gains over N-best MERT
when trained on NIST 2008 translation tasks. More
recently, this algorithm was extended to work with
hypergraphs encoding a huge number of translations
produced by MT systems based on Synchronous
Context Free Grammars (Kumar et al, 2009). All
the methods cited here work on either N-best lists or
from whole translation lattices built by the decoder.
To our knowledge, none of them proposes sampling
translations from the lattice.
3 Sampling candidate translations from
the lattice
In this section we first start by providing an intu-
ition of why we believe it is a good idea to sample
from the translation lattice, and then describe in de-
tail how we do it.
3.1 An intuitive explanation
The limited scope of n-best lists rules out many al-
ternative translations that would receive the highest
score for some values of the parameter vector. The
complete set of translations that can be produced us-
ing a fixed phrase table (also called reachable trans-
lations) for a given source sentence can be repre-
sented as a set of vectors in the space spanned by
the feature functions (Fig. 2). Not all such transla-
tions stand a chance to receive the highest score for
any value of the parameter vector, though. Indeed, if
translations h, h?and h? are such that hk ? h?k ? h??k
for all feature k, then there is no value of ?? that
will give to h? a score higher than both h and h?.
The candidates that would rank first for some value
of the ?? parameter vector are those on the convex
envelope of the overall candidate set. We know of
no effective way to generate this convex envelope in
608
polynomial time. The set of candidates represented
by the decoder lattice is a subset (enclosed in the
larger dashed polygon in the figure) of this set. This
subset is biased to contain translations ranking high
according to the values of the parameter vector (the
direction labelled with ?) used to produce it, because
of the pruning strategies that guide the construction
of the translation lattice. Both the N-best list and
our proposed random sample are further subsets of
the set of translations encoded in the lattice. The N-
best list is very biased towards translations that score
high with the current choice of parameters: its con-
vex envelope (the smaller dashed polygon) is very
different from the one of the complete set of trans-
lations, and also from that of the translations in the
lattice. The convex envelope of a random sample
from the translation lattice (the dotted polygon in the
figure), will generally be somewhat closer to the en-
velope of the whole lattice itself.
The curves in the figure indicate regions of con-
stant loss (e.g. iso-BLEU score, much more irregu-
larly shaped in reality than in the drawing). For this
sentence, then, the optimal choice of the parameters
would be around ??. Performing an optimization
step based on the random sample envelope would
result in a more marked update (??
sample) in the di-
rection of the best parameter vector than if an N-best
list is used (??N-best).
Notice that Figure 2 portraits a situation with only
two features, for obvious reasons. In practice the
number of features will be substantially larger, with
values between five and twenty being common prac-
tice. In real cases, then, a substantially larger frac-
tion of reachable translations will tend to lie on the
convex envelope of the set, and not inside the convex
hull.
3.2 The sampling procedure
We propose to modify the standard MERT algorithm
and sample N candidates from the translation lattice
according to the probability distribution over paths
induced by the model, given the current setting of
the ?? parameters, instead of using an N-best list.
The sampling procedes from the root node of the
lattice, corresponding to an empty translation can-
didate covering no words of the source, by chosing
step by step the next edge to follow. The probability
reference
??
??
??
h1
h2
?
??
best in lattice
best in random sample
best in N?best list
best reachable
N?best
sample
lattice
Figure 2: Envelope of the set of reachable translations
where the model has two feature functions h1 and h2.
The envelope of the lattice is the outer dashed polygon,
while the envelope of the N-best list is the inner one. Us-
ing the whole lattice as translation pool will result in a
more marked update towards the optimal parameters. The
random sample from the lattice is enclosed by the dotted
line. If we use it, we can intuitively expect updates to-
wards the optimum of intermediate effectiveness between
those of the N-best list method and those of the lattice
method.
distribution for each possible follow-up is the poste-
rior probability of following the edge given the path
prefix derived from the lattice: it is obtained via a
preliminary backward sweep.
Since feature functions are incremental over the
edges by design, the non-normalized probability of
a path is given by:
P (e1, . . . , em) = e
Pm
i=1 ?(ei) (3)
where
?(ei) =
K
?
k=1
?khk(ei) (4)
is the score of edge ei. With a small abuse of no-
tation we will also denote it as ?(nj,k), where it is
intended that ei goes from node nj to node nk. Let?s
denote with ?(ni) the score of node ni, i.e. the loga-
rithm of the cumulative unnormalized probability of
all the paths in the lattice that go from node ni to a
final node. The unnormalized probability of select-
ing node nj starting from ni can then be expressed
recursively as follows:
609
S(nj |ni) ? e(?(nj)+?(ni,j)) (5)
The scores required to compute this sampling
probabilities can be obtained by a simple backward
pass in the lattice. Let Pi be the set of successors
of ni. So the total unnormalized log-probability of
reaching a final state (i.e. with a complete transla-
tion) from ni is given by the equation below.
?(ni) = log(
?
nj?Pi
e(?(nj)+?(ni,j))) (6)
where we set ?(ni) = 0 if Pi = ?, that is if ni
is a final node. At the end of the backward sweep,
?(n0) contains the unnormalized cumulative prob-
ability of all paths, i.e. the partition function. No-
tice that this normalising constant cancels out when
computing local sampling probabilities for traversed
nodes in the lattice.
Once we know the transition probability (Eq. 5)
for each node, we sample by starting in the root node
of the lattice and at each step randomly selecting
among its successors, until we end in the final node.
The whole sampling procedure is repeated as many
times as the number of samples sought. After col-
lecting samples for each sentence, the whole list is
used to grow the translation pool.
Notice that when using this sampling method it is
no longer possible to use the stability of the trans-
lation pool as a stopping criterion. The MERT al-
gorithm must thus be run either for a fixed number
of iterations, or until the norm of the update to the
parameter vector goes below a threshold.
3.3 Time Complexity Analysis
For each line search in the inner loop of the MERT
algorithm, all methods considered here need to com-
pute the projection of the convex envelope that can
be scanned by leaving all components unchanged
but one2. If we use either N-best lists or random
samples to form the translation pool, and M is the
size of the translation pool, then computing the en-
velope can be done in time O(M log M) using the
SweepLine algorithm reproduced as Algorithm 1 in
(Macherey et al, 2008). As shown in the same ar-
ticle, the lattice method for computing the envelope
2In general, moving along a 1-dimensional subspace of the
parameter space.
is O(|V ||E| log |E|), where V is the vertex set of the
lattice, and E is its edge set. In standard decoders
there is a maximum limit D to the allowed distor-
tion, and lattice vertices are organized in J priority
queues 3 of size at most a, where J is the length of
the source sentence and a is a parameter of the de-
coder set by the user. Also, there is a limit K to
the maximum number of source words spanned by
a phrase, and only up to c alternative translations
for a same source phrase are kept in the phrase ta-
ble. Under these standard conditions, the number
of outgoing edges E? from each lattice vertex can
be bounded by a constant. A way to see this is by
considering that if an hypothesis is extended with a
phrase, then the extended hypothesis must end up in
a stack at most K stacks to the right of the original
one. There are only aK places in these stacks, so it
must be |E?| ? aK.
Since the number of edges leaving each node is
bounded by a constant, it is |E| = ?(|V |), and the
lattice method is O(|V |2 log(|V |)). The maximum
number of vertices in the lattice is limited by the
capacity of the stacks: |V | ? aJ . This eventually
leads to a complexity of O(J2 log J) for the inner
loop of the lattice method.
It is interesting to observe that the complexity is
driven by the length of the source sentence in the
case of the lattice method, and by the size of the
translation pool in the case of both the N-best list
method and the random sampling method. The lat-
ter two methods are asymptotically more effective as
long as the size of the sample/N-best list grows sub-
quadratically in the length of the sentence. In most
of our experiments we keep the size of the sample
constant, independent of the length of the sentence,
but other choices can be considered. Since the num-
ber of reachable translations grows with the length
of the source sentence, length-independent samples
explore a smaller fraction of the reachable space.
Generating samples (or n-best lists) of size increas-
ing with the length of the source sentence could thus
lead to more homogeneous sampling, and possibly a
better use of CPU time.
We have so far compared methods in term of the
complexity of the innermost loop: the search for a
global optimum along a line in the parameter space.
3Traditionally referred to as stacks.
610
This is indeed the most important analysis, since
the line search is repeated many times. In order to
complete the analysis, we also compare the differ-
ent methods in terms of the operations that need be
performed as part of the outer iteration, that is upon
redecoding the development set with a new parame-
ter vector.
The N-best list method requires simply construct-
ing an N-best list from the lattice. This can be done
in time linear in the size J of the sentence and in N
with a backward sweep in the lattice.
The sampling method requires sampling N times
the lattice according to the probability distribution
induced by the weights on its edges. We use a
dynamic programming approach for computing the
posterior probabilities of traversing edges. In this
phase we visit each edge of the lattice exactly once,
hence this phase is linear in the number of edges
in the lattice, hence under the standard assumptions
above in the length J of the sentence. Once posterior
probabilities are computed for the lattice, we need
to sample N paths from it, each of which is com-
posed of at most J edges4. Under standard assump-
tions, randomly selecting the next edge to follow at
each lattice node can be done in constant time, so
the whole sampling is also O(NJ), like extracting
the N-best list.
No operation at all is required by the lattice
method in the outer loop, since the whole lattice is
passed over for envelope propagation to the inner
loop.
4 Experimental Results
Experiments were conducted on the Europarl corpus
with the split used for the WMT-08 shared task (Eu-
roparl training and test condition) for the language
pairs English-French (En-Fr), English-Spanish (En-
Es) and English-German (En-De), each in both di-
rections. Training corpora contain between 1.2 and
1.3 million sentence pairs each, development and
test datasets are of size 2,000. Detailed token and
type statistics can be found in Callison-Burch et al
(2008). The Moses decoder (Koehn et al, 2007)
was used for generating lattices and n-best lists. The
maximum number of decoding iterations was set to
twelve. Since Moses was run with its lexicalised dis-
4We assume all phrase pairs cover at least one source word.
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0 1 2 3 4 5 6 7 8 9 10 11 12
nb.200.en-fr
s.200.en-fr
m.200.en-fr
s.100.en-fr
nb.100.en-fr
0.3
0.32
0.2
0.22
0.24
0.26
0.28
0 1 2 3 4 5 6 7 8 9 10 11
nb.200.fr-en
s.200.fr-en
m.200.fr-en
s.100.fr-en
nb.100.fr-en
Figure 3: Learning curves (BLEU on the development
set) for different tested conditions for English to French
(top) and French to English (bottom).
tortion model, there were 14 features. Moses L1-
normalises the parameter vector: parameter scaling
only marginally affects n-best list construction (via
threshold pruning during decoding), while it sub-
stantially impacts sampling.
For each of the six configurations, we compared
the BLEU score on the test data when optimizing
feature weights with MERT using n-best and ran-
dom samples of size 100 and 200. In all cases we
used 20 random restarts for MERT. Results are pre-
sented in Table 1. We also ran non systematic ex-
periments on some of the configurations with larger
samples and n-best lists, with results changing very
little from the respective 200 cases: we do not report
them here.
Learning curves (BLEU on the development set)
are shown in Figure 3. Learning curves for the other
tested language pairs follow a similar pattern.
611
5 Analysis of results
All differences of the test scores between optimiz-
ing the parameters using nbest-200 lists and from
randomly sampled lists of size 200 were found to
be statisitically significant at 0.05 level at least. We
used Approximate Randomization Test (Riezler and
Maxwell, 2005) for the purpose, random sampling
being done 1000 times.
S-T NB-100 RS-100 NB-200 RS-200
En-Fr 32.47 31.36 32.32 32.76
Fr-En 32.43 31.77 32.46 32.91
En-Es 29.21 28.98 29.65 30.19
Es-En 30.97 30.41 31.22 31.66
En-De 20.36 19.92 20.55 20.93
De-En 27.48 26.98 27.30 27.62
Table 1: Test set BLEU Scores for six different ?Source-
Target? Pairs
Somewhat surprisingly, while random sampling
with sample size of 200 yields overall the best re-
sults, random sampling with size 100 give system-
atically worse results than n-best lists of the same
size. We conjectured that n-best lists and random
samples could have complementary advantages. In-
deed, it seems intuitive that a good translation pool
should be sufficiently varied, as argued in Section
3.1. However it should also stand high chances to
contain the best reachable translation, or translations
close to the best. It might thus be that 100-best lists
are unable to provide diversity, and random samples
of size 100 to guarantee sufficient quality.
In order to test this conjecture we repeated our
experiments, but at each iteration we used the union
of a 100 random sample and a 100 n-best list. Re-
sults for this experiments are in Table 2. The cor-
responding results with random samples of size 200
are also repeated to ease comparison. Depending on
the language pair, improvements over random sam-
pling range from 0.17 (En-Es) to 0.44 (Fr-En) BLEU
points. Improvements over 200-best lists range from
0.68 (De-En) to 0.89 (Fr-En) BLEU points. These
results indicate quite clearly that N-best lists and
random samples contribute complementary infor-
mation to the translation pool: indeed, in most cases
there is very little or no overlap between the two.
Convergence curves show that RS-200, NB-100
Source-Target Mixed 100 + 100 RS-200
En-Fr 33.17 32.76
Fr-En 33.35 32.91
En-Es 30.37 30.19
Es-En 32.04 31.66
En-De 21.31 20.93
De-En 27.98 27.62
Table 2: Test set BLEU Scores for the same ??Source-
Target? pairs using a mixed strategy combining a 100 N-
best list and a random sample of size 100 after each round
of decoding.
and M-200 (i.e. the hybrid combination) systemati-
cally converge to higher BLEU scores, on the devel-
opment set and on their respective translation pools,
than RS-100 and NB-200. Notice however that it is
misleading to compare scores across different trans-
lation pools, especially if these have substantially
different sizes. On the one hand adding more candi-
dates increases the chances of adding one with high
contribution to the corpus BLEU, and can thus in-
crease the achievable value of the objective function.
On the other hand, adding more candidates reduces
the freedom MERT has to find parameter values se-
lecting high-BLEU candidates for all sentences. To
see this, consider the extreme case when the transla-
tion pools are all of size one and are provided by an
oracle that gives the highest-BLEU reachable trans-
lation for each sentence: the objective surface is un-
informatively flat, all values of the parameters are
equally good, and the BLEU score on the devset is
the highest achievable one. If now we add to each
translation pool the second-best BLEU-scoring can-
didate, BLEU will be maximized in a half-space for
each sentence in the development set: MERT will try
to select ? in the intersection of all the half-spaces, if
this is not empty, but will have to settle for a lower-
scoring compromise otherwise. The larger the trans-
lation pools, the more difficult it becomes for MERT
to ?make all sentences happy?. A special case of this
is when adding more candidates extends the convex
envelopes in such a way that the best candidates fall
in the interior of the convex hull. It is difficult to
tell which of the two opposing effects (the one that
tends to increase the value of the objective function
or the one that tends to depress it) is stronger in any
612
given case, but from the convergence curves it would
seem that the first prevails in the case of random
samples, whereas the second wins in the case of n-
best lists. In the case of random samples going from
size 100 to 200 systematically leads to higher BLEU
score on the devsets, as more high-BLEU candidates
are drawn. In the case of n-best lists, conversely,
this leads to lower BLEU scores, as lower-BLEU (in
average) candidates are added to translation pools
providing a sharper representation of the BLEU sur-
face and growing MERT out of the ?delusion? that a
given high BLEU score is actually achieveable.
In the light of this discussion, it is interesting
to observe that the value achieved by the objective
function on the development set is only a weak pre-
dictor of performance on the test set, e.g. M-200
never converges to values above those of NB-100,
but is systematically superior on the test data.
In Macherey et al (2008) the authors observe a
dip in the value of the objective function at the first
iteration when training using n-best lists. We did
not observe this behaviour in our experiments. A
possible explanation for this resides in the larger size
of the n-best lists we use (100 or 200, compared to
50 in the cited work) and in the smaller number of
dimensions (14 instead of 20-30).
We hinted in Section 3.3 that it would seem rea-
sonable to use samples/nbest-list of size increasing
with the length of the source sentence, so as to sam-
ple reachable translations with a more uniform den-
sity across development sentences. We tested this
idea on the French to English condition, making
samples size depend linearly on the length of the
sentence, and in such a way that the average sam-
ple size is either 100 or 200. For average sample
size 100 we obtained a BLEU of 31.55 (compared
to 31.77 with the constant-size 100 random sample)
and for average size 200 31.84 (32.46 in the cor-
responding constant-size condition). While partial,
these results are not particularly encouraging w.r.t.
using variable size samples.
Finally, in order to assess the stability of the pro-
posed training procedure across variations in devel-
opment datasets, we experimented with extracting
five distinct devsets of size 2,000 each for the French
to English RS-200 condition, keeping the test set
fixed: the maximum difference we observed was of
0.33 BLEU points.
6 Conclusions
We introduced a novel variant to the well-known
MERT method for performing parameter estimation
in Statistical Machine Translation systems based on
log-linear models. This method, of straightforward
implementation, is based on sampling candidates
from the posterior distribution as approximated by
an existing translation lattice in order to progres-
sively expand the translation pool that shapes the
optimization surface. This method compares favor-
ably against existing methods on different accounts.
Compared to the standard method by which N-best
lists are used to grow the translation pool, it yields
empirically better results as shown in our experi-
ments, without significant penalties in terms of com-
putational complexity. These results are in agree-
ment with the intuition that the sampling method
introduces more variety in the translation pool, and
thus allows to perform more effective parameter up-
dates towards the optimum. A hybrid strategy, con-
sisting in combining N-best lists and random sam-
ples, brings about further significant improvements,
indicating that both quality and variety are desire-
able in the translation pool that defines the optimiza-
tion surface. A possible direction to investigate in
the future consists in generalizing this hybrid strat-
egy and combining random samples where the prob-
ability distribution induced on the lattice by the cur-
rent parameters is scaled by a further temperature
parameter ?:
P ?(e, a|f) ? P (e, a|f)? (7)
where for ? = 1 the random samples used in this pa-
per are obtained, for ? tending to infinite the distri-
bution becomes peaked around the single best path,
thus producing samples similar to N-best lists, and
samples from other real values of the temperature
can be combined.
Compared to the method using the whole lat-
tice, the proposed approaches have a substantially
lower computational complexity under very broad
and common assumptions, and yet yield transla-
tion quality improvements of comparable magnitude
over the baseline N-best list method.
While the method presented in this paper oper-
ates on the translation lattices generated by Phrase-
Based SMT decoders, the extension to translation
613
forests generated by hierarchical decoders (Chiang,
2007) seems straightforward. In that case, the back-
ward sweep for propagating unnormalized posterior
probabilities is replaced by a bottom-up sweep, and
the sampling now concerns (binary) trees instead of
paths, but the rest of the procedure is substantially
unchanged. We conjecture however that the exten-
sion to translation forests would be less competitive
compared to working with the whole packed forest
(as in (Kumar et al, 2009)) than lattice sampling is
compared to working with the whole lattice. The
reason we believe this is that hierarchical models
lead to much more spurious ambiguity than phrase-
based models, so that both the N-best method and
the sampling method explore a smaller portion of the
candidate space compared to the compact represen-
tation of all the candidate translations in a beam.
Acknowledgements
We would like to thank Vassilina Nikoulina, Greg
Hanneman, Marc Dymetman for useful discussions
and the anonymous reviewers for their suggestions
and constructive criticism.
References
Chris Callison-Burch, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. Further meta-evaluation of ma-
chine translation. In Proceedings of the
ACL 2008 Workshop on Statistical Ma-
chine Translation, Columbus, Ohio, 2008.
http://www.statmt.org/wmt08/pdf/WMT09.pdf.
Daniel Cer, Daniel Jurafsky, and Christopher D.
Manning. Regularization and search for mini-
mum error rate training. In Proceedings of the
Third Workshop on Statistical Machine Transla-
tion, pages 26?34, Columbus, Ohio, 2008. ISBN
978-1-932432-09-1.
David Chiang. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228,
2007.
David Chiang, Yuval Marton, and Philip Resnik.
Online large-margin training of syntactic and
structural translation features. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP ?08), pages 224?
233, Honolulu, Hawaii, 2008.
Koby Crammer and Yoram Singer. Ultracon-
servative online algorithms for multiclass prob-
lems. Journal of Machine Learning Research
(JMLR), 3:951?991, 2003. ISSN 1532-4435. doi:
http://dx.doi.org/10.1162/jmlr.2003.3.4-5.951.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Ma-
chine Learning Research (JMLR), 7:551?585,
2006. ISSN 1532-4435.
Kevin Knight. Decoding complexity in word-
replacement translation modals. Computational
Linguistics, Squibs and Discussion, 25(4),, 1999.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the Annual Meeting
of the Association for Computationl Linguistics
(ACL ?07), pages 177?180, prague, Czech repub-
lic, 2007.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. Efficient minimum error rate
training and minimum bayes-risk decoding for
translation hypergraphs and lattices. In Proceed-
ings of the Joint 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171, Suntec, Singapore, August 2009.
Wolfgang Macherey, Franz Josef Och, Ignacio
Thayer, and Jakob Uszkoreit. Lattice-based min-
imum error rate training for statistical machine
translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP ?08), pages 725?734, Honolulu,
Hawaii, 2008.
Franz Josef Och. Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics (ACL ?03),
pages 160?167, Sapporo, Japan, 2003. doi:
http://dx.doi.org/10.3115/1075096.1075117.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
614
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting on Association
for Computational Linguistics (ACL ?02), pages
311?318, Philadelphia, Pennsylvania, 2002. doi:
http://dx.doi.org/10.3115/1073083.1073135.
William H. Press. Numerical recipes : the art
of scientific computing. Cambridge University
Press, third edition, September 2007. ISBN
0521880688.
Stefan Riezler and John T. Maxwell. On some pit-
falls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
pages 57?64, Ann Arbor, Michigan, June 2005.
David A. Smith and Jason Eisner. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the Joint International Conference
on Computational Linguistics and Annual meet-
ing of the Association for Computational Linguis-
tics (COLING/ACL ?06), pages 787?794, Sydney,
Australia, 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. Online large-margin training for
statistical machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764?773, Prague, Czech Repub-
lic, June 2007.
Richard Zens, Sasa Hasan, and Hermann Ney. A
systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 524?532, Prague, Czech Repub-
lic, June 2007.
615
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 439?444,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Confidence-Weighted Learning of Factored Discriminative Language
Models
Viet Ha-Thuc
Computer Science Department
The University of Iowa
Iowa City, IA 52241, USA
hviet@cs.uiowa.edu
Nicola Cancedda
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, France
Nicola.Cancedda@xrce.xerox.com
Abstract
Language models based on word surface
forms only are unable to benefit from avail-
able linguistic knowledge, and tend to suffer
from poor estimates for rare features. We pro-
pose an approach to overcome these two lim-
itations. We use factored features that can
flexibly capture linguistic regularities, and we
adopt confidence-weighted learning, a form of
discriminative online learning that can better
take advantage of a heavy tail of rare features.
Finally, we extend the confidence-weighted
learning to deal with label noise in training
data, a common case with discriminative lan-
guage modeling.
1 Introduction
Language Models (LMs) are key components in
most statistical machine translation systems, where
they play a crucial role in promoting output fluency.
Standard n-gram generative language models
have been extended in several ways. Generative
factored language models (Bilmes and Kirchhoff,
2003) represent each token by multiple factors ?
such as part-of-speech, lemma and surface form?
and capture linguistic patterns in the target language
at the appropriate level of abstraction. Instead of
estimating likelihood, discriminative language mod-
els (Roark et al, 2004; Roark et al, 2007; Li and
Khudanpur, 2008) directly model fluency by casting
the task as a binary classification or a ranking prob-
lem. The method we propose combines advantages
of both directions mentioned above. We use factored
features to capture linguistic patterns and discrim-
inative learning for directly modeling fluency. We
define highly overlapping and correlated factored
features, and extend a robust learning algorithm to
handle them and cope with a high rate of label noise.
For discriminatively learning language models,
we use confidence-weighted learning (Dredze et al,
2008), an extension of the perceptron-based on-
line learning used in previous work on discrimi-
native language models. Furthermore, we extend
confidence-weighted learning with soft margin to
handle the case where training data labels are noisy,
as is typically the case in discriminative language
modeling.
The rest of this paper is organized as follows. In
Section 2, we introduce factored features for dis-
criminative language models. Section 3 presents
confidence-weighted learning. Section 4 describes
its extension for the case where training data are
noisy. We present empirical results in Section 5
and differentiate our approach from previous ones
in Section 6. Finally, Section 7 presents some con-
cluding remarks.
2 Factored features
Factored features are n-gram features where each
component in the n-gram can be characterized by
different linguistic dimensions of words such as sur-
face, lemma, part of speech (POS). Each of these
dimensions is conventionally referred to as a factor.
An example of a factored feature is ?pick PRON
up?, where PRON is the part of speech (POS) tag
for pronouns. Appropriately weighted, this feature
can capture the fact that in English that pattern is of-
ten fluent. Compared to traditional surface n-gram
features like ?pick her up?, ?pick me up? etc., the
feature ?pick PRON up? generalizes the pattern bet-
ter. On the other hand, this feature is more precise
439
POS Extended POS
Noun SingNoun, PlurNoun
Pronoun Sing3PPronoun, OtherPronoun
Verb InfVerb, ProgrVerb, SimplePastVerb,
PastPartVerb, Sing3PVerb, OtherVerb
Table 1: Extended tagset used for the third factor in the
proposed discriminative language model.
than the corresponding POS n-gram feature ?VERB
PRON PREP? since the latter also promotes unde-
sirable patterns such as ?pick PRON off? and ?go
PRON in?. So, constructing features with compo-
nents from different abstraction levels allows better
capturing linguistic patterns.
In this study, we use tri-gram factored features to
learn a discriminative language model for English,
where each token is characterized by three factors
including surface, POS, and extended POS. In the
last factor, some POS tags are further refined (Table
1). In other words, we will use all possible trigrams
where each element is either a surface from, a POS,
or an extended POS.
3 Confidence-weighted Learning
Online learning algorithms scale well to large
datasets, and are thus well adapted to discrimina-
tive language modeling. On the other hand, the
perceptron and Passive Aggressive (PA) algorithms1
(Crammer et al, 2006) can be ill-suited for learn-
ing tasks where there is a long tail of rare significant
features as in the case of language modeling.
Motivated by this, we adopt a simplified version
of the CW algorithm of (Dredze et al, 2008). We in-
troduce a score , based on the number of times a fea-
ture has been obseerved in training, indicating how
confident the algorithm is in the current estimate wi
for the weight of feature i. Instead of equally chang-
ing all feature weights upon a mistake, the algorithm
now changes more aggressively the weights it is less
confident in.
At iteration t, if the algorithm miss-ranks the pair
of positive and negative instances (pt, nt), it updates
the weight vector by solving the optimization in Eq.
(1):
1The popular MIRA algorithm is a particular PA algorithm,
suitable for the linearly-separable case.
wt+1 = argminw
1
2(w ?wt)
>?2t (w ?wt)(1)
s.t. w>?t ? 1 (2)
where ?t = ?(pt) ? ?(nt), ?(x) is the vector rep-
resentation of sentence x in factored feature space,
and ?t is a diagonal matrix with confidence scores.
The algorithm thus updates weights aggressively
enough to correctly rank the current pair of instances
(i.e. satisfying the constraint), and preserves as
much knowledge learned so far as possible (i.e. min-
imizing the weighted difference to wt). In the spe-
cial case when ?t = I this is the update of the
Passive-Aggressive algorithm of (Crammer et al,
2006).
By introducing multiple confidence scores with
the diagonal matrix ?, we take into account the
fact that feature weights that the algorithm has more
confidence in (because it has learned these weights
from more training instances) contribute more to
the knowledge the algorithm has accumulated so far
than feature weights it has less confidence in. A
change in the former is more risky than a change
with the same magnitude on the latter. So, to avoid
over-fitting to the current instance pair (thus gener-
alize better to the others), the difference between w
and wt is weighted by confidence matrix ? in the
objective function.
To solve the quadratic optimization problem in
Eq. (1), we form the corresponding Lagrangian:
L(w, ?) = 12(w?wt)
>?2t (w?wt)+?(1?w>?)
(3)
where ? is the Lagrange multiplier corresponding to
the constraint in Eq. (2). Setting the partial deriva-
tives of L with respect to w to zero, and then setting
the derivative of L with respect to ? to zero, we get:
? = 1?wt
>?
???1??2 (4)
Given this, we obtain Algorithm 1 for confidence-
weighted passive-aggressive learning (Figure 1). In
the algorithm, Pi and Ni are sets of fluent and non-
fluent sentences that can be contrasted, e.g. Pi is a
set of fluent translations and Ni is a set of non-fluent
translations of a same source sentence si.
440
Algorithm 1 Confidence-weighted Passive-
Aggressive algorithm for re-ranking.
Input: Tr = {(Pi, Ni), 1 ? i ? K}
w0 ? 0, t? 0
for a predefined number of iterations do
for i from 1 to K do
for all (pj , nj) ? (Pi ?Ni) do
?t ? ?(pj)? ?(nj)
if w>t ?t < 1 then
? ? 1?w
>
t ?t
?>t ?
?2
t ?t
wt+1 ? wt + ???2t ?t
Update ?
t? t + 1
return wt
The confidence matrix ? is updated following the
intuition that the more often the algorithm has seen
a feature, the more confident the weight estimation
becomes. In our work, we set ?ii to the logarithm of
the number of times the algorithm has seen feature
i, but alternative choices are possible.
4 Extension to soft margin
In many practical situations, training data is noisy.
This is particularly true for language modeling,
where even human experts will argue about whether
a given sentence is fluent or not. Moreover, effective
language models must be trained on large datasets,
so the option of requiring extensive human annota-
tion is impractical. Instead, collecting fluency judg-
ments is often done by a less expensive and thus
even less reliable manner. One way is to rank trans-
lations in n-best lists by NIST or BLEU scores, then
take the top ones as fluent instances and bottom ones
as non-fluent instances. Nonetheless, neither NIST
nor BLEU are designed directly for measuring flu-
ency. For example, a translation could have low
NIST and BLEU scores just because it does not con-
vey the same information as the reference, despite
being perfectly fluent. Therefore, in our setting it is
crucial to be robust to noise in the training labels.
The update rule derived in the previous section al-
ways forces the new weights to satisfy the constraint
(Corrective updates): mislabeled training instances
could make feature weights change erratically. To
increase robustness to noise, we propose a soft mar-
gin variant of confidence-weighted learning. The
optimization problem becomes:
argmin
w
1
2(w ?wt)
>?2t (w ?wt) + C?2 (5)
s.t. w>?t ? 1? ? (6)
where C is a regularization parameter, controlling
the relative importance between the two terms in the
objective function. Solving the optimization prob-
lem, we obtain, for the Lagrange multiplier:
? = 1?wt
>?t
?>t ??2t ?t + 12C
(7)
Thus, the training algorithm with soft-margins is the
same as Algorithm 1, but using Eq. 7 to update ?
instead.
5 Experiments
We empirically validated our approach in two ways.
We first measured the effectiveness of the algorithms
in deciding, given a pair of candidate translations
for a same source sentence, whether the first candi-
date is more fluent than the second. In a second ex-
periment we used the score provided by the trained
DLM as an additional feature in an n-best list re-
ranking task and compared algorithms in terms of
impact on NIST and BLEU.
5.1 Dataset
The dataset we use in our study is the Spanish-
English one from the shared task of the WMT-2007
workshop2.
Matrax, a phrase-based statistical machine trans-
lation system (Simard et al, 2005), including a tri-
gram generative language model with Kneser-Ney
smoothing. We then obtain training data for the dis-
criminative language model as follows. We take a
random subset of the parallel training set containing
50,000 sentence pairs. We use Matrax to generate
an n-best list for each source sentence. We define
(Pi, Ni), i = 1 . . . 50, 000 as:
Pi = {s ? nbesti|NIST(s) ? NIST?i ? 1} (8)
Ni = {s ? nbesti|NIST(s) ? NIST?i ? 3} (9)
2http://www.statmt.org/wmt07/
441
Error rate
Baseline model 0.4720
Baseline + DLM0 0.4290
Baseline + DLM1 0.4183
Baseline + DLM2 0.4005
Baseline + DLM3 0.3803
Table 2: Error rates for fluency ranking. See article body
for an explanation of the experiments.
where NIST?i is the highest sentence-level NIST
score achieved in nbesti. The size of n-best lists
was set to 10. Using this dataset, we trained dis-
criminative language models by standard percep-
tron, confidence-weighted learning and confidence-
weighted learning with soft margin.
We then trained the weights of a re-ranker using
eight features (seven from the baseline Matrax plus
one from the DLM) using a simple structured per-
ceptron algorithm on the development set.
For testing, we used the same trained Matrax
model to generate n-best lists of size 1,000 each for
each source sentence. Then, we used the trained dis-
criminative language model to compute a score for
each translation in the n-best list. The score is used
with seven standard Matrax features for re-ranking.
Finally, we measure the quality of the translations
re-ranked to the top.
In order to obtain the required factors for the
target-side tokens, we ran the morphological ana-
lyzer and POS-tagger integrated in the Xerox Incre-
mental Parser (XIP, Ait-Mokhtar et al (2001)) on
the target side of the training corpus used for creat-
ing the phrase-table, and extended the phrase-table
format so as to record, for each token, all its factors.
5.2 Results
In the first experiment, we measure the quality of
the re-ranked n-best lists by classification error rate.
The error rate is computed as the fraction of pairs
from a test-set which is ranked correctly according
to its fluency score (approximated here by the NIST
score). Results are in Table 2.
For the baseline, we use the seven default Ma-
trax features, including a generative language model
score. DLM* are discriminative language mod-
els trained using, respectively, POS features only
NIST BLEU
Baseline model 6.9683 0.2704
Baseline + DLM0 6.9804 0.2705
Baseline + DLM1 6.9857 0.2709
Baseline + DLM2 7.0288 0.2745
Baseline + DLM3 7.0815 0.2770
Table 3: NIST and BLEU scores upon n-best list re-
ranking with the proposed discriminative language mod-
els.
(DLM 0) or factored features by standard percep-
tron (DLM 1), confidence-weighted learning (DLM
2) and confidence-weighted learning with soft mar-
gin (DLM 3). All discriminative language models
strongly reduce the error rate compared to the base-
line (9.1%, 11.4%, 15.1%, 19.4% relative reduc-
tion, respectively). Recall that the training set for
these discriminative language models is a relatively
small subset of the one used to train Matrax?s inte-
grated generative language model. Amongst the four
discriminative learning algorithms, we see that fac-
tored features are slightly better then POS features,
confidence-weighted learning is slightly better than
perceptron, and confidence-weighted learning with
soft margin is the best (9.08% and 5.04% better than
perceptron and confidence-weighted learning with
hard margin).
In the second experiment, we use standard NIST
and BLEU scores for evaluation. Results are in Ta-
ble 3. The relative quality of different methods in
terms of NIST and BLEU correlates well with er-
ror rate. Again, all three discriminative language
models could improve performances over the base-
line. Amongst the three, confidence-weighted learn-
ing with soft margin performs best.
6 Related Work
This work is related to several existing directions:
generative factored language model, discriminative
language models, online passive-aggressive learning
and confidence-weighted learning.
Generative factored language models are pro-
posed by (Bilmes and Kirchhoff, 2003). In this
work, factors are used to define alternative back-
off paths in case surface-form n-grams are not ob-
served a sufficient number of times in the train-
442
ing corpus. Unlike ours, this model cannot con-
sider simultaneously multiple factored features com-
ing from the same token n-gram, thus integrating all
possible available information sources.
Discriminative language models have also been
studied in speech recognition and statistical machine
translation (Roark et al, 2007; Li and Khudanpur,
2008). An attempt to combine factored features and
discriminative language modeling is presented in
(Mahe? and Cancedda, 2009). Unlike us, they com-
bine together instances from multiple n-best lists,
generally not comparable, in forming positive and
negative instances. Also, they use an SVM to train
the DLM, as opposed to the proposed online algo-
rithms.
Our approach stems from Passive-Aggressive al-
gorithms proposed by (Crammer et al, 2006) and
the CW online algorithm proposed by (Dredze et
al., 2008). In the former, Crammer et al propose
an online learning algorithm with soft margins to
handle noise in training data. However, the work
does not consider the confidence associated with es-
timated feature weights. On the other hand, the CW
online algorithm in the later does not consider the
case where the training data is noisy.
While developed independently, our soft-margin
extension is closely related to the AROW(project)
algorithm of (Crammer et al, 2009; Crammer and
Lee, 2010). The cited work models classifiers as
non-correlated Gaussian distributions over weights,
while our approach uses point estimates for weights
coupled with confidence scores. Despite the differ-
ent conceptual modeling, though, in practice the al-
gorithms are similar, with point estimates playing
the same role as the mean vector, and our (squared)
confidence score matrix the same role as the preci-
sion (inverse covariance) matrix. Unlike in the cited
work, however, in our proposal, confidence scores
are updated also upon correct classification of train-
ing examples, and not only on mistakes. The ra-
tionale of this is that correctly classifying an exam-
ple could also increase the confidence on the current
model. Thus, the update formulas are also different
compared to the work cited above.
7 Conclusions
We proposed a novel approach to discriminative lan-
guage models. First, we introduced the idea of us-
ing factored features in the discriminative language
modeling framework. Factored features allow the
language model to capture linguistic patterns at mul-
tiple levels of abstraction. Moreover, the discrimi-
native framework is appropriate for handling highly
overlapping features, which is the case of factored
features. While we did not experiment with this, a
natural extension consists in using all n-grams up
to a certain order, thus providing back-off features
and enabling the use of higher-order n-grams. Sec-
ond, for learning factored language models discrim-
inatively, we adopt a simple confidence-weighted
algorithm, limiting the problem of poor estimation
of weights for rare features. Finally, we extended
confidence-weighted learning with soft margins to
handle the case where labels of training data are
noisy. This is typically the case in discriminative
language modeling, where labels are obtained only
indirectly.
Our experiments show that combining all these el-
ements is important and achieves significant transla-
tion quality improvements already with a weak form
of integration: n-best list re-ranking.
References
Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, Cina.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Fac-
tored language models and generalized parallel back-
off. In Proceedings of HLT/NAACL, Edmonton, Al-
berta, Canada.
Koby Crammer and Daniel D. Lee. 2010. Learning via
gaussian herding. In Pre-proceeding of NIPS 2010.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal Of Machine Learning
Research, 7.
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009.
Adaptive regularization of weight vectors. In Ad-
vances in Neural Processing Information Systems
(NIPS 2009).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classifiers. In Pro-
ceedings of ICML, Helsinki, Finland.
443
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statistical
machine translation. In Proceedings of AMTA.
Pierre Mahe? and Nicola Cancedda. 2009. Linguisti-
cally enriched word-sequence kernels for discrimina-
tive language modeling. In Learning Machine Trans-
lation, NIPSWorkshop Series. MIT Press, Cambridge,
Mass.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proceedings of the annual meeting of
the Association for Computational Linguistics (ACL),
Barcelona, Spain.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2).
M. Simard, N. Cancedda, B. Cavestro, M. Dymetman,
E. Gaussier, C. Goutte, and K. Yamada. 2005. Trans-
lating with non-contiguous phrases. In Association
for Computational Linguistics, editor, Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language,
pages 755?762, October.
444
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 22?30,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Prediction of Learning Curves in Machine Translation
Prasanth Kolachina? Nicola Cancedda? Marc Dymetman? Sriram Venkatapathy?
? LTRC, IIIT-Hyderabad, Hyderabad, India
? Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France
Abstract
Parallel data in the domain of interest is the
key resource when training a statistical ma-
chine translation (SMT) system for a specific
purpose. Since ad-hoc manual translation can
represent a significant investment in time and
money, a prior assesment of the amount of
training data required to achieve a satisfac-
tory accuracy level can be very useful. In this
work, we show how to predict what the learn-
ing curve would look like if we were to manu-
ally translate increasing amounts of data.
We consider two scenarios, 1) Monolingual
samples in the source and target languages are
available and 2) An additional small amount
of parallel corpus is also available. We pro-
pose methods for predicting learning curves in
both these scenarios.
1 Introduction
Parallel data in the domain of interest is the key re-
source when training a statistical machine transla-
tion (SMT) system for a specific business purpose.
In many cases it is possible to allocate some budget
for manually translating a limited sample of relevant
documents, be it via professional translation services
or through increasingly fashionable crowdsourcing.
However, it is often difficult to predict how much
training data will be required to achieve satisfactory
translation accuracy, preventing sound provisional
budgetting. This prediction, or more generally the
prediction of the learning curve of an SMT system
as a function of available in-domain parallel data, is
the objective of this paper.
We consider two scenarios, representative of real-
istic situations.
1. In the first scenario (S1), the SMT developer is
given only monolingual source and target sam-
ples from the relevant domain, and a small test
parallel corpus.
?This research was carried out during an internship at Xerox
Research Centre Europe.
2. In the second scenario (S2), an additional small
seed parallel corpus is given that can be used
to train small in-domain models and measure
(with some variance) the evaluation score at a
few points on the initial portion of the learning
curve.
In both cases, the task consists in predicting an eval-
uation score (BLEU, throughout this work) on the
test corpus as a function of the size of a subset of
the source sample, assuming that we could have it
manually translated and use the resulting bilingual
corpus for training.
In this paper we provide the following contribu-
tions:
1. An extensive study across six parametric func-
tion families, empirically establishing that a
certain three-parameter power-law family is
well suited for modeling learning curves for the
Moses SMT system when the evaluation score
is BLEU. Our methodology can be easily gen-
eralized to other systems and evaluation scores
(Section 3);
2. A method for inferring learning curves based
on features computed from the resources avail-
able in scenario S1, suitable for both the sce-
narios described above (S1) and (S2) (Section
4);
3. A method for extrapolating the learning curve
from a few measurements, suitable for scenario
S2 (Section 5);
4. A method for combining the two approaches
above, achieving on S2 better prediction accu-
racy than either of the two in isolation (Section
6).
In this study we limit tuning to the mixing param-
eters of the Moses log-linear model through MERT,
keeping all meta-parameters (e.g. maximum phrase
length, maximum allowed distortion, etc.) at their
default values. One can expect further tweaking to
lead to performance improvements, but this was a
22
necessary simplification in order to execute the tests
on a sufficiently large scale.
Our experiments involve 30 distinct language pair
and domain combinations and 96 different learning
curves. They show that without any parallel data
we can predict the expected translation accuracy at
75K segments within an error of 6 BLEU points (Ta-
ble 4), while using a seed training corpus of 10K
segments narrows this error to within 1.5 points (Ta-
ble 6).
2 Related Work
Learning curves are routinely used to illustrate how
the performance of experimental methods depend
on the amount of training data used. In the SMT
area, Koehn et al (2003) used learning curves to
compare performance for various meta-parameter
settings such as maximum phrase length, while
Turchi et al (2008) extensively studied the be-
haviour of learning curves under a number of test
conditions on Spanish-English. In Birch et al
(2008), the authors examined corpus features that
contribute most to the machine translation perfor-
mance. Their results showed that the most predic-
tive features were the morphological complexity of
the languages, their linguistic relatedness and their
word-order divergence; in our work, we make use of
these features, among others, for predicting transla-
tion accuracy (Section 4).
In a Machine Learning context, Perlich et al
(2003) used learning curves for predicting maximum
performance bounds of learning algorithms and to
compare them. In Gu et al (2001), the learning
curves of two classification algorithms were mod-
elled for eight different large data sets. This work
uses similar a priori knowledge for restricting the
form of learning curves as ours (see Section 3), and
also similar empirical evaluation criteria for compar-
ing curve families with one another. While both ap-
plication and performance metric in our work are
different, we arrive at a similar conclusion that a
power law family of the form y = c ? a x?? is a
good model of the learning curves.
Learning curves are also frequently used for de-
termining empirically the number of iterations for
an incremental learning procedure.
The crucial difference in our work is that in the
previous cases, learning curves are plotted a poste-
riori i.e. once the labelled data has become avail-
able and the training has been performed, whereas
in our work the learning curve itself is the object of
the prediction. Our goal is to learn to predict what
the learning curve will be a priori without having to
label the data at all (S1), or through labelling only a
very small amount of it (S2).
In this respect, the academic field of Computa-
tional Learning Theory has a similar goal, since it
strives to identify bounds to performance measures1,
typically including a dependency on the training
sample size. We take a purely empirical approach
in this work, and obtain useful estimations for a case
like SMT, where the complexity of the mapping be-
tween the input and the output prevents tight theo-
retical analysis.
3 Selecting a parametric family of curves
The first step in our approach consists in selecting
a suitable family of shapes for the learning curves
that we want to produce in the two scenarios being
considered.
We formulate the problem as follows. For a cer-
tain bilingual test dataset d, we consider a set of
observations Od = {(x1, y1), (x2, y2)...(xn, yn)},
where yi is the performance on d (measured using
BLEU (Papineni et al, 2002)) of a translation model
trained on a parallel corpus of size xi. The corpus
size xi is measured in terms of the number of seg-
ments (sentences) present in the parallel corpus.
We consider such observations to be generated by
a regression model of the form:
yi = F (xi; ?) + i 1 ? i ? n (1)
where F is a function depending on a vector param-
eter ? which depends on d, and i is Gaussian noise
of constant variance.
Based on our prior knowledge of the problem,
we limit the search for a suitable F to families that
satisfies the following conditions- monotonically in-
creasing, concave and bounded. The first condition
just says that more training data is better. The sec-
ond condition expresses a notion of ?diminishing
returns?, namely that a given amount of additional
training data is more advantageous when added to
a small rather than to a big amount of initial data.
The last condition is related to our use of BLEU ?
which is bounded by 1 ? as a performance mea-
sure; It should be noted that some growth patterns
which are sometimes proposed, such as a logarith-
mic regime of the form y ' a + b log x, are not
1More often to a loss, which is equivalent.
23
compatible with this constraint.
We consider six possible families of functions sat-
isfying these conditions, which are listed in Table 1.
Preliminary experiments indicated that curves from
Model Formula
Exp3 y = c? e?ax+b
Exp4 y = c? e?ax
?+b
ExpP3 y = c? e(x?b)
?
Pow3 y = c? ax??
Pow4 y = c? (?ax+ b)??
ILog2 y = c? (a/ log x)
Table 1: Curve families.
the ?Power? and ?Exp? family with only two param-
eters underfitted, while those with five or more pa-
rameters led to overfitting and solution instability.
We decided to only select families with three or four
parameters.
Curve fitting technique Given a set of observa-
tions {(x1, y1), (x2, y2)...(xn, yn)} and a curve fam-
ily F (x; ?) from Table 1, we compute a best fit ??
where:
?? = argmin
?
n?
i=1
[yi ? F (xi; ?)]
2, (2)
through use of the Levenberg-Marquardt
method (More?, 1978) for non-linear regression.
For selecting a learning curve family, and for all
other experiments in this paper, we trained a large
number of systems on multiple configurations of
training sets and sample sizes, and tested each on
multiple test sets; these are listed in Table 2. All
experiments use Moses (Koehn et al, 2007). 2
Domain
Source Target # Test
Language Language sets
Europarl (Koehn, 2005)
Fr, De, Es En
4
En Fr, De, Es
KFTT (Neubig, 2011) Jp, En En, Jp 2
EMEA (Tiedemann, 2009) Da, De En 4
News (Callison-Burch et al, 2011) Cz,En,Fr,De,Es Cz,En,Fr,De,Es 3
Table 2: The translation systems used for the curve fit-
ting experiments, comprising 30 language-pair and do-
main combinations for a total of 96 learning curves.
Language codes: Cz=Czech, Da=Danish, En=English,
De=German, Fr=French, Jp=Japanese, Es=Spanish
The goodness of fit for each of the families is eval-
2The settings used in training the systems are those
described in http://www.statmt.org/wmt11/
baseline.html
uated based on their ability to i) fit over the entire set
of observations, ii) extrapolate to points beyond the
observed portion of the curve and iii) generalize well
over different datasets .
We use a recursive fitting procedure where the
curve obtained from fitting the first i points is used
to predict the observations at two points: xi+1, i.e.
the point to the immediate right of the currently ob-
served xi and xn, i.e. the largest point that has been
observed.
The following error measures quantify the good-
ness of fit of the curve families:
1. Average root mean-squared error (RMSE):
1
N
?
c?S
?
t?Tc
{
1
n
n?
i=1
[yi ? F (xi; ??)]
2
}1/2
ct
where S is the set of training datasets, Tc is the
set of test datasets for training configuration c,
?? is as defined in Eq. 2, N is the total number
of combinations of training configurations and
test datasets, and i ranges on a grid of training
subset sizes.The expressions n, xi, yi, ?? are all
local to the combination ct.
2. Average root mean squared residual at next
point X = xi+1 (NPR):
1
N
?
c?S
?
t?Tc
{
1
n? k ? 1
n?1?
i=k
[yi+1 ? F (xi+1; ??
i)]2
}1/2
ct
where ??i is obtained using only observations
up to xi in Eq. 2 and where k is the number of
parameters of the family.3
3. Average root mean squared residual at the last
point X = xn (LPR):
1
N
?
c?S
?
t?Tc
{
1
n? k ? 1
n?1?
i=k
[yn ? F (xn; ??
i)]2
}1/2
ct
Curve fitting evaluation The evaluation of the
goodness of fit for the curve families is presented
in Table 3. The average values of the root mean-
squared error and the average residuals across all the
learning curves used in our experiments are shown
in this table. The values are on the same scale as the
BLEU scores. Figure 1 shows the curve fits obtained
3We start the summation from i = k, because at least k
points are required for computing ??i.
24
Figure 1: Curve fits using different curve families on a
test dataset
for all the six families on a test dataset for English-
German language pair.
Curve Family RMSE NPR LPR
Exp3 0.0063 0.0094 0.0694
Exp4 0.0030 0.0036 0.0072
ExpP3 0.0040 0.0049 0.0145
Pow3 0.0029 0.0037 0.0091
Pow4 0.0026 0.0042 0.0102
ILog2 0.0050 0.0067 0.0146
Table 3: Evaluation of the goodness of fit for the six fam-
ilies.
Loooking at the values in Table 3, we decided to
use the Pow3 family as the best overall compromise.
While it is not systematically better than Exp4 and
Pow4, it is good overall and has the advantage of
requiring only 3 parameters.
4 Inferring a learning curve from mostly
monolingual data
In this section we address scenario S1: we have
access to a source-language monolingual collec-
tion (from which portions to be manually translated
could be sampled) and a target-language in-domain
monolingual corpus, to supplement the target side of
a parallel corpus while training a language model.
The only available parallel resource is a very small
test corpus. Our objective is to predict the evolution
of the BLEU score on the given test set as a function
of the size of a random subset of the training data
that we manually translate4. The intuition behind
this is that the source-side and target-side mono-
lingual data already convey significant information
about the difficulty of the translation task.
We proceed in the following way. We first train
models to predict the BLEU score at m anchor sizes
s1, . . . , sm, based on a set of features globally char-
acterizing the configuration of interest. We restrict
our attention to linear models:
?j = wj>?, j ? {1 . . .m}
where wj is a vector of feature weights specific to
predicting at anchor size j, and ? is a vector of size-
independent configuration features, detailed below.
We then perform inference using these models to
predict the BLEU score at each anchor, for the test
case of interest. We finally estimate the parameters
of the learning curve by weighted least squares re-
gression using the anchor predictions.
Anchor sizes can be chosen rather arbitrarily, but
must satisfy the following two constraints:
1. They must be three or more in number in order
to allow fitting the tri-parameter curve.
2. They should be spread as much as possible
along the range of sample size.
For our experiments, we take m = 3, with anchors
at 10K, 75K and 500K segments.
The feature vector? consists of the following fea-
tures:
1. General properties: number and average length
of sentences in the (source) test set.
2. Average length of tokens in the (source) test set
and in the monolingual source language corpus.
3. Lexical diversity features:
(a) type-token ratios for n-grams of order 1 to
5 in the monolingual corpus of both source
and target languages
(b) perplexity of language models of order 2
to 5 derived from the monolingual source
corpus computed on the source side of the
test corpus.
4We specify that it is a random sample as opposed to a subset
deliberately chosen to maximize learning effectiveness. While
there are clear ties between our present work and active learn-
ing, we prefer to keep these two aspects distinct at this stage,
and intend to explore this connection in future work.
25
4. Features capturing divergence between lan-
guages in the pair:
(a) average ratio of source/target sentence
lengths in the test set.
(b) ratio of type-token ratios of orders 1 to 5
in the monolingual corpus of both source
and target languages.
5. Word-order divergence: The divergence in the
word-order between the source and the target
languages can be captured using the part-of-
speech (pos) tag sequences across languages.
We use cross-entropy measure to capture sim-
ilarity between the n-gram distributions of the
pos tags in the monolingual corpora of the two
languages. The order of the n-grams ranges be-
tween n = 2, 4 . . . 12 in order to account for
long distance reordering between languages.
The pos tags for the languages are mapped to
a reduced set of twelve pos tags (Petrov et al,
2012) in order to account for differences in
tagsets used across languages.
These features capture our intuition that translation
is going to be harder if the language in the domain
is highly variable and if the source and target lan-
guages diverge more in terms of morphology and
word-order.
The weights wj are estimated from data. The
training data for fitting these linear models is ob-
tained in the following way. For each configuration
(combination of language pair and domain) c and
test set t in Table 2, a gold curve is fitted using the
selected tri-parameter power-law family using a fine
grid of corpus sizes. This is available as a byproduct
of the experiments for comparing different paramet-
ric families described in Section 3. We then compute
the value of the gold curves at the m anchor sizes:
we thus have m ?gold? vectors ?1, . . . ,?m with ac-
curate estimates of BLEU at the anchor sizes5. We
construct the design matrix ? with one column for
each feature vector ?ct corresponding to each com-
bination of training configuration c and test set t.
We then estimate weights wj using Ridge regres-
sion (L2 regularization):
wj = argmin
w
||?>w ? ?j ||2 + C||w||2 (3)
5Computing these values from the gold curve rather than di-
rectly from the observations has the advantage of smoothing the
observed values and also does not assume that observations at
the anchor sizes are always directly available.
where the regularization parameter C is chosen by
cross-validation. We also run experiments using
Lasso (L1) regularization (Tibshirani, 1994) instead
of Ridge. As baseline, we take a constant mean
model predicting, for each anchor size sj , the av-
erage of all the ?jct.
We do not assume the difficulty of predicting
BLEU at all anchor points to be the same. To allow
for this, we use (non-regularized) weighted least-
squares to fit a curve from our parametric family
through the m anchor points6. Following (Croarkin
and Tobias, 2006, Section 4.4.5.2), the anchor con-
fidence is set to be the inverse of the cross-validated
mean square residuals:
?j =
(
1
N
?
c?S
?
t?Tc
(?>ctw
\c
j ? ?jct)
2
)?1
(4)
where w\cj are the feature weights obtained by the
regression above on all training configurations ex-
cept c, ?jct is the gold value at anchor j for train-
ing/test combination c, t, and N is the total number
of such combinations7. In other words, we assign to
each anchor point a confidence inverse to the cross-
validated mean squared error of the model used to
predict it.
For a new unseen configuration with feature vec-
tor ?u, we determine the parameters ?u of the corre-
sponding learning curve as:
?u = argmin
?
?
j
?j
(
F (sj ; ?)? ?>uwj
)2
(5)
5 Extrapolating a learning curve fitted on
a small parallel corpus
Given a small ?seed? parallel corpus, the translation
system can be used to train small in-domain models
and the evaluation score can be measured at a few
initial sample sizes {(x1, y1), (x2, y2)...(xp, yp)}.
The performance of the system for these initial
points provides evidence for predicting its perfor-
mance for larger sample sizes.
In order to do so, a learning curve from the fam-
ily Pow3 is first fit through these initial points. We
6When the number of anchor points is the same as the num-
ber of parameters in the parametric family, the curve can be fit
exactly through all anchor points. However the general discus-
sion is relevant in case there are more anchor points than pa-
rameters, and also in view of the combination of inference and
extrapolation in Section 6.
7Curves on different test data for the same training configu-
ration are highly correlated and are therefore left out.
26
assume that p ? 3 for this operation to be well-
defined. The best fit ?? is computed using the same
curve fitting as in Eq. 2.
At each individual anchor size sj , the accuracy of
prediction is measured using the root mean-squared
error between the prediction of extrapolated curves
and the gold values:
(
1
N
?
c?S
?
t?Tc
[F (sj ; ??ct)? ?ctj ]
2
)1/2
(6)
where ??ct are the parameters of the curve fit using
the initial points for the combination ct.
In general, we observed that the extrapolated
curve tends to over-estimate BLEU for large sam-
ples.
6 Combining inference and extrapolation
In scenario S2, the models trained from the seed par-
allel corpus and the features used for inference (Sec-
tion 4) provide complementary information. In this
section we combine the two to see if this yields more
accurate learning curves.
For the inference method of Section 4, predictions
of models at anchor points are weighted by the in-
verse of the model empirical squared error (?j). We
extend this approach to the extrapolated curves. Let
u be a new configuration with seed parallel corpus of
size xu, and let xl be the largest point in our grid for
which xl ? xu. We first train translation models and
evaluate scores on samples of size x1, . . . , xl, fit pa-
rameters ??u through the scores, and then extrapolate
BLEU at the anchors sj : F (sj ; ??u), j ? {1, . . . ,m}.
Using the models trained for the experiments in Sec-
tion 3, we estimate the squared extrapolation error at
the anchors sj when using models trained on size up
to xl, and set the confidence in the extrapolations8
for u to its inverse:
?<lj =
(
1
N
?
c?S
?
t?Tc
(F (sj ; ?
<l
ct )? ?ctj)
2
)?1
(7)
where N , S, Tc and ?ctj have the same meaning as
in Eq. 4, and ?<lct are parameters fitted for config-
uration c and test t using only scores measured at
x1, . . . , xl. We finally estimate the parameters ?u of
8In some cases these can actually be interpolations.
the combined curve as:
?u = argmin
?
?
j
?j(F (sj ; ?)? ?
>
uwj)
2
+ ?<lj (F (sj ; ?)? F (sj ; ??u))
2
where ?u is the feature vector for u, and wj are the
weights we obtained from the regression in Eq. 3.
7 Experiments
In this section, we report the results of our experi-
ments on predicting the learning curves.
7.1 Inferred Learning Curves
Regression model 10K 75K 500K
Ridge 0.063 0.060 0.053
Lasso 0.054 0.060 0.062
Baseline 0.112 0.121 0.121
Table 4: Root mean squared error of the linear regression
models for each anchor size
In the case of inference from mostly monolingual
data, the accuracy of the predictions at each of the
anchor sizes is evaluated using root mean-squared
error over the predictions obtained in a leave-one-
out manner over the set of configurations from Ta-
ble 2. Table 4 shows these results for Ridge and
Lasso regression models at the three anchor sizes.
As an example, the model estimated using Lasso for
the 75K anchor size exhibits a root mean squared
error of 6 BLEU points. The errors we obtain are
lower than the error of the baseline consisting in tak-
ing, for each anchor size sj , the average of all the
?ctj . The Lasso regression model selected four fea-
tures from the entire feature set: i) Size of the test
set (sentences & tokens) ii) Perplexity of language
model (order 5) on the test set iii) Type-token ratio
of the target monolingual corpus . Feature correla-
tion measures such as Pearsons R showed that the
features corresponding to type-token ratios of both
source and target languages and size of test set have
a high correlation with the BLEU scores at the three
anchor sizes.
Figure 2 shows an instance of the inferred learn-
ing curves obtained using a weighted least squares
method on the predictions at the anchor sizes. Ta-
ble 7 presents the cumulative error of the inferred
learning curves with respect to the gold curves, mea-
sured as the average distance between the curves in
the range x ? [0.1K, 100K].
27
Figure 2: Inferred learning curve for English-Japanese
test set. The error-bars show the anchor confidence for
the predictions.
7.2 Extrapolated Learning Curves
As explained in Section 5, we evaluate the accuracy
of predictions from the extrapolated curve using the
root mean squared error (see Eq. 6) between the pre-
dictions of this curve and the gold values at the an-
chor points.
We conducted experiments for three sets of initial
points, 1) 1K-5K-10K, 2) 5K-10K-20K, and 3) 1K-
5K-10K-20K. For each of these sets, we show the
prediction accuracy at the anchor sizes, 10K9, 75K,
and 500K in Table 5.
Initial Points 10K 75K 500K
1K-5K-10K 0.005 0.017 0.042
5K-10K-20K 0.002 0.015 0.034
1K-5K-10K-20K 0.002 0.008 0.019
Table 5: Root mean squared error of the extrapolated
curves at the three anchor sizes
The root mean squared errors obtained by extrap-
olating the learning curve are much lower than those
obtained by prediction of translation accuracy using
the monolingual corpus only (see Table 4), which
is expected given that more direct evidence is avail-
able in the former case . In Table 5, one can also
see that the root mean squared error for the sets 1K-
5K-10K and 5K-10K-20K are quite close for anchor
9The 10K point is not an extrapolation point but lies within
the range of the set of initial points. However, it does give a
measure of the closeness of the curve fit using only the initial
points with the gold fit using all the points; the value of this gold
fit at 10K is not necessarily equal to the observation at 10K.
sizes 75K and 500K. However, when a configuration
of four initial points is used for the same amount of
?seed? parallel data, it outperforms both the config-
urations with three initial points.
7.3 Combined Learning Curves and Overall
Comparison
In Section 6, we presented a method for combin-
ing the predicted learning curves from inference and
extrapolation by using a weighted least squares ap-
proach. Table 6 reports the root mean squared error
at the three anchor sizes from the combined curves.
Initial Points Model 10K 75K 500K
1K-5K-10K
Ridge 0.005 0.015 0.038
Lasso 0.005 0.014 0.038
5K-10K-20K
Ridge 0.001 0.006 0.018
Lasso 0.001 0.006 0.018
1K-5K-10K-20K
Ridge 0.001 0.005 0.014
Lasso 0.001 0.005 0.014
Table 6: Root mean squared error of the combined curves
at the three anchor sizes
We also present an overall evaluation of all the
predicted learning curves. The evaluation metric is
the average distance between the predicted curves
and the gold curves, within the range of sample sizes
xmin=0.1K to xmax=500K segments; this metric is
defined as:
1
N
?
c?S
?
t?Tc
?xmax
x=xmin |F (x; ??ct)? F (x; ??ct)|
xmax ? xmin
where ??ct is the curve of interest, ??ct is the gold
curve, and x is in the range [xmin, xmax], with a step
size of 1. Table 7 presents the final evaluation.
Initial Points IR IL EC CR CL
1K-5K-10K 0.034 0.050 0.018 0.015 0.014
5K-10K-20K 0.036 0.048 0.011 0.010 0.009
1K-5K-10K-20K 0.032 0.049 0.008 0.007 0.007
Table 7: Average distance of different predicted
learning curves relative to the gold curve. Columns:
IR=?Inference using Ridge model?, IL=?Inference
using Lasso model?, EC=?Extrapolated curve?,
CR=?Combined curve using Ridge?, CL=?Combined
curve using Lasso?
We see that the combined curves (CR and CL)
perform slightly better than the inferred curves (IR
28
and IL) and the extrapolated curves (EC). The aver-
age distance is on the same scale as the BLEU score,
which suggests that our best curves can predict the
gold curve within 1.5 BLEU points on average (the
best result being 0.7 BLEU points when the initial
points are 1K-5K-10K-20K) which is a telling re-
sult. The distances between the predicted and the
gold curves for all the learning curves in our experi-
ments are shown in Figure 3.
Figure 3: Distances between the predicted and the gold
learning curves in our experiments across the range of
sample sizes. The dotted lines indicate the distance from
gold curve for each instance, while the bold line indi-
cates the 95th quantile of the distance between the curves.
IR=?Inference using Ridge model?, EC=?Extrapolated
curve?, CR=?Combined curve using Ridge?.
We also provide a comparison of the different pre-
dicted curves with respect to the gold curve as shown
in Figure 4.
Figure 4: Predicted curves in the three scenarios for
Czech-English test set using the Lasso model
8 Conclusion
The ability to predict the amount of parallel data
required to achieve a given level of quality is very
valuable in planning business deployments of statis-
tical machine translation; yet, we are not aware of
any rigorous proposal for addressing this need.
Here, we proposed methods that can be directly
applied to predicting learning curves in realistic sce-
narios. We identified a suitable parametric fam-
ily for modeling learning curves via an extensive
empirical comparison. We described an inference
method that requires a minimal initial investment in
the form of only a small parallel test dataset. For the
cases where a slightly larger in-domain ?seed? par-
allel corpus is available, we introduced an extrapola-
tion method and a combined method yielding high-
precision predictions: using models trained on up to
20K sentence pairs we can predict performance on a
given test set with a root mean squared error in the
order of 1 BLEU point at 75K sentence pairs, and
in the order of 2-4 BLEU points at 500K. Consider-
ing that variations in the order of 1 BLEU point on
a same test dataset can be observed simply due to
the instability of the standard MERT parameter tun-
ing algorithm (Foster and Kuhn, 2009; Clark et al,
2011), we believe our results to be close to what can
be achieved in principle. Note that by using gold
curves as labels instead of actual measures we im-
plicitly average across many rounds of MERT (14
for each curve), greatly attenuating the impact of the
instability in the optimization procedure due to ran-
domness.
For enabling this work we trained a multitude
of instances of the same phrase-based SMT sys-
tem on 30 distinct combinations of language-pair
and domain, each with fourteen distinct training
sets of increasing size and tested these instances on
multiple in-domain datasets, generating 96 learning
curves. BLEU measurements for all 96 learning
curves along with the gold curves and feature values
used for inferring the learning curves are available
as additional material to this submission.
We believe that it should be possible to use in-
sights from this paper in an active learning setting,
to select, from an available monolingual source, a
subset of a given size for manual translation, in such
a way at to yield the highest performance, and we
plan to extend our work in this direction.
29
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 745?
754, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22?64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better Hypothesis Testing for Statis-
tical Machine Translation: Controlling for Optimizer
Instability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Carroll Croarkin and Paul Tobias. 2006.
NIST/SEMATECH e-Handbook of Statistical Meth-
ods. NIST/SEMATECH, July. Available online:
http://www.itl.nist.gov/div898/handbook/.
George Foster and Roland Kuhn. 2009. Stabilizing
Minimum Error Rate Training. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 242?249, Athens, Greece, March. Association
for Computational Linguistics.
Baohua Gu, Feifang Hu, and Huan Liu. 2001. Mod-
elling Classification Performance for Large Data Sets.
In Proceedings of the Second International Conference
on Advances in Web-Age Information Management,
WAIM ?01, pages 317?328, London, UK. Springer-
Verlag.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of Human Language Technologies: The 2003 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 48?54,
Edmonton, Canada, May. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
10th Machine Translation Summit, Phuket, Thailand,
September.
Jorge J. More?. 1978. The Levenberg-Marquardt Algo-
rithm: Implementation and Theory. Numerical Anal-
ysis. Proceedings Biennial Conference Dundee 1977,
630:105?116.
Graham Neubig. 2011. The Kyoto Free Translation
Task. http://www.phontron.com/kftt.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Claudia Perlich, Foster J. Provost, and Jeffrey S. Si-
monoff. 2003. Tree Induction vs. Logistic Regres-
sion: A Learning-Curve Analysis. Journal of Machine
Learning Research, 4:211?255.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceedings
of the Eighth conference on International Language
Resources and Evaluation (LREC?12), Istanbul, May.
European Language Resources Association (ELRA).
Robert Tibshirani. 1994. Regression Shrinkage and Se-
lection Via the Lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Jo?rg Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume V, pages 237?248. John Benjamins,
Amsterdam/Philadelphia, Borovets, Bulgaria.
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008.
Learning Performance of a Machine Translation Sys-
tem: a Statistical and Computational Analysis. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 35?43, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
30
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 23?27,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Private Access to Phrase Tables for Statistical Machine Translation
Nicola Cancedda
Xerox Research Centre Europe
6, chemin de Maupertuis
38240, Meylan, France
Nicola.Cancedda@xrce.xerox.com
Abstract
Some Statistical Machine Translation systems
never see the light because the owner of the
appropriate training data cannot release them,
and the potential user of the system cannot dis-
close what should be translated. We propose a
simple and practical encryption-based method
addressing this barrier.
1 Introduction
It is generally taken for granted that whoever is
deploying a Statistical Machine Translation (SMT)
system has unrestricted rights to access and use the
parallel data required for its training. This is not al-
ways the case. The ideal resources for training SMT
models are Translation Memories (TM), especially
when they are large, well maintained, coherent in
genre and topic and aligned with the application of
interest. Such TMs are cherished as valuable as-
sets by their owners, who rarely accept to give away
wholesale rights to their use. At the same time, the
prospective user of the SMT system that could be
derived from such TM might be subject to confiden-
tiality constraints on the text stream needing transla-
tion, so that sending out text to translate to an SMT
system deployed by the owner of the PT is not an
option.
We propose an encryption-based method that ad-
dresses such conflicting constraints. In this method,
the owner of the TM generates a Phrase Table (PT)
from it, and makes it accessible to the user following
a special procedure. An SMT decoder is deployed
by the user, with all the required resources to oper-
ate except the PT1.
As a result of following the proposed procedure:
? The user acquires all and only the phrase table
entries required to perform the decoding of a
specific file, thus avoiding complete transfer of
the TM to the user;
? The owner of the PT does not learn anything
about what is being translated, thus satisfying
the user?s confidentiality constraints;
? The owner of the PT can track the number of
phrase-table entries that was downloaded by
the user.
The method assumes that, besides the PT Owner
and the PT User, there is a Trusted Third Party. This
means that both the User and the PT owner trust such
third party not to collude with the other one for vi-
olating their secrets (i.e. the content of the PT, or a
string requiring translation), even if they do not trust
her enough to directly disclose such secrets to her.
While the exposition will focus on phrase tables,
there is nothing in the method precluding its use with
other resources, provided that they can be repre-
sented as look-up tables, a very mild constraint. Pro-
vided speed-related aspects can be dealt with, this
makes the method directly applicable to language
models, or distortion tables for models with lexi-
calized distortion (Al-Onaizan and Papineni, 2006).
The method is also directly applicable to Transla-
tion Memories, which can be seen as ?degenerate?
1If the decoder can operate with multiple PTs, then there
could be other (possibly out-of-domain) PTs installed locally.
23
phrase tables where each record contains only a
translation in the target language, and no associated
statistics.
The rest of this paper is organized as follows: Sec-
tion 2 explains the proposed method; in Section 3 we
make more precise some implementation choices.
We briefly touch on related work on Section 4, pro-
vide an experimental validation in Sec. 5, and offer
some concluding remarks in Sec. 6.
2 Private access to phrase tables
Let Alice2 be the owner of a PT, Bob the owner of
the SMT decoder who would like to use the table,
and Tina a trusted third-party. In broad terms, the
proposed method works like this: in an initializa-
tion phase, Alice first encrypts PT entries one by
one, sends the encrypted PT to Bob, and the en-
cryption/decryption keys to Tina. Alice also sends
a method to map source language phrases to PT in-
dices to Bob.
When translating, Bob uses the mapping method
sent by Alice to check if a given source phrase is
present and has a translation in the PT and, if this is
the case, retrieves the index of the corresponding en-
try in the PT. If the check is positive, then Bob sends
a request to Tina for the corresponding decryption
key. Tina delivers the decryption key to Bob and
communicates that a download has taken place to
Alice, who can then increase a download counter.
Let {(s1, v1), . . . , (sn, vn)} be a PT, where si is
a source phrase and vi is the corresponding record.
In an actual PT there are multiple lines for a same
source phrase, but it is always possible to reconstruct
a single record by concatenating all such lines.
2.1 Initialization
The initialization phase is illustrated in Fig. 1. For
each PT entry (si, vi), Alice:
1. Encrypts vi with key ki We denote the en-
crypted record as vi ? ki
2. Computes a digest di of the source entry si
3. Sends the phrase digests {di}i=1,...,n to Bob
2We adopt a widespread convention in cryptography and as-
sign person names to the parties involved in the exchange.
ivsi kiivkiid
id kiiv ki
Alice
=
Bob Tina
1
2
3 4 5
Figure 1: The initialization phase of the method
(Sec. 2.1). Bob receives an encrypted version of the PT
entries and the corresponding source phrase digests. Tina
receives the decryption keys.
4. Sends the encrypted record (or ciphertext)
{vi ? ki}i=1,...,n to Bob
5. Sends the keys {ki}i=1,...,n to Tina
A digest, or one-way hash function (Schneider,
1996), is a particular type of hash function. It takes
as input a string of arbitrary length, and determin-
istically produces a bit string of fixed length. It is
such that it is virtually impossible to reconstruct a
message given its digest, and that the probability of
collisions, i.e. of two strings being given the same
digest, is negligible.
At the end of the initialization, neither Bob nor
Tina can access the content of the PT, unless they
collude.
2.2 Retrieval
During translation, Bob has a source phrase s and
would like to retrieve from the PT the corresponding
entry, if it is present. To do so (Fig. 2):
1. Bob computes the digest d of s using the same
cryptographic hash function used by Alice in
the initialization phase;
2. Bob checks whether d ? {di}i=1,...,n. If the
check is negative then s does not have an entry
in the PT, and the process stops. If the check is
positive then s has an entry in the PT: let is be
the corresponding index;
24
d is
is
iskisisv isv
isk
isk
s
d =
Bob
k =
+1
Tina
Alice
1
2
3
4
5
Figure 2: The retrieval phase (Sec. 2.2).
3. Bob requests to Tina key kis ;
4. Tina sends Bob kis and notifies Alice, who can
increment a counter of PT entries downloaded
by Bob;
5. Bob decrypts vis ? kis using key kis , and re-
covers vis .
At the end of the process, Bob retrieved from the
PT owned by Alice an entry if and only if it matched
phrase s (this is guaranteed by the virtual absence of
collisions ensured by the cryptographic hash func-
tions used for computing phrase digests). Alice was
notified by Tina that Bob downloaded one entry, as
desired, while neither Tina nor Alice could learn s,
unless they colluded.
3 Implementation
For clarity of exposition, in Section 2.2 we presented
a method for looking up PT entries involving one in-
teraction for each phrase look-up. In our implemen-
tation, we batch all requests for all source phrases
up to a predefined length for all sentences in a given
file. This mirrors the standard practice of filtering
the phrase table for a given source file to translate
before starting the actual decoding.
Out of the large choice of cryptographic hash
functions in the literature (Schneider, 1996), we
chose 128 bits md5 for its widespread availability in
multiple programming languages and environments.
For encrypting entries, we used bit-wise XOR
with a string of random bits (the key) of the same
length as the encrypted item. This symmetric en-
cryption is known as one-time pad, and it is unbreak-
able, provided key bits are really random.
Both keys and ciphertext are indexed and sorted
by increasing md5 digest of the corresponding
source phrase. For retrieving all entries matching
a given text file, Bob generates md5 digests for all
source phrases up to a maximum length, sorts them,
and performs a join with the encrypted entry file.
Matching digests are then sent to Tina for her to join
with the keys. It is important that Bob uses the same
tokenizer/word segmentation scheme used by Alice
in preprocessing training data before extracting the
PT.
Note that it is never necessary to have any massive
data structure in main memory, and all process steps
except the initial sorting by md5 digest are linear in
the number of PT entries or in the number of tokens
to look up. The process results however in increased
storage and bandwidth requirements, since cipher-
text and key have each roughly the same size as the
original PT.
4 Related work
We are not aware of any previous work directly ad-
dressing the problem we solve, i.e. private access
to a phrase table or other resources for the pur-
pose of performing statistical machine translation.
Private access to electronic information in general,
however, is an active research area. While effec-
tive, the scheme proposed here is rather basic, com-
pared to what can be found in specialized literature,
e.g. (Chor et al, 1998; Bellovin and Cheswick,
2004). An interesting and relatively recent sur-
vey of the field of secure multiparty computation
and privacy-preserving data mining is (Lindell and
Pinkas, 2009).
5 Experiments
We validated our simple implementation using a
phrase table of 38,488,777 lines created with the
Moses toolkit3(Koehn et al, 2007) phrase-based
SMT system, corresponding to 15,764,069 entries
for distinct source phrases4.
3http://www.statmt.org/moses/
4The birthday bound for a 128 bit hash like md5 for a col-
lision probability of 10?18 is around 2.6 ? 1010. This means
25
Figure 3: Time required to complete the initialization as
a function of the number of lines in the original PT.
This PT was obtained processing the training data
of the English-Spanish Europarl corpus used in the
WMT 2008 shared task5. We used a 2,000 sentence
test set of the same shared evaluation for experi-
menting with the querying phase.
We conducted all experiments on a single core of
an ordinary Linux server6 with 32Gb of RAM. Both
initialization and retrieval can be easily parallelized.
Figure 3 shows the time required to complete the
initialization phase as a function of the size of the
original PT (in million of lines). The progression
is largely linear, and the overall initialization time
of roughly 45 minutes for the complete PT indicates
that the method can be used in practice. Note that
the Europarl corpus originating the phrase-table is
much larger than most TMs available at even large
language service providers.
Figure 4 displays the time required to complete
retrieval for subsets of increasing size of the 2,000
sentence test set, and for phrase tables uniformly
sampled at 25%, 50%, 75% and 100%. 217,019
distinct digests are generated for all possible phrase
of length up to 6 from the full test set, resulting in
the retrieval of 47,072 entries (596,560 lines) from
the full phrase table. Our implementation of the re-
trieval uses the Unix join command on the ciphertext
and the key tables, and performs a full scan through
that if the hash distributed keys perfectly uniformly, then about
26 billion entries would be required for the collision probabil-
ity to exceed 10?18. While no hash function, including md5,
distributes keys perfectly evenly (Bellare and Kohno, 2004), the
number of entries likely to be handled in our application is or-
ders of magnitude smaller than the bound.
5http://www.statmt.org/wmt08/shared-task.html
6Intel Xeon 3.1 GHz.
Figure 4: Time required for retrieval as a function of the
number of sentences in the query, for different subsets of
the original phrase table.
those files. Complexity hence depends more on the
size of the PT than on the length of the query. An
ad-hoc indexing of the encrypted entries and of the
keys in e.g. a standard database would make the
dependency logarithmic in the number of entries,
and linear in the number of source tokens. Digests?
prefixes are perfectly suited for bucketing ciphertext
and keys. This would be useful if query batches are
small.
6 Conclusions
Some SMT systems never get deployed because
of legitimate and incompatible concerns of the
prospective users and of the training data owners.
We propose a method that guarantees to the owner of
a TM that only some fraction of an artifact derived
from the original resource, a phrase-table, is trans-
ferred, and only in a very controlled way allowing
to track downloads. This same method also guaran-
tees the privacy of the user, who is not required to
disclose the content of what needs translation.
Empirical validation on demanding conditions
shows that the proposed method is practical on or-
dinary computing infrastructure.
This same method can be easily extended to other
resources used by SMT systems, and indeed even
beyond SMT itself, whenever similar constraints on
data access exist.
26
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, ACL-
44, pages 529?536, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Mihir Bellare and Tadayoshi Kohno. 2004. Hash func-
tion balance and its impact on birthday attacks. In
Advances in Cryptology, EUROCRYPT 2004, volume
3027 of Lecture Notes in Computer Science, pages
401?418.
Steven M. Bellovin and William R. Cheswick. 2004.
Privacy-enhanced searches using encrypted bloom fil-
ters. Technical Report CUCS-034-07, Columbia Uni-
versity.
Benny Chor, Oded Goldreich, Eyal Kushilevitz, and
Madhu Sudan. 1998. Private information retrieval.
Journal of the ACM, 45(6):965?982.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yehuda Lindell and Benny Pinkas. 2009. Secure mul-
tiparty computation and privacy-preserving data min-
ing. The Journal of Privacy and Confidentiality,
1(1):59?98.
Bruce Schneider. 1996. Applied Cryptography. John
Wiley and sons.
27
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 1?9,
COLING 2010, Beijing, August 2010.
Intersecting Hierarchical and Phrase-Based Models of Translation:
Formal Aspects and Algorithms
Marc Dymetman Nicola Cancedda
Xerox Research Centre Europe
{marc.dymetman,nicola.cancedda}@xrce.xerox.com
Abstract
We address the problem of construct-
ing hybrid translation systems by inter-
secting a Hiero-style hierarchical sys-
tem with a phrase-based system and
present formal techniques for doing so.
We model the phrase-based component
by introducing a variant of weighted
finite-state automata, called ?-automata,
provide a self-contained description
of a general algorithm for intersect-
ing weighted synchronous context-free
grammars with finite-state automata, and
extend these constructs to ?-automata.
We end by briefly discussing complexity
properties of the presented algorithms.
1 Introduction
Phrase-based (Och and Ney, 2004; Koehn et
al., 2007) and Hierarchical (Hiero-style) (Chi-
ang, 2007) models are two mainstream ap-
proaches for building Statistical Machine Trans-
lation systems, with different characteristics.
While phrase-based systems allow a direct cap-
ture of correspondences between surface-level
lexical patterns, but at the cost of a simplistic
handling of re-ordering, hierarchical systems are
better able to constrain re-ordering, especially
for distant language pairs, but tend to produce
sparser rules and often lag behind phrase-based
systems for less distant language pairs. It might
therefore make sense to capitalize on the com-
plementary advantages of the two approaches by
combining them in some way.
This paper attempts to lay out the formal
prerequisites for doing so, by developing tech-
niques for intersecting a hierarchical model and
a phrase-based model. In order to do so, one first
difficulty has to be overcome: while hierarchical
systems are based on the mathematically well-
understood formalism of weighted synchronous
CFG?s, phrase-based systems do not correspond
to any classical formal model, although they are
loosely connected to weighted finite state trans-
ducers, but crucially go beyond these by allow-
ing phrase re-orderings.
One might try to address this issue by limiting
a priori the amount of re-ordering, in the spirit
of (Kumar and Byrne, 2005), which would allow
to approximate a phrase-based model by a stan-
dard transducer, but this would introduce further
issues. First, limiting the amount of reorder-
ing in the phrase-based model runs contrary to
the underlying intuitions behind the intersection,
namely that the hierarchical model should be
mainly responsible for controlling re-ordering,
and the phrase-based model mainly responsible
for lexical choice. Second, the transducer result-
ing from the operation could be large. Third,
even if we could represent the phrase-based
model through a finite-state transducer, intersect-
ing this transducer with the synchronous CFG
would actually be intractable in the general case,
as we indicate later.
We then take another route. For a fixed source
sentence x, we show how to construct an au-
tomaton that represents all the (weighted) tar-
get sentences that can be produced by apply-
ing the phrase based model to x. However, this
??-automaton? is non-standard in the sense that
each transition is decorated with a set of source
sentence tokens and that the only valid paths are
1
those that do not traverse two sets containing the
same token (in other words, valid paths cannot
?consume? the same source token twice).
The reason we are interested in ?-automata
is the following. First, it is known that inter-
secting a synchronous grammar simultaneously
with the source sentence x and a (standard) target
automaton results in another synchronous gram-
mar; we provide a self-contained description of
an algorithm for performing this intersection, in
the general weighted case, and where x is gener-
alized to an arbitrary source automaton. Second,
we extend this algorithm to ?-automata. The
resulting weighted synchronous grammar repre-
sents, as in Hiero, the ?parse forest? (or ?hy-
pergraph?) of all weighted derivations (that is
of all translations) that can be built over x, but
where the weights incorporate knowledge of the
phrase-based component; it can therefore form
the basis of a variety of dynamic programming
or sampling algorithms (Chiang, 2007; Blunsom
and Osborne, 2008), as is the case with standard
Hiero-type representations. While in the worst
case the intersected grammar can contain an ex-
ponential number of nonterminals, we argue that
such combinatorial explosion will not happen in
practice, and we also briefly indicate formal con-
ditions under which it will not be allowed to hap-
pen.
2 Intersecting weighted synchronous
CFG?s with weighted automata
We assume that the notions of weighted finite-
state automaton [W-FSA] and weighted syn-
chronous grammar [W-SCFG] are known (for
short descriptions see (Mohri et al, 1996) and
(Chiang, 2006)), and we consider:
1. A W-SCFG G, with associated source
grammar Gs (resp. target grammar Gt); the
terminals of Gs (resp. Gt) vary over the
source vocabulary Vs (resp. target vocab-
ulary Vt).
2. A W-FSA As over the source vocabulary
Vs, with initial state s# and final state s$.
3. A W-FSA At over the target vocabulary Vt,
with initial state t# and final state t$.
The grammar G defines a weighted synchronous
language LG over (Vs, Vt), the automaton As a
weighted language Ls over Vs, and the automa-
ton At a weighted language Lt over Vt. We
then define the intersection language L? between
these three languages as the synchronous lan-
guage denoted L? = Ls e LG e Lt over (Vs, Vt)
such that, for any pair (x, y) of a source and a
target sentence, the weight L?(x, y) is defined
by L?(x, y) ? Ls(x) ? LG(x, y) ? Lt(y), where
Ls(x), LG(x, y), Lt(y) are the weights associ-
ated to each of the component languages.
It is natural to ask whether there exists a syn-
chronous grammar G? generating the language
L?, which we will now show to be the case.1
Our approach is inspired by the construction in
(Bar-Hillel et al, 1961) for the intersection of a
CFG and an FSA and the observation in (Lang,
1994) relating this construction to parse forests,
and also partially from (Satta, 2008), although,
by contrast to that work, our construction, (i)
is done simultaneously rather than as the se-
quence of intersecting As with G, then the re-
sulting grammar with At, (ii) handles weighted
formalisms rather than non-weighted ones.
We will describe the construction of G? based
on an example, from which the general construc-
tion follows easily. Consider a W-SCFG gram-
mar G for translating between French and En-
glish, with initial nonterminal S, and containing
among others the following rule:
N? A manque a` B / B misses A : ?, (1)
where the source and target right-hand sides are
separated by a slash symbol, and where ? is a
non-negative real weight (interpreted multiplica-
tively) associated with the rule.
Now let?s consider the following ?rule
scheme?:
t0
s0Nt3s4 ? t2s0At3s1 s1manques2 s2 a`s3 t0s3Bt1s4 /
t0
s3Bt1s4 t1missest2 t2s0At3s1 (2)
1We will actually only need the application of this result
to the case where As is a ?degenerate? automaton describ-
ing a single source sentence x, but the general construction
is not harder to do than this special case and the resulting
format for G? is well-suited to our needs below.
2
This scheme consists in an ?indexed? version of
the original rule, where the bottom indices si
correspond to states of As (?source states?), and
the top indices ti to states of At (?target states?).
The nonterminals are associated with two source
and two target indices, and for the same nonter-
minal, these four indices have to match across
the source and the target RHS?s of the rule. As
for the original terminals, they are replaced by
?indexed terminals?, where source (resp. tar-
get) terminals have two source (resp. target) in-
dices. The source indices appear sequentially
on the source RHS of the rule, in the pattern
s0, s1, s1, s2, s2 . . . sm?1, sm, with the nonter-
minal on the LHS receiving source indices s0
and sm, and similarly the target indices appear
sequentially on the target RHS of the rule, in the
pattern t0, t1, t1, t2, t2 . . . tn?1, tn, with the non-
terminal on the LHS receiving target indices t0
and tn. To clarify, the operation of associating
indices to terminals and nonterminals can be de-
composed into three steps:
s0Ns4 ? s0As1 s1manques2 s2 a` s3 s3Bs4 /
B misses A
t0Nt3 ? A manque a` B /
t0Bt1 t1missest2 t2At3
t0
s0Nt3s4 ? t2s0At3s1 s1manques2 s2 a` s3 t0s3Bt1s4 /
t0
s3Bt1s4 t1missest2 t2s0At3s1
where the first two steps corresponds to handling
the source and target indices separately, and the
third step then assembles the indices in order to
get the same four indices on the two copies of
each RHS nonterminal. The rule scheme (2) now
generates a family of rules, each of which corre-
sponds to an arbitrary instantiation of the source
and target indices to states of the source and tar-
get automata respectively. With every such rule
instantiation, we associate a weight ?? which is
defined as:
?? ? ? ?
?
si s-termsi+1
?As(si, s-term, si+1)
?
?
tj t-termtj+1
?At(tj , t-term, tj+1), (3)
where the first product is over the indexed source
terminals sis-termsi+1 , the second product
over the indexed target terminals tj t-termtj+1 ;
?As(si, s-term, si+1) is the weight of the transi-
tion (si, s-term, si+1) according to As, and sim-
ilarly for ?At(tj , t-term, tj+1). In these prod-
ucts, it may happen that ?As(si, s-term, si+1) is
null (and similarly for At), and in such a case,
the corresponding rule instantiation is consid-
ered not to be realized. Let us consider the multi-
set of all the weighted rule instantiations for (1)
computed in this way, and for each rule in the
collection, let us ?forget? the indices associated
to the terminals. In this way, we obtain a col-
lection of weighted synchronous rules over the
vocabularies Vs and Vt, but where each nonter-
minal is now indexed by four states.2
When we apply this procedure to all the rules
of the grammar G, we obtain a new weighted
synchronous CFG G?, with start symbol t#s#St$s$ ,
for which we have the following Fact, of which
we omit the proof for lack of space.
Fact 1. The synchronous language LG? associ-
ated with G? is equal to L? = Ls e LG e Lt.
The grammar G? that we have just constructed
does fulfill the goal of representing the bilat-
eral intersection that we were looking for, but
it has a serious defect: most of its nontermi-
nals are improductive, that is, can never pro-
duce a bi-sentence. If a rule refers to such an
improductive nonterminal, it can be eliminated
from the grammar. This is the analogue for a
SCFG of the classical operation of reduction for
CFG?s; while, conceptually, we could start from
G? and perform the reduction by deleting the
many rules containing improductive nontermi-
nals, it is equivalent but much more efficient to
do the reverse, namely to incrementally add the
productive nonterminals and rules of G? starting
from an initially empty set of rules, and by pro-
ceeding bottom-up starting from the terminals.
We do not detail this process, which is relatively
2It is possible that the multiset obtained by this simpli-
fying operation contains duplicates of certain rules (pos-
sibly with different weights), due to the non-determinism
of the automata: for instance, two sequences such
as?s1manques2 s2 a`s3 ? and ?s1manques?2 s?2 a`s3 ? become in-distinguishable after the operation. Rather than producing
multiple instances of rules in this way, one can ?conflate?
them together and add their weights.
3
straightforward.3
A note on intersecting SCFGs with transduc-
ers Another way to write Ls e LG e Lt is as
the intersection (Ls ? Lt) ? LG. (Ls ? Lt) can
be seen as a rational language (language gener-
ated by a finite state transducer) of an especially
simple form over Vs ? Vt . It is then natural
to ask whether our previous construction can be
generalized to the intersection of G with an arbi-
trary finite-state transducer. However, this is not
the case. Deciding the emptiness problem for
the intersection between two finite state trans-
ducers is already undecidable, by reduction to
Post?s Correspondence problem (Berstel, 1979,
p. 90) and we have extended the proof of this fact
to show that intersection between a synchronous
CFG and a finite state transducer also has an un-
decidable emptiness problem (the proof relies on
the fact that a finite state transducer can be sim-
ulated by a synchronous grammar). A fortiori,
this intersection cannot be represented through
an (effectively constructed) synchronous CFG.
3 Phrase-based models and ?-automata
3.1 ?-automata: definition
Let Vs be a source vocabulary, Vt a target vocab-
ulary. Let x = x1, . . . , xM be a fixed sequence
of words over a certain source vocabulary Vs.
Let us denote by z a token in the sequence x,
and by Z the set of the M tokens in x. A ?-
automaton over x has the general form of a stan-
dard weighted automaton over the target vocabu-
lary, but where the edges are also decorated with
elements ofP(Z), the powerset ofZ (see Fig. 1).
An edge in the ?-automaton between two states
q and q? then carries a label of the form (?, ?),
where ? ? P(Z) and ? ? Vt (note that here we
do not allow ? to be the empty string ). A path
from the initial state of the automaton to its fi-
nal state is defined to be valid iff each token of x
appears in exactly one label of the path, but not
necessarily in the same order as in x. As usual,
the output associated with the path is the ordered
3This bottom-up process is analogous to chart-parsing,
but here we have decomposed the construction into first
building a semantics-preserving grammar and then reduc-
ing it, which we think is formally neater.
sequence of target labels on that path, and the
weight of the path is the product of the weights
on its edges.
?-automata and phrase-based translation
A mainstream phrase-based translation system
such as Moses (Koehn et al, 2007) can be ac-
counted for in terms of ?-automata in the follow-
ing way. To simplify exposition, we assume that
the language model used is a bigram model, but
any n-gram model can be accommodated. Then,
given a source sentence x, decoding works by at-
tempting to construct a sequence of phrase-pairs
of the form (x?1, y?1), ..., (x?k, y?k) such that each
x?i corresponds to a contiguous subsequence of
tokens of x, the x?i?s do not overlap and com-
pletely cover x, but may appear in a different
order than that of x; the output associated with
the sequence is simply the concatenation of all
the y?i?s in that sequence.4 The weight associ-
ated with the sequence of phrase-pairs is then
the product (when we work with probabilities
rather than log-probabilities) of the weight of
each (x?i+1, y?i+1) in the context of the previous
(x?i, y?i), which consists in the product of several
elements: (i) the ?out-of-context? weight of the
phrase-pair (x?i+1, y?i+1) as determined by its fea-
tures in the phrase table, (ii) the language model
probability of finding y?i+1 following y?i,5 (iii)
the contextual weight of (x?i+1, y?i+1) relative to
(x?i, y?i) corresponding to the distorsion cost of
?jumping? from the token sequence x?i to the to-
ken sequence x?i+1 when these two sequences
may not be consecutive in x.6
Such a model can be represented by a ?-
automaton, where each phrase-pair (x?, y?) ? for
4We assume here that the phrase-pairs (x?i, y?i) are such
that y?i is not the empty string (this constraint could be re-
moved by an adaptation of the -removal operation (Mohri,
2002) to ?-automata).
5This is where the bigram assumption is relevant: for
a trigram model, we may need to encode in the automaton
not only the immediately preceding phrase-pair, but also
the previous one, and so on for higher-order models. An
alternative is to keep the n-gram language model outside
the ?-automaton and intersect it later with the grammar G?
obtained in section 4, possibly using approximation tech-
niques such as cube-pruning (Chiang, 2007).
6Any distorsion model ? in particular ?lexicalized re-
ordering? ? that only depends on comparing two consec-
utive phrase-pairs can be implemented in this way.
4
# h
b a
r
k
$
tcl f
tcl1
tcl2
{ces}
these
{avocats, marrons}
totally
?
lawyers?
corrupt {cuits}
finished
{sont}
are
{avocats}
avocadoes
{sont}
are
{cuits}
cooked
{$}
$
{marrons}
brown
{$}
$
# h
b a
r
k
$
tcl f
tcl1
tcl2
these
totally
lawyers
corrupt
finishedare
avocadoes
are cooked $brown
$
Figure 1: On the top: a ?-automaton with two valid paths shown. Each box denotes a state corresponding to a phrase
pair, while states internal to a phrase pair (such as tcl1 and tcl2) are not boxed. Above each transition we have indicated
the corresponding target word, and below it the corresponding set of source tokens. We use a terminal symbol $ to denote
the end of sentence both on the source and on the target. The solid path corresponds to the output these totally corrupt
lawyers are finished, the dotted path to the output these brown avocadoes are cooked. Note that the source tokens are not
necessarily consumed in the order given by the source, and that, for example, there exists a valid path generating these
are totally corrupt lawyers finished and moving according to h ? r ? tcl1 ? tcl2 ? tcl ? f ; Note, however, that
this does not mean that if a biphrase such as (marrons avocats, avocado chestnuts) existed in the phrase
table, it would be applicable to the source sentence here: because the source words in this biphrase would not match the
order of the source tokens in the sentence, the biphrase would not be included in the ?-automaton at all. On the bottom:
The target W-FSA automaton At associated with the ?-automaton, where we are ignoring the source tokens (but keeping
the same weights).
x? a sequence of tokens in x and (x?, y?) an entry
in the global phrase table ? is identified with a
state of the automaton and where the fact that the
phrase-pair (x??, y??) = ((x1, ..., xk), (y1, ..., yl))
follows (x?, y?) in the decoding sequence is mod-
eled by introducing l ?internal? transitions with
labels (?, y1), (?, y2), ..., (?, yl), where ? =
{x1, ..., xk}, and where the first transition con-
nects the state (x?, y?) to some unique ?internal
state? q1, the second transition the state q1 to
some unique internal state q2, and the last tran-
sition qk to the state (x??, y??).7 Thus, a state
(x??, y??) essentially encodes the previous phrase-
pair used during decoding, and it is easy to see
that it is possible to account for the different
weights associated with the phrase-based model
by weights associated to the transitions of the ?-
automaton.8
7For simplicity, we have chosen to collect the set of all
the source tokens {x1, ..., xk} on the first transition, but we
could distribute it on the l transitions arbitrarily (but keep-
ing the subsets disjoint) without changing the semantics of
what we do. This is because once we have entered one of
the l internal transitions, we will always have to traverse
the remaining internal transitions and collect the full set of
source tokens.
8By creating states such as ((x?, y?), (x??, y??)) that en-
Example Let us consider the following French
source sentence x: ces avocats marrons sont
cuits (idiomatic expression for these totally cor-
rupt lawyers are finished). Let?s assume that the
phrase table contains the following phrase pairs:
h: (ces, these)
a: (avocats, avocados)
b: (marrons, brown)
tcl: (avocats marrons,
totally corrupt lawyers)
r: (sont, are)
k: (cuits, cooked)
f: (cuits, finished).
An illustration of the corresponding ?-
automaton SA is shown at the top of Figure 1,
with only a few transitions made explicit, and
with no weights shown.9
code the two previous phrase-pairs used during decoding,
it is possible in principle to account for a trigram language
model, and similarly for higher-order LMs. This is simi-
lar to implementing n-gram language models by automata
whose states encode the n? 1 words previously generated.
9Only two (valid) paths are shown. If we had shown the
full ?-automaton, then the graph would have been ?com-
plete? in the sense that for any two box states B,B?, we
would have shown a connection B ? B?1... ? B?k?1 ?
B?, where the B?i are internal states, and k is the length of
the target side of the biphrase B?.
5
4 Intersecting a synchronous grammar
with a ?-automaton
Intersection of a W-SCFG with a ?-
automaton If SA is a ?-automaton over
input x, with each valid path in SA we asso-
ciate a weight in the same way as we do for
a weighted automaton. For any target word
sequence in V ?t we can then associate the sum
of the weights of all valid paths outputting that
sequence. The weighted language LSA,x over
Vt obtained in this way is called the language
associated with SA. Let G be a W-SCFG over
Vs, Vt, and let us denote by LG,x the weighted
language over Vs, Vt corresponding to the
intersection {x} e G e V ?t , where {x} denotes
the language giving weight 1 to x and weight 0
to other sequences in V ?s , and V ?t denotes the
language giving weight 1 to all sequences in V ?t .
Note that non-null bi-sentences in LG,x have
their source projection equal to x and therefore
LG,x can be identified with a weighted language
over Vt. The intersection of the languages LSA,x
and LG,x is denoted by LSA,x e LG,x.
Example Let us consider the following W-
SCFG (where again, weights are not explicitly
shown, and where we use a terminal symbol $
to denote the end of a sentence, a technicality
needed for making the grammar compatible with
the SA automaton of Figure 1):
S ? NP VP $ / NP VP $
NP ? ces N A / these A N
VP ? sont A / are A
A ? marrons / brown
A ? marrons / totally corrupt
A ? cuits / cooked
A ? cuits / finished
N ? avocats / avocadoes
N ? avocats / lawyers
It is easy to see that, for instance, the sen-
tences: these brown avocadoes are cooked $,
these brown avocadoes are finished $, and these
totally corrupt lawyers are finished $ all belong
to the intersection LSA,x e LG,x, while the sen-
tences these avocadoes brown are cooked $, to-
tally corrupt lawyers are finished these $ belong
only to LSA,x.
Building the intersection We now describe
how to build a W-SCFG that represents the inter-
section LSA,x eLG,x. We base our explanations
on the example just given.
A Relaxation of the Intersection At the
bottom of Figure 1, we show how we can as-
sociate an automaton At with the ?-automaton
SA: we simply ?forget? the source-sides of the
labels carried by the transitions, and retain all the
weights. As before, note that we are only show-
ing a subset of the transitions here.
All valid paths for SAmap into valid paths for
At (with the same weights), but the reverse is not
true because some validAt paths can correspond
to traversals of SA that either consume several
time the same source token or do not consume all
source tokens. For instance, the sentence these
brown avocadoes brown are $ belongs to the
language of At, but cannot be produced by SA.
Let?s however consider the intersection {x} e
G e At, where, with a slight abuse of notation,
we have notated {x} the ?degenerate? automaton
representing the sentence x, namely the automa-
ton (with weights on all transitions equal to 1):
?
ces marronsavocats sont cuits $
0 1 2 3 4 5 6
This is a relaxation of the true intersection, but
one that we can represent through a W-SCFG, as
we know from section 2.10
This being noted, we now move to the con-
struction of the full intersection.
The full intersection We discussed in sec-
tion 2 how to modify a synchronous grammar
rule in order to produce the indexed rule scheme
(2) in order to represent the bilateral intersection
of the grammar with two automata. Let us redo
that construction here, in the case of our example
10Note that, in the case of our very simple example, any
target string that belongs to this relaxed intersection (which
consists of the eight sentences these {brown | totally cor-
rupt} {avocadoes | lawyers} are {cooked | finished}) actu-
ally belongs to the full intersection, as none of these sen-
tences corresponds to a path in SA that violates the token-
consumption constraint. More generally, it may often be
the case in practice that the W-SCFG, by itself, provides
enough ?control? of the possible target sentences to pre-
vent generation of sentences that would violate the token-
consumption constraints, so that there may be little differ-
ence in practice between performing the relaxed intersec-
tion {x} e G e At and performing the full intersection
{x} eG e LSA,x.
6
W-SCFG, of the target automaton represented on
the bottom of Figure 1, and of the source automa-
ton {x}.
The construction is then done in three steps:
s0NPs3 ? s0cess1 s1Ns2 s2As3 /
these A N
t0NPt3 ? ces N A /
t0theset1 t1At2 t2Nt3
t0
s0NPt3s3 ? s0cess1 t2s1Nt3s2 t1s2At2s3 /
t0theset1 t1s2At2s3 t2s1Nt3s2
In order to adapt that construction to the case
where we want the intersection to be with a ?-
automaton, what we need to do is to further spe-
cialize the nonterminals. Rather than specializ-
ing a nonterminal X in the form tsXt?s? , we spe-
cialize it in the form: tsXt
?,?
s? , where ? representsa set of source tokens that correspond to ?collect-
ing? the source tokens in the ?-automaton along
a path connecting the states t and t?.11
We then proceed to define a new rule scheme
associated to our rule, which is obtained as be-
fore in three steps, as follows.
s0NPs3 ? s0cess1 s1Ns2 s2As3 /
these A N
t0NPt3,?03 ? ces N A /
t0theset1,?01 t1At2,?12 t2Nt3,?23
t0
s0NPt3,?03s3 ? s0cess1 t2s1Nt3,?23s2 t1s2At2,?12s3 /
t0theset1,?01 t1s2At2,?12s3 t2s1Nt3,?23s2
The only difference with our previous tech-
nique is in the addition of the ??s to the top in-
dices. Let us focus on the second step of the an-
notation process:
t0NPt3,?03 ? ces N A /
t0theset1,?01 t1At2,?12 t2Nt3,?23
11To avoid a possible confusion, it is important to note
right away that ? is not necessarily related to the tokens
appearing between the positions s and s? in the source sen-
tence (that is, between these states in the associated source
automaton), but is defined solely in terms of the source to-
kens along the t, t? path. See example with ?persons? and
?people? below.
Conceptually, when instanciating this scheme,
the ti?s may range over all possible states of
the ?-automaton, and the ?ij over all subsets of
the source tokens, but under the following con-
straints: the RHS ??s (here ?01, ?12, ?23) must
be disjoint and their union must be equal to the ?
on the LHS (here ?03). Additionally, a ? associ-
ated with a target terminal (as ?01 here) must be
equal to the token set associated to the transition
that this terminal realizes between ?-automaton
states (here, this means that ?01 must be equal
to the token set {ces} associated with the transi-
tion between t0, t1 labelled with ?these?). If we
perform all these instantiations, compute their
weights according to equation (3), and finally re-
move the indices associated with terminals in the
rules (by adding the weights of the rules only dif-
fering by the indices of terminals, as done previ-
ously), we obtain a very large ?raw? grammar,
but one for which one can prove direct coun-
terpart of Fact 1. Let us call, as before G? the
raw W-SCFG obtained, its start symbol being
t#
s#S
t$,?alls$ , with ?all the set of all source tokens
in x.
Fact 2. The synchronous language LG? associ-
ated with G? is equal to ({x}, LSA,x e LG,x).
The grammar that is obtained this way, despite
correctly representing the intersection, contains
a lot of useless rules, this being due to the fact
that many nonterminals can not produce any out-
put. The situation is wholly similar to the case
of section 2, and the same bottom-up techniques
can be used for activating nonterminals and rules
bottom-up.
The algorithm is illustrated in Figure 2, where
we have shown the result of the process of acti-
vating in turn the nonterminals (abbreviated by)
N1, A1, A2, NP1, VP1, S1. As a consequence
of these activations, the original grammar rule
NP ? ces N A /these A N (for instance)
becomes instantiated as the rule:
#
0 NPtcl,{ces,avocats,marrons}3 ?
0ces1 tcl21 Ntcl,?2 h2Atcl2,{avocats,marrons}3 /
#theseh,{ces} h2Atcl2,{avocats,marrons}3 tcl21 Ntcl,?2
7
# h r $
tcl f
tcl1
tcl2
{ces}
these
{avocats, marrons}
totally
?
lawyers?
corrupt {cuits}
finished
{sont}
are
{$}
$
ces marronsavocats sont cuits $
S 1
S 1
N 1 A1 A2
VP1NP1
NP1 VP1
A2
A1
N 1
0 1 2 3 4 5 6
2 ,
1 2
2,{ , }
2 3
# ,{ , , }
0 3
,{ }
4 5
,{ , }
3 5
# ,{ , , , , ,$ $}
0 6
1:
1:
1:
2 :
1:
1:
tcl tcl
h tcl avocats marrons
tcl ces avocats marrons
r f cuits
tcl f sont cuits
ces avocats marrons sont cuits
N N
A A
NP NP
A A
VP VP
S S
?
Figure 2: Building the intersection. The bottom of the figure shows some active non-terminals associated with the source
sequence, at the top these same non-terminals associated with a sequence of transitions in the ?-automaton, corresponding
to the target sequence these totally corrupt lawyers are finished $. To avoid cluttering the drawing, we have used the
abbreviations shown on the right. Note that while A1 only spans marrons in the bottom chart, it is actually decorated with
the source token set {avocats,marrons}: such a ?disconnect? between the views that the W-SCFG and the ?-automaton
have of the source tokens is not ruled out.
that is, after removal of the indices on terminals:
#
0 NPtcl,{ces,avocats,marrons}3 ?
ces tcl21 Ntcl,?2 h2Atcl2,{avocats,marrons}3 /
these h2Atcl2,{avocats,marrons}3 tcl21 Ntcl,?2
Note that while the nonterminal tcl21 Ntcl,?2 by
itself consumes no source token (it is associated
with the empty token set), any actual use of this
nonterminal (in this specific rule or possibly in
some other rule using it) does require travers-
ing the internal node tcl2 and therefore all the
internal nodes ?belonging? to the biphrase tcl
(because otherwise the path from # to $ would
be disconnected); in particular this involves con-
suming all the tokens on the source side of tcl,
including ?avocats?.12
Complexity considerations The bilateral in-
tersection that we defined between a W-SCFG
12In particular there is no risk that a derivation relative
to the intersected grammar generates a target containing
two instances of ?lawyers?, one associated to the expansion
of tcl21 Ntcl,?2 and consuming no source token, and anotherone associated with a different nonterminal and consuming
the source token ?avocats?: this second instance would in-
volve not traversing tcl1, which is impossible as soon as
tcl2
1 Ntcl,?2 is used.
and two W-FSA?s in section 2 can be shown to
be of polynomial complexity in the sense that it
takes polynomial time and space relative to the
sum of the sizes of the two automata and of the
grammar to construct the (reduced) intersected
grammar G?, under the condition that the gram-
mar right-hand sides have length bounded by a
constant.13
The situation here is different, because the
construction of the intersection can in princi-
ple introduce nonterminals indexed not only by
states of the automata, but also by arbitrary sub-
sets of source tokens, and this may lead in ex-
treme cases to an exponential number of rules.
Such problems however can only happen in sit-
uations where, in a nonterminal tsXt
?,?
s? , the set
? is allowed to contain tokens that are ?unre-
lated? to the token set {personnes} appearing
between s and s? in the source automaton. An il-
lustration of such a situation is given by the fol-
lowing example. Suppose that the source sen-
13If this condition is removed, and for the simpler case
where the source (resp. target) automaton encodes a single
sentence x (resp. y), (Satta and Peserico, 2005) have shown
that the problem of deciding whether (x, y) is recognized
by G is NP-hard relative to the sum of the sizes. A conse-
quence is then that the grammar G? cannot be constructed
in polynomial time unless P = NP .
8
tence contains the two tokens personnes and
gens between positions i, i + 1 and j, j + 1 re-
spectively, with i and j far from each other, that
the phrase table contains the two phrase pairs
(personnes, persons) and (gens, people), but
that the synchronous grammar only contains the
two rules X ? personnes/people and Y ?
gens/persons, with these phrases and rules ex-
hausting the possibilities for translating gens
and personnes; Then the intersected grammar
will contain such nonterminals as tiXt
?,{gens}
i+1 and
r
jY
r?,{personnes}
j+1 , where in the first case the token
set {gens} in the first nonterminal is unrelated to
the tokens appearing between i, i + 1, and simi-
larly in the second case.
Without experimentation on real cases, it
is impossible to say whether such phenomena
would empirically lead to combinatorial explo-
sion or whether the synchronous grammar would
sufficiently constrain the phrase-base component
(whose re-ordering capabilities are responsible
in fine for the potential NP-hardness of the trans-
lation process) to avoid it. Another possible ap-
proach is to prevent a priori a possible combi-
natorial explosion by adding formal constraints
to the intersection mechanism. One such con-
straint is the following: disallow introduction of
t
iX
t?,?
j when the symmetric difference between
? and the set of tokens between positions i and
j in the source sentence has cardinality larger
than a small constant. Such a constraint could
be interpreted as keeping the SCFG and phrase-
base components ?in sync?, and would be better
adapted to the spirit of our approach than limit-
ing the amount of re-ordering permitted to the
phrase-based component, which would contra-
dict the reason for using a hierarchical compo-
nent in the first place.
5 Conclusion
Intersecting hierarchical and phrase-based mod-
els of translation could allow to capitalize on
complementarities between the two approaches.
Thus, one might train the hierarchical compo-
nent on corpora represented at the part-of-speech
level (or at a level where lexical units are ab-
stracted into some kind of classes) while the
phrase-based component would focus on transla-
tion of lexical material. The present paper does
not have the ambition to demonstrate that such
an approach would improve translation perfor-
mance, but only to provide some formal means
for advancing towards that goal.
References
Bar-Hillel, Y., M. Perles, and E. Shamir. 1961. On for-
mal properties of simple phrase structure grammars.
Zeitschrift fu?r Phonetik, Sprachwissenschaft und Kom-
municationsforschung, 14:143?172.
Berstel, Jean. 1979. Transductions and Context-Free Lan-
guages. Teubner, Stuttgart.
Blunsom, P. and M. Osborne. 2008. Probabilistic inference
for machine translation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 215?223. Association for Computational
Linguistics. Slides downloaded.
Chiang, David. 2006. An introduction to synchronous
grammars. www.isi.edu/?chiang/papers/
synchtut.pdf, June.
Chiang, David. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Association
for Computer Linguistics.
Kumar, Shankar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation. In
Proc. HLT/EMNLP.
Lang, Bernard. 1994. Recognition can be harder than pars-
ing. Computational Intelligence, 10:486?494.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech processing.
In ECAI-96 Workshop on Extended Finite State Models
of Language.
Mohri, Mehryar. 2002. Generic epsilon-removal and input
epsilon-normalization algorithms for weighted trans-
ducers. International Journal of Foundations of Com-
puter Science, 13:129?143.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Satta, Giorgio and Enoch Peserico. 2005. Some compu-
tational complexity results for synchronous context-free
grammars. In HLT ?05: Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 803?810,
Morristown, NJ, USA. Association for Computational
Linguistics.
Satta, Giorgio. 2008. Translation algorithms by means of
language intersection. Submitted. www.dei.unipd.
it/?satta/publ/paper/inters.pdf.
9
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 250?260,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Productive Generation of Compound Words in Statistical Machine
Translation
Sara Stymne
Linko?ping University
Linko?ping, Sweden
sara.stymne@liu.se
Nicola Cancedda
Xerox Research Centre Europe
Meylan, France
nicola.cancedda@xrce.xerox.com
Abstract
In many languages the use of compound
words is very productive. A common practice
to reduce sparsity consists in splitting com-
pounds in the training data. When this is done,
the system incurs the risk of translating com-
ponents in non-consecutive positions, or in the
wrong order. Furthermore, a post-processing
step of compound merging is required to re-
construct compound words in the output. We
present a method for increasing the chances
that components that should be merged are
translated into contiguous positions and in the
right order. We also propose new heuristic
methods for merging components that outper-
form all known methods, and a learning-based
method that has similar accuracy as the heuris-
tic method, is better at producing novel com-
pounds, and can operate with no background
linguistic resources.
1 Introduction
In many languages including most of the Germanic
(German, Swedish etc.) and Uralic (Finnish, Hun-
garian etc.) language families so-called closed com-
pounds are used productively. Closed compounds
are written as single words without spaces or other
word boundaries, as the Swedish:
gatstenshuggare gata + sten + huggare
paving stone cutter street stone cutter
To cope with the productivity of the phenomenon,
any effective strategy should be able to correctly
process compounds that have never been seen in the
training data as such, although possibly their com-
ponents have, either in isolation or within a different
compound.
The extended use of compounds make them prob-
lematic for machine translation. For translation into
a compounding language, often fewer compounds
than in normal texts are produced. This can be due
to the fact that the desired compounds are missing in
the training data, or that they have not been aligned
correctly. When a compound is the idiomatic word
choice in the translation, a MT system can often
produce separate words, genitive or other alternative
constructions, or translate only one part of the com-
pound.
Most research on compound translation in com-
bination with SMT has been focused on transla-
tion from a compounding language, into a non-
compounding one, typically into English. A com-
mon strategy then consists in splitting compounds
into their components prior to training and transla-
tion.
Only few have investigated translation into a com-
pounding language. For translation into a com-
pounding language, the process becomes:
? Splitting compounds on the target (compound-
ing language) side of the training corpus;
? Learn a translation model from this split train-
ing corpus from source (e.g. English) into
decomposed-target (e.g. decomposed-German)
? At translation time, translate using the learned
model from source into decomposed-target.
? Apply a post-processing ?merge? step to recon-
struct compounds.
The merging step must solve two problems: identify
which words should be merged into compounds, and
choose the correct form of the compound parts.
250
The former problem can become hopelessly diffi-
cult if the translation did not put components nicely
side by side and in the correct order. Preliminary
to merging, then, the problem of promoting transla-
tions where compound elements are correctly posi-
tioned needs to be addressed. We call this promoting
compound coalescence.
2 Related work
The first suggestion of a compound merging method
for MT that we are aware of was described by
Popovic? et al (2006). Each word in the translation
output is looked up in a list of compound parts, and
merged with the next word if it results in a known
compound. This method led to improved overall
translation results from English to German. Stymne
(2008) suggested a merging method based on part-
of-speech matching, in a factored translation system,
where compound parts had a special part-of-speech
tag, and compound parts are only merged with the
next word if the part-of-speech tags match. This re-
sulted in improved translation quality from English
to German, and from English to Swedish (Stymne
and Holmqvist, 2008). Another method, based on
several decoding runs, was investigated by Fraser
(2009).
Stymne (2009a) investigated and compared merg-
ing methods inspired by Popovic? et al (2006),
Stymne (2008) and a method inspired by morphol-
ogy merging (El-Kahlout and Oflazer, 2006; Virpi-
oja et al, 2007), where compound parts were anno-
tated with symbols, and parts with symbols in the
translation output were merged with the next word.
3 Promoting coalescence of compounds
If compounds are split in the training set, then there
is no guarantee that translations of components will
end up in contiguous positions and in the correct or-
der. This is primarily a language model problem,
and we will model it as such by applying POS lan-
guage models on specially designed part-of-speech
sets, and by applying language model inspired count
features.
The approach proposed in Stymne (2008) consists
in running a POS tagger on the target side of the cor-
pus, decompose only tokens with some predefined
POS (e.g. Nouns), and then marking with special
POS-tags whether an element is a head or a modi-
fier. As an example, the German compound ?Fremd-
sprachenkenntnisse?, originally tagged as N(oun),
would be decomposed and re-tagged before training
as:
fremd sprachen kenntnisse
N-Modif N-Modif N
A POS n-gram language model using these extended
tagset, then, naturally steers the decoder towards
translations with good relative placement of these
components
We modify this approach by blurring distinctions
among POS not relevant to the formation of com-
pounds, thus further reducing the tagset to only three
tags:
? N-p ? all parts of a split compound except the
last
? N ? the last part of the compound (its head) and
all other nouns
? X ? all other tokens
The above scheme assumes that only noun com-
pounds are treated but it could easily be extended to
other types of compounds. Alternatively, splitting
can be attempted irrespective of POS on all tokens
longer than a fixed threshold, removing the need of
a POS tagger.
3.1 Sequence models as count features
We expect a POS-based n-gram language model on
our reduced tagset to learn to discourage sequences
unseen in the training data, such as the sequence
of compound parts not followed by a suitable head.
Such a generative LM, however, might also have a
tendency to bias lexical selection towards transla-
tions with fewer compounds, since the correspond-
ing tag sequences might be more common in text.
To compensate for this bias, we experiment with in-
jecting a little dose of a-priori knowledge, and add a
count feature, which explicitly counts the number of
occurrences of POS-sequences which we deem good
and bad in the translation output.
Table 1 gives an overview of the possible bigram
combinations, using the three symbol tagset, plus
sentence beginning and end markers, and their judg-
ment as good, bad or neutral.
251
Combination Judgment
N-p N-p Good
N-p N Good
N-p < \s > Bad
N-p X Bad
all other combinations Neutral
Table 1: Tag combinations in the translation output
We define two new feature functions: one count-
ing the number of occurrences of Good sequences
(the Boost model) and the other counting the occur-
rences of Bad sequences (the Punish model). The
two models can be used either in isolation or com-
bined, with or without a further POS n-gram lan-
guage model.
4 Merging compounds
Once a translation is generated using a system
trained on split compounds, a post-processing step
is required to merge components back into com-
pounds. For all pairs of consecutive tokens we have
to decide whether to combine them or not. Depend-
ing on the language and on preprocessing choices,
we might also have to decide whether to apply any
boundary transformation like e.g. inserting an ?s? be-
tween components.
The method proposed in Popovic? et al (2006)
maintains a list of known compounds and compound
modifiers. For any pair of consecutive tokens, if the
first is in the list of known modifiers and the com-
bination of the two is in the list of compounds, than
the two tokens are merged.
A somewhat orthogonal approach is the one pro-
posed in Stymne (2008): tokens are labeled with
POS-tags; compound modifiers are marked with
special POS-tags based on the POS of the head. If
a word with a modifier POS-tag is followed by ei-
ther another modifier POS-tag of the same type, or
the corresponding head POS-tag, then the two to-
kens are merged.
In the following sections we describe how we
modify and combine these two heuristics, and how
we alternatively formulate the problem as a se-
quence labelling problem suitable for a machine
learning approach.
4.1 Improving and combining heuristics
We empirically verified that the simple heuristics in
Popovic? et al (2006) tends to misfire quite often,
leading to too many compounds. We modify it by
adding an additional check: tokens are merged if
they appear combined in the list of compounds, but
only if their observed frequency as a compound is
larger than their frequency as a bigram. This blocks
the merging of many consecutive words, which just
happen to form a, often unrelated, compound when
merged, such as fo?r sma? (too small) into fo?rsma?
(spurn) in Swedish. Compound and bigram frequen-
cies can be computed on any available monolingual
corpus in the domain of interest.
We furthermore observed that the (improved) list-
based heuristic and the method based on POS pat-
terns lead to complementary sets of false negatives.
We thus propose to combine the two heuristics in
this way: we merge two consecutive tokens if they
would be combined by either the list-based heuris-
tic or the POS-based heuristic. We empirically veri-
fied improved performance when combining heuris-
tics in this way (Section 5.2).
4.2 Compound merging as sequence labelling
Besides extending and combining existing heuris-
tics, we propose a novel formulation of compound
merging as a sequence labelling problem. The oppo-
site problem, compound splitting, has successfully
been cast as a sequence labelling problem before
(Dyer, 2010), but here we apply this formulation in
the opposite direction.
Depending on choices made at compound split-
ting time, this task can be either a binary or mul-
ticlass classification task. If compound parts were
kept as-is, the merging task is a simple concatena-
tion of two words, and each separation point must
receive a binary label encoding whether the two to-
kens should be merged. An option at splitting time
is to normalize compound parts, which often have
a morphological form specific to compounds, to a
canonical form (Stymne, 2009b). In this case the
compound form has to be restored before concate-
nating the parts. This can be modeled as a multi-
class classifier that have the possible boundary trans-
formations as its classes.
Consider for instance translating into German the
252
English sentence:
Europe should promote the knowledge of
foreign languages
Assuming that the training corpus did not con-
tain occurrences of the pair (?knowledge of foreign
languages?,?fremdsprachenkenntnisse?) but con-
tained occurrences of (?knowledge?,?kenntnisse?),
(?foreign?,?fremd?) and (?languages?,?sprachen?),
then the translation model from English into
decomposed-German could be able to produce:
Europa sollte fremd sprachen kenntnisse
fo?rdern
We cast the problem of merging compounds as one
of making a series of correlated binary decisions,
one for each pair of consecutive words, each decid-
ing whether the whitespace between the two words
should be suppressed (label ?1?) or not (label ?0?).
In the case above, the correct labelling for the sen-
tence would be {0,0,1,1,0}, reconstructing the cor-
rect German:
Europa sollte fremdsprachenkenntnisse
fo?rdern1
If conversely, components are normalized upon
splitting, then labels are no longer binary, but come
from a set describing all local orthographic transfor-
mations possible for the language under considera-
tion. In this work we limited our attention to the case
when compounds are not normalized upon splitting,
and labels are hence binary.
While in principle one could address each atomic
merging decision independently, it seems intuitive
that a decision taken at one point should influence
merging decisions in neighboring separation points.
For this reason, instead of a simple (binary or n-
ary) classification problem, we prefer a sequence la-
belling formulation.
The array of sequence labelling algorithms po-
tentially suitable to our problem is fairly broad, in-
cluding Hidden Markov Models (HMMs) (Rabiner,
1989), Conditional Random Fields (CRFs) (Lafferty
et al, 2001), structured perceptrons (Collins, 2002),
1Nouns in German are capitalized. This is normally dealt
as a further ?truecasing? postprocessing, and is an orthogonal
problem from the one we deal with here.
and more. Since the focus of this work is on the
application rather than on a comparison among al-
ternative structured learning approaches, we limited
ourselves to a single implementation. Considering
its good scaling capabilities, appropriateness in pres-
ence of strongly redundant and overlapping features,
and widespread recognition in the NLP community,
we chose to use Conditional Random Fields.
4.2.1 Features
Each sequence item (i.e. each separation point be-
tween words) is represented by means of a sparse
vector of features. We used:
? Surface words: word-1, word+1
? Part-of-speech: POS-1, POS+1
? Character n-grams around the merge point
? 3 character suffix of word-1
? 3 character prefix of word+1
? Combinations crossing the merge points:
1+3, 3+1, 3+3 characters
? Normalized character n-grams around the
merge point, where characters are replaced by
phonetic approximations, and grouped accord-
ing to phonetic distribution, see Figure 1 (only
for Swedish)
? Frequencies from the training corpus, binned
by the following method:
f? =
{
10blog10(f)c if f > 1
f otherwise
for the following items:
? bigram, word-1,word+1
? Compound resulting from merging word-
1,word+1
? Word-1 as a true prefix of words in the cor-
pus
? Word+1 as a true suffix of words in the
corpus
? Frequency comparisons of two different fre-
quencies in the training corpus, classified into
four categories: freq1 = freq2 = 0, freq1 <
freq2, freq1 = freq2, freq1 > freq2
253
# vowels (soft versus hard)
$word = s/[aoua?]/a/g;
$word = s/[eiya?o?e?]/e/g;
# consonant combinations and
# spelling alternations
$word = s/ng/N/g;
$word = s/gn/G/g;
$word = s/ck/K/g;
$word = s/[lhgd]j/J/g;
$word = s/?ge/Je/g;
$word = s/?ske/Se/g;
$word = s/?s[kt]?j/S/g;
$word = s/?s?ch/S/g;
$word = s/?tj/T/g;
$word = s/?ke/Te/g;
#consonants grouping
$word = s/[ptk]/p/g;
$word = s/[bdg]/b/g;
$word = s/[lvw]/l/g;
$word = s/[cqxz]/q/g;
Figure 1: Transformations performed for normalizing
Swedish consonants (Perl notation).
? word-1,word+1 as bigram vs compound
? word-1 as true prefix vs single word
? word+1 as true suffix vs single word
where -1 refers to the word before the merge point,
and +1 to the word after.
We aimed to include features representing the
knowledge available to the list and POS heuristics,
by including part-of-speech tags and frequencies for
compounds and bigrams, as well as a comparison
between them. Features were also inspired by pre-
vious work on compound splitting, based on the in-
tuition that features that are useful for splitting com-
pounds, could also be useful for merging. Charac-
ter n-grams has successfully been used for splitting
Swedish compounds, as the only knowledge source
by Brodda (1979), and as one of several knowl-
edge sources by Sjo?bergh and Kann (2004). Friberg
(2007) tried to normalize letters, beside using the
original letters. While she was not successful, we
still believe in the potential of this feature. Larson et
al. (2000), used frequencies of prefixes and suffixes
from a corpus, as a basis of their method for splitting
German compounds.
4.2.2 Training data for the sequence labeler
Since features are strongly lexicalized, a suitably
large training dataset is required to prevent overfit-
ting, ruling out the possibility of manual labelling.
We created our training data automatically, using
the two heuristics described earlier, plus a third one
enabled by the availability, when estimating parame-
ters for the CRF, of a reference translation: merge if
two tokens are observed combined in the reference
translation (possibly as a sub-sequence of a longer
word). We compared multiple alternative combina-
tions of heuristics on a validation dataset. The val-
idation and test data were created by applying all
heuristics, and then manually check all positive an-
notations.
A first possibility to automatically generate a
training dataset consists in applying the compound
splitting preprocessing of choice to the target side of
the parallel training corpus for the SMT system: sep-
aration points where merges should occur are thus
trivially identified. In practice, however, merging
decisions will need be taken on the noisy output of
the SMT system, and not on the clean training data.
To acquire training data that is similar to the test
data, we could have held out from SMT training a
large fraction of the training data, used the trained
SMT to translate the source side of it, and then la-
bel decision points according to the heuristics. This
would, however, imply making a large fraction of
the data unavailable to training of the SMT. We thus
settled for a compromise: we trained the SMT sys-
tem on the whole training data, translated the whole
source, then labeled decision points according to the
heuristics. The translations we obtain are thus bi-
ased, of higher quality than those we should expect
to obtain on unseen data. Nevertheless they are sub-
stantially more similar to what will be observed in
operations than the reference translations.
5 Experiments
We performed experiments on translation from En-
glish into Swedish and Danish on two different cor-
pora, an automotive corpus collected from a propri-
etary translation memory, and on Europarl (Koehn,
2005) for the merging experiments. We used fac-
tored translation (Koehn and Hoang, 2007), with
both surface words and part-of-speech tags on the
254
EU-Sv Auto-Sv Auto-Da
Corpus Europarl Automotive Automotive
Languages English?Swedish English?Swedish English?Danish
Compounds split N, V, Adj N, V, Adj N
POS tag-sets POS POS,RPOS RPOS
Decoder Moses in-house in-house
Training sentences SMT 1,520,549 329,090 168,047
Training words SMT (target) 34,282,247 3,061,282 1,553,382
Training sentences CRF 248,808 317,398 164,702
Extra training sentences CRF 3,000 3,000 163,201
Table 2: Overview of the experimental settings
target side, with a sequence model on part-of-
speech. We used two decoders, Matrax (Simard et
al., 2005) and Moses (Koehn et al, 2007), both stan-
dard statistical phrase based decoders. For parame-
ter optimization we used minimum error rate train-
ing (Och, 2003) with Moses and gradient ascent on
smoothed NIST for the in-house decoder. In the
merging experiments we used the CRF++ toolkit.2
Compounds were split before training using a
corpus-based method (Koehn and Knight, 2003;
Stymne, 2008). For each word we explored all pos-
sible segmentations into parts that had at least 3
characters, and choose the segmentation which had
the highest arithmetic mean of frequencies for each
part in the training corpus. We constrained the split-
ting based on part-of-speech by only allowing split-
ting options where the compound head had the same
tag as the full word. The split compound parts kept
their form, which can be special to compounds, and
no symbols or other markup were added.
The experiment setup is summarized in Table 2.
The extra training sentences for CRF are sentences
that were not also used to train the SMT system. For
tuning, test and validation data we used 1,000 sen-
tence sets, except for Swedish auto, where we used
2,000 sentences for tuning. In the Swedish experi-
ments we split nouns, adjectives and verbs, and used
the full POS-set, except in the coalescence exper-
iments where we compared the full and restricted
POS-sets. For Danish we only split nouns, and
used the restricted POS-set. For frequency calcu-
lations of compounds and compound parts that were
needed for compound splitting and some of the com-
2Available at http://crfpp.sourceforge.net/
pound merging strategies, we used the respective
training data in all cases. Significance testing was
performed using approximate randomization (Rie-
zler and Maxwell, 2005), with 10,000 iterations, and
? < 0.05.
5.1 Experiments: Promoting compound
coalescence
We performed experiments with factored translation
models with the restricted part-of-speech set on the
Danish and Swedish automotive corpus. In these ex-
periments we compared the restricted part-of-speech
set we suggest in this work to several baseline sys-
tems without any compound processing and with
factored models using the extended part-of-speech
set suggested by Stymne (2008). Compound parts
were merged using the POS-based heuristic. Results
are reported on two standard metrics, NIST (Dod-
dington, 2002) and Bleu (Papineni et al, 2002), on
lower-cased data. For all sequence models we use
3-grams.
Results on the two Automotive corpora are sum-
marized in Table 3. The scores are very high, which
is due to the fact that it is an easy domain with many
repetitive sentence types. On the Danish dataset,
we observe significant improvements in BLEU and
NIST over the baseline for all methods where com-
pounds were split before translation and merged af-
terwards. Some of the gain is already obtained us-
ing a language model on the extended part-of-speech
set. Additional gains can however be obtained us-
ing instead a language model on a reduced set of
POS-tags (RPOS), and with a count feature explic-
itly boosting desirable RPOS sequences. The count
feature on undesirable sequences did not bring any
255
improvements over any of the systems with com-
pound splitting.
Results on the Swedish automotive corpus are less
clear-cut than for Danish, with mostly insignificant
differences between systems. The system with de-
composition and a restricted part-of-speech model
is significantly better on Bleu than all other systems,
except the system with decomposition and a stan-
dard part-of-speech model. Not splitting actually
gives the highest NIST score, even though the dif-
ference to the other systems is not significant, ex-
cept for the system with a combination of a trained
RPOS model and a boost model, which also has sig-
nificantly lower Bleu score than the other systems
with compound splitting.
5.2 Experiments: Compound merging
We compared alternative combinations of heuristics
on our three validation datasets, see Figure 2. In
order to estimate the amount of false negatives for
all three heuristics, we inspected the first 100 sen-
tences of each validation set, looking for words that
should be merged, but were not marked by any of
the heuristics. In no case we could find any such
words, so we thus assume that between them, the
heuristics can find the overwhelming majority of all
compounds to be merged.
We conducted a round of preliminary experiments
to identify the best combination of the heuristics
available at training time (modified list-based, POS-
based, and reference-based) to use to create auto-
matically the training data for the CRF. Best results
on the validation data are obtained by different com-
bination of heuristics for the three datasets, as could
be expected by the different distribution of errors
in Figure 2. In the experiments below we trained
the CRF using for each dataset the combination of
heuristics corresponding to leaving out the grey por-
tions of the Venn diagrams. This sort of prelimi-
nary optimization requires hand-labelling a certain
amount of data. Based on our experiments, skipping
this optimization and just using ref?(list?POS) (the
optimal configuration for the Swedish-English Eu-
roparl corpus) seems to be a reasonable alternative.
The validation data was also used to set a fre-
quency cut-off for feature occurrences (set at 3 in
the following experiments) and to tune the regu-
larization parameter in the CRF objective function.
448OK
1212/0
0-
154150/4
1411/3
0-154/11
list
POS ref
Automotive, Swedish
48OK1
8282/0
OO/0
-4OK1
5880/88
-4/83088/8O
list
PKS ref
Europarl, Swedish
488OK
8812
/0/012
-0-012
53/154
//12l3l213
istP
SOr ef?
Automotive, Danish
Figure 2: Evaluation of the different heuristics on valida-
tion files from the three corpora. The number in each re-
gion of the Venn diagrams indicates the number of times
a certain combination of heuristics fired (i.e. the num-
ber of positives for that combination). The two smaller
numbers below indicate the number of true and false pos-
itive, respectively. Venn diagram regions corresponding
to unreliable combinations of heuristics have correspond-
ing figures on a grey background. OK means that a large
fraction of the Venn cell was inspected, and no error was
found.
256
Danish auto Swedish auto
BLEU NIST BLEU NIST
No compound
splitting
Base 70.91 8.8816
Base+POSLM 72.08 8.9338 56.79 9.2674
With
compound
splitting
POSLM 74.11* 9.2341* 57.28 9.1717
RPOSLM 74.26* 9.2767* 58.12* 9.1694
punish model 73.34* 9.1543*
boost model 74.96** 9.3028** 57.31 9.1736
RPOSLM + boost 74.76** 9.3368** 55.82 9.1088
Table 3: Results of experiments with methods for promoting coalescence. Compounds are merged based on the POS
heuristic. Scores that are significantly better than Base+POSLM, are marked ?*?, and scores that are also better than
POSLM with ?**?.
Results are largely insensitive to variations in these
hyper-parameters, especially to the CRF regulariza-
tion parameter.
For the Danish auto corpus we had access to train-
ing data that were not also used to train the SMT
system, that we used to compare the performance
with that on the possibly biased training data that
was also used to train the SMT system. There were
no significant differences between the two types of
training data on validation data, which confirmed
that reusing the SMT training data for CRF training
was a reasonable strategy.
The overall merging results of the heuristics, the
best sequence labeler, and the sequence labeler with-
out POS are shown in Table 4. Notice how the (mod-
ified) list and POS heuristics have complementary
sets of false negatives: when merging on the OR of
the two heuristics, the number of false negatives de-
creases drastically, in general compensating for the
inevitable increase in false positives.
Among the heuristics, the combination of the im-
proved list heuristic and the POS-based heuristic has
a significantly higher recall and F-score than the
POS-based heuristic alone in all cases except on the
validation data for Swedish Auto, and than the list-
based strategy in several cases. The list heuristic
alone performs reasonably well on the two Swedish
data sets, but has a very low recall on the Danish
dataset. In all three cases the SMT training data
has been used for the list used by the heuristic, so
this is unexpected, especially considering the fact
that the Danish dataset is in the same domain as
one of the Swedish datasets. The Danish training
data is smaller than the Swedish data though, which
might be an influencing factor. It is possible that this
heuristic could perform better also for Danish given
more data for frequency calculations.
The sequence labeler is competitive with the
heuristics; on F-score it is only significantly worse
than any of the heuristics once, for Danish auto test
data, and in several cases it has a significantly higher
F-score than some of the heuristics. The sequence
labeler has a higher precision, significantly so in
three cases, than the best heuristic, the combina-
tion heuristic, which is positive, since erroneously
merged compounds are usually more disturbing for
a reader or post-editor than non-merged compounds.
The sequence-labelling approach can be used also
in the absence of a POS tagger, which can be impor-
tant if no such tool of suitable quality is available
for the target language and the domain of interest.
We thus also trained a CRF-based compound merger
without using POS features, and without using the
POS-based heuristic when constructing the training
data. Compared to the CRF with access to POS-tags,
on validation data F-score is significantly worse on
the Europarl Swedish condition and the Automotive
Danish condition, and are unchanged on Automo-
tive Swedish. On test data there are no significant
differences of the two sequence labelers on the two
Automotive corpora. On Swedish Europarl, the CRF
without POS has a higher recall at the cost of a lower
precision. Compared to the list heuristic, which is
the only other alternative strategy that works in the
absence of a POS tagger, the CRF without POS per-
forms significantly better on recall and F-score for
Danish automotive, and mostly comparative on the
two Swedish corpora.
257
Validation data Test data
Precision Recall F-score Precision Recall F-score
Swedish auto
list .9889p,lp .9936p .9912p .9900 .9770 .9835
POS .9757 .9632 .9694 .9916lp .9737 .9826
list?POS .9720 1p .9858p .9822 .9984l,p,c,cp .9902l,p,cp
CRF (ref?list) .9873p,lp .9984p .9928p,lp .9869 .9869 .9869
CRF without POS .9873p,lp .9968p .9920p,lp .9836 .9852 .9844
Swedish Europarl
list .9923lp,c,cp .9819 .9871 .9882lp,cp .9849 .9865
POS .9867lp .9785 .9825 .9893lp .9751 .9822
list?POS .9795 .9958l,p,c,cp .9876p,cp .9782 .9993l,p,c,cp .9886p,cp
CRF (ref?(list?POS)) .9841cp .9916l,p .9879p,cp .9953l,p,lp,cp .9790 .9871p
CRF without POS .9780 .9882p .9831 .9805 .9882p,c .9843
Danish auto
list .9250 .7603 .8346 .9905lp .7640 .8626
POS .9814l,lp .9635l,cp .9724l,lp,cp .9779 .9294l .9538l
list?POS .9251 .9863l,p,cp .9547l .9760 .9878l,p,c .9819l,p,c
CRF (ref?list?POS) .9775l,lp .9932l,p,cp .9853l,p,lp,cp .9778 .9659l,p .9718l,p
CRF without POS .9924l,lp,c .8973l .9424l .9826 .9635l,p .9729l,p
Table 4: Precision, Recall, and F-score for compound merging methods based on heuristics or sequence labelling on
validation data and on held-out test data. The superscripts marks the systems that are significantly worse than the
system in question (l-list, p-POS, lp-list?POS, c-best CRF configuration, cp-CRF without POS).
The sequence labeler has the advantage over
the heuristics that it is able to merge completely
novel compounds, whereas the list strategy can
only merge compounds that it has seen, and the
POS-based strategy can create novel compounds,
but only with known modifiers. An inspection of
the test data showed that there were a few novel
compounds merged by the sequence labeler that
were not identified with either of the heuristics. In
the test data we found knap+start (button start)
and vand+neds?nkning (water submersion) in Dan-
ish Auto, and kvarts sekel (quarter century) and
bostad(s)+ersa?ttning (housing grant) in Swedish
Europarl. This confirms that the sequence labeler,
from automatically labeled data based on heuristics,
can learn to merge new compounds that the heuris-
tics themselves cannot find.
6 Discussion and conclusions
In this article, we described several methods for
promoting coalescence and deciding if and how to
merge word compounds that are either competitive
with, or superior to, any currently known method.
For promoting compound coalescence we exper-
imented with introducing additional LMs based on
a restricted set of POS-tags, and with dedicated
SMT model features counting the number of se-
quences known a priori to be desirable and unde-
sirable. Experiments showed that this method can
lead to large improvements over systems using no
compound processing, and over previously known
compound processing methods.
For merging, we improved an existing list-based
heuristic, consisting in checking whether the first of
two consecutive words has been observed in a cor-
pus as a compound modifier and their combination
has been observed as a compound, introducing the
additional constraint that words are merged only if
their corpus frequency as a compound is larger than
their frequency as a bigram.
We observed that the false negatives of this im-
proved list-based heuristic and of another, known,
heuristic based on part-of-speech tags were comple-
mentary, and proposed a logical OR of them that
generally improves over both.
We furthermore cast the compound merging prob-
258
lem as a sequence labelling problem, opening it to
solutions based on a broad array of models and al-
gorithms. We experimented with one model, Condi-
tional Random Fields, designed a set of easily com-
puted features reaching beyond the information ac-
cessed by the heuristics, and showed that it gives
very competitive results.
Depending on the choice of the features, the se-
quence labelling approach has the potential to be
truly productive, i.e. to form new compounds in
an unrestricted way. This is for instance the case
with the feature set we experimented with. The list-
based heuristic is not productive: it can only form
a compound if this was already observed as such.
The POS-based heuristic presents some limited pro-
ductivity. Since it uses special POS-tags for com-
pound modifiers, it can form a compound provided
its head has been seen alone or as a head, and its
modifier(s) have been seen elsewhere, possibly sep-
arately, as modifier(s) of compounds. The sequence
labelling approach can decide to merge two consec-
utive words even if neither was ever seen before in a
compound.
In this paper we presented results on Swedish and
Danish. We believe that the methods would work
well also for other compounding languages such as
German and Finnish. If the linguistic resources re-
quired to extract some of the features, e.g. a POS
tagger, are unavailable (or are available only at train-
ing time but not in operations) for some language,
the sequence-labelling method can still be applied. It
is competitive or better than the list heuristic, which
is the only heuristic available in that scenario.
Experiments on three datasets show that the im-
proved and combined heuristics perform generally
better than any already known method, and that, be-
sides being fully productive, the sequence-labelling
version is highly competitive, tends to generate
fewer false positives than the combination heuristic,
and can be used flexibly with limited or no linguistic
resources.
References
Benny Brodda. 1979. Na?got om de svenska ordens fono-
tax och morfotax: Iakttagelse med utga?ngspunkt fra?n
experiment med automatisk morfologisk analys. In
PILUS nr 38. Inst. fo?r lingvistik, Stockholms univer-
sitet, Sweden.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of the Second International
Conference on Human Language Technology, pages
228?231, San Diego, California, USA.
Chris Dyer. 2010. A Formal Model of Ambiguity and
its Applications in Machine Translation. Ph.D. thesis,
University of Maryland, USA.
I?lknur Durgar El-Kahlout and Kemal Oflazer. 2006. Ini-
tial explorations in English to Turkish statistical ma-
chine translation. In Proceedings of the Workshop
on Statistical Machine Translation, pages 7?14, New
York City, New York, USA.
Alexander Fraser. 2009. Experiments in morphosyntac-
tic processing for translating to and from German. In
Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 115?119, Athens, Greece.
Karin Friberg. 2007. Decomposing Swedish compounds
using memory-based learning. In Proceedings of the
16th Nordic Conference on Computational Linguistics
(Nodalida?07), pages 224?230, Tartu, Estonia.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 868?876, Prague, Czech Republic.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the 10th
Conference of the EACL, pages 187?193, Budapest,
Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL, demon-
stration session, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit X, pages 79?86, Phuket, Thailand.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the Eighteenth International Conference
on Machine Learning, Williamstown, MA.
259
Martha Larson, Daniel Willett, Joachim Ko?hler, and Ger-
hard Rigoll. 2000. Compound splitting and lexi-
cal unit recombination for improved performance of
a speech recognition system for German parliamen-
tary speeches. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing,
volume 3, pages 945?948, Beijing, China, October.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the ACL, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of German compound
words. In Proceedings of FinTAL ? 5th International
Conference on Natural Language Processing, pages
616?624, Turku, Finland. Springer Verlag, LNCS.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of IEEE, 77(2):257?286.
Stefan Riezler and John T. Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the Workshop on Intrin-
sic and Extrinsic Evaluation Measures for MT and/or
Summarization at ACL?05, pages 57?64, Ann Arbor,
Michigan, USA.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-
mada, Philippe Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases. In Proceed-
ings of the Human Language Technology Conference
and the conference on Empirical Methods in Natu-
ral Language Processing, pages 755?762, Vancouver,
British Columbia, Canada.
Jonas Sjo?bergh and Viggo Kann. 2004. Finding the cor-
rect interpretation of Swedish compounds, a statisti-
cal approach. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04), Lisbon, Portugal.
Sara Stymne and Maria Holmqvist. 2008. Processing of
Swedish compounds for phrase-based statistical ma-
chine translation. In Proceedings of the 12th Annual
Conference of the European Association for Machine
Translation, pages 180?189, Hamburg, Germany.
Sara Stymne. 2008. German compounds in factored sta-
tistical machine translation. In Proceedings of Go-
TAL ? 6th International Conference on Natural Lan-
guage Processing, pages 464?475, Gothenburg, Swe-
den. Springer Verlag, LNCS/LNAI.
Sara Stymne. 2009a. A comparison of merging strategies
for translation of German compounds. In Proceedings
of the EACL 2009 Student Research Workshop, pages
61?69, Athens, Greece.
Sara Stymne. 2009b. Compound processing for phrase-
based statistical machine translation. Licentiate the-
sis, Linko?ping University, Sweden.
Sami Virpioja, Jaako J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statis-
tical machine translation based on morphs induced in
an unsupervised manner. In Proceedings of MT Sum-
mit XI, pages 491?498, Copenhagen, Denmark.
260
