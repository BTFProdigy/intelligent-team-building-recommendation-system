A Structured Prediction Approach for Statistical Machine Translation
Dakun Zhang*          Le Sun?          Wenbo Li* 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{dakun04,liwenbo02}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
We propose a new formally syntax-based 
method for statistical machine translation. 
Transductions between parsing trees are 
transformed into a problem of sequence 
tagging, which is then tackled by a search-
based structured prediction method. This 
allows us to automatically acquire transla-
tion knowledge from a parallel corpus 
without the need of complex linguistic 
parsing. This method can achieve compa-
rable results with phrase-based method 
(like Pharaoh), however, only about ten 
percent number of translation table is used. 
Experiments show that the structured pre-
diction approach for SMT is promising for 
its strong ability at combining words. 
1 Introduction 
Statistical Machine Translation (SMT) is attract-
ing more attentions than rule-based and example-
based methods because of the availability of large 
training corpora and automatic techniques. How-
ever, rich language structure is difficult to be inte-
grated in the current SMT framework. Most of the 
SMT approaches integrating syntactic structures 
are based on probabilistic tree transducers (tree-
to-tree model). This leads to a large increase in the 
model complexity (Yamada and Knight 2001; 
Yamada and Knight 2002; Gildea 2003; Galley et 
al. 2004; Knight and Graehl 2005; Liu et al 2006). 
However, formally syntax-based methods propose 
simple but efficient ways to parse and translate 
sentences (Wu 1997; Chiang 2005). 
In this paper, we propose a new model of SMT 
by using structured prediction to perform tree-to-
tree transductions. This model is inspired by Sa-
gae and Lavie (2005), in which a stack-based rep-
resentation of monolingual parsing trees is used. 
Our contributions lie in the extension of this rep-
resentation to bilingual parsing trees based on 
ITGs and in the use of a structured prediction 
method, called SEARN (Daum? III et al 2007), to 
predict parsing structures. 
Furthermore, in order to facilitate the use of 
structured prediction method, we perform another 
transformation from ITG-like trees to label se-
quence with the grouping of stack operations. 
Then the structure preserving problem in transla-
tion is transferred to a structured prediction one 
tackled by sequence labeling method such as in 
Part-of-Speech (POS) tagging. This transforma-
tion can be performed automatically without com-
plex linguistic information. At last, a modified 
search process integrating structure information is 
performed to produce sentence translation. Figure 
1 illustrates the process flow of our model. Be-
sides, the phrase extraction is constrained by ITGs. 
Therefore, in this model, most units are word 
based except that we regard those complex word 
alignments as a whole (i.e. phrase) for the simplic-
ity of ITG-like tree representations. 
B ilingual S en tences
G IZ A + +  T ra in ing
(B id irec tiona l)
W ord  A lignm en ts
(g row -d iag -fina l)
S truc tu red  Info rm ation
(T ra in ing  by  S E A R N )
L anguage  M odel
M ono lingual
S en tences
Search  e*
M ax im ize  P r(e)*P r(f|e )
Inpu t
Source L anguage
S en tence
O utpu t
T arge t L anguage
 S en tence
S tack -based  O pera tions
T rans la tion  M odel
IT G -like  T rees
 
Figure 1: Chart of model framework 
The paper is organized as follows: related work 
is show in section 2. The details of the transforma-
649
tion from word alignments to structured parsing 
trees and then to label sequence are given in sec-
tion 3. The structured prediction method is de-
scribed in section 4. In section 5, a beam search 
decoder with structured information is described. 
Experiments are given for three European lan-
guage pairs in section 6 and we conclude our pa-
per with some discussions. 
2 Related Work 
This method is similar to block-orientation model-
ing (Tillmann and Zhang 2005) and maximum 
entropy based phrase reordering model (Xiong et 
al. 2006), in which local orientations (left/right) of 
phrase pairs (blocks) are learned via MaxEnt clas-
sifiers. However, we assign shift/reduce labeling 
of ITGs taken from the shift-reduce parsing, and 
classifier is learned via SEARN. This paper is 
more elaborated by assigning detailed stack-
operations. 
The use of structured prediction to SMT is also 
investigated by (Liang et al 2006; Tillmann and 
Zhang 2006; Watanabe et al 2007). In contrast, 
we use SEARN to estimate one bilingual parsing 
tree for each sentence pair from its word corre-
spondences. As a consequence, the generation of 
target language sentences is assisted by this struc-
tured information. 
Turian et al (2006) propose a purely discrimi-
native learning method for parsing and translation 
with tree structured models. The word alignments 
and English parse tree were fed into the GenPar 
system (Burbank et al 2005) to produce binarized 
tree alignments. In our method, we predict tree 
structures from word alignments through several 
transformations without involving parser and/or 
tree alignments. 
3 Transformation 
3.1 Word Alignments and ITG-like Tree 
First, following Koehn et al (2003), bilingual sen-
tences are trained by GIZA++ (Och and Ney 2003) 
in two directions (from source to target and target 
to source). Then, two resulting alignments are re-
combined to form a whole according to heuristic 
rules, e.g. grow-diag-final. Second, based on the 
word alignment matrix, one unique parsing tree 
can be generated according to ITG constraints 
where the ?left-first? constraint is posed. That is to 
say, we always make the leaf nodes as the right 
sons as possible as they can. Here we present two 
basic operations for mapping tree items, one is in 
order and the other is in reverse order (see Figure 
2). Basic word alignments are in (a), while (b) is 
their corresponding alignment matrix. They can be 
described using ITG-like trees (c). 
f1 f1       f2
e1        *
e2                  * f1/e1 f2/e2
(1a) (1b) (1c)
f1       f2
e1                  *
e2        * f1/e2 f2/e1
(2a) (2b) (2c)
f1/e1 S
f2/e2 S,R+
(1d)
f1/e2 S
f2/e1 S,R-
(2d)
f2
f1 f2
e1 e2
e1 e2
 
Figure 2: Two basic representations for tree items 
 
Figure 3: ?inside-out? transpositions (a) and (b) with two 
typical complex sequences (c) and (d). In (c) and (d), word 
correspondence f2-e2 is also extracted as sub-alignments. 
The two widely known situations that cannot be 
described by ITGs are called ?inside-out? transpo-
sitions (Figure 3 a & b). Since they cannot be de-
composed in ITGs, we consider them as basic 
units. In this case, phrase alignment is used. In our 
model, more complex situations exist for the word 
correspondences are generated automatically from 
GIZA++. At the same time, we also keep the sub-
alignments in those complex situations in order to 
extend the coverage of translation options. The 
sub-alignments are restricted to those that can be 
described by the two basic operations. In other 
words, for our ITG-like tree, the nodes are mostly 
word pairs, except some indecomposable word 
sequences pairs. Figure 3 shows four typical com-
plex sequences viewed as phrases. 
Therefore, our ITG-like trees take some phrase 
alignments into consideration and we also keep 
the sub-alignments in these situations. Tree items 
in our model are restricted to minimum constitu-
ents for the simplicity of parsing tree generation. 
Then we extract those word pairs from tree items, 
instead of all the possible word sequences, as our 
translation table. In this way, we can greatly re-
duce the number of translation pairs to be consid-
eration. 
650
3.2 SHIFT and REDUCE Operations 
Sagae and Lavie (2005) propose a constituency-
based parsing method to determine sentence de-
pendency structures. This method is simple and 
efficient, which makes use of SHIFT and RE-
DUCE operations within a stack framework. This 
kind of representations can be easily learned by a 
classifier with linear time complexity. 
In their method, they build a parse tree of a sen-
tence one word at a time just as in a stack parser. 
At any time step, they either shift a new word on 
to the stack, or reduce the top two elements on the 
stack into a new non-terminal. 
Sagae and Lavie?s algorithms are designed for 
monolingual parsing problem. We extend it to 
represent our ITG-like tree. In our problem, each 
word pairs can be viewed as tree items (nodes). 
To handle our tree alignment problem, we need to 
define two REDUCE operations: REDUCE in 
order and REDUCE in reverse order. We define 
these three basic operations as follows: 
? S: SHIFT - push the current item onto the 
stack. 
? R+: REDUCE in order - pop the first two 
items from the stack, and combine them in 
the original order on the target side, then 
push back. 
? R-: REDUCE in reverse order - pop the 
first two items from the stack, and combine 
them in the reverse order on the target side, 
then push back. 
Using these operators, our ITG-like tree is 
transformed to serial stack operations. In Figure 2, 
(d) is such a representation for the two basic 
alignments. Therefore, the structure of word 
aligned sentences can be transformed to an opera-
tion sequence, which represents the bilingual pars-
ing correspondences. 
After that, we attach these operations to each 
corresponding tree item like a sequence labeling 
problem. We need to perform another ?grouping? 
step to make sure only one operation is assigned 
to each item, such as ?S,R+?, ?S,R-,R+?, etc. 
Then, those grouped operations are regarded as a 
whole and performed as one label. The number of 
this kind of labels is decided by the training cor-
pus1. Having defined such labels, the prediction of 
                                                 
1 This set of labels is quite small and only 16 for the French-
English training set with 688,031 sentences. 
tree structures is transformed to a label prediction 
one. That is, giving word pairs as input, we trans-
form them to their corresponding labels (stack 
operations) in the output. At the same time, tree 
transductions are encoded in those labels. Once all 
the ?labels? are performed, there should be only 
one element in the stack, i.e. the generating sen-
tence translation pairs. See Appendix A for a more 
complete example in Chinese-English with our 
defined operations. 
Another constraint we impose is to keep the 
least number of elements in stack at any time. If 
two elements on the top of the stack can be com-
bined, we combine them to form a single item. 
This constraint can avoid having too many possi-
ble operations for the last word pair, which may 
make future predictions difficult. 
4 Structured Prediction 
SEARN is a machine learning method proposed 
recently by Daum? III et al (2007) to solve struc-
tured prediction problems. It can produce a high 
prediction performance without compromising 
speed, simplicity and generality. By incorporating 
the search and learning process, SEARN can solve 
the complex problems without having to perform 
explicit decoding any more. 
In most cases, a prediction of input x in domain 
X into output y in domain Y, like SVM and deci-
sion trees, cannot keep the structure information 
during prediction. SEARN considers this problem 
as a cost sensitive classification one. By defining 
features and a loss function, it performs a cost 
sensitive learning algorithm to learn predictions. 
During each iteration, the optimal policy (decided 
by previous classifiers) generates new training 
examples through the search space. These data are 
used to adjust performance for next classifier. 
Then, iterations can keep this algorithm to per-
form better for prediction tasks. Structures are 
preserved for it integrates searching and learning 
at the same time.  
4.1 Parsing Tree Prediction 
For our problem, using SEARN to predict the 
stack-based ITG-like trees, given word alignments 
as input, can benefit from the advantages of this 
algorithm. With the structured learning method, 
we can account for the sentence structures and 
their correspondence between two languages at 
651
the same time. Moreover, it keeps the translating 
structures from source to target. 
As we have transformed the tree-to-tree transla-
tion problem into a sequence labeling one, all we 
need to solve is a tagging problem similar to a 
POS tagging (Daum? III et al 2006). The input 
sequence x is word pairs and output y is the group 
of SHIFT and REDUCE operations. For sequence 
labeling problem, the standard loss function is 
Hamming distance, which measures the difference 
between the true output and the predicting one: 
?=
t
tt yyyyHL )?,()?,( ?                 (1) 
where ? is 0 if two variables are equal, and 1 oth-
erwise. 
5 Decoder 
We use a left-to-right beam search decoder to find 
the best translation given a source sentence. Com-
pared with general phrase-based beam search de-
coder like Pharaoh (Koehn 2004), this decoder 
integrates structured information and does not 
need distortion cost and other costs (e.g. future 
costs) any more. Therefore, the best translation 
can be determined by: 
})()|({maxarg* )(elengthlm
e
epefpe ?=     (2) 
where ? is a factor of word length penalty. Simi-
larly, the translation probability  can be 
further decomposed into: 
)|( efp
?=
i
ii efefp )|()|( ?                  (3) 
and )|( ii ef?  represents the probability distribu-
tion of word pairs. 
Instead of extracting all possible phrases from 
word alignments, we consider those translation 
pairs from the nodes of ITG-like trees only. Like 
Pharaoh, we calculate their probability as a com-
bination of 5 constituents: phrase translation prob-
ability (in both directions), lexical translation 
probability (in both directions) and phrase penalty 
(default is set at 2.718). The corresponding weight 
is trained through minimum error rate method 
(Och 2003). Parameters of this part can be calcu-
lated in advance once tree structures are generated 
and can be stored as phrase translation table. 
5.1 Core Algorithm 
Another important question is how to preserve 
sentence structures during decoding. A left-to-
right monotonous search procedure is needed. 
Giving the source sentence, word translation can-
didates can be determined according to the trans-
lation table. Then, several rich features like cur-
rent and previous source words are extracted 
based on these translation pairs and source sen-
tence. After that, our structured prediction learn-
ing method will be used to predict the output ?la-
bels?, which produces a bilingual parsing tree. 
Then, a target output will be generated for the cur-
rent partial source sentence as soon as bilingual 
parsing trees are formed. The output of this part 
therefore contains syntactic information for struc-
ture. 
For instance, given the current source partial 
like ?f1 f2?, we can generate their translation 
word pair sequences with the translation table, 
like ?f1/e1 f2/e2?, ?f1/e3 f2/e4? and so on. The 
corresponding features are then able to be decided 
for the next predicting process. Once the output 
predictions (i.e. stack operations) are decided, the 
bilingual tree structures are formed at the same 
time. As a consequence, results of these opera-
tions are the final translations which we really 
need. 
At each stage of translation, language model 
parameters can be added to adjust the total costs 
of translation candidates and make the pruning 
process reasonable. The whole sentence is then 
processed by incrementally constructing the trans-
lation hypotheses. Lastly, the element in the last 
beam with the minimum cost is the final transla-
tion. In general, the translation process can be de-
scribed in the following way: 
 
5.2 Recombining and Pruning 
Different translation options can combine to form 
the same fragment by beam search decoder. Re-
combining is therefore needed here to reduce the 
search space. So, only the one with the lowest cost 
is kept when several fragments are identical. This 
recombination is a risk-free operation to improve 
searching efficiency. 
Another pruning method used in our system is 
histogram pruning. Only n-best translations are 
652
allowed for the same source part in each stack (e.g. 
n=100). In contrast with traditional beam search 
decoder, we generate our translation candidates 
from the same input, instead of all allowed word 
pairs elsewhere. Therefore the pruning is much 
more reasonable for each beam. There is no rela-
tive threshold cut off compared with Pharaoh. 
In the end, the complexities for decoding are 
the main concern of our method. In practice, how-
ever, it will not exceed the  (m for 
sentence length, N for stack size and Tn for al-
lowed translation candidates). This is based on the 
assumption that our prediction process (tackled by 
SEARN) is fed with three features (only one for-
mer item is associated), which makes it no need of 
full sentence predictions at each time. 
)**( TnNmO
6 Experiment 
We validate our method using the corpus from the 
shared task on NAACL 2006 workshop for statis-
tical machine translation2. The difference of our 
method lies in the framework and different phrase 
translation table. Experiments are carried on all 
the three language pairs (French-English, Ger-
man-English and Spanish-English) and perform-
ances are evaluated by the providing test sets. Sys-
tem parameters are adjusted with development 
data under minimum error rate training. 
For SEARN, three features are chosen to use: 
the current source word, the word before it and the 
current target word. As we do not know the real 
target word order before decoding, the corre-
sponding target word?s position cannot be used as 
features. Besides, we filter the features less than 5 
times to reduce the training complexities. 
The classifier we used in the training process is 
based on perceptron because of its simplicity and 
performance. We modified Daum? III?s script3 to 
fit our method and use the default 5 iterations for 
each perceptron-based training and 3 itertaions for 
SEARN. 
6.1 Results for different language pairs 
The  final  results  of  our  system,  named Amasis, 
and baseline system Pharaoh (Koehn and Monz 
2006) for three language pairs are listed in Table 1. 
The last three lines are the results of Pharaoh with 
phrase length from 1 to 3. However, the length of 
                                                 
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.cs.utah.edu/~hal/searn/SimpleSearn.tgz 
0
5000
10000
15000
20000
k
Pharaoh 15724573 12667210 19367713
Amasis 1522468 1715732 1572069
F-E G-E S-E
 
Figure 4: Numbers of translation table 
0.0%
5.0%
10.0%
15.0%
20.0%
25.0%
30.0%
35.0%
40.0%
Pharaoh 3.7% 5.1% 3.5%
Amasis 32.2% 33.0% 36.4%
F-E G-E S-E
 
Figure 5: Percent of single word translation pairs (only one 
word in the source side) 
F-E G-E S-E  
In Out In Out In Out
Amasis 27.44 18.41 23.02 15.97 27.51 23.35
Pharaoh1 20.54 14.07 17.53 12.13 23.23 20.24
Pharaoh2 27.71 19.41 23.36 15.77 28.88 25.28
Pharaoh3 30.01 20.77 24.40 16.58 30.58 26.51
Table 1: BLEU scores for different language pairs. In - In-
domain test, Out - Out-of-domain test. 
 
phrases for Amasis is determined by ITG-like tree 
nodes and there is no restriction for it. 
Even without producing higher BLEU scores 
than Pharaoh, our approach is still interesting for 
the following reasons. First, the number of phrase 
translation pairs is greatly reduced in our system. 
The ratio of translation table number in our 
method (Amasis) to Pharaoh, for French-English 
is 9.68%, for German-English is 13.54%, for 
Spanish-English is 8.12% (Figure 4). This means 
that our method is more efficient at combining 
words and phrases during translation. The reasons 
for the different ratio for the three languages are 
not very clear, maybe are related to the flexibility 
of word order of source language. Second, we 
count the single word translation pairs (only one 
word in the source side) as shown in Figure 5. 
There are significantly more single word transla-
tions in our method. However, the translation 
quality can be kept at the same level under this 
circumstance. Third, our current experimental re-
sults are produced with only three common fea-
tures (the corresponding current source and target 
word and the last source one) without any linguis-
tics information. More useful features are ex-
pected to be helpful like POS tags. Finally, the 
performance can be further improved if we use a 
more powerful classifier (such as SVM or ME) 
with more iterations. 
653
7 Conclusion 
Our method provides a simple and efficient way 
to solve the word ordering problem partially 
which is NP-hard (Knight 1999). It is word based 
except for those indecomposable word sequences 
under ITGs. However, it can achieve comparable 
results with phrase-based method (like Pharaoh), 
while much fewer translation options are used. 
For the structure prediction process, only 3 com-
mon features are preserved and perceptron-based 
classifiers are chosen for the use of simplicity. We 
argue that this approach is promising when more 
features and more powerful classifiers are used as 
Daum? III et al (2007) stated. 
Our contributions lie in the integration of struc-
ture prediction for bilingual parsing trees through 
serial transformations. We reinforce the power of 
formally syntax-based method by using structured 
prediction method to obtain tree-to-tree transduc-
tions by the transforming from word alignments to 
ITG-like trees and then to label sequences. Thus, 
the sentence structures can be better accounted for 
during translating. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108. We would like to 
thank anonymous reviewers for their detailed comments. 
Appendix A. A Complete Example in Chinese-English 
with Our Defined Operations 
Word alignments 
 
ITG-like tree 
 
SHIFT-REDUCE label sequence 
??/a   S 
??/to learn about  S 
??/Chinese  S,R+ 
??/music   S,R+ 
?/?   S,R+ 
? ?/great   S 
?/?   S,R+ 
??/way   S,R+,R-,R+ 
Stack status when operations finish 
?? ?? ?? ?? ? ? ? ? ??  
/ a great way to learn about Chinese music 
References 
A. Burbank, M. Carpuat, et al 2005. Final Report of the 2005 
Language Engineering Workshop on Statistical Machine 
Translation by Parsing. Johns Hopkins University 
D. Chiang. 2005. A Hierarchical Phrase-Based Model for 
Statistical Machine Translation. In ACL, pages 263-270. 
M. Galley, M. Hopkins, et al 2004. What's in a translation 
rule? In HLT-NAACL, Boston, MA. 
D. Gildea. 2003. Loosely Tree-Based Alignment for Machine 
Translation. In ACL, pages 80-87, Sapporo, Japan. 
H. Daum? III, J. Langford, et al 2007. Search-based Struc-
tured Prediction. Under review by the Machine Learning 
Journal. http://pub.hal3.name/daume06searn.pdf. 
H. Daum? III, J. Langford, et al 2006. Searn in Practice. 
http://pub.hal3.name/daume06searn-practice.pdf.  
K. Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational Linguis-
tics 25(4): 607-615. 
K. Knight and J. Graehl. 2005. An Overview of Probabilistic 
Tree Transducers for Natural Language Processing. In 
CICLing, pages 1-24. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Models. In 
Proc. of AMTA, pages 115-124. 
P. Koehn and C. Monz. 2006. Manual and Automatic Evalua-
tion of Machine Translation between European Languages. 
In Proc. on the Workshop on Statistical Machine Transla-
tion, pages 102-121, New York City. 
P. Koehn, F. J. Och, et al 2003. Statistical Phrase-Based 
Translation. In HLT-NAACL, pages 127-133. 
P. Liang, A. Bouchard, et al 2006. An End-to-End 
Discriminative Approach to Machine Translation. In ACL. 
Y. Liu, Q. Liu, et al 2006. Tree-to-String Alignment Tem-
plate for Statistical Machine Translation. In ACL. 
F. J. Och. 2003. Minimum Error Rate Training in Statistical 
Machine Translation. In ACL, pages 160-167. 
F. J. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computational 
Linguistics 29(1): 19-51. 
K. Sagae and A. Lavie. 2005. A Classifier-Based Parser with 
Linear Run-Time Complexity. In IWPT, pages 125-132. 
C. Tillmann and T. Zhang. 2005. A Localized Prediction 
Model for Statistical Machine Translation. In ACL. 
C. Tillmann and T. Zhang. 2006. A Discriminative Global 
Training Algorithm for Statistical MT. in ACL. 
J. Turian, B. Wellington, et al 2006. Scalable Discriminative 
Learning for Natural Language Parsing and Translation. In 
Proceedings of NIPS, Vancouver, BC. 
T. Watanabe, J. Suzuki, et al 2007. Online Large-Margin 
Training for Statistical Machine Translation. In EMNLP. 
D. Wu. 1997. Stochastic Inversion Transduction Grammars 
and Bilingual Parsing of Parallel Corpora. Computational 
Linguistics 23(3): 377-404. 
D. Xiong, Q. Liu, et al 2006. Maximum Entropy Based 
Phrase Reordering Model for Statistical Machine Transla-
tion. In ACL, pages 521-528. 
K. Yamada and K. Knight. 2001. A Syntax-based Statistical 
Translation Model. In ACL, pages 523-530. 
K. Yamada and K. Knight. 2002. A Decoder for Syntax-
based Statistical MT. In ACL, pages 303-310. 
654
ISCAS?A System for Chinese Word Sense Induction Based on 
K-means Algorithm 
  Zhenzhong Zhang*           Le Sun? Wenbo Li? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
zhenzhong@nfs.iscas.ac.cn 
?Institute of Software 
Chinese Academy of Sciences 
{sunle,wenbo02}@iscas.ac.cn
 
Abstract 
This paper presents an unsupervised 
method for automatic Chinese word 
sense induction. The algorithm is based 
on clustering the similar words according 
to the contexts in which they occur. First, 
the target word which needs to be 
disambiguated is represented as the 
vector of its contexts. Then, reconstruct 
the matrix constituted by the vectors of 
target words through singular value 
decomposition (SVD) method, and use 
the vectors to cluster the similar words. 
Our system participants in CLP2010 
back off task4-Chinese word sense 
induction. 
1 Introduction 
It has been shown that using word senses instead 
of surface word forms could improve 
performance on many nature language 
processing tasks such as information extraction 
(Joyce and Alan, 1999), information retrieval 
(Ozlem et al, 1999) and machine translation 
(David et al, 2005). Historically, word senses 
are represented as a fixed-list of definitions 
coming from a manually complied dictionary. 
However, there seem to be some disadvantages 
associated with such fixed-list of senses 
paradigm. Since dictionaries usually contain 
general definitions and lack explicit semantic, 
they can?t reflect the exact content of the context 
where the target word appears. Another 
disadvantage is that the granularity of sense 
distinctions is fixed, so it may not be entirely 
suitable for different applications. 
In order to overcome these limitations, some 
techniques like word sense induction (WSI) have 
been proposed for discovering words? senses 
automatically from the unannotated corpus. The 
word sense induction algorithms are usually base 
on the Distributional Hypothesis, proposed by 
(Zellig, 1954), which showed that words with 
similar meanings appear in similar contexts 
(Michael, 2009). And the hypothesis is also 
popularized with the phrase ?a word characte-
rized by the company it keeps? (John, 1957). 
This concept shows us a method to automatical-
ly discover senses of words by clustering the 
target words with similar contexts (Lin, 1998). 
The word sense induction can be regarded as an 
unsupervised clustering problem. First, select 
some features to be used when comparing simi-
larity between words. Second, represent disam-
biguated words as vectors of selected features 
according to target words? contexts. Third, clus-
ter the similar words using the vectors. But 
compared with European languages such as Eng-
lish, Chinese language has its own characteris-
tics. For example, Chinese ideographs have 
senses while the English alphabets don?t have. 
So the methods which work well in English may 
not be entirely suitable for Chinese. 
  This paper proposes a method for Chinese 
word sense induction, which contains two stage 
processes: features selecting and context cluster-
ing. Chinese ideographs and Chinese words 
which have two or more Chinese ideographs are 
used different strategies when selecting features. 
The vectors of target word?s instances are put 
together to constitute a matrix, whose row is in-
stances and column is features. Reconstruct the 
matrix through singular value decomposition to 
get a new vector for each instance. Then, K-
means clustering algorithm is employed to clus-
ter the vectors of disambiguated words? contexts. 
Each cluster to which some instances belong to 
identifies a sense of corresponding target word. 
Our system participants in CLP2010 back off 
task4 - Chinese word sense induction. 
The remainder of this paper is organized as 
follows. Section 2 presents the Chinese word 
senses induction algorithm. Section 3 presents 
the evaluation sheme and the results of our 
system. Section 4 gives some discussions and 
conclusions. 
2 Chinese Word Senses Induction 
This section will present the strategies of select-
ing features for disambiguated Chinese words 
and k-means algorithm for clustering vectors of 
the contexts.  
2.1 Features Selection 
Since the input instances of target words are un-
structured, it's necessary to select features and 
transform them into structured format to fit the 
automatic clustering algorithm. Following the 
example in (Ted, 2007), words are chosen as 
features to represent the contexts where target 
words appear. A word w in the context of the 
target word can be represented as a vector whose 
ith component is the average of the calculated 
conditional probabilities of w and wj.  
The target words are usually removed from 
the corpus in the task of English word sense in-
duction. But Chinese language is very different 
from European languages such as English. Chi-
nese ideographs usually have meanings of their 
own while English   alphabets don?t have. In 
Chinese word senses induction tasks, the target 
word may be a Chinese word which could have 
one or more Chinese ideographs or a Chinese 
ideograph. And the meaning of Chinese ideo-
graphs is determined by the Chinese word where 
it appears. The following example shows us this 
case. 
z ??????????????? 162
???? 
z ?????????????????
??????????? 
In this example, the target word is Chinese 
ideograph ??? displayed in italic in the con-
texts. In the first context, its meaning is paddy 
which is determined by the Chinese word ??
? ?, and similarly in the second context its 
meaning is valley determined by ????. Since 
the meaning of the Chinese ideograph ??? is 
determined by the word where it appears, it may 
not be appropriate to remove it from the con-
texts simply while the others of the word are left. 
Different strategies are employed to remove tar-
get words.  If the target word contains two or 
more Chinese ideographs, it will be removed 
from the context. Otherwise it will be kept. 
  To solve the problem of data sparseness, we 
extracted extra 100 instances for each target 
word from Sogou Data and also used the 
thesauruses (TongYiCi CiLin of HIT) to reduce 
the dimensionality of the word space (feature 
space). Two filtering heuristics are applied when 
selecting features. The first one is the minimum 
frequency p1 of words, and the second one is the 
maximum frequency p2 of words. 
Each selected word (feature) should be as-
signed a weight, which indicates the relative fre-
quency of two co-occurring words. Using condi-
tional probabilities for weighting for object/verb 
and subject/verb pairs is better than point-wise 
mutual information (Philipp et al, 2005). So we 
used conditional probabilities for weighting 
words pairs. Let numi,j denote the number of the 
instances where the word i and word j co-occur , 
and numi denote the number of the instances in 
which the word i appears. Then the jth compo-
nent of the vector of the word i can be calculated 
using the following equation. 
,
( | ) ( | )
2i j
p j i p i j
w
+=  
Where 
  
,( | ) i j
j
n u m
p i j
n u m
=  
The contexts of each target word are represented 
as the centroid of the vectors of the words occur-
ring in the target contexts. Figure 1 shows an 
example of context vector, where the Chinese 
word ???? co-occurs with Chinese words ??
??and ????. 
 
Figure 1: An example of  a context vector for 
????, calculated as the centroid of vectors of 
???? and ????. 
2.2 Clustering Algorithm 
K-means algorithm is applied to cluster the vec-
tors of the target word. It assigns each element to 
one of K clusters according to which centroid 
the element is close to by the similarity function. 
The cosine function is used to measure the simi-
larity between two vectors V and W: 
1
2 2
1 1
( , )
| | | |
n
i i
i
n n
i i
i i
VW
V W
sim V W
V W
V W
=
= =
?= =?
?
? ?
 
where n is the number of features in each vector. 
Before clustering the vectors of instances, we 
put together the vectors of instances in the cor-
pus and obtain a co-occurrence matrix of in-
stances and words. Singular value decomposi-
tion is applied to reduce the dimensionality of 
the resulting multidimensional space and finds 
the major axes of variation in the word space 
(Golub and Van Loan, 1989). After the reduc-
tion, the similarity between two instances can be 
measured using the cosine function mentioned as 
above between the corresponding vectors. The 
clustering algorithm stops when the centroid of 
each cluster does not change or the iteration of 
the algorithm exceed a user-defined threshold p3. 
And the number of the clusters is determined by 
the corpus where the target word appears. Each 
cluster to which some instances belong 
represents one senses of the target word 
represented by the vector. 
We also employed a graph-based clustering 
algorithm -Chinese Whispers (CW) (Chris, 2006) 
to deal with the task of Chinese WSI. CW does 
not require any input parameters and has a good 
performance in WSI (Chris, 2006). For more 
details about CW algorithm please refer to 
(Chris, 2006). We first constructed a graph, 
whose vertexes were instances of target word 
and edges? weight was the similarity of the cor-
responding two vertexes. Then we removed the 
edges with minimum weight until the percentage 
of the kept edges? sum respect the total was be-
low a threshold p4. CW algorithm was employed 
to cluster the graph and each clusters represented 
a sense of target word. 
3 Evaluation 
This section presents the evaluation scheme, set 
of parameters and the result of our system. 
3.1 Evaluation Scheme 
We use standard cluster evaluation methods to 
measure the performance of our WSI system. 
Following the former practice (Zhao and Kary-
pis, 2005), we consider the FScore measure for 
assessing WSI methods. The FScore is used in a 
similar fashion to Information Retrieval exercis-
es. 
Let we assume that the size of a particular 
class sr is nr, the size of a particular cluster hj is 
nj and the size of their common instances set is 
nr,j. The precision can be calculated as follow: 
,( , ) r jr j
j
n
P s h
n
=  
The recall value can be defined as: 
,( , ) r jr j
r
n
R s h
n
=  
Then FScore of this class and cluster is defined 
to be: 
2 ( , ) ( , )
( , )
( , ) ( , )
r j r j
r j
r j r j
P s h R s h
F s h
P s h R s h
? ?= +  
The FScore of class sr, F(sr), is the maximum 
F(sr, hj) value attained by any cluster, and it is 
defined as: 
 ( ) max( ( , ))
j
r r jh
F s F s h=  
Finally, the FScore of the entire clustering solu-
tion is defined as the weighted average FScore 
of each class: 
1( )q r r
r
n F s
FScore
n=
?=?  
  Where q is the number of classes and n is the 
total number of the instances where target word 
appears. 
3.2 Tuning the Parameters 
We tune the parameters of our system on the 
training data. But because of time restrictions, 
we do not optimize these parameters. The max-
imum frequency of a word (p2) and the maxi-
mum number of the K-means? iteration (p3) are 
tuned on the training data. The minimum fre-
quency of a word (p1) was set to two following 
our intuition. The last parameter K -the number 
of the clusters is determined by the test data in 
which the target word appears. When tuning pa-
rameters, we first fixed the parameter p3 and 
found the best value of parameter p2, which 
could lead to the best performance. The results 
have been shown in Table 1 and Table 2. 
 
Parameters FScore 
P3=300,p2=35 0.7502 
P3=400,p2=40 0.7523 
P3=500,p2=40 0.7582 
Table 1: The results of K-means with SVD 
 
Parameters FScore 
P3=300,p2=40 0.7454 
P3=400,p2=40 0.7493 
P3=500,p2=45 0.7404 
Table 2: The results of K-means 
 
The performance of CW algorithm is shown 
in Table 3. The parameter p4 is a threshold for 
pruning graph as describing in section 2.2.  
Parameter FScore 
P4=0.55 0.6325 
P4=0.6 0.6321 
P4=0.65 0.6278 
P4=0.7 0.6393 
P4=0.75 0.6289 
P4=0.8 0.6345 
P4=0.85 0.6326 
P4=0.9 0.6342 
P4=0.95 0.6355 
Table 3: The results of CW. 
The result shows that the K-means algorithm 
has a better performance than CW. That may 
because CW can?t use the information of the 
number of clusters, but K-means could. Another 
problem for CW is that the size of corpus is 
small and the constructed graph can?t reflect the 
inherent relation between the instances.  
Based on the result of experiments, we em-
ployed K-means algorithm for our system and 
the parameters is shown in Table 4. 
 
Parameters Value
P1: Minimum frequency of a word 2 
P2: Maximum frequency of a word 40 
P3: Maximum number of K-means ite-
ration 
500 
K: the number of the cluster - 
Table 4: Parameters for the system. The last pa-
rameter K is provided by the test data. 
3.3 Result 
Our system participants in the CLP2010 back-
off task4 and disambiguate 100 target words, 
total 5000 instances. The F-score of our system 
on the test data is 0.7209 against the F-score 
0.7933 of the best system. 
4 Conclusion 
We have presented a model for Chinese word 
sense induction. Different strategies are applied 
to deal with Chinese ideographs and Chinese 
words that contain two or more Chinese ideo-
graphs. After selecting the features ?words, sin-
gular value decomposition is used to find the 
major axes of variation in the feature space and 
reconstruct the vector of each context. Then we 
employ k-means cluster algorithm to cluster the 
vectors of contexts. Result shows that our sys-
tem is able to induce correct senses. One draw-
back of our system is that it overlooks the infre-
quent senses because of lacking enough data. 
And our system only uses the information of 
word co-occurrences. So in the future we would 
like to integrate different kinds of information 
such as topical information, syntactic informa-
tion and semantic information, and see if we 
could get a better result. 
Acknowledgement 
This work has been partially funded by National 
Natural Science Foundation of China under 
grant #60773027, #60736044 and #90920010 
and by ?863? Key Projects #2006AA010108, 
?863? Projects #2008AA01Z145. We would like 
to thank anonymous reviewers for their detailed 
comments. 
References 
Chris Biemann, 2006.  Chinese whispers - an efficient 
graph clustering algorithm and its application to 
natural language processing problems, In Pro-
ceedings of TextGraphs, pp. 73?80, New York, 
USA. 
David Vickrey, Luke Biewald, Marc Teyssley, and 
Daphne Koller. 2005. Word-sense disambiguation 
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, 
pages 771-778, Vancouver, British Columbia, 
Canada 
Dekang Lin. 1998. Automatic retrieval and clustering 
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics, 
volume 2, pages 768-774, Montreal, Quebec, Can-
ada 
Golub, G. H. and Van Loan, C. F. 1989. Matrix 
Computations. The John Hopkins University Press, 
Baltimore, MD 
John, R., Firth. 1957. A Synopsis of Linguistic Theory 
1930-1955, pages 1-32. 
Joyce Yue Chai and Alan W. Biermann. 1999. The 
use of word sense disambiguation in an informa-
tion extraction system. In Proceedings of the six-
teenth national conference on Artificial intelli-
gence and the eleventh Innovative applications of 
artificial intelligence conference innovative appli-
cations of artificial intelligence, pages 850-855, 
Orlando, Florida, United States. 
Michael Denkowski. 2009. A Survey of Techniques 
for Unsupervised Word Sense Induction. 
Ozlem Uzuner, Boris Katz, and Deniz Yuret. 1999. 
Word sense disambiguation for information re-
trieval. In Proceedings of the sixteenth national 
conference on Artificial intelligence and the ele-
venth Innovative applications of artificial intelli-
gence conference innovative applications of artifi-
cial intelligence, page 985, Orlando, Florida, Unit-
ed States. 
Philipp Cimiano, Andreas Hotho, and Steffen Staab, 
2005.  Learning concept hierarchies from text cor-
pora using formal concept analysis, Journal of Ar-
tificial Intelligence Research (JAIR), 24, 305?339. 
Ted Pedersen, 2007. Umnd2: Senseclusters applied to 
the sense induction task of senseval-4. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations, pages 394?397, Prague, Czech 
Republic. 
Zellig Harris. 1954. Distributional Structure, pages 
146-162.  
Ying Zhao and George Karypis. 2005. Hierarchical 
clustering algorithms for document datasets. Data 
Mining and Knowledge Discovery, 10(2):141.168. 
