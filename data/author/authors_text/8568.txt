Proceedings of the Fourth International Natural Language Generation Conference, pages 63?70,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Algorithms for Generating Referring Expressions:
Do They Do What People Do?
Jette Viethen
Centre for Language Technology
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Robert Dale
Centre for Language Technology
Macquarie University
Sydney NSW 2109
robert.dale@mq.edu.au
Abstract
The natural language generation litera-
ture provides many algorithms for the
generation of referring expressions. In
this paper, we explore the question of
whether these algorithms actually produce
the kinds of expressions that people pro-
duce. We compare the output of three ex-
isting algorithms against a data set consist-
ing of human-generated referring expres-
sions, and identify a number of significant
differences between what people do and
what these algorithms do. On the basis of
these observations, we suggest some ways
forward that attempt to address these dif-
ferences.
1 Introduction
The generation of referring expressions (hence-
forth GRE) ? that is, the process of working
out what properties of an entity should be used
to describe it in such a way as to distinguish
it from other entities in the context ? is a re-
current theme in the natural language generation
literature. The task is discussed informally in
some of the earliest work on NLG (in particular,
see (Winograd, 1972; McDonald, 1980; Appelt,
1981)), but the first formally explicit algorithm
was introduced in (Dale, 1989); this algorithm,
often referred to as the Full Brevity (FB) algo-
rithm, has served as a starting point for many sub-
sequent GRE algorithms. To overcome its limita-
tion to one-place predicates, Dale and Haddock
(1991) introduced a constraint-based procedure
that could generate referring expressions involv-
ing relations; and as a response to the computa-
tional complexity of ?greedy? algorithms like FB,
Reiter and Dale (Reiter and Dale, 1992; Dale and
Reiter, 1995) introduced the psycholinguistically
motivated Incremental Algorithm (IA). In recent
years there have been a number of important ex-
tensions to the IA. The Context-Sensitive exten-
sion (Krahmer and Theune, 2002) is able to gen-
erate referring expressions for the most salient en-
tity in a context; the Boolean Expressions algo-
rithm (van Deemter, 2002) is able to derive ex-
pressions containing boolean operators, as in the
cup that does not have a handle; and the Sets
algorithm (van Deemter, 2002) extends the ba-
sic approach to references to sets, as in the red
cups. Some approaches reuse parts of other al-
gorithms: the Branch and Bound algorithm (Krah-
mer et al, 2003) uses the Full Brevity algorithm,
but is able to generate referring expressions with
both attributes and relational descriptions using a
graph-based technique. There are many other al-
gorithms described in the literature: see, for exam-
ple, (Horacek, 1997; Bateman, 1999; Stone, 2000;
Gardent, 2002). Their general aim is to produce
naturalistic referring expressions, often explicitly
by means of an attempt to follow the same kinds
of principles that we believe people might be fol-
lowing when they produce language ? such as the
Gricean maxims (Grice, 1975). However, the al-
gorithms have rarely been tested against real data
from human referring expression generation.1
In this paper, we present a data set containing
human-produced referring expressions in a limited
domain. Focussing specifically on the algorithms
1The only exceptions we know of to this deficit are not
directly concerned with the kinds of properties people select,
but with phenomena such as how people group entities to-
gether (Funakoshi et al, 2004; Gatt, 2006), or with multi-
modal referring expressions where the linguistic part is not
necessarily distinguishing by itself (van der Sluis and Krah-
mer, 2004).
63
presented in (Dale, 1989), (Dale and Haddock,
1991) and (Reiter and Dale, 1992), we explore
how well these algorithms perform in the same
context. There are significant differences between
the referring expressions produced by humans,
and those produced by the algorithms; we explore
these differences and consider what it means for
work in the generation of referring expressions.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce the data set
of human-produced referring expressions we use;
in Section 3, we introduce the representational
framework we use to model the domain underly-
ing this data; in Section 4 we introduce the three
algorithms considered in this paper; in Section 5
we discuss the results of using these algorithms
on the data that represents the model of our do-
main; in Section 6 we discuss the differences be-
tween the output of the algorithms and the human-
produced data; and in Section 7 we draw some
conclusions and suggest some steps towards ad-
dressing the issues we have identified.
2 The Data
Our human-produced referring expressions are
drawn from a physical experimental setting con-
sisting of four filing cabinets, each of which is
four drawers high, located in a fairly typical aca-
demic office. The cabinets are positioned directly
next to each other, so that the drawers form a four-
by-four grid; each drawer is labelled with a num-
ber between 1 and 16 and is coloured either blue,
pink, yellow, or orange. There are four drawers of
each colour which are distributed randomly over
the grid, as shown in Figure 1.
Subjects were given a randomly generated num-
ber between 1 and 16, and asked to produce a de-
scription of the numbered drawer using any prop-
erties other than the number. There were 20 partic-
ipants in the experiment, resulting in a total of 140
referring expressions. Here are some examples of
the referring expressions produced:
(1) the top drawer second from the right [d3]
(2) the orange drawer on the left [d9]
(3) the orange drawer between two pink ones
[d12]
(4) the bottom left drawer [d16]
Since the selection of which drawer to describe
was random, we do not have an equal number of
Figure 1: The filing cabinets
descriptions of each drawer; in fact, the data set
ranges from two descriptions of Drawer 1 to 12 de-
scriptions of Drawer 16. One of the most obvious
things about the data set is that even the same per-
son may refer to the same entity in different ways
on different occasions, with the differences being
semantic as well as syntactic.
We are interested in comparing how algorithms
for referring expression generation differ in their
outputs from what people do; since these al-
gorithms produce distinguishing descriptions, we
therefore removed from the data set 22 descrip-
tions which were ambiguous or referred to a set of
drawers. This resulted in a total of 118 distinct re-
ferring expressions, with an average of 7.375 dis-
tinct referring expressions per drawer.
As the algorithms under scrutiny here are not
concerned with the final syntactic realisation of
the referring expression produced, we also nor-
malised the human-produced data to remove su-
perficial variations such as the distinction between
relative clauses and reduced relatives, and between
different lexical items that were synonymous in
context, such as column and cabinet.
Four absolute properties used for describing the
drawers can be identified in the natural data pro-
duced by the human participants. These are the
colour of the drawer; its row and column; and in
those cases where the drawer is situated in one of
the corners of the grid, its cornerhood.2 A number
of the natural descriptions also made use of the
2A question we will return to below is that of how we
decide whether to view a particular property as a one-place
predicate or as a relation.
64
Property Count % (out of possible)
Row 95 79.66% (118)
Column 88 73.73% (118)
Colour 63 53.39% (118)
Corner 11 40.74% (27)
Relation 15 12.71% (118)
Table 1: The properties used in descriptions
following relational properties that hold between
drawers: above, below, next to, right of, left of and
between. In Table 1, Count shows the number of
descriptions using each property, and the percent-
ages show the ratio of the number of descriptions
using each property to the number of descriptions
for drawers that possess this property (hence, only
27 of the descriptions referred to corner drawers).
We have combined all uses of relations into one
row in this table to save space, since, interestingly,
their overall use is far below that of the other prop-
erties: 103 descriptions (87.3%) did not use rela-
tions.
Most algorithms in the literature aim at gen-
erating descriptions that are as short as possi-
ble, but will under certain circumstances pro-
duce redundancy. Some authors, for example
(van Deemter and Halldo?rsson, 2001), have sug-
gested that human-produced descriptions are of-
ten not minimal, and this is an intuition that we
would generally agree with. However, a strong
tendency towards minimality is evident in the
human-produced data here: only 29 out of 118 de-
scriptions (24.6%) contain redundant information.
Here are a few examples:
? the yellow drawer in the third column from
the left second from the top [d6]
? the blue drawer in the top left corner [d1]
? the orange drawer below the two yellow
drawers [d14]
In the first case, either the colour or column proper-
ties are redundant; in the second, colour and corner,
or only the grid information, would have been suf-
ficient; and in the third, it would have been suffi-
cient to mention one of the two yellow drawers.
3 Knowledge Representation
In order to use an algorithm to generate referring
expressions in this domain, we must first decide
how to represent the domain. It turns out that this
raises some interesting questions.
We use the symbols {d1, d2 . . . d16} as our
unique identifying labels for the 16 drawers.
Given some di, the goal of any given algorithm
is then to produce a distinguishing description of
that entity with respect to a context consisting of
the other 15 drawers.
As is usual, we represent the properties of the
domain in terms of attribute?value pairs. Thus we
have, for example:
? d2: ?colour, orange?, ?row, 1?, ?column, 2?,
?right-of, d1?, ?left-of, d3?, ?next-to, d1?, ?next-to,
d3?, ?above, d7?
This drawer is in the top row, so it does not have a
property of the form ?below, d2?.
The four corner drawers additionally possess
the property ?position, corner?. Cornerhood can
be inferred from the row and column informa-
tion; however, we added this property explicitly
because several of the natural descriptions use the
property of cornerhood, and it seems plausible that
this is a particularly salient property in its own
right.
This raises the question of what properties
should be encoded explicitly, and which should
be inferred. Note that in the example above, we
explicitly encode relational properties that could
be computed from others, such as left-of and right-
of. Since none of the algorithms explored here
uses inference over knowledge base properties, we
opted here to ?level the playing field? to enable
fairer comparison between human-produced and
machine-produced descriptions.
A similar question of the role of inference arises
with regard to the transitivity of spatial relations.
For example, if d1 is above d9 and d9 is above
d16 , then it can be inferred that d1 is transitively
above d16. In a more complex domain, the imple-
mentation of this kind of knowledge might play
an important role in generating usful referring ex-
pressions. However, the uniformity of our domain
results in this inferred knowledge about transitive
relations being of little use; in fact, in most cases,
the implementation of transitive inference might
even result in the generation of unnatural descrip-
tions, such as the orange drawer (two) right of the
blue drawer for d12.
Another aspect of the representation of relations
that requires a decision is that of generalisation:
65
next-to is a generalisation of the relations left-of and
right-of. The only algorithm of those we exam-
ine here that provides a mechanism for exploring
a generalisation hierarchy is the Incremental Al-
gorithm (Reiter and Dale, 1992), and this cannot
handle relations; so, we take the shortcut of ex-
plicitly representing the next-to relation for every
left-of and right-of relation in the knowledge base.
We then implement special-case handling that en-
sures that, if one of these facts is used, the more
general or more specific case is also deleted from
the set of properties still available for the descrip-
tion.3
4 The Algorithms
As we have already noted above, there is a con-
siderable literature on the generation of referring
expressions, and many papers in the area provide
detailed algorithms. We focus here on the follow-
ing algorithms:
? The Full Brevity algorithm (Dale, 1989) at-
tempts to build a minimal distinguishing de-
scription by always selecting the most dis-
criminatory property available; see Algo-
rithm 1.
Let L be the set of properties to be realised in our
description; let P be the set of properties known to be
true of our intended referent r (we assume that P is
non-empty); and let C be the set of distractors (the
contrast set). The initial conditions are thus as follows:
? C = {?all distractors?};
? P = {?all properties true of r?};
? L = {}
In order to describe the intended referent r with respect
to the contrast set C, we do the following:
1. Check Success:
if |C| = 0 then return L as a distinguishing
description
elseif P = ? then fail
else goto Step 2.
2. Choose Property:
for each pi ? P do:
Ci ? C ? {x|pi(x)}
Chosen property is pj , where Cj is the smallest set.
goto Step 3.
3. Extend Description (wrt the chosen pj):
L ? L ? {pj}
C ? Cj
P ? P ? {pj}
goto Step 1.
Algorithm 1: The Full Brevity Algorithm
3This is essentially a hack; however, there is clearly a need
for some mechanism for handling what we might think of
as equivalence classes of properties, and this is effectively a
simple approach to this question.
1. Check Success
if Stack is empty then return L as a DD
elseif |Cv| = 1 then pop Stack & goto Step 1
elseif Pr = ? then fail
else goto Step 2
2. Choose Property
for each property pi ? Pr do
p?i ? [r\v]pi
Ni ? N ? p?i
Chosen prediction is pj , where Nj contains
the smallest set Cv for v.
goto Step 3
3. Extend Description (w.r.t the chosen p)
Pr ? Pr ? {p}
p ? [r\v]p
for every other constant r? in p do
associate r? with a new, unique variable v?
p ? [r?\v?]p
push Describe(r?,v?) onto Stack
initialise a set P ?r of facts true of r?
N ? N ? p
goto Step 1
Algorithm 2: The Relational Algorithm
MakeReferringExpression(r, C, P ) L ? {}
for each member Ai of list P do
V = FindBestValue(r, Ai, BasicLevelValue(r, Ai))
if RulesOut(?Ai, V ?) 6= nil
then L ? L ? {?Ai, V ?}
C ? C ? RulesOut(?Ai, V ?)
endif
if C = {} then
if ?type, X? ? L for some X
then return L
else return L ? {?type, BasicLevelValue(r,
type)?}
endif
endif
return failure
FindBestValue(r, A, initial-value)
if UserKnows(r, ?A, initial-value?) = true
then value ? initial-value
else value ? no-value
endif
if (more-specific-value ? MoreSpecificValue(r, A,
value)) 6= nil ?
(new-value ? FindBestValue(A,
more-specific-value)) 6= nil ?
(|RulesOut(?A, new-value?)| > |RulesOut(?A,
value?)|)
then value ? new-value
endif
return value
RulesOut(?A, V ?)
if V = no-value
then return nil
else return {x : x ? C ? UserKnows(x, ?A, V ?) =
false}
endifAlgorithm 3: The Incremental Algorithm
66
? The relational algorithm from (Dale and Had-
dock, 1991) uses constraint satisfaction to in-
corporate relational properties while avoiding
infinite regress; see Algorithm 2.
? the Incremental Algorithm (Reiter and Dale,
1992; Dale and Reiter, 1995) considers the
available properties to be used in a descrip-
tion via a preference ordering over those
properties; see Algorithm 3.
For the purpose of this study, the algorithms were
implemented in Common LISP. The mechanism
described in (Dale and Reiter, 1995) to handle
generalisation hierarchies for values for the dif-
ferent properties, referred to in the algorithm here
as FindBestValue, was not implemented since, as
discussed earlier, our representation of the domain
does not make use of a hierarchy of properties.
5 The Output of the Algorithms
Using the knowledge base described in Section 3,
we applied the algorithms from the previous sec-
tion to see whether the referring expressions they
produced were the same as, or similar to, those
produced by the human subjects. This quickly
gave rise to some situations not explicitly ad-
dressed by some of the algorithms; we discuss
these in Section 5.1 below. Section 5.2 discusses
the extent to which the behaviour of the algorithms
matched that of the human data.
5.1 Preference Orderings
The Incremental Algorithm explicitly encodes a
preference ordering over the available properties,
in an attempt to model what appear to be semi-
conventionalised strategies for description that
people use. This also has the consequence of
avoiding a problem that faces the other two algo-
rithms: since the Full Brevity Algorithm and the
Relational Algorithm choose the most discrimina-
tory property at each step, they have to deal with
the case where several properties are equally dis-
criminatory. This turns out to be a common sit-
uation in our domain. Both algorithms implicitly
assume that the choice will be made randomly in
these cases; however, it seems to us more natural
to control this process by imposing some selection
strategy. We do this here by borrowing the idea
of preference ordering from the Incremental Algo-
rithm, and using it as a tie-breaker when multiple
properties are equally discriminatory.
Not including type information (i.e., the fact that
some di is a drawer), which has no discrimina-
tory power and therefore will never be chosen by
any of the algorithms,4 there are only four differ-
ent properties available for the Full Brevity Algo-
rithm and the Incremental Algorithm: row, column,
colour, and position. This gives us 4! = 24 different
possible preference orderings. Since some of the
human-produced descriptions use all four proper-
ties, we tested these two algorithms with all 24
preference orderings.
For the Relational Algorithm, we added the five
relations next to, left of, right of, above, and below.
This results in 9! = 362,880 possible preference
orderings; far too many to test. Since we are
primarily interested in whether the algorithm can
generate the human-produced descriptions, we re-
stricted our testing to those preference orderings
that started with a permutation of the properties
used by the participants; in addition to the 24 pref-
erence orderings above, there are 12 preference or-
derings that incorporate the relational properties.
5.2 Coverage of the Human Data
Overall, the Full Brevity Algorithm is able to gen-
erate 82 out of the 103 non-relational descriptions
from the natural data, providing a recall of 79.6%.
The recall score for the Incremental Algorithm is
95.1%, generating 98 of the 103 descriptions. As
these algorithms do not attempt to generate rela-
tional descriptions, the relational data is not taken
into account in evaluating the performance here.
Both algorithms are able to generate all the
non-relational minimal descriptions found in the
human-produced data. The Full Brevity Algo-
rithm unintentionally replicates the redundancy
found in nine descriptions, and the Incremental
Algorithm produces all but five of the 29 redun-
dant descriptions.
Perhaps surprisingly, the Relational Algorithm
does not generate any of the human-produced de-
scriptions. We will return to consider why this is
the case in the next section.
6 Discussion
There are two significant differences to be consid-
ered here: first, the coverage of redundant descrip-
tions by the Full Brevity and Incremental Algo-
4Consistent with much other work in the field, we as-
sume that the head noun will always be added irrespective
of whether it has any discriminatory power.
67
rithms; and second, the inability of the Relational
Algorithm to replicate any of the human data.
6.1 Coverage of Redundancy
Neither the Full Brevity Algorithm nor the Incre-
mental Algorithm presumes to be able to generate
relational descriptions; however, both algorithms
are able to produce each of the minimal descrip-
tions from the set of natural data with at least one
of the preference orderings. Both also generate
several of the redundant descriptions in the nat-
ural data set, but do not capture all of the human-
generated redundancies.
The Full Brevity Algorithm has as a primary
goal the avoidance of redundant descriptions, so
it is a sign of the algorithm being consistent with
its specification that it covers fewer of the redun-
dant expressions than the Incremental Algorithm.
On the other hand, the fact that it produces any
redundant descriptions signals that the algorithm
doesn?t quite meet its specification. The cases
where the Full Brevity Algorithm produces redun-
dancy are when an entity shares with another en-
tity at least two property-values and, after choos-
ing one of these properties, the next property to
be considered is the other shared one, since it has
the same or a higher discriminatory power than all
other properties. This is a situation that was not
considered in the original algorithm; it is related
to the problem of what to do when two properties
have the same discriminatory power, as noted ear-
lier. In our domain, the situation arises for corner
drawers with the same colour (d4 and d16), and
drawers that are not in a corner but for which there
is another drawer of the same colour in each of the
same row and column (d7 and d8).
The Incremental Algorithm, on the other hand,
generates redundancy when an object shares at
least two property-values with another object and
the two shared properties are the first to be con-
sidered in the preference ordering. This is pos-
sible for corner drawers with the same colour (d4
and d16) and for drawers for which there is another
drawer of the same colour in either the same row,
the same column, or both (d5, d6, d7, d8, d10, d11,
d13, d15).
In these terms, the Incremental Algorithm is
clearly a better model of the human behaviour than
the Full Brevity Algorithm. However, we may ask
why the algorithm does not cover all the redun-
dancy found in the human descriptions. The re-
dundant descriptions which the algorithm does not
generate are as follows:
(5) the blue drawer in the top left corner [d1]
(6) the yellow drawer in the top right corner [d4]
(7) the pink drawer in the top of the column sec-
ond from the right [d3]
(8) the orange drawer in the bottom second from
the right [d14]
(9) the orange drawer in the bottom of the second
column from the right [d14]
The Incremental Algorithm stops selecting prop-
erties when a distinguishing description has been
constructed. In Example (6), for example, the
algorithm would select any of the following, de-
pending on the preference ordering used:
(10) the yellow drawer in the corner
(11) the top left yellow drawer
(12) the drawer in the top left corner
The human subject, however, has added informa-
tion beyond what is required. This could be ex-
plained by our modelling of cornerhood: in Ex-
amples (5) and (6), one has the intuition that the
noun corner is being added simply to provide a
nominal head to the prepositional phrase in an
incrementally-constructed expression of the form
the blue drawer in the top right . . . , in much
the same way as the head noun drawer is added,
whereas we have treated it as a distinct property
that adds discriminatory power. This again em-
phasises the important role the underlying repre-
sentation plays in the generation of referring ex-
pressions: if we want to emulate what people do,
then we not only need to design algorithms which
mirror their behaviour, but these algorithms have
to operate over the same kind of data.
6.2 Relational Descriptions
The fact that the Relational Algorithm generates
none of the human-generated descriptions is quite
disturbing. On closer examination, it transpires
that this is because, in this domain, the discrimi-
natory power of relational properties is generally
always greater than that of any other property, so
a relational property is chosen first. As noted ear-
lier, relational properties appear to be dispreferred
68
in the human data, so the Relational Algorithm is
already disadvantaged. The relatively poor per-
formance of the algorithm is then compounded by
its insistence on continuing to use relational prop-
erties: an absolute property will only be chosen
when either the currently described drawer has no
unused relational properties left, or the number
of distractors has been reduced so much that the
discriminatory power of all remaining relational
properties is lower than that of the absolute prop-
erty, or the absolute property has the same discrim-
inatory power as the best relational one and the ab-
solute property appears before all relations in the
preference ordering.
Consequently, whereas a typical human de-
scription of drawer d2 would be the orange drawer
above the blue drawer, the Relational Algorithm
will produce the description the drawer above the
drawer above the drawer above the pink drawer.
Not only are there no descriptions of this form in
the human-produced data set, but they also sound
more like riddles someone might create to inten-
tionally make it hard for the hearer to figure out
what is meant.
There are a variety of ways in which the be-
haviour of this algorithm might be repaired. We
are currently exploring whether Krahmer et als
(2003) graph-based approach to GRE is able to
provide a better coverage of the data: this algo-
rithm provides the ability to make use of differ-
ent search strategies and weighting mechanisms
when adding properties to a description, and such
a mechanism might be used, for example, to coun-
terbalance the Relational Algorithm?s heavy bias
towards the relations in this domain.
7 Conclusions and Future Work
We have noted a number of regards in which the
algorithms we have explored here do not produce
outputs that are the same as those produced by hu-
mans. Some comments on the generalisability of
these results are appropriate.
First, our results may be idiosyncratic to the
specifics of the particular domain of our experi-
ment. We would point out, however, that the do-
main is more complex, and arguably more real-
istic, than the much-simplified experimental con-
texts that have served as intuitions for earlier work
in the field; we have in mind here in particular the
experiments discussed in (Ford and Olson, 1975),
(Sonnenschein, 1985) and (Pechmann, 1989). In
the belief that the data provides a good test set
for the generation of referring expressions, we are
making the data set publicly available 5, so others
may try to develop algorithms covering the data.
A second concern is that we have only explored
the extent to which three specific algorithms are
able to cover the human data. Many of the other al-
gorithms in the literature take these as a base, and
so are unlikely to deliver significantly different re-
sults. The major exceptions here may be (a) van
Deemter?s (2002) algorithm for sets; recall that we
excluded from the human data used here 16 ref-
erences that involved sets; and, as noted above,
(b) Krahmer et als (2003) graph-based approach
to GRE, which may perform better than the Re-
lational Algorithm on descriptions using relations.
In future work, we intend to explore to what extent
our findings extend to other algorithms.
In conclusion, we point to two directions where
we believe further work is required.
First, as we noted early in this paper, it is clear
that there can be many different ways of refer-
ring to the same entity. Existing algorithms are
all deterministic and therefore produce exactly one
?best? description for each entity; but the human-
produced data clearly shows that there are many
equally valid ways of describing an entity. We
need to find some way to account for this in our
algorithms. Our intuition is that this is likely to
be best cashed out in terms of different ?refer-
ence strategies? that different speakers adopt in
different situations; we are reminded here of Car-
letta?s (1992) distinction between risky and cau-
tious strategies for describing objects in the Map
Task domain. More experimentation is required in
order to determine just what these strategies are:
are they, for example, characterisable as things
like ?Produce a referring expression that is as short
as possible? (the intuition behind the Full Brevity
Algorithm), ?Just say what comes to mind first and
keep adding information until the description dis-
tinguishes the intended referent? (something like
the Incremental Algorithm), or perhaps a strategy
of minimising the cognitive effort for either the
speaker or the hearer? Further psycholinguistic
experiments and data analysis are required to de-
termine the answers here.
Our second observation is that the particular re-
sults we have presented here are, ultimately, en-
5The data set is publicly available from
http://www.ics.mq.edu.au/?jviethen/drawers
69
tirely dependent upon the underlying representa-
tions we have used, and the decisions we have
made in choosing how to represent the properties
and relations in the domain. We believe it is im-
portant to draw attention to the fact that precisely
how we choose to represent the domain has an im-
pact on what the algorithms will do. If we are
aiming for naturalism in our algorithms for refer-
ring expression generation, then ideally we would
like our representations to mirror those used by hu-
mans; but, of course, we don?t have direct access
to what these are.
There is clearly scope for psychological exper-
imentation, perhaps along the lines initially ex-
plored by (Rosch, 1978), to determine some con-
straints here. In parallel, we are considering fur-
ther exploration into the variety of representations
that can be used, particularly with regard to the
question of which properties are considered to be
?primitive?, and which are generated by some in-
ference mechanism; this is a much neglected as-
pect of the referring expression generation task.
References
D. E. Appelt. 1981. Planning Natural Language Ut-
terances to Satisfy Multiple Goals. Ph.D. thesis,
Stanford University.
J. Bateman. 1999. Using aggregation for selecting
content when generating referring expressions. In
Proceedings of the 37th Meeting of the ACL, pages
127?134.
J. C. Carletta. 1992. Risk-taking and Recovery in Task-
oriented Dialogue. Ph.D. thesis, University of Edin-
burgh.
R. Dale and N. Haddock. 1991. Generating referring
expressions involving relations. In Proceedings of
the 5th Meeting of the EACL, pages 161?166, Berlin,
Germany.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Meeting of the ACL, pages
68?75.
W. Ford and D. Olson. 1975. The elaboration of
the noun phrase in children?s description of objects.
Journal of Experimental Child Psychology, 19:371?
382.
K. Funakoshi, S. Watanabe, N. Kuriyama, and T. Toku-
naga. 2004. Generating referring expressions using
perceptual groups. In Proceedings of the 3rd INLG,
pages 51?60.
C. Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Meeting of
the ACL, pages 96?103.
A. Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proceedings
of the 11th Meeting of the EACL.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics Vol-
ume 3: Speech Acts, pages 43?58. Academic Press.
H. Horacek. 1997. An algorithm for generating ref-
erential descriptions with flexible interfaces. In Pro-
ceedings of the 35th Meeting of the ACL, pages 127?
134.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Informa-
tion Sharing: Reference and Presupposition in Lan-
guage Generation and Interpretation, pages 223?
264. CSLI.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
D. D. McDonald. 1980. Natural Language Generation
as a Process of Decision-making Under Constraints.
Ph.D. thesis, Massachusetts Institute of Technology.
T. Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89?110.
E. Reiter and R. Dale. 1992. A fast algorithm for the
generation of referring expressions. In Proceedings
of the 14th Meeting of the ACL, pages 232?238.
E. Rosch. 1978. Principles of categorization. In Cog-
nition and Categorization, pages 27?48. Lawrence
Erlbaum, Hillsdale, NJ.
S. Sonnenschein. 1985. The development of referen-
tial communication skills: Some situations in which
speakers give redundant messages. Journal of Psy-
cholinguistic Research, 14:489?508.
M. Stone. 2000. On identifying sets. In Proceedings
of the 1st INLG, pages 116?123.
K. van Deemter and M. M. Halldo?rsson. 2001. Logi-
cal form equivalence: The case referring expressions
generation. In Proceedings of the 8th ENLG.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
I. van der Sluis and E. Krahmer. 2004. Evaluating
multimodal NLG using production experiments. In
Proceedings of the 4th LREC, pages 209?212, 26-28
May.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
70
Proceedings of the 12th European Workshop on Natural Language Generation, pages 58?65,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Referring Expression Generation through Attribute-Based Heuristics
Robert Dale and Jette Viethen
Centre for Language Technology
Macquarie University
Sydney, Australia
rdale@ics.mq.edu.au|jviethen@ics.mq.edu.au
Abstract
In this paper, we explore a corpus of
human-produced referring expressions to
see to what extent we can learn the referen-
tial behaviour the corpus represents. De-
spite a wide variation in the way subjects
refer across a set of ten stimuli, we demon-
strate that component elements of the re-
ferring expression generation process ap-
pear to generalise across participants to a
significant degree. This leads us to pro-
pose an alternative way of thinking of re-
ferring expression generation, where each
attribute in a description is provided by a
separate heuristic.
1 Introduction
The last few years have witnessed a considerable
move towards empiricism in referring expression
generation; this is evidenced both by the growing
body of work that analyses and tries to replicate
the content of corpora of human-produced refer-
ring expressions, and particularly by the signifi-
cant participation in the TUNA and GREC chal-
lenge tasks built around such activities (see, for
example, (Belz and Gatt, 2007; Belz et al, 2008;
Gatt et al, 2008)). One increasingly widespread
observation?obvious in hindsight, but surpris-
ingly absent from much earlier work on referring
expression generation?is that one person?s refer-
ential behaviour differs from that of another: given
the same referential task, different subjects will
choose different referring expressions to identify
a target referent. Faced with this apparent lack of
cross-speaker consistency in how to refer to enti-
ties, we might question the validity of any exercise
that tries to develop an algorithm on the basis of
data from multiple speakers.
In this paper we revisit the corpus of data
that was introduced and discussed in (Viethen
and Dale, 2008a; Viethen and Dale, 2008b) with
the objective of determining what referential be-
haviour, if any, might be learned automatically
from the data. We find that, despite the apparent
diversity of the data when we consider the pro-
duction of referring expressions across subjects,
a closer examination reveals that individual at-
tributes within referring expressions do appear to
be selected on the basis of contextual factors with
a high degree of consistency. This suggests that re-
ferring behaviour might be best thought of as con-
sisting of a combination of lower-level heuristics,
with each individual?s overall referring behaviour
being constructed from a potentially distinct com-
bination of these common heuristics.
In Section 2 we describe the corpus we use for
the experiments in this paper. In Section 3 we ex-
plore to what extent we can use this corpus to learn
an algorithm for referring expression generation;
in Section 4 we look more closely at the nature of
individual variation within the corpus. Section 5
briefly discusses related work on the use of ma-
chine learning in referring expression generation,
and Section 6 draws some conclusions and points
to future work.
2 The Corpus
2.1 General Overview
The corpus we use was collected via a data gath-
ering experiment described in (Viethen and Dale,
2008a; Viethen and Dale, 2008b). The purpose of
the data gathering was to gain some insight into
how human subjects use relational referring ex-
pressions, a relatively unexplored aspect of refer-
ring expression generation. Participants visited a
website, where they first saw an introductory page
with a set of simple instructions and a sample stim-
ulus scene consisting of three objects. Each par-
ticipant was then assigned one of two trial sets of
ten scenes each; the two trial sets are superficially
58
Figure 1: The stimulus scenes. The letters indi-
cate which schema from Figure 2 each column of
scenes is based on.
different, but the elements of the sets are pairwise
identical in terms of the factors explored in the re-
search. The complete set of 20 scenes is shown in
Figure 1, where Trial Set 1 consists of Scenes 1
through 10, and Trial Set 2 consists of Scenes 11
through 20.1
The scenes were presented successively in a
preset order, which was the same for each partic-
ipant. Below each scene, the participant had to
complete the sentence Please pick up the . . . in a
text box before clicking on a button to see the next
scene. The task was to describe the target referent
in the scene (marked by a grey arrow) in a way that
would enable a friend looking at the same scene to
pick it out from the other objects.
The experiment was completed by 74 partici-
pants from a variety of different backgrounds and
ages; most were university-educated and in their
early or mid twenties. For reasons discussed in
(Viethen and Dale, 2008b), the data of 11 partici-
pants was discarded. Of the remaining 63 partici-
pants, 29 were female, while 34 were male.
2.2 Stimulus Design
The design of the stimuli used in the experiment is
described in detail in (Viethen and Dale, 2008a).
1Scene 1 is paired with Scene 11, Scene 2 with Scene
12, and so on; in each pair, the only differences are the
colour scheme used and the left?right orientation, with these
variations being introduced to make the experiment less
monotonous for subjects; (Viethen and Dale, 2008a) report
that these characteristics of the scenes appear to have no sig-
nificant effect on the forms of reference used.
Figure 2: The schemata which form the basis for
the stimulus scenes.
We provide a summary of the key points here.
In order to explore even the most basic hypothe-
ses with respect to the use of relational expres-
sions, which was the aim of the original study,
scenes containing at least three objects were re-
quired. One of these objects is the intended ref-
erent, which is referred to here as the target. The
subject has to describe the target in such a way as
to distinguish it from the other two objects in the
scene. Although the scenes presented to the sub-
jects are such that spatial relations are never nec-
essary to distinguish the target, they are set up so
that one of the two non-target objects was clearly
closer to the target. This object is referred to as the
(potential) landmark; and we call the third object
in the scene the distractor.
To minimise the number of variables in the ex-
periments, scenes are restricted to only two kinds
of objects, cubes and balls. The objects also vary
in two dimensions: colour (either green, blue,
yellow, or red); and size (either large or small).
To further reduce the number of factors in the
scene design, the landmark and distractor are al-
ways placed clearly side by side, and the target is
located on top of or directly in front of the land-
mark.
Finally, to reduce the set of possible stimuli to a
manageable number, five schemata (see Figure 2)
were created as a basis for the final stimulus set.
The design of these schemata was informed by a
number of research questions with regard to the
use of relations; see (Viethen and Dale, 2008b). A
schema determines the type and size of each object
in the scenes that are based on it, and determines
which objects share colour. So, for example, in
scenes based on Schema C, the target is a small
ball; the landmark is a large cube with different
colour from the target; and the distractor is a large
ball sharing its colour with the target.
59
Label Pattern Example
A ?tg col, tg type? the blue cube
B ?tg col, tg type, rel, lm col, lm type? the blue cube in front of the red ball
C ?tg col, tg type, rel, lm size, lm col, lm type? the blue cube in front of the large red ball
D ?tg col, tg type, rel, lm size, lm type? the blue cube in front of the large ball
E ?tg col, tg type, rel, lm type? the blue cube in front of the ball
F ?tg size, tg col, tg type? the large blue cube
G ?tg size, tg col, tg type, rel, lm col, lm type? the large blue cube in front of the red ball
H ?tg size, tg col, tg type, rel, lm size, lm col, lm type? the large blue cube in front of the large red ball
I ?tg size, tg col, tg type, rel, lm size, lm type? the large blue cube in front of the large ball
J ?tg size, tg col, tg type, rel, lm type? the large blue cube in front of the ball
K ?tg size, tg type? the large cube
L ?tg size, tg type, rel, lm size, lm type? the large cube in front of the large ball
M ?tg size, tg type, rel, lm type? the large cube in front of the ball
N ?tg type? the cube
O ?tg type, rel, lm col, lm type? the cube in front of the red ball
P ?tg type, rel, lm size, lm col, lm type? the cube in front of the large red ball
Q ?tg type, rel, lm size, lm type? the cube in front of the large ball
R ?tg type, rel, lm type? the cube in front of the ball
Table 1: The 18 different patterns corresponding to the different forms of description that occur in the
GRE3D3 corpus.
From each schema, four distinct scenes were
generated, resulting in the 20 stimulus scenes
shown in Figure 1. As noted above, there are really
only 10 distinct ?underlying? scene types here, so
in the remainder of this paper we will talk in terms
of Scenes 1 through 10, where the data from the
pairwise matched scenes are conflated.
2.3 The GRE3D3 Corpus2
Before conducting any quantitative data analysis,
some syntactic and lexical normalisation was car-
ried out on the data provided by the participants.
In particular, spelling mistakes were corrected;
normalised names were used for colour values and
head nouns (for example, box was replaced by
cube); and complex syntactic structures such as
relative clauses were replaced with semantically
equivalent simpler ones such as adjectives. These
normalisation steps should be of no consequence
to the analysis presented here, since we are solely
interested in exploring the semantic content of re-
ferring expressions, not their lexical and syntactic
surface structure.
For the purposes of the machine learning exper-
iments described in this paper, we made a few fur-
ther changes to the data set in order to keep the
number of properties and their possible values low.
We removed locative expressions that made refer-
2The data set resulting from the experiment described
above is known as the GRE3D3 Corpus; the name stands for
?Generation of Referring Expressions in 3D scenes with 3
Objects?.
ence to a part of the scene (58 instances) and ref-
erences to size as the same (4 instances); so, for
example, the blue cube on top of the green cube
in the right and the blue cube on top of the green
cube of the same size both became the blue cube
on top of the green cube. We also removed the
mention of a third object from ten descriptions in
order to keep the number of possible objects per
description to a maximum of two. These changes
resulted in seven descriptions no longer satisfying
the criterion of being fully distinguishing, so we
removed these descriptions from the corpus.
3 Learning Description Patterns
The resulting corpus consists of 623 descriptions.
Every one of these is an instance of one of the 18
patterns shown in Table 1; for ease of reference,
we label these patterns A through R. Each pattern
indicates the sequence of attributes used in the de-
scription, where each attribute is identified by the
object it describes (tg for target, lm for landmark)
and the attribute used (col, size and type for colour,
size and type respectively).
Most work on referring expression generation
attempts to determine what attributes should be
used in a description by taking account of aspects
of the context of reference. An obvious question
is then whether we can learn the description pat-
terns in this data from the contexts in which they
were produced. To explore this, we chose to cap-
ture the relevant aspects of context by means of
the notion of characteristics of scenes. The char-
60
Label Attribute Values
tg type = lm type Target and Landmark share Type TRUE, FALSE
tg type = dr type Target and Distractor share Type TRUE, FALSE
lm type = dr type Landmark and Distractor share Type TRUE, FALSE
tg col = lm col Target and Landmark share Colour TRUE, FALSE
tg col = dr col Target and Distractor share Colour TRUE, FALSE
lm col = dr col Landmark and Distractor share Colour TRUE, FALSE
tg size = lm size Target and Landmark share Size TRUE, FALSE
tg size = dr size Target and Distractor share Size TRUE, FALSE
lm size = dr size Landmark and Distractor share Size TRUE, FALSE
rel Relation between Target and Landmark on top of, in front of
Table 2: The 10 characteristics of scenes
acteristics of scenes which we hypothesize might
have an impact on the choice of referential form
are those summarised in Table 2; these are pre-
cisely the characteristics that were manipulated in
the design of the schemata in Figure 2.
Of course, there is no one correct answer for
how to refer to the target in any given scene.
Figure 3 shows the distribution of different pat-
terns across the different scenes; so, for exam-
ple, some scenes (Scenes 4, 5, 9 and 10) result
in only five semantically distinct referring expres-
sion forms, whereas Scene 7 results in 12 distinct
referring expression forms. All of these are distin-
guishing descriptions, so all are acceptable forms
of reference, although some contain more redun-
dancy than others. Most obvious from the chart
is that, for many scenes, there is a predominant
form of reference used; so, for example, pattern F
(?tg size, tg col, tg type?) accounts for 43 (68%)
of the descriptions used in Scene 4, and pattern
A (?tg col, tg type?) is very frequently used in a
number of scenes.3
We used Weka (Witten and Eibe, 2005) with the
J48 decision tree classifer to see what correspon-
dences might be learned between the character-
isics of the scenes listed in Table 2 and the forms
of referring expression used for the target refer-
ents, as shown in Table 1. The pruned decision
tree learned by this method predicted the actual
form of reference used in only 48% of cases under
10-fold cross-validation, but given that there are
many ?gold standard? descriptions for each scene,
3The chart as presented here is obviously too small to en-
able detailed examination, and our use of colour coding will
be of no value in a monchrome rendering of the paper; how-
ever, the overall shape of the data is sufficient to demonstrate
the points we make here.
this low score is hardly surprising; a mechanism
which learns only one answer will inevitably be
?wrong? in many cases. More revealing, however,
is the rule learned from the data:
if tg type = dr type
then use F (?tg size, tg col, tg type?)
else use A (?tg col, tg type?)
endif
Patterns A and F are the two most prevalent pat-
terns in the data, and indeed one or other appears
at least once in the human data for each scene;
consequently, the learned rule is able to produce
a ?correct? answer for every scene.4
4 Individual Variation
One of the most striking things about the data in
this corpus is the extent to which different subjects
appear to do different things when they construct
referring expressions, as demonstrated by the dis-
tribution of patterns in Figure 3. Another way of
looking at this variation is to characterise the be-
haviour of each subject in terms of the sequence of
descriptions they provide in response to the set of
10 stimuli.
Across the 63 subjects, there are 47 different se-
quences; of these, only four occur more than once
(in other words, 43 subjects did not produce the
same sequence of descriptions for the ten scenes as
anyone else). The recurrent sequences, i.e. those
used by at least two people, are shown in Table 3.
Note that the most frequently recurring sequence,
4The fact that the rule is conditioned on a property of the
distractor object may be an artefact of the stimulus set con-
struction; this would require a more diverse set of scenes to
determine.
61
Figure 3: The profile of different description patterns (A through R) for each of the 10 scenes. The length
of the bar indicates how often each of the 18 patterns is used.
which matches the behaviour of nine separate sub-
jects, consists only of uses of patterns A and F.
It remains to be seen to what extent a larger data
set would demonstrate more convergence; how-
ever, the point to be made at present is that any
attempt to predict the behaviour of a given speaker
by means of a model of referring behaviour is go-
ing to have to take account of a great deal of indi-
viual variation.
Nonetheless, we re-ran the J48 classifier de-
scribed in the previous section, this time using
the participant ID as well as the scene character-
istics in Table 2 as features. This improved pattern
prediction to 57.62%. This suggests that individ-
ual differences may indeed be capturable from the
data, although we would need more data than the
mere 10 examples we have from each subject to
learn a good predictive model.
In the face of this lack of data, another approach
is to look for commonalities in the data in terms
of the constituent elements of the different ref-
erence patterns used for each scene. This way
of thinking about the data was foreshadowed by
(Viethen and Dale, 2008b), who observed that the
subjects could be separated into those who always
used relations, those who never used relations, and
those who sometimes used relations. This leads
us to consider whether there are characteristics of
scenes or speakers which are highly likely to result
in specific attributes being used in descriptions. If
this is the case, a decision tree learner should be
able to learn for each individual attribute whether
it should be included in a given situation.
An appropriate baseline for any experiments
here is the success rate of simply including or not
including each attribute (basically a 0-R majority
class classifier), irrespective of the characteristics
of the scene. Table 4 compares the results for
this ?context-free? approach with one model that
is trained on the characteristics of scenes, and an-
other that takes both the characteristics of scenes
and the participant ID into account.5
Interestingly, the ?context-free? strategies work
suprisingly well for predicting the inclusion of
some attributes in the human data. As has been
noted in other work (see for example (Viethen et
al., 2008)), colour is often included in referring ex-
pressions irrespective of its discriminatory power,
and this is borne out by the data here. Perhaps
more suprising is the large degree to which the in-
clusion of landmark size is captured by a context-
free strategy.
5As before, the results reported are for the accuracy of a
pruned J48 decision tree, under 10-fold cross-validation.
62
Improvement on all attribues other than tar-
get colour improves when we take into account
the characteristics of the scenes, consistent with
our assumptions that context does matter. When
we add participant ID to the features used in the
learner, performance improves further still, indi-
cating that there are speaker-specific consistencies
in the data.
It is instructive to look at the rules learned on
the basis of the scene characteristics. Not surpris-
ingly, the rule derived for target colour inclusion is
simply to always include the colour (i.e., the same
context-free colour inclusion rule that proves most
effective in modelling the data without reference
to scene characteristics). The rules for including
the other attributes on the basis of scene charac-
teristics (but not participant ID) are shown in Fig-
ure 4.
The rules learned when we include participant
ID are more conplex, but can be summarised in a
way that demonstrates how this approach can re-
veal something about the variety of ways in which
speakers appear to approach the task of referring
expression generation. Focussing, as an example,
just on the question of whether or not to use the
target object?s colour in a referring expression, we
find the following:
? 48 participants always used colour, irrespec-
tive of the context (this corresponds to the
baseline rule learned above).
? The other participants always use colour if
the target and the landmark are of the same
type (which again is intuitively quite appro-
priate).
? When the landmark and the target are not
of the same type, we see more variation in
behaviour; 19 participants simply don?t use
colour, and the behaviour of seven can be
captured via a more complex analysis: four
use colour if the target and the distractor are
the same size, two use colour if the target and
distractor are of the same size and the target
is on top of the landmark, and one uses colour
if the target and distractor share colour.
Again, the specific details of the rules learned here
are probably not particularly significant, based as
they are on a limited data set and a set of stimuli
that may give elevated status to incidental proper-
ties. However, the general point remains that we
Target Size:
if tg type = dr type then include tg size
Relation:
if rel = on top of and lm size = dr size
then include rel
Landmark Colour:
if we have used a relation then include lm col
Landmark Size:
if we have used a relation and tg col = lm col
then include lm size
Figure 4: Rules learned on the basis of scene char-
acteristics
can use this kind of analysis to identify possible
rules for the inclusion of individual attributes in
referring expressions.
What this suggests is that we might be able to
capture the behaviour of individual speakers not
in terms of an overall strategy, but as a compos-
ite of heuristics, where each heuristic accounts for
the inclusion of a specific attribute. The rules, or
heuristics, shown in Figure 4 are just those which
are most successful in predicting the data; but
there can be many other rules that might be used
for the inclusion of particular attributes. So, for
example, I might be the kind of speaker who just
automatically includes the colour of an intended
referent without any analysis of the scene; and I
might be the kind of speaker who always uses a
relation to a nearby landmark in describing the in-
tended referent. Or I might be the kind of speaker
who surveys the scene and takes note of whether
the landmark?s colour is distinctive; and so on.
Thought of in this way, each speaker?s approach
to reference is like a set of ?parallel gestalts? that
contribute information to the description being
constructed. The particular rules for inclusion that
any speaker uses might vary depending on their
personal past history, and perhaps even on the ba-
sis of situation-specific factors that on a given oc-
casion might lean the speaker towards either being
?risky? or ?cautious? (Carletta, 1992).
As alluded to earlier, the specific content of the
rules shown in Figure 4 may appear idiosyncratic;
they are just what the limited data in the corpus
63
Pattern Sequence (?Scene#,DescriptionPattern?) Number of subjects
?1,A?, ?2,A?, ?3,G?, ?4,F?, ?5,A?, ?6,A?, ?7,A?, ?8,G?, ?9,F?, ?10,A? 2
?1,B?, ?2,B?, ?3,G?, ?4,H?, ?5,B?, ?6,B?, ?7,B?, ?8,G?, ?9,H?, ?10,B? 2
?1,N?, ?2,N?, ?3,K?, ?4,F?, ?5,A?, ?6,N?, ?7,N?, ?8,K?, ?9,F?, ?10,A? 6
?1,A?, ?2,A?, ?3,F?, ?4,F?, ?5,A?, ?6,A?, ?7,A?, ?8,F?, ?9,F?, ?10,A? 9
Table 3: Sequences of description patterns found more than once
Attribute to Include Baseline (0-R) Using Scene Using Scene
Characteristics Characteristics
and Participant
Target Colour 78.33% 78.33% 89.57%
Target Size 57.46% 90.85% 90.85%
Relation 64.04% 65.00% 81.22%
Landmark Colour 74.80% 87.31% 93.74%
Landmark Size 88.92% 95.02% 95.02%
Table 4: Accuracy of Learning Attribute Inclusion; statistically significant increases (p<.01) in bold.
supports, and some elements of the rules may be
due to artefacts of the specific stimuli used in the
data gathering. We would require a more diverse
set of stimuli to determine whether this is the case,
but the basic point stands: we can find correlations
between characteristics of the scenes and the pres-
ence or absence of particular attributes in referring
expressions, even if we cannot predict so well the
particular combinations of these correlations that
a given speaker will use in a given situation.
5 Related Work
There is a significant body of work on the use
of machine learning in referring expression gen-
eration, although typically focussed on aspects of
the problem that are distinct from those considered
here.
In the context of museum item descriptions,
Poesio et al (1999) explore the decision of what
type of referring expression NP to use to refer to
a given discourse entity, using a statistical model
to choose between using a proper name, a definite
description, or a pronoun. More recently, Stoia et
al. (2006) attempt a similar task, but this time in
an interactive navigational domain; as well as de-
termining what type of referring expression to use,
they also try to learn whether a modifier should be
included. Cheng et al (2001) try to learn rules for
the incorporation of non-referring modifiers into
noun phrases.
A number of the contributions to the 2008 GREC
and TUNA evaluation tasks (Gatt et al, 2008) have
made use of machine learning techniques. The
GREC task is primarily concerned with the choice
of form of reference (i.e. whether a proper name, a
descriptive NP or a pronoun should be used), and
so is less relevant to the focus of the present pa-
per. Much of the work on the TUNA Task is rel-
evant, however, since this also is concerned with
determining the content of referring expressions
in terms of the attributes used to build a distin-
guishing description. In particular, Fabbrizio et al
(2008) explore the impact of individual style and
priming on attribute selection for referring expres-
sion generation, and Bohnet (2008) uses a nearest-
neighbour learning technique to acquire an indi-
vidual referring expression generation model for
each person.
Other related approaches to attribute selection
in the context of the TUNA task are explored in
(Gerva?s et al, 2008; de Lucena and Paraboni,
2008; Kelleher and Mac Namee, 2008; King,
2008).
6 Conclusions
We know that people?s referential behaviour varies
significantly. Despite this apparent variation, we
have demonstrated above that there does appear to
be a reasonable correlation between characteristics
of the scene and the incorporation of particular at-
tributes in a referring expression. One way to con-
ceptualise this is that the decision as to whether or
64
not to incorporate a given feature such as colour
or size may vary from speaker to speaker; this is
evidenced by the data. We might think of these as
individual reference strategies; a good example of
such a strategy, widely attested across many exper-
iments, is the decision to include colour in a refer-
ring expression independent of its discriminatory
power, perhaps because it is an easily perceivable
and often-useful attribute. The overall approach to
reference that is demonstrated by a given speaker
then consists of the gathering together of a number
of strategies; the particular combinations may vary
from speaker to speaker, but as is demonstrated by
the analysis in this paper, some of the strategies
are widely used.
In current work, we are gathering a much larger
data set using more complex stimuli. This will al-
low the further development and testing of the ba-
sic ideas proposed in this paper as well as their
integration into a full referring expression genera-
tion algorithm.
References
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proceedings of UCNLG+MT: Language
Generation and Machine Translation, pages 75?83,
Copenhagen, Denmark.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2008. The GREC challenge 2008: Overview and
evaluation results. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 183?191, Salt Fork OH, USA.
Bernd Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion, pages 207?210, Salt Fork OH, USA.
Jean C. Carletta. 1992. Risk-taking and Recovery in
Task-Oriented Dialogue. Ph.D. thesis, University of
Edinburgh.
Hua Cheng, Massimo Poesio, Renate Henschel, and
Chris Mellish. 2001. Corpus-based NP modifier
generation. In Proceedings of the Second Meeting
of the North American Chapter of the Association
for Computational Linguistics, Pittsburgh PA, USA.
Diego Jesus de Lucena and Ivandre? Paraboni. 2008.
USP-EACH: Frequency-based greedy attribute se-
lection for referring expressions generation. In Pro-
ceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 219?220, Salt
Fork OH, USA.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the Fifth In-
ternational Natural Language Generation Confer-
ence, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA challenge 2008: Overview and evaluation re-
sults. In Proceedings of the Fifth International Nat-
ural Language Generation Conference, pages 198?
206, Salt Fork OH, USA.
Pablo Gerva?s, Raquel Herva?s, and Carlos Leo?n. 2008.
NIL-UCM: Most-frequent-value-first attribute se-
lection and best-scoring-choice realization. In Pro-
ceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 215?218, Salt
Fork OH, USA.
John D. Kelleher and Brian Mac Namee. 2008. Refer-
ring expression generation challenge 2008: DIT sys-
tem descriptions. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 221?223, Salt Fork OH, USA.
Josh King. 2008. OSU-GP: Attribute selection using
genetic programming. In Proceedings of the Fifth
International Natural Language Generation Confer-
ence, pages 225?226, Salt Fork OH, USA.
Massimo Poesio, Renate Henschel, Janet Hitzeman,
and Rodger Kibble. 1999. Statistical NP genera-
tion: A first report. In Proceedings of the ESSLLI
Workshop on NP Generation, Utrecht, The Nether-
lands.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Conference on Natural Lan-
guage Generation, pages 81?88, Sydney, Australia.
Jette Viethen and Robert Dale. 2008a. Generating
referring expressions: What makes a difference?
In Australasian Language Technology Association
Workshop 2008, pages 160?168, Hobart, Australia.
Jette Viethen and Robert Dale. 2008b. The use of
spatial relations in referring expression generation.
In Proceedings of the 5th International Conference
on Natural Language Generation, pages 59?67, Salt
Fork OH, USA.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t
Theune, and Pascal Touset. 2008. Controlling re-
dundancy in referring expressions. In Proceedings
of the 6th Language Resources and Evaluation Con-
ference, Marrakech, Morocco.
Ian H. Witten and Frank Eibe. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
65
Proceedings of the 12th European Workshop on Natural Language Generation, pages 183?184,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Realizing the Costs: Template-Based Surface Realisation in the GRAPH
Approach to Referring Expression Generation
Ivo Brugman
University of Twente
The Netherlands
i.h.g.brugman@student.utwente.nl
Marie?t Theune
University of Twente
The Netherlands
m.theune@utwente.nl
Emiel Krahmer
Tilburg University
The Netherlands
e.j.krahmer@uvt.nl
Jette Viethen
Macquarie University
Australia
jviethen@ics.mq.edu.au
Abstract
We describe a new realiser developed for
the TUNA 2009 Challenge, and present its
evaluation scores on the development set,
showing a clear increase in performance
compared to last year?s simple realiser.
1 Introduction
The TUNA Challenge 2009 is the last in a series
of challenges using the TUNA corpus of refer-
ring expressions (Gatt et al 2007) for compara-
tive evaluation of referring expression generation.
The 2009 Challenge is aimed at end-to-end re-
ferring expression generation, which encompasses
two subtasks: (1) attribute selection, choosing a
number of attributes that uniquely characterize a
target object, distinguishing it from other objects
in a visual scene, and (2) realisation, converting
the selected set of attributes into a word string.
Our contributions to the previous Challenges fo-
cused on subtask (1), but this year we focus on
subtask (2). Below, we briefly sketch how attribute
selection is performed in our system, describe our
newly developed realiser, and present our evalua-
tion results on the TUNA 2009 development set.
2 Attribute selection
We use the Graph-based algorithm of Krahmer
et al (2003) for attribute selection. In this ap-
proach, objects and their attributes are represented
in a graph as nodes and edges respectively, and
attribute selection is seen as a graph search prob-
lem that outputs the cheapest distinguishing graph,
given a particular cost function that assigns costs
to attributes. By assigning zero costs to some at-
tributes, e.g., the type of an object, the human
tendency to mention redundant properties can be
mimicked. For the TUNA Challenge 2009 we
use the same settings as last year (Krahmer et al
2008). The used cost function assigns a zero cost
to attributes that are highly frequent in the TUNA
corpus, while the other attributes have a cost of
either 1 (somewhat infrequent) or 2 (very infre-
quent). The order in which attributes are added
is also controlled: to ensure that the cheapest at-
tributes are added first, they are tried in the order
of their frequency in the TUNA (2008) training
corpus. Using these settings, last year the GRAPH
attribute selection algorithm made the top 3 on all
evaluation measures (Gatt et al 2008, Table 11).
3 Realisation
The main resource for realisation is a set of tem-
plates, derived from the human-produced object
descriptions in the TUNA 2009 training data. To
construct the templates, we first grouped the de-
scriptions by the combination of attributes they
expressed. For instance, in the domain of furni-
ture references, all descriptions expressing the at-
tributes colour, type and orientation were grouped
together. This was done for all combinations of
attributes. Next, for each description, parts of the
word string were related to the attributes in the set.
For instance, for the string ?red couch facing left?,
we linked ?red? to colour, ?couch? to type, and
?facing left? to orientation.1 This provided us with
information on how the attributes were expressed
(e.g., by adjectives or prepositional phrases) and
in which order they appeared in the word string.
For each combination of attributes, the surface or-
der that occurred most frequently was selected as
the basis for a template. If multiple orderings
were equally frequent, we chose the most natural-
seeming one. This resulted in templates such as
?the [colour] [type] facing [orientation]? for the at-
tribute set {type, colour, orientation}.
During realisation, the templates are used as fol-
1This corresponds to the ANNOTATED-WORD-STRING
nodes already present in the TUNA corpus. Unfortunately,
various problems prevented us from automatically deriving
our templates from those existing annotations.
183
lows. When a set of attributes is input to the re-
aliser, it checks if there is a template matching this
particular attribute combination. If so, the tem-
plate is selected, and the gaps in the template are
filled with lexical expressions for the attribute val-
ues. The words used to express the values are
those that occurred most frequently in the train-
ing data for this particular template. If no match-
ing template is found, a description is generated
in a simple rule-based fashion, based on the re-
aliser we used last year, but with improved lexical
choices. For example, the old realiser always used
the word ?person? to express the type attribute in
descriptions of people, whereas in the TUNA cor-
pus ?man? is used most frequently. We changed
the realiser to reflect such human preferences.
Template construction for the furniture domain
was fairly straightforward, resulting in 25 tem-
plates. In practice, only 13 of these are used. Since
the GRAPH attribute selection algorithm adds the
type and colour attributes to a description for free,
these attributes are always selected, making any
templates lacking them irrelevant given the current
settings of the algorithm.
For the more realistic people domain, template
construction was more complicated. For exam-
ple, when the hairColour attribute is mentioned in
human descriptions it can refer either to the hair
on a person?s head (?white-haired?) or his beard
(?with a white beard?). The attribute selection al-
gorithm does not make this distinction, leaving it
unclear which of the two realisations should be
used when hairColour and hasBeard attributes are
both to be included in a description. We solved
this by simply using the expression that occurred
most frequently in the training data for each at-
tribute combination, even allowing hairColour to
be mentioned twice if this happened in most hu-
man descriptions. Another problem is that many
attribute combinations occurred only once in the
training data, leading to a very large number (50+)
of potential templates. We reduced this number in
an ad hoc manner, by ignoring combinations in-
volving attributes (such as hasHair) that are very
unlikely to be selected given the current settings
of the attribute selection algorithm. This approach
left us with 40 templates in the people domain.
4 Evaluation
System performance is measured by comparing
the generated word strings to the human descrip-
MED MNED BLEU 3
Furniture 4.94 (5.48) 0.48 (0.50) 0.27 (0.22)
People 5.15 (7.53) 0.46 (0.67) 0.33 (0.07)
Overall 5.03 (6.42) 0.47 (0.58) 0.30 (0.15)
Table 1: Results on the 2009 development set (be-
tween brackets are those using last year?s realiser).
tions in the TUNA development set, comprising
80 furniture and 68 people descriptions. The eval-
uation measures reported here are mean edit dis-
tance (MED), the mean of the token-based Lev-
enshtein edit distance between the reference word
strings and the system word strings, mean nor-
malised edit distance (MNED), where the edit dis-
tance is normalised by the number of tokens, and
cumulative BLEU 3 score. Table 1 summarizes
our evaluation results. For comparison, we also
provide the results obtained when using last year?s
simple realiser, which we reimplemented in Java.
We see a clear improvement when we compare
the performance of the new and the old realiser, in
particular in the people domain. However, further
evaluation experiments are required to determine
whether the improvements are mostly due to our
use of templates derived from human descriptions,
or to the simple improvements in lexical choice
incorporated in the rules used as fall-back in case
no matching templates are found.
To further improve the realiser, we need to add
templates for all remaining attribute combinations
found in the corpus. This should not be difficult,
as the set-up of the realiser allows easy creation of
templates. It should also be easily portable to other
languages; in fact we intend to explore its use for
the realisation of referring expressions in Dutch.
References
Gatt, A., I. van der Sluis and K. van Deemter 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. Proceedings of
ENLG 2007 49-56.
Gatt, A., A. Belz and E. Kow 2008. The TUNA chal-
lenge 2008: Overview and evaluation results Pro-
ceedings of INLG 2008 198-206.
Krahmer, E., S. van Erk and A. Verleg 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1), 53-72.
Krahmer, E., M. Theune, J. Viethen, and I. Hendrickx
2008. GRAPH: The costs of redundancy in referring
expressions. Proceedings of INLG 2008 227-229.
184
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 79?87,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
The GREC Main Subject Reference Generation Challenge 2009:
Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
The GREC-MSR Task at Generation Chal-
lenges 2009 required participating systems
to select coreference chains to the main
subject of short encyclopaedic texts col-
lected from Wikipedia. Three teams sub-
mitted one system each, and we addition-
ally created four baseline systems. Sys-
tems were tested automatically using ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying corefer-
ence resolution tools to the outputs and
measuring the success of the tools. In ad-
dition, systems were tested in an intrinsic
evaluation involving human judges. This
report describes the GREC-MSR Task and
the evaluation methods applied, gives brief
descriptions of the participating systems,
and presents the evaluation results.
1 Introduction
The GREC-MSR Task is about how to generate ap-
propriate references to an entity in the context of a
piece of discourse longer than a sentence. Rather
than requiring participants to generate referring
expressions from scratch, the GREC-MSR data pro-
vides sets of possible referring expressions for se-
lection. This was the second time we ran a shared
task using the GREC-MSR data (following a first
run in 2008). The task definition was again kept
fairly simple, but in the 2009 round the main aim
for participating systems was to select an appro-
priate word string to serve as a referring expres-
sion, whereas in 2008 it was to select an appropri-
ate type of referring expression (name, common
noun, pronoun, or empty reference).
The immediate motivating application context
for the GREC-MSR Task is the improvement of ref-
erential clarity and coherence in extractive sum-
maries by regenerating referring expressions in
them. There has recently been a small flurry
of work in this area (Steinberger et al, 2007;
Nenkova, 2008). In the longer term, the GREC-
MSR Task is intended to be a step in the direction
of the more general task of generating referential
expressions in discourse context.
The GREC-MSR data is an extension of the
GREC 1.0 Corpus which had about 1,000 texts in
the subdomains of cities, countries, rivers and peo-
ple (Belz and Varges, 2007a). For the purpose of
the GREC-MSR shared task, an additional 1,000
texts in the new subdomain of mountain texts were
obtained and a new XML annotation scheme (Sec-
tion 2.2) was developed.
Team System Name
University of Delaware UDel
ICSI, Berkeley ICSI-CRF
Jadavpur University JUNLG
Table 1: GREC-MSR?09 participating teams.
Nine teams from seven countries registered for
GREC-MSR?09, of which three teams (Table 1)
submitted one system each.1 Participants had to
submit their system reports before downloading
test data inputs, and had to submit test data out-
puts within 48 hours of downloading the test data
inputs. In addition to the participants? systems,
we also used the corpus texts themselves as ?sys-
tem? outputs, and created 4 baseline systems; we
evaluated the resulting 8 systems using a range of
intrinsic and extrinsic evaluation methods (for de-
tails see Sections 5 and 6). This report presents the
results of all evaluations (Section 6), along with
descriptions of GREC-MSR data and task (Sec-
tion 2), test sets (Section 3), evaluation methods
(Section 4), and participating systems (Section 5).
2 Data and Task
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introduc-
1One team submitted by the original deadline (Jan. 2009),
one by the revised deadline (1 June 2009), one slightly later.
79
tory sections in Wikipedia articles, in five different
subdomains (cities, countries, rivers, people and
mountains). In each text, three broad categories
of Main Subject Reference (MSR)2 have been an-
notated, resulting in a total of about 13,000 anno-
tated REs. The GREC-MSR shared task version of
the corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results
on the development data.
2.1 Types of referential expression annotated
Three broad categories of main subject referring
expressions (MSREs) are annotated in the GREC
corpus3 ? subject NPs, object NPs, and geni-
tive NPs and pronouns which function as subject-
determiners within their matrix NP. These cate-
gories of referring expressions (RE) are relatively
straightforward to identify and to achieve high
inter-annotator agreement on (complete agree-
ment among four annotators in 86% of MSRs), and
account for most cases of overt main subject refer-
ence in the GREC texts. The annotators were asked
to identify subject, object and genitive subject-
determiners and decide whether or not they refer
to the main subject of the text. More detail is pro-
vided in Belz and Varges (2007b).
In addition to the above, relative pronouns in
supplementary relative clauses (as opposed to in-
tegrated relative clauses, Huddleston and Pullum,
2002, p. 1058) were annotated, e.g.:
(1) Stoichkov is a football manager and former striker
who was a member of the Bulgaria national team that
finished fourth at the 1994 FIFA World Cup.
We also annotated ?non-realised? subject MSREs
in those cases of VP coordination where an MSRE
is the subject of the coordinated VPs, e.g.:
(2) He stated the first version of the Law of conservation
of mass, introduced the Metric system, and
helped to reform chemical nomenclature.
The motivation for annotating the approximate
place where the subject NP would be if it were
realised (the gap-like underscores above) is that
from a generation perspective there is a choice to
be made about whether to realise the subject NP in
the second and third coordinates or not.
2The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
(and title) of one text is London.
3In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
2.2 XML format
Figure 1 is one of the texts distributed in the
GREC-MSR training/development data set. The
REF element indicates a reference, in the sense of
?an instance of referring? (which could, in princi-
ple, be realised by gesture or graphically, as well
as by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cate-
gory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX ele-
ment (the ?selected? referential expression for the
given reference; in the training/development data
texts it is simply the referential expression found
in the corpus) and one ALT-REFEX element which
in turn is a list of REFEXs which are possible alter-
native referential expressions (see following sec-
tion).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in
the GREC-MSR corpus, the only type of RE that
has EMPHATIC=yes is one which incorporates a re-
flexive pronoun used emphatically (e.g. India it-
self ). The REG08-TYPE attribute indicates basic RE
type. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether
to use an RE that includes a name, such as Mod-
ern India (the corresponding REG08-TYPE value
is name); whether to go for a common-noun RE,
i.e. with a category noun like country as the head
(common); whether to use a pronoun (pronoun); or
whether it can be left unrealised (empty).
2.3 The GREC-MSR Task
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to
the training/development data, except that REF el-
ements contained only an ALT-REFEX list, not the
preceding ?selected? REFEX. ALT-REFEX lists are
generated for each text by an automatic method
80
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TEXT SYSTEM "reg08-grec.dtd">
<TEXT ID="36">
<TITLE>Jean Baudrillard</TITLE>
<PARAGRAPH>
<REF ID="36.1" SEMCAT="person" SYNCAT="np-subj">
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX>
<REFEX REG08-TYPE="empty">_</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX>
</ALT-REFEX>
</REF>
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det">
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX>
</ALT-REFEX>
</REF>
work is frequently associated with postmodernism and post-structuralism.
</PARAGRAPH>
</TEXT>
Figure 1: Example text from the GREC-MSR Training Data.
which collects all the (manually annotated) MSREs
in a text including the title, and adds several de-
faults: pronouns and reflexive pronouns in all sub-
domains; and category nouns (e.g. the river), in
all subdomains except people. The main objec-
tive in the 2009 GREC-MSR Task was to get the
word strings contained in REFEXs right (whereas
in REG?08 it was the REG08-TYPE attributes).
3 Test Data
1. Test Set C-1: a randomly selected 10% sub-
set (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. Test Set C-2: the same subset of texts as in C-
1; however, for C-2 we did not use the MSREs in
the corpus, but replaced them with human-selected
alternatives. These were obtained in an online ex-
periment as described in Belz & Varges (2007a)
where subjects selected MSREs in a setting that du-
plicated the conditions in which the participating
systems in the GREC-MSR Task make selections.4
We obtained three versions of each text, where in
each version all MSREs were selected by the same
person. The motivation for this version of Test Set
C was that having several human-produced chains
of MSREs to compare the outputs of participating
(?peer?) systems against is more reliable than hav-
ing one only; and that Wikipedia texts are edited
4The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
by multiple authors which sometimes adversely
affects MSR chains; we wanted to have additional
reference texts where all references are selected by
a single author.
3. Test Set L: 74 Wikipedia introductory texts
from the subdomain of lakes (there were no lake
texts in the training/development set).
4. Test Set P: 31 short encyclopaedic texts in
the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but of different origin. We
transcribed these texts from printed encyclopae-
dias published in the 1980s which are not avail-
able in electronic form. The texts in this set are
much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs fol-
low very similar patterns. It seems likely that it is
these properties that have resulted in better scores
overall for Test Set P than for the other test sets
in both the 2008 and 2009 runs of the GREC-MSR
task (for the latter, see Section 6).
Each test set was designed to test peer systems
for generalisation to different kinds of unseen data.
Test Set C tests for generalisation to unseen ma-
terial from the same corpus and the same subdo-
mains as the training set; Test Set L tests for gen-
eralisation to unseen material from the same cor-
pus but different subdomain; and Test Set P for
generalisation to a different corpus but the same
subdomains.
81
4 Evaluation methods
4.1 Automatic intrinsic evaluations5
Accuracy of REFEX word strings: when com-
puted against test sets (C-1, L and P), Word String
Accuracy is simply the proportion of REFEX word
strings selected by a participating system that are
identical to the one in the corpus. When computed
against test set C-2, which has three versions of
each text, Word String Accuracy is computed as
follows: first the number of correct REFEX word
strings is computed at the text level for each of the
three versions of a text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number
of REFs in all the texts, which gives the global
Word String Accuracy score. The rationale be-
hind computing the Word String Accuracy scores
in this way for multiple-RE test sets (maximising
scores on RE chains rather than individual REs) is
that an RE is not good or bad in its own right, but
depends on other MSREs in the same text.
Accuracy of REG08-Type: similarly to Word
String Accuracy above, when computed against
test sets C-1, L and P, REG08-Type Accuracy is the
proportion of REFEXs selected by a participating
system that have a REG08-TYPE value identical to
the one in the corpus. When computed against test
set C-2, first the number of correct REG08-Types is
computed at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total num-
ber of REFs in all the texts, which gives the global
REG08-Type Accuracy score.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
cost of 1. We also used a length-normalised ver-
sion of string-edit distance (denoted ?norm. SE? in
results tables below). For test sets C-1, L and P,
the global score is simply the mean of all RE-level
scores. For Test Set C-2, the global score is the
mean of the mean of the three text-level scores.
Other metrics: BLEU is a precision metric
from machine translation that assesses peer trans-
lations in terms of the proportion of word n-grams
5For GREC-MSR?09 we updated the tool that computes all
automatic intrinsic scores and in the course of this eliminated
a character encoding issue; as a result the results for baseline
systems and corpus texts reported here are on the whole very
slightly higher than those reported for GREC-MSR?08.
(n ? 4 is standard) they share with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in
the corpus are less than 4 tokens long. We also
used the NIST version of BLEU which weights in
favour of less frequent n-grams. In both cases,
we assessed just the MSREs selected by peer sys-
tems (leaving out the surrounding text), and com-
puted scores globally (rather than averaging over
RE-level scores), as this is standard for these met-
rics. BLEU, and NIST are designed to work with
one or multiple reference texts, so we did not need
to use a different method for Test Set C-2.
4.2 Automatic extrinsic evaluation
As in GREC-MSR?08, we used an automatic ex-
trinsic evaluation method based on coreference
resolution performance.6 The basic idea is that it
seems likely that badly chosen reference chains af-
fect the ability to resolve REs in automatic coref-
erence resolution tools which will tend to perform
worse with poorly selected MSR reference chains.
To counteract the possibility of results being a
function of a specific coreference resolution algo-
rithm or tool, we used two different resolvers?
those included in LingPipe7 and OpenNLP (Mor-
ton, 2005)?and averaged results.
There does not appear to be a single standard
evaluation metric in the coreference resolution
community, so we opted to use three: MUC-6
(Vilain et al, 1995), CEAF (Luo, 2005), and B-
CUBED (Bagga and Baldwin, 1998), which seem
to be the most widely accepted metrics. All three
metrics compute Recall, Precision and F-Scores
on aligned gold-standard and resolver-tool coref-
erence chains. They differ in how the alignment
is obtained and what components of coreference
chains are counted for calculating scores. Results
for the automatic extrinsic evaluations are reported
below in terms of the F-Scores from these three
metrics, as well as in terms of their mean.
4.3 Human intrinsic evaluation
The intrinsic human evaluation involved 24 ran-
domly selected items from Test Set C and outputs
for these produced by peer and basline systems as
6However, for GREC?09 we overhauled the tool; the cur-
rent version no longer uses JavaRAP, and uses the most recent
versions of the other resolvers; the GREC-MSR?08 and GREC-
MSR?09 results for this method are not entirely comparable
for this reason.
7http://alias-i.com/lingpipe/
82
Figure 2: Example of text presented in human intrinsic evaluation of GREC-MSR systems.
well as those found in the original corpus texts
(8 systems in total). We used a Repeated Latin
Squares design which ensures that each subject
sees the same number of outputs from each sys-
tem and for each test set item. There were three
8x8 squares, and a total of 576 individual judg-
ments in this evaluation (72 per system: 3 criteria
x 3 articles x 8 evaluators).
We recruited 8 native speakers of English from
among post-graduate students currently doing a
linguistics-related degree at University College
London (UCL) and University of Sussex.
Following detailed instructions, subjects did
two practice examples, followed by the 24 texts
to be evaluated, in random order. Subjects carried
out the evaluation over the internet, at a time and
place of their choosing. They were allowed to in-
terrupt and resume the experiment (though discor-
ouged from doing so). According to self-reported
timings, subjects took between 25 and 45 minutes
to complete the evaluation (not counting breaks).
Figure 2 shows what subjects saw during the
evaluation of an individual text. All references to
the MS are highlighted in yellow, and the task is to
evaluate the quality of the REs in terms of three cri-
teria which were explained in the introduction as
follows (the wording of the explanations of Crite-
ria 1 and 3 were taken from the DUC evaluations):
1. Referential Clarity: It should be easy to identify who
or what the referring expressions in the text are refer-
ring to. If a person or other entity is mentioned, it
should be clear what their role in the story is. So, a ref-
erence would be unclear if an entity is referenced, but
their identity or relation to the story remains unclear.
2. Fluency: A referring expression should ?read well?, i.e.
it should be written in good, clear English, and the use
of titles and names etc. should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
3. Structure and Coherence: The text should be well
structured and well organised. The text should not just
be a heap of related information, but should build from
sentence to sentence to a coherent body of information
about a topic. This criterion too is independent of the
others.
Subjects selected evaluation scores by moving
sliders (see Figure 2) along scales ranging from 1
to 5. Slider pointers started out in the middle of
the scale (3). These were continuous scales and
we recorded scores with one decimal place (e.g.
3.2). The meaning of the numbers was explained
in terms of integer scores (1=very poor, 2=poor,
3=neither poor nor good, 4=good, 5=very good).
5 Systems
Base-rand, Base-freq, Base-1st, Base-name:
Baseline system Base-rand selects one of the
REFEXs at random. Base-freq selects the REFEX
that is the overall most frequent given the SYNCAT
and SEMCAT of the reference. Base-1st al-
ways selects the REFEX which appears first in
the ALT-REFEX list; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.8
8Attributes are considered in this order. If for one at-
tribute, the right value is not found, the process ignores that
attribute and moves on the next one.
83
UDel: The UDel system consists of a prepro-
cessing component performing sentence segmen-
tation and identification of non-referring occur-
rences of main subject (MS) names, an RE type
selection component (two C5.0 decision trees, one
optimised for people and mountains, the other for
the other subdomains), and a word string selec-
tion component. The RE type selection decision
trees use the following features: is the MS the sub-
ject of the current, preceding and preceding but
one sentence; was the last MSR in subject position;
are there interfering references to other entities be-
tween the current and the previous MSR; distance
to preceding non-referring occurrences of an MS
name; sentence and reference IDs; other features
indicating whether the reference occurred before
and after certain words and punctuation marks.
Given a selected RE type, the word-string selec-
tion component selects the longest non-emphatic
name for the first named reference in an article,
and the shortest for subsequent named references;
for other types, the first matching word-string is
used, backing off to pronoun or name.
ICSI-CRF: The ICSI-CRF system construes the
GREC-MSR task as a sequence labelling task and
determines the most likely current label given pre-
ceding labels using a Conditional Random Field
model trained using the follow features for the cur-
rent, preceding and preceding but one MSR: pre-
ceding and following word unigram and bigram;
suffix of preceding and following word; preceding
and following punctuation; reference ID; is this is
the beginning of a paragraph. If more than one la-
bel remains, the last in the list of possible REs in
the GREC-MSR data is selected.
JUNLG: The JUNLG system is based on co-
occurrence statistics between REF feature sets and
REFEX feature sets as found in the GREC-MSR data.
REF feature sets were augmented by a paragraph
counter and a within-paragraph REF counter. For
each given set of REF features, the system selects
the most frequent REFEX feature set (as determined
from co-occurrence counts in the training data). If
the current set of possible REFEXs does not include
a REFEX with the selected feature set, then the sec-
ond most likely feature set is selected. Several
hand-coded default rules override the frequency-
based selections, e.g. if the preceding word is a
conjunction, and the current SYNCAT is np-subj,
then the REG08-Type is empty.
6 Results
This section presents the results of all evalua-
tion methods described in Section 4. We start
with Word String Accuracy, the intrinsic auto-
matic metric which participating teams were told
was going to be the chief evaluation method, fol-
lowed by REG08-Type Accuracy and other intrin-
sic automatic metrics (Section 6.2), the intrinsic
human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
System Word String Acc. REG08-Type Acc. Norm. Edit Dist.
ICSI-CRF 0.67 0.75 0.28
UDel 0.6357 0.7027 0.3383
JUNLG 0.532 0.62 0.421
Table 2: Self-reported evaluation scores for devel-
opment set.
6.1 Word String Accuracy
Participants computed Word String Accuracy for
the development set (97 texts) themselves, using
an evaluation tool provided by us. These scores
are shown in column 2 of Table 2, and are also
included in the participants? reports in this vol-
ume. Corresponding results for test set C-1 are
shown in column 2 of Table 3. Surprisingly, Word
String Accuracy results on the test data are better
(than on the development data) for the UDel and
JUNLG systems. Also included in this table are re-
sults for the four baseline systems, and it is clear
that selecting the most frequent word string given
SEMCAT and SYNCAT (as done by the Base-freq sys-
tem) provides a strong baseline.
The other two parts of Table 3 contain results for
test sets L and P. As expected, results for Test Set L
are lower than for Test Set C-1, because in addition
to consisting of unseen texts (like C-1), Test Set L
is also from an unseen subdomain (unlike C-1).
The Word String Accuracy results for Test Set P
are higher than for any other set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 3 we carried out a
univariate ANOVA with System as the fixed factor,
?Number of REFEXs in a text? as a random factor,
and Word String Accuracy as the dependent vari-
able. We found significant main effects of Sys-
tem on Word String Accuracy at p < .001 in the
case of all three test sets (C-1: F(7,1272) = 90.058;
L: F(7,440) = 44.139; P: F(7,168) = 21.991).9
The columns containing capital letters in Table 3
9We included the corpus texts themselves in the analysis,
hence 7 degrees of freedom (8 systems).
84
Test Set C-1 Test Set L Test Set P
UDel 67.68 A UDel 52.89 A UDel 77.16 A
ICSI-CRF 62.98 A JUNLG 50.80 A ICSI-CRF 72.22 A
JUNLG 61.94 A ICSI-CRF 49.20 A JUNLG 71.60 A
Base-freq 47.05 B Base-name 21.06 B Base-freq 53.09 B
Base-name 28.74 C Base-freq 20.74 B Base-name 27.78 C
Base-1st 28.26 C Base-1st 20.74 B Base-1st 27.16 C
Base-rand 18.95 D Base-rand 15.11 B Base-rand 18.52 C
Table 3: Word String Accuracy scores against Test Sets C-1, L and P; homogeneous subsets (Tukey HSD,
alpha = .05) for each test set (systems that do not share a letter are significantly different).
System Word String Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People Mountains
Corpus 71.58 A 65.25 69.11 76.47 80.40 66.87
UDel 70.22 A B 68.09 71.20 76.47 76.63 64.84
JUNLG 64.57 B C 54.61 51.83 73.53 71.86 65.85
ICSI-CRF 63.69 C 58.87 56.54 64.71 72.11 60.98
Base-freq 57.01 D 51.06 57.07 58.82 63.82 53.05
Base-name 40.21 E 51.06 46.07 29.41 29.90 43.90
Base-1st 39.65 E 47.52 41.88 38.24 25.63 47.97
Base-rand 26.99 F 28.37 29.32 23.53 21.61 30.28
Table 4: Word String Accuracy scores against Test Set C-2 for complete set and for subdomains; homo-
geneous subsets (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are
significantly different).
show the homogeneous subsets of systems as de-
termined by post-hoc Tukey HSD comparisons of
means. Systems whose Word String Accuracy
scores are not significantly different (at the .05
level) share a letter.
The results for Word String Accuracy com-
puted against Test Set C-2 are shown in Table 4.
These should be considered the chief results of the
GREC-MSR?09 Task evaluations, as stated in the
participants? guidelines. Here too we performed
a univariate ANOVA with System as the fixed fac-
tor, Number of REFEXs as the random factor and
Word String Accuracy as the dependent variable.
There was a significant main effect of System
(F(7,1272) = 74.892, p < .001). We compared the
mean scores with Tukey?s HSD. As can be seen
from the resulting homogeneous subsets, there is
no significant difference between the corpus texts
(C-1) and the UDel system, but also there is no
significant difference between the latter and the
JUNLG system. In this analysis, all peer systems
outperform all baselines; the Base-freq baseline
outperforms all other baselines; and Base-name
and Base-1st outperform the random baseline.
Overall, there is a marked improvement in Word
String Accuracy compared to GREC-MSR?08
where peer systems? scores ranged from 50.72 to
65.61.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure re-
ported on in the preceding section, we computed
REG08-Type Accuracy and the string similarity
metrics described in Section 4.1. The resulting
scores for Test Set C-2 are shown in Table 5 (re-
call that in Test Set C-2 corpus texts are evalu-
ated against 3 texts with human-selected alterna-
tive REs). The corpus texts again receive the best
scores across the board. Ranks for peer systems
are very similar to those reported in the last sec-
tion.
We performed a univariate ANOVA with Sys-
tem as the fixed factor, Number of REFEXs as the
random factor, and REG08-Type Accuracy as the
dependent variable. The main effect of System
was F(7,1272) = 75.040, p < .001; the homoge-
neous subsets resulting from the Tukey HSD post-
hoc analysis are shown in columns 3?5 of Table 5.
The differences between the scores of the peer sys-
tems and the corpus texts were not found to be sig-
nificant.
6.3 Human-assessed intrinsic measures
Table 6 shows the results of the human intrinsic
evaluation. In each of the three parts of the ta-
ble (showing the results for Fluency, Clarity and
Coherence, respectively) systems are ordered in
terms of their mean scores (shown in the second
column of each part of the table). We first es-
tablished that the main effect of Evaluator was
weak (F between 2.1 and 2.6) on Fluency, Clar-
ity and Coherence, and only of borderline signifi-
cance (just below .05); and that the interaction be-
tween System and Evaluator was very weak and
85
System Other similarity measures for Triple-RE Test Set C-2
REG08-Type BLEU-3 NIST SE norm. SE
Corpus 79.30 A 0.77 5.60 1.04 0.34
UDel 77.71 A 0.74 5.32 1.11 0.37
JUNLG 75.40 A 0.53 4.69 1.34 0.40
ICSI-CRF 75.16 A 0.54 4.68 1.32 0.41
Base-freq 62.50 B 0.54 4.30 1.93 0.50
Base-name 51.04 C 0.46 4.76 1.80 0.63
Base-1st 50.32 C 0.39 4.42 1.93 0.63
Base-rand 48.09 C 0.26 3.02 2.30 0.72
Table 5: REG08-Type Accuracy, BLEU, NIST and string-edit scores, computed on test set C-2 (systems
in order of REG08-Type Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for REG08-Type
Accuracy only (systems that do not share a letter are significantly different).
Fluency Clarity Coherence
Corpus 4.43 A Base-name 4.62 A Corpus 4.40 A
UDel 4.27 A Corpus 4.56 A JUNLG 4.33 A
JUNLG 4.26 A JUNLG 4.50 A UDel 4.27 A B
ICSI-CRF 4.15 A B ICSI-CRF 4.45 A ICSI-CRF 4.02 A B
Base-freq 3.33 B C UDel 4.35 A Base-freq 3.96 A B
Base-name 2.84 C D Base-1st 4.27 A Base-name 3.85 A B
Base-1st 2.76 C D Base-freq 4.10 A Base-1st 3.7 A B
Base-rand 2.15 D Base-rand 3.18 B Base-rand 3.46 B
Table 6: Clarity, Fluency and Coherence scores (with homogeneous subsets) for all systems.
not significant in the case of Clarity and Coher-
ence, and borderline significant in the case of Flu-
ency. We then ran a (non-factorial) multivariate
ANOVA, with Fluency, Coherence and Clarity as
the dependent variables, and (just) System as the
fixed factor. The main effect of System was as
follows: Fluency: F(7,128) = 20.444, p < 0.001;
Clarity: F(7,128) = 5.248, p < 0.001; Coherence:
F(7,128) = 2.680, p < 0.012. The homogeneous
subsets resulting from a post-hoc Tukey analysis
are shown in the letter columns in Table 6.
The effect of System was strongest on Fluency;
here, the system ranks are also the same as for
Word String Accuracy and REG08-Type Accuracy
for Test Set C-2. This, together with the fair
amount of significant differences found, indicates
that the evaluators were able to make sense of the
Fluency criterion and that there were interesting
differences between systems under this criterion.
However, differences between the three peer sys-
tems were not significant.
For Clarity, there were no significant differ-
ences among the peer systems and non-random
baseline systems; all of these were significantly
better than the random baseline. Base-name had
the highest mean Clarity score, possibly because
always chosing the name of an entity when refer-
ring to it ensures high referential clarity.
The Coherence results are perhaps the most dif-
ficult to interpret. Both the main effect of System
on Coherence and its significance were weaker
than for Fluency and Clarity. Only two signifi-
cant pairwise differences were found: Corpus and
JUNLG were better than the random baseline. The
system ranks are roughly the same as for Fluency,
but the mean scores cover a smaller range (from
3.46 to 4.4) than in the case of either of the other
two criteria. Overall, the Coherence results proba-
bly indicate that the evaluators found it somewhat
difficult to make sense of the Coherence criterion.
Computing Pearson?s r for the three criteria
on individual (text-level) scores showed that there
were only moderate correlations between them (all
around r = 0.5) which were all significant at
? = 0.05. This gives some indication that the
evaluators were able to assess the three criteria in-
dependently from each other.
6.4 Automatic extrinsic measures
We fed the outputs of all eight systems through
the two coreference resolvers, and computed mean
MUC, CEAF and B-CUBED F-Scores as described
in Section 4.2. The second column in Table 7
shows the mean of these three F-Scores, to give
a single overall result for this evaluation method.
A univariate ANOVA with mean F-Score as the de-
pendent variable and System as the fixed factor
revealed a significant main effect of System on
mean F-Score (F(7,1456) = 73.061, p < .001).
A post-hoc comparison of the means (Tukey HSD,
alpha = .05) found the significant differences in-
dicated by the homogeneous subsets in columns
3?4 (Table 7). The numbers shown in the last
three columns are the separate MUC, CEAF and B-
CUBED F-Scores for each system, averaged over
the two resolver tools. ANOVAs revealed the fol-
86
lowing effects of System on the separate scoring
methods: on CEAF F(7,1456) = 43.471, p < .001;
on MUC: F(7,1456) =, p < .001; on B-CUBED:
F(7,1456) = 38.574, p < .001. All three scor-
ing methods separately and their mean yielded the
same significant differences (as shown in columns
3?4 of Table 7).
The three F-Score measures (MUC, CEAF and B-
CUBED) are all significantly correlated (p < .001,
2-tailed). However it is not a strong correlation,
with Pearson?s correlation coefficient around 0.5.
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-name 65.19 A 62.35 63.14 70.06
Base-1st 63.77 A 59.95 62.08 69.28
Base-freq 63.14 A 59.08 62.04 68.3
UDel 46.19 B 34.85 46.86 56.86
ICSI-CRF 44.47 B 31.61 45.58 56.21
JUNLG 44.19 B 31.27 45.21 56.10
Base-rand 42.99 B 30.24 43.04 55.7
Corpus 42.52 B 29.53 43.57 54.47
Table 7: MUC, CEAF and B-CUBED F-Scores for
all systems; homogeneous subsets (Tukey HSD),
alpha = .05, for mean of F-Scores.
6.5 Correlations
When assessed on the system-level scores and us-
ing Pearson?s r, all evaluation methods above were
strongly and significantly correlated with each
other (at the 0.01 level, 2-tailed), with the fol-
lowing exceptions. Clarity was not significantly
correlated with any of the other methods except
NIST (r = .902, p < .01); apart from this, NIST
was only correlated with Word String Accuracy on
test set C-2, with non-normalised string-edit dis-
tance, Fluency and Coherence, moreover all at the
weaker 0.05 level. Finally, the extrinsic method
was not correlated with any of the intrinsic meth-
ods (and in fact showed signs of being negatively
correlated with all of them except Clarity).
7 Concluding Remarks
The GREC-MSR Task is still a relatively new task
not only for an NLG shared-task challenge, but also
as a research task in general (post-processing ex-
tractive summaries in order to improve their qual-
ity seems to be just taking off as a research sub-
field). There was substantial interest in the GREC-
MSR Task this year (as indicated by the nine teams
that originally registered). However, only three
teams were ultimately able to participate.
We continued the traditions of previous NLG
shared tasks in that we used a wide range of eval-
uation metrics to obtain a well-rounded view of
the quality of the participating systems. This in-
cluded intrinsic human evaluations for the first
time. However, we decided against an extrinsic
human evaluation this year, given time constraints
as well as the fact that this evaluation type yielded
barely any significant results last year.
Overall, there was an improvement in system
performance compared to last year, to the point
where the performance of the top system was
barely distinguishable from the human topline.
We are not currently planning to run the GREC-
MSR task again next year.
Acknowledgments
Many thanks to the UCL and Sussex students who
participated in the intrinsic evaluation experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC?98, pages
563?566.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz and S. Varges. 2007b. The GREC corpus:
Main subject reference in context. Technical Report
NLTG-07-01, University of Brighton.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of
Pensylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP?08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public refer-
ence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC?04, pages 291?
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management:
Special issue on Summarization, 43(6):1663?1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings of MUC-6, pages
45?52.
87
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 88?98,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
The GREC Named Entity Generation Challenge 2009:
Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Abstract
The GREC-NEG Task at Generation Chal-
lenges 2009 required participating sys-
tems to select coreference chains for all
people entities mentioned in short en-
cyclopaedic texts about people collected
from Wikipedia. Three teams submitted
six systems in total, and we additionally
created four baseline systems. Systems
were tested automatically using a range of
existing intrinsic metrics. We also eval-
uated systems extrinsically by applying
coreference resolution tools to the outputs
and measuring the success of the tools.
In addition, systems were tested in an in-
trinsic evaluation involving human judges.
This report describes the GREC-NEG Task
and the evaluation methods applied, gives
brief descriptions of the participating sys-
tems, and presents the evaluation results.
1 Introduction
The GREC-NEG task is about how to generate ap-
propriate references to people entities in the con-
text of a piece of discourse longer than a sentence.
Rather than requiring participants to generate re-
ferring expressions (REs) from scratch, the GREC-
NEG data provides sets of possible REs for selec-
tion. This was the first time we ran a shared task
using this data. GREC-NEG is a step further from
the related GREC-MSR Task in that it requires sys-
tems to generate plural as well as singular refer-
ences, for all people entities mentioned in a text
(GREC-MSR in contrast only had singular refer-
ences to a single entity). Moreover in GREC-NEG,
possible REs for each entity are provided as one set
for each entity (rather than one set for each con-
text), so the task of selecting an appropriate RE
for a given context is harder than in GREC-MSR.
The main aim for participating systems in GREC-
NEG?09 was to select an appropriate type of RE
(name, common noun, pronoun, or empty refer-
ence).
The immediate motivating application context
for the GREC Tasks is the improvement of referen-
tial clarity and coherence in extractive summaries
and multiply edited texts (such as Wikipedia arti-
cles) by regenerating REs contained in them.
The motivating theoretical interest for the GREC
Tasks is to discover what kind of information is
useful in the input when making decisions about
different properties of referring expressions when
such expressions are being generated in context
(this is in contrast to most traditional referring ex-
pression generation work in NLG which views the
REG task as context-independent).
The GREC-NEG data is derived from the
newly created GREC-People corpus which con-
sists of 1,000 annotated introduction sections from
Wikipedia articles in the category People.
Nine teams from seven countries registered for
the GREC-NEG?09 Task, of which three teams ul-
timately submitted six systems in total (see Ta-
ble 1). We also used the corpus texts themselves
as ?system? outputs, and created four baseline sys-
tems. We evaluated the resulting 11 systems using
a range of intrinsic and extrinsic evaluation meth-
ods. This report presents the results of all evalu-
ations (Section 6), along with descriptions of the
GREC-NEG data (Sections 2) and task (Section 3),
the test sets and evaluation methods (Section 4),
and the participating systems (Section 5).
Team System name(s)
Univ. Delaware UDel-NEG-1, UDel-NEG-2, UDel-NEG-3
ICSI, Berkeley ICSI-CRF
Univ. Wolverhampton WLV-STAND, WLV-BIAS
Table 1: GREC-NEG?09 teams and systems.
2 GREC-NEG Data
The GREC-NEG data is derived from the newly
created GREC-People corpus which consists
88
of 1,000 annotated introduction sections from
Wikipedia articles in the category People. An in-
troduction section was defined as the textual con-
tent of a Wikipedia article from the title up to
(and excluding) the first section heading, the ta-
ble of contents or the end of the text, whichever
comes sooner. Each text belongs to one of three
subcategories: inventors, chefs and early music
composers. For the purposes of the GREC-NEG?09
competition, the GREC-People corpus was divided
into training, development and test data. The num-
ber of texts in the 3 data sets and 3 subdomains are
as follows:
All Inventors Chefs Composers
Total 1,000 307 306 387
Training 809 249 248 312
Development 91 28 28 35
Test 100 31 30 39
In these texts we have annotated mentions of peo-
ple by marking up the word strings that function as
referential expressions (REs) and annotating them
with coreference information as well as syntactic
and semantic features. The subject of each text is a
person, so there is at least one coreference chain in
each text. The numbers of coreference chains (en-
tities) in the 900 texts in the training/development
sets are as shown in Table 2. The texts vary greatly
in length, from 13 words to 935, with an average
of 128.98 words.
2.1 Annotation of REs in GREC-People
This section describes the different types of re-
ferring expression (RE) that we annotated in the
GREC-People corpus. These manual annotations
were then automatically checked and converted to
the XML format described in Section 2.2 (which
encodes slightly less information, as explained be-
low). In terminology and the treatment of syntax
used in the annotation scheme and discussion of it
in this report we rely heavily on The Cambridge
Grammar of the English Language by Huddleston
and Pullum which we will refer to as CGEL for
short below (Huddleston and Pullum, 2002).
In the example sentences below, (unbroken) un-
derlines are used for referential expressions (REs)
that are an example of the specific type of RE they
are intended to illustrate, whereas dashed under-
lines are used for other annotated REs. Corefer-
ence between REs is indicated by subscripts i, j, ...
immediately to the right of an underline (their
scope is one example sentence, i.e. an i in one ex-
ample sentence does not represent the same en-
tity as an i in another example sentence). Square
brackets indicate supplements. The syntactic com-
ponent relativised by a relative pronoun is indi-
cated by vertical bars. Supplements and their an-
chors (in the case of appositive supplements), and
relative clauses and the component they relativise
(in the case of relative-clause supplements) are co-
indexed by superscript x, y, .... Dependents inte-
grated in an RE are indicated by curly brackets.
Supplements and dependents are highlighted in
bold where they specifically are being discussed.
In the XML format of the annotations, the be-
ginning and end of a reference is indicated by
<REF><REFEX>... </REFEX></REF> tags, and
other properties discussed in the following sec-
tions (e.g. syntactic category) are encoded as at-
tributes on these tags (for details see Section 2.2).
For GREC-NEG?09 we decided not to transfer the
annotations of integrated dependents and relative
clauses to the XML format. Such dependents
are included within <REFEX>...</REFEX> annota-
tions where appropriate, but without being marked
up as separate constituents.
2.1.1 Syntactic Category and Function
This section describes the types of REs we annoted
in the GREC-People Corpus.
I Subject NPs: referring subject NPs, including
pronouns and special cases of VP coordination:
1. Hei was born in Ramsay township, near Almonte, On-
tario, Canada, the eldest son of |Scottish immigrants,
{John Naismith and Margaret Young} |xj,k [whoj,k had
arrived in the area in 1851 and j,k worked in the
mining industry]x.
2. The Banu Musa brothersi,j,k were three 9th century
Persian scholars, of Baghdad, active in the House of
Wisdom.
Ia Subjects of gerund-participials:
1. Hisi research on hearing and speech eventually culmi-
nated in Belli being awarded the first U.S. patent for
the invention of the telephone in 1876.
2. Fessendeni used the alternator-transmitter to send out
a short program from Brant Rock, which included hisi
playing the song O Holy Night on the violin and i
reading a passage from the Bible.
II Object NPs: referring NPs including pro-
nouns that function as direct or indirect objects of
VPs and prepositional phrases; e.g.:
1. Many of the alpinists arrested with Vitaly Abalakovi
were executed.
2. Hei entrusted themj,k,l to Ishaq bin Ibrahim
al-Mus?abixm , [a former governor of Baghdad]xm .
89
Entities 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Texts 437 192 80 63 38 31 16 18 4 7 9 1 1 0 0 0 0 0 0 1 1 0 1
Table 2: Numbers of person entities (hence coreference chains) in texts in the training/development data,
e.g. there are 38 texts which mention exactly 5 person entities.
IIa Reflexive pronouns:
1. Smithi called himselfi the ?Komikal Konjurer?.
III Subject-determiner genitives: genitive NPs
(including genitive forms of pronouns) that func-
tion as subject-determiners, i.e. syntactic compo-
nents that ?combine the function of determiner,
marking the NP as definite, with that of comple-
ment (more specifically subject).? (CGEL, p. 56):
1. Theyi,j,k shared the 1956 Nobel Prize in Physics for
theiri,j,k invention.
2. On the eve of hisi death in 1605, the Mughal em-
pire spanned almost 500 million acres (doubling dur-
ing Akbar?si reign).
Note that this category excludes lexicalised cases,
e.g. the so-called ?Newton?s method?.
IIIa REs in composite nominals: this is the
only type of RE we have annotated that is not an
NP, but a nominal. This type functions as inte-
grated attributive complement, e.g.:
1. The Eichengru?ni version was ignored by historians ...
2. The new act was a great success, largely despite the
various things Blacktoni and Smithj were doing be-
tween the Edisonk films.
Note that this category too excludes lexicalised
cases, e.g. the Nobel Prizes; the Gatling gun.
2.1.2 Annotation of supplements
We have annotated two kinds of supplements in
the GREC-People corpus, supplementary relative
clauses (CGEL, p. 1058), and appositive supple-
ments. The former is not transferred to the XML
annotation, for more information see (Belz, 2009).
The following examples illustrate annotation of
appositive supplements (which are in bold):
1. John W. Campbell, Jr.xi
[the editor of Astounding magazinei ]x.
2. was the eldest of the six children of Thomas Aspdinxi ,
[a bricklayer living in the Hunslet district of Leedsi ]x
In the XML version, anchor and supple-
ment are simply annotated as two (or occasion-
ally three) independent, usually adjacent REs
(REFEXs); the syntactic function of the second
(and third) RE is marked as appositive supplement
(SYNFUNC="app-supp").
2.1.3 Further aspects of the annotation
As can be seen from some of the examples above,
we annotated all embedded references. The
maximum depth of embedding that occurs in the
GREC-People corpus is 3.
We annotated all plural REs that refer to groups
of people where the number of group members is
known. For an explanation of our treatment of
REs that are coordinations of NPs, see the GREC-
NEG?09 documentation (Belz, 2009).
We have annotated all mentions of individual
person entities even if they are not actually named
anywhere in the text, and including cases of both
definite and indefinite references, e.g.:
1. The resolution?s sponsori described it as ...
2. ... with the help of Robert Cailliauj and
a {young} student staff {at CERN}k .
2.2 XML Annotation
Figure 1 shows one of the XML-annotated texts
from the GREC-NEG data. Each such text con-
sists of two initial lines of XML declarations fol-
lowed by a GREC-ITEM. A GREC-ITEM consists of a
TEXT element followed by an ALT-REFEX element.
A TEXT has one attribute (an ID unique within the
corpus), and is composed of one TITLE followed
by any number of PARAGRAPHs. A TITLE is just a
string of characters. A PARAGRAPH is any combi-
nation of character strings and REF elements.
The REF element indicates a reference, in the
sense of ?an instance of referring? (which could,
in principle, be realised by gesture or graphically,
as well as by a string of words, or a combination of
these). A REF is composed of one REFEX element
(the ?selected? referential expression for the given
reference; in the corpus texts it is the referential
expression found in the corpus).
The attributes of the REF element are ENTITY
(entity identifier), MENTION (mention identifier),
SEMCAT (semantic category), SYNCAT (syntactic
category), and SYNFUNC (syntactic function). For
full details and ranges of values see (Belz, 2009).
ENTITY and MENTION together constitute a unique
identifier for a reference within a text; together
90
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE GREC-ITEM SYSTEM "genchal09-grec.dtd">
<GREC-ITEM>
<TEXT ID="15">
<TITLE>Alexander Fleming</TITLE>
<PARAGRAPH> <REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX>
</REF> (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist.
<REF ENTITY="0" MENTION="2" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX>
</REF> published many articles on bacteriology, immunology, and chemotherapy.
<REF ENTITY="0" MENTION="3" SEMCAT="person" SYNCAT="np" SYNFUNC="subj-det">
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
</REF> best-known achievements are the discovery of the enzyme lysozyme in 1922 and the discovery
of the antibiotic substance penicillin from the fungus Penicillium notatum in 1928, for which
<REF ENTITY="0" MENTION="4" SEMCAT="person" SYNCAT="np" SYNFUNC="subj">
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
</REF> shared the Nobel Prize in Physiology or Medicine in 1945 with
<REF ENTITY="1" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj">
<REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX>
</REF> and
<REF ENTITY="2" MENTION="1" SEMCAT="person" SYNCAT="np" SYNFUNC="obj">
<REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX>
</REF>.</PARAGRAPH>
</TEXT>
<ALT-REFEX>
<REFEX ENTITY="0" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Fleming?s</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="genitive">Sir Alexander Fleming?s</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Fleming</REFEX>
<REFEX ENTITY="0" REG08-TYPE="name" CASE="plain">Sir Alexander Fleming</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="0" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
<REFEX ENTITY="1" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="1" REG08-TYPE="name" CASE="genitive">Florey?s</REFEX>
<REFEX ENTITY="1" REG08-TYPE="name" CASE="plain">Florey</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="1" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
<REFEX ENTITY="2" REG08-TYPE="empty" CASE="no_case">_</REFEX>
<REFEX ENTITY="2" REG08-TYPE="name" CASE="genitive">Chain?s</REFEX>
<REFEX ENTITY="2" REG08-TYPE="name" CASE="plain">Chain</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="accusative">him</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="genitive">his</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">he</REFEX>
<REFEX ENTITY="2" REG08-TYPE="pronoun" CASE="nominative">who</REFEX>
</ALT-REFEX>
</GREC-ITEM>
Figure 1: Example XML-annotated text from the GREC-NEG?09 data.
with the TEXT ID, they constitute a unique iden-
tifier for a reference within the entire corpus.
A REFEX element indicates a referential expres-
sion (a word string that can be used to refer to an
entity). The attributes of the REFEX element are
REG08-TYPE (name, common, pronoun, empty), and
CASE (nominative, accusative, etc.).
We allow arbitrary-depth embedding of refer-
ences. This means that a REFEX element may have
REF element(s) embedded in it. See also next but
one paragraph for embedding in REFEX elements
that are contained in ALT-REFEX lists.
The second (and last) component of a
GREC-ITEM is an ALT-REFEX element which
is a list of REFEX elements. For the GREC-NEG?09
Task, these were obtained by collecting the set of
all REFEXs that are in the text, and adding several
defaults including pronouns and other cases (e.g.
genitive) of REs already in the list.
REF elements that are embedded in REFEX ele-
ments contained in an ALT-REFEX list have an un-
specified MENTION id (the ??? value). Furthermore,
such REF elements have had their enclosed REFEX
removed. For example:
<ALT-REFEX>
...
<REFEX ENTITY="2" REG08-TYPE="common" CASE="plain">
a friend of <REF ENTITY="1" MENTION="?" SEMCAT=
"person" SYNCAT="np" SYNFUNC="obj"></REF></REFEX>
...
</ALT-REFEX>
3 The GREC-NEG Task
The test data inputs were identical to the train-
ing/development data (Figure 1), except that REF
elements in the test data do not contain a REFEX
element, i.e. they are ?empty?. The task for par-
ticipating systems is to select one REFEX from the
ALT-REFEX list for each REF in each TEXT in the
test sets. If the selected REFEX contains an em-
91
bedded REF then participating systems also need
to select a REFEX for this embedded REF and to set
the value of its MENTION attribute. The same ap-
plies to all further embedded REFEXs, at any depth
of embedding.
4 Evaluation Procedures
The GREC-NEG data set was divided into training,
development and test data. We performed eval-
uations on the test data, using a range of different
evaluation methods, including intrinsic and extrin-
sic, automatically assessed and human-evaluated,
as described in the following sections.
Participants computed evaluation scores on the
development set, using the geval-2.0.pl code
provided by us which computes Word String Ac-
curacy, REG?08-Type Recall and Precision, string-
edit distance and BLEU.
4.1 Test sets
We created two versions of the test data for the
GREC-NEG Task:
1. GREC-NEG Test Set 1a: randomly selected 10% subset
(100 texts) of the GREC-People corpus (with the same
proportion of texts in the 3 subdomains as in the train-
ing/development data).
2. GREC-NEG Test Set 1b: the same subset of texts as in
(1a); for this set we did not use the REs in the corpus,
but replaced each of them with human-selected alterna-
tives obtained in an online experiment as described in
(Belz and Varges, 2007); this test set therefore contains
three versions of each text where all the REFEXs in a
given version were selected by one ?author?.
Test Set 1a has a single version of each text, and
the scoring metrics below that are based on count-
ing matches (Word String Accuracy counts match-
ing word strings, REG08-Type Recall/Precision
count matching REG08-Type attribute values)
simply count the number of matches a system
achieves against that single text.
Test Set 1b, however, has three versions of each
text, so the match-based metrics first calculate the
number of matches for each of the three versions
and then use (just) the highest number of matches.
4.2 Automatic intrinsic evaluations
The chief humanlikeness measures we computed
were REG08-Type Recall and Precision. REG08-
Type Precision is defined as the proportion of
REFEXs selected by a participating system which
match the reference REFEXs (where match counts
are obtained as explained in the preceding sec-
tion). REG08-Type Recall is defined as the propor-
tion of reference REFEXs for which a participating
system has produced a match.
The reason why we use REG08-Type Recall and
Precision for GREC-NEG rather than REG08-Type
Accuracy as in GREC-MSR is that in GREC-NEG
(unlike in GREC-MSR) there may be a different
number of REFEXs in system outputs and the ref-
erence texts in the test set (because there are em-
bedded references in GREC-People, and systems
may select REFEXs with or without embedded ref-
erences for any given REF).
We also computed String Accuracy, defined as
the proportion of word strings selected by a par-
ticipating system that match those in the reference
texts. This was computed on complete, ?flattened?
word strings contained in the outermost REFEX i.e.
embedded REFEX word strings were not considered
separately.
We also computed BLEU-3, NIST, string-edit
distance and length-normalised string-edit dis-
tance, all on word strings defined as for String Ac-
curacy. BLEU and NIST are designed for multiple
output versions, and for the string-edit metrics we
computed the mean of means over the three text-
level scores (computed against the three versions
of a text). For details, see GREC-MSR report in
this volume.
4.3 Human-assessed intrinsic evaluations
Given that the motivating application context for
the GREC-NEG Task is improving referential clar-
ity and coherence in multiply edited texts, we
designed the human-assessed intrinsic evaluation
as a preference-judgment test where subjects ex-
pressed their preference, in terms of two criteria,
for either the original Wikipedia text or the version
of it with system-generated referring expressions
in it. The intrinsic human evaluation involved out-
puts for 30 randomly selected items from the test
set from 5 of the 6 participating systems,1 the four
baselines and the original corpus texts (10 systems
in total). We used a Repeated Latin Squares de-
sign which ensures that each subject sees the same
number of outputs from each system and for each
test set item. There were three 10x10 squares, and
a total of 600 individual judgments in this evalu-
ation (60 per system: 2 criteria x 3 articles x 10
1We left out UDel-NEG-1 given our limited resources and
the fact that this is a kind of baseline system.
92
Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.
evaluators). We recruited 10 native speakers of
English from among students currently complet-
ing a linguistics-related degree at Kings College
London and University College London.
Following detailed instructions, subjects did
two practice examples, followed by the 30 texts
to be evaluated, in random order. Subjects car-
ried out the evaluation over the internet, at a time
and place of their choosing. They were allowed to
interrupt and resume the experiment (though dis-
couraged from doing so).
Figure 2 shows what subjects saw during the
evaluation of an individual text pair. The place
(left/right) of the original Wikipedia article was
randomly determined for each individual evalua-
tion of a text pair. People references are high-
lighted in yellow/orange, those that are identical
in both texts are yellow, those that are different
are orange. The evaluator?s task is to express their
preference in terms of each quality criterion by
moving the slider pointers. Moving the slider to
the left means expressing a preference for the text
on the left, moving it to the right means preferring
the text on the right; the further to the left/right the
slider is moved, the stronger the preference. The
two criteria were explained in the introduction as
follows (the wording of the first is from DUC):
1. Referential Clarity: It should be easy to identify who
the referring expressions are referring to. If a person
is mentioned, it should be clear what their role in the
story is. So, a reference would be unclear if a person
is referenced, but their identity or relation to the story
remains unclear.
2. Fluency: A referring expression should ?read well?,
i.e. it should be written in good, clear English, and the
use of titles and names should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
It was not evident to the evaluators that slid-
ers were associated with numerical values. Slider
pointers started out in the middle of the scale (no
preference). The values associated with the points
on the slider ranged from -10.0 to +10.0.
4.4 Extrinsic automatic evaluation
An evaluation we piloted in REG?08 was an auto-
matic approach to extrinsic evaluation (for a more
detailed description, see the GREC-MSR results re-
port elsewhere in this volume). The basic premise
is that poorly chosen reference chains seem likely
to affect the reader?s ability to resolve REs. In our
automatic extrinsic method, the role of the reader
is played by an automatic coreference resolution
tool and the expectation is that the tool performs
worse (is less able to identify coreference chains)
with more poorly chosen referential expressions.
93
To counteract the possibility of results being a
function of a specific coreference resolution algo-
rithm or tool, we used two different resolvers?
those included in LingPipe2 and OpenNLP (Mor-
ton, 2005)?and averaged results. For the same
reason we used three different performance mea-
sures: MUC-6 (Vilain et al, 1995), CEAF (Luo,
2005), and B-CUBED (Bagga and Baldwin, 1998).
5 Systems
Base-rand, Base-freq, Base-1st, Base-name:
We created four baseline systems each with a
different way of selecting a REFEX from those
REFEXs in the ALT-REFEX list that have match-
ing entity IDs. Base-rand selects a REFEX at ran-
dom. Base-1st selects the first REFEX. Base-freq
selects the first REFEX with a REG08-TYPE that
is the overall most frequent (as determined from
the training/development data) given the SYNCAT,
SYNFUNC and SEMCAT of the reference. Base-
name selects the shortest REFEX with attribute
REG08-TYPE=name.
UDel: The UDel-NEG-1 system is identical to
the UDel system that was submitted to the GREC-
MSR Task (for a description of that system see
GREC-MSR?09 results report in this volume), ex-
cept that it was adapted to the different data for-
mat of GREC-NEG. UDel-NEG-2 is identical to
UDel-NEG-1 except that it was retrained on GREC-
NEG data and the feature set was extended by en-
tity and mention IDs. UDel-NEG-3 additionally
utilised improved identification of other entities.
ICSI-CRF: The ICSI-CRF system construes the
GREC-MSR task as a sequence labelling task and
determines the most likely current class label
given preceding labels using a Conditional Ran-
dom Field model trained using the follow features
for the current reference, the most recent preced-
ing reference, and the most recent reference to the
same entity: preceding and following word uni-
gram and bigram; suffix of preceding and follow-
ing word; preceding and following punctuation;
reference ID; and whether this is the beginning of
a paragraph. If more than one class label remains,
the last in the list of possible REs in the GREC-MSR
data is selected.
WLV: The WLV systems start with sentence
splitting and POS tagging. WLV-STAND then em-
2http://alias-i.com/lingpipe/
ploys a J48 decision tree classifier to obtain a prob-
ability for each REF/REFEX pair that it is a good
pair in the current context. The context is repre-
sented by the following set of features. Features
of the REFEX word string: is it the longest of the
possible REFEXs; number of words; all REFEX fea-
tures supplied in GREC-NEG data. Features of the
REF: is it part of the first chain in the text; is it the
first mention of the entity; is it at the beginning of
the sentence; all REF features supplied in GREC-
NEG data. Other features: do the preceding words
match ?, but?, ?and then? and similar phrases; dis-
tance in sentences to last mention; REG08-Type
selected for the two preceding REFs; POS tags of
4 words before and 3 words after; correlation be-
tween SYNFUNC and CASE values; size of the chain.
WLV-BIAS is the same except that it is retrained
on reweighted training instances. The reweighting
scheme assigns a cost of 3 to false negatives and 1
to false positives.
6 Results
This section presents the results of all the evalua-
tion methods described in Section 4. We start with
REG08-Type Precision and Recall, the intrinsic au-
tomatic metrics which participating teams were
told was going to be the chief evaluation method,
followed by Word String Accuracy and other in-
trinsic automatic metrics (Section 6.2), the intrin-
sic human evaluation (Section 6.3) and the extrin-
sic automatic evaluation (Section 6.4).
System REG08-Type WS Acc. Norm. SERecall Precision
ICSI-CRF 83.05 83.05 0.786 0.197
WLV-BIAS 77.61 80.26 0.735 0.239
UDelNEG-3 75.27 75.27 0.333 0.636
UDelNEG-2 74.95 74.95 0.323 0.646
UDelNEG-1 68.87 68.87 0.315 0.658
WLV-STAND 66.20 68.46 0.626 0.351
Table 5: Self-reported evaluation scores for devel-
opment set.
6.1 REG08-Type Precision and Recall
Participants computed scores for the development
set (91 texts) themselves, using the geval evalua-
tion tool provided by us. These scores are shown
in Table 5, and are also included in the partici-
pants? reports elsewhere in this volume.3
REG08-Type Recall and Precision results for
Test Set 1a are shown in column 2 of Table 3.
As would be expected, results on the test data are
3 ICSI-CRF scores obtained directly from ISCI team.
94
System
REG08-Type Precision and Recall Scores against Corpus (Test Set 1a)
All Chefs Composers Inventors
Precision Recall R P R P R P
ICSI-CRF 79.12 A 76.92 A 70.01 73.54 78.11 80.18 80.05 81.86
WLV-BIAS 73.77 B 72.70 A 69.82 71.52 73.53 74.38 73.65 74.56
WLV-STAND 64.49 C 63.55 B 58.28 59.70 65.38 66.14 64.78 65.59
Base-freq 61.52 C 59.6 B 49.41 51.86 63.95 65.74 60.59 62.12
UDel-NEG-2 53.21 D 51.14 C 44.38 47.17 50.50 52.22 57.88 59.80
UDel-NEG-3 52.49 D 50.45 C 43.49 46.23 49.79 51.48 57.39 59.29
UDel-NEG-1 50.47 D 48.51 C 42.90 45.60 47.78 49.41 54.43 56.23
Base-rand 43.32 E 42.00 D 38.76 40.43 41.77 43.00 45.07 46.21
Base-name 40.60 E 39.09 D 44 97 47.80 39.06 40.32 34.24 35.28
Base-1st 10.99 F 10.81 E 12.43 12.73 9.30 9.43 12.07 12.22
Table 3: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set and
for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.
System
REG08-Type Precision and Recall Scores against human topline (Test Set 1b)
All Chefs Composers Inventors
Precision Recall R P R P R P
Corpus 82.67 A 84.01 A 84.24 82.25 84.47 83.26 83.04 82.02
ICSI-CRF 79.33 A B 78.38 B 76.36 77.54 78.81 79.74 79.30 80.10
WLV-BIAS 77.78 B 77.78 B 77.58 77.58 77.86 77.86 77.81 77.81
WLV-STAND 67.51 C 67.51 C 65.76 65.76 68.60 68.60 67.08 67.08
Base-freq 65.38 C 64.37 C 58.48 59.94 68.07 68.97 62.84 63.64
UDel-NEG-2 57.39 D 56.06 D 55.15 57.23 54.86 55.92 58.85 60.05
UDel-NEG-3 57.25 D 55.92 D 55.76 57.86 54.57 55.62 58.35 59.54
Base-name 55.22 D 54.01 D 54.24 56.29 57.04 58.05 48.63 49.49
UDel-NEG-1 53.57 D 52.32 D E 51.21 53.14 50.80 51.78 55.86 57.00
Base-rand 48.46 E 47.75 E 47.88 48.77 46.44 47.13 49.88 50.51
Base-1st 12.54 F 12.54 F 13.94 13.94 10.45 10.45 14.96 14.96
Table 4: REG08-Type Recall and Precision scores against human topline version of Test Set for complete
set and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.
somewhat worse (than on the development data).
Also included in this table are results for the 4
baseline systems, and it is clear that selecting the
most frequent RE type given SEMCAT, SYNFUNC and
SYNCAT (as done by the Base-freq system) pro-
vides a strong baseline for RE type selection.
The last 6 columns in Table 3 contain Recall (R)
and Precision (P) results for the three subdomains.
For most of the systems results are slightly better
for Inventors than for Composers, and better for
Composers than for Chefs. A contributing factor
to this may be the fact that texts in Chefs tend to
be much more colloquial. Base-1st has by far the
worst results; this is because it selects the empty
reference in almost all cases (because ALT-REFEX
lists are sorted and if a list contains an empty ref-
erence it will end up at the beginning).
We carried out univariate ANOVAs with Sys-
tem as the fixed factor, and ?Number of REFEXs
in a text? as a random factor, and REG08-Type Re-
call as the dependent variable in one ANOVA, and
REG08-Type Precision in the other. The result for
Recall was F(10,704) = 81.547, p < 0.001.4 The
result for Precision was F(10,722) = 79.359, p <
0.001. The columns containing capital letters in
Table 3 show the homogeneous subsets of systems
4We included the corpus texts themselves in the analysis,
hence 10 degrees of freedom (11 systems).
as determined by a post-hoc Tukey HSD analysis.
Systems whose scores are not significantly differ-
ent (at the .05 level) share a letter.
Table 4 shows analogous results computed
against Test Set 1b (which has three versions of
each text). These should be considered as the
chief results of the GREC-NEG?09 Task evalua-
tions, as stated in the participants? guidelines. Ta-
ble 4 includes results for the corpus texts, com-
puted (as are results for the system outputs in Ta-
ble 4) against the three versions of each text in Test
Set 1b. We performed univariate ANOVAs with
System as the fixed factor, Number of REFEXs as
a random factor, and Recall as the dependent vari-
able in one, and Precision in the other. The result
for Recall was F(10,724) = 72.528, p < .001),
and for Precision F(10,722) = 75.476, p < .001.
For both cases, we compared the mean scores with
Tukey?s HSD. As can be seen from the resulting
homogeneous subsets (letter columns in Table 4),
system ranks are the same for Precision and for
Recall. In terms of Precision, the difference be-
tween the corpus texts and the ICSI-CRF system
was not significant.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure re-
ported on in the preceding section, we computed
95
System
String similarity against Corpus (Test Set 1a)
Word String Accuracy
BLEU-3 NIST SE norm. SEAll Chefs Composers Inventors
ICSI-CRF 74.84 A 68.24 76.63 77.10 0.75 5.78 0.70 0.23
WLV-BIAS 68.57 B 66.35 69.08 69.47 0.76 5.62 0.82 0.29
WLV-STAND 59.55 C 54.72 61.24 60.56 0.73 5.34 1.01 0.39
Base-name 28.48 D 35.53 27.51 24.43 0.5 4.09 1.80 0.67
UDel-NEG-1 16.58 E 20.13 15.09 16.28 0.43 2.47 2.1 0.82
UDel-NEG-2 16.44 E 19.81 14.79 16.54 0.45 2.37 2.08 0.83
UDel-NEG-3 16.37 E 19.18 15.09 16.28 0.45 2.41 2.08 0.83
Base-rand 8.22 F 8.49 7.10 9.92 0.17 0.9 2.43 0.89
Base-1st 7.28 F 7.23 6.36 8.91 0.16 0.98 2.54 0.90
Base-freq 2.52 G 4.40 2.37 1.27 0.31 1.91 2.34 0.90
Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1a (systems
in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy
only.
System
String similarity against human topline (Test Set 1b)
Word String Accuracy
BLEU-3 NIST SE norm. SEAll Chefs Composers Inventors
Corpus 81.90 A 83.33 82.25 80.15 0.95 7.15 0.71 0.25
ICSI-CRF 74.55 B 71.70 75.15 75.83 0.86 6.35 0.92 0.31
WLV-BIAS 69.07 C 69.50 68.49 69.72 0.88 6.17 1.03 0.36
WLV-STAND 59.70 D 58.18 60.36 59.80 0.84 5.81 1.21 0.45
Base-name 37.27 E 42.14 36.83 34.10 0.65 5.57 1.73 0.63
UDel-NEG-1 19.25 F 22.96 17.60 19.08 0.51 2.62 2.17 0.82
UDel-NEG-2 18.96 F 22.96 17.31 18.58 0.53 2.42 2.15 0.83
UDel-NEG-3 18.89 F 22.64 17.75 17.81 0.53 2.49 2.15 0.82
Base-rand 10.45 G 10.06 9.91 11.70 0.25 1.11 2.49 0.89
Base-1st 8.65 G 8.49 7.54 10.69 0.24 1.29 2.64 0.92
Base-freq 3.24 H 4.40 3.55 1.78 0.39 2.1 2.40 0.90
Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1b (systems
in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy.
Word String Accuracy and the other string simi-
larity metrics described in Section 4.2. The result-
ing scores for Test Set 1a (the corpus texts) are
shown in Table 6. Ranks for peer systems rela-
tive to each other are very similar to the results
reported in the last section. However, the ranks of
the baseline systems have changed substantially,
both in relation to each other and to the peer sys-
tems. In particular, Base-freq has moved all the
way down to the bottom of the table. The rea-
son is that this method is geared towards select-
ing the correct type of RE, but pays no attention
to whether it selects a syntactically appropriate RE
for the given context, instead simply selecting the
first RE from the ALT-REFEX list that has the se-
lected type; in the GREC-NEG?09 Task (unlike the
GRE-MSR task) this just happens to be an RE in
the genitive case most of the time which is over-
all rarer than nominative/plain. It is likely that the
Word String scores for the UDel-NEG systems are
low for a similar reason.
We performed a univariate ANOVA with System
as the fixed factor and Number of REFEXs as a
random factor and Word String Accuracy as the
dependent variable. The result for System was
F(10,726) = 103.339; the homogeneous subsets re-
sulting from the Tukey HSD post-hoc analysis are
shown in columns 3?9 of Table 6.
Table 7 shows analogous results for human
topline Test Set 1b (which has three versions of
each text). We carried out the same kind of ANOVA
as for Test Set 1a; the result for System on Word
String Accuracy was F(10,726) = 106.755, p <
0.001. System rankings and homogeneous sub-
sets are the same as for Test Set 1a; scores across
the board are somewhat higher, because of the way
scores are computed for Test Set 1b: it is the high-
est score a system achieves (at text-level) against
any of the three versions of a test set text that is
taken into account.
Results for BLEU-3, NIST and the two string-
edit distance metrics are shown in the rightmost 4
columns of Tables 6 and 7. Systems whose Word
String Accuracy scores differ significantly are as-
signed the same ranks by NIST and the two string-
edit distance metrics as by Word String Accuracy
(except for Base-1st and Base-freq which swap
ranks in some. BLEU-3 does the same and also
flips ICSI-CRF and WLV-BIAS.
6.3 Human-assessed intrinsic measures
In the human intrinsic evaluation, evaluators rated
system outputs in terms of whether they preferred
them over the original Wikipedia texts. As a re-
96
Clarity Fluency
System Mean + 0 ? System Mean + 0 ?
Corpus 0 A 0 30 0 Corpus 0 A 0 30 0
ICSI-CRF -1.447 A B 3 17 10 ICSI-CRF -0.353 A 9 14 7
WLV-BIAS -2.437 A B C 3 14 13 WLV-BIAS -2.257 A B 2 14 14
Base-name -2.583 B C 7 7 16 WLV-STAND -5.823 B C 1 3 26
WLV-STAND -4.477 C D 1 9 20 Base-name -4.257 C D 2 5 23
UDelNEG-3 -6.427 D E 1 4 26 UDelNEG-3 -6.263 C D E 1 3 26
UDelNEG-2 -6.667 D E 1 3 26 UDelNEG-2 -7.13 D E 0 3 27
Base-rand -8.183 E F 0 1 29 Base-rand -7.513 D E 0 0 30
Base-freq -8.26 E F 0 0 30 Base-freq -7.57 D E 0 0 30
Base-1st -9.357 F 0 0 30 Base-1st -8.477 E 0 0 30
Table 8: Results for Clarity and Fluency preference judgement experiment. Mean = mean of individual
scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred; ? =
number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.
sult of the experiment we had for each system and
each evaluation criterion a set of scores ranging
from -10.0 to +10.0, where 0 meant no prefer-
ence, negative scores meant a preference for the
Wikipedia text, and positive scores a preference
for the system-produced text.
The second column of the left half of Table 8
summarises the Clarity scores for each system in
terms of their mean; if the mean is negative the
evaluators overall preferred the Wikipedia texts,
if it is positive evaluators overall preferred the
system. The more negative the score, the more
strongly evaluators preferred the Wikipedia texts.
Columns 9-11 show corresponding counts of how
many times each system was preferred (+), dis-
preferred (?), and neither (0), when compared to
Wikipedia.
The other half of Table 8 shows corresponding
results for Fluency.
We ran a factorial multivariate ANOVA with Flu-
ency and Clarity as the dependent variables. In the
first version of the ANOVA, the fixed factors were
System, Evaluator and Wikipedia Side (indicating
whether the Wikipedia text was shown on the left
or right during evaluation). This showed no signif-
icant effect of Wikipedia Side on either Fluency or
Clarity, and no significant interaction between any
of the factors. There was however a mild effect of
Evaluator on both Fluency and Clarity. We ran the
ANOVA again, this time with just System and Eval-
uator as fixed factors. The result for System on
Fluency was F(9,200) = 37.925, p < .001, and for
System on Clarity it was F(9,200) = 35.439, p <
.001. Post-hoc Tukey?s HSD tests revealed the sig-
nificant pairwise differences indicated by the letter
columns in Table 8.
Correlation between individual Clarity and Flu-
ency ratings as estimated with Pearson?s coeffi-
cient was r = 696, p < .01, indicating that the
two criteria covary to some extent.
Apart from Base-name and WLV-STAND
switching places, system ranks are the same for
Fluency and Clarity. Moreover, system ranks
are very similar to those produced by the string-
similarity scores above. Perhaps the most striking
result is that the ICSI-CRF system does succeed
in improving Fluency compared to the original
Wikipedia texts: it is preferred 9 times whereas
the Wikipedia texts are preferred only 7 times.
System (MUC+CEAF+B3)/3 M C B3
WLV-BIAS 62.64 A 57 62 69
ICSI-CRF 61.28 A B 53 61 69
Base-name 61.11 A B 55 61 68
Corpus 59.56 A B C 53 59 67
UDel-NEG-3 56.13 B C D 48 56 65
UDel-NEG-2 55.9 B C D 47 55 65
Base-freq 55.85 B C D 47 56 65
UDel-NEG-1 54.79 C D 46 54 64
WLV-STAND 51.69 D 41 53 61
Base-rand 34.86 E 15 38 51
Base-1st 26.36 F 2 31 46
Table 9: MUC, CEAF and B-CUBED F-Scores for
all systems; homogeneous subsets (Tukey HSD),
alpha = .05, for mean of F-Scores.
6.4 Automatic extrinsic measures
We fed the outputs of all 11 systems through the
two coreference resolvers, and computed mean
MUC, CEAF and B-CUBED F-Scores as described
in Section 4.4. The second column in Table 9
shows the mean of means of these three F-Scores,
to give a single overall result for each of for this
evaluation method. A univariate ANOVA with
(text-level) mean F-Score as the dependent vari-
able and System as the single fixed factor revealed
a significant main effect of System on mean F-
Score (F(10,1089) = 91.634, p < .001). A post-
hoc comparison of the means (Tukey HSD, alpha
= .05) found the significant differences indicated
by the homogeneous subsets in columns 3?8 (Ta-
ble 9). The numbers in the last three columns are
the separate MUC, CEAF and B-CUBED F-Scores
97
for each system, averaged over the two resolver
tools (and rounded for reasons of space.
7 Concluding Remarks
This was the first time the GREC-NEG Task was
run. It is a new task not only for an NLG shared-
task challenge, but also as a research task in gen-
eral (post-processing extractive summaries in or-
der to improve their quality seems to be just taking
off as a research subfield). There was substantial
interest in the GREC-NEG Task (as indicated by the
nine teams that originally registered). However,
only 3 teams were ultimately able to submit a sys-
tem.
In particular because of the inclusion of plural
references, multiple entities per text and embed-
ded references, the GREC-NEG Task has a higher
entrance level than the GREC-MSR Task. We are
planning to run it again at Generation Challenges
2010 next year, and are considering the possibility
of providing participants with a baseline system
which would help e.g. with processing embedded
references.
We are also planning to add a named entity
recognition preprocessing task, so that this new
task in combination with GREC-NEG can be used
to perform end-to-end post-processing of extrac-
tive summaries (and other types of multiply edited
texts) to improve the clarity and fluency of the re-
ferring expressions in them.
Acknowledgments
Many thanks to the members of the Corpora and
SIGGEN mailing lists, and Brighton University
colleagues who helped with the online MSRE se-
lection experiments for GREC-NEG test set 1b.
Thanks are also due to the Kings College Lon-
don and University College London students who
helped with the intrinsic evaluation experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC?98, pages
563?566.
A. Belz and S. Varges. 2007. The GREC corpus:
Main subject reference in context. Technical Report
NLTG-07-01, University of Brighton.
A. Belz, 2009. GREC Named Entity Generation Chal-
lenge 2009: Participants? Pack.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of
Pensylvania.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings of MUC-6, pages
45?52.
98
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1158?1167,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Subsequent Reference in Shared Visual Scenes:
Computation vs. Re-Use
Jette Viethen1,2
jette.viethen@mq.edu.au
1TiCC
University of Tilburg
Tilburg, The Netherlands
Robert Dale2
robert.dale@mq.edu.au
2Centre for Language Technology
Macquarie University
Sydney, Australia
Markus Guhe3
m.guhe@ed.ac.uk
3School of Informatics
University of Edinburgh
Edinburgh, UK
Abstract
Traditional computational approaches to re-
ferring expression generation operate in a de-
liberate manner, choosing the attributes to be
included on the basis of their ability to dis-
tinguish the intended referent from its dis-
tractors. However, work in psycholinguis-
tics suggests that speakers align their refer-
ring expressions with those used previously in
the discourse, implying less deliberate choice
and more subconscious reuse. This raises the
question as to which is a more accurate char-
acterisation of what people do. Using a cor-
pus of dialogues containing 16,358 referring
expressions, we explore this question via the
generation of subsequent references in shared
visual scenes. We use a machine learning ap-
proach to referring expression generation and
demonstrate that incorporating features that
correspond to the computational tradition does
not match human referring behaviour as well
as using features corresponding to the process
of alignment. The results support the view that
the traditional model of referring expression
generation that is widely assumed in work on
natural language generation may not in fact
be correct; our analysis may also help explain
the oft-observed redundancy found in human-
produced referring expressions.
1 Introduction
Computational work on referring expression genera-
tion (REG) has an extensive history, and a wide vari-
ety of algorithms have been proposed, dealing with
various facets of what is recognised to be a com-
plex problem. Almost all of this work sees the task
as being concerned with choosing those attributes
of an intended referent that distinguish it from the
other entities with which it might be confused (see,
for example, Dale (1989), Dale and Reiter (1995),
Krahmer et al (2003), van Deemter and Krahmer
(2007), Gardent and Striegnitz (2007)). Indepen-
dently, an alternative way of thinking about refer-
ence has arisen within the psycholinguistics com-
munity: there is now a long tradition of work that
explores how a dialogue participant?s forms of ref-
erence are influenced by those previously used for
a given entity. Most recently, this line of work has
been discussed in terms of the notions of alignment
(Pickering and Garrod, 2004) and conceptual pacts
(Clark and Wilkes-Gibbs, 1986; Brennan and Clark,
1996).
We suspect that neither approach tells the full
story, and so we are interested in exploring whether
the two perspectives should be integrated. Using a
large corpus of referring expressions in task-oriented
dialogues, this paper presents a machine learning
approach that allows us to combine features corre-
sponding to the two perspectives. Our results show
that models based on the alignment perspective out-
perform models based on traditional REG considera-
tions, as well as a number of simpler baselines.
The paper is structured as follows. In Section 2,
we outline the two perspectives on subsequent ref-
erence, and summarise related work. In Section 3,
we describe the iMAP Corpus and the referring ex-
pressions it contains. In Section 4, we describe the
approach we take to learning models of referential
behaviour using this data, and in Section 5 we dis-
cuss the results of a number of experiments based
1158
on this approach, followed by an error analysis in
Section 6. Section 7 draws some conclusions and
discusses future work.
2 Related Work
2.1 The Algorithmic Approach
We use the term algorithmic approach here to re-
fer to the perspective that is common to the consid-
erable body of work within computational linguis-
tics on the problem of referring expression gener-
ation developed over the last 20 years. Much of
this work takes as its starting point the characterisa-
tion of the problem expressed in (Dale, 1989). This
work has focused on the design of algorithms which
take into account the context of reference in order to
decide what properties of an entity should be men-
tioned in order to distinguish that entity from others
with which it might be confused. Early work was
concerned with subsequent reference in discourse,
inspired by Grosz and Sidner?s (1986) observations
on how the attentional structure of a discourse made
particular referents accessible at any given point.
More recently, attention has shifted to initial ref-
erence in visual domains, driven in large part by
the availability of the TUNA dataset and the shared
tasks that make use of it (Gatt et al, 2008). The con-
struction of distinguishing descriptions has consis-
tently been a key consideration in this body of work.
Scenarios that require the generation of references
in multi-turn dialogues that concern visual scenes
are likely to be among the first where we can ex-
pect computational approaches to referring expres-
sion generation to be practically useful. Surpris-
ingly, however, the more recent work on initial refer-
ence in visual domains and the earlier work on sub-
sequent reference in discourse remain somewhat dis-
tinct and separate from each other, despite much the
same algorithms having been used in both. There
is very little work that brings these two strands to-
gether by looking at both initial and subsequent ref-
erences in dialogues that concern visual scenes. An
exception here is the machine learning approach de-
veloped by Stoia et al (2006), who aimed at building
a dialogue system for a situated agent giving instruc-
tions in a virtual 3D world. However, their approach
was concerned with choosing the type of reference
to use (definite or indefinite, pronominal, bare or
modified head noun), and not with the content of the
reference; and their data set consisted of only 1242
referring expressions.
2.2 The Alignment Approach
Meanwhile, starting with the early work of Carroll
(1980), a quite distinct strand of research in psy-
cholinguistics has explored how a speaker?s form of
reference to an entity is impacted by the way that en-
tity has been previously referred to in the discourse
or dialogue. The general idea behind what we will
call the alignment approach is that a conversational
participant will often adopt the same semantic, syn-
tactic and lexical alternatives as the other party in a
dialogue. This perspective is most strongly associ-
ated with the work of Pickering and Garrod (2004).
With respect to reference in particular, speakers are
said to form conceptual pacts in their use of lan-
guage (Clark and Wilkes-Gibbs, 1986; Brennan and
Clark, 1996). Although there is disagreement about
the exact mechanisms that enable alignment and
conceptual pacts, the implication of much of this
work is that one speaker introduces an entity by
means of some description, and then (perhaps after
some negotiation) both conversational participants
share this form of reference, or a form of reference
derived from it, when they subsequently refer to that
entity.
Recent work by Goudbeek and Krahmer (2010)
supports the view that subconscious alignment does
indeed take place at the level of content selection for
referring expressions. The participants in their study
were more likely to use a dispreferred attribute to
describe a target referent if this attribute had recently
been used in a description by a confederate.
There is some work within natural language gen-
eration that attempts to model the process of align-
ment (Buschmeier et al, 2009; Janarthanam and
Lemon, 2009), but this is predominantly concerned
with what we might think of as the ?lexical perspec-
tive?, focussing on lexical choice rather than the se-
lection of appropriate semantic content for distin-
guishing descriptions.
2.3 Combined Models
This paper is not the first to look at how the algorith-
mic approach and the alignment approach might be
integrated in REG. An early machine learning ap-
1159
Figure 1: An example pair of maps.
proach to content selection was presented by Jor-
dan and Walker (2000; 2005); they were also in-
terested in an exploration of the validity of differ-
ent psycholinguistic models of reference produc-
tion, including Grosz and Sidner?s (1986) model
of discourse structure, the conceptual pacts model
of Clark and colleagues, and the intentional influ-
ences model developed by Jordan (2000). However,
their data set consists of only 393 referring expres-
sions, compared to our 16,358, and these expres-
sions had functions other than identification; most
importantly, the entities referred to were not part of
a shared visual scene as is the case in our data.
Gupta and Stent (2005) instantiated Dale and Re-
iter?s (1995) Incremental Algorithm with a prefer-
ence ordering that favours the attributes that were
used in the previous mention of the same referent. In
a second variant, they even require these attributes
to be included in a subsequent reference. Differ-
ently from most other work on REG, they extended
the task to include ordering of the attributes in the
surface form. They therefore create a special evalu-
ation metric that takes ordering into account, which
makes it hard to compare the performance they re-
port to that of any system that is not concerned with
attribute ordering, such as ours. Their evaluation set
was also considerably smaller than ours: they used
1294 and 471 referring expressions from two differ-
ent corpora, compared to our test set of 4947 refer-
ring expressions.
More recently in (Viethen et al, 2010), we pre-
sented a rule-based system that addressed a specific
instance of the problem we consider here, using the
same corpus as we do: we singled out 2579 first ref-
erences to landmarks by the second speaker (?second
speaker initial references?) and attempted to repro-
duce these using a system based on Dale and Re-
iter?s (1995) Incremental Algorithm. Although the
data set was a subset of the one used here, the system
did not reach the same performance (see Section 5).
3 Referring Expressions in the iMAP
Corpus
The iMAP Corpus (Louwerse et al, 2007) is a col-
lection of 256 dialogues between 32 participant-
pairs who contributed 8 dialogues each. Both par-
ticipants had a map of the same environment, but
one participant?s map showed a route winding its
way between the landmarks on the map; see Fig-
ure 1. The task was for this participant (the in-
struction giver, IG) to describe this route in such a
way that their partner (the instruction follower, IF)
could draw it onto their map; this was complicated
by some discrepancies between the two maps, such
1160
as missing landmarks, the unavailability of colour in
some regions due to ink stains, and small differences
between some landmarks.
The landmarks differ from each other in type,
colour, and one other attribute, which is different
for each type of landmark. For example, there are
different kinds of birds (eagle, ostrich, penguin . . . );
fish differ by their patterns (dotted, checkered, plain
. . . ), aliens have different shapes (circular, hexago-
nal . . . ), and bugs appear in small clusters of differ-
ing numbers. In addition to these inherent attributes
of the landmarks, participants used spatial relations
to other items on the map. Each referring expression
in the corpus is annotated with a unique identifier
corresponding to the landmark that it describes and
the semantic values of the attributes that it contains.
This collection of annotations forms the basic data
we use in our experiments.
For each landmarkR referred to in a dialogue, we
view the sequence of references to this landmark as
a coreference chain, notated ?R1, R2, . . . , Rn?. By
convention, R1 is termed the initial reference, and
all other references in the chain are subsequent ref-
erences. From the corpus as a whole we extracted
34,127 referring expressions in 9558 chains. The av-
erage length of a chain is 4.74; and the longest coref-
erence chain contains 43 references. References
may be contributed to a chain by either speaker, and
can be arbitrarily far apart: in the data, 4201 refer-
ences are in the utterance immediately following the
preceding reference in the chain, but the distance be-
tween references in a chain can be as high as 423
utterances.
We removed from the data any annotation that
was not concerned with the four landmark attributes,
type, colour, relation, or the landmark?s other dis-
tinguishing attribute. For example, ?semantically
empty? head nouns such as thing or set. Ordi-
nal numbers that were annotated as the use of the
number attribute were re-tagged as spatial relations,
as these usually described the position of the target
within a line of landmarks.
As a result of the removal of annotations not per-
taining to the use of the four landmark attributes,
2785 referring expressions had no annotation left;
we removed these instances from the final data set.
We also do not attempt to replicate the remaining
5552 plural referring expressions or the 3062 pro-
Content Pattern Count Proportion
?other? 5893 36.0%
?other, type? 3684 22.5%
?other, colour? 1630 10.0%
?other, colour, type? 1021 6.2%
?colour? 969 5.9%
?relation? 777 4.7%
?other, relation? 587 3.6%
?type? 574 3.5%
?colour, type? 434 2.7%
?other, relation, type? 312 1.9%
?relation, type? 236 1.4%
?colour, relation? 99 0.6%
?other, colour, relation? 81 0.5%
?other, colour, relation, type? 44 0.3%
?colour, relation, type? 17 0.1%
Total 16,358
Table 1: The 15 content patterns by frequency.
nouns found in the corpus.1 However, we do in-
clude all of these instances in the feature extraction
step, on the assumption that they might impact on
the content of subsequent references. Similarly, we
filter out 6369 initial references after we have ex-
tracted features from them, since we focus here on
the generation of subsequent reference only. The re-
maining 16,358 referring expressions form the data
which we use in our experiments.
Contrary to findings from other corpora, in which
colour was used much more frequently (Gatt, 2007;
Viethen and Dale, 2008), the colour attribute was
used in only 26.3% of the referring expressions in
our data set. This is probably due to the often low
reliability of colour in this task caused by the ink
stains. The proportion of referring expressions men-
tioning the target?s type might, at 38.7%, also seem
low. This can be explained by the fact that one quar-
ter of the landmarks, namely birds and buildings, are
more likely to be described in terms of their specific
kind than in terms of their generic type. This also
helps explain why the overall use of the other at-
tribute, which for some landmarks was their kind,
was used in 81.0% of all instances. Spatial relations
were used in 13.16% of the referring expressions,
comparable to other corpora in the literature.
1The additional issues that arise in generating plural refer-
ences and deciding when to use pronouns considerably compli-
cate the problem; see (Gatt, 2007).
1161
We can think of each referring expression as be-
ing a linguistic realisation of a content pattern: this
is the collection of attributes that are used in that
instance. The attributes can be derived from the
property-level annotation given in the corpus. So,
for example, if a particular reference appears as the
noun phrase the blue penguin, annotated seman-
tically as ?blue, penguin?, then the corresponding
content pattern is ?colour, kind?. Our aim is to repli-
cate the content pattern of each referring expression
in the corpus. Table 1 lists the 15 content patterns
that occur in our data set in order of frequency.
4 Modelling Referential Behaviour
4.1 The Two Perspectives
Our task is defined simply as follows: for each sub-
sequent referenceR in the corpus, can we predict the
content pattern that will be used in that reference?
As we noted at the outset of the paper, the literature
would appear to suggest two distinct approaches to
this problem. What we have characterised as the al-
gorithmic approach can be summarised thus:
At the point where a reference is required,
a speaker determines the relevant features
of other entities in the context, then com-
putes the content of a referring expression
which distinguishes the intended referent
from the other entities.
The alignment approach, on the other hand, can be
summarised thus:
Speakers align the forms of reference they
use to be similar or identical to references
that have been used before. In particular,
once a form of reference to the intended
referent has been established, they tend to
re-use that form of reference, or perhaps
an abbreviated version of it.
The alignment approach would appear to be prefer-
able on the grounds of computational cost: we
would expect that retrieving a previously-used refer-
ring expression, or parts thereof, generally requires
less computation than building a new referring ex-
pression from scratch.
On the other hand, if the context has changed
in any way, then a previously-used form of ref-
erence may no longer be effective in identifying
Map Features
Main Map type most frequent type of LM on this map
Main Map other other attribute if the most frequent type of LM
Mixedness are other LM types present on this map?
Ink Orderliness shape of the ink blot(s) on the IF?s map
Lmprop Features
other Att type of the other attribute of the target
[att] Value value for each att of target
[att] Difference was att of target different between the two
maps?
Missing was target missing one of the maps?
Inked Out was target inked] out on the IG?s map?
Speaker Features
Dyad ID ID of the pair of participant-pair
Speaker ID ID of the person who uttered this RE
Speaker Role was the speaker the IG or the IF?
Table 2: The Ind feature set.
the intended referent, and recomputation may be
required.2 This is precisely the consideration on
which the initial work on referring expression gen-
eration was based, inspired by Grosz and Sidner?s
(1986) observations about how the changing atten-
tional structure of a discourse moves different en-
tities in and out of focus. However, a straightfor-
ward recomputation of reference based on the cur-
rrent context carries the risk that the most effective
set of properties to use may change quite radically;
if no account is taken of the history of previous ref-
erences to the entity, it?s conceivable that one could
produce a description that is so different from the
previous description that they are virtually unreco-
gisable as descriptions of the same entity. Ideally,
what we want to do is modify a previous description
to do the job.
These observations suggest that, in order to
choose the most appropriate form of reference for an
entity, we need to simultaneously take account of:
? the other entities from which it must be distin-
guished, both in the visual context and in the
preceding discourse (in other words, exactly
the information that traditional algorithmic ap-
proaches consider);
? how this entity, and perhaps other entities, have
been referred to in the past (precisely the infor-
mation that the alignment approach considers).
2Unfortunately, determining what counts as a change of con-
text, especially in visual scenes, is fraught with difficulty.
1162
TradREG Features (Visual)
Count Vis Distractors number of visual distractors
Prop Vis Same [att] proportion of visual distractors with
same att
Dist Closest distance to the closest visual distrac-
tor
Closest Same [att] has the closest distractor the same
att?
Dist Closest Same [att] distance to the closest distractor of
same att as target
Cl Same type Same [att] has the closest distractor of the same
type also the same att?
TradREG Features (Discourse)
Count Intervening LMs number of other LMs mentioned since
the last mention of the target
Prop Intervening [att] proportion of intervening LMs for
which att was used AND which have
the same att as target
Table 3: The TradREG feature set.
The set of features we describe next attempts to cap-
ture these two aspects of the problem.
4.2 Features
The number of factors that can be hypothesised as
having an impact on the form of a referring expres-
sion in a dialogic setting associated with a visual do-
main is very large. Attempting to incorporate all of
these factors into parameters for rule-based systems,
and then experimenting with different settings for
these parameters, is prohibitively complex. Instead,
we here capture a wide range of factors as features
that can be used by a machine learning algorithm to
automatically induce from the data a classifier that
predicts for a given set of features the attributes that
should be used in a referring expression.
The features we extracted from the data set are
listed in Tables 2?4.3 They fall into five subsets.
Map Features capture design characteristics of the
maps the current dialogue is about; Speaker Fea-
tures capture the identity and role of the partici-
pants; and LMprop Features capture the inherent
visual properties of the target referent. For our ex-
periments, we group the Map, LMprop and Speaker
feature sets into one theory-independent set (Ind).
Most importantly for our present considerations,
3In these tables, att is an abbreviatory variable that is instan-
tiated once for each of the four attributes type, colour, relation,
and the other distinguishing attribute of the landmark. The ab-
breviation LM stands for landmark
Alignment Features (Recency)
Last Men Speaker Same who made the last mention of target?
Last Mention [att] was att used in the last mention of
target?
Dist Last Mention Utts distance to the last mention of target
in utterances
Dist Last Mention REs distance to the last mention of target
in REs
Dist Last [att] LM Utts distance in utterances to last use of
att for target
Dist Last [att] LM REs distance in REs to last use of att for
target
Dist Last [att] Dial Utts distance in utterances to last use of
att
Dist Last [att] Dial REs distance in REs to last use of att
Dist Last RE Utts distance to last RE in utterances
Last RE [att] was att mentioned in the last RE?
Alignment Features (Frequency)
Count [att] Dial how often has att been used in the dialogue?
Count [att] LM how often has att been used for target?
Quartile quartile of the dialogue the RE was uttered in
Dial No number of dialogues already completed +1
Mention No number of previous mentions of target +1
Table 4: The Alignment feature set.
TradREG Features capture factors that the tradi-
tional computational approaches to referring expres-
sion generation take account of, in particular prop-
erties of the discourse and visual distractors; and
Alignment Features capture factors that we would
expect to play a role in the psycholinguistic models
of alignment and conceptual pacts.
4.3 The Models
For the experiments described here, we used a 70?30
split to divide the data into a training set (11,411 in-
stances) and a test set (4,947 instances). In addition
to the main prediction class content pattern, the split
was stratified for Speaker ID and Quartile to ensure
that training and test set contained the same pro-
portion of descriptions from each speaker and each
quartile of the dialogues. We used the J48 algorithm
implemented in the Weka toolkit (Witten and Frank,
2005) to train decision trees with the task of judging,
based on the given features, which content pattern
should be used.
First, we have three separate baseline models:
HeadNounOnly generates only the property that is
the most likely head noun for the target, which
is kind for birds and buildings and type for all
1163
other landmarks. This is a form of ?reduced
reference? strategy.
RepeatLast represents a very simplistic alignment
approach. It generates the same content pattern
that was used in the previous mention of the
target referent.
MajorityClass generates the content pattern most
commonly used in the training set.
We then have a number of models that use subsets
of the features described above:
AllFeatures is a decision tree trained on all fea-
tures;
TradREG is a decision tree trained on the
TradREG features only;
Alignment is a decision tree trained on the Align-
ment features only;
Ind is a decision tree trained on the Ind features
only;
Alignment+Ind is a decision tree trained on all but
the TradREG features;
TradREG+Ind is a decision tree trained on all but
the Alignment features; and
TradREG+Alignment is a decision tree trained on
all but the Ind features.
5 Results
In this section we report how the models described
in the previous section performed on the held-out
test set in comparison to each other and to the three
baselines.
We use Accuracy and average DICE score for our
comparisons; these are the most commonly used
measures in the REG literature (see, for example,
Gatt et al, 2008). Given two sets of attributes, A
and B, DICE is computed as
(1) DICE = 2? |A ?B||A|+ |B| .
This gives some measure of the overlap between two
referring expressions, assigning a partial score if the
two sets share attributes but are not identical. The
Accuracy of a system is the proportion of test in-
stances for which it achieves a DICE score of 1, sig-
nifying a perfect match.
col other type rel Comb. Pattern
Acc Acc Acc Acc Acc DICE
HeadOnly n/a n/a n/a n/a 23.1 0.49
RepLast n/a n/a n/a n/a 38.4 0.55
Majority 73.8 81.0 61.7 86.8 36.0 0.65
predicts no yes no no ?other?
Trad 74.6 84.8 77.1 87.0 47.3 0.73
Align 83.6 84.1 80.7 87.5 54.6 0.78
Ind 81.9 82.8 81.4 88.0 52.7 0.78
Align+Ind 86.1 85.3 82.4 88.7 58.2 0.81
Trad+Ind 82.2 84.1 81.1 87.1 52.5 0.78
Trad+Align 84.1 84.0 80.1 86.8 53.9 0.78
AllFeatures 86.2 85.8 83.2 88.5 58.8 0.81
Table 5: Performance of our models compared to the
baselines. Model names are abbreviated for space rea-
sons. The Accuracy (given in %) of all models is signifi-
cantly better than that of the highest performing baseline
at p<.01 according to the ?2 statistic.
We tested two different ways of generating con-
tent patterns based on the different feature sets de-
scribed above: PatternAtOnce builds a decision
tree that chooses one of the 15 content patterns that
occur in our data set; whereas CombinedPattern
builds attribute-specific decision trees (one for each
of the four attributes that occur in the data: colour,
other, type, and relation), and then combines their
predictions into a complete content pattern. We
found that CombinedPattern slightly outperformed
PatternAtOnce, although the difference is not statis-
tically significant for all feature sets. For space rea-
sons, we report in what follows only on the slightly
better-performing CombinedPattern model.
Table 5 compares the performances of the three
baselines and the decision trees based on the five fea-
ture subsets for each of the individual attributes and
for the combined content pattern; note that the Head-
NounOnly and RepeatLast baselines do not make
attribute-specific predictions. The table shows that
the learned systems outperform all three baselines
for the individual attributes as well as for the com-
bined content pattern.
A comparison of the Alignment feature set and
the TradREG feature set shows that the former out-
performs the latter for the attribute-specific trees
which predict the use of the colour attribute and the
1164
use of relation, and that the combined patterns re-
sulting from the Alignment trees are a better match
of the human-produced patterns both in terms of Ac-
curacy (p<.01 for all three categories, using ?2) and
DICE. Interestingly, even the theory-independent
Ind features outperform the TradREG features.
The comparison between TradREG+Ind and
Alignment+Ind again shows a clear advantage for
the Alignment features: dropping them from the
complete feature set significantly hurts performance
compared to AllFeatures (?2=80.5, p<.01), while
dropping the TradREG features has no significant
impact. Also consistent with the results of the three
individual feature sets, dropping the Ind features
hurts performance more than dropping the TradREG
features, but less than dropping the Alignment fea-
tures. Training on the complete feature set (All-
Features) achieves the highest performance, which
is significantly better than that of all other features
sets (p<.01 using ?2) except Alignment+Ind.
These results suggest that considerations at the
heart of traditional REG approaches do not play as
important a role as those postulated by alignment-
based models for the selection of semantic content
for subsequent referring expressions.
We also note that the Accuracy scores achieved
by our learned systems are similar to the best num-
bers previously reported in the REG literature. While
Jordan and Walker?s (2005) data set is not directly
comparable, they achieved a maximum of 59.9%
Accuracy, against our 58.8%. Stoia et al?s (2006)
best Accuracy was 31.2%, albeit on a slightly dif-
ferent task. Even in the arguably much simpler
non-dialogic domains of the REG competitions con-
cerned with pure content selection, the best perform-
ing system achieved only 53% Accuracy (see Gatt et
al., 2008). The most comparable approach, the rule-
based system we presented in (Viethen et al, 2010)
for a subset of the data used here, was not able to
outperform a RepeatLast baseline at 40.2% Accu-
racy and an average DICE score of 0.67.
6 Error Analysis
An important question to ask is how wrong the mod-
els really are when they do not succeed in perfectly
matching a human-produced reference in our test
set. It might be that they choose a completely dif-
Acc Dice Super Sub Inter Noover
Trad 47.3 0.75 14.4 22.2 5.5 10.5
Align 54.6 0.78 16.0 16.1 3.9 9.4
Ind 52.7 0.78 17.1 17.2 3.9 9.0
Align+Ind 58.2 0.81 16.0 14.8 3.1 7.9
Trad+Ind 52.5 0.78 17.4 17.5 3.8 8.8
Trad+Align 53.9 0.78 17.1 15.6 4.3 9.0
AllFeature 58.8 0.81 16.5 14.5 3.1 7.2
Table 6: The proportions of test instances for which each
model produced a subset, a superset, some other form of
intersection or no-overlap to the human reference.
ferent set of attributes from those included by the
human speaker; however, the Accuracy score also
counts as incorrect any set that only partly overlaps
with the reference found in the test set.
The DICE score gives us a partial answer to this
question, as it assigns a score that is based on the
size of the overlap between the attribute set cho-
sen by the model and that included by the human
speaker. A DICE score that is equal to the Accuracy
score would mean that each referring expression was
either reproduced perfectly, or that a set of attributes
was chosen that did not overlap with the original
one at all. The fact that all our models achieved
DICE scores much higher than their Accuracy scores
shows that they only rarely got it completely wrong.
Table 6 gives a more fine-grained picture by list-
ing, for each model, what percentage of the refer-
ring expressions it produced contained a subset of
the attributes included in the human reference, what
percentage were a superset, what percentage had
another form of partial intersection, and what per-
centage had no commonality with the human refer-
ence. Interestingly, a large number of the referring
expressions produced by the model trained only on
TradREG features are subsets of the human refer-
ence. This indicates that human speakers tend to in-
clude more attributes than are strictly speaking nec-
essary to distinguish the landmark.4 The Alignment
model does not as often produce a subset of the gold
standard content pattern, suggesting that it might be
alignment considerations that account for some of
4That humans often produce ?redundant? descriptions, in op-
position to the target behaviour of some of the early REG algo-
rithms, is of course an oft-observed fact.
1165
both both 1st 2nd either pot.
corr. wrong corr. corr. corr. Acc
Trad vs Ind 1797 1794 545 811 3153 63.7
Trad vs Align 1742 1647 600 958 3300 66.7
Trad vs Align+Ind 1849 1574 493 1031 3373 68.2
Align vs Trad+Ind 1908 1557 792 690 3390 68.5
Align vs Ind 1872 1511 828 736 3436 69.5
Ind vs Trad+Align 1840 1511 768 828 3436 69.5
Table 7: Comparison of the predictions for the combined
content pattern between the models trained on mutually
exclusive feature sets.
the apparent redundancy that human-produced refer-
ring expressions contain.
A second important question is whether the differ-
ent feature sets are doing the same work, or whether
they complement each other. Table 7 lists for those
pairings of our learned models which were based on
mutually exclusive feature sets how many referring
expressions both models predicted correctly, how
many both failed to predict, and how many were pre-
dicted correctly by either of the two models.
Note the high numbers in the columns listing the
counts of instances which both models got either
correct or wrong: these show that there is con-
siderable overlap between all pairings. The small-
est agreement lies at 3424 instances (68.2%) be-
tween TradREG (the least successful model) and
Alignment+Ind (the most successful model). How-
ever, they also each predict correct solutions that the
other misses: 493 (10.0%) for TradREG and 1031
(20.8%) for Alignment+Ind.
The last two columns of Table 7 show the number
of instances that at least one of the two models in
each pairing got correct and the proportion out of
all test instances that this number represents. This
proportion is the maximum Accuracy that could be
achieved by a model that combines the two models
in a pairing and then correctly chooses which one to
use in each instance. The maximum Accuracies that
could be achieved in this way on our data set lie just
below 70%, significantly higher than any numbers
reported in the literature on the task of generating
subsequent reference.
7 Conclusions
Using the largest corpus of referring expressions
to date, we have shown how both the traditional
computational view of REG and the alternative psy-
cholinguistic alignment approach can be captured
via a large set of features for machine learning. Ad-
ditionally, we defined a number of theory indepen-
dent features. Using this approach we have pre-
sented three main findings.
First, we have demonstrated that a model using all
these features to predict content patterns in subse-
quent references in shared visual scenes delivers an
Accuracy of 58.8% and a DICE score of 0.81, out-
performing models based only on features inspired
by one of the two approaches. However, we found
that the features based on traditional REG considera-
tions do not contribute as much to this score as those
based on the alignment approach, and that dropping
the traditional REG features does not significantly
hurt the performance of a model based on alignment
and theory-independent features.
Second, our error analysis showed that the main
reason for the low performance of a model based
on traditional algorithmic features is that it often
chooses too few attributes. The fact that the model
based on the alignment features does not make this
mistake so frequently suggests that it may be the
psycholinguistic considerations incorporated in our
alignment features that lead people to add those ad-
ditional attributes.
Finally, while the different models make the same
correct predictions about the content of referring ex-
pressions in many cases, there are also a consider-
able number of cases where the models based on
either the traditional algorithmic features (10.0%)
or the alignment and independent features (20.8%)
alone make correct predictions that the other gets
wrong; this suggests that a system with the ability
to choose the correct model in each of those cases
(perhaps based on a hypothesis as to whether or not
the relevant context has changed) could reach an ac-
curacy of almost 70% on our data set. In future work
we plan to identify further features that will allow us
to inform this choice so that we can move towards
this level of performance.
1166
References
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 22:1482?1493.
Hendrik Buschmeier, Kirsten Bergmann, and Stefan
Kopp. 2009. An alignment-capable microplanner for
natural language generation. In Proceedings of the
12th European Workshop on Natural Language Gen-
eration, pages 82?89, Athens, Greece.
John M. Carroll. 1980. Naming and describing in social
communication. Language and Speech, 23:309?322.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22(1):1?
39.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, Vancouver B.C.,
Canada.
Claire Gardent and Kristina Striegnitz. 2007. Generat-
ing bridging definite descriptions. In Harry C. Bunt
and Reinhard Muskens, editors, Computing Meaning,
volume 3, pages 369?396. Kluwer, Dordrecht, The
Netherlands.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA
Challenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation, pages 198?206, Salt
Fork OH, USA.
Albert Gatt. 2007. Generating Coherent Reference to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, UK.
Martijn Goudbeek and Emiel Krahmer. 2010. Pref-
erences versus adaptation during referring expression
generation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 55?59, Uppsala, Sweden.
Barbara J. Grosz and Candance L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Surabhi Gupta and Amanda Stent. 2005. Automatic
evaluation of referring expression generation using
corpora. In Proceedings of the Workshop on Using
Corpora for Natural Language Generation, pages 1?
6, Brighton, UK.
Srinivasan Janarthanam and Oliver Lemon. 2009. Learn-
ing lexical alignment policies for generating referring
expressions for spoken dialogue systems. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation (ENLG 2009), pages 74?
81, Athens, Greece, March. Association for Compu-
tational Linguistics.
Pamela W. Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting on As-
sociation for Computational Linguistics, Hong Kong,
China.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
Pamela W. Jordan. 2000. Intentional Influences on Ob-
ject Redescriptions in Dialogue: Evidence from an
Empirical Study. Ph.D. thesis, University of Pitts-
burgh, Pittsburgh PA, USA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Lingustics, 29(1):53?72.
Max M. Louwerse, Nick Benesh, Mohammed E. Hoque,
Patrick Jeuniaux, Gwyneth Lewis, Jie Wu, and Megan
Zirnstein. 2007. Multimodal communication in face-
to-face computer-mediated conversations. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society, pages 1235?1240.
Martin J. Pickering and Simon Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behavioral and
Brain Sciences, 27(2):169?226.
Laura Stoia, Darla M. Shockley, Donna K. Byron, and
Eric Fosler-Lussier. 2006. Noun phrase generation
for situated dialogs. In Proceedings of the 4th Interna-
tional Conference on Natural Language Generation,
pages 81?88, Sydney, Australia, July.
Kees van Deemter and Emiel Krahmer. 2007. Graphs
and Booleans: On the generation of referring expres-
sions. In Harry C. Bunt and Reinhard Muskens, edi-
tors, Computing Meaning, volume 3, pages 397?422.
Kluwer, Dordrecht, The Netherlands.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of the 5th International Conference on Natu-
ral Language Generation, pages 59?67, Salt Fork OH,
USA.
Jette Viethen, Simon Zwarts, Robert Dale, and Markus
Guhe. 2010. Dialogue reference in a visual domain.
In Proceedings of the 7th International Conference on
Language Resources and Evaluation, Valetta, Malta.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco CA, USA.
1167
The Use of Spatial Relations in Referring Expression Generation
Jette Viethen
Centre for Language Technology
Macquarie University
Sydney, Australia
jviethen@ics.mq.edu.au
Robert Dale
Centre for Language Technology
Macquarie University
Sydney, Australia
rdale@ics.mq.edu.au
Abstract
There is a prevailing assumption in the litera-
ture on referring expression generation that re-
lations are used in descriptions only ?as a last
resort?, typically on the basis that including
the second entity in the relation introduces an
additional cognitive load for either speaker or
hearer. In this paper, we describe an experiemt
that attempts to test this assumption; we de-
termine that, even in simple scenes where the
use of relations is not strictly required in order
to identify an entity, relations are in fact often
used. We draw some conclusions as to what
this means for the development of algorithms
for the generation of referring expressions.
1 Introduction
In recent years, researchers working on referring
expression generation have increasingly moved to-
wards collecting their own data on the human pro-
duction of referring expressions (REs) (Krahmer and
Theune, 2002; van der Sluis and Krahmer, 2004;
Gatt and van Deemter, 2006; Belz and Varges,
2007); and the recent Attribute Selection in the
Generation of Referring Expressions (ASGRE) Chal-
lenge used the TUNA corpus (Gatt et al, 2007),
which is the most extensive collection of referring
expressions to date. While there is a substantial
body of experimental work in psycholinguistics that
looks at the human production of referring expres-
sions (see, amongst more recent work, (Clark and
Wilkes-Gibbs, 1986; Stevenson, 2002; Haywood et
al., 2003; Jordan and Walker, 2005)) the large range
of factors that play a role in language production
mean that it is often the case that the specific ques-
tion that one is interested in has not been studied
before. So, NLG researchers have tended towards
data gathering exercises that explore some specific
aspect of referring expression generation, focussing
on hypotheses relevant to algorithm development.
This paper is in the same mold. We are particuarly
interested in how people use spatial relations in re-
ferring expressions, and so in this paper we describe
an experiment that explores the generation of rela-
tional referring expressions in a simple scene. Sec-
tion 2 elaborates on our reasons for exploring this
aspect of reference. Section 3 describes the exper-
iment and provides some discussion of the results:
our primary conclusion is that the assumption in the
literature that relations are used ?as a last resort? does
not appear to hold; relations are often used, even in
simple scenes, when they are not strictly required,
and it is likely that they would be more heavily used
in more complex real-world scenes. We conclude
in Section 4 with some observations as to how the
results presented here might impact on the develop-
ment of algorithms for referring expression genera-
tion, and outline some future work.
2 Spatial Relations in Referring
Expression Generation
The bulk of the existing literature on referring ex-
pression generation (see, for example, Dale (1989),
Dale and Reiter (1995), van Deemter (2006), Ho-
racek (2004), Gatt and van Deemter (2006)) gener-
ally focuses on the use of non-relational properties,
which can either be absolute (for example, colour) or
relative (for example, size). We are interested in the
59
use of relational expressions, and in particular the
use of spatial relations; the contexts of use we are
interested in are task-specific, where, for example,
we might want an omniscient domestic agent to tell
us where we have placed a lost object (You left your
keys under the folder on the desk . . . ), or to identify
a hearer-new object in a cluttered scene (the maga-
zine at the bottom of the pile of papers next to the
lampshade in the corner). To develop agents with
these kinds of referential capabilities, we want to ac-
quire data that will inform the development of algo-
rithms, either by automatically checking their ability
to replicate the corpus, or as a baseline for assessing
the performance of humans in an identification task
based on the output of these algorithms.
In this paper, we describe an experiment that
looks at how and when people use spatial relations
in a simple scene. More specifically, we aim to ex-
plore the hypothesis that relations are always dispre-
ferred over non-relational properties. This hypothe-
sis appears to underly most approaches to referring
expression generation that handle relations:
Gardent (2002) adopts a constraint based ap-
proach to deal with relations specifically geared at
generating referring expressions that are as short as
possible. As including a relation in a referring ex-
pression always entails the additional mention of
at least a head noun for the related object, this ap-
proach inherently prefers properties over relations.
Krahmer and Theune (2002) extend the Incremen-
tal Algorithm (IA; Dale and Reiter (1995)) to han-
dle relations. This requires a preference list over all
properties and relations to be specified in advance.
They explicitly choose to put spatial relations right
at the end of that preference list, on the basis that ?It
seems an acceptable assumption that people prefer
to describe an object in terms of simple properties,
and only shift to relations when properties do not
suffice [. . . ] it takes less effort to consider and de-
scribe only one object?.
As the referents in Varges? 2005 domain are all
points on a map distinguishable only by their spatial
relations to other objects, he has no choice but to use
relations. However, he also adopts brevity as a main
criterion for choosing which spatial relations to use.
Kelleher and Kruijff (2005, 2006) cite Clark and
Wilkes-Gibbs? (1986) Principle of Minimal Cooper-
ative Effort and Dale and Reiter?s (1995) Principle
of Sensitivity, as well as van der Sluis and Krah-
mer?s (2004) production study, to motivate the or-
dering over the types of properties that can be used
by their system; accordingly, their system only in-
cludes spatial (and hence relational) information in
a referring expression if it is not possible to construct
a description from non-relational properties.
These approaches would appear to favour the pro-
duction of referring expressions containing long se-
quences of non-relational properties when a single
relational property might do the job. We are inter-
ested, then, in whether it really is the case that rela-
tional expressions are dispreferred, and in determin-
ing when they might in fact be preferred.
To date, we are not aware of any substantial data
sets that would allow this question to be explored.
Both the TUNA corpus (Gatt et al, 2007) and the
Macquarie Drawer data (Viethen and Dale, 2006)
contain too few relational descriptions to allow us
to draw conclusions about any kind of patterns; the
GREC corpus (Belz and Varges, 2007) is not con-
cerned with content selection at all, but rather stud-
ies the form of referring expressions used over a
whole text; i.e. the choice between fully descriptive
NPs, reduced NPs, one-anaphora and pronouns.
There are a number of corpora resulting from ex-
periments involving human participants which con-
tain referring expressions, such as Brennan and
Clark?s (1996) collection of tangram descriptions,
the HCRC Map Task Corpus (Thompson et al,
1993), the COCONUT corpus (Jordan and Walker,
2005), and Byron and Fosler-Lussier?s (2006) OSU
Quake corpus. However, these contain whole
conversations between communicative partners co-
operating on a task, making it difficult to factor out
the impact of prior discourse context on the referring
expressions used.
3 The Data Gathering Experiment
3.1 General overview
We conducted a web-based production experiment
to elicit referring expressions describing singular ob-
jects in very simple scenes. The study was aimed
at shedding light on the question of whether spatial
relations are indeed as dispreferred as suggested by
the literature in those situations where non-relational
descriptions are possible.
60
The Desiderata section of the report from the
Workshop on Shared Tasks and Comparative Eval-
uation in NLG (Paris et al, 2007) emphasises the
difficulties inherent in evaluating NLG systems due
to the context dependency of language production:
the output appropriate for any given referring ex-
pression generation system entirely depends on the
particular task being performed. The data gathered
in this experiment is intended to inform the develop-
ment and evaluation of algorithms for the production
of one-shot, fully distinguishing descriptions of sim-
ple objects in 3-dimensional scenes. The experiment
is focussed on the adequate use of spatial relations in
referring expressions.
In designing the materials for the experiment, we
were conscious of a number of factors which we
might expect to have an influence on the use of spa-
tial relations: the prominence of other properties
such as colour and size (i.e. whether most objects
are of the same or similar size and colour, so that
none are very distinct from the point of view of their
direct properties); how easy it is to distinguish the
target from the other objects around it; how easy it
is to identify the target in the scene without using
any locational information; and the visual salience
of other objects which could serve as relatees in re-
lational descriptions.
3.2 Method
3.2.1 Participants
In total, 74 participants completed the experiment.
They were recruited by emailing 120 native English
speakers and asking them to pass on the call for par-
ticipation to other native or fluent English speakers.
This resulted in a range of participants from a wide
variety of backgrounds and age groups; most partic-
ipants were in their early or mid twenties.
One participant indicated they were colour-blind,
and another requested that their data be discarded.
The data for a further nine participants was ex-
cluded from the analysis for reasons outlined in Sec-
tion 3.2.4 below. Of the remaining 63 participants,
34 were male and 29 were female.
3.2.2 Materials
The stimuli for this study consisted of 20 jpeg
images of simple scenes generated using Google
SketchUp. Each scene contained three objects; each
Figure 1: Trial Set 1: The five base configurations 1?5
and their counterparts 6?10 using the other type of target?
landmark relation and orientation.
object was either a sphere or a cube. The objects
could also be either large or small and were one of
two colours; scenes either contained blue and green
objects, or red and yellow objects. The target ob-
ject, to be described by the participant, was marked
by a grey arrow pointing to it; the target was al-
ways located either directly in front of or on top of
one of the other two objects. We will refer to this
other related object as the landmark, although there
is of course no guarantee that participants actually
included it into the description as the ground object
in a spatial relation. The third object, which we refer
to as the distractor, was located either to the left or
the right of the target and landmark objects.
The 20 scenes are generated from five base con-
figurations, differing in the type and size of the ob-
jects pictured. Figure 1 shows the five base configu-
rations. They can be categorised by the length of the
shortest possible description for the target object:
? in two of the base configurations it is possible
to identify the target object using its type only;
? in one base configuration size alone would suf-
61
fice, although in line with past observations in
the literature we would expect that type is al-
ways included as well;
? in one base configuration, colour and type are
both necessary; and
? in the final base configuration, both size and
colour are necessary, and again we would ex-
pect type to be included.
Importantly, there is no configuration in which the
spatial relations between the objects are required in
order to identify the target.
For each base configuration, we generated two
scenes: in one scene, the target is located on top of
the landmark object, and in the other, the target lies
in front of the landmark. This allows us to investi-
gate whether people prefer to use one type of spatial
relation more than the other.
Five of the resulting 10 scenes were in the blue?
green colour scheme, while the other five used red
and yellow. The different colour schemes were an
attempt to decrease the monotony of the task, so
that we could show each participant more scenes.
These 10 scenes, numbered 1 through 10, consti-
tuted our first trial set. A second trial set, with scenes
numbered 11 through 20, was generated by produc-
ing the mirror image of each scene and using the
opposite colour scheme. Mirroring the scenes had
the same purpose as using the two different colour
schemes. However, to be able to control any un-
wanted effect of these two variables we always used
both variants.1
3.2.3 Procedure
On the experiment website, each participant was
shown the scenes from one of the two trial sets in
the order of the scene numbers. Under each scene,
they had to complete the sentence Please, pick up
the . . . as if they were describing the object marked
by the arrow to an onlooker.
To encourage the use of fully distinguishing refer-
ring expressions, participants were told that they had
only one chance at describing the object. They were
shown a sample scene for which they could provide
an unrecorded (and unchecked) description. After
1For brevity, where relevant we will use the form ?Scenes
n+m? to refer to paired scenes across the two trial sets.
being presented with all ten scenes in the trial, par-
ticipants were asked to complete an exit question-
naire, which also gave them the option of having
their data discarded, and asked for their opinion on
whether the task became easier over time, and any
other comments they might wish to make.
3.2.4 Data Processing
740 descriptions were elicited in the experiment. 10
of these were discarded in line with the participant?s
request, and 10 because the participant reported that
they were colour-blind. After the data was cleaned
and parsed, another 90 descriptions from 9 partici-
pants were discarded:
? One participant had consistently produced ex-
tremely long and complex descriptions using
the ternary relation between and direct refer-
ence to the onlooker, the ground and parts of
the objects: a typical example is the red cube
which rests on the ground and is between you
and the yellow cube of equal size. While these
descriptions are interesting, in relation to the
rest of the data they are such outliers that no
real conclusions can be drawn from them.
? A further eight participants consistently used
highly under-specified descriptions. We de-
cided to discard the data from these participants
since it seemed that they had not understood
the need to provide a distinguishing descrip-
tion, rather than, for example, just indicating
the type of the object.2
This resulted in a total of 630 referring expressions,
with 30 for each scene in Trial Set 1 and 33 for each
scene in Trial Set 2. We then applied some normal-
isation steps: the data was stripped of punctuation
marks and other extraneous material (such as repe-
tition of the Please, pick up the); in four cases, the
dynamic spatial preposition from was deleted from
descriptions such as the green ball from on top of
the blue cube;3 and spelling was normalised. The
2Of course, underspecified descriptions are justified in many
circumstances, and in real-life situations may even be necessary.
However, the simple scenes used in this study do not fall into
these classes.
3We are only interested in the static locative in these expres-
sions; the use of the dynamic preposition is most likely due to
the movement implied by the indicated picking-up action.
62
Figure 2: Number of participants who delivered
n (0. . . 10) relational descriptions.
second object was stripped from comparatives such
as the smaller of the two green cubes and converted
to the form the smaller green cube, which in the con-
text of our simple scenes is semantically equivalent.
3.3 Results
Over a third (231 or 36.6%) of the 630 descriptions
in the resulting corpus use spatial relations despite
the fact that relations were never necessary for the
identification of the target. These 231 relational de-
scriptions were produced by 40 (63.5%) of the 63
participants, while 23 (36.5%) of the participants
never used spatial relations. This suggests that the
use of relations is very much dependent on personal
preference, a hypothesis that is further supported by
the fact that 11 (i.e. over 25%) of the relation-using
participants did so in all 10 referring expressions
they delivered. Figure 2 shows the number of par-
ticipants who produced exactly n descriptions con-
taining at least one spatial relation, for n in the range
{0 . . . 10}.
From the above, we might hypothesise that some
participants adopt a strategy of always using rela-
tional properties, and that others adopt a strategy of
avoiding relational properties as much as possible.
We further analysed the descriptions produced by
participants who did not follow either of these two
exclusive strategies to see how their choices varied
across the different scenes; the spread is shown in
Figure 3. Looking only at the descriptions produced
by participants who sometimes, but not always, used
spatial relations allows us to get a clearer view on
which objects received most and least relational de-
scriptions. This in turn affords an analysis of the
impact the different features in the respective scenes
Figure 3: % of relational descriptions for each scene out
of all relational descriptions produced by participants not
using an exclusive strategy. Scenes are paired with their
counterpart using the other target?landmark relation.
have on the use of spatial relations.
41.7% of the remaining descriptions used rela-
tions. Interestingly, 63.6% of these relational de-
scriptions were used for scenes where the target was
located on top of the landmark object, while only
36.4% were from scenes where the target was in
front of the landmark, suggesting that the use of the
in-front-of relation may be relatively dispreferred.
Because the first scene always had the target on
top of the landmark, this preference for using rela-
tional descriptions in on-top-of scenes might be due
to a training effect that discourages people from us-
ing relations over time. However, if we do not take
into account descriptions for the first 4 scenes of
each trial set, this ratio is still large: 58.8% of the
the remaining relational descriptions stemmed from
scenes where the target was on top of the landmark,
41.2% of them from scenes with an on-top-of rela-
tion.
As expected, the orientation of the scenes and the
colour scheme used did not have a significant im-
pact on the use of spatial relations. For both these
variables, the difference between values in use of re-
lations was under 6 percentage points.
3.4 Discussion
We noted earlier that existing relation-handling re-
ferring expression generation algorithms generally
disprefer relations and only add them to a descrip-
tion if absolutely necessary. This in essence mimics
the behaviour of our participants who adopted the
63
exclusive Never-Use-Relations strategy.4 These al-
gorithms therefore only represent slightly more than
one third of the participants in our study.
The analysis of the descriptions given by people
who did not follow one of the two exclusive strate-
gies indicates that the distribution of relational de-
scriptions over the scenes is not random. In addition
to modelling exclusive strategies, then, we may also
want to capture in an algorithm the reasons why re-
ferring expressions for some scenes are more likely
to include spatial relations than others.
In the remainder of this section we consider the
conclusions that can be drawn from our data regard-
ing the factors that impact on the choice of whether
to use spatial relations in a referring expression.
Spatial Relations Are Used Even When Unnec-
essary: The main observation that can be made
is that even in very simple scenes, where locatives
are not necessary to distinguish the target from the
other objects present, people show a tendency to use
spatial relations to describe a target object to an on-
looker. This contradicts the prevailing approach to
the use of relations in referring expression genera-
tion. It is important to bear in mind that the scenes
used in this study were extremely simple and could
easily be taken in at one glance; it seems likely that
when faced with a more crowded scene containing
more complex objects, the tendency to incorporate
possibly unnecessary spatial relations into descrip-
tions would increase.
Training Effect: Note in Figure 4 that the targets
in Scenes 1+11 received a disproportionally high
number of descriptions containing spatial relations.
While this fact could be attributed to the similar fea-
tures of the two scenes (they only differed in ori-
entation and colour scheme), it is much more likely
that this is due to Scenes 1 and 11 being the first
scenes of the respective trial sets. The drop-off in
relational descriptions from beginning to end of the
trial sets almost certainly results from a training ef-
fect, where people realised over time that relations
were not necessary in any of the scenes. If we only
consider the first two scenes in each trial set, where
no training effect has taken hold, we find that 36 of
4On the assumption that these participants would also resort
to relations if they had to.
Figure 4: % of relational descriptions for each scene out
of all relational descriptions produced by participants not
using an exclusive strategy. Scenes are paired with their
counterpart from the other trial set.
the 58 (62.1%) descriptions for these scenes use spa-
tial relations. The presence of some kind of training
effect was also reported in the exit questionnaire by
half of the participants.
This training effect in itself is an interesting phe-
nomenon. It suggests that people are much more
likely to use spatial relations when they come anew
to the task of identifying an object rather than when
they are describing an object in a similar domain on
a subsequent occasion.
Landmark Salience Encourages Use of Relations:
Figure 4 shows that the highest spike in usage of spa-
tial relations was recorded for Scenes 3+13; interest-
ingly another, although much less pronounced, peak
occurs for their counterpart scenes only differing in
the type of target?landmark relation, 8+18.
These peaks cannot be explained by the training
effect; in fact, they seem to be running contrary to it,
indicating that some other reasonably strong factors
are prompting the use of relations in these scenes.
Scenes 3, 8, 13, and 18 are the only scenes in
which the landmark object is distinguishable from
both other objects only by its type (cube) or its
colour (see Figure 1). In addition, in each case the
landmark is large resulting in high visual salience for
the landmark. This in turn makes the relation to the
landmark a salient feature of the target. The salience
of the relation then causes people to add it to an al-
ready distinguishing description or even to prefer it
64
over the use of absolute properties.
on top of Is Preferred over in front of: Although
these four scenes all share the same base set of ob-
jects, the usage of spatial relations is considerably
higher for Scenes 3+13 than for 8+18. This could
either be entirely due to the training effect, but may
also be influenced by the only difference between
these two scenes: in Scenes 3+13, the target sits on
top of the landmark, while in Scenes 8+18 it is ly-
ing in front of the landmark. The overall compari-
son of the data for scenes featuring an on-top-of re-
lation with that for scenes with an in-front-of relation
suggests that this also is a factor. Even if we only
take into account Scenes 5?10 and 15?20, where we
might expect the effect of training to have stabilised,
people were almost one and a half times more likely
to use a relation in a scene where the target was on
top of rather than in front of the landmark (30 vs. 21
of the 111 relational descriptions for those scenes
from people not using an exclusive strategy).
This finding is in accordance with Kelleher and
Kruijff?s (2006) approach of preferring topological
spatial relations over projective ones. The seman-
tics of projective spatial relations, such as in front
of, depend on a frame of reference defining direc-
tions from some origin (in this case the landmark
object), while topological relations, such as on top
of, are semantically defined by relations such as in-
tersection, containedness, and contiguity, and pose
a lighter cognitive load on both discourse partners
(see Tenbrink (2005) for an overview).
The impact of landmark salience and the pref-
erence for the on-top-of relation can also explain
the low use of spatial relations for Scenes 4+14,
6+16 and 10+20 (see Figure 1). In these scenes it
is very hard or even impossible to distinguish the
landmark from the other objects using only non-
relational properties, and the target is located in front
of rather than on top of it. The possibility of de-
scribing the target in Scenes 6+16 only by its type
or colour may be the reason for the extremely low
usage of spatial relations in these scenes.
4 Conclusions
4.1 Consequences for Algorithm Development
We noted above that some participants adopted a
Never-Use-Relations strategy, and some adopted
an Always-Use-Relations stratgy. This might be
modelled by the use of a parameter akin to the
the Risky/Cautious distinction proposed by Carletta
(1992) in her work on references in the Map Task
corpus. The effect of this parameter in the context
of the Incremental Algorithm would be to put spa-
tial relations either at the front or at the end of the
preference list of properties; this would ensure that
they are either considered first for inclusion into a
referring expression, or only when the other proper-
ties of the target do not suffice.
A more interesting problem is how to model the
apparent preference of our participants to use rela-
tions in some scenes more than in others. Follow-
ing our discussion above, the factors that lead to this
preference seem to include the folllowing:
? the ease with which a potential landmark can
be distinguished from the other objects in the
scene;
? the visual salience of a potential landmark (in
our case its size);
? the type of spatial relation between the target
and a potential landmark; and
? the ease with which the target can be described
without the use of spatial relations.
The visual salience of the target object is likely to
also play a role; however, this was not tested in the
current study, since all target objects were small.
Factors like these can be incorporated into a refer-
ring expression generation algorithm by taking them
into account in the step that calculates which prop-
erty of the target object should next be considered
for inclusion in the referring expression. Instead of
using a static preference list over all possible do-
main properties, a preference score for each prop-
erty needs to be determined ?at run time?. Such a
dynamic approach would also allow the considera-
tion of the discourse salience of a property (perhaps
due to its recent use in another referring expression),
as well as the consideration that some properties are
more likely to be used in combination with other
specific properties. An example of this phenomenon
is the combination of the property hair-colour with
either has-hair or has-beard in the TUNA data. If
hair-colour is included in a referring expression, at
65
least one of the other two properties is present as
well.
The preference scores of the properties in a re-
ferring expression under construction would then
combine into an adequacy score for the overall de-
scription, similar to Edmonds? (1994) concept of the
speaker?s confidence that a referring expression suf-
fices for the communicative task at hand.
4.2 Future Work
As a next step, we aim to run experiments to sepa-
rately confirm the impact that each of the different
factors listed in Section 3.4 has on the use of spa-
tial relations in referring expressions. In parallel,
we will evaluate the human-produced descriptions
in task-based evaluation schemes to assess whether
the use of relations in certain categories of scenes is
advantageous for an onlooker trying to identify the
object that is being referred to.
Ultimately, the aim of this research is to develop
an algorithm that incorporates the findings from both
types of studies into the generation of referring ex-
pressions. Such an algorithm should not simply
mimic the behaviour that our participants have dis-
played during the production experiment, but also
take into account the findings of the task-based
study, to ensure both naturalness and usefulness for
the listener.
References
Anja Belz and Sebastian Varges. 2007. Generation of re-
peated references to discourse entities. In Proceedings
of the 11th European Workshop on Natural Language
Generation, pages 9?16.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 22:1482?1493.
Donna K. Byron and Eric Fosler-Lussier. 2006.
The OSU Quake 2004 corpus of two-party situated
problem-solving dialogs. In Proceedings of the 15th
Language Resources and Evaluation Conference.
Jean C. Carletta. 1992. Risk-taking and Recovery in
Task-Oriented Dialogue. Ph.D. thesis, University of
Edinburgh.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22(1):1?
39.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, Vancouver, BC.
Philip G. Edmonds. 1994. Collaboration on reference to
objects that are not mutually known. In Proceedings of
the 15th International Conference on Computational
Linguistics, Kyoto, Japan.
Claire Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadelphia, USA.
Albert Gatt and Kees van Deemter. 2006. Conceptual
coherence in the generation of referring expressions.
In Proceedings of the 21st COLING and the 44th ACL
Conference, Sydney, Australia.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation, pages 49?56.
Sarah Haywood, Martin J. Pickering, and Holly P. Brani-
gan. 2003. Co-operation and co-ordination in the
production of noun phrases. In Proceedings of the
25th Annual Meeting of the Cognitive Science Society,
pages 533?538, Boston, MA.
Helmut Horacek. 2004. On referring to sets of objects
naturally. In Proceedings of the 3rd International Con-
ference on Natural Language Generation, pages 70?
79, Brockenhurst, UK.
Pamela W. Jordan and Marilyn A. Walker. 2005. Learn-
ing content selection rules for generating object de-
scriptions in dialogue. Journal of Artificial Intelli-
gence Research, 24:157?194.
John Kelleher and Geert-Jan M. Kruijff. 2005. A
context-dependent model of proximity in physically
situated environments. In Proceedings of the 2nd
ACL-SIGSEM Workshop on The Linguistic Dimen-
sions of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colchester,
U.K.
John Kelleher and Geert-Jan M. Kruijff. 2006. Incre-
mental generation of spatial referring expressions in
situated dialog. In Proceedings of the 21st COLING
and the 44th ACL Conference, Sydney, Australia.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expressions.
In Kees van Deemter and Rodger Kibble, editors, In-
formation Sharing: Reference and Presupposition in
Language Generation and Interpretation, pages 223?
264. CSLI Publications, Stanford, CA.
66
Ce?cile Paris, Donia Scott, Nancy Green, Kathy McCoy,
and David McDonald. 2007. Desiderata for evalua-
tion of natural language generation. In Robert Dale
and Michael White, editors, Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation in
Natural Language Generation, pages 9?15, Arlington,
VA.
Rosemary Stevenson. 2002. The role of salience in the
production of referring expressions: A psycholinguis-
tic perspective. In Kees van Deemter and Rodger Kib-
ble, editors, Information Sharing: Reference and Pre-
supposition in Language Generation and Interpreta-
tion. CSLI, Stanford.
Thora Tenbrink. 2005. Semantics and application of spa-
tial dimensional terms in English and German. Tech-
nical Report Series of the Transregional Collabora-
tive Research Center SFB/TR 8 Spatial Cognition, No.
004-03/2005, Universities of Bremen and Freiburg,
Germany.
Henry S. Thompson, Anne Anderson, Ellen Gurman
Bard, Gwyneth Doherty-Sneddon, Alison Newlands,
and Cathy Sotillo. 1993. The HCRC map task cor-
pus: natural dialogue for speech recognition. In Pro-
ceedings of the 1993 Workshop on Human Language
Technology, pages 25?30, Princeton, New Jersey.
Kees van Deemter. 2006. Generating referring expres-
sions that involve gradable properties. Computational
Linguistics, 32(2):195?222.
Ielka van der Sluis and Emiel Krahmer. 2004. The
influence of target size and distance on the produc-
tion of speech and gesture in multimodal referring
expressions. In Proceedings of the 8th International
Conference on Spoken Language Processing (INTER-
SPEECH 2004), Jeju, Korea.
Sebastian Varges. 2005. Spatial descriptions as referring
expressions in the maptask domain. In Proceedings
of the 10th European Workshop On Natural Language
Generation, Aberdeen, UK.
Jette Viethen and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the 4th International
Conference on Natural Language Generation, pages
63?70, Sydney, Australia, July.
67
The GREC Challenge: Overview and Evaluation Results
Anja Belz Eric Kow
NLT Group
University of Brighton
Brighton BN2 4GJ, UK
{asb,eykk10}@bton.ac.uk
Jette Viethen
Centre for LT
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Albert Gatt
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
a.gatt@abdn.ac.uk
Abstract
The GREC Task at REG?08 required partici-
pating systems to select coreference chains to
the main subject of short encyclopaedic texts
collected from Wikipedia. Three teams sub-
mitted a total of 6 systems, and we addition-
ally created four baseline systems. Systems
were tested automatically using a range of ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying coreference
resolution tools to the outputs and measuring
the success of the tools. In addition, systems
were tested in a reading/comprehension exper-
iment involving human subjects. This report
describes the GREC Task and the evaluation
methods, gives brief descriptions of the par-
ticipating systems, and presents the evaluation
results.
1 Introduction
The GREC task is about how to generate appropri-
ate references to an entity in the context of a piece
of discourse longer than a sentence. Rather than
requiring participants to generate referring expres-
sions from scratch, the GREC data provides sets of
possible referring expressions for selection. As this
is a new referring expression generation (REG) task,
the shared task definition was kept fairly simple
and the aim for participating systems was to select
the appropriate type of referring expression (more
specifically, its REG08-TYPE, full details below).
The immediate motivating application context for
the GREC Task is the improvement of referential
clarity and coherence in extractive summarisation
by regenerating referring expressions in summaries.
There has recently been a small flurry of work in
this area (Steinberger et al, 2007; Nenkova, 2008).
In the longer term, the GREC Task is intended to be a
step in the direction of the more general task of gen-
erating referential expressions in discourse context.
The GREC Task Corpus is an extension of GREC
1.0 which had about 1,000 texts in the subdomains
of cities, countries, rivers and people (Belz and
Varges, 2007a). for the purpose of the REG?08 GREC
Task, we obtained an additional 1,000 texts in the
new subdomain of mountain texts and developed a
new XML annotation scheme (Section 2.2).
Five teams from four countries registered for the
GREC Task, of which three teams eventually submit-
ted 6 systems. We also used the corpus texts them-
selves as ?system? outputs, and created four base-
line systems. We evaluated the resulting 10 sys-
tems using a range of intrinsic and extrinsic evalu-
ation methods. This report presents the results of all
evaluations (Section 6), along with descriptions of
the GREC data and task (Section 2), test sets (Sec-
tion 3), evaluation methods (Section 4), and partici-
pating systems (Section 5).
2 Data and Task
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introductory
sections in Wikipedia articles, in five different do-
mains (cities, countries, rivers, people and moun-
tains). In each text, three broad categories of Main
Subject Reference (MSR)1 have been annotated, re-
1The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
183
sulting in a total of about 13,000 annotated REs.
The corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results on
the development data. Participants had 48 hours to
submit outputs for the (previously unseen) test data.
2.1 Types of referential expression annotated
Three broad categories of main subject referring ex-
pression (MSREs) are annotated in the GREC corpus2
? subject NPs, object NPs, and genitive NPs and pro-
nouns which function as subject-determiners within
their matrix NP. These categories of referring ex-
pression (RE) are relatively straightforward to iden-
tify and achieve high inter-annotator agreement on
(complete agreement among four annotators in 86%
of MSRs), and account for most cases of overt main
subject reference (MSR) in the GREC texts. The an-
notators were asked to identify subject, object and
genitive subject-determiners and decide whether or
not they refer to the main subject of the text. More
detail is provided in Belz and Varges (2007b).
In addition to the above, relative pronouns in sup-
plementary relative clauses (as opposed to integrated
relative clauses, Huddleston and Pullum, 2002, p.
1058) were annotated, e.g.:
(1) Stoichkov is a football manager and former striker who
was a member of the Bulgaria national team that
finished fourth at the 1994 FIFA World Cup.
We also annotated ?non-realised? subject MSREs
in a restricted set of cases of VP coordination where
an MSRE is the subject of the coordinated VPs, e.g.:
(2) He stated the first version of the Law of conservation of
mass, introduced the Metric system, and helped to
reform chemical nomenclature.
The motivation for annotating the approximate
place where the subject NP would be if it were re-
alised (the gap-like underscores above) is that from
a generation perspective there is a choice to be made
about whether to realise the subject NP in the second
and third coordinates or not.
(and title) of one text is London.
2In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
2.2 XML format
Figure 1 is one of the texts distributed in the GREC
data sample for the REG Challenge. The REF el-
ement indicates a reference, in the sense of ?an
instance of referring? (which could, in principle,
be realised by gesture or graphically, as well as
by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cat-
egory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX el-
ement (the ?selected? referential expression for the
given reference; in the corpus texts it is simply
the referential expression found in the corpus) and
one ALT-REFEX element which in turn is a list of
REFEXs which are alternative referential expressions
obtained by other means (see following section).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in the
present version of the GREC corpus, the only type of
RE that has this attribute is one which incorporates
a reflexive pronoun used emphatically (e.g. India it-
self ). The REG08-TYPE attribute indicates basic RE
type as required for the REG?08 GREC task defini-
tion. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether to
use an RE that includes a name, such as Modern In-
dia (the corresponding REG08-TYPE value is name);
whether to go for a common-noun RE, i.e. with a
category noun like country as the head (common);
whether to pronominalise the RE (pronoun); or
whether it can be left unrealised (empty).
2.3 The REG?08 GREC Task
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to the
184
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE TEXT SYSTEM "reg08-grec.dtd">
<TEXT ID="36">
<TITLE>Jean Baudrillard</TITLE>
<PARAGRAPH>
<REF ID="36.1" SEMCAT="person" SYNCAT="np-subj">
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX>
<REFEX REG08-TYPE="empty">_</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX>
</ALT-REFEX>
</REF>
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det">
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX>
<ALT-REFEX>
<REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX>
<REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX>
</ALT-REFEX>
</REF>
work is frequently associated with postmodernism and post-structuralism.
</PARAGRAPH>
</TEXT>
Figure 1: Example text from REG?08 Training Data.
training/development data, except that REF elements
contained only an ALT-REFEX list, not the preced-
ing ?selected? REFEX. ALT-REFEX lists are gener-
ated for each text by an automatic method which
collects all the (manually annotated) MSREs in a text
including the title and adds several defaults: pro-
nouns and reflexive pronouns in all subdomains; and
category nouns (e.g. the river), in all subdomains
except people. The main objective in the REG?08
GREC Task was to get the REG08-TYPE attribute of
REFEXs right.
3 Test Data
1. GREC Test Set C-1: a randomly selected 10%
subset (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. GREC Test Set C-2: the same subset of texts as
in C-1; however, for C-2 we did not use the MSREs
in the corpus, but replaced each of them with three
human-selected alternatives. These were obtained in
an online experiment as described in Belz & Varges
(2007a) where subjects selected MSREs in a setting
that duplicated the conditions in which the partici-
pating systems in the REG?08 GREC Task made se-
lections.3 We obtained three versions of each text,
where in each version all MSREs were selected by
the same person. The motivation for creating this
version of Test Set C was firstly that having sev-
eral human-produced chains of MSREs to compare
the outputs of participating (?peer?) systems against
is more reliable than having one only; and secondly
that Wikipedia texts are edited by multiple authors
and so MSR chains may sometimes be adversely af-
fected by this; we wanted to have additional refer-
ence texts without this characteristic.
3. GREC Test Set L: 74 Wikipedia introductory
texts from the subdomain of lakes; participants did
not know what this subdomain was until they re-
ceived the test data (there were no lake texts in the
training/development set).
4. GREC Test Set P: 31 short encyclopaedic texts
in the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but from a source other than
3The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
185
Wikipedia. We transcribed these texts from printed
encyclopaedias published in the 1980s which are
not available in electronic form, and this provenance
was not revealed to participants. The texts in this set
are much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs follow
very similar patterns. It seems likely that it is these
properties that have resulted in better scores overall
for Test Set P (see Section 6).
Each test set was designed to test peer systems for
a different aspect of generalisation. Test Set C tests
for generalisation to unseen material from the same
corpus and the same subdomains as the training set;
Test Set L tests for generalisation to unseen material
from the same corpus but different subdomain; and
Test Set P tests generalisation to a different corpus
but same subdomains.
4 Evaluation methods
4.1 Automatic intrinsic evaluations
Accuracy of REG08-Type: when computed against
the single-RE test sets (C-1, L and P), REG08-Type
Accuracy is the proportion of REFEXs selected by a
participating system that have a REG08-TYPE value
identical to the one in the corpus.
When computed against the triple-RE test set (C-
2), first the number of correct REG08-Types is com-
puted at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number of
REFs in all the texts, which gives the global REG08-
Type Accuracy score. The rationale behind com-
puting the REG08-Type Accuracy scores in this way
for multiple-RE test sets (maximising scores on RE
chains rather than individual REs) is that an RE is
not good or bad in its own right, but depends on the
other MSRs in the same text.4
String Accuracy: This is defined just like
REG08-Type Accuracy, except here what is deter-
mined is identity between REFEX word strings (the
MSREs themselves), not between REG08-Types.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
4This definition is also slightly different from the one given
in the Participants? Pack.
cost of 1. We also used the version of string-edit
distance described by Bangalore et al (2000) which
normalises for length. This version is denoted ?SEB?
below. For the single-RE test sets, the global score
is simply the average of all RE-level scores. For Test
Set C-2, we used an approach analogous to that de-
scribed above for REG08-Type Accuracy. We first
computed the best string-edit distance at the text
level (here, just the sum of RE-level distances) and
then obtained the global distance by dividing the
sum of best text-level distances by the number of
REFs in all the texts.
Other metrics: BLEU is a precision metric from
MT that assesses the quality of a peer translation
in terms of the proportion of its word n-grams
(n ? 4 is standard) that it shares with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in the
corpus are less than 4 tokens long. We also used
the NIST version of BLEU which weights in favour
of less frequent n-grams, as well as ROUGE-2 and
ROUGE-SU4 (the two official automatic scores from
the DUC summarisation competitions). In all cases,
we assessed just the MSREs selected by peer systems
(leaving out the surrounding text), and computed
scores globally (rather than averaging over RE-level
scores), as this is standard for these metrics.
BLEU, NIST and ROUGE are designed to work
with either one or multiple reference texts, so we did
not need to use a different method for Test Set C-2.
4.2 Human extrinsic evaluation
We designed a reading/comprehension experiment
in which the task for subjects was to read texts
one sentence at a time and then to answer three
brief multiple-choice comprehension questions after
reading each text. The basic idea was that it seemed
likely that badly chosen MSR reference chains would
adversely affect ease of comprehension, and that this
might in turn affect reading speed and accuracy in
answering comprehension questions.
We used a randomly selected subset of 21 texts
from Test Set C, and recruited 21 subjects from
among the staff, faculty and students of Brighton
and Sussex universities. We used a Repeated Latin
Squares design in which each combination of text
and system was allocated three trials. During the
experiment we recorded SRTime, the time subjects
186
took to read sentences (from the point when the sen-
tence appeared on the screen to the point at which
the subject requested the next sentence).
We also recorded the speed and accuracy with
which subjects answered the questions at the end (Q-
Time and Q-Acc). The role of the comprehension
questions was to encourage subjects to read the texts
properly, rather than skimming through them, and
we did not necessarily expect any significant results
from the associated measures.
The questions were designed to be of varying de-
grees of difficulty and predictability. There was one
set of three questions (each with five possible an-
swers) associated with each text, and questions fol-
lowed the same pattern across the texts: the first
question was always about the subdomain of a text;
the second about the location of the main subject; the
third question was designed not to be predictable.
The order of the answers was randomised for each
question and each subject. The order of texts (with
associated questions) was randomised for each sub-
ject. We used the DMDX package for presentation
of sentences and measuring reading times and ques-
tion answering accuracy (Forster and Forster, 2003).
Subjects did the experiment in a quiet room, under
supervision.
4.3 Automatic extrinsic evaluation
As a new and highly experimental method, we tried
out an automatic approach to extrinsic evaluation.
The basic idea was similar to that in the human-
based experiments described above: badly chosen
reference chains seem likely to affect the reader?s
ability to resolve REs. In the automatic version, the
role of the reader is played by an automatic coref-
erence resolution tool and the expectation is that the
tool performs worse (are less able to identify coref-
erence chains correctly) with worse MSR reference
chains.
To counteract the potential problem of results be-
ing a function of a specific coreference resolution
algorithm or tool, we decided to use three differ-
ent resolvers?those included in LingPipe,5 JavaRap
(Qiu et al, 2004) and OpenNLP (Morton, 2005)?
and to average results.
There does not appear to be a single standard eval-
5http://alias-i.com/lingpipe/
uation metric in the coreference resolution commu-
nity, so we opted to use three: MUC-6 (Vilain et al,
1995), CEAF (Luo, 2005), and B-CUBED (Bagga and
Baldwin, 1998), which seem to be the most widely
accepted metrics.
All three metrics compute Recall, Precision and
F-Scores on aligned gold-standard and resolver-tool
coreference chains. They differ in how the align-
ment is obtained and what components of corefer-
ence chains are counted for calculating scores. Re-
sults for the automatic extrinsic evaluations are re-
ported below in terms of the F-Scores from these
three metrics, as well as in terms of their average.
5 Systems
Base-rand, Base-freq, Base-1st, Base-name: We
created four baseline systems. Base-rand selects
one of the REFEXs at random. Base-freq selects
the REFEX that is the overall most frequent given
the SYNCAT and SEMCAT of the reference. Base-
1st always selects the REFEX which appears first
in the list of REFEXs; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.6
CNTS-Type-g, CNTS-Prop-s: The CNTS sys-
tems are trained using memory-based learning with
automatic parameter optimisation. They use a set of
14 features obtained by various kinds of syntactic
preprocessing and named-entity recognition as well
as from the corpus annotations: SEMCAT, SYNCAT,
position of RE in text, neighbouring words and POS-
tags, distance to previous mention, SYNCATs of
three preceding REFEXs, binary feature indicating
whether the most recent named entity was the main
subject (MS), main verb of the sentence. For Type-
g, a single classifier was trained to predict just the
REG08-TYPE property of REFEXs. For Prop-s, four
classifiers were trained, one for each subdomain, to
predict all four properties of REFEXs (rather than just
REG08-TYPE).
OSU-b-all, OSU-b-nonRE, OSU-n-nonRE: The
OSU-2 systems are maximum-entropy classifiers
trained on a range of features obtained by prepro-
6Attributes are tried in this order. If for one attribute, the
right value is not found, the process ignores that attribute and
moves on the next one.
187
System REG08-Type Accuracy for Development SetAll Cities Coun Riv Peop Moun
CNTS-Type-g 76.52 64.65 75 65 85.37 75.42
CNTS-Prop-s 73.93 65.66 69.57 70 79.51 74.58
IS-G 66 54.5 64 80 66.8 65
OSU-n-nonRE 62.50 53.54 63.04 65 67.32 61.67
OSU-b-all 58.54 53.54 57.61 75 65.85 49.58
OSU-b-nonRE 51.07 51.52 53.26 40 57.07 45.83
Table 1: Self-reported REG08-Type Accuracy scores for
development set.
cessing the text, as well as from the corpus anno-
tations: SEMCAT, SYNCAT, position of RE in text,
presence of contrasting discourse entity, distance be-
tween current and preceding reference to the MS,
string similarity measures between REFEXs and ti-
tle of text. OSU-b-all and OSU-b-nonRE are binary
classifiers which give the likelihood of selecting a
given REFEX vs. not selecting it, whereas OSU-n-
nonRE is a 4-class classifier giving the likelihoods
of selecting each of the four REG08-TYPEs. OSU-
b-all also uses the REFEX attributes as features.
IS-G: The IS-G system is a multi-layer percep-
tron which uses four features obtained by prepro-
cessing texts as well as from the corpus annota-
tions: SYNCAT, distance between current and pre-
ceding reference to the MS, position of RE in text,
REG08-TYPE of preceding reference to the MS, fea-
ture indicating whether the preceding MSR is in the
same sentence.
6 Results
This section presents the results of all the evalua-
tion methods described in Section 4. We start with
REG08-Type Accuracy, an intrinsic automatic met-
ric which participating teams were told was going
to be the chief evaluation method, followed by other
intrinsic automatic metrics (Section 6.2), the extrin-
sic human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
6.1 REG08-Type Accuracy
Participants computed REG08-Type Accuracy for
the development set (97 texts) themselves, using a
tool provided by us. These scores are shown in
Table 1, and are also included in the participants?
reports elsewhere in this volume. Systems are or-
dered in terms of their overall REG08-Type Accu-
racy (column 1), and scores for each subdomain are
also shown. Scores are highly consistent across the
subdomains, except for the river subdomain which
was the smallest set (containing only 4 texts), and
results for it may be idiosyncratic for this reason.
Corresponding results for the (unseen) test set C-1
are shown in column 2 of Table 2. As would be ex-
pected, results are slightly worse than for the (seen)
development set (although some systems managed
to improve over their development set scores). Also
included in this table are results for the four base-
line systems, and it is clear that selecting the most
frequent REG08-Type given SEMCAT and SYNCAT
(as done by the Base-freq system) provides a strong
baseline.
Other columns in Table 2 contain results for test
sets L and P. Again as expected, results for Test Set
L are lower than for Test Set C-1, because in ad-
dition to consisting of unseen texts (like C-1), Test
Set L is also from an unseen subdomain (unlike C-
1). The results for Test Set P are higher and on a par
with those for the development set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 2 we carried out a uni-
variate ANOVA with System as the fixed factor. We
found significant main effects at p < .001 in all
three cases (C-1: F = 95.426; L: F = 63.758;
P: F = 21.188). The columns containing capital
letters in Table 2 show the homogeneous subsets of
systems as determined by post-hoc Tukey HSD com-
parisons of means. Systems whose REG08-Type Ac-
curacy scores are not significantly different (at the
.05 level) share a letter.
The results for REG08-Type Accuracy computed
against the triple-RE Test Set C-2 are shown in Ta-
ble 3. These should be considered as the chief results
of the GREC Task evaluations, as stated in the guide-
lines. Here too we performed a univariate ANOVA
with System as the fixed factor and REG08-Type
as the dependent variable. Having established by
ANOVA that there was a significant main effect of
System (F = 86.946, p < .001), we compared the
mean scores with Tukey?s HSD. As can be seen from
the resulting homogeneous subsets, there is no sig-
nificant difference between the corpus texts (C-1)
and system CNTS-Type-g, but also there is no sig-
188
single-RE Test Set C-1 Test Set L Test Set P
CNTS-Type-g 68.15 A CNTS-Type-g 62.06 A CNTS-Type-g 75.31 A
CNTS-Prop-s 67.04 A CNTS-Prop-s 62.06 A CNTS-Prop-s 72.84 A B
IS-G 66.48 A IS-G 60.93 A IS-G 67.90 A B C
OSU-n-nonRE 63.69 A OSU-n-nonRE 41.80 B OSU-n-nonRE 66.67 A B C
OSU-b-nonRE 53.11 B OSU-b-nonRE 39.23 B OSU-b-all 57.41 B C D
OSU-b-all 52.39 B OSU-b-all 37.62 B C OSU-b-nonRE 56.17 C D
Base-freq 43.47 C Base-freq 35.53 B C Base-freq 44.44 D F
Base-name 39.49 C Base-rand 23.63 C D Base-rand 33.95 F
Base-1st 39.17 C Base-name 23.63 D Base-name 32.10 F
Base-rand 32.72 D Base-1st 29.74 D Base-rand 32.10 F
Table 2: REG08-Type Accuracy scores and homogeneous subsets (Tukey HSD, alpha = .05) for single-RE test sets.
Systems that do not share a letter are significantly different.
System REG08-Type Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People Mountains
Corpus 78.58 A 70.92 77.49 85.29 84.67 75.81
CNTS-Type-g 72.61 A B 65.96 71.73 73.53 77.64 70.73
CNTS-Prop-s 71.34 B 64.54 67.02 70.59 75.38 71.75
IS-G 70.78 B 69.50 65.45 76.47 76.88 67.89
OSU-n-nonRE 69.82 B 65.25 64.92 79.41 78.14 65.65
OSU-b-nonRE 58.76 C 52.48 60.73 50.00 59.80 59.55
OSU-b-all 57.48 C 53.90 58.64 47.06 59.05 57.52
Base-name 50.00 D 53.19 54.45 35.29 43.22 53.86
Base-1st 49.28 D 53.19 49.21 38.24 43.22 53.86
Base-freq 48.17 D 43.97 42.41 55.88 56.78 44.11
Base-rand 41.24 E 41.84 36.13 32.35 44.47 41.06
Table 3: REG08-Type Accuracy scores against Test Set C-2 for complete set and for subdomains; homogeneous subsets
(Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are significantly different).
nificant difference between the latter and systems
CNTS-Prop-s, IS-G and OSU-n-nonRE. In this anal-
ysis, all systems outperform the random baseline;
all peer systems outperform all of the baselines; and
the four best peer systems outperform the remaining
two.
6.2 Other automatic intrinsic metrics
In addition to the chief evaluation measure reported
on in the preceding section, we computed the string
similarity metrics described in Section 4.1 for all
four test sets. Results were very similar to those
for REG08-Type Accuracy, so we are reporting only
scores for Test Set C-2 (Table 4). The corpus texts
again receive the best scores across the board (SE is
the odd one out, because here lower scores are bet-
ter). Ranks for peer systems are very similar to the
results reported in the last section.
We performed an ANOVA (F = 138.159, p <
.001) and Tukey HSD post-hoc analysis for String
Accuracy. The resulting homogeneous subsets (Ta-
ble 4, columns 3?8) reveal significant differences
similar to those for REG08-Type Accuracy. We also
computed Pearson product-moment correlation co-
efficients between all automatic intrinsic evaluation
measures we used. All pairwise correlations were
significant at the .01 level (using a two-tailed test).
One of the strongest correlations (.961) was between
REG08-Type Accuracy and String Accuracy, imply-
ing that getting REG08-Type right gets you some
way towards getting the actual RE right.
6.3 Human-based extrinsic measures
As a result of the experiment described in Sec-
tion 4.2 we had SRTime measures (sentence reading
times) for each sentence in each of the 21 texts that
were included in the experiment. Table 5 shows the
resulting SRTimes in milliseconds averaged per sys-
tem. None of the differences were statistically sig-
nificant. We also analysed SRTimes normalised by
sentence length; SRTimes only from sentences that
contained MRSs; and SRTimes normalised for sub-
ject reading speed. There were no significant differ-
ences under any of these analyses.
Much of the variance in SRTimes was due to sub-
jects? very different average reading speeds: means
of SRTime normalised for sentence length ranged
from 188.45ms to 426.10ms for individual subjects.
189
System Word string similarity for Triple-RE Test Set C-2String Accuracy BLEU-3 NIST ROUGE-2 ROUGE-SU4 SE SEB
Corpus 71.18 A 0.7792 7.5080 0.66102 0.70991 0.7229 0.5136
CNTS-Type-g 65.61 A B 0.7377 6.1288 0.60280 0.64998 0.8838 0.3627
CNTS-Prop-s 65.29 A B 0.6760 5.9338 0.60103 0.64963 0.9068 0.3835
OSU-n-nonRE 63.85 B C 0.6715 5.7745 0.53395 0.57459 0.9666 0.0164
IS-G 58.20 C 0.5107 5.6102 0.50270 0.57052 1.1616 0.1818
OSU-b-nonRE 51.11 D 0.4964 5.5363 0.38255 0.42969 1.2834 0.0247
OSU-b-all 50.72 D 0.5050 5.6058 0.35133 0.39570 1.2994 0.3402
Base-freq 41.32 E 0.2684 3.0155 0.27727 0.33007 1.54299 -0.3250
Base-name 39.41 E 0.4641 5.9372 0.20730 0.25379 1.5175 -0.1912
Base-1st 39.09 E 0.3932 5.1597 0.21443 0.24037 1.6449 -0.0751
Base-rand 17.99 F 0.2182 2.9327 0.36056 0.41847 2.3217 -0.7937
Table 4: String Accuracy, BLEU, NIST, ROUGE and string-edit scores, computed on single-RE and triple-RE test
sets (systems in order of String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy only
(systems that do not share a letter are significantly different).
Mean SRTime (msecs)
CNTS-Prop-s 6305.8551
IS-G 6340.5131
OSU-n-nonRE 6422.5073
CNTS-Type-g 6435.6574
OSU-b-all 6451.7624
OSU-b-nonRE 6454.6749
Corpus 6548.2734
Table 5: Mean SRTimes for each system.
There was also variance from Text, i.e. some of the
texts appear to be harder to read than others.
The other two measures from the task-
performance experiment were Q-Acc (question
answering accuracy) and Q-Time (question answer-
ing speed). ANOVAs revealed no significant main
effect of System on Q-Time. For Q-Acc, we looked
at each of the three question types Q1, Q2, Q3
(see Section 4.2) separately. ANOVAs showed no
significant effect of System on Q-Acc for Q2 and
Q3; there was a slight effect (F = 2.193, p < .05)
of System on Q-Acc for Q1 (the easiest of the
questions which simply asked for the subdomain
of a text). Table 6 shows Q-Acc for Q1 and Q2,
and the results of a post-hoc analysis (Tukey HSD)
which revealed two homogeneous subsets with a lot
of overlap (columns 2 and 3).
Table 6 shows the results of this analysis: there
was
6.4 Automatic extrinsic measures
We used the same 21 texts as in the human extrin-
sic experiments, fed the outputs of each peer sys-
Question 1 Q2 Q3
Corpus 1.00 A .78 .63
CNTS-Type-g 1.00 A .83 .71
CNTS-Prop-s .98 A B .86 .75
OSU-b-nonRE .97 A B .83 .67
OSU-b-all .95 A B .75 .62
IS-G .95 A B .81 .63
OSU-n-nonRE .90 B .76 .76
Table 6: Question types 1?3, proportions correct; homo-
geneous subsets for Q1 (Tukey HSD, alpha = .05).
tem as well as the corpus texts through the three
coreference resolvers, and computed average MUC,
CEAF and B-CUBED F-Scores as described in Sec-
tion 4.3. The second column Table 7 shows the av-
erage of these three F-Scores, to give a single over-
all result for this evaluation method. A univariate
ANOVA with the average F-Score (column 2) as the
dependent variable and System as the single fixed
factor revealed a significant main effect of System
on average F-Score (F = 5.051, p < .001). A
post-hoc comparison of the means (Tukey HSD, al-
pha = .05) found the significant differences indi-
cated by the homogeneous subsets in columns 3?
5 (Table 7). The numbers shown in the last three
columns are the separate MUC, CEAF and B-CUBED
F-Scores for each system, averaged over the three
resolver tools. ANOVAs revealed the following ef-
fects of System: on CEAF F = 9.984, p < .001;
on MUC: F = 10.07, p < .001; on B-CUBED:
F = 8.446, p < .001.
The three F-Score measures (MUC, CEAF and B-
CUBED) are all strongly and highly significantly cor-
190
related: Pearson?s correlation coefficient is .947 for
B-CUBED and CEAF, .917 for B-CUBED and MUC,
and .951 for CEAF and MUC (p < .01, 2-tailed).
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-1st 53.50 A 47.59 52.64 60.28
Base-name 52.84 A 45.99 51.73 60.81
OSU-n-nonRE 51.39 A 46.92 49.8 57.45
OSU-b-nonRE 51.27 A 47.68 48.62 57.50
OSU-b-all 50.87 A 47.06 48.40 57.14
CNTS-Type-g 48.64 A B 43.77 46.32 55.82
IS-G 48.05 A B 43.25 46.24 54.66
CNTS-Prop-s 46.35 A B 42.82 43.36 52.88
Corpus 43.32 A B 37.89 41.6 50.47
Base-freq 41.41 B C 34.48 40.28 49.46
Base-rand 35.13 C 21.24 35.60 48.55
Table 7: MUC, CEAF and B-CUBED F-Scores for all sys-
tems; homogeneous subsets (Tukey HSD), alpha = .05,
for average of F-Scores.
7 Concluding Remarks
The GREC Task is a new task not only for an NLG
shared-task challenge, but also as a research task in
general (improving referential clarity in extractive
summaries seems to be just taking off as a research
subfield). It was therefore not unexpected that only
three teams were able to participate in this task.
We continued the traditions of the ASGRE?07
Challenge in that we used a wide range of evalu-
ation metrics to obtain a well-rounded view of the
quality of the participating systems. It had been our
intention to use evaluation methods in all four possi-
ble extrinsic/intrinsic and automatic/human combi-
nations. However, the combination intrinsic/human
is missing from this report and will have to be left to
future research.
There was no indication in the human task perfor-
mance experiment that the different reference chains
selected by different systems had any impact on sub-
jects? reading speeds, and the evidence that there
is an effect on comprehension was scant. This
means that we will need to investigate alternative
task-performance measures. Because of the lack of
significant results from the human extrinsic experi-
ment, we were also unable to validate the automatic
extrinsic experiment against it, and so at this point
we do not really know how useful it is (despite some
correlation with intrinsic measures), something we
will seek to establish in future research.
Acknowledgments
Many thanks to Jason Baldridge and Pascal De-
nis for help with selecting coreference resolution
tools and metrics, and to the colleagues and students
who helped with the task-performance experiment.
Thanks are also due to the members of the Corpora
and SIGGEN mailing lists, colleagues, friends and
friends of friends who helped with the online MSRE
selection experiment.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Proceedings of the Linguistic
Coreference Workshop at LREC?98, pages 563?566.
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proceedings of
INLG?00, pages 1?8.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG?07, pages 9?16.
A. Belz and S. Varges. 2007b. The GREC corpus: Main
subject reference in context. Technical Report NLTG-
07-01, University of Brighton.
K. I. Forster and J. C. Forster. 2003. DMDX: A win-
dows display program with millisecond accuracy. Be-
havior Research Methods, Instruments, & Computers,
35(1):116?124.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar of the English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. of HLT-EMNLP, pages 25?32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of Pen-
sylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP?08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC?04, pages 291?
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summariza-
tion. Information Processing and Management: Spe-
cial issue on Summarization, 43(6):1663?1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. Proceedings of MUC-6, pages 45?52.
191
GRAPH: The Costs of Redundancy in Referring Expressions
Emiel Krahmer
Tilburg University
The Netherlands
e.j.krahmer@uvt.nl
Marie?t Theune
University of Twente
The Netherlands
m.theune@utwente.nl
Jette Viethen
Macquarie University
Australia
jviethen@ics.mq.edu.au
Iris Hendrickx
University of Antwerp
Belgium
iris.hendrickx@ua.ac.be
Abstract
We describe a graph-based generation sys-
tem that participated in the TUNA attribute se-
lection and realisation task of the REG 2008
Challenge. Using a stochastic cost function
(with certain properties for free), and trying
attributes from cheapest to more expensive,
the system achieves overall .76 DICE and .54
MASI scores for attribute selection on the de-
velopment set. For realisation, it turns out
that in some cases higher attribute selection
accuracy leads to larger differences between
system-generated and human descriptions.
1 Introduction
Referring Expression Generation (REG) is a key-
task in NLG, and the topic of the REG 2008 Chal-
lenge.1 In this context, referring expressions are
understood as distinguishing descriptions: descrip-
tions that uniquely characterize a target object in a
visual scene (e.g., ?the red sofa?), and do not ap-
ply to any of the other objects in the scene (the dis-
tractors). Generating such descriptions is usually as-
sumed to be a two-step procedure: first, it has to be
decided which attributes of the target suffice to char-
acterize it uniquely, and then the selected set of at-
tributes should be converted into natural language.
For the first step, attribute selection, we use a ver-
sion of the Graph-based REG algorithm of Krahmer
et al (2003). In this approach, a visual scene is rep-
resented as a directed labelled graph, where vertices
represent the objects in the scene and edges their at-
tributes. A key ingredient of the approach is that
1See http://www.itri.brighton.ac.uk/research/reg08/.
costs can be assigned to attributes; the generation
of referring expressions can then be defined as a
graph search problem, which outputs the cheapest
distinguishing graph (if one exists) given a particu-
lar cost function. For the second step, realisation, we
use a simple template-based realiser written by Irene
Langkilde-Geary from Brighton University that was
made available to all REG 2008 participants.
A version of the Graph-based algorithm was sub-
mitted for the ASGRE 2007 Challenge (Theune et
al. 2007). For us, one of the most striking, gen-
eral outcomes was the observed ?trend for the mean
DICE score obtained by a system to decrease as the
proportion of minimal descriptions increases? (Belz
and Gatt 2007).2 Thus, while REG systems have
a tendency to produce minimal descriptions, hu-
man speakers tend to include redundant properties in
their descriptions, which is in line with recent find-
ings in psycholinguistics on the production of refer-
ring expressions (e.g., Engelhardt et al 2006).
In principle, the graph-based approach has the po-
tential to deal with redundancy by allowing some at-
tributes to have zero costs. Viethen et al (2008),
however, show that merely assigning zero costs to
an attribute is not a sufficient condition for inclu-
sion; if the search terminates before the free prop-
erties are tried, they will not be included. In other
words: the order in which attributes are tried should
be explicitly controlled as well. In the experiment
we describe here, we consider both these factors and
their interplay.
2DICE (like MASI) is a measure for similarity between a pre-
dicted attribute set and a (human produced) reference set.
227
2 Method
We experimentally combine four cost functions and
two search orders (Table 1). (1) Simple simply as-
signs each edge a 1-point cost. (2) Stochastic asso-
ciates each edge with a frequency-based cost, based
on both the 2008 training and development sets (as-
suming that a larger data set alows for more ac-
curate frequency estimates). (3) Free-Stochastic is
like the previous cost function, except that highly
frequent attributes are assigned 0 costs. For the Fur-
niture domain, this applies to ?colour?; for People
to ?hasBeard = 1? and ?hasGlasses = 1.? (4) Free-
Naive, finally, reduces the relatively fine-grained
costs of Free-Stochastic to three values (0 = free,
1 = cheap, 2 = expensive). In addition, we com-
pare results for two property orderings: (A) Proper-
ties are tried in a Random order. (B) Cost-based,
where properties are tried (in stochastic order) from
cheapest to most expensive. Finally, since human
speakers nearly always include the ?type? property,
we decided to simply always include it. Tables 2 to
4 summarize the evaluation results for all combina-
tions of cost functions and search orders.
3 Attribute Selection Results
The measures used to evaluate attribute selection are
DICE, MASI, attribute accuracy (A-A, the proportion
of times the generated attribute set was identical to
the reference set), and minimality (MIN).
Notice first that the order in which attributes are
tried in the search process matters; the B-systems
nearly always outperform their A-counterparts. Sec-
ond, assigning varying costs also helps; both 1-
variants (Simple costs) perform worse than the sys-
tems building on Stochastic cost functions (2, 3
and 4). Third, adding free properties is also ben-
eficial; the 3 and 4 variants clearly outperform the
1 and 2 variants. It is interesting to observe that
the Free-naive cost function (4) performs equally
well as the more principled Free-stochastic (3), but
only in combination with the Cost-based order (B).
To the extent that it is possible to compare the re-
sults, the submitted GRAPH 4+B outperforms our
best 2007 variant (GRAPH FP in Table 2). This sug-
gests that the interplay between property ordering
and cost function is a flexible and efficient approach
to attribute selection.
Table 1: Overview of cost functions and search orders.
The GRAPH 4+B settings were submitted to the REG
2008 Challenge.
Costs Orders
1 Simple A Random
2 Stochastic B Cost-based
3 Free-stochastic
4 Free-naive
Table 2: Furniture development set results (80 trials).
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .61 .32 .12 .29 5.90 .04
1+B .61 .31 .12 .29 5.89 .04
2+A .71 .47 .31 .11 5.06 .05
2+B .69 .44 .28 .16 5.19 .05
3+A .80 .58 .45 .00 4.90 .05
3+B .80 .58 .45 .00 4.90 .05
4+A .80 .59 .48 .00 4.61 .05
4+B .80 .59 .48 .00 4.61 .05
FP 2007 .71 ? ? ? ? ?
Table 3: People development set results (68 trials).
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .59 .36 .24 .00 6.54 .00
1+B .66 .42 .24 .00 6.78 .00
2+A .66 .42 .24 .00 6.78 .00
2+B .66 .42 .24 .00 6.78 .00
3+A .68 .41 .19 .00 6.79 .00
3+B .72 .48 .28 .00 6.96 .00
4+A .59 .34 .18 .00 6.56 .00
4+B .72 .48 .28 .00 6.96 .00
FP 2007 .67 ? ? ? ? ?
Table 4: Combined Furniture and People development set
results.
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .60 .34 .18 .16 6.20 .02
1+B .63 .36 .18 .16 6.30 .02
2+A .69 .45 .28 .06 5.85 .03
2+B .68 .43 .26 .09 5.92 .03
3+A .74 .51 .33 .00 5.77 .03
3+B .76 .54 .37 .00 5.84 .03
4+A .70 .48 .34 .00 5.51 .03
4+B .76 .54 .39 .00 5.69 .03
FP 2007 .69 ? ? ? ? ?
228
4 Realization Results
To evaluate realisation, the following two word-
string comparison measures were used: string-edit
distance (EDIT), which is the Levenshtein distance
between generated word string and human reference
output, and string accuracy (S-A), which is the pro-
portion of times the word string was identical to the
reference string.
For all settings of the algorithm, we see that S-A
is much lower than A-A. This is as expected, since
any set of attributes can be expressed in many differ-
ent ways, and the chance that the realizer produces
exactly the same string as the human reference is
quite small. For the furniture domain, we see that
S-A has a fairly constant low score, while EDIT fol-
lows the same pattern as A-A: including redundant
(free) properties leads to better results. For the peo-
ple domain, S-A is always 0, and surprisingly EDIT
gets worse as A-A gets better.
To explain these results, we inspect those descrip-
tions where A-A = 1 but S-A = 0, i.e., the attribute
set is identical to the human reference but the word
string is not. In setting 4+B (submitted to REG 2008)
this is the case for 34 furniture and 19 people de-
scriptions. For furniture, we see that the low S-A
score can be largely explained by the fact that in 23
of the 34 descriptions the human reference either in-
cluded no determiner or an indefinite one, whereas
the system always included a definite determiner.
This also explains why S-A hardly improves with
higher A-A scores, since determiner choice is inde-
pendent from attribute selection.
In the people domain, the zero scores for S-A can
be explained by the fact that the realizer always uses
?person? to express the type attribute, where the hu-
man references have either ?man? or ?guy? (in line
with the human preference for basic level values; cf.
Krahmer et al 2003). We also encounter the de-
terminer problem again, aggravated by the fact that
many person descriptions include embedded noun
phrases (e.g., ?man with beard?).
To find out why EDIT gets worse as A-A increases
for different system settings in the people domain,
we look at the six descriptions that have A-A = 1
for setting 4+B but not for 4+A. It turns out that
five of these descriptions are realized as ?the light-
haired person with a beard?, while the human refer-
ence strings are variations of ?the man with a white
beard?, resulting in a relatively high EDIT value. The
problem here is that the link between beard and hair
colour has been lost in the data annotation process.
In general, we can conclude that simply combin-
ing more or less human-like attribute selection with
an off-the-shelf surface realiser is not sufficient to
produce human-like referring expressions.
Acknowledgements We thank the REG 2008 orga-
nizers for making the realiser available, and Hendri
Hondorp for his help with installing and using it.
References
Belz, A. and A. Gatt 2007. The attribute selection for
GRE challenge: Overview and evaluation results Pro-
ceedings of UCNLG+MT 75-83
Engelhardt, P., K. Bailey and F. Ferreira 2006. Do speak-
ers and listeners observe the Gricean Maxim of Quan-
tity? Journal of Memory and Language, 54, 554-573.
Krahmer, E., S. van Erk and A. Verleg 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1), 5372.
Theune, M., P. Touset, J. Viethen, and E. Krahmer. 2007.
Cost-based attribute selection for generating referring
expressions (GRAPH-FP and GRAPH-SC). Proceedings
of the ASGRE Challenge 2007, Copenhagen, Denmark
Viethen, J., R. Dale, E. Krahmer, M. Theune and P. Tou-
set. 2008. Controlling redundancy in referring expres-
sions. Proceedings LREC 08, Marrakech, Morroco.
229
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 78?87,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Optimising Natural Language Generation Decision Making
For Situated Dialogue
Nina Dethlefs
Department of Linguistics,
University of Bremen
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
German Research Centre
for Artificial Intelligence (DFKI)
heriberto.cuayahuitl@dfki.de
Jette Viethen
Centre for Language Technology
acquarie University
jviethen@ics.mq.edu.au
Abstract
Natural language generators are faced with a
multitude of different decisions during their
generation process. We address the joint opti-
misation of navigation strategies and referring
expressions in a situated setting with respect to
task success and human-likeness. To this end,
we present a novel, comprehensive framework
that combines supervised learning, Hierarchi-
cal Reinforcement Learning and a hierarchical
Information State. A human evaluation shows
that our learnt instructions are rated similar
to human instructions, and significantly better
than the supervised learning baseline.
1 Introduction
Natural Language Generation (NLG) systems are
typically faced with a multitude of decisions dur-
ing their generation process due to nondeterminacy
between a semantic input to a generator and its re-
alised output. This is especially true in situated set-
tings, where sudden changes of context can occur
at anytime. Sources of uncertainty include (a) the
situational context, such as visible objects, or task
complexity, (b) the user, including their behaviour
and reactions, and (c) the dialogue history, includ-
ing shared knowledge or patterns of linguistic con-
sistency (Halliday and Hasan, 1976) and alignment
(Pickering and Garrod, 2004).
Previous work on context-sensitive generation in
situated domains includes Stoia et al (2006) and
Garoufi and Koller (2010). Stoia et al present a
supervised learning approach for situated referring
expression generation (REG). Garoufi and Koller
use techniques from AI planning for the combined
generation of navigation instructions and referring
expressions (RE). More generally, the NLG prob-
lem of non-deterministic decision making has been
addressed from many different angles, including
PENMAN-style choosers (Mann and Matthiessen,
1983), corpus-based statistical knowledge (Langk-
ilde and Knight, 1998), tree-based stochastic models
(Bangalore and Rambow, 2000), maximum entropy-
based ranking (Ratnaparkhi, 2000), combinatorial
pattern discovery (Duboue and McKeown, 2001),
instance-based ranking (Varges, 2003), chart gen-
eration (White, 2004), planning (Koller and Stone,
2007), or probabilistic generation spaces (Belz,
2008) to name just a few.
More recently, there have been several approaches
towards using Reinforcement Learning (RL) (Rieser
et al, 2010; Janarthanam and Lemon, 2010) or Hi-
erarchical Reinforcement Learning (HRL) (Deth-
lefs and Cuaya?huitl, 2010) for NLG decision mak-
ing. All of these approaches have demonstrated that
HRL/RL offers a powerful mechanism for learn-
ing generation policies in the absence of complete
knowledge about the environment or the user. It
overcomes the need for large amounts of hand-
crafted knowledge or data in rule-based or super-
vised learning accounts. On the other hand, RL
can have difficulties to find an optimal policy in a
large search space, and is therefore often limited to
small-scale applications. Pruning the search space
of a learning agent by including prior knowledge is
therefore attractive, since it finds solutions faster, re-
duces computational demands, incorporates expert
knowledge, and scales to complex problems. Sug-
78
gestions to use such prior knowledge include Lit-
man et al (2000) and Singh et al (2002), who
hand-craft rules of prior knowledge obvious to the
system designer. Cuaya?huitl (2009) suggests us-
ing Hierarchical Abstract Machines to partially pre-
specify dialogue strategies, and Heeman (2007) uses
a combination of RL and Information State (IS)
to also pre-specify dialogue strategies. Williams
(2008) presents an approach of combining Partially-
Observable Markov Decision Processes with con-
ventional dialogue systems. The Information State
approach is well-established in dialogue manage-
ment (e.g., Bohlin et al (1999) and Larsson and
Traum (2000)). It allows the system designer to
specify dialogue strategies in a principled and sys-
tematic way. A disadvantage is that random design
decisions need to be made in cases where the best
action, or sequence of actions, is not obvious.
The contribution of this paper consists in a com-
prehensive account of constrained Hierarchical Re-
inforcement Learning through a combination with
a hierarchical Information State (HIS), which is in-
formed by prior knowledge induced from decision
trees. We apply our framework to the generation
of navigation strategies and referring expressions in
a situated setting, jointly optimised for task suc-
cess and linguistic consistency. An evaluation shows
that humans prefer our learnt instructions to the su-
pervised learning-based instructions, and rate them
equal to human instructions. Simulation-based re-
sults show that our semi-learnt approach learns more
quickly than the fully-learnt baseline, which makes
it suitable for large and complex problems. Our ap-
proach differs from Heeman?s in that we transfer it
to NLG and to a hierarchical setting. Although Hee-
man was able to show that his combined approach
learns more quickly than pure RL, it is limited to
small-scale systems. Our ?divide-and-conquer? ap-
proach, on the other hand, scales up to large search
spaces and allows us to address complex problems.
2 The Generation Tasks
2.1 The GIVE-2 Domain
Our domain is the generation of navigation instruc-
tions and referring expressions in a virtual 3D world
in the GIVE scenario (Koller et al, 2010). In this
task, two people engage in a ?treasure hunt?, where
an instruction giver (IG) navigates an instruction fol-
lower (IF) through the world, pressing a sequence of
buttons and completing the task by obtaining a tro-
phy. Pairs take part in three dialogues (in three dif-
ferent worlds); after the first dialogue, they switch
roles. The GIVE-2 corpus (Gargett et al, 2010) pro-
vides transcripts of such dialogues in English and
German. For this paper, we complemented the En-
glish dialogues of the corpus with a set of seman-
tic annotations.1 The feature set is organised in five
groups (Table 1). The first two groups cover manip-
ulation instructions (i.e., instructions to press a but-
ton), including distractors2 and landmarks (Gargett
et al, 2010). The third group describes high- and
low-level navigation, the fourth group describes the
user. The fifth group finally contains grammatical
information.
2.2 Navigation and Manipulation Instructions
Navigation instructions can take many forms, even
for the same route. For example, a way to another
room can be described as ?go to the room with the
lamp?, ?go left and through the door?, or ?turn 90
degrees, left, straight?. Choosing among these vari-
ants is a highly context- and speaker-dependent task.
Figure 1 shows the six user strategies we identified
from the corpus based on an analysis of the combi-
nation of navigation level (?high? vs. ?low?) and con-
tent (?destination?, ?direction?, ?orientation?, ?path?,
?straight?). User models are based on the navigation
level and content decisions made in a sequence of in-
structions, so that different sequences, with a certain
distribution, lead to different user model classifica-
tions. The proportions are shown in Figure 1. We
found that 75% of all speakers use the same strat-
egy in consecutive rounds/games. 62.5% of pairs
are consistent over all three dialogues, indicating
inter-speaker alignment. These high measures of
human consistency suggest that this phenomenon
is worth modelling in a learning agent, and there-
fore provides the motivation of including linguis-
tic consistency in our agent?s behaviour. Manipula-
tion instructions were treated as an REG task, which
needs to be sensitive to the properties of the referent
and distractors (e.g, size, colour, or spatial relation
1The annotations are available on request.
2Distractors are objects of the same type as the referent.
79
ID Feature Type Description
f1 absolute property(referent) boolean Is the colour of the referent mentioned?
f2 absolute property(distractor) boolean Is the colour of the distractor mentioned?
f3 discriminative colour(referent) boolean Is the colour of the referent discriminating?
f4 discriminative colour(distractor) boolean Is the colour of the distractor discriminating?
f5 mention(distractor) boolean Is a distractor mentioned?
f6 first mention(referent) boolean Is this the first reference to the referent?
f7 mention(macro landmark) boolean Is a macro (non-movable) landmark mentioned?
f8 mention(micro landmark) boolean Is a micro (movable) landmark mentioned?
f9 num(distractors) integer How many distractors are present?
f10 num(micro landmarks) integer How many micro landmarks are present?
f11 spatial rel(referent,obj) string Which spatial relation(s) are used in the RE?
f12 taxonomic property(referent) boolean Is the type of the distractor mentioned?
f13 within field of vision(referent) boolean Is the referent within the user?s field of vision?
f14 mention(colour, lm) boolean Is the colour of a macro- / micro lm mentioned?
f15 mention(size, lm) boolean Is the size of a macro- / micro lm mentioned?
f16 abstractness(nav instruction) string Is the instruction explicit or implicit?
f17 content(nav instruction) string Vals: destination, direction, orientation, path, straight
f18 level(nav instruction) string Is the instruction high- or low-level?
f19 position(user) string Is the user on track or off track?
f20 reaction(user) string Vals: take action, take wrong action, wait, req help
f21 type(user) string Vals: likes waiting, likes exploring, in between
f22 waits(user) boolean Is the user waiting for the next instruction?
f23 model(user) string User model/navig. strategy used (cf. Fig.1)?
f24 actor(instruction) boolean Is the actor of the instruction inserted?
f25 mood(instruction) boolean Is the mood of the instruction inserted?
f26 process(instruction) boolean Is the process of the instruction inserted?
f27 locational phrase(instruction) boolean Is the loc. phrase (path, straight, etc.) inserted?
Table 1: Corpus annotation features that were used as knowledge of the learning agent and the Information State. Fea-
tures are presented in groups, describing the properties of referents in the environment (f1...f13) and their distractors
(f14...f15), features of high- and low-level navigation (f16...f18), the user (f19...f23), and grammatical information
about constituents (f24...f27).
with respect to the referent) to be natural and dis-
tinguishing. We also considered the visual salience
of objects, and the type of spatial relation involved,
since recent studies indicate the potential relevance
of these features (Viethen and Dale, 2008). Given
these observations, we aim to optimise the task suc-
cess and linguistic consistency of instructions. Task
success is measured from user reactions after each
instruction (Section 5.1). Linguistic consistency is
achieved by rewarding the agent for generating in-
structions that belong to the same user model as the
previous one. The agent has the same probability
for choosing any pattern, but is then rewarded for
consistency. Table 3 (in Section 5.2) presents an ex-
ample dialogue generated by our system.
3 Constrained Hierarchical Reinforcement
Learning for NLG
3.1 Hierarchical Reinforcement Learning
Our idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system?s knowledge about
80
Figure 1: Decision tree for the classification of user
models (UM) defined by the use of navigation level and
content. UM 0=high-level, UM 1=low-level (LL), UM
2=orientation-based LL, UM 3=orientation-based mix-
ture (M), UM 4=path-based M, UM 5=pure M.
the generation task (e.g. navigation strategy, or re-
ferring expressions). The action set describes the
system?s capabilities (e.g. ?use high level naviga-
tion strategy?, ?mention colour of referent?, etc.).
The reward function assigns a numeric value for
each action taken. In this way, language generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, where
the goal is to find an optimal strategy automatically.
To do this we use RL with a divide-and-conquer ap-
proach in order to optimise a hierarchy of generation
policies rather than a single policy. The hierarchy of
RL agents consists of L levels and N models per
level, denoted as M ij , where j ? {0, ..., N ? 1}
and i ? {0, ..., L ? 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.
Sij is a set of states, Aij is a set of actions, T ij is
a transition function that determines the next state
s? from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting ? time steps. The random
variable ? represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-
imises the reward for each visited state, according to
??ij(s) = arg maxa?Aij Q
?i
j(s, a), where Q?ij (s, a)
specifies the expected cumulative reward for exe-
cuting action a in state s and then following pol-
icy ??ij . We use HSMQ-Learning (Dietterich, 1999)
for learning a hierarchy of generation policies. This
hierarchical approach has been applied successfully
to dialogue strategy learning by Cuaya?huitl et al
(2010).
3.2 Information State
The notion of an Information State has traditionally
been applied to dialogue, where it encodes all infor-
mation relevant to the current state of the dialogue.
This includes, for example, the context of the in-
teraction, participants and their beliefs, and the sta-
tus of grounding. An IS consists of a set of infor-
mational components, encoding the information of
the dialogue, formal representations of these com-
ponents, a set of dialogue moves leading to the up-
date of the IS, a set of update rules which govern the
update, and finally an update strategy, which speci-
fies which update rule to apply in case more than one
applies (Larsson and Traum (2000), p. 2-3). In this
paper, we apply the theory of IS to language gener-
ation. For this purpose we define the informational
components of an IS to represent the (situational and
linguistic) knowledge of the generator (Section 4.2).
Update rules are triggered by generator actions, such
as the decision to insert a new constituent into the
current logical form, or the decision to prefer one
word order sequence over another. We use the DIP-
PER toolkit (Bos et al, 2003)3 for our implementa-
tion of the IS.
3.3 Combining Hierarchical Reinforcement
Learning and Information State
Previous work has suggested the HSMQ-Learning
algorithm for optimizing text generation strategies
(Dethlefs and Cuaya?huitl, 2010). Because such an
algorithm uses all available actions in each state,
an important extension is to constrain the actions
available with some prior expert knowledge, aim-
ing to combine behaviour specified by human de-
signers and behaviour automatically inferred by re-
inforcement learning agents. To that end, we sug-
3http://www.ltg.ed.ac.uk/dipper
81
Figure 2: (Left:) Hierarchy of learning agents executed from top to bottom for generating instructions. (Right:) State
representations for the agents shown in the hierarchy on the left. The features f1...f27 refer back to the features used
in the annotation given in the first column of Table 1. Note that agents can share information across levels.
gest combining the Information State approach with
hierarchical reinforcement learning. We therefore
re-define the characterisation of each Semi-Markov
Decision Process (SMDP) in the hierarchy as a 5-
tuple model M ij =< Sij, Aij , T ij , Rij , Iij >, where
Sij , Aij , T ij and Rij are as before, and the additional
element Iij is an Information State used as knowl-
edge base and rule-based decision maker. In this ex-
tended model, action selection is based on a con-
strained set of actions provided by the IS update
rules. We assume that the names of update rules
in Iij represent the agent actions Aij . The goal of
each SMDP is then to find an optimal policy that
maximises the reward for each visited state, accord-
ing to ??ij(s) = arg maxa?Aij?Iij Q
?i
j(s, a), where
Q?ij (s, a) specifies the expected cumulative reward
for executing constrained action a in state s and then
following ??ij thereafter. For learning such poli-
cies we use a modified version of HSMQ-Learning.
This algorithm receives subtask M ij and Information
State Iij used to initialise state s, performs similarly
to Q-Learning for primitive actions, but for compos-
ite actions it invokes recursively with a child sub-
task. In contrast to HSMQ-Learning, this algorithm
chooses actions from a subset derived by applying
the IS update rules to the current state of the world.
When the subtask is completed, it returns a cumu-
lative reward rt+? , and continues its execution until
finding a goal state for the root subtask. This process
iterates until convergence occurs to optimal context-
independent policies, as in HSMQ-Learning.
4 Experimental Setting
4.1 Hierarchy of Agents
Figure 2 shows a (hand-crafted) hierarchy of learn-
ing agents for navigating and acting in a situated en-
vironment. Each of these agents represents an indi-
vidual generation task. Model M00 is the root agent
and is responsible for ensuring that a set of naviga-
tion instructions guide the user to the next referent,
where an RE is generated. Model M10 is responsible
for the generation of the RE that best describes an
intended referent. Subtasks M20 ... M22 realise sur-
face forms of possible distractors, or macro- / micro
landmarks. Model M12 is responsible for the gener-
ation of navigation instructions which smoothly fit
into the linguistic consistency pattern chosen. Part
of this task is choosing between a low-level (model
M23 ) and a high-level (model M24 ) instruction. Sub-
tasks M30 ...M34 realise the actual instructions, des-
tination, direction, orientation, path, and ?straight?,
respectively.4 Finally, model M11 can repair previ-
ous system utterances.
4Note that navigation instructions and REs correspond to se-
quences of actions, not to a single one.
82
Model(s) Actions
M00 navigation, manipulation, confirmation, stop, repair system act, repair no system act
M10 insert distractor, insert no distractor, insert no absolute property, insert micro relatum, insert macro relatum
insert no taxonomic property, insert absolute property, insert no macro relatum, insert taxonomic property
M12 choose high level, choose low level, get route, choose easy route, choose short route
M20 ... M22 exp head, exp no head, insert colour, insert no colour, insert size, insert no size, exp spatial relation
M23 choose explicit abstractness, choose implicit abstractness, destination instruction, path instruction
M24 choose explicit abstractness, choose implicit abstractness, direction instr, orientation instr, straight instr
M30 ... M34 exp actor, exp no actor, exp mood, exp loc phrase, exp no loc phrase, exp process, exp no process
Table 2: Action set of the learning agents and Information States.
4.2 State and Action Sets
The HRL agent?s knowledge base consists of all sit-
uational and linguistic knowledge the agent needs
for decision making. Figure 2 shows the hierarchy
of learning agents together with the knowledge base
of the learning agent with respect to the semantic
features shown in Table 1 that were used for the an-
notation of the GIVE-2 corpus dialogues. The first
column of the table in Figure 2 indicates the respec-
tive model, also referred to as agent, or subtask, and
the second column refers to the knowledge variable
it uses (in the form of the feature index given in the
first column of Table 1). In the agent, boolean values
and strings were represented as integers. The HIS
shares all information of the learning agent, but has
an additional set of relational feature-value pairs for
each slot. For example, if the agent knows that the
slot content(nav instruction) has value 1 (mean-
ing ?filled?), the HIS knows also which value it was
filled with, such as path. Such additional knowledge
is required for the supervised learning baseline (Sec-
tion 5). The action set of the hierarchical learning
agent and the hierarchical information state is given
in Table 2. The state-action space size of a flat learn-
ing agent would be |S ?A| = 1011, the hierarchical
setting has a state-action space size of 2.4 ? 107.
The average state-action space size of all subtasks is
|S ? A|/14 = 1.7 ? 107. Generation actions can
be primitive or composite. While the former corre-
spond to single generation decisions, the latter rep-
resent separate generation subtasks (Fig. 2).
4.3 Prior Knowledge
Prior knowledge can include decisions obvious to
the system designer, expert knowledge, or general
intuitions. In our case, we use a supervised learn-
ing approach to induce prior knowledge into our
HRL agent. We trained decision trees on our anno-
tated corpus data using Weka?s (Witten and Frank,
2005) J48 decision tree classifer. A separate tree
was trained for each semantic attribute (cf. Table
1). The obtained decision trees represent our super-
vised learning baseline. They achieved an accuracy
of 91% in a ten-fold cross-validation. For our semi-
learnt combination of HRL and HIS, we performed a
manual analysis of the resulting rules to assess their
impact on a learning agent.5 In the end, the fol-
lowing rules were used to constrain the agent?s be-
haviour: (1) In REs, always use a referent?s colour,
except in cases of repair when colour is not discrim-
inating; (2) mention a distractor or micro landmark,
if the colour of the referent is not discriminating;
(3) in navigation, always make orientation instruc-
tions explicit. All remaining behaviour was subject
to learning.
4.4 Reward Function
We use the following reward function to train the hi-
erarchy of policies of our HRL agent. It aims to re-
duce discourse length at maximal task success6 us-
ing a consistent navigation strategy.
R =
?
?
?
?
?
?
?
?
?
0 for reaching the goal state
-2 for an already invoked subtask
+1 for generating instruction u con-
sistent with instruction u?1
-1 otherwise.
5We excluded rules that always choose the same value, since
they would work against our aim of generating consistent, but
variable instructions.
6Task success is addressed by that the user has to ?accept?
each instruction for a state transition.
83
The third reward that encourages consistency of in-
structions rewards a sequence of actions that allow
the last generated instruction to be classified as be-
longing to the same navigation strategy/user model
as the previously generated instruction (cf. 2.2).
5 Experiments and Results
5.1 The Simulated Environment
The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user?s reaction to a system utterance. The first aspect
is represented by a set of contextual variables de-
scribing the environment, 7 and user behaviour.8 Al-
together, this leads to 115 thousand different contex-
tual configurations, which are estimated from data
(cf. Section 2.1). The uncertainty regarding the
user?s reaction to an utterance is represented by a
Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.9 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.10 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
ten-fold cross validation.
5.2 Learnt Policies
With respect to REs, the fully-learnt policy (only
HRL) uses colour when it is discriminating, and a
distractor or micro landmark otherwise. The semi-
learnt policy (HRL with HIS) behaves as defined in
Section 4.3. The supervised learning policy (only
HIS) uses the rules learnt by the decision trees. Both
learnt policies learn to maximise task success, and
to generate consistent navigation strategies.11 The
7previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)
8previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)
9navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)
10User reactions measure the system?s task success.
11They thereby also learn to adapt their semantic choices to
those most frequently made by humans.
101 102 103 104
?80
?70
?60
?50
?40
?30
?20
?10
0
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Deterministic
Semi?Learnt
Fully?Learnt
Figure 3: Comparison of fully-learnt, semi-learnt, and su-
pervised learning (deterministic) behaviours.
supervised learning policy generates successful in-
structions from the start. Note that we are not ac-
tually learning dialogue strategies, but rather gen-
eration strategies using dialogue features. There-
fore the described policies, fully-learnt, semi-learnt
and supervised-learning, exclusively guide the sys-
tem?s behaviour in the interaction with the simulated
user. An example dialogue is shown in Table 3. We
can observe that the agent starts using a low level
navigation strategy, and then switches to high level.
When the user gets confused, the system temporar-
ily switches back to low level. For referring expres-
sions, it first attempts to locate the referent by ref-
erence to a distractor, and then repairs by using a
micro landmark. The surface forms of instructions
were realised from templates, since the NLG system
so far only generates a sequence of content selection
decisions. We address surface realisation in Dethlefs
and Cuaya?huitl (2011).
We compared our semi-learnt policy against a
fully-learnt, and a supervised learning baseline. All
policies were trained for 40 thousand episodes. For
training, the step-size parameter ?, which indicates
the learning rate, was initiated with 1 and then re-
duced over time by ? = 11+t , where t is the time step.
The discount rate ?, which indicates the relevance of
future rewards in relation to immediate rewards, was
set to 0.99, and the probability of a random action ?
was 0.01. See (Sutton and Barto, 1998) for details
on these parameters.
5.3 Comparison of Policies
Figure 3 compares the rewards averaged over 10
runs. In terms of learning speed, we can see that
84
Agent Policy Action (composite in italics) Utterance
Usr request route How do I get to the trophy?
Sys ?00, ?12 root, navigation, dontRepair, orientation
?23, ?30 low level navigation, add direction+degrees First, turn 90 degrees left.
?23, ?31 low level navigation, ?straight? Then go straight ahead.
Usr perform desired action
Sys ?00 confirmation OK.
?24, ?34, ?32 high level navigation, destination, direction,
?24, ?34, ?32 addLocation Turn left at the end of the hallway.
Usr wait
Sys ?11 repair system act, change navigation strategy
?23, ?32 low level navigation, direction Turn right
?24, ?33 high level navigation, path and go down the hallway.
Usr perform desired action
Sys ?23, ?32 low level navigation, direction Now turn left.
Usr perform desired action
Sys ?00 confirmation OK.
?24, ?34 high level navigation, destination,
?24, ?34 addLocation Enter to the room in front of you.
Usr perform desired action
Sys ?10, ?21 manipulation, use colour, use distractor Press the red button beside the blue.
Usr request help
Sys ?11 repair system act, change granularity
?24, ?10 high level navigation, manipulation
?34, ?22 destination, use colour, use micro landmark Go to the red button beside the chair.
Usr perform desired action
Sys ?10 manipulation Press it.
Usr perform desired action
Sys ?00 confirmation Well done.
Table 3: Sample dialogue in the GIVE-2 scenario showing the dynamics of generation policies. See Figure 2 for the
corresponding hierarchy models, and Table 2 for the action set. See Section 5.2 for an explantation of the dialogue.
while the semi-learnt behaviour is able to follow a
near-optimal policy from the beginning, the fully-
learnt policy takes about 40 thousand episodes to
reach the same performance. In terms of simulated
task success, we see that while the supervised learn-
ing behaviour follows a good policy from the start,
it is eventually beaten by the learnt policies.
5.4 Human Evaluation Study
We asked 11 participants12 to rate altogether 132
sets of instructions, where each set contained a spa-
tial graphical scene containing a person, mapped
with one human, one learnt, and one supervised
126 female, 5 male with an age average of 26.4.
learning instruction. Instructions consisted of a nav-
igation instruction followed by a referring expres-
sion. Subjects were asked to rate instructions on a
1-5 Likert scale (where 5 is the best) for their help-
fulness on guiding the displayed person from its ori-
gin to pressing the intended button. We selected
six different scenarios for the evaluation: (a) only
one button is present, (b) two buttons are present,
the referent and a distractor of the same colour as
the referent, (c) two buttons are present, the referent
and a distractor of a different colour than the refer-
ent, (d) one micro landmark is present and one dis-
tractor of the same colour as the referent, (e) one
micro landmark is present and one distractor of a
different colour than the referent. All scenarios oc-
85
Figure 4: Example scenario of the human evaluation study.
curred twice in each evaluation sheet, their specific
instances were drawn from the GIVE-2 corpus at
random. Scenes and instructions were presented in
a randomised order. Figure 4 presents an example
evaluation scene. Finally, we asked subjects to cir-
cle the object they thought was the intended refer-
ent. Subjects rated the human instructions with an
average of 3.82, the learnt instructions with an aver-
age of 3.55, and the supervised learning instructions
with an average of 2.39. The difference between hu-
man and learnt is not significant. The difference be-
tween learnt and supervised learning is significant at
p < 0.003, and the difference between human and
supervised learning is significant at p < 0.0002. In
96% of all cases, users were able to identify the in-
tended referent.
6 Conclusion and Discussion
We have presented a combination of HRL with a hi-
erarchical IS, which was informed by prior knowl-
edge from decision trees. Such a combined frame-
work has the advantage that it allows us to system-
atically pre-specify (obvious) generation strategies,
and thereby find solutions faster, reduce computa-
tional demands, scale to complex domains, and in-
corporate expert knowledge. By applying HRL to
the remaining (non-obvious) action set, we are able
to learn a flexible, generalisable NLG policy, which
will take the best action even under uncertainty. As
an application of our approach and its generalisabil-
ity across domains, we have presented the joint op-
timisation of two separate NLG tasks, navigation in-
structions and referring expressions, in situated dia-
logue under the aspects of task success and linguis-
tic consistency. Based on an evaluation in a simu-
lated environment estimated from data, we showed
that our semi-learnt behaviour outperformed a fully-
learnt baseline in terms of learning speed, and a su-
pervised learning baseline in terms of average re-
wards. Human judges rated our instructions signif-
icantly better than the supervised learning instruc-
tions, and close to human quality. The study re-
vealed a task success rate of 96%. Future work
can transfer our approach to different applications to
confirm its benefits, and induce the agent?s reward
function from data to test in a more realistic setting.
Acknowledgments
Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ?Spatial Cognition? and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work. Also, thanks to John Bateman for com-
ments on an earlier draft of this paper.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th conference on Computa-
tional linguistics - Volume 1, pages 42?48.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1?26.
86
Peter Bohlin, Robin Cooper, Elisabet Engdahl, and
Staffan Larsson. 1999. Information states and di-
alogue move engines. In IJCAI-99 Workshop on
Knowledge and Reasoning in Practical Dialogue Sys-
tems.
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi Oka.
2003. DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Architec-
ture. In 4th SIGDial Workshop on Discourse and Dia-
logue, pages 115?124.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. thesis,
School of Informatics, University of Edinburgh.
Nina Dethlefs and Heriberto Cuaya?huitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceedings of INLG ?10.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hier-
archical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Genera-
tion. In Proceedings of ACL-HLT 2011, Portland, OR.
Thomas G. Dietterich. 1999. Hierarchical reinforce-
ment learning with the maxq value function decom-
position. Journal of Artificial Intelligence Research,
13:227?303.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In ACL ?01, pages 172?179.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The give-2 corpus of
giving instructions in virtual environments. In LREC.
Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573?1582, July.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Human Tech-
nology Conference (HLT), pages 268?275.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ?10,
pages 69?78.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL-07.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337?361,
Berlin/Heidelberg, Germany. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
ACL-36, pages 704?710.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Nat. Lang. Eng., 6(3-
4):323?340.
Diane J. Litman, Michael S. Kearns, Satinder Singh, and
Marilyn A. Walker. 2000. Automatic optimization of
dialogue management. In Proceedings of the 18th con-
ference on Computational linguistics, pages 502?508.
William Mann and Christian M I M Matthiessen. 1983.
NIGEL: A systemic grammar for text generation.
Technical report, ISI/RR-85-105.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194?201.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In ACL ?10, pages 1009?1018.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase gen-
eration for situated dialogs. In Proceedings of INLG
?06, pages 81?88.
Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.
Sebastian Varges. 2003. Instance-based Natural Lan-
guage Generation. Ph.D. thesis, School of Informat-
ics, University of Edinburgh.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of INLG ?08, INLG ?08, pages 59?67.
Michael White. 2004. Reining in CCG chart realization.
In In Proc. INLG-04, pages 182?191.
Jason Williams. 2008. The best of both worlds: Uni-
fying conventional dialog systems and POMDPs. In
Interspeech, Brisbane.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. 2. edi-
tion.
87
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 12?22,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
GRE3D7: A Corpus of Distinguishing Descriptions
for Objects in Visual Scenes
Jette Viethen1,2
jette.viethen@mq.edu.au
1TiCC
University of Tilburg
Tilburg, The Netherlands
Robert Dale2
robert.dale@mq.edu.au
2Centre for Language Technology
Macquarie University
Sydney, Australia
Abstract
Recent years have seen a trend towards em-
pirically motivated and more data-driven ap-
proaches in the field of referring expression
generation (REG). Much of this work has fo-
cussed on initial reference to objects in visual
scenes. While this scenario of use is one of
the strongest contenders for real-world appli-
cations of referring expression generation, ex-
isting data sets still only embody very sim-
ple stimulus scenes. To move this research
forward, we require data sets built around in-
creasingly complex scenes, and we need much
larger data sets to accommodate their higher
dimensionality. To control the complexity,
we also need to adopt a hypothesis-driven ap-
proach to scene design. In this paper, we de-
scribe GRE3D7, the largest corpus of human-
produced distinguishing descriptions available
to date, discuss the hypotheses that underlie its
design, and offer a number of analyses of the
4480 descriptions it contains.
1 Introduction
Whenever we engage in any form of discourse we
need to find a way to describe to our readers or
listeners the entities that we are talking or writing
about. This act of referring to real-world entities is
one of the central tasks in human language produc-
tion. Of course, it is also central when a machine
is charged with the task of generating natural lan-
guage, which makes referring expression generation
(REG) an important subtask in any natural language
generation (NLG) system.
It is therefore not surprising that REG has attracted
a great deal of attention from the NLG community
over the past three decades. A key factor that has
led to the popularity of REG is the widespread agree-
ment that the central task involved is content selec-
tion: choosing those attributes of a target referent
that best distinguish it from other distractor enti-
ties around it (Dale and Reiter, 1995; van Deemter,
2000; Gardent, 2002; Krahmer et al, 2003; Ho-
racek, 2003; van der Sluis, 2005; Kelleher and Krui-
jff, 2006; Gatt, 2007; Viethen and Dale, 2008).
Recent work in particular has concentrated on the
development of algorithms concerned with the gen-
eration of context-free identifying descriptions of
objects, as emphasised by three shared-task evalu-
ation competitions (STECs) targeting this particular
problem (Belz and Gatt, 2007; Gatt et al, 2008; Gatt
et al, 2009). Referring expressions of this kind are
often referred to as distinguishing descriptions. We
are still far from a full understanding of how such
descriptions should best be generated. Much work
remains to be done before many issues, such as, for
example, the generation of relational descriptions
and over-specified descriptions or the number of the
surrounding objects to be taken into account in vi-
sual settings, can be considered resolved.
Although many authors have explicitly or implic-
itly acknowledged the importance of generating re-
ferring expressions that sound natural (Dale, 1989;
Dale and Reiter, 1995; Gardent et al, 2004; Ho-
racek, 2004; van der Sluis and Krahmer, 2004;
Kelleher and Kruijff, 2006; Gatt, 2007; Gatt et al,
2007), much of the original work in REG was nei-
ther developed based on empirical evidence about
12
Figure 1: The screen showing the first stimulus scene.
how humans refer, nor evaluated against human-
produced referring expressions. The REG STECs on
the task of content determination form part of a re-
cent trend towards more data-oriented development
and evaluation of REG algorithms that responds di-
rectly to this concern (Gupta and Stent, 2005; Jordan
and Walker, 2005; Gatt et al, 2007; Viethen et al,
2010; Belz and Gatt, 2007; Gatt et al, 2008; Gatt et
al., 2009).
However, the existing data sets used in these
experiments involve very simple and usually ab-
stract visual displays of objects rather than coher-
ent scenes. This is a reasonable starting point for
bootstrapping research; but if we want to develop
algorithms that can be used in real-world scenarios,
we ultimately need to work with scenes which are
much more realistic. At the same time, given the
non-deterministic nature of choice in the production
of natural language, corpora based on these scenes
need to be very large, and should ideally contain re-
ferring expressions from as many different speakers
as possible for each target referent in each referential
scenario. The choice of stimuli and data collection
procedure should provide a controlled environment
that allows the isolation of a small number of factors
influencing the choices that have to be made by the
participants, in order to facilitate the replication of
the same controlled environment for REG algorithms
attempting the same reference task in an evaluation
situation. The way forward, we believe, is to build a
succession of corpora with incrementally more com-
plex scenes.
In this paper, we describe the design of a data col-
lection experiment for distinguishing descriptions
and give an overview of the resulting corpus, which
is, at 4480 instances, the largest corpus of distin-
guishing descriptions developed to date.1 Consis-
tent with the common focus on initial reference in
visual scenes, we used visual stimuli containing a
small number of simple objects (cubes and balls) in
a 3D scene, similar to our much smaller GRE3D3
Corpus (Viethen and Dale, 2008), and elicited indi-
vidual descriptions in the absence of a complicating
preceding discourse. Additionally, we introduced
factors that allow the study of the use of spatial re-
lations in referring expressions by creating stimu-
lus scenes that encourage the use of relations be-
tween objects, but do not require them. Most ex-
isting REG algorithms that can make use spatial rela-
tions between objects only do so if no distinguishing
description can be found otherwise (Dale and Had-
dock, 1991; Gardent, 2002; Krahmer and Theune,
2002; van der Sluis and Krahmer, 2005; Kelleher
and Kruijff, 2006), often based on the argument that
mentioning two entities imposes a higher cognitive
load than referring to only one entity. We are inter-
ested in investigating in how far this behaviour cor-
responds to the human use of spatial relations in dis-
tinguishing descriptions, as well as testing a number
of concrete hypotheses about the factors that might
lead people to use spatial relations.
2 Stimulus Design
The stimulus scenes used for the GRE3D7 corpus
are three-dimensional scenes containing only sim-
ple geometric shapes, created in Google SketchUp.
Each stimulus scene contains seven objects; these
are grouped into three pairs of two and one single
object. The target object is always part of one of
the pairs and the second object of that pair is what
we call the landmark object in these scenes. We at-
tempted to place the target?landmark pair as close
to the centre of the scene as possible to encourage
the use of the target?s direct object properties and its
spatial relations to other objects, rather than its over-
all location in the scene, as in in the left. The other
two object pairs were placed slightly further back to
1The corpus is available for download online at
www.clt.mq.edu.au/research/projects/gre3d7.
13
the left and right of the target?landmark pair, and
the single object was always placed in the far right
or the far left of the scene. Objects were of one of
two types (ball or cube) and otherwise distinguish-
able by their size and colour. Each object could be
either large or small, and in each scene we used only
two colours. Figure 1 shows a close-up of one of
the scenes as presented to the subjects, and Figure 2
shows the complete set of stimulus scenes.
The design of the stimulus scenes was based on a
number of hypotheses about the factors that might
influence people?s use of spatial relations to the
landmark object. The two main hypotheses are con-
cerned with the influence of the landmark object?s
size on its visual salience and the likelihood of the
target?landmark relation being used in a referring
expression:
Hypothesis 1: A large landmark is more salient
than a small one because it occupies more of
the visual space of a scene. Therefore, a large
landmark is more likely to be mentioned in a re-
ferring expression via its spatial relation to the
target referent than a small landmark.
Hypothesis 2: A landmark that shares its size with
a number of other objects in the scene is less
salient than one that is unique in size. There-
fore, a landmark with unique size is more likely
to be mentioned in a referring expression via
its spatial relation to the target referent than a
landmark with a common size.
Hypotheses 1 and 2 are concerned with the land-
mark?s overall salience in the scene, or what is usu-
ally called bottom-up salience in the literature on
visual attention (cf., Yantis, 1998). A second con-
sideration that might influence the use of relations is
the top-down salience of the target and landmark ob-
jects, as determined by the task the participants are
performing. At the time when the landmark?s visual
salience is taken into account, the participants are
focusing their attention on the target object. As the
landmark is the closest object to the target, it is likely
that the difference or similarity between these two
objects plays a particularly important role in the de-
cision whether to include the relation between them
or not. Two conflicting hypotheses can be formu-
lated here:
Hypothesis 3: The difference between the land-
mark and the target object impacts on the visual
salience of the landmark because it impacts on
the landmark?s overall uniqueness in the scene.
Therefore, a landmark that is visually different
from the target is more likely to be included in a
referring expression than one that looks similar
to the target.
Hypothesis 4: The more similar the landmark and
target objects are, the more they appear as one
visual unit rather than two separate objects. If
they are perceived and conceptualised as a vi-
sual unit, they are more likely to be mentioned
together. Therefore, the more similar the land-
mark is to the target, the more likely it is to be
included in a referring expression.
The fifth hypothesis that this experiment is designed
to test concerns the preference that participants in
psycholinguistic work have shown for vertical rela-
tions over horizontal ones (Lyons, 1977; Bryant et
al., 1992; Gapp, 1995; Bryant et al, 2000; Landau,
2003; Arts, 2004; Tenbrink, 2004). To make sure
that the landmark is never obscured by the target ob-
ject, we use lateral relations rather than frontal ones
in this experiment.
Hypothesis 5: A target placed on top of a landmark
object is more likely to be described in terms
of its spatial relation to the landmark than a tar-
get that is sitting directly adjacent to the left or
right of the landmark.
We report the results of putting these five hypothe-
ses to the test in Section 5.4. To be able to per-
form these tests systematically, the experiment was
designed as a 2?2?2?2?2 grid with the following
five variables:
? LM Size: the landmark is either large or small.
[Large/Small]
? LM Size Rare: the size of the landmark is ei-
ther a common size in the scene, or it is as
rare as possible, and possibly unique. If it is
common and the landmark is large, it shares
its size with two of the objects; if it is small,
with three. These numbers are not the same
because in each scene in which the landmark
14
size was common, three objects were large
and four small. In +LM Size Rare scenes that
are also +TG Size = LM Size, the landmark
shares size only with the target. Only if the
scene is ?TG Size = LM Size can the land-
mark?s size be truly unique in the scene. [+/?]
? TG Size = LM Size: target and landmark are
either the same size or different. [+/?]
? TG Col = LM Col: The target and the land-
mark are either of the same colour or different
in colour. [+/?]
? Relation: The relation between the target and
the landmark is either vertical (the target is on
top of the landmark) or lateral, in which case
the target is placed directly to the left or right
of the landmark. [Vertical/Lateral]
This resulted in 32 experimental conditions. We cre-
ated one stimulus scene for each of these conditions.
We then split the stimuli into two trial sets along the
factor TG Size = LM Size, so that this variable be-
came a between-participant factor, while the other
four are within-participant factors.
We followed a number of other criteria for the de-
sign of the stimulus scenes to ensure maximum ex-
perimental control over the factors influencing the
content of the referring expressions provided by our
participants:
Target uniqueness: The target was always distin-
guishable in terms of its inherent properties alone,2
which means that the relation to the landmark or
other external properties, such as the location in the
scene, were never necessary to fully distinguish the
target from all other objects in the scene.
Landmark uniqueness: As the target, the land-
mark was always distinguishable in terms of its in-
herent properties alone.
Colour balance: Each scene followed one of two
colour schemes: either blue?green or red?yellow.
The colour schemes were distributed in a balanced
way across the five experimental variables, so that
2We use the term inherent property to refer to any property
of an entity which that entity has independent of the context in
which it appears.
half of the scenes in each condition were blue?green
and the other half red?yellow. The colour scheme
was not expected to have an influence on the con-
tent of the referring expressions people produced. In
each scene, four objects were of one colour of the
colour scheme for this scene and three had the other
colour.
Relation balance: The relation between the target
and the object was never unique. One of the two
other object pairs in each scene was arranged in the
same spatial relation as the target?landmark pair and
the third pair had the other relation. However, the
objects in the pair with the same relation were never
of the same types as the target and landmark, so that
a description containing the type of the target, a re-
lation to the landmark and the type of the landmark
was always fully distinguishing.
Constant landmark and target types: The land-
mark was always a cube, in order to avoid scenes
where the target would have to be balanced on top
of a ball, which might look unnatural. The target
was always a ball to make sure that the similarity in
type between these two objects was always constant.
No obscured objects: The objects were placed in
the scenes in such a way that no object occluded any
other. In particular, as mentioned above, there were
no frontal relations within the object pairs, to avoid
larger objects obscuring smaller ones completely or
to a large degree.
Figure 2 shows the 2?2?2?2?2 grid of the 32
stimuli scenes. Scenes 1?16, shown on a green back-
ground, constitute Trial Set 1, and Scenes 17?32,
shown on a blue background, constitute Trial Set 2.
3 Procedure and Participants
The data gathering experiment was designed as a
self-paced on-line language production study. Par-
ticipants visited a website, where they first saw an
introductory page with a set of simple instructions
and a sample stimulus scene. Each participant was
assigned one of the two trial sets containing 16 stim-
ulus scenes each. After the instruction page, the
scenes were presented consecutively in an order that
was randomised for every participant. Below each
scene, the participants had to complete the sentence
15
TG_Col 
=/= 
LM_Col
TG_Col 
= 
LM_Col
TG_Col 
=/= 
LM_Col
1 135 9
2 106 14
151173
4 8 1612
TG_Col 
= 
LM_Col
29252117
18 22 3026
3123 2719
3224 2820
LM_Size 
Common
LM_Size 
Rare
LM_Size 
Common
LM_Size 
Rare
V
e
r
t
i
c
a
l
 
R
e
l
a
t
i
o
n
L
a
t
e
r
a
l
 
R
e
l
a
t
i
o
n
LM Large LM Small
LM_Size 
Common
LM_Size 
Rare
LM_Size 
Common
LM_Size 
Rare
LM Large LM Small
TG_Size =/= LM_Size TG_Size = LM_Size
Figure 2: The 32 stimulus scenes for GRE3D7: The left half constitutes Trial Set 1 and the right half is Trial Set 2.
Please pick up the . . . in a text box before click-
ing a button labelled ?DONE? to move on to the next
scene, as shown in Figure 1. The task was to de-
scribe the target referent in the scene (marked by a
grey arrow) in a way that would enable a friend look-
ing at the same scene to pick it out from the other
objects. To encourage the use of fully distinguish-
ing descriptions, participants were told that they had
only one chance at describing the object.
Before each of the 16 stimulus scenes, the partic-
ipants were shown a filler scene, which means each
participant had to describe 32 scenes in total. The
main motivation for using filler scenes was to min-
imise the decline in relation use over time, which
might otherwise happen if participants realised that
relations were never necessary.
The filler scenes were also designed with the in-
tention of making the experiment less monotonous,
and to stop participants from noticing the strict de-
sign features of the stimulus scenes. In particular,
each participant saw: four scenes with twelve ob-
jects in all four colours, as opposed to the two-colour
schemes; two scenes containing only three objects;
and ten further filler scenes which intentionally vio-
lated the above design criteria. The filler scenes for
each participant were chosen such that in eleven or
twelve scenes the target was a cube instead of a ball,
in two scenes the landmark was a ball, in four scenes
there was no obvious landmark close to the target, in
eight scenes the target was unique (i.e. it could not
be described by its inherent visual properties alone),
in nine or ten scenes the target and landmark shared
type, and in two or three scenes target and landmark
were of the same size; for participants who saw Trial
Set 2 all stimulus scenes also had a target and land-
mark of the same size.
The sequence of the 32 scenes that were shown to
a particular participant was determined by the fol-
lowing three steps:
1. Pick the opposite trial set to the one that the last
participant saw and randomise its order.
2. Pick the set of 16 filler scenes to be shown to
this participant and randomise their order.
3. Interleave the two sets so that each stimulus
scene is preceded by one filler scene.
After having described all 32 scenes in the trial,
participants were asked to complete an exit ques-
tionnaire, which gave them the option of having
their data discarded and asked for their opinion on
whether the task became easier over time and any
other comments they might wish to make.
The experiment was started by 318 native English
speakers, of which 294 completed all 32 scenes.
They were recruited by word of mouth via a widely-
circulated call for participation and two electronic
mailing lists.3 The participants were predominantly
in their twenties or thirties and mostly university-
educated. A slight majority (54%) were female.
None of them reported colour-blindness. Each re-
ferring expression in the corpus is tagged with an
anonymous ID number linking it to some simple de-
mographic data about the contributing participant,
including gender, age, type of English spoken, and
field of education.
3The Corpora List and the SIGGEN List.
16
4 Data Filtering and Annotation
Of the 294 participants who completed the experi-
ment, five consistently used only type, although the
target?s type was never fully distinguishing in any of
the stimulus scenes. For example, these participants
described the target in Figure 1 simply as ball, which
does not distinguish it from the two other balls in
the scene. We discarded the data of these partici-
pants under the assumption that they had not under-
stood the instruction that their descriptions were to
uniquely identify the target. Two participants? data
were discarded because they provided text that was
unrelated to the displayed scenes. Of the remaining
287 participants, 140 saw Trial Set 2 and 147 saw
Trial Set 1. The data from seven randomly-chosen
participants from Trial Set 1 were discarded to bal-
ance the corpus in terms of the between-participant
feature TG Size = LM Size. Each person described
the 16 scenes contained in either of the trial sets, re-
sulting in a corpus of 4480 descriptions in total, with
140 descriptions for each scene. No other corpus of
referring expressions contains as many descriptions
for each referential scenario from different speak-
ers, which makes this corpus ideal for the study of
speaker-specific preferences and non-deterministic
choices in content selection.
Only five of the 4480 descriptions used the ternary
spatial relation between, and one description men-
tioned two distinct spatial relations, one to the in-
tended landmark and one to another object. The re-
lation to the third object in these six descriptions was
disregarded in the analysis presented here.
In order to be able to analyse the semantic content
of the referring expressions, we semi-automatically
annotated the inherent attributes and relations con-
tained in each of them. The attributes annotated are
? type[ball, cube]
? colour[blue, green, red, yellow]
? size[large, small]
? location[right, left, front, top, bottom, centre]
? relation[horizontal, vertical]
Each attribute (except relation) is prefixed by either
tg or lm to mark which of the objects it pertains
to. For example, tg size indicates that the size of the
target was mentioned.
% of total % of all 600
4480 relational
attribute count descriptions descriptions
tg size 2587 57.8 ?
tg colour 4423 98.7 ?
tg location 81 1.8 ?
relation 600 13.4 ?
lm size 327 7.3 54.5
lm colour 521 11.6 86.8
lm location 10 0.2 1.7
Table 1: Attribute counts in GRE3D7
In the 83 descriptions containing comparatives,
such as Example (1), we ignored the second object
that the target was being compared to. In all of these
cases, the target?s colour and type were also men-
tioned, which means that in the context of the sim-
ple scenes at stake here, Example (1) is semantically
equivalent to Example (2).
(1) the smaller of the two red balls
(2) the small red ball
The question of how to deal with the relative na-
ture of size is a separate, non-trivial, issue; see (van
Deemter, 2000; van Deemter, 2006).
5 Analysis of the GRE3D7 Corpus
In this section we examine the content of the 4480
descriptions that make up the GRE3D7 Corpus. We
first give an overview of the use of the non-relational
attributes, and then proceed to investigate the hy-
potheses from Section 2 regarding the use of spatial
relations.
The target object?s type was mentioned in each
description in the corpus, and each relational de-
scription contained the landmark object?s type. Ta-
ble 1 shows the number of descriptions containing
each of the other attributes.
5.1 Sparing Use of location
Only 81 descriptions (1.8%) made reference to the
target referent?s location in the scene, as in Exam-
ple (3); and of the 600 relational descriptions in the
corpus, only ten (1.7%) contained the location of the
landmark, as in Example (4).
(3) the large yellow ball on the left [Scene 9]
17
(4) the small ball next to the large cube on the left
hand side [Scene 6]
There were no descriptions containing both
tg location and lm location. This might indicate
that participants who used a relation were more
likely to conceptualise the target?landmark pair
as a unit with just one location rather than as two
individual entities. However, the corpus was not
designed to investigate this issue and the numbers
for use of location are too low to draw any definite
conclusions.
5.2 Abundant Use of colour
Colour was used in the vast majority of descriptions:
98.7% of all descriptions included the colour of the
target object and 86.8% of the relational descriptions
included the colour of the landmark object. A high
number of descriptions containing colour could be
expected, as colour was part of the shortest possi-
ble minimal description not containing any spatial
information (we call this the inherent MD of the tar-
get) for 20 of the 32 scenes (all but Scenes 17?24
and 29?32). However, the fact that colour was also
included in the majority of the descriptions contain-
ing spatial information, in the form of a relation or
the location, confirms previous findings to the effect
that colour is often included in descriptions redun-
dantly (Belke and Meyer, 2002; Arts, 2004; Gatt,
2007).
5.3 Utilitarian Use of size
The target?s size was mentioned in 57.8% of all de-
scriptions, and the landmark?s size in 54.8% of the
relational descriptions.
Considering that tg size was part of the inherent
MD in only 12 of 32 scenes (37.5%) of the stimu-
lus scenes (Scenes 2, 4, 9?12, 18, 20 and 25?28),
57.8% seems like a high proportion of descriptions
to be using this attribute. The use of tg size for
scenes where it was part of the inherent MD was at
90.2% very high, but this only accounts for just un-
der 60% of all the descriptions that contained this
attribute. The remaining 40% of descriptions con-
taining tg size were given for scenes in which this
attribute was not strictly necessary to distinguish the
target from the other objects.
Findings from eye-tracking experiments in psy-
cholinguistics have shown that size is rarely used in
situations where it adds no discriminatory power to
the referring expression at all, and that it is more
likely to be used to compare to or distinguish from
other objects of the same type, while the same is
not true for colour (Sedivy, 2003; Brown-Schmidt
and Tanenhaus, 2006). Let us therefore consider in
particular the scenes where tg size was not part of
the inherent MD, and look at the differing utility of
tg size in these scenes: 12 of the 20 scenes where
tg size was not necessarily part of the inherent MD
(Scenes 1, 3, 5?8, 13?16, 17, 19, 21?24 and 29?
32) nonetheless contained another object that shared
the target?s type (ball) but not its size (Scenes 1, 3,
17, 19, 21?24 and 29?32). In these scenes, tg size
remains a useful attribute to use, even if tg type is
also included.
Based on the psycholinguistic findings mentioned
above, one might expect that the use of tg size is
higher for these scenes because here it helps distin-
guish from another object of the same type rather
than only from objects of a different type. This hy-
pothesis is supported by the data: tg size was used
in 45.6% of the descriptions for scenes where it was
not part of the inherent MD but there was another ob-
ject of same type and different size as the target. For
scenes where tg size could only distinguish the tar-
get from objects of the other type, it was only used
in 27.3% of cases (?2=94.97, df=1, p.01).
5.4 The Use of Spatial Relations
600 of the 4480 descriptions in the GRE3D7 Cor-
pus (13.4%) mentioned a spatial relation. This was
despite the fact that spatial information was not re-
quired in any of the stimulus scenes. Most existing
approaches to spatial relations in REG would there-
fore never include a relation for any of the stimuli.
In this section, we examine the circumstances un-
der which the participants of the GRE3D7 data col-
lection experiment used the spatial relation between
the target object and the intended landmark. We
will first examine participant-dependent and tempo-
ral factors and then move on to analyse the impact
that the design features of the scenes, described in
Section 2, had on the use of relations.
General Factors
We first checked for broad participant-dependent
preferences for or against using relations in the
18
GRE3D7 Corpus. The behaviour of participants
who use an exclusive strategy of either always or
never including a relation in their referring expres-
sions would be easy to predict in a computational
model and does not contribute to any variation
across different scenes. In order to gain a clear un-
derstanding of this variation, we will concentrate on
the data from participants who varied their use of
relations between scenes.
Half of the participants (50.3%) adopted an ex-
clusive strategy regarding the use of relations. How-
ever, the split between the two exclusive strategies
was very uneven: 135 participants never used a spa-
tial relation and only six used a spatial relation for
all 16 stimulus scenes they saw. In the following, we
analyse the data from the 139 participants who used
a relation for some scenes but not for others. On av-
erage, these participants used a relation in 22.7% of
their descriptions.
In (Viethen and Dale, 2008), we observed a ?lazi-
ness effect? whereby participants? use of relations
decreased over the course of the experiment. A num-
ber of participants mentioned in the exit interview
that they noticed over time that relations were never
required and stopped using them. Such a conscious,
or semi-conscious, adjustment masks people?s nat-
ural propensity to use a relation in a reference situ-
ation where they come anew at the task rather than
describing one object after another.
In the GRE3D7 collection experiment, each par-
ticipant saw eight filler scenes in which spatial rela-
tions were required to distinguish the target. These
filler scenes were included to stop participants from
consciously noticing that relations were never re-
quired in the stimulus scenes. We hoped that this
would reduce the laziness effect and thereby pro-
duce results that better approximate people?s natu-
ral tendency to use a relation. However, Figure 3
shows that, despite the use of these filler scenes, the
use of relations declined over the course of the ex-
periment. Participants who did not follow an exclu-
sive strategy clearly used more relations for scenes
they saw early on than for those they saw towards
the end. We divided the data set into quartiles in
order to test the statistical significance of this de-
cline. The falling trend was statistically significant
at p.01 (?2=55.42, df=3). However, any tem-
poral effect in GRE3D7 should not interfere with
!"#
$"#
%!"#
%$"#
&!"#
&$"#
'!"#
'$"#
(!"#
($"#
%)*
#
&+,
#
'-,
#
(*.
#
$*.
#
/*.
#
0*.
#
1*.
#
2*.
#
%!*
.#
%%*
.#
%&*
.#
%'*
.#
%(*
.#
%$*
.#
%/*
.#!"
#$#
"%#
&'#
(')*
+,%
#&,
+'-
*./
"0$
%#
&.'
1/*&*.'0&'2*3$#",+'4"5*"'
Figure 3: Temporal effect on use of relation
!"#$%&
!'#"%&
!(#"%&
!)#*%&
()#"%&
!!#"%&
(*#$%&
!+#$%&
('#,%&
"$#$%&
$%&
)%&
($%&
()%&
!$%&
!)%&
"$%&
-./0123& -./0123/4563& 78/0123&9&-./0123& 78/:;<&9&-./:;<& 43<5=;>&
!"#$
#"%#
&''#(
')*+,
%#&
,+'-
*./"
0$%#
&.'
3?@3AB3C&<;D& 3?@3AB3C&E1FE&
!"#$$ $#%&' (%)' (%)' (%)' $#('%#$*#$!' *#$!' *#$!' +'%(,-#$
./01023/45/53675726889/54:;4<26;3/63/=>>?@A
./
./
./
Figure 4: Effect of design variables on use of relation
between-stimulus effects, as the stimuli were pre-
sented in a randomised order.
Influence of Scene Features on Relation Use
We will now turn to the examination of Hypothe-
ses 1?5 from Section 2. Figure 4 shows the impact
that each of the five variables of the scene design had
on the use of relations. The left (green) columns rep-
resent the conditions for which we expected fewer
relations to be used, and the right (yellow) columns
represent the conditions for which we expected a
higher use of relations, according to Hypotheses 1?
3 and 5. Hypothesis 4 expected the reverse results
for TG Size = LM Size and TG Col = LM Col. All
factors except LM Size and TG Size = LM Size had
a statistically significant effect.
Hypotheses 1 and 2, which expected a large
landmark with a rare or unique size to be more
salient and therefore more likely to be used, are
not supported by the data here. LM Size did not
have a reliable effect (?2=0.16, df=1, p>.6) and
19
LM Size Rare shows the opposite effect of the one
we expected: a relation to a landmark with a com-
mon size is significantly more likely to be included
in a referring expression than one to a landmark with
a rare or unique size (?2=56.19, df=1, p.01). On
closer inspection, this is likely to be due to a fac-
tor that was not explicitly tested or controlled for in
this experiment: the length of the inherent MD of the
target referent. In most scenes with a common land-
mark size (all but Scenes 1, 3, 17, and 19), all three
inherent attributes (size, colour and type) are neces-
sary to distinguish the target from the other objects
without using locational information. In all scenes
where the landmark?s size is rare or unique, colour
and type suffice. In other words, targets which are
harder to describe using inherent visual properties
only are more likely to be described by a relation to
a nearby landmark.
Hypotheses 3 and 4 predicted two mutually ex-
clusive scenarios based on the assumption that the
similarity between the target and the landmark ob-
ject is of special importance, as the participant?s vi-
sual attention is likely to be focussed on these two
objects. Hypothesis 3 predicted that a visual dif-
ference between the landmark and the target would
increase the landmark?s salience and therefore the
use of the spatial relation to this landmark. Hypoth-
esis 4 predicted that high visual similarity between
target and landmark might result in these two ob-
jects being conceptualised as a unit, which would
increase the likelihood of both objects being men-
tioned. The target and landmark object were al-
ways of different types, so their similarity depends
on their size and their colour, captured in the vari-
ables TG Size = LM Size and TG Col = LM Col.
TG Size = LM Size did not show a significant ef-
fect on the use of relations (?2=2.29, df=1, p>.1).
The effect of TG Col = LM Col favours Hypothe-
sis 4, as a landmark of the same colour as the target
is more likely to be included in the target?s descrip-
tion than one that has a different colour from the tar-
get (?2=11.18, df=1, p0.01).
The variable Relation had the expected effect: A
vertical relation is significantly more likely to be
used than a lateral one (?2=69.00, df=1, p.01).
This confirms Hypotheses 5.
6 Conclusion
We have described the GRE3D7 Corpus, a collec-
tion of human-produced distinguishing descriptions
that is considerably larger than any other existing
corpus. The collection also uses scenes that are a
degree more complex than those found in existing
corpora; these are based on a principled design in
order to provide a measure of control over what can
be learned from the data. In this paper we have de-
scribed the details of the collection experiment and
have presented an analysis of the impacts that the de-
sign variables had on the content of the resulting de-
scriptions. The main outcomes of this analysis are:
Colour is used in 99% of all descriptions. It is
also used redundantly in 87% of all relational de-
scriptions. This is in accordance with findings in
other corpora and psycholinguistic studies.
Size is used when it is distinguishing. The size
of the target referent was much more likely to be
included when it was useful in distinguishing from
another object in the scene, especially those of the
same type.
Just over half of the participants follow an ex-
clusive strategy for the use of relations. A large
proportion of participants (135) opted to never use
a relation, while a much smaller number of people
(6) used a relation in all of their descriptions. The
remaining 139 participants are responsible for the
variation in the data, as they used a relation to de-
scribe the target in some but not all scenes.
The target?landmark relation is used more often
if it is vertical than if it is lateral. This confirms
previous psycholinguistic findings showing that hu-
mans prefer vertical relations and prepositions over
horizontal, and in particular lateral, ones.
If a landmark shares colour with the target it is
more likely to be used in a referring expression. This
lends support to the hypothesis that visual similar-
ity between target and landmark increases the likeli-
hood of the relation between them being used.
The data thus sheds additional light on the nature
of human-produced descriptions of objects in visual
scenes. It also, of course, provides a rich corpus of
data that can be readily used to evaluate the perfor-
mance of computational algorithms for the genera-
tion of referring expressions.
20
References
Anja Arts. 2004. Overspecification in Instructive Texts.
Ph.D. thesis, University of Tilburg, The Netherlands.
Eva Belke and Antje S. Meyer. 2002. Tracking the
time course of multidimensional stimulus discrimina-
tion: Analysis of viewing patterns and processing time
during same-different decisions. European Journal of
Cognitive Psychology, 14(2):237?266.
Anja Belz and Albert Gatt. 2007. The Attribute Selection
for GRE Challenge: Overview and evaluation results.
In Proceedings of the Workshop on Using Corpora for
NLG: Language Generation and Machine Translation
(UCNLG+MT), pages 75?83, Copenhagen, Denmark.
Sarah Brown-Schmidt and Michael K. Tanenhaus. 2006.
Watching the eyes when talking about size: An investi-
gation of message formulation and utterance planning.
Journal of Memory and Language, 54:592?609.
David J. Bryant, Barbara Tversky, and Nancy Franklin.
1992. Internal and external spatial frameworks rep-
resenting described scenes. Journal of Memory and
Language, 31:74?98.
David J. Bryant, Barbara Tversky, and M. Lanca.
2000. Retrieving spatial relations from observation
and memory. In Emile van der Zee and Urpo Nikanne,
editors, Cognitive interfaces: Constraints on linking
cognitive information, pages 94?115. Oxford Univer-
sity Press, Oxford, UK.
Robert Dale and Nicholas Haddock. 1991. Content de-
termination in the generation of referring expressions.
Computational Intelligence, 7(4):252?265.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the As-
sociation for Computational Linguistics, pages 68?75,
Vancouver BC, Canada.
Klaus-Peter Gapp. 1995. Angle, distance, shape, and
their relationship to projective relations. In Proceed-
ings of the 17th Annual Meeting of the Cognitive Sci-
ence Society, pages 112?117, Pittsburgh PA, USA.
Claire Gardent, He?le`ne Manue?lian, Kristina Striegnitz,
and Marilisa Amoia. 2004. Generating definite de-
scriptions: Non incrementality, inference and data.
In Thomas Pechmann and Christopher Habel, edi-
tors, Multidisciplinary Approaches to Language Pro-
duction, pages 53?86. Walter de Gruyter, Berlin, Ger-
many.
Claire Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 96?103, Philadelphia PA, USA.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation, pages 49?56, Schlo? Dagstuhl,
Germany.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA
Challenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation, pages 198?206, Salt
Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Work-
shop on Natural Language Generation, pages 174?
182, Athens, Greece.
Albert Gatt. 2007. Generating Coherent Reference to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, UK.
Surabhi Gupta and Amanda Stent. 2005. Automatic
evaluation of referring expression generation using
corpora. In Proceedings of the Workshop on Using
Corpora for Natural Language Generation, pages 1?
6, Brighton, UK.
Helmut Horacek. 2003. A best-first search algorithm
for generating referring expressions. In Proceedings
of the 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
103?106, Budapest, Hungary.
Helmut Horacek. 2004. On referring to sets of objects
naturally. In Proceedings of the 3rd International Con-
ference on Natural Language Generation, pages 70?
79, Brockenhurst, UK.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
John Kelleher and Geert-Jan Kruijff. 2006. Incremen-
tal generation of spatial referring expressions in situ-
ated dialog. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational
Linguistics, pages 1041?1048, Sydney, Australia.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expressions.
In Kees van Deemter and Rodger Kibble, editors, In-
formation Sharing: Reference and Presupposition in
Language Generation and Interpretation, pages 223?
264. CSLI Publications, Stanford CA, USA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Lingustics, 29(1):53?72.
Barbara Landau. 2003. Axes and direction in spatial
language and spatial cognition. In Emilie van der Zee
21
and Jon M. Slack, editors, Representing Direction in
Language and Space, pages 18?38. Oxford University
Press, Oxford, UK.
John Lyons. 1977. Semantics, volume 2. Cambridge
University Press, Cambridge, UK.
Julie C. Sedivy. 2003. Pragmatic versus form-based ac-
counts of referential contrast: Evidence for effects of
informativity expectations. Journal of Psycholinguis-
tic Research, 32(1):3?23.
Thora Tenbrink. 2004. Identifying objects on the basis
of spatial contrast: An empirical study. In Christian
Freksa, Markus Knauff, Bernd Krieg-Brckner, Bern-
hard Nebel, and Thomas Barkowsky, editors, Spatial
cognition IV: Reasoning, action, interaction, number
3343 in Lecture Notes in Computer Science, pages
124?146. Springer, Berlin/Heidelberg, Germany.
Kees van Deemter. 2000. Generating vague descrip-
tions. In Proceedings of the 1st International Con-
ference on Natural Language Generation, pages 179?
185, Mitzpe Ramon, Israel.
Kees van Deemter. 2006. Generating referring expres-
sions that involve gradable properties. Computational
Linguistics, 32(2):195?222.
Ielka van der Sluis and Emiel Krahmer. 2004. Evalu-
ating multimodal NLG using production experiments.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal, 26-28 May.
Ielka van der Sluis and Emiel Krahmer. 2005. Towards
the generation of overspecified multimodal referring
expressions. In Proceedings of the Symposium on Di-
alogue Modelling and Generation at the 15th Annual
Meeting of the Society for Text and Discourse, Ams-
terdam, The Netherlands, 6-9 July.
Ielka van der Sluis. 2005. Multimodal Reference, Stud-
ies in Automatic Generation of Multimodal Referring
Expressions. Ph.D. thesis, Tilburg University, The
Netherlands.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of the 5th International Conference on Natu-
ral Language Generation, pages 59?67, Salt Fork OH,
USA.
Jette Viethen, Simon Zwarts, Robert Dale, and Markus
Guhe. 2010. Dialogue reference in a visual domain.
In Proceedings of the 7th International Conference on
Language Resources and Evaluation, Valetta, Malta.
Steven Yantis. 1998. Control of visual attention. In
Harold Pashler, editor, Attention, chapter 6, pages
223?256. Psychology Press, Hove, UK.
22
Proceedings of the 14th European Workshop on Natural Language Generation, pages 72?81,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Graphs and Spatial Relationsin the Generation of Referring ExpressionsJette Viethen
h.a.e.viethen@uvt.nl
TiCC
University of Tilburg
Tilburg, The Netherlands
Margaret Mitchell
m.mitchell@jhu.edu
HLT Centre of Excellence
Johns Hopkins University
Baltimore, USA
Emiel Krahmer
e.j.krahmer@uvt.nl
TiCC
University of Tilburg
Tilburg, The NetherlandsAbstract
When they introduced the Graph-Based
Algorithm (GBA) for referring expression
generation, Krahmer et al (2003) flaunted
the natural way in which it deals with re-
lations between objects; but this feature
has never been tested empirically. We fill
this gap in this paper, exploring referring
expression generation from the perspec-
tive of the GBA and focusing in particu-
lar on generating human-like expressions
in visual scenes with spatial relations. We
compare the original GBA against a variant
that we introduce to better reflect human
reference, and find that although the orig-
inal GBA performs reasonably well, our
new algorithm offers an even better match
to human data (77.91% Dice). Further, it
can be extended to capture speaker vari-
ation, reaching an 82.83% Dice overlap
with human-produced expressions.1 Introduction
Ten years ago, Krahmer et al (2003) published the
Graph-Based Algorithm (GBA) for referring ex-
pression generation (REG). REG has since become
one of the most researched areas within Natural
Language Generation, due in a large part to the
central role it plays in communication: referring
allows humans and language generation systems
alike to invoke the entities that the discourse is
about in the mind of a listener or reader.
Like most REG algorithms, the GBA is focussed
on the task of selecting the semantic content for a
referring expression, uniquely identifying a target
referent among all objects in its visual or linguistic
context. The framework used by the GBA is par-
ticularly attractive because it provides fine-grained
control for finding the ?best? referring expression,
encompassing several previous approaches. This
control is made possible by defining a desired
cost function over object properties to guide the
construction of the output expression and using a
search mechanism that does not stop at the first
solution found.
One characteristic of the GBA particularly em-
phasized by Krahmer et al (2003), advancing
from research on algorithms such as the Incre-
mental Algorithm (Dale and Reiter, 1995) and the
Greedy Algorithm (Dale, 1989), was the treatment
of relations between entities. Relations such as ontop of or to the left of fall out naturally from the
graph-based representation of the domain, a facet
missing in earlier algorithms. We believe that this
makes the GBA particularly well-suited for gener-
ating language in spatial visual domains.
In the years since the inception of the GBA,
the REG community has become increasingly in-
terested in evaluating algorithms against human-
produced data in visual domains, aiming to mimic
human references to objects. This interest has
manifested most prominently in the 2007-2009
REG Challenges (Belz and Gatt, 2007; Gatt et al,
2008; Gatt et al, 2009) based on the TUNA Cor-
pus (van Deemter et al, 2012). The GBA per-
formed among the best algorithms in all three of
these challenges. However, in particular its abil-
ity to analyze relational information could not be
assessed, because the TUNA Corpus does not con-
tain annotated relational descriptions.
We rectify this omission in the current work by
testing the GBA on the GRE3D3 Corpus, which
was designed to study the use of spatial rela-
tions in referring expressions (Viethen and Dale,
2008). We compare against a variant of the GBA
that we introduce to build longer referring expres-
72
sions, following the observation that humans tend
to overspecify (i.e., not be maximally brief) in
their referring expressions (Sonnenschein, 1985;
Pechmann, 1989; Engelhardt et al, 2006; Arts et
al., 2011). For both algorithms, we experiment
with cost functions defined at different granular-
ities to produce the best match to human data. We
find that we can match human data better than
the original GBA with the variant that encourages
overspecification.
With this model, we aim to further ad-
vance towards human-like reference by develop-
ing a method to capture speaker-specific varia-
tion. Speaker variation cannot easily be modeled
by the classic input variables of REG algorithms,
but a number of authors have shown that system
output can be improved by using speaker identity
as an additional feature; this has often been ac-
companied by the observation that commonalities
can be found in the reference behaviour of differ-
ent speakers (Bohnet, 2008; Di Fabbrizio et al,
2008a; Mitchell et al, 2011b), particularly for spa-
tial relations (Viethen and Dale, 2009). In the sec-
ond experiment reported in this paper, we combine
these insights by automatically clustering groups
of speakers with similar behaviour and then defin-
ing separate cost functions for each group to better
guide the algorithms.
Before we assess the ability of the GBA and our
variant to produce human-like referring expres-
sions containing relations (Sections 5 and 6), we
will give an overview of the relevant background
to the treatment of relations in REG, a short history
of the GBA, and the relevance of individual vari-
ation (Section 2). We introduce our new variant
graph-based algorithm, LongestFirst, in Section 3.2 Relations, Graphs and IndividualVariation2.1 Relations in REG
In the knowledge representation underlying most
work in REG, each object in a scene is modeled as
a set of attribute-value pairs describing the object?s
properties, such as hsize, largei. Such a represen-
tation is used in the two of the classic algorithms,
the Greedy Algorithm (Dale, 1989) and the Incre-
mental Algorithm (IA) (Dale and Reiter, 1995).
Neither of these was originally intended to process
relations between objects.
Several attempts have been made to adapt the
traditional REG algorithms to include relations be-
tween objects in their output, but all of them suf-
fer from problems with the knowledge representa-
tion not being suited to relations. Dale and Had-
dock (1991) use a constraint network and a recur-
sive loop to extend the Greedy Algorithm, which
uses the discriminatory power of an attribute as
the main selection criterion. They treat relations
the same as other attributes; but in most cases a
certain spatial relation to a particular other ob-
ject is fully distinguishing, which easily leads to
strange chains of relations in the output omitting
most other attributes (Viethen and Dale, 2006).
Krahmer and Theune (2002) suggest a simi-
lar adjustment for the IA by introducing a re-
cursive loop if a relation to another object is in-
troduced to the referring expression under con-
struction. They treat relations as fundamentally
different from other attributes in order to recog-
nize when to enter the recursive loop, however,
they fail to address the problem of infinite regress,
whereby the objects in a domain might be de-
scribed in a circular manner by the relations hold-
ing between them. Another relational extension to
the IA has been proposed by Kelleher and Kruijff
(2006), treating relations as a completely different
class from other attributes. Both extensions of the
IA make the simplifying assumption that relations
should only be considered if it is not possible to
fully distinguish the target referent from the sur-
rounding objects in any other way, with the idea
that it takes less effort to consider and describe
only one object (Krahmer and Theune, 2002; Vie-
then and Dale, 2008).2.2 A Short History of the GBA
A new approach to REG was proposed by Krah-
mer et al (2003). In this approach, a scene is
represented as a labeled directed graph (see Fig-
ure 1(b)), and content selection is a subgraph con-
struction problem. Assuming a scene graph G =
hVG, EGi, where vertices VG represent objects and
edges EG represent the properties and relations of
these objects with associated costs, their algorithm
returns the cheapest distinguishing subgraph that
uniquely refers to the target object v 2 VG. Re-
lations between objects (i.e., edges between dif-
ferent vertices) are a natural part of this repre-
sentation, without requiring special computational
mechanisms. In addition to cost functions, the
GBA requires a preference ordering (PO) over the
edges to arbitrate between equally cheap descrip-
tions (Viethen et al, 2008).
73
(a) Scene 7 from the GRE3D3 Corpus.
b
e
l
o
w
a
b
o
v
e
left-of
right-of
yellow
small
ball
large
small
ball
red
cube
yellow
right-
hand
l
e
f
t
-
o
f
r
i
g
h
t
-
o
f
right-
hand
right-
hand
(b) A graph representing the scene to the left.
Figure 1: An example scene from the GRE3D3 Corpus and the corresponding domain graph.
As the cost functions and preference orders are
specified over edges (i.e., properties), they allow
much more fine-grained control over which prop-
erties to generate for a target referent than the
attribute-based preference orders employed by the
IA and its descendants. The cost functions can be
used to give preference to a commonly used size
value, such as large, over a rarely used color value,
such as mauve, although in general color is de-
scribed more often than size. This process is aided
by a branch-and-bound search that guarantees to
find the cheapest (i.e., ?best?) referring expression.
Since its inception, the GBA has been shown to
be useful for several referential phenomena. Krah-
mer and van der Sluis (2003) combined verbal
descriptions with pointing gestures by modelling
each such gesture as additional looping edges on
all objects that it might be aimed at. While the au-
thors confirmed the ideas implemented in the al-
gorithm in psycholinguistic studies (van der Sluis,
2005), they never assessed its output in an actual
domain.
van Deemter and Krahmer (2007) demonstrated
how the GBA could be used to generate reference
to sets as well as to negated and gradable prop-
erties by representing implicit information as ex-
plicit edges in domain graphs. They also presented
a simple way to account for discourse salience
based on restricting the distractor set. Its ability
to cover such a breadth of referential phenomena
makes the GBA a reasonably robust algorithm for
further exploring the generation of human-like ref-
erence.
The GBA was systematically tested against
human-produced referring expressions for the first
time in the ASGRE Challenge 2007 (Belz and
Gatt, 2007). This entry is described in detail in
(Viethen et al, 2008) and was very successful as
well in the following 2008 and 2009 REG Chal-
lenges (Gatt et al, 2008; Gatt et al, 2009) with
a free-na??ve cost function. This cost function as-
signs 0 cost to the most common attributes, 2 to
the rarest, and 1 to all others. By making the most
common attributes free, it became possible to in-
clude these attributes redundantly in a referring
expression, even if they were not strictly neces-
sary for identifying the target. The cost functions
used in the challenges were attribute-based, and
did therefore not make use of the refined control
capabilities of the GBA.
Theune et al (2011) used k-means clustering
on the property frequencies in order to provide
a more systematic method to transfer the FREE-
NAI?VE cost function to new domains. They found
that using only two clusters (a high frequency and
a low frequency group with associated costs of 0
and 1) achieves the best results, with no significant
differences to the FREE-NAI?VE cost function on
the TUNACorpus. Subsequently they showed that
on this corpus, a training set of only 20 descrip-
tions suffices to determine a 2-means cost function
that performs as well as one based on 165 descrip-
tions. In (Koolen et al, 2012), the same authors
extended these experiments to a Dutch version of
the TUNA Corpus (Koolen and Krahmer, 2010)
and came to a similar conclusion. Neither of the
corpora used in these experiments included rela-
tions between objects.2.3 Individual Variation in REG
A number of authors have argued that to be able to
produce human-like referring expressions, an al-
gorithm must account for speaker variation: Dif-
ferent speakers will refer to the same object in
different ways, and modeling this variation can
bring us closer to generating the rich variety of ex-
74
pressions that people produce. Several approaches
have been made in this direction.
Although this was not explicitly discussed in
(Jordan and Walker, 2005), the machine-learned
models presented there performed significantly
better at replicating human-produced referring
expressions when a feature set was used that
included information about the identity of the
speaker. In (Viethen and Dale, 2010), the impact
of speaker identity as a machine-learning feature
is more systematically tested. They show that ex-
act knowledge about which speaker produced a
referring expression boosts performance, but also
find many commonalities between different speak-
ers? strategies for content selection. Mitchell et
al. (2011b) used participant identity in a machine
learner to successfully predict the kind of size
modifier to be used in a referring expression. Ad-
ditionally, various submissions to the REG chal-
lenges, particularly by Bohnet and Fabbrizio et al
(Bohnet, 2008; Bohnet, 2009; Di Fabbrizio et al,
2008a; Di Fabbrizio et al, 2008b) used speaker-
specific POs to increase performance in their adap-
tations of the IA.
All of these systems used the exact speaker
identity as input, although many of the authors
noted that groups of speakers behave similarly
(Viethen and Dale, 2010; Mitchell et al, 2011b).
We build off of this idea by clustering similar
speakers together before learning parameters, and
then generate for speaker-specific clusters. This
method results in a significant improvement in per-
formance.3 LongestFirst: a New Search Strategy
The GBA guarantees to return the cheapest pos-
sible subgraph that fully distinguishes the target.
However, many distinguishing subgraphs can have
the same cost, for example, if a target can be iden-
tified either by its color or by its size, and color
and size have the same cost. Viethen et al (2008)
discuss some examples in more detail.
In the case that more than one cheapest sub-
graph exists, the original GBA will generate the
first it encountered. Due to its branch-and-bound
search strategy, this is also the smallest subgraph,
corresponding to the shortest possible description
that can be found at the cheapest cost. Because
its pruning mechanism does not allow further ex-
pansion of a graph once it is distinguishing, the
number of attributes that the algorithm can include
redundantly is limited, in particular if relations
are involved. Attributes of visually salient nearby
landmark objects that are introduced to the refer-
ring subgraph by a relation are only considered af-
ter all other attributes of the target object. This is
the case even if these attributes are free and feature
early in the preference order.
The GBA is therefore not able to replicate many
overspecified descriptions that human speakers
may use: if a subgraph containing a relation is
already distinguishing before the attributes of a
landmark object are considered, the algorithm will
not include any information about the landmark.
Not only is it unlikely that a landmark object
should be included in a description without any
further information about it, it also seems intu-
itive that speakers with a preference for certain
attributes (such as color) would include these at-
tributes not only for the target referent, but for a
landmark object as well.
We solve this problem by amending the search
algorithm in a way that finds the longest of all
the cheapest subgraphs, and call the resulting al-
gorithm LongestFirst. This search strategy results
in a much larger number of subgraphs to check, in
particular, when used with cost functions that in-
volve a lot of free edges. In order to keep our sys-
tems tractable, we therefore limit the number of
attributes the LongestFirst algorithm can include
to four, based on the finding from (Mitchell et al,
2011a) that people rarely include more than four
modifiers in a noun phrase. In Experiment 2 we
additionally test a setting in which the maximum
number of attributes is determined on the basis of
the average description length in the training data.4 Implementation Note
The original implementation of the GBA did not
provide a method to specify the order in which
edges were tried, although the edge order deter-
mines the order in which distinguishing subgraphs
are found by the algorithm (Krahmer et al, 2003).
This was fixed in (Viethen et al, 2008) by adding
a PO as parameter to the GBA to arbitrate between
equally cheap solutions.
A further issue arose in this implementation
when tested on the GRE3D3 domain, because
there was no simple way to specify which object
each property belonged to; for the TUNA domain
where the GBA has traditionally been evaluated, it
is safe to always assume a property belongs to the
75
target referent. We have therefore provided addi-
tional functionality to the GBA that requires that
not only hattribute, valuei pairs are specified, but
hentity1, attribute, value, entity2i tuples, which
can be translated directly into graph edges. For ex-
ample the tuple htg:relation:above:lmi represents
the edge labelled above between the yellow ball
and the red cube in Figure 1. For direct attributes,
such as size or color, entity1 and entity2 in these
tuples are identical, resulting in loop edges. This
Java implementation of the GBA and the Python
implementation of the LongestFirst algorithm are
available at www.m-mitchell.com/code.5 Experiment 1: Relational Descriptions
In our first experiment, we evaluate how well the
GBA produces human-like reference in a corpus
that uses spatial relations. We compare against the
LongestFirst variant that encourages overspecifi-
cation.5.1 Material
To evaluate the different systems, we use the
GRE3D3 Corpus. It consists of 630 distinguish-
ing descriptions for objects in simple 3D scenes.
Each of the 20 scenes contains three objects in
different spatial relations relative to one another
(see Figure 1). The target referent, marked by an
arrow, was always in a direct adjacency relation
(on   top   of or in   front   of) to one of the
other two objects, while the third object was al-
ways placed at a small distance to the left or right.
The objects are either spheres or cubes and differ
in size and color. In addition to these attributes, the
63 human participants who contributed to the cor-
pus used the objects? location as well as the spatial
relation between the target referent and the closest
landmark object. Each participant described one
of two sets of 10 scenes. The scenes in the two sets
are not identical, but equivalent, so the sets can be
conflated for most analyses. Spatial relations were
used in 36.6% (232) of the descriptions, although
they were never necessary to distinguish the target
object. Further details about the corpus may be
found in (Viethen and Dale, 2008).5.2 Approaches to Parameter Settings
As discussed above, the GBA behaves differently
depending on the PO and the cost functions over
its edges. To find the best match with human
data, we explore several different approaches to
setting these two parameters. An important dis-
tinction between the approaches we try hinges
on the difference between attributes and proper-ties. Attributes correspond to, e.g., color, size, orlocation, while properties are attribute-value pairs,
e.g., hcolor, redi, hsize, largei, hlocation,middlei.
Previous evaluations of the GBA typically used
parameter settings based on either attribute fre-
quency (Viethen et al, 2008) or property fre-
quency (Koolen et al, 2012). We compare both
methods for setting the parameters. Because the
scenes on which the corpus is based were not bal-
anced for the different attribute-values, the fre-
quency of a property is calculated as the pro-
portion of descriptions in which it was used for
those scenes where the target actually possessed
this property. For our evaluation, the trainable
costs and the POs are determined using cross-
validation (see Section 5.3). We use the following
approaches:
0-COST-PROP: All edges have 0 cost, and the
PO is based on property frequency. Each property
is included (regardless of how distinguishing it is)
until a distinguishing subgraph is found.
0-COST-ATT: As 0-COST-PROP, but the PO is
based on attribute frequency.
FREE-NAI?VE-PROP: Properties that occur in
more than 75% of descriptions where they could
be used cost 0, properties with a frequency below
20% cost 2, and all others cost 1 (Viethen et al,
2008). The PO is based on property frequency.
FREE-NAI?VE-ATT: As FREE-NAI?VE-PROP:, but
costs and PO are based on attribute frequency.
K-PROP: Costs are assigned using k-means clus-
tering over property frequencies with k=2 (Theune
et al, 2011). The PO is based on property fre-
quency.
K-ATT: As K-PROP, but the k-means clustering
and the PO are based on attribute frequency.5.3 Evaluation Setup
We evaluate the version of the GBA used by Vie-
then et al (2008), with additional handling for
relations between entities (see Section 4). We
compare against our LongestFirst algorithm from
Section 3 on all approaches described in Sec-
tion 5.2. As baselines, we compare against the
Incremental Algorithm (Dale and Reiter, 1995)
and a simple informed approach that includes at-
tributes/properties seen in more than 50% of the
76
training descriptions. We do not use the IA?s re-
lational extensions (Krahmer and Theune, 2002;
Kelleher and Kruijff, 2006), because these would
deliver the same relation-free output as the basic
IA (relations are never necessary for identifying
the target in GRE3D3). These two baselines are
tried with an attribute-based PO and a property-
based one. We do not expect a difference between
the attribute- and the property-based PO on the IA,
as this difference would only come to the fore in a
situation where a choice has to be made between
two values of the same attribute. In the IA?s anal-
ysis of the GRE3D3 domain, this can only happen
with relations, which it will not use in this domain.
We use Accuracy and Dice, the two most com-
mon metrics for human-likeness in REG (Gatt and
Belz, 2008; Gatt et al, 2009), to assess our sys-
tems. Accuracy reports the relative frequency with
which the generated attribute set and the human-
produced attribute set match exactly. Dice mea-
sures the overlap between the two attribute sets.
For details, see, for example, Krahmer and van
Deemter?s (2012) survey paper. We train and test
our systems using 10-fold cross-validation.5.4 Results
The original version of the Graph-Based Algo-
rithm shows identical performance for all ap-
proaches (See Table 1). All use a preference order
starting with type, followed by color and size, and
a cost function that favors the same attributes. As
these attributes always suffice to distinguish the in-
tended referent, the algorithm stops before spatial
relations are considered. For the scene in Figure 1
it includes the minimal content htg:type:balli, but
for a number of scenes it overspecifies the descrip-
tion.
The LongestFirst/0-COST systems and the
LongestFirst/K-PROP system are the only sys-
tems that include relations in their output.
The LongestFirst/0-COST systems both in-
clude a relation in every description; however,
not always the one that was included in the
human-produced reference, resulting in 521
false-positives for the attribute-based version
and 398 for the property-based one. For the
scene in Figure 1 they include htg:color:yellow,tg:size:small, tg:type:ball, tg:right of:obj3i and
htg:color:yellow, tg:size:small, tg:type:ball,tg:on top of:lmi, respectively. The first
one of these two attribute sets (produced by
Original Longest
GBA First
0-COST- Acc 39.21 0.16
PROP Dice 73.40 68.75
0-COST- Acc 39.21 0.00
ATT Dice 73.40 64.34
FREE-NAI?VE Acc 39.21 46.51
-ATT Dice 73.40 77.91
FREE-NAI?VE Acc 39.21 38.10
-PROP Dice 73.40 74.99
K-PROP Acc 39.21 35.08Dice 73.40 74.66
K-ATT Acc 39.21 35.08Dice 73.40 74.56
50%-Base IA
prop- Acc 27.30 37.14
based PO Dice 72.17 72.21
att- Acc 24.92 37.14
based PO Dice 71.16 72.21
Table 1: Experiment 1: System performance in %.
We used  2 on Accuracy and paired t-tests on Dice
to check for statistical significance. The best per-
formance is highlighted in boldface. It is statisti-
cally significantly different from all other systems
(Acc: p < 0.02, Dice: p < 0.0001).
LongestFirst/0-COST-ATT) includes the rela-
tion between the target and the third object
to the right, which was almost never included
in the human-produced references, leading to
many false-positives. The LongestFirst/K-PROP
system results in only 45 true-positives and
81 false-positives. It includes the attribute set
htg:color:yellow, tg:type:balli for Figure 1.
One of its relational descriptions (for Scene 5)
contains the set htg:size:small, tg:color:blue,tg:on top of:lmi.
The 50%-baseline system outperforms the
LongestFirst/0-COST systems, which illustrates
the utility of cost functions in combination with
a PO. It includes the attribute set htg:color:yellow,tg:type:balli for the scene in Figure 1. The best
performing system is the LongestFirst algorithm
with the attribute-based FREE-NAI?VE approach,
although this system produces no spatial relations.6 Experiment 2: Individual Variation
We now extend our methods to take into account
individual variation in the content selection for
referring expressions, and evaluate whether we
have better success at reproducing participants? re-
lational descriptions. Rather than using speaker
identity as an input parameter to the system (Sec-
tion 2.3), we automatically find groups of people
77
who behave similarly to each other, but signifi-
cantly different to speakers in the other groups.6.1 Evaluation Setup
We use k-means clustering to group the speak-
ers in the GRE3D3 Corpus based on the number
of times they used each attribute and the average
length of their descriptions. We tried values be-
tween 2 and 5 for k, but found that any value
above 2 resulted in two very large clusters accom-
panied by a number of extremely small clusters.
As these small clusters would not be suitable for
x-fold cross-validation, we proceed with two clus-
ters, one consisting of speakers preferring rela-
tively long descriptions that often contain spatial
relations (Cluster CL0, 16 speakers, 160 descrip-
tions), and one consisting of speakers preferring
short, non-relational descriptions (Cluster CL1, 47
speakers, 470 descriptions).
We train cost functions and POs separately for
the two clusters in order to capture the different
behaviour patterns they are based on. We use the
FREE-NAI?VE cost functions for this experiment,
which outperformed all others in Experiment 1.
We again use 10-fold cross-validation for the eval-
uation. In this experiment, we vary the maximum
length setting for the LongestFirst algorithm. In
Experiment 1, the maximum length for a referring
expression was set to 4 based on previous empiri-
cal findings. Here we additionally test setting it to
the rounded average length for each training fold.
On Cluster CL0 this average length is 6 in all folds,
on Cluster CL1 it is 3.6.2 Results
As shown in Table 2, the LongestFirst algorithm
performs best at generating human-like spatial re-
lations (Cluster CL0), with property-based param-
eters and a maximum description length deter-
mined by the training set. It produces the attribute
set hlm:type:cube, tg:on top of:lm, tg:type:ball,tgcolouryellow, lm:colour:redi for Figure 1. The
difference to the other systems is statistically sig-
nificant for both Accuracy ( 2>15, p<0.0001)
and Dice (t>13, p<0.0001). The attribute-based
parameters and the original GBA perform very
badly on this cluster. For participants who do
not tend to use spatial relations (Cluster CL1),
the maximum length setting has no influence,
but attribute-based parameters perform better than
property-based ones. The attribute-based Longest-
First systems also outperform the original GBA
CL0 CL1 avg
FN Acc 19.38 48.94 41.43
LongestFirst -PROP Dice 75.61 80.27 79.08
-max-av FN Acc 0.00 60.00 44.76
-ATT Dice 55.74 85.28 77.78
FN Acc 0.63 48.94 36.67
LongestFirst -PROP Dice 72.15 80.21 78.17
-max4 FN Acc 0.00 60.00 44.76
-ATT Dice 59.01 85.28 78.61
FN Acc 5.00 48.30 37.30
Original -PROP Dice 49.36 80.77 72.79
GBA FN Acc 5.00 50.85 39.21
-ATT Dice 49.36 81.58 73.40
Table 2: Experiment 2: Performance in % of the
LongestFirst and OriginalGraph algorithms on the
two speaker clusters and overall using the FREE-
NAI?VE (FN) approaches. We used  2 on Accu-
racy and paired t-tests on Dice to check for statis-
tical significance. The best performance in each
column and those that are statistically not signifi-
cantly different are highlighted in boldface.
on CL1, but interestingly none of the differences
are as large as on CL0. For the scene in Fig-
ure 1 they produce the attribute set htg:type:ball,tg:colour:yellowi.
The average results over both clusters (shown
in the last column Table 2) are not conclusive
as to which setting should be used overall, al-
though it is clear that the LongestFirst version is
preferable when evaluated by Dice. The differ-
ent result patterns on the two clusters suggest that
the different referential behaviour of the partici-
pants in the two clusters are ideally modeled us-
ing different parameters. In particular, it appears
that property-based costs are useful for replicat-
ing descriptions containing relations to other ob-
jects, while attribute-based costs are useful for
replicating shorter descriptions. The best over-
all performance, achieved by combining the best
performing systems on each cluster (LongestFirst-
max-av/FN-PROP on CL0 and LongestFirst/FN-
ATT with either maximum length setting on CL1),
lies at 49.68% Accuracy and 82.83% Dice. The
Dice score in this combined model is significantly
higher than the best achieved by LongestFirst-
max-av/FN-PROP and from the best Dice score
achieved on the unclustered data in Experiment 1
(t=8.2, p<0.0001). The difference in Accuracy is
not significant ( 2=1.2, p> 0.2).
To get an idea of how successful the new
LongestFirst approach is at replicating the use of
relations on the clustered data, we take a closer
look at the output of the best-performing systems
78
on the two clusters. On CL0, the cluster of partic-
ipants who produce longer descriptions contain-
ing more spatial relations, the best match to the
human data comes from LongestFirst-max-av/FN-
PROP. 147 of the 160 descriptions in this cluster
contain a relation, and the system includes the cor-
rect relation for all 147. It falsely also includes a
relation for the remaining 13 descriptions. This
shows that with the appropriate parameter settings
the LongestFirst algorithm is able to replicate hu-
man relational reference behaviour, but personal
speaker preferences are the main driving factor for
the human use of relations.
CL1, the cluster with shorter descriptions,
contains only 85 (18%) relational descriptions.
The best performing system on this cluster
(LongestFirst/FN-ATT) does not produce any rela-
tions. This is not surprising as the cost functions
and POs for this cluster are necessarily dominated
by the non-relational attributes used more regu-
larly. The cases in which relations are used stem
from participants who do not show a clear prefer-
ence for or against relations and would therefore
be hard to model in any system. With more data it
might be possible to group these participants into
a third cluster and find suitable parameter settings
for them. This would only be possible if their use
of relations is influenced by other factors available
to the algorithm, such as the spatial configuration
of the scene. Viethen and Dale?s (2008) analysis of
the GRE3D3 Corpus suggests that this is the case
at least to some extent.7 Conclusions and Future Work
We have evaluated the Graph-Based Algorithm for
REG (Krahmer et al, 2003) as well as a novel
search algorithm, LongestFirst, that functions on
the same graph-based representation, to assess
their ability to generate referring expressions that
contain spatial relations. We coupled the search
algorithms with a number of different approaches
to setting the cost functions and preference orders
that guide the search.
In Experiment 1, we found that ignoring the cost
function (our 0-cost approaches) is not helpful; but
the LongestFirst algorithm, which produces longer
descriptions, leads to more human-like output for
the visuospatial domain we evaluate on than the
original Graph-Based Algorithm or the Incremen-
tal Algorithm (Dale and Reiter, 1995). However,
in order for spatial relations to be included in a
human-like way, it was necessary to take into ac-
count speaker preferences. We modeled these in
Experiment 2 by clustering the participants who
had contributed to the evaluation corpus based on
their referential behaviour. By training separate
cost functions and preference orders for the dif-
ferent clusters, we enabled the LongestFirst al-
gorithm to correctly reproduce 100% of relations
used by people who regularly mentioned relations.
Our findings suggest that the graph-based rep-
resentation proposed by Krahmer et al (2003)
can be used to successfully generate relational de-
scriptions, however their original search algorithm
needs to be amended to allow more overspecifica-
tion. Furthermore, we have shown that variation
in the referential behaviour of individual speak-
ers has to be taken into account in order to suc-
cessfully model the use of relations in referring
expressions. We have proposed a clustering ap-
proach to advance this goal based directly on the
referring behaviour of speakers rather than speaker
identity. We have found that the best models use
fine-grained property-based parameters for speak-
ers who tend to use spatial relations, and coarser
attribute-based parameters for speakers who tend
to use shorter descriptions.
In future work, we hope to expand to more
complex domains, beyond the simple properties
available in the GRE3D3 Corpus. We also aim
to explore further graph-based representations and
search strategies, modeling non-spatial properties
as separate vertices, similar to the approach by
Croitoru and van Deemter (2007).8 Acknowledgements
Viethen and Krahmer received financial support
from The Netherlands Organization for Scientific
Research, (NWO, Vici grant 277-70-007), and
Mitchell received financial support from the Scot-
tish Informatics and Computer Science Alliance
(SICSA), which is gratefully acknowledged.References
Anja Arts, AlfonsMaes, Leonard Noordman, and Carel
Jansen. 2011. Overspecification in written instruc-
tion. Linguistics, 49(3):555?574.
Anja Belz and Albert Gatt. 2007. The Attribute Se-
lection for GRE Challenge: Overview and evalu-
ation results. In Proceedings of the Workshop onUsing Corpora for NLG: Language Generation and
79
Machine Translation (UCNLG+MT), pages 75?83,
Copenhagen, Denmark.
Bernd Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 207?210, Salt Fork OH, USA.
Bernd Bohnet. 2009. Generation of referring expres-
sion with an individual imprint. In Proceedings ofthe 12th European Workshop on Natural LanguageGeneration, pages 185?186, Athens, Greece.
Madalina Croitoru and Kees van Deemter. 2007. A
conceptual graph approach to the generation of re-
ferring expressions. In Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence, pages 2456?2461, Hyderabad, India.
Robert Dale and Nicholas Haddock. 1991. Gener-
ating referring expressions involving relations. InProceedings of the 5th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 161?166, Berlin, Germany.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the As-sociation for Computational Linguistics, pages 68?
75, Vancouver BC, Canada.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008a. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 211?214, Salt Fork OH, USA.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008b. Referring expression generation
using speaker-based attribute selection and train-
able realization. In Twelfth Conference on Com-putational Natural Language Learning, Manchester,
UK.
Paul E. Engelhardt, Karl D. Bailey, and Fernanda Fer-
reira. 2006. Do speakers and listeners observe the
gricean maxim of quantity? Journal of Memory andLanguage, 54:554?573.
Albert Gatt and Anja Belz. 2008. Attribute selection
for referring expression generation: New algorithms
and evaluation methods. In Proceedings of the5th International Conference on Natural LanguageGeneration, pages 50?58, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation
results. In Proceedings of the 5th InternationalConference on Natural Language Generation, pages
198?206, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation, pages
174?182, Athens, Greece.
Pamela W. Jordan and Marilyn Walker. 2005. Learn-
ing content selection rules for generating object de-
scriptions in dialogue. Journal of Artificial Intelli-gence Research, 24:157?194.
John Kelleher and Geert-Jan Kruijff. 2006. Incre-
mental generation of spatial referring expressions in
situated dialog. In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 1041?1048, Syd-
ney, Australia.
Ruud Koolen and Emiel Krahmer. 2010. The D-
TUNA Corpus: A dutch dataset for the evaluation
of referring expression generation algorithms. InProceedings of the 7th International Conference onLanguage Resources and Evaluation, Valetta, Malta.
Ruud Koolen, Emiel Krahmer, and Marie?t Theune.
2012. Learning preferences for referring expression
generation: Effects of domain, language and algo-
rithm. In Proceedings of the 7th International Nat-ural Language Generation Conference, pages 3?11,
Starved Rock, IL, USA.
Emiel Krahmer and Marie?t Theune. 2002. Effi-
cient context-sensitive generation of referring ex-
pressions. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing: Reference and Pre-supposition in Language Generation and Interpre-tation, pages 223?264. CSLI Publications, Stanford
CA, USA.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Emiel Krahmer and Ielka van der Sluis. 2003. A new
model for generating multimodal referring expres-
sions. In Proceedings of the 9th European Workshopon Natural Language Generation, pages 47?57, Bu-
dapest, Hungary.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011a. Semi-supervised modeling for prenominal
modifier ordering. In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages
236?241, Portland OR, USA.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2011b. Applying machine learning to the
choice of size modifiers. In Proceedings of the 2ndWorkshop on the Production of Referring Expres-sions, Boston MA, USA.
80
Thomas Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89?110.
Susan Sonnenschein. 1985. The development of ref-
erential communication skills: Some situations in
which speakers give redundant messages. Journalof Psycholinguistic Research, 14(5):489?508.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter - how
much data is required to train a REG algorithm? InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 660?664, Portland OR,
USA.
Kees van Deemter and Emiel Krahmer. 2007. Graphs
and Booleans: On the generation of referring ex-
pressions. In Harry C. Bunt and Reinhard Muskens,
editors, Computing Meaning, volume 3, pages 397?
422. Kluwer, Dordrecht, The Netherlands.
Kees van Deemter, Albert Gatt, Ielka van der Sluis,
and Richard Power. 2012. Generation of referring
expressions: Assessing the incremental algorithm.Cognitive Science, 36(5):799?836.
Ielka van der Sluis. 2005. Multimodal Reference, Stud-ies in Automatic Generation of Multimodal Refer-ring Expressions. Ph.D. thesis, Tilburg University,
The Netherlands.
Jette Viethen and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the 4th InternationalConference on Natural Language Generation, pages
63?70, Sydney, Australia.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-ceedings of the 5th International Conference on Nat-ural Language Generation, pages 59?67, Salt Fork
OH, USA.
Jette Viethen and Robert Dale. 2009. Referring ex-
pression generation: What can we learn from human
data? In Proceedings of the 2009 Workshop on Pro-duction of Referring Expressions: Bridging the GapBetween Computational and Empirical Approachesto Reference, Amsterdam, The Netherlands.
Jette Viethen and Robert Dale. 2010. Speaker-
dependent variation in content selection for refer-
ring expression generation. In Proceedings of the8th Australasian Language Technology Workshop,
pages 81?89, Melbourne, Australia.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t
Theune, and Pascal Touset. 2008. Controlling re-
dundancy in referring expressions. In Proceedingsof the 6th International Conference on LanguageResources and Evaluation, Marrakech, Morocco.
81
