Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1081?1091,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Harnessing WordNet Senses for Supervised Sentiment Classification
Balamurali A R1,2 Aditya Joshi2 Pushpak Bhattacharyya2
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
Mumbai, India - 400076
{balamurali,adityaj,pb}@cse.iitb.ac.in
Abstract
Traditional approaches to sentiment classifica-
tion rely on lexical features, syntax-based fea-
tures or a combination of the two. We pro-
pose semantic features using word senses for
a supervised document-level sentiment classi-
fier. To highlight the benefit of sense-based
features, we compare word-based representa-
tion of documents with a sense-based repre-
sentation where WordNet senses of the words
are used as features. In addition, we highlight
the benefit of senses by presenting a part-of-
speech-wise effect on sentiment classification.
Finally, we show that even if a WSD engine
disambiguates between a limited set of words
in a document, a sentiment classifier still per-
forms better than what it does in absence of
sense annotation. Since word senses used as
features show promise, we also examine the
possibility of using similarity metrics defined
on WordNet to address the problem of not
finding a sense in the training corpus. We per-
form experiments using three popular similar-
ity metrics to mitigate the effect of unknown
synsets in a test corpus by replacing them with
similar synsets from the training corpus. The
results show promising improvement with re-
spect to the baseline.
1 Introduction
Sentiment Analysis (SA) is the task of prediction of
opinion in text. Sentiment classification deals with
tagging text as positive, negative or neutral from the
perspective of the speaker/writer with respect to a
topic. In this work, we follow the definition of Pang
et al (2002) & Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Traditional supervised approaches for SA have
explored lexeme and syntax-level units as features.
Approaches using lexeme-based features use bag-
of-words (Pang and Lee, 2008) or identify the
roles of different parts-of-speech (POS) like adjec-
tives (Pang et al, 2002; Whitelaw et al, 2005).
Approaches using syntax-based features construct
parse trees (Matsumoto et al, 2005) or use text
parsers to model valence shifters (Kennedy and
Inkpen, 2006).
Our work explores incorporation of semantics
in a supervised sentiment classifier. We use the
synsets in Wordnet as the feature space to represent
word senses. Thus, a document consisting of
words gets mapped to a document consisting of
corresponding word senses. Harnessing WordNet
senses as features helps us address two issues:
1. Impact of WordNet sense-based features on the
performance of supervised SA
2. Use of WordNet similarity metrics to solve the
problem of features unseen in the training cor-
pus
The first points deals with evaluating sense-based
features against word-based features. The second is-
sue that we address is in fact an opportunity to im-
prove the performance of SA that opens up because
of the choice of sense space. Since sense-based
features prove to generate superior sentiment clas-
sifiers, we get an opportunity to mitigate unknown
1081
synsets in the test corpus by replacing them with
known synsets in the training corpus. Note that such
replacement is not possible if word-based represen-
tation were used as it is not feasible to make such a
large number of similarity comparisons.
We use the corpus by Ye et al (2009) that con-
sists of travel domain reviews marked as positive or
negative at the document level. Our experiments on
studying the impact of Wordnet sense-based features
deal with variants of this corpus manually or auto-
matically annotated with senses. Besides showing
the overall impact, we perform a POS-wise analysis
of the benefit to SA. In addition, we compare the ef-
fect of varying training samples on a sentiment clas-
sifier developed using word based features and sense
based features. Through empirical evidence, we also
show that disambiguating some words in a docu-
ment also provides a better accuracy as compared
to not disambiguating any words. These four sets of
experiments highlight our hypothesis that WordNet
senses are better features as compared to words.
Wordnet sense-based space allows us to mitigate
unknown features in the test corpus. Our synset re-
placement algorithm uses Wordnet similarity-based
metrics which replace an unknown synset in the test
corpus with the closest approximation in the training
corpus. Our results show that such a replacement
benefits the performance of SA.
The roadmap for the rest of the paper is as fol-
lows: Existing related work in SA and the differ-
entiating aspects of our work are explained in sec-
tion 2 Section 3 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
in section 4. Our experiments have been described
in section 5. In section 6, we present our results
and related discussions. Section 7 analyzes some of
the causes for erroneous classification. Finally, sec-
tion 8 concludes the paper and points to future work.
2 Related Work
This work studies the benefit of a word sense-based
feature space to supervised sentiment classification.
However, a word sense-based feature space is feasi-
ble subject to verification of the hypothesis that sen-
timent and word senses are related. Towards this,
Wiebe and Mihalcea (2006) conduct a study on hu-
man annotation of 354 words senses with polarity
and report a high inter-annotator agreement. The
work in sentiment analysis using sense-based fea-
tures, including ours, assumes this hypothesis that
sense decides the sentiment.
The novelty of our work lies in the following.
Firstly our approach is distinctly. Akkaya et al
(2009) and Martn-Wanton et al (2010) report per-
formance of rule-based sentiment classification us-
ing word senses. Instead of a rule-based implemen-
tation, We used supervised learning. The supervised
nature of our approach renders lexical resources un-
necessary as used in Martn-Wanton et al (2010).
Rentoumi et al (2009) suggest using word senses
to detect sentence level polarity of news headlines.
The authors use graph similarity to detect polarity of
senses. To predict sentence level polarity, a HMM
is trained on word sense and POS as the observa-
tion. The authors report that word senses partic-
ularly help understanding metaphors in these sen-
tences. Our work differs in terms of the corpus and
document sizes in addition to generating a general
purpose classifier.
Another supervised approach of creating an emo-
tional intensity classifier using concepts as features
has been reported by Carrillo de Albornoz et al
(2010). This work is different based on the feature
space used. The concepts used for the purpose are
limited to affective classes. This restricts the size of
the feature space to a limited set of labels. As op-
posed to this, we construct feature vectors that map
to a larger sense-based space. In order to do so, we
use synset offsets as representation of sense-based
features.
Akkaya et al (2009), Martn-Wanton et al (2010)
and Carrillo de Albornoz et al (2010) perform sen-
timent classification of individual sentences. How-
ever, we consider a document as a unit of sentiment
classification i.e. our goal is to predict a document
on the whole as positive or negative. This is different
from Pang and Lee (2004) which suggests that sen-
timent is associated only with subjective content. A
document in its entirety is represented using sense-
based features in our experiments. Carrillo de Al-
bornoz et al (2010) suggests expansion using Word-
Net relations which we also follow. This is a benefit
that can be achieved only in a sense-based space.
1082
3 Features based on WordNet Senses
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This pa-
per refers to synset offset as synset identifiers or sim-
ply, senses.
This section first gives the motivation for using
word senses and then, describes the approaches that
we use for our experiments.
3.1 Motivation
Consider the following sentences as the first sce-
nario.
1. ?Her face fell when she heard that she had
been fired.?
2. ?The fruit fell from the tree.?
The word ?fell? occurs in different senses in the
two sentences. In the first sentence, ?fell? has the
meaning of ?assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ?descend in free fall under the influence
of gravity?. A user will infer the negative polarity of
the first sentence from the negative sense of ?fell? in
it while the user will state that the second sentence
does not carry any sentiment. This implies that there
is at least one sense of the word ?fell? that carries
sentiment and at least one that does not.
In the second scenario, consider the following ex-
amples.
1. ?The snake bite proved to be deadly for the
young boy.?
2. ?Shane Warne is a deadly spinner.?
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
1. ?He speaks a vulgar language.?
2. ?Now that?s real crude behavior!?
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ?conspic-
uously and tastelessly indecent?. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
3.2 Sense versus Lexeme-based Feature
Representation
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on two
subsets of the corpus, the details of which are given
in Section 5.1. This is done to determine the ideal
case scenario- the skyline performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
1. Word senses that have been manually annotated
(M)
2. Word senses that have been annotated by an au-
tomatic WSD (I)
1083
3. Manually annotated word senses and words
(both separately as features) (Words +
Sense(M))
4. Automatically annotated word senses and
words (both separately as features) (Words +
Sense(I))
Our first set of experiments compares the four
feature representations to find the feature represen-
tation with which sentiment classification gives the
best performance. W+S(M) and W+S(I) are used to
overcome non-coverage of WordNet for some noun
synsets. In addition to this, we also present a part-
of-speech-wise analysis of benefit to SA as well as
effect of varying the training samples on sentiment
classification accuracy.
3.3 Partial disambiguation as opposed to no
disambiguation
The state-of-the-art automatic WSD engine that we
use performs (approximately) with 70% accuracy on
tourism domain (Khapra et al, 2010). This means
that the performance of SA depends on the perfor-
mance of WSD which is not very high in case of the
engine we use.
A partially disambiguated document is a docu-
ment which does not contain senses of all words.
Our hypothesis is that disambiguation of even few
words in a document can give better results than
no disambiguation. To verify this, we create differ-
ent variants of the corpus by disambiguating words
which have candidate senses within a threshold. For
example, a partially disambiguated variant of the
corpus with threshold 3 for candidate senses is cre-
ated by disambiguating words which have a maxi-
mum of three candidate senses. These synsets are
then used as features for classification along with
lexeme based features. We conduct multiple experi-
ments using this approach by varying the number of
candidate senses.
4 Advantage of senses: Similarity Metrics
and Unknown Synsets
4.1 Synset Replacement Algorithm
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
1. ? In the night, it is a lovely city and... ?
2. ? The city has many beautiful hot spots for hon-
eymooners. ?
The synset of ?beautiful? is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in Algo-
rithm 1. The algorithm follows from the fact that the
similarity value for a synset with itself is maximum.
4.2 Similarity metrics used
We conduct different runs of the replacement
algorithm using three similarity metrics, namely
LIN?s similarity metric, Lesk similarity metric and
Leacock and Chodorow (LCH) similarity metric.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
simLIN (A,B) =
2? logPr(lso(A,B))
logPr(A) + logPr(B) (1)
1084
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value > temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
C = temp concept ;
replace synset corpus(C,X);
end
Return X ;
Algorithm 1: Synset replacement using similarity
metric
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
simLCH(A,B) = ? log
( len(A,B)
2D
)
(2)
5 Experimentation
We describe the variants of the corpus generated and
the experiments in this section.
5.1 Data Preparation
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al (2009). The inter-annotation agree-
ment for a subset of the corpus showed 91% sense
overlap. The manually annotated corpus consists of
34508 words with 6004 synsets.
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjective
POS based synsets can be detrimental to classifica-
tion since adjectives are known to express direct sen-
timent (Pang et al, 2002). Hence, in the context of
sentiment classification, disambiguation of adjective
synsets is more critical as compared to disambigua-
tion of noun synsets.
5.2 Experimental setup
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al, 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are not
stemmed since stemming is known to be detrimen-
tal to sentiment classification (Leopold and Kinder-
mann, 2002). To conduct the experiments based on
1C=0.0,=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
1085
Feature Representations Accuracy(%) PF NF PP NP PR NR
Words (Baseline) 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-
Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
the synset representation, words in the corpus are an-
notated with synset identifiers along with POS cat-
egory identifiers. For automatic sense disambigua-
tion, we used the trained IWSD engine from Khapra
et al (2010). These synset identifiers along with
POS category identifiers are then used as features.
For replacement using semantic similarity measures,
we used WordNet::Similarity 2.05 package by Ped-
ersen et al (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
score.
6 Results and Discussions
6.1 Comparison of various feature
representations
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Baseline).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
While reported results suggest that it is more diffi-
cult to detect negative sentiment than positive senti-
ment (Gindl and Liegl, 2008), our results show that
negative recall increases by around 8% in case of
sense-based representation of documents.
The combined model of words and manually an-
notated senses (Words + Senses (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
1086
Sens
e
81.2
4
78.3
0
66.1
4
73 .
70.0
0
80.0
0
90.0
0
50.0
0
60.0
0
racy(%)
20.0
0
30.0
0
40.0
0
Accu
0.0010.0
0
20.0
0
Adve
rb?
Verb
? PO
Wor
ds 74.99
66.8
3
.78
71.8
1
80.0
3
Nou
n?
Adje
ctive
OS?c
ateg
ory
Figure 1: POS-wise statistics of manually annotated se-
mantic space
6.2 POS-wise analysis
For each POS, we compare the performance of two
models:
? Model trained on words of only that POS
? Model trained on word senses of only that POS
Figure 1 shows the parts-of-speech-wise classifica-
tion accuracy of sentiment classification for senses
(manual) and words. In the lexeme space, adjectives
directly impact the classification performance. But it
can be seen that disambiguation of adverb and verb
synsets impact the performance of SA higher than
disambiguation of nouns and adjectives.
While it is believed that adjectives carry direct
sentiments, our results suggest that using adjectives
alone as features may not improve the accuracy. The
results prove that sentiment may be subtle at times
and not expressed directly through adjectives.
As manual sense annotation is an effort and cost
intensive process, the parts-of-speech-wise results
suggest improvements expected from an automatic
WSD engine so that it can aid sentiment classifica-
tion. Table 1 suggests that the WSD engine works
better for noun synsets compared to adjective and
adverb synsets. While this is expected in a typical
WSD setup, it is the adverbs and verbs that are more
important for detecting sentiment in semantics space
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
than nouns. The future WSD systems will have to
show an improvement in their accuracy with respect
to adverb and verb synsets.
Sense Words
POS Category PF NF PF NF
Adverb 79.65 80.45 70.25 73.68
Verb 75.50 79.28 62.23 63.12
Noun 73.39 75.40 69.77 72.55
Adjective 63.11 65.03 78.29 79.20
Table 3: POS-wise F-score for sense (M) and Words;PF-
Positive F-score(%), NF- Negative F-score (%)
Table 3 shows the positive and negative F-score
statistics with respect to different POS. Detection
of negative reviews using lexeme space is difficult.
POS-wise statistics also suggest the same. It should
be noted that adverb and verb synsets play an im-
portant role in negative class detection. Thus, an au-
tomatic WSD engine should give importance to the
correct disambiguation of these POS categories.
6.3 Effect of size of training corpus
#Training
Documents
W M I W+S(M) W+S(I)
100 76.5 87 79.5 82.5 79.5
200 81.5 88.5 82 90 84
300 79.5 92 81 89.5 82
400 82 90.5 81 94 85.5
500 83.5 91 85 96 82.5
Table 4: Accuracy (%) with respect to number of training
documents; W: Words, M: Manual Annotation, I: IWSD-
based sense annotation, W+S(M): Word+Senses (Manual
annotation), W+S(I): Word+Senses(IWSD-based sense
annotation)
From table 2, the benefit of sense disambigua-
tion to sentiment prediction is evident. In addition,
Table 4 shows variation of classification accuracy
with respect to different number of training sam-
ples based on different approaches of annotation ex-
plained in previous sections. The results are based
on a blind set of 90 test samples from both the po-
larity labels 4.
4No cross validation is performed for this experiment
1087
Compared to lexeme-based features, manually an-
notated sense based features give better performance
with lower number of training samples. IWSD is
also better than lexeme-based features. A SA sys-
tem trained on 100 training samples using manually
annotated senses gives an accuracy of 87%. Word-
based features never achieve this accuracy. An
IWSD-based system requires lesser samples when
compared to lexeme space for an equivalent accu-
racy. Note that model based on words + senses(M)
features achieve an accuracy of 96% on this test set.
This implies that the synset space, in addition
to benefit to sentiment prediction in general, re-
quires lesser number of training samples in order to
achieve the accuracy that lexeme space can achieve
with a larger number of samples.
6.4 Effect of Partial disambiguation
Figure 2 shows the accuracy, positive F-score and
negative F-score with respect to different thresholds
of candidate senses for partially disambiguated doc-
uments as described in Section 3.3. We compare the
performance of these documents with word-based
features (B) and sense-based features based on man-
ually (M) or automatically obtained senses (I). Note
that Sense (I) and Sense (M) correspond to com-
pletely disambiguated documents.
In case of partial disambiguation using manual
annotation, disambiguating words with less than
three candidate senses performs better than others.
For partial disambiguation that relies on an auto-
matic WSD engine, a comparable performance to
full disambiguation can be obtained by disambiguat-
ing words which have a maximum of four candidate
senses.
As expected, completely disambiguated docu-
ments provide the best F-score and accuracy fig-
ures5. However, a performance comparable to com-
plete disambiguation can be attained by disam-
biguating selective words.
Our results show that even if highly ambiguous
(in terms of senses) words are not disambiguated by
a WSD engine, the performance of sentiment classi-
fication improves.
5All results are statistically significant with respect to base-
line
6se
nse
s(M
)
6?se
nse
s?(I
)Ne
gat
ive
?Fsc
ore
Pos
5?se
nse
s?(M
)
5?se
nse
s?(I
)
6?se
nse
s?(M
)
3?se
nse
s?(I
)
4?se
nse
s?(M
)
4?se
nse
s?(I
)
2?se
nse
s?(M
)
2?se
nse
s?(I
)
3?se
nse
s?(M
)
Wo
rds
?(B)
Sen
se(
M)
Sen
se(
I) 81
.00
82.
00
83.
00
84.
00
85.
00
Fsc
ore
/itiv
e?F
sco
re
Acc
ura
cy
86.
00
87.
00
88.
00
89.
00
90.
00
91.
00
/Ac
cur
acy
?(%
)
Figure 2: Partial disambiguation statistics: Accu-
racy,Positive F-score, Negative F-score variation with re-
spect to sense disambiguation difficult level is shown.
Words(B): baseline system
6.5 Synset replacement using similarity metrics
Table 5 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 4. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach6. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
6Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
?=0.05
1088
Feature Representation Similarity
Metric
Accuracy PF NF PP NP PR NR
Words (Baseline) NA 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Words + Sense(M) NA 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Words + Sense(I) NA 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Words + Sense (M) LCH 90.60 90.20 90.85 92.85 88.61 87.70 93.21
Words + Sense(M) LIN 90.70 90.26 90.97 93.17 88.50 87.53 93.57
Words + Sense (M) Lesk 91.12 90.70 91.38 93.55 88.97 88.03 93.92
Words + Sense (I) LCH 85.66 85.85 85.52 85.67 85.76 86.02 85.28
Words + Sense(I) LIN 86.16 86.37 86.00 86.06 86.40 86.69 85.61
Words + Sense (I) Lesk 86.25 86.41 86.10 86.31 86.26 86.52 85.95
Table 5: Similarity Metric Analysis using different similarity metrics with synsets and a combinations of synset and
words;PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-
Positive Recall (%), NR-Negative Recall (%)
Top information
content features
(in %)
IWSD
synset #
Manual
synsets #
Match
synset #
Match
Synsets (%)
Unmatched
Synset(%)
10 601 722 288 39.89 60.11
20 1199 1443 650 45.05 54.95
30 1795 2165 1005 46.42 53.58
40 2396 2889 1375 47.59 52.41
50 2997 3613 1730 47.88 52.12
Table 6: Comparison of top information gain-based features of manually annotated corpora and automatically anno-
tated corpora
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated
as LIN or LCH.
Thus, we observe a marginal improvement by us-
ing similarity-based metrics for WordNet. A good
metric which covers all POS categories can provide
substantial improvement in the classification accu-
racy.
7 Error Analysis
For sentiment classification based on semantic
space, we classify the errors into four categories.
The examples quoted are from manual evaluation of
the results.
1. Effect of low disambiguation accuracy of IWSD
engine: SA using automatic sense annotation
depends on the annotation system used. To as-
sess the impact of IWSD system on sentiment
classification, we compare the feature set based
on manually annotated senses with the feature
set based on automatically annotated senses.
We compare the most informative features of
the two classifiers. Table 6 shows the number
of top informative features (synset) selected as
the percentage of total synset features present
when the semantic representation of documen-
tation is used. The matched synset column rep-
resents the number of IWSD synsets that match
1089
with manually annotated synsets.
The number of top performing features is more
in case of manually annotated synsets. This
can be attributed to the total number of synsets
tagged in the two variant of the corpus. The re-
duction in the performance of SA for automati-
cally annotated senses is because of the number
of unmatched synsets.
Thus, although the accuracy of IWSD is cur-
rently 70%, the table indicates that IWSD can
match the performance of manually annotated
senses for SA if IWSD is able to tag correctly
those top information content synsets. This as-
pect needs to be investigated further.
2. Negation Handling: For the purpose of this
work, we concentrate on words as units for sen-
timent determination. Syntax and its contri-
bution in understanding sentiment is neglected
and hence, positive documents which con-
tain negations are wrongly classified as nega-
tive. Negation may be direct as in the excerpt
?....what is there not to like about Vegas.? or
may be double as in the excerpt?...that aren?t
insecure?.
3. Interjections and WordNet coverage: Recent
informal words are not covered in WordNet and
hence, do not get disambiguated. The same
is the case for interjections like ?wow?,?duh?
which sometimes carry direct sentiment. Lex-
ical resources which include them can be used
to incorporate information about these lexical
units.
4. Document Specificity: The assumption under-
lying our analysis is that a document contains
description of only one topic. However, re-
views are generic in nature and tend to express
contrasting sentiment about sub-topics . For
example, a travel review about Paris can talk
about restaurants in Paris, traffic in Paris, pub-
lic behaviour, etc. with opposing sentiments.
Assigning an overall sentiment to a document
is subjective in such cases.
8 Conclusion & Future Work
This work presents an empirical benefit of WSD to
sentiment analysis. The study shows that supervised
sentiment classifier modeled on wordNet senses per-
form better than word-based features. We show how
the performance impact differs for different auto-
matic and manual techniques, parts-of-speech, dif-
ferent training sample size and different levels of
disambiguation. In addition, we also show the bene-
fit of using WordNet based similarity metrics for re-
placing unknown features in the test set. Our results
support the fact that not only does sense space im-
prove the performance of a sentiment classification
system, but also opens opportunities for improve-
ment using better similarity metrics.
Incorporation of syntactical information along
with semantics can be an interesting area of work.
More sophisticated features which include the two
need to be explored. Another line of work is in the
context of cross-lingual sentiment analysis. Current
solutions are based on machine translation which is
very resource-intensive. Using a bi-lingual dictio-
nary which maps WordNet across languages, such a
machine translation sub-system can be avoided.
Acknowledgment
We thank Jaya Saraswati and Rajita Shukla from
CFILT Lab, IIT Bombay for annotating the dataset
used for this work. We also thank Mitesh Khapra
and Salil Joshi, IIT Bombay for providing us with
the IWSD engine for the required experiments.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ?09, pages 190?199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing?02, pages 136?145,
London, UK.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131?142.
1090
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91?112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35?43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110?
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC?10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and Jo?rg Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423?
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296?304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing?10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD?05,, Lecture Notes in Computer Science,
pages 301?311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL?04, pages
271?278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL?04, pages 38?41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP?09, pages 370?375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL?02, pages 417?424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ?05, pages 625?631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL?06, pages
1065?1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 ? 6535.
1091
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 127?132,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
C-Feel-It: A Sentiment Analyzer for Micro-blogs
Aditya Joshi1 Balamurali A R2 Pushpak Bhattacharyya1 Rajat Mohanty 3
1Dept. of Computer Science and Engineering, IIT Bombay, Mumbai
2 IITB-Monash Research Academy, IIT Bombay, Mumbai
3 AOL India (R&D), Bangalore
India
{adityaj,balamurali,pb}@cse.iitb.ac.in r.mohanty@teamaol.com
Abstract
Social networking and micro-blogging sites
are stores of opinion-bearing content created
by human users. We describe C-Feel-It, a sys-
tem which can tap opinion content in posts
(called tweets) from the micro-blogging web-
site, Twitter. This web-based system catego-
rizes tweets pertaining to a search string as
positive, negative or objective and gives an ag-
gregate sentiment score that represents a senti-
ment snapshot for a search string. We present
a qualitative evaluation of this system based
on a human-annotated tweet corpus.
1 Introduction
A major contribution of Web 2.0 is the explosive rise
of user-generated content. The content has been a
by-product of a class of Internet-based applications
that allow users to interact with each other on the
web. These applications which are highly accessible
and scalable represent a class of media called social
media. Some of the currently popular social media
sites are Facebook (www.facebook.com), Myspace
(www.myspace.com), Twitter (www.Twitter.com)
etc. User-generated content on the social media rep-
resents the views of the users and hence, may be
opinion-bearing. Sales and marketing arms of busi-
ness organizations can leverage on this information
to know more about their customer base. In addi-
tion, prospective customers of a product/service can
get to know what other users have to say about the
product/service and make an informed decision.
C-Feel-It is a web-based system which
predicts sentiment in micro-blogs on
Twitter (called tweets). (Screencast at:
http://www.youtube.com/user/cfeelit/ ) C-Feel-
It uses a rule-based system to classify tweets as
positive, negative or objective using inputs from
four sentiment-based knowledge repositories. A
weighted-majority voting principle is used to predict
sentiment of a tweet. An overall sentiment score for
the search string is assigned based on the results of
predictions for the tweets fetched. This score which
is represented as a percentage value gives a live
snapshot of the sentiment of users about the topic.
The rest of the paper is organized as follows: Sec-
tion 2 gives background study of Twitter and related
work in the context of sentiment analysis for Twitter.
The system architecture is explained in section 3. A
qualitative evaluation of our system based on anno-
tated data is described in section 4. Section 5 sum-
marizes the paper and points to future work.
2 Background study
Twitter is a micro-blogging website and ranks sec-
ond among the present social media websites (Prelo-
vac, 2010). A micro-blog allows users to exchange
small elements of content such as short sentences,
individual pages, or video links (Kaplan and Haen-
lein, 2010). More about Twitter can be found here 1.
In Twitter, a micro-blogging post is called a
tweet which can be upto 140 characters in length.
Since the length is constrained, the language used in
tweets is highly unstructured. Misspellings, slangs,
contractions and abbreviations are commonly used
in tweets. The following example highlights these
problems in a typical tweet:
?Big brother doing sian massey no favours.
Let her ref. She?s good at it you know#lifesapitch?
We choose Twitter as the data source because
of the sheer quantity of data generated and its fast
reachability across masses. Additionally, Twitter al-
lows information to flow freely and instantaneously
unlike FaceBook or MySpace. These aspects of
1http://support.twitter.com/groups/31-twitter-basics
127
Twitter makes it a source for getting a live snapshot
of the things happenings on the web.
In the context of sentiment classification of tweets
Alec et al (2009a) describes a distant supervision-
based approach for sentiment classification. The
training data for this purpose is created following a
semi-supervised approach that exploits emoticons in
tweets. In their successive work, Alec et al (2009b)
additionally use hashtags in tweets to create train-
ing data. Topic-dependent clustering is performed
on this data and classifiers corresponding to each are
modeled. This approach is found to perform better
than a single classifier alone.
We believe that the models trained on data cre-
ated using semi-supervised approaches cannot clas-
sify all variants of tweets. Hence, we follow a rule-
based approach for predicting sentiment of a tweet.
An approach like ours provides a generic way of
solving sentiment classification problems in micro-
blogs.
3 Architecture
keywor
d (s)
Tweet fetcher
Tweet Sentime
nt 
Predicto
r
C-Feel-I
t
Sentime
nt score
Tweet Sentime
nt 
Collabo
rator
score
Figure 1: Overall Architecture
The overall architecture of C-Feel-It is shown in
Figure 1. C-Feel-It is divided into three parts: Tweet
Fetcher, Tweet Sentiment Predictor and Tweet
Sentiment Collaborator. All predictions are pos-
itive, negative or objective/neutral. C-Feel-It offers
two implementations of a rule-based sentiment pre-
diction system. We refer to them as version 1 and
2. The two versions differ in the Tweet Sentiment
Predictor module. This section describes different
modules of C-Feel-It and is organized as follows. In
subsections 3.1, 3.2 & 3.3, we describe the three
functional blocks of C-FeeL-It. In subsection 3.4,
we explain how four lexical resources are mapped
to the desired output labels. Finally, subsection 3.5
gives implementation details of C-Feel-It.
Input to C-Feel-It is a search string and a version
number. The versions are described in detail in sub-
section 3.2.
Output given by C-Feel-It is two-level: tweet-wise
prediction and overall prediction. For tweet-wise
prediction, sentiment prediction by each of the re-
sources is returned. On the other hand, overall pre-
diction combines sentiment from all tweets to return
the percentage of positive, negative and objective
content retrieved for the search string.
3.1 Tweet Fetcher
Tweet fetcher obtains tweets pertaining to a search
string entered by a user. To do so, we use live feeds
from Twitter using an API 2. The parameters passed
to the API ensure that system receives the latest 50
tweets about the keyword in English. This API re-
turns results in XML format which we parse using a
Java SAX parser.
3.2 Tweet Sentiment Predictor
Tweet sentiment predictor predicts sentiment for
a single tweet. The architecture of Tweet Senti-
ment Predictor is shown in Figure 2 and can be di-
vided into three fundamental blocks: Preprocessor,
Emoticon-based Sentiment Predictor, Lexicon-based
Sentiment Predictor (refer Figure 3 & 4). The first
two blocks are same for both the versions of C-Feel-
It. The two versions differ in the working of the
Lexicon-based Sentiment Predictor.
Preprocessor
The noisy nature of tweets is a classical challenge
that any system working on tweets needs to en-
counter. Preprocessor deals with obtaining clean
tweets. We do not deploy any spelling correction
module. However, the preprocessor handles exten-
sions and contractions found in tweets as follows.
Handling extensions: Extensions like ?besssssst?
are common in tweets. However, to look up re-
sources, it is essential that these words are normal-
ized to their dictionary equivalent. We replace con-
secutive occurrences of the same letter (if more than
2http://search.Twitter.com/search.atom
128
Lexicon
-based sentime
nt 
predicto
r
Word e
xtensio
n
handler
Tweet
if no em
oticonS
entimen
t 
predicti
on
Chat lin
go 
normali
zation
Emotico
n-based
 
sentime
nt 
predicto
r
Tweet P
reproce
ssing
Sentime
nt 
predicti
on
Figure 2: Tweet Sentiment Predictor: Version 1 and 2
three occurrences of the same letter) with a single
letter and replace the word.
An important issue here is that extensions are in fact
strong indicators of sentiment. Hence, we replace an
extended word by two occurences of the contracted
word. This gives a higher weight to the extended
word and retains its contribution to the sentiment of
the tweet.
Chat lingo normalization: Words used in
chat/Internet language that are common in tweets are
not present in the lexical resources. We use a dictio-
nary downloaded from http://chat.reichards.net/ . A
chat word is replaced by its dictionary equivalent.
Emoticon-based Sentiment Predictor
Emoticons are visual representations of emo-
tions frequently used in the user-generated con-
tent on the Internet. We observe that in most
cases, emoticons pinpoint the sentiment of a
tweet. We use an emoticon mapping from
http://chat.reichards.net/smiley.shtml. An emoticon
is mapped to an output label: positive or negative. A
tweet containing one of these emoticons that can be
mapped to the desired output labels directly. While
we understand that this heuristic does not work in
case of sarcastic tweets, it does provide a benefit in
most cases.
Lexicon-based Sentiment Predictor
For a tweet, the Lexicon-based Sentiment Predic-
tor gives a prediction each for four resources. In
addition, it returns one prediction which combines
the four predictions by weighting them on the ba-
Tweet
Lexical Resourc
e
Get 
sentime
nt pred
iction
For all w
ords Return 
output 
label 
corresp
onding 
to 
majorit
y of wo
rds
Sentime
nt 
Predicti
on
Figure 3: Lexicon-based Sentiment Predictor: C-Feel-It
Version 1
sis of their accuracies. We remove stop words 3
from the tweet and stem the words using Lovins
stemmer (Lovins, 1968). Negation in tweets is han-
dled by inverting sentiment of words after a negat-
ing word. The words ?no?, ?never?, ?not? are consid-
ered negating words and a context window of three
words after a negative words is considered for in-
version. The two versions of C-Feel-It vary in their
Lexicon-based Sentiment Predictor. Figure 3 shows
the Lexicon-based Sentiment Predictor for version
1. For each word in the tweet, it gets the predic-
tion from a lexical resource. We use the intuition
that a positive tweet has positive words outnumber-
ing other words, a negative tweet has negative words
outnumbering other words and an objective tweet
has objective words outnumbering other words.
Figure 4 shows the Lexicon-based Sentiment Predic-
tor for version 2. As opposed to the earlier version,
version 2 gets prediction from the lexical resource
for some words in the tweet. This is because certain
parts-of-speech have been found to be better indi-
cators of sentiment (Pang and Lee, 2004). A tweet
is annotated with parts-of-speech tags and the POS
bi-tags (i.e. a pattern of two consecutive POS) are
marked. The words corresponding to a set of opti-
mal POS bi-tags are retained and only these words
used for lookup. The prediction for a tweet uses
majority vote-based approach as for version 1. The
optimal POS bi-tags have been derived experimen-
tally by using top 10% features on information gain-
based-pruning classifier on polarity dataset by (Pang
and Lee, 2005). We used Stanford POS tagger(Tou,
3http://www.ranks.nl/resources/stopwords.html
129
2000) for tagging the tweets.
Note: The dataset we use to find optimal POS
bi-tags consists of movie reviews. We understand
that POS bi-tags hence derived may not be universal
across domains.
Tweet
Lexical Resourc
e
Get sentime
nt 
predicti
on
For all w
ords
POS tag
 the tweet
Retain words corresp
ond
Return 
output label corresp
ondin
g to ma
jority of word
s
Sentime
nt 
Predicti
on
corresp
ond
ing to select P
OS 
bi-tags
Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It
Version 2
3.3 Tweet Sentiment Collaborator
Based on predictions of individual tweets, the Tweet
Sentiment Collaborator gives overall prediction
with respect to a keyword in form of percentage
of positive, negative and objective content. This
is on the basis of predictions by each resource by
weighting them according to their accuracies. These
weights have been assigned to each resource based
on experimental results. For each resource, the
following scores are determined.
posscore[r] =
m?
i=1
piwpi
negscore[r] =
m?
i=1
niwni
objscore[r] =
m?
i=1
oiwoi
where
posscore[r] = Positive score for search string r
negscore[r] = Negative score for search string r
objscore[r] = Objective score for search string r
m = Number of resources used for prediction
pi, ni, oi = Positive,negative & objective count of tweet
predicted respectively using resource i
wpi, wni, ooi = Weights for respective classes derived
for each resource i
We normalize these scores to get the final positive, neg-
ative and objective pertaining to search string r. These
scores are represented in form of percentage.
3.4 Resources
Sentiment-based lexical resources annotate
words/concepts with polarity. The completeness
of these resources individually remains a question.
To achieve greater coverage, we use four different
sentiment-based lexical resources for C-Feel-It. They are
described as follows.
1. SentiWordNet (Esuli and Sebastiani, 2006) assigns
three scores to synsets of WordNet: positive score,
negative score and objective score. When a word is
looked up, the label corresponding to maximum of
the three scores is returned. For multiple synsets of
a word, the output label returned by majority of the
synsets becomes the prediction of the resource.
2. Subjectivity lexicon (Wiebe et al, 2004) is a re-
source that annotates words with tags like parts-of-
speech, prior polarity, magnitude of prior polarity
(weak/strong), etc. The prior polarity can be posi-
tive, negative or neutral. For prediction using this
resource, we use this prior polarity.
3. Inquirer (Stone et al, 1966) is a list of words
marked as positive, negative and neutral. We use
these labels to use Inquirer resource for our predic-
tion.
4. Taboada (Taboada and Grieve, 2004) is a word-list
that gives a count of collocations with positive and
negative seed words. A word closer to a positive
seed word is predicted to be positive and vice versa.
3.5 Implementation Details
The system is implemented in JSP (JDK 1.6) using Net-
Beans IDE 6.9.1. For the purpose of tweet annotation,
an internal interface was written in PHP 5 with MySQL
5.0.51a-3ubuntu5.7 for storage.
4 System Analysis
4.1 Evaluation Data
For the purpose of evaluation, a total of 7000 tweets
were downloaded by using popular trending topics of
20 domains (like books, movies, electronic gadget, etc.)
as keywords for searching tweets. In order to download
the tweets, we used the API provided by Twitter 4 that
crawls latest tweets pertaining to keywords.
Human annotators assigned to a tweet one out of 4
classes: positive, negative, objective and objective-spam.
4http://search.twitter.com/search.atom?
130
A tweet is assigned to objective-spam category if it con-
tains promotional links or incoherent text which was pos-
sibly not created by a human user. Apart from these nom-
inal class labels, we also assigned the positive/negative
tweets scores ranging from +2 to -2 with +2 being the
most positive and -2 being the most negative score re-
spectively. If the tweet belongs to the objective category,
a value of zero is assigned as the score.
The spam category has been included in the annotation
as a future goal of modeling a spam detection layer prior
to the sentiment detection. However, the current version
of C-Feel-It does not have a spam detection module and
hence for evaluation purpose, we use only the data be-
longing to classes other than objective-spam.
4.2 Qualitative Analysis
In this section, we perform a qualitative evaluation of ac-
tual results returned by C-Feel-It. The errors described
in this section are in addition to the errors due to mis-
spellings and informal language. These erroneous results
have been obtained from both version 1 and 2. They
have been classified into eleven categories and explained
henceforth.
4.2.1 Sarcastic Tweets
Tweet: Hoge, Jaws, and Palantonio are brilliant to-
gether talking X?s and O?s on ESPN right now.
Label by C-Feel-It: Positive
Label by human annotator: Negative
The sarcasm in the above tweet lies in the use of a pos-
itive word ?brilliant? followed by a rather trivial action of
?talking Xs and Os?. The positive word leads to the pre-
diction by C-Feel-It where in fact, it is a negative tweet
for the human annotator.
4.2.2 Lack of Sense Understanding
Tweet: If your tooth hurts drink some pain killers and
place a warm/hot tea bag like chamomile on your tooth
and hold it. it will relieve the pain
Label by C-Feel-It: Negative
This tweet is objective in nature. The words ?pain?,
?killers?, etc. in the tweet give an indication to C-Feel-It
that the tweet is negative. This misguided implication is
because of multiple senses of these words (for example,
?pain? can also be used in the sentence ?symptoms of the
disease are body pain and irritation in the throat? where
it is non-sentiment-bearing). The lack of understanding
of word senses and being unable to distinguish between
them leads to this error.
4.2.3 Lack of Entity Specificity
Tweet: Casablanca and a lunch comprising of rice
and fish: a good sunday
Keyword: Casablanca
Label by C-Feel-It: Positive
Label by human annotator: Objective
In the above tweet, the human annotator understood
that though the tweet contains the keyword ?Casablanca?,
it is not Casablanca about which sentiment is expressed.
The system finds a positive word ?good? and marks the
tweet as positive. This error arises because the system
cannot find out which sentence/parts of sentence is ex-
pressing opinion about the target entity.
4.2.4 Coverage of Resources
Tweet: I?m done with this bullshit. You?re the psycho
not me.
Label by SentiWordNet: Negative
Label by Taboada/Inquirer: Objective
Label by human annotator: Negative
On manual verification, it was observed that an entry
for the emotion-bearing word ?bullshit? is present in Sen-
tiWordNet while Inquirer and Taboada resource do not
have them. This shows that the coverage of the lexical
resource affects the performance of a system and may in-
troduce errors.
4.2.5 Absence of Named Entity Recognition
Tweet: @user I don?t think I need to guess, but ok,
close encounters of the third kind? Lol
Entity: Close encounters of the third kind
Label by C-Feel-It: Positive
The words comprising the name of the film ?Close en-
counters of the third kind? are also looked up. Inability to
identify the named entity leads the system into this trap.
4.2.6 Requirement of World Knowledge
Tweet: The soccer world cup boasts an audience twice
that of the Summer Olympics.
Label by C-Feel-It: Negative
To judge the opinion of this tweet, one requires an un-
derstanding of the fact that larger the audience, more fa-
vorable it is for a sports tournament. This world knowl-
edge is important for a system that aims to handle tweets
like these.
4.2.7 Mixed Emotion Tweets
Tweet: oh but that last kiss tells me it?s goodbye, just
like nothing happened last night. but if i had one chance,
i?d do it all over again
Label by C-Feel-It: Positive
The tweet contains emotions of positive as well as neg-
ative variety and it would in fact be difficult for a human
as well to identify the polarity. The mixed nature of the
tweet leads to this error by the system.
4.2.8 Lack of Context
Tweet: I?ll have to say it?s a tie between Little Women
or To kill a Mockingbird
131
Label by C-Feel-It: Negative
Label by human user: Positive
The tweet has a sentiment which will possibly be clear
in the context of the conversation. Going by the tweet
alone, while one understands that an comparative opinion
is being expressed, it is not possible to tag it as positive
or negative.
4.2.9 Concatenated Words
Tweet: To Kill a Mockingbird is a #goodbook.
Label by C-Feel-It: Negative
The tweet has a hashtag containing concatenated
words ?goodbook? which get overlooked as out-of-
dictionary words and hence, are not used for sentiment
prediction. The sentiment of ?good? is not detected.
4.2.10 Interjections
Tweet: Oooh. Apocalypse Now is on bluray now.
Label by C-Feel-It: Objective
Label by human user: Positive
The extended interjection ?Oooh? is an indicator of
sentiment. Since it does not have a direct prior polar-
ity, it is not present in any of the resources. However, this
interjection is an important carrier of sentiment.
4.2.11 Comparatives
Tweet: The more years I spend at Colbert Heights..the
more disgusted I get by the people there. I?m soooo ready
to graduate.
Label by C-Feel-It: Positive
Label by human user: Negative
The comparatives in the sentence expressed by ?..more
disgusted I get..? have to be handled as a special case
because ?more? is an intensification of the negative senti-
ment expressed by the word ?disgusted?.
5 Summary & Future Work
In this paper, we described a system which categorizes
live tweets related to a keyword as positive, negative
and objective based on the predictions of four sentiment-
based resources. We also presented a qualitative evalua-
tion of our system pointing out the areas of improvement
for the current system.
A sentiment analyzer of this kind can be tuned to take in-
puts from different sources on the internet (for example,
wall posts on facebook). In order to improve the qual-
ity of sentiment prediction, we propose two additions.
Firstly, while we use simple heuristics to handle exten-
sions of words in tweets, a deeper study is required to
decipher the pragmatics involved. Secondly, a spam de-
tection module that eliminates promotional tweets before
performing sentiment detection may be added to the cur-
rent system. Our goal with respect to this system is to de-
ploy it for predicting share market values of firms based
on sentiment on social networks with respect to related
entitites.
Acknowledgement
We thank Akshat Malu and Subhabrata Mukherjee, IIT
Bombay for their assistance during generation of evalua-
tion data.
References
Go Alec, Huang Lei, and Bhayani Richa. 2009a. Twit-
ter sentiment classification using distant supervision.
Technical report, Standford University.
Go Alec, Bhayani Richa, Raghunathan Karthik, and
Huang Lei. 2009b. May.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A publicly available lexical resource for opinion
mining. In Proceedings of LREC-06, Genova, Italy.
Andreas M. Kaplan and Michael Haenlein. 2010. The
early bird catches the news: Nine things you should
know about micro-blogging. Business Horizons,
54(2):05 ? 113.
Julie B. Lovins. 1968. Development of a Stemming Al-
gorithm. June.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL-05.
Vladimir Prelovac. 2010. Top social media sites. Web,
May.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal Automatically. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, pages 158?161, Stan-
ford, US.
2000. Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computional Linguistics, 30:277?308,
September.
132
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?422,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Haves and the Have-Nots: Leveraging Unlabelled Corpora for
Sentiment Analysis
Kashyap Popat2 Balamurali A R1,2,3 Pushpak Bhattacharyya2 Gholamreza Haffari3
1IITB-Monash Research Academy, IIT Bombay 3Monash University
2Dept. of Computer Science and Engineering, IIT Bombay Australia
{kashyap,balamurali,pb}@cse.iitb.ac.in reza@monash.edu
Abstract
Expensive feature engineering based on
WordNet senses has been shown to
be useful for document level sentiment
classification. A plausible reason for
such a performance improvement is the
reduction in data sparsity. However,
such a reduction could be achieved with
a lesser effort through the means of
syntagma based word clustering. In
this paper, the problem of data sparsity
in sentiment analysis, both monolingual
and cross-lingual, is addressed through
the means of clustering. Experiments
show that cluster based data sparsity
reduction leads to performance better than
sense based classification for sentiment
analysis at document level. Similar idea
is applied to Cross Lingual Sentiment
Analysis (CLSA), and it is shown that
reduction in data sparsity (after translation
or bilingual-mapping) produces accuracy
higher than Machine Translation based
CLSA and sense based CLSA.
1 Introduction
Data sparsity is the bane of Natural Language
Processing (NLP) (Xue et al, 2005; Minkov et al,
2007). Language units encountered in the test data
but absent in the training data severely degrade the
performance of an NLP task. NLP applications
innovatively handle data sparsity through various
means. A special, but very common kind of
data sparsity viz., word sparsity, can be addressed
in one of the two obvious ways: 1) sparsity
reduction through paradigmatically related words
or 2) sparsity reduction through syntagmatically
related words.
Paradigmatic analysis of text is the analysis
of concepts embedded in the text (Cruse, 1986;
Chandler, 2012). WordNet is a byproduct of such
an analysis. In WordNet, paradigms are manually
generated based on the principles of lexical and
semantic relationship among words (Fellbaum,
1998). WordNets are primarily used to address the
problem of word sense disambiguation. However,
at present there are many NLP applications
which use WordNet. One such application is
Sentiment Analysis (SA) (Pang and Lee, 2002).
Recent research has shown that word sense based
semantic features can improve the performance of
SA systems (Rentoumi et al, 2009; Tamara et al,
2010; Balamurali et al, 2011) compared to word
based features.
Syntagmatic analysis of text concentrates on
the surface properties of the text. Compared
to paradigmatic property extraction, syntagmatic
processing is relatively light weight. One of
the obvious syntagmas is words, and words are
grouped into equivalence classes or clusters, thus
reducing the model parameters of a statistical NLP
system (Brown et al, 1992). When used as
an additional feature with word based language
models, it has been shown to improve the system
performance viz., machine translation (Uszkoreit
and Brants, 2008; Stymne, 2012), speech
recognition (Martin et al, 1995; Samuelsson and
Reichl, 1999), dependency parsing (Koo et al,
2008; Haffari et al, 2011; Zhang and Nivre, 2011;
Tratz and Hovy, 2011) and NER (Miller et al,
2004; Faruqui and Pado?, 2010; Turian et al, 2010;
Ta?ckstro?m et al, 2012).
In this paper, the focus is on alleviating the
data sparsity faced by supervised approaches
for SA through the means of cluster based
features. As WordNets are essentially word
412
clusters wherein words with the same meaning
are clubbed together, they address the problem of
data sparsity at word level. The abstraction and
dimensionality reduction thus achieved attributes
to the superior performance for SA systems that
employs WordNet senses as features. However,
WordNets are manually created. Automatic
creation of the same is challenging and not much
successful because of the linguistic complexity
involved. In case of SA, manually creating the
features based on WordNet senses is a tedious and
an expensive process. Moreover, WordNets are
not present for many languages. All these factors
make the paradigmatic property based cluster
features like WordNet senses a less promising
pursuit for SA.
The syntagmatic analysis essentially makes use
of distributional similarity and may in many
circumstances subsume the paradigmatic analysis.
In the current work, this particular insight is
used to solve the data sparsity problem in
the sentiment analysis by leveraging unlabelled
monolingual corpora. Specifically, experiments
are performed to investigate whether features
developed from manually crafted clusterings
(coming from WordNet) can be replaced by those
generated from clustering based on syntagmatic
properties.
Further, cluster based features are used to
address the problem of scarcity of sentiment
annotated data in a language. Popular
approaches for Cross-Lingual Sentiment Analysis
(CLSA) (Wan, 2009; Duh et al, 2011) depend
on Machine Translation (MT) for converting
the labeled data from one language to the
other (Hiroshi et al, 2004; Banea et al, 2008;
Wan, 2009). However, many languages which
are truly resource scarce, do not have an MT
system or existing MT systems are not ripe to
be used for CLSA (Balamurali et al, 2013). To
perform CLSA, this study leverages unlabelled
parallel corpus to generate the word alignments.
These word alignments are then used to link
cluster based features to obliterate the language
gap for performing SA. No MT systems or
bilingual dictionaries are used for this study.
Instead, language gap for performing CLSA is
bridged using linked cluster or cross-lingual
clusters (explained in section 4) with the
help of unlabelled monolingual corpora. The
contributions of this paper are two fold:
1. Features created from manually built and
finer clusters can be replaced by inexpensive
cluster based features generated solely from
unlabelled corpora. Experiments performed
on four publicly available datasets in three
languages viz., English, Hindi and Marathi1
suggest that cluster based features can
considerably boost the performance of an SA
system. Moreover, state of the art result
is obtained for one of the publicly available
dataset.
2. An alternative and effective approach for
CLSA is demonstrated using clusters as
features. Word clustering is a powerful
mechanism to ?transfer? a sentiment
classifier from one language to another. Thus
can be used in truly resource scarce scenarios
like that of English-Marathi CLSA.
The rest of the paper is organized as follows:
section 2 presents related work. Section 3 explains
different word cluster based features employed
to reduce data sparsity for monolingual SA. In
section 4, alternative CLSA approaches based
on word clustering are elucidated. Experimental
details are explained in section 5. Results and
discussions are presented in section 6 and section
7 respectively. Finally, section 8 concludes
the paper pointing to some future research
possibilities.
2 Related Work
The problem of SA at document level is defined
as the classification of document into different
polarity classes (positive and negative) (Turney,
2002). Both supervised (Benamara et al, 2007;
Martineau and Finin, 2009) and unsupervised
approaches (Mei et al, 2007; Lin and He, 2009)
exist for this task.
Supervised approaches are popular because
of their superior classification accuracy (Mullen
and Collier, 2004; Pang and Lee, 2008).
Feature engineering plays an important role
in these systems. Apart from the commonly
used bag-of-words features based on
unigrams/bigrams/ngrams (Dave et al, 2003;
Ng et al, 2006; Martineau and Finin, 2009),
1Hindi and Marathi belong to the Indo-Aryan subgroup
of the Indo-European language family and are two widely
spoken Indian languages with a speaker population of 450
million and 72 million respectively.
413
syntax (Matsumoto et al, 2005; Nakagawa et
al., 2010), semantic (Balamurali et al, 2011)
and negation (Ikeda et al, 2008) have also been
explored for this task. There has been research
related to clustering and sentiment analysis. In
Rooney et al (2011), documents are clustered
based on the context of each document and
sentiment labels are attached at the cluster level.
Zhai et al (2011) attempts to cluster features of a
product to perform sentiment analysis on product
reviews. In this work, word clusters (syntagmatic
and paradigmatic) encoding a mixture of syntactic
and semantic information are used for feature
engineering.
In situations where labeled data is not present
in a language, approaches based on cross-lingual
sentiment analysis are used. Most often these
methods depend on an intermediary machine
translation system (Wan, 2009; Brooke et al,
2009) or a bilingual dictionary (Ghorbel and
Jacot, 2011; Lu et al, 2011) to bridge the
language gap. Given the subtle and different
ways the sentiment can be expressed which itself
manifested as a result of cultural diversity amongst
different languages, an MT system has to be of a
superior quality to capture them.
3 Clustering for Sentiment Analysis
The goal of this paper, to remind the reader, is to
investigate whether superior word cluster features
based on manually crafted and fine grained lexical
resource like WordNet can be replaced with the
syntagmatic property based word clusters created
from unlabelled monolingual corpora.
In this section, different clustering approaches
are presented for feature engineering in a
monolingual setting.
3.1 Approach 1: Clustering based on
WordNet Sense
A synonymous set of words in a WordNet is called
a synset. Each synset can be considered as a word
cluster comprising of semantically similar words.
Balamurali et al (2011) showed that WordNet
synsets can act as good features for document level
sentiment classification.
Motivation for their study stems from the fact
that different senses of a word can have different
polarities. To empirically prove the superiority
of sense based features, different variants of
a travel review domain corpus were generated
by using automatic/manual sense disambiguation
techniques. Thereafter, accuracies of classifiers
based on different sense-based and word-based
features were compared. The results suggested
that WordNet synset based features performed
better than word-based features.
In this study, synset identifiers are extracted
from manually/automatically sense annotated
corpora and used as features for creating sentiment
classifiers. The classifier thus build is used as
a baseline. Apart from this, another baseline
employing word based features are used for a
comprehensive comparison.
3.2 Approach 2: Syntagmatic Property based
Clustering
For this particular study, a co-occurrence based
algorithm is used to create word clusters. As
the algorithm is based on co-occurrence, one
can extract the classes that have the flavour of
syntagmatic grouping, depending on the nature
of underlying statistics. Agglomerative clustering
algorithm by Brown et al (1992) is used for this
purpose. It is a hard clustering algorithm i.e., each
word belongs to one cluster only.
Formally, as mentioned in Brown et al (1992),
let C be a hard clustering function which maps
vocabulary V to one of the K clusters. Then,
the likelihood (L()) of a sequence of word tokens,
w = [wj ]mj=1, with wj ? V , can be factored as,
L(w;C) =
m?
j=1
p(wj|C(wj))p(C(wj)|C(wj?1)))
(1)
Words are assigned to clusters such that the
above quantity is maximized. For the purpose
of sentiment classification, cluster identifiers
representing words in the document are used as
features for training.
4 Clustering for Cross Lingual
Sentiment Analysis
Existing approaches for CLSA depend on an
intermediary machine translation system to bridge
the language gap (Hiroshi et al, 2004; Banea et
al., 2008). Machine translation is very resource
intensive. If a language is truly resource scarce, it
is mostly unlikely to have an MT system. Given
that sentiment analysis is a less resource intensive
task compared to machine translation, the use of
an MT system is hard to justify for performing
414
CLSA. As a viable alternative, cluster linkages
could be learned from a bilingual parallel corpus
and these linkages can be used to bridge the
language gap for CLSA.
In this section, three approaches using clusters
as features for CLSA are compared. The language
whose annotated data is used for training is
called the source language (S), while the language
whose documents are to be sentiment classified is
referred to as the target language (T ).
4.1 Approach 1: Projection based on Sense
(PS)
In this approach, a Multidict is used to bridge the
language gap for SA. A Multidict is an instance
of WordNet where the same sense from different
languages are linked (Mohanty et al, 2008).
An entry in the multidict will have a WordNet
sense identifier from S and the corresponding
WordNet sense identifier from T . The approach
of projection based on sense is explained in
Algorithm 1. Note that after the Sense Mark
operation, each document will be represented as
a vector of WordNet sense identifiers.
Algorithm 1 Projection based on sense
Input: Polarity labeled data in source language
(S) and data in target language (T ) to be
labeled
Output: Classified documents
1: Sense mark the polarity labeled data from S
2: Project the sense marked corpora from S to T
using a Multidict
3: Model the sentiment classifier using the data
obtained in step-2
4: Sense mark the unlabelled data from T
5: Test the sentiment classifier on data obtained
in step-4 using model obtained in step-3
Sense identifiers are the features for the
classifier. For those sense identifiers which do not
have a corresponding entry in the Multidict, no
projection is performed.
4.2 Approach 2: Direct Cluster Linking
(DCL)
Given a parallel bilingual corpus, word clusters in
S can be aligned to clusters in T . Word alignments
are created using parallel corpora. Given two
aligned word sequences wS = [wSj ]mj=1 and
wT = [wTk ]nk=1, let ?T |S be a set of scored
alignments from the source language to the target
language. Here, an alignment from the akth source
word to the kth target word, with score sk,ak > ?
is represented as (wTk , wSak , sk,ak ) ? ?T |S . To
simplify, k ? ?T |S is used to denote those target
words wTk that are aligned to some source word
wSak .
The source and the target side clusters are linked
using the Equation (2).
LC(l) = argmax
t
?
k??T |S ? ?S|T
s.t.CT (wTk )=t
CS (wSak )=l
sk,ak (2)
Here, a target side cluster t ? CT is linked to
a source side cluster l ? CS such that the total
alignment score between words in l and words in
t is maximum. CS and CT stands for source and
target side cluster list respectively. LC(l) gives
the target side cluster t to which l is linked.
4.3 Approach 3: Cross-Lingual Clustering
(XC)
Direct cluster linking approach suffers from the
size of alignment dataset in the form of parallel
corpora. The size of the alignment dataset is
typically smaller than the monolingual dataset.
To circumvent this problem, Ta?ckstro?m et al
(2012) introduced cross-lingual clustering. In
cross-lingual clustering, the objective function
maximizes the joint likelihood of monolingual
and cross-lingual factors. Given a list of
words and clusters it belongs to, a clustering
algorithm tries to obtain word-cluster association
which maximizes the joint likelihood of words
and clusters. Whereas in case of cross-
lingual clustering, the same clustering can be
explained in terms of maximizing the likelihood
of monolingual word-cluster pairs of the source,
the target and alignments between them.
Formally, as stated in Ta?ckstro?m et al (2012),
Using the model of Uszkoreit and Brants (2008),
the likelihood of a sequence of word tokens,
w = [wj ]mj=1, with wj ? V , can be factored as,
L(w;C) =
m?
j=1
p(wj|C(wj))p(C(wj)|wj?1))
(3)
Note this is different from the likelihood
estimation of Brown et al (1992) (Equation (1)),
where C(wj) was conditioned on C(wj?1). This
415
makes the computation easier as suggested in the
original paper. The Equation (3) in a cross lingual
setting will be transformed as given below:
LS,T (wS , wT ;?T |S , ?S|T , CS , CT ) =
LS(...).LT (...).LT |S(...).LS|T (...) (4)
Here, LT |S(...) and LS|T (...) are factors based on
word alignments, which can be represented as:
LT |S(wT ;?T |S , CT , CS) =
?
k??T |S
p(wTk |CT (wTk ))p(CT (wTk )|CS(wSak)))
(5)
Based on the optimization objective in
Equation (4), a pseudo algorithm is defined in
Algorithm 2. For more information, readers are
requested to refer Ta?ckstro?m et al (2012).
Algorithm 2 Cross-lingual Clustering (XC)
Input: Source and target language corpus
Output: Cross-lingual clusters
1: ## CS , CT randomly initialized
2: for i? 1 to N do
3: Find CS? ? argmaxCS LS(wS ;CS)
4: Project CS? to CT
5: Find CT? ? argmaxCT LT (wT ;CT )
6: Project CT? to CS
7: end for
An MT based CLSA approach is used as the
baseline. Training data from S is translated to T
and classification model is learned using unigram
based features. Thereafter, the classifier is directly
tested on data from T .
5 Experimental Setup
Analysis was performed on three languages, viz.,
English (En), Hindi (Hi) and Marathi (Mar).
CLSA was performed on two language
pairs, English-Hindi and English-Marathi.
For clustering the words, monolingual data of
Indian Languages Corpora Initiative (ILCI)2 was
used. It should also be noted that sentiment
annotated data was also included in the data used
for the word clusterings process. For Brown
clustering, an implementation by Liang (2005)
was used. Cross-lingual clustering for CLSA
2http://sanskrit.jnu.ac.in/ilci/index.
jsp
was implemented as directed in Ta?ckstro?m et al
(2012).
Monolingual SA: For experiments in English,
two polarity datasets were used. The first
one (En-TD) by Ye et al (2009) contains user-
written reviews on travel destinations. The
dataset consists of approximately 600 positive
and 591 negative reviews. Reviews were also
manually sense annotated using WordNet 2.1.
The sense annotation was performed by two
annotators with an inter-annotation agreement of
93%. The second dataset (En-PD)3 on product
reviews (music instruments) from Amazon by
Blitzer et al (2007) contains 1000 positive and
1000 negative reviews. This dataset was sense
annotated using an automatic WSD engine which
was trained on tourism domain (Khapra et al,
2010). Experiments using this dataset were
done to study the effect of domain on CLSA.
For experiments in Hindi and Marathi, polarity
datasets by Balamurali et al (2012) were used.4
These are reviews collected from various Hindi
and Marathi blogs and Sunday editorials. Hindi
dataset consist of 98 positive and 100 negative
reviews. Whereas Marathi dataset contains 75
positive and 75 negative reviews. Apart from
being marked with polarity labels at document
level, they are also manually sense annotated using
Hindi and Marathi WordNet respectively.
CLSA: The same datasets used in SA are also
used for CLSA. Three approaches (as described
in section 4) were tested for English-Hindi
and English-Marathi language pairs. To create
alignments, English-Hindi and English-Marathi
parallel corpora from ILCI were used. English-
Hindi parallel corpus contains 45992 sentences
and English-Marathi parallel corpus contains
47881 sentences. To create alignments, GIZA++5
was used (Och and Ney, 2003).
As a preprocessing step, all stop words
were removed. Stemming was performed on
English and Hindi whereas for Marathi data,
Morphological Analyzer was used to reduce the
words to their respective lemmas.
All experiments were performed using C-SVM
3http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/
4http://www.cfilt.iitb.ac.
in/resources/senti/MPLC_tour_
downloaderInfo.php
5http://www-i6.informatik.rwth-aachen.
de/Colleagues/och/software/GIZA++.html
416
Features En-TD En-PD Hi Mar
Words 87.02 77.60 77.36 92.28
WordNet Sense (Paradigmatic) 89.13 74.50 85.80 96.88
Clusters (Syntagmatic) 97.45 87.80 83.50 z 98.66
Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on
two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
Features Words Clust-200 Clust-500 Clust-1000 Clust-1500 Clust-2000 Clust-2500 Clust-3000
En-TD 87.02 97.37 97.45 96.94 96.94 96.52 96.52 96.52
En-PD 77.60 73.20 82.30 84.30 86.35 86.45 87.80 87.40
Table 2: Classification accuracy (in %) versus cluster size (number of clusters to be used).
(linear kernel with parameter optimized over
training set using 5 fold cross validation) available
as a part of LibSVM package6. SVM was used
since it is known to perform well for sentiment
classification (Pang et al, 2002). Results reported
are based on the average of ten-fold cross-
validation accuracies. Standard text metrics are
used for reporting the experimental results.
6 Results
Monolingual classification results are shown in
Table71. Table shows accuracies of SA systems
developed on feature set based on words, senses
and clusters. It must be noted that accuracies
reported for cluster based features are with
respect to the best accuracy based on different
cluster sizes. The improvements in results of
cluster features based approach is found to be
statistically significant over the word features
based approach and sense features based approach
at 95% confidence level when tested using a paired
t-test (except for Hindi cluster features based
approach). But in general, their accuracies do not
significantly vary after cluster size crosses 1500.
Table 2 shows the classification accuracy
variation when cluster size is altered. For,
En-TD and En-PD experiments, the cluster size
was varied between 200-3000 with an interval
of 500 (after a size of 500). In the En-TD
experiment, the best accuracy is achieved for
cluster size 500, which is lesser than the number of
unique-words/unique-senses (6435/6004) present
in the data. Similarly, for the En-PD experiment,
6http://www.csie.ntu.edu.tw/
?
cjlin/
libsvm
7All results reported here are based on 10-fold except for
Marathi (2-fold-5-repeats), as it had comparatively lesser data
samples.
the optimal cluster size of 2500 is also lesser
than the number of unique-words/unique-senses
(30468/4735) present in the data.
To see the effect of training data size variation
for different SA approaches in the En-TD
experiment, the training data size is varied
between 50 to 500. For this, a test set consisting
of 100 positive and 100 negative documents is
fixed. The training data size is varied by selecting
different number of documents from rest of the
dataset (?500 negative and ?500 positive) as a
training set. For each training data set 10 repeats
are performed, e.g., for training data size of 50, 50
negative and 50 positive documents are randomly
selected from the training data pool of ?500
negative and ?500 positive. This was repeated
10 times (with replacement). The results of this
experiment are presented in Figure 1.
 70
 75
 80
 85
 90
 95
 100
 0  100  200  300  400  500
Ac
cu
ra
cy
(%
)
Training data size
Words
Senses (Paradigmatic)
Clusters (Syntagmatic)
Figure 1: Training data variation on En-TD
dataset.
Cross-lingual SA accuracies are presented in
Table 3. As in monolingual case, the reported
accuracies are for features based on the best
cluster size.
417
Target Language MT PS DCL XC
T=Hi 63.13 53.80 51.51 66.16
T=Mar NA 54.00 56.00 60.30
Table 3: Cross-Lingual SA accuracy (%) on T=Hi and T=Mar with S=En for different approaches
(MT=Machine Translation, PS=Projection based on Sense, DCL=Direct Cluster Linking , XC=Cross-
Lingual Clustering. There is no MT system available for (S=En, T=Mar).
7 Discussions
In this section, some important observations from
the results are discussed.
1. Syntagmatic analysis may be used in lieu
of paradigmatic analysis for SA: The results
suggest that word cluster based features using
syntagmatic analysis is comparatively better than
cluster (sense) based features using paradigmatic
analysis. For two datasets in English and for
the one in Marathi this holds true. For English,
the gap between classification accuracy based on
sense features and cluster features is around 10%.
A state-of-art accuracy is obtained for the public
dataset on travel domain (En-TD).
The difference in accuracy reduces as the
language gets morphologically rich. In a
morphologically rich language, morphology
encompasses syntactical information, limiting the
context it can provide for clustering. This can be
seen from the classification results on Marathi.
However for Hindi, classifier built on features
based on syntagmatic analysis trails the one based
on paradigmatic analysis.
Compared to Marathi, Hindi is a less
morphologically rich language, hence, a better
result was expected. However, a contrary result
was obtained.z In Hindi, the subject and the
object of the sentence are linked using a case
marker. Upon error analysis, it was found that
there was a lot of irregular compounding based
on case markers. Case markers were compounded
with the succeeding word. This is a deviation
from the real scenario which would have resulted
in incorrect clustering leading to an unexpected
result. However, the same would not have
occurred for a classifier developed on sense based
features as it was manually sense tagged.
Clustering induces a reduction in the data
sparsity. For example, on En-PD, percentage of
features present in the test set and not present in
the training set to those present in the test set
are 34.17%, 11.24%, 0.31% for words, synsets
and cluster based features respectively. The
improvement in the performance of classifiers
may be attributed to this feature size reduction.
However, it must be noted that clustering based
on unlabelled corpora is less taxing than manually
creating paradigmatic property based clusters like
WordNet synsets.
Barring one instance, both cluster based
features outperform word based features. The
reason for the drop in the accuracy of approach
based on sense features for En-PD dataset
is the domain specific nature of sentiment
analysis (Blitzer et al, 2007), which is explained
in the next point.
2. Domain issues are resolved while using
cluster based features: For En-PD, the classifier
developed using sense features based on
paradigmatic analysis performs inferior to
word based features. Compared to other datasets
used for analysis, this dataset was sense annotated
using an automatic WSD engine. This engine was
trained on a travel domain corpus and as WSD
is also domain specific, the final classification
performance suffered. Additionally, as the target
domain was on products, the automatic WSD
engine employed had an in-domain accuracy
of 78%. The sense disambiguation accuracy of
the same would have lowered in a cross-domain
setting. This might have had a degrading effect on
the SA accuracy.
However, it was seen that classifier developed
on cluster features based on syntagmatic analysis
do not suffer from this. Such clusters
obliterate domain relates issues. In addition, as
more unlabelled data is included for clustering,
the classification accuracy improves.8 Thus,
clustering may be employed to tackle other
specific domain related issues in SA.
8It was observed that adding 0.1 million unlabelled
documents, SA accuracy improved by 1%. This was observed
in the case of English for which there is abundant unlabelled
corpus.
418
3. Cluster based features using syntagmatic
analysis requires lesser training data: Cluster
based features drastically reduces the dimension
of the feature vector. For instance, the size
of sense based features for En-TD dataset was
1/6th of the size of word based features. This
reduces the perplexity of the classification model.
The reduction in the perplexity leads to the
reduction of training documents to attain the same
classification accuracy without any dimensionality
reduction. This is evident from Figure 1
where accuracy of the cluster features based on
unlabelled corpora are higher even with lesser
training data.
4. Effect of cluster size: The cluster size
(number of clusters employed) has an implication
on the purity of each cluster with respect to the
application. The system performance improved
upon increasing the cluster size and converged
after attaining a certain level of accuracy. In
general, it was found that the best classification
accuracy was obtained for a cluster size between
1000 and 2500. As evident from Table 2, once
the optimal accuracy is obtained, no significant
changes were observed by increasing the cluster
size.
5. Clustering based CLSA is effective:
For target language as Hindi, CLSA accuracy
based on cross-lingual clustering (syntagmatic)
outperforms the one based on MT (refer to
Table 3). This was true for the constraint
clustering approach based on cross-lingual
clustering. Whereas, sentiment classifier using
sense (PS) or direct cluster linking (DCL) is
not very effective. In case of PS approach, the
coverage of the multidict was a problem. The
number of a linkages between sense from English
to Hindi is only around 1/3rd the size of Princeton
WordNet (Fellbaum, 1998). Similarly in case
of DCL approach, monolingual likelihood is
different from the cross-lingual likelihood in
terms of the linkages.
6. A note on CLSA for truly resource scarce
languages: Note that there is no publicly available
MT system for English to Marathi. Moreover,
the digital content in Marathi language does not
have a standard encoding format. This impedes
the automatic crawling of the web for corpora
creation for SA. Much manual effort has to be put
to collect enough corpora for analysis. However,
even in these languages, unlabelled corpora is
easy to obtain. Marathi was chosen to depict
a truly resource scarce SA scenario. Cluster
features based classifier comparatively performed
well with 60% classification accuracy. An MT
based system would have suffered in this case as
Marathi, as stated earlier, is a morphologically
rich language and as compared to English, has a
different word ordering. This could degrade the
accuracy of the machine translation itself, limiting
the performance of an MT based CLSA system.
All this is obliterated by the use of a cluster based
CLSA approach. Moreover, as more monolingual
copora is added for clustering, the cross lingual
cluster linkages could be refined. This can further
boost the CLSA accuracy.
8 Conclusion and Future Work
This paper explored feasibility of using word
cluster based features in lieu of features based on
WordNet senses for sentiment analysis to alleviate
the problem of data sparsity. Abstractly, the
motivation was to see if highly effective features
based on paradigmatic property based clustering
could be replaced with the inexpensive ones based
on syntagmatic property for SA.
The study was performed for both monolingual
SA and cross-lingual SA. It was found that
cluster features based on syntagmatic analysis
are better than the WordNet sense features based
on paradigmatic analysis for SA. Invesitgation
revealed that a considerable decrease in the
training data could be achieved while using such
class based features. Moreover, as syntagma based
word clusters are homogenous, it was able to
address domain specific nature of SA as well.
For CLSA, clusters linked together using
unlabelled parallel corpora do away with the need
of translating labelled corpora from one language
to another using an intermediary MT system or
bilingual dictionary. Such a method outperforms
an MT based CLSA approach. Further, this
approach was found to be useful in cases where
there are no MT systems to perform CLSA and
the language of analysis is truly resource scarce.
Thus, wider implication of this study is that many
widely spoken yet resource scare languages like
Pashto, Sundanese, Hausa, Gujarati and Punjabi
which do not have an MT system could now be
analysed for sentiment. The approach presented
here for CLSA will still require a parallel corpora.
However, the size of the parallel corpora required
419
for CLSA can considerably be much lesser than
the size of the parallel corpora required to train an
MT system.
A naive cluster linkage algorithm based on word
alignments was used to perform CLSA. As a
result, there were many erroneous linkages which
lowered the final SA accuracy. Better cluster-
linking approaches could be explored to alleviate
this problem. There are many applications which
use WordNet like IR, IE etc. It would be
interesting to see if these could be replaced by
clusters based on the syntagmatic property.
References
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2011. Harnessing wordnet senses for su-
pervised sentiment classification. In Proceedings of
EMNLP 2011, pages 1081?1091, Stroudsburg, PA,
USA.
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2012. Cross-lingual sentiment analysis
for Indian languages using linked wordnets. In Pro-
ceedings of COLING 2012, pages 73?82, Mumbai,
India.
A. R. Balamurali, Mitesh M. Khapra, and Pushpak
Bhattacharyya. 2013. Lost in translation: viability
of machine translation for cross language sentiment
analysis. In Proceedings of CICLing 2013, pages
38?49, Berlin, Heidelberg.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of EMNLP 2008, pages 127?135, Honolulu, Hawaii.
Farah Benamara, Sabatier Irit, Carmine Cesarano,
Napoli Federico, and Diego Reforgiato. 2007. Sen-
timent analysis: Adjectives and adverbs are better
than adjectives alone. In Proceedings of the Inter-
national Conference on Weblogs and Social Media.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL 2007, pages 440?
447, Prague, Czech Republic.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of the International
Conference RANLP-2009, pages 50?54, Borovets,
Bulgaria.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, pages 467?479,
December.
D. Chandler. 2012. Semiotics for begin-
ners. http://users.aber.ac.uk/dgc/
Documents/S4B/sem01.html. Online, ac-
cessed 20-February-2013.
D. A. Cruse. 1986. Lexical Semantics. Cambridge
University Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003, pages 519?528, New
York, NY, USA.
Kevin Duh, Akinori Fujino, and Masaaki Nagata.
2011. Is machine translation ripe for cross-lingual
sentiment classification? In Proceedings of ACL-
HLT 2011, pages 429?433, Stroudsburg, PA, USA.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and Evaluating a German Named Entity Recognizer
with Semantic Generalization. In Proceedings of
KONVENS 2010, Saarbru?cken, Germany.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Hatem Ghorbel and David Jacot. 2011. Further ex-
periments in sentiment analysis of french movie re-
views. In Proceedings of AWIC 2011, pages 19?28,
Fribourg, Switzerland.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL-HLT
2011, pages 710?714, Stroudsburg, PA, USA.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING 2004, Stroudsburg, PA, USA.
Daisuke Ikeda, Hiroya Takamura, Lev arie Ratinov, and
Manabu Okumura. 2008. Learning to shift the po-
larity of words for sentiment classification. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In Proceedings of
Global Wordnet Conference.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-HLT 2008, pages 595?603,
Columbus, Ohio.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. M. eng. thesis, Massachusetts Institute
of Technology.
420
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM 2009, pages 375?384, New York,
NY, USA.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of
ACL-HLT 2011, pages 320?330, Stroudsburg, PA,
USA.
Sven Martin, Jrg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253?1256.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis.
In Proceedings of ICWSM.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Advances in Knowledge Discovery and Data Min-
ing, Lecture Notes in Computer Science, pages 301?
311.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW 2007, pages 171?180, New
York, NY, USA.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT-NAACL
2004: Main Proceedings, pages 337?342, Boston,
Massachusetts, USA.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of ACL 2007, pages
128?135, Prague, Czech Republic.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictio-
nary: Insights, applications and challenges. In Pro-
ceedings of Global Wordnet Conference.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings of EMNLP 2004,
pages 412?418, Barcelona, Spain.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings of HLT-NAACL 2010, pages 786?794, Strouds-
burg, PA, USA.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classifi-
cation of reviews. In Proceedings of the COLING
2006, pages 611?618, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Bo Pang and Lillian Lee. 2002. Thumbs up? sen-
timent classification using machine learning tech-
niques. In Proceedings of EMNLP 2002, pages 79?
86, Stroudsburg, PA, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135, January.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Senti-
ment analysis of figurative language using a word
sense disambiguation approach. In Proceedings of
RANLP 2009, pages 370?375, Borovets, Bulgaria,
September.
Niall Rooney, Hui Wang, Fiona Browne, Fergal Mon-
aghan, Jann Mller, Alan Sergeant, Zhiwei Lin,
Philip Taylor, and Vladimir Dobrynin. 2011. An ex-
ploration into the use of contextual document clus-
tering for cluster sentiment analysis. In Proceedings
of RANLP 2011, pages 140?145, Hissar, Bulgaria.
C. Samuelsson and W. Reichl. 1999. A class-based
language model for large-vocabulary speech recog-
nition extracted from part-of-speech statistics. In
Proceedings of ICASSP 1999, pages 537?540.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of the Joint Workshop on Unsupervised and
Semi-Supervised Learning in NLP, pages 28?34.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings
of NAACL-HLT 2012, pages 477?487, Montre?al,
Canada.
Martin Tamara, Balahur Alexandra, and Montoyo An-
dres. 2010. Word sense disambiguation in opinion
mining: Pros and cons. Journal Research in Com-
puting Science, 46:119?130.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP 2011, pages 1257?1268,
Stroudsburg, PA, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384?394, Stroudsburg, PA, USA.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417?424, Stroudsburg, PA, USA.
421
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings of ACL-HLT 2008, pages 755?762, Colum-
bus, Ohio.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of ACL 2009,
pages 235?243, Stroudsburg, PA, USA.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi,
Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005.
Scalable collaborative filtering using cluster-based
smoothing. In Proceedings of SIGIR 2005, pages
114?121, New York, NY, USA.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009.
Sentiment classification of online reviews to travel
destinations by supervised machine learning ap-
proaches. Expert Systems with Applications, 36(3,
Part 2):6527?6535.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Clustering product features for opinion mining. In
Proceedings of WSDM 2011, pages 347?354, New
York, NY, USA.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of ACL-HLT 2011, pages 188?
193, Stroudsburg, PA, USA.
422
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 132?138,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Robust Sense-Based Sentiment Classification
Balamurali A R1 Aditya Joshi2 Pushpak Bhattacharyya2
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
Mumbai, India - 400076
{balamurali,adityaj,pb}@cse.iitb.ac.in
Abstract
The new trend in sentiment classification is
to use semantic features for representation
of documents. We propose a semantic space
based on WordNet senses for a supervised
document-level sentiment classifier. Not only
does this show a better performance for sen-
timent classification, it also opens opportuni-
ties for building a robust sentiment classifier.
We examine the possibility of using similar-
ity metrics defined on WordNet to address the
problem of not finding a sense in the training
corpus. Using three popular similarity met-
rics, we replace unknown synsets in the test
set with a similar synset from the training set.
An improvement of 6.2% is seen with respect
to baseline using this approach.
1 Introduction
Sentiment classification is a task under Sentiment
Analysis (SA) that deals with automatically tagging
text as positive, negative or neutral from the perspec-
tive of the speaker/writer with respect to a topic.
Thus, a sentiment classifier tags the sentence ?The
movie is entertaining and totally worth your money!?
in a movie review as positive with respect to the
movie. On the other hand, a sentence ?The movie is
so boring that I was dozing away through the second
half.? is labeled as negative. Finally, ?The movie is
directed by Nolan? is labeled as neutral. For the pur-
pose of this work, we follow the definition of Pang
et al (2002) & Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Lexeme-based (bag-of-words) features are com-
monly used for supervised sentiment classifica-
tion (Pang and Lee, 2008). In addition to this, there
also has been work that identifies the roles of dif-
ferent parts-of-speech (POS) like adjectives in sen-
timent classification (Pang et al, 2002; Whitelaw et
al., 2005). Complex features based on parse trees
have been explored for modeling high-accuracy po-
larity classifiers (Matsumoto et al, 2005). Text
parsers have also been found to be helpful in mod-
eling valence shifters as features for classifica-
tion (Kennedy and Inkpen, 2006). In general, the
work in the context of supervised SA has focused on
(but not limited to) different combinations of bag-
of-words-based and syntax-based models.
The focus of this work is to represent a document
as a set of sense-based features. We ask the follow-
ing questions in this context:
1. Are WordNet senses better features as com-
pared to words?
2. Can a sentiment classifier be made robust with
respect to features unseen in the training cor-
pus using similarity metrics defined for con-
cepts in WordNet?
We modify the corpus by Ye et al (2009) for the
purpose of our experiments related to sense-based
sentiment classification. To address the first ques-
tion, we show that the approach that uses senses (ei-
ther manually annotated or obtained through auto-
matic WSD techniques) as features performs better
than the one that uses words as features.
Using senses as features allows us to achieve ro-
bustness for sentiment classification by exploiting
the definition of concepts (sense) and hierarchical
structure of WordNet. Hence to address the second
question, we replace a synset not present in the test
set with a similar synset from the training set us-
ing similarity metrics defined on WordNet. Our re-
sults show that replacement of this nature provides a
boost to the classification performance.
The road map for the rest of the paper is as fol-
lows: Section 2 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
132
in section 3. Details about our experiments are de-
scribed in Section 4. In section 5, we present our
results and discussions. We contextualize our work
with respect to other related works in section 6. Fi-
nally, section 7 concludes the paper and points to
future work.
2 WordNet Senses as Features
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This
paper refers to POS category identifier along with
synset offset as synset identifiers or as senses.
2.1 Motivation
We describe three different scenarios to show the
need of sense-based analysis for SA. Consider the
following sentences as the first scenario.
1. ?Her face fell when she heard that she had
been fired.?
2. ?The fruit fell from the tree.?
The word ?fell? occurs in different senses in the
two sentences. In the first sentence, ?fell? has the
meaning of ?assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ?descend in free fall under the influence
of gravity?. A user will infer the negative polarity
of the first sentence from the negative sense of ?fell?
in it. This implies that there is at least one sense of
the word ?fell? that carries sentiment and at least one
that does not.
In the second scenario, consider the following ex-
amples.
1. ?The snake bite proved to be deadly for the
young boy.?
2. ?Shane Warne is a deadly spinner.?
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
1. ?He speaks a vulgar language.?
2. ?Now that?s real crude behavior!?
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ?conspic-
uously and tastelessly indecent?. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
2.2 Sense versus Lexeme-based Feature
Representations
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on
two subsets of the corpus, the details of which are
given in Section 4.1. The experiments conducted on
this set determine the ideal case scenario- the skyline
performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
1. A group of word senses that have been manu-
ally annotated (M)
133
2. A group of word senses that have been anno-
tated by an automatic WSD (I)
3. A group of manually annotated word senses
and words (both separately as features) (Sense
+ Words(M))
4. A group of automatically annotated word
senses and words (both separately as features)
(Sense + Words(I))
Our first set of experiments compares the four fea-
ture representations to find the feature representa-
tion with which sentiment classification gives the
best performance. Sense + Words(M) and Sense
+ Words(I) are used to overcome non-coverage of
WordNet for some noun synsets.
3 Similarity Metrics and Unknown Synsets
3.1 Synset Replacement Algorithm
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
1. ? In the night, it is a lovely city and... ?
2. ? The city has many beautiful hot spots for hon-
eymooners. ?
The synset of ?beautiful? is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in
Algorithm 1. The term concept is used in place
of synset though the two essentially mean the
same in this context. The algorithm aims to find a
concept temp concept for each concept in the test
corpus. The temp concept is the concept closest to
some concept in the training corpus based on the
similarity metrics. The algorithm follows from the
fact that the similarity value for a synset with itself
is maximum.
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value > temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
replace synset corpus(C,temp concept,X);
end
Return X ;
Algorithm 1: Synset replacement using similarity
metric
The for loop over C finds a concept temp concept
in the training corpus with the maximum
similarity value. The method replace synset corpus
replaces the concept C in the test corpus with
temp concept in the test corpus X.
3.2 Similarity Metrics Used
We evaluate the benefit of three similarity metrics,
namely LIN?s similarity metric, Lesk similarity
metric and Leacock and Chodorow (LCH) similarity
metric for the synset replacement algorithm stated.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
134
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
simLIN (A,B) =
2? logPr(lso(A,B))
logPr(A) + logPr(B)
(1)
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
simLCH(A,B) = ? log
(
len(A,B)
2D
)
(2)
4 Experimentation
We describe the variants of the corpus generated and
the experiments in this section.
4.1 Data Preparation
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al (2009). The inter-annotation agree-
ment for a subset(20 positive reviews) of the corpus
showed 91% sense overlap. The manually annotated
corpus consists of 34508 words with 6004 synsets.
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjec-
tive POS based synsets can be detrimental to classi-
fication since adjectives are known to express direct
sentiment (Pang et al, 2002).
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
4.2 Experimental Setup
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al, 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are
not stemmed as per observations by (Leopold and
Kindermann, 2002). To conduct the experiments
based on the synset representation, words in the
corpus are annotated with synset identifiers along
with POS category identifiers. For automatic sense
disambiguation, we used the trained IWSD engine
(trained on tourism domain) from Khapra et al
(2010). These synset identifiers along with POS cat-
egory identifiers are then used as features. For re-
placement using semantic similarity measures, we
used WordNet::Similarity 2.05 package by Pedersen
et al (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
1C=0.0,=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
135
Feature Representation Accuracy PF NF PP NP PR NR
Words 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Sense + Words (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Sense + Words(I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Table 2: Classification Results; M-Manual, I-IWSD, W-Words, PF-Positive F-score(%), NF-Negative F-score (%),
PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
score.
5 Results and Discussions
5.1 Comparison of various feature
representations
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Words).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
Reported results suggest that it is more difficult to
detect negative sentiment than positive sentiment
(Gindl and Liegl, 2008). However, using sense-
based representation, it is important to note that neg-
ative recall increases by around 8%.
The combined model of words and manually an-
notated senses (Sense + Words (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
5.2 Synset replacement using similarity metrics
Table 3 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 3. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach4. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
4Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
?=0.05
136
Features Representa-
tion
SM A PF NF
Words (Baseline) NA 84.90 85.07 84.76
Sense+Words (M) NA 90.20 89.81 90.43
Sense+Words (I) NA 86.08 86.28 85.92
Sense+Words (M) LCH 90.60 90.20 90.85
Sense+Words (M) LIN 90.70 90.26 90.97
Sense+Words (M) Lesk 91.12 90.70 91.38
Sense+Words (I) LCH 85.66 85.85 85.52
Sense+Words (I) LIN 86.16 86.37 86.00
Sense+Words (I) Lesk 86.25 86.41 86.10
Table 3: Similarity Metric Analysis using different
similarity metrics with synsets and a combinations of
synset and words; SM-Similarity Metric, A-Accuracy,
PF-Positive F-score(%), NF-Negative F-score (%)
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated as
LIN or LCH. A good metric which covers all POS
categories can provide substantial improvement in
the classification accuracy.
6 Related Work
This work deals with studying benefit of a word
sense-based feature space to supervised sentiment
classification. This work assumes the hypothesis
that word sense is associated with the sentiment as
shown by Wiebe and Mihalcea (2006) through hu-
man interannotator agreement.
Akkaya et al (2009) and Martn-Wanton et al
(2010) study rule-based sentiment classification us-
ing word senses where Martn-Wanton et al (2010)
uses a combination of sentiment lexical resources.
Instead of a rule-based implementation, our work
leverages on benefits of a statistical learning-based
methods by using a supervised approach. Rentoumi
et al (2009) suggest an approach to use word senses
to detect sentence level polarity using graph-based
similarity. While Rentoumi et al (2009) targets us-
ing senses to handle metaphors in sentences, we deal
with generating a general-purpose classifier.
Carrillo de Albornoz et al (2010) create an emo-
tional intensity classifier using affective class con-
cepts as features. By using WordNet synsets as fea-
tures, we construct feature vectors that map to a
larger sense-based space.
Akkaya et al (2009), Martn-Wanton et al (2010)
and Carrillo de Albornoz et al (2010) deal with
sentiment classification of sentences. On the other
hand, we associate sentiment polarity to a document
on the whole as opposed to Pang and Lee (2004)
which deals with sentiment prediction of subjectiv-
ity content only. Carrillo de Albornoz et al (2010)
suggests expansion using WordNet relations which
we perform in our experiments.
7 Conclusion & Future Work
We present an empirical study to show that sense-
based features work better as compared to word-
based features. We show how the performance im-
pact differs for different automatic and manual tech-
niques. We also show the benefit using WordNet
based similarity metrics for replacing unknown fea-
tures in the test set. Our results support the fact that
not only does sense space improve the performance
of a sentiment classification system but also opens
opportunities for building robust sentiment classi-
fiers that can handle unseen synsets.
Incorporation of syntactical information along
with semantics can be an interesting area of
work. Another line of work is in the context of
cross-lingual sentiment analysis. Current solutions
are based on machine translation which is very
resource-intensive. Using a bi-lingual dictionary
which maps WordNet across languages can prove to
be an alternative.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ?09, pages 190?199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing?02, pages 136?145,
London, UK.
137
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131?142.
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91?112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35?43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110?
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC?10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and Jo?rg Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423?
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296?304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing?10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD?05,, Lecture Notes in Computer Science,
pages 301?311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL?04, pages
271?278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL?04, pages 38?41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP?09, pages 370?375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL?02, pages 417?424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ?05, pages 625?631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL?06, pages
1065?1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 ? 6535.
138
