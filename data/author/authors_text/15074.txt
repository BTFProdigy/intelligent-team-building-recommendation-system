Building and using comparable corpora for domain-specific bilingual lexicon extraction 
Darja Fi?er Nikola Ljube?i? University of Ljubljana, Faculty of Arts, Deparment of Translation University of Zagreb, Faculty of Humanities and Social Sciences A?ker?eva 2 Ivana Lu?i?a 3 Ljubljana, Slovenia Zagreb, Croatia darja.fiser@ff.uni-lj.si nikola.ljubesic@ffzg.hr  ?pela Vintar Senja Pollak University of Ljubljana, Faculty of Arts, Deparment of Translation University of Ljubljana, Faculty of Arts, Deparment of Translation A?ker?eva 2 A?ker?eva 2 Ljubljana, Slovenia Ljubljana, Slovenia spela.vintar@ff.uni-lj.si senja.pollak@ff.uni-lj.si  Abstract This paper presents a series of experiments aimed at inducing and evaluating domain-specific bilingual lexica from comparable corpora. First, a small English-Slovene comparable corpus from health magazines was manually constructed and then used to compile a large comparable corpus on health-related topics from web corpora. Next, a bilingual lexicon for the domain was extracted from the corpus by comparing context vectors in the two languages. Evaluation of the results shows that a 2-way translation of context vectors significantly improves precision of the extracted translation equivalents. We also show that it is sufficient to increase the corpus for one language in order to obtain a higher recall, and that the increase of the number of new words is linear in the size of the corpus. Finally, we demonstrate that by lowering the frequency threshold for context vectors, the drop in precision is much slower than the increase of recall. 1 Introduction Research into using comparable corpora in NLP has gained momentum in the past decade largely due to limited availability of parallel data for many 
language pairs and domains. As an alternative to already established parallel approaches (e.g. Och 2000, Tiedemann 2005) the comparable corpus-based approach relies on texts in two or more languages which are not parallel but nevertheless share several parameters, such as topic, time of publication and communicative goal (Fung 1998, Rapp 1999). The main advantage of this approach is the simpler, faster and more time efficient compilation of comparable corpora, especially from the rich web data (Xiao & McEnery 2006). In this paper we describe the compilation process of a large comparable corpus of texts on health-related topics for Slovene and English that were published on the web. Then we report on a set of experiments we conducted in order to automatically extract translation equivalents for terms from the health domain. The parameters we tested and analysed are: 1- and 2-way translations of context vectors with a seed lexicon, the size of the corpus used for bilingual lexicon extraction, and the word frequency threshold for vector construction. The main contribution of this paper is a much-desired language- and domain-independent approach to bootstrapping bilingual lexica with minimal manual intervention as well as minimal reliance on the existing linguistic resources. The paper is structured as follows: in the next section we give an overview of previous work relevant for our research. In Section 3 we present the construction of the corpus. Section 4 describes 
19
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 19?26,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
the experiments for bilingual lexicon extraction the results of which are reported, evaluated and discussed in Section 5. We conclude the paper with final remarks and ideas for future work. 2 Related work Bilingual lexica are the key component of all cross-lingual NLP applications and their compilation remains a major bottleneck in computational linguistics. In this paper we follow the line of research that was inspired by Fung (1998) and Rapp (1999) who showed that texts do not need to be parallel in order to extract translation equivalents from them. Instead, their main assumption is that the term and its translation appear in similar contexts anyhow. The task of finding the appropriate translation equivalent of a term is therefore reduced to finding the word in the target language whose context vector is most similar to the source term?s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure:  (1) Building context vectors. When representing a word?s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh co-occurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al, 2008). (2) Translating context vectors. Finding the most similar context vectors in the source and target language is not straightforward because a direct comparison of vectors in two different languages is not possible. This is why most researchers first translate features of source context vectors with machine-readable dictionaries and compute similarity measures on those. Koehn and Knight 
(2002) construct the seed dictionary automatically based on identical spelled words in the two languages. Similarly, cognate detection is used by Saralegi et al (2008) by computing the longest common subsequence ratio. D?jean et al (2005), on the other hand, use a bilingual thesaurus instead of a bilingual dictionary. (3) Selecting translation candidates. After source context vectors have been translated, they are ready to be compared to the target context vectors. A number of different vector similarity measures have been investigated. Rapp (1999) applies city-block metric, while Fung (1998) works with cosine similarity. Recent work often uses Jaccard index or Dice coefficient (Saralegi et al, 2008). In addition, some approaches include a subsequent re-ranking of translation candidates based on cognates detection (e.g. Shao and Ng, 2004). 3 Corpus construction A common scenario in the NLP community is a project on a specific language pair in a new domain for which no ready-made resources are available. This is why we propose an approach that takes advantage of the existing general resources, which are then fine-tuned and enriched to be better suited for the task at hand. In this section we describe the construction of a domain-specific corpus that we use for extraction of translation equivalents in the second part of the paper. 3.1 Initial corpus We start with a small part of the Slovene PoS tagged and lemmatized reference corpus FidaPLUS (Arhar et al, 2007) that contains collections of articles from the monthly health and lifestyle magazine called Zdravje1 , which were published between 2003 and 2005 and contain 1 million words. We collected the same amount of text from the most recent issues of the Health Magazine, which is a similar magazine for the English-speaking readers. We PoS-tagged and lemmatized the English part of the corpus with the TreeTagger (Schmid, 1994). 
                                                1 http://www.zdravje.si/category/revija-zdravje [1.4.2010] 
20
3.2 Corpus extension We then extended the initial corpus automatically from the 2 billion-word ukWaC (Ferraresi et al, 2008) and the 380 million-word slWaC (Ljube?i? and Erjavec, 2011), very large corpora that were constructed from the web by crawling the .uk and .si domain respectively. We took into account all the documents from these two corpora that best fit the initial corpora by computing a similarity measure between models of each document and the initial corpus in the corresponding language. The models were built with content words lemmas as their parameters and TF-IDF values as the corresponding parameter values. The inverse document frequency was computed for every language on a newspaper domain of 20 million words. The similarity measure used for calculating the similarity between a document model and a corpus model was cosine with a similarity threshold of 0.2. This way, we were able to extend the Slovene part of the corpus from 1 to 6 million words and the English part to as much as 50 million words. We are aware of more complex methods for building comparable corpora, such as (Li and Gaussier, 2010), but the focus of this paper is on using comparable corpora collected from the web on the bilingual lexicon extraction task, and not the corpus extension method itself. Bilingual lexicon extraction from the extended corpus is described in the following section. 4 Bilingual lexicon extraction In this section we describe the experiments we conducted in order to extract translation equivalents of key terms in the health domain. We ran a series of experiments in which we adjusted the following parameters:  (1) 1- and 2-way translation of context vectors with a seed dictionary; (2) corpus size of the texts between the languages; (3) the word frequency threshold for vector construction.  Although several parameters change in each run of the experiment, the basic algorithm for finding translation equivalents in comparable corpora is always the same: 
 (1) build context vectors for all unknown words in the source language that satisfy the minimum frequency criterion and translate the vectors with a seed dictionary; (2) build context vectors for all candidate translations satisfying the frequency criterion in the target language; (3) compute the similarity of all translated source vectors  with the target vectors and rank translation candidates according to this score.  Previous research (Ljube?i? et al, 2011) has shown that best results are achieved by using content words as features in context vectors and a context window of 7 with encoded position. The highest-scoring combination of vector association and similarity measures turned out to be Log Likelihood (Dunning, 1993) and Jensen-Shannon divergence (Lin, 1991), so we are using those throughout the experiments presented in this paper. 4.1 Translation of context vectors In order to be able to compare two vectors in different languages, a seed dictionary to translate features in context vectors of source words is needed. We tested our approach with a 1-way translation of context features of English vectors into Slovene and a 2-way translation of the vectors from English into Slovene and vice versa where we then take the harmonic mean of the context similarity in both directions for every word pair. A similar 2-way approach is described in (Chiao et al 2004) with the difference that they average on rank values, not on similarity measures. An empirical comparison with their method is given in the automatic evaluation section. A traditional general large-sized English-Slovene dictionary was used for the 1-way translation, which was then complemented with another general large-sized Slovene-English dictionary by the same author in the 2-way translation setting. Our technique relies on the assumption that additional linguistic knowledge is encoded in the independent dictionary in the opposite direction and was indirectly inspired by a common approach to filter out the noise in bilingual lexicon extraction from parallel corpora with source-to-target and target-to-source word-alignment. 
21
Only content-word dictionary entries were taken into account. No multi-word entries were considered either. And, since we do not yet deal with polysemy at this stage of our research, we only extracted the first sense for each dictionary entry. The seed dictionaries we obtained in this way contained 41.405 entries (Eng-Slo) and 30.955 entries (Slo-Eng). 4.2 Corpus size Next, we tested the impact of the extended corpus on the quality and quantity of the extracted translation equivalents by gradually increasing the size of the corpus from 1 to 6 million words. Not only did we increase corpus size for each language equally, we also tested a much more realistic setting in which the amount of data available for one language is much higher than for the other, in our case English for which we were able to compile a 50 million word corpus, which is more than eight times more than for Slovene. 4.3 Word frequency threshold Finally, we tested the precision and recall of the extracted lexica based on the minimum frequency of the words in the corpus from as high as 150 and down to 25 occurrences. This is an important parameter that shows the proportion of the corpus lexical inventory our method can capture and with which quality. 5 Evaluation of the results At this stage of our research we have limited the experiments to nouns. This speeds up and simplifies our task but we believe it still gives an adequate insight into the usefulness of the approach for a particular domain since nouns carry the highest domain-specific terminological load. 5.1 Automatic evaluation Automatic evaluation of the results was performed against a gold standard lexicon of health-related terms that was obtained from the top-ranking nouns in the English health domain model of the initial corpus and that at the same time appeared in the comprehensive dictionary of medical terms mediLexicon2 and were missing from the general bilingual seed dictionary. The gold standard                                                 2 http://www.medilexicon.com [1.4.2010] 
contains 360 English single-word terms with their translations into Slovene. If more than one translation variant is possible for a single English term, all variants appear in the gold standard and any of these translations suggested by the algorithm is considered as correct. Below we present the results of three experiments that best demonstrate the performance and impact of the key parameters for bilingual lexicon extraction from comparable corpora that we were testing in this research. The evaluation measure for precision used throughout this research is mean reciprocal rank (Vorhees, 2001) on first ten translation candidates. Recall is calculated as the percentage of goldstandard entries we were able to calculate translation candidates for. Additionally, a global recall impact of our methods is shown as the overall number of entries for which we were able to calculate translation candidates. Unless stated otherwise, the frequency threshold for the generation of context vectors in the experiments was set to 50. We begin with the results of 1- and 2-way context vector translations that we tested on the initial 1-million-word corpus we constructed from health magazines as well as on a corpus of the same size we extracted from the web. We compared the results of our method with that proposed in (Chiao et al 2004) strengthening our claim that it is the additional information in the reverse dictionary that makes the significant impact, not the reversing itself. As Table 1 shows, using two general dictionaries (2-way two dict) significantly improves the results as a new dictionary brings additional information. That it is the dictionary improving the results is proven by using just one, inverted dictionary in the 2-way manner, which produced worse results than the 1-way approach (2-way inverse dict). The approach of Chiao et al(2004) is also based on new dictionary knowledge since using only one inverted dictionary with their 2-way method yielded results that were almost identical to the 1-way computation. Using rank, not similarity score in averaging results proved to be a good approach (2-way Chiao two dict), but not as efficient as our approach which uses similarity scores (2-way two dict). Our approach yields higher precision and is also easier to compute. Namely, for every candidate pair only the reverse similarity score has 
22
to be computed, and not all similarity scores for every inverse pair to obtain a rank value. Therefore, only the 2-way translation setting averaging on similarity scores is used in the rest of the experiments. It is interesting that the results on the web corpus have a higher precision but a lower recall (0.355 on the initial corpus and 0.198 on the web corpus). Higher precision can be explained with the domain modelling technique that was used to extract web data, which may have contributed to a terminologically more homogenous collection of documents in the health domain. On the other hand, the lower recall can be explained with the extracted web documents being less terminologically loaded than the initial corpus.  
Corpus 1-way 2-way inverse dict 
2-way Chiao two dict 
2-way two dict 
1 M initial 0.591 0.566 0.628 0.641 1 M web 0.626 0.610 0.705 0.710  Table 1: Precision regarding the corpus source and the translation method  The second parameter we tested in our experiments was the impact of corpus size on the quality and amount of the extracted translation equivalents. For the first 6 million words the Slovene and English parts of the corpus were enlarged in equal proportions and after that only the English part of the corpus was increased up to 18 million words.  Corpus size P R No. of translated words Not already in dict 1 0.718 0.198 1246 244 6 0.668 0.565 4535 1546 18 0.691 0.716 9122 4184  Table 2: Precision, recall, number of translated words and number of new words (not found in the dictionary) obtained with different corpus sizes  
 Figure 1: Precision and recall as a function of corpus size  
 Figure 2: The number of new words (not found in the seed dictionary) as a function of corpus size  Figure 1 shows that precision with regard to the gold standard is more or less constant with an average of 0.68 if we disregard the first two measurements that are probably bad estimates since the intersection with the gold standard is small (as shown in Table 1) and evens out as the size of the corpus increases. When analyzing recall against the gold standard we see the typical logarithmic recall behavior when depicted as a function of corpus size. On the other hand, when we consider the number of new translation equivalents (i.e. the number of source words that do not appear in the seed dictionary), the function behaves almost linearly (see Figure 2). This can be explained with the fact that in the dictionary the most frequent words are best represented. Because of that we can observe a steady increase in the number of words not present in the seed lexicon that pass the frequency threshold with the increasing corpus size. Finally, we study the impact of the word frequency threshold for context vector generation on the quality and amount of the extracted translation equivalents on the six million corpora in both languages.   
23
Frequency P No. of translated words 
F1 
25 0.561 7203 0.719 50 0.668 4535 0.648 75 0.711 3435 0.571 100 0.752 2803 0.513 125 0.785 2374 0.464 150 0.815 2062 0.424  Table 3: Precision, number of new words and F1 obtained with different frequency thresholds  As can be seen in Table 3, by lowering the frequency criterion, the F1 measure increases showing greater gain in recall than loss in precision. For calculating recall, the number of new words passing the frequency criterion is normalized with the assumed number of obtainable lexicon entries set to 7.203 (the number of new words obtained with the lowest frequency criterion). This is a valuable insight since the threshold can be set according to different project scenarios. If, for example, lexicographers can be used in order to check the translation candidates and choose the best ones among them, the threshold may well be left low and they will still be able to identify the correct translation very quickly. If, on the other hand, the results will be used directly by another application, the threshold will be raised in order to reduce the amount of noise introduced by the lexicon for the following processing stages. 5.2 Manual evaluation For a more qualitative inspection of the results we performed manual evaluation on a random sample of 100 translation equivalents that are not in the general seed dictionary or present in our gold standard. We were interested in finding out to what extent these translation equivalents belong to the health domain and if their quality is comparable to the results of the automatic evaluation. Manual evaluation was performed on translation equivalents extracted from the comparable corpus containing 18 million English words and 6 million Slovene words, where the frequency threshold was set to 50. 51% of the manually evaluated words belonged to the health domain, 23% were part of general vocabulary, 10% were proper names and 
the rest were acronyms and errors arising from PoS-tagging and lemmatization in the ukWaC corpus. Overall, in 45% the first translation equivalent was correct and additional 11% contained the correct translation among the ten best-ranked candidates. For 44 % of the extracted translation equivalents no appropriate translation was suggested. Among the evaluated health-domain terms, 61% were translated correctly with the first candidate and for the additional 20% the correct translation appeared among the first 10 candidates. Of the 19% health-domain terms with no appropriate translation suggestion, 4 terms, that is 21% of the wrongly translated terms, were translated as direct hypernyms and could loosely be considered as correct (e.g. the English term bacillus was translated as mikroorganizem into Slovene, which means microorganism). Even most other translation candidates were semantically closely related, in fact, there was only one case in the manually inspected sample that provided completely wrong translations. Manual evaluation shows that the quality of translations for out-of-goldstandard terms is consistent with the results of automatic evaluation. A closer look revealed that we were able to obtain translation equivalents not only for the general vocabulary but especially terms relevant for the health domain, and furthermore, that their quality is also considerably higher than for the general vocabulary which is not of our primary interest in this research. The results could be further improved by filtering out the noise obtained from errors in PoS-tagging and lemmatization and, more importantly, by identifying proper names. Multi-word expressions should also be tackled as they present problems, especially in cases of 1:many mappings, such as the English single-word term immunodeficiency that is translated with a multi-word expression in Slovene (imunska pomanjkljivost). 6 Conclusions In this paper we described the compilation process of a domain-specific comparable corpus from already existing general resources. The corpus compiled from general web corpora was used in a set of experiments to extract translation equivalents 
24
for the domain vocabulary by comparing contexts in which terms appear in the two languages. The results show that a 2-way translation of context vectors consistently improves the quality of the extracted translation equivalents by using additional information given from the reverse dictionary. Next, increasing the size of only one part of the comparable corpus brings a slight increase in precision but a very substantial increase in recall.  If we are able to translate less than 20% of the gold standard with a 1 million word corpus, the recall is exceeds 70% when we extend the English part of the corpus to 15 million words. Moreover, the increase of the number of new words we obtain in this way keeps being linear for even large corpus sizes. We can also expect the amount of available text to keep rising in the future. This is a valuable finding because a scenario in which much more data is available for one of the two languages in question is a very common one.  Finally, we have established that the word frequency threshold for building context vectors can be lowered in order to obtain more translation equivalents without a big sacrifice in their quality. For example, a 10% drop in precision yields almost twice as many translation equivalents. Manual evaluation has shown that the quality of health-related terms that were at the center of our research is considerably higher than the rest of the vocabulary but has also revealed some noise in POS-tagging and lemmatization of the ukWaC corpus that consequently lowers the results of our method and should be dealt with in the future.  A straightforward extension of this research is to tackle other parts of speech in addition to nouns. Other shortcomings of our method that will have to be addressed in our future work are multi-word expressions and multiple senses of polysemous words and their translations. We also see potential in using cognates for re-ranking translation candidates as they are very common in the health domain. Acknowledgments Research reported in this paper has been supported by the ACCURAT project within the EU 7th Framework Programme (FP7/2007-2013), grant agreement no. 248347, and by the Slovenian Research Agency, grant no. Z6-3668. 
References  Arhar, ?., Gorjanc, V., and Krek, S. (2007). FidaPLUS corpus of Slovenian - The New Generation of the Slovenian Reference Corpus: Its Design and Tools. In Proceedings of the Corpus Linguistics Conference (CL2007), Birmingham, pp. 95-110. D?jean, H., Gaussier, E., Renders, J.-M. and Sadat, F. (2005). Automatic processing of multilingual medical terminology: Applications to thesaurus enrichment and cross-language information retrieval. Artificial Intelligence in Medicine, 33(2): 111?124.  Doe, J. (2011): Bilingual lexicon extraction from comparable corpora: A comparative study. Doe, J. (2011): Compiling web corpora for Croatian and Slovene. Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics - Special issue on using large corpora, 19(1). Fung, P. (1998). A statistical view on bilingual lexicon extraction: From parallel corpora to nonparallel corpora. In Proc. of the 3rd Conference of the Association for Machine Translation in the Americas, pp. 1?17. Fung, P., Prochasson, E. and Shi, S. (2010). Trillions of Comparable Documents. In Proc. of the 3rd workshop on Building and Using Comparable Corpora (BUCC'10), Language Resource and Evaluation Conference (LREC2010), Malta, May 2010, pp. 26?34. Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell, MA. Koehn, P. and Knight, K. (2002). Learning a translation lexicon from monolingual corpora. In Proc. of the workshop on Unsupervised lexical acquisition (ULA '02) at ACL 2002, Philadelphia, USA, pp. 9?16. Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1): 145-151. Ljube?i?, N. and Erjavec, T. (2011). hrWaC and slWaC: Compiling Web Corpora for Croatian and Slovene. (submitted to International Workshop on Balto-Slavonic Natural Language Processing). Ljube?i?, N., Fi?er D., Vintar ?. and  Pollak S. Bilingual Lexicon Extraction from Comparable Corpora: A Comparative Study. (accepted for WoLeR 2011 at ESSLLI International Workshop on Lexical Resources). 
25
Marsi, E. and Krahmer, E. (2010). Automatic analysis of semantic similarity in comparable text through syntactic tree matching. In Proc. of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752?760. Och, F. J. and Ney, H. (2000). Improved Statistical Alignment Models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, Hongkong, China, pp. 440?447. Otero, P. G. (2007). Learning Bilingual Lexicons from Comparable English and Spanish Corpora. In Proc. of the Machine Translation Summit (MTS 2007), pp. 191?198. Rapp, R. (1999). Automatic identification of word translations from unrelated English and German corpora. In Proc. of the 37th annual meeting of the Association for Computational Linguistics (ACL '99), pp. 519?526. Schmid, H. (1994): Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proc. of International Conference on New Methods in Language Processing. Saralegi, X., San Vicente, I. and Gurrutxaga, A. (2008). Automatic Extraction of Bilingual Terms from Comparable Corpora in a Popular Science Domain. In Proc. of the 1st Workshop on Building and Using Comparable Corpora (BUCC) at LREC 2008. Shao, L. and Ng, H. T. (2004). Mining New Word Translations from Comparable Corpora. In  Proc. 
of the 20th International Conference on Computational Linguistics (COLING '04), Geneva, Switzerland. Shezaf, D. and Rappoport, A. (2010). Bilingual Lexicon Generation Using Non-Aligned Signatures. In Proc.of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, pp. 98?107. Steinberger, R.,  Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufi?, D. and Varga, D. (2006). The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pp. 2142?2147. Tiedemann, J. (2005). Optimisation of Word Alignment Clues. Natural Language Engineering, 11(03): 279?293. Vorhees, E. M. (2001). Overview of the TREC-9 Question Answering Track. In Proceedings of the Ninth Text REtrieval Conference (TREC-9), 2001. Xiao, Z., McEnery, A. (2006). Collocation, semantic prosody and near synonymy: a cross-linguistic perspective. Applied Linguistics 27(1): 103?129. Yu, K. and Tsujii, J. (2009). Bilingual dictionary extraction from Wikipedia. In Proc. of the 12th Machine Translation Summit (MTS 2009), Ottawa, Ontario, Canada. 
26
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 48?57,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Lemmatization and Morphosyntactic Tagging of Croatian and Serbian
Z?eljko Agic?? Nikola Ljubes?ic?? Danijela Merkler?
?Department of Information and Communication Sciences
?Department of Linguistics
Faculty of Humanities and Social Sciences, University of Zagreb
Ivana Luc?ic?a 3, 10000 Zagreb, Croatia
zagic@ffzg.hr nljubesi@ffzg.hr dmerkler@ffzg.hr
Abstract
We investigate state-of-the-art statistical
models for lemmatization and morphosyn-
tactic tagging of Croatian and Serbian.
The models stem from a new manually
annotated SETIMES.HR corpus of Croa-
tian, based on the SETimes parallel cor-
pus. We train models on Croatian text
and evaluate them on samples of Croat-
ian and Serbian from the SETimes corpus
and the two Wikipedias. Lemmatization
accuracy for the two languages reaches
97.87% and 96.30%, while full morphosyn-
tactic tagging accuracy using a 600-tag
tagset peaks at 87.72% and 85.56%, respec-
tively. Part of speech tagging accuracies
reach 97.13% and 96.46%. Results indicate
that more complex methods of Croatian-to-
Serbian annotation projection are not re-
quired on such dataset sizes for these par-
ticular tasks. The SETIMES.HR corpus, its
resulting models and test sets are all made
freely available.
1 Introduction
Part of speech tagging (POS tagging) is an natu-
ral language processing task in which words are
annotated with the corresponding grammatical cate-
gories ? parts of speech: verb, noun, adjective, pro-
noun, etc. ? in a given context. It is also frequently
called morphosyntactic tagging (MSD tagging, i.e.,
tagging with morphosyntactic descriptions), espe-
cially when addressing highly inflected languages,
for which the tagging process often includes as-
signing additional subcategories to words, such as
gender and case for nouns or tense and person for
verbs. POS/MSD tagging is a well-known task and
an important preprocessing step in natural language
processing. It is often preceded or followed by
lemmatization ? the process of mapping inflected
word forms to corresponding base forms or lemmas.
State of the art in POS/MSD tagging and lemma-
tization across languages is generally achieved ?
both in terms of per token accuracy and speed and
robustness ? by statistical methods, which involve
training annotation models on manually annotated
corpora.
In this paper, we investigate the possibility of uti-
lizing statistical models trained on corpora of Croa-
tian in lemmatization and MSD tagging of Croatian
and Serbian. We present a new manually annotated
corpus of Croatian ? the SETIMES.HR corpus. We
test a number of lemmatizers and MSD taggers on
Croatian and Serbian test sets from two different
domains and consider options of annotation trans-
fer between the two languages. We also outline a
first version of the Multext East v5 tagset and three
usable reductions of this tagset. Special emphasis
is given to rapid resource development and public
availability of our research. Thus, the SETIMES.HR
corpus, the test sets and the best lemmatization and
MSD tagging models are made publicly available.1
In the following section, we discuss related work
on lemmatization and tagging of Croatian and Ser-
bian. We then present the SETIMES.HR corpus and
the test sets, selected lemmatizers and morphosyn-
tactic taggers and the experimental method. Finally
we provide a discussion of the evaluation results
and indicate future work directions.
2 Related work
The task of tagging English sentences with parts of
speech is generally considered a closed issue. This
is due to the fact that, over the course of the past 11
years, from (Brants, 2000) to (S?gaard, 2011), the
current state of the art in tagging English has im-
proved by 1.04 ? to 97.50% in terms of per token
accuracy. This is, however, not the case for lan-
guages with richer morphology and free sentence
1http://nlp.ffzg.hr/resources/models/
48
word order, such as Croatian and Serbian.
Current state of the art for statistical MSD tag-
ging of Croatian is reported at 86.05% (Agic? et
al., 2008). It involves a hidden Markov model tri-
gram tagger CroTag, trained on the Croatia Weekly
100 thousand wordform (100 kw) subcorpus of
Croatian newspaper text from Croatian National
Corpus (Tadic?, 2009), manually MSD-tagged and
lemmatized using the Multext East v3 tagset (MTE
v3) (Erjavec, 2004) and Croatian Lemmatization
Server (Tadic?, 2005) for guided annotation. The
tagger is not publicly available. Just recently, the
Croatia Weekly corpus has been made publicly
available through META-SHARE.2 Another line of
research reports on a prototype constraint grammar
tagger for Croatian (Peradin and S?najder, 2012),
which scores at 86.36% using a MTE-based tagset.
This tagger is also not publicly available as it is in
prototype stage and it currently does not analyze
out-of-vocabulary word forms. The top score for
lemmatizing Croatian text is reported at 96.96%
by combining CroTag and Croatian Morphological
Lexicon (Agic? et al, 2009). The lemmatizer is not
publicly available.
Lemmatization and tagging of Serbian text
was recently addressed in (Gesmundo and
Samardz?ic?, 2012a; Gesmundo and Samardz?ic?,
2012b). It involves BTagger, a combined bidirec-
tional tagger-lemmatizer tool which implements a
lemmatization-as-tagging paradigm. Models are
trained on the Serbian Multext East 1984 corpus,
they are publicly available3 under a permissive li-
cense, reaching overall accuracies of 97.72% for
lemmatization and 86.65% for MSD tagging. It
should be noted, however, that BTagger evaluation
in terms of spatial and temporal complexity was not
documented and that the results provided for Ser-
bian are obtained on specific in-domain data, i.e.,
a corpus of fiction and are thus not directly com-
parable to, e.g., results for Croatian on the Croatia
Weekly newspaper corpus.
Other lines of research in Serbian lemmatization
and tagging exists. Delic? et al (2009) deals with
transformation-based tagging of Serbian text, but
it does not provide state-of-the-art results or freely
available resources. Rule-based approaches to pro-
cessing Serbian using NooJ 4 and similar linguistic
development environments have been thoroughly
2http://metashare.elda.org/
3https://github.com/agesmundo/BTagger
4http://www.nooj4nlp.net/
explored (Vitas et al, 2003). Several resources rel-
evant for Serbian lemmatization and tagging are
provided to the public. The Serbian version of
Jules Verne 60 kw manually lemmatized and MTE-
tagged corpus implements a small deviation from
MTE v4 and deals with specific fictional closed-
vocabulary data. SrpLemKor is a 3.7 Mw corpus of
Serbian newspaper text, automatically lemmatized
and POS-tagged using TreeTagger (Schmid, 1995)
with a tagset of 16 POS tags. A morphological dic-
tionary of 85 thousand Serbian lemmas with sligtly
deviated MTE v4 tagset is available through NooJ.
Public availability of these resources is enabled
through META-SHARE, with somewhat more re-
strictive licensing that involves non-commercial
use in all cases and for some of them it also im-
poses no redistribution.
Related work on lemmatizer and tagger compar-
ison exists for many languages. Restraining the
search to closely related Slavic languages, exten-
sive work in this domain has been done for Bul-
garian (Georgiev et al, 2012), Czech (Spoustova?
et al, 2007) and Slovene (Erjavec and Dz?eroski,
2004; Rupnik et al, 2008). For Croatian, prelim-
inary work on tagger evaluation for tagger voting
has been conducted (Agic? et al, 2010).
3 SETIMES.HR corpus
SETIMES.HR is a new manually lemmatized and
MSD-tagged corpus of Croatian. It is built on top
of the SETimes parallel newspaper corpus involv-
ing 10 languages from the SEE region,5 Croatian
and Serbian included. This initial dataset selection
was deliberate in terms of enabling us with possibil-
ity of cross-lingual annotation projection and other
cross-lingual experiments. SETIMES.HR was anno-
tated by experts using the Croatian Lemmatization
Server (HML)6 (Tadic?, 2005) to facilitate the pro-
cess. We made a number of changes to the initial
annotation provided by human annotators. Namely,
HML provides MSD tags using an undocumented
alteration of the initial MTE tagset, which we cor-
rected to conform entirely to the MTE v4 standard
(Erjavec, 2012). Also, for certain lemmas HML
provides lemmatization with morphosemantic cues
encoded by lemma numbering ? e.g. biti1 (en. to
be) and biti2 (en. to beat) ? which we omitted as
they are used only in the process of generating the
morphological lexicon (Tadic? and Fulgosi, 2003)
5http://www.nljubesic.net/resources/corpora/setimes/
6http://hml.ffzg.hr
49
Corpus Sent?s Tokens Types Lemmas
SETIMES.HR 4 016 89 785 18 089 8 930
set.test.hr 100 2 297 1 270 991
set.test.sr 100 2 320 1 251 981
wiki.test.hr 100 1 887 1 027 802
wiki.test.sr 100 1 953 1 055 795
Table 1: Stats for SETIMES.HR and test sets
and are thus not required for purposes of lemmati-
zation and MSD tagging. We make the resulting 90
kw SETIMES.HR corpus, along with the four test
sets, publicly available under the CC-BY-SA-3.0
license.7 Corpus stats are given in Table 1.
For purposes of this experiment, we propose an
alteration of the baseline MTE v4 tagset in form
of a first version for the MTE v5 standard.8 The
biggest changes in the new version are participal ad-
jectives and adverbs moving from the verbal subset
? which was very complex in v4 ? to the adjectival
and adverbial subsets. Additionally, acronyms are
moved from the abbreviation subset to the noun
subset. A general shrinking of the length of many
tags was performed as well because from v4 on-
wards the MTE standard does not require one tagset
for all languages in the standard. We also suggest
three reductions of the suggested MTE v5 tagset:
1. without adjective definiteness (v5r1),
2. without common (Nc) vs. proper (Np) distinc-
tion for nouns (v5r2) and
3. without both (v5r3).
Adjectival definiteness is a category which is easy
to implement in a morphological lexicon, but is
very hard to distinguish in context as many of its
variants are homographs. We question the distinc-
tion between common and proper nouns as well
since they are contextually very hard to discrimi-
nate. On the other hand, some foreign proper nouns
are inflected by specific paradigms and suffix tries
used on unknown words could profit from this dis-
tinction. Stats for the MTE v5 and the reduced
tagset versions in comparison with the baseline
MTE v4 tagset version of SETIMES.HR are given
in Table 2. They reflect the design choices we
made: MTE v5 has a comparable amount of tags
as MTE v4, gaining additional tags in the adjective
subset, but losing tags in the verb and abbreviation
subsets, while the reductions subsequently lower
the overall MSD tag count.
7http://creativecommons.org/licenses/by-sa/3.0/
8http://nl.ijs.si/ME/V5/msd/html/
set.test wiki.test
Tagset SETIMES.HR hr sr hr sr
MTE v4 660 235 236 188 192
MTE v5 663 233 234 192 195
MTE v5r1 618 213 216 176 180
MTE v5r2 634 216 217 178 181
MTE v5r3 589 196 199 162 166
Table 2: Tagset variation in tag counts
4 Experiment setup
In this section, we define specific experiment goals
and the experiment design. We also present the
datasets and tools used in the experiment.
4.1 Objectives
The principal goal of this experiment is to provide
prospective users with freely available ? download-
able, retrainable and usable, both for research pur-
poses and for commercial use ? state-of-the-art
lemmatization and tagging modules for Croatian
and Serbian. An additional goal of our experi-
ment is to inspect lemmatization and tagging tools
available under permissive licenses and give an
overview regarding their accuracy and time com-
plexity when used on languages of morphological
complexity such as Croatian and Serbian.
Regarding the previously discussed constraints
on existing corpora and tools for Croatian and Ser-
bian tagging and lemmatization, our objective im-
plies exclusive usage of the SETIMES.HR corpus in
the experiment.9 Since SETIMES.HR is part of the
SETimes parallel corpus which, among other lan-
guages, includes both Croatian and Serbian, manu-
ally annotated SETIMES.HR text has a freely avail-
able Serbian equivalent. Our first course of action
was thus to train a number of taggers and lemma-
tizers on SETIMES.HR and test it on Croatian and
Serbian held out text to verify state-of-the-art ac-
curacy on Croatian text and to observe whether
the expected decline in accuracy on Serbian text is
substantial or not.
In case of substantial decrease in accuracy for
lemmatizing and tagging Serbian using Croatian
models, we designed multiple schemes for project-
ing annotation from SETIMES.HR to its Serbian
9Considering corpora of Croatian and Serbian stated in
related work, we chose not to use non-MTE resources and
corpora of fiction as an experiment basis. Importance of en-
coding the full set of morphological features from the MTE
tagset is illustrated by its benefits for dependency parsing of
Croatian (Agic? and Merkler, 2013).
50
equivalent from the SETimes parallel corpus. The
general directions for identifying the bitext sub-
set for annotation projection were using parallel
sentences which have the highest longest common
subsequence or using statistical machine transla-
tion to produce Serbian sentences with minimum
difference to the Croatian counterpart. Projecting
tags on a bitext of high similarity would include
heuristics of annotating the variation with the same
morphosyntactic category if the variation was one
token long or annotating it with the existing model
for tagging if the variation was longer than that.
Lemmatization of the single-token variation would
be reapplied if the token ending in both languages
was identical while other cases would be annotated
with the existing lemmatization model.
4.2 Experiment workflow
We do four batches of experiments:
1. to identify the best available tool and underly-
ing paradigm for lemmatization and tagging
of both languages by observing overall accu-
racy and execution time,
2. to establish the need for annotation projec-
tion from Croatian SETIMES.HR corpus to its
Serbian counterpart,
3. to select the best of the proposed MTE-based
tagsets for both tasks and
4. to provide in-depth evaluation of the selected
top-performing lemmatizer and tagger on both
languages by using the top-performing tagset.
In the first experiment batch, we test the tools only
on Croatian data from SETimes. The second batch
establishes the need for ? or needlessness of ? an-
notation projection for improved processing of Ser-
bian text by testing the tools selected in the first
batch on both languages. The in-depth evaluation
of the third and fourth experiment batch includes,
for both languages and all test sets, observing the
influence of tagset selection to overall accuracy and
investigating tool performance in more detail. We
measure precision, recall and F1 scores for selected
parts of speech and inspect lemmatization and tag-
ging confusion matrices for detailed analysis and
possible prediction of tool operation in real-world
language processing environments.
We aim for the experiment to serve as underly-
ing documentation for enabling prospective users
in implementing more complex natural language
processing systems for Croatian and Serbian by us-
ing these resources. Additionally, the overview of
the usability of tools available is informative for re-
searchers developing basic language technologies
for other languages. We test statistical significance
of observed differences in our results by using the
approximate randomization test.
4.3 Datasets
All models are trained on SETIMES.HR. To at
least partially avoid the possible pitfall of exclu-
sive in-domain testing, we define two test sets for
each language. The first test set consists of 100
Croatian-Serbian parallel sentence pairs taken by
random sampling from the relative complement
of the SETimes parallel corpus and SETIMES.HR.
The second test set is taken from the Croatian and
Serbian Wikipedia by manually selecting 20 match-
ing Wikipedia articles and manually extracting 100
approximate sentence pairs. We chose manual over
random sampling from Wikipedia to account for
the fact that a certain number of articles is virtu-
ally identical between the two Wikipedias due to
language similarity and mutual copying between
Wikipedia users. All four test sets were manually
annotated using the same procedure that was used
for SETIMES.HR. The stats are given in Table 1. In
addition, we have verified the difference between
language test sets by measuring lexical coverage
using HML as a high-coverage morphological lex-
icon of Croatian. For the Croatian SETimes and
Wikipedia samples, we detected 5.2% and 3.9%
out-of-vocabulary word forms and 11.40% and
8.86% were observed for the corresponding Ser-
bian samples, supporting well-foundedness of the
test sets in terms of maintaining the differences
between the two languages.
4.4 Lemmatizers and taggers
As lemmatizers and taggers with permissive licens-
ing schemes and documented cross-lingual state-of-
the-art performance have become largely available,
we chose not to implement our own but to obtain a
set of tools and test them using our data, i.e., train
them on the SETIMES.HR corpus and test them
on Croatian and Serbian SETimes and Wikipedia
test samples. We selected the tools on the basis of
availability and underlying stochastic paradigms as
to identify the best tools and best paradigms.
We tested hidden Markov model trigram taggers
HunPos10 (Hala?csy et al, 2007) and lemmatization-
capable PurePos11 (Orosz and Nova?k, 2012),
10https://code.google.com/p/hunpos/
11https://github.com/ppke-nlpg/purepos
51
Tool Lem. MSD Train (sec) Test (sec)
BTagger 96.22 86.63 24 864.47 87.01
CST 97.78 ? 1.80 0.03
+ lex 97.04 ? 1.87 0.12
HunPos ? 87.11 1.10 0.11
+ lex ? 84.81 10.79 0.45
PurePos 74.40 86.63 5.49 4.42
SVMTool ? 84.99 1 897.08 3.28
TreeTagger 90.51 85.07 7.49 0.19
+ lex 94.12 87.01 17.48 0.31
Table 3: Preliminary evaluation
lemmatization-capable decision-tree-based Tree-
Tagger12 (Schmid, 1995), support vector machine
tagger SVMTool13 (Gime?nez and Ma`rquez, 2004)
and CST?s14 data-driven rule-based lemmatizer (In-
gason et al, 2008). Keeping in mind the previously
mentioned state-of-the-art scores on Serbian 1984
corpus and statistical lemmatization capability, we
also tested BTagger (Gesmundo and Samardz?ic?,
2012a; Gesmundo and Samardz?ic?, 2012b). Since
some lemmatizers and taggers are capable of using
an external morphological lexicon, we used a MTE
v5r1 version of Apertium?s lexicon of Croatian15
(Peradin and Tyers, 2012) where applicable.16 All
tools are well-documented and successfully applied
across languages, as indicated in related work.
5 Results and discussion
A discussion of the experiment results follows in
the next four subsections. Each subsection repre-
sents one batch of experiments. First we select the
best lemmatizer and tagger, next we check for a
need of annotation projection to the Serbian corpus,
then the best MTE-based tagset using the best tool
combination. Finally we provide a more detailed
insight into the results of the top-performing pair
of selected tools and tagset.
5.1 Tool selection
Results of the first experimental batch, consisting
of testing the selected set of lemmatizers and tag-
gers on the MTE v5r1 version of Croatian SETimes
test set, are given in Table 3. In terms of lemmati-
12http://www.cis.uni-muenchen.de/ schmid/tools/TreeTagger/
13http://www.lsi.upc.edu/ nlp/SVMTool/
14http://cst.dk/online/lemmatiser/uk/
15http://www.apertium.org/
16As with already existing Croatian annotated corpora,
HML is not fully MTE compliant. For future work, we might
utilize a compliant version in our experiment and resulting
models, being that its coverage is generally greater than the
one of Apertium?s lexicon due to size difference.
set.test wiki.test
POS hr sr hr sr
HunPos 97.04 95.47 94.25 96.46
+ lex 96.60 95.09 94.62 95.58
MSD
HunPos 87.11 85.00 80.83 82.74
+ lex 84.81 81.59 78.49 79.20
Table 4: Overall tagging accuracy with and without
the inflectional lexicon
set.test wiki.test
Model hr sr hr sr
CST 97.78 95.95 96.59 96.30
+ lex 97.04 95.52 96.38 96.61
Table 5: Overall lemmatization accuracy with and
without the inflectional lexicon
zation and tagging accuracy as well as processing
speed in both training and testing, the top perform-
ing tools are CST lemmatizer and HunPos tagger.
Thus, we chose these two for further investigation
in the following batches of experiments. It should
be noted that, even though its performance is com-
parable to the one of CST and HunPos, BTagger
was not chosen for the other batches primarily be-
cause of its temporal complexity, as it is orders of
magnitude higher than for the selected tools. Given
that lemmatization and tagging are considered pre-
requisites for further processing of text tata, the
data itself often being fed to these modules in large
quantities (e.g., web corpora), we insist on the sig-
nificance of temporal complexity in tool selection.
The other results are comparable with previous re-
search in tagging Croatian. Where applicable, we
tried assisting the tools by providing Apertium?s
lexicon as an optional input for improved lemma-
tization and tagging. Only TreeTagger lemmatiza-
tion and tagging benefited from lexicon inclusion.
However, it should be noted that TreeTagger imple-
ments a very simple approach to lemmatization, as
it only performs dictionary matching and does not
lemmatize unknown words. Inclusion of a larger
lexicon such as HML might be more beneficial for
all the tools.
5.2 Annotation projection
HunPos tagging accuracy on all Croatian and Ser-
bian test sets for both POS only and full MSD is
given in Table 4 for the default variant and for the
52
Tagset set.test wiki.test
POS hr sr hr sr
MTE v4 96.08 94.61 93.96 95.85
MTE v5 97.04 95.52 94.30 96.40
MTE v5r1 97.04 95.47 94.25 96.46
MTE v5r2 97.00 95.60 94.20 96.30
MTE v5r3 97.13 95.56 94.09 96.15
MSD
MTE v4 86.24 83.45 80.45 81.98
MTE v5 86.77 84.48 80.46 82.43
MTE v5r1 87.11 85.00 80.83 82.74
MTE v5r2 87.11 84.96 81.20 82.38
MRE v5r3 87.72 85.56 81.52 82.79
Table 6: HunPos POS and MSD tagging accuracy
for all tagsets
set.test wiki.test
Tagset hr sr hr sr
MTE v4 97.78 95.82 96.66 96.11
MTE v5 97.82 95.86 96.81 96.30
MTE v5r1 97.78 95.95 96.59 96.30
MTE v5r2 97.87 95.99 96.75 96.20
MTE v5r3 97.74 95.99 96.54 96.20
Table 7: CST lemmatization accuracy for all tagsets
one using Apertium?s lexicon. These results serve
as the first decision point regarding the need for
Croatian-to-Serbian annotation projection, the sec-
ond one being the lemmatization scores in Table
5. Here we observed an unsubstantial decrease
in POS and MSD tagging between Croatian and
Serbian test sets ? the observed difference is, in
fact, more substantial across domains than across
languages. Overall, Croatian and Serbian scores
differ less than 3%. Results for Serbian Wikipedia
sample are even consistently better than for Croa-
tian Wikipedia, emphasizing domain significance
over language difference. The tagger does not ben-
efit from the inclusion of the inflectional lexicon
in POS tagging and it even incurs a substantial 2%
to 4% penalty in MSD tagging. Since such obser-
vations were not made while including the lexicon
with the TreeTagger tool ? which implements the
simplest form of dictionary lemmatization ? we
performed a small results analysis and noticed an
unnaturally high percentage of categories that are
as expected present in the lexicon, but very rare in
the training corpus (like the vocative case) point-
ing to a na??ve implementation of the procedure.
Thus we chose not to use the lexicon in further
observations. Lack of more substantial differences
Tagsets v5 v5r1 v5r2 v5r3
v4 0.268 <0.05 <0.05 <0.01
v5 / <0.01 <0.05 <0.01
v5r1 / / 0.877 <0.05
v5r2 / / / <0.01
Table 8: Statistical significance of differences in
full MSD tagging between tagsets (p-values using
approximate randomization)
in tagging scores between Croatian and Serbian
for this specific test scenario implied no need for
annotation projection.
This is further supported by overall lemmatiza-
tion scores in Table 5. Even with the observed
lexical differences between the languages, as we
indicated in the description of the test sets by mea-
suring lexical coverage using HML, the learned
CST lemmatizer rules are more robust consider-
ing language alteration than the trigram tagging
model of HunPos. Lemmatization accuracy stays
in the margins of approximately 97%?1% for both
languages. Average accuracy on Croatian is less
than 2% higher than for Serbian and the domain
patterns observed for tagging are also observed for
lemmatization. Benefits of an inflectional lexicon
for lemmatization are minor, if any, which can be
followed back to the small size of the lexicon and
high quality of the CST lemmatizer. On the con-
trary, TreeTagger?s simple lemmatization does gain
four points by using the lexicon, but it initially
performs seven points worse than CST.
5.3 Tagset selection
Tables 6 and 7 show the influence of tagset de-
sign on tagging and lemmatization accuracy. They
are accompanied by Table 8, i.e., results of testing
statistical significance of differences between the
tagsets in the task of full MSD tagging from Table 6.
Statistical significance is calculated with all test
sets merged into one. Differences in lemmatization
accuracy are virtually non-existent regarding the
tagset choice. Full MSD tagging follows the usual
pattern of inverse proportionality between tagset
size and overall accuracy. It should be noted that
MTE v5 accuracy is not significantly higher than
MTE v4 accuracy (p = 0.268), but we consider the
new tagset to be easier to use for humans since its
tags are shortened by removing placehodlers for
features used in other MTE languages. Consider-
ing that only tagging accuracy using the MTE v5r3
tagset is significantly better than tagging using all
53
Croatian Serbian
POS P R F1 P R F1
Adj 94.33 90.14 92.19 94.34 93.98 94.16
66.80 63.83 65.28 66.79 66.54 66.66
Adv 84.56 82.73 83.63 82.57 73.77 77.92
84.56 82.73 83.63 82.57 73.77 77.92
Conj 95.29 93.82 94.55 97.92 95.29 96.59
94.12 92.66 93.38 96.89 94.28 95.57
Noun 95.70 96.34 96.02 95.42 96.59 96.00
76.78 77.30 77.04 75.38 76.30 75.84
Num 94.57 97.75 96.13 96.51 93.26 94.86
91.30 94.38 92.81 94.19 91.01 92.57
Prep 98.10 99.72 98.90 98.45 98.70 98.57
95.93 97.52 96.72 94.30 94.55 94.42
Pron 95.97 97.54 96.75 95.78 97.42 96.59
81.85 83.20 82.52 81.43 82.83 82.12
Verb 95.88 98.07 96.96 95.23 95.72 95.47
93.81 95.96 94.87 93.36 93.84 93.60
Table 9: Precision (P), recall (R) and F1 score for
POS only (1st column) and full MSD (2nd column)
on Croatian and Serbian
other suggested tagsets, we chose this tagset and
tagging model for further observation of lemmatiza-
tion and tagging properties in the remainder of the
paper. Still, in this section, we present the results
on all tagsets to serve as underlying documentation
of the observed differences, mainly because of the
fact that only MTE v4 is officially supported at
this moment and MTE v5 is a newly-introduced
prototype that displays better performance in this
specific experiment.
5.4 In-depth analysis
In Table 9 we merge SETimes and Wikipedia test
sets by language and provide POS and MSD tag-
ging precision, recall and F1 score for selected
Croatian and Serbian parts of speech. In terms of
POS only, the most difficult-to-tag part of speech
is the adverb, followed by the adjective in both
Croatian and Serbian. The other categories are
consistently POS-tagged with an F1 score of ap-
proximately 95% or higher. The decrease for ad-
verbs and adjectives is somewhat more evident in
precision than in recall and the POS confusion ma-
trix for both languages, given in Table 10, shows
that these two parts of speech are often mistaken
for each other by the tagger. Regarding full MSD
tagging using the MTE v5r3 tagset, for both lan-
guages, the lowest F1 scores are observed for ad-
jectives (approximately 66%), nouns (76%) and
pronouns (82%). This is most likely due to the fact
that these parts of speech have the largest tagset
subsets, making it easier for the tagger to get con-
fused.17 Performance for other parts of speech is
satisfactory, especially for verbs, keeping in mind,
e.g., possible subsequent dependency parsing of the
two languages. The absolute difference between
POS and MSD tagging score is most substantial
for adjectives (approximately 27%), indicating that
certain MSD features might be triggering the de-
crease. This is partially supported by our tagset
design investigation as dropping adjective definite-
ness atribute yielded substantial overall tagging
accuracy increase when compared with the tagsets
in which this attribute is still encoded.
In Table 10 we provide a part of speech confu-
sion matrix for Croatian and Serbian on test sets
merged by language. In Croatian test sets, the most
frequent confusions are those between adjectives
and nouns (28.9%), nouns and verbs (14.5%), ad-
jectives and adverbs (11.6%) and nouns and ad-
verbs (6.9%). In Serbian text, the tagger most fre-
quently confuses nouns ? for adjectives (21.1%),
verbs (20%) and adverbs (16%). Merging the test
sets by language mostly evens out the tagging dif-
ferences as there is a total of 173 MSD confusions
in Croatian test sets and only 3 more, i.e., 175 in
the Serbian test sets.
POS scores for both languages neared the level
of human error in our experiment. Keeping that
in mind, upon observing the confusion instances
themselves, we spotted a confusion between adjec-
tives and nouns (e.g. names of countries (Hrvatska
(en. Croatia, Croatian)), homographic forms
(strana (en. foreign, side), svet (en. world, holy))
and confusion between adjectives and adverbs. Ad-
verbs and prepositions are sometimes confused
with nouns, especially for nouns in instrumental
case (e.g. godinama (en. year, yearly), tijekom
(en. duration, during)). Conjuctions are at times
incorrently tagged because various words can have
a conjuctional function, most frequently pronouns
and adverbs: s?to (en. what), kako (en. how), kada
(en. when). Interestingly, there is some confusion
between nouns and verbs in Wikipedia test sets,
while in SETimes test sets there are almost none.
This confusion arises from the homographic forms
? e.g. mora (en. must, seas) ? or from nouns with
17There are 589 MTE v5r3 tags in SETIMES.HR. Out of
these, 164 are used for tagging adjectives, 42 for nouns and
268 for pronouns, thus accounting for 80.47% of the tagset.
There are also 50 verb tags.
54
POS Abbr Adj Adv Conj Noun Num Part Prep Pron Res Verb
Abbr 0 0 0 1 3 0 0 0 0 0
Adj 0 20 0 50 0 1 0 3 1 4
Adv 0 10 9 12 0 0 2 0 0 2
Conj 0 0 5 2 0 5 5 7 0 0
Noun 0 37 28 0 4 0 1 5 7 25
Num 2 4 0 0 2 0 0 0 0 0
Part 0 0 0 3 0 0 0 0 0 3
Prep 0 0 2 3 2 0 1 0 0 0
Pron 0 2 1 9 3 0 1 0 0 1
Res 0 0 1 0 4 0 0 2 0 0
Verb 0 9 4 0 35 1 2 1 0 1
Table 10: POS confusion matrix for Croatian (top right) and Serbian (bottom left)
Figure 1: Learning curves for Croatian and Serbian lemmatization and tagging
suffixes -la and -lo, which are used for denoting
participles in feminine and neuter gender, or with
suffix -ti, which is also a suffix for infinitive.
Most MSD tag confusions arise from the fact that
the same suffix can denote different cases in dif-
ferent declensions. We observed confused number
and gender category (mostly in adjectives in mas-
culine and neuter gender), but the most frequent
confusion occurs for accusative forms in masculine
gender, which have different suffixes when they de-
note animacy (suffix is the same as in the genitive
case: pobjednika (en. winner), kandidata (en. can-
didate)) and when they denote inanimacy (suffix
is the same as in the nominative case: metak (en.
bullet), bubnjar (en. drummer)).
In lemmatization, as in POS tagging, errors are
generally very infrequent. Some occur with adjec-
tives, when an assigned lemma represents a definite
form of an adjective, instead of an indefinitive form
(and less frequently vice versa). Besides, adjec-
tives are sometimes confused with adverbs (e.g.,
target lemma is znac?ajno (en. significantly), but
the lemma znac?ajan (en. significant) is assigned,
and vice versa). Other less frequent examples in-
clude cases in which the assigned lemma is not in
its canonical form, but a case other than the nomi-
native case, or when the assigned lemma is a word
stem. A small number of errors also occurs due
to slight differences in Croatian and Serbian word-
forms, e.g., when a Serbian nominative form is not
a nominative form in Croatian (planeta as Serbian
nominative and Croatian genitive, planet being the
Croatian nominative).
Figure 1 provides lemmatization, POS and MSD
tagging learning curves for both languages on
merged test sets. Apart from the slight difference
in lemmatization scores in favor of Croatian, the
learning curves and overall scores on merged test
sets are virtually identical. The easiest task to learn
is lemmatization while the most complex one is
applying MSD.
6 Conclusions and future work
In this paper, we have addressed the issue of lemma-
tization and morphosyntactic tagging of two gener-
ally under-resourced languages, Croatian and Ser-
bian. Our goal was to provide the general public
with freely available language resources and state-
55
of-the-art models for lemmatization and tagging of
these two languages in terms of accuracy, robust-
ness and speed. We also aimed at using lemmati-
zation and tagging as a platform for implicit com-
parison of the two languages in natural language
processing terms, as to provide partial insight to
how difficult and lossy ? or, more desireably, how
easy and straightforward ? would it be to port lin-
guistic resources and language processing tools
from one language to another.
While developing the models, we completed a
series of experiments. We used the Croatian text
from the freely available SETimes parallel corpus
to create a new manually lemmatized and mor-
phosyntactically tagged corpus of Croatian ? the
SETIMES.HR corpus. Beside the Multext East v4
morphosyntactic tagset specification for Croatian
which was used for initial corpus annotation, we
designed and implemented a first version of the
Multext East v5 tagset and its three reductions and
applied these to SETIMES.HR. Using SETimes
and Wikipedia as starting point resources, we cre-
ated two gold standard test sets for each language
in order to test existing state-of-the-art lemmatiz-
ers and taggers. We ran preliminary tests on a
number of tools to select CST lemmatizer and
HunPos tagger as tools of choice considering ob-
served accuracy, training time and text processing
time. In an in-depth evaluation of these tools, we
obtained peak overall lemmatization accuracy of
97.87% and 96.30% for Croatian and Serbian and
full morphosyntactic tagging accuracy of 87.72%
and 85.56%, with basic part of speech tagging ac-
curacy at 97.13% and 96.46%. In this specific test
scenario and with this specific training set, we have
shown the differences in results between Croatian
and Serbian not to be significant enough to justify
an effort in more elaborate strategy of adapting
Croatian models to Serbian data ? simply training
the models on Croatian text from SETIMES.HR
corpus and using them on Serbian text provided
state-of-the-art results in lemmatization and tag-
ging, while maintaining and even topping previ-
ously documented state of the art for Croatian.
The SETIMES.HR corpus, Croatian and Serbian
test sets and top-performing lemmatization and tag-
ging models are publicly available and freely down-
loadable18 under the CC-BY-SA-3.0 license.
Our future work plans include both enlarging
and enhancing SETIMES.HR. The presented learn-
18http://nlp.ffzg.hr/resources/models/
ing curves show significant room for improvement
by annotating additional data. The dataset aleady
serves as a basis for the SETIMES.HR treebank of
Croatian (Agic? and Merkler, 2013), implementing
a novel dependency syntactic formalism and en-
abling experiments with joint dependency parsing
of Croatian and Serbian. Should dependency pars-
ing experiments show the need for more elaborate
language adaptation strategies, we will most likely
implement them also on the level of lemmas and
morphosyntactic tags before addressing syntactic
issues. This will possibly be helped by statistical
machine translation between Croatian and Serbian
to enhance bitext similarity and empower projec-
tion strategies. An effort could be made to adapt
existing Croatian and Serbian resources and subse-
quently to attempt achieving better lemmatization
and tagging performance by combining these with
SETIMES.HR. We will use the models presented in
this paper to annotate the web corpora of Croatian
and Serbian (Ljubes?ic? and Erjavec, 2011) ? hrWaC
and srWaC.
Acknowledgement
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 un-
der grant agreement n? PIAP-GA-2012-324414
(project Abu-MaTran).
References
Z?eljko Agic? and Danijela Merkler. 2013. Three Syn-
tactic Formalisms for Data-Driven Dependency Pars-
ing of Croatian. In Text, Speech and Dialogue. Lec-
ture Notes in Computer Science. Springer.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving Part-of-Speech Tagging Accuracy
for Croatian by Morphological Analysis. Informat-
ica, 32(4):445?451.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2009. Evaluating Full Lemmatization of Croatian
Texts. In Recent Advances in Intelligent Information
Systems, pages 175?184. Exit Warsaw.
Z?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2010. Tagger Voting Improves Morphosyntactic
Tagging Accuracy on Croatian Texts. In Proceed-
ings of ITI, pages 61?66.
Thorsten Brants. 2000. TnT: A Statistical Part-of-
Speech Tagger. In Proceedings of ANLP, pages 224?
231.
56
Vlado Delic?, Milan Sec?ujski, and Aleksandar Ku-
pusinac. 2009. Transformation-Based Part-of-
Speech Tagging for Serbian Language. In Proceed-
ings of CIMMACS.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine
Learning of Morphosyntactic Structure: Lemmatiz-
ing Unknown Slovene Words. Applied Artificial In-
telligence, 18:17?41.
Tomaz? Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In Proceedings of LREC.
Tomaz? Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic Resources for Central and Eastern European
Languages. Language Resources and Evaluation,
46(1):131?142.
Georgi Georgiev, Valentin Zhikov, Kiril Simov, Petya
Osenova, and Preslav Nakov. 2012. Feature-Rich
Part-of-speech Tagging for Morphologically Com-
plex Languages: Application to Bulgarian. In Pro-
ceedings of EACL, pages 492?502.
Andrea Gesmundo and Tanja Samardz?ic?. 2012a. Lem-
matisation as a Tagging Task. In Proceedings of
ACL.
Andrea Gesmundo and Tanja Samardz?ic?. 2012b. Lem-
matising Serbian as Category Tagging with Bidirec-
tional Sequence Classification. In Proceedings of
LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS Tagger Generator Based on Support
Vector Machines. In Proceedings of LREC.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007. HunPos: An Open Source Trigram Tagger.
In Proceedings of ACL, pages 209?212.
Anton Karl Ingason, Sigru?n Helgado?ttir, Hrafn Lofts-
son, and Eir??kur Ro?gnvaldsson. 2008. A Mixed
Method Lemmatization Algorithm Using a Hierar-
chy of Linguistic Identities (HOLI). In Proceedings
of GoTAL, pages 205?216.
Nikola Ljubes?ic? and Tomaz? Erjavec. 2011. hrWaC
and slWaC: Compiling Web Corpora for Croatian
and Slovene. In Text, Speech and Dialogue, pages
395?402. Springer.
Gyo?rgy Orosz and Attila Nova?k. 2012. PurePos ?
An Open Source Disambiguator. In Proceedings of
NLPCS.
Hrvoje Peradin and Jan S?najder. 2012. Towards a
Constraint Grammar Based Morphological Tagger
for Croatian. In Text, Speech and Dialogue, pages
174?182. Springer.
Hrvoje Peradin and Francis M. Tyers. 2012. A Rule-
Based Machine Translation System from Serbo-
Croatian to Macedonian. In Proceedings of
FREERBMT12, pages 55?65.
Jan Rupnik, Miha Grc?ar, and Tomaz? Erjavec. 2008.
Improving Morphosyntactic Tagging of Slovene
Language Through Meta-Tagging. Informatica,
32(4):437?444.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging With an Application to German. In
Proceedings of ACL SIGDAT Workshop.
Anders S?gaard. 2011. Semi-Supervised Condensed
Nearest Neighbor for Part-of-Speech Tagging. In
Proceedings of ACL-HLT, pages 48?52.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan
Votrubec, Pavel Krbec, and Pavel Kve?ton?. 2007.
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech. In Proceedings
of BSNLP, pages 67?74.
Marko Tadic? and Sanja Fulgosi. 2003. Building the
Croatian Morphological Lexicon. In Proceedings of
EACL 2003 Workshop on Morphological Processing
of Slavic Languages, pages 41?46.
Marko Tadic?. 2005. Croatian Lemmatization Server.
Southern Journal of Linguistics, 29(1):206?217.
Marko Tadic?. 2009. New Version of the Croatian Na-
tional Corpus. After Half a Century of Slavonic Nat-
ural Language Processing, pages 199?205.
Dus?ko Vitas, Cvetana Krstev, Ivan Obradovic?,
Ljubomir Popovic?, and Gordana Pavlovic?-Laz?etic?.
2003. An Overview of Resources and Basic Tools
for Processing of Serbian Written Texts. In Pro-
ceedings of the Workshop on Balkan Language Re-
sources, 1st Balkan Conference in Informatics.
57
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 69?77,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Identifying False Friends between Closely Related Languages
Nikola Ljubes?ic?
Faculty of Humanities and Social Sciences
University of Zagreb
Ivana Luc?ic?a 3
10000 Zagreb, Croatia
nikola.ljubesic@ffzg.hr
Darja Fis?er
Faculty of Arts
University of Ljubljana
As?kerc?eva 2
1000 Ljubljana, Slovenija
darja.fiser@ff.uni-lj.si
Abstract
In this paper we present a corpus-based ap-
proach to automatic identification of false
friends for Slovene and Croatian, a pair
of closely related languages. By taking
advantage of the lexical overlap between
the two languages, we focus on measuring
the difference in meaning between iden-
tically spelled words by using frequency
and distributional information. We ana-
lyze the impact of corpora of different ori-
gin and size together with different associ-
ation and similarity measures and compare
them to a simple frequency-based base-
line. With the best performing setting
we obtain very good average precision of
0.973 and 0.883 on different gold stan-
dards. The presented approach works on
non-parallel datasets, is knowledge-lean
and language-independent, which makes it
attractive for natural language processing
tasks that often lack the lexical resources
and cannot afford to build them by hand.
1 Introduction
False friends are words in two or more languages
that are orthographically or semantically similar
but do not have the same meaning, such as the
noun burro, which means butter in Italian but don-
key in Spanish (Allan, 2009). For that reason, they
represent a dangerous pitfall for translators, lan-
guage students as well as bilingual computer tools,
such as machine translation systems, which would
all benefit greatly from a comprehensive collection
of false friends for a given language pair.
False friends between related languages, such
as English and French, have been discussed by
lexicographers, translators and language teachers
for decades (Chaco?n Beltra?n, 2006; Granger and
Swallow, 1988; Holmes and Ramos, 1993). How-
ever, they have so far played a minor role in NLP
and have been almost exclusively limited to par-
allel data (Inkpen et al, 2005; Nakov and Nakov,
2009). In this paper we tackle the problem of auto-
matically identifying false friends in weakly com-
parable corpora by taking into account the distri-
butional and frequency information collected from
non-parallel texts.
Identifying false friends automatically has the
same prerequisite as the problem of detecting
cognates ? identifying similarly (and identically)
spelled words between two languages, which is far
from trivial if one takes into account the specificity
of inter-language variation of a specific language
pair. In this contribution we focus on the prob-
lem of false friends on two quite similar languages
with a high lexical overlap ? Croatian and Slovene
? which enables us to circumvent the problem of
identifying similarly spelled words and use identi-
cal words only as the word pair candidate list for
false friends.
Our approach to identifying false friends relies
on two types of information extracted from cor-
pora. The first one is the frequency of a false friend
candidate pair in the corresponding corpora where
the greater the difference in frequency, the more
certain one can be that the words are used in dif-
ferent meanings. The second information source is
the context from corresponding corpora where the
context dissimilarity of the two words in question
is calculated through a vector space model.
The paper is structured as follows: in Section 2
we give an overview of the related work. In Sec-
tion 3 we describe the resources we use and in Sec-
tion 4 we present the gold standards used for eval-
uation. Section 5 describes the experimental setup
and Section 6 reports on the results. We conclude
the paper with final remarks and ideas for future
work.
69
2 Related Work
Automatic detection of false friends was initially
limited to parallel corpora but has been extended
to comparable corpora and web snippets (Nakov et
al., 2007). The approaches to automatically iden-
tify false friends fall into two categories: those that
only look at orthographic features of the source
and the target word, and those that combine ortho-
graphic features with the semantic ones.
Orthographic approaches typically rely on com-
binations of a number of orthographic similarity
measures and machine learning techniques to clas-
sify source and target word pairs to cognates, false
friends or unrelated words and evaluate the differ-
ent combinations against a manually compiled list
of legitimate and illegitimate cognates. This has
been attempted for English and French (Inkpen et
al., 2005; Frunza and Inkpen, 2007) as well as
for Spanish and Portuguese (Torres and Alu??sio,
2011).
Most of the approaches that combine ortho-
graphic features with the semantic ones have been
performed on parallel corpora where word fre-
quency information and alignments at paragraph,
sentence as well as word level play a crucial role at
singling out false friends, which has been tested on
Bulgarian and Russian (Nakov and Nakov, 2009).
Work on non-parallel data, on the other hand, of-
ten treats false friend candidates as search queries,
and considers the retrieved web snippets for these
queries as contexts that are used to establish the
degree of semantic similarity of the given word
pair (Nakov and Nakov, 2007).
Apart from the web snippets, comparable cor-
pora have also been used to extract and clas-
sify pairs of cognates and false friends between
English and German, English and Spanish, and
French and Spanish (Mitkov et al, 2007). In
their work, the traditional distributional approach
is compared with the approach of calculating n-
nearest neighbors for each false friend candidate in
the source language, translating the nearest neigh-
bors via a seed lexicon and calculating the set in-
tersection to the N nearest neighbors of the false
friend candidate from the target language.
A slightly different setting has been investigated
by Schultz et al (2004) who built a medical do-
main lexicon from a closely related language pair
(Spanish-Portuguese) and used the standard distri-
butional approach to filter out false friends from
cognate candidates by catching orthographically
most similar but contextually most dissimilar word
pairs.
The feature weighting used throughout the re-
lated work is mostly plain frequency with one
case of using TF-IDF (Nakov and Nakov, 2007)
whereas cosine is the most widely used similar-
ity measure (Nakov and Nakov, 2007; Nakov and
Nakov, 2009; Schulz et al, 2004) while Mitkov
et al (2007) use skew divergence which is very
similar to Jensen-Shannon divergence.
The main differences between the work we re-
port on in this paper and the related work are:
1. we identify false friends on a language pair
with a large lexical overlap ? hence we can
look for false friends only among identically
spelled words, such as boja, which means
buoy in Slovene but colour in Croatian, and
not among similarly spelled words, such as
the Slovene adjective buc?en (made of pump-
kins and noisy) and its Croatian counterpart
buc?an (only noisy);
2. we inspect multiple association and similarity
measure combinations on two different cor-
pora pairs, which enables us to assess the sta-
bility of those parameters in the task at hand;
3. we work on two different corpora pairs which
we have full control over (that is not the case
with web snippets), and are therefore able to
examine the impact of corpus type and corpus
size on the task;
4. we use three categories for the identically
spelled words:
(a) we use the term true equivalents (TE)
to refer to the pairs that have the same
meaning and usage in both languages
(e.g. adjective bivs?i, which means for-
mer in both languages),
(b) the term partial false friends (PFF) de-
scribes pairs that are polysemous and
are equivalent in some of the senses but
false friends in others (e.g. verb draz?iti,
which can mean either irritate or make
more expensive in Slovene but only irri-
tate in Croatian), and
(c) we use the term false friends (FF) for
word pairs which represent different
concepts in the two languages (e.g. noun
slovo, which means farewell in Slovene
and letter of the alphabet in Croatian)
70
By avoiding the problem of identifying relevant
similarly spelled words prior to the identification
of false friends, in this paper we focus only on the
latter and avoid adding noise from the preceding
task.
3 Resources Used
In this paper we use two types of corpora:
Wikipedia corpora (hereafter WIKI) which have
gained in popularity lately because of their sim-
ple construction and decent size and web corpora
(hereafter WAC) which are becoming the standard
for building big corpora.
We prepared the WIKI corpora from the dumps
of the Croatian and Slovene Wikipedias by ex-
tracting their content, tokenizing and annotat-
ing them with morphosyntactic descriptions and
lemma information. The web corpora of Croat-
ian and Slovene were built in previous work of
Ljubes?ic? and Erjavec (2011). They were created
by crawling the whole top-level Slovene and Croa-
tian domains and applying generic text extraction,
language identification, near-duplicate removal,
linguistic filtering and morphosyntactic annotation
and lemmatization.
In terms of content, it is to expect that web cor-
pora are much richer genre-wise while articles in
Wikipedia corpora all belong to the same genre.
As far as topics are concerned, web corpora are
believed to be more diverse but contain a less uni-
form topic distribution than Wikipedia corpora.
Finally, it is to expect that Wikipedia corpora con-
tain mostly standard language while web corpora
contain a good portion of user-generated content
and thereby non-standard language as well.
Some basic statistical information on the cor-
pora is given in Table 1.
CORPUS MWORDS MTOKENS DOC #
HR.WIKI 31.21 37.35 146,737
SL.WIKI 23.47 27.85 131,984
HRWAC 787.23 906.81 2,550,271
SLWAC 450.06 525.55 1,975,324
Table 1: Basic statistics about the corpora used
Both types of corpora are regularly used in to-
day?s NLP research and one of the tasks of this
paper is to compare those two not only in relation
to the specific task of false friends identification,
but on a broader scale of exploiting their contex-
tual and frequency information as well.
4 Gold Standards
The gold standards for this research were built
from identically spelled nouns, adjectives and
verbs that appeared with a frequency equal or
higher than 50 in the web corpora for both lan-
guages.
The false friend candidates were categorized in
the three categories defined in Section 2: false
friends, partial false friends and true equivalents.
Manual classification was performed by three
annotators, all of them linguists. Since identify-
ing false friends is hard even for a well-trained lin-
guist, all of them consulted monolingual dictionar-
ies and corpora for both languages before making
the final decision.
The first annotation session was performed by a
single annotator only. Out of 8491 candidates, he
managed to identify 117 FFs, 110 PFFs and 8264
(97.3%) TEs. All the identified FFs and PFFs as
well as 380 TEs were then given to two more an-
notators, shrinking the dataset to be annotated by
the other two annotators down to 607 entries, i.e.
to only 7% of the initial dataset. The agreement
between all three annotators on the smaller dataset
is given in Table 2.
ANNOTATORS INTERSECTION KAPPA
A1 A2 0.766 0.549
A1 A3 0.786 0.598
A2 A3 0.743 0.501
average 0.765 0.546
Table 2: Inter-annotator agreement on building the
gold standards
The obtained average kappa inter-annotator
agreement is considered moderate and proves the
problem to be quite complex, even for humans
well trained in both languages with all the avail-
able resources at hand. Since we did not have
sufficient resources for all the annotators to re-
vise their divergent annotations, we proceeded by
building the following two gold standards:
1. the first gold standard (GOLD1) contains only
FFs and TEs on which all the three annotators
agreed (60 FFs and 324 TEs) and
2. the second gold standard (GOLD2) contains
all entries where at least the first and one of
the other two annotators agreed (81 FFs, 33
PFFs and 351 TEs).
71
We consider GOLD1 to be simpler and cleaner
while GOLD2 contains the full complexity of the
task at hand.
5 Experimental Setup
We experimented with the following parameters:
corpus type, corpus size, association measure for
feature weighting, similarity measure for compar-
ing context vectors and gold standard type.
We ran our experiments on two pairs of corpora:
1. one pair originating from local Wikipedia
dumps (WIKI) and
2. one pair originating from the top-level-
domain web corpora of the two languages
(WAC)
We took under consideration the following as-
sociation measures:
1. TF-IDF (TF-IDF) is well known from infor-
mation retrieval but frequently applied on
other problems as well; we consider context
vectors to be information entities and calcu-
late the IDF statistic for a term t and vector
set V as follows:
IDF (t, V ) = log
|V |
|{v ? V : t ? v}|
2. log-likelihood (LL) (Dunning, 1993) which
has proven to perform very well in a num-
ber of experiments on lexicon extraction i.e.
finding words with the most similar context,
performing similarity well as TF-IDF and
3. discounted log-odds (LO) first used in lexicon
extraction by Laroche and Langlais (2010),
showing consistently better performance than
LL; it is calculated from contingency table in-
formation as follows:
LO = log
(O11 + 0.5)(O22 + 0.5)
(O12 + 0.5)(O21 + 0.5)
The following similarity measures were taken
into account:
1. the well-known cosine measure (COSINE),
2. the Dice measure (DICE), defined in (Otero,
2008) as DiceMin, which has proven to be
very good in various tasks of distributional
semantics (v1f is the feature weight of feature
f in vector v1):
DICE(v1, v2) =
2 ?
?
f min(v1f , v2f )
?
f v1f + v2f
3. and the Jensen-Shannon divergence (JEN-
SHAN) which shows consistent performance
on various tasks:
JS(v1, v2) =
KL(v1|v2)
2
+
KL(v2|v1)
2
KL(v1|v2) =
?
f
v1f log
v1f
v1f + v2f
We used the standard approach for extracting
context and building context vectors and calcu-
lated the frequency distribution of three content
words to the left and to the right of the head-
word without encoding their position. We did not
perform any cross-lingual feature projection via a
seed lexicon or similar, but relied completely on
the lexical overlap between the two similar lan-
guages.
Apart from the context and its dissimilarity,
there is another, very fundamental source of in-
formation that can be used to assess the difference
in usage and therefore meaning ? the frequency
of the word pair in question in specific languages.
That is why we also calculated pointwise mutual
information (PMI) between candidate pairs.
PMI(w1, w2) = log
p(w1, w2)
p(w1) ? p(w2)
We estimated the joint probability of the two
words by calculating the maximum likelihood esti-
mate of the identically spelled word on the merged
corpora. We considered this measure to be a strong
baseline. For a weak baseline we took a random
ordering of pairs of words (RANDOM).
Since the result of the procedure of identifying
false friends in this setting is a single ranked list
of lemma pairs where the ranking is performed
by contextual or frequency dissimilarity, the same
evaluation method can be applied as to evaluating
a single query response in information retrieval.
That is why we evaluated the output of each setting
with average precision (AP), which averages over
all precisions calculated on lists of false friend
candidates built from each positive example up-
wards.
72
As three categories were encoded in the GOLD2
gold standard, we weighted FFs with 1, TEs with
0 and PFFs with 0.5. In the GOLD1 gold standard
FFs were, naturally, weighted with 1 and TEs with
0.
6 Results
In our initial set of experiments we ran a Cartesian
product on the sets of corpora types, gold stan-
dards, association measures and similarity mea-
sures. The results of those experiments are given
in Table 3.
WIKI
GOLD1 COSINE DICE JENSHAN
TF-IDF 0.326 0.349 0.337
LL 0.333 0.401 0.355
LO 0.340 0.539 0.434
PMI 0.634
GOLD2 COSINE DICE JENSHAN
TF-IDF 0.376 0.392 0.380
LL 0.390 0.440 0.406
LO 0.442 0.561 0.470
PMI 0.581
WAC
GOLD1 COSINE DICE JENSHAN
TF-IDF 0.777 0.757 0.739
LL 0.773 0.934 0.880
LO 0.973 0.324 0.903
PMI 0.629
GOLD2 COSINE DICE JENSHAN
TF-IDF 0.694 0.714 0.659
LL 0.714 0.828 0.782
LO 0.883 0.384 0.837
PMI 0.600
RANDOM GOLD1 0.267
RANDOM GOLD2 0.225
Table 3: Average precision obtained over corpora
types, gold standards, association measures and
similarity measures
The first observation is that the overall results
on the WAC corpus pair are about twice as high
as the results obtained on the WIKI corpus pair.
Since the first is more than 20 times larger than
the second, we assumed the amount of informa-
tion available to be the main cause for such drastic
difference.
We then analyzed the difference in the results
obtained on the two gold standards. As expected,
the results are better on PMI baselines, the RAN-
DOM baseline and in the distributional approach
on the WAC corpus pair. The reverse result was
obtained with the distributional approach on the
WIKI corpus pair and at this point we assumed that
it is the result of chance since the results are quite
low and close to each other.
6.1 The Baselines
All the results outperformed the weak RANDOM
baseline. On the contrary, the strong PMI baseline,
which uses only frequency information, proved to
be a better method for identifying false friends in
the WIKI corpus pair, while it was outperformed
by distributional methods on the WAC corpus pair.
An important observation regarding PMI in gen-
eral is that its results relies solely on frequencies
of words and having more information than nec-
essary to make good frequency estimates for all
the words analyzed cannot improve the results any
further. This is the reason why the PMI scores on
both corpora pairs regarding the specific gold stan-
dard are so close to each other (0.634 and 0.629 on
GOLD1, 0.581 and 0.600 on GOLD2), regardless of
the much larger size of the WAC corpora pair. This
shows that both corpora pairs are large enough for
good frequency estimates of the gold standard en-
tries.
Since frequency was not directly encoded in the
distributional approach, it seemed reasonable to
combine the PMI results with those obtained by the
distributional approach. We therefore performed
linear combinations of the PMI baseline and var-
ious distributional results. They yielded no im-
provements except in the case of TF-IDF, which
still performed worse than most other distribu-
tional approaches.
The conclusion regarding PMI is that if one
does not have access to a large amount of textual
data, pointwise mutual information or some other
frequency-based method could be the better way
to approach the problem of false friend identifi-
cation. However, having a lot of data does give
advantage to distributional methods. We will look
into the exact amount of the data needed to outper-
form PMI in subsection 6.5.
6.2 Document Alignments on the WIKI Pair
Since PMI performed so well, especially on the
WIKI corpus pair on which we have access to
document alignments as well, we decided to per-
form another experiment in which we use that
73
additional information. We calculated the joint
probability p(w1, w2) not by calculating the maxi-
mum likelihood estimate of the identically spelled
words in a merged corpus but by taking into ac-
count the number of co-occurrences of the iden-
tically spelled words in aligned documents only.
Naturally, this produced much lower joint proba-
bilities than our initial PMI calculation.
The results of this experiment showed to be
as low as the random baseline (0.189 on GOLD1
and 0.255 on GOLD2). The reason was that low-
frequency lemmas, many of which are TEs, never
occurred together in aligned documents giving
those pairs the lowest possible score. When re-
moving the entries that never co-occur, the results
did rise slightly over the initial PMI score (0.669
on GOLD1 and 0.549 on GOLD2), but roughly half
of the lemma pairs were excluded from the calcu-
lation.
To conclude, identifying false friends with a
simple measure like pointwise mutual information
in case of a limited amount of available data can-
not benefit from the additional structure like the
Wikipedia document alignments. Having much
more data, which would be the case in larger
Wikipedias, or applying a more sophisticated mea-
sure that would be resistant to scarce data, could
prove to be beneficial and is considered a direc-
tion for future work.
6.3 Association and Similarity Measures
We continued our analysis by observing the inter-
play of association and similarity measures. First,
we performed our analysis on the much better re-
sults obtained on the WAC corpus pair. DICE and
LL turned out to be a once-again winning combi-
nation. TF-IDF underperformed when compared
to LL, showing that LL is the superior associa-
tion measure in this problem as well. JENSHAN
showed a very high consistency, regardless of the
association measure used, which is an interesting
property, but it never obtained the highest score.
The big surprise was the LO association mea-
sure. On the WAC corpus pair it resulted in the
overall best score when used with COSINE, but
failed drastically when combined with DICE. The
situation got even more puzzling once we com-
pared these results with those obtained on the
WIKI corpus pair where DICE and LO gave the best
overall result. Laroche and Langlais (2010) report
to get slightly better or identical results when us-
ing LO with COSINE in comparison to DICE.
Trying to find an explanation for such variable
results of the LO association measure, we decided
to analyze the strongest features in the context vec-
tors of both LO and LL on both corpora pairs. We
present our findings in Table 4 on the example of
the word gripa which means flu in both languages.
We analyzed the 50 strongest features and classi-
fied them in one of the following categories: typo,
foreign name, rare term and expected term.
The presented data does shed light on the un-
derlying situation, primarily on the LO association
measure, and secondly on the difference between
the corpora pairs. LL is a very stable association
measure that, regardless of the noise present in the
corpora, gave the highest weight to the features
one would associate with the concept in question.
On the contrary, LO is quite good at emphasizing
the noise from the corpora. Since more noise is
present in web corpora than in Wikipedia corpora,
LO got very good results on the WIKI corpus pair
but failed on the WAC corpus pair.
WIKI
SL-LO SL-LL HR-LO HR-LL
typo 0.24 0.00 0.56 0.16
foreign 0.06 0.00 0.22 0.08
rare 0.10 0.00 0.04 0.00
ok 0.60 1.00 0.18 0.76
WAC
SL-LO SL-LL HR-LO HR-LL
typo 0.62 0.00 0.72 0.12
foreign 0.20 0.00 0.26 0.00
rare 0.04 0.00 0.00 0.00
ok 0.14 1.00 0.02 0.88
Table 4: Results of the analysis of the 50 strongest
features in the eight different LL and LO vectors
This still did not offer an explanation why LO
performed as well as it did on the WAC corpus pair
when it was paired with COSINE, or to a smaller
extent with JENSHAN. The reason for such behav-
ior lies in the primary difference between DICE
and the remaining similarity measures: the latter
take into account only the features defined in both
vectors while DICE works on a union of the fea-
tures. Transforming DICE in such a way that it
takes into account only the intersection of the de-
fined features did improve the results when using
it with LO (from 0.324 and 0.384 to 0.575 and
0.591), but the results deteriorated when used with
74
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
gold1
recall
pre
cis
ion
WaC.lo.cosine
WaC.ll.dice
WaC.pmi
wp.pmi
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
gold2
recall
pre
cis
ion
WaC.lo.cosine
WaC.ll.dice
WaC.pmi
wp.pmi
Figure 1: Precision-recall curve of chosen settings on both gold standards
LL (0.934 and 0.828 to 0.768 and 0.719).
We can conclude that LL is a much more stable
association measure than LO, but LO performs ex-
tremely well as long as the corpora are not noisy
or it is not combined with a similarity score that
calculates the similarity on a union of the defined
features.
6.4 Precision-Recall Curves
We visualized the results obtained with best per-
forming and most interesting settings in Figure 1
with two precision-recall curves, one for each gold
standard.
The PR curves stressed the similarity of the re-
sults of the PMI method on same gold standards
between corpora pairs along the whole precision-
recall trade-off spectrum. They also emphasized
the significance of the higher quality of the re-
sults obtained by the distributional approach on
the large WAC corpus pair.
Although somewhat unpredictable, the LO as-
sociation measure, when coupled with the correct
similarity measure, consistently outperformed LL
on the whole spectrum on both gold standards.
6.5 Corpus Size
We performed a final set of experiments, which
focused on experimenting with the parameter of
corpus size. In general, we were interested in
the learning curves on different corpora pairs with
best performing settings. We also looked for the
point where the distributional approach overtakes
the frequency approach and a direct comparison
between the two corpora pairs.
The learning curves, calculated on random por-
tions of both corpora pairs on GOLD1, are pre-
sented in Figure 2. Both PMI learning curves
proved our claim that with a sufficient amount of
information required to make good frequency es-
timates, no further improvement can be achieved.
On these datasets good estimates were obtained on
5 million words (both languages combined). The
PMI learning curve on the WAC corpus pair was
steady on the whole scale and we identified the
point up to which PMI is more suitable for iden-
tifying false friends than distributional methods
somewhere around 130 million words (both cor-
pora combined) from where distributional meth-
ods surpass the ? 0.63 plain frequency result.
The WIKI.LL.DICE and the WAC.LL.DICE
curves on the left plot enabled us to compare the
suitability of the two corpora pairs for the task of
identifying false friends and distributional tasks in
general. At lower corpus sizes the results were
very close, but from 10 million words onwards,
the WAC corpus pair outperformed the WIKI cor-
pus pair, consistently pointing toward the conclu-
sion that web corpora are more suitable for distri-
butional approaches than Wikipedia corpora.
The performance of the two distributional ap-
proaches depicted on the second graph evened out
75
0 10 20 30 40
0.0
0.2
0.4
0.6
0.8
1.0
wiki
corpus pair size (Mwords)
av
era
ge
 pr
ec
isi
on
wiki.lo.dice
wiki.ll.dice
wiki.pmi
WaC.ll.dice
0 200 400 600 800 1000 1200
0.0
0.2
0.4
0.6
0.8
1.0
WaC
corpus pair size (Mwords)
av
era
ge
 pr
ec
isi
on
WaC.lo.cosine
WaC.ll.dice
WaC.pmi
Figure 2: Learning curve on both corpora pairs on GOLD1
around the 500 million word mark, showing that
around 250 million words per language should
suffice for this task. Having lower-frequency en-
tries in the gold standard would, naturally, call
for more data. However, the criterion of 50 oc-
currences in 500+ million tokens web corpora we
used for constructing our gold standards should
cover most cases.
Finally, let us point out that the WIKI.LO.DICE
curve on the left graph climbed much faster than
the WIKI.LL.DICE curve, showing faster learning
with the LO association measure in comparison to
LL. An interesting observation is that the LO curve
obtained its maximum slightly after the 20 million
words mark, after which it started a slow decline.
Although it could be surprising to see a learn-
ing curve declining, this is in line with our previ-
ous insights regarding the LO association measure
not responding well to many new low-frequency
features included in the vector space making the
LO+DICE combination struggle. This is one ad-
ditional reminder that the LO association measure
should be used with caution.
7 Conclusion
In this paper we compared frequency-based
and distributional approaches to identifying false
friends from two frequently used types of corpora
pairs ? Wikipedia and web corpora. We have used
the PMI method for frequency-based ranking and
three association and three similarity measures for
distributional-based ranking.
The PMI method has proven to be a very good
method if one does not have more than 75 mil-
lion words available per language, in which case it
outperformed the more complex distributional ap-
proach. Good frequency estimates for PMI were
obtained on 2.5 million words per language, after
which introducing more data did not yield any fur-
ther improvement.
Using document alignments from Wikipedia as
an additional source for the frequency-based ap-
proach did not perform well because of the small
size of the Wikipedias in question (slightly above
100,000 articles), often producing zero joint prob-
abilities for non-false friends. A more thought-
through approach that could resist data sparsity or
using larger Wikipedias is one of our future re-
search directions.
The DICE+LL similarity and association mea-
sures proved to be a very stable combination as is
the case on the opposite task of translation equiv-
alence extraction (Ljubes?ic? et al, 2011).
The LO association measure gave excellent re-
sults, but only if it was paired with a similarity
measure that takes into account only the intersec-
tion of the features or if the context vectors were
calculated on very clean corpora since LO tends to
overemphasize low frequency features. We would
recommend using this association measure in dis-
76
tributional approaches, but only if one of the above
criteria is satisfied.
The amount of data on which the distributional
approach stopped benefitting from more data on
this task was around 250 million words per lan-
guage.
Overall, web corpora showed to be better can-
didates for distributional methods than Wikipedia
corpora for two reasons: 1. the WAC learning
curve is steeper, and 2. there are few languages
which contain 75 million words per language that
are necessary to outperform the frequency-based
approach and even fewer for which there are 250
million words per language needed for the learn-
ing curve to even out.
Our two primary directions for future research
are 1. preceding this procedure with identifying
language-pair-specific similarly spelled words and
2. including additional language pairs such as
Croatian and Czech or Slovene and Czech.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme FP7/2007-2013 under grant
agreement no. PIAP-GA-2012-324414 (project
Abu-MaTran) and from the Slovene national
research programme no. P6-0215.
We would like to thank Das?a Berovic? and Jelena
Tus?ek for their help in annotating the datasets.
References
Keith Allan, editor. 2009. Concise Encyclopedia of
Semantics. Elsevier Science.
Rube?n Chaco?n Beltra?n. 2006. Towards a typological
classification of false friends (Spanish-English). Re-
vista Espan?ola de Lingu??stica Aplicada, 19:29?39.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74.
Oana Frunza and Diana Inkpen. 2007. A tool for de-
tecting French-English cognates and false friends.
In Proceedings of the 14th conference Traitement
Automatique des Langues Naturelles, TALN?07,,
Toulouse.
Sylviane Granger and Helen Swallow. 1988. False
friends: a kaleidoscope of translation difficulties.
Langage et l?Homme, 23(2):108?120.
John Holmes and Rosinda Guerra Ramos. 1993. False
friends and reckless guessers: Observing cognate
recognition strategies. In Thomas Huckin, Mar-
got Haynes, and James Coady, editors, Second Lan-
guage Reading and Vocabulary Learning. Norwood,
New Jersey: Ablex.
Diana Inkpen, Oana Frunza, and Grzegorz Kondrak.
2005. Automatic identification of cognates and false
friends in French and English. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP 2005), pages
251?257.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
617?625, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Nikola Ljubes?ic? and Tomaz? Erjavec. 2011. hrWaC and
slWac: Compiling Web Corpora for Croatian and
Slovene. In Ivan Habernal and Va?clav Matousek,
editors, Text, Speech and Dialogue - 14th Interna-
tional Conference, TSD 2011, Pilsen, Czech Repub-
lic, September 1-5, 2011. Proceedings, volume 6836
of Lecture Notes in Computer Science, pages 395?
402. Springer.
Nikola Ljubes?ic?, Darja Fis?er, S?pela Vintar, and Senja
Pollak. 2011. Bilingual lexicon extraction from
comparable corpora: A comparative study. In First
International Workshop on Lexical Resources, An
ESSLLI 2011 Workshop, Ljubljana, Slovenia - Au-
gust 1-5, 2011.
Ruslan Mitkov, Viktor Pekar, Dimitar Blagoev, and An-
drea Mulloni. 2007. Methods for extracting and
classifying pairs of cognates and false friends. Ma-
chine Translation, 21(1):29?53.
Svetlin Nakov and Preslav Nakov. 2007. Cognate or
false friend? Ask the Web. In Proceedings of the
RANLP?2007 workshop: Acquisition and manage-
ment of multilingual lexicons.
Svetlin Nakov and Preslav Nakov. 2009. Unsupervised
extraction of false friends from parallel bi-texts us-
ing the web as a corpus. In Proceedings of the
6th International Conference on Recent Advances
in Natural Language Processing (RANLP?09), pages
292?298.
Stefan Schulz, Korne?l Marko?, Eduardo Sbrissia, Percy
Nohama, and Udo Hahn. 2004. Cognate mapping
- A heuristic strategy for the semi-supervised acqui-
sition of a Spanish lexicon from a Portuguese seed
lexicon. In Proceedings of the 20th International
Conference on Computational Linguistics.
Lianet Sepu?lveda Torres and Sandra Maria Alu??sio.
2011. Using machine learning methods to avoid
the pitfall of cognates and false friends in Spanish-
Portuguese word pairs. In Proceedings of the 8th
Brazilian Symposium in Information and Human
Language Technology, STIL?11.
77
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Cross-lingual WSD for Translation Extraction
from Comparable Corpora
Marianna Apidianaki
LIMSI-CNRS
Rue John Von Neumann
BP 133, 91403
Orsay Cedex, France
marianna@limsi.fr
Nikola Ljubes?ic?
Dept. of Information Sciences
University of Zagreb
Ivana Luc?ic?a 3, HR-10000
Zagreb, Croatia
nljubesi@ffzg.hr
Darja Fis?er
Department of Translation
University of Ljubljana
As?kerc?eva 2, SI-1000
Ljubljana, Slovenia
darja.fiser@ff.uni-lj.si
Abstract
We propose a data-driven approach to en-
hance translation extraction from compa-
rable corpora. Instead of resorting to an
external dictionary, we translate source
vector features by using a cross-lingual
Word Sense Disambiguation method. The
candidate senses for a feature correspond
to sense clusters of its translations in a
parallel corpus and the context used for
disambiguation consists of the vector that
contains the feature. The translations
found in the disambiguation output con-
vey the sense of the features in the source
vector, while the use of translation clusters
permits to expand their translation with
several variants. As a consequence, the
translated vectors are less noisy and richer,
and allow for the extraction of higher qual-
ity lexicons compared to simpler methods.
1 Introduction
Large-scale comparable corpora are available in
many language pairs and are viewed as a source
of valuable information for multilingual applica-
tions. Identifying translation correspondences in
this type of corpora permits to construct bilingual
lexicons for low-resourced languages, and to com-
plement and reduce the sparseness of existing re-
sources (Munteanu and Marcu, 2005; Snover et
al., 2008). The main assumption behind transla-
tion extraction from comparable corpora is that a
source word and its translation appear in similar
contexts (Fung, 1998; Rapp, 1999). So, in order
to identify a translation correspondence between
the two languages, the contexts of the source word
and the candidate translation have to be compared.
For this comparison to take place, the same vector
space has to be produced, which means that the
vectors of the one language have to be translated
in the other language. This generally assumes the
availability of a bilingual dictionary which might
however not be the case for some language pairs
and domains. Moreover, the classic way in which
a dictionary is put into use, which consists in trans-
lating vector features by their first translation in
the dictionary, neglects semantics. We expect that
a method capable of identifying the correct sense
of the features and translating them accordingly
could contribute to producing cleaner vectors and
to extracting higher quality lexicons.
In this paper, we show how source vectors
can be translated into the target language by a
cross-lingual Word Sense Disambiguation (WSD)
method which exploits the output of data-driven
Word Sense Induction (WSI) (Apidianaki, 2009),
and demonstrate how feature disambiguation en-
hances the quality of the translations extracted
from the comparable corpus. This study extends
our previous work on the topic (Apidianaki et al,
2012) by applying the proposed methods to a com-
parable corpus of general language (built from
Wikipedia) and optimizing various parameters that
affect the quality of the extracted translations. We
expect the disambiguation to have a beneficial im-
pact on the results given that polysemy is a fre-
quent phenomenon in a general, mixed-domain
corpus. Our experiments are carried out on the
English-Slovene language pair but as the methods
are totally data-driven, the approach can be easily
applied to other languages.
The paper is organized as follows: In the next
section, we present some related work on bilin-
gual lexicon extraction from comparable corpora.
Section 3 presents the data used in our experiments
and Section 4 provides details on the approach and
the experimental setup. In Section 5, we report and
discuss the obtained results before concluding and
presenting some directions for future work.
1
2 Related work
The traditional approch to translation extraction
from comparable corpora and most of its exten-
sions (Fung, 1998; Rapp, 1999; Shao and Ng,
2004; Otero, 2007; Yu and Tsujii, 2009; Marsi
and Krahmer, 2010) presuppose the availability
of a bilingual lexicon for translating source vec-
tors into the target language. A translation can-
didate is generally considered as correct if it is
an appropriate translation for at least one sense
of the source word in the dictionary, which of-
ten corresponds to its most frequent sense. An
alternative consists in considering all translations
provided for a word in the dictionary but weight-
ing them by their frequency in the target lan-
guage (Prochasson et al, 2009; Hazem and Morin,
2012). The high quality of the exploited hand-
crafted resources, combined to the skewed distri-
bution of the translations corresponding to differ-
ent word senses, often lead to satisfying results.
Nevertheless, the applicability of the methods is
limited to languages and domains where bilingual
resources are available. Moreover, by promoting
the most frequent sense/translation, this approach
neglects polysemy. We believe that feature dis-
ambiguation can lead to the production of cleaner
vectors and, consequently, to higher quality re-
sults.
The need to bypass pre-existing dictionaries
has been addressed by Koehn and Knight (2002)
who built the initial seed dictionary automatically,
based on identical spelling features between En-
glish and German. Cognate detection has also
been used by Saralegi et al (2008) for extract-
ing word translations from English-Basque com-
parable corpora. The cognate and seed lexicon
approaches have been successfully combined by
Fis?er and Ljubes?ic? (2011) who showed that the re-
sults with an automatically created seed lexicon,
based on language similarity, can be as good as
with a pre-existing dictionary. But all these ap-
proaches work on closely-related languages and
cannot be used as successfully for language pairs
with little lexical overlap, such as English and
Slovene, which is the case in this experiment.
Regarding the translation of the source vectors,
we use contextual information to disambiguate
their features and translate them using clusters
of semantically similar translations in the target
language. A similar idea has been implemented
by Kaji (2003) who performed sense-based word
clustering to extract sets of synonymous transla-
tions from comparable corpora with the help of a
bilingual dictionary.
Using translation clusters permits to expand
feature translation and to suggest multiple seman-
tically correct translations. A similar approach has
been adopted by De?jean et al (2005) who expand
vector translation by using a bilingual thesaurus
instead of a lexicon. In contrast to their work, the
method proposed here does not rely on any exter-
nal knowledge source to determine word senses
or translation equivalents, and is thus fully data-
driven and language independent.
3 Resources
3.1 Comparable corpus
The comparable corpus from which the bilin-
gual lexicon will be extracted is a collection of
English (EN) and Slovene (SL) texts extracted
from Wikipedia. The February 2013 dumps of
Wikipedia articles were downloaded and cleaned
for both languages after which the English cor-
pus was tokenized, part-of-speech (PoS) tagged
and lemmatized with the TreeTagger (Schmid,
1994). The same pre-processing was applied to the
Slovene corpus with the ToTaLe analyzer (Erjavec
et al, 2010) which uses the TnT tagger (Brants,
2000) and was trained on MultextEast corpora.
The Wikipedia corpus contains about 1.5 billion
tokens for English and almost 24 million tokens
for Slovene.
In previous work, we applied our approach to a
specialized comparable corpus from the health do-
main (Apidianaki et al, 2012). The results were
encouraging, showing how translation clustering
and vector disambiguation help to improve the
quality of the translations extracted from the com-
parable corpus. We believe that the positive im-
pact of this approach will be more significant on
lexicon extraction from a general language com-
parable corpus, in which polysemy is more promi-
nent.
3.2 Parallel corpus
The parallel corpus used for clustering and word
sense induction consists of the Slovene-English
parts of Europarl (release v6) (Koehn, 2005) and
of JRC-Acquis (Steinberger et al, 2006) and
amounts to approximately 35M words per lan-
guage. A number of pre-processing steps are ap-
plied to the corpus prior to sense induction, such
2
Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD.
as elimination of sentence pairs with a great dif-
ference in length, lemmatization and PoS tagging
with the TreeTagger (for English) and ToTaLe (for
Slovene) (Erjavec et al, 2010). Next, the cor-
pus is word-aligned with GIZA++ (Och and Ney,
2003) and two bilingual lexicons are extracted,
one for each translation direction (EN?SL/SL?
EN). To clean the lexicons from noisy alignments,
the translations are filtered on the basis of their
alignment score and PoS, keeping only transla-
tions that pertain to the same grammatical cate-
gory as the source word. We retain only intersect-
ing alignments and use for clustering translations
that translate a source word more than 10 times
in the training corpus. This threshold reduces
data sparseness issues that affect the clustering
and eliminates erroneous word alignments. The
filtered EN-SL lexicon contains entries for 6,384
nouns, 2,447 adjectives and 1,814 verbs having
more than three translations in the training corpus.
The parallel corpus, which contains EU texts, is
more specialized than the comparable corpus built
from Wikipedia. This is not the ideal scenario for
this experiment; domain adaptation is important
for the type of semantic processing we want to ap-
ply as there might be a shift in the senses present in
the two corpora. However, as EU texts often con-
tain a lot of general vocabulary, we expect that this
discrepancy will not strongly affect the quality of
the results.
3.3 Gold standard
We evaluate the quality of the bilingual lexicons
extracted from the comparable corpus by compar-
ing them to a gold standard lexicon, which was
built from the aligned English (Fellbaum, 1998)
and Slovene wordnets (Fis?er and Sagot, 2008). We
extracted all English synsets from the Base Con-
cept sets that belong to the Factotum domain and
contain literals with polysemy levels 1-5 and their
Slovene equivalents which have been validated by
a lexicographer. Of 1,589 such synsets, 200 were
randomly selected and used as a gold standard for
automatic evaluation of the method proposed in
this paper.
4 Experimental setup
4.1 Overview of the method
Figure 1 gives an overview of the way informa-
tion mined from the parallel training corpus is ex-
ploited for discovering translations of source (En-
glish) words in the comparable corpus. The par-
allel corpus serves to extract an English-Slovene
seed lexicon and source language context vec-
tors (Par vectors) for the Slovene translations of
English words. These vectors form the input to
the Word Sense Induction (WSI) method which
groups the translations of an English word into
clusters.
The clusters of semantically related Slovene
translations constitute the candidate senses which,
together with the Par vectors, are used for dis-
ambiguating and translating the vectors extracted
from the source (English) side of the comparable
corpus (Comp source). The translated vectors are
then compared to the ones extracted from the tar-
get language (Slovene) side of the comparable cor-
pus (Comp target) and the best translations are se-
lected, for a list of unknown words. All steps of
the proposed method illustrated in Figure 1 will
be detailed in the following sections.
4.2 Translation clustering
The translations of the English words in the lex-
icon built as described in 3.2 are clustered ac-
cording to their semantic proximity using a cross-
lingual Word Sense Induction method (Apidi-
anaki, 2008). For each translation Ti of a word
w, a vector is built from the content word co-
3
Language POS Source word Slovene sense clusters
EN?SL
Nouns
sphere
{krogla} (geometrical shape)
{sfera, podroc?je} (area)
address
{obravnava, res?evanje, obravnavanje} (dealing with)
{naslov} (postal address)
portion
{kos} (piece)
{obrok, porcija} (serving)
{delez?} (share)
figure
{s?tevilka, podatek, znesek} (amount)
{slika} (image)
{osebnost} (person)
Verbs
seal
{tesniti} (to be water-/airtight)
{zapreti, zapec?atiti} (to close an envelope or some other container)
weigh
{pretehtati} (consider possibilities)
{tehtati, stehtati} (check weight)
educate
{pouc?iti} (give information)
{izobraz?evati, izobraziti} (give education)
consume
{potros?iti} (spend money/goods)
{uz?ivati, zauz?iti} (eat/drink)
Adjs
mature
{zrel, odrasel} (adult)
{zorjen, zrel} (ripe)
minor
{nepomemben} (not very important)
{mladoleten, majhen} (under 18 years old)
juvenile
{nedorasel} (not adult/biologically mature yet)
{mladoleten, mladoletnis?ki} (not 18/legally adult yet)
remote
{odmaknjen, odroc?en} (far away and not easily accessible)
{oddaljen daljinski} (controlled from a distance (e.g. remote control))
Table 1: Entries from the English-Slovene sense cluster inventory.
occurrences of w in the parallel sentences where it
is translated by Ti. Let N be the number of features
retained for each Ti from the corresponding source
contexts. Each feature Fj (1 ? j ? N) receives a
total weight with a translation Ti, tw(Fj,Ti), de-
fined as the product of the feature?s global weight,
gw(Fj), and its local weight with that translation,
lw(Fj,Ti). The global weight of a feature Fj is a
function of the number Ni of translations (Ti?s) to
which Fj is related, and of the probabilities (pi j)
that Fj co-occurs with instances of w translated by
each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(1)
Each pi j is computed as the ratio of the co-
occurrence frequency of Fj with w when translated
as Ti to the total number of features seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(2)
The local weight lw(Fj,Ti) between Fj and Ti di-
rectly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (3)
The pairwise similarity of the translations is cal-
culated using the Weighted Jaccard Coefficient
(Grefenstette, 1994).
WJ(Tm,Tn) =
? j min(tw(Tm,Fj), tw(Tn,Fj))
? j max(tw(Tm,Fj), tw(Tn,Fj))
(4)
The similarity score of each translation pair is
compared to a threshold locally defined for each w
using an iterative procedure. The threshold (T ) for
a word w is initially set to the mean of the scores
(above 0) of its translation pairs. The set of trans-
lation pairs of w is then divided into two sets (G1
and G2) according to whether they exceed, or are
inferior to, the threshold. The average of scores of
the translation pairs in each set is computed (m1
and m2) and a new threshold is calculated that is
the average of m1 and m2 (T = (m1+m2)/2). The
new threshold serves to separate again the transla-
tion pairs into two sets, a new threshold is calcu-
lated and the procedure is repeated until conver-
gence.
The semantically similar translations of w are
grouped into clusters. Translation pairs with a
score above the threshold form initial clusters that
4
might be further enriched provided that there exist
additional strongly related translations. Cluster-
ing stops when all translations of w are clustered
and all their relations have been checked. An im-
portant feature of the algorithm is that it performs
soft clustering, so translations can be found in dif-
ferent clusters. The final clusters are characterized
by global connectivity, i.e. all their elements are
linked by pertinent relations.
Table 1 gives examples of clusters obtained for
English words of different PoS with clear sense
distinctions in the parallel corpus. For each En-
glish word, we provide the obtained clusters of
Slovene translations including a description of the
sense described by each cluster. For instance, the
translations for the adjective minor from the train-
ing corpus (nepomemben, mladoleten and majhen)
are grouped into two clusters describing its two
senses: {nepomemben} - ?not very important?
and {mladoleten, majhen} - ?under 18 years old?.
The resulting cluster inventory contains 13,352
clusters in total, for 8,892 words. 2,585 of the
words (1,518 nouns, 554 verbs and 513 adjectives)
have more than one cluster.
In the next section, we explain how the clus-
ters and the corresponding translation vectors are
used for disambiguating the source language vec-
tors extracted from the comparable corpus.
4.3 Cross-lingual vector comparison
4.3.1 Vector building
We build context vectors in the two languages for
nouns occurring at least 50 times in the compa-
rable corpus. The frequency threshold is impor-
tant for the lexicon extraction approach to produce
good results. As features we use three content
words to the left and to the right of the retained
nouns, stopping at the sentence boundary, without
taking into account their position. Log-likelihood
is used to calculate feature weights.
In the reported experiments we focus on the
1,000 strongest features. A portion of these fea-
tures is disambiguated for each headword, de-
pending on the availability of clustering informa-
tion. We observed that disambiguating a smaller
amount of features yielded similar results and in-
cluding additional features did not improve the re-
sults.
4.3.2 Vector translation and disambiguation
Translation correspondences between the two lan-
guages of the comparable corpus are identified by
comparing the source language vectors, built as
described in Section 4.3.1, to the ones of the candi-
date translations. This comparison serves to quan-
tify the similarity of the source and target words
represented by the vectors and the highest ranked
pairs are retained.
For the comparison to take place, the source
vectors have to be translated in the target language.
In most previous work, the vectors were translated
using external seed dictionaries: the first transla-
tion proposed for a word in the dictionary was
used to translate all instances of the word in the
vectors irrespective of their sense. Here, we re-
place the external dictionary with the output of
a data-driven cross-lingual WSD method (Apidi-
anaki, 2009) which renders the method knowledge
light and adaptable to other language pairs.
The translation clusters obtained during WSI
(cf. Section 4.2) describe the senses of the En-
glish words in the parallel corpus. We exploit this
sense inventory for disambiguating the features in
the English vectors extracted from the comparable
corpus. More precisely, we ask the WSD method
to select among the available clusters the one that
correctly translates in Slovene the sense of the En-
glish features in the vectors built from the compa-
rable corpus. The selection is performed by com-
paring information from the context of a feature,
which corresponds to the rest of the vector where
the feature appears, to the source language vectors
of the translations which served to their cluster-
ing. Inside the vectors, the features are ordered
according to their score, calculated as described in
Section 4.3.1. Feature weights filter out the weak
features, i.e. features with a score below the ex-
perimentally set threshold of 0.01. The retained
features are then considered as a bag of words.
On the clusters? side, the information used for
disambiguation is found in the source language
vectors that revealed the similarity of the transla-
tions. If common features (CFs) exist between the
context of a feature and the vectors of the transla-
tions in a cluster, a score is calculated correspond-
ing to the mean of the weights of the CFs with the
clustered translations, where weights correspond
to the total weights (tw?s) computed between fea-
tures and translations during WSI. In formula 5,
CFj is the set of CFs and NCF is the number of
translations Ti characterized by a CF.
wsd score =
?
NCF
i=1 ? j w(Ti,CFj)
NCF ? |CFj|
(5)
5
PoS Feature Assigned Cluster MFT
Nouns
party {oseba, stran, pogodbenica, stranka} stranka
matter {zadeva, vpras?anje} zadeva
Verbs
settle {urediti, res?iti, res?evati} res?iti
follow {upos?tevati, spremljati, slediti} slediti
Adjs
alternative {nadomesten, alternativen} alternativen
involved {vkljuc?en, vpleten} vkljuc?en
Table 2: Disambiguation results.
The cluster that receives the highest score is se-
lected and assigned to the feature as a sense tag.
The features are also tagged with their most fre-
quent translation (MFT) in the parallel corpus,
which sometimes already exists in the cluster se-
lected during WSD.
In Table 2, we present examples of disam-
biguated features of different PoS from the vec-
tor of the word transition. The context used for
disambiguation consists of the other strong fea-
tures in the vector and the cluster that best de-
scribes the sense of the features in this context
is selected. In the last column, we provide the
MFT of the feature in the parallel corpus. In the
examples shown here the MFT translation already
exists in the cluster selected by the WSD method
but this is not always the case. As we will show
in the Evaluation section, the configuration where
the MFT from the cluster assigned during disam-
biguation is selected (called CLMFT) gives better
results than MFT, which shows that the MFT in
the selected cluster is not always the most frequent
alignment for the word in the parallel corpus. Fur-
thermore, the clusters provide supplementary ma-
terial (i.e. multiple semantically correct transla-
tions) for comparing the vectors in the target lan-
guage and improving the baseline results. Still,
MFT remains a very powerful heuristic due to the
skewed distribution of word senses and transla-
tions.
4.4 Vector comparison
The translation clusters proposed during WSD for
the features in the vectors built from the source
side of the comparable corpus serve to translate the
vectors in the target language. In our experiments,
we compare three different ways of translating the
source language features.
1. by keeping the most frequent transla-
tion/alignment of the feature in the parallel
corpus (MFT);
2. by keeping the most frequent translation from
the cluster assigned to the feature during dis-
ambiguation (CLMFT); and
3. by using the same cluster as in the second ap-
proach, but producing features for all transla-
tions in the cluster with the same weight (CL).
The first approach (MFT) serves as the base-
line since, instead of the sense clustering and
WSD results, it just uses the most frequent
sense/alignment heuristic. In the first batch of ex-
periments, we noticed that the results of the CL and
CLMFT approaches heavily depend on the part-of-
speech of the features. So, we divided the CL and
CLMFT approaches into three sub-approaches:
1. translate only nouns, verbs or adjectives with
the clusters and other features with the MFT
approach (CLMFT N, CLMFT V, CLMFT A);
2. translate nouns and adjectives with the clus-
ters and verbs with the MFT approach
(CLMFT NA); and
3. translate nouns and verbs with the clus-
ters and adjectives with the MFT approach
(CLMFT NV).
The distance between the translated source and
the target-language vectors is computed by the
Dice metric. By comparing the translated source
vectors to the target language ones, we obtain a
ranked list of candidate translations for each gold
standard entry.
5 Evaluation
5.1 Metrics
The final result of our method consists in ranked
lists of translation candidates for gold standard en-
tries. We evaluate this output by the mean recipro-
cal rank (MRR) measure which takes into account
6
the rank of the first good translation found for each
entry. Formally, MRR is defined as
MRR =
1
|Q|
|Q|
?
i=1
1
ranki
(6)
where |Q| is the length of the query, i.e. the num-
ber of gold standard entries we compute transla-
tion candidates for, and ranki is the position of the
first correct translation in the candidate list.
5.2 Results
Table 4 shows the translation extraction results
for different configurations. The MFT score is
used as the baseline. We observe that disam-
biguating all features in the vectors (CL) yields
lower results than the baseline compared to se-
lecting only the most frequent translation from the
cluster which slightly outperforms the MFT base-
line. In the CLMFT N, CLMFT NA, CLMFT NV
configurations we disambiguate noun features,
nouns and adjectives, and nouns and verbs, respec-
tively, and translate words of other PoS using the
MFT. In CLMFT N, for instance, nouns are dis-
ambiguated while verbs and adjectives are trans-
lated by the word to which they were most fre-
quently aligned in the parallel corpus. The three
configurations where nouns are disambiguated
(CLMFT N, CLMFT NA, CLMFT NV) give better
results compared to those addressing verbs or ad-
jectives alone. Interestingly, disambiguating only
adjectives gives worse results than disambiguating
only verbs, but the combination of nouns and ad-
jectives outperforms the combination of nouns and
verbs.
In CLMFT, features of all PoS are disambiguated
but we only keep the most frequent translation in
the cluster and ignore the other translations. This
setting gives much better results than CL, where
the whole cluster is used, which highlights two
facts: first, that disambiguation is beneficial for
translation extraction and, second, that the noise
present in the automatically built clusters harms
the quality of the translations extracted from the
comparable corpus. The better score obtained for
CLMFT compared to MFT also shows that, in many
cases, the most frequent translation in the cluster
does not coincide with the most frequent align-
ment of the word in the parallel corpus. So, disam-
biguation helps to select a more appropriate trans-
lation than the MFT approach. This improvement
compared to the baseline shows again that WSD is
MRR
MFT 0.0685
CLMFT 0.0807
CL 0.0434
CLMFT N 0.0817
CLMFT A 0.07
CLMFT V 0.0714
CLMFT NA 0.0842
CLMFT NV 0.08048
Table 3: Results of the experiment.
MRR diff p-value
MFT CLMFT 0.0122 0.1830
MFT CL 0.0251 0.0410
CLMFT CL 0.0373 0.0120
MFT CLMFT NA 0.0157 0.4296
MFT CLMFT NV 0.0120 0.5195
Table 4: Comparison of different configurations.
useful in this setting.
In Table 4, the results for different configura-
tions are compared. The statistical significance of
the difference in the results was calculated by ap-
proximate randomization (1,000 repetitions). We
observe that the differences between the CL and
MFT configurations and the CL and CLMFT ones,
are statistically significant. This confirms that tak-
ing most frequent translations, disambiguated or
not, works better than exploiting all the informa-
tion in the clusters. The remainder of the dif-
ferences in the results are not statistically signif-
icant. One could wonder why the p-values are that
high in case of the MFT setting on one side and
CLMFT NA and CLMFT NV settings on the other
side although the differences in the results are not
that high. The most probable explanation is that
there is a low intersection in correct results and
errors. Because of that, flipping the results be-
tween the two systems ? as performed in approx-
imate randomization ? often generates differences
higher than the initial difference on the original re-
sults.
5.3 Qualitative analysis
Manual evaluation of the results shows that the
procedure can deal with concrete words much bet-
ter than with abstract ones. For example, the cor-
rect translation of the headword enquiry is the
third highest-ranked translation. The results are
7
also much better with monosemous and domain-
specific terms (e.g. the correct translation for cat-
aclysm is the top-ranking candidate). On the other
hand, general and polysemous expressions that
can appear in a wide range of contexts are a much
tougher nut to crack. For example, the correct
translation candidate for word role, which can be
used in a variety of contexts as well as metaphor-
ically, is in the tenth position, whereas no correct
translation was found for transition. However, it
must be noted that even if the correct translation is
not found in the results, the output of our method
is in most cases a very coherent and solid descrip-
tion of the semantic field of the headword in ques-
tion. This means that the list can still be useful for
lexicographers to illicit the correct translation that
is missing, or organize the vocabulary in terms of
their relational-semantic principles.
We have also performed an error analysis in
cases where the correct translation could not be
found among the candidates, which consisted of
checking the 30 strongest disambiguated features
of an erroneously translated headword. We ob-
served cases where the strongest features in the
vectors are either very abstract and generic or too
heterogeneous for our method to be able to per-
form well. This was the case with the headwords
characterisation, antecedent and thread. In cases
where the strongest features represented the con-
cept clearly but the correct translation was not
found, we examined cluster, WSD and MFT qual-
ity, as suggested by the parallel corpus. The main
source of errors in these cases is the noise in the
clusters which is often due to pre-processing er-
rors, especially in the event of multi-word expres-
sions. It seems that clustering is also problematic
for abstract or generic words, where senses might
be lumped together. The WSD step, on the other
hand, does not seem to introduce noise to the pro-
cedure as it is correct in almost all the cases we
have examined.
6 Discussion and conclusion
We have shown how cross-lingual WSD can be
applied to bilingual lexicon extraction from com-
parable corpora. The disambiguation of source
language features using translation clusters con-
stitutes the main contribution of this work and
presents several advantages. First, the method per-
forms disambiguation by using sense descriptions
derived from the data, which clearly differentiates
our method from the approaches based on external
lexicons and extends its applicability to resource-
poor languages. The translation clusters acquired
through WSI serve to disambiguate the features in
the source language context vectors and to pro-
duce less noisy translated vectors. An additional
advantage is that the sense clusters often contain
more than one translation and, therefore, provide
supplementary material for the comparison of the
vectors in the target language.
The results show that data-driven semantic anal-
ysis can help to circumvent the need for an exter-
nal seed dictionary, traditionally considered as a
prerequisite for translation extraction from paral-
lel corpora. Moreover, it is clear that disambiguat-
ing the vectors improves the quality of the ex-
tracted lexicons and manages to beat the simpler,
but yet powerful, most frequent translation heuris-
tic. These encouraging results pave the way to-
wards pure data-driven methods for bilingual lex-
icon extraction. This knowledge-light approach
can be applied to languages and domains that do
not dispose of large-scale seed dictionaries but for
which parallel corpora are available.
An avenue that we intend to explore in future
work is to extract translations corresponding to
different senses of the headwords. Up to now,
research on translation extraction has most of-
ten aimed the identification of one good trans-
lation for a source word in the comparable cor-
pus. This has also been the case because most
works have focused on identifying translations
for specialized terms that do not convey differ-
ent senses. However, words in a general lan-
guage corpus like Wikipedia can be polysemous
and it is important to identify translations corre-
sponding to their different senses. Moreover, pol-
ysemy makes the translation extraction procedure
more difficult, as features corresponding to differ-
ent senses are mingled in the same vector. A way
to discover translations corresponding to different
word senses would be to apply a monolingual WSI
method on the source side of the comparable cor-
pus which would group the closely related usages
of the headwords together, and to then build vec-
tors for each usage group hopefully describing a
distinct sense. Using the generated sets of vectors
separately will allow to extract translations corre-
sponding to different senses of the source words.
8
References
Marianna Apidianaki, Nikola Ljubes?ic?, and Darja
Fis?er. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15, Ljubljana, Slovenia.
Marianna Apidianaki. 2008. Translation-oriented
sense induction based on parallel corpora. In Pro-
ceedings of the 6th International Conference on
Language Resources and Evaluation (LREC-08),
pages 3269?3275, Marrakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selec-
tion in Translation. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-09),
pages 77?85, Athens, Greece.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing (ANLP-2000), Seat-
tle, WA.
Herve? De?jean, Eric Gaussier, Jean-Michel Renders,
and Fatiha Sadat. 2005. Automatic processing of
multilingual medical terminology: applications to
thesaurus enrichment and cross-language informa-
tion retrieval. Artificial Intelligence in Medicine,
33(2):111?124, February.
Tomaz? Erjavec, Darja Fis?er, Simon Krek, and Nina
Ledinek. 2010. The JOS Linguistically Tagged
Corpus of Slovene. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Darja Fis?er and Nikola Ljubes?ic?. 2011. Bilingual lexi-
con extraction from comparable corpora for closely
related languages. In Proceedings of the Inter-
national Conference Recent Advances in Natural
Language Processing 2011, pages 125?131, Hissar,
Bulgaria. RANLP 2011 Organising Committee.
Darja Fis?er and Beno??t Sagot. 2008. Combining mul-
tiple resources to build reliable wordnets. In TSD
2008 - Text Speech and Dialogue, Lecture Notes in
Computer Science, Brno, Czech Republic. Springer.
Pascale Fung. 1998. Machine translation and the in-
formation soup, third conference of the association
for machine translation in the americas, amta ?98,
langhorne, pa, usa, october 28-31, 1998, proceed-
ings. In AMTA, volume 1529 of Lecture Notes in
Computer Science. Springer.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA.
Amir Hazem and Emmanuel Morin. 2012. Ica for
bilingual lexicon extraction from comparable cor-
pora. In Proceedings of the 5th Workshop on Build-
ing and Using Comparable Corpora (BUCC), Istan-
bul, Turkey.
Hiroyuki Kaji. 2003. Word sense acquisition from
bilingual comparable corpora. In HLT-NAACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Erwin Marsi and Emiel Krahmer. 2010. Automatic
analysis of semantic similarity in comparable text
through syntactic tree matching. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 752?760,
Beijing, China, August. Coling 2010 Organizing
Committee.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Computational Lin-
guistics, 31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable english and spanish corpora.
In Proceedings of MT Summit XI, pages 191?198.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Machine Translation Summit 2009, page 8.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Xabier Saralegi, In?aki San Vicente, and Antton Gur-
rutxaga. 2008. Automatic extraction of bilingual
terms from comparable corpora in a popular sci-
ence domain. In Proceedings of the Building and
using Comparable Corpora workshop, 6th Interna-
tional Conference on Language Resources and Eval-
uations (LREC), Marrakech, Morocco.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of the International Conference on New Methods
in Language Processing, pages 44?49, Manchester,
UK.
9
Li Shao and Hwee Tou Ng. 2004. Mining new
word translations from comparable corpora. In Pro-
ceedings of Coling 2004, pages 618?624, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
Matthew G. Snover, Bonnie J. Dorr, and Richard M.
Schwartz. 2008. Language and translation model
adaptation using comparable corpora. In EMNLP,
pages 857?866.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, and Dan Tufi. 2006.
The jrc-acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In In Proceedings of the 5th
International Conference on Language Resources
and Evaluation (LREC?2006), pages 2142?2147.
Kun Yu and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 121?124, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
10
