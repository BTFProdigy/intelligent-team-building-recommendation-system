Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 535?545, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Modelling Sequential Text with an Adaptive Topic Model
Lan Du?
Department of Computing
Macquarie University
Sydney, Australia
lan.du@mq.edu.au
Wray Buntine?
Canberra Research Lab
National ICT Australia
Canberra, Australia
wray.buntine@nicta.com.au
Huidong Jin?
CSIRO Mathematics, Informatics
and Statistics,
Canberra, Australia
warren.jin@csiro.au
Abstract
Topic models are increasingly being used for
text analysis tasks, often times replacing ear-
lier semantic techniques such as latent seman-
tic analysis. In this paper, we develop a novel
adaptive topic model with the ability to adapt
topics from both the previous segment and the
parent document. For this proposed model, a
Gibbs sampler is developed for doing poste-
rior inference. Experimental results show that
with topic adaptation, our model significantly
improves over existing approaches in terms of
perplexity, and is able to uncover clear se-
quential structure on, for example, Herman
Melville?s book ?Moby Dick?.
1 Introduction
Natural language text usually consists of topically
structured and coherent components, such as groups
of sentences that form paragraphs and groups of
paragraphs that form sections. Topical coherence in
documents facilitates readers? comprehension, and
reflects the author?s intended structure. Capturing
this structural topical dependency should lead to im-
proved topic modelling. It also seems reasonable
to propose that text analysis tasks that involve the
structure of a document, for instance, summarisation
and segmentation, should also be improved by topic
models that better model that structure.
Recently, topic models are increasingly being
used for text analysis tasks such as summarisa-
?This work was partially done when Du was at College of
Engineering & Computer Science, the Australian National Uni-
versity when working together with Buntine and Jin there.
tion (Arora and Ravindran, 2008) and segmenta-
tion (Misra et al2011; Eisenstein and Barzilay,
2008), often times replacing earlier semantic tech-
niques such as latent semantic analysis (Deerwester
et al1990). Topic models can be improved by bet-
ter modelling the semantic aspects of text, for in-
stance integrating collocations into the model (John-
son, 2010; Hardisty et al2010) or encouraging top-
ics to be more semantically coherent (Newman et
al., 2011) based on lexical coherence models (New-
man et al2010), modelling the structural aspects
of documents, for instance modelling a document
as a set of segments (Du et al2010; Wang et al
2011; Chen et al2009), or improving the under-
lying statistical methods (Teh et al2006; Wallach
et al2009). Topic models, like statistical parsing
methods, are using more sophisticated latent vari-
able methods in order to model different aspects of
these problems.
In this paper, we are interested in developing a
new topic model which can take into account the
structural topic dependency by following the higher
level document subject structure, but we hope to re-
tain the general flavour of topic models, where com-
ponents (e.g., sentences) can be a mixture of topics.
Thus we need to depart from the earlier HMM style
models, see, e.g., (Blei and Moreno, 2001; Gruber
et al2007). Inspired by the idea that documents
usually exhibits internal structure (e.g., (Wang et al
2011)), in which semantically related units are clus-
tered together to form semantically structural seg-
ments, we treat documents as sequences of segments
(e.g., sentences, paragraphs, sections, or chapters).
In this way, we can model the topic correlation be-
535
~?
~?1 ~?2 ~?3 ~?4
~?
~?1 ~?2 ~?3 ~?4
~?
~?1 ~?2 ~?3 ~?4
(H) (S)
(M)~?
~?1 ~?2 ~?3 ~?4
(B)
Figure 1: Different structural relationships for topics of
sections in a 4-part document, hierarchical (H), sequen-
tial (S), both (B) or mixed (M).
tween the segments in a ?bag of segments? fashion,
i.e., beyond the ?bag of words? assumption, and re-
veal how topics evolve among segments.
Indeed, we were impressed by the improvement
in perplexity obtained by the segmented topic model
(STM) (Du et al2010), so we considered the prob-
lem of whether one can add sequence information
into a structured topic model as well. Figure 1 illus-
trates the type of structural information being con-
sidered, where the vectors are some representation
of the content. STM is represented by the hierar-
chical model. A strictly sequential model would
seem unrealistic for some documents, for instance
books. A topic model using the strictly sequential
model was developed (Du et al2012) but it report-
edly performs halfway between STM and LDA. In
this paper, we develop an adaptive topic model to
go beyond a strictly sequential model while allow
some hierarchical influence. There are two possible
hybrids, one called ?mixed? has distinct breaks in
the sequence, while the other called ?both? overlays
both sequence and hierarchy and there could be rel-
ative strengths associated with the arrows. We em-
ploy the ?both? hybrid but use the relative strengths
to adaptively allow it to approximate the ?mixed?
hybrid.
Research in Machine Learning and Natural Lan-
guage Processing has attempted to model various
topical dependencies. Some work considers struc-
ture within the sentence level by mixing hidden
Markov models (HMMs) and topics on a word by
word basis: the aspect HMM (Blei and Moreno,
2001) and the HMM-LDA model (Griffiths et al
2005) that models both short-range syntactic depen-
dencies and longer semantic dependencies. These
models operate at a finer level than we are consider-
ing at a segment (like paragraph or section) level. To
make a tool like the HMM work at higher levels, one
needs to make stronger assumptions, for instance as-
signing each sentence a single topic and then topic
specific word models can be used: the hidden topic
Markov model (Gruber et al2007) that models the
transitional topic structure; a global model based on
the generalised Mallows model (Chen et al2009),
and a HMM based content model (Barzilay and
Lee, 2004). Researchers have also considered time-
series of topics: various kinds of dynamic topic
models, following early work of (Blei and Lafferty,
2006), represent a collection as a sequence of sub-
collections in epochs. Here, one is modelling the
collections over broad epochs, not the structure of a
single document that our model considers.
This paper is organised as follows. We first
present background theory in Section 2. Then the
new model is presented in Section 3, followed by
Gibbs sampling theory and algorithm in Sections 4
and 5 respectively. Experiments are reported in Sec-
tion 6 with a conclusion in Section 7.
2 Background
The basic topic model is first presented in Sec-
tion 2.1, as a point of departure. In seeking to de-
velop a general sequential topic model, we hope
to go beyond a strictly sequential model and allow
some hierarchical influence. This, however, presents
two challenges: modelling and statistical inference.
Hierarchical inference (and thus sequential infer-
ence) over probability vectors can be handled us-
ing the theory of hierarchical Poisson-Dirichlet pro-
cesses (PDPs). This is presented in Section 2.2.
2.1 The LDA model
The benchmark model for topic modelling is latent
Dirichlet alation (LDA) (Blei et al2003), a la-
tent variable model of documents. Documents are
indexed by i, and words ~w are observed data. The
latent variables are ~?i (the topic distribution for a
document) and ~z (the topic assignments for observed
words), and the model parameter of ~?k?s (word dis-
tributions). These notation are later extended in Ta-
536
ble 1. The generative model is as follows:
~?k ? DirichletW (~?) ? k
~?i ? DirichletK (~?) ? i
zi,l ? DiscreteK (~?i) ? i, l
wi,l ? DiscreteK
(
~?zi,l
)
? i, l .
DirichletK(?) is a K-dimensional Dirichlet distribu-
tion. The hyper-parameter ~? is a Dirichlet prior on
word distributions (i.e., a Dirichlet smoothing on the
multinomial parameter ~?k (Blei et al2003)) and the
Dirichlet prior ~? on topic distributions.
2.2 Hierarchical PDPs
A discrete probability vector ~? of finite dimension
K is sampled from some distribution F? (~?0) with
a parameter set, say ? , and is also dependent on
a parent probability vector ~?0 also of finite dimen-
sion K. Then a sample of size N is taken ac-
cording to the probability vector ~?, represented as
~z ? {1, ...,K}N . This data is collected into counts
~n = (n1, ..., nK) where nk is the number of data in
~z with value k and
?
k nk = N . This situation is
represented as follows:
~? ? F? (~?0); ~zi ? DiscreteK(~?) for i = 1, ..., N .
Commonly in topic modelling, the Dirichlet distri-
bution is used for discrete probability vectors. In
this case F? (~?0) ? DirichletK(b~?0), ? ? (K, b)
where b is the concentration parameter. Bayesian
analysis yields a marginalised likelihood, after inte-
grating out ~?, of
p
(
~z
?
??, ~?0,Dirichlet
)
=
Beta (~n+ b~?0)
Beta (b~?0)
, (1)
where Beta(?) is the vector valued function normal-
ising the Dirichlet distribution. A problem here is
that p(~z|b, ~?0) is an intractable function of ~?0.
Dirichlet processes and Poisson-Dirichlet pro-
cesses alleviate this problem by using an auxiliary
variable trick (Robert and Casella, 2004). That is,
we introduce an auxiliary variable over which we
also sample but do not need to record. The auxiliary
variable is the table count1 which is a tk for each nk
1Based on the Chinese Restaurant analogy (Teh et al2006),
each table has a dish, a data value, while data, the customer, is
assigned to tables, and multiple tables can serve the same dish.
and it represents the number of ?tables? over which
the nk ?customers? are spread out. Thus the follow-
ing constraints hold:
0 ? tk ? nk and tk = 0 iff nk = 0 . (2)
When the distribution over probability vectors fol-
lows a Poisson-Dirichlet process which has two pa-
rameters ? ? (a, b) and the parent distribution ~?0,
then F? (~?0) ? PDP(a, b, ~?0). Here a is the dis-
count parameter, b the concentration parameter and
~?0 the base measure. In this case Bayesian analysis
yields an augmented marginalised likelihood (Bun-
tine and Hutter, 2012), after integrating out ~?, of
p
(
~z,~t
?
??, ~?0,PDP
)
=
(b|a)T
(b)N
?
k
Snktk,a (?0,k)
tk (3)
where T =
?
k tk, (x|y)N =
?N?1
n=0 (x + ny) de-
notes the Pochhammer symbol, (x)N = (x|1)N , and
SNM,a is a generalized Stirling number that is readily
tabulated (Buntine and Hutter, 2012).
There are two fundamental things to notice about
Equation (3). Positively, the term in ~?0 takes the
form of a multinomial likelihood, so we can prop-
agate it up and perform inference on ~?0 unen-
cumbered by the functional mess of Equation (1).
Thus Poisson-Dirichlet processes allow one to do
Bayesian reasoning on hierarchies of probability
vectors (Teh, 2006; Teh et al2006). Negatively,
however, one needs to sample the auxiliary vari-
ables ~t leading to some problems: The range of tk,
{0, ..., nk}, is broad. Also, contributions from in-
dividual data zi have been lost so the mixing of the
MCMC can sometimes be slow. We confirmed these
problems on our first implementation of the Adap-
tive Topic Model presented next in Section 3.
A further improvement on PDP sampling is
achieved in (Chen et al2011), where another aux-
iliary variable is introduced, a so-called table in-
dicator, that for each datum zi indicates whether
it is the ?head of its table? (recall the nk data are
spread over tk tables, each table has one and only
one ?head?). Let ri = 1 if zi is the ?head of its
table,? and zero otherwise. According to this ?ta-
ble? logic, the number of tables for nk must be the
number of data zi that are also head of table, so
tk =
?N
i=1 1zi=k1ri=1. Moreover, given this def-
inition, the first constraint of Equation (2) on tk is
537
automatically satisfied. Finally, with tk tables then
there must be exactly tk heads of table, and we are
indifferent about which data are heads of table, thus
p
(
~z, ~r
?
??, ~?0,PDP
)
= p
(
~z,~t
?
??, ~?0,PDP
)?
k
(
nk
tk
)?1
.
(4)
When using this marginalised likelihood in a Gibbs
sampler, the zi themselves are usually latent so also
sampled, and we develop a blocked Gibbs sampler
for (zi, ri). Since ~r only appears indirectly through
the table counts ~t, one does not need to store the ~r,
instead just resamples an ri when needed according
to the proportion tw/nw where zi = w.
3 The proposed Adaptive Topic Model
In this section an adaptive topic model (AdaTM) is
developed, a fully structured topic model, by using
a PDP to simultaneously model the hierarchical and
the sequential topic structures. Documents are as-
sumed to be broken into a sequence of segments.
Topic distributions are used to mimic the subjects of
documents and subtopics of their segments. The no-
tations and terminologies used in the following sec-
tions are given in Table 1.
In AdaTM, the two topic structures are captured
by drawing topic distributions from the PDPs with
two base distributions as follows. The document
topic distribution ~?i and the jth segment topic dis-
Table 1: List of notation for AdaTM
K number of topics
I number of documents
Ji number of segments in document i
Li,j number of words in document i, segment j
W number of words in dictionary
~?i document topic probabilities for document i
~? K-dimensional prior for each ~?i
~?i,j segment topic probabilities for document i and
segment j
?i,j mixture weight associating with the link be-
tween ~?i.j and ~?i,j?1
~? word probability vectors as a K ?W matrix
~?k word probability vector for topic k, entries in ?
~? W -dimensional prior for each ~?k
wi,j,l word in document i, segment j, position l
zi,j,l topic for word wi,j,l
w
L
z
I
K
?
?
?
?
?
1
?
2
1
w
L
z
2
???
?
J
w
L
z
J
???
?
Figure 2: The adaptive topic model: ~? is the document
topic distribution, ~?1, ~?2, . . . , ~?J are the segment topic
distributions, and ~? is a set of the mixture weights.
tribution ~?i,j are linearly combined to give a base
distribution for the (j + 1)th segment?s topic dis-
tribution ~?i,j+1. The topic distribution of the first
segment, i.e., ~?i,1, is drawn directly with the base
distribution ~?i. Call this generative process topic
adaptation. The graphical representation of AdaTM
is shown in Figure 2, and clearly shows the combi-
nation of sequence and hierarchy for the topic prob-
abilities. Note the linear combination at each node
~?i,j is weighted with latent proportions ?i,j .
The resultant model for AdaTM is:
~?k ? DirichletW (~?) ? k
~?i ? DirichletK (~?) ? i
?i,j ? Beta(?S , ?T ) ? i, j
~?i,j ? PDP (?i,j~?i,j?1 + (1? ?i,j)~?i, a, b)
zi,j,l ? DiscreteK (~?i,j) ? i, j, l
wi,j,l ? DiscreteK
(
~?zi,j,l
)
? i, j, l .
For notational convenience, let ~?i,0 = ~?i. Assume
the dimensionality of the Dirichlet distribution (i.e.,
the number of topics) is known and fixed, and word
probabilities are parameterised with aK?W matrix
~? = (~?1, ..., ~?K).
4 Gibbs Sampling Formulation
Given observations and model parameters, comput-
ing the posterior distribution of latent variables is in-
feasible for AdaTM due to the intractable computa-
538
Table 2: List of statistics for AdaTM
Mi,k,w the total number of words in document i with
dictionary index w and being assigned to topic
k
Mk,w total Mi,k,w for document i, i.e.,
?
iMi,k,w
~Mk vector of W values Mk,w
ni,j,k topic count in document i segment j for topic k
Ni,j topic total in document i segment j, i.e.,?K
k=1 ni,j,k
ti,j,k table count in the CPR for document i and para-
graph j, for topic k that is inherited back to
paragraph j ? 1 and ~?i,j?1.
si,j,k table count in the CPR for document i and para-
graph j, for topic k that is inherited back to the
document and ~?i.
Ti,j total table count in the CRP for document i and
segment j, equal to
?K
k=1 ti,j,k.
Si,j total table count in the CRP for document i and
segment j, equal to
?K
k=1 si,j,k.
~ti,j table count vector of ti,j,k?s for segment j.
~si,j table count vector of si,j,k?s for segment j.
tion of marginal probabilities. Therefore, we have to
use approximate inference techniques. This section
proposes a blocked Gibbs sampling algorithm based
on methods from Chen et al2011). Table 2 lists
all statistics needed in the algorithm. Note for easier
understanding, terminologies of the Chinese Restau-
rant Process (Teh et al2006) will be used, i.e., cus-
tomers, dishes and restaurants, correspond to words,
topics and segments respectively.
The first major complication, over the use of the
hierarchical PDP and Equation (3) and the table in-
dicator trick of Equation (4), is handling the lin-
ear combination of ?i,j~?i,j?1 + (1 ? ?i,j)~?i used
in the PDPs. We manage this as follows: First,
Equation (3) shows that a contribution of the form
(?0,k)tk results. In our case, this becomes
?
k
(?i,j?i,j?1,k + (1? ?i,j)?i,k)
t?i,j,k
where t?i,j,k is the corresponding introduced auxil-
iary variable the table count which is involved with
constraints on ni,j,k+ti,j+1,k, from Equation (2). To
deal with this power of a sum, we break the counts
t?i,j,k into two parts, those that contribute to ~?i,j?1
and those that contribute to ~?i. We call these parts
ti,j,k and si,j,k respectively. The product can then be
expanded and ?i,j integrated out. This yields:
Beta (Si,j + ?S , Ti,j + ?T )
?
k
?
ti,j,k
i,j?1,k?
si,j,k
i,k .
The powers ?
ti,j,k
i,j?1,k and ?
si,j,k
i,k can then be pushed
up to the next nodes in the PDP/Dirichlet hierarchy.
Note the standard constraints and table indicators are
also needed here.
The precise form of the table indicators needs to
be considered as well since there is a hierarchy for
them, and this is the second major complication in
the model. As discussed in Chen et al2011), table
indicators are not required to be recorded, instead,
randomly sampled in Gibbs cycles. The table indi-
cators when known can be used to reconstruct the
table counts ti,j,k and si,j,k, and are reconstructed
by sampling from them. For now, denote the table
indicators as ui,j,l for word wi,j,l.
To complete a formulation suitable for Gibbs
sampling, we first compute the marginal distribu-
tion of the observations ~w1:I,1:J (words), the topic
assignments ~z1:I,1:J and the table indicators ~u1:I,1:J .
The Dirichlet integral is used to integrate out the
document topic distributions ~?1:I and the topic-
by-words matrix ~?, and the joint posterior dis-
tribution computed for a PDP is used to recur-
sively marginalise out the segment topic distribu-
tions ~?1:I,1:J . With these variables marginalised out,
we derive the following marginal distribution
p(~z1:I,1:J , ~w1:I,1:J , ~u1:I,1:J
?
? ~?,~?, a, b) = (5)
I?
i=1
BetaK
(
~?+
?Ji
j=1 ~si,j
)
BetaK (~?)
K?
k=1
BetaW
(
~? + ~Mk
)
BetaW (~?)
I?
i=1
Ji?
j=1
Beta (Si,j + ?S , Ti,j + ?T )
(b|a)Ti,j+Si,j
(b)Ni,j+Ti,j+1
I?
i=1
Ji?
j=1
K?
k=1
(
(ni,j,k + ti,j+1,k)
(ti,j,k + si,j,k)
)?1
S
ni,j,k+ti,j+1,k
ti,j,k+si,j,k,a .
And the following constraints apply:
ti,j,k + si,j,k ? ni,j,k + ti,j+1,k, (6)
ti,j,k + si,j,k = 0 iff ni,j,k + ti,j+1,k = 0 . (7)
The first constraint falls out naturally when table in-
dicators are used. For convenience of the formulas,
539
set ti,Ji+1,k = 0 (there is no Ji + 1 segment) and
ti,1,k = 0 (the first segment only uses ~?i).
Now let us consider again the table indicators
ui,j,l for word wi,j,l. If this word is in topic k at doc-
ument i and segment j, then it contributes a count to
ni,j,k. It also indicates if it contributes a new table,
or a count to t?i,j,k for the PDP at this node. How-
ever, as we discussed above, this then contributes to
either ti,j,k or si,j,k. If it contributes to ti,j,k, then
it recurses up to contribute a data count to the PDP
for document i segment j ? 1. Thus it also needs a
table indicator at that node. Consequently, the table
indicator ui,j,l for word wi,j,l must specify whether
it contributes a table to all PDP nodes reachable by
it in the graph.
We define ui,j,l specifically as ui,j,l = (u1, u2)
such that u1 ? [?1, 0, 1] and u2 ? [1, ? ? ? , j],
where u2 indicates segment denoted by node ?j
up to which wi,j,l contributes a table. Given u2,
u1 = ?1 denotes wi,j,l contributes a table count to
si,u2,k and ti,j?,k for u2 < j
? ? j; u1 = 0 denotes
wi,j,l does not contribute a table to node u2, but con-
tributes a table count to ti,j?,k for u2 < j? ? j; and
u1 = 1 denotes wi,j,l contributes a table count to
each ti,j?,k for u2 ? j? ? j.
Now, we are ready to compute the conditional
probabilities for jointly sampling topics and table in-
dicators from the model posterior of Equation (5).
5 Gibbs Sampling Algorithm
The Gibbs sampler iterates over words, doing a
blocked sample of (zi,j,l, ui,j,l). The first task is to
reconstruct ui,j,l since it is not stored. Since the pos-
terior of Equation (5) does not explicitly mention
the ui,j,l?s, they occur indirectly through the table
counts, and we can randomly reconstruct them by
sampling them uniformly from the space of possi-
bilities. Following this, we then remove the values
(zi,j,l, ui,j,l) from the full set of statistics. Finally,
we block sample new values for (zi,j,l, ui,j,l) and
add them to the statistics. The new ui,j,l is subse-
quently forgotten and the zi,j,l recorded.
Reconstructing table indicator ui,j,l: We start at
the node indexed i, j. If si,j,k+ti,j,k = 1 and ni,j,k+
ti,j+1,k > 1 then no tables can be removed since
there is only one table but several customers at the
table. Thus ui,j,l = (u1, u2) = (0, j) and there is no
sampling. Otherwise, by symmetry arguments, we
sample u1 via
p(u1 = ?1, 0, 1|u2 = j, zi,j,l = k) ?
(si,j,k, ti,j,k, ni,j,k + ti,j+1,k ? si,j,k ? ti,j,k) ,
since there are ni,j,k+ti,j+1,k data distributed across
the three possibilities. If after sampling u1 = ?1,
the data contributes a table count up to ~?i and so
ui,j,l = (u1, u2) = (?1, j). If u1 = 0, the ui,j,l =
(u1, u2) = (0, j). Otherwise, the data contributes a
table count up to the parent PDP for ~?i,j?1 and we
recurse, repeating the sampling process at the parent
node. Note, however, that the table indicator (0, j?)
for j? < j is equivalent to the table indicator (1, j?+
1) as far as statistics is concerned.
Block sampling (zi,j,l, ui,j,l): The full set of pos-
sibilities are, for each possible topic zi,j,l = k:
? no tables are created, so ui,j,l = (0, j),
? tables are created contributing a table count all
the way up to node j? (? j) but stop at j? and
do not subsequently contribute a count to ~?i, so
ui,j,l = (1, j?),
? tables are created contributing a table count all
the way up to node j? ? j but stop at j? and
also subsequently contribute a count to ~?i, so
ui,j,l = (?1, j?).
These three possibilities lead to detailed but fairly
straight forward changes to the posterior of Equa-
tion (5). Thus a full blocked sampler for (zi,j,l, ui,j,l)
can be constructed.
Estimates: learnt values of ~?i, ~?i,j , ~?k are needed
for evaluation, perplexity calculations, etc. These
are estimated by taking averages after the Gibbs
sampler has burnt in, using the standard posterior
means for Dirichlets and Poisson-Dirichlets.
6 Experiments
In the experimental work, we have three objectives:
(1) to explore the setting of hyper-parameters, (2) to
compare the model with the earlier sequential LDA
(SeqLDA) of (Du et al2012), STM of (Du et al
2010) and standard LDA, and (3) to view the results
in detail on a number of characteristic problems.
540
Table 3: Datasets
#docs #segs #words vocab
Pat-A 500 51,748 2,146,464 16,573
Pat-B 397 9,123 417,631 7,663
Pat-G06 500 11,938 655,694 6,844
Pat-H 500 11,662 562,439 10,114
Pat-F 140 3,181 166,091 4,674
Prince-C 1 26 10,588 3,292
Prince-P 1 192 10.588 3,292
Moby Dick 1 135 88,802 16,223
6.1 Datasets
For general testing, five patent datasets are ran-
domly selected from U.S. patents granted in 2009
and 2010. Patents in Pat-A are selected from in-
ternational patent class (IPC) ?A?, which is about
?HUMAN NECESSITIES?; those in Pat-B are se-
lected from class ?B60? about ?VEHICLES IN
GENERAL?; those in Pat-H are selected from
class ?H? about ?ELECTRICITY?; those in Pat-
F are selected from class ?F? about ?MECHAN-
ICAL ENGINEERING; LIGHTING; HEATING;
WEAPONS; BLASTING?; and those in Pat-G are
selected from class ?G06? about ?COMPUTING;
CALCULATING; COUNTING?. All the patents in
these five datasets are split into paragraphs that are
taken as segments, and the sequence of paragraphs
in each patent is reserved in order to maintain the
original layout. All the stop words, the top 10 com-
mon words, the uncommon words (i.e., words in less
than five patents) and numbers have been removed.
Two books used for more detailed investigation
are ?The Prince? by Niccolo` Machiavelli and ?Moby
Dick? by Herman Melville. They are split into chap-
ters and/or paragraphs which are treated as seg-
ments, and only stop-words are removed. Table 3
shows in detail the statistics of these datasets after
preprocessing.
6.2 Design
Perplexity, a standard measure of dictionary-based
compressibility, is used for comparison. When re-
porting test perplexities, the held-out perplexity
measure (Rosen-Zvi et al2004) is used to evaluate
the generalisation capability to the unseen data. This
is known to be unbiased. To compute the held-out
perplexity, 20% of patents in each data set was ran-
1025 50 100 150 200 250 300800900
10001100
12001300
b
Perplexity
 
 Pat?BPat?FPat?GPat?H
(a) fix a = 0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9800
950
1100
1250
a
Perplexity
 
 
Pat?BPat?GPat?HPat?F
(b) fix b = 10
Figure 3: Analysis of parameters of Poisson-Dirichlet
process. (a) shows how perplexity changes with b; (b)
shows how it changes with a.
0 50 100 150 200600700
8009001000
11001200
Lamda_S
Perplexity
 
 
Pat?BPat?FPat?GPat?HPat?A
(a) fix ?T = 1
0 50 100 150 200600700
8009001000
11001200
Lamda_T
Perplexity
 
 
Pat?APat?BPat?FPat?HPat?G
(b) fix ?S = 1
Figure 4: Analysis of the two parameters for Beta distri-
bution. (a) how perplexity changes with ?S ; (b) how it
changes with ?T .
domly held out from training to be used for testing.
For this, 1000 Gibbs cycles were done for burn-in
followed by 500 cycles with a lag for 100 for pa-
rameter estimation.
We implemented all the four models, e.g., LDA,
STM, SeqTM and AdaTM in C, and ran them on a
desktop with Intel Core i5 CPU (2.8GHz?4), even
though our code is not multi-threaded. Perplexity
calculations, data input and handling, etc., were the
same for all algorithms. We note that the current
AdaTM implementation is an order of magnitude
slower than regular LDA per major Gibbs cycle.
6.3 Hyper-parameters in AdaTM
Experiments on the impact of the hyper-parameters
on the patent data sets were as follows: First, fixing
K = 50, the Beta parameters ?T = 1 and ?S = 1,
optimise symmetric ?, and do two variations fix-a:
a = 0.0, trying b = 1, 5, 10, 25, ..., 300, and fix-b:
b = 10, trying a = 0.1, 0.2, ..., 0.9. Second, fix-?T
(fix-?S): fix a = 0.2 and ?T (?S) = 1, optimise
b and ?, change ?S(?T ) = 0.1, 1, 10, 50, 100, 200.
Figures 3 and 4 show the corresponding plots. Fig-
ure 3(b) and Figure 4(a) show that varying the val-
ues of a and ?S does not significantly change the
541
510 25 50 100 150630930
12301530
18302130
2430
Number of Topics
Perple
xity
 
 LDA_DLDA_PSeqLDASTMAdaTM
(a) Pat-A
0510 25 50 100 1509101060
12101360
15101660
1810
Number of Topics
Perple
xity
 
 LDA_DLDA_PSeqLDASTMAdaTM
(b) Pat-H
510 25 50 100 150700850
10001150
13001450
1600
Number of Topics
Perple
xity
 
 LDA_DLDA_PSeqLDASTMAdaTM
(c) Pat-B
0510 25 50 100 150670
820970
11201270
1420
Number of Topics
Perple
xity
 
 LDA_DLDA_PSeqLDASTMAdaTM
(d) Pat-F
0510 25 50 100 150910
11101310
15101710
1910
Number of Topics
Perple
xity
 
 LDA_DLDA_PSeqLDASTMAdaTM
(e) Pat-G
510 25 50 100 150?10015
4065
90115
140160
Number of Topics
Perplex
ity Diffe
rence
 
 Pat?APat?BPat?FPat?GPat?H
(f) Shuffle
Figure 5: Perplexity comparisons.
perplexity. In contrast, Figure 3(a) shows different
b values significantly change perplexity. Therefore,
we sought to optimise b. The experiment of fixing
?S = 1 and changing ?T shows a small ?T is pre-
ferred.
6.4 Perplexity Comparison
Perplexity comparisons were done with the default
settings a = 0.2, ? = 0.1, ? = 0.01, ?S = 1,
?T = 1 and b optimised automatically using the
scheme from (Du et al2012). Figure 5 shows
the results on these five patent datasets for differ-
ent numbers of topics. LDA D is LDA run on whole
patents, and LDA P is LDA run on the paragraphs
within patents. Table 4 gives the p-values of a one-
tail paired t-test for AdaTM versus the others, where
lower p-value indicates AdaTM has statistically sig-
nificant lower perplexity. From this we can see that
AdaTM is statistically significantly better than Se-
qLDA and LDA, and somewhat better than STM.
In addition, we ran another set of experiments
by randomly shuffling the order of paragraphs in
each patent several times before running AdaTM.
Then, we calculate the difference between perplex-
ities with and without random shuffle. Figure 5(f)
shows the plot of differences in each data sets. The
positive difference means randomly shuffling the or-
der of paragraphs indeed increases the perplexity.
It can further prove that there does exist sequential
topic structure in patents, which confirms the finding
in (Du et al2012).
6.5 Topic Evolution Comparisons
All the comparison experiments reported in this sec-
tion are run with 20 topics, the upper limit for easy
visualisation, and without optimising any parame-
ters. The Dirichlet Priors are fixed as ?k = 0.1
and ?w = 0.01. For AdaTM, SeqLDA, and STM,
a = 0.0 and b = 100 for ?The Prince? and b = 200
for ?Moby Dick?. These settings have proven ro-
bust in experiments. To align the topics so visual-
isations match, the sequential models are initialised
using an LDA model built at the chapter level. More-
over, all the models are run at both the chapter and
the paragraph level. With the common initialisation,
both paragraph level and chapter level models can
Table 4: P-values for one-tail paired t-test on the five
patent datasets.
AdaTM
Pat-G Pat-A Pat-F Pat-H Pat-B
LDA D .0001 .0001 .0002 .0001 .0001
LDA P .0041 .0030 .0022 .0071 .0096
SeqLDA .0029 .0047 .0003 .0012 .0023
STM .0220 .0066 .0210 .0629 .0853
542
(a) Evolution of paragraph topics for LDA
(b) Topic alignment of LDA versus AdaTM top-
ics for chapters
Figure 6: Analysis on ?The Prince?.
be aligned.
To visualise topic evolution, we use a plot with
one colour per topic displayed over the sequence.
Figure 6(a) shows this for LDA run on paragraphs
of ?The Prince?. The proportion of 20 topics is the
Y-axis, spread across the unit interval. The para-
graphs run along the X-axis, so the topic evolution
is clearly displayed. One can see there is no se-
quential structure in this derived by the LDA model,
and similar plots result from ?Moby Dick? for LDA.
Figure 6(b) shows the alignment of topics between
the initialising model (LDA+chapters) and AdaTM
run on chapters. Each point in the matrix gives the
Hellinger distance between the corresponding top-
ics, color coded. The plots for the other models,
chapters or paragraphs, are similar so plots like Fig-
ure 6(a) for the other models can be meaningfully
compared.
Figure 7 then shows the corresponding evolution
plots for AdaTM and SeqLDA on chapters and para-
graphs. The contrast of these with LDA is stark.
The large improvement in perplexity for AdaTM
(see Section 6.4) along with no change in lexi-
cal coherence (see Section 6.2) means that the se-
(a) AdaTM on chapters
(b) AdaTM on paragraphs
(c) SeqLDA on chapters
(d) SeqLDA on paragraphs
Figure 7: Topic Evolution on ?The Prince?.
quential information is actually beneficial statisti-
cally. Note that SeqLDA, while exhibiting slightly
stronger sequential structure than AdaTM in these
543
(a) LDA on chapters
(b) STM on Chapters
(c) AdaTM on Chapters
Figure 8: Topic Evolution on ?Moby Dick?.
figures has significantly worse test perplexity, so its
sequential affect is too strong and harming results.
Also, note that some topics have different time se-
quence profiles between AdaTM and SeqLDA. In-
deed, inspection of the top words for each show
these topics differ somewhat. So while the LDA
to AdaTM/SeqLDA topic correspondences are quite
good due to the use of LDA initialisation, the cor-
respondences between AdaTM and SeqLDA have
degraded. We see that AdaTM has nearly as good
sequential characteristics as SeqLDA. Furthermore,
segment topic distribution ?i,j of SeqLDA are grad-
ually deviating from the document topic distribution
?i, which is not the case for AdaTM.
Results for ?Moby Dick? on chapters are com-
parable. Figure 8 shows similar topic evolution
plots for LDA, STM and AdaTM. In contrast, the
AdaTM topic evolutions are much clearer for the
less frequent topics, as shown in Figure 8(c). Var-
ious parts of this are readily interpreted from the
storyline. Here we briefly discuss topics by their
colour: black: Captain Peleg and the business of
signing on; yellow: inns, housing, bed; mauve:
Queequeg; azure: (around chapters 60-80) details
of whales aqua: (peaks at 8, 82, 88) pulpit, schools
and mythology of whaling.
We see that AdaTM can be used to understand the
topics with regards to the sequential structure of a
book. In contrast, the sequential nature for LDA and
STM is lost in the noise. It can be very interesting to
apply the proposed topic models to some text anal-
ysis tasks, such as topic segmentation, summarisa-
tion, and semantic title evaluation, which are subject
to our future work.
7 Conclusion
A model for adaptive sequential topic modelling has
been developed to improve over a simple exchange-
able segments model STM (Du et al2010) and a
naive sequential model SeqLDA (Du et al2012) in
terms of perplexity and its confirmed ability to un-
cover sequential structure in the topics. One could
extract meaningful topics from a book like Herman
Melville?s ?Moby Dick? and concurrently gain their
sequential profile. The current Gibbs sampler is
slower than regular LDA, so future work is to speed
up the algorithm.
Acknowledgments
The authors would like to thank all the anonymous re-
viewers for their valuable comments. Lan Du was
supported under the Australian Research Council?s
Discovery Projects funding scheme (project numbers
DP110102506 and DP110102593). Dr. Huidong Jin
was partly supported by CSIRO Mathematics, Informat-
ics and Statistics for this work. NICTA is funded by the
Australian Government as represented by the Department
of Broadband, Communications and the Digital Econ-
omy and the Australian Research Council through the
ICT Center of Excellence program.
544
References
R. Arora and B. Ravindran. 2008. Latent Dirichlet al
cation and singular value decomposition based multi-
document summarization. In ICDM ?08: Proc. of
2008 Eighth IEEE Inter. Conf. on Data Mining, pages
713?718.
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to genera-
tion and summarization. In HLT-NAACL 2004: Main
Proceedings, pages 113?120. Association for Compu-
tational Linguistics.
D.M. Blei and J.D. Lafferty. 2006. Dynamic topic mod-
els. In ICML ?06: Proc. of 23rd international confer-
ence on Machine learning, pages 113?120.
D.M. Blei and P.J. Moreno. 2001. Topic segmenta-
tion with an aspect hidden Markov model. In Proc.
of 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 343?348.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alation. Journal of Machine Learning Re-
search, 3:993?1022.
W. Buntine and M. Hutter. 2012. A Bayesian view
of the Poisson-Dirichlet process. Technical Report
arXiv:1007.0296v2, ArXiv, Cornell, February.
H. Chen, S.R.K. Branavan, R. Barzilay, and D.R. Karger.
2009. Global models of document structure using la-
tent permutations. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conf. of the
North American Chapter of the Association for Com-
putational Linguistics, pages 371?379, Stroudsburg,
PA, USA. Association for Computational Linguistics.
C. Chen, L. Du, and W. Buntine. 2011. Sampling for the
Poisson-Dirichlet process. In European Conf. on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Database, pages 296?311.
S.C. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Fur-
nas, and R.A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
L. Du, W. Buntine, and H. Jin. 2010. A segmented topic
model based on the two-parameter Poisson-Dirichlet
process. Machine Learning, 81:5?19.
L. Du, W. Buntine, H. Jin, and C. Chen. 2012. Sequential
latent dirichlet alation. Knowledge and Information
Systems, 31(3):475?503.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsuper-
vised topic segmentation. In Proc. of Conf. on Empir-
ical Methods in Natural Language Processing, pages
334?343. Association for Computational Linguistics.
T.L. Griffiths, M. Steyvers, D.M. Blei, and J.B. Tenen-
baum. 2005. Integrating topics and syntax. In Ad-
vances in Neural Information Processing Systems 17,
pages 537?544.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
topic markov models. Journal of Machine Learning
Research - Proceedings Track, 2:163?170.
E.A. Hardisty, J. Boyd-Graber, and P. Resnik. 2010.
Modeling perspective using adaptor grammars. In
Proc. of the 2010 Conf. on Empirical Methods in Nat-
ural Language Processing, pages 284?292, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
M. Johnson. 2010. PCFGs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proc. of 48th Annual Meeting
of the ACL, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
H. Misra, F. Yvon, O. Capp, and J. Jose. 2011. Text seg-
mentation: A topic modeling perspective. Information
Processing & Management, 47(4):528?544.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010.
Automatic evaluation of topic coherence. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies,
pages 100?108.
D. Newman, E.V. Bonilla, and W. Buntine. 2011. Im-
proving topic coherence with regularized topic mod-
els. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems 24,
pages 496?504.
C.P. Robert and G. Casella. 2004. Monte Carlo statisti-
cal methods. Springer. second edition.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. In Proc. of 20th conference on Uncertainty in
Artificial Intelligence, pages 487?494.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
21st Inter. Conf. on Computational Linguistics and the
44th annual meeting of the Association for Computa-
tional Linguistics, pages 985?992.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking LDA: Why priors matter. In Advances in
Neural Information Processing Systems 19.
H. Wang, D. Zhang, and C. Zhai. 2011. Structural topic
model for latent topical structure analysis. In Proc. of
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 1526?1535, Stroudsburg, PA, USA.
Association for Computational Linguistics.
545
Proceedings of NAACL-HLT 2013, pages 190?200,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Topic Segmentation with a Structured Topic Model
Lan Du
Department of Computing
Macquarie University
Sydney, Australia
lan.du@mq.edu.au
Wray Buntine
Canberra Research Lab
National ICT Australia
Canberra, Australia
wray.buntine@nicta.com.au
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.au
Abstract
We present a new hierarchical Bayesian model
for unsupervised topic segmentation. This new
model integrates a point-wise boundary sam-
pling algorithm used in Bayesian segmenta-
tion into a structured topic model that can cap-
ture a simple hierarchical topic structure latent
in documents. We develop an MCMC infer-
ence algorithm to split/merge segment(s). Ex-
perimental results show that our model out-
performs previous unsupervised segmentation
methods using only lexical information on
Choi?s datasets and two meeting transcripts
and has performance comparable to those pre-
vious methods on two written datasets.
1 Introduction
Documents are usually comprised of topically co-
herent text segments, each of which contains some
number of text passages (e.g., sentences or para-
graphs) (Salton et al, 1996). Within each topically
coherent segment, one would expect that the word
usage demonstrates more consistent lexical distri-
butions (known as lexical cohesion (Eisenstein and
Barzilay, 2008)) than that across segments. A linear
partition of texts into topic segments may reveal in-
formation about, for example, themes of segments
and the overall thematic structure of the text, and
can subsequently be useful for text analysis tasks,
such as information retrieval (e.g., passage retrieval
(Salton et al, 1996)), document summarisation and
discourse analysis (Galley et al, 2003).
In this paper we consider how to automatically
find a topic segmentation. It involves identifying
the most prominent topic changes in a sequence
of text passages, and splits those passages into a
sequence of topically coherent segments (Hearst,
1997; Beeferman et al, 1999). This task can be cast
as an unsupervised machine learning problem: plac-
ing topic boundaries in unannotated text.
Although a variety of cues in text can be used for
topic segmentation, such as cue phases (Beeferman
et al, 1999; Reynar, 1999; Eisenstein and Barzi-
lay, 2008)) and discourse information (Galley et al,
2003), in this paper, we focus on lexical cohesion
and use it as the primary cue in developing an un-
supervised segmentation model. The effectiveness
of lexical cohesion has been demonstrated by Text-
Tiling (Hearst, 1997), c99 (Choi, 2000), MinCut
(Malioutov and Barzilay, 2006), PLDA (Purver et
al., 2006), Bayesseg (Eisenstein and Barzilay, 2008),
TopicTiling (Riedl and Biemann, 2012), etc.
Our work uses recent progress in hierarchi-
cal topic modelling with non-parametric Bayesian
methods (Du et al, 2010; Chen et al, 2011; Du et
al., 2012a), and is based on Bayesian segmentation
methods (Goldwater et al, 2009; Purver et al, 2006;
Eisenstein and Barzilay, 2008) using topic mod-
els. This can also be viewed as a multi-topic exten-
sion of hierarchical Bayesian segmentation (Eisen-
stein, 2009), although our use of hierarchies is used
to improve the performance of linear segmentation,
rather than develop hierarchical segmentation.
Recently, topic models are increasingly used in
various text analysis tasks including topic segmen-
tation. Previous work (Purver et al, 2006; Misra
et al, 2008; Sun et al, 2008; Misra et al, 2009;
Riedl and Biemann, 2012) has shown that using
190
topic assignments or topic distributions instead of
word frequency can significantly improve segmen-
tation performance. Here we consider more ad-
vanced topic models that model dependencies be-
tween (sub-)sections in a document, such as struc-
tured topic models (STMs) presented in (Du et al,
2010; Du et al, 2012b). STMs treat each text as
a sequence of segments, each of which is a set of
text passages (e.g., a paragraph or sentence). Text
passages in a segment share the same prior distribu-
tion on their topics. The topic distributions of seg-
ments in a single document are then encouraged to
be similar via a hierarchical prior. This gives a sub-
stantial improvement in modelling accuracy. How-
ever, instead of explicitly learning the segmentation,
STMs just leverage the existing structure of docu-
ments from the given segmentation.
Given a sequence of text passages, how can we
automatically learn the segmentation? The word
boundary sampling algorithm introduced in (Gold-
water et al, 2009) uses point-wise sampling of word
boundaries after phonemes in an utterance. Simi-
larly, the segmentation method of PLDA (Purver
et al, 2006) samples segment boundaries, but also
jointly samples a topic model. This is different to
other topic modelling approaches that run LDA as
a precursor to a separate segmentation step (Misra
et al, 2009; Riedl and Biemann, 2012). While con-
ceptually similar to PLDA, our non-parametric ap-
proach built on STM required new methods to im-
plement, but the resulting improvement by the stan-
dard segmentation scores is substantial.
This paper presents a new hierarchical Bayesian
unsupervised topic segmentation model, integrating
a point-wise boundary sampling algorithm with a
structured topic model. This new model takes ad-
vantage of the high modelling accuracy of structured
topic models (Du et al, 2010) to produce a topic
segmentation based on the distribution of latent top-
ics. We show that this model provides high quality
segmentation performance on Choi?s dataset, as well
as two sets of meeting transcripts and written texts.
In the following sections we describe our topic
segmentation model and an MCMC inference al-
gorithm for the non-parametric split/merge pro-
cess. The rest of the paper is organised as follows. In
Section 2 we review recent related work in the topic
segmentation literature. Section 3 presents the new
topic segmentation model, followed by the deriva-
tion of a sampling algorithm in Section 4. We report
the experimental results by comparing several re-
lated topic segmentation methods in Section 5. Sec-
tion 6 concludes the paper.
2 Related Work
We are interested in unsupervised topic segmenta-
tion in either written or spoken language. There is a
large body of work on unsupervised topic segmen-
tation of text based on lexical cohesion. It can be
characterised by how lexical cohesion is modelled.
One branch of this work represents the lexical co-
hesion in a vector space by exploring the word co-
occurrence patterns, e.g., TF or TF-IDF. Work fol-
lowing this line includes TextTiling (Hearst, 1997),
which calculates the cosine similarity between two
adjacent blocks of words purely based on the word
frequency; C99 (Choi, 2000), an algorithm based
on divisive clustering with a matrix-ranking scheme;
LSeg (Galley et al, 2003), which uses a lexical
chain to identify and weight word repetitions; U00
(Utiyama and Isahara, 2001), a probalistic approach
using dynamic programming to find a segmenta-
tion with a minimum cost; MinCut (Malioutov and
Barzilay, 2006), which casts segmentation as a graph
cut problem, and APS (Kazantseva and Szpakowicz,
2011), which uses affinity propagation to learn clus-
tering for segmentation.
The other branch of this work characterises the
lexical cohesion using topic models, to which the
model introduced in Section 3 belongs. Lexical co-
hesion in this line of research is modelled by a
probabilistic generative process. PLDA presented by
Purver et al (2006) is an unsupervised topic mod-
elling approach for segmentation. It chains a set of
LDAs (Blei et al, 2003) by assuming a Markov
structure on topic distributions. A binary topic shift
variable is attached to each text passage (i.e., an ut-
terance in (Purver et al, 2006)). It is sampled to in-
dicate whether the jth text passage shares the topic
distribution with the (j ? 1)th passage.
Using a similar Markov structure, SITS (Nguyen
et al, 2012) chains a set of HDP-LDAs (Teh et al,
2006). Unlike PLDA, SITS assumes each text pas-
sage is associated with a speaker identity that is at-
tached to the topic shift variable as supervising in-
191
formation. SITS further assumes speakers have dif-
ferent topic change probabilities that work as pri-
ors on topic shift variables. Instead of assuming
documents in a dataset share the same set of top-
ics, Bayesseg (Eisenstein and Barzilay, 2008) treats
words in a segment generated from a segment spe-
cific multinomial language model, i.e., it assumes
each segment is generated from one topic, and a
later hierarchical extension (Eisenstein, 2009) as-
sumes each segment is generated from one topic or
its parents. Other methods using as input the output
of topic models include (Sun et al, 2008), (Misra et
al., 2009), and (Riedl and Biemann, 2012).
In this paper we take a generative approach ly-
ing between PLDA and SITS. In contrast to PLDA,
which uses a flat topic model (i.e., LDA), we assume
each text has a latent topic structure that can reflect
the topic coherence pattern, and the model adapts its
parameters to the segments to further improve per-
formance. Unlike SITS that targets analysing multi-
party meeting transcripts, where speaker identities
are available, we are interested in more general texts
and assume each text has a specific topic change
probability, since (1) the identity information is not
always available for all kinds of texts (e.g., continu-
ous broadcast news transcripts (Allan et al, 1998)),
(2) even for the same author, topic change probabil-
ities for his/her different articles might be different.
3 Segmentation with Topic Models
In documents, topically coherent segments usually
encapsulate a set of consecutive passages that are
semantically related (Wang et al, 2011). However,
the topic boundaries between segments are often un-
available a priori. Thus we treat all passage bound-
aries (e.g., sentence boundaries, paragraph bound-
aries or pauses between utterances) as possible topic
boundaries. To recover the topic boundaries we de-
velop a structured topic segmentation model by inte-
grating ideas from the segmented topic model (Du et
al., 2010, STM) and Bayesian segmentation models.
The basic idea of our model is that each docu-
ment consists of a set of segments where text pas-
sages in the same segment are generated from the
same topic distribution, called segment level topic
distribution. The segment level topic distribution is
drawn from a topic distribution associated with the
whole document, called document level topic distri-
bution. The relationships between the levels is man-
aged using Bayesian non-parametric methods and a
significant change in segment level topic distribution
indicates a segment change.
Our unsupervised topic segmentation model is
based on the premise that using a hierarchical topic
model like the STM with a point-wise segment
sampling algorithm should allow better detection
of topic boundaries. We believe that (1) segment
change should be associated with significant change
in the topic distribution, (2) topic cohesion can be
reflected in document topic structure, (3) the log-
likelihood of a topically coherent segment is typi-
cally higher than an incoherent segment (Misra et
al., 2008).
Assume we have a corpus of D documents, each
document d consists of a sequence of Ud text pas-
sages, and each passage u contains a set of Nd,u
words denoted by wd,u that are from a vocabulary
W . Our model consists of:
Modelling topic boundary: We assume each
document has its own topic shift probability
pid, a Beta distributed random variable, i.e.,
pid?Beta(?0, ?1). Then, we associate a bound-
ary indicator variable ?d,u with u, like the
topic shift variable in PLDA and SITS. ?d,u
is Bernoulli distributed with parameter pid, i.e.,
?d,u?Bernoulli(pid). It indicates whether there is a
topic boundary after text passage u or not. To sample
?d,u, we use a point-wise sampling algorithm. Con-
sequently, a sequence of ??s defines a set of seg-
ments, i.e., a topic segmentation of d. For example,
let a ? vector ? = (0, 0, 1, 0, 1, 0, 0, 1)1, it gives
us three segments, which are {1, 2, 3}, {4, 5} and
{6, 7, 8}.
Modelling topic structure: Following the idea of
the STM, we assume each document d is associated
with a document level topic distribution ?d, which
is drawn from a Dirichlet distribution with param-
eter ?; and text passages in topic segment s in d
are generated from ?d,s, a segment level topic dis-
tribution. The number of segments Sd can be com-
puted as Sd=1 +
?Ud?1
u=1 ?d,u. Then, a Pitman-Yor
1The last 1 in ? is the document boundary that is know a
priori. This means one does not need to sample it.
192
DK
?
?
?
?
?
?
?
U
s
w
z
S
U
N
Figure 1: The topic segmentation model
process with a discount parameter a and a concen-
tration parameter b is used to link ?d and ?d,s by
?d,s?PYP(a, b, ?d), which forms a simple topic
hierarchy. The idea here is that topics discussed in
segments can be variants of topics of the whole
document. Du et al (2010) have shown that this
topic structure can significantly improve the mod-
elling accuracy, which should contribute to more ac-
curate segmentation. This generative process is dif-
ferent from PLDA. PLDA does not assume the docu-
ment level topic distribution and each time generates
the segment level topic distribution directly from a
Dirichlet distribution.
The complete probabilistic generative process,
shown as a graph in Figure 1 is as follows:
1. For each topic k ? {1, . . . , K}, draw a word distribution
?k ? DirichletW (?).
2. For each document d ? {1, . . . , D},
(a) Draw topic shift probability pid ? Beta(?0, ?1).
(b) Draw ?d ? DirichletK (?).
(c) For each text passage (except last) u ?
{1, . . . , Ud ? 1}, draw ?d,u ? Bernoulli(pid).
(d) Compute Sd the number of segments as 1 +?Ud?1
u=1 ?d,u.
(e) For each segment s ? {1, . . . , Sd}, draw ?d,s ?
PYP(a, b, ?d).
(f) For each text passage u ? {1, . . . , Ud},
i. Set segment sd,u = 1 +
?u?1
v=1 ?d,v .
ii. For each word index n ? {1, . . . , Nd,u},
A. Draw topic zd,u,n ? DiscreteK
(
?d,sd,u
)
.
B. Draw word wd,u,n ? DiscreteK(?zd,u,n).
where sd,u indicates which segment text passage u
belongs to. We assume the dimensionality of the
Dirichlet distribution (i.e., the number of topics) is
known and fixed, and word probabilities are param-
eterized with a K ? Wmatrix ? = (?1, . . . , ?K).
In future work we plan to investigate replace the
Table 1: List of statistics
Mk,w total number of words with topic k.
Mk a vector of Mk,w.
nd,s,k total number of words with topic k in segment
s in document d.
Nd,s total number of words in segment s.
td,s,k table count of topic k in the CRP for segment
s in document d.
td,s a vector of td,s,k for segment s in d.
Td,s total table count in segment s.
cd,1 total number of topic boundaries in d.
cd,0 total number of non-topic boundaries in d.
Dirichlet prior ? on ? with a Pitman-Yor prior (Pit-
man and Yor, 1997) to make the model fully non-
parametric, like SITS.
4 Posterior Inference
In this section we develop a collapsed Gibbs sam-
pling algorithm to do an approximate inference
by integrating out some latent variables (i.e., ??s,
??s and pid?s). The hierarchy in our model can be
well explained with the Chinese restaurant franchise
metaphor introduced in (Teh et al, 2006). For easier
understanding, terminologies of the Chinese Restau-
rant Process (CRP) will be used throughout this sec-
tion, i.e., customers, dishes and restaurants, corre-
spond to words, topics, and segments respectively.
Statistics used are listed in Table 1.
To integrate out the ?d,s?s generated from the
PYP, we use the technique presented in (Chen et
al., 2011), which computes the joint posterior for
the PYP by summing out all the possible seating
arrangements for a sequence of customers (Teh,
2006). In this technique an auxiliary binary variable,
called table indicator (?d,u,n), is introduced to fa-
cilitate computing table count td,s,k for topic k. This
method has two effects: (1) faster mixing of the sam-
pler, and (2) elimination of the need for dynamic
memory to store the populations/counts of each ta-
ble in the CRP. In the CRP each word wd,u,n in topic
k (i.e., where zd,u,n=k) contributes a count to nd,s,k
for u ? s; and, if wd,u,n, as a customer, also opens
a new table to the CRP, it leads to increasing td,s,k
by one. In this case, ?d,u,n=1 indicates wd,u,n is the
first customer on the table, called table head. Thus,
td,s,k =
?
u?s
Nd,u?
n=1
?d,u,n1zd,u,n=k . (1)
Note the two constraints on these two counts, i.e.,
nd,s,k?td,s,k?0 and td,s,k=0 iff nd,s,k=0 (2)
193
can be replaced be a simpler constraint in the table
indicator representation.
The sampler we develop is an MCMC sampler
on the space ? = {z, ?,?} where z defines the
topic assignments of words, ? maintains the needed
CRP configuration (from which t is derived) and ?
defines the segmentation. Moreover, it is not a tra-
ditional Gibbs sampler changing one variable at a
time, but is a block Gibbs sampler where two dif-
ferent kinds of blocks are used. The first block is
(zd,u,n, ?d,u,n) (for each word wd,u,n), which can
be sampled with a table indicator variant of a hier-
archical topic sampler (Du et al, 2010), described
in Section 4.1. This corresponds to Equation (6) in
(Purver et al, 2006). The second kind of block is
a boundary indicator ?d,u together with a particular
constrained set of table counts designed to handle
splitting and merging, which corresponds to Equa-
tion (7) in (Purver et al, 2006). Sampling this sec-
ond kind of block is harder in our non-parametric
model requiring a potentially exponential summa-
tion, a problem we overcome using symmetric poly-
nomials, shown in Section 4.2.
4.1 Sampling Topics
One step in our model is to sample the assignments
of topics to words conditioned on all ??s. As dis-
cussed in Section 3, given the sequence of ?d,u?s,
?d, one can figure out which segment s text passage
u belongs to. Thus, conditioned on a set of segments
s given by ?, the joint posterior distribution ofw, z
and ? is computed as p(z,w, ? |?, ?, a, b, ?)
=
?
d
BetaK
(
?+
?
s td,s
)
BetaK (?)
?
k
BetaW (? +Mk)
BetaW (?)
?
d
?
s?s
(b|a)Td,s
(b)Nd,s
?
k
Snd,s,ktd,s,k,a
(
nd,s,k
td,s,k
)?1
, (3)
where BetaK(?) is a K-dimension Beta function,
(x|y)n the Pochhammer symbol2, and Snt,a the gen-
eralised Stirling number of the second kind (Hsu
and Shiue, 1998)3 precomputed in a table so cost-
2The Pochhammer symbol (x|y)n denotes the rising facto-
rial with a specified increment, i.e., y. It is defined as (x|y)n =
x(x+ y)...(x+ (n? 1)y).
3A Stirling number of the second kind is used to study
the number of ways of partitioning a set of n objects into
k nonempty subsets. The generalised version given by Hsu
and Shiue (1998) has a linear recursion which in our case is
Sn+1m,a = S
n
m?1,a + (n?ma)S
n
m,a.
ing O(1) to use (Buntine and Hutter, 2012).Eq (3)
is an indicator variant of Eq (1) in (Du et al, 2010)
with applying Theorem 1 in (Chen et al, 2011).
Given the current segmentation and topic assign-
ments for all other words, using Bayes rule, we can
derive the following two conditionals from Eq (3):
1. The joint probability of assigning topic k to word
wd,u,n and wd,u,n being a table head, p(zd,u,n =
k, ?d,u,n = 1 |?
?)
=
?wi,j,n +Mk,wi,j,n?
w(?w +Mk,w)
?k +
?
s td,s,k?
k ?k +
?
s,k td,s,k
b+ aTd,s
b+Nd,s
Snd,s,k+1td,s,k+1,a
Snd,s,ktd,s,k,a
td,s,k + 1
nd,s,k + 1
(4)
2. The joint probability of assigning k to wd,u,n
and wd,u,n not being a table head, p(zd,u,n =
k, ?d,u,n = 0 |?
?)
=
?wi,j,l +Mk,wi,j,l?
w ?w +Mk,w
1
b+Nd,s
Snd,s,k+1td,s,k,a
Snd,s,ktd,s,k,a
nd,s,k + 1? td,s,k
nd,s,k + 1
(5)
where ?? = {z?zd,u,n ,w, ???d,u,n ,?,?, a, b,?}.
From the two conditionals, we develop a blocked
Gibbs sampling algorithm for (zd,u,n, ?d,u,n).
4.2 Sampling Segmentation Boundaries
In our model, each segment corresponds to a
Chinese restaurant in the CRP. Sampling topic
boundaries corresponds to splitting/merging restau-
rant(s). This is different from the split-merge process
proposed by Jian and Neal (2004), where one actu-
ally splits/merges table(s). To our knowledge, there
has been no method developed to split/merge restau-
rant(s). We tried different approximations, such
as the minimum-path-assumption (Wallach, 2008),
which in our case assumes one table for each topic
k, and all words in k are placed in the same ta-
ble. Although this simplifies the split-merge pro-
cess, it yielded poor results. We instead developed a
novel approximate block Gibbs sampling algorithm
using symmetric polynomials. Its segmentation per-
formance worked well in our development dataset.
For simplicity, we consider a passage u in doc-
ument d, and assume: (1) If ?d,u=1, there are two
segments, sl and sr; sl ends at text passage u, and sr
starts at text passage u+1. (2) If ?d,u=0, there is one
194
segment, sm, where u is is somewhere in the middle
of sm. The split-merge choice we sample is one to
many, for a given split pair (sl, sr) we consider a set
of merged states sm (represented by different possi-
ble table counts). Then, to compute the Gibbs prob-
ability for splitting/merging restaurant(s), we con-
sider the probability of the single split, the probabil-
ity of the corresponding set of merges, and then if a
merge is selected, we have to sample from the set of
merges. These are as follows:
Splitting: split sm into sr and sl by placing a
boundary after u. Since passages have a fixed order
in each document, all the words are put into sr and
sl based on which passages they belong to. Then,
given all the topic assignments, we first sample all
table indicators ?d,u?,n, for n ? {1, ..., Nd,u?} and
u? ? sm using Bernoulli sampling without replace-
ment. It runs as follows: 1) sample ?d,u?,n according
to probability td,sm,k/nd,sm,k; 2) decrease td,sm,k if
?d,u?,n = 1, otherwise, just decrease nd,sm,k. Us-
ing the sampled ?d,u?,n?s we compute the inferred ta-
ble counts td,s,k (from Eq (1)) and customer counts
nd,s,k respectively for segments s=sl and sr and
topics k. The computation may result in the follow-
ing cases: for a given topic k,
(I) Both sl and sr have nd,s,k>0 and td,s,k?1, which
means both segments have words assigned to k and
words being labelled with table head. According
to constraints (2), after splitting, restaurants corre-
sponding to sl and sr are valid. We do not make any
change on table counts.
(II) Either sl or sr has nd,s,k=0 and td,s,k=0. In this
case, for example, all the words assigned to k in sm
are in sl after splitting, and all those labelled with
table head should also be in sl. sr has no words as-
signed to k. Thus, there is no need to change table
counts.
(III) Either sl or sr has nd,s,k>0 and td,s,k=0. Both seg-
ments have words assigned to k, but those labelled
with table head only exist in one segment. For in-
stance, if they only exist in sl then sr has no table
head, which means the restaurant of sr has customers
eating a dish, but no tables serving that dish. Thus,
we set td,sr,k=1 to make the constraints (2) satisfied.
The Gibbs probability for splitting a segment is
p(?d,u = 1 |?
??) ?
?1 + cd,1
?0 + ?1 + cd,0 + cd,1
(6)
BetaK
(
?+
Sd?
s=1
td,s
) ?
s?{sl,sr}
(b|a)Td,s
(b)Nd,s
?
k
Snd,s,ktd,s,k,a ,
where ??? = {z,w, ?,???d,u ,?, a, b, ?0, ?1}.
Merging: remove the boundary after u, and merge
sr and sl to one segment sm. For this case, both
sr and sl satisfy constraints (2) for all k?s, and set
nd,sm,k=nd,sr,k + nd,sl,k. The following cases are
considered: for a topic k
(I) Both sl and sr have nd,s,k>0 and td,s,k>1. We
compute td,sm,k using Eq (7). Thus table counts
before and after merging are equal.
(II) Either sl or sr has nd,s,k=0 and td,s,k=0. Similar
to the above case, we use Eq (7).
(III) Both sl and sr have nd,s,k>0, and either of them
has td,s,k=1 or both. We have to choose between
Eq (7) and Eq (8), i.e., to decide whether a table
should be removed or not.
td,sm,k = td,sl,k + td,sr,k (7)
td,sm,k = td,sl,k + td,sr,k ? 1 (8)
Note that choosing Eq (8) means we need to de-
crease the table count td,sm,k by one. The idea here
is that we sample to decide whether the remove table
was added due to splitting case (III) or not. Clearly,
we have a one-to-many split-merge choice. To com-
pute the probability of a set of possible merges,
we use elementary symmetric polynomials as fol-
lows: letKS be a set of topic-segment combinations
that satisfy the condition in merging case (III), for
(k, s) ? KS , we sample either Eq (7) or Eq (8).
Let T = {td,s,k : (k, s) ? KS} be the set of table
counts affected by the changes of Eq (7) or Eq (8).
The Gibbs probability for merging two segments is
p(?d,u = 0 |?
???) =
?
T
p(?d,u = 0, T |?
???) (9)
?
?
T
(
?0 + cd,0
?0 + ?1 + cd,0 + cd,1
BetaK
(
?+
Sd?
s=1
td,s
)
(b|a)Td,sm
(b)Nd,sm
?
k
Snd,sm,ktd,sm,k,a
)
,
where ???? = {z,w, t ? T ,???d,u ,?, a, b, ?0, ?1}.
This is converted to a sum on |T | booleans with
independent terms and evaluated recursively in
O(|T |2) by symmetric polynomials. If a merge is
chosen, one then samples according to the terms in
the sum using a similar recursion.
5 Experiments
To demonstrate the effectiveness of our model (de-
noted by TSM) in topic segmentation tasks, we
195
evaluate it on three different kinds of corpora4: a
set of synthetic documents, two meeting transcripts
and two sets of text books (see Tables 2 and 3);
and compare TSM with the following methods: two
baselines (the Random algorithm that places topic
boundaries uniformly at random, and the Even al-
gorithm that places a boundary after every mth text
passage, where m is the average gold-standard seg-
ment length (Beeferman et al, 1999)), C99, MinCut,
Bayesseg, APS (Kazantseva and Szpakowicz, 2011),
and PLDA.
Metrics: We evaluated the segmentation perfor-
mance with PK (Beeferman et al, 1999) and Win-
dowDiff (WDr) (Pevzner and Hearst, 2002), which
are two common metrics used in topic segmenta-
tion. Both move a sliding window of fixed size k
over the document, and compare the inferred seg-
mentation with the gold-standard segmentation for
each window. The window size is usually set to
the half of the average gold-standard segment size
(Pevzner and Hearst, 2002). In addition, we also
used an extended WindowDiff proposed by Lam-
prier et al (2007), denoted by WDe. One problem
of WDr is that errors near the two ends of a text are
penalised less than those in the middle. To solve the
problem WDe adds k fictive text passages at the be-
ginning and the end of the text when computing the
score. We evaluated all the methods with the same
Java code for the three metrics.
Parameter Settings: In order to make all the
methods comparable, we chose for each method
the parameter settings that give the gold-standard
number of segments5. Specifically, we used a
11 ? 11 rank mask for C99, as suggested by
Choi (2000), the configurations included in the code
(http://groups.csail.mit.edu/rbg/code)
for Bayesseg and manually tuned parameters for
MinCut. For APS, a greedy approach was used to
search parameter settings that can approximately
give the gold-standard number of segments. For
PLDA, two randomly initialised Gibbs chains were
used. Each chain ran for 75,000 burn-in iterations,
then 1000 samples were drawn at a lag of 25 from
each chain. For TSM, 10 randomly initialised
4For preprocessing, we only removed stop words.
5The segments learnt by those methods will differ, but just
the segment count will be the same as the gold-standard count.
Table 2: The Choi?s dataset
Range of n 3-11 3-5 6-8 9-11
#docs 400 100 100 100
DocLen
mean 69.7 39.3 69.6 98.6
std 8.2 2.6 2.9 3.5
SegLen
mean 7 4 7 10
std 2.57 0.84 0.87 1.03
Table 3: Real dataset statistics
ICSI Election Fiction Clinical
# doc 25 4 84 227
DocLen
mean 994.5 144.3 325.0 139.5
std 354.5 16.4 230.1 110.4
SegLen
mean 188 7 22 35
std 219.1 8.9 23.8 41.7
Gibbs chains were used. Each chain ran for 30,000
iterations with 25,000 for burn-in, then 200 samples
were drawn. The concentration parameter b in TSM
was sampled using the Adaptive-Reject sampling
scheme introduced in (Du et al, 2012b), the dis-
count parameter a = 0.2, and ?0 = ?1 = 0.1. To
derive the final segmentation for PLDA and TSM,
we first estimated the marginal probabilities of
placing boundaries after text passages from the total
of 2000 samples. These probabilities were then
thresholded to give the gold-standard number of
segments. Precisely, we apply a small amount of
Gaussian smoothing to the marginal probabilities
(except for Choi?s dataset), like Puerver et al (2006)
does. Finally, we used a symmetric Dirichlet prior
in PLDA and STM, the one on topic distributions is
? = 0.1, the other on word distributions ? = 0.01.
5.1 Evaluation on Choi?s Dataset
Choi?s dataset (Choi, 2000) is commonly used in
evaluating topic segmentation methods. It consists
of 700 documents, each being a concatenation of 10
segments. Each segment is the first n sentences of
a randomly selected document from the Brown cor-
pus, s.t. 3 ? n ? 11. Those documents are divided
into 4 subsets with different range of n, as shown in
Table 2. We ran PLDA and STM with 50 topics. Re-
sults in Table 4 show that our model significantly
outperforms all the other methods on the four sub-
sets over all the metrics. Furthermore, comparing to
other published results, this also outperforms (Misra
et al, 2009) (see their table 2), and (Riedl and Bie-
mann, 2012) (they report an average of 1.04 and 1.06
in Tables 1 and 2, whereas TSM averages 0.93). This
gives TSM the best reported results to date.
196
Table 4: Comparison on Choi?s datasets with WD and PK (%)
3-11 3-5 6-8 9-11
WDr WDe PK WDr WDe PK WDr WDe PK WDr WDe PK
Random 51.7 49.1 48.7 51.4 50.0 48.4 52.5 49.9 49.2 52.4 48.9 49.2
Even 49.1 46.7 49.0 46.3 45.8 46.3 38.8 37.3 38.8 30.0 28.6 30.0
MinCut 30.4 29.8 26.7 41.6 41.5 37.3 28.2 27.4 25.5 23.6 22.7 21.6
APS 40.7 38.8 38.4 32.0 30.6 31.8 34.4 32.6 32.7 34.5 32.2 33.2
C99 13.5 12.3 12.3 11.3 10.2 10.8 10.2 9.3 9.8 8.9 8.1 8.6
Bayesseg 11.6 10.9 10.9 11.8 11.5 11.1 7.7 7.2 7.3 6.1 5.7 5.7
PLDA 2.4 2.2 1.8 4.0 3.9 3.3 3.6 3.5 2.7 3.0 2.8 2.0
TSM 0.8 0.8 0.6 1.3 1.3 1.0 1.4 1.4 0.9 1.9 1.8 1.2
Table 5: Comparison on the meeting transcripts and written texts with WD and PK (%)
ICSI Election Fiction Clinical
WDr WDe PK WDr WDe PK WDr WDe PK WDr WDe PK
Random 46.3 41.7 44.1 51.0 49.7 45.1 51.0 48.7 47.5 45.9 38.5 44.1
Even 48.3 43.0 46.4 56.0 55.1 51.2 48.1 45.9 46.3 49.2 42.0 48.8
C99 42.9 37.4 39.9 43.1 41.5 37.0 48.1 45.1 42.1 39.7 31.9 38.7
MinCut 40.6 36.9 36.9 43.6 43.3 39.0 40.5 39.7 37.1 38.2 36.2 36.8
APS 58.2 49.7 54.6 47.7 36.8 40.6 48.0 45.8 45.1 39.9 32.8 39.6
Bayesseg 32.4 29.7 26.7 41.1 41.3 34.1 33.7 32.8 27.8 35.0 28.8 34.0
PLDA 32.6 28.8 29.4 40.6 41.1 32.0 43.0 41.3 36.1 37.3 32.1 32.4
TSM 30.2 26.8 25.8 38.1 38.9 31.3 40.8 38.7 32.5 34.5 29.1 30.6
Note the lexical transitions in these concatenated
documents are very sharp (Malioutov and Barzi-
lay, 2006). The sharp transitions lead to significant
change in segment level topic distributions, which
further implies the variance of these distributions is
large. In TSM, a large variance causes a small con-
centration parameter b. We observed that the sam-
pled b?s (about 0.1) are indeed small for the four sub-
sets, which shows there is no topic sharing among
segments. Therefore, TSM is able to recognise the
segments are unrelated text.
5.2 Evaluation on Meeting Transcripts
We applied our model to segmenting the two meet-
ing transcripts, which are the ICSI meeting tran-
scripts (Janin et al, 2003) and the 2008 presidential
election debates (Boydstun et al, 2011). The ICSI
meeting has 75 transcripts, we used the 25 annotated
transcripts provided by Galley et al (2003) for eval-
uation. For the election debates, we used the four
annotated debates used in (Nguyen et al, 2012). The
statistics are shown in Table 3. PLDA and TSM were
trained with 10 topics on the ICSI and 50 on the
Election. In this set of experiments, we show that
our model is robust to meeting transcripts.
00.2
0.40.6p(l=
1) TSM
0 100 200 300 400 500 600 700 8000
0.20.4
0.6 PLDA
Utterance position in sequence
p(l =
 1)
Figure 2: Probability of a topic boundary, compared with
gold-standard segmentation (shown in red and at the top
of each diagram) on one ICSI transcript.
As shown in Table 5, topic modelling based meth-
ods (i.e., Bayesseg, PLDA and TSM) outperform
those using either TF or TF-IDF, which is consistent
with previously reported results (Misra et al, 2009;
Riedl and Biemann, 2012). Among the topic model
based methods, TSM achieves the best results on all
the three metrics. On the ICSI transcripts, TSM per-
forms 6.8%, 9.7% and 3.4% better than Bayesseg
on the WDr, WDe and PK metrics respectively. Fig-
ure 2 shows an example of how the inferred topic
boundary probabilities at utterances compare with
the gold-standard boundaries on one ICSI meeting
transcript. The gold-standard segmentation is {77,
95, 189, 365, 508, 609, 860}, TSM and PLDA in-
fer {85, 96, 188, 363, 499, 508, 860} and {96, 136,
197
Table 6: Sampled concentration parameters
Choi ICSI Election Fiction Clinical
b 0.1 5.2 5.4 18.4 4.8
203, 226, 361, 508, 860} respectively. Both models
miss the boundary after the 609th utterance, but put a
boundary after the 508th utterance. Note the bound-
aries placed by TSM are always within 10 utterances
with respect to the gold standard.
Although TSM still performs the best on the de-
bates, all the methods have relatively worse perfor-
mance than on the ICSI meeting transcripts. Nguyen
et al (2012) pointed out that the ICSI meetings are
characterised by pragmatic topic changes, in con-
trast, the debates are characterised by strategic topic
changes with strong rewards for setting the agenda,
dodging a question, etc. Thus, considering the prop-
erties of debates might further improve the segmen-
tation performance.
5.3 Evaluation on Written Texts
We further tested TSM on two written text datasets,
Clinical (Eisenstein and Barzilay, 2008) and Fiction
(Kazantseva and Szpakowicz, 2011). The statistics
are shown in Table 3. Each document in the Clinical
dataset is a chapter of a medical textbook. Section
breaks are selected to be the true topic boundaries.
For the Fiction dataset, each document is a fiction
downloaded from Project Gutenberg, the true topic
boundaries are chapter breaks. We trained PLDA
and TSM with 25 topics on the Fiction and 50 on the
Clinical. Results are shown in Table 5. TSM com-
pares favourably with Bayesseg and outperforms the
other methods on the Clinical dataset, but it does not
perform as well as Bayesseg on the Fiction dataset.
In fiction books, the topic boundaries between
sections are usually blurred by the authors for rea-
sons of continuity (Reynar, 1999). We observed that
the sampled concentration (or inverse variance) pa-
rameter b in TSM is about 18.4 on Fiction, but 4.8 on
Clinical, as shown in Table 6. This means the vari-
ance of segment level topic distributions ? learnt by
TSM is not large for the fiction, so chapter breaks
may not necessarily indicate topic changes. For ex-
ample, there is a document in the Fiction dataset
where gold-standard topic boundaries are placed af-
ter each block of text. In contrast, Bayesseg assumes
each segment has its own distribution over words,
i.e., one topic per segment, which means topics are
not shared among segments. We hypothesize that
for certain kinds of documents where the change in
topic distribution is subtle, such as fiction, assuming
one topic per segment can capture subtle changes in
word usage. This is an area for future investigation.
6 Conclusion
In this paper, we have presented a hierarchical
Bayesian model for unsupervised topic segmen-
tation. This new model takes advances of both
Bayesian segmentation and structured topic mod-
elling. It uses a point-wise boundary sampling al-
gorithm to sample a topic segmentation, while con-
currently building a structured topic model. We
have developed a novel approximation to com-
pute the Gibbs probabilities of spliting/merging seg-
ment(s). Our model shows prominent segmentation
performance on both written or spoken texts.
In future work, we would like to make the model
fully nonparametric and investigate the effects of
adding different cues in texts, such as cue phrases,
pronoun usage, prosody, etc. Currently, our model
uses marginal boundary probabilities to generate
the final segmentation. Instead, we could develop a
Metropolis-Hasting sampling algorithm to move one
boundary at a time, given the gold-standard number
of segments. To further study the effectiveness of
our model, we would like to compare it with other
methods, like SITS (Nguyen et al, 2012) and to run
on more datasets, like email (Joty et al, 2010). For
example, in order to compare with SITS, one can
make an assumption that each document just has one
speaker.
Acknowledgments
The authors would like to thank all the anony-
mous reviewers for their valuable comments.
This research was supported under Australian
Research Council?s Discovery Projects funding
scheme (project numbers DP110102506 and
DP110102593). NICTA is funded by the Australian
Government as represented by the Department
of Broadband, Communications and the Digital
Economy and the Australian Research Council
through the ICT Centre of Excellence program.
198
References
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic detection and tracking pi-
lot study: Final report. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 194?218.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Mach.
Learn., 34(1-3):177?210.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
A.E. Boydstun, C. Phillips, and R.A. Glazier. 2011. Its
the economy again, stupid: Agenda control in the 2008
presidential debates.
W. Buntine and M. Hutter. 2012. A Bayesian review
of the Poisson-Dirichlet process. Technical Report
arXiv:1007.0296v2, ArXiv, Cornell.
Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling for the Poisson-Dirichlet process. In Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Database,
pages 296?311.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, NAACL 2000,
pages 26?33.
Lan Du, Wray Buntine, and Huidong Jin. 2010. A
segmented topic model based on the two-parameter
Poisson-Dirichlet process. Mach. Learn., 81(1):5?19.
Lan Du, Wray Buntine, and Huidong Jin. 2012a. Mod-
elling sequential text with an adaptive topic model.
In Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
535?545.
Lan Du, Wray Buntine, Huidong Jin, and Changyou
Chen. 2012b. Sequential latent Dirichlet alocation.
Knowledge and Information Systems, 31(3):475?503.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP?08, pages 334?343.
Jacob Eisenstein. 2009. Hierarchical text segmentation
from multi-scale lexical cohesion. In Human Lan-
guage Technologies: Conference of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics, pages 353?361. The Association for Compu-
tational Linguistics.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse segmen-
tation of multi-party conversation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 562?569.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?53.
Marti A. Hearst. 1997. TextTiling: segmenting text
into multi-paragraph subtopic passages. Comput. Lin-
guist., 23(1):33?64.
Leetsch C. Hsu and Peter Jau-Shyong Shiue. 1998. A
unified approach to generalized Stirling numbers. Adv.
Appl. Math., 20:366?384, April.
Sonia Jain and Radford Neal. 2004. A split-merge
Markov chain Monte Carlo procedure for the Dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13(1):158?182.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus. In
Proceedings of 2003 IEEE International Conference
on Acoustics, Speech, and Signal (ICASSP ?03), pages
364?367.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond T. Ng. 2010. Exploiting conversation struc-
ture in unsupervised topic segmentation for emails.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 388?
398.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear
text segmentation using affinity propagation. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 284?293.
Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and
Frederic Saubion. 2007. On evaluation methodologies
for text segmentation algorithms. In Proceedings of
the 19th IEEE International Conference on Tools with
Artificial Intelligence - Volume 02, ICTAI ?07, pages
19?26.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, ACL-
44, pages 25?32.
Hemant Misra, Olivier Cappe, and Francois Yvon. 2008.
Using LDA to detect semantically incoherent docu-
ments. In Proceedings of CoNLL-08, pages 41?48.
Hemant Misra, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe. 2009. Text segmentation via topic
modeling: an analytical study. In Proceedings of the
18th ACM conference on Information and knowledge
management, CIKM ?09, pages 1553?1556.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip
Resnik. 2012. SITS: A hierarchical nonparametric
199
model using speaker identity for topic segmentation in
multiparty conversations. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 78?87.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Comput. Linguist., 28(1):19?36.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Diriclet distribution derived from a stable subordina-
tor. Annals Probability, 25:855?900.
Matthew Purver, Thomas L. Griffiths, Konrad P. Ko?rding,
and Joshua B. Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, ACL-
44, pages 17?24.
Jeffrey C. Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics,
pages 357?364.
Martin Riedl and Chris Biemann. 2012. How text seg-
mentation algorithms gain from topic models. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Gerard Salton, Amit Singhal, Chris Buckley, and Mandar
Mitra. 1996. Automatic text decomposition using text
segments and text themes. In Proceedings of the the
seventh ACM conference on Hypertext, pages 53?65.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher ker-
nel. In Proceedings of ACL-08: HLT, Short Papers,
pages 269?272.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Y. W. Teh. 2006. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, School
of Computing, National University of Singapore.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of 39th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 499?506.
H.M. Wallach. 2008. Structured topic models for lan-
guage. doctoral dissertation, Univ. of Cambridge.
Hongning Wang, Duo Zhang, and ChengXiang Zhai.
2011. Structural topic model for latent topical struc-
ture analysis. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 1526?1535.
200
