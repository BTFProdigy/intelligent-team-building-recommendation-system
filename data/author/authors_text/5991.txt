Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 391?399,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
A Comparison of Model Free versus Model Intensive Approaches to
Sentence Compression
Tadashi Nomoto
National Institute of Japanese Literature
10-3 Midori Tachikawa
Tokyo 190-0014 Japan
nomoto@acm.org
Abstract
This work introduces a model free approach to
sentence compression, which grew out of ideas
from Nomoto (2008), and examines how it com-
pares to a state-of-art model intensive approach
known as Tree-to-Tree Transducer, or T3 (Cohn
and Lapata, 2008). It is found that a model free
approach significantly outperforms T3 on the par-
ticular data we created from the Internet. We also
discuss what might have caused T3?s poor perfor-
mance.
1 Introduction
While there are a few notable exceptions (Hori and
Furui, 2004; Yamagata et al, 2006), it would be
safe to say that much of prior research on sen-
tence compression has been focusing on what we
might call ?model-intensive approaches,? where
the goal is to mimic human created compressions
as faithfully as possible, using probabilistic and/or
machine learning techniques (Knight and Marcu,
2002; Riezler et al, 2003; Turner and Charniak,
2005; McDonald, 2006; Clarke and Lapata, 2006;
Cohn and Lapata, 2007; Cohn and Lapata, 2008;
Cohn and Lapata, 2009). Because of this, the
question has never been raised as to whether a
model free approach ? where the goal is not to
model what humans would produce as compres-
sion, but to provide compressions just as useful as
those created by human ? will offer a viable alter-
native to model intensive approaches. This is the
question we take on in this paper.
1
1
One caveat would be in order. By model free approach,
we mean a particular approach which does not furnish any
parameters or weights that one can train on human created
compressions. An approach is said to be model-intensive if it
does. So as far as the present paper is concerned, we might
do equally well with a mention of ?model free? (?model-
intensive?) replaced with ?unsupervised? (?supervised?), or
?non-trainable? (?trainable?).
An immediate benefit of the model-free ap-
proach is that we could free ourselves from the
drudgery of collecting gold standard data from hu-
mans, which is costly and time-consuming. An-
other benefit is intellectual; it opens up an alterna-
tive avenue to addressing the problem of sentence
compression hitherto under-explored.
Also breaking from the tradition of previous re-
search on sentence compression, we explore the
use of naturally occurring data from the Internet
as the gold standard. The present work builds on
and takes further an approach called ?Generic Sen-
tence Trimmer? (GST) (Nomoto, 2008), demon-
strating along the way that it could be adapted for
English with relative ease. (GST was originally
intended for Japanese.) In addition, to get a per-
spective on where we stand with this approach, we
will look at how it fares against a state-of-the-art
model intensive approach known as ?Tree-to-Tree
Transducer? (T3) (Cohn, 2008), on the corpus we
created.
2 Approach
Nomoto (2008) discusses a two-level model
for sentence compression in Japanese termed
?Generic Sentence Trimmer? (GST), which con-
sists of a component dedicated to producing gram-
matical sentences, and another to reranking sen-
tences in a way consistent with gold standard com-
pressions. For the convenience?s sake, we refer
to the generation component as ?GST/g? and the
ranking part as ?GST/r.? The approach is moti-
vated largely by the desire to make compressed
sentences linguistically fluent, and what it does
is to retain much of the syntax of the source sen-
tence as it is, in compression, which stands in con-
trast to Filippova and Strube (2007) and Filippova
and Strube (2008), who while working with de-
pendency structure (as we do), took the issue to be
something that can be addressed by selecting and
reordering constituents that are deemed relevant.
391
CD
E
B
A
F
G
H
P 1
P 2
P 3
Figure 1: Dependency Structure for ?ABCDEFGH?
Getting back to GST, let us consider a sentence,
(1) The bailout plan was likely to depend on
private investors to purchase the toxic assets
that wiped out the capital of many banks.
Among possible compressions GST/g produces
for the sentence are:
(2)
a. The bailout plan was likely to depend on
private investors to purchase the toxic assets.
b. The bailout plan was likely to depend on
private investors.
c. The bailout plan was likely to depend on
investors.
d. The bailout plan was likely.
Notice that they do not differ much from the
source sentence (1), except that they get some of
the parts chopped off. In the following, we talk
about how this could done systematically.
3 Compression with Terminating
Dependency Paths
One crucial feature of GST is the notion of Ter-
minating Dependency Paths or TDPs, which en-
ables us to factorize a dependency structure into
a set of independent fragments. Consider string
s = ABCDEFGH with a dependency structure as
shown in Figure 1. We begin by locating terminal
nodes, i.e., those which have no incoming edges,
depicted as filled circles in Figure 1. Next we find
a dependency (singly linked) path from each ter-
minal node to the root (labeled E). This would give
us three paths p
1
= A-C-D-E, p
2
= B-C-D-E, and
p
3
= H-G-F-E (represented by dashed arrows in
Figure 1).
C
D
E
B
#
C
D
E
A
#
G
F
E
H
#
@ %
Figure 2: TDP Trellis and POTs.
Given TDPs, we set out to find a set T of all
suffixes for each TDP, including an empty string,
which would look like:
T (p
1
) = {?A C D E?, ?C D E?, ?D E?, ?E?, ??}
T (p
2
) = {?B C D E?, ?C D E?, ?D E?, ?E?, ??}
T (p
3
) = {?G F E?, ?F E?, ?E?, ??}
Next we combine suffixes, one from each set T ,
while removing duplicates if any. Combining, for
instance, ?A C D E? ? T (p
1
), ?C D E? ? T (p
2
),
and ?G F E? ? T (p
3
), would produce {A C D
E G F}, which we take to correspond to a string
ACDEGF, a short version of s.
As a way of doing this systematically, we put
TDPs in a trellis format as in Figure 2, each file
representing a TDP, and look for a path across the
trellis, which we call ?POT.? It is easy to see that
traveling across the trellis (while keeping record
of nodes visited), gives you a particular way in
which to combine TDPs: thus in Figure 2, we have
three POTs, C-B-F, A-C-H, and A-B-F, giving rise
to BCDEF, ACDEFGH, and ABCDEF, respectively
(where ?&? denotes a starting node, ?%? an ending
node, and ?#? an empty string). Note that the POT
in effect determines what compression we get.
Take for instance a POT C-B-F. To get to a com-
pression, we first expand C-B-F to get {?C D E?
1
,
?B C D E?
2
, ?F E?
3
} (call it E(C-B-F)). (Note that
each TDP is trimmed to start with a node at a cor-
responding position of the POT.) Next we take a
union of TDPs treating them as if they were sets:
thus
?
E(C-B-F) = {B C D E F} = BCDEF.
4 N-Best Search over TDP Trellis
An obvious problem of this approach, however,
is that it spawns hundreds of thousands of possi-
ble POTs. We would have as many as 5
3
= 125
of them for the eight-character long string in Fig-
ure 1.
392
p l a n
w
a s
t o
d e
p
e
n
d
l i k
e
l y
t h
e
b
a i l
o u t
p
u r c h
a s
e
o
n
i n v
e
s
t o r
s
t o
t h
e
t o x
i
c
a s s
e
t
s
w
i p
e d
t h
a
t
o u t
b
a n k s
m a n y
c
a p i
t
a l
t h
e
o f
0
1
23
45
6
7
d e p t h
Figure 3: Dependency Structure
What we propose to deal with this problem is to
call on a particular ranking scheme to discriminate
among candidates we get. Our scheme takes the
form of Equation 3 and 4.
W (x) = idf(x) + exp(?depth(x)) (3)
S(p) =
?
x
0
,...x
n
?E(p)
W (x
i
) (4)
depth(x) indicates the distance between x and the
root, measured by the number of edges one need
to walk across to reach the root from x. Figure 3
shows how the depth is gauged for nodes in a de-
pendency structure. idf(x) represents the log of
the inverse document frequency of x. The equa-
tions state that the score S of a POT p is given as
the sum of weights of nodes that comprise
?
E(p).
Despite their being simple, equations 3 and 4
nicely capture our intuition about the way the trim-
ming or compression should work, i.e., that the
deeper we go down the tree, or the further away
you are from the main clause, the less important
information becomes. Putting aside idf(x) for
the moment, we find in Figure 3, W (assets) >
W (capital) > W (banks) > W (many). Also de-
picted in the figure are four TDPs starting with
many, the (preceding toxic), investors, and the
(preceding bailout).
Finally, we perform a best-first search over the
trellis to pick N highest scoring POTs, using For-
Table 1: Drop-me-not rules. A ?|? stands for or.
?a:b? refers to an element which has both a and b
as attributes. Relation names such as nsubj, aux,
neg, etc., are from de Marneffe et al (2006).
R1. VB ? nsubj | aux | neg | mark
R2. VB ? WDT | WRB
R3. JJ ? cop
R4. NN ? det | cop
R5. NN ? poss:WP (=?whose?)
R6. ? ? conj & cc
ward DP/Backward A* (Nagata, 1994), with the
evaluation function given by Equation 4. We
found that the beam search, especially when used
with a small width value, does not work as well
as the best first search as it tends to produce very
short sentences due to its tendency to focus on
inner nodes, which generally carry more weights
compared to those on the edge. In the experiments
described later, we limited the number of candi-
dates to explore at one time to 3,000, to make the
search computationally feasible.
5 ?Drop-me-not? Rules
Simply picking a path over the TDP trellis (POT),
however, does not warrant the grammaticality of
the tree that it generates. Take for instance, a de-
pendency rule, ?likely?plan, was, depend,? which
forms part of the dependency structure for sen-
tence (1). It gives rise to three TDPs, ?plan, likely?,
?was, likely?, and ?depend, likely?. Since we may
arbitrarily choose either of the two tokens in each
TDP with a complete disregard for a syntagmatic
context that each token requires, we may end up
with sequences such as ?plan likely,? ?plan was
likely,? or ?plan likely depend? (instances of a same
token are collapsed into one). This would obvi-
ously suggest the need for some device to make
the way we pick a path syntagmatically coherent.
The way we respond to the issue is by introduc-
ing explicit prohibitions, or ?drop-me-not? rules
for POTs to comply with. Some of the major
rules are shown in Table 1. A ?drop-me-not? rule
(DMN) applies to a local dependency tree consist-
ing of a parent node and its immediate child nodes.
The intent of a DMN rule is to prohibit any one of
the elements specified on the right hand side of the
arrow from falling off in the presence of the head
393
node; they will be gone only if their head node is.
R1 says that if you have a dependency tree
headed by VB with nsubj, aux, neg, ormark among
its children, they should stay with VB; R2 pro-
hibits against eliminating a WDT or WRB-labeled
word in a dependency structure headed by VB; R6
disallows either cc or conj to drop without accom-
panying the other, for whatever type the head node
assumes.
In Table 2, we find some examples that moti-
vate the kinds of DMN rules we have in Table 1.
Note that given the DMNs, the generation of ?was
likely depend,? or ?plan likely depend? is no longer
possible for the sentence in Figure 3.
6 Reranking with CRFs
Pipelining GST/g with CRFs allows us to tap into
a host of features found in the sentence that could
usefully be exploited toward generating compres-
sion, and requires no significant change in the way
it is first conceived in Nomoto (2008), in order to
make it work for English. It simply involves trans-
lating an output by GST/g into the form that al-
lows the use of CRFs; this could be done simply
by labeling words included in compression as ?1?
and those taken out as ?0,? which would produce
a binary representation of an output generated by
GST/g. Given a source sentence x and a set G(S)
of candidate compressions generated by GST/g ?
represented in binary format ? we seek to solve
the following,
y
?
= argmax
y?G(S)
p(y|x;?). (5)
where y
?
could be found using regular linear-
chain CRFs (Lafferty et al, 2001). ? stands for
model parameters. In building CRFs, we made use
of features representing lexical forms, syntactic
categories, dependency relations, TFIDF, whether
a given word appears in the title of an article, and
the left and right lexical contexts of a word.
7 T3
Cohn and Lapata (2008; 2009) are a recent attempt
to bring a machine learning framework known as
?Structured SVM? to bear on sentence compres-
sion and could be considered to be among the
current state-of-art approaches. Roughly speak-
ing, their approach or what they call ?Tree-to-Tree
Transducer? (T3) takes sentence compression to
be the problem of classifying the source sentence
Table 3: RSS item and its source
R Two bombings rocked Iraq today, killing at
least 80 in attacks at a shrine in Karbala
and a police recruiting station in Ramadi.
S Baghdad, Jan. 5 ? Two new suicide bomb-
ings rocked Iraq today, killing at least 80
in an attack at a shrine in the Shiite city of
Karbala and a police recruiting station in
the Sunni city of Ramadi.
with its target sentence, where one seeks to find
some label y, which represents a compression, for
a given source sentence x, that satisfies the follow-
ing equation,
f(x;w) = argmax
y?Y
F (y, x;w), (6)
and
F (y, x;w) = ?w,?(y, x)?, (7)
where w, a vector representing model parameters,
is determined in such a way that for a target class
y and a prediction y
?
, F (x, y;w) ? F (x, y
?
;w) >
?(y, y
?
) ? ?, ?y
?
?= y; ?(y, y
?
) represents a loss
function and ? a slack variable (Tsochantaridis et
al., 2005). ?(y, x) represents a vector of features
culled from y and x, and ??, ?? a dot product.
For each of the rules used to derive a source sen-
tence, T3 makes a decision on how or whether to
transform the rule, with reference to ??, ??, which
takes into account such features as the number
of terminals, root category, and lengths of fron-
tiers, which eventually leads to a compression via
a chart style dynamic programming.
8 Corpus
Parting ways with previous work on sentence com-
pression, which heavily relied on humans to create
gold standard references, this work has a particu-
lar focus on using data gathered from RSS feeds,
which if successful, could open a door to building
gold standard data in large quantities rapidly and
with little human effort. The primary objective of
the present work is to come up with an approach
capable of exploiting naturally occurring data as
references for compression. So we are interested
394
Table 2: Examples. a ?
r
b means that b stands in an r-relation to a.
rel, nsubj In defying the President, Bill Frist was veering to the political center in a year
during which he had artfully courted his party?s right wing.
couted ?
rel
during
veering?
nsubj
Bill Frist
neg Isaac B. Weisfuse says that the idea that a pandemic flu will somehow skip the 21st
century does not make any sense.
make?
neg
not
mark Prime Minister Ariel Sharon of Israel lashed out at protesters as troops finished
clearing all but the last of the 21 Gaza settlements.
finished ?
mark
as
WDT The announcement offered few details that would convince protestants that they
should resume sharing power with the I.R.A.?s political wing.
convince ?
wdt
that
WRB Arbitron, a company best known for its radio ratings, is testing a portable, pager-
size device that tracks exposure to media throughout the day, wherever its wearer
may go.
go ?
wrb
wherever
cop Buildings in a semi-abandoned town just inside Mexico that is a haven for would-
be immigrants and smugglers will be leveled.
haven ?
cop
is
aux, poss:WP Harutoshi Fukui has penned a handful of best sellers whose common themes
resonate in a country shedding its pacifism and rearming itself.
resonate ?
poss:WP
whose
penned ?
aux
has
Table 4: RSS Corpus from NYTimes.com.
areas # of items
INTERNATIONAL 2052
NYREGION 1153
NATIONAL 1351
OBITUARIES 541
OPINION 1031
SCIENCE 465
SPORTS 1451
TECHNOLOGY 978
WASHINGTON 1297
in finding out how GST compares with T3 from
this particular perspective.
We gathered RSS feeds at NYTimes.com over a
period of several months, across different sections,
including INTERNATIONAL, NATIONAL, NYRE-
GION, BUSINESS, and so forth, out of which we
randomly chose 2,000 items for training data and
116 for testing data. For each RSS summary, we
located its potential source sentence in the linked
page, using a similarity metric known as Soft-
TFIDF (Cohen et al, 2003).
2
Table 4 gives a run-
down on areas items came from and how many of
them we collected for each of these areas.
For the ease of reference, we refer to a corpus of
the training and test data combined as ?NYT-RSS,?
and let ?NYT-RSS(A)? denote the training part of
2
SoftTFIDF is a hybrid of the TFIDF scheme and an edit-
distance model known as Jaro-Winkler(Cohen et al, 2003).
NYT-RSS, and ?NYT-RSS(B)? the testing part.
9 Experiments
We ran the Stanford Parser on NYT-RSS to extract
dependency structures for sentences involved, to
be used with GST/g (de Marneffe et al, 2006;
Klein and Manning, 2003). We manually devel-
oped 28 DMN rules out of NYT-RSS(A), some of
which are presented in Table 1. An alignment be-
tween the source sentence and its corresponding
gold standard compression was made by SWA or
a standard sequence alignment algorithm by Smith
and Waterman (1981). Importantly, we set up
GST/g and T3 in such a way that they rely on the
same set of dependency analyses and alignments
when they are put into operation. We trained T3
on NYT-RSS(A) with default settings except for
??epsilon? and ??delete? options which we turned
off, as preliminary runs indicated that their use led
to a degraded performance (Cohn, 2008). We also
set the loss function as was given in the default
settings. We trained both GST/r, and T3 on NYT-
RSS(A).
We ran GST/g and GST/g+r, i.e., GST/r
pipelined with GST/g, varying the compression
rate from 0.4 to 0.7. This involved letting GST/g
rank candidate compressions by S(p) and then
choosing the first candidate to satisfy a given com-
pression rate, whereas GST/g+r was made to out-
put the highest ranking candidate as measured by
p(y | x; ?), which meets a particular compression
rate. It should be emphasized, however, that in T3,
varying compression rate is not something the user
has control over; so we accepted whatever output
395
Table 5: Results on NYT-RSS. ?*?-marked figures
mean that performance of GST/g is different from
that of GST/g+r (on the comparable CompR) at
5% significance level according to t-test. The fig-
ures indicate average ratings.
Model CompR Intelligibility Rep.
GST/g+r 0.446 2.836 2.612
GST/g 0.469 3.095 2.569
GST/g+r 0.540 2.957 2.767
GST/g 0.562 3.069 3.026
?
GST/g+r 0.632 2.931 2.957
GST/g 0.651 3.060 3.259
?
GST/g+r 0.729 3.155 3.345
GST/g 0.743 3.328 3.621
?
T3 0.353 1.750 1.586
Gold Std. 0.657 4.776 3.931
T3 generated for a given sentence.
Table 5 shows how GST/g, GST/g+r, and T3
performed on NYT-RSS, along with the gold stan-
dard, on a scale of 1 to 5. Ratings were solicited
from 4 native speakers of English. ?CompR? in-
dicates compression rate. ?Intelligibility? means
how well the compression reads; ?representative-
ness? how well the compression represents its
source sentence. Table 6 presents a guideline for
rating, describing what each rating should mean,
which was also presented to human judges to fa-
cilitate evaluation.
The results in Table 5 indicate a clear supe-
riority of GST/g and GST/g+r over T3, while
differences in intelligibility between GST/g and
GST/g+r were found not statistically significant.
What is intriguing, though, is that GST/g produced
performance statistically different in representa-
tiveness from GST/g+r at 5% level as marked by
the asterisk.
Shown in Table 8 are examples of compression
created by GST/g+r, GST/g and T3, together with
gold standard compressions and relevant source
sentences. One thing worth noting about the ex-
amples is that T3 keeps inserting out-of-the-source
information into compression, which obviously
has done more harm than good to compression.
Table 6: Guideline for Rating
MEANING EXPLANATION SCORE
very bad For intelligbility, it means that the
sentence in question is rubbish; no
sense can be made out of it. As
for representativeness, it means that
there is no way in which the com-
pression could be viewed as repre-
senting its source.
1
poor Either the sentence is broken or fails
to make sense for the most part, or it
is focusing on points of least signifi-
cance in the source.
2
fair The sentence can be understood,
though with some mental effort; it
covers some of the important points
in the source sentence.
3
good The sentence allows easy compre-
hension; it covers most of important
points talked about in the source sen-
tence.
4
excellent The sentence reads as if it were writ-
ten by human; it gives a very good
idea of what is being discussed in the
source sentence.
5
Table 7: Examples from corpora. ?C? stands for
reference compression; ?S? source sentence.
NYT-RSS
C Jeanine F. Pirro said that she would abandon her
plans to unseat senator Hillary Rodham Clinton and
would instead run for state attorney general .
S Jeanine F. Pirro, whose campaign to unseat United
States senator Hillary Rodham Clinton was in up-
heaval almost from the start, said yesterday that she
would abandon the race and would instead run for
attorney general of New York.
CLwritten
C Montserrat, the Caribbean island, is bracing itself
for arrests following a fraud investigation by Scot-
land Yard.
S Montserrat, the tiny Caribbean island that once
boasted one bank for every 40 inhabitants, is brac-
ing itself this Easter for a spate of arrests following
a three-year fraud and corruption investigation by
Scotland Yard.
CLspoken
C This gives you the science behind the news, with top-
ics explained in detail, from Mad Cow disease to
comets.
S This page gives you the science behind the news,
with hundreds of topics explained in detail, from
Mad Cow disease to comets.
396
Table 8: form GST/g+r, GST/g, T3, and Gold standard. (?Source? represents a source sentence.)
GST/g+r The Corporation plans to announce today at the Game Show that it will begin selling the
Xbox 360, its new video console , on Nov 22.
GST/g The Microsoft Corporation plans to announce at the Tokyo Game Show that it will begin
selling Xbox 360, new video console , on Nov.
T3 The Microsoft Corporation in New York plans to announce today at the Tokyo Game
Show it will begin selling the Xbox 360 , its new video game console, on Nov 22.
Gold The Microsoft Corporation plans to announce Thursday at the Tokyo Game Show that it
will begin selling the Xbox 360 , its new video game console, on Nov. 22.
Source The Microsoft Corporation plans to announce today at the Tokyo Game Show that it will
begin selling the Xbox 360, its new video game console, on Nov 22.
GST/g+r Scientists may have solved the chemical riddle of why the SARS virus causes such pneu-
monia and have developed a simple therapy.
GST/g Scientists may have solved the chemical riddle of why the virus causes such a pneumonia
and have developed a simple therapy.
T3 The scientists may solved the chemical riddle of the black river of why the SARS virus
causes such a deadly pneumonia.
Gold Scientists may have solved the riddle of why the SARS virus causes such a deadly pneu-
monia.
Source Scientists may have solved the chemical riddle of why the SARS virus causes such a
deadly pneumonia and have developed a simple therapy that promises to decrease the
extraordinarily high death rate from the disease, according to a report in the issue of the
journal nature-medicine that came out this week.
GST/g+r A flu shot from GlaxoSmithKline was approved by American regulators and the Corpo-
ration vaccine plant, shut year because of, moved closer to being opened work to avoid.
GST/g A flu shot was approved by regulators yesterday and the Chiron Corporation vaccine
plant, shut , moved closer to being opened as officials work to avoid shortage.
T3 A flu shot from gaza was the Chiron Corporation?s Liverpool vaccine plant, shut last year
of a contamination shortage,, but critics suggest he is making it worse.
Gold The Chiron Corporation?s liverpool vaccine plant , shut last year because of contamina-
tion, moved closer to being opened as officials work to avoid another shortage.
Source A flu shot fromGlaxoSmithKline was approved by American regulators yesterday and the
Chiron Corporation?s Liverpool vaccine plant , shut last year because of contamination,
moved closer to being opened as officials work to avoid another shortage.
CLwritten
De
ns
it
y
?20 ?10 0 10 20 30
0.
00
0.
02
0.
04
0.
06
0.
08
CLspoken
De
ns
it
y
?20 ?10 0 10 20 30
0.
00
0.
02
0.
04
0.
06
0.
08
NYT
De
ns
it
y
?10 0 10 20 30 40
0.
00
0.
05
0.
10
0.
15
0.
20
Figure 4: Density distribution of alignment scores. The x-dimension represents the degree of alignment
between gold standard compression and its source sentence.
397
Table 9: Alignment Scores by SWA
NYT-RSS CLwritten CLspoken
-3.061 (2000) -1.882 (1629) 0.450 (4110)
10 Why T3 fails
It is interesting and worthwhile to ask what caused
T3, heavily clad in ideas from the recent ma-
chine learning literature, to fail on NYT-RSS, as
opposed to the ?CLwritten? and ?CLspoken? cor-
pora on which T3 reportedly prevailed compared
to other approaches (Cohn and Lapata, 2009).
The CLwritten corpus comes from written sources
in the British National Corpus and the American
News Text corpus; the CLspoken corpus comes
from transcribed broadcast news stories (cf. Ta-
ble 7).
We argue that there are some important dif-
ferences between the NYT-RSS corpus and the
CLwritten/CLspoken corpora that may have led to
T3?s poor record with the former corpus.
The CLwritten and CLspoken corpora were cre-
ated with a specific purpose in mind: namely to
assess the compression-by-deletion approach. So
their authors had a very good reason to limit gold
standard compressions to those that can be arrived
at only through deletion; annotators were care-
fully instructed to create compression by delet-
ing words from the source sentence in a way that
preserves the gist of the original sentence. By
contrast, NYT-RSS consists of naturally occurring
compressions sampled from live feeds on the In-
ternet, where relations between compression and
its source sentence are often not as straightfor-
ward. For instance, to arrive at a compression in
NYT-RSS in Table 7 involves replacing race with
her plans to unseat senator Hillary Rodam Clin-
ton, which is obviously beyond what is possible
with the deletion based approach.
In CLwritten and CLspoken, on the other hand,
compressions are constructed out of parts that ap-
pear in verbatim in the original sentence, as Ta-
ble 7 shows: thus one may get to the compres-
sions by simply crossing off words from the origi-
nal sentence.
To see whether there is any significant differ-
ence among NYT-RSS, CLwritten and CLspoken,
we examined how well gold standard compres-
sions are aligned with source sentences on each
of the corpora, using SWA. Table 9 shows what
we found. Parenthetical numbers represent how
many pairs of compression and source are found in
each corpus. A larger score means a tighter align-
ment between gold standard compression and its
source sentence: we find in Table 9 that CLspoken
has a source sentence more closely aligned with
its compression than CLwritten, whose alignments
are more closely tied than NYT-RSS?s.
Figure 4 (found in the previous page) shows
how SWA alignment scores are distributed over
each of the corpora. CLwritten and CLspoken
have peaks at around 0, with an almost entire
mass of scores concentrating in an area close to or
above 0. This means that for the most of the cases
in either CLwritten or CLspoken, compression is
very similar in form to its source. In contrast,
NYT-RSS has a heavy concentration of scores in
a stretch between -5 and -10, indicating that for
the most of time, the overlap between compres-
sion and its source is rather modest compared to
CLwritten and CLspoken.
So why does T3 fails on NYT-RSS? Because
NYT-RSS contains lots of alignments that are only
weakly related: in order for T3 to perform well,
the training corpus should be made as free of spu-
rious data as possible, so that most of the align-
ments are rated over and around 0 by SWA. Our
concern is that such data may not happen naturally,
as the density distribution of NYT-RSS shows,
where the majority of alignments are found far be-
low 0, which could raise some questions about the
robustness of T3.
11 Conclusions
This paper introduced the model free approach,
GST/g, which works by creating compressions
only in reference to dependency structure, and
looked at how it compares with a model intensive
approach T3 on the data gathered from the Inter-
net. It was found that the latter approach appears
to crucially rely on the way the corpus is con-
structed in order for it to work, which may mean a
huge compromise.
Interestingly enough, GST/g came out a winner
on the particular corpus we used, even outperform-
398
ing its CRFs harnessed version, GST/g+r in repre-
sentativeness. This suggests that we might gain
more by improving fluency of GST/g than by fo-
cusing on its representativeness, which in any case
came close to that of human at 70% compression
level. The future work should also look at how the
present approach fares on CLwritten and CLspo-
ken, for which T3 was found to be effective.
Acknowledgements
The author likes to express gratitude to the review-
ers of EMNLP for the time and trouble they took
to review the paper. Their efforts are greatly ap-
preciated.
References
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st COLING and 44th
ACL, pages 377?384, Sydney, Australia, July.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks. In
Subbarao Kambhampati and Craig A. Knoblock,
editors, IIWeb, pages 73?78.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the 2007
EMNLP-CoNLL, pages 73?82, Prague, Czech Re-
public, June.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd COLING, pages 137?144, Manchester,
UK, August.
Trevor Cohn and Mirella Lapata. 2009. Sen-
tence compression as tree transduction. Draft at
http://homepages.inf.ed.ac.uk/tcohn/t3/.
Trevor Cohn. 2008. T3: Tree Transducer Toolkit.
http://homepages.inf.ed.ac.uk/tcohn/t3/.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proceedings
of the 45th ACL, pages 320?327, Prague, Czech Re-
public, June.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the 2008 EMNLP, pages 177?185, Hon-
olulu, Hawaii, October.
C. Hori and Sadaoki Furui. 2004. Speech summa-
rization: an approach through word extraction and
a method for evaluation. IEICE Transactions on In-
formation and Systems, E87-D(1):15?25.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st ACL, pages 423?430, Sapporo, Japan, July.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
John Lafferty, Andrew MacCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th ICML-2001.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th EACL, pages 297?304.
Masaaki Nagata. 1994. A stochastic japanese morpho-
logical analyzer using a forward-dp backward-a* n-
best search algorithm. In Proceedings of COLING-
94.
Tadashi Nomoto. 2008. A generic sentence trimmer
with CRFs. In Proceedings of ACL-08: HLT, pages
299?307, Columbus, Ohio, June.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence conden-
sation using ambiguity packing and stochastic dis-
ambiguation methods for lexical functional gram-
mar. In Proceedings of HLT-NAACL 2003, pages
118?125, Edmonton.
T. F. Smith and M. S. Waterman. 1981. Identifica-
tion of common molecular subsequence. Journal of
Molecular Biology, 147:195?197.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2005. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. Journal of Machine Learning
Research, 6:1453?1484.
Jenie Turner and Eugen Charniak. 2005. Supervised
and unsupervised learning for sentence compres-
sion. In Proceedings of the 43rd ACL, pages 290?
297, Ann Arbor, June.
Kiwamu Yamagata, Satoshi Fukutomi, Kazuyuki Tak-
agi, and Kzauhiko Ozeki. 2006. Sentence compres-
sion using statistical information about dependency
path length. In Proceedings of TSD 2006 (Lecture
Notes in Computer Science, Vol. 4188/2006), pages
127?134, Brno, Czech Republic.
399
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 249?256, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Bayesian Learning in Text Summarization
Tadashi Nomoto
National Institute of Japanese Literature
1-16-10 Yutaka Shinagawa
Tokyo 142-8585 Japan
nomoto@acm.org
Abstract
The paper presents a Bayesian model for
text summarization, which explicitly en-
codes and exploits information on how hu-
man judgments are distributed over the
text. Comparison is made against non
Bayesian summarizers, using test data
from Japanese news texts. It is found that
the Bayesian approach generally lever-
ages performance of a summarizer, at
times giving it a significant lead over non-
Bayesian models.
1 Introduction
Consider figure 1. What is shown there is the pro-
portion of the times that sentences at particular lo-
cations are judged as relevant to summarization, or
worthy of inclusion in a summary. Each panel shows
judgment results on 25 Japanese texts of a particular
genre; columns (G1K3), editorials (G2K3) and news
stories (G3K3). All the documents are from a sin-
gle Japanese news paper, and judgments are elicited
from some 100 undergraduate students. While more
will be given on the details of the data later (Sec-
tion 3.2), we can safely ignore them here.
Each panel has the horizontal axis representing lo-
cation or order of sentence in a document, and the
vertical axis the proportion of the times sentences at
particular locations are picked as relevant to summa-
rization. Thus in G1K3, we see that the first sentence
(to appear in a document) gets voted for about 12%
of the time, while the 26th sentence is voted for less
than 2% of the time.
Curiously enough, each of the panels exhibits a
distinct pattern in the way votes are spread across
a document: G1K3 has the distribution of votes
(DOV) with sharp peaks around 1 and 14; in G2K3,
the distribution is peaked around 1, with a small
bump around 19; in G3K3, the distribution is sharply
skewed to the left, indicating that the majority of
votes went to the initial section of a document. What
is interesting about the DOV is that we could take
it as indicating a collective preference for what to
extract for a summary. A question is then, can we
somehow exploit the DOV in summarization? To
our knowledge, no prior work seems to exist that
addresses the question. The paper discusses how
we could do this under a Bayesian modeling frame-
work, where we explicitly represent and make use
of the DOV by way of Dirichlet posterior (Congdon,
2003).1
2 Bayesian Model of Summaries
Since the business of extractive summarization, such
as one we are concerned with here, is about ranking
sentences according to how useful/important they
are as part of summary, we will consider here a par-
ticular ranking scheme based on the probability of a
sentence being part of summary under a given DOV,
i.e.,
P (y|v), (1)
where y denotes a given sentence, and v =
(v1, . . . , vn) stands for a DOV, an array of observed
vote counts for sentences in the text; v1 refers to the
count of votes for a sentence at the text initial posi-
tion, v2 to that for a sentence occurring at the second
place, etc.
Thus given a four sentence long text, if we have
three people in favor of a lead sentence, two in favor
1See Yu et al (2004) and Cowans (2004) for its use in IR.
249
1 4 7 10 14 18 22 26
g1k3
sentences
vo
tes
 w
on
 (%
)
0.0
0
0.0
5
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
1 4 7 10 14 18 22 26
g2k3
sentences
vo
tes
 w
on
 (%
)
0.0
0
0.0
5
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
1 4 7 10 14 18 22
g3k3
sentences
vo
tes
 w
on
 (%
)
0.0
0
0.0
5
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
Figure 1: Genre-by-genre vote distribution
of the second, one for the third, and none for the
fourth, then we would have v = (3, 2, 1, 0).
Now suppose that each sentence yi (i.e., a sen-
tence at the i-th place in the order of appearance) is
associated with what we might call a prior prefer-
ence factor ?i, representing how much a sentence at
a particular position is favored as part of a summary
in general. Then the probability that yi finds itself in
a summary is given as:
?(yi|?i)P (?i), (2)
where ? denotes some likelihood function, and
P (?i) a prior probability of ?i.
Since the DOV is something we could actually
observe about ?i, we might as well couple ?i with
v by making a probability of ?i conditioned on v .
Formally, this would be written as:
?(yi|?i)P (?i|v). (3)
The problem, however, is that we know nothing
about what each ?i looks like, except that it should
somehow be informed by v . A typical Bayesian so-
lution to this is to ?erase? ?i by marginalizing (sum-
ming) over it, which brings us to this:
P (yi|v) =
?
?(yi|?i)P (?i |v) d?i. (4)
Note that equation 4 no longer talks about the proba-
bility of yi under a particular ?i; rather it talks about
the expected probability for yi with respect to a pref-
erence factor dictated by v . All we need to know
v //
??
? // yi
Figure 2: A graphical view
about P (?i|v) to compute the expectation is v and a
probability distribution P , and not ?i?s, anymore.
We know something about v , and this would
leave us P . So what is it? In principle it could
be any probability distribution. However largely
for the sake of technical convenience, we assume
it is one component of a multinomial distribution
known as the Dirichlet distribution. In particular,
we talk about Dirichlet(?|v), namely a Dirichlet
posterior of ?, given observations v , where ? =
(?1, . . . , ?i, . . . , ?n), and
?n
i ?i = 1 (?i > 0). (Re-
markably, if P (?) is a Dirichlet, so is P (?|v).) ?
here represents a vector of preference factors for n
sentences ? which constitute the text.2
Accordingly, equation 4 could be rewritten as:
P (yi|v) =
?
?(yi|?)P (? |v) d?. (5)
An interesting way to look at the model is by way
of a graphical model (GM), which gives some in-
tuitive idea of what the model looks like. In a GM
perspective, our model is represented as a simple tri-
partite structure (figure 2), in which each node corre-
sponds to a variable (parameter), and arcs represent
2Since texts generally vary in length, we may set n to a suf-
ficiently large number so that none of texts of interest may ex-
ceed it in length. For texts shorter than n, we simply add empty
sentences to make them as long as n.
250
dependencies among them. x ? y reads ?y depends
on x.? An arc linkage between v and yi is meant to
represent marginalization over ?.
Moreover, we will make use of a scale parame-
ter ? ? 1 to have some control over the shape
of the distribution, so we will be working with
Dirichlet(?|?v) rather than Dirichlet(?|v). Intu-
itively, we might take ? as representing a degree of
confidence we have in a set of empirical observa-
tions we call v , as increasing the value of ? has the
effect of reducing variance over each ?i in ?.
The expectation and variance of Dirichlet(?|v) are
given as follows.3
E[?i] = viv0 (6)
V ar[?i] = vi(v0 ? vi)v20(v0 + 1)
, (7)
where v0 =
?n
i vi. Therefore the variance of a
scaled Dirichlet is:
V ar[?i|?v] = vi(v0 ? vi)v20(?v0 + 1)
. (8)
See how ? is stuck in the denominator. Another ob-
vious fact about the scaling is that it does not affect
the expectation, which remains the same.
To get a feel for the significance of ?, con-
sider figure 3; the left panel shows a histogram
of 50,000 variates of p1 randomly drawn from
Dirichlet(p1, p2|?c1, ?c2), with ? = 1, and both c1
and c2 set to 1. The graph shows only the p1 part
but things are no different for p2. (The x-dimension
represents a particular value p1 takes (which ranges
between 0 and 1) and the y-dimension records the
number of the times p1 takes that value.) We see that
points are spread rather evenly over the probability
space. Now the right panel shows what happens if
you increase ? by a factor of 1,000 (which will give
you P (p1, p2|1000, 1000)); points take a bell shaped
form, concentrating in a small region around the ex-
pectation of p1. In the experiments section, we will
return to the issue of ? and discuss how it affects
performance of summarization.
Let us turn to the question of how to find a solu-
tion to the integral in equation 5. We will be con-
cerned here with two standard approaches to the is-
sue: one is based on MAP (maximum a posteriori)
3http://www.cis.hut.fi/ahonkela/dippa/dippa.html
and another on numerical integration. We start off
with a MAP based approach known as Bayesian In-
formation Criterion or BIC.
For a given model m, BIC seeks an analytical ap-
proximation for equation 4, which looks like the fol-
lowing:
lnP (yi|m) = ln?(yi|??,m)? k2 lnN, (9)
where k denotes the number of free parameters in
m, and N that of observations. ?? is a MAP estimate
of ? under m, which is E[?]. It is interesting to note
that BIC makes no reference to prior. Also worthy of
note is that a minus of BIC equals MDL (Minimum
Description Length).
Alternatively, one might take a more straightfor-
ward (and fully Bayesian) approach known as the
Monte Carlo integration method (MacKay, 1998)
(MC, hereafter) where the integral is approximated
by:
P (yi|v) ? 1n
n?
j=1
?(yi|x(j)), (10)
where we draw each sample x(j) randomly from the
distribution P (?|v), and n is the number of x(i)?s
so collected. Note that MC gives an expectation of
P (yi|v) with respect to P (?|v).
Furthermore, ? could be any probabilistic func-
tion. Indeed any discriminative classifier (such as
C4.5) will do as long as it generates some kind of
probability. Given ?, what remains to do is essen-
tially training it on samples bootstrapped (i.e., re-
sampled) from the training data based on ? ? which
we draw from Dirichlet(?|v).4 To be more spe-
cific, suppose that we have a four sentence long text
and an array of probabilities ? = (0.4, 0.3, 0.2, 0.1)
drawn from a Dirichlet distribution: which is to say,
we have a preference factor of 0.4 for the lead sen-
tence, 0.3 for the second sentence, etc. Then we re-
sample with replacement lead sentences from train-
ing data with the probability of 0.4, the second with
the probability of 0.3, and so forth. Obviously, a
4It is fairly straightforward to sample from a Dirichlet pos-
terior by resorting to a gamma distribution, which is what is
happening here. In case one is working with a distribution it is
hard to sample from, one would usually rely on Markov chain
Monte Carlo (MCMC) or variational methods to do the job.
251
L=1
p1
Fre
que
ncy
0.0 0.2 0.4 0.6 0.8 1.0
0
100
300
500
L=1000
p1
Fre
que
ncy
0.46 0.48 0.50 0.52 0.54
0
500
100
0
150
0
Figure 3: Histograms of random draws from Dirichlet(p1, p2|?c1, ?c2) with ? = 1 (left panel), and ? =
1000 (right panel).
high preference factor causes the associated sen-
tence to be chosen more often than those with a low
preference.
Thus given a text T = (a, b, c, d) with ? =
(0.4, 0.3, 0.2, 0.1), we could end up with a data set
dominated by a few sentence types, such as T ? =
(a, a, a, b), which we proceed to train a classifier on
in place of T . Intuitively, this amounts to induc-
ing the classifier to attend to or focus on a partic-
ular region or area of a text, and dismiss the rest.
Note an interesting parallel to boosting (Freund and
Schapire, 1996) and the alternating decision tree
(Freund and Mason, 1999).
In MC, for each ?(k) drawn from Dirichlet(?|v),
we resample sentences from the training data using
probabilities specified by ?(k), use them for train-
ing a classifier, and run it on a test document d to
find, for each sentence in d, its probability of being
a ?pick? (summary-worthy) sentence,i.e., P (yi|?(k)),
which we average across ??s. In experiments later
described, we apply the procedure for 20,000 runs
(meaning we run a classifier on each of 20,000 ??s
we draw), and average over them to find an estimate
for P (yi|v).
As for BIC, we generally operate along the lines
of MC, except that we bootstrap sentences using
only E[?], and the model complexity term, namely,
?k2 lnN is dropped as it has no effect on ranking
sentences. As with MC, we train a classifier on the
bootstrapped samples and run it on a test document.
Though we work with a set of fixed parameters, a
bootstrapping based on them still fluctuates, produc-
ing a slightly different set of samples each time we
run the operation. To get a reasonable convergence
in experiments, we took the procedure to 5,000 iter-
ations and averaged over the results.
Either with BIC or with MC, building a summa-
rizer on it is a fairly straightforward matter. Given
a document d and a compression rate r, what a
summarizer would do is simply rank sentences in d
based on P (yi|v) and pick an r portion of highest
ranking sentences.
3 Working with Bayesian Summarist
3.1 C4.5
In what follows, we will look at whether and how the
Bayesian approach, when applied for the C4.5 deci-
sion tree learner (Quinlan, 1993), leverages its per-
formance on real world data. This means our model
now operates either by
P (yi|v) ? 1n
n?
j=1
?c4.5(yi|x(j)), (11)
or by
lnP (yi|m) = ln?c4.5(yi|??,m)? k2 lnN, (12)
with the likelihood function ? filled out by C4.5.
Moreover, we compare two versions of the classifier;
one with BIC/MC and one without. We used Weka
implementations of the algorithm (with default set-
tings) in experiments described below (Witten and
Frank, 2000).
252
While C4.5 here is configured to work in a bi-
nary (positive/negative) classification scheme, we
run it in a ?distributional? mode, and use a particular
class membership probability it produces, namely,
the probability of a sentence being positive, i.e., a
pick (summary-worthy) sentence, instead of a cate-
gory label.
Attributes for C4.5 are broadly intended to repre-
sent some aspects of a sentence in a document, an
object of interest here. Thus for each sentence ?, its
encoding involves reference to the following set of
attributes or features. ?LocSen? gives a normalized
location of ? in the text, i.e., a normalized distance
from the top of the text; likewise, ?LocPar? gives a
normalized location of the paragraph in which ? oc-
curs, and ?LocWithinPar? records its normalized lo-
cation within a paragraph. Also included are a few
length-related features such as the length of text and
sentence. Furthermore we brought in some language
specific feature which we call ?EndCue.? It records
the morphology of a linguistic element that ends ?,
such as inflection, part of speech, etc.
In addition, we make use of the weight feature
(?Weight?) for a record on the importance of ? based
on tf.idf. Let ? = w1, . . . , wn, for some word wi.
Then the weight W (?) is given as:
W (?) =
?
w
(1 + log(tf(w))) ? log(N/df(w)).
Here ?tf(w)? denotes the frequency of word w in a
given document, ?df(w)? denotes the ?document fre-
quency? of w, or the number of documents which
contain an occurrence of w. N represents the total
number of documents.5
Also among the features used here is ?Pos,? a fea-
ture intended to record the position or textual order
of ?, given by how many sentences away it occurs
from the top of text, starting with 0.
While we do believe that the attributes discussed
above have a lot to do with the likelihood that a given
sentence becomes part of summary, we choose not
to consider them parameters of the Bayesian model,
just to keep it from getting unduly complex. Recall
the graphical model in figure 2.
5Although one could reasonably argue for normalizing
W (?) by sentence length, it is not entirely clear at the moment
whether it helps in the way of improving performance.
3.2 Test Data
Here is how we created test data. We collected three
pools of texts from different genres, columns, edito-
rials and news stories, from a Japanese financial pa-
per (Nihon Keizai Shinbun) published in 1995, each
with 25 articles. Then we asked 112 Japanese stu-
dents to go over each article and identify 10% worth
of sentences they find most important in creating
a summary for that article. For each sentence, we
recorded how many of the subjects are in favor of
its inclusion in summary. On average, we had about
seven people working on each text. In the follow-
ing, we say sentences are ?positive? if there are three
or more people who like to see them in a summary,
and ?negative? otherwise. For convenience, let us
call the corpus of columns G1K3, that of editorials
G2K3 and that of news stories G3K3. Additional
details are found in table 1.
4 Results and Discussion
Tables 2 through 4 show how the Bayesian sum-
marist performs on G1K3, G2K3, and G3K3. The
tables list results in precision at compression rates
(r) of interest (0 < r < 1). The figures thereof indi-
cate performance averaged over leave-one-out cross
validation folds. What this means is that you leave
out one text for testing and use the rest for training,
which you repeat for each one of the texts in the data.
Since we have 25 texts for each data set, this leads
to a 25-fold cross validation. Precision is defined by
the ratio of hits (positive sentences) to the number
of sentences retrieved, i.e., r-percent of sentences in
the text.6
In each table, figures to the left of the verti-
cal line indicate performance of summarizers with
BIC/MC and those to the right that of summarizers
without them. Parenthetical figures like ?(5K)? and
?(20K)? indicate the number of iterations we took
them to: thus BIC(5K) refers to a summarizer based
on C4.5/BIC with scores averaged over 5,000 runs.
BSE denotes a reference summarizer based on a reg-
ular C4.5, which it involves no resampling of train-
ing data. LEAD refers to a summarizer which works
6We do not use recall for a evaluation measure, as the num-
ber of positive instances varies from text to text, and may indeed
exceed the length of a summary under a particular compression
rate.
253
Table 1: N represents the number of sentences in G1K3 to G3K3. Sentences with three or more votes in
their favor are marked positive, that is, for each sentence marked positive, at least three people are in favor
of including it in a summary.
Genre N Positive (? 3) Negative P/N Ratio
G1K3 426 67 359 0.187
G2K3 558 93 465 0.200
G3K3 440 76 364 0.210
Table 2: G1K3. ? = 5. Dashes indicate no meaningful results.
r BIC (5K) MC (20K) BSE LEAD
0.05 0.4583 0.4583 ? 0.3333
0.10 0.4167 0.4167 ? 0.3472
0.15 0.3333 0.3472 ? 0.2604
0.20 0.2757 0.2861 ? 0.2306
0.25 0.2525 0.2772 ? 0.2233
0.30 0.2368 0.2535 ? 0.2066
Table 3: G2K3. ? = 5.
r BIC (5K) MC (20K) BSE LEAD
0.05 0.6000 0.5800 0.4200 0.5400
0.10 0.4200 0.4200 0.3533 0.3933
0.15 0.3427 0.3560 0.2980 0.3147
0.20 0.3033 0.3213 0.2780 0.2767
0.25 0.2993 0.2776 0.2421 0.2397
0.30 0.2743 0.2750 0.2170 0.2054
Table 4: G3K3. ? = 5.
r BIC (5K) MC (20K) BSE LEAD
0.05 0.9600 0.9600 0.8400 0.9600
0.10 0.7600 0.7600 0.6800 0.7000
0.15 0.6133 0.6000 0.5867 0.5133
0.20 0.5233 0.5233 0.4967 0.4533
0.25 0.4367 0.4367 0.3960 0.3840
0.30 0.4033 0.4033 0.3640 0.3673
0 (411.0/65.0)
Figure 4: A non Bayesian C4.5 trained on G1K3.
254
LenSenA
0 (199.0/23.0)
<= 64
EndCueA
> 64
Weight
= 0
LocWithinPar
= 1
0 (0.0)
= 2
Weight
= 3
0 (5.0/1.0)
= 4
LocWithinPar
= 5
0 (7.0)
= 6
LocWithinPar
<= 2.338
0 (17.0)
> 2.338
Weight
<= 0
LenSenA
> 0
LocPar
<= 2.255
1 (7.0)
> 2.255
1 (4.0)
<= 0
0 (22.0/7.0)
> 0
LocWithinPar
<= 114
LocWithinPar
> 114
0 (38.0/4.0)
<= 0.8
1 (2.0)
> 0.8
1 (10.0/2.0)
<= 0.7
0 (3.0)
> 0.7
0 (11.0/1.0)
<= 0.286
LocPar
> 0.286
LenSenA
<= 0.667
1 (8.0)
> 0.667
1 (13.0/1.0)
<= 72
0 (8.0)
> 72
0 (15.0)
<= 1.707
LocSen
> 1.707
LocWithinPar
<= 0.917
0 (7.0)
> 0.917
0 (5.0/1.0)
<= 0
LenSenA
> 0
1 (17.0)
<= 110
LocSen
> 110
0 (3.0)
<= 0.333
1 (2.0)
> 0.333
1 (3.0/1.0)
<= 0.429
0 (6.0)
> 0.429
Figure 5: A Bayesian (MC) C4.5 trained on G1K3.
by selecting sentences from the top of the text. It is
generally considered a hard-to-beat approach in the
summarization literature.
Table 4 shows results for G3K3 (a news story do-
main). There we find a significantly improvement to
performance of C4.5, whether it operates with BIC
or MC. The effect is clearly visible across a whole
range of compression rates, and more so at smaller
rates.
Table 3 demonstrates that the Bayesian approach
is also effective for G2K3 (an editorial domain), out-
performing both BSE and LEAD by a large margin.
Similarly, we find that our approach comfortably
beats LEAD in G1K3 (a column domain). Note the
dashes for BSE. What we mean by these, is that we
obtained no meaningful results for it, because we
were unable to rank sentences based on predictions
by BSE. To get an idea of how this happens, let us
look at a decision tree BSE builds for G1K3, which
is shown in figure 4. What we have there is a deci-
sion tree consisting of a single leaf.7 Thus for what-
ever sentence we feed to the tree, it throws back the
same membership probability, which is 65/411. But
then this would make a BSE based summarizer ut-
terly useless, as it reduces to generating a summary
by picking at random, a particular portion of text.8
7This is not at all surprising as over 80% of sentences in a
non resampled text are negative for the most of the time.
8Its expected performance (averaged over 106 runs) comes
Now Figure 5 shows what happens with the
Bayesian model (MC), for the same data. There
we see a tree of a considerable complexity, with 24
leaves and 18 split nodes.
Let us now turn to the issues with ?. As we might
recall, ? influences the shape of a Dirichlet distri-
bution: a large value of ? causes the distribution
to have less variance and therefore to have a more
acute peak around the expectation. What this means
is that increasing the value of ? makes it more likely
to have us drawing samples closer to the expecta-
tion. As a consequence, we would have the MC
model acting more like the BIC model, which is
based on MAP estimates. That this is indeed the
case is demonstrated by table 5, which gives results
for the MC model on G1K3 to G3K3 at ? = 1. We
see that the MC behaves less like the BIC at ? = 1
than at ? = 5 (table 2 through 4).
Of a particular interest in table 5 is G1K3, where
the MC suffers a considerable degradation in per-
formance, compared to when it works with ? = 5.
G2K3 and G3K3, again, witness some degradation
in performance, though not as extensive as in G1K3.
It is interesting that at times the MC even works bet-
ter with ? = 1 than ? = 5 in G2K3 and G3K3.9
to: 0.1466 (r = 0.05), 0.1453 (r = 0.1), 0.1508 (r = 0.15),
0.1530 (r = 0.2), 0.1534 (r = 0.25), and 0.1544 (r = 0.3).
9The results suggest that if one like to have some improve-
ment, it is probably a good idea to set ? to a large value. But
255
Table 5: MC (20K). ? = 1.
r G1K3 G2K3 G3K3
0.05 0.3333 0.5400 0.9600
0.10 0.3333 0.3867 0.7800
0.15 0.2917 0.3960 0.5867
0.20 0.2549 0.3373 0.5200
0.25 0.2480 0.2910 0.4347
0.30 0.2594 0.2652 0.4100
All in all, the Bayesian model proves more effec-
tive in leveraging performance of the summarizer on
a DOV exhibiting a complex, multiply peaked form
as in G1K3 and G2K3, and less on a DOV which
has a simple, single-peak structure as in G3K3 (cf.
figure 1).10
5 Concluding Remarks
The paper showed how it is possible to incorporate
information on human judgments for text summa-
rization in a principled manner through Bayesian
modeling, and also demonstrated how the approach
leverages performance of a summarizer, using data
collected from human subjects.
The present study is motivated by the view that
that summarization is a particular form of collabo-
rative filtering (CF), wherein we view a summary
as a particular set of sentences favored by a par-
ticular user or a group of users just like any other
things people would normally have preference for,
such as CDs, books, paintings, emails, news articles,
etc. Importantly, under CF, we would not be asking,
what is the ?correct? or gold standard summary for
document X? ? the question that consumed much of
the past research on summarization. Rather, what we
are asking is, what summary is popularly favored for
X?
Indeed the fact that there could be as many sum-
maries as angles to look at the text from may favor
in general how to best set ? requires some experimenting with
data and the optimal value may vary from domain to domain.
An interesting approach would be to empirically optimize ? us-
ing methods suggested in MacKay and Peto (1994).
10Incidentally, summarizers, Bayesian or not, perform con-
siderably better on G3K3 than on G1K3 or G2K3. This hap-
pens presumably because a large portion of votes concentrate
in a rather small region of text there, a property any classifier
should pick up easily.
the CF view of summary: the idea of what consti-
tutes a good summary may vary from person to per-
son, and may well be influenced by particular inter-
ests and concerns of people we elicit data from.
Among some recent work with similar concerns,
one notable is the Pyramid scheme (Nenkova and
Passonneau, 2004) where one does not declare a
particular human summary a absolute reference to
compare summaries against, but rather makes every
one of multiple human summaries at hand bear on
evaluation; Rouge (Lin and Hovy, 2003) represents
another such effort. The Bayesian summarist rep-
resents yet another, whereby one seeks a summary
most typical of those created by humans.
References
Peter Congdon. 2003. Bayesian Statistical Modelling.
John Wiley and Sons.
Philip J. Cowans. 2004. Information Retrieval Using
Hierarchical Dirichlet Processes. In Proc. 27th ACM
SIGIR.
Yoav Freund and Llew Mason. 1999. The alternating
decision tree learning algorithm,. In Proc. 16th ICML.
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a new boosting algorithm. In Proc. 13th ICML.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurance statis-
tics. In Proc. HLT-NAACL 2003.
David J. C. MacKay and Linda C. Bauman Peto. 1994. A
Hierarchical Dirichlet Language Model. Natural Lan-
guage Engineering.
D. J. C. MacKay. 1998. Introduction to Monte Carlo
methods. In M. I. Jordan, editor, Learning in Graphi-
cal Models, Kluwer Academic Press.
Ani Nenkova and Rebecca Passonneau. 2004. Evalua-
tion Content Selection in Summarization: The Pyra-
mid Method. In Proc. HLT-NAACL 2004.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
Kai Yu, Volker Tresp, and Shipeng Yu. 2004. A Non-
parametric Hierarchical Bayesian Framework for In-
formation Filtering. In Proc. 27th ACM SIGIR.
256
Supervised Ranking in Open-Domain Text Summarization
Tadashi Nomoto
National Institute of Japanese Literature
1-16-10 Yutaka Shinagawa
Tokyo 142-8585, Japan
nomoto@nijl.ac.jp
Yuji Matsumoto
Nara Institute of Science and Technology
8916-5 Takayama Ikoma
Nara 630-0101, Japan
matsu@is.aist-nara.ac.jp
Abstract
The paper proposes and empirically moti-
vates an integration of supervised learning
with unsupervised learning to deal with
human biases in summarization. In par-
ticular, we explore the use of probabilistic
decision tree within the clustering frame-
work to account for the variation as well
as regularity in human created summaries.
The corpus of human created extracts is
created from a newspaper corpus and used
as a test set. We build probabilistic de-
cision trees of different flavors and in-
tegrate each of them with the clustering
framework. Experiments with the cor-
pus demonstrate that the mixture of the
two paradigms generally gives a signif-
icant boost in performance compared to
cases where either of the two is considered
alone.
1 Introduction
Nomoto and Matsumoto (2001b) have recently
made an interesting observation that an unsu-
pervised method based on clustering sometimes
better approximates human created extracts than a
supervised approach. That appears somewhat con-
tradictory given that a supervised approach should
be able to exploit human supplied information about
which sentence to include in an extract and which
not to, whereas an unsupervised approach blindly
chooses sentences according to some selection
scheme. An interesting question is, why this should
be the case.
The reason may have to do with the variation in
human judgments on sentence selection for a sum-
mary. In a study to be described later, we asked stu-
dents to select 10% of a text which they find most
important for making a summary. If they agree per-
fectly on their judgments, then we will have only
10% of a text selected as most important. However,
what we found was that about half of a text were
marked as important, indicating that judgments can
vary widely among humans.
Curiously, however, Nomoto and Matsumoto
(2001a) also found that a supervised system fares
much better when tested on data exhibiting high
agreement among humans than an unsupervised sys-
tem. Their finding suggests that there are indeed
some regularities (or biases) to be found.
So we might conclude that there are two aspects to
human judgments in summarization; they can vary
but may exhibit some biases which could be usefully
exploited. The issue is then how we might model
them in some coherent framework.
The goal of the paper is to explore a possible in-
tegration of supervised and unsupervised paradigms
as a way of responding to the issue. Taking a de-
cision tree and clustering as representing the respec-
tive paradigm, we will show how coupling them pro-
vides a summarizer that better approximates human
judgments than either of the two considered alone.
To our knowledge, none of the prior work on sum-
marization (e.g., Kupiec et al (1995)) explicitly ad-
dressed the issue of the variability inherent in human
judgments in summarization tasks.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 465-472.
                         Proceedings of the 40th Annual Meeting of the Association for
X1
0
||zzzz
zz 1
??BB
BBB
B
Y1
(?1y, ?1n) X2
0
?????
??? 1
??BB
BBB
B
Y2
(?2y, ?2n)
Y3
(?3y, ?3n)
Figure 1: Probabilistic Decision Tree
2 Supervised Ranking with Probabilistic
Decision Tree
One technical problem associated with the use of a
decision tree as a summarizer is that it is not able to
rank sentences, which it must be able do, to allow for
the generation of a variable-length summary. In re-
sponse to the problem, we explore the use of a prob-
abilistic decision tree as a ranking model. First, let
us review some general features of probabilistic de-
cision tree (ProbDT, henceforth) (Yamanishi, 1997;
Rissanen, 1997).
ProbDT works like a usual decision tree except
that rather than assigning each instance to a single
class, it distributes each instance among classes. For
each instance xi, the strength of its membership to
each of the classes is determined by P (ck | xi) for
each class ck.
Consider a binary decision tree in Fig 1. Let X1
and X2 represent non-terminal nodes, and Y1 and
Y2 leaf nodes. ?1? and ?0? on arcs denote values
of some attribute at X1 and X2. ?iy and ?in repre-
sent the probability that a given instance assigned
to the node i is labeled as yes and no, repectively.
Abusing the terms slightly, let us assume that X1 and
X2 represent splitting attributes as well at respective
nodes. Then the probability that a given instance
with X1 = 1 and X2 = 0 is labeled as yes (no) is
?2y (?2n). Note that
?
c ?jc = 1 for a given node j.
Now to rank sentences with ProbDT simply in-
volves finding the probability that each sentence is
assigned to a particular class designating sentences
worthy of inclusion in a summary (call it ?Select?
class) and ranking them accordingly. (Hereafter and
throughout the rest of the paper, we say that a sen-
tence is wis if it is worthy of inclusion in a summary:
thus a wis sentence is a sentence worthy of inclusion
in a summary.) The probabiliy that a sentence u is
labeled as wis is expressed as in Table 1, where ~u
is a vector representation of u, consisting of a set of
values for features of u; ? is a smoothing function,
e.g., Laplace?s law; t(~u) is some leaf node assigned
to ~u; and DT represents some decision tree used to
classify ~u.
3 Diversity Based Summarization
As an unsupervised summarizer, we use diversity
based summarization (DBS) (Nomoto and Mat-
sumoto, 2001c). It takes a cluster-and-rank approach
to generating summaries. The idea is to form a sum-
mary by collecting sentences representative of di-
verse topics discussed in the text. A nice feature
about their approach is that by creating a summary
covering potential topics, which could be marginal
to the main thread of the text, they are in fact able to
accommodate the variability in sentence selection:
some people may pick up subjects (sentences) as
important which others consider irrelevant or only
marginal for summarization. DBS accomodates this
situation by picking them all, however marginal they
might be.
More specifically, DBS is a tripartite process con-
sisting of the following:
1. Find-Diversity: find clusters of lexically sim-
ilar sentences in text. (In particular, we repre-
sent a sentence here a vector of tfidf weights of
index terms it contains.)
2. Reduce-Redundancy: for each cluster found,
choose a sentence that best represents that clus-
ter.
3. Generate-Summary: collect the representa-
tive sentences, put them in some order, and re-
turn them to the user.
Find-Diversity is based on the K-means clustering
algorithm, which they extended with Minimum De-
scription Length Principle (MDL) (Li, 1998; Ya-
manishi, 1997; Rissanen, 1997) as a way of optimiz-
ing K-means. Reduce-Redundancy is a tfidf based
ranking model, which assigns weights to sentences
in the cluster and returns a sentence that ranks high-
est. The weight of a sentence is given as the sum of
tfidf scores of terms in the sentence.
Table 1: Probabilistic Classification with DT. ~u is a vector representation of sentence u. ? is a smoothing
function. t(~u) is some leaf node assigned to ~u by DT.
P (Select | ~u,DT) = ?
(
the number of ?Select? sentences at t(~u)
the total number of sentences at t(~u)
)
4 Combining ProbDT and DBS
Combining ProbDT and DBS is done quite straight-
forwardly by replacing Reduce-Redundacy with
ProbDT. Thus instead of picking up a sentence with
the highest tfdif based weight, DBS/ProbDT at-
tempts to find a sentences with the highest score for
P (Select | ~u,DT).
4.1 Features
The following lists a set of features used for encod-
ing a sentence in ProbDT. Most of them are either
length- or location-related features.1
<LocSen> The location of a sentence X defined
by:
#S(X)? 1
#S(Last Sentence)
?#S(X)? denotes an ordinal number indicating the
position of X in a text, i.e. #S(kth sentence) = k.
?Last Sentence? refers to the last sentence in a text.
LocSen takes values between 0 and N?1N . N is the
number of sentences in the text.
<LocPar> The location of a paragraph in which
a sentence X occurs given by:
#Par(X)? 1
#Last Paragraph
?#Par(X)? denotes an ordinal number indicat-
ing the position of a paragraph containing X .
?#Last Paragraph? is the position of the last para-
graph in a text, represented by the ordinal number.
<LocWithinPar> The location of a sentence
X within a paragraph in which it appears.
#S(X)?#S(Par Init Sen)
Length(Par(X))
1Note that one may want to add tfidf to a set of features for
a decision tree or, for that matter, to use features other than tfidf
for representing sentences in clustering. The idea is worthy of
consideration, but not pursued here.
Table 2: Linguistic cues
code category
1 non-past
2 past /-ta/
3 copula /-da/
4 noun
5 symbols, e.g., parentheses
6 sentence-ending particles, e.g., /-ka/
0 none of the above
?Par Init Sen? refers to the initial sentence of a para-
graph in which X occurs, ?Length(Par(X))? denotes
the number of sentences that occur in that paragraph.
LocWithinPar takes continuous values ranging
from 0 to l?1l , where l is the length of a paragraph:
a paragraph initial sentence would have 0 and a para-
graph final sentence l?1l .
<LenText> The text length in Japanese charac-
ter i.e. kana, kanji.
<LenSen> The sentence length in kana/kanji.
Some work in Japanese linguistics found that a
particular grammatical class a sentence final ele-
ment belongs to could serve as a cue to identifying
summary sentences. These include categories like
PAST/NON-PAST, INTERROGATIVE, and NOUN and
QUESTION-MARKER. Along with Ichikawa (1990),
we identified a set of sentence-ending cues and
marked a sentence as to whether it contains a cue
from the set.2 Included in the set are inflectional
classes PAST/NON-PAST (for the verb and verbal
adjective), COPULA, and NOUN, parentheses, and
QUESTION-MARKER -ka. We use the following at-
tribute to encode a sentence-ending form.
<EndCue> The feature encodes one of sentence-
2Word tokens are extracted by using CHASEN, a Japanese
morphological analyzer which is reported to achieve the accu-
racy rate of over 98% (Matsumoto et al, 1999).
ending forms described above. It is a discrete valued
feature. The value ranges from 0 to 6. (See Table 2
for details.)
Finally, one of two class labels, ?Select? and
?Don?t Select?, is assigned to a sentence, depend-
ing on whether it is wis or not. The ?Select? label
is for wis sentences, and the ?Don?t Select? label for
non-wis sentences.
5 Decision Tree Algorithms
To examine the generality of our approach, we con-
sider, in addition to C4.5 (Quinlan, 1993), the fol-
lowing decision tree algorithms. C4.5 is used with
default options, e.g., CF=25%.
5.1 MDL-DT
MDL-DT stands for a decision tree with MDL based
pruning. It strives to optimize the decision tree
by pruning the tree in such a way as to produce
the shortest (minimum) description length for the
tree. The description length refers to the num-
ber of bits required for encoding information about
the decision tree. MDL ranks, along with Akaike
Information Criterion (AIC) and Bayes Informa-
tion Criterion (BIC), as a standard criterion in ma-
chine learning and statistics for choosing among
possible (statistical) models. As shown empirically
in Nomoto and Matsumoto (2000) for discourse do-
main, pruning DT with MDL significantly reduces
the size of tree, while not compromising perfor-
mance.
5.2 SSDT
SSDT or Subspace Splitting Decision Tree repre-
sents another form of decision tree algorithm.(Wang
and Yu, 2001) The goal of SSDT is to discover pat-
terns in highly biased data, where a target class, i.e.,
the class one likes to discover something about, ac-
counts for a tiny fraction of the whole data. Note that
the issue of biased data distribution is particularly
relevant for summarization, as a set of sentences to
be identified as wis usually account for a very small
portion of the data.
SSDT begins by searching the entire data space
for a cluster of positive cases and grows the cluster
by adding points that fall within some distance to
the center of the cluster. If the splitting based on the
cluster offers a better Gini index than simply using
Figure 2: SSDT in action. Filled circles represent
positive class, white circles represent negative class.
SSDT starts with a small spherical cluster of pos-
itive points (solid circle) and grows the cluster by
?absorbing? positive points around it (dashed circle).
one of the attributes to split the data, SSDT splits the
data space based on the cluster, that is, forms one re-
gion outside of the cluster and one inside.3 It repeats
the process recursively on each subregions spawned
until termination conditions are met. Figure 2 gives
a snapshot of SSDT at work. SSDT locates some
clusters of positive points, develops spherical clus-
ters around them.
With its particular focus on positive cases, SSDT
is able to provide a more precise characterization of
them, compared, for instance, to C4.5.
6 Test Data and Procedure
We asked 112 Japanese subjects (students at grad-
uate and undergraduate level) to extract 10% sen-
tences in a text which they consider most important
in making a summary. The number of sentences to
extract varied from two to four, depending on the
length of a text. The age of subjects varied from 18
to 45. We used 75 texts from three different cate-
gories (25 for each category); column, editorial and
news report. Texts were of about the same size in
terms of character counts and the number of para-
graphs, and were selected randomly from articles
that appeared in a Japanese financial daily (Nihon-
Keizai-Shimbun-Sha, 1995). There were, on aver-
age, 19.98 sentences per text.
3For a set S of data with k classes, its Gini index is given
as: Gini(S) = 1?Pki p2i , where pi denotes the probability of
observing class i in S.
Table 3: Test Data. N denotes the total number of
sentences in the test data. K ? n means that a wis
(positive) sentence gets at least n votes.
K N positive negative
? 1 1424 707 717
? 2 1424 392 1032
? 3 1424 236 1188
? 4 1424 150 1274
? 5 1424 72 1352
The kappa agreement among subjects was
0.25. The result is in a way consistent with
Salton et al (1999), who report a low inter-subject
agreement on paragraph extracts from encyclope-
dias and also with Gong and Liu (2001) on a sen-
tence selection task in the cable news domain. While
there are some work (Marcu, 1999; Jing et al, 1998)
which do report high agreement rates, their success
may be attributed to particularities of texts used, as
suggested by Jing et al (1998). Thus, the question
of whether it is possible to establish an ideal sum-
mary based on agreement is far from settled, if ever.
In the face of this, it would be interesting and per-
haps more fruitful to explore another view on sum-
mary, that the variability of a summary is the norm
rather than the exception.
In the experiments that follow, we decided not
to rely on a particular level of inter-coder agree-
ment to determine whether or not a given sentence
is wis. Instead, we used agreement threshold to dis-
tinguish between wis and non-wis sentences: for a
given threshold K, a sentence is considered wis (or
positive) if it has at least K votes in favor of its in-
clusion in a summary, and non-wis (negative) if not.
Thus if a sentence is labeled as positive at K ? 1,
it means that there are one or more judges taking
that sentence as wis. We examined K from 1 to 5.
(On average, seven people are assigned to one arti-
cle. However, one would rarely see all of them unan-
imously agree on their judgments.)
Table 3 shows how many positive/negative in-
stances one would get at a given agreement thresh-
old. At K ? 1, out of 1424 instances, i.e., sen-
tences, 707 of them are marked positive and 717 are
marked negative, so positive and negative instances
are evenly spread across the data. On the other hand,
at K ? 5, there are only 72 positive instances. This
means that there is less than one occurrence of wis
case per article.
In the experiments below, each probabilistic ren-
dering of the DTs, namely, C4.5, MDL-DT, and
SSDT is trained on the corpus, and tested with
and without the diversity extension (Find-Diversity).
When used without the diversity component, each
ProbDT works on a test article in its entirety, pro-
ducing the ranked list of sentences. A summary
with compression rate ? is obtained by selecting
top ? percent of the list. When coupled with Find-
Diversity, on the other hand, each ProbDT is set
to work on each cluster discovered by the diversity
component, producing multiple lists of sentences,
each corresponding to one of the clusters identified.
A summary is formed by collecting top ranking sen-
tences from each list.
Evaluation was done by 10-fold cross vali-
dation. For the purpose of comparison, we
also ran the diversity based model as given in
Nomoto and Matsumoto (2001c) and a tfidf based
ranking model (Zechner, 1996) (call it Z model),
which simply ranks sentences according to the tfidf
score and selects those which rank highest. Recall
that the diversity based model (DBS) (Nomoto and
Matsumoto, 2001c) consists in Find-Diversity and
the ranking model by Zechner (1996), which they
call Reduce-Redundancy.
7 Results and Discussion
Tables 4-8 show performance of each ProbDT and
its combination with the diversity (clustering) com-
ponent. It also shows performance of Z model and
DBS. In the tables, the slashed ?V? after the name
of a classifier indicates that the relevant classifier is
diversity-enabled, meaning that it is coupled with
the diversity extension. Notice that each decision
tree here is a ProbDT and should not be confused
with its non-probabilistic counterpart. Also worth
noting is that DBS is in fact Z/V, that is, diversity-
enabled Z model.
Returning to the tables, we find that for most
of the times, the diversity component has clear ef-
fects on ProbDTs, significantly improving their per-
formance. All the figures are in F-measure, i.e.,
F = 2?P?RP+R . In fact this happens regardless of a par-
ticular choice of ranking model, as performance of
Z is also boosted with the diversity component. Not
surprisingly, effects of supervised learning are also
evident: diversity-enabled ProbDTs generally out-
perform DBS (Z/V) by a large margin. What is sur-
prising, moreover, is that diversity-enabled ProbDTs
are superior in performance to their non-diversity
counterparts (with a notable exception for SSDT at
K ? 1), which suggests that selecting marginal sen-
tences is an important part of generating a summary.
Another observation about the results is that as
one goes along with a larger K, differences in per-
formance among the systems become ever smaller:
at K ? 5, Z performs comparably to C4.5, MDL,
and SSDT either with or without the diversity com-
ponent. The decline of performance of the DTs may
be caused by either the absence of recurring patterns
in data with a higher K or simply the paucity of
positive instances. At the moment, we do not know
which is the case here.
It is curious to note, moreover, that MDL-DT is
not performing as well as C4.5 and SSDT at K ? 1,
K ? 2, and K ? 3. The reason may well have
to do with the general properties of MDL-DT. Re-
call that MDL-DT is designed to produce as small
a decision tree as possible. Therefore, the resulting
tree would have a very small number of nodes cov-
ering the entire data space. Consider, for instance,
a hypothetical data space in Figure 3. Assume that
MDL-DT bisects the space into region A and B, pro-
ducing a two-node decision tree. The problem with
the tree is, of course, that point x and y in region B
will be assigned to the same probability under the
probabilistic tree model, despite the fact that point x
is very close to region A and point y is far out. This
problem could happen with C4.5, but in MDL-DT,
which covers a large space with a few nodes, points
in a region could be far apart, making the problem
more acute. Thus the poor performance of MDL-DT
may be attributable to its extensive use of pruning.
8 Conclusion
As a way of exploiting human biases towards an in-
creased performance of the summarizer, we have ex-
plored approaches to embedding supervised learn-
ing within a general unsupervised framework. In the
A
y
B
x
Figure 3: Hypothetical Data Space
paper, we focused on the use of decision tree as a
plug-in learner. We have shown empirically that the
idea works for a number of decision trees, including
C4.5, MDL-DT and SSDT. Coupled with the learn-
ing component, the unsupervised summarizer based
on clustering significantly improved its performance
on the corpus of human created summaries. More
importantly, we found that supervised learners per-
form better when coupled with the clustering than
when working alone. We argued that that has to do
with the high variation in human created summaries:
the clustering component forces a decision tree to
pay more attention to sentences marginally relevant
to the main thread of the text.
While ProbDTs appear to work well with rank-
ing, it is also possible to take a different approach:
for instance, we may use some distance metric in in-
stead of probability to distinguish among sentences.
It would be interesting to invoke the notion like pro-
totype modeler (Kalton et al, 2001) and see how it
might fare when used as a ranking model.
Moreover, it may be worthwhile to explore
some non-clustering approaches to representing
the diversity of contents of a text, such as
Gong and Liu (2001)?s summarizer 1 (GLS1, for
short), where a sentence is selected on the basis of
its similarity to the text it belongs to, but which ex-
cludes terms that appear in previously selected sen-
tences. While our preliminary study indicates that
GLS1 produces performance comparable and even
superior to DBS on some tasks in the document re-
trieval domain, we have no results available at the
moment on the efficacy of combining GLS1 and
ProbDT on sentence extraction tasks.
Finally, we note that the test corpus used for
Table 4: Performance at varying compression rates for K ? 1. MDL-DT denotes a summarizer based
on C4.5 with the MDL extension. DBS (=Z/V) denotes the diversity based summarizer. Z represents the
Z-model summarizer. Performance figures are in F-measure. ?V? indicates that the relevant classifier is
diversity-enabled. Note that DBS =Z/V.
cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z
0.2 0.371 0.459 0.353 0.418 0.437 0.454 0.429 0.231
0.3 0.478 0.507 0.453 0.491 0.527 0.517 0.491 0.340
0.4 0.549 0.554 0.535 0.545 0.605 0.553 0.529 0.435
0.5 0.614 0.600 0.585 0.593 0.639 0.606 0.582 0.510
Table 5: K ? 2
cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z
0.2 0.381 0.441 0.343 0.391 0.395 0.412 0.386 0.216
0.3 0.420 0.441 0.366 0.418 0.404 0.431 0.421 0.290
0.4 0.434 0.444 0.398 0.430 0.415 0.444 0.444 0.344
0.5 0.427 0.447 0.409 0.437 0.423 0.439 0.443 0.381
Table 6: K ? 3
cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z
0.2 0.320 0.354 0.297 0.345 0.328 0.330 0.314 0.314
0.3 0.300 0.371 0.278 0.350 0.321 0.338 0.342 0.349
0.4 0.297 0.357 0.298 0.348 0.325 0.340 0.339 0.337
0.5 0.297 0.337 0.301 0.329 0.307 0.327 0.322 0.322
Table 7: K ? 4
cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z
0.2 0.272 0.283 0.285 0.301 0.254 0.261 0.245 0.245
0.3 0.229 0.280 0.234 0.284 0.249 0.267 0.269 0.269
0.4 0.238 0.270 0.243 0.267 0.236 0.248 0.247 0.247
0.5 0.235 0.240 0.245 0.246 0.227 0.233 0.232 0.232
Table 8: K ? 5
cmp.rate C4.5 C4.5/V MDL-DT MDL-DT/V SSDT SSDT/V DBS Z
0.2 0.242 0.226 0.252 0.240 0.188 0.189 0.191 0.191
0.3 0.194 0.220 0.197 0.231 0.171 0.206 0.194 0.194
0.4 0.184 0.189 0.189 0.208 0.175 0.173 0.173 0.173
0.5 0.174 0.175 0.176 0.191 0.145 0.178 0.167 0.167
evaluation is somewhat artificial in the sense that
we elicit judgments from people on the summary-
worthiness of a particular sentence in the text. Per-
haps, we should look at naturally occurring ab-
stracts or extracts as a potential source for train-
ing/evaluation data for summarization research. Be-
sides being natural, they usually come in large num-
ber, which may alleviate some concern about the
lack of sufficient resources for training learning al-
gorithms in summarization.
References
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Interna-
tional ACM/SIGIR Conference on Research and De-
velopment, New Orleans. ACM-Press.
Takashi Ichikawa. 1990. Bunsho?ron-gaisetsu. Kyo?iku-
Shuppan, Tokyo.
Hongyan Jing, Regina Barzilay, Kathleen McKeown, and
Machael Elhadad. 1998. Summarization evaluation
methods: Experiments and analysis. In AAAI Sym-
posium on Intelligent Summarization, Stanford Uni-
vesisty, CA, March.
Annaka Kalton, Pat Langely, Kiri Wagstaff, and Jung-
soon Yoo. 2001. Generalized clustering, supervised
learning, and data assignment. In Proceedings of the
Seventh International Conference on Knowledge Dis-
covery and Data Mining (KDD2001), San Francisco,
August. ACM.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
the Fourteenth Annual International ACM/SIGIR Con-
ference on Research and Developmnet in Information
Retrieval, pages 68?73, Seattle.
Hang Li. 1998. A Probabilistic Approach to Lexical Se-
mantic Knowledge Acquistion and Structural Disam-
biguation. Ph.D. thesis, University of Tokyo, Tokyo.
Daniel Marcu. 1999. Discourse trees are good indicators
of importance in text. In Indejeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 123?136. The MIT Press.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, and
Yoshitaka Hirano. 1999. Japanese morphological
analysis system chasen version 2.0 manual. Technical
report, NAIST, Ikoma, April. NAIST-IS-TR99008.
Nihon-Keizai-Shimbun-Sha. 1995. Nihon keizai shim-
bun 95 nen cd-rom ban. CD-ROM. Tokyo, Nihon
Keizai Shimbun, Inc.
Tadashi Nomoto and Yuji Matsumoto. 2000. Comparing
the minimum description length principle and boosting
in the automatic analysis of discourse. In Proceedings
of the Seventeenth International Conference on Ma-
chine Learning, pages 687?694, Stanford University,
June-July. Morgan Kaufmann.
Tadashi Nomoto and Yuji Matsumoto. 2001a. The diver-
sity based approach to open-domain text summariza-
tion. Unpublished Manuscript.
Tadashi Nomoto and Yuji Matsumoto. 2001b. An exper-
imental comparison of supervised and unsupervised
approaches to text summarization. In Proceedings of
2001 IEEE International Conference on Data Mining,
pages 630?632, San Jose. IEEE Computer Society.
Tadashi Nomoto and Yuji Matsumoto. 2001c. A new
approach to unsupervised text summarization. In Pro-
ceedings of the 24th International ACM/SIGIR Confer-
ence on Research and Development in Informational
Retrieval, New Orleans, September. ACM.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Jorma Rissanen. 1997. Stochastic complexity in learn-
ing. Journal of Computer and System Sciences, 55:89?
95.
Gerald Salton, Amit Singhal, Mandara Mitra, and Chris
Buckley. 1999. Automatic text structuring and sum-
marization. In Inderjeet Mani and Mark T. Maybury,
editors, Advances in Automatic Text Summarization,
pages 342?355. The MIT Press. Reprint.
Haixun Wang and Philip Yu. 2001. SSDT: A scalable
subspace-splitting classifier for biased data. In Pro-
ceedings of 2001 IEEE International Conference on
Data Mining, pages 542?549, San Jose, December.
IEEE Computer Society.
Kenji Yamanishi. 1997. Data compression and learning.
Journal of Japanese Society for Artificial Intelligence,
12(2):204?215. in Japanese.
Klaus Zechner. 1996. Fast generation of abstracts from
general domain text corpora by extracting relevant sen-
tences. In Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 986?989,
Copenhagen.
Multi-Engine Machine Translation with Voted Language Model
Tadashi Nomoto
National Institute of Japanese Literature
1-16-10 Yutaka Shinagawa
Tokyo 142-8585 Japan
nomoto@acm.org
Abstract
The paper describes a particular approach to multi-
engine machine translation (MEMT), where we
make use of voted language models to selectively
combine translation outputs from multiple off-the-
shelf MT systems. Experiments are done using
large corpora from three distinct domains. The
study found that the use of voted language models
leads to an improved performance of MEMT sys-
tems.
1 Introduction
As the Internet grows, an increasing number of
commercial MT systems are getting on line ready
to serve anyone anywhere on the earth. An inter-
esting question we might ponder is whether it is not
possible to aggregate the vast number of MT sys-
tems available on the Internet into one super MT
which surpasses in performance any of those MTs
that comprise the system. And this is what we
will be concerned with in the paper, with somewhat
watered-down settings.
People in the speech community pursued the idea
of combining off-the-shelf ASRs (automatic speech
recognizers) into a super ASR for some time, and
found that the idea works (Fiscus, 1997; Schwenk
and Gauvain, 2000; Utsuro et al, 2003). In IR (in-
formation retrieval), we find some efforts going (un-
der the name of distributed IR or meta-search) to se-
lectively fuse outputs from multiple search engines
on the Internet (Callan et al, 2003). So it would be
curious to see whether we could do the same with
MTs.
Now back in machine translation, we do find
some work addressing such concern: Frederking
and Nirenburg (1994) develop a multi-engine MT
or MEMT architecture which operates by com-
bining outputs from three different engines based
on the knowledge it has about inner workings of
each of the component engines. Brown and Fred-
erking (1995) is a continuation of Frederking and
Nirenburg (1994) with an addition of a ngram-
based mechanism for a candidate selection. Nomoto
(2003), however, explores a different line of re-
search whose goal is to combine black box MTs us-
ing statistical confidence models. Similar efforts are
also found in Akiba et al (2002).
The present paper builds on the prior work by
Nomoto (2003). We start by reviewing his ap-
proach, and go on to demonstrate that it could be im-
proved by capitalizing on dependence of the MEMT
model there on language model. Throughout the
paper, we refer to commercial black box MT sys-
tems as OTS (off-the-shelf) systems, or more sim-
ply, OTSs.
2 Confidence Models
We take it here that the business of MEMT is about
choosing among translation outputs from multiple
MT systems, whether black box or not, for each in-
put text. Therefore the question we want to address
is, how do we go about choosing among MT outputs
so that we end up with a best one?
What we propose to do is to use some confidence
models for translations generated by OTSs, and let
them decide which one we should pick. We essen-
tially work along the lines of Nomoto (2003). We
review below some of the models proposed there,
together with some motivation behind them.
Confidence models he proposes come in two va-
rieties: Fluency based model (FLM) and Alignment
based model (ALM), which is actually an extension
of FLM. Now suppose we have an English sentence
e and its Japanese translation j generated by some
OTS. (One note here: throughout the paper we work
on English to Japanese translation.) FLM dictates
that the quality of j as a translation of e be deter-
mined by:
FLM(e, j) = logPl(j) (1)
Pl(j) is the probability of j under a particular lan-
guage model (LM) l.1 What FLM says is that the
quality of a translation essentially depends on its log
likelihood (or fluency) and has nothing to do with
what it is a translation of.
ALM extends FLM to include some information
on fidelity. That is, it pays some attention to how
faithful a translation is to its source text. ALM does
this by using alignment models from the statistical
machine translation literature (Brown et al, 1993).
Here is what ALM looks like.
ALM(e, j) = logPl(j)Q(e | j)
Q(e | j) is the probability estimated using IBM
Model 1. ALM takes into account the fluency of
a translation output (given by Pl(j)) and the degree
of association between e and j (given by Q(e | j)),
which are in fact two features generally agreed in
the MT literature to be most relevant for assessing
the quality of translations (White, 2001).
One problem with FLM and ALM is that they fail
to take into account the reliability of an OTS sys-
tem. As Nomoto (2003) argues, it is reasonable to
believe that some MT systems could inherently be
more prone to error and outputs they produce tend
to be of less quality than those from other systems,
no matter what the outputs? fluency or translation
probability may be. ALM and FLM work solely
on statistical information that can be gathered from
source and target sentences, dismissing any opera-
tional bias that an OTS might have on a particular
task.
Nomoto (2003) responds to the problem by intro-
ducing a particular regression model known as Sup-
port Vector regression (SVR), which enables him to
exploit bias in performance of OTSs. What SVR
is intended to do is to modify confidence scores
FLM and ALM produce for MT outputs in such a
way that they may more accurately reflect their in-
dependent evaluation involving human translations
or judgments. SVR is a multi-dimensional regres-
sor, and works pretty much like its enormously pop-
ular counterpart, Support Vector classification, ex-
cept that we are going to work with real numbers for
target values and construct the margin, using Vap-
nik?s ?-insensitive loss function (Scho?lkopf et al,
1998).
1Note that Pl(j) = P (l)
Qm
i P (wi | wi?2, wi?1, l) wherej = w1 ? ? ?wm. Assume a uniform prior for l.
SVR looks something like this.
h(~x) = ~w ? ~x+ b,
with input data ~x = (x1, . . . , xm) and the corre-
sponding weights ~w = (w1, . . . , wm). ?x ? y? de-
notes the inner product of x and y. ~x could be a set
of features associated with e and j. Parameters ~w
and b are something determined by SVR.
It is straightforward to extend the ALM and FLM
with SVR, which merely consists of plugging in ei-
ther model as an input variable in the regressor. This
would give us the following two SVR models with
m = 1.
Regressive FLM (rFLM)
h(FLM(e, j)) = w1 ? FLM(e, j) + b
Regressive ALM (rALM)
h(ALM(e, j)) = w1 ?ALM(e, j) + b
Notice that h(?) here is supposed to relate FLM or
ALM to some independent evaluation metric such
as BLEU (Papineni et al, 2002), not the log likeli-
hood of a translation.
With confidence models in place, define a MEMT
model ? by:
?(e, J, l) = arg maxj?J(?(e, j | l))
Here e represents a source sentence, J a set of trans-
lations for e generated by OTSs, and ? denotes some
confidence model under an LM l. Throughout the
rest of the paper, we let FLM? and ALM? denote
MEMT systems based on FLM and ALM, respec-
tively, and similarly for others.
3 Notes on Evaluation
We assume here that the MEMT works on a
sentence-by-sentence basis. That is, it takes as in-
put a source sentence, gets it translated by several
OTSs, and picks up the best among translations it
gets. Now a problem with using BLEU in this setup
is that translations often end up with zero because
model translations they refer to do not contain n-
grams of a particular length.2 This would make im-
possible a comparison and selection among possible
translations.
2In their validity study of BLEU, Reeder and White (2003)
finds that its correlation with human judgments increases with
the corpus size, and warns that to get a reliable score for BLEU,
one should run it on a corpus of at least 4,000 words. Also Tate
et al (2003) reports about some correlation between BLEU and
task based judgments.
One way out of this, Nomoto (2003) suggests,
is to back off to a somewhat imprecise yet robust
metric for evaluating translations, which he calls m-
precision.3 The idea of m-precision helps define
what an optimal MEMT should look like. Imagine
a system which operates by choosing, among can-
didates, a translation that gives a best m-precision.
We would reasonably expect the system to outper-
form any of its component OTSs. Indeed Nomoto
(2003) demonstrates empirically that it is the case.
Moreover, since rFLM? and rALM? work on a sen-
tence, not on a block of them, what h(?) relates to is
not BLEU, but m-precision.
Hogan and Frederking (1998) introduces a new
kind of yardstick for measuring the effectiveness
of MEMT systems. The rationale for this is that
it is often the case that the efficacy of MEMT sys-
tems does not translate into performance of outputs
that they generate. We recall that with BLEU, one
measures performance of translations, not how of-
ten a given MEMT system picks the best translation
among candidates. The problem is, even if a MEMT
is right about its choices more often than a best com-
ponent engine, BLEU may not show it. This happens
because a best translation may not always get a high
score in BLEU. Indeed, differences in BLEU among
candidate translations could be very small.
Now what Hogan and Frederking (1998) suggest
is the following.
d(?m) =
?N
i ?(?m(e),max{?e1 ? ? ??eM })
N
where ?(i, j) is the Kronecker delta function, which
gives 1 if i = j and 0 otherwise. Here ?m rep-
resents some MEMT system, ?m(e) denotes a par-
ticular translation ?m chooses for sentence e, i.e.,
?m(e) = ?(e, J, l). ?e1 . . . ?eM ? J denotes a set
of candidate translations. max here gives a transla-
tion with the highest score in m-precision. N is the
number of source sentences. ?(?) says that you get
1 if a particular translation the MEMT chooses for a
given sentences happens to rank highest among can-
3For a reference translation r and a machine-generated
translation t, m-precision is defined as:
m-precision =
NX
i
P
v?Sit C(v, r)P
v?Sit C(v, t)
,
which is nothing more than Papineni et al (2002)?s modified
n-gram precision applied to a pair of a single reference and the
associated translation. Sit here denotes a set of i-grams in t,
v an i-gram. C(v, t) indicates the count of v in t. Nomoto
(2003) finds that m-precision strongly correlates with BLEU,
which justifies the use of m-precision as a replacement of BLEU
at the sentence level.
didates. d(?m) gives the average ratio of the times
?m hits a right translation. Let us call d(?m) HF
accuracy (HFA) for the rest of the paper.
4 LM perplexity and MEMT performance
Now the question we are interested in asking is
whether the choice of LM really matters. That is,
does a particular choice of LM gives a better per-
forming FLM? or ALM? than something else, and
if it does, do we have a systematic way of choosing
one LM over another?
Let us start with the first question. As a way of
shedding some light on the issue, we ran FLM? and
ALM? using a variety of LMs, derived from various
domains with varying amount of training data. We
worked with 24 LMs from various genres, with vo-
cabulary of size ranging from somewhere near 10K
to 20K in words (see below and also Appendix A
for details on train sets). LMs here are trigram based
and created using an open source speech recognition
tool called JULIUS.4
Now train data for LMs are collected from five
corpora, which we refer to as CPC, EJP, PAT, LIT,
NIKMAI for the sake of convenience. CPC is a
huge set of semi-automatically aligned pairs of En-
glish and Japanese texts from a Japanese news pa-
per which contains as many as 150,000 sentences
(Utiyama and Isahara, 2002), EJP represents a rel-
atively small parallel corpus of English/Japanese
phrases (totaling 15,187) for letter writing in busi-
ness (Takubo and Hashimoto, 1999), PAT is a bilin-
gual corpus of 336,971 abstracts from Japanese
patents filed in 1995, with associated translations
in English (a.k.a NTCIR-3 PATENT).5 LIT contains
100 Japanese literary works from the early 20th cen-
tury, and NIKMAI 1,536,191 sentences compiled
from several Japanese news paper sources. Both
LIT and NIKMAI are monolingual.
Fig.1 gives a plot of HF accuracy by perplexity
for FLM??s on test sets pulled out of PAT, EJP and
CPC.6 Each dot there represents an FLM? with a
particular LM plugged into it. The HFA of each
FLM? in Fig.1 represents a 10-fold cross validated
HFA score, namely an HFA averaged over evenly-
4http://julius.sourceforge.jp
5A bibliographic note. NTCIR-3 PATENT: NII Test Col-
lection for Information Retrieval Systems distributed through
National Institute of Informatics (www.nii.ac.jp).
6A test set from EJP and CPC each contains 7,500 bilingual
sentences, that from PAT contains 4,600 bilingual abstracts (ap-
proximately 9,200 sentences). None of them overlaps with the
remaining part of the corresponding data set. Relevant LMs are
built on Japanese data drawn from the data sets. We took care
not to train LMs on test sets. (See Section 6 for further details.)
?
?
?
??
?
?
?
??
?
?
?
? ??
?
? ?
? ? ?
?
LM Perplexity
HF A
ccur
acy
500 1000 1500 20000
.55
0.65
0.75
PAT
??
?
? ?? ?
?
?
?
?
?
?
??
?
?
?
?
??
?
LM Perplexity
HF A
ccur
acy
500 1000 1500
0.38
0.40
0.42
0.44 CPC
?
?? ?? ?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
LM Perplexity
HF A
ccur
acy
500 1000 1500 20000
.28
0.32
0.36
0.40 EJP
Figure 1: HF accuracy-by-perplexity plots for FLM? with four OTSs, Ai, Lo, At, Ib, on PAT (left), CPC
(center) and EJP (right). Dots represent FLM??s with various LMs .
split 10 blocks of a test set. The perplexity is that
of Pl(j) averaged over blocks, with a particular LM
plugged in for l (see Equation 1).
We can see there an apparent tendency for an LM
with lower perplexity to give rise to an FLM? with
higher HFA, indicating that the choice of LM does
indeed influence the performance of FLM?. Which
is somewhat surprising given that the perplexity of
a machine generated translation should be indepen-
dent of how similar it is to a model translation,
which dictates the HFA.7
Now let us turn to the question of whether there
is any systematic way of choosing an LM so that
it gives rise to a FLM? with high HFA. Since we
are working with multiple OTS systems here, we
get multiple outputs for a source text. Our idea
is to let them vote for an LM to plug into FLM?
or for that matter, any other forms of MEMT dis-
cussed earlier. Note that we could take an alternate
approach of letting a model (or human) translation
(associated with a source text) pick an LM by alone.
An obvious problem with this approach, however,
is that a mandatory reference to model translations
would compromise the robustness of the approach.
We would want the LM to work for MEMT regard-
less of whether model translations are available. So
our concern here is more with choosing an LM in
the absence of model translations, to which we will
return below.
5 Voting Language Model
We consider here a simple voting scheme a` la
ROVER (Fiscus, 1997; Schwenk and Gauvain,
2000; Utsuro et al, 2003), which works by picking
7Recall that the HFA does not represent the confidence score
such as one given by FLM (Equation 1), but the average ratio
of the times that an MEMT based on FLM picks a translation
with the best m-precision.
Table 1: A MEMT algorithm implementing V-by-
M. S represents a set of OTS systems, L a set of
language models. ? is some confidence model such
(r)FLM or (r)ALM. V-by-M chooses a most-voted-
for LM among those in L, given the set J of trans-
lations for e.
MEMT(e,S,L)
begin
J = {j | j is a translation of e generated by s ? S.}
l = V-by-M(J, L)
jk = arg maxj?J(?(e, j | l))
return jk
end
up an LM voted for by the majority. More specif-
ically, for each output translation for a given input,
we first pick up an LM which gives it the smallest
perplexity, and out of those LMs, one picked by the
majority of translations will be plugged into MEMT.
We call the selection scheme voting-by-majority or
simply V-by-M. The V-by-M scheme is motivated
by the results in Fig.1, where perplexity is found to
be a reasonably good predictor of HFA.
Formally, we could put the V-by-M scheme as
follows. For each of the translation outputs je1 . . . jen
associated with a given input sentence e, we want to
find some LM M from a set L of LMs such that:
Mi = arg minm?LPP (jei | m),
where PP (j | m) is the perplexity of j under m.
Now assume M1 . . .Mn are such LMs for je1 . . . jen.
Then we pick up an M with the largest frequency
and plug it into ? such as FLM.8
Suppose, for instance, that Ma, Mb, Ma and Mc
are lowest perplexity LMs found for translations
je1 ,je2 ,je3 and je4 , respectively. Then we choose Ma
as an LM most voted for, because it gets two votes
from je1 and je3 , meaning that Ma is nominated as
an LM with lowest perplexity by je1 and je3 , while
Mb and Mc each collect only one vote. In case of
ties, we randomly choose one of the LMs with the
largest count of votes.
6 Experiment Setup and Procedure
Let us describe the setup of experiments we have
conducted. The goal here is to learn how the V-
by-M affects the overall MEMT performance. For
test sets, we carry over those from the perplexity
experiments (see Footnote 6, Section 4), which are
derived from CPC, EJP, and PAT. (Call them tCPC,
tEJP, and tPAT hereafter.)
In experiments, we begin by splitting a test set
into equal-sized blocks, each containing 500 sen-
tences for tEJP and tCPC, and 100 abstracts (ap-
proximately 200 sentences) for tPAT.9 We had the
total of 15 blocks for tCPC and tEJP, and 46 blocks
for tPAT. We leave one for evaluation and use the
rest for training alignment models, i.e., Q(e | j),
SV regressors and some inside-data LMs. (Again
we took care not to inadvertently train LMs on test
sets.) We send a test block to OTSs Ai, Lo, At, and
Ib, for translation and combine their outputs using
the V-by-M scheme, which may or may not be cou-
pled with regression SVMs. Recall that the MEMT
operates on a sentence by sentence basis. So what
happens here is that for each of the sentences in a
block, the MEMT works the four MT systems to
get translations and picks one that produces the best
score under ?.
We evaluate the MEMT performance by run-
ning HFA and BLEU on MEMT selected translations
block by block,10 and giving average performance
over the blocks. Table 1 provides algorithmic de-
tails on how the MEMT actually operates.
8It is worth noting that the voted language model readily
lends itself to a mixture model: P (j) =Pm?M ?mP (j | m)
where ?m = 1 if m is most voted for and 0 otherwise.
9tCPC had the average of 15,478 words per block, whereas
tEJP had about 11,964 words on the average in each block.
With tPAT, however, the average per block word length grew
to 16,150.
10We evaluate performance by block, because of some re-
ports in the MT literature that warn that BLEU behaves errati-
cally on a small set of sentences (Reeder and White, 2003). See
also Section 3 and Footnote 2 for the relevant discussion.
Table 2: HF accuracy of MEMT models with V-by-
M.
Model tCPC tEJP tPAT avg.
rFLM? 0.4230 0.4510 0.8066 0.5602
rALM? 0.4194 0.4346 0.8093 0.5544
FLM? 0.4277 0.4452 0.7342 0.5357
ALM? 0.4453 0.4485 0.7702 0.5547
Table 3: HF accuracy of MEMT models with ran-
domly chosen LMs. Note how FLM? and ALM?
drop in performance.
Model tCPC tEJP tPAT avg.
rFLM? 0.4207 0.4186 0.8011 0.5468
rALM? 0.4194 0.4321 0.8095 0.5537
FLM? 0.4126 0.3520 0.6350 0.4665
ALM? 0.4362 0.3597 0.6878 0.4946
7 Results and Discussion
Now let us see what we found from the experiments.
We ran the MEMT on a test set with (r)FLM or
(r)ALM embedded in it. Recall that our goal here
is to find how the V-by-M affects performance of
MEMT on tCPC, tEJP, and tPAT.
First, we look at whether the V-by-M affects in
any way, the HFA of the MEMT, and if it does, then
how much. Table 2 and Table 3 give summaries of
results on HFA versus V-by-M. Table 2 shows how
things are with V-by-M on, and Table 3 shows what
happens to HFA when we turn off V-by-M, that is,
when we randomly choose an LM from the same set
that the V-by-M chooses from. The results indicate
a clear drop in performance of FLM? and ALM?
when one chooses an LM randomly.11
Curiously, however, rFLM? and rALM? are af-
fected less. They remain roughly at the same level
of HFA over Table 2 and Table 3. What this means
11Another interesting question to ask at this point is, how
does one huge LM trained across domains compare to the V-
by-M here? By definition of perplexity, the increase in size of
the training data leads to an increase in perplexity of the LM.
So if general observations in Fig.1 hold, then we would expect
the ?one-huge-LM? approach to perform poorly compared to
the V-by-M, which is indeed demonstrated by the following
results. HFLM? below denotes a FLM? based on a composite
LM trained over CPC, LIT, PAT, NIKMAI, and EJP. The testing
procedure is same as that described in Sec.6
Model tCPC tEJP tPAT avg.
HFLM? (HFA) 0.4182 0.4081 0.6927 0.5063
HFLM? (BLEU) 0.1710 0.2619 0.1874 0.2067
Table 4: Performance in BLEU of MEMT models
with V-by-M.
Model tCPC tEJP tPAT avg.
rFLM? 0.1743 0.2861 0.1954 0.2186
rALM? 0.1735 0.2869 0.1954 0.2186
FLM? 0.1736 0.2677 0.1907 0.2107
ALM? 0.1763 0.2622 0.1934 0.2106
Table 5: Performance in BLEU of MEMT models
with randomly chosen LMs.
Model tCPC tEJP tPAT avg.
rFLM? 0.1738 0.2717 0.1950 0.2135
rALM? 0.1735 0.2863 0.1954 0.2184
FLM? 0.1710 0.2301 0.1827 0.1946
ALM? 0.1745 0.2286 0.1871 0.1967
is that there is some discrepancy in the effective-
ness of V-by-M between the fluency based and re-
gression based models. We have no explanation for
the cause of the discrepancy at this time, though we
may suspect that in learning, as long as there is some
pattern to exploit in m-precision and the probability
estimates of test sentences, how accurate those esti-
mates are may not matter much.
Table 4 and Table 5 give results in BLEU.12 The
results tend to replicate what we found with HFA.
rFLM? and rALM? keep the edge over FLM?
and ALM? whether or not V-by-M is brought into
action. The differences in performance between
rFLM? and rALM? with or without the V-by-M
scheme are rather negligible. However, if we turn
to FLM? and ALM?, the effects of the V-by-M are
clearly visible. FLM? scores 0.2107 when coupled
with the V-by-M. However, when disengaged, the
score slips to 0.1946. The same holds for ALM?.
Table 6: HF accuracy of OTS systems
Model tCPC tEJP tPAT avg.
Ai 0.2363 0.4319 0.0921 0.2534
Lo 0.1718 0.2124 0.0504 0.1449
At 0.4211 0.1681 0.8037 0.4643
Ib 0.1707 0.1876 0.0537 0.1373
OPM 1.0000 1.0000 1.0000 1.0000
12The measurements in BLEU here take into account up to
trigrams.
Table 7: Performance of OTS systems in BLEU.
Model tCPC tEJP tPAT avg.
Ai 0.1495 0.2874 0.1385 0.1918
Lo 0.1440 0.1711 0.1402 0.1518
At 0.1738 0.1518 0.1959 0.1738
Ib 0.1385 0.1589 0.1409 0.1461
OPM 0.2111 0.3308 0.1995 0.2471
Leaving the issue of MEMT models momentar-
ily, let us see how the OTS systems Ai, Lo, At, and
Ib are doing on tCPC, tEJP, and tPAT. Note that the
whole business of MEMT would collapse if it slips
behind any of the OTS systems that compose it.
Table 6 and Table 7 show performance of the
four OTS systems plus OPM, by HFA and by BLEU.
OPM here denotes an oracle MEMT which operates
by choosing in hindsight a translation that gives the
best score in m-precision, among those produced
by OTSs. It serves as a practical upper bound for
MEMT while OTSs serve as baselines.
First, let us look at Table 6 and compare it to Ta-
ble 2. A good news is that most of the OTS sys-
tems do not even come close to the MEMT mod-
els. At, a best performing OTS system, gets 0.4643
on the average, which is about 20% less than that
scored by rFLM?. Turning to BLEU, we find again
in Table 7 that a best performing system among the
OTSs, i.e., Ai, is outperformed by FLM?, ALM?
and all their varieties (Table 4). Also something of
note here is that on tPAT, (r)FLM? and (r)ALM?
in Table 4, which operate by the V-by-M scheme,
score somewhere from 0.1907 to 0.1954 in BLEU,
coming close to OPM, which scores 0.1995 on tPAT
(Table 7).
It is interesting to note, incidentally, that there is
some discrepancy between BLEU and HFA in per-
formance of the OTSs: A top performing OTS in
Table 6, namely At, achieves the average HFA of
0.4643, but scores only 0.1738 for BLEU (Table 7),
which is worse than what Ai gets. Apparently,
high HFA does not always mean a high BLEU score.
Why? The reason is that a best MT output need
not mark a high BLEU score. Notice that ?best? here
means the best among translations by the OTSs. It
could happen that a poor translation still gets chosen
as best, because other translations are far worse.
To return to the discussion of (r)FLM? and
(r)ALM?, an obvious fact about their behavior is
that regressor based systems rFLM? and rALM?,
whether V-by-M enabled or not, surpass in per-
formance their less sophisticated counterparts (see
Table 8: HF accuracy of MEMTs with perturbed SV
regressor in the V-by-M scheme.
Model tCPC tEJP tPAT avg.
rFLM? 0.4230 0.4353 0.6712 0.5098
rALM? 0.4195 0.4302 0.5582 0.4693
FLM? 0.4277 0.4452 0.7342 0.5357
ALM? 0.4453 0.4485 0.7702 0.5547
Table 9: Performance in BLEU of MEMTs with per-
turbed SV regressor in the V-by-M scheme.
Model tCPC tEJP tPAT avg.
rFLM? 0.1743 0.2823 0.1835 0.2134
rALM? 0.1736 0.2843 0.1696 0.2092
FLM? 0.1736 0.2677 0.1907 0.2107
ALM? 0.1763 0.2622 0.1934 0.2106
Table 2,4 and also Table 3,5). Regression allows
the MEMT models to correct themselves for some
domain-specific bias of the OTS systems. But the
downside of using regression to capitalize on their
bias is that you may need to be careful about data
you train a regressor on.
Here is what we mean. We ran experiments using
SVM regressors trained on a set of data randomly
sampled from tCPC, tEJP, and tPAT. (In contrast,
rFLM? and rALM? in earlier experiments had a re-
gressor trained separately on each data set.) They
all operated in the V-by-M mode. The results are
shown in Table 8 and Table 9. What we find there
is that with regressors trained on perturbed data,
both rFLM? and rALM? are not performing as well
as before; in fact they even fall behind FLM? and
ALM? in HFA and their performance in BLEU turns
out to be just about as good as FLM? and ALM?.
So regression may backfire when trained on wrong
data.
8 Conclusion
Let us summarize what we have done and learned
from the work. We started with a finding that the
choice of language model could affect performance
of MEMT models of which it is part. The V-by-M
was introduced as a way of responding to the prob-
lem of how to choose among LMs so that we get
the best MEMT. We have shown that the V-by-M
scheme is indeed up to the task, predicting a right
LM most of the time. Also worth mentioning is that
the MEMT models here, when coupled with V-by-
M, are all found to surpass component OTS systems
by a respectable margin (cf., Tables 4, 7 for BLEU,
2, 6 for HFA).
Regressive MEMTs such as rFLM? and rALM?,
are found to be not affected as much by the choice
of LM as their non-regressive counterparts. We sus-
pect this happens because they have access to ex-
tra information on the quality of translation derived
from human judgments or translations, which may
cloud effects of LMs on them. But we also pointed
out that regressive models work well only when they
are trained on right data; if you train them across
different sources of varying genres, they could fail.
An interesting question that remains to be ad-
dressed is how we might deal with translations from
a novel domain. One possible approach would be
to use a dynamic language model which adapts it-
self for a new domain by re-training itself on data
sampled from the Web (Berger and Miller, 1998).
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro
Sumita. 2002. Using language and translation
models to select the best among outputs from
multiple mt systems. In Proceedings of the 19th
International Conference on Computational Lin-
guistics (COLING 2002), Taipei.
Adam Berger and Robert Miller. 1998. Just-in-
time language modelling. In Proceedings of
ICASSP98.
Ralf Brown and Robert Frederking. 1995. Ap-
plying statistical English language modelling to
symbolic machine translation. In Proceedings of
the Sixth International Conference on Theoretical
and Methodological Issues in Machine Transla-
tion (TMI?95), pages 221?239, Leuven, Belgium,
July.
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J.Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19(2):263?311, June.
Jamie Callan, Fabio Crestani, Henrik Nottelmann,
Pietro Pala, and Xia Mang Shou. 2003. Re-
source selection and data fusion in multimedia
distributed digital libaries. In Proceedings of the
26th Annual International ACM/SIGIR Confer-
ence on Research and Development in Informa-
tion Retrieval. ACM.
Jonathan G. Fiscus. 1997. A post-processing sys-
tem to yield reduced word error rates: Recogniser
output voting error reduction (ROVER). In Proc.
IEEE ASRU Workshop, pages 347?352, Santa
Barbara.
Rober Frederking and Sergei Nirenburg. 1994.
Three heads are better than one. In Proceed-
ings of the Fourth Conference on Applied Natural
Language Processing, Stuttgart.
Christopher Hogan and Robert E. Frederking. 1998.
An evaluation of the multi-engine MT architec-
ture. In Proceedings of the Third Conference of
the Association for Machine Translation in the
Americas (AMTA ?98), pages 113?123, Berlin,
October. Springer-Verlag. Lecture Notes in Ar-
tificial Intelligence 1529.
Tadashi Nomoto. 2003. Predictive models of per-
formance in multi-engine machine translation. In
Proceedings of Machine Translation Summit IX,
New Orleans, September. IAMT.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei ing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages
311?318, July.
Florence Reeder and John White. 2003. Granular-
ity in MT evaluation. In MT Summit Workshop on
Machine Translation Evaluation: Towards Sys-
tematizing MT Evaluation, pages 37?42, New Or-
leans. AMTA.
Bernhard Scho?lkopf, Chirstpher J. C. Burges, and
Alexander J. Smola, editors. 1998. Advances in
Kernel Methods: Support Vector Learning. The
MIT Press.
Holger Schwenk and Jean-Luc Gauvain. 2000.
Combining multiple speech recognizers using
voting and language model information. In Pro-
ceedings of the IEEE International Conference
on Speech and Language Proceesing (ICSLP),
volume 2, pages 915?918, Beijin, October. IEEE.
Kohei Takubo and Mitsunori Hashimoto. 1999.
A Dictionary of English Business Letter Ex-
pressions. Published in CDROM. Nihon Keizai
Shinbun Sha.
Calandra Tate, Sooyon Lee, and Clare R. Voss.
2003. Task-based MT evaluation: Tackling soft-
ware, experimental design, & statistical models.
In MT Summit Workshop on Machine Translation
Evaluation: Towards Systematizing MT Evalua-
tion, pages 43?50. AMTA.
Masao Utiyama and Hitoshi Isahara. 2002. Align-
ment of japanese-english news articles and sen-
tences. In IPSJ Proceedings 2002-NL-151, pages
15?22. In Japanese.
Takehito Utsuro, Yasuhiro Kodama, Tomohiro
Watanabe, Hiromitsu Nishizaki, and Seiichi Nak-
agawa. 2003. Confidence of agreement among
multiple LVCSR models and model combination
by svm. In Proceedings of the 28th IEEE Interna-
Table 10: Language models in MEMT
Models Train Size Voc. Genre
paj98j102t 1,020K 20K PAT
paj96j5t 50K 20K PAT
paj96j3t 30K 20K PAT
paj98j5t 50K 20K PAT
paj96j102t 1,020K 20K PAT
paj98j3t 30K 20K PAT
paj98j1t 10K 14K PAT
paj1t 10K 14K PAT
paj98j5k 5K 10K PAT
paj5k 5K 10K PAT
lit8t 80K 20K LIT
lit5t 50K 20K LIT
lit3t 30K 20K LIT
lit5k 5K 13K LIT
lit1t 10K 13K LIT
nikmai154t 1,540K 20K NWS
nikmai5t 50K 20K NWS
crl14t 40K 20K NWS
crl5t 50K 20K NWS
nikmai3t 30K 20K NWS
nikmai1t 10K 17K NWS
nikmai5k 5K 12K NWS
crl3t 30K 20K NWS
ejp8k 8K 8K BIZ
tional Conference on Acoustics, Speech and Sig-
nal Processing, pages 16?19. IEEE, April.
John White. 2001. Predicting intelligibility from fi-
delity in MT evaluation. In Proceedings of the
workshop ?MT Evaluation: Who did What to
Whom?, pages 35?37.
Appendix
A Language Models
Table 10 lists language models used in the voting
based MEMTs discussed in the paper. They are
more or less arbitrarily built from parts of the co-
pora CPC, EJP, NIKMAI, EJP, and LIT. ?Train size?
indicates the number of sentences, given in kilo,
in a corpus on which a particular model is trained.
Under ?Voc(abulary)? is listed the number of type
words for each LM (also given in kilo). Notice
the difference in the way the train set and vocabu-
lary are measured. ?Genre? indicates the genre of
a trainig data used for a given LM: PAT stands for
patents (from PAT), LIT literary texts (from LIT),
NWS news articles (from CPC and NIKMAI), and
BIZ business related texts (from EJP).
Proceedings of ACL-08: HLT, pages 299?307,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Generic Sentence Trimmer with CRFs
Tadashi Nomoto
National Institute of Japanese Literature
10-3, Midori Tachikawa
Tokyo, 190-0014, Japan
nomoto@acm.org
Abstract
The paper presents a novel sentence trimmer
in Japanese, which combines a non-statistical
yet generic tree generation model and Con-
ditional Random Fields (CRFs), to address
improving the grammaticality of compres-
sion while retaining its relevance. Experi-
ments found that the present approach out-
performs in grammaticality and in relevance
a dependency-centric approach (Oguro et al,
2000; Morooka et al, 2004; Yamagata et al,
2006; Fukutomi et al, 2007)? the only line of
work in prior literature (on Japanese compres-
sion) we are aware of that allows replication
and permits a direct comparison.
1 Introduction
For better or worse, much of prior work on sentence
compression (Riezler et al, 2003; McDonald, 2006;
Turner and Charniak, 2005) turned to a single cor-
pus developed by Knight and Marcu (2002) (K&M,
henceforth) for evaluating their approaches.
The K&M corpus is a moderately sized corpus
consisting of 1,087 pairs of sentence and compres-
sion, which account for about 2% of a Ziff-Davis
collection from which it was derived. Despite its
limited scale, prior work in sentence compression
relied heavily on this particular corpus for establish-
ing results (Turner and Charniak, 2005; McDonald,
2006; Clarke and Lapata, 2006; Galley and McKe-
own, 2007). It was not until recently that researchers
started to turn attention to an alternative approach
which does not require supervised data (Turner and
Charniak, 2005).
Our approach is broadly in line with prior work
(Jing, 2000; Dorr et al, 2003; Riezler et al, 2003;
Clarke and Lapata, 2006), in that we make use of
some form of syntactic knowledge to constrain com-
pressions we generate. What sets this work apart
from them, however, is a novel use we make of
Conditional Random Fields (CRFs) to select among
possible compressions (Lafferty et al, 2001; Sut-
ton and McCallum, 2006). An obvious benefit of
using CRFs for sentence compression is that the
model provides a general (and principled) proba-
bilistic framework which permits information from
various sources to be integrated towards compress-
ing sentence, a property K&M do not share.
Nonetheless, there is some cost that comes with
the straightforward use of CRFs as a discriminative
classifier in sentence compression; its outputs are
often ungrammatical and it allows no control over
the length of compression they generates (Nomoto,
2007). We tackle the issues by harnessing CRFs
with what we might call dependency truncation,
whose goal is to restrict CRFs to working with can-
didates that conform to the grammar.
Thus, unlike McDonald (2006), Clarke and Lap-
ata (2006) and Cohn and Lapata (2007), we do not
insist on finding a globally optimal solution in the
space of 2n possible compressions for an n word
long sentence. Rather we insist on finding a most
plausible compression among those that are explic-
itly warranted by the grammar.
Later in the paper, we will introduce an approach
called the ?Dependency Path Model? (DPM) from
the previous literature (Section 4), which purports to
provide a robust framework for sentence compres-
299
sion in Japanese. We will look at how the present
approach compares with that of DPM in Section 6.
2 A Sentence Trimmer with CRFs
Our idea on how to make CRFs comply with gram-
mar is quite simple: we focus on only those la-
bel sequences that are associated with grammati-
cally correct compressions, by making CRFs look
at only those that comply with some grammatical
constraints G, and ignore others, regardless of how
probable they are.1 But how do we find compres-
sions that are grammatical? To address the issue,
rather than resort to statistical generation models as
in the previous literature (Cohn and Lapata, 2007;
Galley and McKeown, 2007), we pursue a particular
rule-based approach we call a ?dependency trunca-
tion,? which as we will see, gives us a greater control
over the form that compression takes.
Let us denote a set of label assignments for S that
satisfy constraints, by G(S).2 We seek to solve the
following,
y? = arg max
y?G(S)
p(y|x;?). (2)
There would be a number of ways to go about the
problem. In the context of sentence compression, a
linear programming based approach such as Clarke
and Lapata (2006) is certainly one that deserves con-
sideration. In this paper, however, we will explore a
much simpler approach which does not require as
involved formulation as Clarke and Lapata (2006)
do.
We approach the problem extentionally, i.e.,
through generating sentences that are grammatical,
or that conform to whatever constraints there are.
1Assume as usual that CRFs take the form,
p(y|x) ?
exp
 
P
k,j ?jfj(yk, yk?1,x) +
P
i ?igi(xk, yk,x)
!
= exp[w?f(x,y)]
(1)
fj and gi are ?features? associated with edges and vertices, re-
spectively, and k ? C, where C denotes a set of cliques in CRFs.
?j and ?i are the weights for corresponding features. w and f
are vector representations of weights and features, respectively
(Tasker, 2004).
2Note that a sentence compression can be represented as an
array of binary labels, one of themmarking words to be retained
in compression and the other those to be dropped.
S
V
N P
N
V
NA D J
N P
N
V N
Figure 1: Syntactic structure in Japanese
Consider the following.
(3) Mushoku-no
unemployed
John
John
-ga
SBJ
takai
expensive
kuruma
car
-wo
ACC
kat-ta.
buy PAST
?John, who is unemployed, bought an
expensive car.?
whose grammatically legitimate compressions
would include:
(4) (a) John -ga takai kuruma -wo kat-ta.
?John bought an expensive car.?
(b) John -ga kuruma -wo kat-ta.
?John bought a car.?
(c) Mushoku-no John -ga kuruma -wo kat-ta.
?John, who is unemployed, bought a car.
(d) John -ga kat-ta.
?John bought.?
(e) Mushoku-no John -ga kat-ta.
?John, who is unemployed, bought.?
(f) Takai kuruma-wo kat-ta.
? Bought an expensive car.?
(g) Kuruma-wo kat-ta.
? Bought a car.?
(h) Kat-ta.
? Bought.?
This would give us G(S)={a, b, c, d, e, f, g, h}, for
the input 3. Whatever choice we make for compres-
sion among candidates in G(S), should be gram-
matical, since they all are. One linguistic feature
300
B S 2
B S 4
B S 5
B S 3
B S 1
N P V
S
Figure 2: Compressing an NP chunk
C
D
E
B
A
Figure 3: Trimming TDPs
of the Japanese language we need to take into ac-
count when generating compressions, is that the sen-
tence, which is free of word order and verb-final,
typically takes a left-branching structure as in Fig-
ure 1, consisting of an array of morphological units
called bunsetsu (BS, henceforth). A BS, which we
might regard as an inflected form (case marked in the
case of nouns) of verb, adjective, and noun, could
involve one or more independent linguistic elements
such as noun, case particle, but acts as a morpholog-
ical atom, in that it cannot be torn apart, or partially
deleted, without compromising the grammaticality.3
Noting that a Japanese sentence typically consists
of a sequence of case marked NPs and adjuncts, fol-
lowed by a main verb at the end (or what would
be called ?matrix verb? in linguistics), we seek to
compress each of the major chunks in the sentence,
leaving untouched the matrix verb, as its removal of-
ten leaves the sentence unintelligible. In particular,
starting with the leftmost BS in a major constituent,
3Example 3 could be broken into BSs: / Mushuku -no / John
-ga / takai / kuruma -wo / kat-ta /.
we work up the tree by pruning BSs on our way up,
which in general gives rise to grammatically legiti-
mate compressions of various lengths (Figure 2).
More specifically, we take the following steps to
construct G(S). Let S = ABCDE. Assume that
it has a dependency structure as in Figure 3. We
begin by locating terminal nodes, i.e., those which
have no incoming edges, depicted as filled circles
in Figure 3, and find a dependency (singly linked)
path from each terminal node to the root, or a node
labeled ?E? here, which would give us two paths
p1 = A-C-D-E and p2 = B-C-D-E (call them ter-
minating dependency paths, or TDPs). Now create
a set T of all trimmings, or suffixes of each TDP,
including an empty string:
T (p1) = {<A C D E>, <C D E>, <D E>, <E>, <>}
T (p2) = {<B C D E>, <C D E>, <D E>, <E>, <>}
Then we merge subpaths from the two sets in every
possible way, i.e., for any two subpaths t1 ? T (p1)
and t2 ? T (p2), we take a union over nodes in t1 and
t2; Figure 4 shows how this might done. We remove
duplicates if any. This would give us G(S)={{A B C
D E}, {A C D E}, {B C D E}, {C D E}, {D E}, {E},
{}}, a set of compressions over S based on TDPs.
What is interesting about the idea is that creating
G(S) does not involve much of anything that is spe-
cific to a given language. Indeed this could be done
on English as well. Take for instance a sentence at
the top of Table 1, which is a slightly modified lead
sentence from an article in the New York Times. As-
sume that we have a relevant dependency structure
as shown in Figure 5, where we have three TDPs,
i.e., one with southern, one with British and one with
lethal. Then G(S) would include those listed in Ta-
ble 1. A major difference from Japanese lies in the
direction in which a tree is branching out: right ver-
sus left.4
Having said this, we need to address some lan-
guage specific constraints: in Japanese, for instance,
we should keep a topic marked NP in compression
as its removal often leads to a decreased readability;
and also it is grammatically wrong to start any com-
pressed segment with sentence nominalizers such as
4We stand in a marked contrast to previous ?grafting? ap-
proaches which more or less rely on an ad-hoc collection
of transformation rules to generate candidates (Riezler et al,
2003).
301
Table 1: Hedge-clipping English
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British
troops in southern Iraq
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British
troops in Iraq
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British
troops
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on troops
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks
An official was quoted yesterday as accusing Iran of supplying explosive technology used in attacks
An official was quoted yesterday as accusing Iran of supplying explosive technology
An official was quoted yesterday as accusing Iran of supplying technology
< A C D E >
< B C D E >
< C D E >
< D E >
< E >
< >
{
A B C D E }
{
A C D E }
{
A C D E }
{
A C D E }
{
A C D E }
< D E >
< B C D E >
< C D E >
< D E >
< E >
< >
{
B C D E }
{
C D E }
{
D E }
{
D E }
{
D E }
< >
< B C D E >
< C D E >
< D E >
< E >
< >
{
B C D E }
{
C D E }
{
D E }
{
E }
{
}
< C D E >
< B C D E >
< C D E >
< D E >
< E >
< >
{
B C D E }
{
C D E }
{
C D E }
{
C D E }
{
C D E }
< E >
< B C D E >
< C D E >
< D E >
< E >
< >
{
B C D E }
{
C D E }
{
D E }
{
E }
{
E }
Figure 4: Combining TDP suffixes
-koto and -no. In English, we should keep a prepo-
sition from being left dangling, as in An official was
quoted yesterday as accusing Iran of supplying tech-
nology used in. In any case, we need some extra
rules on G(S) to take care of language specific is-
sues (cf. Vandeghinste and Pan (2004) for English).
An important point about the dependency truncation
is that for most of the time, a compression it gener-
ates comes out reasonably grammatical, so the num-
ber of ?extras? should be small.
Finally, in order for CRFs to work with the com-
pressions, we need to translate them into a sequence
of binary labels, which involves labeling an element
token, bunsetsu or a word, with some label, e.g., 0
for ?remove? and 1 for ?retain,? as in Figure 6.
i n
s o u t h e r
n
I
r a q
t r o o p s
B
r
i
t
i
s h
o
n
a t t a c k s
l
e t h a
l
i n
u s e d
Figure 5: An English dependency structure and TDPs
Consider following compressions y1 to y4 for
x = ?1?2?3?4?5?6. ?i denotes a bunsetsu (BS).
?0? marks a BS to be removed and ?1? that to be re-
tained.
?1 ?2 ?3 ?4 ?5 ?6
y1 0 1 1 1 1 1
y2 0 0 1 1 1 1
y3 0 0 0 0 0 1
y4 0 0 1 0 0 0
Assume that G(S) = {y1,y2,y3}. Because y4
is not part of G(S), it is not considered a candidate
for a compression for y, even if its likelihood may
exceed those of others in G(S). We note that the
approach here does not rely on so much of CRFs
as a discriminative classifier as CRFs as a strategy
for ranking among a limited set of label sequences
which correspond to syntactically plausible simpli-
fications of input sentence.
Furthermore, we could dictate the length of com-
pression by putbting an additional constraint on out-
302
S0 0
0
1
0 0
0
1
Figure 6: Compression in binary representation.
put, as in:
y? = arg max
y?G?(S)
p(y|x;?), (5)
where G?(S) = {y : y ? G(S), R(y,x) = r}.
R(y,x) denotes a compression rate r for which y is
desired, where r = # of 1 in ylength of x . The constraint forces
the trimmer to look for the best solution among can-
didates that satisfy the constraint, ignoring those that
do not.5
Another point to note is thatG(S) is finite and rel-
atively small ? it was found, for our domain, G(S)
usually runs somewhere between a few hundred and
ten thousand in length ? so in practice it suffices
that we visit each compression in G(S), and select
one that gives the maximum value for the objective
function. We will have more to say about the size of
the search space in Section 6.
3 Features in CRFs
We use an array of features in CRFs which are ei-
ther derived or borrowed from the taxonomy that
a Japanese tokenizer called JUMAN and KNP,6 a
Japanese dependency parser (aka Kurohashi-Nagao
Parser), make use of in characterizing the output
they produce: both JUMAN and KNP are part of the
compression model we build.
Features come in three varieties: semantic, mor-
phological and syntactic. Semantic features are used
for classifying entities into semantic types such as
name of person, organization, or place, while syn-
tactic features characterize the kinds of dependency
5It is worth noting that the present approach can be recast
into one based on ?constraint relaxation? (Tromble and Eisner,
2006).
6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html
relations that hold among BSs such as whether a BS
is of the type that combines with the verb (renyou),
or of the type that combines with the noun (rentai),
etc.
A morphological feature could be thought of as
something that broadly corresponds to an English
POS, marking for some syntactic or morphological
category such as noun, verb, numeral, etc. Also
we included ngram features to encode the lexi-
cal context in which a given morpheme appears.
Thus we might have something like: for some
words (morphemes) w1, w2, and w3, fw1?w2(w3) =
1 if w3 is preceded by w1, w2; otherwise, 0. In ad-
dition, we make use of an IR-related feature, whose
job is to indicate whether a given morpheme in the
input appears in the title of an associated article.
The motivation for the feature is obviously to iden-
tify concepts relevant to, or unique to the associ-
ated article. Also included was a feature on tfidf,
to mark words that are conceptually more important
than others. The number of features came to around
80,000 for the corpus we used in the experiment.
4 The Dependency Path Model
In what follows, we will describe somewhat in
detail a prior approach to sentence compression
in Japanese which we call the ?dependency path
model,? or DPM. DPM was first introduced in
(Oguro et al, 2000), later explored by a number of
people (Morooka et al, 2004; Yamagata et al, 2006;
Fukutomi et al, 2007).7
DPM has the form:
h(y) = ?f(y) + (1 ? ?)g(y), (6)
where y = ?0, ?1, . . . , ?n?1, i.e., a compression
consisting of any number of bunsetsu?s, or phrase-
like elements. f(?) measures the relevance of con-
tent in y; and g(?) the fluency of text. ? is to provide
a way of weighing up contributions from each com-
ponent.
We further define:
f(y) =
n?1
?
i=0
q(?i), (7)
7Kikuchi et al (2003) explore an approach similar to DPM.
303
d i s a p p e a r e d
d o g s
f r o m
T h
r e e l e g g e d
s i g
h t
Figure 7: A dependency structure
and
g(y) = max
s
n?2
?
i=0
p(?i, ?s(i)). (8)
q(?) is meant to quantify how worthy of inclusion
in compression, a given bunsetsu is; and p(?i, ?j)
represents the connectivity strength of dependency
relation between ?i and ?j . s(?) is a linking function
that associates with a bunsetsu any one of those that
follows it. g(y) thus represents a set of linked edges
that, if combined, give the largest probability for y.
Dependency path length (DL) refers to the num-
ber of (singly linked) dependency relations (or
edges) that span two bunsetsu?s. Consider the de-
pendency tree in Figure 7, which corresponds to
a somewhat contrived sentence ?Three-legged dogs
disappeared from sight.? Take an English word for a
bunsetsu here. We have
DL(three-legged, dogs) = 1
DL(three-legged, disappeared) = 2
DL(three-legged, from) = ?
DL(three-legged, sight) = ?
Since dogs is one edge away from three-legged, DL
for them is 1; and we have DL of two for three-
legged and disappeared, as we need to cross two
edges in the direction of arrow to get from the for-
mer to the latter. In case there is no path between
words as in the last two cases above, we take the DL
to be infinite.
DPM takes a dependency tree to be a set of
linked edges. Each edge is expressed as a triple
< Cs(?i), Ce(?j),DL(?i, ?j) >, where ?i and ?j
represent bunsestu?s that the edge spans. Cs(?) de-
notes the class of a bunsetsu where the edge starts
and Ce(?) that of a bunsetsu where the edge ends.
What we mean by ?class of bunsetsu? is some sort of
a classificatory scheme that concerns linguistic char-
acteristics of bunsetsu, such as a part-of-speech of
the head, whether it has an inflection, and if it does,
what type of inflection it has, etc. Moreover, DPM
uses two separate classificatory schemes for Cs(?)
and Ce(?).
In DPM, we define the connectivity strength p by:
p(?i, ?j) =
{
logS(t) if DL(?i, ?j) ?= ?
?? otherwise (9)
where t =< Cs(?i), Ce(?j),DL(?i, ?j) >, and
S(t) is the probability of t occurring in a compres-
sion, which is given by:
S(t) = # of t?s found in compressions
# of triples found in the training data
(10)
We complete the DPM formulation with:
q(?) = log pc(?) + tfidf(?) (11)
pc(?) denotes the probability of having bunsetsu ?
in compression, calculated analogously to Eq. 10,8
and tfidf(?) obviously denotes the tfidf value of ?.
In DPM, a compression of a given sentence can be
obtained by finding argmaxy h(y), where y ranges
over possible candidate compressions of a particular
length one may derive from that sentence. In the
experiment described later, we set ? = 0.1 for DPM,
following Morooka et al (2004), who found the best
performance with that setting for ?.
5 Evaluation Setup
We created a corpus of sentence summaries based
on email news bulletins we had received over five
to six months from an on-line news provider called
Nikkei Net, which mostly deals with finance and
politics.9 Each bulletin consists of six to seven news
briefs, each with a few sentences. Since a news brief
contains nothing to indicate what its longer version
8DPM puts bunsetsu?s into some groups based on linguis-
tic features associated with them, and uses the statistics of the
groups for pc rather than that of bunsetsu?s that actually appear
in text.
9http://www.nikkei.co.jp
304
Table 2: The rating scale on fluency
RATING EXPLANATION
1 makes no sense
2 only partially intelligible/grammatical
3 makes sense; seriously flawed in gram-
mar
4 makes good sense; only slightly flawed
in grammar
5 makes perfect sense; no grammar flaws
might look like, we manually searched the news site
for a full-length article that might reasonably be con-
sidered a long version of that brief.
We extracted lead sentences both from the brief
and from its source article, and aligned them, us-
ing what is known as the Smith-Waterman algorithm
(Smith and Waterman, 1981), which produced 1,401
pairs of summary and source sentence.10 For the
ease of reference, we call the corpus so produced
?NICOM? for the rest of the paper. A part of our sys-
tem makes use of a modeling toolkit called GRMM
(Sutton et al, 2004; Sutton, 2006). Throughout the
experiments, we call our approach ?Generic Sen-
tence Trimmer? or GST.
6 Results and Discussion
We ran DPM and GST on NICOM in the 10-fold
cross validation format where we break the data into
10 blocks, use 9 of them for training and test on the
remaining block. In addition, we ran the test at three
different compression rates, 50%, 60% and 70%, to
learn how they affect the way the models perform.
This means that for each input sentence in NICOM,
we have three versions of its compression created,
corresponding to a particular rate at which the sen-
tence is compressed. We call a set of compressions
so generated ?NICOM-g.?
In order to evaluate the quality of outputs GST
and DPM generate, we asked 6 people, all Japanese
natives, to make an intuitive judgment on how each
compression fares in fluency and relevance to gold
10The Smith-Waterman algorithm aims at finding a best
match between two sequences which may include gaps, such
as A-C-D-E and A-B-C-D-E. The algorithm is based on an idea
rather akin to dynamic programming.
Table 3: The rating scale on content overlap
RATING EXPLANATION
1 no overlap with reference
2 poor or marginal overlap w. ref.
3 moderate overlap w. ref.
4 significant overlap w. ref.
5 perfect overlap w. ref.
standards (created by humans), on a scale of 1 to 5.
To this end, we conducted evaluation in two sepa-
rate formats; one concerns fluency and the other rel-
evance. The fluency test consisted of a set of com-
pressions which we created by randomly selecting
200 of them from NICOM-g, for each model at com-
pression rates 50%, 60%, and 70%; thus we have
200 samples for each model and each compression
rate.11 The total number of test compressions came
to 1,200.
The relevance test, on the other hand, consisted of
paired compressions along with the associated gold
standard compressions. Each pair contains compres-
sions both from DPM and from GST at a given com-
pression rate. We randomly picked 200 of them from
NICOM-g, at each compression rate, and asked the
participants to make a subjective judgment on how
much of the content in a compression semantically
overlap with that of the gold standard, on a scale of
1 to 5 (Table 3). Also included in the survey are 200
gold standard compressions, to get some idea of how
fluent ?ideal? compressions are, compared to those
generated by machine.
Tables 4 and 5 summarize the results. Table 4
looks at the fluency of compressions generated by
each of the models; Table 5 looks at how much of
the content in reference is retained in compressions.
In either table, CR stands for compression rate. All
the results are averaged over samples.
We find in Table 4 a clear superiority of GST over
DPM at every compression rate examined, with flu-
ency improved by as much as 60% at 60%. How-
ever, GST fell short of what human compressions
achieved in fluency ? an issue we need to address
11As stated elsewhere, by compression rate, we mean r =
# of 1 in y
length of x .
305
Table 4: Fluency (Average)
MODEL/CR 50% 60% 70%
GST 3.430 3.820 3.810
DPM 2.222 2.372 2.660
Human ? 4.45 ?
Table 5: Semantic (Content) Overlap (Average)
MODEL/CR 50% 60% 70%
GST 2.720 3.181 3.405
DPM 2.210 2.548 2.890
in the future. Since the average CR of gold standard
compressions was 60%, we report their fluency at
that rate only.
Table 5 shows the results in relevance of con-
tent. Again GST marks a superior performance over
DPM, beating it at every compression rate. It is in-
teresting to observe that GST manages to do well
in the semantic overlap, despite the cutback on the
search space we forced on GST.
As for fluency, we suspect that the superior per-
formance of GST is largely due to the depen-
dency truncation the model is equipped with; and
its performance in content overlap owes a lot to
CRFs. However, just how much improvement GST
achieved over regular CRFs (with no truncation) in
fluency and in relevance is something that remains
to be seen, as the latter do not allow for variable
length compression, which prohibits a straightfor-
ward comparison between the two kinds of models.
We conclude the section with a few words on the
size of |G(S)|, i.e., the number of candidates gener-
ated per run of compression with GST.
Figure 8 shows the distribution of the numbers of
candidates generated per compression, which looks
like the familiar scale-free power curve. Over 99%
of the time, the number of candidates or |G(S)| is
found to be less than 500.
7 Conclusions
This paper introduced a novel approach to sentence
compression in Japanese, which combines a syntac-
tically motivated generation model and CRFs, in or-
Number of Candidates
Fr
eq
ue
nc
y
0 500 1500 2500
0
40
0
80
0
12
00
Figure 8: The distribution of |G(S)|
der to address fluency and relevance of compres-
sions we generate. What distinguishes this work
from prior research is its overt withdrawal from a
search for global optima to a search for local optima
that comply with grammar.
We believe that our idea was empirically borne
out, as the experiments found that our approach out-
performs, by a large margin, a previously known
method called DPM, which employs a global search
strategy. The results on semantic overlap indicates
that the narrowing down of compressions we search
obviously does not harm their relevance to refer-
ences.
An interesting future exercise would be to explore
whether it is feasible to rewrite Eq. 5 as a linear inte-
ger program. If it is, the whole scheme of ours would
fall under what is known as ?Linear Programming
CRFs? (Tasker, 2004; Roth and Yih, 2005). What re-
mains to be seen, however, is whether GST is trans-
ferrable to languages other than Japanese, notably,
English. The answer is likely to be yes, but details
have yet to be worked out.
References
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer programming
306
approach. In Proceedings of the COLING/ACL 2006,
pages 144?151.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 73?82, Prague, June.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generataion. In Proceedings of the HLT-NAACL
Text Summarization Workshop and Document Under-
standing Conderence (DUC03), pages 1?8, Edmon-
ton, Canada.
Satoshi Fukutomi, Kazuyuki Takagi, and Kazuhiko
Ozeki. 2007. Japanese Sentence Compression using
Probabilistic Approach. In Proceedings of the 13th
Annual Meeting of the Association for Natural Lan-
guage Processing Japan.
Michel Galley and Kathleen McKeown. 2007. Lexical-
ized Markov grammars for sentence compression. In
Proceedings of the HLT-NAACL 2007, pages 180?187.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th Confer-
ence on Applied Natural Language Processing, pages
310?315.
Tomonori Kikuchi, Sadaoki Furui, and Chiori Hori.
2003. Two-stage automatic speech summarization by
sentence extraction and compaction. In Proceedings
of ICASSP 2003.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
John Lafferty, Andrew MacCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference
on Machine Learning (ICML-2001).
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of the 11th Conference of EACL, pages 297?304.
Yuhei Morooka, Makoto Esaki, Kazuyuki Takagi, and
Kazuhiko Ozeki. 2004. Automatic summarization of
news articles using sentence compaction and extrac-
tion. In Proceedings of the 10th Annual Meeting of
Natural Language Processing, pages 436?439, March.
(In Japanese).
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management, 43:1571 ? 1587.
Rei Oguro, Kazuhiko Ozeki, Yujie Zhang, and Kazuyuki
Takagi. 2000. An efficient algorithm for Japanese
sentence compaction based on phrase importance
and inter-phrase dependency. In Proceedings of
TSD 2000 (Lecture Notes in Artificial Intelligence
1902,Springer-Verlag), pages 65?81, Brno, Czech Re-
public.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical functional grammar. In Pro-
ceedings of HLT-NAACL 2003, pages 118?125, Ed-
monton.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd International Conference on
Machine Learning (ICML 05).
T. F. Smith and M. S. Waterman. 1981. Identification of
common molecular subsequence. Journal of Molecu-
lar Biology, 147:195?197.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic labeling and segment-
ing sequence data. In Proceedings of the 21st In-
ternational Conference on Machine Learning, Banff,
Canada.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Ben Tasker. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
Roy W. Tromble and Jason Eisner. 2006. A fast finite-
state relaxation method for enforcing global constraint
on sequence decoding. In Proceeings of the NAACL,
pages 423?430.
Jenie Turner and Eugen Charniak. 2005. Supervised and
unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 290?297, Ann Arbor, June.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automatic subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization, Barcelona.
Kiwamu Yamagata, Satoshi Fukutomi, Kazuyuki Takagi,
and Kzauhiko Ozeki. 2006. Sentence compression
using statistical information about dependency path
length. In Proceedings of TSD 2006 (Lecture Notes in
Computer Science, Vol. 4188/2006), pages 127?134,
Brno, Czech Republic.
307
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1996?2006, Dublin, Ireland, August 23-29 2014.
Lexico-syntactic text simplification and compression with typed
dependencies
Mandya Angrosh
Computing Science,
University of Aberdeen,
Aberdeen, UK.
angroshmandya@abdn.ac.uk
Tadashi Nomoto
National Institute of
Japanese Literature,
Tokyo, Japan.
nomoto@acm.org
Advaith Siddharthan
Computing Science,
University of Aberdeen,
Aberdeen, UK.
advaith@abdn.ac.uk
Abstract
We describe two systems for text simplification using typed dependency structures, one that
performs lexical and syntactic simplification, and another that performs sentence compression
optimised to satisfy global text constraints such as lexical density, the ratio of difficult words, and
text length. We report a substantial evaluation that demonstrates the superiority of our systems,
individually and in combination, over the state of the art, and also report a comprehension based
evaluation of contemporary automatic text simplification systems with target non-native readers.
1 Introduction
Text simplification has often been defined as the process of reducing the grammatical and lexical com-
plexity of a text, while still retaining the original information content and meaning. However, text can
also be simplified in other ways; for instance, by removing peripheral information to reduce text length,
through sentence compression or summarisation. A key goal of automatic text simplification is to make
information more accessible to the large numbers of people with reduced literacy, motivated by a large
body of evidence that manual text simplification is an effective intervention (Anderson and Freebody,
1981; L?Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski
et al., 2008). However automatic text simplification systems have rarely been evaluated in a manner that
sheds light on whether they can facilitate target users.
To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few
as 20?25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and
Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the
inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and
(c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend
and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent read-
ers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011;
Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent,
2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic
question: How good is automatic text simplification; i.e., would it facilitate poor readers?
Our goals in this paper are twofold. First, we want to evaluate text simplification systems more sys-
tematically than has been attempted before, using both human judgements on a larger scale, and directly
testing comprehension on longer passages for target reader populations. Second, we want to compare
two different approaches to text simplification. In this paper, we present a text simplification system that
can perform lexical and syntactic simplification (?3), as well as a novel sentence compression system
designed specifically for the text simplification task (?4), in that it favours compressions with fewer diffi-
cult words and with more function words such as connectives that are known to improve readability. We
evaluate both, as well as a hybrid system that performs both text simplification and compression (?5, 6).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1996
2 Related Work
Text simplification systems differ primarily in the level of linguistic knowledge they encode. Phrase
Based Machine Translation (PBMT) systems (Specia, 2010; Wubben et al., 2012; Coster and Kauchak,
2011) use the least knowledge, and as such are ill equipped to handle simplifications that require mor-
phological changes, syntactic reordering, sentence splitting or insertions. While syntax based MT ap-
proaches use syntactic knowledge, existing systems do not offer a treatment of morphology (Zhu et al.,
2010; Woodsend and Lapata, 2011; Paetzold and Specia, 2013). This means that while some syntactic
reordering operations can be performed well, others requiring morphological changes cannot. Consider
converting passive to active voice (e.g., from ?trains are liked by John? to ?John likes trains?). Besides
deleting auxiliaries and reordering the arguments of the verb, there is also a requirement to modify the
verb to make it agree in number with the new subject ?John?, and take the tense of the auxiliary ?are?.
Hand crafted systems such as Siddharthan (2010) and Siddharthan (2011) use transformation rules that
encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting,
and can handle voice change correctly. However, hand crafted systems are limited in scope to syntactic
simplification as there are too many lexico-syntactic and lexical simplifications to enumerate manually.
Some contemporary work in text simplification has evolved from research in sentence compression,
a related research area that aims to shorten sentences for the purpose of summarising the main content.
Sentence compression has historically been addressed in a generative framework, where transformation
rules are learnt from parsed corpora of sentences aligned with manually compressed versions, using
ideas adapted from statistical machine translation. The compression rules learnt are typically syntactic
tree-to-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003;
Cohn and Lapata, 2009; Nomoto, 2008) of some variety. Indeed, Woodsend and Lapata (2011) develop
this line of research. Their model is based on quasi-synchronous tree substitution grammar (QTSG)
(Smith and Eisner, 2006) and integer linear programming. Quasi-synchronous grammars aim to relax
the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment
between parse trees. Woodsend and Lapata (2011) use QTSG to generate all possible rewrite operations
for a source tree, and then integer linear programming to select the most appropriate simplification. Their
system performs lexical and syntactic simplification as well as compression.
Recently, there have been attempts to combine approaches. Narayan and Gardent (2014) use an ap-
proach based on semantics to perform syntactic simplification, and PBMT for lexical simplifications.
We have also created a hybrid system, but one using linguistically sound hand written rules for syntac-
tic simplification and automatically acquired rules for lexicalised constructs (Siddharthan and Mandya,
2014; Mandya and Siddharthan, 2014). In this paper we combine this work (summarised in ?3) with a
new method for sentence compression (described in ?4).
3 Text Simplification with Synchronous Dependency Grammars
We use the RegenT text simplification (Siddharthan, 2011), augmented with automatically acquired rules,
as described in detail elsewhere (Mandya and Siddharthan, 2014; Siddharthan and Mandya, 2014). In
this section, we will restrict ourselves to summarising the key features of the system.
Our text simplification system follows the architecture proposed in Ding and Palmer (2005) for Syn-
chronous Dependency Insertion Grammars, reproduced in Fig. 1. It uses the same dataset
1
as Woodsend
and Lapata (2011) for learning lexicalised rules. The rules are acquired in the format required by the Re-
genT text simplification system (Siddharthan, 2011), which is used to implement the simplification. This
1
consisting of ?140K aligned simplified and original sentence pairs obtained from Simple English Wikipedia and English
Wikipedia.
Input Sentence ?? Dependency Parse ?? Source ETs Target ETs ?? Generation ?? Output Sentences
? ?
ET Transfer
Figure 1: System Architecture
1997
RULE 1: MOST INTENSIVE2STRONGEST
1. DELETE
(a) advmod(?X0[intensive], ?X1[most])
(b) advmod(?X2[storm], ?X0[intensive])
2. INSERT
(a) advmod(?X2, ?X3[strongest])
storm
advmod
intensive
advmod
most
storm
advmod
strongest
Figure 2: Simplification as a Transfer rule and a transduction of Elementary Trees (ETs)
requires dependency parses from the Stanford Parser, and generates output sentences from dependency
parses using the generation-light approach described in (Siddharthan, 2011).
In short, we extract a synchronous grammar from dependency parses of aligned English and sim-
ple English sentences, starting from the differences in the parses. For example, consider two aligned
sentences from the aligned corpus described in Woodsend and Lapata (2011):
1. (a) It was the second most intensive storm on the planet in 1989.
(b) It was the second strongest storm on the planet in 1989.
An automatic comparison of the dependency parses for the two sentences reveals that there are two
typed dependencies that occur only in the parse of the first sentence, and one that occurs only in the parse
of the second. Thus, to convert the first sentence into the second, two dependencies need to be deleted
and one inserted. From this example, the rule shown in Fig. 2 is extracted. The rule contains variables
(?Xn), which can be forced to match certain words in square brackets.
Such deletion and insertion operations are central to text simplification, but a few other operations
are also needed to handle morphology and to avoid broken dependency links in the Target ETs. These
are enumerated in (Siddharthan, 2011). By collecting such rules, a meta-grammar is produced that can
translate dependency parses in one language (English) into the other (simplified English). The rule
above will translate ?most intensive? to ?strongest?, in the immediate lexical context of ?storm?. The
ET Transfer component can be presented either as transformation rules or as a transduction of ETs, as
shown in Fig. 2. In Mandya and Siddharthan (2014), we describe how such automatically acquired rules
can be generalised to apply in new contexts; for instance, by expanding lexical context to include related
words derived from WordNet, or by removing the lexical context for lexical simplifications that are not
context dependent.
Learning paraphrase with typed dependency representations has certain advantages to PBMT; for ex-
ample, consider the rule that simplifies ?described as? to ?called?:
RULE: DESCRIBED_AS2CALLED
1. DELETE:
(a) prep_as(?X0[described], ?X1)
2. INSERT:
(a) dobj(?X2[called], ?X1)
This single rule can simplify ?Coulter was described as a polemicist? to ?Coulter was called a polemi-
cist? as well as cases where the words are not adjacent, such as ?Coulter has described herself as a
polemicist? to ?Coulter has called herself a polemicist?.
Our text simplification system, as evaluated in this paper, combines a set of 278 hand crafted grammar
for syntactic simplification (from the original RegenT system) and 5172 automatically acquired rules,
based on the principles described above.
4 Sentence Compression with Reluctant Trimmer
This section describes the mechanics of the reluctant trimmer (RT), or how it works to create a simplified
form of sentence. We will explain later where the word ?reluctant? comes from. Broadly, RT comes
in two parts: generation and selection. For a given sentence it takes as input, it generates a number of
1998
detention
2009 of
hikers
American
by
Iran
Figure 3: Dependency structure for ?2009 de-
tention of American hikers by Iran?
detention
2009 ofhikers
American
by Iran
 C1
C2C3
Figure 4: Cropping dependency tree
truncations of the sentence, each of which has some elements removed in a way that largely complies
with English syntax. It does this by first parsing the sentence into a a dependency representation, and
creating what we call terminating dependency paths out of the representation. After placing them in a
lattice format, we run a K-best search over the lattice to generate K best truncations of the sentence. We
repeat the process for each sentence found in the text, which will produce a collection of sets of truncation
candidates. We then run integer linear programming over the collection, selecting one sentence for each
set in a way that satisfies global constraints such as lexical density, the ratio of hard words, and text
length. In particular, we regard RT not as an operation that works sentence by sentence, but one that
works with text as a whole. We argue that how the sentence is to be compressed is not only dictated by
the sentence itself, but also by the text in which it appears.
We start off with an example shown in Figure 3, where we have a phrase ?2009 detention of American
hikers by Iran.? Our goal here is to develop a systematic method that will prune the dependency tree so
as to generate shorter versions of the sentence largely in compliance with the English grammar. Figure 4
provides an intuitive picture of how this could be done: by cropping the tree along the arrows. We
implement the idea by borrowing the notion of Terminating Dependency Path (TDP) (Nomoto, 2008),
which gives us a way to translate a dependency tree into a trellis of nodes, which in turn allows us to find
truncations through dynamic programming.
Figure 5 shows a TDP lattice derived from the dependency tree given in Figure 3. TDPs are depicted
as solid blue lines in the figure. It is easy to see that each TDP corresponds to a path in the dependency
tree that runs from a leaf to the root. The conversion from dependency tree to TDP lattice is thus
straightforward. We perform A
?
search over the TDP lattice to find the best compression. Assume
that we have a path or a sequence of nodes, ?n[1], n[2] . . . , n[j], . . . , n[z ? 1], n[z]?, that takes you
from the starting node, n[1], to the goal, n[z], on the TDP lattice. Define the cost C of node n[x] by:
C(x) = g(x)+h(x) where g(x) is the cost incurred for the travel from the starting node to n[x] and h(x)
the future estimate for the cost of travelling from n[x] to the goal. Let g(x) = ?
?
j?V (1,x)
backward(j)
and h(x) = ?
?
j?W (x,z?1)
forward(j), with:
backward(x) = tfidf(n[x]) + pr(seq(n[x? 1], n[x])|M), (1)
forward(x) = backward(x+ 1) (2)
V (1, x) is a sequence of nodes that appeared on the path we took to reach n[x] from the starting node,
W (x, z ? 1) a sequence of nodes that gives the shortest possible path (i.e. the path that incurs least
cost) from n[x] to the goal. tfidf(n) represents a tfidf score for a word associated with the node n, with
tfidf(n[1]) = 0 and tfidf(n[g]) = 0, and is normalised so that it falls between 1 and 0.
2
seq(n,m)
refers to an uninterrupted sequence of words you find on the path that extends from n to m via the root,
ignoring duplicates. Figure 6 gives an intuitive sense of how this works. seq (2009, hiker), for instance,
can be found by following the blue line in the figure, which results in ?2009 detention of hikers.? ?M?
refers to a language model.
3
pr(seq(n,m)|M) is the probability of sequence ?seq(n,m)? under language
2
Document frequencies (df) we used for present purposes are based on those given in the British National Corpus (www.
kilgarriff.co.uk/bnc-readme.html), which keeps record of the number of files a particular word occurred.
3
The language model is built here by running SRLM (www.speech.sri.com/projects/srilm) on the English
1999
detention
2009
of
hikers
American
by
Iran
detention detention
tdp tdp tdpstart end
<s> </s>
Figure 5: TDP lattice. ??s??
is a label for the starting node,
??/s?? that for the goal.
detention
2009
of
hikers
American
by
Iran
detention detention
seq(2009, hikers) = "2009 detention of hikers"
Figure 6: seq(2009,hiker)
x(1,1)
x(1,2)
x(1,3)
x(1,4)
x(2,1)
x(2,3)
x(2,4)
x(3,1)
x(3,2)
x(3,3)
x(3,4)
x(4,1)
x(4,2)
x(4,3)
x(4,4)
x(2,2)
S1 S2 S3 S4
Figure 7: Decoding with ILP
model M .
4
Traversing over the TDP lattice while picking nodes with least costs will produce the best
compression, to which we apply Yen (1971)?s algorithm to find K-best alternatives (where K is set to 10
in the experiments reported below).
We now turn to the second part of the story, which is about choosing from each pool of K-best candi-
dates, to create a simplified version of the text. (Recall that we keep a pool of K-best compressions for
each of the sentences in a text, and create a simplification by choosing a compression from each pool.) In
this paper, we build on a particular simplification approach based on integer linear programming (ILP),
by (Dras, 1999), which he dubbed ?reluctant paraphrasing.? In a nutshell, Dras claims that we should
make as little change to the text as possible, arguing that any change may run the risk of muddling the
meaning of the original text: hence the name ?reluctant paraphrasing.? The following linear program
(LP) represents our adaptation of Dras?s method. Formula 3 represents the objective function, with 4
through 7 expressing constraints:
min z =
?
c
i,j
x
i,j
(3)
subject to:
?i
?
j
x
ij
= 1, x
ij
? {0, 1}, ?ij (4)
W +
?
w
ij
? x
ij
S
? k
1
(5)
H +
?
h
ij
? x
ij
W +
?
w
ij
? x
ij
? k
2
(6)
F +
?
f
ij
? x
ij
W +
?
w
ij
? x
ij
? k
3
(7)
x
i,j
denotes a candidate for which we are to make a decision on whether to include it in the simplification
of a given text d. In particular we mean x
i,j
to represent the j-th best compression for the i-th sentence
in d. Constraint 4 dictates that we have exactly one compression candidate for each sentence in d. w
ij
indicates the number of changes or deletions we performed on the original sentence to create x
ij
: -1 if
x
ij
has one less term than the original sentence it is a compression of; 0 if there is no change. W is the
number of terms in d, S the number of sentences in d. Constraint 5 states that proportion of the number
of terms to that of sentences should be less than or equal to k
1
; in other words, changes made to the
text should not exceed k
1
. H in constraint 6 denotes the total number of ?hard? or difficult words in the
original text; h
ij
the number of changes made to hard words in x
ij
, namely how many less or more words
there remain that are deemed ?hard,? compared to the sentence it comes from.
5
: h
ij
= ?3, for example,
means that we have three less hard words in x
ij
than in the original sentence.
Constraint 6 is included here to keep the proportion of hard words in text from growing beyond a
portion of TDT5 corpus and TDT Pilot Study Corpus (both available at Linguistic Data Consortium), the total number of
sentences combined reaching 293,971.
4
We note here that we did not compensate the probability for the length of a word sequence, as we were unable to find an
empirical evidence that suggested we should do otherwise.
5
?Hard words? are defined here as those that fall off of the New General Service List (www.newgeneralservicelist.org) which
currently contains 2,881most frequently used words.
2000
particular threshold k
2
. The values of k
1
, k
2
and k
3
were determined based on the Breaking News
English (BNE) corpus (described later), which provides for each story, simplified versions at two levels
of difficulty, one being called ?easy? and the other ?hard.? If we take the ?easy? as a gold standard
simplification for the ?hard,? we will be able to get estimates of k
1
through k
3
. None of the data we used
for this purpose, however, is part of the BNE reading test discussed below.
F in constraint 7 represents the total number of function words (those that are not of JJ, MD, NN, RB,
or VB in the Penn scheme) while f
ij
indicates that of changes to function words (the way it works is
analogous to h
ij
). The motivation for the constraint is to prevent function words from being eliminated
excessively, which Dras argues, reduces the readability of text. The objective function includes param-
eters c
i,j
which serve to indicate the cost of transforming the sentence. In this paper, we define c
ij
as
Levenshtein edit distance between compression and original sentence. In ordinary language, the linear
program may read like ?Keep changes to a minimum. Accept compressions that look much like the
original sentences from which they arise, with less of hard words and content terms and more of function
words." Further, we made use of an array of hand-coded constraints in addition to a language model,
to ensure that a compression we generate remains as grammatical as possible. Included were those that
prohibit the generation of a compression that involves a dangling preposition or breaks apart multi-word
prepositions (MWPs) such as according to, compared to, in front of, etc. (the complete list of MWPs
we used for this purpose can be found in de Marneffe and Manning (2008)). Added to these were some
"don?t drop" rules that demanded we keep intact subjects and verbs as well.
Figure 7 illustrates how compression variables x
i,j
are organised (each of which is depicted as ?x(i, j)?
in the figure). Each vertical line represents a pool of K-best compressions generated for a particular
sentence s
i
. LP seeks to find a candidate from each pool so that the resulting set of compressions best
meets the objective function and conditions it dictates.
6
5 Evaluation of Fluency, Simplicity and Meaning Preservation
We performed a manual evaluation of how fluent and simple the text produced by our simplification
system is, and the extent to which it preserves meaning. We evaluate 3 systems:
TS: The Text Simplification system based on synchronous dependency grammars (?3).
RT: The Reluctant Trimmer for sentence compression (?4).
HYB: A hybrid text simplification system that applies RT to the output of TS.
We used as a baseline Woodsend and Lapata (2011)?s QTSG system that learns a quasi-synchronous
tree substitution grammar from the same EW-SEW dataset used by TS. QTSG is the best performing
system in the literature with a similar scope to ours in terms of the syntactic, lexical and compression
operations performed
7
. QTSG relies entirely on an automatically acquired grammar of 1431 rules, for
lexical and syntactic simplification as well as sentence compression. Our TS system has an automatically
extracted grammar with 5172 lexicalised rules to augment the existing 278 manually written syntactic
rules in RegenT. The RT system is not trained on simplified text. We also compare against the manual
simplification (SEW), and the original EW sentences.
Data: We use an evaluation set consisting of 100 sentences from English Wikipedia (EW) aligned
with Simple English Wikipedia (SEW) sentences, following recent work (Woodsend and Lapata, 2011;
Wubben et al., 2012; Zhu et al., 2010; Mandya and Siddharthan, 2014; Siddharthan and Mandya, 2014).
These 100 sentences have been excluded from our training data for rule acquisition, as is standard.
Following Wubben et al. (2012), we used all the sentences from the evaluation set for which each of the
four systems had performed at least one simplification (as selecting sentences where no simplification is
performed by one system is likely to boost its fluency and meaning preservation ratings). This gave us a
test set of 50 sentences from the original 100.
6
As an LP solver, we used lp_solve 5.5.2.0, a mixed integer programming solver, available under public license at Source-
Forge (lpsolve.sourceforge.net/5.5).
7
The PBMT system of Wubben et al. (2012) reports better results than QTSG, but is not directly comparable because it does
not perform syntactic simplifications such as sentence splitting.
2001
FLUENCY SIMPLICITY MEANING
EW SEW QTSG TS RT HYB EW SEW QTSG TS RT HYB EW SEW QTSG TS RT HYB
Mean 3.97 4.09 2.20 3.53 3.19 3.01 3.40 3.54 2.41 3.79 3.15 2.83 - 4.14 2.52 3.44 3.43 3.28
SD 0.92 0.90 1.35 1.12 1.22 1.22 1.08 1.15 1.28 1.18 1.21 1.23 - 0.89 1.31 1.08 1.15 1.14
Median 4 4 2 4 3 3 3 4 2 4 3 3 - 4 2 4 4 3
Table 1: Results of human evaluation of different versions of simplified text
Method: We recruited participants on Amazon Mechanical Turk, filtered to live in the US and have
an approval rating of 80%, and paid $3 for a HIT (Human Intelligence Task). Each HIT contained 10
sentences from Wikipedia (EW), each alongside 5 simplified versions: QTSG, TS, RT, HYB and SEW
in a randomised manner. For each of these 10 sets, participants were asked to rate each simplified version
for fluency, simplicity and the extent to which it preserved the meaning of the original EW sentence.
Participants were also asked to rate the fluency and simplicity of the original EW sentence. We used a
Likert scale of 1?5, where 1 is totally unusable output, and 5 is output that is perfectly usable.
Results: The results are shown in Table 1. As seen, our HYB system, and the individual components
TS and RT all outperform QTSG with all three metrics. In particular, TS is comparable to the SEW
version when one looks at the median scores. Interestingly, TS performs better than SEW with respect
to simplicity, suggesting that the system is indeed capable of a wide range of simplification operations.
The ANOVA tests carried out to measure significant differences between versions is presented below.
Table 3 (Row 1) shows the average number of words in the original and each simplified version.
Fluency: A one-way ANOVA was conducted with fluency as the dependent variable and text version
as the fixed effect. We report a significant effect of version (EW, SEW, QTSG, HYB, TS, RT) on the
fluency score (F=173.1, p<10
-16
). A Tukey?s pairwise comparison test (Tukey?s HSD, overall ? = 0.05)
indicated significant differences between all pairs, except SEW-EW at p < 0.05.
Simplicity: A one-way ANOVA was conducted with simplicity as the dependent variable and text
version as the fixed effect. We report a significant effect of version on the simplicity score (F=29.9,
p<10
-16
). A Tukey?s pairwise comparison test (Tukey?s HSD, overall ? = 0.05) indicated significant
differences between all pairs except: EW-SEW, RT-EW, and SEW-TS at p < 0.05.
Meaning: A one-way ANOVA was conducted with meaning preservation as the dependent variable
and text version as the fixed effect. We report a significant effect of version on the meaning preservation
score (F=130.12, p=2x10
-16
). A Tukey?s pairwise comparison test (Tukey?s HSD, overall ? = 0.05)
indicated significant differences between all pairs except: RT-TS, RT-HYB and HYB-TS at p < 0.05.
Error Analysis: We manually examined sentences that had average ratings below 2. The main cause
of error for TS was misparsing, particularly errorful relative clause attachment and the parsing of comma
separated lists as apposition. TS fails badly in such cases, and it is possible that methods such as those
described in Siddharthan (2003b) are still relevant for correcting parser output. RT suffers mainly when it
removes punctuation, which make reading difficult, or names that contain meaning (e.g., ?Seven volumes
in length , it was composed by Buddhist priest Jien of the Tendai sect c. 1220.? got compressed to
?Seven volumes in length it was composed by Jien of the sect c. 1220.?). The hybrid system can create
inconsistencies when TS has split a sentence and RT removes names from only one part (?Moles can
be found in most parts of North America, Asia, and Europe, although there are no moles in Ireland.? got
simplified to ?Moles can be found in parts of America, and Asia and Europe. But, there are no moles.?).
6 Evaluation of Reading Comprehension
We also investigate, for the first time, the effect of contemporary text simplification systems on reading
comprehension for non-native speakers with a range of English skills.
Method: The test was conducted on Amazon Mechanical Turk with participants chosen from India and
paid $0.75 each. There is no method to selectively recruit low reading skill participants on Turk, so these
setting were selected to recruit non-native speakers (India) and minimise participants with postgraduate
2002
degrees (low pay). The test comprised of two components - (a) pre-test for English vocabulary skills;
and (b) a reading comprehension test to measure the effect of text simplification.
Pre-test: Reading skills are multifaceted and typically assessed through test batteries that test a range
of skills. As such there is no comprehensive assessment possible using a single short online test. As we
are recruiting non-native speakers, we chose to use the vocabulary size test (Nation and Beglar, 2007),
designed to estimate both first language and second language learners? written receptive vocabulary size
in English. The test ranks words based on their corpus frequency, and creates 14 levels, each with
1000 words, so that level 10 for example would contain the 9001
th
to 10000
th
most frequent words in
English. We designed our vocabulary test by using 28 items, 2 at each level
8
. Each word is tested by
showing a short sentence containing it and asking the participant to select the meaning of the word from
four options. An estimate of vocabulary size can be got by multiplying the score on this test by 500,
so the maximum vocabulary size estimate is 28*500=14,000. Nation and Beglar (2007) spell out three
important milestones in terms of word family vocabulary size:
5000: Minimum for Non-native speakers of non-European backgrounds to cope at English speaking Universities
8000: Critical goal for language learners to deal with a range of unsimplified language (98% coverage for newspapers)
9000: Level of non-native English speaking PhD students (98% coverage for English novels)
In addition, we asked participants to self-report their English language skills by selecting from follow-
ing options: (a) native; (b) fluent (non-native); (c) good (non-native); and (d) basic (non-native).
Main test: The reading comprehension tests were conducted using 5 news summaries chosen from the
Breaking News English
9
(BNE) website, with the permission of its creator and maintainer. The BNE
website is a resource that provides high quality news summaries at various levels of simplification for
second language learners, and has recently been nominated by the British Council for the 2014 ELTons
award for Innovation in Learner Resources. We selected five news stories which had manually con-
structed summaries at reading levels 6 (hard) and 4 (easy). The website provides a range of exercises
following each summary at level 6. We chose to use the multiple choice test to assess reading compre-
hension. For each of these summaries, we created automatically simplified texts by running our systems
on the level 6 text. This resulted in a total of five versions for each news summary - L6 (original); L4
(manual simplification); TS (automatic simplification of L6); RT (compression of L6); and HYB (RT
applied to output of TS applied to L6).
We used a balanced design where each participant would (after taking the vocabulary pre-test described
above) see each of the 5 news stories in exactly one of the 5 versions in a Latin square design. For each
comprehension test, the news summary was shown for a maximum of 150 seconds, after which it was
removed and 5 multiple choice comprehension questions presented, which was available for another 150
seconds (2.5 minutes). Participants could finish before the 150 seconds by clicking a ?finished? button.
Table 3 shows the average length of text in each version.
Results: The first row in Table 2 shows the accuracy (proportion of comprehension questions answered
correctly) on the main comprehension test for participants divided into four categories based on their
estimated vocabulary from the pre-test. We do not find any significant differences, but it appears that the
main benefits of automatic text simplification are for moderate readers (vocabulary between 5K and 8K).
We found a very poor correlation between participants? self reported English language skills and their
performance on the vocabulary test (? = ?0.01; p = 0.55). The poor correlation was due to certain
participants over-estimating their skills. Out of 50 participants, 3 rated themselves as native. However,
they could get only about 28% of the answers correct, showing the fact that the participants had over-
estimated themselves.
This caused us to doubt the reliability of our version of the vocabulary test
10
. We therefore also
attempted to categorise participants based on their overall accuracy over all 25 questions in the com-
8
The original test uses 10 words from each level, but we required a shorter version.
9
www.breakingnewsenglish.com
10
The published results are for a 140 question test taking 40 minutes, which we have had to reduce to 28 questions for
practical reasons.
2003
L4 L6 TS RT HYB L4 L6 TS RT HYB L4 L6 TS RT HYB L4 L6 TS RT HYB
Skills Excellent (Vocab?9000) Good (9000>Vocab?8000) Mod (8000>Vocab?5000) Poor (Vocab<5000)
Accuracy 0.69 0.92 0.94 0.85 0.78 0.84 0.87 0.80 0.84 0.77 0.74 0.78 0.80 0.82 0.80 0.64 0.77 0.72 0.58 0.55
Size 13 Participants 10 Participants 14 Participants 13 Participants
Skills Excellent (acc?.9) Good (.9>acc?.8) Mod (.8>acc?.5) Poor (acc<.5)
Accuracy 0.88 0.98 0.95 0.90 0.83 0.75 0.87 0.84 0.82 0.77 0.60 0.70 0.75 0.63 0.58 0.53 0.53 0.40 0.33 0.33
Size 8 Participants 31 Participants 8 Participants 3 Participants
Table 2: Results of comprehension tests: Mean accuracy (proportion of comprehension questions an-
swered correctly) by reading comprehension skills. Row 1: Participants categorised by estimated vocab-
ulary from pretest. Row 2: Participants categorised based on accuracy on comprehension tests.
Dataset Original Simplified TS RT HYB QTSG
Average words per text Wikipedia Evaluation Set 27.0 (EW) 20.4 (SEW) 25.3 22.0 20.6 24.0
Average words per text Breaking News Evaluation Set 172.6 (L6) 152.8 (L4) 184.4 149.2 151.4 -
Table 3: Effect of simplification of sentence and document lengths
prehension test. While the thresholds of 5000, 8000 and 9000 for vocabulary size are derived from the
literature, we had to set these threshold for comprehension scores. To do this in an objective (though still
arbitrary) manner, we selected thresholds numerically similar to the vocabulary size thresholds: Excel-
lent (acc ? 0.9), Good(0.9 > acc ?= 0.8), Moderate (0.8 > acc ? 0.5) and Poor (acc < 0.5).
The second row in Table 2 shows the accuracy of participants when categorised by average accu-
racy on the comprehension questions. Note that this categorisation is posthoc (though we have used
thresholds derived from the vocabulary test to be objective), and the results pertaining to this categori-
sation should be regarded as preliminary. This new categorisation based on observed reading ability,
rather than predicted language skills, throws up more definitive results. We fitted a Generalised Linear
Mixed Model (GLMM), with ?correct? answer as the (binary) dependent variable, text ?version? (L4,
L6, TS, RT, HYB) and ?comprehension? (Excellent, Good, Moderate, Poor) as the fixed effects and
participant and question as the random effects. We found a strong main effect of comprehension (com-
prehension=moderate, z = ?3.178, p = 0.001; comprehension=poor, z = ?4.858, p < 0.0001) and
a weak effect of version (version=L4, z = ?1.797, p = 0.073); i.e., these three conditions predict a
reduced accuracy on the test. We also found a weak interaction between comprehension and version
(comprehension=moderate:version=TS, z = 1.78, p = 0.075); i.e., that TS increases correct answers
for readers with moderate reading skills (p = 0.075).
Note that L4, RT and HYB all omit information through compression (Table 3 shows text lengths).
This explains the drop in comprehension for these versions, as some information needed to answer a
question might have been omitted from the summary. Note also that RT and the HYB systems are
competitive with the manual simplification L4 for moderate and good readers. Table 4 provides sample
texts to illustrate differences.
L6 The United Nations has warned that the Central African Republic (CAR) needs urgent help. The UN Deputy Secretary-
General Jan Eliasson said it was ?descending into complete chaos before our eyes?. The landlocked nation has been slowly
moving towards a state of total anarchy since rebels seized power in March.
L4 The U.N. has asked for urgent help for the Central African Republic. The UN?s Jan Eliasson said it was ?descending into
complete chaos?. There is almost a state of anarchy after rebels took power in March.
TS The United Nations has warned that the Central African Republic , CAR , needs urgent help. The UN Deputy Secretary-
General Jan Eliasson said: It was ? descending into complete chaos before our eyes ?. The landlocked nation has been
slowly moving towards a state of total anarchy. This happened since rebels seized power in March.
RT The Nations has warned that the Republic needs help. The Deputy Secretary-General Jan Eliasson said it was descending
into complete chaos before our eyes. The nation has been slowly moving towards a state of anarchy since rebels seized
power in March.
HYB The Nations has warned that the Central African Republic CAR needs urgent help. The Deputy Secretary-General Jan
Eliasson said It was descending into complete chaos before our eyes. The nation has been moving towards a state This
happened since rebels seized power.
Table 4: Example of system output to illustrate differences (Beginning of comprehension story 3).
2004
7 Conclusions
We have described and evaluated two different text simplification systems, one that performs lexical
and syntactic simplification, and another that performs sentence compression, optimised for the text
simplification task. Both systems and their combination outperform a leading contemporary system.
The evaluation of reading comprehension with non-native speakers provides preliminary results that
automatic text simplification can facilitate comprehension for moderate readers, but not for good ones.
A larger evaluation with moderate readers in necessary to confirm this. Finally we plan to make the TS
and RT systems available to the public under the Creative Commons license.
11
Acknowledgements
This research is supported by an award made by the EPSRC; award reference: EP/J018805/1.
References
Richard C Anderson and Alice Davison. 1988. Conceptual and empirical bases of readability formulas. Lawrence
Erlbaum Associates, Inc.
Richard Anderson and Peter Freebody. 1981. Vocabulary knowledge. In John Guthrie, editor, Comprehension
and Teaching: Research Reviews, pages 77?117. International Reading Association, Newark, DE.
Isabel L. Beck, Margaret G. McKeown, Gale M. Sinatra, and Jane A. Loxterman. 1991. Revising social studies
text from a text-processing perspective: Evidence of improved comprehensibility. Reading Research Quarterly,
26(3):251?276.
T. Cohn and M. Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence
Research, 34(1):637?674.
William Coster and David Kauchak. 2011. Learning to simplify sentences using wikipedia. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation, pages 1?9. Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual.
http://nlp.stanford.edu/software/dependencies_manual.pdf.
Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion
grammars. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages
541?548. Association for Computational Linguistics.
Mark Dras. 1999. Tree adjoining grammar and the reluctant paraphrasing of text. Ph.D. thesis, Macquarie
University NSW 2109 Australia.
Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.
Association for Computational Linguistics.
J. Kamalski, T. Sanders, and L. Lentz. 2008. Coherence marking, prior knowledge, and comprehension of infor-
mative and persuasive texts: Sorting things out. Discourse Processes, 45(4):323?345.
K. Knight and D. Marcu. 2000. Statistics-based summarization ? step one: Sentence compression. In Proceeding
of The American Association for Artificial Intelligence Conference (AAAI-2000), pages 703?710.
J.J. L?Allier. 1980. An evaluation study of a computer-based lesson that adjusts reading level by monitoring on
task reader characteristics. Ph.D. thesis, University of Minnesota, Minneapolis, MN.
T. Linderholm, M.G. Everson, P. van den Broek, M. Mischinski, A. Crittenden, and J. Samuels. 2000. Effects of
Causal Text Revisions on More-and Less-Skilled Readers? Comprehension of Easy and Difficult Texts. Cogni-
tion and Instruction, 18(4):525?556.
Angrosh Mandya and Advaith Siddharthan. 2014. Text simplification using synchronous dependency grammars:
Generalising automatically harvested rules. In INLG 2014 Proceedings of the Eighth International Natural Lan-
guage Generation Conference, pages 16?25, Philadelphia, PA, June. Association for Computational Linguistics.
11
For information on the availability of systems, visit us at: www.quantmedia.org/coling2014/.
2005
Shashi Narayan and Claire Gardent. 2014. Hybrid simplification using deep semantics and machine translation. In
Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics., pages 435?445, Baltimore,
MD. Association for Computational Linguistics.
I. S. P. Nation and David Beglar. 2007. A vocabulary size test. The Language Teacher, 31(7):9?13.
Tadashi Nomoto. 2008. A generic sentence trimmer with CRFs. In Proceedings of ACL-08: HLT, pages 299?307,
Columbus, Ohio, June. Association for Computational Linguistics.
Gustavo H. Paetzold and Lucia Specia. 2013. Text simplification as tree transduction. In Proceedings of the 9th
Brazilian Symposium in Information and Human Language Technology, pages 116?125.
Stefan Riezler, Tracy H. King, Richard Crouch, and Annie Zaenen. 2003. Statistical sentence condensation using
ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In Proceedings of the
Human Language Technology Conference and the 3rd Meeting of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL?03), Edmonton, Canada.
Advaith Siddharthan and Angrosh Mandya. 2014. Hybrid text simplification using synchronous dependency
grammars with hand-written and automatically harvested rules. In Proceedings of the 14th Conference of the
European Chapter of the Association for Computational Linguistics, pages 722?731, Gothenburg, Sweden,
April. Association for Computational Linguistics.
Advaith Siddharthan. 2003a. Preserving discourse structure when simplifying text. In Proceedings of the Eu-
ropean Natural Language Generation Workshop (ENLG), 11th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?03), pages 103?110, Budapest, Hungary.
Advaith Siddharthan. 2003b. Resolving pronouns robustly: Plumbing the depths of shallowness. In Proceedings
of the Workshop on Computational Treatments of Anaphora, 11th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?03), pages 7?14, Budapest, Hungary.
Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation,
4(1):77?109.
Advaith Siddharthan. 2010. Complex lexico-syntactic reformulation of sentences using typed dependency rep-
resentations. In Proceedings of the 6th International Natural Language Generation Conference (INLG 2010),
pages 125?133, Dublin, Ireland.
Advaith Siddharthan. 2011. Text simplification using typed dependencies: a comparison of the robustness of dif-
ferent generation strategies. In Proceedings of the 13th European Workshop on Natural Language Generation,
pages 2?11. Association for Computational Linguistics.
David A Smith and Jason Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of the Workshop on Statistical Machine Translation, pages 23?30. Association
for Computational Linguistics.
Lucia Specia. 2010. Translating from complex to simplified sentences. In Proceedings of the Conference on
Computational Processing of the Portuguese Language, pages 30?39. Springer.
Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, pages 409?420. Association for Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual ma-
chine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 1015?1024. Association for Computational Linguistics.
Jin Y. Yen. 1971. Finding the k shortest loopless paths in a network. Management Science, 17(11):712?716, July.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of the 23rd international conference on computational linguistics, pages
1353?1361. Association for Computational Linguistics.
2006
