Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 248?256, Prague, June 2007. c?2007 Association for Computational Linguistics
Instance Based Lexical Entailment for Ontology Population
Claudio Giuliano and Alfio Gliozzo
FBK-irst, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Trento, ITALY
{giuliano,gliozzo}@itc.it
Abstract
In this paper we propose an instance based
method for lexical entailment and apply
it to automatic ontology population from
text. The approach is fully unsupervised and
based on kernel methods. We demonstrate
the effectiveness of our technique largely
surpassing both the random and most fre-
quent baselines and outperforming current
state-of-the-art unsupervised approaches on
a benchmark ontology available in the liter-
ature.
1 Introduction
Textual entailment is formally defined as a relation-
ship between a coherent text T and a language ex-
pression, the hypothesis H . T is said to entail H ,
denoted by T ? H , if the meaning of H can be in-
ferred from the meaning of T (Dagan et al, 2005;
Dagan and Glickman., 2004). Even though this no-
tion has been recently proposed in the computational
linguistics literature, it has already attracted a great
attention due to the very high generality of its set-
tings and to the indubitable usefulness of its (poten-
tial) applications.
In this paper, we concentrate on the problem of
lexical entailment, a textual entailment subtask in
which the system is asked to decide whether the sub-
stitution of a particular word w with the word e in a
coherent text Hw = H lwHr generates a sentence
He = H leHr such that Hw ? He, where H l and
Hr denote the left and the right context of w, re-
spectively. For example, given the word ?weapon? a
system may substitute it with the synonym ?arm?, in
order to identify relevant texts that denote the sought
concept using the latter term. A particular case of
lexical entailment is recognizing synonymy, where
both Hw ? He and He ? Hw hold.
In the literature, slight variations of this problem
are also referred to as sense matching (Dagan et al,
2006), lexical reference (Glickman et al, 2006a)
and lexical substitution (Glickman et al, 2006b).
They have been applied to a wide variety of tasks,
such as semantic matching, subtitle generation and
Word Sense Disambiguation (WSD). Modeling lex-
ical entailment is also a prerequisite to approach the
SemEval-2007 lexical substitution task1, consisting
of finding alternative words that can occur in given
context.
In this paper, we propose to apply an approach for
lexical entailment to the ontology population task.
The basic idea is that if a word entails another one
in a given context then the former is an instance or
a subclass of the latter. This approach is intuitively
appealing because lexical entailment is intrinsically
an unsupervised task, therefore it does not require
lexical resources, seed examples or manually anno-
tated data sets. Unsupervised approaches are partic-
ularly suited for ontology population, whose goal is
to find instances of concepts from corpora, because
both corpus and the ontology sizes can scale up to
millions of documents and thousands of concepts,
preventing us from applying supervised learning. In
addition, the top level part of the ontology (i.e., the
Tbox in the Description Logics terminology) is very
1http://nlp.cs.swarthmore.edu/semeval/
tasks/task10/description.shtml
248
often modified during the ontology engineering life-
cycle, for example by introducing new concepts and
restructuring the subclass of hierarchy according to
the renewed application needs required by the evo-
lution of the application domain. It is evident that
to preserve the consistency between the Tbox and
the Abox (i.e., the set of instances and their rela-
tions) in such a dynamic ontology engineering pro-
cess, supervised approaches are clearly inadequate,
as small changes in the TBox will be reflected into
dramatic annotation effort to keep instances in the
Abox aligned.
The problem of populating a predefined ontol-
ogy of concepts with novel instances implies a WSD
task, as the entities in texts are ambiguous with re-
spect to the domain ontology. For example, the en-
tity Washington is both the name of a state and the
name of a city. In the ontology population settings
traditional WSD approaches cannot be directly ap-
plied since entities are not reported into dictionar-
ies, making the lexical entailment alternative more
viable. In particular, we model the problem of on-
tology population as the problem of recognizing for
each mention of an entity of a particular coarse-
grained type (e.g., location) the fine-grained con-
cept (e.g., lake or mountain) that can be substi-
tuted in texts preserving the meaning. For example,
in the sentence ?the first man to climb the Everest
without oxygen?, ?Everest? can be substituted with
the word mountain preserving the meaning, while
the sentence is meaningless when ?Everest? is re-
placed with the word lake. Following the lexical
entailment approach, the ontology population task
is transformed into the problem of recognizing the
term from a fine-grained set of categories (e.g., city,
country, river, lake and mountain) that can be substi-
tuted in the contexts where the entity is mentioned
(e.g., Everest in the example above).
The main contributions of this paper are summa-
rized as follows. First, we propose a novel approach
to lexical entailment, called Instance Based Lexi-
cal Entailment (IBLE), that allows approaching the
problem as a classification task, in which a given
target word (i.e., the entailing word) in a particu-
lar context is judged to entail a different word taken
from a (pre-defined) set of (possible) candidate en-
tailed words (see Section 3). Second, we exploit the
IBLE approach to model the ontology population
task as follows. Given a set of candidate concepts
belonging to generic ontological types (e.g., peo-
ple or locations), and a set of pre-recognized men-
tions of entities of these types in the corpus (e.g.,
Newton, Ontario), we assign the entity to the class
whose lexicalization is more frequently entailed in
the corpus. In particular, as training set to learn
the fine-grained category models, we use all the oc-
currences of their corresponding expressions in the
same corpus (e.g., we collected all occurrences in
context of the word scientist to describe the concept
scientist). Then, we apply the trained model
to classify the pre-recognized coarse-grained entities
into the fine-grained categories.
Our approach is fully unsupervised as for training
it only requires occurrences of the candidate entailed
words taken in their contexts. Restricted to the on-
tology population task, for each coarse-grained en-
tity (e.g., location), the candidate entailed words are
the terms corresponding to the fine-grained classes
(e.g., lake or mountain) and the entailing words are
mentions of entities (e.g., New York, Ontario) be-
longing to the coarse-grained class, recognized by
an entity tagger.
Experiments show that our method for recog-
nizing lexical entailment is effective for the on-
tology population task, reporting improvements
over a state-of-the-art unsupervised technique based
on contextual similarity measures (Cimiano and
Vo?lker, 2005). In addition, we also compared it to
a supervised approach (Tanev and Magnini, 2006),
that we regarded as an upper bound, obtaining com-
parable results.
2 The Ontology Population Task
Populating concepts of a predefined ontology with
instances found in a corpus is a primary goal of
knowledge management systems. As concepts in
the ontology are generally structured into hierar-
chies belonging to a common ontological type (e.g.,
people or locations), the problem of populating on-
tologies can be solved hierarchically, firstly identi-
fying instances in texts as belonging to the topmost
concepts, and then assigning them to a fine-grained
class. Supervised named entity recognition (NER)
systems can be used for accomplishing the first step.
State-of-the-art NER systems are characterized by
249
high accuracy, but they require a large amount of
training data. However, domain specific ontologies
generally contains many ?fine-grained? categories
(e.g., particular categories of people, such as writ-
ers, scientists, and so on) and, as a consequence, su-
pervised methods cannot be used because the anno-
tation costs would become prohibitive.
Therefore, in the literature, the fine-grained clas-
sification task has been approached by adopting
weakly supervised (Tanev and Magnini, 2006; Fleis-
chman and Hovy, 2002) or unsupervised methods
(Cimiano and Vo?lker, 2005). Tanev and Magnini
(2006) proposed a weakly supervised method that
requires as training data a list of terms without con-
text for each class under consideration. Such list can
be automatically acquired from existing ontologies
or other sources (i.e., database fields, web sites like
Wikipedia, etc.) since the approach imposes virtu-
ally no restrictions on them. Given a generic syntac-
tically parsed corpus containing at least each train-
ing entity twice, the algorithm learns, for each class,
a feature vector describing the contexts where those
entities occur. Then it compares the new (unknown)
entity with the so obtained feature vectors, assigning
it to the most similar class. Fleischman and Hovy
(2002) approached the ontology population problem
as a classification task, providing examples of in-
stances in their context as training examples for their
respective fine-grained categories.
The aforementioned approaches are clearly inad-
equate to recognize such fine-grained distinctions,
as they would require a time consuming and costly
annotation process for each particular class, that
is clearly infeasible when the number of concepts
in the ontology scales up. Therefore, most of the
present research in ontology population is focus-
ing on either unsupervised approaches (Cimiano
and Vo?lker, 2005) or weakly supervised approaches
(Tanev and Magnini, 2006).
Unsupervised approaches are mostly based on
term similarity metrics. Cimiano and Vo?lker (2005)
assign a particular entity to the fine-grained class
such that the contextual similarity is maximal among
the set of fine-grained subclasses of a coarse-grained
category. Contextual similarity has been measured
by adopting lexico-syntactic features provided by a
dependency parser, as proposed in (Lin, 1998).
3 Instance Based Lexical Entailment
Dagan et al (2006) adapted the classical supervised
WSD setting to approach the sense matching prob-
lem (i.e., the binary lexical entailment problem of
deciding whether a word, such as position, entails
a different word, such as job, in a given context)
by defining a one-class learning algorithm based on
support vector machines (SVM). They train a one-
class model for each entailed word (e.g., all the oc-
currences of the word job in the corpus) and, then,
apply it to classify all the occurrences of the entail-
ing words (e.g., the word position), providing a bi-
nary decision criterion2. Similarly to the WSD case,
examples are represented by feature vectors describ-
ing their contexts, and then compared to the feature
vectors describing the context of the target word.
In this paper, we adopt a similar strategy to ap-
proach a multi-class lexical entailment problem.
The basic hypothesis is that if a word w entails
e in a particular context (Hw ? He), then some
of the contexts T je in which e occurs in the train-
ing corpus are similar to Hw. Given a word w
and an (exhaustive) set of candidate entailed words
E = {e1, e2, . . . , en}, to which we refer hereafter
with the expression ?substitution lexica?, our goal is
to select the word ei ? E that can be substituted to
w in the context Hw generating a sentence He such
that Hw ? He. In the multi-class setting, super-
vised learning approaches can be used. In particular,
we can apply a one-versus-all learning methodology,
in which each class ei is trained from both positive
(i.e., all the occurrences of ei in the corpus) and neg-
ative examples (i.e., all the occurrences of the words
in the set {ej |j 6= i}).
Our approach is clearly a simplification of the
more general lexical entailment settings, where
given two generic words w and e, and a context
H = H lwHr, the system is asked to decide whether
w entails e or not. In fact, the latter is a binary
classification problem, while the former is easier as
the system is required to select ?the best? option
among the substitution lexicon. Of course providing
such set could be problematic in many cases (e.g.,
it could be incomplete or simply not available for
2This approach resembles the pseudo-words technique pro-
posed to evaluate WSD algorithms at the earlier stages of the
WSD studies (Gale et al, 1992), when large scale sense tagged
corpora were not available for training supervised algorithms.
250
many languages or rare words). On the other hand,
such a simplification is practically effective. First of
all, it allows us to provide both positive and nega-
tive examples, avoiding the use of one-class classi-
fication algorithms that in practice perform poorly
(Dagan et al, 2006). Second, the large availabil-
ity of manually constructed substitution lexica, such
as WordNet (Fellbaum, 1998), or the use of reposi-
tories based on statistical word similarities, such as
the database constructed by Lin (1998), allows us to
find an adequate substitution lexicon for each target
word in most of the cases.
For example, as shown in Table 1, the word job
has different senses depending on its context, some
of them entailing its direct hyponym position (e.g.,
?looking for permanent job?), others entailing the
word task (e.g., ?the job of repairing?). The prob-
lem of deciding whether a particular instance of job
can be replaced by position, and not by the word
place, can be solved by looking for the most simi-
lar contexts where either position or place occur in
the training data, and then selecting the class (i.e.,
the entailed word) characterized by the most similar
ones, in an instance based style. In the first example
(see row 1), the word job is strongly associated to
the word position, because the contexts of the latter
in the examples 1 and 2 are similar to the context
of the former, and not to the word task, whose con-
texts (4, 5 and 6) are radically different. On the other
hand, the second example (see row 2) of the word
job is similar to the occurrences 4 and 5 of the word
task, allowing its correct substitution.
It is worthwhile to remark that, due to the ambi-
guity of the entailed words (e.g., position could also
entail either perspective or place), not every occur-
rence of them should be taken into account, in order
to avoid misleading predictions caused by the irrele-
vant senses. Therefore, approaches based on a more
classical contextual similarity technique (Lin, 1998;
Dagan, 2000), where words are described ?globally?
by context vectors, are doomed to fail. We will pro-
vide empirical evidence of this in the evaluation sec-
tion.
Choosing an appropriate similarity function for
the contexts of the words to be substituted is a pri-
mary issue. In this work, we exploited similar-
ity functions already defined in the WSD literature,
relying on the analogy between the lexical entail-
ment and the WSD task. The state-of-the-art super-
vised WSD methodology, reporting the best results
in most of the Senseval-3 lexical sample tasks in dif-
ferent languages, is based on a combination of syn-
tagmatic and domain kernels (Gliozzo et al, 2005)
in a SVM classification framework. Therefore, we
adopted exactly the same strategy for our purposes.
A great advantage of this methodology is that it
is totally corpus based, as it does not require nei-
ther the availability of lexical databases, nor the use
of complex preprocessing steps such as parsing or
anaphora resolution, allowing us to apply it on dif-
ferent languages and domains once large corpora are
available for training. Therefore, we exploited ex-
actly the same strategy to implement the IBLE clas-
sifier required for our purposes, defining a kernel
composed by n simple kernels, each representing
a different aspect to be considered when estimating
contextual similarity among word occurrences. In
fact, by using the closure properties of the kernel
functions, it is possible to define the kernel combi-
nation schema as follows3:
KC(xi, xj) =
n?
l=1
Kl(xi, xj)
?
Kl(xj , xj)Kl(xi, xi)
, (1)
where Kl are valid kernel functions, measuring sim-
ilarity between the objects xi and xj from different
perspectives4.
One means to satisfy both the WSD and the lex-
ical entailment requirements is to consider two dif-
ferent aspects of similarity: domain aspects, mainly
related to the topic (i.e., the global context) of the
texts in which the word occurs, and syntagmatic as-
pects, concerning the lexico-syntactic pattern in the
local context. Domain aspects are captured by the
domain kernel, described in Section 3.1, while syn-
tagmatic aspects are taken into account by the syn-
tagmatic kernel, presented in Section 3.2.
3Some recent works (Zhao and Grishman, 2005; Gliozzo
et al, 2005) empirically demostrate the effectiveness of com-
bining kernels in this way, showing that the combined kernel
always improves the performance of the individual ones. In ad-
dition, this formulation allows evaluating the individual contri-
bution of each information source.
4An exhaustive discussion about kernel methods for NLP
can be found in (Shawe-Taylor and Cristianini, 2004).
251
Entailed job Training
position ... looking for permanent academic job in ... 1 ... from entry-level through permanent positions.
2 My academic position ...
3 ... put the lamp in the left position ...
task The job of repairing 4 The task of setting up ...
5 Repairing the engine is an hard task.
6 ... task based evaluation.
Table 1: IBLE example.
3.1 The Domain Kernel
(Magnini et al, 2002) claim that knowing the do-
main of the text in which the word is located is a cru-
cial information forWSD. For example the (domain)
polysemy among the Computer Science and
the Medicine senses of the word virus can be
solved by simply considering the domain of the con-
text in which it is located. Domain aspects are also
crucial in recognizing lexical entailment. For exam-
ple, the term virus entails software agent in
the Computer Science domain (e.g., ?The lap-
top has been infected by a virus?), while it entails
bacterium when located in the Medicine domain
(e.g., ?HIV is a virus?). As argued in (Magnini et
al., 2002), domain aspects can be considered by an-
alyzing the lexicon in a large context of the word
to be disambiguated, regardless of the actual word
order. We refer to (Gliozzo et al, 2005) for a de-
tailed description of the domain kernel. The sim-
plest methodology to estimate the domain similar-
ity among two texts is to represent them by means
of vectors in the Vector Space Model (VSM), and
to exploit the cosine similarity. The VSM is a k-
dimensional space Rk, in which the text tj is rep-
resented by means of the vector ~tj such that the ith
component of ~tj is the term frequency of the term
wi in it. The similarity between two texts in the
VSM is estimated by computing the cosine between
them, providing the kernel function KV SM that can
be used as a basic tool to estimate domain similarity
between texts5.
5In (Gliozzo et al, 2005), in addition to the standard VSM,
a domain kernel, exploiting external information acquired from
unlabeled data, has been also used to reduce the amount of (la-
beled) training data. Here, given that our approach is fully un-
supervised, i.e., we can obtain as many examples as we need,
we do not use the domain kernel.
3.2 The Syntagmatic Kernel
Syntagmatic aspects are probably the most impor-
tant evidence for recognizing lexical entailment. In
general, the strategy adopted to model syntagmatic
relations in WSD is to provide bigrams and trigrams
of collocated words as features to describe local con-
texts (Yarowsky, 1994). The main drawback of this
approach is that non contiguous or shifted colloca-
tions cannot be identified, decreasing the general-
ization power of the learning algorithm. For ex-
ample, suppose that the word job has to be disam-
biguated into the sentence ?. . . permanent academic
job in. . . ?, and that the occurrence ?We offer per-
manent positions. . . ? is provided for training. A
traditional feature mapping would extract the con-
text words w?1:academic, w?2:permanent
to represent the former, and w?1:permanent,
w?2:offer to index the latter. Evidently such fea-
tures will not match, leading the algorithm to a mis-
classification.
The syntagmatic kernel, proposed by Gliozzo et
al. (2005), is an attempt to solve this problem. It
is based on a gap-weighted subsequences kernel
(Shawe-Taylor and Cristianini, 2004). In the spirit
of kernel methods, this kernel is able to compare
sequences directly in the input space, avoiding any
explicit feature mapping. To perform this opera-
tion, it counts how many times a (non-contiguous)
subsequence of symbols u of length n occurs in
the input string s, and penalizes non-contiguous oc-
currences according to the number of the contained
gaps. To define our syntagmatic kernel, we adapted
the generic definition of the sequence kernels to the
problem of recognizing collocations in local word
contexts. We refer to (Giuliano et al, 2006) for a
detailed description of the syntagmatic kernel.
252
4 Lexical Entailment for Ontology
Population
In this section, we apply the IBLE technique, de-
scribed in Section 3, to recognize lexical entailment
for ontology population. To this aim, we cast ontol-
ogy population as a lexical entailment task, where
the fine-grained categories are the candidate entailed
words, and the named entities to be subcategorized
are the entailing words. Below, we present the main
steps of our algorithm in details.
Step 1 By using a state-of-the-art supervised NER
system, we recognize the named entities belonging
to a set of coarse-grained categories (e.g., location
and people) of interest for the domain.
Step 2 For all fine-grained categories belonging to
the same coarse-grained type, we extract from a do-
main corpus all the occurrences of their lexicaliza-
tions in context (e.g., for the category actor, we
extract all contexts where the term actor occurs),
and use them as input to train the IBLE classifier. In
this way, we obtain a multi-class classifier for each
ontological type. Then, we classify all the occur-
rences of the named entities recognized in the first
step. The output of this process is a list of tagged
named entities; where the elements of the list could
have been classified into different fine-grained cat-
egories even though they refer to the same phrase
(e.g., the occurrences of the entity ?Jack London?
could have been classified both as writer and
actor, depending on the contexts where they oc-
cur).
Step 3 A distinct category is finally assigned to the
entities referring to the same phrase in the list. This
is done on the basis of the tags that have been as-
signed to all its occurrences during the previous step.
To this purpose, we implemented a voting mecha-
nism. The basic idea is that an entity belongs to a
specific category if its occurrences entail a particu-
lar superclass ?more often than expected by chance?,
where the expectation is modeled on the basis of the
overall distribution of fine-grained category labels,
assigned during the second step, in the corpus. This
intuition is formalized by applying a statistical reli-
ability measure, that depends on the distribution of
positive assignments for each class, defined by the
following formula:
R(e, c) =
P (c|e)? ?c
?c
, (2)
where P (c|e) is estimated by the relative frequency
of the fine-grained class c among the different oc-
currences of the entity e, ?c and ?c measure the
mean and the standard deviation of the distribution
P (c|E), and E is an (unlabeled) training set of in-
stances of the coarse-grained type classified by the
IBLE algorithm. Finally, each entity is assigned to
the category c? such that
c? = argmax
c
R(e, c). (3)
5 Evaluation
Evaluating a lexical entailment algorithm in itself
is rather complex. Therefore, we performed a task
driven evaluation of our system, measuring its use-
fulness in an ontology population task, for which
evaluation benchmarks are available, allowing us to
compare our technique to existing state-of-the-art
approaches.
As introduced in Section 4, the ontology popu-
lation task can be modeled as a lexical entailment
problem, in which the fine-grained classes are the
entailed words and the named entities belonging to
the coarse-grained ontological type are the entailing
words.
In the following, we first introduce the experimen-
tal settings (Section 5.1). Then we evaluate our tech-
nique by comparing it to state-of-the-art unsuper-
vised approaches for ontology population (Section
5.2).
5.1 Experimental Settings
For all experiments, we adopted the evaluation
benchmark proposed in (Tanev and Magnini, 2006).
It considers two high-level named entity cate-
gories both having five fine-grained sub-classes (i.e.,
mountain, lake, river, city, and country
as subtypes of LOCATION; statesman, writer,
athlete, actor, and inventor are subtypes of
PERSON). The authors usedWordNet andWikipedia
as primary data sources for populating the evaluation
ontology. In total, the ontology is populated with
280 instances which were not ambiguous (with re-
spect to the ontology) and appeared at least twice in
253
the English CLEF corpus6. Even the evaluation task
is rather small and can be perceived as an artificial
experimental setting, it is the best available bench-
mark we can use to compare our system to existing
approaches in the literature, as we are not aware of
other available resources.
To perform NER we used CRFs (Lafferty et al,
2001). We trained a first-order CRF on the MUC
data set to annotate locations and people. In our
experiments, we used the implementation provided
in MALLET (McCallum, 2002). We used a stan-
dard feature set inspired by the literature on text
chunking and NER (Tjong Kim Sang and Buch-
holz, 2000; Tjong Kim Sang and De Meulder, 2003;
Tjong Kim Sang, 2002) to train a first-order CRFs.
Each instance is represented by encoding all the
following families of features, all time-shifted by -
2,-1,0,1,2: (a) the word itself, (b) the PoS tag of
the token, (c) orthographic predicates, such as cap-
italization, upper-case, numeric, single character,
and punctuation, (d) gazetteers of locations, people
names and organizations, (e) character-n-gram pred-
icates for 2 6 n 6 3.
As an (unsupervised) training set for the fine-
grained categories, we exploited all occurrences in
context of their corresponding terms we found in
the CLEF corpus (e.g., for the category actor we
used all the occurrences of the term actor). We did
not use any prior estimation of the class frequency,
adopting a pure unsupervised approach. Table 2
lists the fine-grained concepts and the number of
the training examples found for each of them in the
CLEF corpus.
As a reference for a comparison of the outcomes
of this study, we used the results presented in (Tanev
and Magnini, 2006) for the Class-Word and Class-
Example approaches. The Class-Word approach ex-
ploits a similarity metric between terms and con-
cepts based on the comparison of the contexts where
they appear. Details of this technique can be found
in (Cimiano and Vo?lker, 2005). Tanev and Magnini
(2006) proposed a variant of the Class-Word algo-
rithm, called Class-Example, that relies on syntactic
features extracted from corpus and uses as an addi-
tional input a set of training examples for each class.
Overall, it required 1, 194 examples to accomplish
6http://www.clef-campaign.org
this task.
All experiments were performed using the SVM
package LIBSVM7 customized to embed our own
kernel. In all the experiments, we used the default
parameter setting.
location person
mountain 1681 statesman 119
lake 730 writer 3436
river 1411 athlete 642
city 35000 actor 2356
country 15037 inventor 105
Table 2: Number of training examples for each class.
5.2 Results
Table 4 shows our results compared with two base-
lines (i.e., random and most frequent, estimated
from the test data) and the two alternative ap-
proaches for ontology population described in the
previous section. Our system outperforms both
baselines and largely surpasses the Class-Word un-
supervised method.
It is worthwhile to remark here that, being the
IBLE algorithm fully unsupervised, improving the
most frequent baseline is an excellent result, rarely
achieved in the literature on unsupervised methods
for WSD (McCarthy et al, 2004). In addition, our
system is also competitive when compared to super-
vised approaches, being it only 5 points lower than
the Class-Example method, while it does not require
seed examples and syntactic parsing. This charac-
teristic makes our system flexible and adaptable to
different languages and domains.
System Micro F1 Macro F1
RND Baseline 0.20 0.20
Class-Word 0.42 0.33
MF baseline 0.52 NA
IBLE 0.57 0.47
Class-Example 0.62 0.68
Table 3: Comparison of different ontology popula-
tion techniques.
7http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
254
Finally, we performed a disaggregated evaluation
of our system, assessing the performance for differ-
ent ontological types and different concepts. Re-
sults show that our method performs better on larger
fine-grained classes (i.e., writer and country),
while the results on smaller categories are affected
by low recall, even if the predictions provided by
the system tends to be highly accurate. Taking into
consideration that our system is fully unsupervised,
this behavior is highly desirable because it implies
that it is somehow able to identify the predominant
class. In addition the high precision on the smaller
classes can be explained by our instance based ap-
proach.
Person N Prec Rec F1
Inventor 11 1 0.18 0.31
Statesman 20 1.0 0.05 0.10
Writer 88 0.61 0.89 0.72
Actor 25 0.57 0.68 0.62
Athlete 20 1 0.1 0.18
Micro 164 0.61 0.61 0.61
Macro 5 0.83 0.38 0.52
Table 4: Performance of the IBLE approach on peo-
ple.
Location N Prec Rec F1
City 23 0.35 0.26 0.30
Country 40 0.61 0.70 0.65
River 10 0.8 0.4 0.53
Mountain 5 0.25 0.2 0.22
Lake 4 0.2 0.5 0.29
Micro 82 0.50 0.50 0.50
Macro 5 0.44 0.41 0.42
Table 5: Performance of the IBLE approach on lo-
cations.
6 Conclusions and Future Work
In this paper, we presented a novel unsupervised
technique for recognizing lexical entailment in texts,
namely instance based lexical entailment, and we
exploited it to approach an ontology population task.
The basic assumption is that if a word is entailed
by another in a given context, then some of the
contexts of the entailed word should be similar to
that of the word to be disambiguated. Our tech-
nique is effective, as it largely surpasses both the
random and most frequent baselines. In addition, it
improves over the state-of-the-art for unsupervised
approaches, achieving performances close to the su-
pervised rivaling techniques requiring hundreds of
examples for each class.
Ontology population is only one of the possible
applications of lexical entailment. For the future,
we plan to apply our instance based approach to a
wide variety of tasks, e.g., lexical substitution, word
sense disambiguation and information retrieval. In
addition, we plan to exploit our lexical entailment as
a subcomponent of a more complex system to rec-
ognize textual entailment. Finally, we are going to
explore more elaborated kernel functions to recog-
nize lexical entailment and more efficient learning
strategies to apply our method to web-size corpora.
Acknowledgments
The authors would like to thank Bernardo Magnini
and Hristo Tanev for providing the benchmark,
Ido Dagan for useful discussions and comments
regarding the connections between lexical entail-
ment and ontology population, and Alberto Lavelli
for his thorough review. Claudio Giuliano is sup-
ported by the X-Media project (http://www.
x-media-project.org), sponsored by the Eu-
ropean Commission as part of the Information So-
ciety Technologies (IST) program under EC grant
number IST-FP6-026978. Alfio Gliozzo is sup-
ported by the FIRB-Israel research project N.
RBIN045PXH.
References
Philipp Cimiano and Johanna Vo?lker. 2005. Towards
large-scale, open-domain and ontology-based named
entity classification. In Proceedings of RANLP?05,
pages 66? 166?172, Borovets, Bulgaria.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In Proceedings of the PASCAL Workshop
on LearningMethods for Text Understanding andMin-
ing, Grenoble.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
255
challenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings ACL-2006, pages 449?456, Sydney, Australia,
July.
I. Dagan. 2000. Contextual word similarity. In Rob
Dale, Hermann Moisl, and Harold Somers, editors,
Handbook of Natural Language Processing, chap-
ter 19, pages 459?476. Marcel Dekker Inc.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of ACL-2002, pages 1?7, Morristown, NJ, USA.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In R. Goldman et al, editor,
Working Notes of the AAAI Fall Symposium on Prob-
abilistic Approaches to Natural Language, pages 54?
60.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2006. Syntagmatic kernels: a word
sense disambiguation case study. In Proceedings of
the EACL-2006 Workshop on Learning Structured In-
formation in Natural Language Applications, Trento,
Italy, 5-7 April.
O. Glickman, E. Shnarch, and I. Dagan. 2006a. Lexical
reference: a semantic matching subtask. In proceed-
ings of EMNLP 2006.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006b. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of CoNLL-2006.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005. Do-
main kernels for word sense disambiguation. In Pro-
ceedings of ACL-2005, pages 403?410, Ann Arbor,
Michigan, June.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of ICML-2002, pages 282?289, Williams
College, MA. Morgan Kaufmann, San Francisco, CA.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL-98, pages 768?
774, Morristown, NJ, USA.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of ACL-2004,
Barcelona, Spain, July.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Hristo Tanev and Bernardo Magnini. 2006. Weakly su-
pervised approaches for ontology population. In Pro-
ceedings of EACL-2006, Trento, Italy.
Erik Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL-2000 shared task: Chunking.
In Proceedings of CoNLL-2000, Lisbon, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147, Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL-
2002, pages 155?158, Taipei, Taiwan.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in Span-
ish and French. In Proceedings of ACL-94, pages 88?
95, Las Cruces, New Mexico.
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of ACL 2005, Ann Arbor, Michi-
gan, June.
256
The GOD model
Alfio Massimiliano Gliozzo
ITC-irst
Trento, Italy
gliozzo@itc.it
Abstract
GOD (General Ontology Discovery) is an
unsupervised system to extract semantic
relations among domain specific entities
and concepts from texts. Operationally,
it acts as a search engine returning a set
of true predicates regarding the query in-
stead of the usual ranked list of relevant
documents. Our approach relies on two
basic assumptions: (i) paradigmatic rela-
tions can be established only among terms
in the same Semantic Domain an (ii) they
can be inferred from texts by analyzing the
Subject-Verb-Object patterns where two
domain specific terms co-occur. A quali-
tative analysis of the system output shows
that GOD provide true, informative and
meaningful relations in a very efficient
way.
1 Introduction
GOD (General Ontology Discovery) is an un-
supervised system to extract semantic relations
among domain specific entities and concepts from
texts. Operationally, it acts as a search engine re-
turning a set of true predicates regarding the query
instead of the usual ranked list of relevant docu-
ments. Such predicates can be perceived as a set
of semantic relations explaining the domain of the
query, i.e. a set of binary predicated involving do-
main specific entities and concepts. Entities and
concepts are referred to by domain specific terms,
and the relations among them are expressed by the
verbs of which they are arguments.
To illustrate the functionality of the system, be-
low we report an example for the query God.
god:
lord hear prayer
god is creator
god have mercy
faith reverences god
lord have mercy
jesus_christ is god
god banishing him
god commanded israelites
god was trinity
abraham believed god
god requires abraham
god supply human_need
god is holy
noah obeyed god
From a different perspective, GOD is first of all
a general system for ontology learning from texts
(Buitelaar et al, 2005). Likewise current state-
of-the-art methodologies for non-hierarchical re-
lation extraction it exploits shallow parsing tech-
niques to identify syntactic patterns involving do-
main specific entities (Reinberger et al, 2004),
and statistical association measures to detect rel-
evant relations (Ciaramita et al, 2005). In con-
trast to them, it does not require any domain spe-
cific collection of texts, allowing the user to de-
scribe the domain of interest by simply typing
short queries. This feature is of great advantage
from a practical point of view: it is obviously more
easy to formulate short queries than to collect huge
amounts of domain specific texts.
Even if, in principle, an ontology is supposed to
represent a domain by a hierarchy of concepts and
entities, in this paper we concentrate only on the
non-hyrarchical relation extraction process. In ad-
dition, in this work we do not address the problem
of associating synonyms to the same concept (e.g.
god and lord in the example above).
147
In this paper we just concentrate on describ-
ing our general framework for ontology learning,
postponing the solution of the already mentioned
problems. The good quality of the results and the
well foundedness of the GOD framework motivate
our future work.
2 The GOD algorithm
The basic assumption of the GOD model is that
paradigmatic relations can be established only
among terms in the same Semantic Domain, while
concepts belonging to different fields are mainly
unrelated (Gliozzo, 2005). Such relations can
be identified by considering Subject-Verb-Object
(SVO) patterns involving domain specific terms
(i.e. syntagmatic relations).
When a query Q = (q1, q2, . . . , qn) is formu-
lated, GOD operates as follows:
Domain Discovery Retrieve the ranked list
dom(Q) = (t1, t2, . . . , tk) of domain spe-
cific terms such that sim(ti, Q) > ??, where
sim(Q, t) is a similarity function capturing
domain proximity and ?? is the domain
specificity threshold.
Relation Extraction For each SVO pattern in-
volving two different terms ti ? dom(Q) and
tj ? dom(Q) such that the term ti occurs in
the subject position and the term tj occurs in
the object position return the relation tivtj if
score(ti, v, tj) > ???, where score(ti, v, tj)
measures the syntagmatic association among
ti, v and tj .
In Subsection 2.1 we describe into details the
Domain Discovery step. Subsection 2.2 is about
the relation extraction step.
2.1 Domain Discovery
Semantic Domains (Magnini et al, 2002) are clus-
ters of very closely related concepts, lexicalized
by domain specific terms. Word senses are de-
termined and delimited only by the meanings of
other words in the same domain. Words belonging
to a limited number of domains are called domain
words. Domain words can be disambiguated by
simply identifying the domain of the text.
As a consequence, concepts belonging to dif-
ferent domains are basically unrelated. This ob-
servation is crucial from a methodological point
of view, allowing us to perform a large scale struc-
tural analysis of the whole lexicon of a language,
otherwise computationally infeasible. In fact, re-
stricting the attention to a particular domain is a
way to reduce the complexity of the overall rela-
tion extraction task, that is evidently quadratic in
the number of terms.
Domain information can be expressed by ex-
ploiting Domain Models (DMs) (Gliozzo et al,
2005). A DM is represented by a k ? k? rectan-
gular matrix D, containing the domain relevance
for each term with respect to each domain, where
k is the cardinality of the vocabulary, and k? is the
size of the Domain Set.
DMs can be acquired from texts in a totally
unsupervised way by exploiting a lexical coher-
ence assumption (Gliozzo, 2005). To this aim,
term clustering algorithms can be adopted: each
cluster represents a Semantic Domain. The de-
gree of association among terms and clusters, es-
timated by the learning algorithm, provides a do-
main relevance function. For our experiments we
adopted a clustering strategy based on Latent Se-
mantic Analysis, following the methodology de-
scribed in (Gliozzo, 2005). This operation is done
off-line, and can be efficiently performed on large
corpora. To filter out noise, we considered only
those terms having a frequency higher than 5 in
the corpus.
Once a DM has been defined by the matrix D,
the Domain Space is a k? dimensional space, in
which both texts and terms are associated to Do-
main Vectors (DVs), i.e. vectors representing their
domain relevances with respect to each domain.
The DV ~t?i for the term ti ? V is the ith row of D,
where V = {t1, t2, . . . , tk} is the vocabulary of
the corpus. The similarity among DVs in the Do-
main Space is estimated by means of the cosine
operation.
When a query Q = (q1, q2, . . . , qn) is formu-
lated, its DV ~Q? is estimated by
~Q? =
n
?
j=1
~q?j (1)
and then compared to the DVs of each term ti ? V
by adopting the cosine similarity metric
sim(ti, Q) = cos(~t?i, ~Q?) (2)
where ~t?i and ~q?j are the DVs for the terms ti and
qj , respectively.
All those terms whose similarity with the query
is above the domain specificity threshold ?? are
148
then returned as an output of the function dom(Q).
Empirically, we fixed this threshold to 0.5. In gen-
eral, the higher the domain specificity threshold,
the higher the relevance of the discovered relations
for the query (see Section 3), increasing accuracy
while reducing recall. In the previous example,
dom(god) returns the terms lord, prayer, creator
and mercy, among the others.
2.2 Relation extraction
As a second step, the system analyzes all the syn-
tagmatic relations involving the retrieved entities.
To this aim, as an off-line learning step, the sys-
tem acquires Subject-Verb-Object (SVO) patterns
from the training corpus by using regular expres-
sions on the output of a shallow parser.
In particular, GOD extracts the relations tivtj
for each ordered couple of domain specific terms
(ti, tj) such that ti ? dom(Q), tj ? dom(Q)
and score(ti, v, tj) > ???. The confidence score
is estimated by adopting the heuristic confidence
measure described in (Reinberger et al, 2004), re-
ported below:
score(ti, v, tj) =
F (ti,v,tj)
min(F (ti),F (tj))
F (ti,v)
F (ti) +
F (v,tj)
F (tj)
(3)
where F (t) is the frequency of the term t in the
corpus, F (t, v) is the frequency of the SV pattern
involving both t and v, F (v, t) is the frequency
of the VO pattern involving both v and t, and
F (ti, v, tj) is the frequency of the SVO pattern in-
volving ti, v and tj . In general, augmenting ??? is a
way to filter out noisy relations, while decreasing
recall.
It is important to remark here that all the ex-
tracted predicates occur at least once in the corpus,
then they have been asserted somewhere. Even if
it is not a sufficient condition to guarantee their
truth, it is reasonable to assume that most of the
sentences in texts express true assertions.
The relation extraction process is performed on-
line for each query, then efficiency is a crucial re-
quirement in this phase. It would be preferable
to avoid an extensive search of the required SVO
patterns, because the number of sentences in the
corpus is huge. To solve this problem we adopted
an inverted relation index, consisting of three hash
tables: the SV(VO) table report, for each term,
the frequency of the SV(VO) patterns where it oc-
curs as a subject(object); the SVO table reports,
for each ordered couple of terms in the corpus,
the frequency of the SVO patterns in which they
co-occur. All the information required to estimate
Formula 3 can then be accessed in a time propor-
tional to the frequencies of the involved terms. In
general, domain specific terms are not very fre-
quent in a generic corpus, allowing a fast compu-
tation in most of the cases.
3 Evaluation
Performing a rigorous evaluation of an ontology
learning process is not an easy task (Buitelaar et
al., 2005) and it is outside the goals of this paper.
Due to time constraints, we did not performed a
quantitative and objective evaluation of our sys-
tem. In Subsection 3.1 we describe the data and
the NLP tools adopted by the system. In Subsec-
tion 3.2 we comment some example of the system
output, providing a qualitative analysis of the re-
sults after having proposed some evaluation guide-
lines. Finally, in Subsection 3.3 we discuss issues
related to the recall of the system.
3.1 Experimental Settings
To expect high coverage, the system would be
trained on WEB scale corpora. On the other hand,
the analysis of very large corpora needs efficient
preprocessing tools and optimized memory allo-
cation strategies. For the experiments reported in
this paper we adopted the British National Cor-
pus (BNC-Consortium, 2000), and we parsed each
sentence by exploiting a shallow parser on the out-
put of which we detected SVO patterns by means
of regular expressions1.
3.2 Accuracy
Once a query has been formulated, and a set of
relations has been extracted, it is not clear how to
evaluate the quality of the results. The first four
columns of the example below show the evaluation
we did for the query Karl Marx.
Karl Marx:
TRIM economic_organisation determines superstructure
TRUM capitalism needs capitalists
FRIM proletariat overthrow bourgeoisie
TRIM marx understood capitalism
???E marx later marxists
TRIM labour_power be production
TRIM societies are class_societies
?RIM private_property equals exploitation
TRIM primitive_societies were classless
TRIM social_relationships form economic_basis
TRIM max_weber criticised marxist_view
1For the experiments reported in this paper we used a
memory-based shallow parser developed at CNTS Antwerp
and ILK Tilburg (Daelemans et al, 1999) together with a set
of scripts to extract SVO patterns (Reinberger et al, 2004)
kindly put at our disposal by the authors.
149
TRIM contradictions legitimizes class_structure
?R?E societies is political_level
?R?E class_society where false_consciousness
?RUE social_system containing such_contradictions
TRIM human_societies organizing production
Several aspects are addressed: truthfulness (i.e.
True vs. False in the first column), relevance
for the query (i.e. Relevant vs. Not-relevant in
the second column), information content (i.e. In-
formative vs. Uninformative, third column) and
meaningfulness (i.e. Meaningful vs. Error, fourth
column). For most of the test queries, the majority
of the retrieved predicates were true, relevant, in-
formative and meaningful, confirming the quality
of the acquired DM and the validity of the relation
extraction technique2.
From the BNC, GOD was able to extract good
quality information for many different queries in
very different domains, as for example music,
unix, painting and many others.
3.3 Recall
An interesting aspect of the behavior of the system
is that if the domain of the query is not well rep-
resented in the corpus, the domain discovery step
retrieves few domain specific terms. As a conse-
quece, just few relations (and sometimes no re-
lations) have been retrieved for most of our test
queries. An analysis of such cases showed that the
low recall was mainly due to the low coverage of
the BNC corpus. We believe that this problem can
be avoided by training the system on larger scale
corpora (e.g. from the Web).
4 Conclusion and future work
In this paper we reported the preliminary results
we obtained from the development of GOD, a
system that dynamically acquires ontologies from
texts. In the GOD model, the required domain is
formulated by typing short queries in an Informa-
tion Retrieval style. The system is efficient and
accurate, even if the small size of the corpus pre-
vented us from acquiring domain ontologies for
many queries. For the future, we plan to evaluate
the system in a more rigorous way, by contrast-
ing its output to hand made reference ontologies
for different domains. To improve the coverage of
the system, we are going to train it on WEB scale
2It is worthwhile to remark here that evaluation strongly
depends on the point of view from which the query has
been formulated. For example, the predicate private property
equals exploitation is true in the Marxist view, while it is ob-
viously false with respect to the present economic system.
text collections and to explore the use of super-
vised relation extraction techniques. In addition,
we are improving relation extraction by adopting
a more sophisticated syntactic analisys (e.g. Se-
matic Role Labeling). Finally, we plan to explore
the usefulness of the extracted relations into NLP
systems for Question Answering, Information Ex-
traction and Semantic Entailment.
Acknowledgments
This work has been supported by the ONTOTEXT
project, funded by the Autonomous Province of
Trento under the FUP-2004 research program.
Most of the experiments have been performed
during my research stage at the University of
Antwerp. Thanks to Walter Daelemans and Carlo
Strapparava for useful suggestions and comments
and to Marie-Laure Reinberger for having pro-
vided the SVO extraction scripts.
References
BNC-Consortium. 2000. British national corpus.
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric, and
I. Rojas. 2005. Unsupervised learning of seman-
tic relations between concepts of a molecular biol-
ogy ontology. In In proceedings of IJCAI-05, Edim-
burgh, Scotland.
W. Daelemans, S. Buchholz, and J. Veenstra. 1999.
Memory-based shallow parsing. In Proceedings of
CoNLL-99.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain kernels for word sense disambiguation. In
Proceedings of ACL-05, pages 403?410, Ann Arbor,
Michigan.
A. Gliozzo. 2005. Semantic Domains in Compu-
tational Linguistics. Ph.D. thesis, University of
Trento.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain information
in word sense disambiguation. Natural Language
Engineering, 8(4):359?373.
M.L. Reinberger, P. Spyns, A. J. Pretorius, and
W. Daelemans. 2004. Automatic initiation of an on-
tology. In Proceedings of ODBase?04, pages 600?
617. Springer-Verlag.
150
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 129?136, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Investigating Unsupervised Learning
for Text Categorization Bootstrapping
Alfio Gliozzo and Carlo Strapparava
ITC-irst
Istituto per la Ricerca Scientifica e Tecnologica
I-38050 Trento, Italy
{gliozzo,strappa}@itc.it
Ido Dagan
Computer Science Department
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
We propose a generalized bootstrapping
algorithm in which categories are de-
scribed by relevant seed features. Our
method introduces two unsupervised steps
that improve the initial categorization step
of the bootstrapping scheme: (i) using La-
tent Semantic space to obtain a general-
ized similarity measure between instances
and features, and (ii) the Gaussian Mixture
algorithm, to obtain uniform classification
probabilities for unlabeled examples. The
algorithm was evaluated on two Text Cate-
gorization tasks and obtained state-of-the-
art performance using only the category
names as initial seeds.
1 Introduction
Supervised classification is the task of assigning cat-
egory labels, taken from a predefined set of cate-
gories (classes), to instances in a data set. Within the
classical supervised learning paradigm, the task is
approached by providing a learning algorithm with
a training data set of manually labeled examples. In
practice it is not always easy to apply this schema
to NLP tasks. For example supervised systems for
Text Categorization (TC) require a large amount of
hand labeled texts, while in many applicative cases
it is quite difficult to collect the required amounts of
hand labeled data. Unlabeled text collections, on the
other hand, are in general easily available.
An alternative approach is to provide the neces-
sary supervision by means of sets of ?seeds? of in-
tuitively relevant features. Adopting terminology
from computability theory, we refer to the stan-
dard example-based supervision mode as Exten-
sional Learning (EL), as classes are being specified
by means of examples of their elements (their ex-
tension). Feature-based supervision is referred to as
Intensional Learning (IL), as features may often be
perceived as describing the intension of a category,
such as providing the name or prominent key terms
for a category in text categorization.
The IL approach reflects on classical rule-based
classification methods, where the user is expected
to specify exact classification rules that operate in
the feature space. Within the machine learning
paradigm, IL has been incorporated as a technique
for bootstrapping an extensional learning algorithm,
as in (Yarowsky, 1995; Collins and Singer, 1999;
Liu et al, 2004). This way the user does not
need to specify exact classification rules (and fea-
ture weights), but rather perform a somewhat sim-
pler task of specifying few typical seed features for
the category. Given the list of seed features, the
bootstrapping scheme consists of (i) preliminary un-
supervised categorization of the unlabeled data set
based on the seed features, and (ii) training an (ex-
tensional) supervised classifier using the automatic
classification labels of step (i) as the training data
(the second step is possibly reiterated, such as by
an Expectation-Maximization schema). The core
part of IL bootstrapping is step (i), i.e. the initial
unsupervised classification of the unlabeled dataset.
This step was often approached by relatively sim-
ple methods, which are doomed to obtain mediocre
quality. Even so, it is hoped that the second step of
supervised training would be robust enough to the
noise in the initial training set.
129
The goal of this paper is to investigate additional
principled unsupervised mechanisms within the ini-
tial classification step, applied to the text catego-
rization. In particular, (a) utilizing a Latent Se-
mantic Space to obtain better similarity assessments
between seeds and examples, and (b) applying a
Gaussian Mixture (GM) algorithm, which provides a
principled unsupervised estimation of classification
probability. As shown in our experiments, incor-
porating these steps consistently improved the ac-
curacy of the initial categorization step, which in
turn yielded a better final classifier thanks to the
more accurate training set. Most importantly, we ob-
tained comparable or better performance than previ-
ous IL methods using only the category names as the
seed features; other IL methods required collecting
a larger number of seed terms, which turns out to be
a somewhat tricky task.
Interesting results were revealed when compar-
ing our IL method to a state-of-the-art extensional
classifier, trained on manually labeled documents.
The EL classifier required 70 (Reuters dataset) or
160 (Newsgroup dataset) documents per category to
achieve the same performance that IL obtained using
only the category names. These results suggest that
IL may provide an appealing cost-effective alterna-
tive when sub-optimal accuracy suffices, or when it
is too costly or impractical to obtain sufficient la-
beled training. Optimal combination of extensional
and intensional supervision is raised as a challeng-
ing topic for future research.
2 Bootstrapping for Text Categorization
The TC task is to assign category labels to docu-
ments. In the IL setting, a category Ci is described
by providing a set of relevant features, termed an
intensional description (ID), idci ? V , where V
is the vocabulary. In addition a training corpus
T = {t1, t2, . . . tn} of unlabeled texts is provided.
Evaluation is performed on a separate test corpus
of labeled documents, to which standard evaluation
metrics can be applied.
The approach of categorizing texts based on lists
of keywords has been attempted rather rarely in the
literature (McCallum and Nigam, 1999; Ko and Seo,
2000; Liu et al, 2004; Ko and Seo, 2004). Several
names have been proposed for it ? such as TC by
bootstrapping with keywords, unsupervised TC, TC
by labelling words ? where the proposed methods
fall (mostly) within the IL settings described here1.
It is possible to recognize a common structure of
these works, based on a typical bootstrap schema
(Yarowsky, 1995; Collins and Singer, 1999):
Step 1: Initial unsupervised categorization. This
step was approached by applying some similar-
ity criterion between the initial category seed
and each unlabeled document. Similarity may
be determined as a binary criterion, consider-
ing each seed keyword as a classification rule
(McCallum and Nigam, 1999), or by applying
an IR style vector similarity measure. The re-
sult of this step is an initial categorization of (a
subset of) the unlabeled documents. In (Ko and
Seo, 2004) term similarity techniques were ex-
ploited to expand the set of seed keywords, in
order to improve the quality of the initial cate-
gorization.
Step 2: Train a supervised classifier on the ini-
tially categorized set. The output of Step
1 is exploited to train an (extensional) su-
pervised classifier. Different learning algo-
rithms have been tested, including SVM, Naive
Bayes, Nearest Neighbors, and Rocchio. Some
works (McCallum and Nigam, 1999; Liu et
al., 2004) performed an additional Expectation
Maximization algorithm over the training data,
but reported rather small incremental improve-
ments that do not seem to justify the additional
effort.
(McCallum and Nigam, 1999) reported catego-
rization results close to human agreement on the
same task. (Liu et al, 2004) and (Ko and Seo,
2004) contrasted their word-based TC algorithm
with the performance of an extensional supervised
algorithm, achieving comparable results, while in
general somewhat lower. It should be noted that it
has been more difficult to define a common evalua-
tion framework for comparing IL algorithms for TC,
due to the subjective selection of seed IDs and to the
lack of common IL test sets (see Section 4).
1The major exception is the work in (Ko and Seo, 2004),
which largely follows the IL scheme but then makes use of la-
beled data to perform a chi-square based feature selection be-
fore starting the bootstrap process. This clearly falls outside the
IL setting, making their results incomparable to other IL meth-
ods.
130
3 Incorporating Unsupervised Learning
into Bootstrap Schema
In this section we show how the core Step 1 of the IL
scheme ? the initial categorization ? can be boosted
by two unsupervised techniques. These techniques
fit the IL setting and address major constraints of it.
The first is exploiting a generalized similarity metric
between category seeds (IDs) and instances, which
is defined in a Latent Semantic space. Applying
such unsupervised similarity enables to enhance the
amount of information that is exploited from each
seed feature, aiming to reduce the number of needed
seeds. The second technique applies the unsuper-
vised Gaussian Mixture algorithm, which maps sim-
ilarity scores to a principled classification probabil-
ity value. This step enables to obtain a uniform scale
of classification scores across all categories, which
is typically obtained only through calibration over
labeled examples in extensional learning.
3.1 Similarity in Latent Semantic Space
As explained above, Step 1 of the IL scheme as-
sesses a degree of ?match? between the seed terms
and a classified document. It is possible first to
follow the intuitively appealing and principled ap-
proach of (Liu et al, 2004), in which IDs (category
seeds) and instances are represented by vectors in a
usual IR-style Vector Space Model (VSM), and sim-
ilarity is measured by the cosine function:
simvsm(idci , tj) = cos (~idci , ~tj) (1)
where ~idci ? R|V | and ~tj ? R|V | are the vectorial
representations in the space R|V | respectively of the
category ID idci and the instance tj , and V is the set
of all the features (the vocabulary).
However, representing seeds and instances in a
standard feature space is severely affected in the IL
setting by feature sparseness. In general IDs are
composed by short lists of features, possibly just
a single feature. Due to data sparseness, most in-
stances do not contain any feature in common with
any category?s ID, which makes the seeds irrelevant
for most instances (documents in the text categoriza-
tion case). Furthermore, applying direct matching
only for a few seed terms is often too crude, as it ig-
nores the identity of the other terms in the document.
The above problems may be reduced by consid-
ering some form of similarity in the feature space,
as it enables to compare additional document terms
with the original seeds. As mentioned in Section
2, (Ko and Seo, 2004) expanded explicitly the orig-
inal category IDs with more terms, using a con-
crete query expansion scheme. We preferred using a
generalized similarity measure based on represent-
ing features and instances a Latent Semantic (LSI)
space (Deerwester et al, 1990). The dimensions of
the Latent Semantic space are the most explicative
principal components of the feature-by-instance ma-
trix that describes the unlabeled data set. In LSI
both coherent features (i.e. features that often co-
occur in the same instances) and coherent instances
(i.e. instances that share coherent features) are rep-
resented by similar vectors in the reduced dimen-
sionality space. As a result, a document would be
considered similar to a category ID if the seed terms
and the document terms tend to co-occur overall in
the given corpus.
The Latent Semantic Vectors for IDs and docu-
ments were calculated by an empirically effective
variation (self-reference omitted for anonymity) of
the pseudo-document methodology to fold-in docu-
ments, originally suggested in (Berry, 1992). The
similarity function simlsi is computed by the cosine
metric, following formula 1, where ~idci and ~tj are
replaced by their Latent Semantic vectors. As will
be shown in section 4.2, using such non sparse rep-
resentation allows to drastically reduce the number
of seeds while improving significantly the recall of
the initial categorization step.
3.2 The Gaussian Mixture Algorithm and the
initial classification step
Once having a similarity function between category
IDs and instances, a simple strategy is to base the
classification decision (of Step 1) directly on the
obtained similarity values (as in (Liu et al, 2004),
for example). Typically, IL works adopt in Step 1
a single-label classification approach, and classify
each instance (document) to only one category. The
chosen category is the one whose ID is most simi-
lar to the classified instance amongst all categories,
which does not require any threshold tuning over la-
beled examples. The subsequent training in Step 2
yields a standard EL classifier, which can then be
used to assign multiple categories to a document.
Using directly the output of the similarity func-
tion for classification is problematic, because the ob-
tained scales of similarity values vary substantially
across different categories. The variability in sim-
131
ilarity value ranges is caused by variations in the
number of seed terms per category and the levels of
their generality and ambiguity. As a consequence,
choosing the class with the highest absolute similar-
ity value to the instance often leads to selecting a
category whose similarity values tend to be gener-
ally higher, while another category could have been
more similar to the classified instance if normalized
similarity values were used.
As a solution we propose using an algorithm
based on unsupervised estimation of Gaussian Mix-
tures (GM), which differentiates relevant and non-
relevant category information using statistics from
unlabeled instances. We recall that mixture mod-
els have been widely used in pattern recognition and
statistics to approximate probability distributions. In
particular, a well-known nonparametric method for
density estimation is the so-called Kernel Method
(Silverman, 1986), which approximates an unknow
density with a mixture of kernel functions, such as
gaussians functions. Under mild regularity condi-
tions of the unknown density function, it can be
shown that mixtures of gaussians converge, in a sta-
tistical sense, to any distribution.
More formally, let ti ? T be an instance described
by a vector of features ~ti ? R|V | and let idci ? V
be the ID of category Ci; let sim(idci , tj) ? R be
a similarity function among instances and IDs, with
the only expectation that it monotonically increases
according to the ?closeness? of idci and tj (see Sec-
tion 3.1).
For each category Ci, GM induces a mapping
from the similarity scores between its ID and any
instance tj , sim(idci , tj), into the probability of Ci
given the text tj , P (Ci|tj). To achieve this goal GM
performs the following operations: (i) it computes
the set Si = {sim(idci , tj)|tj ? T} of the sim-
ilarity scores between the ID idci of the category
Ci and all the instances tj in the unlabeled train-
ing set T ; (ii) it induces from the empirical distri-
bution of values in Si a Gaussian Mixture distribu-
tion which is composed of two ?hypothetic? distri-
butions Ci and Ci, which are assumed to describe re-
spectively the distributions of similarity scores for
positive and negative examples; and (iii) it estimates
the conditional probability P (Ci|sim(idci , tj)) by
applying the Bayes theorem on the distributions Ci
and Ci. These steps are explained in more detail be-
low.
The core idea of the algorithm is in step (ii). Since
we do not have labeled training examples we can
only obtain the set Si which includes the similar-
ity scores for all examples together, both positive
and negative. We assume, however, that similar-
ity scores that correspond to positive examples are
drawn from one distribution, P (sim(idci , tj)|Ci),
while the similarity scores that correspond to neg-
ative examples are drawn from another distribution,
P (sim(idci , tj)|Ci). The observed distribution of
similarity values in Si is thus assumed to be a mix-
ture of the above two distributions, which are recov-
ered by the GM estimation.
Figure 1 illustrates the mapping induced by GM
from the empirical mixture distribution: dotted lines
describe the Probability Density Functions (PDFs)
estimated by GM for Ci, Ci, and their mixture from
the empirical distribution (Si) (in step (ii)). The
continuous line is the mapping induced in step (iii)
of the algorithm from similarity scores between in-
stances and IDs (x axis) to the probability of the in-
stance to belong to the category (y axis).
0
1
2
3
4
5
6
7
8
9
-0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
GM-score
Ci gaussian (relevant)
Ci gaussian (not relevant)Mixture
Similarity Score
Pro
bab
ility
 / P
DF
Figure 1: Mapping induced by GM for the category
rec.motorcycles in the 20newsgroups data set.
The probabilistic mapping estimated in step (iii)
for a category Ci given an instance tj is computed
by applying Bayes rule:
P (Ci|tj) = P (Ci|sim(idci , tj)) = (2)
=
P (sim(idci ,tj)|Ci)P (Ci)
P (sim(idci ,tj)|Ci)P (Ci)+P (sim(Ci,tj)|Ci)P (Ci)
where P (sim(idci , tj)|Ci) is the value of
the PDF of Ci at the point sim(idci , tj),
P (sim(idci , tj)|Ci) is the value of the PDF of Ci at
the same point, P (Ci) is the area of the distribution
132
Ci and P (Ci) is the area of the distribution Ci. The
mean and variance parameters of the two distribu-
tions Ci and Ci, used to evaluate equation 2, are esti-
mated by the rather simple application of the Expec-
tation Maximization (EM) algorithm for Gaussian
Mixtures, as summarized in (Gliozzo et al, 2004).
Finally, following the single-labeled categoriza-
tion setting of Step 1 in the IL scheme, the most
likely category is assigned to each instance, that is,
argmaxCiP (Ci|tj).
3.3 Summary of the Bootstrapping Algorithm
step 1.a: Latent Semantic Space. Instances and
Intensional Descriptions of categories (the seeds) are
represented by vectors in Latent Semantic space. As
an option, the algorithm can work with the classi-
cal Vector Space Model using the original feature
space. Similarity scores between IDs and instances
are computed by the Cosine measure.
step 1.b: GM. The mapping functions P (Ci|tj)
for each category, conditioned on instances tj , are
induced by the GM algorithm. To that end, an Ex-
pectation Maximization algorithm estimates the pa-
rameters of the two component distributions of the
observed mixture, which correspond to the distribu-
tions of similarity values for positive and negative
examples. As an option, the GM mapping can be
avoided.
step 1.c: Categorization. Each instance
is classified to the most probable category -
argmaxCiP (Ci|tj).
step 2: Bootstrapping an extensional classifier.
An EL classifier (SVM) is trained on the set of la-
beled instances resulting from step 1.c.
4 Evaluation
4.1 Intensional Text Categorization Datasets
Even though some typical data sets have been used
in the TC literature (Sebastiani, 2002), the datasets
used for IL learning were not standard. Often there
is not sufficient clarity regarding details such as the
exact version of the corpus used and the training/test
splitting. Furthermore, the choice of categories was
often not standard: (Ko and Seo, 2004) omitted 4
categories from the 20-Newsgroup dataset, while
(Liu et al, 2004) evaluated their method on 4 sepa-
rate subsets of the 20-Newsgroups, each containing
only 4-5 categories. Such issues make it rather diffi-
cult to compare thoroughly different techniques, yet
we have conducted several comparisons in Subsec-
tion 4.5 below. In the remainder of this Subsection
we clearly state the corpora used in our experiments
and the pre-processing steps performed on them.
20newsgroups. The 20 Newsgroups data set is
a collection of newsgroup documents, partitioned
(nearly) evenly across 20 different newsgroups. As
suggested in the dataset Web site2, we used the
?bydate? version: the corpus (18941 documents)
is sorted by date and divided in advance into a
training (60%) set and a chronologically follow-
ing test set (40%) (so there is no randomness in
train/test set selection), it does not include cross-
posts (duplicates), and (more importantly) does not
include non-textual newsgroup-identifying headers
which often help classification (Xref, Newsgroups,
Path, Followup-To, Date).
We will first report results using initial seeds
for the category ID?s, which were selected using
only the words in the category names, with some
trivial transformations (i.e. cryptography#n
for the category sci.crypt, x-windows#n
for the category comp.windows.x). We
also tried to avoid ?overlapping? seeds, i.e.
for the categories rec.sport.baseball
and rec.sport.hockey the seeds are only
{baseball#n} and {hockey#n} respec-
tively and not {sport#n, baseball#n} and
{sport#n, hockey#n}3.
Reuters-10. We used the top 10 categories
(Reuters-10) in the Reuters-21578 collection
Apte` split4. The complete Reuters collection
includes 12,902 documents for 90 categories,
with a fixed splitting between training and test
data (70/30%). Both the Apte` and Apte`-10
splits are often used in TC tasks, as surveyed
in (Sebastiani, 2002). To obtain the Reuters-10
2The collection is available at
www.ai.mit.edu/people/jrennie/20Newsgroups.
3One could propose as a guideline for seed selection
those seeds that maximize their distances in the LSI vec-
tor space model. On this perspective the LSI vectors
built from {sport#n, baseball#n} and {sport#n,
hockey#n} are closer than the vectors that represent
{baseball#n} and {hockey#n}. It may be noticed that
this is a reason for the slight initial performance decrease in the
learning curve in Figure 2 below.
4available at http://kdd.ics.uci.edu/databases/-
reuters21578/reuters21578.html).
133
Apte` split we selected the 10 most frequent cate-
gories: Earn, Acquisition, Money-fx,
Grain, Crude, Trade, Interest,
Ship, Wheat and Corn. The final data set
includes 9296 documents. The initial seeds are only
the words appearing in the category names.
Pre-processing. In both data sets we tagged the
texts for part-of-speech and represented the docu-
ments by the frequency of each pos-tagged lemma,
considering only nouns, verbs, adjectives, and ad-
verbs. We induced the Latent Semantic Space from
the training part5 and consider the first 400 dimen-
sions.
4.2 The impact of LSI similarity and GM on IL
performance
In this section we evaluate the incremental impact
of LSI similarity and the GM algorithm on IL per-
formance. When avoiding both techniques the algo-
rithm uses the simple cosine-based method over the
original feature space, which can be considered as a
baseline (similar to the method of (Liu et al, 2004)).
We report first results using only the names of the
categories as initial seeds.
Table 1 displays the F1 measure for the 20news-
groups and Reuters data sets, with and without LSI
and with and without GM. The performance figures
show the incremental benefit of both LSI and GM. In
particular, when starting with just initial seeds and
do not exploit the LSI similarity mechanism, then
the performance is heavily penalized.
As mentioned above, the bootstrapping step of the
algorithm (Step 2) exploits the initially classified in-
stances to train a supervised text categorization clas-
sifier based on Support Vector Machines. It is worth-
while noting that the increment of performance after
bootstrapping is generally higher when GM and LSI
are incorporated, thanks to the higher quality of the
initial categorization which was used for training.
4.3 Learning curves for the number of seeds
This experiment evaluates accuracy change as a
function of the number of initial seeds. The ex-
5From a machine learning point of view, we could run the
LSA on the full corpus (i.e. training and test), the LSA being a
completely unsupervised technique (i.e. it does not take into ac-
count the data annotation). However, from an applicative point
of view it is much more sensible to have the LSA built on the
training part only. If we run the LSA on the full corpus, the
performance figures increase in about 4 points.
Reuters 20 Newsgroups
LSI GM F1 F1
no no 0.38 0.25
+ bootstrap 0.42 0.28
no yes 0.41 0.30
+ bootstrap 0.46 0.34
yes no 0.46 0.50
+ bootstrap 0.47 0.53
yes yes 0.58 0.60
+ bootstrap 0.74 0.65
Table 1: Impact of LSI vector space and GM
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
1 5 10 15 20
F1
number of seeds (1 means only the category names)
LSI VSMClassical VSM
Figure 2: Learning curves on initial seeds for 20
newsgroups, LSI and Classical VSM (no LSI)
periment was performed for the 20 newsgroups cor-
pus using both the LSI and the Classical vector
space model. Additional seeds, beyond the cate-
gory names, were identified by two lexicographers.
For each category, the lexicographers were provided
with a list of 100 seeds produced by the LSI similar-
ity function applied to the category name (one list of
100 candidate terms for each category). From these
lists the lexicographers selected the words that were
judged as significantly related to the respective cat-
egory, picking a mean of 40 seeds per category.
As seen in Figure 2, the learning curve using
LSI vector space model dramatically outperforms
the one using classical vector space. As can be
expected, when using the original vector space (no
generalization) the curve improves quickly with a
few more terms. More surprisingly, with LSI sim-
ilarity the best performance is obtained using the
minimal initial seeds of the category names, while
adding more seeds degrades performance. This
might suggest that category names tend to be highly
134
indicative for the intensional meaning of the cate-
gory, and therefore adding more terms introduces
additional noise. Further research is needed to find
out whether other methods for selecting additional
seed terms might yield incremental improvements.
The current results, though, emphasize the bene-
fit of utilizing LSI and GM. These techniques ob-
tain state-of-the-art performance (see comparisons
in Section 4.5) using only the category names as
seeds, allowing us to skip the quite tricky phase of
collecting manually a larger number of seeds.
4.4 Extensional vs. Intensional Learning
A major point of comparison between IL and EL is
the amount of supervision effort required to obtain a
certain level of performance. To this end we trained
a supervised classifier based on Support Vector Ma-
chines, and draw its learning curves as a function
of percentage of the training set size (Figure 3). In
the case of 20newsgroups, to achieve the 65% F1
performance of IL the supervised settings requires
about 3200 documents (about 160 texts per cate-
gory), while our IL method requires only the cate-
gory name. Reuters-10 is an easier corpus, there-
fore EL achieves rather rapidly a high performance.
But even here using just the category name is equal
on average to labeling 70 documents per-category
(700 in total). These results suggest that IL may pro-
vide an appealing cost-effective alternative in prac-
tical settings when sub-optimal accuracy suffices, or
when it is too costly or impractical to obtain suffi-
cient amounts of labeled training sets.
It should also be stressed that when using the
complete labeled training corpus state-of-the-art EL
outperforms our best IL performance. This result
deviates from the flavor of previous IL literature,
which reported almost comparable performance rel-
ative to EL. As mentioned earlier, the method of (Ko
and Seo, 2004) (as we understand it) utilizes labeled
examples for feature selection, and therefore cannot
be compared with our strict IL setting. As for the
results in (Liu et al, 2004), we conjecture that their
comparable performance for IL and EL may not be
sufficiently general, for several reasons: the easier
classification task (4 subsets of 20-Newsgroups of
4-5 categories each); the use of the usually weaker
Naive-Bayes as the EL device; the use of cluster-
ing as an aid for selecting the seed terms from the
20-Newsgroup subsets, which might not scale up
well when applied to a large number of categories
of varying size.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
Percentage of training
20 NewsgroupsReuters
3200 docs
700 docs
Figure 3: Extensional learning curves on as percent-
age of the training set.
4.5 Comparisons with other algorithms
As mentioned earlier it is not easy to conduct a thor-
ough comparison with other algorithms in the litera-
ture. Most IL data sets used for training and evalua-
tion are either not available (McCallum and Nigam,
1999) or are composed by somewhat arbitrary sub-
sets of a standard data set. Another crucial aspect
is the particular choice of the seed terms selected to
compose an ID, which affects significantly the over-
all performance of the algorithm.
As a baseline system, we implemented a rule
based approach in the spirit of (McCallum and
Nigam, 1999). It is based on two steps. First, all
the documents in the unlabeled training corpus con-
taining at least one word in common with one and
only one category ID are assigned to the respective
class. Second, a supervised classifier based on SVM
is trained on the labeled examples. Finally, the su-
pervised classifier is used to perform the final cate-
gorization step on the test corpus. Table 2 reports
the F1 measure of our replication of this method, us-
ing the category name as seed, which is substantially
lower than the performance of the method we pre-
sented in this paper.
Reuters 20 Newsgroups
0.34 0.30
+ bootstrap 0.42 0.47
Table 2: Rule-based baseline performance
135
We also tried to replicate two of the non-standard
data sets used in (Liu et al, 2004)6. Table 3 displays
the performance of our approach in comparison to
the results reported in (Liu et al, 2004). Follow-
ing the evaluation metric adopted in that paper we
report here accuracy instead of F1. For each data
set (Liu et al, 2004) reported several results vary-
ing the number of seed words (from 5 to 30), as well
as varying some heuristic thresholds, so in the ta-
ble we report their best results. Notably, our method
obtained comparable accuracy by using just the cat-
egory name as ID for each class instead of multiple
seed terms. This result suggests that our method en-
ables to avoid the somewhat fuzzy process of col-
lecting manually a substantial number of additional
seed words.
Our IDs per cat. Liu et al IDs per cat.
REC 0.94 1 0.95 5
TALK 0.80 1 0.80 20
Table 3: Accuracy on 4 ?REC? and 4 ?TALK? news-
groups categories
5 Conclusions
We presented a general bootstrapping algorithm for
Intensional Learning. The algorithm can be applied
to any categorization problem in which categories
are described by initial sets of discriminative fea-
tures and an unlabeled training data set is provided.
Our algorithm utilizes a generalized similarity mea-
sure based on Latent Semantic Spaces and a Gaus-
sian Mixture algorithm as a principled method to
scale similarity scores into probabilities. Both tech-
niques address inherent limitations of the IL setting,
and leverage unsupervised information from an un-
labeled corpus.
We applied and evaluated our algorithm on some
text categorization tasks and showed the contribu-
tion of the two techniques. In particular, we obtain,
for the first time, competitive performance using
only the category names as initial seeds. This mini-
mal information per category, when exploited by the
IL algorithm, is shown to be equivalent to labeling
about 70-160 training documents per-category for
state of the art extensional learning. Future work is
6We used sequential splitting (70/30) rather than random
splitting and did not apply any feature selection. This setting
might be somewhat more difficult than the original one.
needed to investigate optimal procedures for collect-
ing seed features and to find out whether additional
seeds might still contribute to better performance.
Furthermore, it may be very interesting to explore
optimal combinations of intensional and extensional
supervision, provided by the user in the forms of
seed features and labeled examples.
Acknowledgments
This work was developed under the collaboration
ITC-irst/University of Haifa.
References
M. Berry. 1992. Large-scale sparse singular value com-
putations. International Journal of Supercomputer
Applications, 6(1):13?49.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proc. of EMNLP99,
College Park, MD, USA.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
Y. Ko and J. Seo. 2000. Automatic text categorization by
unsupervised learning. In Proc. of COLING?2000.
Y. Ko and J. Seo. 2004. Learning with unlabeled data
for text categorization using bootstrapping abd fea-
ture projection techniques. In Proc. of the ACL-04,
Barcelona, Spain, July.
B. Liu, X. Li, W. S. Lee, and P. S. Yu. 2004. Text clas-
sification by labeling words. In Proc. of AAAI-04, San
Jose, July.
A. McCallum and K. Nigam. 1999. Text classification
by bootstrapping with keywords, em and shrinkage. In
ACL99 - Workshop for Unsupervised Learning in Nat-
ural Language Processing.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
B. W. Silverman. 1986. Density Estimation for Statistics
and Data Analysis. Chapman and Hall.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL-95, pages 189?196, Cambridge, MA.
136
Proceedings of NAACL HLT 2007, pages 131?138,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Domain Restriction Hypothesis:
Relating Term Similarity and Semantic Consistency
Alfio Massimiliano Gliozzo
ITC-irst
Trento, Italy
gliozzo@itc.it
Marco Pennacchiotti
University of Rome Tor Vergata
Rome, Italy
pennacchiotti@info.uniroma2.it
Patrick Pantel
USC, Information Sciences Institute
Marina del Rey, CA
pantel@isi.edu
Abstract
In this paper, we empirically demonstrate
what we call the domain restriction hy-
pothesis, claiming that semantically re-
lated terms extracted from a corpus tend
to be semantically coherent. We apply
this hypothesis to define a post-processing
module for the output of Espresso, a state
of the art relation extraction system, show-
ing that irrelevant and erroneous relations
can be filtered out by our module, in-
creasing the precision of the final output.
Results are confirmed by both quantita-
tive and qualitative analyses, showing that
very high precision can be reached.
1 Introduction
Relation extraction is a fundamental step in
many natural language processing applications such
as learning ontologies from texts (Buitelaar et
al., 2005) and Question Answering (Pasca and
Harabagiu, 2001).
The most common approach for acquiring con-
cepts, instances and relations is to harvest semantic
knowledge from texts. These techniques have been
largely explored and today they achieve reasonable
accuracy. Harvested lexical resources, such as con-
cept lists (Pantel and Lin, 2002), facts (Etzioni et
al., 2002) and semantic relations (Pantel and Pen-
nacchiotti, 2006) could be then successfully used in
different frameworks and applications.
The state of the art technology for relation extrac-
tion primarily relies on pattern-based approaches
(Snow et al, 2006). These techniques are based on
the recognition of the typical patterns that express
a particular relation in text (e.g. ?X such as Y?
usually expresses an is-a relation). Yet, text-based
algorithms for relation extraction, in particular
pattern-based algorithms, still suffer from a number
of limitations due to complexities of natural lan-
guage, some of which we describe below.
Irrelevant relations. These are valid relations
that are not of interest in the domain at hand. For
example, in a political domain, ?Condoleezza Rice
is a football fan? is not as relevant as ?Condoleezza
Rice is the Secretary of State of the United States?.
Irrelevant relations are ubiquitous, and affect ontol-
ogy reliability, if used to populate it, as the relation
drives the wrong type of ontological knowledge.
Erroneous or false relations. These are particu-
larly harmful, since they directly affect algorithm
precision. A pattern-based relation extraction
algorithm is particularly likely to extract erroneous
relations if it uses generic patterns, which are
defined in (Pantel and Pennacchiotti, 2006) as
broad coverage, noisy patterns with high recall and
low precision (e.g. ?X of Y? for part-of relation).
Harvesting algorithms either ignore generic patterns
(Hearst, 1992) (affecting system recall) or use man-
ually supervised filtering approaches (Girju et al,
2006) or use completely unsupervised Web-filtering
methods (Pantel and Pennacchiotti, 2006). Yet,
these methods still do not sufficiently mitigate the
problem of erroneous relations.
Background knowledge. Another aspect that
makes relation harvesting difficult is related to the
131
nature of semantic relations: relations among enti-
ties are mostly paradigmatic (de Saussure, 1922),
and are usually established in absentia (i.e., they are
not made explicit in text). According to Eco?s posi-
tion (Eco, 1979), the background knowledge (e.g.
?persons are humans?) is often assumed by the
writer, and thus is not explicitly mentioned in text.
In some cases, such widely-known relations can be
captured by distributional similarity techniques but
not by pattern-based approaches.
Metaphorical language. Even when paradigmatic
relations are explicitly expressed in texts, it can
be very difficult to distinguish between facts and
metaphoric usage (e.g. the expression ?My mind is
a pearl? occurs 17 times on the Web, but it is clear
that mind is not a pearl, at least from an ontological
perspective).
The considerations above outline some of the dif-
ficulties of taking a purely lexico-syntactic approach
to relation extraction. Pragmatic issues (background
knowledge and metaphorical language) and onto-
logical issues (irrelevant relation) can not be solved
at the syntactic level. Also, erroneous relations can
always arise. These considerations lead us to the
intuition that extraction can benefit from imposing
some additional constraints.
In this paper, we integrate Espresso with a lex-
ical distribution technique modeling semantic co-
herence through semantic domains (Magnini et al,
2002). These are defined as common discourse top-
ics which demonstrate lexical coherence, such as
ECONOMICS or POLITICS. We explore whether se-
mantic domains can provide the needed additional
constraints to mitigate the acceptance of erroneous
relations. At the lexical level, semantic domains
identify clusters of (domain) paradigmatically re-
lated terms. We believe that the main advantage of
adopting semantic domains in relation extraction is
that relations are established mainly among terms in
the same Domain, while concepts belonging to dif-
ferent fields are mostly unrelated (Gliozzo, 2005),
as described in Section 2. For example, in a chem-
istry domain, an is-a will tend to relate only terms of
that domain (e.g., nitrogen is-a element), while out-
of-domain relations are likely to be erroneous e.g.,
driver is-a element.
By integrating pattern-based and distributional ap-
proaches we aim to capture the two characteristic
properties of semantic relations:
? Syntagmatic properties: if two terms X and
Y are in a given relation, they tend to co-
occur in texts, and are mostly connected by spe-
cific lexical-syntactic patterns (e.g., the patter
?X is a Y ? connects terms in is-a relations).
This aspect is captured using a pattern-based
approach.
? Domain properties: if a semantic relation
among two terms X and Y holds, both X
and Y should belong to the same semantic
domain (i.e. they are semantically coherent),
where semantic domains are sets of terms
characterized by very similar distributional
properties in a (possibly domain specific)
corpus.
In Section 2, we develop the concept of semantic do-
main and an automatic acquisition procedure based
on Latent Semantic Analysis (LSA) and we provide
empirical evidence of the connection between rela-
tion extraction and domain modelling. Section 3 de-
scribes the Espresso system. Section 4 concerns our
integration of semantic domains and Espresso. In
Section 5, we evaluate the impact of our LSA do-
main restriction module on improving a state of the
art relation extraction system. In Section 6 we draw
some interesting research directions opened by our
work.
2 Semantic Domains
Semantic domains are common areas of human
discussion, which demonstrate lexical coherence,
such as ECONOMICS, POLITICS, LAW, SCIENCE,
(Magnini et al, 2002). At the lexical level, se-
mantic domains identify clusters of (domain) related
lexical-concepts, i.e. sets of highly paradigmatically
related words also known as Semantic Fields.
In the literature, semantic domains have been
inferred from corpora by adopting term clustering
methodologies (Gliozzo, 2005), and have been used
for several NLP tasks, such as Text Categorization
and Ontology Learning (Gliozzo, 2006).
Semantic domains can be described by Domain
Models (DMs) (Gliozzo, 2005). A DM is a com-
132
putational model for semantic domains, that repre-
sents domain information at the term level, by defin-
ing a set of term clusters. Each cluster represents a
Semantic Domain, i.e. a set of terms that often co-
occur in texts having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of a Domain Model
DMs can be acquired from texts in a completely
unsupervised way by exploiting a lexical coherence
assumption. To this end, term clustering algorithms
can be used with each cluster representing a Se-
mantic Domain. The degree of association among
terms and clusters, estimated by the learning algo-
rithm, provides a domain relevance function. For
our experiments we adopted a clustering strategy
based on LSA (Deerwester et al, 1990), following
the methodology described in (Gliozzo, 2005). The
input of the LSA process is a term-by-document ma-
trix T reporting the term frequencies in the whole
corpus for each term. The matrix is decomposed by
means of a Singular Value Decomposition (SVD),
identifying the principal components of T. This op-
eration is done off-line, and can be efficiently per-
formed on large corpora. SVD decomposes T into
three matrixes T ' V?k?UT where ?k? is the di-
agonal k ? k matrix containing the highest k? ? k
eigenvalues of T on the diagonal, and all the re-
maining elements are 0. The parameter k? is the
dimensionality of the domain and can be fixed in
advance1. Under this setting we define the domain
matrix DLSA2 as
DLSA = INV
?
?k? (1)
where IN is a diagonal matrix such that iNi,i =
1
q
? ~w?i, ~w?i?
and ~w?i is the ith row of the matrix V
??k? .
1It is not clear how to choose the right dimensionality. In
our experiments we used 100 dimensions.
2Details of this operation can be found in (Gliozzo, 2005).
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevancies with respect to each domain. The DV
~t?i for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the corpus.
The domain similarity ?d(ti, tj) among terms is then
estimated by the cosine among their corresponding
DVs in the Domain Space, defined as follows:
?d(ti, tj) = ?
~ti, ~tj??
?~ti, ~ti??~tj , ~tj?
(2)
Figure 1: Probability of finding paradigmatic rela-
tions
The main advantage of adopting semantic do-
mains for relation extraction is that they allow us to
impose a domain restriction on the set of candidate
pairs of related terms. In fact, semantic relations can
be established mainly among terms in the same Se-
mantic Domain, while concepts belonging to differ-
ent fields are mostly unrelated.
To show the validity of the domain restriction we
conducted a preliminary experiment, contrasting the
probability for two words to be related in Word-
Net (Magnini and Cavaglia`, 2000) with their domain
similarity, measured in the Domain Space induced
from the British National Corpus. In particular, for
each couple of words, we estimated the domain sim-
ilarity, and we collected word pairs in sets charac-
terized by different ranges of similarity (e.g. all the
pairs between 0.8 and 0.9). Then we estimated the
133
probability of each couple of words in different sets
to be linked by a semantic relation in WordNet, such
as synonymy, hyperonymy, co-hyponymy and do-
main in WordNet Domains (Magnini et al, 2002).
Results in Figure 1 show a monotonic crescent rela-
tion between these two quantities. In particular the
probability for two words to be related tends to 0
when their similarity is negative (i.e., they are not
domain related), supporting the basic hypothesis of
this work. In Section 4 we will show that this prop-
erty can be used to improve the overall performances
of the relation extraction algorithm.
3 The pattern-based Espresso system
Espresso (Pantel and Pennacchiotti, 2006) is a
corpus-based general purpose, broad, and accurate
relation extraction algorithm requiring minimal su-
pervision, whose core is based on the framework
adopted in (Hearst, 1992). Espresso introduces two
main innovations that guarantee high performance:
(i) a principled measure for estimating the reliabil-
ity of relational patterns and instances; (ii) an algo-
rithm for exploiting generic patterns. Generic pat-
terns are broad coverage noisy patterns (high recall
and low precision), e.g. ?X of Y? for the part-of re-
lation. As underlined in the introduction, previous
algorithms either required significant manual work
to make use of generic patterns, or simply ignore
them. Espresso exploits an unsupervised Web-based
filtering method to detect generic patterns and to dis-
tinguish their correct and incorrect instances.
Given a specific relation (e.g. is-a) and a POS-
tagged corpus, Espresso takes as input few seed
instances (e.g. nitrogen is-a element) or seed surface
patterns (e.g. X/NN such/JJ as/IN Y/NN). It then
incrementally learns new patterns and instances
by iterating on the following three phases, until a
specific stop condition is met (i.e., new patterns are
below a pre-defined threshold of reliability).
Pattern Induction. Given an input set of seed
instances I , Espresso infers new patterns connecting
as many instances as possible in the given corpus.
To do so, Espresso uses a slight modification of the
state of the art algorithm described in (Ravichandran
and Hovy, 2002). For each instance in input, the
sentences containing it are first retrieved and then
generalized, by replacing term expressions with a
terminological label using regular expressions on
the POS-tags. This generalization allows to ease
the problem of data sparseness in small corpora.
Unfortunately, as patterns become more generic,
they are more prone to low precision.
Pattern Ranking and Selection. Espresso ranks
all extracted patterns using a reliability measure rpi
and discards all but the top-k P patterns, where k is
set to the number of patterns from the previous iter-
ation plus one. rpi captures the intuition that a reli-
able pattern is one that is both highly precise and one
that extracts many instances. rpi is formally defined
as the average strength of association between a pat-
tern p and each input instance i in I , weighted by the
reliability r? of the instance i (described later):
rpi(p) =
?
i?I
(
pmi(i,p)
maxpmi ? r?(i)
)
|I|
where pmi(i, p) is the pointwise mutual information
(pmi) between i and p (estimated with Maximum
Likelihood Estimation), and maxpmi is the maxi-
mum pmi between all patterns and all instances.
Instance Extraction, Ranking, Selection.
Espresso extracts from the corpus the set of in-
stances I matching the patterns in P . In this phase
generic patterns are detected, and their instances
are filtered, using a technique described in detail in
(Pantel and Pennacchiotti, 2006). Instances are then
ranked using a reliability measure r?, similar to that
adopted for patterns. A reliable instance should be
highly associated with as many reliable patterns as
possible:
r?(i) =
?
p?P
(
pmi(i,p)
maxpmi ? rpi(i)
)
|P |
Finally, the best scoring instances are selected for
the following iteration. If the number of extracted
instances is too low (as often happens in small
corpora) Espresso enters an expansion phase, in
which instances are expanded by using web based
and syntactic techniques.
134
The output Espresso is a list of instances
i = (X,Y ) ? I , ranked according to r?(i). This
score accounts for the syntagmatic similarity be-
tween X and Y , i.e., how strong is the co-occurrence
of X and Y in texts with a given pattern p.
A key role in the Espresso algorithm is played
by the reliability measures. The accuracy of the
whole extraction process is in fact highly sensitive
to the ranking of patterns and instances because, at
each iteration, only the best scoring entities are re-
tained. For instance, if an erroneous instance is se-
lected after the first iteration, it could in theory af-
fect the following pattern extraction phase and cause
drift in consequent iterations. This issue is criti-
cal for generic patterns (where precision is still a
problem, even with Web-based filtering), and could
sometimes also affect non-generic patterns.
It would be then useful to integrate Espresso with
a technique able to retain only very precise in-
stances, without compromising recall. As syntag-
matic strategies are already in place, another strategy
is needed. In the next Section, we show how this can
be achieved using instance domain information.
4 Integrating syntagmatic and domain
information
The strategy of integrating syntagmatic and do-
main information has demonstrated to be fruitful in
many NLP tasks, such as Word Sense Disambigua-
tion and open domain Ontology Learning (Gliozzo,
2006). According to the structural view (de Saus-
sure, 1922), both aspects contribute to determine
the linguistic value (i.e. the meaning) of words:
the meaning of lexical constituents is determined
by a complex network of semantic relations among
words. This suggests that relation extraction can
benefit from accounting for both syntagmatic and
domain aspects at the same time.
To demonstrate the validity of this claim we can
explore many different integration schemata. For ex-
ample we can restrict the search space (i.e. the set of
candidate instances) to the set of all those terms be-
longing to the same domain. Another possibility is
to exploit a similarity metric for domain relatedness
to re-rank the output instances I of Espresso, hoping
that the top ranked ones will mostly be those which
are correct. One advantage of this latter method-
ology is that it can be applied to the output of any
relation extraction system without any modification
to the system itself. In addition, this methodology
can be evaluated by adopting standard Information
Retrieval (IR) measures, such as mean average pre-
cision (see Section 5). Because of these advantages,
we decided to adopt the re-ranking procedure.
The procedure is defined as follows: each in-
stance extracted by Espresso is assigned a Domain
Similarity score ?d(X,Y ) estimated in the domain
space according to Equation 2; a higher score is
then assigned to the instances that tend to co-occur
in the same documents in the corpus. For exam-
ple, the candidate instances ethanol is-a nonaro-
matic alcohol has a higher score than ethanol is-a
something, as ethanol and alcohol are both from the
chemistry domain, while something is a generic term
and is thus not associated to any domain.
Instances are then re-ranked according to
?d(X,Y ), which is used as the new index of
reliability instead of the original reliability scores
of Espresso. In Subsection 5.2 we will show that
the re-ranking technique improves the original
reliability scores of Espresso.
5 Evaluation
In this Section we evaluate the benefits of applying
the domain information to relation extraction (ESP-
LSA), by measuring the improvements of Espresso
due to domain based re-ranking.
5.1 Experimental Settings
As a baseline system, we used the ESP- implemen-
tation of Espresso described in (Pantel and Pennac-
chiotti, 2006). ESP- is a fully functioning Espresso
system, without the generic pattern filtering module
(ESP+). We decided to use ESP- for two main rea-
sons. First, the manual evaluation process would
have been too time consuming, as ESP+ extracts
thousands of relations. Also, the small scale experi-
ment for EXP- allows us to better analyse and com-
pare the results.
To perform the re-ranking operation, we acquired
a Domain Model from the input corpus itself. To this
aim we performed a SVD of the term by document
matrix T describing the input corpus, indexing all
the candidate terms recognized by Espresso.
135
As an evaluation benchmark, we adopted the
same instance sets extracted by ESP- in the ex-
periment described in (Pantel and Pennacchiotti,
2006). We used an input corpus of 313,590 words,
a college chemistry textbook (Brown et al 2003),
pre-processed using the Alembic Workbench POS-
tagger (Day et al 1997). We considered the fol-
lowing relations: is-a, part-of, reaction (a relation
of chemical reaction among chemical entities) and
production (a process or chemical element/object
producing a result). ESP- extracted 200 is-a, 111
part-of, 40 reaction and 196 production instances.
5.2 Quantitative Analysis
The experimental evaluation compared the accuracy
of the ranked set of instances extracted by ESP- with
the re-ranking produced on these instances by ESP-
LSA. By analogy to IR, we are interested in ex-
tracting positive instances (i.e. semantically related
words). Accordingly, we utilize the standard defi-
nitions of precision and recall typically used in IR .
Table 2 reports the Mean Average Precision obtained
by both ESP- and ESP-LSA on the extracted rela-
tions, showing the substantial improvements on all
the relations due to domain based re-ranking.
ESP- ESP-LSA
is-a 0.54 0.75 (+0.21)
part-of 0.65 0.82 (+0.17)
react 0.75 0.82 (+0.07)
produce 0.55 0.62 (+0.07)
Table 2: Mean Average Precision reported by ESP-
and ESP-LSA
Figures 2, 3, 4 and 5 report the precision/recall
curves obtained for each relation, estimated by mea-
suring the precision / recall at each point of the
ranked list. Results show that precision is very high
especially for the top ranked relations extracted by
ESP-LSA. Precision reaches the upper bound for the
top ranked part of the part-of relation, while it is
close to 0.9 for the is-a relation. In all cases, the
precision reported by the ESP-LSA system surpass
those of the ESP- system at all recall points.
5.3 Qualitative Analysis
Table 3 shows the best scoring instances for ESP-
and ESP-LSA on the evaluated relations. Results
Figure 2: Syntagmatic vs. Domain ranking for the
is-a relation
Figure 3: Syntagmatic vs. Domain ranking for the
produce relation
show that ESP-LSA tends to assign a much lower
score to erroneous instances, as compared to the
original Espresso reliability ranking. For exam-
ple for the part-of relation, the ESP- ranks the er-
roneous instance geometry part-of ion in 23th po-
sition, while ESP-LSA re-ranks it in 92nd. In
this case, a lower score is assigned because ge-
ometry is not particularly tied to the domain of
chemistry. Also, ESP-LSA tends to penalize in-
stances derived from parsing/tokenization errors:
136
Figure 4: Syntagmatic vs. Domain ranking for the
part-of relation
Figure 5: Syntagmatic vs. Domain ranking for the
react relation
] binary hydrogen compounds hydrogen react ele-
ments is 16th for ESP-, while in the last tenth of
the ESP-LSA. In addition, out-of-domain relations
are successfully interpreted by ESP-LSA. For ex-
ample, the instance sentences part-of exceptions is
a possibly correct relation, but unrelated to the do-
main, as an exception in chemistry has nothing to
do with sentences. This instance lies at the bottom
of the ESP-LSA ranking, while is in the middle of
ESP- list. Also, low ranked and correct relations ex-
tracted by ESP- emerge with ESP-LSA. For exam-
ple, magnesium metal react elemental oxygen lies at
the end of ESP- rank, as there are not enough syntag-
matic evidence (co-occurrences) that let the instance
emerge. The domain analysis of ESP-LSA promotes
this instance to the 2nd rank position. However, in
few cases, the strategy adopted by ESP-LSA tends
to promote erroneous instances (e.g. high voltage
produce voltage). Yet, results show that these are
isolated cases.
6 Conclusion and future work
In this paper, we propose the domain restriction hy-
pothesis, claiming that semantically related terms
extracted from a corpus tend to be semantically co-
herent. Applying this hypothesis, we presented a
new method to improve the precision of pattern-
based relation extraction algorithms, where the inte-
gration of domain information allows the system to
filter out many irrelevant relations, erroneous can-
didate pairs and metaphorical language relational
expressions, while capturing the assumed knowl-
edge required to discover paradigmatic associations
among terms. Experimental evidences supports this
claim both qualitatively and quantitatively, opening
a promising research direction, that we plan to ex-
plore much more in depth. In the future, we plan
to compare LSA to other term similarity measures,
to train the LSA model on large open domain cor-
pora and to apply our technique to both generic and
specific corpora in different domains. We want also
to increase the level of integration of the LSA tech-
nique in the Espresso algorithm, by using LSA as an
alternative reliability measure at each iteration. We
will also explore the domain restriction property of
semantic domains to develop open domain ontology
learning systems, as proposed in (Gliozzo, 2006).
The domain restriction hypothesis has potential
to greatly impact many applications where match-
ing textual expressions is a primary component. It is
our hope that by combining existing ranking strate-
gies in applications such as information retrieval,
question answering, information extraction and doc-
ument classification, with knowledge of the coher-
ence of the underlying text, one will see significant
improvements in matching accuracy.
137
Relation ESP- ESP - LSA
X is-a Y Aluminum ; metal F ; electronegative atoms
nitride ion ; strong Br O ; electronegative atoms
heat flow ; calorimeter NaCN ; cyanide salt
complete ionic equation ; spectator NaCN ; cyanide salts
X part-of Y elements ; compound amino acid building blocks ; tripeptide
composition ; substance acid building blocks ; tripeptide
blocks ; tripeptide powdered zinc metal ; battery
elements ; sodium chloride building blocks ; tripeptide
X react Y hydrazine ; water magnesium metal ; elemental oxygen
magnesium metal ; hydrochloric acid nitrogen ; ammonia
magnesium ; oxygen sodium metal ; chloride
magnesium metal ; acid carbon dioxide ; methane
X produce Y bromine ; bromide high voltage ; voltage
oxygen ; oxide reactions ; reactions
common fuels ; dioxide dr jekyll ; hyde
kidneys ; stones yellow pigments ; green pigment
Table 3: Top scoring relations extracted by ESP- and ESP-LSA.
Acknowledgments
Thanks to Roberto Basili for his precious comments,
suggestions and support. Alfio Gliozzo was sup-
ported by the OntoText project, funded by the Au-
tonomous Province of Trento under the FUP-2004,
and the FIRB founded project N.RBIN045PXH.
References
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
F. de Saussure. 1922. Cours de linguistique ge?ne?rale.
Payot, Paris.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
U. Eco. 1979. Lector in fabula. Bompiani.
O. Etzioni, M.J. Cafarella, D. Downey, A.-M
A.M. Popescu, T. Shaked, S. Soderland, D.S.
Weld, and A. Yates. 2002. Unsupervised named-
entity extraction from the web: An experimental
study. Artificial Intelligence, 165(1):91?143.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proceedings of HLT/NAACL-
03, pages 80?87, Edmonton, Canada, July.
A. Gliozzo. 2005. Semantic Domains in Computational
Linguistics. Ph.D. thesis, University of Trento.
A. Gliozzo. 2006. The god model. In Proceedings of
EACL.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics.
Nantes, France.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413?1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM Conference on
Knowledge Discovery and Data Mining, pages 613?
619.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL-COLING-06, pages 113?
120, Sydney, Australia.
M. Pasca and S. Harabagiu. 2001. The informative role
of wordnet in open-domain question answering. In
Proceedings of NAACL-01 Workshop on WordNet and
Other Lexical Resources, pages 138?143, Pittsburgh,
PA.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL-02, pages 41?47, Philadelphia, PA.
R. Snow, D. Jurafsky, and A.Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proceedings of the ACL/COLING-06, pages 801?808,
Sydney, Australia.
138
Proceedings of the 43rd Annual Meeting of the ACL, pages 403?410,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Domain Kernels for Word Sense Disambiguation
Alfio Gliozzo and Claudio Giuliano and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica
I-38050, Trento, ITALY
{gliozzo,giuliano,strappa}@itc.it
Abstract
In this paper we present a supervised
Word Sense Disambiguation methodol-
ogy, that exploits kernel methods to model
sense distinctions. In particular a combi-
nation of kernel functions is adopted to
estimate independently both syntagmatic
and domain similarity. We defined a ker-
nel function, namely the Domain Kernel,
that allowed us to plug ?external knowl-
edge? into the supervised learning pro-
cess. External knowledge is acquired from
unlabeled data in a totally unsupervised
way, and it is represented by means of Do-
main Models. We evaluated our method-
ology on several lexical sample tasks in
different languages, outperforming sig-
nificantly the state-of-the-art for each of
them, while reducing the amount of la-
beled training data required for learning.
1 Introduction
The main limitation of many supervised approaches
for Natural Language Processing (NLP) is the lack
of available annotated training data. This problem is
known as the Knowledge Acquisition Bottleneck.
To reach high accuracy, state-of-the-art systems
for Word Sense Disambiguation (WSD) are de-
signed according to a supervised learning frame-
work, in which the disambiguation of each word
in the lexicon is performed by constructing a dif-
ferent classifier. A large set of sense tagged exam-
ples is then required to train each classifier. This
methodology is called word expert approach (Small,
1980; Yarowsky and Florian, 2002). However this
is clearly unfeasible for all-words WSD tasks, in
which all the words of an open text should be dis-
ambiguated.
On the other hand, the word expert approach
works very well for lexical sample WSD tasks (i.e.
tasks in which it is required to disambiguate only
those words for which enough training data is pro-
vided). As the original rationale of the lexical sam-
ple tasks was to define a clear experimental settings
to enhance the comprehension of WSD, they should
be considered as preceding exercises to all-words
tasks. However this is not the actual case. Algo-
rithms designed for lexical sample WSD are often
based on pure supervision and hence ?data hungry?.
We think that lexical sample WSD should regain
its original explorative role and possibly use a min-
imal amount of training data, exploiting instead ex-
ternal knowledge acquired in an unsupervised way
to reach the actual state-of-the-art performance.
By the way, minimal supervision is the basis
of state-of-the-art systems for all-words tasks (e.g.
(Mihalcea and Faruque, 2004; Decadt et al, 2004)),
that are trained on small sense tagged corpora (e.g.
SemCor), in which few examples for a subset of the
ambiguous words in the lexicon can be found. Thus
improving the performance of WSD systems with
few learning examples is a fundamental step towards
the direction of designing a WSD system that works
well on real texts.
In addition, it is a common opinion that the per-
formance of state-of-the-art WSD systems is not sat-
isfactory from an applicative point of view yet.
403
To achieve these goals we identified two promis-
ing research directions:
1. Modeling independently domain and syntag-
matic aspects of sense distinction, to improve
the feature representation of sense tagged ex-
amples (Gliozzo et al, 2004).
2. Leveraging external knowledge acquired from
unlabeled corpora.
The first direction is motivated by the linguistic
assumption that syntagmatic and domain (associa-
tive) relations are both crucial to represent sense
distictions, while they are basically originated by
very different phenomena. Syntagmatic relations
hold among words that are typically located close
to each other in the same sentence in a given tempo-
ral order, while domain relations hold among words
that are typically used in the same semantic domain
(i.e. in texts having similar topics (Gliozzo et al,
2004)). Their different nature suggests to adopt dif-
ferent learning strategies to detect them.
Regarding the second direction, external knowl-
edge would be required to help WSD algorithms to
better generalize over the data available for train-
ing. On the other hand, most of the state-of-the-art
supervised approaches to WSD are still completely
based on ?internal? information only (i.e. the only
information available to the training algorithm is the
set of manually annotated examples). For exam-
ple, in the Senseval-3 evaluation exercise (Mihal-
cea and Edmonds, 2004) many lexical sample tasks
were provided, beyond the usual labeled training
data, with a large set of unlabeled data. However,
at our knowledge, none of the participants exploited
this unlabeled material. Exploring this direction is
the main focus of this paper. In particular we ac-
quire a Domain Model (DM) for the lexicon (i.e.
a lexical resource representing domain associations
among terms), and we exploit this information in-
side our supervised WSD algorithm. DMs can be
automatically induced from unlabeled corpora, al-
lowing the portability of the methodology among
languages.
We identified kernel methods as a viable frame-
work in which to implement the assumptions above
(Strapparava et al, 2004).
Exploiting the properties of kernels, we have de-
fined independently a set of domain and syntagmatic
kernels and we combined them in order to define a
complete kernel for WSD. The domain kernels esti-
mate the (domain) similarity (Magnini et al, 2002)
among contexts, while the syntagmatic kernels eval-
uate the similarity among collocations.
We will demonstrate that using DMs induced
from unlabeled corpora is a feasible strategy to in-
crease the generalization capability of the WSD al-
gorithm. Our system far outperforms the state-of-
the-art systems in all the tasks in which it has been
tested. Moreover, a comparative analysis of the
learning curves shows that the use of DMs allows
us to remarkably reduce the amount of sense-tagged
examples, opening new scenarios to develop sys-
tems for all-words tasks with minimal supervision.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model. In particular
an automatic acquisition technique based on Latent
Semantic Analysis (LSA) is described. In Section 3
we present a WSD system based on a combination
of kernels. In particular we define a Domain Ker-
nel (see Section 3.1) and a Syntagmatic Kernel (see
Section 3.2), to model separately syntagmatic and
domain aspects. In Section 4 our WSD system is
evaluated in the Senseval-3 English, Italian, Spanish
and Catalan lexical sample tasks.
2 Domain Models
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let C = {t1, t2, . . . , tn} be a corpus, let
V = {w1, w2, . . . , wk} be its vocabulary, let T be
the k ? n term-by-document matrix representing C ,
such that ti,j is the frequency of word wi into the text
tj . The VSM is a k-dimensional space Rk, in which
the text tj ? C is represented by means of the vec-
tor ~tj such that the ith component of ~tj is ti,j. The
similarity among two texts in the VSM is estimated
by computing the cosine among them.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences ?he is affected by AIDS? and ?HIV is
a virus? do not have any words in common. In the
404
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences ?the laptop has
been infected by a virus? and ?HIV is a virus? would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM in which texts and
terms are represented in a uniform way.
A DM is composed by soft clusters of terms. Each
cluster represents a semantic domain, i.e. a set of
terms that often co-occur in texts having similar top-
ics. A DM is represented by a k?k? rectangular ma-
trix D, containing the degree of association among
terms and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Matrix
DMs can be used to describe lexical ambiguity
and variability. Lexical ambiguity is represented
by associating one term to more than one domain,
while variability is represented by associating dif-
ferent terms to the same domain. For example the
term virus is associated to both the domain COM-
PUTER SCIENCE and the domain MEDICINE (ambi-
guity) while the domain MEDICINE is associated to
both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2, ..., Dk?} be a
set of domains, such that k?  k. A DM is fully
defined by a k?k? domain matrix D representing in
each cell di,z the domain relevance of term wi with
respect to the domain Dz . The domain matrix D is
used to define a function D : Rk ? Rk? , that maps
the vectors ~tj expressed into the classical VSM, into
the vectors ~t?j in the domain VSM. D is defined by1
D(~tj) = ~tj(IIDFD) = ~t?j (1)
1In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
where IIDF is a k ? k diagonal matrix such that
iIDFi,i = IDF (wi), ~tj is represented as a row vector,
and IDF (wi) is the Inverse Document Frequency of
wi.
Vectors in the domain VSM are called Domain
Vectors (DVs). DVs for texts are estimated by ex-
ploiting the formula 1, while the DV ~w?i, correspond-
ing to the word wi ? V is the ith row of the domain
matrix D. To be a valid domain matrix such vectors
should be normalized (i,e. ? ~w?i, ~w?i? = 1).
In the Domain VSM the similarity among DVs is
estimated by taking into account second order rela-
tions among terms. For example the similarity of the
two sentences ?He is affected by AIDS? and ?HIV
is a virus? is very high, because the terms AIDS,
HIV and virus are highly associated to the domain
MEDICINE.
A DM can be estimated from hand made lexical
resources such as WORDNET DOMAINS (Magnini
and Cavaglia`, 2000), or by performing a term clus-
tering process on a large corpus. We think that the
second methodology is more attractive, because it
allows us to automatically acquire DMs for different
languages.
In this work we propose the use of Latent Seman-
tic Analysis (LSA) to induce DMs from corpora.
LSA is an unsupervised technique for estimating the
similarity among texts and terms in a corpus. LSA
is performed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix T
describing the corpus. The SVD algorithm can be
exploited to acquire a domain matrix D from a large
corpus C in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T ' V?k?UT where ?k? is the diagonal
k ? k matrix containing the highest k ?  k eigen-
values of T, and all the remaining elements set to
0. The parameter k? is the dimensionality of the Do-
main VSM and can be fixed in advance2 . Under this
setting we define the domain matrix DLSA as
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
q
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .3
2It is not clear how to choose the right dimensionality. In
our experiments we used 50 dimensions.
3When DLSA is substituted in Equation 1 the Domain VSM
405
3 Kernel Methods for WSD
In the introduction we discussed two promising di-
rections for improving the performance of a super-
vised disambiguation system. In this section we
show how these requirements can be efficiently im-
plemented in a natural and elegant way by using ker-
nel methods.
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping ?, we can use a kernel
function K : X ? X ? R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Exploiting the properties of the kernel func-
tions, it is possible to define the kernel combination
schema as
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj, xj)Kl(xi, xi)
(3)
Our WSD system is then defined as combination
of n basic kernels. Each kernel adds some addi-
tional dimensions to the feature space. In particular,
we have defined two families of kernels: Domain
and Syntagmatic kernels. The former is composed
by both the Domain Kernel (KD) and the Bag-of-
Words kernel (KBoW ), that captures domain aspects
(see Section 3.1). The latter captures the syntag-
matic aspects of sense distinction and it is composed
by two kernels: the collocation kernel (KColl) and
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
the Part of Speech kernel (KPoS) (see Section 3.2).
The WSD kernels (K ?WSD and KWSD) are then de-
fined by combining them (see Section 3.3).
3.1 Domain Kernels
In (Magnini et al, 2002), it has been claimed that
knowing the domain of the text in which the word
is located is a crucial information for WSD. For
example the (domain) polysemy among the COM-
PUTER SCIENCE and the MEDICINE senses of the
word virus can be solved by simply considering
the domain of the context in which it is located.
This assumption can be modeled by defining a
kernel that estimates the domain similarity among
the contexts of the words to be disambiguated,
namely the Domain Kernel. The Domain Kernel es-
timates the similarity among the topics (domains) of
two texts, so to capture domain aspects of sense dis-
tinction. It is a variation of the Latent Semantic Ker-
nel (Shawe-Taylor and Cristianini, 2004), in which a
DM (see Section 2) is exploited to define an explicit
mapping D : Rk ? Rk? from the classical VSM into
the Domain VSM. The Domain Kernel is defined by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(ti),D(tj)??D(ti),D(tj)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. Thus the Domain Kernel requires a Domain
Matrix D. For our experiments we acquire the ma-
trix DLSA, described in equation 2, from a generic
collection of unlabeled documents, as explained in
Section 2.
A more traditional approach to detect topic (do-
main) similarity is to extract Bag-of-Words (BoW)
features from a large window of text around the
word to be disambiguated. The BoW kernel, de-
noted by KBoW , is a particular case of the Domain
Kernel, in which D = I, and I is the identity ma-
trix. The BoW kernel does not require a DM, then it
can be applied to the ?strictly? supervised settings,
in which an external knowledge source is not pro-
vided.
3.2 Syntagmatic kernels
Kernel functions are not restricted to operate on vec-
torial objects ~x ? Rk. In principle kernels can be
defined for any kind of object representation, as for
406
example sequences and trees. As stated in Section 1,
syntagmatic relations hold among words collocated
in a particular temporal order, thus they can be mod-
eled by analyzing sequences of words.
We identified the string kernel (or word se-
quence kernel) (Shawe-Taylor and Cristianini, 2004)
as a valid instrument to model our assumptions.
The string kernel counts how many times a (non-
contiguous) subsequence of symbols u of length
n occurs in the input string s, and penalizes non-
contiguous occurrences according to the number of
gaps they contain (gap-weighted subsequence ker-
nel).
Formally, let V be the vocabulary, the feature
space associated with the gap-weighted subsequence
kernel of length n is indexed by a set I of subse-
quences over V of length n. The (explicit) mapping
function is defined by
?nu(s) =
?
i:u=s(i)
?l(i), u ? V n (5)
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and ? ?]0, 1] is the decay factor used to penal-
ize non-contiguous subsequences.
The associate gap-weighted subsequence kernel is
defined by
kn(si, sj) = ??n(si), ?n(sj)? =
X
u?V n
?n(si)?n(sj) (6)
We modified the generic definition of the string
kernel in order to make it able to recognize collo-
cations in a local window of the word to be disam-
biguated. In particular we defined two Syntagmatic
kernels: the n-gram Collocation Kernel and the n-
gram PoS Kernel. The n-gram Collocation ker-
nel KnColl is defined as a gap-weighted subsequence
kernel applied to sequences of lemmata around the
word l0 to be disambiguated (i.e. l?3, l?2, l?1, l0,
l+1, l+2, l+3). This formulation allows us to esti-
mate the number of common (sparse) subsequences
of lemmata (i.e. collocations) between two exam-
ples, in order to capture syntagmatic similarity. In
analogy we defined the PoS kernel KnPoS , by setting
s to the sequence of PoSs p?3, p?2, p?1, p0, p+1,
p+2, p+3, where p0 is the PoS of the word to be dis-
ambiguated.
The definition of the gap-weighted subsequence
kernel, provided by equation 6, depends on the pa-
rameter n, that represents the length of the sub-
sequences analyzed when estimating the similarity
among sequences. For example, K2Coll allows us to
represent the bigrams around the word to be disam-
biguated in a more flexible way (i.e. bigrams can be
sparse). In WSD, typical features are bigrams and
trigrams of lemmata and PoSs around the word to
be disambiguated, then we defined the Collocation
Kernel and the PoS Kernel respectively by equations
7 and 84.
KColl(si, sj) =
p
?
l=1
K lColl(si, sj) (7)
KPoS(si, sj) =
p
?
l=1
K lPoS(si, sj) (8)
3.3 WSD kernels
In order to show the impact of using Domain Models
in the supervised learning process, we defined two
WSD kernels, by applying the kernel combination
schema described by equation 3. Thus the following
WSD kernels are fully specified by the list of the
kernels that compose them.
Kwsd composed by KColl, KPoS and KBoW
K?wsd composed by KColl, KPoS , KBoW and KD
The only difference between the two systems is
that K ?wsd uses Domain Kernel KD. K ?wsd exploits
external knowledge, in contrast to Kwsd, whose only
available information is the labeled training data.
4 Evaluation and Discussion
In this section we present the performance of our
kernel-based algorithms for WSD. The objectives of
these experiments are:
? to study the combination of different kernels,
? to understand the benefits of plugging external
information using domain models,
? to verify the portability of our methodology
among different languages.
4The parameters p and ? are optimized by cross-validation.
The best results are obtained setting p = 2, ? = 0.5 for KColl
and ? ? 0 for KPoS .
407
4.1 WSD tasks
We conducted the experiments on four lexical sam-
ple tasks (English, Catalan, Italian and Spanish)
of the Senseval-3 competition (Mihalcea and Ed-
monds, 2004). Table 2 describes the tasks by re-
porting the number of words to be disambiguated,
the mean polysemy, and the dimension of training,
test and unlabeled corpora. Note that the organiz-
ers of the English task did not provide any unlabeled
material. So for English we used a domain model
built from a portion of BNC corpus, while for Span-
ish, Italian and Catalan we acquired DMs from the
unlabeled corpora made available by the organizers.
#w pol # train # test # unlab
Catalan 27 3.11 4469 2253 23935
English 57 6.47 7860 3944 -
Italian 45 6.30 5145 2439 74788
Spanish 46 3.30 8430 4195 61252
Table 2: Dataset descriptions
4.2 Kernel Combination
In this section we present an experiment to em-
pirically study the kernel combination. The basic
kernels (i.e. KBoW , KD , KColl and KPoS) have
been compared to the combined ones (i.e. Kwsd and
K ?wsd) on the English lexical sample task.
The results are reported in Table 3. The results
show that combining kernels significantly improves
the performance of the system.
KD KBoW KPoS KColl Kwsd K?wsd
F1 65.5 63.7 62.9 66.7 69.7 73.3
Table 3: The performance (F1) of each basic ker-
nel and their combination for English lexical sample
task.
4.3 Portability and Performance
We evaluated the performance of K ?wsd and Kwsd on
the lexical sample tasks described above. The results
are showed in Table 4 and indicate that using DMs
allowed K ?wsd to significantly outperform Kwsd.
In addition, K ?wsd turns out the best systems for
all the tested Senseval-3 tasks.
Finally, the performance of K ?wsd are higher than
the human agreement for the English and Spanish
tasks5.
Note that, in order to guarantee an uniform appli-
cation to any language, we do not use any syntactic
information provided by a parser.
4.4 Learning Curves
The Figures 1, 2, 3 and 4 show the learning curves
evaluated on K ?wsd and Kwsd for all the lexical sam-
ple tasks.
The learning curves indicate that K ?wsd is far su-
perior to Kwsd for all the tasks, even with few ex-
amples. The result is extremely promising, for it
demonstrates that DMs allow to drastically reduce
the amount of sense tagged data required for learn-
ing. It is worth noting, as reported in Table 5, that
K ?wsd achieves the same performance of Kwsd using
about half of the training data.
% of training
English 54
Catalan 46
Italian 51
Spanish 50
Table 5: Percentage of sense tagged examples re-
quired by K ?wsd to achieve the same performance of
Kwsd with full training.
5 Conclusion and Future Works
In this paper we presented a supervised algorithm
for WSD, based on a combination of kernel func-
tions. In particular we modeled domain and syn-
tagmatic aspects of sense distinctions by defining
respectively domain and syntagmatic kernels. The
Domain kernel exploits Domain Models, acquired
from ?external? untagged corpora, to estimate the
similarity among the contexts of the words to be dis-
ambiguated. The syntagmatic kernels evaluate the
similarity between collocations.
We evaluated our algorithm on several Senseval-
3 lexical sample tasks (i.e. English, Spanish, Ital-
ian and Catalan) significantly improving the state-ot-
the-art for all of them. In addition, the performance
5It is not clear if the inter-annotator-agreement can be con-
siderated the upper bound for a WSD system.
408
MF Agreement BEST Kwsd K ?wsd DM+
English 55.2 67.3 72.9 69.7 73.3 3.6
Catalan 66.3 93.1 85.2 85.2 89.0 3.8
Italian 18.0 89.0 53.1 53.1 61.3 8.2
Spanish 67.7 85.3 84.2 84.2 88.2 4.0
Table 4: Comparative evaluation on the lexical sample tasks. Columns report: the Most Frequent baseline,
the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of Kwsd, the F1 of K ?wsd,
DM+ (the improvement due to DM, i.e. K ?wsd ?Kwsd).
0.5
0.55
0.6
0.65
0.7
0.75
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 1: Learning curves for English lexical sample
task.
0.65
0.7
0.75
0.8
0.85
0.9
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 2: Learning curves for Catalan lexical sample
task.
of our system outperforms the inter annotator agree-
ment in both English and Spanish, achieving the up-
per bound performance.
We demonstrated that using external knowledge
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 3: Learning curves for Italian lexical sample
task.
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 4: Learning curves for Spanish lexical sam-
ple task.
inside a supervised framework is a viable method-
ology to reduce the amount of training data required
for learning. In our approach the external knowledge
is represented by means of Domain Models automat-
409
ically acquired from corpora in a totally unsuper-
vised way. Experimental results show that the use
of Domain Models allows us to reduce the amount
of training data, opening an interesting research di-
rection for all those NLP tasks for which the Knowl-
edge Acquisition Bottleneck is a crucial problem. In
particular we plan to apply the same methodology to
Text Categorization, by exploiting the Domain Ker-
nel to estimate the similarity among texts. In this im-
plementation, our WSD system does not exploit syn-
tactic information produced by a parser. For the fu-
ture we plan to integrate such information by adding
a tree kernel (i.e. a kernel function that evaluates the
similarity among parse trees) to the kernel combi-
nation schema presented in this paper. Last but not
least, we are going to apply our approach to develop
supervised systems for all-words tasks, where the
quantity of data available to train each word expert
classifier is very low.
Acknowledgments
Alfio Gliozzo and Carlo Strapparava were partially
supported by the EU project Meaning (IST-2001-
34460). Claudio Giuliano was supported by the EU
project Dot.Kom (IST-2001-34038). We would like
to thank Oier Lopez de Lacalle for useful comments.
References
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
B. Decadt, V. Hoste, W. Daelemens, and A. van den
Bosh. 2004. Gambl, genetic algorithm optimiza-
tion of memory-based wsd. In Proc. of Senseval-3,
Barcelona, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18(3):275?299.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413?1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceedings
of SENSEVAL-3, Barcelona, Spain, July.
R. Mihalcea and E. Faruque. 2004. Senselearner: Min-
imally supervised WSD for all words in open text. In
Proceedings of SENSEVAL-3, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
S. Small. 1980. Word Expert Parsing: A Theory of Dis-
tributed Word-based Natural Language Understand-
ing. Ph.D. Thesis, Department of Computer Science,
University of Maryland.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229?234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
D. Yarowsky and R. Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter space. Natural
Language Engineering, 8(4):293?310.
410
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 449?456,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Direct Word Sense Matching for Lexical Substitution
Ido Dagan1, Oren Glickman1, Alfio Gliozzo2, Efrat Marmorshtein1, Carlo Strapparava2
1Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel
2ITC-Irst, via Sommarive, I-38050, Trento, Italy
Abstract
This paper investigates conceptually and
empirically the novel sense matching task,
which requires to recognize whether the
senses of two synonymous words match in
context. We suggest direct approaches to
the problem, which avoid the intermediate
step of explicit word sense disambigua-
tion, and demonstrate their appealing ad-
vantages and stimulating potential for fu-
ture research.
1 Introduction
In many language processing settings it is needed
to recognize that a given word or term may be sub-
stituted by a synonymous one. In a typical in-
formation seeking scenario, an information need
is specified by some given source words. When
looking for texts that match the specified need the
source words might be substituted with synony-
mous target words. For example, given the source
word ?weapon? a system may substitute it with the
target synonym ?arm?.
This scenario, which is generally referred here
as lexical substitution, is a common technique
for increasing recall in Natural Language Process-
ing (NLP) applications. In Information Retrieval
(IR) and Question Answering (QA) it is typically
termed query/question expansion (Moldovan and
Mihalcea, 2000; Negri, 2004). Lexical Substi-
tution is also commonly applied to identify syn-
onyms in text summarization, for paraphrasing in
text generation, or is integrated into the features of
supervised tasks such as Text Categorization and
Information Extraction. Naturally, lexical substi-
tution is a very common first step in textual en-
tailment recognition, which models semantic in-
ference between a pair of texts in a generalized ap-
plication independent setting (Dagan et al, 2005).
To perform lexical substitution NLP applica-
tions typically utilize a knowledge source of syn-
onymous word pairs. The most commonly used
resource for lexical substitution is the manually
constructed WordNet (Fellbaum, 1998). Another
option is to use statistical word similarities, such
as in the database constructed by Dekang Lin (Lin,
1998). We generically refer to such resources as
substitution lexicons.
When using a substitution lexicon it is assumed
that there are some contexts in which the given
synonymous words share the same meaning. Yet,
due to polysemy, it is needed to verify that the
senses of the two words do indeed match in a given
context. For example, there are contexts in which
the source word ?weapon? may be substituted by
the target word ?arm?; however one should recog-
nize that ?arm? has a different sense than ?weapon?
in sentences such as ?repetitive movements could
cause injuries to hands, wrists and arms.?
A commonly proposed approach to address
sense matching in lexical substitution is applying
Word Sense Disambiguation (WSD) to identify
the senses of the source and target words. Then,
substitution is applied only if the words have the
same sense (or synset, in WordNet terminology).
In settings in which the source is given as a sin-
gle term without context, sense disambiguation
is performed only for the target word; substitu-
tion is then applied only if the target word?s sense
matches at least one of the possible senses of the
source word.
One might observe that such application of WSD
addresses the task at hand in a somewhat indi-
rect manner. In fact, lexical substitution only re-
quires knowing that the source and target senses
449
do match, but it does not require that the match-
ing senses will be explicitly identified. Selecting
explicitly the right sense in context, which is then
followed by verifying the desired matching, might
be solving a harder intermediate problem than re-
quired. Instead, we can define the sense match-
ing problem directly as a binary classification task
for a pair of synonymous source and target words.
This task requires to decide whether the senses of
the two words do or do not match in a given con-
text (but it does not require to identify explicitly
the identity of the matching senses).
A highly related task was proposed in (Mc-
Carthy, 2002). McCarthy?s proposal was to ask
systems to suggest possible ?semantically similar
replacements? of a target word in context, where
alternative replacements should be grouped to-
gether. While this task is somewhat more com-
plicated as an evaluation setting than our binary
recognition task, it was motivated by similar ob-
servations and applied goals. From another per-
spective, sense matching may be viewed as a lex-
ical sub-case of the general textual entailment
recognition setting, where we need to recognize
whether the meaning of the target word ?entails?
the meaning of the source word in a given context.
This paper provides a first investigation of the
sense matching problem. To allow comparison
with the classical WSD setting we derived an
evaluation dataset for the new problem from the
Senseval-3 English lexical sample dataset (Mihal-
cea and Edmonds, 2004). We then evaluated alter-
native supervised and unsupervised methods that
perform sense matching either indirectly or di-
rectly (i.e. with or without the intermediate sense
identification step). Our findings suggest that in
the supervised setting the results of the direct and
indirect approaches are comparable. However, ad-
dressing directly the binary classification task has
practical advantages and can yield high precision
values, as desired in precision-oriented applica-
tions such as IR and QA.
More importantly, direct sense matching sets
the ground for implicit unsupervised approaches
that may utilize practically unlimited volumes
of unlabeled training data. Furthermore, such
approaches circumvent the sisyphean need for
specifying explicitly a set of stipulated senses.
We present an initial implementation of such an
approach using a one-class classifier, which is
trained on unlabeled occurrences of the source
word and applied to occurrences of the target
word. Our current results outperform the unsuper-
vised baseline and put forth a whole new direction
for future research.
2 WSD and Lexical Expansion
Despite certain initial skepticism about the useful-
ness of WSD in practical tasks (Voorhees, 1993;
Sanderson, 1994), there is some evidence that
WSD can improve performance in typical NLP
tasks such as IR and QA. For example, (Shu?tze
and Pederson, 1995) gives clear indication of the
potential for WSD to improve the precision of an IR
system. They tested the use of WSD on a standard
IR test collection (TREC-1B), improving precision
by more than 4%.
The use of WSD has produced successful exper-
iments for query expansion techniques. In partic-
ular, some attempts exploited WordNet to enrich
queries with semantically-related terms. For in-
stance, (Voorhees, 1994) manually expanded 50
queries over the TREC-1 collection using syn-
onymy and other WordNet relations. She found
that the expansion was useful with short and in-
complete queries, leaving the task of proper auto-
matic expansion as an open problem.
(Gonzalo et al, 1998) demonstrates an incre-
ment in performance over an IR test collection us-
ing the sense data contained in SemCor over a
purely term based model. In practice, they ex-
perimented searching SemCor with disambiguated
and expanded queries. Their work shows that
a WSD system, even if not performing perfectly,
combined with synonymy enrichment increases
retrieval performance.
(Moldovan and Mihalcea, 2000) introduces the
idea of using WordNet to extend Web searches
based on semantic similarity. Their results showed
that WSD-based query expansion actually im-
proves retrieval performance in a Web scenario.
Recently (Negri, 2004) proposed a sense-based
relevance feedback scheme for query enrichment
in a QA scenario (TREC-2003 and ACQUAINT),
demonstrating improvement in retrieval perfor-
mance.
While all these works clearly show the potential
usefulness of WSD in practical tasks, nonetheless
they do not necessarily justify the efforts for refin-
ing fine-grained sense repositories and for build-
ing large sense-tagged corpora. We suggest that
the sense matching task, as presented in the intro-
450
duction, may relieve major drawbacks of applying
WSD in practical scenarios.
3 Problem Setting and Dataset
To investigate the direct sense matching problem
it is necessary to obtain an appropriate dataset of
examples for this binary classification task, along
with gold standard annotation. While there is
no such standard (application independent) dataset
available it is possible to derive it automatically
from existing WSD evaluation datasets, as de-
scribed below. This methodology also allows
comparing direct approaches for sense matching
with classical indirect approaches, which apply an
intermediate step of identifying the most likely
WordNet sense.
We derived our dataset from the Senseval-3 En-
glish lexical sample dataset (Mihalcea and Ed-
monds, 2004), taking all 25 nouns, adjectives and
adverbs in this sample. Verbs were excluded since
their sense annotation in Senseval-3 is not based
on WordNet senses. The Senseval dataset includes
a set of example occurrences in context for each
word, split to training and test sets, where each ex-
ample is manually annotated with the correspond-
ing WordNet synset.
For the sense matching setting we need exam-
ples of pairs of source-target synonymous words,
where at least one of these words should occur in
a given context. Following an applicative moti-
vation, we mimic an IR setting in which a sin-
gle source word query is expanded (substituted)
by a synonymous target word. Then, it is needed
to identify contexts in which the target word ap-
pears in a sense that matches the source word. Ac-
cordingly, we considered each of the 25 words in
the Senseval sample as a target word for the sense
matching task. Next, we had to pick for each target
word a corresponding synonym to play the role of
the source word. This was done by creating a list
of all WordNet synonyms of the target word, under
all its possible senses, and picking randomly one
of the synonyms as the source word. For example,
the word ?disc? is one of the words in the Sense-
val lexical sample. For this target word the syn-
onym ?record? was picked, which matches ?disc?
in its musical sense. Overall, 59% of all possible
synsets of our target words included an additional
synonym, which could play the role of the source
word (that is, 41% of the synsets consisted of the
target word only). Similarly, 62% of the test exam-
ples of the target words were annotated by a synset
that included an additional synonym.
While creating source-target synonym pairs it
was evident that many WordNet synonyms corre-
spond to very infrequent senses or word usages,
such as the WordNet synonyms germ and source.
Such source synonyms are useless for evaluat-
ing sense matching with the target word since the
senses of the two words would rarely match in per-
ceivable contexts. In fact, considering our motiva-
tion for lexical substitution, it is usually desired to
exclude such obscure synonym pairs from substi-
tution lexicons in practical applications, since they
would mostly introduce noise to the system. To
avoid this problem the list of WordNet synonyms
for each target word was filtered by a lexicogra-
pher, who excluded manually obscure synonyms
that seemed worthless in practice. The source syn-
onym for each target word was then picked ran-
domly from the filtered list. Table 1 shows the 25
source-target pairs created for our experiments. In
future work it may be possible to apply automatic
methods for filtering infrequent sense correspon-
dences in the dataset, by adopting algorithms such
as in (McCarthy et al, 2004).
Having source-target synonym pairs, a classifi-
cation instance for the sense matching task is cre-
ated from each example occurrence of the target
word in the Senseval dataset. A classification in-
stance is thus defined by a pair of source and target
words and a given occurrence of the target word in
context. The instance should be classified as pos-
itive if the sense of the target word in the given
context matches one of the possible senses of the
source word, and as negative otherwise. Table 2
illustrates positive and negative example instances
for the source-target synonym pair ?record-disc?,
where only occurrences of ?disc? in the musical
sense are considered positive.
The gold standard annotation for the binary
sense matching task can be derived automatically
from the Senseval annotations and the correspond-
ing WordNet synsets. An example occurrence of
the target word is considered positive if the an-
notated synset for that example includes also the
source word, and Negative otherwise. Notice that
different positive examples might correspond to
different senses of the target word. This happens
when the source and target share several senses,
and hence they appear together in several synsets.
Finally, since in Senseval an example may be an-
451
source-target source-target source-target source-target source-target
statement-argument subdivision-arm atm-atmosphere hearing-audience camber-bank
level-degree deviation-difference dissimilar-different trouble-difficulty record-disc
raging-hot ikon-image crucial-important sake-interest bare-simple
opinion-judgment arrangement-organization newspaper-paper company-party substantial-solid
execution-performance design-plan protection-shelter variety-sort root-source
Table 1: Source and target pairs
sentence annotation
This is anyway a stunning disc, thanks to the playing of the Moscow Virtuosi with Spivakov. positive
He said computer networks would not be affected and copies of information should be made on
floppy discs.
negative
Before the dead soldier was placed in the ditch his personal possessions were removed, leaving
one disc on the body for identification purposes
negative
Table 2: positive and negative examples for the source-target synonym pair ?record-disc?
notated with more than one sense, it was consid-
ered positive if any of the annotated synsets for the
target word includes the source word.
Using this procedure we derived gold standard
annotations for all the examples in the Senseval-
3 training section for our 25 target words. For the
test set we took up to 40 test examples for each tar-
get word (some words had fewer test examples),
yielding 913 test examples in total, out of which
239 were positive. This test set was used to eval-
uate the sense matching methods described in the
next section.
4 Investigated Methods
As explained in the introduction, the sense match-
ing task may be addressed by two general ap-
proaches. The traditional indirect approach would
first disambiguate the target word relative to a pre-
defined set of senses, using standard WSD meth-
ods, and would then verify that the selected sense
matches the source word. On the other hand, a
direct approach would address the binary sense
matching task directly, without selecting explicitly
a concrete sense for the target word. This section
describes the alternative methods we investigated
under supervised and unsupervised settings. The
supervised methods utilize manual sense annota-
tions for the given source and target words while
unsupervised methods do not require any anno-
tated sense examples. For the indirect approach
we assume the standard WordNet sense repository
and corresponding annotations of the target words
with WordNet synsets.
4.1 Feature set and classifier
As a vehicle for investigating different classifica-
tion approaches we implemented a ?vanilla? state
of the art architecture for WSD. Following com-
mon practice in feature extraction (e.g. (Yarowsky,
1994)), and using the mxpost1 part of speech tag-
ger and WordNet?s lemmatization, the following
feature set was used: bag of word lemmas for the
context words in the preceding, current and fol-
lowing sentence; unigrams of lemmas and parts
of speech in a window of +/- three words, where
each position provides a distinct feature; and bi-
grams of lemmas in the same window. The SVM-
Light (Joachims, 1999) classifier was used in the
supervised settings with its default parameters. To
obtain a multi-class classifier we used a standard
one-vs-all approach of training a binary SVM for
each possible sense and then selecting the highest
scoring sense for a test example.
To verify that our implementation provides a
reasonable replication of state of the art WSD we
applied it to the standard Senseval-3 Lexical Sam-
ple WSD task. The obtained accuracy2 was 66.7%,
which compares reasonably with the mid-range of
systems in the Senseval-3 benchmark (Mihalcea
and Edmonds, 2004). This figure is just a few
percent lower than the (quite complicated) best
Senseval-3 system, which achieved about 73% ac-
curacy, and it is much higher than the standard
Senseval baselines. We thus regard our classifier
as a fair vehicle for comparing the alternative ap-
proaches for sense matching on equal grounds.
1ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
2The standard classification accuracy measure equals pre-
cision and recall as defined in the Senseval terminology when
the system classifies all examples, with no abstentions.
452
4.2 Supervised Methods
4.2.1 Indirect approach
The indirect approach for sense matching fol-
lows the traditional scheme of performing WSD
for lexical substitution. First, the WSD classifier
described above was trained for the target words
of our dataset, using the Senseval-3 sense anno-
tated training data for these words. Then, the clas-
sifier was applied to the test examples of the target
words, selecting the most likely sense for each ex-
ample. Finally, an example was classified as pos-
itive if the selected synset for the target word in-
cludes the source word, and as negative otherwise.
4.2.2 Direct approach
As explained above, the direct approach ad-
dresses the binary sense matching task directly,
without selecting explicitly a sense for the target
word. In the supervised setting it is easy to ob-
tain such a binary classifier using the annotation
scheme described in Section 3. Under this scheme
an example was annotated as positive (for the bi-
nary sense matching task) if the source word is
included in the Senseval gold standard synset of
the target word. We trained the classifier using the
set of Senseval-3 training examples for each tar-
get word, considering their derived binary anno-
tations. Finally, the trained classifier was applied
to the test examples of the target words, yielding
directly a binary positive-negative classification.
4.3 Unsupervised Methods
It is well known that obtaining annotated training
examples for WSD tasks is very expensive, and
is often considered infeasible in unrestricted do-
mains. Therefore, many researchers investigated
unsupervised methods, which do not require an-
notated examples. Unsupervised approaches have
usually been investigated within Senseval using
the ?All Words? dataset, which does not include
training examples. In this paper we preferred us-
ing the same test set which was used for the super-
vised setting (created from the Senseval-3 ?Lexi-
cal Sample? dataset, as described above), in order
to enable comparison between the two settings.
Naturally, in the unsupervised setting the sense la-
bels in the training set were not utilized.
4.3.1 Indirect approach
State-of-the-art unsupervised WSD systems are
quite complex and they are not easy to be repli-
cated. Thus, we implemented the unsupervised
version of the Lesk algorithm (Lesk, 1986) as a
reference system, since it is considered a standard
simple baseline for unsupervised approaches. The
Lesk algorithm is one of the first algorithms de-
veloped for semantic disambiguation of all-words
in unrestricted text. In its original unsupervised
version, the only resource required by the algo-
rithm is a machine readable dictionary with one
definition for each possible word sense. The algo-
rithm looks for words in the sense definitions that
overlap with context words in the given sentence,
and chooses the sense that yields maximal word
overlap. We implemented a version of this algo-
rithm using WordNet sense-definitions with con-
text length of ?10 words before and after the tar-
get word.
4.3.2 The direct approach: one-class learning
The unsupervised settings for the direct method
are more problematic because most of unsuper-
vised WSD algorithms (such as the Lesk algo-
rithm) rely on dictionary definitions. For this rea-
son, standard unsupervised techniques cannot be
applied in a direct approach for sense matching, in
which the only external information is a substitu-
tion lexicon.
In this subsection we present a direct unsuper-
vised method for sense matching. It is based on
the assumption that typical contexts in which both
the source and target words appear correspond to
their matching senses. Unlabeled occurrences of
the source word can then be used to provide evi-
dence for lexical substitution because they allow
us to recognize whether the sense of the target
word matches that of the source. Our strategy is
to represent in a learning model the typical con-
texts of the source word in unlabeled training data.
Then, we exploit such model to match the contexts
of the target word, providing a decision criterion
for sense matching. In other words, we expect that
under a matching sense the target word would oc-
cur in prototypical contexts of the source word.
To implement such approach we need a learning
technique that does not rely on the availability of
negative evidence, that is, a one-class learning al-
gorithm. In general, the classification performance
of one-class approaches is usually quite poor, if
compared to supervised approaches for the same
tasks. However, in many practical settings one-
class learning is the only available solution.
For our experiments we adopted the one-class
SVM learning algorithm (Scho?lkopf et al, 2001)
453
implemented in the LIBSVM package,3 and repre-
sented the unlabeled training examples by adopt-
ing the feature set described in Subsection 4.1.
Roughly speaking, a one-class SVM estimates the
smallest hypersphere enclosing most of the train-
ing data. New test instances are then classified
positively if they lie inside the sphere, while out-
liers are regarded as negatives. The ratio between
the width of the enclosed region and the number
of misclassified training examples can be varied
by setting the parameter ? ? (0, 1). Smaller val-
ues of ? will produce larger positive regions, with
the effect of increasing recall.
The appealing advantage of adopting one-class
learning for sense matching is that it allows us to
define a very elegant learning scenario, in which it
is possible to train ?off-line? a different classifier
for each (source) word in the lexicon. Such a clas-
sifier can then be used to match the sense of any
possible target word for the source which is given
in the substitution lexicon. This is in contrast to
the direct supervised method proposed in Subsec-
tion 4.2, where a different classifier for each pair
of source - target words has to be defined.
5 Evaluation
5.1 Evaluation measures and baselines
In the lexical substitution (and expansion) set-
ting, the standard WSD metrics (Mihalcea and Ed-
monds, 2004) are not suitable, because we are in-
terested in the binary decision of whether the tar-
get word matches the sense of a given source word.
In analogy to IR, we are more interested in positive
assignments, while the opposite case (i.e. when the
two words cannot be substituted) is less interest-
ing. Accordingly, we utilize the standard defini-
tions of precision, recall and F1 typically used in
IR benchmarks. In the rest of this section we will
report micro averages for these measures on the
test set described in Section 3.
Following the Senseval methodology, we evalu-
ated two different baselines for unsupervised and
supervised methods. The random baseline, used
for the unsupervised algorithms, was obtained by
choosing either the positive or the negative class
at random resulting in P = 0.262, R = 0.5,
F1 = 0.344. The Most Frequent baseline has
been used for the supervised algorithms and is ob-
tained by assigning the positive class when the
3Freely available from www.csie.ntu.edu.tw/
/?cjlin/libsvm.
percentage of positive examples in the training set
is above 50%, resulting in P = 0.65, R = 0.41,
F1 = 0.51.
5.2 Supervised Methods
Both the indirect and the direct supervised meth-
ods presented in Subsection 4.2 have been tested
and compared to the most frequent baseline.
Indirect. For the indirect methodology we
trained the supervised WSD system for each tar-
get word on the sense-tagged training sample. As
described in Subsection 4.2, we implemented a
simple SVM-based WSD system (see Section 4.2)
and applied it to the sense-matching task. Results
are reported in Table 3. The direct strategy sur-
passes the most frequent baseline F1 score, but the
achieved precision is still below it. We note that in
this multi-class setting it is less straightforward to
tradeoff recall for precision, as all senses compete
with each other.
Direct. In the direct supervised setting, sense
matching is performed by training a binary clas-
sifier, as described in Subsection 4.2.
The advantage of adopting a binary classifica-
tion strategy is that the precision/recall tradeoff
can be tuned in a meaningful way. In SVM learn-
ing, such tuning is achieved by varying the param-
eter J , that allows us to modify the cost function
of the SVM learning algorithm. If J = 1 (default),
the weight for the positive examples is equal to the
weight for the negatives. When J > 1, negative
examples are penalized (increasing recall), while,
whenever 0 < J < 1, positive examples are penal-
ized (increasing precision). Results obtained by
varying this parameter are reported in Figure 1.
Figure 1: Direct supervised results varying J
454
Supervised P R F1 Unsupervised P R F1
Most Frequent Baseline 0.65 0.41 0.51 Random Baseline 0.26 0.50 0.34
Multiclass SVM Indirect 0.59 0.63 0.61 Lesk Indirect 0.24 0.19 0.21
Binary SVM (J = 0.5) Direct 0.80 0.26 0.39 One-Class ? = 0.3 Direct 0.26 0.72 0.39
Binary SVM (J = 1) Direct 0.76 0.46 0.57 One-Class ? = 0.5 Direct 0.29 0.56 0.38
Binary SVM (J = 2) Direct 0.68 0.53 0.60 One-Class ? = 0.7 Direct 0.28 0.36 0.32
Binary SVM (J = 3) Direct 0.69 0.55 0.61 One-Class ? = 0.9 Direct 0.23 0.10 0.14
Table 3: Classification results on the sense matching task
Adopting the standard parameter settings (i.e.
J = 1, see Table 3), the F1 of the system
is slightly lower than for the indirect approach,
while it reaches the indirect figures when J in-
creases. More importantly, reducing J allows us
to boost precision towards 100%. This feature is
of great interest for lexical substitution, particu-
larly in precision oriented applications like IR and
QA, for filtering irrelevant candidate answers or
documents.
5.3 Unsupervised methods
Indirect. To evaluate the indirect unsupervised
settings we implemented the Lesk algorithm, de-
scribed in Subsection 4.3.1, and evaluated it on
the sense matching task. The obtained figures,
reported in Table 3, are clearly below the base-
line, suggesting that simple unsupervised indirect
strategies cannot be used for this task. In fact, the
error of the first step, due to low WSD accuracy
of the unsupervised technique, is propagated in
the second step, producing poor sense matching.
Unfortunately, state-of-the-art unsupervised sys-
tems are actually not much better than Lesk on all-
words task (Mihalcea and Edmonds, 2004), dis-
couraging the use of unsupervised indirect meth-
ods for the sense matching task.
Direct. Conceptually, the most appealing solu-
tion for the sense matching task is the one-class
approach proposed for the direct method (Section
4.3.2). To perform our experiments, we trained a
different one-class SVM for each source word, us-
ing a sample of its unlabeled occurrences in the
BNC corpus as training set. To avoid huge train-
ing sets and to speed up the learning process, we
fixed the maximum number of training examples
to 10000 occurrences per word, collecting on av-
erage about 6500 occurrences per word.
For each target word in the test sample, we ap-
plied the classifier of the corresponding source
word. Results for different values of ? are reported
in Figure 2 and summarized in Table 3.
Figure 2: One-class evaluation varying ?
While the results are somewhat above the base-
line, just small improvements in precision are re-
ported, and recall is higher than the baseline for
? < 0.6. Such small improvements may suggest
that we are following a relevant direction, even
though they may not be useful yet for an applied
sense-matching setting.
Further analysis of the classification results for
each word revealed that optimal F1 values are ob-
tained by adopting different values of ? for differ-
ent words. In the optimal (in retrospect) param-
eter settings for each word, performance for the
test set is noticeably boosted, achieving P = 0.40,
R = 0.85 and F1 = 0.54. Finding a principled un-
supervised way to automatically tune the ? param-
eter is thus a promising direction for future work.
Investigating further the results per word, we
found that the correlation coefficient between the
optimal ? values and the degree of polysemy of
the corresponding source words is 0.35. More in-
terestingly, we noticed a negative correlation (r
= -0.30) between the achieved F1 and the degree
of polysemy of the word, suggesting that polyse-
mous source words provide poor training models
for sense matching. This can be explained by ob-
serving that polysemous source words can be sub-
stituted with the target words only for a strict sub-
455
set of their senses. On the other hand, our one-
class algorithm was trained on all the examples
of the source word, which include irrelevant ex-
amples that yield noisy training sets. A possible
solution may be obtained using clustering-based
word sense discrimination methods (Pedersen and
Bruce, 1997; Schu?tze, 1998), in order to train dif-
ferent one-class models from different sense clus-
ters. Overall, the analysis suggests that future re-
search may obtain better binary classifiers based
just on unlabeled examples of the source word.
6 Conclusion
This paper investigated the sense matching task,
which captures directly the polysemy problem in
lexical substitution. We proposed a direct ap-
proach for the task, suggesting the advantages of
natural control of precision/recall tradeoff, avoid-
ing the need in an explicitly defined sense reposi-
tory, and, most appealing, the potential for novel
completely unsupervised learning schemes. We
speculate that there is a great potential for such
approaches, and suggest that sense matching may
become an appealing problem and possible track
in lexical semantic evaluations.
Acknowledgments
This work was partly developed under the collab-
oration ITC-irst/University of Haifa.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with wordnet synsets can improve
text retrieval. In ACL, Montreal, Canada.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
ACM-SIGDOC Conference, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Automatic identification of infre-
quent word senses. In Proceedings of COLING,
pages 1220?1226.
Diana McCarthy. 2002. Lexical substitution as a task
for wsd evaluation. In Proceedings of the ACL-
02 workshop on Word sense disambiguation, pages
109?115, Morristown, NJ, USA. Association for
Computational Linguistics.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Anal-
ysis of Text, Barcelona, Spain, July.
D. Moldovan and R. Mihalcea. 2000. Using wordnet
and lexical operators to improve internet searches.
IEEE Internet Computing, 4(1):34?43, January.
M. Negri. 2004. Sense-based blind relevance feedback
for question answering. In SIGIR-2004 Workshop
on Information Retrieval For Question Answering
(IR4QA), Sheffield, UK, July.
T. Pedersen and R. Bruce. 1997. Distinguishing word
sense in untagged text. In EMNLP, Providence, Au-
gust.
M. Sanderson. 1994. Word sense disambiguation and
information retrieval. In SIGIR, Dublin, Ireland,
June.
B. Scho?lkopf, J. Platt, J. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Compu-
tation, 13:1443?1471.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
H. Shu?tze and J. Pederson. 1995. Information retrieval
based on word senses. In Proceedings of the 4th
Annual Symposium on Document Analysis and In-
formation Retrieval, Las Vegas.
E. Voorhees. 1993. Using WordNet to disambiguate
word sense for text retrieval. In SIGIR, Pittsburgh,
PA.
E. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th ACM
SIGIR Conference, Dublin, Ireland, June.
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration
in spanish and french. In ACL, pages 88?95, Las
Cruces, New Mexico.
456
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553?560,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Comparable Corpora and Bilingual Dictionaries
for Cross-Language Text Categorization
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
{gliozzo,strappa}@itc.it
Abstract
Cross-language Text Categorization is the
task of assigning semantic classes to docu-
ments written in a target language (e.g. En-
glish) while the system is trained using la-
beled documents in a source language (e.g.
Italian).
In this work we present many solutions ac-
cording to the availability of bilingual re-
sources, and we show that it is possible
to deal with the problem even when no
such resources are accessible. The core
technique relies on the automatic acquisi-
tion of Multilingual Domain Models from
comparable corpora.
Experiments show the effectiveness of our
approach, providing a low cost solution for
the Cross Language Text Categorization
task. In particular, when bilingual dictio-
naries are available the performance of the
categorization gets close to that of mono-
lingual text categorization.
1 Introduction
In the worldwide scenario of the Web age, mul-
tilinguality is a crucial issue to deal with and
to investigate, leading us to reformulate most of
the classical Natural Language Processing (NLP)
problems into a multilingual setting. For in-
stance the classical monolingual Text Categoriza-
tion (TC) problem can be reformulated as a Cross
Language Text Categorization (CLTC) task, in
which the system is trained using labeled exam-
ples in a source language (e.g. English), and it
classifies documents in a different target language
(e.g. Italian).
The applicative interest for the CLTC is im-
mediately clear in the globalized Web scenario.
For example, in the community based trade (e.g.
eBay) it is often necessary to archive texts in dif-
ferent languages by adopting common merceolog-
ical categories, very often defined by collections
of documents in a source language (e.g. English).
Another application along this direction is Cross
Lingual Question Answering, in which it would
be very useful to filter out the candidate answers
according to their topics.
In the literature, this task has been proposed
quite recently (Bel et al, 2003; Gliozzo and Strap-
parava, 2005). In those works, authors exploited
comparable corpora showing promising results. A
more recent work (Rigutini et al, 2005) proposed
the use of Machine Translation techniques to ap-
proach the same task.
Classical approaches for multilingual problems
have been conceived by following two main direc-
tions: (i) knowledge based approaches, mostly im-
plemented by rule based systems and (ii) empirical
approaches, in general relying on statistical learn-
ing from parallel corpora. Knowledge based ap-
proaches are often affected by low accuracy. Such
limitation is mainly due to the problem of tun-
ing large scale multilingual lexical resources (e.g.
MultiWordNet, EuroWordNet) for the specific ap-
plication task (e.g. discarding irrelevant senses,
extending the lexicon with domain specific terms
and their translations). On the other hand, em-
pirical approaches are in general more accurate,
because they can be trained from domain specific
collections of parallel text to represent the appli-
cation needs. There exist many interesting works
about using parallel corpora for multilingual appli-
cations (Melamed, 2001), such as Machine Trans-
lation (Callison-Burch et al, 2004), Cross Lingual
553
Information Retrieval (Littman et al, 1998), and
so on.
However it is not always easy to find or build
parallel corpora. This is the main reason why
the ?weaker? notion of comparable corpora is a
matter of recent interest in the field of Computa-
tional Linguistics (Gaussier et al, 2004). In fact,
comparable corpora are easier to collect for most
languages (e.g. collections of international news
agencies), providing a low cost knowledge source
for multilingual applications.
The main problem of adopting comparable cor-
pora for multilingual knowledge acquisition is that
only weaker statistical evidence can be captured.
In fact, while parallel corpora provide stronger
(text-based) statistical evidence to detect transla-
tion pairs by analyzing term co-occurrences in
translated documents, comparable corpora pro-
vides weaker (term-based) evidence, because text
alignments are not available.
In this paper we present some solutions to deal
with CLTC according to the availability of bilin-
gual resources, and we show that it is possible
to deal with the problem even when no such re-
sources are accessible. The core technique relies
on the automatic acquisition of Multilingual Do-
main Models (MDMs) from comparable corpora.
This allows us to define a kernel function (i.e. a
similarity function among documents in different
languages) that is then exploited inside a Support
Vector Machines classification framework. We
also investigate this problem exploiting synset-
aligned multilingual WordNets and standard bilin-
gual dictionaries (e.g. Collins).
Experiments show the effectiveness of our ap-
proach, providing a simple and low cost solu-
tion for the Cross-Language Text Categorization
task. In particular, when bilingual dictionar-
ies/repositories are available, the performance of
the categorization gets close to that of monolin-
gual TC.
The paper is structured as follows. Section 2
briefly discusses the notion of comparable cor-
pora. Section 3 shows how to perform cross-
lingual TC when no bilingual dictionaries are
available and it is possible to rely on a compa-
rability assumption. Section 4 present a more
elaborated technique to acquire MDMs exploiting
bilingual resources, such as MultiWordNet (i.e.
a synset-aligned WordNet) and Collins bilingual
dictionary. Section 5 evaluates our methodolo-
gies and Section 6 concludes the paper suggesting
some future developments.
2 Comparable Corpora
Comparable corpora are collections of texts in dif-
ferent languages regarding similar topics (e.g. a
collection of news published by agencies in the
same period). More restrictive requirements are
expected for parallel corpora (i.e. corpora com-
posed of texts which are mutual translations),
while the class of the multilingual corpora (i.e.
collection of texts expressed in different languages
without any additional requirement) is the more
general. Obviously parallel corpora are also com-
parable, while comparable corpora are also multi-
lingual.
In a more precise way, let L =
{L1, L2, . . . , Ll} be a set of languages, let
T i = {ti1, ti2, . . . , tin} be a collection of texts ex-
pressed in the language Li ? L, and let ?(tjh, tiz)
be a function that returns 1 if tiz is the translation
of tjh and 0 otherwise. A multilingual corpus is
the collection of texts defined by T ? = ?i T i. If
the function ? exists for every text tiz ? T ? and
for every language Lj , and is known, then the
corpus is parallel and aligned at document level.
For the purpose of this paper it is enough to as-
sume that two corpora are comparable, i.e. they
are composed of documents about the same top-
ics and produced in the same period (e.g. possibly
from different news agencies), and it is not known
if a function ? exists, even if in principle it could
exist and return 1 for a strict subset of document
pairs.
The texts inside comparable corpora, being
about the same topics, should refer to the same
concepts by using various expressions in different
languages. On the other hand, most of the proper
nouns, relevant entities and words that are not yet
lexicalized in the language, are expressed by using
their original terms. As a consequence the same
entities will be denoted with the same words in
different languages, allowing us to automatically
detect couples of translation pairs just by look-
ing at the word shape (Koehn and Knight, 2002).
Our hypothesis is that comparable corpora contain
a large amount of such words, just because texts,
referring to the same topics in different languages,
will often adopt the same terms to denote the same
entities1 .
1According to our assumption, a possible additional cri-
554
However, the simple presence of these shared
words is not enough to get significant results in
CLTC tasks. As we will see, we need to exploit
these common words to induce a second-order
similarity for the other words in the lexicons.
2.1 The Multilingual Vector Space Model
Let T = {t1, t2, . . . , tn} be a corpus, and V =
{w1, w2, . . . , wk} be its vocabulary. In the mono-
lingual settings, the Vector Space Model (VSM)
is a k-dimensional space Rk, in which the text
tj ? T is represented by means of the vector ~tj
such that the zth component of ~tj is the frequency
of wz in tj . The similarity among two texts in the
VSM is then estimated by computing the cosine of
their vectors in the VSM.
Unfortunately, such a model cannot be adopted
in the multilingual settings, because the VSMs of
different languages are mainly disjoint, and the
similarity between two texts in different languages
would always turn out to be zero. This situation
is represented in Figure 1, in which both the left-
bottom and the rigth-upper regions of the matrix
are totally filled by zeros.
On the other hand, the assumption of corpora
comparability seen in Section 2, implies the pres-
ence of a number of common words, represented
by the central rows of the matrix in Figure 1.
As we will show in Section 5, this model is
rather poor because of its sparseness. In the next
section, we will show how to use such words as
seeds to induce a Multilingual Domain VSM, in
which second order relations among terms and
documents in different languages are considered
to improve the similarity estimation.
3 Exploiting Comparable Corpora
Looking at the multilingual term-by-document
matrix in Figure 1, a first attempt to merge the
subspaces associated to each language is to exploit
the information provided by external knowledge
sources, such as bilingual dictionaries, e.g. col-
lapsing all the rows representing translation pairs.
In this setting, the similarity among texts in dif-
ferent languages could be estimated by exploit-
ing the classical VSM just described. However,
the main disadvantage of this approach to esti-
mate inter-lingual text similarity is that it strongly
terion to decide whether two corpora are comparable is to
estimate the percentage of terms in the intersection of their
vocabularies.
relies on the availability of a multilingual lexical
resource. For languages with scarce resources a
bilingual dictionary could be not easily available.
Secondly, an important requirement of such a re-
source is its coverage (i.e. the amount of possible
translation pairs that are actually contained in it).
Finally, another problem is that ambiguous terms
could be translated in different ways, leading us to
collapse together rows describing terms with very
different meanings. In Section 4 we will see how
the availability of bilingual dictionaries influences
the techniques and the performance. In the present
Section we want to explore the case in which such
resources are supposed not available.
3.1 Multilingual Domain Model
A MDM is a multilingual extension of the concept
of Domain Model. In the literature, Domain Mod-
els have been introduced to represent ambiguity
and variability (Gliozzo et al, 2004) and success-
fully exploited in many NLP applications, such as
Word Sense Disambiguation (Strapparava et al,
2004), Text Categorization and Term Categoriza-
tion.
A Domain Model is composed of soft clusters
of terms. Each cluster represents a semantic do-
main, i.e. a set of terms that often co-occur in
texts having similar topics. Such clusters iden-
tify groups of words belonging to the same seman-
tic field, and thus highly paradigmatically related.
MDMs are Domain Models containing terms in
more than one language.
A MDM is represented by a matrix D, contain-
ing the degree of association among terms in all
the languages and domains, as illustrated in Table
1. For example the term virus is associated to both
MEDICINE COMPUTER SCIENCE
HIV e/i 1 0
AIDSe/i 1 0
viruse/i 0.5 0.5
hospitale 1 0
laptope 0 1
Microsofte/i 0 1
clinicai 1 0
Table 1: Example of Domain Matrix. we denotes
English terms, wi Italian terms and we/i the com-
mon terms to both languages.
the domain COMPUTER SCIENCE and the domain
MEDICINE while the domain MEDICINE is associ-
ated to both the terms AIDS and HIV. Inter-lingual
555
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
English texts Italian texts
te1 te2 ? ? ? ten?1 ten ti1 ti2 ? ? ? tim?1 tim
we1 0 1 ? ? ? 0 1 0 0 ? ? ?
English
Lexicon
we2 1 1 ? ? ? 1 0 0
. . .
... . . . . . . . . . . . . . . . . . . . . . . . ... 0 ...
wep?1 0 1 ? ? ? 0 0
. . . 0
wep 0 1 ? ? ? 0 0 ? ? ? 0 0
common wi we/i1 0 1 ? ? ? 0 0 0 0 ? ? ? 1 0... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
wi1 0 0 ? ? ? 0 1 ? ? ? 1 1
Italian
Lexicon
wi2 0
. . . 1 1 ? ? ? 0 1
... ... 0 ... . . . . . . . . . . . . . . . . . . . . . . . .
wiq?1
. . . 0 0 1 ? ? ? 0 1
wiq ? ? ? 0 0 0 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Multilingual term-by-document matrix
domain relations are captured by placing differ-
ent terms of different languages in the same se-
mantic field (as for example HIV e/i, AIDSe/i,
hospitale, and clinicai). Most of the named enti-
ties, such as Microsoft and HIV are expressed us-
ing the same string in both languages.
Formally, let V i = {wi1, wi2, . . . , wiki} be the
vocabulary of the corpus T i composed of doc-
ument expressed in the language Li, let V ? =
?
i V i be the set of all the terms in all the lan-
guages, and let k? = |V ?| be the cardinality of
this set. Let D = {D1, D2, ..., Dd} be a set of do-
mains. A DM is fully defined by a k? ? d domain
matrix D representing in each cell di,z the domain
relevance of the ith term of V ? with respect to the
domain Dz . The domain matrix D is used to de-
fine a function D : Rk? ? Rd, that maps the doc-
ument vectors ~tj expressed into the multilingual
classical VSM (see Section 2.1), into the vectors
~t?j in the multilingual domain VSM. The function
D is defined by2
D(~tj) = ~tj(IIDFD) = ~t?j (1)
where IIDF is a diagonal matrix such that iIDFi,l =
IDF (wli), ~tj is represented as a row vector, and
IDF (wli) is the Inverse Document Frequency of
2In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM
is a particular instance.
wli evaluated in the corpus T l.
In this work we exploit Latent Semantic Anal-
ysis (LSA) (Deerwester et al, 1990) to automat-
ically acquire a MDM from comparable corpora.
LSA is an unsupervised technique for estimating
the similarity among texts and terms in a large
corpus. In the monolingual settings LSA is per-
formed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix
T describing the corpus. SVD decomposes the
term-by-document matrix T into three matrixes
T ' V?k?UT where ?k? is the diagonal k ? k
matrix containing the highest k?  k eigenval-
ues of T, and all the remaining elements are set
to 0. The parameter k? is the dimensionality of
the Domain VSM and can be fixed in advance (i.e.
k? = d).
In the literature (Littman et al, 1998) LSA
has been used in multilingual settings to define
a multilingual space in which texts in different
languages can be represented and compared. In
that work LSA strongly relied on the availability
of aligned parallel corpora: documents in all the
languages are represented in a term-by-document
matrix (see Figure 1) and then the columns corre-
sponding to sets of translated documents are col-
lapsed (i.e. they are substituted by their sum) be-
fore starting the LSA process. The effect of this
step is to merge the subspaces (i.e. the right and
the left sectors of the matrix in Figure 1) in which
556
the documents have been originally represented.
In this paper we propose a variation of this strat-
egy, performing a multilingual LSA in the case in
which an aligned parallel corpus is not available.
It exploits the presence of common words among
different languages in the term-by-document ma-
trix. The SVD process has the effect of creating a
LSA space in which documents in both languages
are represented. Of course, the higher the number
of common words, the more information will be
provided to the SVD algorithm to find common
LSA dimension for the two languages. The re-
sulting LSA dimensions can be perceived as mul-
tilingual clusters of terms and document. LSA can
then be used to define a Multilingual Domain Ma-
trix DLSA. For further details see (Gliozzo and
Strapparava, 2005).
As Kernel Methods are the state-of-the-art su-
pervised framework for learning and they have
been successfully adopted to approach the TC task
(Joachims, 2002), we chose this framework to per-
form all our experiments, in particular Support
Vector Machines3 . Taking into account the exter-
nal knowledge provided by a MDM it is possible
estimate the topic similarity among two texts ex-
pressed in different languages, with the following
kernel:
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(2)
where D is defined as in equation 1.
Note that when we want to estimate the similar-
ity in the standard Multilingual VSM, as described
in Section 2.1, we can use a simple bag of words
kernel. The BoW kernel is a particular case of the
Domain Kernel, in which D = I, and I is the iden-
tity matrix. In the evaluation typically we consider
the BoW Kernel as a baseline.
4 Exploiting Bilingual Dictionaries
When bilingual resources are available it is possi-
ble to augment the the ?common? portion of the
matrix in Figure 1. In our experiments we ex-
ploit two alternative multilingual resources: Mul-
tiWordNet and the Collins English-Italian bilin-
gual dictionary.
3We adopted the efficient implementation freely available
at http://svmlight.joachims.org/.
MultiWordNet4. It is a multilingual computa-
tional lexicon, conceived to be strictly aligned
with the Princeton WordNet. The available lan-
guages are Italian, Spanish, Hebrew and Roma-
nian. In our experiment we used the English and
the Italian components. The last version of the
Italian WordNet contains around 58,000 Italian
word senses and 41,500 lemmas organized into
32,700 synsets aligned whenever possible with
WordNet English synsets. The Italian synsets
are created in correspondence with the Princeton
WordNet synsets, whenever possible, and seman-
tic relations are imported from the corresponding
English synsets. This implies that the synset index
structure is the same for the two languages.
Thus for the all the monosemic words, we aug-
ment each text in the dataset with the correspond-
ing synset-id, which act as an expansion of the
?common? terms of the matrix in Figure 1. Adopt-
ing the methodology described in Section 3.1, we
exploit these common sense-indexing to induce
a second-order similarity for the other terms in
the lexicons. We evaluate the performance of the
cross-lingual text categorization, using both the
BoW Kernel and the Multilingual Domain Kernel,
observing that also in this case the leverage of the
external knowledge brought by the MDM is effec-
tive.
It is also possible to augment each text with all
the synset-ids of all the words (i.e. monosemic and
polysemic) present in the dataset, hoping that the
SVM machine learning device cut off the noise
due to the inevitable spurious senses introduced in
the training examples. Obviously in this case, dif-
ferently from the ?monosemic? enrichment seen
above, it does not make sense to apply any dimen-
sionality reduction supplied by the Multilingual
Domain Model (i.e. the resulting second-order re-
lations among terms and documents produced on
a such ?extended? corpus should not be meaning-
ful)5.
Collins. The Collins machine-readable bilingual
dictionary is a medium size dictionary includ-
ing 37,727 headwords in the English Section and
32,602 headwords in the Italian Section.
This is a traditional dictionary, without sense in-
dexing like the WordNet repository. In this case
4Available at http://multiwordnet.itc.it.
5The use of a WSD system would help in this issue. How-
ever the rationale of this paper is to see how far it is possible
to go with very few resources. And we suppose that a multi-
lingual all-words WSD system is not easily available.
557
English Italian
Categories Training Test Total Training Test Total
Quality of Life 5759 1989 7748 5781 1901 7682
Made in Italy 5711 1864 7575 6111 2068 8179
Tourism 5731 1857 7588 6090 2015 8105
Culture and School 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
Table 2: Number of documents in the data set partitions
we follow the way, for each text of one language,
to augment all the present words with the transla-
tion words found in the dictionary. For the same
reason, we chose not to exploit the MDM, while
experimenting along this way.
5 Evaluation
The CLTC task has been rarely attempted in the
literature, and standard evaluation benchmark are
not available. For this reason, we developed
an evaluation task by adopting a news corpus
kindly put at our disposal by AdnKronos, an im-
portant Italian news provider. The corpus con-
sists of 32,354 Italian and 27,821 English news
partitioned by AdnKronos into four fixed cat-
egories: QUALITY OF LIFE, MADE IN ITALY,
TOURISM, CULTURE AND SCHOOL. The En-
glish and the Italian corpora are comparable, in
the sense stated in Section 2, i.e. they cover the
same topics and the same period of time. Some
news stories are translated in the other language
(but no alignment indication is given), some oth-
ers are present only in the English set, and some
others only in the Italian. The average length of
the news stories is about 300 words. We randomly
split both the English and Italian part into 75%
training and 25% test (see Table 2). We processed
the corpus with PoS taggers, keeping only nouns,
verbs, adjectives and adverbs.
Table 3 reports the vocabulary dimensions of
the English and Italian training partitions, the vo-
cabulary of the merged training, and how many
common lemmata are present (about 14% of the
total). Among the common lemmata, 97% are
nouns and most of them are proper nouns. Thus
the initial term-by-document matrix is a 43,384 ?
45,132 matrix, while the DLSA was acquired us-
ing 400 dimensions.
As far as the CLTC task is concerned, we tried
the many possible options. In all the cases we
trained on the English part and we classified the
Italian part, and we trained on the Italian and clas-
# lemmata
English training 22,704
Italian training 26,404
English + Italian 43,384
common lemmata 5,724
Table 3: Number of lemmata in the training parts
of the corpus
sified on the English part. When used, the MDM
was acquired running the SVD only on the joint
(English and Italian) training parts.
Using only comparable corpora. Figure 2 re-
ports the performance without any use of bilingual
dictionaries. Each graph show the learning curves
respectively using a BoW kernel (that is consid-
ered here as a baseline) and the multilingual do-
main kernel. We can observe that the latter largely
outperform a standard BoW approach. Analyzing
the learning curves, it is worth noting that when
the quantity of training increases, the performance
becomes better and better for the Multilingual Do-
main Kernel, suggesting that with more available
training it could be possible to improve the results.
Using bilingual dictionaries. Figure 3 reports
the learning curves exploiting the addition of the
synset-ids of the monosemic words in the corpus.
As expected the use of a multilingual repository
improves the classification results. Note that the
MDM outperforms the BoW kernel.
Figure 4 shows the results adding in the English
and Italian parts of the corpus all the synset-ids
(i.e. monosemic and polisemic) and all the transla-
tions found in the Collins dictionary respectively.
These are the best results we get in our experi-
ments. In these figures we report also the perfor-
mance of the corresponding monolingual TC (we
used the SVM with the BoW kernel), which can
be considered as an upper bound. We can observe
that the CLTC results are quite close to the perfor-
mance obtained in the monolingual classification
tasks.
558
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on English, test on Italian)
Multilingual Domain Kernel
Bow Kernel
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on Italian, test on English)
Multilingual Domain Kernel
Bow Kernel
Figure 2: Cross-language learning curves: no use of bilingual dictionaries
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on English, test on Italian)
Multilingual Domain Kernel
Bow Kernel
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on Italian, test on English)
Multilingual Domain Kernel
Bow Kernel
Figure 3: Cross-language learning curves: monosemic synsets from MultiWordNet
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on English, test on Italian)
Monolingual (Italian) TC
Collins
MultiWordNet
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data (train on Italian, test on English)
Monolingual (English) TC
Collins
MultiWordNet
Figure 4: Cross-language learning curves: all synsets from MultiWordNet // All translations from Collins
559
6 Conclusion and Future Work
In this paper we have shown that the problem of
cross-language text categorization on comparable
corpora is a feasible task. In particular, it is pos-
sible to deal with it even when no bilingual re-
sources are available. On the other hand when it is
possible to exploit bilingual repositories, such as a
synset-aligned WordNet or a bilingual dictionary,
the obtained performance is close to that achieved
for the monolingual task. In any case we think
that our methodology is low-cost and simple, and
it can represent a technologically viable solution
for multilingual problems. For the future we try to
explore also the use of a word sense disambigua-
tion all-words system. We are confident that even
with the actual state-of-the-art WSD performance,
we can improve the actual results.
Acknowledgments
This work has been partially supported by the ON-
TOTEXT (From Text to Knowledge for the Se-
mantic Web) project, funded by the Autonomous
Province of Trento under the FUP-2004 program.
References
N. Bel, C. Koster, and M. Villegas. 2003. Cross-
lingual text categorization. In Proceedings of Eu-
ropean Conference on Digital Libraries (ECDL),
Trondheim, August.
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-and
sentence-aligned parallel corpora. In Proceedings of
ACL-04, Barcelona, Spain, July.
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41(6):391?407.
E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and
H. Dejean. 2004. A geometric view on bilingual
lexicon extraction from comparable corpora. In Pro-
ceedings of ACL-04, Barcelona, Spain, July.
A. Gliozzo and C. Strapparava. 2005. Cross language
text categorization by acquiring multilingual domain
models from comparable corpora. In Proc. of the
ACL Workshop on Building and Using Parallel Texts
(in conjunction of ACL-05), University of Michigan,
Ann Arbor, June.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
T. Joachims. 2002. Learning to Classify Text using
Support Vector Machines. Kluwer Academic Pub-
lishers.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of ACL Workshop on Unsupervised Lexical Acquisi-
tion, Philadelphia, July.
M. Littman, S. Dumais, and T. Landauer. 1998. Auto-
matic cross-language information retrieval using la-
tent semantic indexing. In G. Grefenstette, editor,
Cross Language Information Retrieval, pages 51?
62. Kluwer Academic Publishers.
D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
L. Rigutini, M. Maggini, and B. Liu. 2005. An EM
based training algorithm for cross-language text cat-
egorizaton. In Proceedings of Web Intelligence Con-
ference (WI-2005), Compie`gne, France, September.
C. Strapparava, A. Gliozzo, and C. Giuliano.
2004. Pattern abstraction and term similarity for
word sense disambiguation. In Proceedings of
SENSEVAL-3, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985.
Generalized vector space model in information re-
trieval. In Proceedings of the 8th ACM SIGIR Con-
ference.
560
Pattern Abstraction and Term Similarity for Word Sense Disambiguation:
IRST at Senseval-3
Carlo Strapparava and Alfio Gliozzo and Claudio Giuliano
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY
{strappa, gliozzo, giuliano}@itc.it
Abstract
This paper summarizes IRST?s participation in
Senseval-3. We participated both in the English all-
words task and in some lexical sample tasks (En-
glish, Basque, Catalan, Italian, Spanish). We fol-
lowed two perspectives. On one hand, for the all-
words task, we tried to refine the Domain Driven
Disambiguation that we presented at Senseval-2.
The refinements consist of both exploiting a new
technique (Domain Relevance Estimation) for do-
main detection in texts, and experimenting with the
use of Latent Semantic Analysis to avoid reliance on
manually annotated domain resources (e.g. WORD-
NET DOMAINS). On the other hand, for the lexical
sample tasks, we explored the direction of pattern
abstraction and we demonstrated the feasibility of
leveraging external knowledge using kernel meth-
ods.
1 Introduction
The starting point for our research in the Word
Sense Disambiguation (WSD) area was to explore
the use of semantic domains in order to solve lex-
ical ambiguity. At the Senseval-2 competition we
proposed a new approach to WSD, namely Domain
Driven Disambiguation (DDD). This approach con-
sists of comparing the estimated domain of the con-
text of the word to be disambiguated with the do-
mains of its senses, exploiting the property of do-
mains to be features of both texts and words. The
domains of the word senses can be either inferred
from the learning data or derived from the informa-
tion in WORDNET DOMAINS.
For Senseval-3, we refined the DDD methodol-
ogy with a fully unsupervised technique - Domain
Relevance Estimation (DRE) - for domain detection
in texts. DRE is performed by an expectation maxi-
mization algorithm for the gaussian mixture model,
which is exploited to differentiate relevant domain
information in texts from noise. This refined DDD
system was presented in the English all-words task.
Originally DDD was developed to assess the use-
fulness of domain information for WSD. Thus it
did not exploit other knowledge sources commonly
used for disambiguation (e.g. syntactic patterns or
collocations). As a consequence the performance of
the DDD system is quite good for precision (it dis-
ambiguates well the ?domain? words), but as far as
recall is concerned it is not competitive compared
with other state of the art techniques. On the other
hand DDD outperforms the state of the art for unsu-
pervised systems, demonstrating the usefulness of
domain information for WSD.
In addition, the DDD approach requires domain
annotations for word senses (for the experiments we
used WORDNET DOMAINS, a lexical resource de-
veloped at IRST). Like all manual annotations, such
an operation is costly (more than two man years
have been spent for labeling the whole WORDNET
DOMAINS structure) and affected by subjectivity.
Thus, one drawback of the DDD methodology was
a lack of portability among languages and among
different sense repositories (unless we have synset-
aligned WordNets).
Besides the improved DDD, our other proposals
for Senseval-3 constitute an attempt to overcome
these previous issues.
To deal with the problem of having a domain-
annotated WORDNET, we experimented with a
novel methodology to automatically acquire domain
information from corpora. For this aim we esti-
mated term similarity from a large scale corpus, ex-
ploiting the assumption that semantic domains are
sets of very closely related terms. In particular we
implemented a variation of Latent Semantic Analy-
sis (LSA) in order to obtain a vector representation
for words, texts and synsets. LSA performs a di-
mensionality reduction in the feature space describ-
ing both texts and words, capturing implicitly the
notion of semantic domains required by DDD. In
order to perform disambiguation, LSA vectors have
been estimated for the synsets in WORDNET. We
participated in the English all-words task also with
a first prototype (DDD-LSA) that exploits LSA in-
stead of WORDNET DOMAINS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Task Systems
English All-Words DDD DDD-LSA
English Lex-sample Kernels-WSD Ties
Italian Lex-Sample Kernels-WSD Ties
Basque Lex-Sample Kernels-WSD
Catalan Lex-Sample Kernels-WSD
Spanish Lex-Sample Kernels-WSD
Table 1: IRST participation at Senseval-3
As far as lexical sample tasks are concerned, we
participated in the English, Italian, Spanish, Cata-
lan, and Basque tasks. For these tasks, we ex-
plored the direction of pattern abstraction for WSD.
Pattern abstraction is an effective methodology for
WSD (Mihalcea, 2002). Our preliminary experi-
ments have been performed using TIES, a general-
ized Information Extraction environment developed
at IRST that implements the boosted wrapper induc-
tion algorithm (Freitag and Kushmerick, 2000). The
main limitation of such an approach is, once more,
the integration of different knowledge sources. In
particular, paradigmatic information seems hard to
be represented in the TIES framework, motivating
our decision to exploit kernel methods for WSD.
Kernel methods is an area of recent interest in
Machine Learning. Kernels are similarity functions
between instances that allows to integrate different
knowledge sources and to model explicitly linguis-
tic insights inside the powerful framework of sup-
port vector machine classification. For Senseval-3
we implemented the Kernels-WSD system, which
exploits kernel methods to perform the following
operations: (i) pattern abstraction; (ii) combination
of different knowledge sources, in particular domain
information and syntagmatic information; (iii) inte-
gration of unsupervised term proximity estimation
in the supervised framework.
The paper is structured as follows. Section 2 in-
troduces LSA and its relations with semantic do-
mains. Section 3 presents the systems for the En-
glish all-words task (i.e. DDD and DDD-LSA). In
section 4 our supervised approaches are reported.
In particular the TIES system is described in section
4.1, while the approach based on kernel methods is
discussed in section 4.2.
2 Semantic Domains and LSA
Domains are common areas of human discussion,
such as economics, politics, law, science etc., which
are at the basis of lexical coherence. A substantial
part of the lexicon is composed by ?domain words?,
that refer to concepts belonging to specific domains.
In (Magnini et al, 2002) it has been claimed that
domain information provides generalized features at
the paradigmatic level that are useful to discriminate
among word senses.
The WORDNET DOMAINS1 lexical resource
is an extension of WORDNET which provides
such domain labels for all synsets (Magnini and
Cavaglia`, 2000). About 200 domain labels were se-
lected from a number of dictionaries and then struc-
tured in a taxonomy according to the Dewey Deci-
mal Classification (DDC). The annotation method-
ology was mainly manual and took about 2 person
years.
WORDNET DOMAINS has been proven a useful
resource for WSD. However some aspects induced
us to explore further developments. These issues
are: (i) it is difficult to find an objective a-priori
model for domains; (ii) the annotation procedure
followed to develop WORDNET DOMAINS is very
expensive, making hard the replicability of the lexi-
cal resource for other languages or domain specific
sub-languages; (iii) the domain distinctions are rigid
in WORDNET DOMAINS, while a more ?fuzzy? as-
sociation between domains and concepts is often
more appropriate to describe term similarity.
In order to generalize the domain approach and to
overcome these issues, we explored the direction of
unsupervised learning on a large-scale corpus (we
used the BNC corpus for all the experiments de-
scribed in this paper).
In particular, we followed the LSA approach
(Deerwester et al, 1990). In LSA, term co-
occurrences in the documents of the corpus are cap-
tured by means of a dimensionality reduction oper-
ated on the term-by-document matrix. The result-
ing LSA vectors can be exploited to estimate both
term and document similarity. Regarding document
similarity, Latent Semantic Indexing (LSI) is a tech-
nique that allows one to represent a document by
a LSA vector. In particular, we used a variation
of the pseudo-document methodology described in
(Berry, 1992). Each document can be represented in
the LSA space by summing up the normalized LSA
vectors of all the terms contained in it.
By exploiting LSA vectors for terms, it is pos-
sible to estimate domain vectors for the synsets of
WORDNET, in order to obtain similarity values be-
tween concepts that can be used for synset cluster-
ing and WSD. Thus, term and document vectors can
be used instead of WORDNET DOMAINS for WSD
and other applications in which term similarity and
domain relevance estimation is required.
1WORDNET DOMAINS is freely available for research pur-
poses at wndomains.itc.it
3 All-Words systems: DDD and DDD-LSA
DDD with DRE. DDD assignes the right sense of
a word in its context comparing the domain of the
context to the domain of each sense of the word.
This methodology exploits WORDNET DOMAINS
information to estimate both the domain of the tex-
tual context and the domain of the senses of the
word to disambiguate.
The basic idea to estimate domain relevance for
texts is to exploit lexical coherence inside texts. A
simple heuristic approach to this problem, used in
Senseval-2, is counting the occurrences of domain
words for every domain inside the text: the higher
the percentage of domain words for a certain do-
main, the more relevant the domain will be for the
text.
Unfortunately, the simple local frequency count
is not a good domain relevance measure for sev-
eral reasons. Indeed irrelevant senses of ambigu-
ous words contribute to augment the final score of
irrelevant domains, introducing noise. Moreover,
the level of noise is different for different domains
because of their different sizes and possible dif-
ferences in the ambiguity level of their vocabular-
ies. We refined the original Senseval-2 DDD system
with the Domain Relevance Estimation (DRE) tech-
nique. Given a certain domain, DRE distinguishes
between relevant and non-relevant texts by means
of a Gaussian Mixture model that describes the fre-
quency distribution of domain words inside a large-
scale corpus (in particular we used the BNC corpus
also in this case). Then, an Expectation Maximiza-
tion algorithm computes the parameters that maxi-
mize the likelihood of the model on the empirical
data (Gliozzo et al, 2004).
In order to represent domain information we in-
troduced the notion of Domain Vectors (DV), which
are data structures that collect domain information.
These vectors are defined in a multidimensional
space, in which each domain represents a dimen-
sion of the space. We distinguish between two
kinds of DVs: (i) synset vectors, which represent
the relevance of a synset with respect to each con-
sidered domain and (ii) text vectors, which repre-
sent the relevance of a portion of text with respect
to each domain in the considered set. The core of
the DDD algorithm is based on scoring the compar-
ison of these kinds of vectors. The synset vectors
are built considering WORDNET DOMAINS, while
in the calculation of scoring the system takes into
account synset probabilities on SemCor. The sys-
tem makes use of a threshold th-cut, ranging in the
interval [0,1], that allows us to tune the tradeoff be-
tween precision and recall.
th-cut Prec Recall Attempted
0.0 0.583 0.583 99.76
0.9 0.729 0.441 60.51
Table 2: DDD on the English all-words task.
Latent Semantic Domains for DDD. As seen in
Section 2, it is possible to implement a DDD ver-
sion that does not use WORDNET DOMAINS and
instead it exploits LSA term and document vectors
for estimating synset vectors and text vectors, leav-
ing the core of DDD algorithm unchanged. As for
text vectors, we used the psedo-document technique
also for building synset vectors: in this case we con-
sider the synonymous terms contained in the synset
itself.
The system presented at Senseval-3 does not
make use of any statistics on SemCor, and conse-
quently it can be considered fully unsupervised. Re-
sults are reported in table 3 and do not differ much
from the results obtained by DDD in the same task.
th-cut Prec Recall Attempted
0.5 0.661 0.496 75.01
Table 3: DDD-LSA on the English all-words task.
4 Lexical Sample Systems: Pattern
abstraction and Kernel Methods
One of the most discriminative features for lexi-
cal disambiguation is the lexical/syntactic pattern in
which the word appears. A well known issue in the
WSD area is the one sense per collocation claim
(Yarowsky, 1993) stating that the word meanings
are strongly associated with the particular colloca-
tion in which the word is located. Collocations are
sequences of words in the context of the word to
disambiguate, and can be associated to word senses
performing supervised learning.
Another important knowledge source for WSD is
the shallow-syntactic pattern in which a word ap-
pears. Syntactic patterns, like lexical patterns, can
be obtained by exploiting pattern abstraction tech-
niques on POS sequences. In the WSD literature
both lexical and syntactic patterns have been used
as features in a supervised learning schema by rep-
resenting each instance using bigrams and trigrams
in the surrounding context of the word to be ana-
lyzed2.
2More recently deep-syntactic features have been also con-
sidered by several systems, as for example modifiers of nouns
and verbs, object and subject of the sentence, etc. In order to
Representing each instance by a ?bag of features?
presents several disadvantages from the point of
view of both machine learning and computational
linguistics: (1) Sparseness in the learning data: most
of the collocations found in the learning data occur
just once, reducing the generalization power of the
learning algorithm. In addition most of the collo-
cations found in the test data are often unseen in
the training data. (2) Low flexibility for pattern ab-
straction purposes: bigram and trigram extraction
schemata are fixed in advance. (3) Knowledge ac-
quisition bottleneck: the size of the training data is
not large enough to cover each possible collocation
in the language.
To overcome problems 1 and 2 we investigated
some pattern abstraction techniques from the area
of Information Extraction (IE) and we adapted them
to WSD. To overcome problem 3 we developed La-
tent Semantic Kernels, which allow us to integrate
external knowledge provided by unsupervised term
similarity estimation.
4.1 TIES
Our first experiments have been performed exploit-
ing TIES, an environment developed at IRST for IE
that induces patterns from the marked entities in the
training phase, and then applies those patterns in the
test phase in order to assign a category if the pat-
tern is satisfied. For our experiments, we used the
Boosted Wrapper Induction (BWI) algorithm (Fre-
itag and Kushmerick, 2000) that is implemented in
TIES.
For Senseval-3 we used very few features (lemma
and POS). We proposed the system in this configu-
ration as a ?baseline? system for pattern abstraction.
Task Prec Recall Attempted
English LS 0.706 0.505 71.50
English LS (coarse) 0.767 0.548 71.50
Italian LS 0.552 0.309 55.92
Table 4: Performance of the TIES system
Our preliminary experiments with BWI have
shown that pattern abstraction is very attractive for
WSD, allowing us to achieve a very high precision
for a restricted number of words, in which the syn-
tagmatic information is sufficient for disambigua-
tion. However, we still had some restrictions. In
particular, the integration with different knowledge
sources for classification is not trivial.
obtain such features parsing of the data is required. However,
we decided to do not use such information, while we plan to
introduce it in the next future.
4.2 Kernel-WSD
Our choice of exploiting kernel methods for WSD
has been motivated by the observation that pattern-
based approaches for disambiguation are comple-
mentary to the domain based ones: they require dif-
ferent knowledge sources and different techniques
for classification and feature description. Both ap-
proaches have to be simultaneously taken into ac-
count in order to perform accurate disambiguation.
Our aim was to combine them into a common
framework.
Kernel methods, e.g. Support Vector Machines
(SVMs), are state-of-the-art learning algorithms,
and they are successfully adopted in many NLP
tasks.
The idea of SVM (Cristianini and Shawe-Taylor,
2000) is to map the set of training data into a higher-
dimensional feature space F via a mapping func-
tion ? : ? ? F , and construct a separating hy-
perplane with maximum margin (distance between
planes and closest points) in the new space. Gen-
erally, this yields a nonlinear decision boundary in
the input space. Since the feature space is high di-
mensional, performing the transformation has of-
ten a high computational cost. Rather than use the
explicit mapping ?, we can use a kernel function
K : ??? ? < , that corresponds to the inner prod-
uct in a feature space which is, in general, different
from the input space.
Therefore, a kernel function provides a way
to compute (efficiently) the separating hyperplane
without explicitly carrying out the map ? into the
feature space - this is called the kernel trick. In this
way the kernel acts as an interface between the data
and the learning algorithm by defining an implicit
mapping into the feature space. Intuitively, we can
see the kernel as a function that measures the sim-
ilarity between pairs of objects. The learning algo-
rithm, which compares all pairs of data items, ex-
ploits the information encoded in the kernel. An
important characteristic of kernels is that they are
not limited to vector objects but are applicable to
virtually any kind of object representation.
In this work we use kernel methods to combine
heterogeneous sources of information that we found
relevant for WSD. For each of these aspects it is
possible to define kernels independently. Then they
are combined by exploiting the property that the
sum of two kernels is still a kernel (i.e. k(x, y) =
k1(x, y) + k2(x, y)), taking advantage of each sin-
gle contribution in an intuitive way3.
3In order to keep the kernel values comparable for dif-
ferent values and to be independent from the length of the
examples, we considered the normalized version K?(x, y) =
lsa Task Prec Recall Attempted MF-Baseline
? English LS 0.726 0.726 100 0.552
? English LS (coarse) 0.795 0.795 100 0.645
- English LS (no-lsa) 0.704 0.704 100 0.552
- Basque LS 0.655 0.655 100 0.558
- Italian LS 0.531 0.531 100 0.183
- Catalan LS 0.858 0.846 98.62 0.663
- Spanish LS 0.842 0.842 100 0.677
Table 5: Performance of the Kernels-WSD system
The Word Sense Disambiguation Kernel is de-
fined in this way:
KWSD(x, y) = KS(x, y) + KP (x, y) (1)
where KS is the Syntagmatic Kernel and KP is
the Paradigmatic Kernel.
The Syntagmatic Kernel. The syntagmatic ker-
nel generalizes the word-sequence kernels defined
by (Cancedda et al, 2003) to sequences of lem-
mata and POSs. Word sequence kernels are based
on the following idea: two sequences are similar
if they have in common many sequences of words
in a given order. The similarity between two ex-
amples is assessed by the number (possibly non-
contiguous) of the word sequences matching. Non-
contiguous occurrences are penalized according to
the number of gaps they contain. For example the
sequence of words ?I go very quickly to school? is
less similar to ?I go to school? than ?I go quickly to
school?. Different than the bag-of-word approach,
word sequence kernels capture the word order and
allow gaps between words. The word sequence ker-
nels are parametric with respect to the length of the
(sparse) sequences they want to capture.
We have defined the syntagmatic kernel as the
sum of n distinct word-sequence kernels for lem-
mata (i.e. Collocation Kernel - KC ) and sequences
of POSs (i.e. POS Kernel - KPOS), according to the
formula (for our experiments we set n to 2):
KS(x, y) =
n
X
i=1
KCi(x, y) +
n
X
i=1
KPOSi(x, y) (2)
In the above definition of syntagmatic kernel,
only exact lemma/POS matches contribute to the
similarity. One shortcoming of this approach is
that (near-)synonyms will never be considered sim-
ilar. We address this problem by considering soft-
matching of words employing a term similarity
K(x, y)/sqrt(K(x, x)K(y, y))
measure based on LSA4. In particular we consid-
ered equivalent two words having the same POS and
a similarity value higher than an empirical thresh-
old. For example, if we consider as equivalent
the terms Ronaldo and football player the sequence
The football player scored the first goal can be con-
sidered equivalent to the sentence Ronaldo scored
the first goal. The properties of the kernel methods
offer a flexible way to plug additional information,
in this case unsupervised (we could also take this in-
formation from a semantic network such as WORD-
NET).
The Paradigmatic Kernel. The paradigmatic
kernel takes into account the paradigmatic aspect of
sense distinction (i.e. domain aspects) (Gliozzo et
al., 2004). For example the word virus can be dis-
ambiguated by recognizing the domain of the con-
text in which it is placed (e.g. computer science
vs. biology). Usually such an aspect is captured
by ?bag-of-words?, in analogy to the Vector Space
Model, widely used in Text Categorization and In-
formation Retrieval. The main limitation of this
model for WSD is the knowledge acquisition bot-
tleneck (i.e. the lack of sense tagged data). Bag of
words are very sparse data that require a large scale
corpus to be learned. To overcome such a limita-
tion, Latent Semantic Indexing (LSI) can provide a
solution.
Thus we defined a paradigmatic kernel composed
by the sum of a ?traditional? bag of words kernel
and an LSI kernel (Cristianini et al, 2002) as de-
fined by formula 3:
KP (x, y) = KBoW (x, y) + KLSI(x, y) (3)
where KBoW computes the inner product be-
tween the vector space model representations and
KLSI computes the cosine between the LSI vectors
representing the texts.
4For languages other than English, we did not exploit this
soft-matching and the KLSI kernel described below. See the
first column in the table 5.
Table 5 displays the performance of Kernel-
WSD. As a comparison, we also report the figures
on the English task without using LSA. The last col-
umn reports the recall of the most-frequent baseline.
Acknowledgments
Claudio Giuliano is supported by the IST-Dot.Kom
project sponsored by the European Commission
(Framework V grant IST-2001-34038). TIES and
the kernel package have been developed in the con-
text of the Dot.Kom project.
References
M. Berry. 1992. Large-scale sparse singular value
computations. International Journal of Super-
computer Applications, 6(1):13?49.
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Ren-
ders. 2003. Word-sequence kernels. Journal of
Machine Learning Research, 3(6):1059?1082.
N. Cristianini and J. Shawe-Taylor. 2000. Support
Vector Machines. Cambridge University Press.
N. Cristianini, J. Shawe-Taylor, and H. Lodhi.
2002. Latent semantic kernels. Journal of Intel-
ligent Information Systems, 18(2):127?152.
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science, 41(6):391?407.
D. Freitag and N. Kushmerick. 2000. Boosted
wrapper induction. In Proc. of AAAI-00, pages
577?583, Austin, Texas.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004.
Unsupervised and supervised exploitation of se-
mantic domains in lexical disambiguation. Com-
puter Speech and Language, Forthcoming.
B. Magnini and G. Cavaglia`. 2000. Integrating sub-
ject field codes into WordNet. In Proceedings of
LREC-2000, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
R. F. Mihalcea. 2002. Word sense disambiguation
with pattern learning and automatic feature selec-
tion. Natural Language Engineering, 8(4):343?
358.
D. Yarowsky. 1993. One sense per collocation. In
Proceedings of ARPA Human Language Technol-
ogy Workshop, pages 266?271, Princeton.
Unsupervised Domain Relevance Estimation
for Word Sense Disambiguation
Alfio Gliozzo and Bernardo Magnini and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY
{gliozzo, magnini, strappa}@itc.it
Abstract
This paper presents Domain Relevance Estima-
tion (DRE), a fully unsupervised text categorization
technique based on the statistical estimation of the
relevance of a text with respect to a certain cate-
gory. We use a pre-defined set of categories (we
call them domains) which have been previously as-
sociated to WORDNET word senses. Given a cer-
tain domain, DRE distinguishes between relevant
and non-relevant texts by means of a Gaussian Mix-
ture model that describes the frequency distribution
of domain words inside a large-scale corpus. Then,
an Expectation Maximization algorithm computes
the parameters that maximize the likelihood of the
model on the empirical data.
The correct identification of the domain of the
text is a crucial point for Domain Driven Dis-
ambiguation, an unsupervised Word Sense Disam-
biguation (WSD) methodology that makes use of
only domain information. Therefore, DRE has been
exploited and evaluated in the context of a WSD
task. Results are comparable to those of state-of-
the-art unsupervised WSD systems and show that
DRE provides an important contribution.
1 Introduction
A fundamental issue in text processing and under-
standing is the ability to detect the topic (i.e. the do-
main) of a text or of a portion of it. Indeed, domain
detection allows a number of useful simplifications
in text processing applications, such as, for instance,
in Word Sense Disambiguation (WSD).
In this paper we introduce Domain Relevance Es-
timation (DRE) a fully unsupervised technique for
domain detection. Roughly speaking, DRE can be
viewed as a text categorization (TC) problem (Se-
bastiani, 2002), even if we do not approach the
problem in the standard supervised setting requir-
ing category labeled training data. In fact, recently,
unsupervised approaches to TC have received more
and more attention in the literature (see for example
(Ko and Seo, 2000).
We assume a pre-defined set of categories, each
defined by means of a list of related terms. We
call such categories domains and we consider them
as a set of general topics (e.g. SPORT, MEDICINE,
POLITICS) that cover the main disciplines and ar-
eas of human activity. For each domain, the list
of related words is extracted from WORDNET DO-
MAINS (Magnini and Cavaglia`, 2000), an extension
of WORDNET in which synsets are annotated with
domain labels. We have identified about 40 domains
(out of 200 present in WORDNET DOMAINS) and
we will use them for experiments throughout the pa-
per (see Table 1).
DRE focuses on the problem of estimating a de-
gree of relatedness of a certain text with respect to
the domains in WORDNET DOMAINS.
The basic idea underlying DRE is to combine the
knowledge in WORDNET DOMAINS and a proba-
bilistic framework which makes use of a large-scale
corpus to induce domain frequency distributions.
Specifically, given a certain domain, DRE considers
frequency scores for both relevant and non-relevant
texts (i.e. texts which introduce noise) and represent
them by means of a Gaussian Mixture model. Then,
an Expectation Maximization algorithm computes
the parameters that maximize the likelihood of the
empirical data.
DRE methodology originated from the effort to
improve the performance of Domain Driven Dis-
ambiguation (DDD) system (Magnini et al, 2002).
DDD is an unsupervised WSD methodology that
makes use of only domain information. DDD as-
signes the right sense of a word in its context com-
paring the domain of the context to the domain of
each sense of the word. This methodology exploits
WORDNET DOMAINS information to estimate both
Domain #Syn Domain #Syn Domain #Syn
Factotum 36820 Biology 21281 Earth 4637
Psychology 3405 Architecture 3394 Medicine 3271
Economy 3039 Alimentation 2998 Administration 2975
Chemistry 2472 Transport 2443 Art 2365
Physics 2225 Sport 2105 Religion 2055
Linguistics 1771 Military 1491 Law 1340
History 1264 Industry 1103 Politics 1033
Play 1009 Anthropology 963 Fashion 937
Mathematics 861 Literature 822 Engineering 746
Sociology 679 Commerce 637 Pedagogy 612
Publishing 532 Tourism 511 Computer Science 509
Telecommunication 493 Astronomy 477 Philosophy 381
Agriculture 334 Sexuality 272 Body Care 185
Artisanship 149 Archaeology 141 Veterinary 92
Astrology 90
Table 1: Domain distribution over WORDNET synsets.
the domain of the textual context and the domain of
the senses of the word to disambiguate. The former
operation is intrinsically an unsupervised TC task,
and the category set used has to be the same used
for representing the domain of word senses.
Since DRE makes use of a fixed set of target cat-
egories (i.e. domains) and since a document col-
lection annotated with such categories is not avail-
able, evaluating the performance of the approach is
a problem in itself. We have decided to perform an
indirect evaluation using the DDD system, where
unsupervised TC plays a crucial role.
The paper is structured as follows. Section 2
introduces WORDNET DOMAINS, the lexical re-
source that provides the underlying knowledge to
the DRE technique. In Section 3 the problem of es-
timating domain relevance for a text is introduced.
In particular, Section 4 briefly sketchs the WSD sys-
tem used for evaluation. Finally, Section 5 describes
a number of evaluation experiments we have carried
out.
2 Domains, WORDNET and Texts
DRE heavily relies on domain information as its
main knowledge source. Domains show interesting
properties both from a lexical and a textual point of
view. Among these properties there are: (i) lexi-
cal coherence, since part of the lexicon of a text is
composed of words belonging to the same domain;
(ii) polysemy reduction, because the potential am-
biguity of terms is sensibly lower if the domain of
the text is specified; and (iii) lexical identifiability
of text?s domain, because it is always possible to as-
sign one or more domains to a given text by consid-
ering term distributions in a bag-of-words approach.
Experimental evidences of these properties are re-
ported in (Magnini et al, 2002).
In this section we describe WORDNET DO-
MAINS1 (Magnini and Cavaglia`, 2000), a lexical re-
source that attempts a systematization of relevant
aspects in domain organization and representation.
WORDNET DOMAINS is an extension of WORD-
NET (version 1.6) (Fellbaum, 1998), in which each
synset is annotated with one or more domain la-
bels, selected from a hierarchically organized set of
about two hundred labels. In particular, issues con-
cerning the ?completeness? of the domain set, the
?balancing? among domains and the ?granularity?
of domain distinctions, have been addressed. The
domain set used in WORDNET DOMAINS has been
extracted from the Dewey Decimal Classification
(Comaroni et al, 1989), and a mapping between the
two taxonomies has been computed in order to en-
sure completeness. Table 2 shows how the senses
for a word (i.e. the noun bank) have been associated
to domain label; the last column reports the number
of occurrences of each sense in Semcor2.
Domain labeling is complementary to informa-
tion already present in WORDNET. First of all,
a domain may include synsets of different syn-
tactic categories: for instance MEDICINE groups
together senses from nouns, such as doctor#1
and hospital#1, and from verbs, such as
operate#7. Second, a domain may include
senses from different WORDNET sub-hierarchies
(i.e. deriving from different ?unique beginners? or
from different ?lexicographer files?). For example,
SPORT contains senses such as athlete#1, deriv-
ing from life form#1, game equipment#1
from physical object#1, sport#1
1WORDNET DOMAINS is freely available at
http://wndomains.itc.it
2SemCor is a portion of the Brown corpus in which words
are annotated with WORDNET senses.
Sense Synset and Gloss Domains Semcor frequencies
#1 depository financial institution, bank, banking con-
cern, banking company (a financial institution. . . )
ECONOMY 20
#2 bank (sloping land. . . ) GEOGRAPHY, GEOLOGY 14
#3 bank (a supply or stock held in reserve. . . ) ECONOMY -
#4 bank, bank building (a building. . . ) ARCHITECTURE, ECONOMY -
#5 bank (an arrangement of similar objects...) FACTOTUM 1
#6 savings bank, coin bank, money box, bank (a con-
tainer. . . )
ECONOMY -
#7 bank (a long ridge or pile. . . ) GEOGRAPHY, GEOLOGY 2
#8 bank (the funds held by a gambling house. . . ) ECONOMY, PLAY
#9 bank, cant, camber (a slope in the turn of a road. . . ) ARCHITECTURE -
#10 bank (a flight maneuver. . . ) TRANSPORT -
Table 2: WORDNET senses and domains for the word ?bank?.
from act#2, and playing field#1 from
location#1.
Domains may group senses of the same word
into thematic clusters, which has the important side-
effect of reducing the level of ambiguity when we
are disambiguating to a domain. Table 2 shows
an example. The word ?bank? has ten differ-
ent senses in WORDNET 1.6: three of them (i.e.
bank#1, bank#3 and bank#6) can be grouped
under the ECONOMY domain, while bank#2 and
bank#7 both belong to GEOGRAPHY and GEOL-
OGY. Grouping related senses is an emerging topic
in WSD (see, for instance (Palmer et al, 2001)).
Finally, there are WORDNET synsets that do not
belong to a specific domain, but rather appear in
texts associated with any domain. For this reason,
a FACTOTUM label has been created that basically
includes generic synsets, which appear frequently
in different contexts. Thus the FACTOTUM domain
can be thought of as a ?placeholder? for all other
domains.
3 Domain Relevance Estimation for Texts
The basic idea of domain relevance estimation for
texts is to exploit lexical coherence inside texts.
From the domain point of view lexical coherence
is equivalent to domain coherence, i.e. the fact that
a great part of the lexicon inside a text belongs to
the same domain.
From this observation follows that a simple
heuristic to approach this problem is counting the
occurrences of domain words for every domain in-
side the text: the higher the percentage of domain
words for a certain domain, the more relevant the
domain will be for the text. In order to perform this
operation the WORDNET DOMAINS information is
exploited, and each word is assigned a weighted list
of domains considering the domain annotation of
its synsets. In addition, we would like to estimate
the domain of the text locally. Local estimation
of domain relevance is very important in order to
take into account domain shifts inside the text. The
methodology used to estimate domain frequency is
described in subsection 3.1.
Unfortunately the simple local frequency count
is not a good domain relevance measure for sev-
eral reasons. The most significant one is that very
frequent words have, in general, many senses be-
longing to different domains. When words are used
in texts, ambiguity tends to disappear, but it is not
possible to assume knowing their actual sense (i.e.
the sense in which they are used in the context) in
advance, especially in a WSD framework. The sim-
ple frequency count is then inadequate for relevance
estimation: irrelevant senses of ambiguous words
contribute to augment the final score of irrelevant
domains, introducing noise. The level of noise is
different for different domains because of their dif-
ferent sizes and possible differences in the ambigu-
ity level of their vocabularies.
In subsection 3.2 we propose a solution for that
problem, namely the Gaussian Mixture (GM) ap-
proach. This constitutes an unsupervised way to es-
timate how to differentiate relevant domain infor-
mation in texts from noise, because it requires only
a large-scale corpus to estimate parameters in an
Expectation Maximization (EM) framework. Using
the estimated parameters it is possible to describe
the distributions of both relevant and non-relevant
texts, converting the DRE problem into the problem
of estimating the probability of each domain given
its frequency score in the text, in analogy to the
bayesian classification framework. Details about
the EM algorithm for GM model are provided in
subsection 3.3.
3.1 Domain Frequency Score
Let t ? T , be a text in a corpus T composed by a list
of words wt1, . . . , wtq . Let D = {D1, D2, ..., Dd} be
the set of domains used. For each domain Dk the
domain ?frequency? score is computed in a window
of c words around wtj . The domain frequency scoreis defined by formula (1).
F (Dk, t, j) =
j+c
X
i=j?c
Rword(Dk, wti)G(i, j, (
c
2)
2
) (1)
where the weight factor G(x, ?, ?2) is the density
of the normal distribution with mean ? and standard
deviation ? at point x and Rword(D,w) is a function
that return the relevance of a domain D for a word
w (see formula 3). In the rest of the paper we use the
notation F (Dk, t) to refer to F (Dk, t,m), where m
is the integer part of q/2 (i.e. the ?central? point of
the text - q is the text length).
Here below we see that the information contained
in WORDNET DOMAINS can be used to estimate
Rword(Dk, w), i.e. domain relevance for the word
w, which is derived from the domain relevance of
the synsets in which w appears.
As far as synsets are concerned, domain informa-
tion is represented by the function Dom : S ?
P (D)3 that returns, for each synset s ? S, where
S is the set of synsets in WORDNET DOMAINS, the
set of the domains associated to it. Formula (2) de-
fines the domain relevance estimation function (re-
member that d is the cardinality of D):
Rsyn(D, s) =
8
<
:
1/|Dom(s)| : if D ? Dom(s)
1/d : if Dom(s) = {FACTOTUM}
0 : otherwise
(2)
Intuitively, Rsyn(D, s) can be perceived as an es-
timated prior for the probability of the domain given
the concept, as expressed by the WORDNET DO-
MAINS annotation. Under these settings FACTO-
TUM (generic) concepts have uniform and low rel-
evance values for each domain while domain con-
cepts have high relevance values for a particular do-
main.
The definition of domain relevance for a word is
derived directly from the one given for concepts. In-
tuitively a domain D is relevant for a word w if D
is relevant for one or more senses c of w. More
formally let V = {w1, w2, ...w|V |} be the vocab-
ulary, let senses(w) = {s|s ? S, s is a sense of
w} (e.g. any synset in WORDNET containing the
word w). The domain relevance function for a word
R : D ? V ? [0, 1] is defined as follows:
Rword(Di, w) =
1
|senses(w)|
X
s?senses(w)
Rsyn(Di, s) (3)
3P (D) denotes the power set of D
3.2 The Gaussian Mixture Algorithm
As explained at the beginning of this section, the
simple local frequency count expressed by formula
(1) is not a good domain relevance measure.
In order to discriminate between noise and rel-
evant information, a supervised framework is typ-
ically used and significance levels for frequency
counts are estimated from labeled training data. Un-
fortunately this is not our case, since no domain
labeled text corpora are available. In this section
we propose a solution for that problem, namely the
Gaussian Mixture approach, that constitutes an un-
supervised way to estimate how to differentiate rel-
evant domain information in texts from noise. The
Gaussian Mixture approach consists of a parameter
estimation technique based on statistics of word dis-
tribution in a large-scale corpus.
The underlying assumption of the Gaussian Mix-
ture approach is that frequency scores for a cer-
tain domain are obtained from an underlying mix-
ture of relevant and non-relevant texts, and that the
scores for relevant texts are significantly higher than
scores obtained for the non-relevant ones. In the
corpus these scores are distributed according to two
distinct components. The domain frequency distri-
bution which corresponds to relevant texts has the
higher value expectation, while the one pertaining to
non relevant texts has the lower expectation. Figure
1 describes the probability density function (PDF )
for domain frequency scores of the SPORT domain
estimated on the BNC corpus4 (BNC-Consortium,
2000) using formula (1). The ?empirical? PDF ,
describing the distribution of frequency scores eval-
uated on the corpus, is represented by the continu-
ous line.
From the graph it is possible to see that the empir-
ical PDF can be decomposed into the sum of two
distributions, D = SPORT and D = ?non-SPORT?.
Most of the probability is concentrated on the left,
describing the distribution for the majority of non
relevant texts; the smaller distribution on the right
is assumed to be the distribution of frequency scores
for the minority of relevant texts.
Thus, the distribution on the left describes the
noise present in frequency estimation counts, which
is produced by the impact of polysemous words
and of occasional occurrences of terms belonging
to SPORT in non-relevant texts. The goal of the
technique is to estimate parameters describing the
distribution of the noise along texts, in order to as-
4The British National Corpus is a very large (over 100 mil-
lion words) corpus of modern English, both spoken and written.
050
100
150
200
0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04
Density
Non-relevant
Relevant
F(D, t)
de
ns
ity
 fu
nc
tio
n
Figure 1: Gaussian mixture for D = SPORT
sociate high relevance values only to relevant fre-
quency scores (i.e. frequency scores that are not re-
lated to noise). It is reasonable to assume that such
noise is normally distributed because it can be de-
scribed by a binomial distribution in which the prob-
ability of the positive event is very low and the num-
ber of events is very high. On the other hand, the
distribution on the right is the one describing typical
frequency values for relevant texts. This distribution
is also assumed to be normal.
A probabilistic interpretation permits the evalu-
ation of the relevance value R(D, t, j) of a certain
domain D for a new text t in a position j only by
considering the domain frequency F (D, t, j). The
relevance value is defined as the conditional prob-
ability P (D|F (D, t, j)). Using Bayes theorem we
estimate this probability by equation (4).
R(D, t, j) = P (D|F (D, t, j)) = (4)
= P (F (D, t, j)|D)P (D)
P (F (D, t, j)|D)P (D) + P (F (D, t, j)|D)P (D)
where P (F (D, t, j)|D) is the value of the PDF
describing D calculated in the point F (D, t, j),
P (F (D, t, j)|D) is the value of the PDF describ-
ing D, P (D) is the area of the distribution describ-
ing D and P (D) is the area of the distribution for
D.
In order to estimate the parameters describing the
PDF of D and D the Expectation Maximization
(EM) algorithm for the Gaussian Mixture Model
(Redner and Walker, 1984) is exploited. Assuming
to model the empirical distribution of domain fre-
quencies using a Gaussian mixture of two compo-
nents, the estimated parameters can be used to eval-
uate domain relevance by equation (4).
3.3 The EM Algorithm for the GM model
In this section some details about the algorithm for
parameter estimation are reported.
It is well known that a Gaussian mixture (GM)
allows to represent every smooth PDF as a linear
combination of normal distributions of the type in
formula 5
p(x|?) =
m
?
j=1
ajG(x, ?j , ?j) (5)
with
aj ? 0 and
m
?
j=1
aj = 1 (6)
and
G(x, ?, ?) = 1?
2pi?
e?
(x??)2
2?2 (7)
and ? = ?a1, ?1, ?1, . . . , am, ?m, ?m? is a pa-
rameter list describing the gaussian mixture. The
number of components required by the Gaussian
Mixture algorithm for domain relevance estimation
is m = 2.
Each component j is univocally determined by its
weight aj , its mean ?j and its variance ?j . Weights
represent also the areas of each component, i.e. its
total probability.
The Gaussian Mixture algorithm for domain rele-
vance estimation exploits a Gaussian Mixture to ap-
proximate the empirical PDF of domain frequency
scores. The goal of the Gaussian Mixture algorithm
is to find the GM that maximize the likelihood on
the empirical data, where the likelihood function is
evaluated by formula (8).
L(T , D, ?) =
?
t?T
p(F (D, t)|?) (8)
More formally, the EM algorithm for GM models
explores the space of parameters in order to find the
set of parameters ? such that the maximum likeli-
hood criterion (see formula 9) is satisfied.
?D = argmax
??
L(T , D, ??) (9)
This condition ensures that the obtained model
fits the original data as much as possible. Estima-
tion of parameters is the only information required
in order to evaluate domain relevance for texts us-
ing the Gaussian Mixture algorithm. The Expecta-
tion Maximization Algorithm for Gaussian Mixture
Models (Redner and Walker, 1984) allows to effi-
ciently perform this operation.
The strategy followed by the EM algorithm is
to start from a random set of parameters ?0, that
has a certain initial likelihood value L0, and then
iteratively change them in order to augment like-
lihood at each step. To this aim the EM algo-
rithm exploits a growth transformation of the like-
lihood function ?(?) = ?? such that L(T , D, ?) 6
L(T , D, ??). Applying iteratively this transforma-
tion starting from ?0 a sequence of parameters is
produced, until the likelihood function achieve a
stable value (i.e. Li+1 ? Li 6 ). In our settings
the transformation function ? is defined by the fol-
lowing set of equations, in which all the parameters
have to be solved together.
?(?) = ?(?a1, ?1, ?1, a2, ?2, ?2?) (10)
= ?a?1, ??1, ??1, a?2, ??2, ??2?
a?j =
1
|T |
|T |
?
k=1
ajG(F (D, tk), ?j , ?j)
p(F (D, tk), ?)
(11)
??j =
?|T |
k=1 F (D, tk) ?
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?)
?|T |
k=1
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?)
(12)
??j =
?|T |
k=1 (F (D, tk) ? ??j)2 ?
aiG(F (D,tk),?i,?i)
p(F (D,tk),?)
?|T |
k=1
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?) (13)
As said before, in order to estimate distribu-
tion parameters the British National Corpus (BNC-
Consortium, 2000) was used. Domain frequency
scores have been evaluated on the central position
of each text (using equation 1, with c = 50).
In conclusion, the EM algorithm was used to es-
timate parameters to describe distributions for rele-
vant and non-relevant texts. This learning method
is totally unsupervised. Estimated parameters has
been used to estimate relevance values by formula
(4).
4 Domain Driven Disambiguation
DRE originates to improve the performance of Do-
main Driven Disambiguation (DDD). In this sec-
tion, a brief overview of DDD is given. DDD is a
WSD methodology that only makes use of domain
information. Originally developed to test the role of
domain information for WSD, the system is capable
to achieve a good precision disambiguation. Its re-
sults are affected by a low recall, motivated by the
fact that domain information is sufficient to disam-
biguate only ?domain words?. The disambiguation
process is done comparing the domain of the con-
text and the domains of each sense of the lemma to
disambiguate. The selected sense is the one whose
domain is relevant for the context5 .
In order to represent domain information we in-
troduced the notion of Domain Vectors (DV), that
are data structures that collect domain information.
These vectors are defined in a multidimensional
space, in which each domain represents a dimen-
sion of the space. We distinguish between two kinds
of DVs: (i) synset vectors, which represent the rel-
evance of a synset with respect to each considered
domain and (ii) text vectors, which represent the rel-
evance of a portion of text with respect to each do-
main in the considered set.
More formally let D = {D1, D2, ..., Dd} be the
set of domains, the domain vector ~s for a synset s
is defined as ?R(D1, s), R(D2, s), . . . , R(Dd, s)?
where R(Di, s) is evaluated using equation
(2). In analogy the domain vector ~tj for
a text t in a given position j is defined as
?R(D1, t, j), R(D2, t, j), . . . , R(Dd, t, j)? where
R(Di, t, j) is evaluated using equation (4).
The DDD methodology is performed basically in
three steps:
1. Compute ~t for the context t of the word w to be disam-
biguated
2. Compute s? = argmaxs?Senses(w)score(s, w, t) where
score(s,w, t) = P (s|w) ? sim(~s,
~t)
P
s?Senses(w) P (s|w) ? sim(~s,~t)
3. if score(s?, w, t) > k (where k ? [0, 1] is a confidence
threshold) select sense s?, else do not provide any answer
The similarity metric used is the cosine vector
similarity, which takes into account only the direc-
tion of the vector (i.e. the information regarding the
domain).
P (s|w) describes the prior probability of sense
s for word w, and depends on the distribution of
the sense annotations in the corpus. It is esti-
mated by statistics from a sense tagged corpus (we
used SemCor)6 or considering the sense order in
5Recent works in WSD demonstrate that an automatic es-
timation of domain relevance for texts can be profitable used
to disambiguate words in their contexts. For example, (Escud-
ero et al, 2001) used domain relevance extraction techniques
to extract features for a supervised WSD algorithm presented
at the Senseval-2 competion, improving the system accuracy of
about 4 points for nouns, 1 point for verbs and 2 points for ad-
jectives, confirming the original intuition that domain informa-
tion is very useful to disambiguate ?domain words?, i.e. words
which are strongly related to the domain of the text.
6Admittedly, this may be regarded as a supervised compo-
nent of the generally unsupervised system. Yet, we considered
this component as legitimate within an unsupervised frame-
WORDNET, which roughly corresponds to sense
frequency order, when no example of the word
to disambiguate are contained in SemCor. In the
former case the estimation of P (s|w) is based on
smoothed statistics from the corpus (P (s|w) =
occ(s,w)+?
occ(w)+|senses(w)|?? , where ? is a smoothing fac-
tor empirically determined). In the latter case
P (s|w) can be estimated in an unsupervised way
considering the order of senses in WORDNET
(P (s|w) = 2(|senses(w)|?sensenumber(s,w)+1)|senses(w)|(|senses(w)|+1) where
sensenumber(s, w) returns the position of sense
s of word w in the sense list for w provided by
WORDNET.
5 Evaluation in a WSD task
We used the WSD framework to perform an evalu-
ation of the DRE technique by itself.
As explained in Section 1 Domain Relevance Es-
timation is not a common Text Categorization task.
In the standard framework of TC, categories are
learned form examples, that are used also for test.
In our case information in WORDNET DOMAINS is
used to discriminate, and a test set, i.e. a corpus of
texts categorized using the domain of WORDNET
DOMAINS, is not available. To evaluate the accu-
racy of the domain relevance estimation technique
described above is thus necessary to perform an in-
direct evaluation.
We evaluated the DDD algorithm described in
Section 4 using the dataset of the Senseval-2 all-
words task (Senseval-2, 2001; Preiss and Yarowsky,
2002). In order to estimate domain vectors for the
contexts of the words to disambiguate we used the
DRE methodology described in Section 3. Varying
the confidence threshold k, as described in Section
4, it is possible to change the tradeoff between preci-
sion and recall. The obtained precision-recall curve
of the system is reported in Figure 2.
In addition we evaluated separately the perfor-
mance on nouns and verbs, suspecting that nouns
are more ?domain oriented? than verbs. The effec-
tiveness of DDD to disambiguate domain words is
confirmed by results reported in Figure 3, in which
the precision recall curve is reported separately for
both nouns and verbs. The performances obtained
for nouns are sensibly higher than the one obtained
for verbs, confirming the claim that domain infor-
mation is crucial to disambiguate domain words.
In Figure 2 we also compare the results ob-
tained by the DDD system that make use of the
DRE technique described in Section 3 with the re-
work since it relies on a general resource (SemCor) that does
not correspond to the test data (Senseval all-words task).
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6
Pr
ec
isi
on
Recall
DDD new
DDD old
Figure 2: Performances of the system for all POS
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Pr
ec
isi
on
Recall
Nouns
Verbs
Figure 3: Performances of the system for Nouns and
Verbs
sults obtained by the DDD system presented at the
Senseval-2 competition described in (Magnini et al,
2002), that is based on the same DDD methodol-
ogy and exploit a DRE technique that consists ba-
sically on the simply domain frequency scores de-
scribed in subsection 3.1 (we refer to this system
using the expression old-DDD, in contrast to the ex-
pression new-DDD that refers to the implementation
described in this paper).
Old-DDD obtained 75% precision and 35% re-
call on the official evaluation at the Senseval-2 En-
glish all words task. At 35% of recall the new-DDD
achieves a precision of 79%, improving precision
by 4 points with respect to old-DDD. At 75% pre-
cision the recall of new-DDD is 40%. In both cases
the new domain relevance estimation technique im-
proves the performance of the DDD methodology,
demonstrating the usefulness of the DRE technique
proposed in this paper.
6 Conclusions and Future Works
Domain Relevance Estimation, an unsupervised TC
technique, has been proposed and evaluated in-
side the Domain Driven Disambiguation frame-
work, showing a significant improvement on the
overall system performances. This technique also
allows a clear probabilistic interpretation providing
an operative definition of the concept of domain rel-
evance. During the learning phase annotated re-
sources are not required, allowing a low cost imple-
mentation. The portability of the technique to other
languages is allowed by the usage of synset-aligned
wordnets, being domain annotation language inde-
pendent.
As far as the evaluation of DRE is concerned, for
the moment we have tested its usefulness in the con-
text of a WSD task, but we are going deeper, con-
sidering a pure TC framework.
Acknowledgements
We would like to thank Ido Dagan and Marcello
Federico for many useful discussions and sugges-
tions.
References
BNC-Consortium. 2000. British national corpus,
http://www.hcu.ox.ac.uk/BNC/.
J. P. Comaroni, J. Beall, W. E. Matthews, and G. R.
New, editors. 1989. Dewey Decimal Classica-
tion and Relative Index. Forest Press, Albany,
New York, 20th edition.
G. Escudero, L. Ma`rquez, and G. Rigau. 2001.
Using lazy boosting for word sense disambigua-
tion. In Proc. of SENSEVAL-2 Second Inter-
national Workshop on Evaluating Word Sense
Disambiguation System, pages 71?74, Toulose,
France, July.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. The MIT Press.
Y. Ko and J. Seo. 2000. Automatic text categoriza-
tion by unsupervised learning. In Proceedings of
COLING-00, the 18th International Conference
on Computational Linguistics, Saarbru?cken, Ger-
many.
B. Magnini and G. Cavaglia`. 2000. Integrating sub-
ject field codes into WordNet. In Proceedings
of LREC-2000, Second International Conference
on Language Resources and Evaluation, Athens,
Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H.T. Dang. 2001. English tasks: All-words
and verb lexical sample. In Proceedings of
SENSEVAL-2, Second International Workshop on
Evaluating Word Sense Disambiguation Systems,
Toulouse, France, July.
J. Preiss and D. Yarowsky, editors. 2002. Pro-
ceedings of SENSEVAL-2: Second International
Workshop on Evaluating Word Sense Disam-
biguation Systems, Toulouse, France.
R. Redner and H. Walker. 1984. Mixture densi-
ties, maximum likelihood and the EM algorithm.
SIAM Review, 26(2):195?239, April.
F. Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
Senseval-2. 2001. http://www.senseval.org.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 56?63, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Domain Kernels for Text Categorization
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
{gliozzo,strappa}@itc.it
Abstract
In this paper we propose and evaluate
a technique to perform semi-supervised
learning for Text Categorization. In
particular we defined a kernel function,
namely the Domain Kernel, that allowed
us to plug ?external knowledge? into the
supervised learning process. External
knowledge is acquired from unlabeled
data in a totally unsupervised way, and it
is represented by means of Domain Mod-
els.
We evaluated the Domain Kernel in two
standard benchmarks for Text Categoriza-
tion with good results, and we compared
its performance with a kernel function that
exploits a standard bag-of-words feature
representation. The learning curves show
that the Domain Kernel allows us to re-
duce drastically the amount of training
data required for learning.
1 Introduction
Text Categorization (TC) deals with the problem of
assigning a set of category labels to documents. Cat-
egories are usually defined according to a variety
of topics (e.g. SPORT vs. POLITICS) and a set of
hand tagged examples is provided for training. In the
state-of-the-art TC settings supervised classifiers are
used for learning and texts are represented by means
of bag-of-words.
Even if, in principle, supervised approaches reach
the best performance in many Natural Language
Processing (NLP) tasks, in practice it is not always
easy to apply them to concrete applicative settings.
In fact, supervised systems for TC require to be
trained a large amount of hand tagged texts. This
situation is usually feasible only when there is some-
one (e.g. a big company) that can easily provide al-
ready classified documents to train the system.
In most of the cases this scenario is quite unprac-
tical, if not infeasible. An example is the task of
categorizing personal documents, in which the cate-
gories can be modified according to the user?s inter-
ests: new categories are often introduced and, pos-
sibly, the available labeled training for them is very
limited.
In the NLP literature the problem of providing
large amounts of manually annotated data is known
as the Knowledge Acquisition Bottleneck. Cur-
rent research in supervised approaches to NLP often
deals with defining methodologies and algorithms to
reduce the amount of human effort required for col-
lecting labeled examples.
A promising direction to solve this problem is to
provide unlabeled data together with labeled texts
to help supervision. In the Machine Learning lit-
erature this learning schema has been called semi-
supervised learning. It has been applied to the
TC problem using different techniques: co-training
(Blum and Mitchell, 1998), EM-algorithm (Nigam
et al, 2000), Transduptive SVM (Joachims, 1999b)
and Latent Semantic Indexing (Zelikovitz and Hirsh,
2001).
In this paper we propose a novel technique to per-
form semi-supervised learning for TC. The under-
lying idea behind our approach is that lexical co-
56
herence (i.e. co-occurence in texts of semantically
related terms) (Magnini et al, 2002) is an inherent
property of corpora, and it can be exploited to help a
supervised classifier to build a better categorization
hypothesis, even if the amount of labeled training
data provided for learning is very low.
Our proposal consists of defining a Domain
Kernel and exploiting it inside a Support Vector
Machine (SVM) classification framework for TC
(Joachims, 2002). The Domain Kernel relies on the
notion of Domain Model, which is a shallow repre-
sentation for lexical ambiguity and variability. Do-
main Models can be acquired in an unsupervised
way from unlabeled data, and then exploited to de-
fine a Domain Kernel (i.e. a generalized similarity
function among documents)1 .
We evaluated the Domain Kernel in two stan-
dard benchmarks for TC (i.e. Reuters and 20News-
groups), and we compared its performance with a
kernel function that exploits a more standard Bag-
of-Words (BoW) feature representation. The use of
the Domain Kernel got a significant improvement in
the learning curves of both tasks. In particular, there
is a notable increment of the recall, especially with
few learning examples. In addition, F1 measure in-
creases by 2.8 points in the Reuters task at full learn-
ing, achieving the state-of-the-art results.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model and describes
an automatic acquisition technique based on Latent
Semantic Analysis (LSA). In Section 3 we illustrate
the SVM approach to TC, and we define a Domain
Kernel that exploits Domain Models to estimate sim-
ilarity among documents. In Section 4 the perfor-
mance of the Domain Kernel are compared with a
standard bag-of-words feature representation, show-
ing the improvements in the learning curves. Section
5 describes the previous attempts to exploit semi-
supervised learning for TC, while section 6 con-
cludes the paper and proposes some directions for
future research.
1The idea of exploiting a Domain Kernel to help a super-
vised classification framework, has been profitably used also in
other NLP tasks such as word sense disambiguation (see for ex-
ample (Strapparava et al, 2004)).
2 Domain Models
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let T = {t1, t2, . . . , tn} be a corpus, let
V = {w1, w2, . . . , wk} be its vocabulary, let T be
the k ? n term-by-document matrix representing T ,
such that ti,j is the frequency of word wi into the text
tj . The VSM is a k-dimensional space Rk, in which
the text tj ? T is represented by means of the vec-
tor ~tj such that the ith component of ~tj is ti,j. The
similarity among two texts in the VSM is estimated
by computing the cosine.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences ?he is affected by AIDS? and ?HIV is
a virus? do not have any words in common. In the
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences ?the laptop has
been infected by a virus? and ?HIV is a virus? would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM, in which texts and
terms are represented in a uniform way.
A Domain Model is composed by soft clusters of
terms. Each cluster represents a semantic domain
(Gliozzo et al, 2004), i.e. a set of terms that often
co-occur in texts having similar topics. A Domain
Model is represented by a k ? k? rectangular matrix
D, containing the degree of association among terms
and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Matrix
Domain Models can be used to describe lexical
ambiguity and variability. Lexical ambiguity is rep-
57
resented by associating one term to more than one
domain, while variability is represented by associat-
ing different terms to the same domain. For example
the term virus is associated to both the domain
COMPUTER SCIENCE and the domain MEDICINE
(ambiguity) while the domain MEDICINE is associ-
ated to both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2, ..., Dk?} be
a set of domains, such that k?  k. A Domain
Model is fully defined by a k ? k? domain matrix
D representing in each cell di,z the domain rele-
vance of term wi with respect to the domain Dz .
The domain matrix D is used to define a function
D : Rk ? Rk? , that maps the vectors ~tj , expressed
into the classical VSM, into the vectors ~t?j in the do-
main VSM. D is defined by2
D(~tj) = ~tj(IIDFD) = ~t?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wi), ~tj is represented as a row vector, and
IDF (wi) is the Inverse Document Frequency of wi.
Vectors in the domain VSM are called Domain
Vectors. Domain Vectors for texts are estimated by
exploiting formula 1, while the Domain Vector ~w?i,
corresponding to the word wi ? V , is the ith row of
the domain matrix D. To be a valid domain matrix
such vectors should be normalized (i.e. ? ~w?i, ~w?i? =
1).
In the Domain VSM the similarity among Domain
Vectors is estimated by taking into account second
order relations among terms. For example the simi-
larity of the two sentences ?He is affected by AIDS?
and ?HIV is a virus? is very high, because the terms
AIDS, HIV and virus are highly associated to the
domain MEDICINE.
In this work we propose the use of Latent Se-
mantic Analysis (LSA) (Deerwester et al, 1990) to
induce Domain Models from corpora. LSA is an
unsupervised technique for estimating the similar-
ity among texts and terms in a corpus. LSA is per-
formed by means of a Singular Value Decomposi-
tion (SVD) of the term-by-document matrix T de-
scribing the corpus. The SVD algorithm can be ex-
ploited to acquire a domain matrix D from a large
2In (Wong et al, 1985) a similar schema is adopted to define
a Generalized Vector Space Model, of which the Domain VSM
is a particular instance.
corpus T in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T ' V?k?UT where ?k? is the diagonal
k ? k matrix containing the highest k ?  k eigen-
values of T, and all the remaining elements set to
0. The parameter k? is the dimensionality of the Do-
main VSM and can be fixed in advance3 . Under this
setting we define the domain matrix DLSA4 as
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
?
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .
3 The Domain Kernel
Kernel Methods are the state-of-the-art supervised
framework for learning, and they have been success-
fully adopted to approach the TC task (Joachims,
1999a).
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear
algorithm for discovering nonlinear patterns. Kernel
methods allow us to build a modular system, as the
kernel function acts as an interface between the data
and the learning algorithm. Thus the kernel function
becomes the only domain specific module of the sys-
tem, while the learning algorithm is a general pur-
pose component. Potentially a kernel function can
work with any kernel-based algorithm, such as for
example SVM.
During the learning phase SVMs assign a weight
?i ? 0 to any example xi ? X . All the labeled
instances xi such that ?i > 0 are called support vec-
tors. The support vectors lie close to the best sepa-
rating hyper-plane between positive and negative ex-
amples. New examples are then assigned to the class
of its closest support vectors, according to equation
3.
3It is not clear how to choose the right dimensionality. In
our experiments we used 400 dimensions.
4When DLSA is substituted in Equation 1 the Domain VSM
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
58
f(x) =
n
?
i=1
?iK(xi, x) + ?0 (3)
The kernel function K returns the similarity be-
tween two instances in the input space X , and can
be designed in order to capture the relevant aspects
to estimate similarity, just by taking care of satis-
fying set of formal requirements, as described in
(Scho?lkopf and Smola, 2001).
In this paper we define the Domain Kernel and we
apply it to TC tasks. The Domain Kernel, denoted
by KD, can be exploited to estimate the topic simi-
larity among two texts while taking into account the
external knowledge provided by a Domain Model
(see section 2). It is a variation of the Latent Seman-
tic Kernel (Shawe-Taylor and Cristianini, 2004), in
which a Domain Model is exploited to define an ex-
plicit mapping D : Rk ? Rk? from the classical
VSM into the domain VSM. The Domain Kernel is
defined by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. To be fully defined, the Domain Kernel re-
quires a Domain Matrix D. In principle, D can be
acquired from any corpora by exploiting any (soft)
term clustering algorithm. Anyway, we belive that
adequate Domain Models for particular tasks can be
better acquired from collections of documents from
the same source. For this reason, for the experi-
ments reported in this paper, we acquired the matrix
DLSA, defined by equation 2, using the whole (un-
labeled) training corpora available for each task, so
tuning the Domain Model on the particular task in
which it will be applied.
A more traditional approach to measure topic sim-
ilarity among text consists of extracting BoW fea-
tures and to compare them in a vector space. The
BoW kernel, denoted by KBoW , is a particular case
of the Domain Kernel, in which D = I, and I is the
identity matrix. The BoW Kernel does not require
a Domain Model, so we can consider this setting
as ?purely? supervised, in which no external knowl-
edge source is provided.
4 Evaluation
We compared the performance of both KD and
KBoW on two standard TC benchmarks. In sub-
section 4.1 we describe the evaluation tasks and the
preprocessing steps, in 4.2 we describe some algo-
rithmic details of the TC system adopted. Finally
in subsection 4.3 we compare the learning curves of
KD and KBoW .
4.1 Text Categorization tasks
For the experiments reported in this paper, we se-
lected two evaluation benchmarks typically used in
the TC literature (Sebastiani, 2002): the 20news-
groups and the Reuters corpora. In both the data sets
we tagged the texts for part of speech and we consid-
ered only the noun, verb, adjective, and adverb parts
of speech, representing them by vectors containing
the frequencies of each disambiguated lemma. The
only feature selection step we performed was to re-
move all the closed-class words from the document
index.
20newsgroups. The 20Newsgroups data set5 is
a collection of approximately 20,000 newsgroup
documents, partitioned (nearly) evenly across 20
different newsgroups. This collection has become
a popular data set for experiments in text appli-
cations of machine learning techniques, such as
text classification and text clustering. Some of
the newsgroups are very closely related to each
other (e.g. comp.sys.ibm.pc.hardware
/ comp.sys.mac.hardware), while others
are highly unrelated (e.g. misc.forsale /
soc.religion.christian). We removed
cross-posts (duplicates), newsgroup-identifying
headers (i.e. Xref, Newsgroups, Path, Followup-To,
Date), and empty documents from the original
corpus, so to obtain 18,941 documents. Then we
randomly divided it into training (80%) and test
(20%) sets, containing respectively 15,153 and
3,788 documents.
Reuters. We used the Reuters-21578 collec-
tion6, and we splitted it into training and test
5Available at http://www.ai.mit.edu-
/people/jrennie/20Newsgroups/.
6Available at http://kdd.ics.uci.edu/databases/-
reuters21578/reuters21578.html.
59
partitions according to the standard ModApt

e
split. It includes 12,902 documents for 90 cat-
egories, with a fixed splitting between training
and test data. We conducted our experiments by
considering only the 10 most frequent categories,
i.e. Earn, Acquisition, Money-fx,
Grain, Crude, Trade, Interest,
Ship, Wheat and Corn, and we included in
our dataset al the non empty documents labeled
with at least one of those categories. Thus the final
dataset includes 9295 document, of which 6680 are
included in the training partition, and 2615 are in
the test set.
4.2 Implementation details
As a supervised learning device, we used the SVM
implementation described in (Joachims, 1999a).
The Domain Kernel is implemented by defining an
explicit feature mapping according to formula 1, and
by normalizing each vector to obtain vectors of uni-
tary length. All the experiments have been per-
formed on the standard parameter settings, using a
linear kernel.
We acquired a different Domain Model for each
corpus by performing the SVD processes on the
term-by-document matrices representing the whole
training partitions, and we considered only the first
400 domains (i.e. k? = 400)7.
As far as the Reuters task is concerned, the TC
problem has been approached as a set of binary fil-
tering problems, allowing the TC system to pro-
vide more than one category label to each document.
For the 20newsgroups task, we implemented a one-
versus-all classification schema, in order to assign a
single category to each news.
4.3 Domain Kernel versus BoW Kernel
Figure 1 and Figure 2 report the learning curves for
both KD and KBoW , evaluated respectively on the
Reuters and the 20newgroups task. Results clearly
show that KD always outperforms KBoW , espe-
cially when very limited amount of labeled data is
provided for learning.
7To perform the SVD operation we adopted
LIBSVDC, an optimized package for sparse ma-
trix that allows to perform this step in few minutes
even for large corpora. It can be downloaded from
http://tedlab.mit.edu/?dr/SVDLIBC/.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 1: Micro-F1 learning curves for Reuters
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 2: Micro-F1 learning curves for 20news-
groups
Table 2 compares the performances of the two
kernels at full learning. KD achieves a better micro-
F1 than KBoW in both tasks. The improvement is
particularly significant in the Reuters task (+ 2.8 %).
Tables 3 shows the number of labeled examples
required by KD and KBoW to achieve the same
micro-F1 in the Reuters task. KD requires only
146 examples to obtain a micro-F1 of 0.84, while
KBoW requires 1380 examples to achieve the same
performance. In the same task, KD surpass the per-
formance of KBoW at full learning using only the
10% of the labeled data. The last column of the ta-
ble shows clearly that KD requires 90% less labeled
data than KBoW to achieve the same performances.
A similar behavior is reported in Table 4 for the
60
F1 Domain Kernel Bow Kernel
Reuters 0.928 0.900
20newsgroups 0.886 0.880
Table 2: Micro-F1 with full learning
F1 Domain Kernel Bow Kernel Ratio
.54 14 267 5%
.84 146 1380 10%
.90 668 6680 10%
Table 3: Number of training examples needed by
KD and KBoW to reach the same micro-F1 on the
Reuters task
20newsgroups task. It is important to notice that the
number of labeled documents is higher in this corpus
than in the previous one. The benefits of using Do-
main Models are then less evident at full learning,
even if they are significant when very few labeled
data are provided.
Figures 3 and 4 report a more detailed analysis
by comparing the micro-precision and micro-recall
learning curves of both kernels in the Reuters task8.
It is clear from the graphs that the main contribute
of KD is about increasing recall, while precision is
similar in both cases9. This last result confirms our
hypothesis that the information provided by the Do-
main Models allows the system to generalize in a
more effective way over the training examples, al-
lowing to estimate the similarity among texts even if
they have just few words in common.
Finally, KD achieves the state-of-the-art in the
Reuters task, as reported in section 5.
5 Related Works
To our knowledge, the first attempt to apply the
semi-supervised learning schema to TC has been
reported in (Blum and Mitchell, 1998). Their co-
training algorithm was able to reduce significantly
the error rate, if compared to a strictly supervised
8For the 20-newsgroups task both micro-precision and
micro-recall are equal to micro-F1 because a single category
label has been assigned to every instance.
9It is worth noting that KD gets a F1 measure of 0.54 (Preci-
sion/Recall of 0.93/0.38) using just 14 training examples, sug-
gesting that it can be profitably exploited for a bootstrapping
process.
F1 Domain Kernel Bow Kernel Ratio
.50 30 500 6%
.70 98 1182 8%
.85 2272 7879 29%
Table 4: Number of training examples needed by
KD and KBoW to reach the same micro-F1 on the
20newsgroups task
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pr
ec
is
io
n
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 3: Learning curves for Reuters (Precision)
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R
ec
al
l
Fraction of labeled training data
Domain Kernel
BoW Kernel
Figure 4: Learning curves for Reuters (Recall)
classifier.
(Nigam et al, 2000) adopted an Expectation Max-
imization (EM) schema to deal with the same prob-
lem, evaluating extensively their approach on sev-
eral datasets. They compared their algorithm with
a standard probabilistic approach to TC, reporting
substantial improvements in the learning curve.
61
A similar evaluation is also reported in (Joachims,
1999b), where a transduptive SVM is compared
to a state-of-the-art TC classifier based on SVM.
The semi-supervised approach obtained better re-
sults than the standard with few learning data, while
at full learning results seem to converge.
(Bekkerman et al, 2002) adopted a SVM classi-
fier in which texts have been represented by their as-
sociations to a set of Distributional Word Clusters.
Even if this approach is very similar to ours, it is not
a semi-supervised learning schema, because authors
did not exploit any additional unlabeled data to in-
duce word clusters.
In (Zelikovitz and Hirsh, 2001) background
knowledge (i.e. the unlabeled data) is exploited to-
gether with labeled data to estimate document sim-
ilarity in a Latent Semantic Space (Deerwester et
al., 1990). Their approach differs from the one pro-
posed in this paper because a different categoriza-
tion algorithm has been adopted. Authors compared
their algorithm with an EM schema (Nigam et al,
2000) on the same dataset, reporting better results
only with very few labeled data, while EM performs
better with more training.
All the semi-supervised approaches in the liter-
ature reports better results than strictly supervised
ones with few learning, while with more data the
learning curves tend to converge.
A comparative evaluation among semi-supervised
TC algorithms is quite difficult, because the used
data sets, the preprocessing steps and the splitting
partitions adopted affect sensibly the final results.
Anyway, we reported the best F1 measure on the
Reuters corpus: to our knowledge, the state-of-the-
art on the 10 top most frequent categories of the
ModApte split at full learning is F1 92.0 (Bekker-
man et al, 2002) while we obtained 92.8. It is im-
portant to notice here that this results has been ob-
tained thanks to the improvements of the Domain
Kernel. In addition, on the 20newsgroups task, our
methods requires about 100 documents (i.e. five
documents per category) to achieve 70% F1, while
both EM (Nigam et al, 2000) and LSI (Zelikovitz
and Hirsh, 2001) requires more than 400 to achieve
the same performance.
6 Conclusion and Future Works
In this paper a novel technique to perform semi-
supervised learning for TC has been proposed and
evaluated. We defined a Domain Kernel that allows
us to improve the similarity estimation among docu-
ments by exploiting Domain Models. Domain Mod-
els are acquired from large collections of non anno-
tated texts in a totally unsupervised way.
An extensive evaluation on two standard bench-
marks shows that the Domain Kernel allows us to re-
duce drastically the amount of training data required
for learning. In particular the recall increases sen-
sibly, while preserving a very good accuracy. We
explained this phenomenon by showing that the sim-
ilarity scores evaluated by the Domain Kernel takes
into account both variability and ambiguity, being
able to estimate similarity even among texts that do
not have any word in common.
As future work, we plan to apply our semi-
supervised learning method to some concrete ap-
plicative scenarios, such as user modeling and cat-
egorization of personal documents in mail clients.
In addition, we are going deeper in the direction of
semi-supervised learning, by acquiring more com-
plex structures than clusters (e.g. synonymy, hyper-
onymy) to represent domain models. Furthermore,
we are working to adapt the general framework pro-
vided by the Domain Models to a multilingual sce-
nario, in order to apply the Domain Kernel to a Cross
Language TC task.
Acknowledgments
This work has been partially supported by the ON-
TOTEXT (From Text to Knowledge for the Se-
mantic Web) project, funded by the Autonomous
Province of Trento under the FUP-2004 research
program.
References
R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Win-
ter. 2002. Distributional word clusters vs. words for
text categorization. Journal of Machine Learning Re-
search, 1:1183?1208.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory, Morgan Kaufmann Publishers.
62
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
T. Joachims. 1999a. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. MIT Press,
Cambridge, MA, USA.
T. Joachims. 1999b. Transductive inference for text
classification using support vector machines. In Pro-
ceedings of ICML-99, 16th International Conference
on Machine Learning, pages 200?209. Morgan Kauf-
mann Publishers, San Francisco, US.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer Academic Publishers.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
K. Nigam, A. K. McCallum, S. Thrun, and T. M.
Mitchell. 2000. Text classification from labeled and
unlabeled documents using EM. Machine Learning,
39(2/3):103?134.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
B. Scho?lkopf and A. J. Smola. 2001. Learning with Ker-
nels. Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229?234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
S. Zelikovitz and H. Hirsh. 2001. Using LSI for text clas-
sification in the presence of background text. In Hen-
rique Paques, Ling Liu, and David Grossman, editors,
Proceedings of CIKM-01, 10th ACM International
Conference on Information and Knowledge Manage-
ment, pages 113?118, Atlanta, US. ACM Press, New
York, US.
63
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 9?16,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Cross language Text Categorization by acquiring
Multilingual Domain Models from Comparable Corpora
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
{gliozzo,strappa}@itc.it
Abstract
In a multilingual scenario, the classical
monolingual text categorization problem
can be reformulated as a cross language
TC task, in which we have to cope with
two or more languages (e.g. English and
Italian). In this setting, the system is
trained using labeled examples in a source
language (e.g. English), and it classifies
documents in a different target language
(e.g. Italian).
In this paper we propose a novel ap-
proach to solve the cross language text
categorization problem based on acquir-
ing Multilingual Domain Models from
comparable corpora in a totally unsuper-
vised way and without using any external
knowledge source (e.g. bilingual dictio-
naries). These Multilingual Domain Mod-
els are exploited to define a generalized
similarity function (i.e. a kernel function)
among documents in different languages,
which is used inside a Support Vector Ma-
chines classification framework. The re-
sults show that our approach is a feasi-
ble and cheap solution that largely outper-
forms a baseline.
1 Introduction
Text categorization (TC) is the task of assigning cat-
egory labels to documents. Categories are usually
defined according to a variety of topics (e.g. SPORT,
POLITICS, etc.) and, even if a large amount of
hand tagged texts is required, the state-of-the-art su-
pervised learning techniques represent a viable and
well-performing solution for monolingual catego-
rization problems.
On the other hand in the worldwide scenario of
the web age, multilinguality is a crucial issue to deal
with and to investigate, leading us to reformulate
most of the classical NLP problems. In particular,
monolingual Text Categorization can be reformu-
lated as a cross language TC task, in which we have
to cope with two or more languages (e.g. English
and Italian). In this setting, the system is trained
using labeled examples in a source language (e.g.
English), and it classifies documents in a different
target language (e.g. Italian).
In this paper we propose a novel approach to solve
the cross language text categorization problem based
on acquiring Multilingual Domain Models (MDM)
from comparable corpora in an unsupervised way.
A MDM is a set of clusters formed by terms in dif-
ferent languages. While in the monolingual settings
semantic domains are clusters of related terms that
co-occur in texts regarding similar topics (Gliozzo et
al., 2004), in the multilingual settings such clusters
are composed by terms in different languages ex-
pressing concepts in the same semantic field. Thus,
the basic relation modeled by a MDM is the domain
similarity among terms in different languages. Our
claim is that such a relation is sufficient to capture
relevant aspects of topic similarity that can be prof-
itably used for TC purposes.
The paper is organized as follows. After a brief
discussion about comparable corpora, we introduce
9
a multilingual Vector Space Model, in which docu-
ments in different languages can be represented and
then compared. In Section 4 we define the MDMs
and we present a totally unsupervised technique
to acquire them from comparable corpora. This
methodology does not require any external knowl-
edge source (e.g. bilingual dictionaries) and it is
based on Latent Semantic Analysis (LSA) (Deer-
wester et al, 1990). MDMs are then exploited to
define a Multilingual Domain Kernel, a generalized
similarity function among documents in different
languages that exploits a MDM (see Section 5). The
Multilingual Domain Kernel is used inside a Sup-
port Vector Machines (SVM) classification frame-
work for TC (Joachims, 2002). In Section 6 we will
evaluate our technique in a Cross Language catego-
rization task. The results show that our approach is
a feasible and cheap solution, largely outperforming
a baseline. Conclusions and future works are finally
reported in Section 7.
2 Comparable Corpora
Comparable corpora are collections of texts in dif-
ferent languages regarding similar topics (e.g. a col-
lection of news published by agencies in the same
period). More restrictive requirements are expected
for parallel corpora (i.e. corpora composed by texts
which are mutual translations), while the class of
the multilingual corpora (i.e. collection of texts ex-
pressed in different languages without any addi-
tional requirement) is the more general. Obviously
parallel corpora are also comparable, while compa-
rable corpora are also multilingual.
In a more precise way, let L = {L1, L2, . . . , Ll}
be a set of languages, let T i = {ti1, ti2, . . . , tin} be a
collection of texts expressed in the language Li ? L,
and let ?(tjh, tiz) be a function that returns 1 if tiz is
the translation of tjh and 0 otherwise. A multilingual
corpus is the collection of texts defined by T ? =
?
i T i. If the function ? exists for every text tiz ? T ?
and for every language Lj , and is known, then the
corpus is parallel and aligned at document level.
For the purpose of this paper it is enough to as-
sume that two corpora are comparable, i.e. they are
composed by documents about the same topics and
produced in the same period (e.g. possibly from dif-
ferent news agencies), and it is not known if a func-
tion ? exists, even if in principle it could exist and
return 1 for a strict subset of document pairs.
There exist many interesting works about us-
ing parallel corpora for multilingual applications
(Melamed, 2001), such as Machine Translation,
Cross language Information Retrieval (Littman et
al., 1998), lexical acquisition, and so on.
However it is not always easy to find or build par-
allel corpora. This is the main reason because the
weaker notion of comparable corpora is a matter re-
cent interest in the field of Computational Linguis-
tics (Gaussier et al, 2004).
The texts inside comparable corpora, being about
the same topics (i.e. about the same semantic do-
mains), should refer to the same concepts by using
various expressions in different languages. On the
other hand, most of the proper nouns, relevant enti-
ties and words that are not yet lexicalized in the lan-
guage, are expressed by using their original terms.
As a consequence the same entities will be denoted
with the same words in different languages, allow-
ing to automatically detect couples of translation
pairs just by looking at the word shape (Koehn and
Knight, 2002). Our hypothesis is that comparable
corpora contain a large amount of such words, just
because texts, referring to the same topics in differ-
ent languages, will often adopt the same terms to
denote the same entities1 .
However, the simple presence of these shared
words is not enough to get significant results in TC
tasks. As we will see, we need to exploit these com-
mon words to induce a second-order similarity for
the other words in the lexicons.
3 The Multilingual Vector Space Model
Let T = {t1, t2, . . . , tn} be a corpus, and V =
{w1, w2, . . . , wk} be its vocabulary. In the mono-
lingual settings, the Vector Space Model (VSM) is a
k-dimensional space Rk, in which the text tj ? T
is represented by means of the vector ~tj such that
the zth component of ~tj is the frequency of wz in tj .
The similarity among two texts in the VSM is then
estimated by computing the cosine of their vectors
in the VSM.
1According to our assumption, a possible additional crite-
rion to decide whether two corpora are comparable is to esti-
mate the percentage of terms in the intersection of their vocab-
ularies.
10
Unfortunately, such a model cannot be adopted in
the multilingual settings, because the VSMs of dif-
ferent languages are mainly disjoint, and the similar-
ity between two texts in different languages would
always turn out zero. This situation is represented
in Figure 1, in which both the left-bottom and the
rigth-upper regions of the matrix are totally filled by
zeros.
A first attempt to solve this problem is to ex-
ploit the information provided by external knowl-
edge sources, such as bilingual dictionaries, to col-
lapse all the rows representing translation pairs. In
this setting, the similarity among texts in different
languages could be estimated by exploiting the clas-
sical VSM just described. However, the main dis-
advantage of this approach to estimate inter-lingual
text similarity is that it strongly relies on the avail-
ability of a multilingual lexical resource containing
a list of translation pairs. For languages with scarce
resources a bilingual dictionary could be not eas-
ily available. Secondly, an important requirement
of such a resource is its coverage (i.e. the amount
of possible translation pairs that are actually con-
tained in it). Finally, another problem is that am-
biguos terms could be translated in different ways,
leading to collapse together rows describing terms
with very different meanings.
On the other hand, the assumption of corpora
comparability seen in Section 2, implies the pres-
ence of a number of common words, represented by
the central rows of the matrix in Figure 1.
As we will show in Section 6, this model is rather
poor because of its sparseness. In the next section,
we will show how to use such words as seeds to in-
duce a Multilingual Domain VSM, in which second
order relations among terms and documents in dif-
ferent languages are considered to improve the sim-
ilarity estimation.
4 Multilingual Domain Models
A MDM is a multilingual extension of the concept
of Domain Model. In the literature, Domain Mod-
els have been introduced to represent ambiguity and
variability (Gliozzo et al, 2004) and successfully
exploited in many NLP applications, such us Word
Sense Disambiguation (Strapparava et al, 2004),
Text Categorization and Term Categorization.
A Domain Model is composed by soft clusters of
terms. Each cluster represents a semantic domain,
i.e. a set of terms that often co-occur in texts hav-
ing similar topics. Such clusters identifies groups of
words belonging to the same semantic field, and thus
highly paradigmatically related. MDMs are Domain
Models containing terms in more than one language.
A MDM is represented by a matrix D, contain-
ing the degree of association among terms in all the
languages and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV e/i 1 0
AIDSe/i 1 0
viruse/i 0.5 0.5
hospitale 1 0
laptope 0 1
Microsofte/i 0 1
clinicai 1 0
Table 1: Example of Domain Matrix. we denotes
English terms, wi Italian terms and we/i the com-
mon terms to both languages.
MDMs can be used to describe lexical ambiguity,
variability and inter-lingual domain relations. Lexi-
cal ambiguity is represented by associating one term
to more than one domain, while variability is rep-
resented by associating different terms to the same
domain. For example the term virus is associated
to both the domain COMPUTER SCIENCE and the
domain MEDICINE while the domain MEDICINE is
associated to both the terms AIDS and HIV. Inter-
lingual domain relations are captured by placing dif-
ferent terms of different languages in the same se-
mantic field (as for example HIV e/i, AIDSe/i,
hospitale, and clinicai). Most of the named enti-
ties, such as Microsoft and HIV are expressed using
the same string in both languages.
When similarity among texts in different lan-
guages has to be estimated, the information con-
tained in the MDM is crucial. For example the two
sentences ?I went to the hospital to make an HIV
check? and ?Ieri ho fatto il test dell?AIDS in clin-
ica? (lit. yesterday I did the AIDS test in a clinic)
are very highly related, even if they share no to-
kens. Having an ?a priori? knowledge about the
inter-lingual domain similarity among AIDS, HIV,
hospital and clinica is then a useful information to
11
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
English documents Italian documents
de1 de2 ? ? ? den?1 den di1 di2 ? ? ? dim?1 dim
we1 0 1 ? ? ? 0 1 0 0 ? ? ?
English
Lexicon
we2 1 1 ? ? ? 1 0 0
. . .
... . . . . . . . . . . . . . . . . . . . . . . . . ... 0 ...
wep?1 0 1 ? ? ? 0 0
. . . 0
wep 0 1 ? ? ? 0 0 ? ? ? 0 0
common wi we/i1 0 1 ? ? ? 0 0 0 0 ? ? ? 1 0... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
wi1 0 0 ? ? ? 0 1 ? ? ? 1 1
Italian
Lexicon
wi2 0
. . . 1 1 ? ? ? 0 1
... ... 0 ... . . . . . . . . . . . . . . . . . . . . . . . . .
wiq?1
. . . 0 0 1 ? ? ? 0 1
wiq ? ? ? 0 0 0 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Multilingual term-by-document matrix
recognize inter-lingual topic similarity. Obviously
this relation is less restrictive than a stronger associ-
ation among translation pair. In this paper we will
show that such a representation is sufficient for TC
puposes, and easier to acquire.
In the rest of this section we will provide a formal
definition of the concept of MDM, and we define
some similarity metrics that exploit it.
Formally, let V i = {wi1, wi2, . . . , wiki} be the vo-
cabulary of the corpus T i composed by document
expressed in the language Li, let V ? = ?i V i be
the set of all the terms in all the languages, and
let k? = |V ?| be the cardinality of this set. Let
D = {D1, D2, ..., Dd} be a set of domains. A DM
is fully defined by a k? ? d domain matrix D rep-
resenting in each cell di,z the domain relevance of
the ith term of V ? with respect to the domain Dz .
The domain matrix D is used to define a function
D : Rk? ? Rd, that maps the document vectors ~tj
expressed into the multilingual classical VSM, into
the vectors ~t?j in the multilingual domain VSM. The
function D is defined by2
2In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
D(~tj) = ~tj(IIDFD) = ~t?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wli), ~tj is represented as a row vector, and
IDF (wli) is the Inverse Document Frequency of wli
evaluated in the corpus T l.
The matrix D can be determined for example us-
ing hand-made lexical resources, such as WORD-
NET DOMAINS (Magnini and Cavaglia`, 2000). In
the present work we followed the way to acquire
D automatically from corpora, exploiting the tech-
nique described below.
4.1 Automatic Acquisition of Multilingual
Domain Models
In this work we propose the use of Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990) to in-
duce a MDM from comparable corpora. LSA is an
unsupervised technique for estimating the similar-
ity among texts and terms in a large corpus. In the
monolingual settings LSA is performed by means
of a Singular Value Decomposition (SVD) of the
term-by-document matrix T describing the corpus.
SVD decomposes the term-by-document matrix T
into three matrixes T ' V?k?UT where ?k? is the
diagonal k?k matrix containing the highest k ?  k
12
eigenvalues of T, and all the remaining elements are
set to 0. The parameter k? is the dimensionality of
the Domain VSM and can be fixed in advance (i.e.
k? = d).
In the literature (Littman et al, 1998) LSA has
been used in multilingual settings to define a mul-
tilingual space in which texts in different languages
can be represented and compared. In that work LSA
strongly relied on the availability of aligned parallel
corpora: documents in all the languages are repre-
sented in a term-by-document matrix (see Figure 1)
and then the columns corresponding to sets of trans-
lated documents are collapsed (i.e. they are substi-
tuted by their sum) before starting the LSA process.
The effect of this step is to merge the subspaces (i.e.
the right and the left sectors of the matrix in Figure
1) in which the documents have been originally rep-
resented.
In this paper we propose a variation of this strat-
egy, performing a multilingual LSA in the case in
which an aligned parallel corpus is not available.
It exploits the presence of common words among
different languages in the term-by-document matrix.
The SVD process has the effect of creating a LSA
space in which documents in both languages are rep-
resented. Of course, the higher the number of com-
mon words, the more information will be provided
to the SVD algorithm to find common LSA dimen-
sion for the two languages. The resulting LSA di-
mensions can be perceived as multilingual clusters
of terms and document. LSA can then be used to
define a Multilingual Domain Matrix DLSA.
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
?
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .
Thus DLSA3 can be exploited to estimate simi-
larity among texts expressed in different languages
(see Section 5).
3When DLSA is substituted in Equation 1 the Domain VSM
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema, widely adopted in Information Retrieval.
4.2 Similarity in the multilingual domain space
As an example of the second-order similarity pro-
vided by this approach, we can see in Table 2 the five
most similar terms to the lemma bank. The similar-
ity among terms is calculated by cosine among the
rows in the matrix DLSA, acquired from the data
set used in our experiments (see Section 6.2). It is
worth noting that the Italian lemma banca (i.e. bank
in English) has a high similarity score to the English
lemma bank. While this is not enough to have a pre-
cise term translation, it is sufficient to capture rele-
vant aspects of topic similarity in a cross-language
text categorization task.
Lemma#Pos Similarity Score Language
banking#n 0.96 Eng
credit#n 0.90 Eng
amro#n 0.89 Eng
unicredito#n 0.85 Ita
banca#n 0.83 Ita
Table 2: Terms with high similarity to the English
lemma bank#n, in the Multilingual Domain Model
5 The Multilingual Domain Kernel
Kernel Methods are the state-of-the-art supervised
framework for learning, and they have been success-
fully adopted to approach the TC task (Joachims,
2002).
The basic idea behind kernel methods is to em-
bed the data into a suitable feature space F via a
mapping function ? : X ? F , and then to use a
linear algorithm for discovering nonlinear patterns.
Kernel methods allow us to build a modular system,
as the kernel function acts as an interface between
the data and the learning algorithm. Thus the ker-
nel function becomes the only domain specific mod-
ule of the system, while the learning algorithm is a
general purpose component. Potentially any kernel
function can work with any kernel-based algorithm,
as for example Support Vector Machines (SVMs).
During the learning phase SVMs assign a weight
?i ? 0 to any example xi ? X . All the labeled
instances xi such that ?i > 0 are called Support
Vectors. Support Vectors lie close to the best sepa-
rating hyper-plane between positive and negative ex-
amples. New examples are then assigned to the class
13
of the closest support vectors, according to equation
3.
f(x) =
n
?
i=1
?iK(xi, x) + ?0 (3)
The kernel function K(xi, x) returns the simi-
larity between two instances in the input space X ,
and can be designed just by taking care that some
formal requirements are satisfied, as described in
(Scho?lkopf and Smola, 2001).
In this section we define the Multilingual Domain
Kernel, and we apply it to a cross language TC task.
This kernel can be exploited to estimate the topic
similarity among two texts expressed in different
languages by taking into account the external knowl-
edge provided by a MDM. It defines an explicit map-
ping D : Rk ? Rk? from the Multilingual VSM
into the Multilingual Domain VSM. The Multilin-
gual Domain Kernel is specified by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. Thus the Multilingual Domain Kernel re-
quires Multilingual Domain Matrix D, in particular
DLSA that can be acquired from comparable cor-
pora, as explained in Section 4.1.
To evaluate the Multilingual Domain Kernel we
compared it to a baseline kernel function, namely the
bag of words kernel, that simply estimates the topic
similarity in the Multilingual VSM, as described in
Section 3. The BoW kernel is a particular case of
the Domain Kernel, in which D = I, and I is the
identity matrix.
6 Evaluation
In this section we present the data set (two compara-
ble English and Italian corpora) used in the evalua-
tion, and we show the results of the Cross Language
TC tasks. In particular we tried both to train the
system on the English data set and classify Italian
documents and to train using Italian and classify the
English test set. We compare the learning curves of
the Multilingual Domain Kernel with the standard
BoW kernel, which is considered as a baseline for
this task.
6.1 Implementation details
As a supervised learning device, we used the SVM
implementation described in (Joachims, 1999). The
Multilingual Domain Kernel is implemented by
defining an explicit feature mapping as explained
above, and by normalizing each vector. All the ex-
periments have been performed with the standard
SVM parameter settings.
We acquired a Multilingual Domain Model by
performing the Singular Value Decomposition pro-
cess on the term-by-document matrices representing
the merged training partitions (i.e. English and Ital-
ian), and we considered only the first 400 dimen-
sions4.
6.2 Data set description
We used a news corpus kindly put at our dis-
posal by ADNKRONOS, an important Italian news
provider. The corpus consists of 32,354 Ital-
ian and 27,821 English news partitioned by
ADNKRONOS in a number of four fixed cate-
gories: Quality of Life, Made in Italy,
Tourism, Culture and School. The corpus
is comparable, in the sense stated in Section 2, i.e.
they covered the same topics and the same period of
time. Some news are translated in the other language
(but no alignment indication is given), some others
are present only in the English set, and some others
only in the Italian. The average length of the news
is about 300 words. We randomly split both the En-
glish and Italian part into 75% training and 25% test
(see Table 3). In both the data sets we postagged the
texts and we considered only the noun, verb, adjec-
tive, and adverb parts of speech, representing them
by vectors containing the frequencies of each lemma
with its part of speech.
6.3 Monolingual Results
Before going to a cross-language TC task, we con-
ducted two tests of classical monolingual TC by
training and testing the system on Italian and En-
glish documents separately. For these tests we used
the SVM with the BoW kernel. Figures 2 and 3 re-
port the results.
4To perform the SVD operation we used LIBSVDC
http://tedlab.mit.edu/?dr/SVDLIBC/.
14
English Italian
Categories Training Test Total Training Test Total
Quality of Life 5759 1989 7748 5781 1901 7682
Made in Italy 5711 1864 7575 6111 2068 8179
Tourism 5731 1857 7588 6090 2015 8105
Culture and School 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
Table 3: Number of documents in the data set partitions
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
BoW Kernel
Figure 2: Learning curves for the English part of the
corpus
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
BoW Kernel
Figure 3: Learning curves for the Italian part of the
corpus
6.4 A Cross Language Text Categorization task
As far as the cross language TC task is concerned,
we tried the two possible options: we trained on the
English part and we classified the Italian part, and
we trained on the Italian and classified on the En-
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
Multilingual Domain Kernel
Bow Kernel
Figure 4: Cross-language (training on Italian, test on
English) learning curves
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F1
 m
ea
su
re
Fraction of training data
Multilingual Domain Kernel
Bow Kernel
Figure 5: Cross-language (training on English, test
on Italian) learning curves
glish part. The Multilingual Domain Model was ac-
quired running the SVD only on the joint (English
and Italian) training parts.
Table 4 reports the vocabulary dimensions of the
English and Italian training partitions, the vocabu-
15
# lemmata
English training 22,704
Italian training 26,404
English + Italian 43,384
common lemmata 5,724
Table 4: Number of lemmata in the training parts of
the corpus
lary of the merged training, and how many com-
mon lemmata are present (about 14% of the total).
Among the common lemmata, 97% are nouns and
most of them are proper nouns. Thus the initial term-
by-document matrix is a 43,384 ? 45,132 matrix,
while the DLSA matrix is 43,384 ? 400. For this
task we consider as a baseline the BoW kernel.
The results are reported in Figures 4 and 5. An-
alyzing the learning curves, it is worth noting that
when the quantity of training increases, the per-
formance becomes better and better for the Multi-
lingual Domain Kernel, suggesting that with more
available training it could be possible to go closer to
typical monolingual TC results.
7 Conclusion
In this paper we proposed a solution to cross lan-
guage Text Categorization based on acquiring Mul-
tilingual Domain Models from comparable corpora
in a totally unsupervised way and without using any
external knowledge source (e.g. bilingual dictionar-
ies). These Multilingual Domain Models are ex-
ploited to define a generalized similarity function
(i.e. a kernel function) among documents in differ-
ent languages, which is used inside a Support Vec-
tor Machines classification framework. The basis of
the similarity function exploits the presence of com-
mon words to induce a second-order similarity for
the other words in the lexicons. The results have
shown that this technique is sufficient to capture rel-
evant aspects of topic similarity in cross-language
TC tasks, obtaining substantial improvements over
a simple baseline. As future work we will investi-
gate the performance of this approach to more than
two languages TC task, and a possible generaliza-
tion of the assumption about equality of the common
words.
Acknowledgments
This work has been partially supported by the
ONTOTEXT project, funded by the Autonomous
Province of Trento under the FUP-2004 program.
References
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and
H. Dejean. 2004. A geometric view on bilingual lexi-
con extraction from comparable corpora. In Proceed-
ings of ACL-04, Barcelona, Spain, July.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275?299.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 ? 184. The MIT Press.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer Academic Publishers.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
ACL Workshop on Unsupervised Lexical Acquisition,
Philadelphia, July.
M. Littman, S. Dumais, and T. Landauer. 1998. Auto-
matic cross-language information retrieval using latent
semantic indexing. In G. Grefenstette, editor, Cross
Language Information Retrieval, pages 51?62. Kluwer
Academic Publishers.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, Athens, Greece, June.
D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
B. Scho?lkopf and A. J. Smola. 2001. Learning with Ker-
nels. Support Vector Machines, Regularization, Opti-
mization, and Beyond. The MIT Press.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation. In Proceedings of SENSEVAL-3,
Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
16
Syntagmatic Kernels:
a Word Sense Disambiguation Case Study
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Trento, ITALY
{giuliano,gliozzo,strappa}@itc.it
Abstract
In this paper we present a family of ker-
nel functions, named Syntagmatic Ker-
nels, which can be used to model syn-
tagmatic relations. Syntagmatic relations
hold among words that are typically collo-
cated in a sequential order, and thus they
can be acquired by analyzing word se-
quences. In particular, Syntagmatic Ker-
nels are defined by applying a Word Se-
quence Kernel to the local contexts of the
words to be analyzed. In addition, this
approach allows us to define a semi su-
pervised learning schema where external
lexical knowledge is plugged into the su-
pervised learning process. Lexical knowl-
edge is acquired from both unlabeled data
and hand-made lexical resources, such as
WordNet. We evaluated the syntagmatic
kernel on two standard Word Sense Dis-
ambiguation tasks (i.e. English and Ital-
ian lexical-sample tasks of Senseval-3),
where the syntagmatic information plays
a crucial role. We compared the Syntag-
matic Kernel with the standard approach,
showing promising improvements in per-
formance.
1 Introduction
In computational linguistics, it is usual to deal with
sequences: words are sequences of letters and syn-
tagmatic relations are established by sequences of
words. Sequences are analyzed to measure morpho-
logical similarity, to detect multiwords, to represent
syntagmatic relations, and so on. Hence modeling
syntagmatic relations is crucial for a wide variety
of NLP tasks, such as Named Entity Recognition
(Gliozzo et al, 2005a) and Word Sense Disambigua-
tion (WSD) (Strapparava et al, 2004).
In general, the strategy adopted to model syntag-
matic relations is to provide bigrams and trigrams of
collocated words as features to describe local con-
texts (Yarowsky, 1994), and each word is regarded
as a different instance to classify. For instance, oc-
currences of a given class of named entities (such
as names of persons) can be discriminated in texts
by recognizing word patterns in their local contexts.
For example the token Rossi, whenever is preceded
by the token Prof., often represents the name of a
person. Another task that can benefit from modeling
this kind of relations is WSD. To solve ambiguity it
is necessary to analyze syntagmatic relations in the
local context of the word to be disambiguated. In
this paper we propose a kernel function that can be
used to model such relations, the Syntagmatic Ker-
nel, and we apply it to two (English and Italian)
lexical-sample WSD tasks of the Senseval-3 com-
petition (Mihalcea and Edmonds, 2004).
In a lexical-sample WSD task, training data are
provided as a set of texts, in which for each text
a given target word is manually annotated with a
sense from a predetermined set of possibilities. To
model syntagmatic relations, the typical supervised
learning framework adopts as features bigrams and
trigrams in a local context. The main drawback of
this approach is that non contiguous or shifted col-
57
locations cannot be identified, decreasing the gener-
alization power of the learning algorithm. For ex-
ample, suppose that the verb to score has to be dis-
ambiguated into the sentence ?Ronaldo scored the
goal?, and that the sense tagged example ?the foot-
ball player scores#1 the first goal? is provided for
training. A traditional feature mapping would ex-
tract the bigram w+1 w+2:the goal to represent the
former, and the bigram w+1 w+2:the first to index
the latter. Evidently such features will not match,
leading the algorithm to a misclassification.
In the present paper we propose the Syntagmatic
Kernel as an attempt to solve this problem. The
Syntagmatic Kernel is based on a Gap-Weighted
Subsequences Kernel (Shawe-Taylor and Cristian-
ini, 2004). In the spirit of Kernel Methods, this
kernel is able to compare sequences directly in the
input space, avoiding any explicit feature mapping.
To perform this operation, it counts how many times
a (non-contiguous) subsequence of symbols u of
length n occurs in the input string s, and penalizes
non-contiguous occurrences according to the num-
ber of the contained gaps. To define our Syntag-
matic Kernel, we adapted the generic definition of
the Sequence Kernels to the problem of recognizing
collocations in local word contexts.
In the above definition of Syntagmatic Kernel,
only exact word-matches contribute to the similar-
ity. One shortcoming of this approach is that (near-
)synonyms will never be considered similar, lead-
ing to a very low generalization power of the learn-
ing algorithm, that requires a huge amount of data
to converge to an accurate prediction. To solve this
problem we provided external lexical knowledge to
the supervised learning algorithm, in order to define
a ?soft-matching? schema for the kernel function.
For example, if we consider as equivalent the terms
Ronaldo and football player, the proposition ?The
football player scored the first goal? is equivalent to
the sentence ?Ronaldo scored the first goal?, pro-
viding a strong evidence to disambiguate the latter
occurrence of the verb.
We propose two alternative soft-matching criteria
exploiting two different knowledge sources: (i) hand
made resources and (ii) unsupervised term similar-
ity measures. The first approach performs a soft-
matching among all those synonyms words in Word-
Net, while the second exploits domain relations, ac-
quired from unlabeled data, for the same purpose.
Our experiments, performed on two standard
WSD benchmarks, show the superiority of the Syn-
tagmatic Kernel with respect to a classical flat vector
representation of bigrams and trigrams.
The paper is structured as follows. Section 2 in-
troduces the Sequence Kernels. In Section 3 the
Syntagmatic Kernel is defined. Section 4 explains
how soft-matching can be exploited by the Collo-
cation Kernel, describing two alternative criteria:
WordNet Synonymy and Domain Proximity. Sec-
tion 5 gives a brief sketch of the complete WSD
system, composed by the combination of different
kernels, dealing with syntagmatic and paradigmatic
aspects. Section 6 evaluates the Syntagmatic Kernel,
and finally Section 7 concludes the paper.
2 Sequence Kernels
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping ?, we can use a kernel
function K : X ? X ? R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Sequence Kernels (or String Kernels) are a fam-
ily of kernel functions developed to compute the
inner product among images of strings in high-
dimensional feature space using dynamic program-
ming techniques (Shawe-Taylor and Cristianini,
2004). The Gap-Weighted Subsequences Kernel is
the most general Sequence Kernel. Roughly speak-
ing, it compares two strings by means of the num-
ber of contiguous and non-contiguous substrings of
a given length they have in common. Non contigu-
ous occurrences are penalized according to the num-
ber of gaps they contain.
58
Formally, let ? be an alphabet of |?| symbols,
and s = s1s2 . . . s|s| a finite sequence over ? (i.e.
si ? ?, 1 6 i 6 |s|). Let i = [i1, i2, . . . , in], with
1 6 i1 < i2 < . . . < in 6 |s|, be a subset of the
indices in s: we will denote as s[i] ? ?n the sub-
sequence si1si2 . . . sin . Note that s[i] does not nec-
essarily form a contiguous subsequence of s. For
example, if s is the sequence ?Ronaldo scored the
goal? and i = [2, 4], then s[i] is ?scored goal?. The
length spanned by s[i] in s is l(i) = in ? i1 + 1.
The feature space associated with the Gap-Weighted
Subsequences Kernel of length n is indexed by I =
?n, with the embedding given by
?nu(s) =
X
i:u=s[i]
?l(i), u ? ?n, (1)
where ? ?]0, 1] is the decay factor used to penalize
non-contiguous subsequences1 . The associate ker-
nel is defined as
Kn(s, t) = ??n(s), ?n(t)? =
X
u??n
?nu(s)?nu(t). (2)
An explicit computation of Equation 2 is unfea-
sible even for small values of n. To evaluate more
efficiently Kn, we use the recursive formulation pro-
posed in (Lodhi et al, 2002; Saunders et al, 2002;
Cancedda et al, 2003) based on a dynamic program-
ming implementation. It is reported in the following
equations:
K?0(s, t) = 1, ?s, t, (3)
K?i(s, t) = 0, if min(|s|, |t|) < i, (4)
K??i (s, t) = 0, if min(|s|, |t|) < i, (5)
K??i (sx, ty) =
(
?K??i (sx, t), if x 6= y;
?K??i (sx, t) + ?2K?i?1(s, t), otherwise.
(6)
K?i(sx, t) = ?K?i(s, t) + K??i (sx, t), (7)
Kn(s, t) = 0, if min(|s|, |t|) < n, (8)
Kn(sx, t) = Kn(s, t) +
X
j:tj=x
?2K?n?1(s, t[1 : j ? 1]),
(9)
K ?n and K ??n are auxiliary functions with a sim-
ilar definition as Kn used to facilitate the compu-
tation. Based on all definitions above, Kn can be
1Notice that by choosing ? = 1 sparse subsequences are
not penalized. On the other hand, the kernel does not take into
account sparse subsequences with ? ? 0.
computed in O(n|s||t|). Using the above recursive
definition, it turns out that computing all kernel val-
ues for subsequences of lengths up to n is not signif-
icantly more costly than computing the kernel for n
only.
In the rest of the paper we will use the normalised
version of the kernel (Equation 10) to keep the val-
ues comparable for different values of n and to be
independent from the length of the sequences.
K?(s, t) = K(s, t)p
K(s, s)K(t, t)
. (10)
3 The Syntagmatic Kernel
As stated in Section 1, syntagmatic relations hold
among words arranged in a particular temporal or-
der, hence they can be modeled by Sequence Ker-
nels. The Syntagmatic Kernel is defined as a linear
combination of Gap-Weighted Subsequences Ker-
nels that operate at word and PoS tag level. In partic-
ular, following the approach proposed by Cancedda
et al (2003), it is possible to adapt sequence kernels
to operate at word level by instancing the alphabet ?
with the vocabulary V = {w1, w2, . . . , wk}. More-
over, we restricted the generic definition of the Gap-
Weighted Subsequences Kernel to recognize collo-
cations in the local context of a specified word. The
resulting kernel, called n-gram Collocation Kernel
(KnColl), operates on sequences of lemmata around a
specified word l0 (i.e. l?3, l?2, l?1, l0, l+1, l+2, l+3).
This formulation allows us to estimate the number of
common (sparse) subsequences of lemmata (i.e. col-
locations) between two examples, in order to capture
syntagmatic similarity.
Analogously, we defined the PoS Kernel (KnPoS)
to operate on sequences of PoS tags p?3, p?2, p?1,
p0, p+1, p+2, p+3, where p0 is the PoS tag of l0.
The Collocation Kernel and the PoS Kernel are
defined by Equations 11 and 12, respectively.
KColl(s, t) =
n
?
l=1
K lColl(s, t) (11)
and
KPoS(s, t) =
n
?
l=1
K lP oS(s, t). (12)
Both kernels depend on the parameter n, the length
of the non-contiguous subsequences, and ?, the de-
59
cay factor. For example, K2Coll allows us to repre-
sent all (sparse) bi-grams in the local context of a
word.
Finally, the Syntagmatic Kernel is defined as
KSynt(s, t) = KColl(s, t) + KPoS(s, t). (13)
We will show that in WSD, the Syntagmatic Ker-
nel is more effective than standard bigrams and tri-
grams of lemmata and PoS tags typically used as
features.
4 Soft-Matching Criteria
In the definition of the Syntagmatic Kernel only ex-
act word matches contribute to the similarity. To
overcome this problem, we further extended the def-
inition of the Gap-Weigthed Subsequences Kernel
given in Section 2 to allow soft-matching between
words. In order to develop soft-matching criteria,
we follow the idea that two words can be substi-
tuted preserving the meaning of the whole sentence
if they are paradigmatically related (e.g. synomyns,
hyponyms or domain related words). If the meaning
of the proposition as a whole is preserved, the mean-
ing of the lexical constituents of the sentence will
necessarily remain unchanged too, providing a vi-
able criterion to define a soft-matching schema. This
can be implemented by ?plugging? external paradig-
matic information into the Collocation kernel.
Following the approach proposed by (Shawe-
Taylor and Cristianini, 2004), the soft-matching
Gap-Weighted Subsequences Kernel is now calcu-
lated recursively using Equations 3 to 5, 7 and 8,
replacing Equation 6 by the equation:
K??i (sx, ty) = ?K??i (sx, t) + ?2axyK?i?1(s, t),?x, y, (14)
and modifying Equation 9 to:
Kn(sx, t) = Kn(s, t) +
|t|
X
j
?2axtjK
?
n?1(s, t[1 : j ? 1]).
(15)
where axy are entries in a similarity matrix A be-
tween symbols (words). In order to ensure that the
resulting kernel is valid, A must be positive semi-
definite.
In the following subsections, we describe two al-
ternative soft-matching criteria based on WordNet
Synonymy and Domain Proximity. In both cases, to
show that the similarity matrices are a positive semi-
definite we use the following result:
Proposition 1 A matrix A is positive semi-definite
if and only if A = BTB for some real matrix B.
The proof is given in (Shawe-Taylor and Cristianini,
2004).
4.1 WordNet Synonymy
The first solution we have experimented exploits a
lexical resource representing paradigmatic relations
among terms, i.e. WordNet. In particular, we used
WordNet-1.7.1 for English and the Italian part of
MultiWordNet2.
In order to find a similarity matrix between terms,
we defined a vector space where terms are repre-
sented by the WordNet synsets in which such terms
appear. Hence, we can view a term as vector in
which each dimension is associated with one synset.
The term-by-synset matrix S is then the matrix
whose rows are indexed by the synsets. The en-
try xij of S is 1 if the synset sj contains the term
wi, and 0 otherwise. The term-by-synset matrix S
gives rise to the similarity matrix A = SST be-
tween terms. Since A can be rewritten as A =
(ST )TST = BTB, it follows directly by Proposi-
tion 1 that it is positive semi-definite.
It is straightforward to extend the soft-matching
criterion to include hyponym relation, but we
achieved worse results. In the evaluation section we
will not report such results.
4.2 Domain Proximity
The approach described above requires a large scale
lexical resource. Unfortunately, for many languages,
such a resource is not available. Another possibility
for implementing soft-matching is introducing the
notion of Semantic Domains.
Semantic Domains are groups of strongly
paradigmatically related words, and can be acquired
automatically from corpora in a totally unsuper-
vised way (Gliozzo, 2005). Our proposal is to ex-
ploit a Domain Proximity relation to define a soft-
matching criterion on the basis of an unsupervised
similarity metric defined in a Domain Space. The
Domain Space can be determined once a Domain
2http://multiwordnet.itc.it
60
Model (DM) is available. This solution is evidently
cheaper, because large collections of unlabeled texts
can be easily found for every language.
A DM is represented by a k ? k? rectangular ma-
trix D, containing the domain relevance for each
term with respect to each domain, as illustrated in
Table 1. DMs can be acquired from texts by exploit-
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Model.
ing a lexical coherence assumption (Gliozzo, 2005).
To this aim, Term Clustering algorithms can be used:
a different domain is defined for each cluster, and
the degree of association between terms and clusters,
estimated by the unsupervised learning algorithm,
provides a domain relevance function. As a clus-
tering technique we exploit Latent Semantic Analy-
sis (LSA), following the methodology described in
(Gliozzo et al, 2005b). This operation is done off-
line, and can be efficiently performed on large cor-
pora.
LSA is performed by means of SVD of the term-
by-document matrixT representing the corpus. The
SVD algorithm can be exploited to acquire a domain
matrix D from a large corpus in a totally unsuper-
vised way. SVD decomposes the term-by-document
matrix T into three matrices T = V?kUT where
?k is the diagonal k ? k matrix containing the k
singular values of T. D = V?k? where k?  k.
Once a DM has been defined by the matrixD, the
Domain Space is a k? dimensional space, in which
both texts and terms are represented by means of
Domain Vectors (DVs), i.e. vectors representing the
domain relevances among the linguistic object and
each domain. The DV ~w?i for the term wi ? V is the
ith row of D, where V = {w1, w2, . . . , wk} is the
vocabulary of the corpus.
The term-by-domain matrix D gives rise to the
term-by-term similarity matrix A = DDT among
terms. It follows from Proposition 1 that A is posi-
tive semi-definite.
5 Kernel Combination for WSD
To improve the performance of a WSD system, it
is possible to combine different kernels. Indeed,
we followed this approach in the participation to
Senseval-3 competition, reaching the state-of-the-
art in many lexical-sample tasks (Strapparava et al,
2004). While this paper is focused on Syntagmatic
Kernels, in this section we would like to spend some
words on another important component for a com-
plete WSD system: the Domain Kernel, used to
model domain relations.
Syntagmatic information alone is not sufficient to
define a full kernel for WSD. In fact, in (Magnini
et al, 2002), it has been claimed that knowing the
domain of the text in which the word is located is a
crucial information for WSD. For example the (do-
main) polysemy among the COMPUTER SCIENCE
and the MEDICINE senses of the word virus can
be solved by simply considering the domain of the
context in which it is located.
This fundamental aspect of lexical polysemy can
be modeled by defining a kernel function to esti-
mate the domain similarity among the contexts of
the words to be disambiguated, namely the Domain
Kernel. The Domain Kernel measures the similarity
among the topics (domains) of two texts, so to cap-
ture domain aspects of sense distinction. It is a vari-
ation of the Latent Semantic Kernel (Shawe-Taylor
and Cristianini, 2004), in which a DM is exploited
to define an explicit mapping D : Rk ? Rk? from
the Vector Space Model (Salton and McGill, 1983)
into the Domain Space (see Section 4), defined by
the following mapping:
D(~tj) = ~tj(IIDFD) = ~t?j (16)
where IIDF is a k ? k diagonal matrix such that
iIDFi,i = IDF (wi), ~tj is represented as a row vector,
and IDF (wi) is the Inverse Document Frequency of
wi. The Domain Kernel is then defined by:
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(17)
The final system for WSD results from a com-
bination of kernels that deal with syntagmatic and
paradigmatic aspects (i.e. PoS, collocations, bag of
words, domains), according to the following kernel
61
combination schema:
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj , xj)Kl(xi, xi)
(18)
6 Evaluation
In this section we evaluate the Syntagmatic Kernel,
showing that it improves over the standard feature
extraction technique based on bigrams and trigrams
of words and PoS tags.
6.1 Experimental settings
We conducted the experiments on two lexical sam-
ple tasks (English and Italian) of the Senseval-3
competition (Mihalcea and Edmonds, 2004). In
lexical-sample WSD, after selecting some target
words, training data is provided as a set of texts.
For each text a given target word is manually anno-
tated with a sense from a predetermined set of pos-
sibilities. Table 2 describes the tasks by reporting
the number of words to be disambiguated, the mean
polysemy, and the dimension of training, test and
unlabeled corpora. Note that the organizers of the
English task did not provide any unlabeled material.
So for English we used a domain model built from
the training partition of the task (obviously skipping
the sense annotation), while for Italian we acquired
the DM from the unlabeled corpus made available
by the organizers.
#w pol # train # test # unlab
English 57 6.47 7860 3944 7860
Italian 45 6.30 5145 2439 74788
Table 2: Dataset descriptions.
6.2 Performance of the Syntagmatic Kernel
Table 3 shows the performance of the Syntagmatic
Kernel on both data sets. As baseline, we report
the result of a standard approach consisting on ex-
plicit bigrams and trigrams of words and PoS tags
around the words to be disambiguated (Yarowsky,
1994). The results show that the Syntagmatic Ker-
nel outperforms the baseline in any configuration
(hard/soft-matching). The soft-matching criteria
further improve the classification performance. It
is interesting to note that the Domain Proximity
methodology obtained better results than WordNet
Standard approach
English Italian
Bigrams and trigrams 67.3 51.0
Syntagmatic Kernel
Hard matching 67.7 51.9
Soft matching (WordNet) 67.3 51.3
Soft matching (Domain proximity) 68.5 54.0
Table 3: Performance (F1) of the Syntagmatic Ker-
nel.
Synonymy. The different results observed between
Italian and English using the Domain Proximity
soft-matching criterion are probably due to the small
size of the unlabeled English corpus.
In these experiments, the parameters n and ? are
optimized by cross-validation. For KnColl, we ob-
tained the best results with n = 2 and ? = 0.5. For
KnPoS , n = 3 and ? ? 0. The domain cardinality k?
was set to 50.
Finally, the global performance (F1) of the full
WSD system (see Section 5) on English and Italian
lexical sample tasks is 73.3 for English and 61.3 for
Italian. To our knowledge, these figures represent
the current state-of-the-art on these tasks.
7 Conclusion and Future Work
In this paper we presented the Syntagmatic Kernels,
i.e. a set of kernel functions that can be used to
model syntagmatic relations for a wide variety of
Natural Language Processing tasks. In addition, we
proposed two soft-matching criteria for the sequence
analysis, which can be easily modeled by relax-
ing the constraints in a Gap-Weighted Subsequences
Kernel applied to local contexts of the word to be
analyzed. Experiments, performed on two lexical
sample Word Sense Disambiguation benchmarks,
show that our approach further improves the stan-
dard techniques usually adopted to deal with syntag-
matic relations. In addition, the Domain Proximity
soft-matching criterion allows us to define a semi-
supervised learning schema, improving the overall
results.
For the future, we plan to exploit the Syntagmatic
Kernel for a wide variety of Natural Language Pro-
cessing tasks, such as Entity Recognition and Re-
lation Extraction. In addition we are applying the
soft matching criteria here defined to Tree Kernels,
62
in order to take into account lexical variability in
parse trees. Finally, we are going to further improve
the soft-matching criteria here proposed by explor-
ing the use of entailment criteria for substitutability.
Acknowledgments
The authors were partially supported by the Onto-
Text Project, funded by the Autonomous Province
of Trento under the FUP-2004 research program.
References
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Renders.
2003. Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059?1082.
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
A. Gliozzo, C. Giuliano, and R. Rinaldi. 2005a. Instance
filtering for entity recognition. ACM SIGKDD Explo-
rations, special Issue on Natural Language Processing
and Text Mining, 7(1):11?18, June.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005b. Do-
main kernels for word sense disambiguation. In Pro-
ceedings of the 43rd annual meeting of the Association
for Computational Linguistics (ACL-05), pages 403?
410, Ann Arbor, Michigan, June.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
H. Lodhi, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Learning Research,
2(3):419?444.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Analy-
sis of Text, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other string kernel extensions. In Pro-
ceedings of 19th International Conference onMachine
Learning (ICML02).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, C. Giuliano, and A. Gliozzo. 2004. Pat-
tern abstraction and term similarity for word sense dis-
ambiguation: IRST at Senseval-3. In Proceedings of
SENSEVAL-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, Barcelona, Spain, July.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in span-
ish and french. In Proceedings of the 32nd Annual
Meeting of the ACL, pages 88?95, Las Cruces, New
Mexico.
63
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 145?148,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-irst: Lexical Substitution Task Exploiting
Domain and Syntagmatic Coherence
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
FBK-irst, I-38050, Povo, Trento, ITALY
{giuliano, gliozzo, strappa}@itc.it
Abstract
This paper summarizes FBK-irst participa-
tion at the lexical substitution task of the
SEMEVAL competition. We submitted two
different systems, both exploiting synonym
lists extracted from dictionaries. For each
word to be substituted, the systems rank the
associated synonym list according to a simi-
larity metric based on Latent Semantic Anal-
ysis and to the occurrences in the Web 1T
5-gram corpus, respectively. In particular,
the latter system achieves the state-of-the-art
performance, largely surpassing the baseline
proposed by the organizers.
1 Introduction
The lexical substitution (Glickman et al, 2006a) can
be regarded as a subtask of the lexical entailment,
in which for a given word in context the system is
asked to select an alternative word that can be re-
placed in that context preserving the meaning. Lex-
ical Entailment, and in particular lexical reference
(Glickman et al, 2006b)1 , is in turn a subtask of tex-
tual entailment, which is formally defined as a rela-
tionship between a coherent text T and a language
expression, the hypothesis H . T is said to entail H ,
denoted by T ? H , if the meaning of H can be in-
ferred from the meaning of T (Dagan et al, 2005;
Dagan and Glickman., 2004). Even though this no-
tion has been only recently proposed in the computa-
tional linguistics literature, it attracts more and more
attention due to the high generality of its settings and
to the usefulness of its (potential) applications.
1In the literature, slight variations of this problem have been
also referred to as sense matching (Dagan et al, 2006).
With respect to lexical entailment, the lexical sub-
stitution task has a more restrictive criterion. In
fact, two words can be substituted when meaning is
preserved, while the criterion for lexical entailment
is that the meaning of the thesis is implied by the
meaning of the hypothesis. The latter condition is in
general ensured by substituting either hyperonyms
or synonyms, while the former is more rigid because
only synonyms are in principle accepted.
Formally, in a lexical entailment task a system is
asked to decide whether the substitution of a par-
ticular term w with the term e in a coherent text
Hw = H lwHr generates a sentence He = H leHr
such that Hw ? He, where H l and Hr denote the
left and the right context of w, respectively. For
example, given the source word ?weapon? a system
may substitute it with the target synonym ?arm?, in
order to identify relevant texts that denote the sought
concept using the latter term.
A particular case of lexical entailment is recog-
nizing synonymy, where both Hw ? He and He ?
Hw hold. The lexical substitution task at SEMEVAL
addresses exactly this problem. The task is not easy
since lists of candidate entailed words are not pro-
vided by the organizers. Therefore the system is
asked first to identify a set of candidate words, and
then to select only those words that fit in a particu-
lar context. To promote unsupervised methods, the
organizers did not provide neither labeled data for
training nor dictionaries or list of synonyms explain-
ing the meanings of the entailing words.
In this paper, we describe our approach to the
Lexical Substitution task at SEMEVAL 2007. We
developed two different systems (named IRST1-lsa
and IRST2-syn in the official task ranking), both ex-
ploiting a common lists of synonyms extracted from
dictionaries (i.e. WordNet and the Oxford Dictio-
145
nary) and ranking them according to two different
criteria:
Domain Proximity: the similarity between each
candidate entailed word and the context of the
entailing word is estimated by means of a co-
sine between their corresponding vectors in the
LSA space.
Syntagmatic Coherence: querying a large corpus,
the system finds all occurrences of the target
sentence, in which the entailing word is substi-
tuted with each synonym, and it assigns scores
proportional to the occurrence frequencies.
Results show that both methods are effective. In
particular, the second method achieved the best per-
formance in the competition, defining the state-of-
the-art for the lexical substitution task.
2 Lexical Substitution Systems
The lexical substitution task is a textual entailment
subtask in which the system is asked to provide one
or more terms e ? E ? syn(w) that can be sub-
stituted to w in a particular context Hw = H lwHr
generating a sentence He = H leHr such that both
Hw ? He and He ? Hw hold, where syn(w) is the
set of synonyms lemmata obtained from all synset in
which w appears in WordNet and H l and Hr denote
the left and the right context of w, respectively.
The first step, common to both systems, consists
of determining the set of synonyms syn(w) for each
entailing word (see Section 2.1). Then, each system
ranks the extracted lists according to the criteria de-
scribed in Section 2.2 and 2.3.
2.1 Used Lexical Resources
For selecting the synonym candidates we used two
lexical repositories: WordNet 2.0 and the Oxford
American Writer Thesaurus (1st Edition). For each
target word, we simply collect all the synonyms for
all the word senses in both these resources.
We exploited two corpora for our systems: the
British National Corpus for acquiring the LSA space
for ranking with domain proximity measure (Sec-
tion 2.2) and the Web 1T 5-gram Version 1 corpus
from Google (distributed by Linguistic Data Consor-
tium)2 for ranking the proposed synonyms accord-
ing to syntagmatic coherence (Section 2.3).
2Available from http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
No other resources were used and the sense rank-
ing in WordNet was not considered at all. Therefore
our system is fully unsupervised.
2.2 Domain Proximity
Semantic Domains are common areas of human dis-
cussion, such as Economics, Politics, Law (Magnini
et al, 2002). Semantic Domains can be described
by DMs (Gliozzo, 2005), by defining a set of term
clusters, each representing a Semantic Domain, i.e.
a set of terms having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain.
DMs can be acquired from texts by exploiting
term clustering algorithms. The degree of associ-
ation among terms and clusters, estimated by the
learning algorithm, provides a domain relevance
function. For our experiments we adopted a clus-
tering strategy based on Latent Semantic Analy-
sis (LSA) (Deerwester et al, 1990), following the
methodology described in (Gliozzo, 2005).
The input of the LSA process is a Term by Docu-
ment matrix T of the frequencies in the whole cor-
pus for each term. In this work we indexed all lem-
matized terms. The so obtained matrix is then de-
composed by means of a Singular Value Decompo-
sition, identifying the principal components of T.
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevance with respect to each domain. The DV ~t?i
for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the cor-
pus. The DVs for texts are obtained by mapping the
document vectors ~dj , represented in the vector space
model, into the vectors ~d?j in the Domain Space, de-fined by
D(~dj) = ~dj(IIDFD) = ~d?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wi) and IDF (wi) is the Inverse Document
Frequency of wi. The similarity among both texts
and terms in the Domain Space is then estimated by
the cosine operation.
To implement our lexical substitution criterion we
ranked the candidate entailed words according to
their domain proximity, following the intuition that
if two words can be substituted in a particular con-
text, then the entailed word should belong to the
146
same semantic domain of the context in which the
entailing word is located.
The intuition above can be modeled by estimating
the similarity in the LSA space between the pseudo
document, estimated by Equation 1, formed by all
the words in the context of the entailing word (i.e.
the union of H l and Hr), and each candidate en-
tailed word in syn(w).
2.3 Syntagmatic Coherence
The syntagmatic coherence criterion is based on the
following observation. If the entailing word w in
its context Hw = H lwHr is actually entailed by
a word e, then there exist some occurrences on the
WEB of the expression He = H leHr, obtained
by replacing the entailing word with the candidate
entailed word. This intuition can be easily imple-
mented by looking for occurrences of He in the Web
1T 5-gram Version 1 corpus.
Figure 1 presents pseudo-code for the synonym
scoring procedure. The procedure takes as input the
set of candidate entailed words E = syn(w) for the
entailing word w, the context Hw in which w oc-
curs, the length of the n-gram (2 6 n 6 5) and the
target word itself. For each candidate entailed word
ei, the procedure ngrams(Hw, w, ei, n) is invoked
to substitute w with ei in Hw, obtaining Hei , and re-turns the set Q of all n-grams containing ei. For ex-
ample, all 3-grams obtained replacing ?bright? with
the synonym ?intelligent? in the sentence ?He was
bright and independent and proud.? are ?He was in-
telligent?, ?was intelligent and? and ?intelligent and
independent?. The maximum number of n-grams
generated is ?5n=2 n. Each candidate synonym isthen assigned a score by summing all the frequen-
cies in the Web 1T corpus of the so generated n-
grams3. The set of synonyms is ranked according
the so obtained scores. However, candidates which
appear in longer n-grams are preferred to candidates
appearing in shorter ones. Therefore, the ranked list
contains first the candidate entailed words appearing
in 5-grams, if any, then those appearing in 4-grams,
and so on. For example, a candidate e1 that appears
only once in 5-grams is preferred to a candidate e2
that appears 1000 times in 4-grams. Note that this
strategy could lead to an output list with repetitions.
3Note that n-grams with frequency lower than 40 are not
present in the corpus.
1: Given E, the set of candidate synonyms
2: Given H , the context in which w occurs
3: Given n, the length of the n-gram
4: Given w, the word to be substituted
5: E? ? ?
6: for each ei in E do
7: Q? ngrams(H,w, ei, n)
8: scorei ? 0
9: for each qj in Q do
10: Get the frequency fj of qj
11: scorei ? scorei + fj
12: end for
13: if scorei > 0 then add the pair {scorei, ei}
in E?
14: end for
15: Return E?
Figure 1: The synonym scoring procedure
3 Evaluation
There are basically two scoring methodologies: (i)
BEST, which scores the best substitute for a given
item, and (ii) OOT, which scores for the best 10 sub-
stitutes for a given item, and systems do not benefit
from providing less responses4 .
BEST. Table 1 and 2 report the performance for the
domain proximity and syntagmatic coherence rank-
ing. Please note that in Table 2 we report both the
official score and a score that takes into account just
the first proposal of the systems, as the usual in-
terpretation of BEST score methodology would sug-
gest5.
OOT. Table 4 and 5 report the performance for the
domain proximity and syntagmatic coherence rank-
ing, scoring for the 10 best substitutes. The results
are quite good especially in the case of syntagmatic
coherence ranking.
Baselines. Table 3 displays the baselines respec-
tively for the BEST and OOT using WordNet 2.1
as calculated by the task organizers. They pro-
pose many baseline measures, but we report only the
4The task proposed a third scoring measure MW that scores
precision and recall for detection and identification of multi-
words in the input sentences. However our systems were not
designed for this functionality. For the details of all scoring
methodologies please refer to the task description documents.
5We misinterpreted that the official scorer divides anyway
the figures by the number of proposals. So for the competition
we submitted the oot result file without cutting the words after
the first one.
147
P R Mode P Mode R
all 8.06 8.06 13.09 13.09
Table 1: BEST results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 12.93 12.91 20.33 20.33
all (official) 6.95 6.94 20.33 20.33
Table 2: BEST results for Syntagmatic ranking
(IRST2-syn)
WordNet one, as it is the higher scoring baseline. We
can observe that globally our systems perform quite
good with respect to the baselines.
4 Conclusion
In this paper we reported a detailed description of
the FBK-irst systems submitted to the Lexical En-
tailment task at the SEMEVAL 2007 evaluation cam-
paign. Our techniques are totally unsupervised, as
they do not require neither the availability of sense
tagged data nor an estimation of sense priors, not
considering the WordNet sense order information.
Results are quite good, as in general they signifi-
cantly outperform all the baselines proposed by the
organizers. In addition, the method based on syn-
tagmatic coherence estimated on the WEB outper-
forms, to our knowledge, the other systems sub-
mitted to the competition. For the future, we plan
to avoid the use of dictionaries by adopting term
similarity techniques to select the candidate entailed
words and to exploit this methodology in some spe-
cific applications such as taxonomy induction and
ontology population.
Acknowledgments
Claudio Giuliano is supported by the X-Media
project (http://www.x-media-project.
org), sponsored by the European Commission
as part of the Information Society Technologies
(IST) programme under EC grant number IST-FP6-
026978. Alfio Gliozzo is supported by FIRB-Israel
P R Mode P Mode R
WN BEST 9.95 9.95 15.28 15.28
WN OOT 29.70 29.35 40.57 40.57
Table 3: WordNet Baselines
P R Mode P Mode R
all 41.23 41.20 55.28 55.28
Table 4: OOT results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 69.03 68.90 58.54 58.54
Table 5: OOT results for Syntagmatic ranking
(IRST2-syn)
research project N. RBIN045PXH.
References
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In proceedings of the PASCAL Workshop
on Learning Methods for Text Understanding and Min-
ing, Grenoble.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. Proceed-
ings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings ACL-2006,
pages 449?456, Sydney, Australia, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
O. Glickman, I. Dagan, M. Keller, S. Bengio, and
W. Daelemans. 2006a. Investigating lexical substi-
tution scoring for subtitle generation tenth conference
on computational natural language learning. In Pro-
ceedings of CoNLL-2006.
O. Glickman, E. Shnarch, and I. Dagan. 2006b. Lexical
reference: a semantic matching subtask. In proceed-
ings of EMNLP 2006.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
148
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 136?142,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Bridging Languages
by SuperSense Entity Tagging
Davide Picca and Alfio Massimiliano Gliozzo* and Simone Campora**
University of Lausanne, CH 1015-Lausanne-Switzerland
*Semantic Technology Lab (STLab - ISTC - CNR), Via Nomentana 56-0016, Rome, Italy
**Ecole Polytechnique Federale de Lausanne (EPFL)
davide.picca@unil.ch, alfio.gliozzo@istc.cnr.it, simone.campora@gmail.com
Abstract
This paper explores a very basic linguis-
tic phenomenon in multilingualism: the
lexicalizations of entities are very often
identical within different languages while
concepts are usually lexicalized differ-
ently. Since entities are commonly re-
ferred to by proper names in natural lan-
guage, we measured their distribution in
the lexical overlap of the terminologies ex-
tracted from comparable corpora. Results
show that the lexical overlap is mostly
composed by unambiguous words, which
can be regarded as anchors to bridge lan-
guages: most of terms having the same
spelling refer exactly to the same entities.
Thanks to this important feature of Named
Entities, we developed a multilingual su-
per sense tagging system capable to distin-
guish between concepts and individuals.
Individuals adopted for training have been
extracted both by YAGO and by a heuristic
procedure. The general F1 of the English
tagger is over 76%, which is in line with
the state of the art on super sense tagging
while augmenting the number of classes.
Performances for Italian are slightly lower,
while ensuring a reasonable accuracy level
which is capable to show effective results
for knowledge acquisition.
1 Introduction
The Semantic Web paradigm is often required
to provide a structured view of the unstructured
information expressed in texts (Buitelaar et al,
2005; Cimiano, 2006). Semantic technology re-
quires abundance of such kind of knowledge in
order to cover the web scale in almost any lan-
guage. Natural Language Processing (NLP) has
been adopted with the purpose of knowledge ac-
quisition, and in particular for ontology learn-
ing and information extraction. Structured infor-
mation in ontologies is often expressed by tax-
onomies of concepts, and then populated by in-
stances.
Nonetheless, automatically distinguish con-
cepts from entities in taxonomies is not an easy
task, especially as far as the problem of acquiring
such knowledge from texts is concerned (Zirn et
al., 2008; Picca and Popescu, 2007; Miller and
Hristea, 2006). First of all because such a dis-
tinction is quite vague. From a description log-
ics perspective, that is incidently widely adopted
in ontology engineering, instances are the leaves
of any taxonomy as they cannot be further sub-
categorized and populated by other instances. For
example,?Bill Clinton? is clearly an individual,
since it is instance of many concepts, such as per-
son or president, but at the same time it is a non
sense describing individuals belonging to the class
Bill Clinton.
In order to tackle this issue, we aim to provide
empirical evidence to a very basic linguistic phe-
nomenon in multilingualism, which allows the ex-
ploitation of comparable corpora for bilingual lex-
ical acquisition. It consists on the fact that the lexi-
calizations of entities is very often identical within
different languages while concepts are usually lex-
icalized differently (de Pablo et al, 2006). The
existence of this phenomenon is quite intuitive and
can be easily justified by considering entities as of-
ten referred to by means of ostensive acts (i.e. the
act of nominating objects by indicating them), per-
formed in presentia during every day life. Since
entities are usually referred to using proper names
in natural language, we measured their distribu-
tion in the lexical overlap of the terminologies ex-
tracted from comparable corpora in two different
sample languages (i.e. Italian and English).
Named Entities are instances of particular con-
cepts (such as person or location) and are referred
136
to by proper names. Named Entity Recognition
(NER) is a basic task in NLP that has the in-
tent of automatically recognizing Named Entities.
Incidentally, NER systems can be a useful step
for broad-coverage ontology engineering but they
have two main limitations:
? Traditional categories (e.g., person, location,
and organization) are too few and generic. It
is quite evident that taxonomies require more
categories than the three mentioned above.
? Even though NER systems are supposed to
recognize individuals, very often they also re-
turns common names and no clear distinction
with concepts is made.
A Super Sense Tagger (SST) (Ciaramita and
Johnson, 2003) is an extended NER system that
uses the wider set of categories composed by the
41 most general concepts defined by WordNet.
WordNet has been organized according to psy-
cholinguistic theories on the principles governing
lexical memory (Beckwith et al, 1991). Thus the
broadest WordNet?s categories can serve as basis
for a set of categories which exhaustively covers,
at least as a first approximation, all possible con-
cepts occurring in a sentence.
The aim of this paper is to develop and explore
the property of instances being lexicalized identi-
cally in different languages in order to produce a
SST having the following two features:
? Make explicit distinction between instances
and concepts.
? Analyze the terminology of different lan-
guages adopting a common category set.
Nevertheless, the first point demands to face
with the vague distinction between concepts and
individuals belonging to those concepts. So one
of the main issues explored in this paper is the au-
tomatic tagging of which categories clearly have
this distinction.
The paper is organized as follows. In Section
2 we describe the multilingual SST, an Italian ex-
tension of the English SST that we exploited in
Section 3 to show that the lexical overlap between
languages is mostly composed by unambiguous
words, which can be also regarded as anchors to
bridge the two languages. Most of terms having
the same spelling in the two languages exactly re-
fer to the same entities. We measured those oc-
currencies with respect to all different ontologi-
cal types identified by our tagging device, observ-
ing that most of the overlapped terms are proper
names of persons, organization, locations and ar-
tifact, while the remaining ontological types are
mostly lexicalized by common nouns and have a
quite empty overlap. This confirms our claim that
entities of tangible types are always lexicalized by
the same terms.
In Section 4 we extended the SuperSense Tag-
ger in order to distinguish instances from individ-
uals, while Section 5 is about evaluation. Finally
Section 6 concludes the paper proposing new di-
rections for future investigation.
2 Multilingual Supersense Tagging
SuperSense Tagging is the problem to identify
terms in texts, assigning a ?supersense? category
(e.g. person, act) to their senses within their
context and apply it to recognize concepts and in-
stances in large scale textual collections of texts.
An example of tagging is provided here:
GunsB?noun.group andI?noun.group
RosesI?noun.group playsB?verb.communication
atO theO stadiumB?noun.location
These categories are extracted from WordNet.
WordNet (Fellbaum, 1998) defines 45 lexicogra-
pher?s categories, also called supersenses (Cia-
ramita and Johnson, 2003). They are used by lex-
icographers to provide an initial broad classifica-
tion for the lexicon entries 1.
Although simplistic in many ways, the super-
sense ontology has several attractive features for
NLP purposes. First of all, concepts are easily rec-
ognizable, however very general. Secondly, the
small number of classes makes the implementa-
tion of state of the art methods possible (e.g. se-
quence taggers) to annotate text with supersenses.
Finally, similar word senses tend to be merged to-
gether reducing ambiguity. This technology has
been also adopted for Ontology Learning (Picca et
al., 2007), as the top level WordNet supersenses
cover almost any high level ontological type of
interest in ontology design. Compared to other
semantic tagsets, supersenses have the advantage
of being designed to cover all possible open class
words. Thus, in principle there is a supersense cat-
1We have used the WordNet version 2.0 for all the exper-
iments in the paper.
137
egory for each word, known or novel. Addition-
ally, no distinction is made between proper and
common nouns, whereas standard NER systems
tends to be biased towards the former.
Following the procedure described in (Picca et
al., 2008), we developed a multilingual SST work-
ing on both Italian and English languages by train-
ing the same system on MultiSemcor (Bentivogli
et al, 2004), a parallel English/Italian corpus com-
posed of 116 texts which are the translation of
their corresponding English texts in SemCor. This
resource has been developed by manually trans-
lating the English texts to Italian. Then, the so
generated parallel corpus has been automatically
aligned at the Word Level. Finally, sense labels
have been automatically transferred from the En-
glish words to their Italian translations.
The sense labels adopted in the Italian part of
MultiSemCor (Bentivogli et al, 2004) have been
extracted by Multi WordNet 2. It is a multilingual
computational lexicon, conceived to be strictly
aligned with the Princeton WordNet. The avail-
able languages are Italian, Spanish, Hebrew and
Romanian. In our experiment we used the En-
glish and the Italian components. The last version
of the Italian WordNet contains around 58,000
Italian word senses and 41,500 lemmas organized
into 32,700 synsets aligned with WordNet English
synsets. The Italian synsets are created in cor-
respondence with the Princeton WordNet synsets
whenever possible, and the semantic relations are
ported from the corresponding English synsets.
This implies that the synset index structure is the
same for the two languages.
The full alignment between the English and the
Italian WordNet is guaranteed by the fact that both
resources adopts the same synset IDs to refer to
concepts. This nice feature has allowed us to in-
fer the correct super-sense for each Italian sense
by simply looking at the English structure. In this
way, we assign exactly the same ontological types
to both Italian and English terms, thus obtaining an
Italian corpus tagged by its supersenses as shown
below:
IO GunsB?noun.group andI?noun.group
RosesI?noun.group suonanoB?verb.communication
alloO stadioB?noun.location
2Available at http://multi WordNet.itc.it.
3 Lexical Overlap in Comparable
Corpora
Comparable corpora are collections of texts in dif-
ferent languages that regard similar topics (e.g.
a collection of news published by press agencies
in the same period). More restrictive require-
ments are expected for parallel corpora (i.e. cor-
pora composed of texts which are mutual transla-
tions), while the class of the multilingual corpora
(i.e. collection of texts expressed in different lan-
guages without any additional requirement) is the
more general. Obviously parallel corpora are also
comparable, while comparable corpora are also
multilingual.
In comparable corpora, most of the individu-
als preserve the same spelling across different lan-
guages, while most concepts are translated differ-
ently. The analysis of the acquired terms for differ-
ent ontological types shows a huge percentage of
overlapped Named Entities. For our experiments,
we assumed that the distinction between common
names and proper names reflect as well the dif-
ference between concepts and entities in a formal
ontology. Since proper names are recognized by
the PoS tagger with relatively high precision, we
interpreted occurrences of proper names in the ac-
quired terminology as an evidence for detecting
entities.
The Leipzig Corpora Collection (Quasthoff,
2006) presents corpora in different languages us-
ing the same format and comparable sources. The
corpora are identical in format and similar in size
and content. They contain randomly selected sen-
tences in the language of the corpus. For the ex-
periments reported in this paper, we used the Ital-
ian and the English part composed by 300,000
sentences. As shown in Figure 1 and in Figure
2, Named Entities are mostly concentrated into
tangible types: Groups (organizations), Locations,
Persons and Artifacts.
The results analysis is more impressive. Figure
3 shows that the lexical overlap (i.e. the subset
of terms in common between English and Italian)
is composed almost exclusively by entities (i.e.
proper nouns). Instead if we take a look at Figure
4, we can observe that concepts are generally not
shared, having an average percentage lower than
0.1%, independently of the ontological type. We
can also observe the predictable result that onto-
logical categories denoting material objects (i.e.
persons, locations and groups, artifacts) still have
138
Figure 1: Distribution of discovered entity types
in English
Figure 2: Distribution of discovered entity types
in Italian
greater percentage of shared entities.
This is in line with the common practice of
training NER on these categories. Examples of
shared terms (entities) in concrete categories are:
? noun.group: e.g. NATO, Boeing, NASA;
? noun.location: e.g. Canada, Austria, Hous-
ton;
? noun.person: e.g. Romano Prodi, Blair,
Kofi Annan.
Incidentally, exceptions can be found to our
hypothesis (i.e. some concept is also shared).
Figure 3: Shared Named Entities in both lan-
guages
Figure 4: Shared Concepts in both languages
Examples are terms belonging to the supersense
noun.object such as Radio and Computer.
Anyhow, being them ported from one language
to another, they generally do not cause problems,
since they tend to share the same meaning. In our
experiments (i.e. in the sample we manually ana-
lyzed), we did not find any false friend, suggesting
that the impact of those words is relatively small,
in spite of the fact that it is very often overempha-
sized.
Inversely, many abstract types (e.g.
noun.possession and noun.feeling) do not
share terminology at all.
4 Distinguishing entities from concepts
Successively, we subdivided each category into
two sub-categories for both languages, Instance
and Concept so that now the term ?president? is
tagged as noun.person Concept and the term ?Bill
Clinton? as noun.person Instance. In order to au-
tomate this task and create a reliable training set,
we adopted the following strategy.
We used the concept/instances distinction pro-
vided by YAGO (Suchanek et al, 2007b). YAGO
is a huge semantic knowledge base developed by
the Max-Plack-Institute of Saarbrcken. YAGO
knows over 1.7 million entities (like persons,
organizations, cities, etc.). YAGO, exploits
Wikipedia?s info-boxes and category pages. Info-
boxes are standardized tables that contain basic in-
formation about the entity described in the article
(Suchanek et al, 2007a). For our purposes it is
fundamental that YAGO?s components are repre-
sented as entities. In our experiment we exploit
entities as proper names and we use only YAGO
entity database containing named entities.
For each term belonging to one of the concrete
categories, we check if it appears in YAGO en-
tity dataset, otherwise, if the term is not found in
139
YAGO, it has to satisfy all the following condi-
tions to be tagged as Instance:
? The part of speech of the term belongs to
one of the noun categories as ?NN?, ?NNS?,
?NNP? or ?NNPS?.
? The first letter of the term is a capital letter.
? The term does not come after a full stop.
Upon a total of 12817 instances, almost 14 have
been found in YAGO, 3413 have been found using
the heuristic strategy and the rest have been classi-
fied as concepts. If we take the previous example,
the new output has now this form:
? GunsB?noun.group?Instance
andI?noun.group?Instance
RosesI?noun.group?Instance
playsB?verb.communication atO theO
stadiumB?noun.location?Concept
or
? GunsB?noun.group?Instance
andI?noun.group?Instance
RosesI?noun.group?Instance
suonanoB?verb.communication alloO
stadioB?noun.location?Concept
Afterwards, we trained the SST engine. It im-
plements a Hidden Markov Model, trained with
the perceptron algorithm introduced in (Collins,
2002) and it achieves a recall of 77.71% and
a precision of 76.65% . Perception sequence
learning provides an excellent trade-off accu-
racy/performance, sometimes outperforming more
complex models such as Conditional Random
Fields (Nguyen and Guo, 2007). We optimized the
required parameters by adopting a cross validation
technique. As for the settings developed by (Cia-
ramita and Johnson, 2003), the best results have
been obtained by setting 50 trials and 10 epochs to
train the perceptron algorithm. The basic feature
set used for the training process, includes:
? word = lower-cased form of each token for
the current position i and in addition for i-1
and i+1
? sh = shape of the token as a simple regular
expression-like representation
? pos = POS of i, i-1 and i+1
Category Recall Prec. F1
noun.artifact Concept 0.72 0.73 0.73
noun.artifact Instance 0.59 0.64 0.62
noun.group Concept 0.72 0.73 0.73
noun.group Instance 0.68 0.70 0.69
noun.location Concept 0.68 0.65 0.66
noun.location Instance 0.75 0.80 0.77
noun.person Concept 0.83 0.80 0.82
noun.person Instance 0.92 0.88 0.90
Table 1: Recall, precision and F1 for each category
for English
? sb= bi- and tri-grams of characters of the suf-
fix of word i
? pr= bi- and tri-grams of characters of the pre-
fix of word i
? rp = coarse relative position of word i,
rp=begin if i = 0, rp=end if i = ?sentence?-
1, sb=mid otherwise
? kf = constant features on each token for reg-
ularization purposes
Finally, we trained the SST engine in the Italian
corpus generated so far, and we evaluated the su-
per sense tagging accuracy by adopting the same
evaluation method as described in (Ciaramita and
Johnson, 2003), obtaining F1 close to 0.70. How-
ever quite lower than the English F1, this result is
in line with the claim, since the Italian corpus is
smaller and lower in quality.
5 SST Performance and Evaluation
We evaluated the performances of the SST gen-
erated so far by adopting a n-fold cross valida-
tion strategy on the Semcor adopted for training.
Results for the chosen categories are illustrated
in Table 1 and Table 2, reporting precision, re-
call and F1 for any Supersense. If we cast a
deeper glance at the tables, we can clearly no-
tice that for some category the F1 is exception-
ally high. Some of those best categorized cat-
egories are really essential for ontology learn-
ing. For example, important labels as noun.person
or noun.group achieve results among the 70%.
For some categories we have found a F1 over
0.80% as noun.person Instance (F1 0.90% ) or
noun.person Concept (F1 0.85% )
On the other hand, the Italian tagger achieved
lower performances if compared with the English.
140
Category Recall Prec. F1
noun.artifact Concept 0.64 0.63 0.63
noun.artifact Instance 0.66 0.67 0.66
noun.group Concept 0.61 0.65 0.63
noun.group Instance 0.66 0.66 0.66
noun.location Concept 0.55 0.53 0.54
noun.location Instance 0.56 0.76 0.64
noun.person Concept 0.81 0.76 0.78
noun.person Instance 0.88 0.81 0.85
Table 2: Recall, precision and F1 for each category
for Italian
It can be explained by (i) the lower quality of the
training resource, (ii) the lower quantity of training
data and (iii) the unavailability of the first sense
info.
Regarding the first point, it is worthwhile to re-
mark that even if the quality of transfer developed
by (Bentivogli et al, 2004) is high, many incor-
rect sense transfers (around 14%) can be found.
Because of that our work suffers of the same in-
herited faults by the automatic alignment. For in-
stance, we report here the most relevant errors we
faced with during the preprocessing step. One of
the main errors that has badly influenced the train-
ing set especially for multiword recognition is the
case in which the translation equivalent is indeed a
cross-language synonym of the source expression
but not a lexical unit. It occurs when a language
expresses a concept with a lexical unit whereas the
other language expresses the same concept with a
free combination of words (for instance occhiali
da sole annotated with the sense of sunglasses).
Regarding the second problem, we noticed
that the quantity of sense labeled words adopted
for English is higher than 200,000, whereas the
amount of Italian tokens adopted is around 92,000.
Therefore, the amount of Italian training data
is sensibly lower, explaining the lower perfor-
mances.
Moreover, the italian SST lacks in one of the
most important feature used for the English SST,
first sense heuristics. In fact, for the Italian lan-
guage, the first sense baseline cannot be estimated
by simply looking at the first sense in WordNet,
since the order of the Italian WordNet does not re-
flect the frequency of senses. Therefore, we did
not estimate this baseline for the Italian SST, in
contrast to what has been done for the English
SST.
6 Conclusion and Future Work
In this work, we presented an empirical investiga-
tion about the role of Named Entities in compara-
ble corpora, showing that they largely contribute
in finding bridges between languages since they
tend to refer to the same entities. This feature
allows us to discover bridges among languages
by simply looking for common Named Entities in
corpora that are generally not parallels since such
terms are usually associated to the same objects
in the external world. We demonstrated that most
terms in the lexical overlap between languages are
entities, and we showed that they belong to few
fundamentals categories (including persons, loca-
tions and groups).
A predominant amount of entities in the lexi-
cal overlap could be conceived as a support to our
claim that Named Entities can be used to bridge
the languages, since they preserve meaning and
provide a set of highly accurate anchors to bridge
languages in multilingual knowledge bases. Those
anchors can be used as a set of seeds to boost fur-
ther statistical or logical lexical acquisition pro-
cesses. In addition, the impact of false friends re-
vealed to be less problematic than expected.
We trained a multilingual super sense tagger
on the Italian and English language and we in-
troduced the distinction between concept and in-
stance in a subset of its target classes, where our
investigation suggested to look for concrete types.
The resulting tagger largely extend the capabilities
of the state of art supersense technology, by pro-
viding a multilingual tool which can be effectively
used for multilingual knowledge induction.
For the future, we are going to further explore
the direction of multilingual knowledge induction,
exploiting the tagger developed so far for ontology
engineering and knowledge retrieval. In addition,
we plan to leverage more on the lexical overlap
property analyzed in this paper, for example to de-
velop unsupervised super sense taggers for all lan-
guages where annotated corpora are not available.
Acknowledgments
Alfio Massimiliano Gliozzo has been supported by
the BONy project, financed by the Education and
culture DG of the EU, grant agreement N 135263-
2007-IT-KA3-KA3MP, under the Lifelong Learn-
ing Programme 2007 managed by EACEA.
141
References
R. Beckwith, C. Fellbaum, D. Gross, and G. Miller.
1991. 9. wordnet: A lexical database organized on
psycholinguistic principles. Lexicons: Using On-
Line Resources to Build a Lexicon, pages 211?232,
Jan.
L. Bentivogli, P. Forner, and E. Pianta. 2004. Evalu-
ating cross-language annotation transfer in the mul-
tisemcor corpus. In COLING ?04: Proceedings of
the 20th international conference on Computational
Linguistics, page 364, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. On-
tology learning from texts: methods, evaluation and
applications. IOS Press.
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in wordnet. In Proceedings
of EMNLP-03, pages 168?175, Sapporo, Japan.
P. Cimiano. 2006. Ontology Learning and Popula-
tion from Text: Algorithms, Evaluation and Appli-
cations. Springer-Verlag New York, Inc., Secaucus,
NJ, USA.
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP-02.
C. de Pablo, J.L. Mart??nez, and P. Mart??nez. 2006.
Named entity processing for cross-lingual and mul-
tilingual ir applications. In proceedings of the SI-
GIR2006 workshop on New Directions In Multilin-
gual Information Access.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
G. A. Miller and F. Hristea. 2006. Wordnet nouns:
Classes and instances. Computational Linguistics,
32(1):1?3.
N. Nguyen and Y. Guo. 2007. Comparison of se-
quence labeling algorithms and extensions. In Pro-
ceedings of ICML 2007, pages 681?688.
D. Picca and A. Popescu. 2007. Using wikipedia and
supersense tagging for semi-automatic complex tax-
onomy construction. In proceedings RANLP.
D. Picca, A. Gliozzo, and M. Ciaramita. 2007. Se-
mantic domains and supersens tagging for domain-
specific ontology learning. In proceedings RIAO
2007.
D. Picca, A. M. Gliozzo, and M. Ciaramita. 2008.
Supersense tagger for italian. In proceedings of
the sixth international conference on Language Re-
sources and Evaluation (LREC 2008).
C. B. Quasthoff, U. M. Richter. 2006. Corpus portal
for search in monolingual corpora,. In Proceedings
of the fifth international conference on Language
Resources and Evaluation, LREC, pages pp. 1799?
1802.
F. Suchanek, G. Kasneci, and G. Weikum. 2007a.
Yago: A large ontology from wikipedia and word-
net. Technical Report.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007b.
Yago: a core of semantic knowledge. In WWW ?07:
Proceedings of the 16th international conference on
World Wide Web, pages 697?706, New York, NY,
USA. ACM Press.
C. Zirn, V. Nastase, and M. Strube. 2008. Distinguish-
ing between instances and classes in the wikipedia
taxonomy. Lecture notes in computer science, Jan.
142
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 265?272
Manchester, August 2008
Instance-Based Ontology Population
Exploiting Named-Entity Substitution
Claudio Giuliano
Fondazione Bruno Kessler
Trento, Italy
giuliano@fbk.eu
Alfio Gliozzo
Laboratory for Applied Ontology
Italian National Research Council
Rome, Italy
alfio.gliozzo@cnr.istc.it
Abstract
We present an approach to ontology popu-
lation based on a lexical substitution tech-
nique. It consists in estimating the plausi-
bility of sentences where the named entity
to be classified is substituted with the ones
contained in the training data, in our case,
a partially populated ontology. Plausibility
is estimated by using Web data, while the
classification algorithm is instance-based.
We evaluated our method on two different
ontology population tasks. Experiments
show that our solution is effective, out-
performing existing methods, and it can
be applied to practical ontology population
problems.
1 Introduction
Semantic Web and knowledge management appli-
cations require to populate the concepts of their
domain ontologies with individuals and find their
relationships from various data sources, including
databases and natural language texts. As the ex-
tensional part of an ontology (the ABox) is often
manually populated, this activity can be very time-
consuming, requiring considerable human effort.
The development of automatic techniques for on-
tology population is then a crucial research area.
Natural language processing techniques are natu-
ral candidates to solve this problem as most of the
data contained in the Web and in the companies?
intranets is free text. Information extraction (IE)
is commonly employed to (semi-) automate such a
task.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Current state-of-the-art IE systems are mostly
based on general purpose supervised machine
learning techniques (e.g., kernel methods). How-
ever, supervised systems achieve acceptable accu-
racy only if they are supplied with a sufficiently
large amount of training data, usually consisting of
manually annotated texts. Consequently, they can
be only used to populate top-level concepts of on-
tologies (e.g., people, locations, organizations). In
fact, when the number of subclasses increases the
number of annotated documents required to find
sufficient positive examples for all subclasses be-
comes too large to be practical. As domain ontolo-
gies usually contain hundreds of concepts arranged
in deep class/subclass hierarchies, alternative tech-
niques have to be found to recognize fine-grained
distinctions (e.g., to categorize people as scientists
and scientists as physicists, mathematicians, biol-
ogists, etc.).
In this paper, we present an approach to the clas-
sification of named entities into fine-grained onto-
logical categories based on a method successfully
employed in lexical substitution.
1
In particular, we
predict the fine-grained category of a named en-
tity, previously recognized, by simply estimating
the plausibility of sentences where the entity to be
classified is substituted with the ones contained in
the training data, in our case, a partially populated
ontology.
In most of the cases, ontologies are partially
populated during the development phase and af-
ter that the annotation cost is practically negligi-
ble, making this method highly attractive in many
applicative domains. This allows us to define an
instance-based learning approach for fine-grained
1
Lexical substitution consists in identifying the most
likely alternatives (substitutes) of a target word given its con-
text (McCarthy, 2002).
265
entity categorization that exploits the Web to col-
lect evidence of the new entities and does not re-
quire any labeled text for supervision, only a par-
tially populated ontology. Therefore, it can be used
in different domains and languages to enrich an
existing ontology with new entities extracted from
texts by a named-entity recognition system and/or
databases.
We evaluated our method on the benchmark pro-
posed by Tanev and Magnini (2006) to provide a
fair comparison with other approaches, and on a
general purpose ontology of people derived from
WordNet (Fellbaum, 1998) to perform a more ex-
tensive evaluation. Specifically, the experiments
were designed to investigate the effectiveness of
our approach at different levels of generality and
with different amounts of training data. The results
show that it significantly outperforms the base-
line methods and, where a comparison is possible,
other approaches and achieves a good performance
with a small number of examples per category. Er-
ror analysis shows that most of the misclassifica-
tion errors are due to the finer-grained distinctions
between instances of the same super-class.
2 Lexical Substitutability and Ontology
Population
Our approach is based on the assumption that en-
tities that occur in similar contexts belong to the
same concept(s). This can be seen as a special
case of the distributional hypothesis, that is, terms
that occur in the same contexts tend to have similar
meanings (Harris, 1954).
If our assumption is correct, then given an in-
stance in different contexts one can substitute it
with another of the same ontological type (i.e.,
of the same category) and probably generate true
statements. In fact, most of the predicates that
can be asserted for an instance of a particular cate-
gory can also be asserted for other instances of the
same category. For instance, the sentence ?Ayr-
ton Senna is a F1 Legend? preserves its truthful-
ness when Ayrton Senna is replaced with Michael
Schumacher, while it is false when Ayrton Senna
is replaced with the MotoGP champion Valentino
Rossi.
For our purposes, the Web provides a simple and
effective solution to the problem of determining
whether a statement is true or false. Due to the high
redundancy of the Web, the high frequency of a
statement generated by a substitution usually pro-
vides sufficient evidence for its truth, allowing us
to easily implement an automatic method for fine-
grained entity classification. Following this intu-
ition, we developed an ontology population tech-
nique adopting pre-classified entities as training
data (i.e., a partially populated ontology) to clas-
sify new ones.
When a new instance has to be classified, we
first collect snippets containing it from the Web.
Then, for each snippet, we substitute the new in-
stance with each of the training instances. The
snippets play a crucial role in our approach be-
cause we expect that they provide the features that
characterize the category to which the entity be-
longs. Thus, it is important to collect a sufficiently
large number of snippets to capture the features
that allow a fine-grained classification.
To estimate the correctness of each substitution,
we calculate a plausibility score using a modified
version of the lexical substitution algorithm intro-
duced in Giuliano et al (2007), that assigns higher
scores to the substitutions that generate highly fre-
quent sentences on the Web. In particular, this
technique ranks a given list of synonyms accord-
ing to a similarity metric based on the occur-
rences in the Web 1T 5-gram corpus,
2
which spec-
ify n-grams frequencies in a large Web sample.
This technique achieved the state-of-the-art perfor-
mance on the English Lexical Substitution task at
SemEval 2007 (McCarthy and Navigli, 2007).
Finally, on the basis of these plausibility scores,
the algorithm assigns the new instance to the cat-
egory whose individuals show a closer linguistic
behavior (i.e., they can be substituted generating
plausible statements).
3 The IBOP algorithm
In this section, we describe the algorithmic and
mathematical details of our approach. The
instance-based ontology population (IBOP) algo-
rithm is an instance-based supervised machine
learning approach.
3
The proposed algorithm is
summarized as follows:
Step 1 For each candidate instance i, we collect
the first N snippets containing i from the Web.
For instance, 3 snippets for the candidate instance
Ayrton Senna are ?The death of Ayrton Senna at
2
http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
3
An analogy between instance-based learning methods
and our approach is left to future work.
266
the 1994 San Marino GP?, ?triple world cham-
pion Ayrton Senna?, and ?about F1 legend Ayrton
Senna?.
Step 2 Then, for each retrieved snippet q
k
(1 6
k 6 N ), we derive a list of hypothesis phrases
by replacing i with each training instance j from
the given ontology. For instance, from the snippet
?about F1 legend Ayrton Senna?, we derive ?about
F1 legend Michael Schumacher? and ?about F1
legend Valentino Rossi?, assuming to have the for-
mer classified as F1 driver and the latter as Mo-
toGP driver.
Step 3 For each hypothesis phrase h
j
, we calcu-
late the plausibility score s
j
using a variant of the
scoring procedure defined in Giuliano et al (2007).
In our case, s
j
is given by the sum of the point-
wise mutual information (PMI) of all the n-grams
(1 < n 6 5) that contain j divided by the self-
information of the right and left contexts.
4
Divid-
ing by the self-information allows us to penalize
the hypotheses that have contexts with a low infor-
mation content, such as sequences of stop words.
The frequency of the n-grams is estimated from
the Web 1T 5-gram corpus. For instance, from
the hypothesis phrase ?about F1 legend Michael
Schumacher?, we generate and score the following
n-grams: ?legend Michael Schumacher?, ?F1 leg-
end Michael Schumacher?, and ?about F1 legend
Michael Schumacher?.
Step 4 To obtain an overall score s
c
for the cat-
egory c, we sum the scores obtained from each
training instance of category c for all snippets, as
defined in Equation 1.
s
c
=
N
X
k=1
M
X
l=1
s
l
, (1)
where M is the number of training instances for
the category c.
5
Step 5 Finally, the instance i is categorized with
that concept having the maximum score:
c
?
=
(
argmax
c
s
c
if s
c
> ?;
? otherwise.
(2)
4
The pointwise mutual information is defined as the log of
the deviation between the observed frequency of a n-gram and
the probability of that n-gram if it were independent and the
self-information is a measure of the information content of a
n-gram (? log p, where p is the probability of the n-gram).
5
Experiments using the sum of average or argmax score
yield worst results.
Where a higher value of the parameter ? increases
precision but degrades recall.
4 Benchmarks
For evaluating the proposed algorithm and com-
paring it with other algorithms, we adopted the two
benchmarks described below.
4.1 Tanev and Magnini Benchmark
Tanev and Magnini (2006) proposed a benchmark
ontology that consists of two high-level named
entity categories (i.e., person and location) both
having five fine-grained subclasses (i.e., mountain,
lake, river, city, and country as subtypes of loca-
tion; statesman, writer, athlete, actor, and inventor
are subtypes of person). WordNet and Wikipedia
were used as primary data sources for populating
the evaluation ontology. In total, the ontology is
populated with 280 instances which were not am-
biguous (with respect to the ontology). We ex-
tracted the training set from WordNet, collecting
20 examples per sub-category, of course, not al-
ready contained in the test set.
4.2 People Ontology
The benchmark described in the previous section
is clearly a toy problem, and it does not allow us
to evaluate the effectiveness of our method, in par-
ticular the ability to perform fine-grained classifi-
cations. To address this problem, we developed
a larger ontology of people (called People Ontol-
ogy), characterized by a complex taxonomy hav-
ing multiple layers and containing thousands of in-
stances. This ontology has been extracted from
WordNet, that we adapted to our purpose after a
re-engineering phase. In fact, we need a formal
specification of the conceptualizations that are ex-
pressed by means of WordNet?s synsets, and, in
particular, we need a clear distinction between in-
dividuals and categories, as well as a robust cate-
gorization mechanism to assign individuals to gen-
eral concepts.
This result can be achieved by following the di-
rectives defined by Gangemi et al (2003) for On-
toWordNet, in which the informal WordNet se-
mantics is re-engineered in terms of a description
logic. We follow an analogous approach. Firstly,
any possible instance in WordNet 1.7.1 has been
identified by looking for all those synsets contain-
ing at least one word starting with a capital letter.
The result is a set of instances I . All the remaining
267
Figure 1: The taxonomy of the People Ontology extracted from WordNet 1.7.1. Numbers in brackets are
the total numbers of individuals per category. Concepts that have less than 40 instances were removed.
synsets are then regarded as concepts, collected in
the set C. Then, is a relations between synsets are
converted into one of the following standard OWL-
DL constructs:
X subclass of Y if X is a Y and X ? C and Y ? C
X instance of Y if X is a Y and X ? I and Y ? C
The formal semantics of both subclass of
and instance of is formally defined in OWL-
DL. subclass of is a transitive relation (i.e.,
Xsubclass ofY and Y subclass ofZ implies
Xsubclass ofZ) and the instance of relation
has the following property: Xinstance ofY and
Y subclass ofZ implies Xinstance ofZ.
To define the People Ontology, we selected
the sub-hierarchy of WordNet representing peo-
ple, identifying the corresponding top-level synset
X = {person, individual, someone, somebody,
mortal, soul}, and collecting all the classes Y such
that Y is a subclass of X and all the instances I
such that I is an instance of Y . We discovered
that many concepts in the derived hierarchy were
empty or scarcely populated. As we need a suffi-
cient amount data to obtain statistically significant
results, we eliminated the classes that contain less
than 40 instances from the ontology. The derived
ontology contains 1627 instances structured in 21
sub-categories (Figure 1). Finally, we randomly
split its individuals into two equally sized subsets.
The results reported in the following section were
evaluated using two-fold cross-validation on these
two subsets.
5 Evaluation
In this section, we present the performance of the
IBOP algorithm on the evaluation benchmarks de-
scribed in the previous section.
5.1 Experimental Setting
For each individual, we collected 100 entity men-
tions in their context by querying Google
TM
. As
most of them are names of celebrities, the Web
provided sufficient data.
6
We approached the population task as a stan-
dard categorization problem, trying to assign new
instances to the most specific category. We mea-
sured standard precision/recall figures. In addition,
we evaluated the classifier accuracy at the most ab-
stract level, by inheriting the predictions from sub-
concepts to super-concepts. For example, when
an instance is assigned to a specific category (e.g.,
Musician), it is also (implicitly) assigned to all its
super-classes (e.g., Artist and Creator). This op-
eration is performed according to the extensional
semantics of the description logic, as described in
the previous section. Following this approach, we
are able to evaluate the effectiveness of our algo-
rithm at any level of generality. The micro- and
macro-averaged F
1
have been evaluated by taking
into account both specific and generic classes at
the same time. In this way, we tend to penalize the
6
A study of how the number of snippets N would impact
the performance of the IBOP algorithm has been deferred to
future work.
268
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 10  20  30  40  50
Mic
ro-
F 1
Number of examples
Figure 2: Learning curve on the People Ontology.
gross misclassification errors (e.g., Biologist vs.
Poet), while minor errors (e.g., Poet vs. Drama-
tist) are less relevant. This approach is similar to
the one proposed by Melamed and Resnik (2000)
for a similar hierarchical categorization task.
5.2 Accuracy
Table 1 shows micro- and macro-averaged results
of the proposed method obtained on the Tanev
and Magnini (2006) benchmark and compares
them with the class-example (Tanev and Magnini,
2006), IBLE (Giuliano and Gliozzo, 2007), and
class-word (Cimiano and V?olker, 2005) methods,
respectively. Table 2 shows micro- and macro-
averaged results of the proposed method obtained
on the People Ontology and compares them with
the random and most frequent baseline methods.
7
In both experiments, the IBOP algorithm was
trained on 20 examples per category and setting
the parameter ? = 0 in Equation 2.
For the People Ontology, we performed a dis-
aggregated evaluation, whose results are shown in
Table 3, while Figure 2 shows the learning curve.
The experiment was conducted setting the param-
eter ? = 0.
System Micro-F
1
Macro-F
1
IBOP 73 71
Class-Example 68 62
IBLE 57 47
Class-Word 42 33
Table 1: Comparison of different ontology popula-
tion techniques on the Tanev and Magnini (2006)
benchmark.
7
The most frequent category has been estimated on the
training data.
System Micro-F
1
Macro-F
1
IBOP 70.1 62.3
Random 15.4 15.5
Most Frequent 20.7 3.3
Table 2: Comparison between the IBOP algorithm
and the baseline methods on the People Ontology.
Class Prec Recall F
1
Scientist 84.4 73.3 78.4
Physicist 63.0 39.3 48.4
Mathematician 25.0 67.5 36.5
Chemist 44.2 52.0 47.7
Biologist 62.5 13.2 21.7
Social scientist 43.1 30.1 35.5
Performer 76.5 66.9 71.4
Actor 67.5 67.9 67.7
Musician 68.1 48.9 56.9
Creator 70.6 84.5 76.9
Film Maker 52.9 68.7 59.7
Artist 72.8 85.5 78.6
Painter 74.4 86.1 79.8
Musician 68.9 81.6 74.7
Comunicator 76.4 83.1 79.6
Writer 78.6 76.6 77.6
Poet 67.4 61.2 64.1
Dramatist 65.0 70.7 67.7
Representative 84.8 76.7 80.6
Business man 47.2 40.5 43.6
Health professional 29.3 25.0 27.0
micro 69.6 70.7 70.1
macro 62.3 70.7 62.3
Table 3: Results for each category of the People
Ontology.
5.3 Confusion Matrix
Table 4 shows the confusion matrix for the People
Ontology task, in which the rows are ground truth
classes and the columns are predictions. The ex-
periment was conducted using 20 training exam-
ples per category and setting the parameter ? =
0. The matrix has been calculated for the finer-
grained categories and, then, grouped according to
their top-level concepts.
5.4 Precision/Recall Tradeoff
Figure 3 shows the precision/recall curve for the
People Ontology task obtained varying the param-
eter ? in Equation 2. The experiment was con-
ducted using 20 training examples per category.
5.5 Discussion
The results obtained are undoubtedly satisfactory.
Table 1 shows that our approach outperforms the
other three methods on the Tanev and Magnini
(2006) benchmark. Note that the Class-Example
approach has been trained on 1194 named enti-
269
Scientist Performer Creator Communicator Business Health
Phy Mat Che Bio Soc Act Mus Fil Pai Mus Poe Dra Rep man prof
Phy 68 40 25 3 11 2 0 0 3 1 7 1 7 2 3
Mat 3 27 1 0 0 0 0 1 0 0 4 0 2 1 1
Che 12 10 53 2 7 3 1 2 2 0 1 0 4 4 1
Bio 4 12 13 10 3 3 0 1 5 2 4 1 11 2 5
Soc 6 3 4 1 22 4 0 2 2 3 4 1 12 0 9
Act 3 1 2 0 0 106 6 20 0 3 2 4 7 1 1
Mus 1 1 2 0 0 16 64 5 2 28 2 2 7 0 1
Fil 0 0 0 0 0 7 0 46 0 4 1 1 4 3 1
Pai 2 1 0 0 1 1 1 2 93 3 1 0 2 1 0
Mus 1 0 0 0 0 1 16 2 3 142 1 3 2 1 2
Poe 1 2 1 0 1 2 3 3 6 12 93 20 6 1 1
Dra 0 2 1 0 0 3 0 2 2 3 9 65 1 2 2
Rep 0 6 7 0 3 6 1 0 3 2 5 0 189 1 0
Bus 3 3 6 0 0 0 1 1 0 1 2 0 6 17 2
Hea 4 0 5 0 3 3 1 0 4 2 2 2 10 0 12
Table 4: Confusion matrix for the finer-grained categories grouped according to their top-level concepts
of the People Ontology.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pre
cis
ion
Recall
terminal conceptsall concepts
Figure 3: Precision/recall curve on the People On-
tology.
ties, almost 60 examples per category, whereas we
achieved the same result with around 10 examples
per category. On the other hand, as Table 2 shows,
the IBOP algorithm is effective in populating a
more complex ontology and significantly outper-
forms the random and most frequent baselines.
An important characteristic of the algorithm is
the small number of examples required per cate-
gory. This affects both the prediction accuracy and
the computation time (this is generally a common
property of instance-based algorithms). Therefore,
finding an optimal tradeoff between the training
size and the performance is crucial. The learn-
ing curve (Figure 2) shows that the algorithm out-
performs the baselines with only 1 example per
category and achieves a good accuracy (F
1
?
67%) with only 10 examples per category, while
it reaches a plateau around 20 examples (F
1
?
70%), but leaving a little room for improvement.
Table 4 shows that misclassification errors are
largely distributed among categories belonging to
the same super-class (i.e., the blocks on the main
diagonal are more densely populated than others).
As expected, the algorithm is much more accurate
for the top-level concepts (i.e., Scientist, Commu-
nicator, etc.), where the category distinctions are
clearer, while a further fine-grained classification,
in some cases, is even difficult for human anno-
tators. In particular, results are higher for fine-
grained categories densely populated and with a
small number of sibling categories (i.e., Painter
and Musician). We have observed that the results
on sparse categories can be made more precise by
increasing the training size, generally at the ex-
pense of a lower recall.
We tried to maximize precision by varying the
parameter ? in Equation 2, that is, avoiding all
assignments where the plausibility score is lower
than a given threshold. Figure 3 shows that the
precision can be significantly enhanced (? 90%)
at the expense of poor recall (? 20%), while the
algorithm achieves 80% precision at around 50%
recall.
Finally, we performed some preliminary error
analysis, investigating the misclassifications in the
categories Scientists and Musicians. Several errors
are due to lack of information in WordNet, For ex-
ample, Leonhard Euler was a mathematician and
physicist, however, in WordNet, he is classified as
physicist, and our system classifies him as math-
ematician. On the other hand, for simplicity, the
algorithm returns a single category per instance,
270
however, the test set contains many entities that are
classified in more than one category. For instance,
Bertolt Brecht is both poet and dramatist and the
system classified him as dramatist. Another inter-
esting case is the presence of two categories Musi-
cian, one is subclass of Performer and the other of
Artist, in which, for instance, Ringo Starr is a per-
former while John Lennon is an artist, while the
system classified both as performers.
6 Related work
Brin (1998) defined a methodology to extract in-
formation from the Web starting from a small set
of seed examples, then alternately learning extrac-
tion patterns from seeds, and further seeds from
patterns. Despite the fact that the evaluation was
on relation extraction the method is general and
might be applied to entity extraction and catego-
rization. The approach was further extended by
Agichtein and Gravano (2000). Our approach dif-
fers from theirs in that we do not learn patterns.
Thus, we do not require ad hoc strategies for gen-
erating patterns and estimating their reliability, a
crucial issue in these approaches as ?bad? patterns
may extract wrong seeds instances that in turn may
generate even more inaccurate patterns in the fol-
lowing iteration.
Fleischman and Hovy (2002) approached the
ontology population problem as a supervised clas-
sification task. They compare different machine
learning algorithms, providing instances in their
context as training examples as well as more global
semantic information derived from topic signature
and WordNet.
Alfonseca and Manandhar (2002) and Cimiano
and V?olker (2005) present similar approaches re-
lying on the Harris? distributional hypothesis and
the vector-space model. They assign a particu-
lar instance represented by a certain context vec-
tor to the concept corresponding to the most simi-
lar vector. Contexts are represented using lexical-
syntactic features.
KnowItAll (Etzioni et al, 2005) uses a search
engine and semantic patterns (similar to those de-
fined by Hearst (1992)) to classify named entities
on the Web. The approach uses simple techniques
from the ontology learning field to perform extrac-
tion and then annotation. It also is able to perform
very simple pattern induction, consisting of look-
ing at n words before and n words after the occur-
rence of an example in the document. With pat-
tern learning, KnowItAll becomes a bootstrapped
learning system, where rules are used to learn new
seeds, which in turn are used to learn new rules.
A similar approach is used in C-PANKOW (Cimi-
ano et al, 2005). Compared to KnowItAll and
C-PANKOW, our approach does not need hand-
crafted patterns as input. They are implicitly found
by substituting the training instances in the con-
texts of the input entities. Another key difference
is that concepts in the ontology do not need to be
lexicalized.
Tanev and Magnini (2006) proposed a weakly-
supervised method that requires as training data a
list of terms without context for each category un-
der consideration. Given a generic syntactically
parsed corpus containing at least each training en-
tity twice, the algorithm learns, for each category, a
feature vector describing the contexts where those
entities occur. Then, it compares the new (un-
known) entity with the so obtained feature vec-
tors, assigning it to the most similar category. Even
though we used a significantly smaller number of
training instances, we obtained better results on
their benchmark.
More recently, Giuliano and Gliozzo (2007)
proposed an unsupervised approach based on lexi-
cal entailment, consisting in assigning an entity to
the category whose lexicalization can be replaced
with its occurrences in a corpus preserving the
meaning. A disadvantage is that the concepts in the
ontology have to be lexicalized, as they are used
as training examples. Our approach is based on a
similar idea, but with the main difference that an
instance is substituted with other instances rather
than with their category names. Considering that,
in most of the cases, ontologies are partially popu-
lated during the development phase, and hence the
annotation cost is marginal, our approach is a re-
alistic alternative for practical ontology population
problems.
7 Conclusions and Future Work
We have described an instance-based algorithm
for automatic fine-grained categorization of named
entities, previously identified by an entity recogni-
tion system or already present in a database. This
method is meant to provide an effective solution to
the ontology population problem. It exploits the
Web or a domain corpus to collect evidence of the
new instances and does not require labeled texts
for supervision, but a partially populated ontology.
271
The experimental results show that, where a com-
parison is possible, our method outperforms previ-
ous methods and it can be applied to different do-
mains and languages to (semi-) automatically en-
rich an existing ontology.
Future work will address the definition of a hi-
erarchical categorization strategy where instances
are classified in a top-down manner, in order to ef-
ficiently populate very large ontologies, since we
plan to apply this method to extract structured in-
formation from Wikipedia. Furthermore, we will
investigate how co-reference resolution might well
benefit from our ontology classification. Finally,
we plan to exploit the IBOP algorithm for ontol-
ogy mapping and multilingual alignment of lexical
resources.
Acknowledgments
Claudio Giuliano is supported by the X-Media
project (http://www.x-media-project.
org), sponsored by the European Commission as
part of the Information Society Technologies (IST)
program under EC grant number IST-FP6-026978.
References
Agichtein, Eugene and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In DL ?00: Proceedings of the fifth ACM conference
on Digital libraries, pages 85?94, New York, NY,
USA. ACM.
Alfonseca, Enrique and Suresh Manandhar. 2002. Ex-
tending a lexical ontology by a combination of dis-
tributional semantics signatures. In EKAW ?02: Pro-
ceedings of the 13th International Conference on
Knowledge Engineering and Knowledge Manage-
ment. Ontologies and the Semantic Web, pages 1?7,
London, UK. Springer-Verlag.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, EDBT?98.
Cimiano, Philipp and Johanna V?olker. 2005. Towards
large-scale, open-domain and ontology-based named
entity classification. In Proceedings of RANLP?05,
pages 66? 166?172, Borovets, Bulgaria.
Cimiano, Philipp, G?unter Ladwig, and Steffen Staab.
2005. Gimme the context: Context-driven automatic
semantic annotation with C-PANKOW. In Ellis, Al-
lan and Tatsuya Hagino, editors, Proceedings of the
14th World Wide Web Conference, pages 332 ? 341,
Chiba, Japan, MAY. ACM Press.
Etzioni, Oren, Michael Cafarella, Doug Downey,
Ana M. Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):191?134.
Fellbaum, Christiane. 1998. WordNet. An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Fleischman, Michael and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, Taipei, Taiwan.
Gangemi, Aldo, Roberto Navigli, and Paola Velardi.
2003. Axiomatizing WordNet glosses in the On-
toWordNet project. In Proocedings of the Workshop
on Human Language Technology for the Semantic
Web and Web Services at ISWC 2003, Sanibel Island,
Florida.
Giuliano, Claudio and Alfio Gliozzo. 2007. In-
stance based lexical entailment for ontology popu-
lation. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 248?256.
Giuliano, Claudio, Alfio Gliozzo, and Carlo Strappar-
ava. 2007. Fbk-irst: Lexical substitution task ex-
ploiting domain and syntagmatic coherence. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 145?
148, Prague, Czech Republic, June.
Harris, Zellig. 1954. Distributional structure. WORD,
10:146?162.
Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Compu-
tational Linguistics, Nantes, France, July.
McCarthy, Diana and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 48?
53, Prague, Czech Republic, June.
McCarthy, Diana. 2002. Lexical substitution as a task
for WSD evaluation. In Proceedings of the ACL-
02 workshop on Word Sense Disambiguation, pages
109?115, Morristown, NJ, USA.
Melamed, I. Dan and Philip Resnik. 2000. Tagger eval-
uation given hierarchical tag sets. Computers and
the Humanities, pages 79?84.
Tanev, Hristo and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), Trento, Italy.
272
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 610?614,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Lexical Substitution for the Medical Domain
Martin Riedl
1
Michael R. Glass
2
Alfio Gliozzo
2
(1) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
(2) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
riedl@cs.tu-darmstadt.de, {mrglass,gliozzo}@us.ibm.com
Abstract
In this paper we examine the lexical substitu-
tion task for the medical domain. We adapt
the current best system from the open domain,
which trains a single classifier for all instances
using delexicalized features. We show sig-
nificant improvements over a strong baseline
coming from a distributional thesaurus (DT).
Whereas in the open domain system, features
derived from WordNet show only slight im-
provements, we show that its counterpart for
the medical domain (UMLS) shows a signif-
icant additional benefit when used for feature
generation.
1 Introduction
The task of lexical substitution (McCarthy and Navigli,
2009) deals with the substitution of a target term within
a sentence with words having the same meaning. Thus,
the task divides into two subtasks:
? Identification of substitution candidates, i.e.
terms that are, for some contexts, substitutable for
a given target term.
? Ranking the substitution candidates according to
their context
Such a substitution system can help for semantic text
similarity (B?ar et al., 2012), textual entailment (Dagan
et al., 2013) or plagiarism detection (Chong and Specia,
2011).
Datasets provided by McCarthy and Navigli (2009)
and Biemann (2012) offer manually annotated substi-
tutes for a given set of target words within a context
(sentence). Contrary to these two datasets in Kremer et
al. (2014) a dataset is offered where all words have are
annotated with substitutes. All the datasets are suited
for the open domain.
But a system performing lexical substitution is not
only of interest for the open domain, but also for the
medical domain. Such a system could then be applied
to medical word sense disambiguation, entailment or
question answering tasks. Here we introduce a new
dataset and adapt the lexical substitution system, pro-
vided by Szarvas et al. (2013), to the medical domain.
Additionally, we do not make use of WordNet (Miller,
1995) to provide similar terms, but rather employ a Dis-
tributional Thesaurus (DT), computed on medical texts.
2 Related Work
For the general domain, the lexical substitution task
was initiated by a Semeval-2007 Task (McCarthy and
Navigli, 2009). This task was won by an unsupervised
method (Giuliano et al., 2007), which uses WordNet for
the substitution candidate generation and then relies on
the Google Web1T n-grams (Brants and Franz, 2006)
1
to rank the substitutes.
The currently best system, to our knowledge, is pro-
posed by Szarvas et al. (2013). This is a supervised ap-
proach, where a single classifier is trained using delex-
icalized features for all substitutes and can thus be ap-
plied even to previously unseen substitutes. Although
there have been many approaches for solving the task
for the general domain, only slight effort has been done
in adapting it to different domains.
3 Method
To perform lexical substitution, we follow the delex-
icalization framework of Szarvas et al. (2013). We
automatically build Distributional Thesauri (DTs) for
the medical domain and use features from the Uni-
fied Medical Language System (UMLS) ontology. The
dataset for supervised lexical substitution consists of
sentences, containing an annotated target word t. Con-
sidering the sentence being the context for the target
word, the target word might have different meanings.
Thus annotated substitute candidates s
g
1
. . . s
g
n
? s
g
,
need to be provided for each context. The negative ex-
amples are substitute candidates that either are incor-
rect for the target word, do not fit into the context or
both. We will refer to these substitutes as false substi-
tute candidates s
f
1
. . . s
f
m
? s
f
with s
f
? s
g
= ?.
For the generation of substitute candidates we do not
use WordNet, as done in previous works (Szarvas et al.,
2013), but use only substitutes from a DT. To train a
single classifier, features that distinguishing the mean-
ing of words in different context need to be considered.
Such features could be e.g. n-grams, features from dis-
tributional semantics or features which are extracted
1
http://catalog.ldc.upenn.edu/
LDC2006T13
610
relative to the target word, such as the ratio between
frequencies of the substitute candidate and the target
word. After training, we apply the algorithm to un-
seen substitute candidates and rank them according to
their positive probabilities, given by the classifier. Con-
trary to Szarvas et al. (2013), we do not use any weight-
ing in the training if a substitute has been supplied by
many annotators, as we could not observe any improve-
ments. Additionally, we use logistic regression (Fan et
al., 2008) as classifier
2
.
4 Resources
For the substitutes and for the generation of delexical-
ized features, we rely on DTs, the UMLS and Google
Web1T.
4.1 Distributional thesauri (DTs)
We computed two different DTs using the framework
proposed in Biemann and Riedl (2013)
3
.
The first DT is computed based on Medline
4
ab-
stracts. This thesaurus uses the left and the right word
as context features. To include multi-word expressions,
we allow the number of tokens that form a term to be
up to the length of three.
The second DT is based on dependencies as context
features from a English Slot Grammar (ESG) parser
(McCord et al., 2012) modified to handle medical data.
The ESG parser is also capable of finding multi-word
expressions. As input data we use 3.3 GB of texts
from medical textbooks, encyclopedias and clinical ref-
erence material as well as selected journals. This DT is
also used for the generation of candidates supplied to
annotators when creating the gold standard and there-
fore is the main resource to provide substitute candi-
dates.
4.2 UMLS
The Unified Medical Language System (UMLS) is an
ontology for the medical domain. In contrast to Szarvas
et al. (2013), which uses WordNet (Miller, 1995) to
generate substitute candidates and also for generating
features, we use UMLS solely for feature generation.
4.3 Google Web1T
We use the Google Web1T to generate n-gram features
as we expect this open domain resource to have consid-
erable coverage for most specific domains as well. For
accessing the resource, we use JWeb1T
5
(Giuliano et
al., 2007).
2
We use a Java port of LIBLINEAR (http://www.
csie.ntu.edu.tw/
?
cjlin/liblinear/) available
from http://liblinear.bwaldvogel.de/
3
We use Lexicographer?s Mutual Information (LMI) (Ev-
ert, 2005) as significance measure and consider only the top
1000 (p = 1000) features per term.
4
http://www.nlm.nih.gov/bsd/licensee/
2014_stats/baseline_med_filecount.html
5
https://code.google.com/p/jweb1t/
5 Lexical Substitution dataset
Besides the lexical substitution data sets for the open
domain (McCarthy and Navigli, 2009; Biemann, 2012;
Kremer et al., 2014) there is no dataset available that
can be used for the medical domain. Therefore, we
constructed an annotation task for the medical domain
using a medical corpus and domain experts.
In order to provide the annotators with a clear task,
we presented a question, and a passage that contains
the correct answer to the question. We restricted this to
a subset of passages that were previously annotated as
justifying the answer to the question. This is related to
a textual entailment task, essentially the passage entails
the question with the answer substituted for the focus of
the question. We instructed the annotators to first iden-
tify the terms that were relevant for the entailment rela-
tion. For each relevant term we randomly extracted 10
terms from the ESG-based DT within the top 100 most
similar terms. Using this list of distributionally similar
terms, the annotators selected those terms that would
preserve the entailment relation if substituted. This re-
sulted in a dataset of 699 target terms with substitutes.
On average from the 10 terms 0.846 are annotated as
correct substitutes. Thus, the remaining terms can be
used as false substitute candidates.
The agreement on this task by Fleiss Kappa was
0.551 indicating ?moderate agreement? (Landis and
Koch, 1977). On the metric of pairwise agreement,
as defined in the SemEval lexical substitution task, we
achieve 0.627. This number is not directly comparable
to the pairwise agreement score of 0.277 for the Se-
mEval lexical substitution task (McCarthy and Navigli,
2009) since in our task the candidates are given. How-
ever, it shows promise that subjectivity may be reduced
by casting lexical substitution into a task of maintain-
ing entailment.
6 Evaluation
For the evaluation we use a ten-fold cross validation
and report P@1 (also called Average Precision (AP) at
1) and Mean Average Precision (MAP) (Buckley and
Voorhees, 2004) scores. The P@1 score indicates how
often the first substitute of the system matches the gold
standard. The MAP score is the mean of all AP from 1
to the number of all substitutes.
? Google Web 1T:
We use the same Google n-gram features, as
used in Giuliano et al. (2007) and Szarvas et al.
(2013). These are frequencies of n-grams formed
by the substitute candidate s
i
and the left and right
words, taken from the context sentence, normal-
ized by the frequency of the same context n-gram
with the target term t. Additionally, we add the
same features, normalized by the frequency sum
of all n-grams of the substitute candidates. An-
other feature is generated using the frequencies
where t and s are listed together using the words
611
and, or and ?,? as separator and also add the left
and right words of that phrase as context. Then we
normalize this frequency by the frequency of the
context occurring only with t.
? DT features:
To characterize if t and s
i
have similar words
in common, and therefore are similar, we com-
pute the percentage of words their thesauri en-
tries share, considering the top n words in each
entry with n = 1, 5, 20, 50, 100, 200. During
the DT calculation we also calculate the signif-
icances between each word and its context fea-
tures (see Section 4.1). Using this information,
we compute if the words in the sentences also
occur as context features for the substitute can-
didate. A third feature group relying on DTs
is created by the overlapping context features
for the top m entries of t and s
i
with m =
1, 5, 20, 50, 100, 1000, which are ranked regard-
ing their significance score. Whereas, the simi-
larities between the trigram-based and the ESG-
based DT are similar, the context features are dif-
ferent. Both feature types can be applied to the
two DTs. Additionally, we extract the thesaurus
entry for the target word t and generate a feature
indicating whether the substitute s
i
is within the
top k entries with k = 1, 5, 10, 20, 100 entries
6
.
? Part-of-speech n-grams:
To identify the context of the word we use the
POS-tag (only the first letter) of s
i
and t as feature
and POS-tag combinations of up to three neigh-
boring words.
? UMLS:
Considering UMLS we look up all concept unique
identifiers (CUIs) for s
i
and t. The first two fea-
tures are the number of CUIs for s
i
and t. The next
features compute the number of CUIs that s
i
and t
share, starting from the minimal to the maximum
number of CUIs. Additionally, we use a feature
indicating that s
i
and t do not share any CUI.
6.1 Substitute candidates
The candidates for the substitution are taken from the
ESG based DT. For each target term we use the gold
substitute candidates as correct instances and add all
possible substitutes for the same target term occurring
in a different context and do not have been annotated
as valid in the present context as false instances.
7 Results
Running the experiment, we get the results as shown
in Table 1. As baseline system we use the ranking of
6
Whereas in Szarvas et al. (2013) only k = 100 is used,
we gained an improvement in performance when also adding
smaller values of k.
the ESG-based DT. As can be seen, the baseline is al-
ready quite high, which can be attributed to the fact
that this resource was used to generate substitutes und
thus contains all positive instances. Using the super-
vised approach, we can beat the baseline by 0.10 for
the MAP score and by 0.176 for the P@1 score, which
is a significant improvement (p < 0.0001, using a two
tailed permutation test). To get insights of the contri-
System MAP P@1
Baseline 0.6408 0.5365
ALL 0.7048 0.6366
w/o DT 0.5798 0.4835
w/o UMLS 0.6618 0.5651
w/o Ngrams 0.7009 0.6252
w/o POS 0.7027 0.6323
Table 1: Results for the evaluation using substitute can-
didates from the DT.
bution of individual feature types, we perform an abla-
tion test. We observe that the most prominent features
are coming from the two DTs as we only achieve re-
sults below the baseline, when removing DT features.
We still obtain significant improvements over the base-
line when removing other feature groups. The second
most important feature comes from the UMLS. Fea-
tures coming from the Google n-grams improve the
system only slightly. The lowest improvement is de-
rived from the part-of-speech features. This leads us
to summarize that a hybrid approach for feature gen-
eration using manually created resources (UMLS) and
unsupervised features (DTs) leads to the best result for
lexical substitution for the medical domain.
8 Analysis
For a better insight into the lexical substitution we ana-
lyzed how often we outperform the baseline, get equal
results or get decreased scores. According to Table 2 in
performance # of instances Avg. ? MAP
decline 180 -0.16
equal 244 0
improvements 275 0.26
Table 2: Error analysis for the task respectively to the
MAP score.
around 26% of the cases we observe a decreased MAP
score, which is on average 0.16 smaller then the scores
achieved with the baseline. On the other hand, we see
improvements in around 39% of the cases: an average
improvements of 0.26, which is much higher then the
loss. For the remaining 25% of cases we observe the
same score.
Looking inside the data, the largest error class is
caused by antonyms. A sub-class of this error are
multi-word expressions having an adjective modifier.
This problems might be solved by additional features
using the UMLS resource. An example is shown in
Figure 1.
612
Figure 1: Example sentence for the target term mild
thrombocytopenia. The system returns a wrong rank-
ing, as the adjective changes the meaning and turns the
first ranked term into an antonym.
For feature generation, we currently lookup multi-
word expressions as one term, both in the DT and the
UMLS resource and do not split them into their sin-
gle tokens. This error also suggests considering the
single words inside the multi-word expression, espe-
cially adjectives, and looking them up in a resource
(e.g. UMLS) to detect synonymy and antonymy.
Figure 2 shows the case, where the ranking is per-
formed correctly, but the precise substitute is not an-
notated as a correct one. The term nail plate might be
even more precise in the context as the manual anno-
tated term nail bed. Due to the missing annotation the
Figure 2: Example sentence for the target term nails.
Here the ranking from the system is correct, but the first
substitute from the system was not annotated as such.
baseline gets better scores then the result from the sys-
tem.
9 Conclusion
In summary, we have examined the lexical substitution
task for the medical domain and could show that a sys-
tem for open domain text data can be applied to the
medical domain. We can show that following a hybrid
approach using features from UMLS and distributional
semantics leads to the best results. In future work, we
will work on integrating DTs using other context fea-
tures, as we could see an impact of using two different
DTs. Furthermore, we want to incorporate features us-
ing n-grams computed on a corpus from the domain
and include co-occurrence features.
Acknowledgments
We thank Adam Lally, Eric Brown, Edward A. Epstein,
Chris Biemann and Faisal Chowdhury for their helpful
comments.
References
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proceedings of the 6th In-
ternational Workshop on Semantic Evaluation, held
in conjunction with the 1st Joint Conference on Lex-
ical and Computational Semantics, pages 435?440,
Montreal, Canada.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! A Framework for Lexical Expansion with Con-
textual Similarity. Journal of Language Modelling,
1(1):55?95.
Chris Biemann. 2012. Turk bootstrap word sense in-
ventory 2.0: A large-scale resource for lexical sub-
stitution. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram corpus version 1. Technical report, Google Re-
search.
Chris Buckley and Ellen M. Voorhees. 2004. Re-
trieval evaluation with incomplete information. In
Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ?04, pages 25?32,
Sheffield, United Kingdom.
Miranda Chong and Lucia Specia. 2011. Lexical gen-
eralisation for word-level matching in plagiarism de-
tection. In Recent Advances in Natural Language
Processing, pages 704?709, Hissar, Bulgaria.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio M.
Zanzotto. 2013. Recognizing Textual Entailment:
Models and Applications. Synthesis Lectures on Hu-
man Language Technologies, 6(4):1?220.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Institut f?ur maschinelle Sprachverarbeitung, Univer-
sity of Stuttgart.
613
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strappar-
ava. 2007. Fbk-irst: Lexical substitution task ex-
ploiting domain and syntagmatic coherence. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 145?148,
Prague, Czech Republic.
Gerhard Kremer, Katrin Erk, Sebastian Pad?o, and Ste-
fan Thater. 2014. What Substitutes Tell Us - Anal-
ysis of an ?All-Words? Lexical Substitution Corpus.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2014), pages 540?549, Gothen-
burg, Sweden.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Michael C. McCord, J. William Murdock, and Bran-
imir K. Boguraev. 2012. Deep Parsing in Watson.
IBM J. Res. Dev., 56(3):264?278.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised All-Words Lexical Substitution
using Delexicalized Features. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2013),
pages 1131?1141, Atlanta, GA, USA.
614
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1522?1531,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Word Semantic Representations using Bayesian Probabilistic Tensor
Factorization
Jingwei Zhang and Jeremy Salwen
Columbia University
Computer Science
New York, NY 10027, USA
{jz2541,jas2312}@columbia.edu
Michael Glass and Alfio Gliozzo
IBM T.J. Waston Research
Yorktown Heights, NY 10598, USA
{mrglass,gliozzo}@us.ibm.com
Abstract
Many forms of word relatedness have been
developed, providing different perspec-
tives on word similarity. We introduce
a Bayesian probabilistic tensor factoriza-
tion model for synthesizing a single word
vector representation and per-perspective
linear transformations from any number
of word similarity matrices. The result-
ing word vectors, when combined with the
per-perspective linear transformation, ap-
proximately recreate while also regulariz-
ing and generalizing, each word similarity
perspective.
Our method can combine manually cre-
ated semantic resources with neural word
embeddings to separate synonyms and
antonyms, and is capable of generaliz-
ing to words outside the vocabulary of
any particular perspective. We evaluated
the word embeddings with GRE antonym
questions, the result achieves the state-of-
the-art performance.
1 Introduction
In recent years, vector space models (VSMs)
have been proved successful in solving various
NLP tasks including named entity recognition,
part-of-speech tagging, parsing, semantic role-
labeling and answering synonym or analogy ques-
tions (Turney et al., 2010; Collobert et al., 2011).
Also, VSMs are reported performing well on
tasks involving the measurement of word related-
ness (Turney et al., 2010). Many existing works
are distributional models, based on the Distribu-
tional Hypothesis, that words occurring in simi-
lar contexts tend to have similar meanings (Har-
ris, 1954). The limitation is that word vectors de-
veloped from distributional models cannot reveal
word relatedness if its information does not lie in
word distributions. For instance, they are believed
to have difficulty distinguishing antonyms from
synonyms, because the distribution of antonymous
words are close, since the context of antonymous
words are always similar to each other (Moham-
mad et al., 2013). Although some research claims
that in certain conditions there do exist differ-
ences between the contexts of different antony-
mous words (Scheible et al., 2013), the differences
are subtle enough that it can hardly be detected by
such language models, especially for rare words.
Another important class of lexical resource for
word relatedness is a lexicon, such as Word-
Net (Miller, 1995) or Roget?s Thesaurus (Kipfer,
2009). Manually producing or extending lexi-
cons is much more labor intensive than generat-
ing VSM word vectors using a corpus. Thus, lex-
icons are sparse with missing words and multi-
word terms as well as missing relationships be-
tween words. Considering the synonym / antonym
perspective as an example, WordNet answers less
than 40% percent of the the GRE antonym ques-
tions provided by Mohammad et al. (2008) di-
rectly. Moreover, binary entries in lexicons do not
indicate the degree of relatedness, such as the de-
gree of lexical contrast between happy and sad or
happy and depressed. The lack of such informa-
tion makes it less fruitful when adopted in NLP
applications.
In this work, we propose a Bayesian tensor fac-
torization model (BPTF) for synthesizing a com-
posite word vector representation by combining
multiple different sources of word relatedness.
The input is a set of word by word matrices, which
may be sparse, providing a number indicating the
presence or degree of relatedness. We treat word
relatedness matrices from different perspectives as
slices, forming a word relatedness tensor. Then the
composite word vectors can be efficiently obtained
by performing BPTF. Furthermore, given any two
words and any trained relatedness perspective, we
1522
can create or recreate the pair-wise word related-
ness with regularization via per-perspective linear
transformation.
This method allows one set of word vectors to
represent word relatednesses from many different
perspectives (e.g. LSA for topic relatedness / cor-
pus occurrences, ISA relation and YAGO type) It
is able to bring the advantages from both word re-
latedness calculated by distributional models, and
manually created lexicons, since the former have
much more vocabulary coverage and many varia-
tions, while the latter covers word relatedness that
is hard to detect by distributional models. We can
use information from distributional perspectives to
create (if does not exist) or re-create (with regular-
ization) word relatedness from the lexicon?s per-
spective.
We evaluate our model on distinguishing syn-
onyms and antonyms. There are a number of re-
lated works (Lin and Zhao, 2003; Turney, 2008;
Mohammad et al., 2008; Mohammad et al., 2013;
Yih et al., 2012; Chang et al., 2013). A number of
sophisticated methods have been applied, produc-
ing competitive results using diverse approaches.
We use the GRE antonym questions (Mohammad
et al., 2008) as a benchmark, and answer these
questions by finding the most contrasting choice
according to the created or recreated synonym /
antonym word relatedness. The result achieves
state-of-the-art performance.
The rest of this paper is organized as fol-
lows. Section 2 describes the related work of
word vector representations, the BPTF model and
antonymy detection. Section 3 presents our BPTF
model and the sampling method. Section 4 shows
the experimental evaluation and results with Sec-
tion 5 providing conclusion and future work.
2 Related Work
2.1 Word Vector Representations
Vector space models of semantics have a long his-
tory as part of NLP technologies. One widely-
used method is deriving word vectors using la-
tent semantic analysis (LSA) (Deerwester et al.,
1990), for measuring word similarities. This pro-
vides a topic based perspective on word simi-
larity. In recent years, neural word embeddings
have proved very effective in improving various
NLP tasks (e.g. part-of-speech tagging, chunking,
named entity recognition and semantic role label-
ing) (Collobert et al., 2011). The proposed neural
models have a large number of variations, such as
feed-forward networks (Bengio et al., 2003), hi-
erarchical models (Mnih and Hinton, 2008), re-
current neural networks (Mikolov, 2012), and re-
cursive neural networks (Socher et al., 2011).
Mikolov et al. (2013) reported their vector-space
word representation is able to reveal linguistic
regularities and composite semantics using sim-
ple vector addition and subtraction. For example,
?King?Man+Woman? results in a vector very
close to ?Queen?. Luong et al. (2013) proposed
a recursive neural networks model incorporating
morphological structure, and has better perfor-
mance for rare words.
Some non-VSM models
1
also generate word
vector representations. Yih et al. (2012) apply po-
larity inducing latent semantic analysis (PILSA)
to a thesaurus to derive the embedding of words.
They treat each entry of a thesaurus as a docu-
ment giving synonyms positive term counts, and
antonyms negative term counts, and preform LSA
on the signed TF-IDF matrix In this way, syn-
onyms will have cosine similarities close to one
and antonyms close to minus one.
Chang et al. (2013) further introduced Multi-
Relational LSA (MRLSA), as as extension of
LSA, that performs Tucker decomposition over a
three-way tensor consisting of multiple relations
(document-term like matrix) between words as
slices, to capture lexical semantics. The purposes
of MRLSA and our model are similar, but the dif-
ferent factorization techniques offer different ad-
vantages. In MRLSA, the k-th slice of tensor W
is approximated by
W
:,:,k
? X
:,:,k
= US
:,:,k
V
T
,
where U and V are both for the same word list
but are not guaranteed (or necessarily desired) to
be the same. Thus, this model has the ability to
capture asymmetric relations, but this flexibility is
a detriment for symmetric relatedness. In order to
expand word relatedness coverage, MRLSA needs
to choose a pivot slice (e.g. the synonym slice),
thus there always must existence such a slice, and
the model performance depends on the quality of
this pivot slice. Also, while non-completeness is
a pervasive issue in manually created lexicons,
MRLSA is not flexible enough to treat the un-
known entries as missing. Instead it just sets them
1
As defined by Turney et al. (2010), VSM must be derived
from event frequencies.
1523
to zero at the beginning and uses the pivot slice
to re-calculate them. In contrast, our method of
BPTF is well suited to symmetric relations with
many unknown relatedness entries.
2.2 BPTF Model
Salakhutdinov and Mnih (2008) introduced a
Bayesian Probabilistic Matrix Factorization
(BPMF) model as a collaborative filtering algo-
rithm. Xiong et al. (2010) proposed a Bayesian
Probabilistic Tensor Factorization (BPTF) model
which further extended the original model to
incorporate temporal factors. They modeled latent
feature vector for users and items, both can be
trained efficiently using Markov chain Monte
Carlo methods, and they obtained competitive
results when applying their models on real-world
recommendation data sets.
2.3 Antonomy Detection
There are a number of previous works in detect-
ing antonymy. Lin and Zhao (2003) identifies
antonyms by looking for pre-identified phrases in
corpus datasets. Turney (2008) proposed a su-
pervised classification method for handling analo-
gies, then apply it to antonyms by transforming
antonym pairs into analogy relations. Mohammad
et al. (Mohammad et al., 2008; Mohammad et
al., 2013) proposed empirical approaches consid-
ering corpus co-occurrence statistics and the struc-
ture of a published thesaurus. Based on the as-
sumption that the strongly related words of two
words in a contrasting pair are also often antony-
mous, they use affix patterns (e.g. ?un-?, ?in-? and
?im-?) and a thesaurus as seed sets to add con-
trast links between word categories. Their best
performance is achieved by further manually an-
notating contrasting adjacent categories. This ap-
proach relies on the Contrast Hypothesis, which
will increase false positives even with a carefully
designed methodology. Furthermore, while this
approach can expand contrast relationships in a
lexicon, out-of-vocabulary words still pose a sub-
stancial challenge.
Yih et al. (2012) and Chang et al. (2013) also
applied their vectors on antonymy detection, and
Yih et al. achieves the state-of-the-art performance
in answering GRE antonym questions. In addition
to the word vectors generated from PILSA, they
use morphology and k-nearest neighbors from dis-
tributional word vector spaces to derive the em-
beddings for out-of-vocabulary words. The latter
is problematic since both synonyms and antonyms
are distributionally similar. Their approach is two
stage: polarity inducing LSA from a manually
created thesaurus, then falling back to morphol-
ogy and distributional similarity when the lexicon
lacks coverage. In contrast, we focus on fusing
the information from thesauruses and automati-
cally induced word relatedness measures during
the word vector space creation. Then prediction
is done in a single stage, from the latent vectors
capturing all word relatedness perspectives and the
appropriate per-perspective transformation vector.
3 Methods
3.1 The Bayesian Probabilistic Tensor
Factorization Model
Our model is a variation of the BPMF model
(Salakhutdinov and Mnih, 2008), and is similar
to the temporal BPTF model (Xiong et al., 2010).
To model word relatedness from multiple perspec-
tives, we denote the relatedness between word i
and word j from perspective k as R
k
ij
. Then we
can organize these similarities to form a three-way
tensor R ? R
N?N?K
.
Table 1 shows an example, the first slice of the
tensor is a N ? N matrix consists of 1/-1 corre-
sponding to the synonym/antonym entries in the
Roget?s thesaurus, and the second slice is aN?N
matrix consists of the cosine similarity from neural
word embeddings created by Luong et al. (2013),
where N is the number of words in the vocabu-
lary. Note that in our model the entries missing
in Table 1a do not necessarily need to be treated
as zero. Here we use the indicator variable I
k
ij
to denote if the entry R
k
ij
exists (I
k
ij
= 1) or not
(I
k
ij
= 0). If K = 1, the BPTF model becomes to
BPMF. Hence the key difference between BPTF
and BPMF is that the former combines multi-
ple complementary word relatedness perspectives,
while the later only smooths and generalizes over
one.
We assume the relatedness R
k
ij
to be Gaussian,
and can be expressed as the inner-product of three
D-dimensional latent vectors:
R
k
ij
|V
i
, V
j
, P
k
? N (< V
i
, V
j
, P
k
>,?
?1
),
where< ?, ?, ? > is a generalization of dot product:
< V
i
, V
j
, P
k
>?
D
?
d=1
V
(d)
i
V
(d)
j
P
(d)
k
,
1524
happy joyful lucky sad depressed
happy 1 1 -1 -1
joyful 1 -1
lucky 1 -1
sad -1 -1 -1 1
depressed -1 1
(a) The first slice: synonym & antonym relatedness
happy joyful lucky sad depressed
happy .03 .61 .65 .13
joyful .03 .25 .18 .23
lucky .61 .25 .56 .31
sad .65 .18 .56 -.01
depressed .13 .23 .31 -.01
(b) The second slice: distributional similarity
Table 1: Word Relatedness Tensor
and ? is the precision, the reciprocal of the vari-
ance. V
i
and V
j
are the latent vectors of word i and
word j, and P
k
is the latent vector for perspective
k.
We follow a Bayesian approach, adding Gaus-
sian priors to the variables:
V
i
? N (?
V
,?
?1
V
),
P
i
? N (?
P
,?
?1
P
),
where ?
V
and ?
P
are D dimensional vectors and
?
V
and ?
P
are D-by-D precision matrices.
Furthermore, we model the prior distribution of
hyper-parameters as conjugate priors (following
the model by (Xiong et al., 2010)):
p(?) =W(?|
?
W
0
, ?
0
),
p(?
V
,?
V
) = N (?
V
|?
0
, (?
0
?
V
)
?1
)W(?
V
|W
0
, ?
0
),
p(?
P
,?
P
) = N (?
P
|?
0
, (?
0
?
P
)
?1
)W(?
P
|W
0
, ?
0
),
where W(W
0
, ?
0
) is the Wishart distribution of
degree of freedom ? and a D-by-D scale matrix
W , and
?
W
0
is a 1-by-1 scale matrix for ?. The
graphical model is shown in Figure 1 (with ?
0
set
to 1). After choosing the hyper-priors, the only re-
maining parameter to tune is the dimension of the
latent vectors.
Due to the existence of prior distributions, our
model can capture the correlation between dif-
ferent perspectives during the factorization stage,
then create or re-create word relatedness using this
correlation for regularization and generalization.
This advantage is especially useful when such cor-
relation is too subtle to be captured by other meth-
ods. On the other hand, if perspectives (let?s say k
and l) are actually unrelated, our model can handle
it as well by making P
k
and P
l
orthogonal to each
other.
3.2 Inference
To avoid calculating intractable distributions, we
use a numerical method to approximate the re-
sults. Here we use the Gibbs sampling algorithm
R
k
ij
P
k
?
P
?
P
?
0
W
0
, ?
0
?
V
i
V
j
?
V
?
V
W
0
, ?
0
?
0
? ? ? ? ? ?? ? ?
k = 1, ..., K
I
k
i,j
= 1
i 6= j
i, j = 1, ..., N
Figure 1: The graphical model for BPTF.
to perform the Markov chain Monte Carlo method.
When sampling a block of parameters, all other
parameters are fixed, and this procedure is re-
peated many times until convergence. The sam-
pling algorithm is shown in Algorithm 1.
With conjugate priors, and assuming I
k
i,i
=
0, ?i, k (we do not consider a word?s relatedness
to itself), the posterior distributions for each block
of parameters are:
p(?|R,V,P) =W(
?
W
0
?
, ??
0
?
) (1)
Where:
??
?
0
= ??
0
+
2?
k=1
N?
i,j=1
I
k
ij
,
(
?
W
?
0
)
?1
=
?
W
?1
0
+
2?
k=1
N?
i,j=1
I
k
ij
(R
k
ij
? < V
i
, V
j
, P
k
>)
2
1525
p(?
V
,?
V
|V) = N (?
V
|?
?
0
, (?
?
0
?
V
)
?1
)W(?
V
|W
?
0
, ?
?
0
)
(2)
Where:
?
?
0
=
?
0
?
0
+N
?
V
?
0
+N
, ?
?
0
= ?
0
+N, ?
?
0
= ?
0
+N,
(W
?
0
)
?1
= W
?1
0
+N
?
S +
?
0
N
?
0
+N
(?
0
?
?
V )(?
0
?
?
V )
T
,
?
V =
1
N
N?
i=1
V
i
,
?
S =
1
N
N?
i=1
(V
i
?
?
V )(V
i
?
?
V )
T
p(?
P
,?
P
|P) = N (?
P
|?
?
0
, (?
?
0
?
P
)
?1
)W(?
P
|W
?
0
, ?
?
0
)
(3)
Which has the same form as p(?
V
,?
V
|V).
p(V
i
|R,V
?i
,P, ?
V
,?
V
, ?) = N (?
?
i
, (?
?
i
)
?1
) (4)
Where:
?
?
i
= (?
?
i
)
?1
(?
V
?
V
+ ?
2?
k=1
N?
j=1
I
k
ij
R
k
ij
Q
jk
),
?
?
i
= ?
V
+ ?
2?
k=1
N?
j=1
I
k
ij
Q
jk
Q
T
jk
,
Q
jk
= V
j
 P
k
 is the element-wise product.
p(P
i
|R,V,P
?i
, ?
P
,?
P
, ?) = N (?
?
i
, (?
?
i
)
?1
) (5)
Where:
?
?
k
= (?
?
k
)
?1
(?
P
?
P
+ ?
N?
i,j=1
I
k
ij
R
k
ij
X
ij
),
?
?
k
= ?
P
+ ?
N?
i,j=1
I
k
ij
X
ij
X
T
ij
,
X
ij
= V
i
 V
j
The influence each perspective k has on the la-
tent word vectors is roughly propotional to the
number of non-empty entries n
k
=
?
i,j
I
k
i,j
. If
one wants to adjust the weight of each slices, this
can easily achieved by adjusting (e.g. down sam-
pling) the number of entries of each slice sampled
at each iteration.
3.2.1 Out-of-Vocabulary words
It often occurs that some of the perspectives have
greater word coverage than the others. For ex-
ample, hand-labeled word relatedness usually has
much less coverage than automatically acquired
similarities. Of course, it is typically for the hand-
labeled perspectives that the generalization is most
Algorithm 1 Gibbs Sampling for BPTF
Initialize the parameters.
repeat
Sample the hyper-parameters ?, ?
V
, ?
V
, ?
P
,
?
P
(Equation 1, 2, 3)
for i = 1 to N do
Sample V
i
(Equation 4)
end for
for k = 1 to 2 do
Sample P
k
(Equation 5)
end for
until convergence
desired. In this situation, our model can generalize
word relatedness for the sparse perspective. For
example, assume perspective k has larger vocabu-
lary coverageN
k
, while perspective l has a smaller
coverage N
l
.
There are two options for using the high vocab-
ulary word relation matrix to generalize over the
perspective with lower coverage. The most direct
way simply considers the larger vocabulary in the
BPTF R ? R
N
k
?N
k
?K
directly. A more efficient
method trains on a tensor using the smaller vocab-
ulary R ? R
N
l
?N
l
?K
, then samples the N
k
?N
l
word vectors using Equation 4.
3.3 Predictions
With MCMC method, we can approximate the
word relatedness distribution easily by averaging
over a number of samples (instead of calculating
intractable marginal distribution):
p(
?
R
k
ij
|R) ?
1
M
M
?
m=1
p(
?
R
k
ij
|V
m
i
, V
m
j
, P
m
k
, ?
m
),
wherem indicate parameters sampled from differ-
ent sampling iterations.
3.4 Scalability
The time complexity of training our model is
roughly O(n?D
2
), where n is the number of ob-
served entries in the tensor. If one is only inter-
ested in creating and re-creating word relatedness
of one single slice rather than synthesizing word
vectors, then entries in other slices can be down-
sampled at every iteration to reduce the training
time. In our model, the vector length D is not
sensitive and does not necessarily need to be very
long. Xiong et al. (2010) reported in their collab-
orative filtering experiment D = 10 usually gives
satisfactory performance.
1526
4 Experimental Evaluation
In this section, we evaluate our model by answer-
ing antonym questions. This task is especially
suitable for evaluating our model since the perfor-
mance of straight-forward look-up from the the-
sauruses we considered is poor. There are two ma-
jor limitations:
1. The thesaurus usually only contains antonym
information for word pairs with a strong con-
trast.
2. The vocabulary of the antonym entries in the
thesaurus is limited, and does not contain
many words in the antonym questions.
On the other hand, distributional similarities can
be trained from large corpora and hence have a
large coverage for words. This implies that we can
treat the thesaurus data as the first slice, and the
distributional similarities as the second slice, then
use our model to create / recreate word relatedness
on the first slice to answer antonym questions.
4.1 The GRE Antonym Questions
There are several publicly available test datasets
to measure the correctness of our word embed-
dings. In order to be able to compare with pre-
vious works, we follow the widely-used GRE test
dataset provided by (Mohammad et al., 2008),
which has a development set (consisting of 162
questions) and a test set (consisting of 950 ques-
tions). The GRE test is a good benchmark because
the words are relatively rare (19% of the words in
Mohammad?s test are not in the top 50,000 most
frequent words from Google Books (Goldberg and
Orwant, 2013)), thus it is hard to lookup answers
from a thesaurus directly with high recall. Below
is an example of the GRE antonym question:
adulterate: a. renounce b. forbid
c. purify d. criticize e. correct
The goal is to choose the most opposite word from
the target, here the correct answer is purify.
4.2 Data Resources
In our tensor model, the first slice (k = 1) con-
sists of synonyms and antonyms from public the-
sauruses, and the second slice (k = 2) consists of
cosine similarities from neural word embeddings
(example in Table 1)
4.2.1 Thesaurus
Two popular thesauruses used in other research are
the Macquarie Thesaurus and the Encarta The-
saurus. Unfortunately, their electronic versions
are not publicly available. In this work we use two
alternatives:
WordNet Words in WordNet (version 3.0) are
grouped into sense-disambiguated synonym sets
(synsets), and synsets have links between each
other to express conceptual relations. Previ-
ous works reported very different look-up perfor-
mance using WordNet (Mohammad et al., 2008;
Yih et al., 2012), we consider this difference
as different understanding of the WordNet struc-
ture. By extending ?indirect antonyms? defined in
WordNet to nouns, verbs and adverbs that similar
words share the antonyms,we achieve a look-up
performance close to Yih et al. (2012). Using this
interpretation of WordNet synonym and antonym
structure we obtain a thesaurus containing 54,239
single-token words. Antonym entries are present
for 21,319 of them with 16.5 words per entry on
average, and 52,750 of them have synonym entries
with 11.7 words per entry on average.
Roget?s Only considering single-token words,
the Roget?s Thesaurus (Kipfer, 2009) contains
47,282 words. Antonym entries are present for
8,802 of them with 4.2 words per entry on av-
erage, and 22,575 of them have synonym entries
with 20.7 words per entry on average. Although
the Roget?s Thesaurus has a less coverage on both
vocabulary and antonym pairs, it has better look-
up precision in the GRE antonym questions.
4.2.2 Distributional Similarities
We use cosine similarity of the morphRNN word
representations
2
provided by Luong et al. (2013)
as a distributional word relatedness perspective.
They used morphological structure in training re-
cursive neural networks and the learned mod-
els outperform previous works on word similarity
tasks, especially a task focused on rare words. The
vector space models were initialized from exist-
ing word embeddings trained on Wikipedia. We
use word embeddings adapted from Collobert et
al. (2011). This advantage complements the weak-
ness of the thesaurus perspective ? that it has less
coverage on rare words. The word vector data con-
tains 138,218 words, and it covers 86.9% of the
words in the GRE antonym questions. Combining
the two perspectives, we can cover 99.8% of the
1527
Dev. Set Test Set
Prec. Rec. F
1
Prec. Rec. F
1
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA 0.66 0.65 0.65 0.61 0.59 0.60
Encarta lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA 0.87 0.82 0.84 0.82 0.74 0.78
Encarta PILSA + S2Net + Emebed 0.88 0.87 0.87 0.81 0.80 0.81
W&E MRLSA 0.88 0.85 0.87 0.81 0.77 0.79
WordNet lookup* 0.93 0.32 0.48 0.95 0.33 0.49
WordNet lookup 0.48 0.44 0.46 0.46 0.43 0.44
WordNet BPTF 0.63 0.63 0.63 0.63 0.62 0.62
Roget lookup* 1.00 0.35 0.52 0.99 0.31 0.47
Roget lookup 0.61 0.44 0.51 0.55 0.39 0.45
Roget BPTF 0.80 0.80 0.80 0.76 0.75 0.76
W&R lookup* 1.00 0.48 0.64 0.98 0.45 0.62
W&R lookup 0.62 0.54 0.58 0.59 0.51 0.55
W&R BPMF 0.59 0.59 0.59 0.52 0.52 0.52
W&R BPTF 0.88 0.88 0.88 0.82 0.82 0.82
Table 2: Development and test results on the GRE antonym questions. *Note: to allow comparison, in
look-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target word
is in the vocabulary while none of the choices are. Asterisk indicates the look-up results without random
guessing.
GRE antonym question words. Further using mor-
phology information from WordNet, the coverage
achieves 99.9%.
4.3 Tests
To answer the GRE questions, we calculateR
1
ij
for
word pair (i, j), where i is the target word and j
is one of the question?s candidates. The candidate
with the smallest similarity is then the predicted
answer. If a target word is missing in the vocabu-
lary, that question will not be answered, while if a
choice is missing, that choice will be ignored.
We first train on a tensor from a subset consist-
ing of words with antonym entries, then add all
other words using the out-of-vocabulary method
described in Section 3. During each iteration, ze-
ros are randomly added into the first slice to keep
the model from overfitting. In the meantime, the
second slice entries is randomly downsampled to
match the number of non-empty entries in the first
slice. This ensures each perspective has approxi-
mately equal influence on the latent word vectors.
We sample the parameters iteratively, and
choose the burn-in period and vector length D ac-
cording to the development set. We choose the
vector length D = 40, the burn-in period starting
from the 30
th
iterations, then averaging the relat-
edness over 200 runs. The hyper-priors used are
?
0
= 0, ?
0
= ??
0
= D, ?
0
= 1 and W
0
=
?
W
0
= I
(not tuned). Note that Yih et al. (2012) use a vec-
tor length of 300, which means our embeddings
save considerable storage space and running time.
Our model usually takes less than 30 minutes to
meet the convergence criteria (on a machine with
an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In
contrast, the MRLSA requires about 3 hours for
tensor decomposition (Chang et al., 2013).
4.4 Results
The results are summarized in Table 2. We list the
results of previous works (Yih et al., 2012; Chang
et al., 2013) at the top of the table, where the
best performance is achieved by PILSA on Encarta
with further discriminative training and embed-
ding. For comparison, we adopt the standard first
used by (Mohammad et al., 2008), where preci-
sion is the number of questions answered correctly
2
http://www-nlp.stanford.edu/ lmthang/morphoNLM/
1528
20 40 60 80 100 120 140
Number of Iterations
0.0
0.5
1.0
1.5
2.0
2.5
R
M
S
E
BPMF
BPTF
Figure 2: Convergence curves of BPMF and BPTF
in training the W&R dataset. MAE is the mean
absolute error over the synonym & antonym slice
in the training tensor.
divided by the number of questions answered. Re-
call is the number of questions answered correctly
divided by the total number of questions. BPMF
(Bayesian Probabilistic Matrix Factorization) re-
sult is derived by only keeping the synonym &
antonym slice in our BPTF model.
By using Roget?s and WordNet together, our
method increases the baseline look-up recall from
51% to 82% on the test set, while Yih?s method
increases the recall of Encarta from 56% to 80%.
This state-of-the-art performance is achieved with
the help of a neural network for fine tuning and
multiple schemes of out-of-vocabulary embed-
ding, while our method has inherent and straight-
forward ?out-of-vocabulary embedding?. While
MRLSA, which has this character as well, only
has a recall 77% when combining WordNet and
Encarta together.
WordNet records less antonym relations for
nouns, verbs and adverbs, while the GRE antonym
questions has a large coverage of them. Al-
though by extending these antonym relations us-
ing the ?indirect antonym? concept achieves better
look-up performance than Roget?s, in contrast, the
BPTF performance is actually much lower. This
implies Roget?s has better recording of antonym
relations. Mohammad et al. (2008) reproted a 23%
F-score look-up performance of WordNet which
support this claim as well. Combining WordNet
and Roget?s together can improve the look-up per-
formance further to 59% precision and 51% recall
(still not as good as Encarta look-up).
Notably, if we strictly follow our BPTF ap-
proach but only use the synonym & antonym slice
(i.e. a matrix factorization model instead of ten-
sor factorization model), this single-slice model
BPMF has performance that is only slightly bet-
ter than look-up. Meanwhile Figure 1 shows the
convergence curves of BPMF and BPTF. BPMF
actually has lower MAE after convergence. Such
behavior is caused by overfitting of BPMF on the
training data. While known entries were recreated
well, empty entries were not filled correctly. On
the other hand, note that although our BPTF model
has a higher MAE, it has much better performance
in answering the GRE antonym questions. We in-
terpret this as the regularization and generalization
effect from other slice(s). Instead of focusing on
one-slice training data, our model fills the missing
entries with the help of inter-slice relations.
We also experimented with a linear metric
learning method over the generated word vectors
(to learn a metric matrix A to measure the word
relatedness via V
T
i
AV
j
) using L-BFGS. By op-
timizing the mean square error on the synonym
& antonym slice, we can reduce 8% of the mean
square error on a held out test set, and improve
the F-score by roughly 0.5% (of a single iteration).
Although this method doesn?t give a significant
improvement, it is general and has the potential
to boost the performance in other scenarios.
5 Conclusion
In this work, we propose a method to map words
into a metric space automatically using thesaurus
data, previous vector space models, or other word
relatedness matrices as input, which is capable
of handling out-of-vocabulary words of any par-
ticular perspective. This allows us to derive the
relatedness of any given word pair and any per-
spective by the embedded word vectors with per-
perspective linear transformation. We evaluated
the word embeddings with GRE antonym ques-
tions, and the result achieves the state-of-the-art
performance.
For future works, we will extend the model and
its applications in three main directions. First, in
this model we only use a three-way tensor with
two slices, while more relations may be able to
add into it directly. Possible additional perspec-
tive slices include LSA for topic relatedness, and
corpus occurrences in engineered or induced se-
mantic patterns.
Second, we will apply the method to other tasks
that require completing a word relatedness matrix.
We evaluated the performance of our model on
1529
creating / recreating one perspective of word re-
latedness: antonymy. Perhaps using vectors gen-
erated from many kinds of perspectives would im-
prove the performance on other NLP tasks, such
as term matching employed by textual entailment
and machine translation metrics.
Third, if our model does learn the relation be-
tween semantic similarities and distributional sim-
ilarities, there may be fruitful information con-
tained in the vectors V
i
and P
k
that can be ex-
plored. One straight-forward idea is that the dot
product of perspective vectors P
k
? P
l
should be a
measurement of correlation between perspectives.
Also, a straightforward adaptation of our model
has the potential ability to capture asymmet-
ric word relatedness as well, by using a per-
perspective matrix instead of vector for the asym-
metric slices (i.e. use V
T
i
A
k
V
j
instead of
?
D
d=1
V
(d)
i
P
(d)
k
V
(d)
j
for calculating word related-
ness, where A
k
is a square matrix).
Acknowledgments
We thank Christopher Kedzie for assisting the
Semantic Technologies in IBM Watson seminar
course in which this work has been carried out,
and Kai-Wei Chang for giving detailed explana-
tion of the evaluation method in his work.
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
EMNLP.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harsh-
man. 1990. Indexing by latent semantic analysis.
JASIS, 41(6):391?407.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241?247.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Barbara Ann Kipfer. 2009. Roget?s 21st Century The-
saurus, Third Edition. Philip Lief Group.
Dekang Lin and Shaojun Zhao. 2003. Identifying syn-
onyms among distributionally similar words. In In
Proceedings of IJCAI-03, page 14921493.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL, Sofia, Bulgaria.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751. The Association for Computational Linguistics.
Tom Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
NIPS, pages 1081?1088.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In EMNLP,
pages 982?991. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics, 39(3):555?590.
Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
markov chain monte carlo. In ICML, pages 880?
887. ACM.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. International Joint Conference on
Natural Language Processing, pages 489?497.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
129?136.
Peter D. Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. Col-
ing, pages 905?912, August.
1530
Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G.
Schneider, and Jaime G. Carbonell. 2010. Tempo-
ral collaborative filtering with bayesian probabilis-
tic tensor factorization. In SDM, volume 10, pages
211?222. SIAM.
Wen-tau Yih, Geoffrey Zweig, and John C. Platt.
2012. Polarity inducing latent semantic analysis. In
EMNLP-CoNLL, pages 1212?1222. Association for
Computational Linguistics.
1531
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?193,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
When Did that Happen? ? Linking Events and Relations to Timestamps
Dirk Hovy*, James Fan, Alfio Gliozzo, Siddharth Patwardhan and Chris Welty
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
dirkh@isi.edu, {fanj,gliozzo,siddharth,welty}@us.ibm.com
Abstract
We present work on linking events and flu-
ents (i.e., relations that hold for certain
periods of time) to temporal information
in text, which is an important enabler for
many applications such as timelines and
reasoning. Previous research has mainly
focused on temporal links for events, and
we extend that work to include fluents
as well, presenting a common methodol-
ogy for linking both events and relations
to timestamps within the same sentence.
Our approach combines tree kernels with
classical feature-based learning to exploit
context and achieves competitive F1-scores
on event-time linking, and comparable F1-
scores for fluents. Our best systems achieve
F1-scores of 0.76 on events and 0.72 on flu-
ents.
1 Introduction
It is a long-standing goal of NLP to process natu-
ral language content in such a way that machines
can effectively reason over the entities, relations,
and events discussed within that content. The ap-
plications of such technology are numerous, in-
cluding intelligence gathering, business analytics,
healthcare, education, etc. Indeed, the promise
of machine reading is actively driving research in
this area (Etzioni et al 2007; Barker et al 2007;
Clark and Harrison, 2010; Strassel et al 2010).
Temporal information is a crucial aspect of this
task. For a machine to successfully understand
natural language text, it must be able to associate
time points and temporal durations with relations
and events it discovers in text.
?The first author conducted this research during an in-
ternship at IBM Research.
In this paper we present methods to estab-
lish links between events (e.g. ?bombing? or
?election?) or fluents (e.g. ?spouseOf? or ?em-
ployedBy?) and temporal expressions (e.g. ?last
Tuesday? and ?November 2008?). While previ-
ous research has mainly focused on temporal links
for events only, we deal with both events and flu-
ents with the same method. For example, consider
the sentence below
Before his death in October, Steve Jobs
led Apple for 15 years.
For a machine reading system processing this
sentence, we would expect it to link the fluent
CEO of (Steve Jobs, Apple) to time duration ?15
years?. Similarly we expect it to link the event
?death? to the time expression ?October?.
We do not take a strong ?ontological? position
on what events and fluents are, as part of our
task these distinctions are made a priori. In other
words, events and fluents are input to our tempo-
ral linking framework. In the remainder of this pa-
per, we also do not make a strong distinction be-
tween relations in general and fluents in particu-
lar, and use them interchangeably, since our focus
is only on the specific types of relations that rep-
resent fluents. While we only use binary relations
in this work, there is nothing in the framework
that would prevent the use of n-ary relations. Our
work focuses on accurately identifying temporal
links for eventual use in a machine reading con-
text.
In this paper, we describe a single approach that
applies to both fluents and events, using feature
engineering as well as tree kernels. We show that
we can achieve good results for both events and
fluents using the same feature space, and advocate
185
the versatility of our approach by achieving com-
petitive results on yet another similar task with a
different data set.
Our approach requires us to capture contextual
properties of text surrounding events, fluents and
time expressions that enable an automatic system
to detect temporal linking within our framework.
A common strategy for this is to follow standard
feature engineering methodology and manually
develop features for a machine learning model
from the lexical, syntactic and semantic analysis
of the text. A key contribution of our work in this
paper is to demonstrate a shallow tree-like repre-
sentation of the text that enables us to employ tree
kernel models, and more accurately detect tempo-
ral linking. The feature space represented by such
tree kernels is far larger than a manually engi-
neered feature space, and is capable of capturing
the contextual information required for temporal
linking.
The remainder of this paper goes into the de-
tails of our approach for temporal linking, and
presents empirical evidence for the effectiveness
of our approach. The contributions of this paper
can be summarized as follows:
1. We define a common methodology to link
events and fluents to timestamps.
2. We use tree kernels in combination with clas-
sical feature-based approaches to obtain sig-
nificant gains by exploiting context.
3. Empirical evidence illustrates that our
framework for temporal linking is very ef-
fective for the task, achieving an F1-score of
0.76 on events and 0.72 on fluents/relations,
as well as 0.65 for TempEval2, approaching
state-of-the-art.
2 Related Work
Most of the previous work on relation extraction
focuses on entity-entity relations, such as in the
ACE (Doddington et al 2004) tasks. Temporal
relations are part of this, but to a lesser extent.
The primary research effort in event temporality
has gone into ordering events with respect to one
another (e.g., Chambers and Jurafsky (2008)), and
detecting their typical durations (e.g., Pan et al
(2006)).
Recently, TempEval workshops have focused
on the temporal related issues in NLP. Some of
the TempEval tasks overlap with ours in many
ways. Our task is similar to task A and C of
TempEval-1 (Verhagen et al 2007) in the sense
that we attempt to identify temporal relation be-
tween events and time expressions or document
dates. However, we do not use a restricted set of
events, but focus primarily on a single temporal
relation tlink instead of named relations like BE-
FORE, AFTER or OVERLAP (although we show
that we can incorporate these as well). Part of our
task is similar to task C of TempEval-2 (Verha-
gen et al 2010), determining the temporal rela-
tion between an event and a time expression in
the same sentence. In this paper, we do apply our
system to TempEval-2 data and compare our per-
formance to the participating systems.
Our work is similar to that of Boguraev and
Ando (2005), whose research only deals with
temporal links between events and time expres-
sions (and does not consider relations at all). They
employ a sequence tagging model with manual
feature engineering for the task and achieved
state-of-the-art results on Timebank (Pustejovsky
et al 2003) data. Our task is slightly different be-
cause we include relations in the temporal linking,
and our use of tree kernels enables us to explore a
wider feature space very quickly.
Filatova and Hovy (2001) also explore tempo-
ral linking with events, but do not assume that
events and time stamps have been provided by an
external process. They used a heuristics-based ap-
proach to assign temporal expressions to events
(also relying on the proximity as a base case).
They report accuracy of the assignment for the
correctly classified events, the best being 82.29%.
Our best event system achieves an accuracy of
84.83%. These numbers are difficult to compare,
however, since accuracy does not efficiently cap-
ture the performance of a system on a task with so
many negative examples.
Mirroshandel et al(2011) describe the use of
syntactic tree kernels for event-time links. Their
results on TempEval are comparable to ours. In
contrast to them, we found, though, that syntactic
tree kernels alone do not perform as well as using
several flat tree representations.
3 Problem Definition
The task of linking events and relations to time
stamps can be defined as the following: given a set
of expressions denoting events or relation men-
186
tions in a document, and a set of time expressions
in the same document, find all instances of the
tlink relation between elements of the two input
sets. The existence of a tlink(e, t) means that e,
which is an event or a relation mention, occurs
within the temporal context specified by the time
expression t.
Thus, our task can be cast as a binary rela-
tion classification task: for each possible pair
of (event/relation, time) in a document, decide
whether there exists a link between the two, and
if so, express it in the data.
In addition, we make these assumptions about
the data:
1. There does not exist a timestamp for ev-
ery event/relation in a document. Although
events and relations typically have temporal
context, it may not be explicitly stated in a
document.
2. Every event/relation has at most one time ex-
pression associated with it. This is a simpli-
fying assumption, which in the case of rela-
tions we explore as future work.
3. Each temporal expression can be linked to
one or more events or relations. Since mul-
tiple events or relations may happen for a
given time, it is safe to assume that each tem-
poral expression can be linked to more than
one event/relation.
In general, the events/relations and their associ-
ated timestamps may occur within the same sen-
tence or may occur across different sentences. In
this paper, we focus on our effort and our evalua-
tion on the same sentence linking task.
In order to solve the problem of temporal link-
ing completely, however, it will be important to
also address the links that hold between entities
across sentences. We estimate, based on our data
set, that across sentence links account for 41% of
all correct event-time pairs in a document. For flu-
ents, the ratio is much higher, more than 80% of
the correct fluent-time links are across sentences.
One of the main obstacles for our approach in the
cross-sentence case is the very low ratio of posi-
tive to negative instances (3 : 100) in the set of all
pairs in a document. Most pairs are not linked to
one another.
4 Temporal Linking Framework
As previously mentioned, we approach the tem-
poral linking problem as a classification task. In
the framework of classification, we refer to each
pair of (event/relation, temporal expression) oc-
curring within a sentence as an instance. The goal
is to devise a classifier that separates positive (i.e.,
linked) instances from negative ones, i.e., pairs
where there is no link between the event/relation
and the temporal expression in question. The lat-
ter case is far more frequent, so we have an inher-
ent bias toward negative examples in our data.1
Note that the basis of the positive and nega-
tive links is the context around the target terms.
It is impossible even for humans to determine the
existence of a link based only on the two terms
without their context. For instance, given just two
words (e.g., ?said? and ?yesterday?) there is no
way to tell if it is a positive or a negative example.
We need the context to decide.
Therefore, we base our classification models on
contextual features drawn from lexical and syn-
tactic analyses of the text surrounding the target
terms. For this, we first define a feature-based
approach, then we improve it by using tree ker-
nels. These two subsections, plus the treatment
of fluent relations, are the main contributions of
this paper. In all of this work, we employ SVM
classifiers (Vapnik, 1995) for machine learning.
4.1 Feature Engineering
A manual analysis of development data provided
several intuitions about the kinds of features that
would be useful in this task. Based on this anal-
ysis and with inspiration from previous work (cf.
Boguraev and Ando (2005)) we established three
categories of features whose description follows.
Features describing events or relations. We
check whether the event or relation is phrasal, a
verb, or noun, whether it is present tense, past
tense, or progressive, the type assigned to the
event/relation by the UIMA type system used for
processing, and whether it includes certain trig-
ger words, such as reporting verbs (?said?, ?re-
ported?, etc.).
1Initially, we employed an instance filtering method to
address this, which proved to be ineffective and was subse-
quently left out.
187
Features describing temporal expressions.
We check for the presence of certain trigger words
(last, next, old, numbers, etc.) and the type of
the expression (DURATION, TIME, or DATE) as
specified by the UIMA type system.
Features describing context. We also in-
clude syntactic/structural features, such as testing
whether the relation/event dominates the temporal
expression, which one comes first in the sentence
order, and whether either of them is dominated
by a separate verb, preposition, ?that? (which of-
ten indicates a subordinate sentence) or counter-
factual nouns or verbs (which would negate the
temporal link).
It is not surprising that some of the most in-
formative features (event comes before tempo-
ral expression, time is syntactic child of event)
are strongly correlated with the baselines. Less
salient features include the test for certain words
indicating the event is a noun, a verb, and if so
which tense it has and whether it is a reporting
verb.
4.2 Tree Kernel Engineering
We expect that there exist certain patterns be-
tween the entities of a temporal link, which mani-
fest on several levels: some on the lexical level,
others expressed by certain sequences of POS
tags, NE labels, or other representations. Kernels
provide a principled way of expanding the number
of dimensions in which we search for a decision
boundary, and allow us to easily model local se-
quences and patterns in a natural way (Giuliano et
al., 2009). While it is possible to define a space
in which we find a decision boundary that sepa-
rates positive and negative instances with manu-
ally engineered features, these features can hardly
capture the notion of context as well as those ex-
plored by a tree kernel.
Tree Kernels are a family of kernel functions
developed to compute the similarity between tree
structures by counting the number of subtrees
they have in common. This generates a high-
dimensional feature space that can be handled ef-
ficiently using dynamic programming techniques
(Shawe-Taylor and Christianini, 2004). For our
purposes we used an implementation of the Sub-
tree and Subset Tree (SST) (Moschitti, 2006).
The advantages of using tree kernels are
two-fold: thanks to an existing implementation
(SVMlight with tree kernels, Moschitti (2004)), it
is faster and easier than traditional feature engi-
neering. The tree structure also allows us to use
different levels of representations (POS, lemma,
etc.) and combine their contributions, while at the
same time taking into account the ordering of la-
bels. We use POS, lemma, semantic type, and a
representation that replaces each word with a con-
catenation of its features (capitalization, count-
able, abstract/concrete noun, etc.).
We developed a shallow tree representation that
captures the context of the target terms, without
encoding too much structure (which may prevent
generalization). In essence, our tree structure in-
duces behavior somewhat similar to a string ker-
nel. In addition, we can model the tasks by pro-
viding specific markup on the generated tree. For
example, in our experiment we used the labels
EVENT (or equivalently RELATION) and TIME-
STAMP to mark our target terms. In order to re-
duce the complexity of this comparison, we focus
on the substring between event/relation and time
stamp and the rest of the tree structure is trun-
cated.
Figure 1 illustrates an example of the structure
described so far for both lemmas and POS tags
(note that the lowest level of the tree contains tok-
enized items, so their number can differ form the
actual words, as in ?attorney general?). Similar
trees are produced for each level of representa-
tions used, and for each instance (i.e., pair of time
expressions and event/relation). If a sentence con-
tains more than one event/relation, we create sep-
arate trees for each of them, which differ in the po-
sition of the EVENT/RELATION marks (at level
1 of the tree).
The tree kernel implicitly expands this struc-
ture into a number of substructures allowing us
to capture sequential patterns in the data. As we
will see, this step provides significant boosts to
the task performance.
Curiously, using a full-parse syntactic tree as
input representation did not help performance.
This is in line with our finding that syntactic re-
lations are less important than sequential patterns
(see also Section 5.2). Therefore we adopted the
?string kernel like? representation illustrated in
Figure 1.
188
Scores of supporters of detained Egyptian opposition leader Nur demonstrated outside the attorney general?s
office in Cairo last Saturday, demanding he be freed immediately.
BOW
TIME
TOK
saturday
TOK
last
TERM
TOK
cairo
TERM
TOK
in
TERM
TOK
office
TERM
TOK
attorney general
TERM
TOK
outside
EVENT
TOK
demonstrate
BOP
TIME
TOK
NNP
TOK
JJ
TERM
TOK
NNP
TERM
TOK
IN
TERM
TOK
NN
TERM
TOK
NNP
TERM
TOK
ADV
EVENT
TOK
VBD
Figure 1: Input Sentence and Tree Kernel Representations for Bag of Words (BOW) and POS tags (BOP)
5 Evaluation
We now apply our models to real world data, and
empirically demonstrate their effectiveness at the
task of temporal linking. In this section, we de-
scribe the data sets that were used for evaluation,
the baselines for comparison, parameter settings,
and the results of the experiments.
5.1 Benchmark
We evaluated our approach in 3 different tasks:
1. Linking Timestamps and Events in the IC
domain
2. Linking Timestamps and Relations in the IC
domain
3. Linking Events to Temporal Expressions
(TempEval-2, task C)
The first two data sets contained annotations
in the intelligence community (IC) domain, i.e.,
mainly news reports about terrorism. It com-
prised 169 documents. This dataset has been de-
veloped in the context of the machine reading pro-
gram (MRP) (Strassel et al 2010). In both cases
our goal is to develop a binary classifier to judge
whether the event (or relation) overlaps with the
time interval denoted by the timestamp. Success
of this classification can be measured by precision
and recall on annotated data.
We originally considered using accuracy as a
measure of performance, but this does not cor-
rectly reflect the true performance of the system:
given the skewed nature of the data (much smaller
number of positive examples), we could achieve a
high accuracy simply by classifying all instances
as negative, i.e., not assigning a time stamp at all.
We thus decided to report precision, recall and F1.
Unless stated otherwise, results were achieved via
10-fold cross-validation (10-CV).
The number of instances (i.e., pairs of event
and temporal expression) for each of the differ-
ent cases listed above was (in brackets the ratio of
positive to negative instances).
? events: 2046 (505 positive, 1541 negative)
? relations: 6526 (1847 positive, 4679 nega-
tive)
The size of the relation data set after filtering is
5511 (1847 positive, 3395 negative).
In order to increase the originally lower number
of event instances, we made use of the annotated
event-coreference as a sort of closure to add more
instances: if events A and B corefer, and there
is a link between A and time expression t, then
there is also a link between B and t. This was not
explicitly expressed in the data.
For the task at hand, we used gold standard
annotations for timestamps, events and relations.
The task was thus not the identification of these
objects (a necessary precursor and a difficult task
in itself), but the decision as to which events and
time expressions could and should be linked.
We also evaluated our system on TempEval-
2 (Verhagen et al 2010) for better comparison
189
to the state-of-the-art. TempEval-2 data included
the task of linking events to temporal expressions
(there called ?task C?), using several link types
(OVERLAP, BEFORE, AFTER, BEFORE-OR-
OVERLAP, OVERLAP-OR-AFTER). This is a
bit different from our settings as it required the
implementation of a multi-class classifier. There-
fore we trained three different binary classifiers
(using the same feature set) for the first three of
those types (for which there was sufficient train-
ing data) and we used a one-versus-all strategy to
distinguish positive from negative examples. The
output of the system is the category with the high-
est SVM decision score. Since we only use three
labels, we incur an error every time the gold la-
bel is something else. Note that this is stricter
than the evaluation in the actual task, which left
contestants with the option of skipping examples
their systems could not classify.
5.2 Baselines
Intuitively, one would expect temporal expres-
sions to be close to the event they denote, or even
syntactically related. In order to test this, we ap-
plied two baselines. In the first, each temporal ex-
pression was linked to the closest event (as mea-
sured in token distance). In the second, we at-
tached each temporal expression to its syntactic
head, if the head was an event. Results are re-
ported in Figure 2.
While these results are encouraging for our
task, it seems at first counter-intuitive that the
syntactic baseline does worse than the proximity-
based one. It does, however, reveal two facts:
events are not always synonymous with syntactic
units, and they are not always bound to tempo-
ral expressions through direct syntactic links. The
latter makes even more sense given that the links
can even occur across sentence boundaries. Pars-
ing quality could play a role, yet seems far fetched
to account for the difference.
More important than syntactic relations seem
to be sequential patterns on different levels, a fact
we exploit with the different tree representations
used (POS tags, NE types, etc.).
For relations, we only applied the closest-
relation baseline. Since relations consist of two or
more arguments that occur in different, often sep-
arated syntactic constituents, a syntactic approach
seems futile, especially given our experience with
events. Results are reported in Figure 3.
baseline comparison
Page 1
Precision Recall F1
0
20
40
60
80
100
35.0
63.0
45.048.0
88.0
62.063.0
75.4 68.376.6 76.5 76.2
Evaluation Measures Events
BL-parent BL-closest features +tree kernel
metric
%
Figure 2: Performance on events
System Accuracy
TRIOS 65%
this work 64.5%
JU-CSE, NCSU-indi
TRIPS, USFD2
all 63%
Table 1: Comparison to Best Systems in TempEval-2
5.3 Events
Figure 2 shows the improvements of the feature-
based approach over the two baseline, and the ad-
ditional gain obtained by using the tree kernel.
Both the features and tree kernels mainly improve
precision, while the tree kernel adds a small boost
in recall. It is remarkable, though, that the closest-
event baseline has a very high recall value. This
suggests that most of the links actually do occur
between items that are close to one another. For a
possible explanation for the low precision value,
see the error analysis (Section 5.5).
Using a two-tailed t-test, we compute the sig-
nificance in the difference between the F1-scores.
Both the feature-based and the tree kernel ap-
proach improvements are statistically significant
at p < 0.001 over the baseline scores.
Table 1 compares the performances of our sys-
tem to the state-of-the-art systems on TempEval-2
Data, task C, showing that our approach is very
competitive. The best systems there used sequen-
tial models. We attribute the competitive nature
of our results to the use of tree kernels, which en-
ables us to make use of contextual information.
5.4 Relations
In general, performance for relations is not as high
as for events (see Figure 3). The reason here is
two-fold: relations consist of two (or more) ele-
ments, which can be in various positions with re-
spect to one another and the temporal expression,
and each relation can be expressed in a number of
190
baseline comparison
Page 1
Precision Recall F1
0
10
20
30
40
50
60
70
80
90
100
35.0
24.0 29.0
63.1
80.6
70.470.8 74.0 72.2
Evaluation Metric Relations
BL-closest features +tree kernel
metric
%
Figure 3: Performance on relations/fluents
learning curves
Page 1
0 10 20 30 40 50 60 70 80 90 100
40
45
50
55
60
65
70
75
80
Learning Curves Relations
features w/ tree 
kernel
% of data
F1
 sc
or
e
Figure 4: Learning curves for relation models
different ways.
Again, we perform significance tests on the dif-
ference in F1 scores and find that our improve-
ments over the baseline are statistically significant
at p < 0.001. The improvement of the tree kernel
over the feature-based approach, however, are not
statistically significant at the same value.
The learning curve over parts of the training
data (exemplary shown here for relations, Figure
4)2 indicates that there is another advantage to us-
ing tree kernels: the approach can benefit from
more data. This is conceivably because it allows
the kernel to find more common subtrees in the
various representations the more examples it gets,
while the feature space rather finds more instances
that invalidate the expressiveness of features (i.e.,
it encounters positive and negative instances that
have very similar feature vectors). The curve sug-
gests that tree kernels could yield even better re-
sults with more data, while there is little to no ex-
pected gain using only features.
5.5 Error Analysis
Examining the misclassified examples in our data,
we find that both feature-based and tree-kernel
approaches struggle to correctly classify exam-
2The learning curve for events looks similar and is omit-
ted due to space constraints.
ples where time expression and event/relation are
immediately adjacent, but unrelated, as in ?the
man arrested last Tuesday told the police ...?,
where last Tuesday modifies arrested. It limits
the amount of context that is available to the tree
kernels, since we truncate the tree representations
to the words between those two elements. This
case closely resembles the problem we see in the
closest-event/relation baseline, which, as we have
seen, does not perform too well. In this case, the
incorrect event (?told?) is as close to the time ex-
pression as the correct one (?arrested?), resulting
in a false positive that affects precision. Features
capturing the order of the elements do not seem
help here, since the elements can be arranged in
any order (i.e., temporal expression before or af-
ter the event/relation). The only way to solve this
problem would be to include additional informa-
tion about whether a time expression is already
attached to another event/relation.
5.6 Ablations
To quantify the utility of each tree representation,
we also performed all-but-one ablation tests, i.e.,
left out each of the tree representations in turn, ran
10-fold cross-validation on the data and observed
the effect on F1. The larger the loss in F1, the
more informative the left-out-representation. We
performed ablations for both events and relations,
and found that the ranking of the representations
is the same for both.
In events and relations alike, leaving out POS
trees has the greatest effect on F1, followed by
the feature-bundle representation. Lemma and se-
mantic type representation have less of an impact.
We hypothesize that the former two capture un-
derlying regularities better by representing differ-
ent words with the same label. Lemmas in turn
are too numerous to form many recurring pat-
terns, and semantic type, while having a smaller
label alphabet, does not assign a label to every
word, thus creating a very sparse representation
that picks up more noise than signal.
In preliminary tests, we also used annotated
dependency trees as input to the tree kernel, but
found that performance improved when they were
left out. This is at odds with work that clearly
showed the value of syntactic tree kernels (Mir-
roshandel et al 2011). We identify two poten-
tial causes?either our setup was not capable of
correctly capturing and exploiting the information
191
from the dependency trees, or our formulation of
the task was not amenable to it. We did not inves-
tigate this further, but leave it to future work.
6 Conclusion and Future Work
We cast the problem of linking events and rela-
tions to temporal expressions as a classification
task using a combination of features and tree ker-
nels, with probabilistic type filtering. Our main
contributions are:
? We showed that within-sentence temporal
links for both events and relations can be ap-
proached with a common strategy.
? We developed flat tree representations and
showed that these produce considerable
gains, with significant improvements over
different baselines.
? We applied our technique without great ad-
justments to an existing data set and achieved
competitive results.
? Our best systems achieve F1 score of 0.76
on events and 0.72 on relations, and are ef-
fective at the task of temporal linking.
We developed the models as part of a machine
reading system and are currently evaluating it in
an end-to-end task.
Following tasks proposed in TempEval-2, we
plan to use our approach for across-sentence clas-
sification, as well as a similar model for linking
entities to the document creation date.
Acknowledgements
We would like to thank Alessandro Moschitti for
his help with the tree kernel setup, and the review-
ers who supplied us with very constructive feed-
back. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA
Machine Reading Program.
References
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw,
James Fan, Noah Friedland, Michael Glass, Jerry
Hobbs, Eduard Hovy, David Israel, Doo Soon Kim,
Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce
Porter, Dan Tecuci, and Peter Yeh. 2007. Learn-
ing by reading: A prototype system, performance
baseline and lessons learned. In Proceedings of
the 22nd National Conference for Artificial Intelli-
gence, Vancouver, Canada, July.
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal rea-
soning. In Proceedings of IJCAI, volume 5, pages
997?1003. IJCAI.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. pages
789?797. Association for Computational Linguis-
tics.
Peter Clark and Phil Harrison. 2010. Machine read-
ing as a process of partial question-answering. In
Proceedings of the NAACL HLT Workshop on For-
malisms and Methodology for Learning by Reading,
Los Angeles, CA, June.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion program ? tasks, data and evaluation. In Pro-
ceedings of the LREC Conference, Canary Islands,
Spain, July.
Oren Etzioni, Michele Banko, and Michael Cafarella.
2007. Machine reading. In Proceedings of the
AAAI Spring Symposium Series, Stanford, CA,
March.
Elena Filatova and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Proceedings of
the workshop on Temporal and spatial information
processing, volume 13, pages 1?8. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and
Carlo Strapparava. 2009. Kernel methods for min-
imally supervised wsd. Computational Linguistics,
35(4).
Seyed A. Mirroshandel, Mahdy Khayyamian, and
Gholamreza Ghassem-Sani. 2011. Syntactic tree
kernels for event-time temporal relation learning.
Human Language Technology. Challenges for Com-
puter Science and Linguistics, pages 213?223.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 335?es. Associa-
tion for Computational Linguistics.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL, volume 6.
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs. 2006.
Learning event durations from event descriptions.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 393?400. Association for Computa-
tional Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
192
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics
2003, pages 647?656.
John Shawe-Taylor and Nello Christianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA Machine Read-
ing Program-Encouraging Linguistic and Reason-
ing Research with a Series of Reading Tasks. In
Proceedings of LREC 2010.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. Semeval-2007 task 15: Tempeval
temporal relation identification. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 75?80. Association for Computational
Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
193
Kernel Methods for Minimally Supervised WSD
Claudio Giuliano?
Fondazione Bruno Kessler ? IRST
Alfio Massimiliano Gliozzo??
Fondazione Bruno Kessler ? IRST
Carlo Strapparava?
Fondazione Bruno Kessler ? IRST
We present a semi-supervised technique for word sense disambiguation that exploits external
knowledge acquired in an unsupervised manner. In particular, we use a combination of basic
kernel functions to independently estimate syntagmatic and domain similarity, building a set of
word-expert classifiers that share a common domain model acquired from a large corpus of un-
labeled data. The results show that the proposed approach achieves state-of-the-art performance
on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although
it uses a considerably smaller number of training examples than other methods.
1. Introduction
A significant challenge in many natural language processing tasks is to reduce the need
for labeled training data while maintaining an acceptable performance. This is espe-
cially true for word sense disambiguation (WSD) because when moving from the some-
what artificial lexical-sample task to the more realistic all-words task it is practically
impossible to collect a large number of training examples for each word sense. Thus,
many supervised approaches, explicitly designed for the lexical-sample task, cannot be
applied to the all-words task, even though they exhibit excellent performance. This has
led to the somewhat paradoxical situation in which completely different methods have
been developed for the two tasks, although they represent two sides of the same coin.
To address this problem, in recent work we presented a semi-supervised approach
based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo,
Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006). In particular,
we explored the following research directions: (1) independently modeling domain and
syntagmatic aspects of sense distinction to improve feature representativeness; and
(2) exploiting external knowledge acquired from unlabeled data, with the purpose of
drastically reducing the amount of labeled training data. The first direction is based on
the linguistic assumption that syntagmatic and domain (associative) relations are crucial
for representing sense distinctions, but they are originated by different phenomena.
Regarding the second direction, one can hope to obtain a more accurate prediction
? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: giuliano@fbk.eu.
?? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: gliozzo@fbk.eu.
? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: strappa@fbk.eu.
Submission received: 23 December 2006; revised submission received: 28 February 2008; accepted for
publication: 17 April 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
by taking into account unlabeled data relevant to the learning problem (Chapelle,
Scho?lkopf, and Zien 2006). As a matter of fact, to test this hypothesis, most of the lexical
sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large
amount of unlabeled training data, as well as the usual labeled training data. However,
at that time, we were the only team to use the unlabeled data (Strapparava, Gliozzo,
and Giuliano 2004).
In this article, we review our technique that combines domain and syntagmatic
information in order to define a complete kernel for WSD. The rest of the article is
organized as follows. In Section 2, we provide a general introduction to the kernel
methods, in which we give the basis for understanding our approach. Exploiting kernel
methods, we can define and combine individual kernels representing information from
different sources in a principled way. After this introductory section, in Section 3 we
present the kernels that we developed for WSD. This includes a detailed description
of the individual kernels and the way we define the composite ones. We present our
experiments in Section 4. The results obtained on a range of lexical-sample tasks and on
the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our
approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclu-
sions and some directions for future research.
2. Kernel Methods
Kernel methods are a popular machine learning approach within the natural lan-
guage processing community. They are theoretically well founded in statistical learn-
ing theory and have shown good empirical results in many applications (Vapnik 1999;
Cristianini and Shawe-Taylor 2000; Scho?lkopf and Smola 2002; Shawe-Taylor and
Cristianini 2004).
The strategy adopted by kernel methods consists of splitting the learning problem
into two parts. They first embed the input data in a suitable feature space, and then
use a linear algorithm to discover nonlinear patterns in the input space. Typically, the
mapping is performed implicitly by a so-called kernel function. The kernel function
is a similarity measure between the input data that depends exclusively on the specific
data type and domain. A typical similarity function is the inner product between feature
vectors. Characterizing the similarity of the inputs plays a crucial role in determining
the success or failure of the learning algorithm, and it is one of the central questions in
the field of machine learning.
Formally, the kernel is a function K : X? X ? R that takes as input two data objects
(e.g., vectors, texts, or parse trees) and outputs a real number characterizing their
similarity, with the property that the function is symmetric and positive semi-definite.
That is, for all xi, xj ? X satisfies
K(xi, xj) = ??(xi),?(xj)? (1)
where ? is an (implicit) mapping from X to an (inner product) feature space F .
Kernels are used inside learning algorithms such as support vector machines (SVM)
or kernel perceptrons as the interface between the algorithm and the data. The kernel
function is then the only domain specific element of the system, while the learning
algorithm is a general purpose component.
The idea behind the SVM (one of the best known kernel-based learning algorithms)
is to map the set of training data into a high-dimensional feature space F via a mapping
function? : X ? F , and construct a separating hyperplane with maximummargin (i.e.,
514
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
the minimum distance between the hyperplane and data points) in that space. The use
of an appropriate non-linear transformation ? of the input yields a nonlinear decision
boundary in the input space. Kernel functions make possible the use of feature spaces
with an exponential or even infinite number of dimensions. Instead of performing the
explicit feature mapping ?, one can use a kernel function, which permits the (efficient)
computation of inner products in high-dimensional feature spaces without explicitly
carrying out the mapping ?. This is called the kernel trick in the machine learning
literature (Boser, Guyon, and Vapnik 1992).
Finally, we point out the theoretical tools required to create new kernels, and com-
bine individual kernels to form composite ones. Of course, not every similarity function
is a valid kernel because, by definition, kernels should be equivalent to some inner
product in a feature space. The function K : X? X ? R is a valid kernel provided that
its kernel matrices1 are positive semi-definite2 for all training sets S = {x1, ..., xl}, the
so-called finitely positive semi-definite property. Note that defining similarity measures
by means of kernels may be more intuitive than performing the explicit mapping in the
feature space. Furthermore, this formulation does not require the set X to be a vector
space: for example, we shall define kernels that take strings as input.
This result is not only useful because it opens new perspectives to define kernel
functions that only implicitly correspond to a feature mapping ?. Another consequence
is that it can be used to prove a set of rules for combining basic kernels to obtain compos-
ite ones. This will allow us to integrate heterogeneous sources of information in a simple
and effective way.We shall use the following properties of kernels to define our compos-
ite kernels. Let k1 and k2 be kernels over X? X; then the following functions are kernels:
 k(xi, xj) = k1(xi, xj)+ k2(xi, xj)
 k(xi, xj) = c ? k1(xi, xj), c ? R+
 k(xi, xj) =
k1(xi,xj )?
k1(xi,xi )?k1(xj,xj )
(normalization)
In summary, we can define a kernel function by following different strategies: (1)
providing an explicit feature mapping ? : X ? Rn; (2) defining a similarity function
that is symmetric and positive semi-definite; and (3) composing different valid kernels,
using the closure properties of kernels. This forms the basis for the approach described
in the following section.
3. Kernel Methods for WSD
Our approach toWSD consists of representing linguistic phenomena independently and
then defining a combinationmethod to integrate them. As described in the previous sec-
tion, the kernel function is the only task-specific component of the learning algorithm.
Thus, to develop a WSD system, we only need to define appropriate kernel functions to
represent the domain and syntagmatic aspects of sense distinction and, second, exploit
the properties of kernel functions to define a composite kernel to combine and extend
the individual kernels.
The resulting WSD system consists of two families of kernels: the domain and the
syntagmatic kernels. The former family, described in Section 3.1, models the domain
1 Given a set of vectors S = {x1, ..., xl}, the kernel matrix K is defined as the l? lmatrix Kwhose entries
are Kij = k(xi, xj ) = ??(xi ),?(xj )?, where k is a kernel function that evaluates the inner products in a
feature space with feature map ?.
2 A symmetric matrix is positive semi-definite if its eigenvalues are all non-negative. Actually, as we will
see in Section 3.2 using Proposition 1, it is quite easy to verify this property.
515
Computational Linguistics Volume 35, Number 4
Table 1
An example of a domain matrix.
Medicine Computer Science
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-of-
words kernel (KBoW). The latter, described in Section 3.2, represents the syntagmatic
aspects of sense distinction; it is composed of the collocation kernel (KColl) and the part-
of-speech kernel (KPoS). Finally, Section 3.3 describes the composite kernel for WSD.
3.1 Domain Kernels
It has been shown that domain information is fundamental for WSD (Magnini et al
2002). For instance, the (domain) polysemy between the computer science and the
medicine senses of the word virus can be solved by considering the domain of the
context in which it appears. Gliozzo, Strapparava, and Dagan (2004) proposed a WSD
method that exploits only domain information.
In the context of kernel methods, domain information can be exploited by defining
a kernel function that estimates the domain similarity between the contexts of the
words to be disambiguated. The simplest method to estimate the domain similarity
between two texts is to compute the cosine similarity of their vector representations
in the vector space model (VSM). The VSM is a k-dimensional space Rk, in which the
text tj is represented by a vector tj, where the i
th component is the term frequency of
the term wi in tj. However, such an approach does not deal well with lexical variability
and ambiguity. For instance, despite the fact that the sentences He is affected by AIDS
and HIV is a virus express closely-related concepts, their similarity is zero in the VSM
because they have no words in common (they are represented by orthogonal vectors).
On the other hand, due to the ambiguity of the word virus, the similarity between the
sentences The laptop has been infected by a virus and HIV is a virus is greater than zero,
even though they convey very different messages.
To overcome this problem, we introduce the domain model (DM) and show how to
use it to define a domain VSM in which texts and terms are represented in a uniform
way. A DM is composed of soft clusters of terms. Each cluster represents a semantic
domain, that is, a set of terms that often co-occur in texts having similar topics. A DM
is represented by a k? k? rectangular matrix D, containing the degree of association
among terms and domains, as illustrated in Table 1.
The matrix D is used to define a function D : Rk ? Rk
?
, that maps the vector tj
represented in the standard VSM into the vector t?j in the domain VSM. D is defined
as follows:3
D(tj) = tj(I
IDFD) = t?j (2)
3 In Wong, Ziarko, and Wong (1985), Equation (2) is used to define a generalized vector space model, of
which the domain VSM is a particular instance.
516
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where tj is represented as a row vector, I
IDF is a k? k diagonal matrix such that iIDFi,i =
IDF(wi), and IDF(wi) is the inverse document frequency of wi.
In the domain space, the similarity is estimated by taking into account second order
relations among terms. For example, the similarity of the two sentences He is affected
by AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus are
strongly associated with the medicine domain.
A DM can be estimated frommanually constructed lexical resources, such as Word-
Net Domains (Magnini and Cavaglia` 2000), or by performing a term-clustering process
on a (large) corpus. However, the second approach is more attractive because it allows
us to automatically acquire DMs for different languages and domains.
In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposi-
tion (SVD) to acquire DMs from a corpus represented by its term-by-document matrix
T, in a unsupervised way.4 SVD decomposes the term-by-document matrix T into
three matrixes T  V?k?UT, where V and U are orthogonal matrices (i.e., VTV = I and
UTU = I) whose columns are the eigenvectors of TTT and TTT, respectively, and ?k?
is the diagonal k? k matrix containing the highest k?  k eigenvalues of T, and all the
remaining elements set to 0. The parameter k? is the dimensionality of the domain VSM
and can be fixed in advance. Under this setting, we define the domain matrix D as
follows:
D = INV
?
?k? (3)
where IN is a diagonal matrix such that iNi,i =
1
?
? w?i ,
w?i ?
, w?i is the i
th row of the matrix
V
?
?k? .
5
Note that in this case, with respect to Table 1, the domains are represented by the
columns of the matrix D and they do not have an explicit name. By using a small
number of domains, we can define a very compact representation of the DM and, con-
sequently, reduce the memory requirements while preserving most of the information.
There exist very efficient algorithms to perform the SVD process on sparse matrices,
allowing us to perform this operation on large corpora in a very limited time and with
reduced memory requirements.6
Therefore, we can define the domain kernel to estimate the domain similarity
between the contexts of the words to be disambiguated. It is a variant of the latent
semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to
define an explicit mapping D : Rk ? Rk
?
from the classical VSM into the domain VSM.
The domain kernel is explicitly defined as follows:
KD(ti, tj) = ?D(ti),D(tj)? (4)
where D is the domain mapping defined in Equation (2).
4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic
indexing of documents in large corpora (Deerwester et al 1990).
5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space
(Deerwester et al 1990). The only difference in our formulation is that the vectors representing the terms
in the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by
matrix IIDF. Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widely
used in information retrieval.
6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows
us to perform this step in a few minutes even for large corpora. It can be downloaded from
http://tedlab.mit.edu/?dr/SVDLIBC/.
517
Computational Linguistics Volume 35, Number 4
A standard approach for detecting topic (domain) similarity is to extract bag-of-
words features from a wide window of text around the words to be disambiguated.
Based on this representation, we define a linear kernel called the bag-of-words kernel
(KBoW). KBoW is a particular case of the domain kernel in which D = I in Equation (2),
where I is the identity matrix. The BoW kernel does not require a DM; therefore, it
can be applied to the strictly supervised settings, in which external knowledge is not
available.
To summarize, the domain kernel allows us to plug external knowledge into the
supervised learning process; it will be compared and combined with the standard bag-
of-words approach in Section 4. In the following section, we shall see that domain
models are also useful for defining soft-matching collocation kernels.
3.2 Syntagmatic Kernels
Collocations (such as bigrams and trigrams) extracted from the local context of the word
to be disambiguated are typically used to capture syntagmatic relations (Yarowsky
1994). However, traditional approaches to WSD fail to represent non-contiguous or
shifted collocations, and fail to consider lexical variability. For example, suppose we
have to disambiguate the verb to score in the sentence Ronaldo scored the first goal, given
the labeled example The football player scored two goals in the second half as training. A
traditional approach has no clues to return the right answer because the two sentences
have no features in common.
The use of kernels on strings allows us to overcome the aforementioned problems
by representing (non-contiguous) collocations and exploiting external lexical knowl-
edge sources to define non-zero measures of similarity between words (soft-matching
criteria). In this formulation, words taken in their context are compared by kernels that
sum the number of common (non-contiguous) collocations of words, considering lexical
variability, and part-of-speech tags, avoiding an explicit feature mapping that would
lead to an exponential number of features.
String kernels (or sequence kernels) are a family of kernel functions developed
to compute the inner product among images of strings in high-dimensional feature
space using dynamic programming techniques. The gap-weighted subsequences kernel
is one of the most general types of kernel based on sequences. Roughly speaking,
it compares two strings by means of the number of contiguous and non-contiguous
substrings of a given length they have in common. Non-contiguous occurrences are
penalized according to the number of gaps they contain. Formally, let ? be an al-
phabet of |?| symbols, and s = s1s2 . . . s|s| be a finite sequence over ? (i.e., si ? ?, 1 
i  |s|). Let i = [i1, i2, . . . , in], with 1  i1 < i2 < . . . < in  |s|, be a subset of the indices
in s; we will denote as s[i] ? ?n the subsequence si1si2 . . . sin . Note that s[i] does not
necessarily form a contiguous subsequence of s; for example, if s is the sequence
?Ronaldo scored the first goal? and i = [2, 5], then s[i] is ?scored goal?. The length
spanned by s[i] in s is l(i) = in ? i1 + 1. The feature space associated with the gap-
weighted subsequences kernel of length n is indexed by I = ?n, with the embedding
given by
?nu(s) =
?
i:u=s[i]
?l(i),u ? ?n (5)
518
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where 0<? 1 is the decay factor used to penalize non-contiguous subsequences.7 The
associate kernel is defined as
Kn(s, t) = ??n(s),?n(t)? =
?
u??n
?nu(s)?
n
u(t) (6)
An explicit computation of Equation (6) is unfeasible even for small values of n.
To evaluate Kn more efficiently, we use the recursive formulation based on a dynamic
programming implementation (Lodhi et al 2002; Saunders, Tschach, and Shawe-Taylor
2002; Cancedda et al 2003). It is defined in the following equations:
K?0(s, t) = 1,?s, t (7)
K?i (s, t) = 0, if min(|s|, |t|) < i (8)
K??i (s, t) = 0, if min(|s|, |t|) < i (9)
K??i (sx, ty) =
{
?K??i (sx, t) if x = y;
?K??i (sx, t)+ ?
2K?i?1(s, t) otherwise.
(10)
K?i (sx, t) = ?K
?
i (s, t)+ K
??
i (sx, t) (11)
Kn(s, t) = 0, if min(|s|, |t|) < n (12)
Kn(sx, t) = Kn(s, t)+
?
j:tj=x
?2K?n?1(s, t[1 : j? 1]) (13)
where K?n and K
??
n are auxiliary functions with a similar definition to Kn used to facilitate
the computation. Based on these definitions, Kn can be computed in O(n|s||t|). Using
this recursive definition, it turns out that computing all kernel values for subsequences
of lengths up to n is not significantly more costly than computing the kernel for n only.
The syntagmatic kernel is defined as a sum of gap-weighted subsequences kernels
that operate at word and part-of-speech tag level. In particular, following the approach
proposed by Cancedda et al (2003), it is possible to adapt sequence kernels to operate
at word level by instancing the alphabet ? with the vocabulary V = {w1,w2, . . . ,wk}.
Moreover, we restrict the generic definition of the gap-weighted subsequences kernel
to recognize collocations in the local context of a specified word. The resulting kernel,
called the n-gram collocation kernel (KnColl), operates on sequences of lemmata around
a specified word l0 (i.e., l?3, l?2, l?1, l0, l+1, l+2, l+3). This formulation allows us to
estimate the number of common (sparse) subsequences of lemmata (i.e., collocations)
between two examples, in order to capture syntagmatic similarity. Analogously, we
define the part-of-speech kernel (KnPoS) to operate on sequences of part-of-speech tags
p?3, p?2, p?1, p0, p+1, p+2, p+3, where p0 is the part-of-speech tag of l0.
The collocation kernel and the part-of-speech kernel are defined by Equations (14)
and (15), respectively.
KColl(s, t) =
n
?
l=1
KlColl(s, t) (14)
7 Notice that by choosing ? = 1, sparse subsequences are not penalized. On the other hand, the kernel does
not take into account sparse subsequences with ? ? 0.
519
Computational Linguistics Volume 35, Number 4
KPoS(s, t) =
n
?
l=1
KlPoS(s, t) (15)
Both kernels depend on the parameter n, the length of the non-contiguous subse-
quences, and ?, the decay factor. For example, K2Coll allows us to represent all (sparse)
bigrams in the local context of a word. Finally, the syntagmatic kernel is defined as
KSynt(s, t) = KColl(s, t)+ KPoS(s, t) (16)
In the preceding definition, only exact word-matches contribute to the similarity.
To solve this problem, external lexical knowledge is fed into the supervised learning
process, allowing us to define the soft-matching collocation kernel. In particular, we de-
fine two alternative soft-matching criteria by exploiting synonymy relations inWordNet
and DMs acquired from corpora. Both criteria are based on the assumption that every
word in a sentence can be substituted by another preserving the original meaning, if
these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related
words). For example, if we consider as equivalent the terms Ronaldo and football player,
then the sentence The football player scores the first goal is equivalent to Ronaldo scores the
first goal, providing a strong evidence to disambiguate the verb to score in the second
sentence.
Following the approach proposed by Shawe-Taylor and Cristianini (2004), the soft-
matching gap-weighted subsequences kernel is now calculated recursively using Equa-
tions (7)?(9), (11), and (12), replacing Equation (10) by the equation:
K??i (sx, ty) = ?K
??
i (sx, t)+ ?
2axyK
?
i?1(s, t),?x, y (17)
and modifying Equation (13) to:
Kn(sx, t) = Kn(s, t)+
|t|
?
j
?2axtjK
?
n?1(s, t[1 : j? 1]) (18)
where axy are entries in a similarity matrix A between terms. In order to ensure that the
resulting kernel is still valid, Amust be positive semi-definite.
In the following sections, we describe the two alternative soft-matching criteria
based on WordNet Synonymy and Domain Proximity, respectively. To show that the
similarity matrices are positive semi-definite, we use the following result.
Proposition 1
A matrix A is positive semi-definite if and only if A = BTB for some real matrix B.
The proof is given in Shawe-Taylor and Cristianini (2004).
WordNet Synonymy. The first soft-matching criterion is based on WordNet8 to define
a similarity matrix between words. In particular, we substitute two words if they are
synonyms. To this end, a word is represented as vector whose dimensions are associated
8 We used WordNet 1.7.1 and MultiWordNet for English and Italian experiments, respectively.
520
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
with the synsets. Formally, we define the term-by-synset matrix S as the matrix whose
rows are indexed by the terms and whose columns are indexed by the synsets. The
(i, j)th entry of S is 1 if the synset sj contains the term wi; 0 otherwise. The matrix
S gives rise to the similarity matrix A = SST between terms. Because A can be re-
written as A = (ST )TST = BTB, it follows directly from Proposition 1 that it is positive
semi-definite.
Domain Proximity. The second soft-matching criterion exploits the domain models intro-
duced in Section 3.1 to define a similarity matrix between words. Once a DM has been
defined by the matrixD, the domain space is a k? dimensional space, in which both texts
and terms are represented by means of domain vectors, that is, vectors representing the
domain relevances among the linguistic object and each domain. The domain vector w?i
for the term wi ? V is the ith row of D, where V = {w1,w2, . . . ,wk} is the vocabulary of
the corpus. The term-by-domain matrix D gives rise to the similarity matrix A = DDT
between terms. It follows by Proposition 1 that A is positive semi-definite.
We shall show that the syntagmatic kernel is more effective than standard bigrams
and trigrams of lemmata and part-of-speech tags typically used as features in WSD.
3.3 Composite Kernel
Having defined all the individual kernels representing syntagmatic and domain aspects
of sense distinction, we can define the composite kernel to combine and extend the
individual kernels. The closure properties of the kernel functions allows us to define
the composite kernel as
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj, xj)Kl(xi, xi)
(19)
where Kl is a valid individual kernel. The individual kernels are normalized?this plays
an important role in allowing us to integrate information from heterogeneous feature
spaces.
Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and
Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effec-
tiveness of combining kernels in this way: The composite kernel consistently improves
the performance of the individual ones. In addition, this formulation allows us to
evaluate the individual contribution of each information source.
In order to show the effectiveness of the proposed domain model in supervised
learning, we defined twoWSD kernels, Kwsd and K
?
wsd. They are completely specified by
the n individual kernels that compose them in Equation (19).
Kwsd is composed by KColl, KPoS, and KBoW ;
K?wsd is composed by KColl, KPoS, KBoW , and KD.
The only difference between the two is that K?wsd uses the domain kernel KD to exploit
external knowledge while Kwsd only uses the labeled training data.
521
Computational Linguistics Volume 35, Number 4
4. Evaluation
Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds
2004). First of all, we conducted a preliminary set of experiments on the Catalan,
English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.
Second, in order to show the general applicability of the proposed method, we evalu-
ated the system on the English all-words task; the results are presented in Section 4.2.
All the experiments were performed using the SVM package (Chang and Lin 2001)
customized to embed our own kernels. The parameters were optimized by five-fold
cross-validation on the training set.
4.1 Lexical-Sample Tasks
In this section, we report the evaluation of our method on the Catalan, English, Italian,
and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004). Table 2
describes the tasks we have considered. For each task, it summarizes the number of
words to be disambiguated, the mean polysemy, the size of the labeled training set,
the size of the test set, and the size of the unlabeled training set, respectively. For the
Catalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corpora
made available by the task organizers. For the English task, we used a DM acquired
from the British National Corpus (BNC) as the task organizers have not provided
any unlabeled training data. The objectives of these experiments are to (a) estimate
the impact of different knowledge sources in WSD; (b) study the effectiveness of the
kernel combination; (c) understand the benefits of plugging external information in a
supervised framework; and (d) verify the portability of our methodology to different
languages.
4.1.1 Results. Table 3 reports the results of the individual kernels KBoW , KD, KColl, and
KPoS and their combinations Kwsd and K
?
wsd (the baselines for the tasks are reported in
Table 5). In our experiments, the parameters n and ? (see Equation (5)) are optimized
by five-fold cross-validation. For KnColl, we obtained the best results with n = 2 and
? = 0.5. For KnPoS, n = 3 and ? ? 0. The domain cardinality k
? was set to 50. Table 4
shows the performance of the syntagmatic kernel in different configurations: hard and
soft matching. As a baseline, we report the result of a standard approach consisting of
explicit bigrams and trigrams of words and part-of-speech tags around the words to
be disambiguated (Yarowsky 1994). We evaluated the impact of the domain kernel on
the overall performance by comparing the learning curves of K?wsd and Kwsd on the four
lexical-sample tasks. Figure 1 shows the results of our experiments. The points of the
learning curves are obtained by sampling the same percentage of training examples for
Table 2
Description of the lexical-sample tasks of Senseval-3.
Task #w mean polysemy #train #test #unlab
Catalan 27 3.11 4,469 2,253 23,935
English 57 6.47 7,860 3,944 -
Italian 45 6.30 5,145 2,439 74,788
Spanish 46 3.30 8,430 4,195 61,252
522
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
Table 3
The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, and
Spanish lexical-sample tasks of Semeval-3.
Kernel Catalan English Italian Spanish
KBoW 81.3 63.7 43.3 78.2
KD 85.2 65.5 44.5 84.4
KColl 84.2 68.5 54.0 83.6
KPoS 79.6 64.0 44.4 79.5
Kwsd 85.2 69.7 53.1 84.2
K?wsd 89.0 73.3 61.3 88.2
Table 4
Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
Method Catalan English Italian Spanish
Bigrams and trigrams 82.6 67.3 51.0 81.9
Hard matching 83.8 67.7 51.9 82.9
Soft matching (WordNet) - 67.3 51.3 -
Soft matching (Domain proximity) 84.2 68.5 54.0 83.6
each word. Finally, Table 5 summarizes the results we obtained, providing a comparison
with the state of the art.
4.1.2 Discussion. Table 3 shows that domain information and syntagmatic information
are crucial for WSD, and their combination significantly outperforms the individual
kernels, showing the effectiveness of the kernel combination method.
In addition, the domain kernel KD outperforms the bag-of-words kernel KBoW ,
and the composite kernel K?wsd that makes use of domain information outperforms the
one Kwsd based only on the labeled training data, demonstrating our assumption (see
Section 3).
Table 4 shows that the syntagmatic kernel outperforms the baseline (bigrams and
trigrams) in any configuration (hard-/soft-matching). The soft-matching criteria further
improve the classification performance. It is interesting to note that the domain proxim-
ity obtained better results thanWordNet synonymy (note that we do not have a Catalan
or a Spanish WordNet). The different results observed for Italian and English using
the domain proximity soft-matching criterion are probably due to the small size of the
unlabeled English corpus.
Figure 1 shows that K?wsd outperforms Kwsd on all lexical sample tasks, even with a
small number of examples. It is worth noting, as reported in Table 5, that K?wsd achieves
the same performance as Kwsd using about half of the labeled training data. This result
shows that the proposed semi-supervised learning approach consisting of acquiring
domain models from unlabeled corpora is effective, as it allows us to drastically reduce
the amount of labeled training data and provide a viable solution for the knowledge
acquisition bottleneck problem in WSD.
To the best of our knowledge, K?wsd turns out to be the best system for all the tested
tasks of Senseval-3, further improving the state of the art by 0.4% to 8.2% for English
and Italian, respectively. Finally, we have demonstrated the language independency
523
Computational Linguistics Volume 35, Number 4
Figure 1
From left to right, top to bottom, learning curves for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
of our approach. The DMs have been acquired for different languages from different
unlabeled corpora by adopting exactly the same methodology, without requiring any
external lexical resource or ad hoc rule.
4.2 All-Words Task
Encouraged by the excellent results obtained on the lexical-sample tasks, we evaluated
our approach on the all-words task, in which a very small amount of labeled training
Table 5
Comparative evaluation on the lexical sample tasks.
Task MF Agreement BEST Kwsd K
?
wsd DM+ % of training
Catalan 66.3 93.1 85.2 85.2 89.0 3.8 46
English 55.2 67.3 72.9 69.7 73.3 3.6 54
Italian 18.0 89.0 53.1 53.1 61.3 8.2 51
Spanish 67.7 85.3 84.2 84.2 88.2 4.0 50
Columns report: theMost Frequent baseline, the inter-annotator agreement, the F1 of the best system
at Senseval-3, the F1 of Kwsd, the F1 of K
?
wsd, DM+ (the improvement due to DM, i.e., K
?
wsd ? Kwsd),
and the percentage of sense-tagged examples required by K?wsd to achieve the same performance
as Kwsd with full training.
524
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
Table 6
The performance (F1) of the basic kernels and composite kernels on the English all-words task of
Senseval-3.
basic kernels composite kernels
KbncD K
sem
D KBoW KPoS KColl Kwsd K?
bnc
wsd K?
sem
wsd
F1 63.0 63.2 63.2 63.4 64.0 64.4 65.0 65.2
data is typically available. We performed the evaluation on the English all-words task
of Senseval-3 (Snyder and Palmer 2004). The test set was extracted from twoWall Street
Journal articles and one text from the Brown Corpus. The test set consists of 945 words
(2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses. The inter-
annotator agreement rate in the preparation of the corpus was approximately 72.5%.
Themost frequent (MF) baseline using the firstWordNet sense heuristic obtained 60.9%.
We have trained and tested the system exploiting the following resources: (1) Word-
Net 1.7.1 as sense repository; (2) SemCor,9 considering only those words appearing
in the Senseval-3 all-words data set?we extracted about 61,700 tagged examples that
constitute the only labeled training set exploited by the system; and (3) the BNC, from
which we extracted the unlabeled training data.
4.2.1 Results.We trained 734 word-expert classifiers on the SemCor corpus. The labeled
examples for each classifier range from a minimum of one example to a maximum
of 2,275 examples. We return a random sense for those words that have no training
examples in SemCor.10 We have acquired two DMs, one from the BNC (i.e., K?bncD ; the
same we used in the lexical-sample task) and one from SemCor (i.e., K?semD ), obtaining a
slightly better performance with the latter.
Table 6 shows the performance of the individual kernels KBoW , KD, KColl, and KPoS,
and their composite kernels Kwsd, K?
bnc
D and K?
sem
D .
Since for 210 words in the test set we have no training examples, to better under-
stand the results obtained, we performed an evaluation on the subset of the test set for
which at least one training example is available in SemCor. Evaluating only on these
words the performance increases from 65.2% to 70.0%, and the most frequent baseline
becomes 65.7%. Tables 7 and 8 present a more detailed analysis that considers results
grouped according to the amount of training available and the mean polysemy of the
words in the test set, excluding from the data set the monosemous words. Table 7 shows
the results (F1) of K?semwsd at different ranges of polysemy. Table 8 presents the results (F1)
of K?semwsd on those words that have a given number of training examples. This evaluation
is limited to the best composite kernel K?semwsd.
4.2.2 Discussion. We compared our approach with the three best systems that par-
ticipated in the English all-words task of Senseval-3. The best system (Decadt et al
2004) has comparable performance (65.2) to ours; however, it uses a larger training set
composed of 563,129 sense-tagged words. The training corpus was built by merging
9 Texts semantically annotated with WordNet 1.6 senses (created at Princeton University), and
automatically mapped to WordNet 1.7, WordNet 1.7.1, and WordNet 2.0. Downloadable from
http://www.cs.unt.edu/?rada/downloads.html.
10 Note that for these words the WordNet first sense is not necessarily the most frequent sense.
525
Computational Linguistics Volume 35, Number 4
Table 7
The performance (F1) of K?semwsd at different ranges of polysemy. Most Frequent baseline (MF) is
also reported.
Range of polysemy
2?5 6?10 11?15 16?20 21?25 26?30 31+
K?semwsd 73.2 61.4 59.1 33.8 55.2 50.2 37.3
MF 70.0 53.4 56.4 25.7 47.2 39.0 21.7
Table 8
The performance (F1) of K?semwsd on words with a given number of training examples. Most
Frequent baseline (MF) and mean polysemy for each partition are also reported.
Range of training examples
1?10 11?50 51?100 101?200 201+
K?semwsd 76.1 70.8 54.2 67.4 60.0
MF 73.5 66.4 49.4 63.2 53.0
Mean polysemy 3 5 7 9 15
SemCor, and English lexical-sample and all-words data sets taken from all the previous
editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored sec-
ond (64.6). The dimension of their training set is comparable to ours; however, they also
use additional information drawn from WordNet to derive semantic generalizations
using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using
a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind
Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks).
The small difference between the two domain models seems to indicate that a
limited amount of unlabeled data is sufficient to improve the overall performance,
and the use of unlabeled data taken from the training set helps to slightly improve
the overall performance. However, the domain model can be acquired from a different
corpus (e.g., the BNC) without significantly affecting the overall performance.
Finally, the results reported in Tables 7 and 8 show that our approach is able to dis-
ambiguate with good accuracy (F1 = 76%) words with a number of training examples
that ranges from 1 to 10, outperforming the most frequent baseline by 3%. This is an
interesting result given the extremely small number of training examples available. On
the other hand, the more training is available for a given word, the more polysemous
that word is. Nevertheless, the algorithm always outperforms the baseline and has a
more significant difference for increasing values of the mean polysemy (from 3% to
16%). These results, together with the ones obtained in the lexical sample tasks, show
that the domain kernel is able to boost the overall performance when little training data
are available, as well as with enough training data. The benefit is evenmore pronounced
for the latter case, even though the disambiguation task is more complex due to the high
polysemy of highly frequent words.
5. Conclusions
This article summarizes the results of a word expert semi-supervised algorithm for
WSD based on a combination of kernel functions. First, we evaluated our methodology
526
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
on four lexical-sample tasks of Senseval-3, significantly improving the state of the art
for all of them. In particular, we demonstrated that using external knowledge inside a
supervised framework is a viable methodology to reduce the amount of training data
required for learning. In our approach, the external knowledge is represented by means
of domain models automatically acquired from corpora in a totally unsupervised way.
Then, we applied the method so defined to the English all-words task of Senseval-
3, achieving state-of-the-art performance while requiring less labeled training data
compared to the other systems we have found in the literature.
Some slight improvement may be possible by exploiting syntactic information pro-
duced by a parser. In the framework of kernel methods, this expansion can be done by
adding a tree kernel (i.e., a kernel function that evaluates the similarity among parse
trees) to our composite kernel. However, the performance achieved is close to the upper
bound, if we consider the inter-annotator agreement as an indication of the upper-
bound performance.
Finally, we think that our semi-supervised approach is at the moment an effective
solution for developing a sense-tagging system. Indeed, we tested the system on the
English lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance
(Pradhan et al 2007). Therefore, we plan to make available an optimized version of
our system, and to exploit it for ontology learning, textual entailment, and information
retrieval.
Acknowledgments
Claudio Giuliano was supported by the
X-Media project (www.x-media-project.org),
sponsored by the European Commission as
part of the Information Society Technologies
(IST) programme under EC grant IST-FP6-
026978. Alfio Massimiliano Gliozzo and
Carlo Strapparava were supported by the
ONTOTEXT project, sponsored by the
Autonomous Province of Trento under the
FUP-2004 research program.
References
Boser, Bernhard, Isabelle Guyon, and
Vladimir Vapnik. 1992. A training
algorithm for optimal margin classifier.
In Proceedings of the 5th Annual ACM
Workshop on Computational Learning
Theory, pages 144?152, Pittsburgh, PA.
Cancedda, Nicola, Eric Gaussier, Cyril
Goutte, and Jean-Michel Renders. 2003.
Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059?1082.
Chang, Chih-Chung and Chih-Jen Lin,
2001. LIBSVM: A library for support vector
machines. Software available at www.csie.
ntu.edu.tw/?cjlin/libsvm.
Chapelle, Olivier, Bernhard Scho?lkopf, and
Alexander Zien. 2006. Semi-Supervised
Learning. MIT Press, Cambridge, MA.
Cristianini, Nello and John Shawe-Taylor.
2000. An Introduction to Support Vector
Machines. Cambridge University Press.
Decadt, Bart, Veronique Hoste, Walter
Daelemans, and Antal van den Bosch.
2004. GAMBL, genetic algorithm
optimization of memory-based WSD. In
Proceedings of Senseval-3, pages 108?112,
Barcelona.
Deerwester, Scott, Susan Dumais, George
Furnas, Thomas Landauer, and Richard
Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American
Society of Information Science, 41:391?407.
Giuliano, Claudio, Alfio Gliozzo, and Carlo
Strapparava. 2006. Syntagmatic kernels: A
word sense disambiguation case study. In
In Proceedings of the EACL-06 Workshop on
Learning Structured Information in Natural
Language Applications, pages 57?63, Trento.
Giuliano, Claudio, Alberto Lavelli, and
Lorenza Romano. 2006. Exploiting shallow
linguistic information for relation
extraction from biomedical literature. In
Proceedings of the Eleventh Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-2006),
pages 401?408, Trento.
Gliozzo, Alfio, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for
word sense disambiguation. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-05),
pages 403?410, Ann Arbor, MI.
Gliozzo, Alfio, Carlo Strapparava, and
Ido Dagan. 2004. Unsupervised and
supervised exploitation of semantic
527
Computational Linguistics Volume 35, Number 4
domains in lexical disambiguation.
Computer Speech and Language,
18(3):275?299.
Lodhi, Huma, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal
of Machine Learning Research, 2(3):419?444.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings of Senseval-3: Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text.
Barcelona.
Mihalcea, Rada and Ehsanul Faruque. 2004.
SenseLearner: Minimally supervised WSD
for all words in open text. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 155?158, Barcelona.
Moschitti, Alessandro. 2004. A study on
convolution kernels for shallow statistic
parsing. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 335?342,
Barcelona.
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007.
Semeval-2007 task-17: English lexical
sample, SRL and all words. In Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
pages 87?92, Prague.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Saunders, Craig, Hauke Tschach, and John
Shawe-Taylor. 2002. Syllables and other
string kernel extensions. In Proceedings of
19th International Conference on Machine
Learning (ICML02), pages 530?537, Sydney.
Scho?lkopf, Bernhard and Alexander Smola.
2002. Learning with Kernels. MIT Press,
Cambridge, MA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 41?43, Barcelona.
Strapparava, Carlo, Alfio Gliozzo, and
Claudio Giuliano. 2004. Pattern abstraction
and term similarity for word sense
disambiguation: Irst at senseval-3. In
Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic
Analysis of Text, pages 229?234, Barcelona.
Vapnik, Vladimir N. 1999. The Nature of
Statistical Learning Theory (Information
Science and Statistics). Springer, Berlin.
Wong, S. K. M., Wojciech Ziarko, and
Patrick C. N. Wong. 1985. Generalized
vector space model in information
retrieval. In Proceedings of the 8th ACM
SIGIR Conference, pages 18?25, Montreal.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics (ACL 1994), pages 88?95,
Las Cruces, NM.
Yuret, Deniz. 2004. Some experiments with a
naive Bayes WSD system. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 265?268, Barcelona.
Zhao, Shubin and Ralph Grishman. 2005.
Extracting relations with integrated
information using kernel methods. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL 2005), pages 419?426, Ann Arbor, MI.
528
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 85?92,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Technologies in IBM WatsonTM
Alfio Gliozzo
IBM Watson Research Center
Yorktown Heights, NY 10598
gliozzo@us.ibm.com
Or Biran
Columbia University
New York, NY 10027
orb@cs.columbia.edu
Siddharth Patwardhan
IBM Watson Research Center
Yorktown Heights, NY 10598
siddharth@us.ibm.com
Kathleen McKeown
Columbia University
New York, NY 10027
kathy@cs.columbia.edu
Abstract
This paper describes a seminar course de-
signed by IBM and Columbia University
on the topic of Semantic Technologies,
in particular as used in IBM WatsonTM
? a large scale Question Answering sys-
tem which famously won at Jeopardy! R?
against two human grand champions. It
was first offered at Columbia University
during the 2013 spring semester, and will
be offered at other institutions starting in
the fall semester. We describe the course?s
first successful run and its unique features:
a class centered around a specific indus-
trial technology; a large-scale class project
which student teams can choose to par-
ticipate in and which serves as the ba-
sis for an open source project that will
continue to grow each time the course is
offered; publishable papers, demos and
start-up ideas; evidence that the course can
be self-evaluating, which makes it poten-
tially appropriate for an online setting; and
a unique model where a large company
trains instructors and contributes to creat-
ing educational material at no charge to
qualifying institutions.
1 Introduction
In 2007, IBM Research took on the grand chal-
lenge of building a computer system that can per-
form well enough on open-domain question an-
swering to compete with champions at the game of
Jeopardy! In 2011, the open-domain question an-
swering system dubbed Watson beat the two high-
est ranked players in a two-game Jeopardy! match.
To be successful at Jeopardy!, players must re-
tain enormous amounts of information, must have
strong language skills, must be able to understand
precisely what is being asked, and must accurately
determine the likelihood they know the right an-
swer. Over a four year period, the team at IBM
developed the Watson system that competed on
Jeopardy! and the underlying DeepQA question
answering technology (Ferrucci et al, 2010). Wat-
son played many games of Jeopardy! against cel-
ebrated Jeopardy! champions and, in games tele-
vised in February 2011, won against the greatest
players of all time, Ken Jennings and Brad Rutter.
DeepQA has applications well beyond Jeop-
ardy!, however. DeepQA is a software architec-
ture for analyzing natural language content in both
questions and knowledge sources. DeepQA dis-
covers and evaluates potential answers and gathers
and scores evidence for those answers in both un-
structured sources, such as natural language doc-
uments, and structured sources such as relational
databases and knowledge bases. Figure 1 presents
a high-level view of the DeepQA architecture.
DeepQA utilizes a massively parallel, component-
based pipeline architecture (Ferrucci, 2012) which
uses an extensible set of structured and unstruc-
tured content sources as well as a broad range of
pluggable search and scoring components that al-
low integration of many different analytic tech-
niques. Machine Learning techniques are used to
learn the weights for each scoring component in
order to combine them into a single final score.
Watson components include a large variety of state
of the art solutions originating in the fields of Nat-
ural Language Processing (NLP), Machine Learn-
ing (ML), Information Retrieval (IR), Semantic
Web and Cloud Computing. IBM is now aggres-
sively investing in turning IBM Watson from a re-
search prototype to an industry level highly adapt-
able system to be applied in dozens of business ap-
85
Figure 1: Overview of the DeepQA architecture
plications ranging from healthcare to finance (Fer-
rucci et al, 2012).
Finding that particular combination of skills in
the entry-level job market is hard: in many cases
students have some notion of Machine Learning
but are not strong in Natural Language Processing;
in other cases they have background in Knowledge
Management and some of the basics of Semantic
Web, but lack an understanding of statistical mod-
els and Machine Learning. In most cases semantic
integration is not a topic of interest, and so un-
derstanding sophisticated platforms like Apache
UIMATM (Ferrucci and Lally, 2004) is a chal-
lenge. Learning how to develop the large scale in-
frastructure and technology needed for IBM Wat-
son prepares students for the real-world challenges
of large-scale natural language projects that are
common in industry settings and which students
have little experience with before graduation.
Of course, IBM is interested in hiring entry-
level students as a powerful way of scaling Wat-
son. Therefore, it has resolved to start an ed-
ucational program focused on these topics. Ini-
tially, tutorials were given at scientific conferences
(NAACL, ISWC and WWW, among others), uni-
versities and summer schools. The great number
of attendees (usually in the range of 50 to 150)
and strongly positive feedback received from the
students was a motivation to transform the didac-
tic material collected so far into a full graduate-
level course, which has been offered for the first
time at Columbia University. The course (which
is described in the rest of this paper) received very
positive evaluations from the students and will be
used as a template to be replicated by other part-
ner universities in the following year. Our ultimate
goal is to develop high quality didactic material
for an educational curriculum that can be used by
interested universities and professors all over the
world.
2 Syllabus and Didactic Material
The syllabus1 is divided equally between classes
specifically on the Watson system, its architec-
ture and technologies used within it, and classes
on more general topics that are relevant to these
technologies. In particular, background classes on
Natural Language Processing; Distributional Se-
mantics; the Semantic Web; Domain Adaptation
and the UIMA framework are essential for under-
standing the Watson system and producing suc-
cessful projects.
The course at Columbia included four lectures
by distinguished guest speakers from IBM, which
were advertised to the general Columbia commu-
nity as open talks. Instead of exams, the course
included two workshop-style presentation days:
one at the mid term and another at the end of the
1The syllabus is accessible on line http://www.
columbia.edu/?ag3366
86
course. During these workshops, all student teams
gave presentations on their various projects. At the
mid-term workshop, teams presented their project
idea and timeline, as well as related work and the
state-of-the-art of the field. At the final workshop,
they presented their completed projects, final re-
sults and demos. This workshop was also made
open to the Columbia community and in particu-
lar to faculty and affiliates interested in start-ups.
The workshops will be discussed in further detail
in the following sections. The syllabus is briefly
detailed here.
? Introduction: The Jeopardy! Challenge
The motivation behind Watson, the task and
its challenges (Prager et al, 2012; Tesauro et
al., 2012; Lewis, 2012).
? The DeepQA Architecture Chu-Carroll et
al. (2012b), Ferrucci (2012), Chu-Carroll et
al. (2012a), Lally et al (2012).
? Natural Language Processing Background
Pre-processing, tokenization, POS tagging,
named entity recognition, syntactic parsing,
semantic role labeling, word sense disam-
biguation, evaluation best practices and met-
rics.
? Natural Language Processing in Watson
Murdock et al (2012a), McCord et al (2012).
? Structured Knowledge in Watson Murdock
et al (2012b), Kalyanpur et al (2012), Fan et
al. (2012).
? Semantic Web OWL, RDF, Semantic Web
resources.
? Domain Adaptation Ferrucci et al (2012).
? UIMA The UIMA framework, Annotators,
Types, Descriptors, tools. Hands-on exercise
with the class project architecture (Epstein et
al., 2012).
? Midterm Workshop Presentations of each
team?s project idea and their research into re-
lated work and the state of the art.
? Distributional Semantics Miller et al
(2012), Gliozzo and Isabella (2005).
? Machine Learning and Strategy in Watson
? What Watson Tells Us About Cognitive
Computing
? Final Workshop Presentations of each
team?s final project implementation, evalua-
tion, demo and future plans.
3 Watson-like Architecture for Projects
The goal of the class projects was for the stu-
dents to learn to design and develop language tech-
nology components in an environment very sim-
ilar to IBM?s Watson architecture. We provided
the students with a plug-in framework for seman-
tic search, into which they could integrate their
project code. Student projects will be described
in the following section. This section details the
framework that was made available to the students
in order to develop their projects.
Like the Watson system, the project framework
for this class was built on top of Apache UIMA
(Ferrucci and Lally, 2004)2 ? an open-source
software architecture for building applications that
handle unstructured information.
The Watson system makes extensive use of
UIMA to enable interoperability and scale-out of a
large question answering system. The architecture
(viz., DeepQA) of Watson (Ferrucci, 2012) defines
several high-level ?stages? of analysis in the pro-
cessing pipeline, such as Question and Topic Anal-
ysis, Primary Search, Candidate Answer Genera-
tion, etc. Segmentation of the system into high-
level stages enabled a group of 25 researchers at
IBM to independently work on different aspects
of the system with little overhead for interoper-
ability and system integration. Each stage of the
pipeline clearly defined the inputs and outputs ex-
pected of components developed for that particu-
lar stage. The researchers needed only to adhere
to these input/output requirements for their indi-
vidual components to easily integrate them into
the system. Furthermore, the high-level stages in
Watson, enabled massive scale-out of the system
through the use of the asynchronous scaleout ca-
pability of UIMA-AS.
Using the Watson architecture for inspitration,
we developed a semantic search framework for the
class projects. As shown in Figure 2, the frame-
work consists of a UIMA pipeline that has several
high-level stages (similar to those of the Watson
system):
2http://uima.apache.org
87
Figure 2: Overview of the class project framework
1. Query Analysis
2. Primary Document Search
3. Structured Data Search
4. Query Expansion
5. Expanded Query Analysis
6. Secondary Document Search
The input to this system is provided by a Query
Collection Reader, which reads a list of search
queries from a text file. The Query Collec-
tion Reader is a UIMA ?collection reader? that
reads the text queries into memory data struc-
tures (UIMA CAS structures) ? one for each
text query. These UIMA CASes flow through the
pipeline and are processed by the various process-
ing stages. The processing stages are set up so
that new components designed to perform the task
of each processing stage can easily be added to the
pipeline (or existing components easily modified).
The expected inputs and outputs of components in
each processing stage are clearly defined, which
makes the task of the team building the component
simpler: they no longer have to deal with man-
aging data structures and are spared the overhead
of converting from and into formats of data ex-
changed between various components. All of the
overhead is handled by UIMA. Furthermore, some
of the processing stages generate new CAS struc-
tures and the flow of all the UIMA CAS structures
through this pipeline is controlled by a ?Flow Con-
troller? designed by us for this framework.
The framework was made available to each of
the student teams, and their task was to build
their project by extending this framework. Even
though we built the framework to perform seman-
tic search over a text corpus, many of the teams
in this course had projects that went far beyond
just semantic search. Our hope was that each team
would be able to able independently develop inter-
esting new components for the processing stages
of the pipeline, and at the end of the course we
would be able to merge the most interesting com-
ponents to create a single useful application. In the
following section, we describe the various projects
undertaken by the student teams in the class, while
Section 5 discusses the integration of components
from student projects and the demo application
that resulted from the integrated system.
4 Class Projects
Projects completed for this course fall into three
types: scientific projects, where the aim is to
produce a publishable paper; integrated projects,
where the aim is to create a component that will be
integrated into the class open-source project; and
independent demo projects, where the aim is to
produce an independent working demo/prototype.
The following section describes the integrated
projects briefly.
4.1 Selected Project Descriptions
As described in section 3, the integrated class
project is a system with an architecture which, al-
though greatly simplified, is reminiscent of Wat-
son?s. While originally intended to be simply a
semantic search tool, some of the student teams
created additional components which resulted in
a full question answering system. Those projects
88
as well as a few other related ones are described
below.
Question Categorization: Using the DBPedia
ontology (Bizer et al, 2009) as a semantic
type system, this project classifies questions
by their answer type. It can be seen as a sim-
plified version of the question categorization
system in Watson. The classification is based
on a simple bag-of-words approach with a
few additional features.
Answer Candidate Ranking: Given the answer
type as well as additional features derived by
the semantic search component, this project
uses regression to rank the candidate an-
swers which themselves come from semantic
search.
Twitter Semantic Search: Search in Twitter is
difficult due to the huge variations among
tweets in lexical terms, spelling and style, and
the limited length of the tweets. This project
employs LSA (Landauer and Dumais, 1997)
to cluster similar tweets and increase search
accuracy.
Fine-Grained NER in the Open Domain: This
project uses DBPedia?s ontology as a type
system for named entities of type Person.
Given results from a standard NER system,
it attempts to find the fine-grained classifica-
tion of each Person entity by finding the most
similar type. Similarity is computed using
traditional distributional methods, using the
context of the entity and the contexts of each
type, collected from Wikipedia.
News Frame Induction: Working with a large
corpus of news data collected by Columbia
Newsblaster, this team used the Machine
Linking API to tag entities with semantic
types. From there, they distributionally col-
lected ?frames? prevalent in the news do-
main such as ?[U.S President] meeting with
[British Prime Minister]?.
Other projects took on problems such as Sense
Induction, NER in the Biomedical domain, Se-
mantic Role Labeling, Semantic Video Search,
and a mobile app for Event Search.
5 System Integration and Demonstration
The UIMA-based architecture described in section
3 allows us to achieve a relatively easy integra-
tion of different class projects, independently de-
veloped by different teams, in a common archi-
tecture and expose their functionality with a com-
bined class project demo. The demo is a collab-
oratively developed semantic search engine which
is able to retrieve knowledge from structured data
and visualize it for the user in a very concise way.
The input is a query; it can be a natural language
question or simply a set of keywords. The output
is a set of entities and their relations, visualized
as an entity graph. Figure 3 shows the results of
the current status of our class project demo on the
following Jeopardy! question.
This nation of 200 million has fought
small independence movements like
those in Aceh and East Timor.
The output is a set of DBPedia entities related to
the question, grouped by Type (provided by the
DBPedia ontology). The correct answer, ?Indone-
sia?, is among the candidate entities of type Place.
Note that only answers of type Place and Agent
have been selected: this is due to the question cate-
gorization component, implemented by one of the
student teams, that allows us to restrict the gener-
ated answer set to those answers having the right
types.
The demo will be hosted for one year fol-
lowing the end of the course at http://
watsonclass.no-ip.biz. Our goal is to
incrementally improve this demo, leveraging any
new projects developed in future versions of the
course, and to build an open source software com-
munity involving students taking the course.
6 Evaluation
The course at Columbia drew a relatively large au-
dience. A typical size for a seminar course on a
special topic is estimated at 15-20 students, while
ours drew 35. The vast majority were Master?s stu-
dents; there were also three PhD students and five
undergraduates.
During the student workshops, students were
asked to provide grades for each team?s presen-
tation and project. After the instructor indepen-
dently gave his own grades, we looked at the cor-
relation between the average grades given by the
students and those give by the instructor. While
89
Figure 3: Screenshot of the project demo
90
Team 1 2 3 4 5 6 7 8 9 10 11
Instructor?s grade B+ B C+ A- B- A+ B B- B+ A B-
TA?s grade B+ B B A B- A B- B+ B+ A C+
Class? average grade B/B+ B+/A- B/B+ A- B/B+ A- B+ A-/A B+/A- A-/A B/B+
Table 1: Grades assigned to class projects
the students tended to be more ?generous? (their
average grade for each team was usually half a
grade above the instructor?s), the agreement was
quite high. Table 1 shows the grades given by the
instructor, the teaching assistant and the class av-
erage for the midterm workshop.
Feedback about the course from the students
was very good. Columbia provides electonic
course evaluations to the students which are com-
pletely optional. Participation in the evaluation for
this course was just under 50% in the midterm
evaluation and just over 50% in the final eval-
uation. The scores (all in the 0-5 range) given
by the students in relevant categories were quite
high: ?Overall Quality? got an average score of
4.23, ?Amount Learned? got 4, ?Appropriateness
of Workload? 4.33 and ?Fairness of Grading Pro-
cess? got 4.42.
The course resulted in multiple papers that are
or will soon be under submission, as well as a few
projects that may be developed into start-ups. Al-
most all student teams agreed to share their code
in an open source project that is currently being
set up, and which will include the current question
answering and semantic search system as well as
additional side projects.
7 Conclusion
We described a course on the topic of Semantic
Technologies and the IBM Watson system, which
features a diverse curriculum tied together by its
relevance to an exciting, demonstrably successful
real-world system. Through a combined architec-
ture inspired by Watson itself, the students get the
experience of developing an NLP-heavy compo-
nent with specifications mandated by the larger
architecture, which requires a combination of re-
search and software engineering skills that is com-
mon in the industry.
An exciting result of this course is that the
class project architecture and many of the student
projects are to be maintained as an open source
project which the students can, if they choose,
continue to be involved with. The repository and
community of this project can be expected to grow
each time the class is offered. Even after one class,
it already contains an impressive semantic search
system.
Feedback for this course from the students
was excellent, and many teams have achieved
their personal goals as stated at the beginning of
the semester, including paper submissions, opera-
tional web demos and mobile apps.
Our long term goal is to replicate this course in
multiple top universities around the world. While
IBM does not have enough resources to always
do this with its own researchers, it is instead go-
ing to provide the content material and the open
source code generated so far to other universities,
encouraging professors to teach the course them-
selves. Initially we will work on a pilot phase
involving only a restricted number of professors
and researchers that are already in collaboration
with IBM Research, and eventually (if the posi-
tive feedback we have seen so far is repeated in
the pilot phase) give access to the same content to
a larger group.
References
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia?
Crystallization Point for the Web of Data. Journal
of Web Semantics: Science, Services and Agents on
the World Wide Web, 7(3):154?165, September.
J. Chu-Carroll, J. Fan, B. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012a. Finding Nee-
dles in the Haystack: Search and Candidate Gener-
ation. IBM Journal of Research and Development,
56(3.4):6:1?6:12.
J. Chu-Carroll, J. Fan, N. Schlaefer, and W. Zadrozny.
2012b. Textual Resource Acquisition and Engineer-
ing. IBM Journal of Research and Development,
56(3.4):4:1?4:11.
E. Epstein, M. Schor, B. Iyer, A. Lally, E. Brown, and
J. Cwiklik. 2012. Making Watson Fast. IBM Jour-
nal of Research and Development, 56(3.4):15:1?
15:12.
J. Fan, A. Kalyanpur, D. Gondek, and D. Ferrucci.
2012. Automatic Knowledge Extraction from Doc-
uments. IBM Journal of Research and Development,
56(3.4):5:1?5:10.
91
D. Ferrucci and A. Lally. 2004. UIMA: an Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan,
D. Gondek, A. Kalyanpur, A. Lally, J. W. Murdock,
E. Nyberg, J. Prager, N. Schlaefer, and C. Welty.
2010. Building Watson: An Overview of the
DeepQA project. AI magazine, 31(3):59?79.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and
E. Mueller. 2012. Watson: Beyond Jeopardy. Arti-
ficial Intelligence (in press).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1:1?1:15.
A. Gliozzo and T. Isabella. 2005. Semantic Domains
in Computational Linguistics. Technical report.
A. Kalyanpur, B. Boguraev, S. Patwardhan, J. W.
Murdock, A. Lally, C. Welty, J. Prager, B. Cop-
pola, A. Fokoue-Nkoutche, L. Zhang, Y. Pan, and
Z. Qiu. 2012. Structured Data and Inference in
DeepQA. IBM Journal of Research and Develop-
ment, 56(3.4):10:1?10:14.
A. Lally, J. Prager, M. McCord, B. Boguraev, S. Pat-
wardhan, J. Fan, P. Fodor, and J. Chu-Carroll. 2012.
Question Analysis: How Watson Reads a Clue. IBM
Journal of Research and Development, 56(3.4):2:1?
2:14.
T. Landauer and S. Dumais. 1997. A Solution to
Plato?s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction and Representation
of Knowledge. Psychological Review, 104(2):211?
240.
B. Lewis. 2012. In the Game: The Interface between
Watson and Jeopardy! IBM Journal of Research and
Development, 56(3.4):17:1?17:6.
M. McCord, J. W. Murdock, and B. Boguraev. 2012.
Deep Parsing in Watson. IBM Journal of Research
and Development, 56(3.4):3:1?3:15.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych.
2012. Using Distributional Similarity for Lexical
Expansion in Knowledge-based Word Sense Disam-
biguation. In Proceedings of the International Con-
ference on Computational Linguistics, pages 1781?
1796, Mumbai, India, December.
J. W. Murdock, J. Fan, A. Lally, H. Shima, and
B. Boguraev. 2012a. Textual Evidence Gathering
and Analysis. IBM Journal of Research and Devel-
opment, 56(3.4):8:1?8:14.
J. W. Murdock, A. Kalyanpur, C. Welty, J. Fan, D. Fer-
rucci, D. Gondek, L. Zhang, and H. Kanayama.
2012b. Typing Candidate Answers Using Type Co-
ercion. IBM Journal of Research and Development,
56(3.4):7:1?7:13.
J. Prager, E. Brown, and J. Chu-Carroll. 2012. Spe-
cial Questions and Techniques. IBM Journal of Re-
search and Development, 56(3.4):11:1?11:13.
G. Tesauro, D. Gondek, J. Lenchner, J. Fan, and
J. Prager. 2012. Simulation, Learning, and Op-
timization Techniques in Watson?s Game Strate-
gies. IBM Journal of Research and Development,
56(3.4):16:1?16:11.
92
Proceedings of the TextGraphs-8 Workshop, pages 6?10,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
JoBimText Visualizer:
A Graph-based Approach to Contextualizing Distributional Similarity
Alfio Gliozzo1 Chris Biemann2 Martin Riedl2
Bonaventura Coppola1 Michael R. Glass1 Matthew Hatem1
(1) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
(2) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
{gliozzo,mrglass,mhatem}@us.ibm.com coppolab@gmail.com
{biem,riedl}@cs.tu-darmstadt.de
Abstract
We introduce an interactive visualization com-
ponent for the JoBimText project. JoBim-
Text is an open source platform for large-scale
distributional semantics based on graph rep-
resentations. First we describe the underly-
ing technology for computing a distributional
thesaurus on words using bipartite graphs of
words and context features, and contextualiz-
ing the list of semantically similar words to-
wards a given sentential context using graph-
based ranking. Then we demonstrate the ca-
pabilities of this contextualized text expan-
sion technology in an interactive visualization.
The visualization can be used as a semantic
parser providing contextualized expansions of
words in text as well as disambiguation to
word senses induced by graph clustering, and
is provided as an open source tool.
1 Introduction
The aim of the JoBimText1 project is to build a
graph-based unsupervised framework for computa-
tional semantics, addressing problems like lexical
ambiguity and variability, word sense disambigua-
tion and lexical substitutability, paraphrasing, frame
induction and parsing, and textual entailment. We
construct a semantic analyzer able to self-adapt to
new domains and languages by unsupervised learn-
ing of semantics from large corpora of raw text. At
the moment, this analyzer encompasses contextual-
ized similarity, sense clustering, and a mapping of
senses to existing knowledge bases. While its pri-
mary target application is functional domain adap-
tation of Question Answering (QA) systems (Fer-
1http://sf.net/projects/jobimtext/
rucci et al, 2013), output of the semantic analyzer
has been successfully utilized for word sense disam-
biguation (Miller et al, 2012) and lexical substitu-
tion (Szarvas et al, 2013). Rather than presenting
the different algorithms and technical solutions cur-
rently implemented by the JoBimText community in
detail, in this paper we will focus on available func-
tionalities and illustrate them using an interactive vi-
sualization.
2 Underlying Technologies
While distributional semantics (de Saussure, 1959;
Harris, 1951; Miller and Charles, 1991) and the
computation of distributional thesauri (Lin, 1998)
has been around for decades, its full potential has yet
to be utilized in Natural Language Processing (NLP)
tasks and applications. Structural semantics claims
that meaning can be fully defined by semantic oppo-
sitions and relations between words. In order to per-
form a reliable knowledge acquisition process in this
framework, we gather statistical information about
word co-occurrences with syntactic contexts from
very large corpora. To avoid the intrinsic quadratic
complexity of the similarity computation, we have
developed an optimized process based on MapRe-
duce (Dean and Ghemawat, 2004) that takes advan-
tage of the sparsity of contexts, which allows scal-
ing the process through parallelization. The result of
this computation is a graph connecting the most dis-
criminative contexts to terms and explicitly linking
the most similar terms. This graph represents local
models of semantic relations per term rather than a
model with fixed dimensions. This representation
departs from the vector space metaphor (Schu?tze,
1993; Erk and Pado?, 2008; Baroni and Zamparelli,
6
2010), commonly employed in other frameworks for
distributional semantics such as LSA (Deerwester et
al., 1990) or LDA (Blei et al, 2003).
The main contribution of this paper is to de-
scribe how we operationalize semantic similarity in
a graph-based framework and explore this seman-
tic graph using an interactive visualization. We de-
scribe a scalable and flexible computation of a dis-
tributional thesaurus (DT), and the contextualization
of distributional similarity for specific occurrences
of language elements (i.e. terms). For related works
on the computation of distributional similarity, see
e.g. (Lin, 1998; Lin and Dyer, 2010).
2.1 Holing System
To keep the framework flexible and abstract with re-
spect to the pre-processing that identifies structure
in language material, we introduce the holing op-
eration, cf. (Biemann and Riedl, 2013). It is ap-
plied to observations over the structure of text, and
splits these observations into a pair of two parts,
which we call the ?Jo? and the ?Bim?2. All JoBim
pairs are maintained in the bipartite First-Order Jo-
Bim graph TC(T,C,E) with T set of terms (Jos),
C set of contexts (Bims), and e(t, c, f) ? E edges
between t ? T , c ? C with frequency f . While
these parts can be thought of as language elements
referred to as terms, and their respective context fea-
tures, splits over arbitrary structures are possible (in-
cluding pairs of terms for Jos), which makes this
formulation more general than similar formulations
found e.g. in (Lin, 1998; Baroni and Lenci, 2010).
These splits form the basis for the computation of
global similarities and for their contextualization. A
Holing System based on dependency parses is illus-
trated in Figure 1: for each dependency relation, two
JoBim pairs are generated.
2.2 Distributed Distributional Thesaurus
Computation
We employ the Apache Hadoop MapReduce Fram-
work3, and Apache Pig4, for parallelizing and dis-
tributing the computation of the DT. We describe
this computation in terms of graph transformations.
2arbitrary names to emphasize the generality, should be
thought of as ?term? and ?context?
3http://hadoop.apache.org
4http://pig.apache.org/
Figure 1: Jos and Bims generated applying a dependency
parser (de Marneffe et al, 2006) to the sentence I suffered
from a cold and took aspirin. The @@ symbolizes the
hole.
Staring from the JoBim graph TC with counts as
weights, we first apply a statistical test5 to com-
pute the significance of each pair (t, c), then we only
keep the p most significant pairs per t. This consti-
tutes our first-order graph for Jos FOJO. In analogy,
when keeping the p most significant pairs per c, we
can produce the first-order graph for Bims FOBIM .
The second order similarity graph for Jos is defined
as SOJO(T,E) with Jos t1, t2 ? T and undirected
edges e(t1, t2, s) with similarity s = |{c|e(t1, c) ?
FOJO, e(t2, c) ? FOJO}|, which defines similar-
ity between Jos as the number of salient features
two Jos share. SOJO defines a distributional the-
saurus. In analogy, SOBIM is defined over the
shared Jos for pairs of Bims and defines similar-
ity of contexts. This method, which can be com-
puted very efficiently in a few MapReduce steps, has
been found superior to other measures for very large
datasets in semantic relatedness evaluations in (Bie-
mann and Riedl, 2013), but could be replaced by any
other measure without interfering with the remain-
der of the system.
2.3 Contextualization with CRF
While the distributional thesaurus provides the sim-
ilarity between pairs of terms, the fidelity of a par-
ticular expansion depends on the context. From the
term-context associations gathered in the construc-
tion of the distributional thesaurus we effectively
have a language model, factorized according to the
holing operation. As with any language model,
smoothing is critical to performance. There may be
5we use log-likelihood ratio (Dunning, 1993) or LMI (Evert,
2004)
7
many JoBim (term-context) pairs that are valid and
yet under represented in the corpus. Yet, there may
be some similar term-context pair that is attested in
the corpus. We can find similar contexts by expand-
ing the term arguments with similar terms. However,
again we are confronted with the fact that the simi-
larity of these terms depends on the context.
This suggests some technique of joint inference
to expand terms in context. We use marginal in-
ference in a conditional random field (CRF) (Laf-
ferty et al, 2001). A particular world, x is defined
as single, definite sequence of either original or ex-
panded words. The weight of the world, w(x) de-
pends on the degree to which the term-context as-
sociations present in this sentence are present in the
corpus and the general out-of-context similarity of
each expanded term to the corresponding term in the
original sentence. Therefore the probability associ-
ated with any expansion t for any position xi is given
by Equation 1. Where Z is the partition function, a
normalization constant.
P (xi = t) =
1
Z
?
{x | xi=t}
ew(x) (1)
The balance between the plausibility of an ex-
panded sentence according to the language model,
and its per-term similarity to the original sentence is
an application specific tuning parameter.
2.4 Word Sense Induction, Disambiguation
and Cluster Labeling
The contextualization described in the previous sub-
section performs implicit word sense disambigua-
tion (WSD) by ranking contextually better fitting
similar terms higher. To model this more explicitly,
and to give rise to linking senses to taxonomies and
domain ontologies, we apply a word sense induction
(WSI) technique and use information extracted by
IS-A-patterns (Hearst, 1992) to label the clusters.
Using the aggregated context features of the clus-
ters, the word cluster senses are assigned in con-
text. The DT entry for each term j as given in
SOJO(J,E) induces an open neighborhood graph
Nj(Vj , Ej) with Vj = {j?|e(j, j?, s) ? E) and Ej
the projection of E regarding Vj , consisting of sim-
ilar terms to j and their similarities, cf. (Widdows
and Dorow, 2002).
We cluster this graph using the Chinese Whispers
graph clustering algorithm (Biemann, 2010), which
finds the number of clusters automatically, to ob-
tain induced word senses. Running shallow, part-
of-speech-based IS-A patterns (Hearst, 1992) over
the text collection, we obtain a list of extracted IS-
A relationships between terms, and their frequency.
For each of the word clusters, consisting of similar
terms for the same target term sense, we aggregate
the IS-A information by summing the frequency of
hypernyms, and multiplying this sum by the number
of words in the cluster that elicited this hypernym.
This results in taxonomic information for labeling
the clusters, which provides an abstraction layer for
terms in context6. Table 1 shows an example of this
labeling from the model described below. The most
similar 200 terms for ?jaguar? have been clustered
into the car sense and the cat sense and the high-
est scoring 6 hypernyms provide a concise descrip-
tion of these senses. This information can be used
to automatically map these cluster senses to senses
in an taxonomy or ontology. Occurrences of am-
biguous words in context can be disambiguated to
these cluster senses comparing the actual context
with salient contexts per sense, obtained by aggre-
gating the Bims from the FOJO graph per cluster.
sense IS-A labels similar terms
jaguar
N.0
car, brand,
company,
automaker,
manufacturer,
vehicle
geely, lincoln-mercury,
tesla, peugeot, ..., mit-
subishi, cadillac, jag, benz,
mclaren, skoda, infiniti,
sable, thunderbird
jaguar
N.1
animal, species,
wildlife, team,
wild animal, cat
panther, cougar, alligator,
tiger, elephant, bull, hippo,
dragon, leopard, shark,
bear, otter, lynx, lion
Table 1: Word sense induction and cluster labeling exam-
ple for ?jaguar?. The shortened cluster for the car sense
has 186 members.
3 Interactive Visualization
3.1 Open Domain Model
The open domain model used in the current vi-
sualization has been trained from newspaper cor-
6Note that this mechanism also elicits hypernyms for unam-
biguous terms receiving a single cluster by the WSI technique.
8
Figure 2: Visualization GUI with prior expansions for
?cold?. Jobims are visualized on the left, expansions on
the right side.
pora using 120 million sentences (about 2 Giga-
words), compiled from LCC (Richter et al, 2006)
and the Gigaword (Parker et al, 2011) corpus. We
constructed a UIMA (Ferrucci and Lally, 2004)
pipeline, which tokenizes, lemmatizes and parses
the data using the Stanford dependency parser (de
Marneffe et al, 2006). The last annotator in the
pipeline annotates Jos and Bims using the collapsed
dependency relations, cf. Fig. 1. We define the lem-
matized forms of the terminals including the part-
of-speech as Jo and the lemmatized dependent word
and the dependency relation name as Bim.
3.2 Interactive Visualization Features
Evaluating the impact of this technology in applica-
tions is an ongoing effort. However, in the context
of this paper, we will show a visualization of the ca-
pabilities allowed by this flavor of distributional se-
mantics. The visualization is a GUI as depicted in
Figure 2, and exemplifies a set of capabilities that
can be accessed through an API. It is straightfor-
ward to include all shown data as features for seman-
tic preprocessing. The input is a sentence in natural
language, which is processed into JoBim pairs as de-
scribed above. All the Jos can be expanded, showing
their paradigmatic relations with other words.
We can perform this operation with and without
taking the context into account (cf. Sect. 2.3). The
latter performs an implicit disambiguation by rank-
ing similar words higher if they fit the context. In
the example, the ?common cold? sense clearly dom-
inates in the prior expansions. However, ?weather?
and ?chill? appear amongst the top-similar prior ex-
pansions.
We also have implemented a sense view, which
displays sense clusters for the selected word, see
Figure 3. Per sense, a list of expansions is pro-
vided together with a list of possible IS-A types. In
this example, the algorithm identified two senses of
?cold? as a temperature and a disease (not all clus-
ter members shown). Given the JoBim graph of the
context (as displayed left in Fig. 2), the particular
occurrence of ?cold? can be disambiguated to Clus-
ter 0 in Fig. 3, since its Bims ?amod(@@,nasty)?
and ?-dobj(catch, @@)? are found in FOJO for far
more members of cluster 0 than for members of clus-
ter 1. Applications of this type of information in-
clude knowledge-based word sense disambiguation
(Miller et al, 2012), type coercion (Kalyanpur et al,
2011) and answer justification in question answering
(Chu-Carroll et al, 2012).
4 Conclusion
In this paper we discussed applications of the Jo-
BimText platform and introduced a new interactive
visualization which showcases a graph-based unsu-
pervised technology for semantic processing. The
implementation is operationalized in a way that it
can be efficiently trained ?off line? using MapRe-
duce, generating domain and language specific mod-
els for distributional semantics. In its ?on line? use,
those models are used to enhance parsing with con-
textualized text expansions of terms. This expansion
step is very efficient and runs on a standard laptop,
so it can be used as a semantic text preprocessor. The
entire project, including pre-computed data models,
is available in open source under the ASL 2.0, and
allows computing contextualized lexical expansion
on arbitrary domains.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Comp. Ling., 36(4):673?721.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: representing adjective-noun
constructions in semantic space. In Proc. EMNLP-
2010, pages 1183?1193, Cambridge, Massachusetts.
C. Biemann and M. Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual simi-
larity. Journal of Language Modelling, 1(1):55?95.
C. Biemann. 2010. Co-occurrence cluster features for
lexical substitutions in context. In Proceedings of
TextGraphs-5, pages 55?59, Uppsala, Sweden.
9
Figure 3: Senses induced for the term ?cold?.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022,
March.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: search and candidate generation. IBM
J. Res. Dev., 56(3):300?311.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. LREC-2006, Genova,
Italy.
Ferdinand de Saussure. 1916. Cours de linguistique
ge?ne?rale. Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. OSDI
?04, San Francisco, CA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proc. EMNLP-
2008, pages 897?906, Honolulu, Hawaii.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
D. Ferrucci and A. Lally. 2004. UIMA: An Architectural
Approach to Unstructured Information Processing in
the Corporate Research Environment. In Nat. Lang.
Eng. 2004, pages 327?348.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T.
Mueller. 2013. Watson: Beyond Jeopardy! Artificial
Intelligence, 199-200:93?105.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING-
1992, pages 539?545, Nantes, France.
A. Kalyanpur, J.W. Murdock, J. Fan, and C. Welty. 2011.
Leveraging community-built knowledge for type co-
ercion in question answering. In Proc. ISWC 2011,
pages 144?156. Springer.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML 2001, pages 282?289, San Francisco, CA, USA.
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan & Claypool Publishers, San
Rafael, CA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768?774,
Montre?al, Quebec, Canada.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych. 2012.
Using distributional similarity for lexical expansion
in knowledge-based word sense disambiguation. In
Proc. COLING-2012, pages 1781?1796, Mumbai, In-
dia.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proc. IS-LTC 2006, Ljubljana, Slovenia.
H. Schu?tze. 1993. Word space. In Advances in Neu-
ral Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.
G. Szarvas, C. Biemann, and I. Gurevych. 2013. Super-
vised all-words lexical substitution using delexicalized
features. In Proc. NAACL-2013, Atlanta, GA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING-
2002, pages 1?7, Taipei, Taiwan.
10
