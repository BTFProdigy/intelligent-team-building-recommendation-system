Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 74?81,
New York, June 2006. c?2006 Association for Computational Linguistics
Exploiting Domain Structure for Named Entity Recognition
Jing Jiang and ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{jiang4,czhai}@cs.uiuc.edu
Abstract
Named Entity Recognition (NER) is a
fundamental task in text mining and nat-
ural language understanding. Current ap-
proaches to NER (mostly based on super-
vised learning) perform well on domains
similar to the training domain, but they
tend to adapt poorly to slightly different
domains. We present several strategies
for exploiting the domain structure in the
training data to learn a more robust named
entity recognizer that can perform well on
a new domain. First, we propose a sim-
ple yet effective way to automatically rank
features based on their generalizabilities
across domains. We then train a classifier
with strong emphasis on the most general-
izable features. This emphasis is imposed
by putting a rank-based prior on a logis-
tic regression model. We further propose
a domain-aware cross validation strategy
to help choose an appropriate parameter
for the rank-based prior. We evaluated
the proposed method with a task of recog-
nizing named entities (genes) in biology
text involving three species. The exper-
iment results show that the new domain-
aware approach outperforms a state-of-
the-art baseline method in adapting to new
domains, especially when there is a great
difference between the new domain and
the training domain.
1 Introduction
Named Entity Recognition (NER) is the task of
identifying and classifying phrases that denote cer-
tain types of named entities (NEs), such as per-
sons, organizations and locations in news articles,
and genes, proteins and chemicals in biomedical lit-
erature. NER is a fundamental task in many natural
language processing applications, such as question
answering, machine translation, text mining, and in-
formation retrieval (Srihari and Li, 1999; Huang and
Vogel, 2002).
Existing approaches to NER are mostly based on
supervised learning. They can often achieve high
accuracy provided that a large annotated training set
similar to the test data is available (Borthwick, 1999;
Zhou and Su, 2002; Florian et al, 2003; Klein et al,
2003; Finkel et al, 2005). Unfortunately, when the
test data has some difference from the training data,
these approaches tend to not perform well. For ex-
ample, Ciaramita and Altun (2005) reported a per-
formance degradation of a named entity recognizer
trained on CoNLL 2003 Reuters corpus, where the
F1 measure dropped from 0.908 when tested on a
similar Reuters set to 0.643 when tested on a Wall
Street Journal set. The degradation can be expected
to be worse if the training data and the test data are
more different.
The performance degradation indicates that exist-
ing approaches adapt poorly to new domains. We
believe one reason for this poor adaptability is that
these approaches have not considered the fact that,
depending on the genre or domain of the text, the
entities to be recognized may have different mor-
74
phological properties or occur in different contexts.
Indeed, since most existing learning-based NER ap-
proaches explore a large feature space, without regu-
larization, a learned NE recognizer can easily overfit
the training domain.
Domain overfitting is a serious problem in NER
because we often need to tag entities in completely
new domains. Given any new test domain, it is gen-
erally quite expensive to obtain a large amount of
labeled entity examples in that domain. As a result,
in many real applications, we must train on data that
do not fully resemble the test data.
This problem is especially serious in recognizing
entities, in particular gene names, from biomedical
literature. Gene names of one species can be quite
different from those of another species syntactically
due to their different naming conventions. For exam-
ple, some biological species such as yeast use sym-
bolic gene names like tL(CAA)G3, while some other
species such as fly use descriptive gene names like
wingless.
In this paper, we present several strategies for ex-
ploiting the domain structure in the training data to
learn a more robust named entity recognizer that can
perform well on a new domain. Our work is mo-
tivated by the fact that in many real applications,
the training data available to us naturally falls into
several domains that are similar in some aspects but
different in others. For example, in biomedical lit-
erature, the training data can be naturally grouped
by the biological species being discussed, while for
news articles, the training data can be divided by
the genre, the time, or the news agency of the arti-
cles. Our main idea is to exploit such domain struc-
ture in the training data to identify generalizable fea-
tures which, presumably, are more useful for rec-
ognizing named entities in a new domain. Indeed,
named entities across different domains often share
certain common features, and it is these common
features that are suitable for adaptation to new do-
mains; features that only work for a particular do-
main would not be as useful as those working for
multiple domains. In biomedical literature, for ex-
ample, surrounding words such as expression and
encode are strong indicators of gene mentions, re-
gardless of the specific biological species being dis-
cussed, whereas species-specific name characteris-
tics (e.g., prefix = ?-less?) would clearly not gener-
alize well, and may even hurt the performance on a
new domain. Similarly, in news articles, the part-of-
speeches of surrounding words such as ?followed by
a verb? are more generalizable indicators of name
mentions than capitalization, which might be mis-
leading if the genre of the new domain is different;
an extreme case is when every letter in the new do-
main is capitalized.
Based on these intuitions, we regard a feature as
generalizable if it is useful for NER in all training
domains, and propose a generalizability-based fea-
ture ranking method, in which we first rank the fea-
tures within each training domain, and then combine
the rankings to promote the features that are ranked
high in all domains. We further propose a rank-
based prior on logistic regression models, which
puts more emphasis on the more generalizable fea-
tures during the learning stage in a principled way.
Finally, we present a domain-aware validation strat-
egy for setting an appropriate parameter value for
the rank-based prior. We evaluated our method on
a biomedical literature data set with annotated gene
names from three species, fly, mouse, and yeast, by
treating one species as the new domain and the other
two as the training domains. The experiment results
show that the proposed method outperforms a base-
line method that represents the state-of-the-art NER
techniques.
The rest of the paper is organized as follows: In
Section 2, we introduce a feature ranking method
based on the generalizability of features across do-
mains. In Section 3, we briefly introduce the logistic
regression models for NER. We then propose a rank-
based prior on logistic regression models and de-
scribe the domain-aware validation strategy in Sec-
tion 4. The experiment results are presented in Sec-
tion 5. Finally we discuss related work in Section 6
and conclude our work in Section 7.
2 Generalizability-Based Feature Ranking
We take a commonly used approach and treat NER
as a sequential tagging problem (Borthwick, 1999;
Zhou and Su, 2002; Finkel et al, 2005). Each token
is assigned the tag I if it is part of an NE and the tag
O otherwise. Let x denote the feature vector for a
token, and let y denote the tag for x. We first com-
pute the probability p(y|x) for each token, using a
75
learned classifier. We then apply Viterbi algorithm
to assign the most likely tag sequence to a sequence
of tokens, i.e., a sentence. The features we use fol-
low the common practice in NER, including surface
word features, orthographic features, POS tags, sub-
strings, and contextual features in a local window of
size 5 around the target token (Finkel et al, 2005).
As in any learning problem, feature selection
may affect the NER performance significantly. In-
deed, a very likely cause of the domain overfit-
ting problem may be that the learned NE recog-
nizer has picked up some non-generalizable fea-
tures, which are not useful for a new domain. Below,
we present a generalizability-based feature ranking
method, which favors more generalizable features.
Formally, we assume that the training examples
are divided into m subsets T1, T2, . . . , Tm, corre-
sponding to m different domains D1, D2, . . . , Dm.
We further assume that the test set Tm+1 is from
a new domain Dm+1, and this new domain shares
some common features of the m training domains.
Note that these are reasonable assumptions that re-
flect the situation in real problems.
We use generalizability to denote the amount of
contribution a feature can make to the classification
accuracy on any domain. Thus, a feature with high
generalizability should be useful for classification
on any domain. To identify the highly generalizable
features, we must then compare their contributions
to classification among different domains.
Suppose in each individual domain, the features
can be ranked by their contributions to the classifi-
cation accuracy. There are different feature ranking
methods based on different criteria. Without loss of
generality, let us use rT : F ? {1, 2, . . . , |F |} to
denote a ranking function that maps a feature f ? F
to a rank rT (f) based on a set of training examples
T , where F is the set of all features, and the rank de-
notes the position of the feature in the final ranked
list. The smaller the rank rT (f) is, the more impor-
tant the feature f is in the training set T . For the m
training domains, we thus have m ranking functions
rT1 , rT2 , . . . , rTm .
To identify the generalizable features across the m
different domains, we propose to combine the m in-
dividual domain ranking functions in the following
way. The idea is to give high ranks to features that
are useful in all training domains . To achieve this
goal, we first define a scoring function s : F ? R
as follows:
s(f) = mmin
i=1
1
rTi(f) . (1)
We then rank the features in decreasing order of their
scores using the above scoring function. This is es-
sentially to rank features according to their maxi-
mum rank maxi rTi(f) among the m domains. Let
function rgen return the rank of a feature in this com-
bined, generalizability-based ranked list.
The original ranking function rT used for indi-
vidual domain feature ranking can use different cri-
teria such as information gain or ?2 statistic (Yang
and Pedersen, 1997). In our experiments, we used a
ranking function based on the model parameters of
the classifier, which we will explain in Section 5.2.
Next, we need to incorporate this preference for
generalizable features into the classifier. Note that
because this generalizability-based feature ranking
method is independent of the learning algorithm, it
can be applied on top of any classifier. In this work,
we choose the logistic regression classifier. One way
to incorporate the feature ranking into the classifier
is to select the top-k features, where k is chosen by
cross validation. There are two potential problems
with this hard feature selection approach. First, once
k features are selected, they are treated equally dur-
ing the learning stage, resulting in a loss of the pref-
erence among these k features. Second, this incre-
mental feature selection approach does not consider
the correlation among features. We propose an al-
ternative way to incorporate the feature ranking into
the classifier, where the preference for generalizable
features is transformed into a non-uniform prior over
the feature parameters in the model. This can be re-
garded as a soft feature selection approach.
3 Logistic Regression for NER
In binary logistic regression models, the probability
of an observation x being classified as I is
p(I|x,?) = exp(?0 +
?|F |
i=1 ?ixi)
1 + exp(?0 +?|F |i=1 ?ixi)
(2)
= exp(? ? x
?)
1 + exp(? ? x?) , (3)
76
where ?0 is the bias weight, ?i (1 ? i ? |F |)
are the weights for the features, and x? is the aug-
mented feature vector with x0 = 1. The weight vec-
tor ? can be learned from the training examples by
a maximum likelihood estimator. It is worth point-
ing out that logistic regression has a close relation
with maximum entropy models. Indeed, when the
features in a maximum entropy model are defined as
conjunctions of a feature on observations only and
a Kronecker delta of a class label, which is a com-
mon practice in NER, the maximum entropy model
is equivalent to a logistic regression model (Finkel
et al, 2005). Thus the logistic regression method we
use for NER is essentially the same as the maximum
entropy models used for NER in previous work.
To avoid overfitting, a zero mean Gaussian prior
on the weights is usually used (Chen and Rosenfeld,
1999; Bender et al, 2003), and a maximum a poste-
rior (MAP) estimator is used to maximize the poste-
rior probability:
?? = arg max
?
p(?)
N?
j=1
p(yj |xj,?), (4)
where yj is the true class label for xj, N is the num-
ber of training examples, and
p(?) =
|F |?
i=1
1?
2pi?2i
exp(? ?
2
i
2?2i
). (5)
In previous work, ?i are set uniformly to the same
value for all features, because there is in general no
additional prior knowledge about the features.
4 Rank-Based Prior
Instead of using the same ?i for all features, we pro-
pose a rank-based non-uniform Gaussian prior on
the weights of the features so that more general-
izable features get higher prior variances (i.e., low
prior strength) and features on the bottom of the list
get low prior variances (i.e., high prior strength).
Since the prior has a zero mean, such a prior would
force features on the bottom of the ranked list, which
have the least generalizability, to have near-zero
weights, but allow more generalizable features to be
assigned higher weights during the training stage.
4.1 Transformation Function
We need to find a transformation function h :
{1, 2, . . . , |F |} ? R+ so that we can set ?2i =
h(rgen(fi)), where rgen(fi) is the rank of feature
fi in the generalizability-based ranked feature list,
as defined in Section 2. We choose the following
h function because it has the desired properties as
described above:
h(r) = ar1/b , (6)
where a and b (a, b > 0) are parameters that control
the degree of the confidence in the generalizability-
based ranked feature list. Note that a corresponds to
the prior variance assigned to the top-most feature in
the ranked list. When b is small, the prior variance
drops rapidly as the rank r increases, giving only a
small number of top features high prior variances.
When b is larger, there will be less discrimination
among the features. When b approaches infinity, the
prior becomes a uniform prior with the variance set
to a for all features. If we set a small threshold ? on
the variance, then we can derive that at least m =(a
?
)b features have a prior variance greater than ? .
Thus b is proportional to the logarithm of the number
of features that are assigned a variance greater than
the threshold ? when a is fixed. Figure 1 shows the
h function when a is set to 20 and b is set to a set of
different values.
 0
 5
 10
 15
 20
 25
 0  200  400  600  800  1000
h(
r)
r
b = 2
b = 4
b = 6
b = ?
Figure 1: Transformation Function h(r) = 20r1/b
4.2 Parameter Setting using Domain-Aware
Validation
We need to set the appropriate values for the param-
eters a and b. For parameter a, we use the following
77
simple strategy to obtain an estimation. We first train
a logistic regression model on all the training data
using a Gaussian prior with a fixed variance (set to
1 in our experiments). We then find the maximum
weight
?max = |F |maxi=1 |?i| (7)
in this trained model. Finally we set a = ?2max. Our
reasoning is that since a is the variance of the prior
for the best feature, a is related to the ?permissible
range? of ? for the best feature, and ?max gives us a
way for adjusting a according to the empirical range
of ?i?s.
As we pointed out in Section 4.1, when a is fixed,
parameter b controls the number of top features that
are given a relatively high prior variance, and hence
implicitly controls the number of top features to
choose for the classifier to put the most weights on.
To select an appropriate value of b, we can use a
held-out validation set to tune the parameter value
b. Here we present a validation strategy that exploits
the domain structure in the training data to set the
parameter b for a new domain. Note that in regular
validation, both the training set and the validation
set contain examples from all training domains. As
a result, the average performance on the validation
set may be dominated by domains in which the NEs
are easy to classify. Since our goal is to build a clas-
sifier that performs well on new domains, we should
pay more attention to hard domains that have lower
classification accuracy. We should therefore exam-
ine the performance of the classifier on each training
domain individually in the validation stage to gain
an insight into the appropriate value of b for a new
domain, which has an equal chance of being similar
to any of the training domains.
Our domain-aware validation strategy first finds
the optimal value of b for each training domain. For
each subset Ti of the training data belonging to do-
main Di, we divide it into a training set T ti and a val-
idation set T vi . Then for each domain Di, we train a
classifier on the training sets of all domains, that is,
we train on ?mj=1 T tj . We then test the classifier on
T vi . We try a set of different values of b with a fixed
value of a, and choose the optimal b that gives the
best performance on T vi . Let this optimal value of b
for domain Di be bi.
Given bi (1 ? i ? m), we can choose an appropri-
ate value of bm+1 for an unknown test domain Dm+1
based on the assumption that Dm+1 is a mixture of
all the training domains. bm+1 is then chosen to be
a weighted average of bi, (1 ? i ? m):
bm+1 =
m?
i=1
?ibi, (8)
where ?i indicates how similar Dm+1 is to Di. In
many cases, the test domain Dm+1 is completely
unknown. In this case, the best we can do is to set
?i = 1/m for all i, that is, to assume that Dm+1 is
an even mixture of all training domains.
5 Empirical Evaluation
5.1 Experimental Setup
We evaluated our domain-aware approach to NER
on the problem of gene recognition in biomedical
literature. The data we used is from BioCreAtIvE
Task 1B (Hirschman et al, 2005). We chose this
data set because it contains three subsets of MED-
LINE abstracts with gene names from three species
(fly, mouse, and yeast), while no other existing an-
notated NER data set has such explicit domain struc-
ture. The original BioCreAtIvE 1B data was not
provided with every gene annotated, but for each ab-
stract, a list of genes that were mentioned in the ab-
stract was given. A gene synonym list was also given
for each species. We used a simple string matching
method with slight relaxation to tag the gene men-
tions in the abstracts. We took 7500 sentences from
each species for our experiments, where half of the
sentences contain gene mentions. We further split
the 7500 sentences of each species into two sets,
5000 for training and 2500 for testing.
We conducted three sets of experiments, each
combining the 5000-sentence training data of two
species as training data, and the 2500-sentence test
data of the third species as test data. The 2500-
sentence test data of the training species was used
for validation. We call these three sets of experi-
ments F+M?Y, F+Y?M, and M+Y?F.
we use FEX1 for feature extraction and BBR2 for
logistic regression in our experiments.
1http://l2r.cs.uiuc.edu/ cogcomp/asoftware.php?skey=FEX
2http://www.stat.rutgers.edu/ madigan/BBR/
78
5.2 Comparison with Baseline Method
Because the data set was generated by our automatic
tagging procedure using the given gene lists, there is
no previously reported performance on this data set
for us to compare with. Therefore, to see whether
using the domain structure in the training data can
really help the adaptation to new domains, we com-
pared our method with a state-of-the-art baseline
method based on logistic regression. It uses a Gaus-
sian prior with zero mean and uniform variance on
all model parameters. It also employs 5-fold regular
cross validation to pick the optimal variance for the
prior. Regular feature selection is also considered
in the baseline method, where the features are first
ranked according to some criterion, and then cross
validation is used to select the top-k features. We
tested three popular regular feature ranking meth-
ods: feature frequency (F), information gain (IG),
and ?2 statistic (CHI). These methods were dis-
cussed in (Yang and Pedersen, 1997). However, with
any of the three feature ranking criteria, cross valida-
tion showed that selecting all features gave the best
average validation performance. Therefore, the best
baseline method which we compare our method with
uses all features. We call the baseline method BL.
In our method, the generalizability-based feature
ranking requires a first step of feature ranking within
each training domain. While we could also use F,
IG or CHI to rank features in each domain, to make
our method self-contained, we used the following
strategy. We first train a logistic regression model
on each domain using a zero-mean Gaussian prior
with variance set to 1. Then, features are ranked
in decreasing order of the absolute values of their
weights. The rationale is that, in general, features
with higher weights in the logistic regression model
are more important. With this ranking within each
training domain, we then use the generalizability-
based feature ranking method to combine the m
domain-specific rankings. The obtained ranked fea-
ture list is used to construct the rank-based prior,
where the parameters a and b are set in the way as
discussed in Section 4.2. We call our method DOM.
In Table 1, we show the precision, recall, and F1
measures of our domain-aware method (DOM) and
the baseline method (BL) in all three sets of exper-
iments. We see that the domain-aware method out-
performs the baseline method in all three cases when
F1 is used as the primary performance measure. In
F+Y?M and M+Y?F, both precision and recall are
also improved over the baseline method.
Exp Method P R F1
F+M?Y BL 0.557 0.466 0.508
DOM 0.575 0.516 0.544
F+Y?M BL 0.571 0.335 0.422
DOM 0.582 0.381 0.461
M+Y?F BL 0.583 0.097 0.166
DOM 0.591 0.139 0.225
Table 1: Comparison of the domain-aware method
and the baseline method, where in the domain-aware
method, b = 0.5b1 + 0.5b2
Note that the absolute performance shown in Ta-
ble 1 is lower than the state-of-the-art performance
of gene recognition (Finkel et al, 2005).3 One rea-
son is that we explicitly excluded the test domain
from the training data, while most previous work on
gene recognition was conducted on a test set drawn
from the same collection as the training data. An-
other reason is that we used simple string match-
ing to generate the data set, which introduced noise
to the data because gene names often have irregular
lexical variants.
5.3 Comparison with Regular Feature Ranking
Methods
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 1  2  3  4  5  6  7  8  9  10
F1
b
F+M?Y
DOM
F
IG
CHI
BL
Figure 2: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
F+M?Y
3Our baseline method performed comparably to the state-of-
the-art systems on the standard BioCreAtIvE 1A data.
79
 0.3
 0.35
 0.4
 0.45
 0.5
 1  2  3  4  5  6  7  8  9  10
F1
b
F+Y?M
DOM
F
IG
CHI
BL
Figure 3: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
F+Y?M
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 1  2  3  4  5  6  7  8  9  10
F1
b
M+Y?F
DOM
F
IG
CHI
BL
Figure 4: Comparison between regular feature rank-
ing and generalizability-based feature ranking on
M+Y?F
To further understand how our method improved
the performance, we compared the generalizability-
based feature ranking method with the three regular
feature ranking methods, F, IG, and CHI, that were
used in the baseline method. To make fair compar-
ison, for the regular feature ranking methods, we
also used the rank-based prior transformation as de-
scribed in Section 4 to incorporate the preference for
top-ranked features. Figure 2, Figure 3 and Figure 4
show the performance of different feature ranking
methods in the three sets of experiments as the pa-
rameter b for the rank-based prior changes. As we
pointed out in Section 4, b is proportional to the log-
arithm of the number of ?effective features?.
From the figures, we clearly see that the curve for
the generalizability-based ranking method DOM is
always above the curves of the other methods, indi-
cating that when the same amount of top features are
being emphasized by the prior, the features selected
by DOM give better performance on a new domain
than the features selected by the other methods. This
suggests that the top-ranked features in DOM are in-
deed more suitable for adaptation to new domains
than the top features ranked by the other methods.
The figures also show that the ranking method
DOM achieved better performance than the baseline
over a wide range of b values, especially in F+Y?M
and M+Y?F, whereas for methods F, IG and CHI,
the performance quickly converged to the baseline
performance as b increased.
It is interesting to note the comparison between F
and IG (or CHI). In general, when the test data is
similar to the training data, IG (or CHI) is advanta-
geous over F (Yang and Pedersen, 1997). However,
in this case when the test domain is different from
the training domains, F shows advantages for adap-
tation. A possible explanation is that frequent fea-
tures are in general less likely to be domain-specific,
and therefore feature frequency can also be used as a
criterion to select generalizable features and to filter
out domain-specific features, although it is still not
as effective as the method we proposed.
6 Related Work
The NER problem has been extensively studied in
the NLP community. Most existing work has fo-
cused on supervised learning approaches, employ-
ing models such as HMMs (Zhou and Su, 2002),
MEMMs (Bender et al, 2003; Finkel et al, 2005),
and CRFs (McCallum and Li, 2003). Collins and
Singer (1999) proposed an unsupervised method for
named entity classification based on the idea of co-
training. Ando and Zhang (2005) proposed a semi-
supervised learning method to exploit unlabeled data
for building more robust NER systems. In all these
studies, the evaluation is conducted on unlabeled
data similar to the labeled data.
Recently there have been some studies on adapt-
ing NER systems to new domains employing tech-
niques such as active learning and semi-supervised
learning (Shen et al, 2004; Mohit and Hwa, 2005),
80
or incorporating external lexical knowledge (Cia-
ramita and Altun, 2005). However, there has not
been any study on exploiting the domain structure
contained in the training examples themselves to
build generalizable NER systems. We focus on
the domain structure in the training data to build
a classifier that relies more on features generaliz-
able across different domains to avoid overfitting the
training domains. As our method is orthogonal to
most of the aforementioned work, they can be com-
bined to further improve the performance.
7 Conclusion and Future Work
Named entity recognition is an important problem
that can help many text mining and natural lan-
guage processing tasks such as information extrac-
tion and question answering. Currently NER faces
a poor domain adaptability problem when the test
data is not from the same domain as the training
data. We present several strategies to exploit the
domain structure in the training data to improve the
performance of the learned NER classifier on a new
domain. Our results show that the domain-aware
strategies we proposed improved the performance
over a baseline method that represents the state-of-
the-art NER techniques.
Acknowledgments
This work was in part supported by the National
Science Foundation under award numbers 0425852,
0347933, and 0428472. We would like to thank
Bruce Schatz, Xin He, Qiaozhu Mei, Xu Ling, and
some other BeeSpace project members for useful
discussions. We would like to thank Mark Sammons
for his help with FEX. We would also like to thank
the anonymous reviewers for their comments.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for text
chunking. In Proceedings of ACL-2005.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL-2003.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, School of Com-
puter Science, Carnegie Mellon University.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Workshop on Advances
in Structured Learning for Text and Speech Processing
(NIPS-2005).
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP/VLC-1999.
Jenny Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics, 6(Suppl 1):S5.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
Lynette Hirschman, Marc Colosimo, Alexander Morgan,
and Alexander Yeh. 2005. Overview of BioCreAtIvE
task 1B: normailized gene lists. BMC Bioinformatics,
6(Suppl 1):S11.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI-2002.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recogni-
tion with character-level models. In Proceedings of
CoNLL-2003.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Behrang Mohit and Rebecca Hwa. 2005. Syntax-based
semi-supervised named entity tagging. In Proceedings
of ACL-2005.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of ACL-
2004.
Rohini Srihari and Wei Li. 1999. Information extraction
supported question answering. In TREC-8.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML-1997.
Guodong Zhou and Jian Su. 2002. Named entity recog-
nition using an HMM-based chunk tagger. In Proceed-
ings of ACL-2002.
81
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407?414,
New York, June 2006. c?2006 Association for Computational Linguistics
Language Model Information Retrieval with Document Expansion
Tao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana Champaign
Abstract
Language model information retrieval de-
pends on accurate estimation of document
models. In this paper, we propose a docu-
ment expansion technique to deal with the
problem of insufficient sampling of docu-
ments. We construct a probabilistic neigh-
borhood for each document, and expand
the document with its neighborhood infor-
mation. The expanded document provides
a more accurate estimation of the docu-
ment model, thus improves retrieval ac-
curacy. Moreover, since document expan-
sion and pseudo feedback exploit different
corpus structures, they can be combined to
further improve performance. The experi-
ment results on several different data sets
demonstrate the effectiveness of the pro-
posed document expansion method.
1 Introduction
Information retrieval with statistical language mod-
els (Lafferty and Zhai, 2003) has recently attracted
much more attention because of its solid theoreti-
cal background as well as its good empirical per-
formance. In this approach, queries and documents
are assumed to be sampled from hidden generative
models, and the similarity between a document and
a query is then calculated through the similarity be-
tween their underlying models.
Clearly, good retrieval performance relies on the
accurate estimation of the query and document mod-
els. Indeed, smoothing of document models has
been proved to be very critical (Chen and Good-
man, 1998; Kneser and Ney, 1995; Zhai and Laf-
ferty, 2001b). The need for smoothing originated
from the zero count problem: when a term does not
occur in a document, the maximum likelihood esti-
mator would give it a zero probability. This is un-
reasonable because the zero count is often due to in-
sufficient sampling, and a larger sample of the data
would likely contain the term. Smoothing is pro-
posed to address the problem.
While most smoothing methods utilize the global
collection information with a simple interpolation
(Ponte and Croft, 1998; Miller et al, 1999; Hiemstra
and Kraaij, 1998; Zhai and Lafferty, 2001b), sev-
eral recent studies (Liu and Croft, 2004; Kurland and
Lee, 2004) have shown that local corpus structures
can be exploited to improve retrieval performance.
In this paper, we further study the use of local cor-
pus structures for document model estimation and
propose to use document expansion to better exploit
local corpus structures for estimating document lan-
guage models.
According to statistical principles, the accuracy of
a statistical estimator is largely determined by the
sampling size of the observed data; a small data
set generally would result in large variances, thus
can not be trusted completely. Unfortunately, in re-
trieval, we often have to estimate a model based on a
single document. Since a document is a small sam-
ple, our estimate is unlikely to be very accurate.
A natural improvement is to enlarge the data sam-
ple, ideally in a document-specific way. Ideally, the
enlarged data sample should come from the same
original generative model. In reality, however, since
407
the underlying model is unknown to us, we would
not really be able to obtain such extra data. The
essence of this paper is to use document expansion
to obtain high quality extra data to enlarge the sam-
ple of a document so as to improve the accuracy
of the estimated document language model. Docu-
ment expansion was previously explored in (Sing-
hal and Pereira, 1999) in the context of the vec-
tor space retrieval model, mainly involving selecting
more terms from similar documents. Our work dif-
fers from this previous work in that we study doc-
ument expansion in the language modeling frame-
work and implement the idea quite differently.
Our main idea is to augment a document prob-
abilistically with potentially all other documents in
the collection that are similar to the document. The
probability associated with each neighbor document
reflects how likely the neighbor document is from
the underlying distribution of the original document,
thus we have a ?probabilistic neighborhood?, which
can serve as ?extra data? for the document for es-
timating the underlying language model. From the
viewpoint of smoothing, our method extends the ex-
isting work on using clusters for smoothing (Liu and
Croft, 2004) to allow each document to have its own
cluster for smoothing.
We evaluated our method using six representative
retrieval test sets. The experiment results show that
document expansion smoothing consistently outper-
forms the baseline smoothing methods in all the data
sets. It also outperforms a state-of-the-art cluster-
ing smoothing method. Analysis shows that the
improvement tends to be more significant for short
documents, indicating that the improvement indeed
comes from the improved estimation of the docu-
ment language model, since a short document pre-
sumably would benefit more from the neighborhood
smoothing. Moreover, since document expansion
and pseudo feedback exploit different corpus struc-
tures, they can be combined to further improve per-
formance. As document expansion can be done in
the indexing stage, it is scalable to large collections.
2 Document Expansion Retrieval Model
2.1 The KL-divergence retrieval model
We first briefly review the KL-divergence retrieval
model, on which we will develop the document
expansion technique. The KL-divergence model
is a representative state-of-the-art language model-
ing approach for retrieval. It covers the basic lan-
guage modeling approach (i.e., the query likelihood
method) as a special case and can support feedback
more naturally.
In this approach, a query and a document are as-
sumed to be generated from a unigram query lan-
guage model ?Q and a unigram document languagemodel ?D, respectively. Given a query and a docu-ment, we would first compute an estimate of the cor-
responding query model (??Q) and document model
(??D), and then score the document w.r.t. the querybased on the KL-divergence of the two models (Laf-
ferty and Zhai, 2001):
D(??Q || ??d) =
?
w?V
p(w|??Q) ? log
p(w|??Q)
p(w|??d)
.
where V is the set of all the words in our vocabulary.
The documents can then be ranked according to the
ascending order of the KL-divergence values.
Clearly, the two fundamental problems in such a
model are to estimate the query model and the doc-
ument model, and the accuracy of our estimation of
these models would affect the retrieval performance
significantly. The estimation of the query model
can often be improved by exploiting the local cor-
pus structure in a way similar to pseudo-relevance
feedback (Lafferty and Zhai, 2001; Lavrenko and
Croft, 2001; Zhai and Lafferty, 2001a). The esti-
mation of the document model is most often done
through smoothing with the global collection lan-
guage model (Zhai and Lafferty, 2001b), though re-
cently there has been some work on using clusters
for smoothing (Liu and Croft, 2004). Our work is
mainly to extend the previous work on document
smoothing and improve the accuracy of estimation
by better exploiting the local corpus structure. We
now discuss all these in detail.
2.2 Smoothing of document models
Given a document d, the simplest way to estimate
the document language model is to treat the docu-
ment as a sample from the underlying multinomial
word distribution and use the maximum likelihood
estimator: P (w|??d) = c(w,d)|d| , where c(w, d) isthe count of word w in document d, and |d| is the
408
length of d. However, as discussed in virtually all
the existing work on using language models for re-
trieval, such an estimate is problematic and inaccu-
rate; indeed, it would assign zero probability to any
word not present in document d, causing problems
in scoring a document with query likelihood or KL-
divergence (Zhai and Lafferty, 2001b). Intuitively,
such an estimate is inaccurate because the document
is a small sample.
To solve this problem, many different smoothing
techniques have been proposed and studied, usually
involving some kind of interpolation of the maxi-
mum likelihood estimate and a global collection lan-
guage model (Hiemstra and Kraaij, 1998; Miller et
al., 1999; Zhai and Lafferty, 2001b). For exam-
ple, Jelinek-Mercer(JM) and Dirichlet are two com-
monly used smoothing methods (Zhai and Lafferty,
2001b). JM smoothing uses a fixed parameter ? to
control the interpolation:
P (w|??d) = ?
c(w, d)
|d| + (1 ? ?)P (w|?C),
while the Dirichlet smoothing uses a document-
dependent coefficient (parameterized with ?) to con-
trol the interpolation:
P (w|??d) =
c(w, d) + ?P (w|?C)
|d| + ? .
Here P (w|?C) is the probability of word w given bythe collection language model ?C , which is usuallyestimated using the whole collection of documents
C , e.g., P (w|?C) =
P
d?C c(d,w)
P
d?C |d|
.
2.3 Cluster-based document model (CBDM)
Recently, the cluster structure of the corpus has been
exploited to improve language models for retrieval
(Kurland and Lee, 2004; Liu and Croft, 2004). In
particular, the cluster-based language model pro-
posed in (Liu and Croft, 2004) uses clustering infor-
mation to further smooth a document model. It di-
vides all documents into K different clusters (K =
1000 in their experiments). Both cluster informa-
tion and collection information are used to improve
the estimate of the document model:
P (w|??d) = ?
c(w, d)
|d| + (1 ? ?)
?[?P (w|?Ld) + (1 ? ?)P (w|?C )],
where ?Ld stands for document d?s cluster modeland ? and ? are smoothing parameters. In this
clustering-based smoothing method, we first smooth
a cluster model with the collection model using
Dirichlet smoothing, and then use smoothed cluster
model as a new reference model to further smooth
the document model using JM smoothing; empirical
results show that the added cluster information in-
deed enhances retrieval performance (Liu and Croft,
2004).
2.4 Document expansion
From the viewpoint of data augmentation, the
clustering-based language model can be regarded as
?expanding? a document with more data from the
cluster that contains the document. This is intu-
itively better than simply expanding every document
with the same collection language model as in the
case of JM or Dirichlet smoothing. Looking at it
from this perspective, we see that, as the ?extra data?
for smoothing a document model, the cluster con-
taining the document is often not optimal. Indeed,
the purpose of clustering is to group similar doc-
uments together, hence a cluster model represents
well the overall property of all the documents in the
cluster. However, such an average model is often not
accurate for smoothing each individual document.
We illustrate this problem in Figure 1(a), where we
show two documents d and a in cluster D. Clearly
the generative model of cluster D is more suitable
for smoothing document a than document d. In gen-
eral, the cluster model is more suitable for smooth-
ing documents close to the centroid, such as a, but is
inaccurate for smoothing a document at the bound-
ary, such as d.
To achieve optimal smoothing, each document
should ideally have its own cluster centered on the
document, as shown in Figure 1(b). This is pre-
cisely what we propose ? expanding each document
with a probabilistic neighborhood around the doc-
ument and estimate the document model based on
such a virtual, expanded document. We can then ap-
ply any simple interpolation-based method (e.g., JM
or Dirichlet) to such a ?virtual document? and treat
the word counts given by this ?virtual document? as
if they were the original word counts.
The use of neighborhood information is worth
more discussion. First of all, neighborhood is not a
409
cluster D
d d
d?s neighbors
(a) (b)
a
Figure 1: Clusters, neighborhood, and document ex-
pansion
clearly defined concept. In the narrow sense, only
a few documents close to the original one should
be included in the neighborhood, while in the wide
sense, the whole collection can be potentially in-
cluded. It is thus a challenge to define the neighbor-
hood concept reasonably. Secondly, the assumption
that neighbor documents are sampled from the same
generative model as the original document is not
completely valid. We probably do not want to trust
them so much as the original one. We solve these
two problems by associating a confidence value with
every document in the collection, which reflects our
belief that the document is sampled from the same
underlying model as the original document. When a
document is close to the original one, we have high
confidence, but when it is farther apart, our confi-
dence would fade away. In this way, we construct
a probabilistic neighborhood which can potentially
include all the documents with different confidence
values. We call a language model based on such a
neighborhood document expansion language model
(DELM).
Technically, we are looking for a new enlarged
document d? for each document d in a text collec-
tion, such that the new document d? can be used
to estimate the hidden generative model of d more
accurately. Since a good d? should presumably be
based on both the original document d and its neigh-
borhood N(d), we define a function ?:
d? = ?(d,N(d)). (1)
The precise definition of the neighborhood con-
cept N(d) relies on the distance or similarity be-
tween each pair of documents. Here, we simply
choose the commonly used cosine similarity, though
other choices may also be possible. Given any two
document models X and Y , the cosine similarity is
d
Figure 2: Normal distribution of confidence values.
defined as:
sim(X,Y ) =
?
i xi ? yi
?
?
i(xi)2 ?
?
i(yi)2
.
To model the uncertainty of neighborhood, we as-
sign a confidence value ?d(b) to every document b inthe collection to indicate how strongly we believe b
is sampled from d?s hidden model. In general, ?d(b)can be set based on the similarity of b and d ? the
more similar b and d are, the larger ?d(b) wouldbe. With these confidence values, we construct a
probabilistic neighborhood with every document in
it, each with a different weight. The whole problem
is thus reduced to how to define ?d(b) exactly.Intuitively, an exponential decay curve can help
regularize the influence from remote documents. We
therefore want ?d(b) to satisfy a normal distributioncentered around d. Figure 2 illustrates the shape
of this distribution. The black dots are neighbor-
hood documents centered around d. Their proba-
bility values are determined by their distances to the
center. We fortunately observe that the cosine sim-
ilarities, which we use to decide the neighborhood,
are roughly of this decay shape. We thus use them
directly without further transformation because that
would introduce unnecessary parameters. We set
?d(b) by normalizing the cosine similarity scores :
?d(b) =
sim(d, b)
?
b??C?{d} sim(d, b?)
.
Function ? serves to balance the confidence be-
tween d and its neighborhood N(d) in the model es-
timation step. Intuitively, a shorter document is less
sufficient, hence needs more help from its neighbor-
hood. Conversely, a longer one can rely more on
itself. We use a parameter ? to control this balance.
Thus finally, we obtain a pseudo document d? with
410
the following pseudo term count:
c(w, d?) = ?c(w, d) + (1 ? ?)
?
?
b?C?{d}
(?d(b) ? c(w, b)),
We hypothesize that, in general, ?d can be estimatedmore accurately from d? rather than d itself because
d? contains more complete information about ?d.This hypothesis can be tested by by comparing the
retrieval results of applying any smoothing method
to d with those of applying the same method to d?.
In our experiments, we will test this hypothesis with
both JM smoothing and Dirichlet smoothing.
Note that the proposed document expansion tech-
nique is quite general. Indeed, since it transforms
the original document to a potentially better ?ex-
panded document?, it can presumably be used to-
gether with any retrieval method, including the vec-
tor space model. In this paper, we focus on evalu-
ating this technique with the language modeling ap-
proach.
Because of the decay shape of the neighborhood
and for the sake of efficiency, we do not have to ac-
tually use all documents in C?{d}. Instead, we can
safely cut off the documents on the tail, and only use
the top M closest neighbors for each document. We
show in the experiment section that the performance
is not sensitive to the choice of M when M is suf-
ficiently large (for example 100). Also, since doc-
ument expansion can be done completely offline, it
can scale up to large collections.
3 Experiments
We evaluate the proposed method over six repre-
sentative TREC data sets (Voorhees and Harman,
2001): AP (Associated Press news 1988-90), LA
(LA Times), WSJ (Wall Street Journal 1987-92),
SJMN (San Jose Mercury News 1991), DOE (De-
partment of Energy), and TREC8 (the ad hoc data
used in TREC8). Table 1 shows the statistics of these
data.
We choose the first four TREC data sets for per-
formance comparison with (Liu and Croft, 2004).
To ensure that the comparison is meaningful, we use
identical sources (after all preprocessing). In addi-
tion, we use the large data set TREC8 to show that
our algorithm can scale up, and use DOE because its
#document queries #total qrel
AP 242918 51-150 21819
LA 131896 301-400 2350
WSJ 173252 51-100 and 151-200 10141
SJMN 90257 51-150 4881
TREC8 528155 401-450 4728
DOE 226087 DOE queries 2047
Table 1: Experiment data sets
documents are usually short, and our previous expe-
rience shows that it is a relatively difficult data set.
3.1 Neighborhood document expansion
Our model boils down to a standard query likelihood
model when no neighborhood document is used. We
therefore use two most commonly used smoothing
methods, JM and Dirichlet , as our baselines. The re-
sults are shown in Table 2, where we report both the
mean average precision (MAP) and precision at 10
documents. JM and Dirichlet indicate the standard
language models with JM smoothing and Dirichlet
smoothing respectively, and the other two are the
ones combined with our document expansion. For
both baselines, we tune the parameters (? for JM,
and ? for Dirichlet) to be optimal. We then use the
same values of ? or ? without further tuning for the
document expansion runs, which means that the pa-
rameters may not necessarily optimal for the docu-
ment expansion runs. Despite this disadvantage, we
see that the document expansion runs significantly
outperform their corresponding baselines, with more
than 15% relative improvement on AP. The parame-
ters M and ? were set to 100 and 0.5, respectively.
To understand the improvement in more detail, we
show the precision values at different levels of recall
for the AP data in Table 3. Here we see that our
method significantly outperforms the baseline at ev-
ery precision point.
In our model, we introduce two additional param-
eters: M and ?. We first examine M here, and then
study ? in Section 3.3. Figure 3 shows the perfor-
mance trend with respect to the values of M . The
x-axis is the values of M , and the y-axis is the non-
interpolated precision averaging over all 50 queries.
We draw two conclusions from this plot: (1) Neigh-
borhood information improves retrieval accuracy;
adding more documents leads to better retrieval re-
sults. (2) The performance becomes insensitive to
411
Data JM DELM+JM (impr. %) Dirichlet DELM + Diri.(impr. %)
AP AvgPrec 0.2058 0.2405 (16.8%***) 0.2168 0.2505 (15.5%***)
P@10 0.3990 0.4444 (11.4%***) 0.4323 0.4515 (4.4%**)
DOE AvgPrec 0.1759 0.1904 (8.3%***) 0.1804 0.1898 (5.2%**)
P@10 0.2629 0.2943 (11.9%*) 0.2600 0.2800 (7.7%*)
TREC8 AvgPrec 0.2392 0.2539 (6.01%**) 0.2567 0.2671 (4.05%*)
P@10 0.4300 0.4460 (3.7%) 0.4500 0.4740 (5.3%*)
Table 2: Comparisons with baselines. *,**,*** indicate that we accept the improvement hypothesis by
Wilcoxon test at significance level 0.1, 0.05, 0.01 respectively.
AP, TREC queries 51-150
Dirichlet DELM+Diri Improvement(%)
Rel. 21819 21819
Rel.Retr. 10126 10917 7.81% ***
Prec.
0.0 0.6404 0.6605 3.14% *
0.1 0.4333 0.4785 10.4% ***
0.2 0.3461 0.3983 15.1% ***
0.3 0.2960 0.3496 18.1% ***
0.4 0.2436 0.2962 21.6% ***
0.5 0.2060 0.2418 17.4% ***
0.6 0.1681 0.1975 17.5% ***
0.7 0.1290 0.1580 22.5% ***
0.8 0.0862 0.1095 27.0% **
0.9 0.0475 0.0695 46.3% **
1.0 0.0220 0.0257 16.8%
ave. 0.2168 0.2505 15.5% ***
Table 3: PR curve on AP data. *,**,*** indicate that
we accept the improvement hypothesis by Wilcoxon
test at significant level 0.1, 0.05, 0.01 respectively.
M when M is sufficiently large, namely 100. The
reason is twofold: First, since the neighborhood is
centered around the original document, when M is
large, the expansion may be evenly magnified on all
term dimensions. Second, the exponentially decay-
ing confidence values reduce the influence of remote
documents.
3.2 Comparison with CBDM
In this section, we compare the CBDM method us-
ing the model performing the best in (Liu and Croft,
2004)1. Furthermore, we also set Dirichlet prior pa-
rameter ? = 1000, as mentioned in (Liu and Croft,
2004), to rule out any potential influence of Dirichlet
smoothing.
Table 4 shows that our model outperforms CBDM
in MAP values on four data sets; the improvement
1We use the exact same data, queries, stemming and all
other preprocessing techniques. The baseline results in (Liu and
Croft, 2004) are confirmed.
0.17
0.18
0.19
0.2
0.21
0.22
0.23
0.24
0.25
0.26
0.27
0 100 200 300 400 500 600 700 800
a
ve
ra
ge
 p
re
ce
sio
n
M : the number of  neighborhood documents
AP
DOE
TREC8
Figure 3: Performance change with respect to M
CBDM DELM+Diri. improvement(%)
AP 0.2326 0.2505 7.7%
LA 0.2590 0.2655 2.5%
WSJ 0.3006 0.3113 3.6%
SJMN 0.2171 0.2266 4.3%
Table 4: Comparisons with CBDM.
presumably comes from a more principled way of
exploiting corpus structures. Given that clustering
can at least capture the local structure to some ex-
tent, it should not be very surprising that the im-
provement of document expansion over CBDM is
much less than that over the baselines.
Note that we cannot fulfill Wilcoxon test because
of the lack of the individual query results of CBDM.
3.3 Impact on short documents
Document expansion is to solve the insufficient sam-
pling problem. Intuitively, a short document is less
sufficient than a longer one, hence would need more
?help? from its neighborhood. We design experi-
ments to test this hypothesis.
Specifically, we randomly shrink each document
in AP88-89 to a certain percentage of its original
length. For example, a shrinkage factor of 30%
means each term has 30% chance to stay, or 70%
chance to be filtered out. In this way, we reduce the
original data set to a new one with the same number
412
average doc length 30% 50% 70% 100%
baseline 0.1273 0.1672 0.1916 0.2168
document expansion 0.1794 0.2137 0.2307 0.2505
optimal ? 0.2 0.3 0.3 0.4
improvement(%) 41% 28% 20% 16%
Table 5: Impact on short documents (in MAP)
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
a
ve
ra
ge
 p
re
cis
io
n
alpha
30%
50%
70%
100%
Figure 4: Performance change with respect to ?
of documents but a shorter average document length.
Table 5 shows the experiment results over docu-
ment sets with different average document lengths.
The results indeed support our hypothesis that doc-
ument expansion does help short documents more
than longer ones. While we can manage to improve
41% on a 30%-length corpus, the same model only
gets 16% improvement on the full length corpus.
To understand how ? affects the performance we
plot the sensitivity curves in Figure 4. The curves all
look similar, but the optimal points slightly migrate
when the average document length becomes shorter.
A 100% corpus gets optimal at ? = 0.4, but 30%
corpus has to use ? = 0.2 to obtain its optimum.
(All optimal ? values are presented in the fourth row
of Table 5.)
3.4 Further improvement with pseudo
feedback
Query expansion has been proved to be an effec-
tive way of utilizing corpus information to improve
the query representation (Rocchio, 1971; Zhai and
Lafferty, 2001a). It is thus interesting to examine
whether our model can be combined with query ex-
pansion to further improve the retrieval accuracy.
We use the model-based feedback proposed in (Zhai
and Lafferty, 2001a) and take top 5 returned docu-
ments for feedback. There are two parameters in the
model-based pseudo feedback process: the noisy pa-
DELM pseudo DELM+pseudo Impr.(%)
AP 0.2505 0.2643 0.2726 3.14%*
LA 0.2655 0.2769 0.2901 4.77%
TREC8 0.2671 0.2716 0.2809 3.42%**
DOE 0.1898 0.1918 0.2046 6.67%***
Table 6: Combination with pseudo feed-
back.*,**,*** indicate that we accept the improve-
ment hypothesis by Wilcoxon test at significant
level 0.1, 0.05, 0.01 respectively.
pseu. inter. combined (%) z-score
AP 0.2643 0.2450 0.2660 (0.64%) -0.2888
LA 0.2769 0.2662 0.2636 (-0.48%) -1.0570
TREC8 0.2716 0.2702 0.2739 (0.84%) -1.6938
Table 7: Performance of the interpolation algorithm
combined with the pseudo feedback.
rameter ? and the interpolation parameter ?2. We fix
? = 0.9 and tune ? to optimal, and use them directly
in the feedback process combined with our models.
(It again means that ? is probably not optimal in our
results.) The combination is conducted in the fol-
lowing way: (1) Retrieve documents by our DELM
method; (2) Choose top 5 document to do the model-
based feedback; (3) Use the expanded query model
to retrieve documents again with DELM method.
Table 6 shows the experiment results (MAP); in-
deed, by combining DELM with pseudo feedback,
we can obtain significant further improvement of
performance.
As another baseline, we also tested the algorithm
proposed in (Kurland and Lee, 2004). Since the al-
gorithm overlaps with pseudo feedback process, it is
not easy to further combine them. We implement its
best-performing algorithm, ?interpolation? (labeled
as inter. ), and show the results in Table 7. Here,
we use the same three data sets as used in (Kurland
and Lee, 2004). We tune the feedback parameters to
optimal in each experiment. The second last column
in Table 7 shows the performance of combination of
the ?interpolation? model with the pseudo feedback
and its improvement percentage. The last column is
the z-scores of Wilcoxon test. The negative z-scores
indicate that none of the improvement is significant.
2 (Zhai and Lafferty, 2001a) uses different notations. We
change them because ? has already been used in our own
model.
413
4 Conclusions
In this paper, we proposed a novel document expan-
sion method to enrich the document sample through
exploiting the local corpus structure. Unlike pre-
vious cluster-based models, we smooth each doc-
ument using a probabilistic neighborhood centered
around the document itself.
Experiment results show that (1) The proposed
document expansion method outperforms both the
?no expansion? baselines and the cluster-based mod-
els. (2) Our model is relatively insensitive to the set-
ting of parameter M as long as it is sufficiently large,
while the parameter ? should be set according to the
document length; short documents need a smaller
? to obtain more help from its neighborhood. (3)
Document expansion can be combined with pseudo
feedback to further improve performance. Since any
retrieval model can be presumably applied on top of
the expanded documents, we believe that the pro-
posed technique can be potentially useful for any re-
trieval model.
5 Acknowledgments
This work is in part supported by the National Sci-
ence Foundation under award number IIS-0347933.
We thank Xiaoyong Liu for kindly providing us sev-
eral processed data sets for our performance com-
parison. We thank Jing Jiang and Azadeh Shakery
for helping improve the paper writing, and thank the
anonymous reviewers for their useful comments.
References
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-cal Report TR-10-98, Harvard University.
D. Hiemstra and W. Kraaij. 1998. Twenty-one at trec-7:
Ad-hoc and cross-language track. In Proc. of Seventh
Text REtrieval Conference (TREC-7).
R. Kneser and H. Ney. 1995. Improved smoothing for m-
gram languagemodeling. In Proceedings of the Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Oren Kurland and Lillian Lee. 2004. Corpus structure,language models, and ad hoc information retrieval. In
SIGIR ?04: Proceedings of the 27th annual interna-
tional conference on Research and development in in-
formation retrieval, pages 194?201. ACM Press.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-tion for information retrieval. In Proceedings of SI-
GIR?2001, pages 111?119, Sept.
John Lafferty and ChengXiang Zhai. 2003. Probabilistic
relevance models based on document and query gen-eration.
Victor Lavrenko and Bruce Croft. 2001. Relevance-based language models. In Proceedings of SI-
GIR?2001, Sept.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-basedretrieval using language models. In SIGIR ?04: Pro-
ceedings of the 27th annual international conference
on Research and development in information retrieval,pages 186?193. ACM Press.
D. H. Miller, T. Leek, and R. Schwartz. 1999. A hid-den markov model information retrieval system. In
Proceedings of the 1999 ACM SIGIR Conference on
Research and Development in Information Retrieval,pages 214?221.
J. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
the ACM SIGIR, pages 275?281.
J. Rocchio. 1971. Relevance feedback in information re-trieval. In The SMART Retrieval System: Experiments
in Automatic Document Processing, pages 313?323.Prentice-Hall Inc.
Amit Singhal and Fernando Pereira. 1999. Documentexpansion for speech retrieval. In SIGIR ?99: Pro-
ceedings of the 22nd annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 34?41. ACM Press.
E. Voorhees and D. Harman, editors. 2001. Proceedings
of Text REtrieval Conference (TREC1-9). NIST Spe-cial Publications. http://trec.nist.gov/pubs.html.
Chengxiang Zhai and John Lafferty. 2001a. Model-based feedback in the KL-divergence retrieval model.
In Tenth International Conference on Information and
Knowledge Management (CIKM 2001), pages 403?410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied toad hoc information retrieval. In Proceedings of SI-
GIR?2001, pages 334?342, Sept.
414
Proceedings of NAACL HLT 2007, pages 113?120,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Systematic Exploration of the Feature Space for Relation Extraction
Jing Jiang and ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{jiang4,czhai}@cs.uiuc.edu
Abstract
Relation extraction is the task of find-
ing semantic relations between entities
from text. The state-of-the-art methods
for relation extraction are mostly based
on statistical learning, and thus all have
to deal with feature selection, which can
significantly affect the classification per-
formance. In this paper, we systemat-
ically explore a large space of features
for relation extraction and evaluate the ef-
fectiveness of different feature subspaces.
We present a general definition of fea-
ture spaces based on a graphic represen-
tation of relation instances, and explore
three different representations of relation
instances and features of different com-
plexities within this framework. Our ex-
periments show that using only basic unit
features is generally sufficient to achieve
state-of-the-art performance, while over-
inclusion of complex features may hurt
the performance. A combination of fea-
tures of different levels of complexity and
from different sentence representations,
coupled with task-oriented feature prun-
ing, gives the best performance.
1 Introduction
An important information extraction task is relation
extraction, whose goal is to detect and characterize
semantic relations between entities in text. For ex-
ample, the text fragment ?hundreds of Palestinians
converged on the square? contains the located rela-
tion between the Person entity ?hundreds of Pales-
tinians? and the Bounded-Area entity ?the square?.
Relation extraction has applications in many do-
mains, including finding affiliation relations from
web pages and finding protein-protein interactions
from biomedical literature.
Recent studies on relation extraction have shown
the advantages of discriminative model-based sta-
tistical machine learning approach to this problem.
There are generally two lines of work following this
approach. The first utilizes a set of carefully se-
lected features obtained from different levels of text
analysis, from part-of-speech (POS) tagging to full
parsing and dependency parsing (Kambhatla, 2004;
Zhao and Grishman, 2005; Zhou et al, 2005)1. The
second line of work designs kernel functions on
some structured representation (sequences or trees)
of the relation instances to capture the similarity be-
tween two relation instances (Zelenko et al, 2003;
Culotta and Sorensen, 2004; Bunescu and Mooney,
2005a; Bunescu and Mooney, 2005b; Zhang et al,
2006a; Zhang et al, 2006b). Of particular interest
among the various kernels proposed are the convolu-
tion kernels (Bunescu and Mooney, 2005b; Zhang et
al., 2006a), because they can efficiently compute the
similarity between two instances in a huge feature
space due to their recursive nature. Apart from their
computational efficiency, convolution kernels also
implicitly correspond to some feature space. There-
fore, both lines of work rely on an appropriately de-
1Although Zhao and Grishman (2005) defined a number of
kernels for relation extraction, the method is essentially similar
to feature-based methods.
113
fined set of features. As in any learning problem, the
choice of features can affect the performance signif-
icantly.
Despite the importance of feature selection, there
has not been any systematic exploration of the fea-
ture space for relation extraction, and the choices
of features in existing work are somewhat arbitrary.
In this paper, we conduct a systematic study of the
feature space for relation extraction, and evaluate
the effectiveness of different feature subspaces. Our
motivations are twofold. First, based on previous
studies, we want to identify and characterize the
types of features that are potentially useful for rela-
tion extraction, and define a relatively complete and
structured feature space that can be systematically
explored. Second, we want to compare the effective-
ness of different features. Such a study can guide us
to choose the most effective feature set for relation
extraction, or to design convolution kernels in the
most effective way.
We propose and define a unified graphic repre-
sentation of the feature space, and experiment with
three feature subspaces, corresponding to sequences,
syntactic parse trees and dependency parse trees.
Experiment results show that each subspace is ef-
fective by itself, with the syntactic parse tree sub-
space being the most effective. Combining the three
subspaces does not generate much improvement.
Within each feature subspace, using only the basic
unit features can already give reasonably good per-
formance. Adding more complex features may not
improve the performance much or may even hurt
the performance. Task-oriented heuristics can be
used to prune the feature space, and when appropri-
ately done, can improve the performance. A com-
bination of features of different levels of complex-
ity and from different sentence representations, cou-
pled with task-oriented feature pruning, gives the
best performance.
2 Related Work
Zhao and Grishman (2005) and Zhou et al (2005)
explored a large set of features that are potentially
useful for relation extraction. However, the feature
space was defined and explored in a somewhat ad
hoc manner. We study a broader scope of features
and perform a more systematic study of different
feature subspaces. Zelenko et al (2003) and Culotta
and Sorensen (2004) used tree kernels for relation
extraction. These kernels can achieve high precision
but low recall because of the relatively strict match-
ing criteria. Bunescu and Mooney (2005a) proposed
a dependency path kernel for relation extraction.
This kernel also suffers from low recall for the same
reason. Bunescu and Mooney (2005b) and Zhang
et. al. (2006a; 2006b) applied convolution string ker-
nels and tree kernels, respectively, to relation extrac-
tion. The convolution tree kernels achieved state-
of-the-art performance. Since convolution kernels
correspond to some explicit large feature spaces, the
feature selection problem still remains.
General structural representations of natural lan-
guage data have been studied in (Suzuki et al,
2003; Cumby and Roth, 2003), but these models
were not designed specifically for relation extrac-
tion. Our feature definition is similar to these mod-
els, but more specifically designed for relation ex-
traction and systematic exploration of the feature
space. Compared with (Cumby and Roth, 2003), our
feature space is more compact and provides more
guidance on selecting meaningful subspaces.
3 Task Definition
Given a small piece of text that contains two entity
mentions, the task of relation extraction is to decide
whether the text states some semantic relation be-
tween the two entities, and if so, classify the rela-
tion into one of a set of predefined semantic rela-
tion types. Formally, let r = (s, arg1, arg2) de-
note a relation instance, where s is a sentence, arg1
and arg2 are two entity mentions contained in s, and
arg1 precedes arg2 in the text. Given a set of rela-
tion instances {ri}, each labeled with a type ti ? T ,
where T is the set of predefined relation types plus
the type nil, our goal is to learn a function that maps
a relation instance r to a type t ? T . Note that we
do not specify the representation of s here. Indeed, s
can contain more structured information in addition
to merely the sequence of tokens in the sentence.
4 Feature Space for Relation Extraction
Ideally, we would like to define a feature space with
at least two properties: (1) It should be complete in
the sense that all features potentially useful for the
114
classification problem are included. (2) It should
have a good structure so that a systematic search in
the space is possible. Below we show how a unified
graph-based feature space can be defined to satisfy
these two properties.
4.1 A Unified View of Features for Relation
Extraction
Before we introduce our definition of the feature
space, let us first look at some typical features used
for relation extraction. Consider the relation in-
stance ?hundreds of Palestinians converged on the
square? with arg1 = ?hundreds of Palestinians? and
arg2 = ?the square?. Various types of information
can be useful for classifying this relation instance.
For example, knowing that arg1 is an entity of type
Person can be useful. This feature involves the sin-
gle token ?Palestinians?. Another feature, ?the head
word of arg1 (Palestinians) is followed by a verb
(converged)?, can also be useful. This feature in-
volves two tokens, ?Palestinians? and ?converged?,
with a sequence relation. It also involves the knowl-
edge that ?Palestinians? is part of arg1 and ?con-
verged? is a verb. If we have the syntactic parse tree
of the sentence, we can obtain even more complex
and discriminative features. For example, the syn-
tactic parse tree of the same relation instance con-
tains the following subtree: [VP ? VBD [PP ? [IN
? on] NP] ]. If we know that arg2 is contained in the
NP in this subtree, then this subtree states that arg2
is in a PP that is attached to a VP, and the proposition
is ?on?. This subtree therefore may also a useful
feature. Similarly, if we have the dependency parse
tree of the relation instance, then the dependency
link ?square ? on? states that the token ?square?
is dependent on the token ?on?, which may also be
a useful feature.
Given that useful features are of various forms, in
order to systematically search the feature space, we
need to first have a unified view of features. This
problem is not trivial because it is not immediately
clear how different types of features can be unified.
We observe, however, that in general features fall
into two categories: (1) properties of a single token,
and (2) relations between tokens. Features that in-
volve attributes of a single token, such as bag-of-
word features and entity attribute features, belong
to the first category, while features that involve se-
quence, syntactic or dependency relations between
tokens belong to the second category. Motivated by
this observation, we can represent relation instances
as graphs, with nodes denoting single tokens or syn-
tactic categories such as NPs and VPs, and edges de-
noting various types of relations between the nodes.
4.2 Relation Instance Graphs
We represent a relation instance as a labeled, di-
rected graph G = (V,E,A,B), where V is the set
of nodes in the graph, E is the set of directed edges
in the graph, and A, B are functions that assign la-
bels to the nodes.
First, for each node v ? V , A(v) =
{a1, a2, . . . , a|A(v)|} is a set of attributes associated
with node v, where ai ? ?, and ? is an alphabet that
contains all possible attribute values. The attributes
are introduced to help generalize the node. For ex-
ample, if node v represents a token, then A(v) can
include the token itself, its morphological base form,
its POS, its semantic class (e.g. WordNet synset),
etc. If v also happens to be the head word of arg1 or
arg2, then A(v) can also include the entity type and
other entity attributes. If node v represents a syntac-
tic category such as an NP or VP, A(v) can simply
contain only the syntactic tag.
Next, function B : V ? {0, 1, 2, 3} is introduced
to distinguish argument nodes from non-argument
nodes. For each node v ? V , B(v) indicates how
node v is related to arg1 and arg2. 0 indicates that
v does not cover any argument, 1 or 2 indicates that
v covers arg1 or arg2, respectively, and 3 indicates
that v covers both arguments. We will see shortly
that only nodes that represent syntactic categories in
a syntactic parse tree can possibly be assigned 3. We
refer to B(v) as the argument tag of v.
We now consider three special instantiations of
this general definition of relation instance graphs.
See Figures 1, 2 and 3 for examples of each of the
three representations.
Sequence: Without introducing any additional
structured information, a sequence representation
preserves the order of the tokens as they occur in the
original sentence. Each node in this graph is a token
augmented with its relevant attributes. For example,
head words of arg1 and arg2 are augmented with the
corresponding entity types. A token is assigned the
argument tag 1 or 2 if it is the head word of arg1 or
115
NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area
00 1 0 0 0 2
Person VBD1 0 Bounded-Area2
Figure 1: An example sequence representation. The
subgraph on the left represents a bigram feature. The
subgraph on the right represents a unigram feature
that states the entity type of arg2.
NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area
00 1 0 0 0 2
NPB NPB
PP
NP
1
1
0
1
S
VP
PP
NPB
3
2
2
2
on DT Bounded-Area0 0 2
PP
NPB
2 2
Figure 2: An example syntactic parse tree represen-
tation. The subgraph represents a subtree feature
(grammar production feature).
arg2. Otherwise, it is assigned the argument tag 0.
There is a directed edge from u to v if and only if
the token represented by v immediately follows that
represented by u in the sentence.
Syntactic Parse Tree: The syntactic parse tree
of the relation instance sentence can be augmented
to represent the relation instance. First, we modify
the tree slightly by conflating each leaf node in the
original parse tree with its parent, which is a preter-
minal node labeled with a POS tag. Then, each node
is augmented with relevant attributes if necessary.
Argument tags are assigned to the leaf nodes in the
same way as in the sequence representation. For an
internal node v, argument tag 1 or 2 is assigned if
either arg1 or arg2 is inside the subtree rooted at v,
and 3 is assigned if both arguments are inside the
subtree. Otherwise, 0 is assigned to v.
Dependency Parse Tree: Similarly, the depen-
dency parse tree can also be modified to represent
the relation instance. Assignment of attributes and
argument tags is the same as for the sequence repre-
sentation. To simplify the representation, we ignore
NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area
00 1 0 0 0 2
of Palestinians10
Figure 3: An example dependency parse tree rep-
resentation. The subgraph represents a dependency
relation feature between arg1 ?Palestinians? and
?of?.
the dependency relati types.
4.3 Features
Given the above definition of relation instance
graphs, we are now ready to define features. Intu-
itively, a feature of a relation instance captures part
of the attributive and/or structural properties of the
relation instance graph. Therefore, it is natural to de-
fine a feature as a subgraph of the relation instance
graph. Formally, given a graph G = (V,E,A,B),
which represents a single relation instance, a fea-
ture that exists in this relation instance is a sub-
graph G? = (V ?, E?, A?, B?) that satisfies the fol-
lowing conditions: V ? ? V , E? ? E, and ?v ?
V ?, A?(v) ? A(v), B?(v) = B(v).
We now show that many features that have been
explored in previous work on relation extraction can
be transformed into this graphic representation. See
Figures 1, 2 and 3 for some examples.
Entity Attributes: Previous studies have shown
that entity types and entity mention types of arg1
and arg2 are very useful (Zhao and Grishman, 2005;
Zhou et al, 2005; Zhang et al, 2006b). To represent
a single entity attribute, we can take a subgraph that
contains only the node representing the head word of
the argument, labeled with the entity type or entity
mention type. A particularly useful type of features
are conjunctive entity features, which are conjunc-
tions of two entity attributes, one for each argument.
To represent a conjunctive feature such as ?arg1 is
a Person entity and arg2 is a Bounded-Area entity?,
we can take a subgraph that contains two nodes, one
for each argument, and each labeled with an en-
tity attribute. Note that in this case, the subgraph
contains two disconnected components, which is al-
lowed by our definition.
Bag-of-Words: These features have also been
116
explore by Zhao and Grishman (2005) and Zhou
et. al. (2005). To represent a bag-of-word feature,
we can simply take a subgraph that contains a single
node labeled with the token. Because the node also
has an argument tag, we can distinguish between ar-
gument word and non-argument word.
Bigrams: A bigram feature (Zhao and Grishman,
2005) can be represented by a subgraph consisting
of two connected nodes from the sequence represen-
tation, where each node is labeled with the token.
Grammar Productions: The features in convo-
lution tree kernels for relation extraction (Zhang et
al., 2006a; Zhang et al, 2006b) are sequences of
grammar productions, that is, complete subtrees of
the syntactic parse tree. Therefore, these features
can naturally be represented by subgraphs of the re-
lation instance graphs.
Dependency Relations and Dependency Paths:
These features have been explored by Bunescu and
Mooney (2005a), Zhao and Grishman (2005), and
Zhou et. al. (2005). A dependency relation can be
represented as an edge connecting two nodes from
the dependency tree. The dependency path between
the two arguments can also be easily represented as
a path in the dependency tree connecting the two
nodes that represent the two arguments.
There are some features that are not covered by
our current definition, but can be included if we
modify our relation instance graphs. For example,
gapped subsequence features in subsequence ker-
nels (Bunescu and Mooney, 2005b) can be repre-
sented as subgraphs of the sequence representation
if we add more edges to connect any pair of nodes u
and v provided that the token represented by u oc-
curs somewhere before that represented by v in the
sentence. Since our feature definition is very gen-
eral, our feature space also includes many features
that have not been explored before.
4.4 Searching the Feature Space
Although the feature space we have defined is rel-
atively complete and has a clear structure, it is still
too expensive to exhaustively search the space be-
cause the number of features is exponential in terms
of the size of the relation instance graph. We thus
propose to search the feature space in the follow-
ing bottom-up manner: We start with the conjunc-
tive entity features (defined in Section 4.3), which
have been found effective in previous studies and
are intuitively necessary for relation extraction. We
then systematically add unit features with different
granularities. We first consider the minimum (i.e.
most basic) unit features. We then gradually include
more complex features. The motivations for this
strategy are the following: (1) Using the smallest
features to represent a relation instance graph pre-
sumably covers all unit characteristics of the graph.
(2) Using small subgraphs allows fuzzy matching,
which is good for our task because relation instances
of the same type may vary in their relation instance
graphs, especially with the noise introduced by ad-
jectives, adverbs, or irrelevant propositional phrases.
(3) The number of features of a fixed small size is
polynomial in terms of the size of the relation in-
stance graph. It is therefore feasible to generate all
the small unit features and use any classifier such as
a maximum entropy classifier or an SVM.
In our experiments, we consider three levels of
small unit features in increasing order of their com-
plexity. First, we consider unigram features Guni =
({u}, ?, Auni , B), where Auni(u) = {ai} ? A(u).
In another word, unigram features consist of a sin-
gle node labeled with a single attribute. Examples
of unigram features include bag-of-word features
and non-conjunctive entity attribute features. At the
second level, we consider bigram features Gbi =
({u, v}, {(u, v)}, Auni , B). Bigram features are
therefore single edges connecting two nodes, where
each node is labeled with a single attribute. The
third level of attributes we consider are trigram fea-
tures Gtri = ({u, v, w}, {(u, v), (u,w)}, Auni , B)
or Gtri = ({u, v, w}, {(u, v), (v, w)}, Auni , B).
Thus trigram features consist of two connected
edges and three nodes, where each node is also la-
beled with a single attribute.
We treat the three relation instance graphs (se-
quences, syntactic parse trees, and dependency parse
trees) as three feature subspaces, and search in each
subspace. For each feature subspace, we incremen-
tally add the unigram, bigram and trigram features
to the working feature set. For the syntactic parse
tree representation, we also consider a fourth level of
small unit features, which are single grammar pro-
ductions such as [VP ? VBD PP], because these
are the smallest features in convolution tree kernels.
After we explore each feature subspace, we try to
117
combine the features from the three subspaces to see
whether the performance can be improved, that is,
we test whether the sequence, syntactic and depen-
dency relations can complement each other.
5 Experiments
5.1 Data Set and Experiment Setup
We used the data set from ACE (Automatic Con-
tent Extraction) 2004 evaluation to conduct our ex-
periments. This corpus defines 7 types of relations:
Physical, Personal / Social, Empolyment / Memeber-
ship / Subsidiary, Agent-Artifact, PER / ORG Affili-
ation, GPE Affiliation and Discourse.
We used Collins parser to parse the sentences in
the corpus because Collins parser gives us the head
of each syntactic category, which allows us to trans-
form the syntactic parse trees into dependency trees.
We discarded sentences that could not be parsed
by Collins parser. The candidate relation instances
were generated by considering all pairs of entities
that occur in the same sentence. We obtained 48625
candidate relation instances in total, among which
4296 instances were positive.
As in most existing work, instead of using the en-
tire sentence, we used only the sequence of tokens
that are inside the minimum complete subtree cov-
ering the two arguments. Presumably, tokens out-
side of this subtree are not so relevant to the task. In
our graphic representation of relation instances, the
attribute set for a token node includes the token it-
self, its POS tag, and entity type, entity subtype and
entity mention type when applicable. The attribute
set for a syntactic category node includes only the
syntactic tag. We used both maximum entropy clas-
sifier and SVM for all experiments. We adopted one
vs. others strategy for the multi-class classification
problem. In all experiments, the performance shown
was based on 5-fold cross validation.
5.2 General Search in the Feature Subspaces
Following the general search strategy, we conducted
the following experiments. For each feature sub-
space, we started with the conjunctive entity features
plus the unigram features. We then incrementally
added bigram and trigram features. For the syntac-
tic parse tree feature space, we conducted an addi-
tional experiment: We added basic grammar produc-
tion features on top of the unigram, bigram and tri-
gram features. Adding production features allows us
to study the effect of adding more complex and pre-
sumably more specific and discriminative features.
Table 1 shows the precision (P), recall (R) and F1
measure (F) from the experiments with the maxi-
mum entropy classifier (ME) and the SVM classi-
fier (SVM). We can compare the results in two di-
mensions. First, within each feature subspace, while
bigram features improved the performance signifi-
cantly over unigrams, trigrams did not improve the
performance very much. This trend is observed for
both classifiers. In the case of the syntactic parse tree
subspace, adding production features even hurt the
performance. This suggests that inclusion of com-
plex features is not guaranteed to improve the per-
formance.
Second, if we compare the best performance
achieved in each feature subspace, we can see that
for both classifiers, syntactic parse tree is the most
effective feature space, while sequence and depen-
dency tree are similar. However, the difference in
performance between the syntactic parse tree sub-
space and the other two subspaces is not very large.
This suggests that each feature subspace alone al-
ready captures most of the useful structural informa-
tion between tokens for relation extraction. The rea-
son why the sequence feature subspace gave good
performance although it contained the least struc-
tural information is probably that many relations de-
fined in the ACE corpus are short-range relations,
some within single noun phrases. For such kind of
relations, sequence information may be even more
reliable than syntactic or dependency information,
which may not be accurate due to parsing errors.
Next, we conducted experiments to combine the
features from the three subspaces to see whether
this could further improve the performance. For se-
quence subspace and dependency tree subspace, we
used up to bigram features, and for syntactic parse
tree subspace, we used up to trigram features. In Ta-
ble 2, we show the experiment results. We can see
that for both classifiers, adding features from the se-
quence subspace or from the dependency tree sub-
space to the syntactic parse tree subspace can im-
prove the performance slightly. But combining se-
quence subspace and dependency tree subspace does
not generate any performance improvement. Again,
118
Uni +Bi +Tri +Prod
P 0.647 0.662 0.717
Seq R 0.614 0.701 0.653 N/A
F 0.630 0.681 0.683
P 0.651 0.695 0.726 0.702
ME Syn R 0.645 0.698 0.688 0.691
F 0.648 0.697 0.707 0.696
P 0.647 0.673 0.718
Dep R 0.614 0.676 0.652 N/A
F 0.630 0.674 0.683
P 0.583 0.666 0.684
Seq R 0.586 0.650 0.648 N/A
F 0.585 0.658 0.665
P 0.598 0.645 0.679 0.674
SVM Syn R 0.611 0.663 0.681 0.672
F 0.604 0.654 0.680 0.673
P 0.583 0.644 0.682
Dep R 0.586 0.638 0.645 N/A
F 0.585 0.641 0.663
Table 1: Comparison among the three feature sub-
spaces and the effect of including larger features.
Seq+Syn Seq+Dep Syn+Dep All
P 0.737 0.687 0.695 0.724
ME R 0.694 0.682 0.731 0.702
F 0.715 0.684 0.712 0.713
P 0.689 0.669 0.687 0.691
SVM R 0.686 0.653 0.682 0.686
F 0.688 0.661 0.684 0.688
Table 2: The effect of combining the three feature
subspaces.
this suggests that since many of the ACE relations
are local, there is likely much overlap between se-
quence information and dependency information.
We also tried the convolution tree kernel
method (Zhang et al, 2006a), using an SVM tree
kernel package2. The performance we obtained was
P = 0.705, R = 0.685, and F = 0.6953. This F mea-
sure is higher than the best SVM performance in Ta-
ble 1. The convolution tree kernel uses large subtree
features, but such features are deemphasized with
an exponentially decaying weight. We found that
the performance was sensitive to this decaying fac-
tor, suggesting that complex features can be useful
if they are weighted appropriately, and further study
of how to optimize the weights of such complex fea-
tures is needed.
2http://ai-nlp.info.uniroma2.it/moschitti/Tree-Kernel.htm
3The performance we achieved is lower than that reported
in (Zhang et al, 2006b), due to different data preprocessing,
data partition, and parameter setting.
5.3 Task-Oriented Feature Pruning
Apart from the general bottom-up search strategy we
have proposed, we can also introduce some task-
oriented heuristics based on intuition or domain
knowledge to prune the feature space. In our ex-
periments, we tried the following heuristics.
H1: Zhang et al (2006a) found that using path-
enclosed tree performed better than using minimum
complete tree, when convolution tree kernels were
applied. In path-enclosed trees, tokens before arg1
and after arg2 as well as their links with other nodes
in the tree are removed. Based on this previous
finding, our first heuristic was to change the syntac-
tic parse tree representation of the relation instances
into path-enclosed trees.
H2: We hypothesize that words such as articles,
adjectives and adverbs are not very useful for rela-
tion extraction. We thus removed sequence unigram
features and bigram features that contain an article,
adjective or adverb.
H3: Similar to H2, we can remove bigrams in the
syntactic parse tree subspace if the bigram contains
an article, adjective or adverb.
H4: Similar to H1, we can also remove the to-
kens before arg1 and after arg2 from the sequence
representation of a relation instance.
In Table 3, we show the performance after apply-
ing these heuristics. We started with the best con-
figuration from our previous experiments, that is,
combing up to bigram features in the sequence sub-
space and up to trigram features in the syntactic tree
subspace. We then applied heuristics H1 to H4 in-
crementally unless we saw that a heuristic was not
effective. We found that H1, H2 and H4 slightly
improved the performance, but H3 hurt the perfor-
mance. On the one hand, the improvement suggests
that our original feature configuration included some
irrelevant features, and in turn confirmed that over-
inclusion of features could hurt the performance. On
the other hand, since the improvement brought by
H1, H2 and H4 was rather small, and H3 even hurt
the performance, we could see that it is in general
very hard to find good feature pruning heuristics.
6 Conclusions and Future Work
In this paper, we conducted a systematic study of
the feature space for relation extraction. We pro-
119
ME SVM
P R F P R F
Best 0.737 0.694 0.715 0.689 0.686 0.688
+H1 0.714 0.729 0.721 0.698 0.699 0.699
+H2 0.730 0.723 0.726 0.704 0.704 0.704
+H3 0.739 0.704 0.721 0.701 0.696 0.698
-H3+H4 0.746 0.713 0.729 0.702 0.701 0.702
Table 3: The effect of various heuristic feature prun-
ing methods.
posed and defined a unified graphic representation
of features for relation extraction, which serves as a
general framework for systematically exploring fea-
tures defined on natural language sentences. With
this framework, we explored three different repre-
sentations of sentences?sequences, syntactic parse
trees, and dependency trees?which lead to three
feature subspaces. In each subspace, starting with
the basic unit features, we systematically explored
features of different levels of complexity. The stud-
ied feature space includes not only most of the ef-
fective features explored in previous work, but also
some features that have not been considered before.
Our experiment results showed that using a set of
basic unit features from each feature subspace, we
can achieve reasonably good performance. When
the three subspaces are combined, the performance
can improve only slightly, which suggests that the
sequence, syntactic and dependency relations have
much overlap for the task of relation extraction. We
also found that adding more complex features may
not improve the performance much, and may even
hurt the performance. A combination of features
of different levels of complexity and from different
sentence representations, coupled with task-oriented
feature pruning, gives the best performance.
In our future work, we will study how to auto-
matically conduct task-oriented feature search, fea-
ture pruning and feature weighting using statistical
methods instead of heuristics. In this study, we only
considered features from the local context, i.e. the
sentence that contains the two arguments. Some ex-
isting studies use corpus-based statistics for relation
extraction (Hasegawa et al, 2004). In the future, we
will study the effectiveness of these global features.
Acknowledgments
This work was in part supported by the National Sci-
ence Foundation under award numbers 0425852 and
0428472. We thank Alessandro Moschitti for pro-
viding the SVM tree kernel package. We also thank
Min Zhang for providing the implementation details
of the convolution tree kernel for relation extraction.
References
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kenrel for relation extrac-
tion. In Proceedings of HLT/EMNLP.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of ICML.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku
Maeda. 2003. Hierarchical directed acyclic graph ker-
nel: Methods for structured natural language data. In
Proceedings of ACL.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083?1106.
Min Zhang, Jie Zhang, and Jian Su. 2006a. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In Proceedings of HLT/NAACL.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006b. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of ACL.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In Proceedings of ACL.
120
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Named Entity Transliteration with Comparable Corpora
Richard Sproat, Tao Tao, ChengXiang Zhai
University of Illinois at Urbana-Champaign, Urbana, IL, 61801
rws@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate Chinese-
English name transliteration using compa-
rable corpora, corpora where texts in the
two languages deal in some of the same
topics ? and therefore share references
to named entities ? but are not transla-
tions of each other. We present two dis-
tinct methods for transliteration, one ap-
proach using phonetic transliteration, and
the second using the temporal distribu-
tion of candidate pairs. Each of these ap-
proaches works quite well, but by com-
bining the approaches one can achieve
even better results. We then propose a
novel score propagation method that uti-
lizes the co-occurrence of transliteration
pairs within document pairs. This prop-
agation method achieves further improve-
ment over the best results from the previ-
ous step.
1 Introduction
As part of a more general project on multilin-
gual named entity identification, we are interested
in the problem of name transliteration across lan-
guages that use different scripts. One particular is-
sue is the discovery of named entities in ?compara-
ble? texts in multiple languages, where by compa-
rable we mean texts that are about the same topic,
but are not in general translations of each other.
For example, if one were to go through an English,
Chinese and Arabic newspaper on the same day,
it is likely that the more important international
events in various topics such as politics, business,
science and sports, would each be covered in each
of the newspapers. Names of the same persons,
locations and so forth ? which are often translit-
erated rather than translated ? would be found in
comparable stories across the three papers.1 We
wish to use this expectation to leverage translit-
eration, and thus the identification of named enti-
ties across languages. Our idea is that the occur-
rence of a cluster of names in, say, an English text,
should be useful if we find a cluster of what looks
like the same names in a Chinese or Arabic text.
An example of what we are referring to can be
found in Figure 1. These are fragments of two
stories from the June 8, 2001 Xinhua English and
Chinese newswires, each covering an international
women?s badminton championship. Though these
two stories are from the same newswire source,
and cover the same event, they are not translations
of each other. Still, not surprisingly, a lot of the
names that occur in one, also occur in the other.
Thus (Camilla) Martin shows up in the Chinese
version as ??? ma-er-ting; Judith Meulendijks
is ??????? yu mo-lun-di-ke-si; and Mette
Sorensen is ?????mai su-lun-sen. Several
other correspondences also occur. While some of
the transliterations are ?standard? ? thus Martin
is conventionally transliterated as ??? ma-er-
ting ? many of them were clearly more novel,
though all of them follow the standard Chinese
conventions for transliterating foreign names.
These sample documents illustrate an important
point: if a document in language L1 has a set of
names, and one finds a document in L2 containing
a set of names that look as if they could be translit-
erations of the names in the L1 document, then
this should boost one?s confidence that the two sets
of names are indeed transliterations of each other.
We will demonstrate that this intuition is correct.
1Many names, particularly of organizations, may be trans-
lated rather than transliterated; the transliteration method we
discuss here obviously will not account for such cases, though
the time correlation and propagation methods we discuss will
still be useful.
73
Dai Yun Nips World No. 1 Martin to Shake off Olympic
Shadow . . . In the day?s other matches, second seed Zhou Mi
overwhelmed Ling Wan Ting of Hong Kong, China 11-4, 11-
4, Zhang Ning defeat Judith Meulendijks of Netherlands 11-
2, 11-9 and third seed Gong Ruina took 21 minutes to elimi-
nate Tine Rasmussen of Denmark 11-1, 11-1, enabling China
to claim five quarterfinal places in the women?s singles.
? ? ? ? ? ?  ? ? ? ? ? ? ? ? ? ? ?
. . . ??????,????????4???,?? . . .
? ? ? ? ? ? ? ? ? ? ? ? 11:1? ? ? ? ? ?
?? ????,??????11:2?11:9????? ?
???????,??????11:4?11:1? ???
??????
Figure 1: Sample from two stories about an inter-
national women?s badminton championship.
2 Previous Work
In previous work on Chinese named-entity
transliteration ? e.g. (Meng et al, 2001; Gao
et al, 2004), the problem has been cast as the
problem of producing, for a given Chinese name,
an English equivalent such as one might need in
a machine translation system. For example, for
the name ??????wei wei-lian-mu-si, one
would like to arrive at the English name V(enus)
Williams. Common approaches include source-
channel methods, following (Knight and Graehl,
1998) or maximum-entropy models.
Comparable corpora have been studied exten-
sively in the literature (e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et al,
2000; Sadat et al, 2003)), but transliteration in the
context of comparable corpora has not been well
addressed.
The general idea of exploiting frequency corre-
lations to acquire word translations from compara-
ble corpora has been explored in several previous
studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka
and Iwasaki, 1996)).Recently, a method based on
Pearson correlation was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005), an idea similar to the method used in (Kay
and Roscheisen, 1993) for sentence alignment. In
our work, we adopt the method proposed in (Tao
and Zhai, 2005) and apply it to the problem of
transliteration. We also study several variations of
the similarity measures.
Mining transliterations from multilingual web
pages was studied in (Zhang and Vines, 2004);
Our work differs from this work in that we use
comparable corpora (in particular, news data) and
leverage the time correlation information naturally
available in comparable corpora.
3 Chinese Transliteration with
Comparable Corpora
We assume that we have comparable corpora, con-
sisting of newspaper articles in English and Chi-
nese from the same day, or almost the same day. In
our experiments we use data from the English and
Chinese stories from the Xinhua News agency for
about 6 months of 2001.2 We assume that we have
identified names for persons and locations?two
types that have a strong tendency to be translit-
erated wholly or mostly phonetically?in the En-
glish text; in this work we use the named-entity
recognizer described in (Li et al, 2004), which
is based on the SNoW machine learning toolkit
(Carlson et al, 1999).
To perform the transliteration task, we propose
the following general three-step approach:
1. Given an English name, identify candi-
date Chinese character n-grams as possible
transliterations.
2. Score each candidate based on how likely the
candidate is to be a transliteration of the En-
glish name. We propose two different scoring
methods. The first involves phonetic scoring,
and the second uses the frequency profile of
the candidate pair over time. We will show
that each of these approaches works quite
well, but by combining the approaches one
can achieve even better results.
3. Propagate scores of all the candidate translit-
eration pairs globally based on their co-
occurrences in document pairs in the compa-
rable corpora.
The intuition behind the third step is the following.
Suppose several high-confidence name transliter-
ation pairs occur in a pair of English and Chi-
nese documents. Intuitively, this would increase
our confidence in the other plausible translitera-
tion pairs in the same document pair. We thus pro-
pose a score propagation method to allow these
high-confidence pairs to propagate some of their
2Available from the LDC via the English Gigaword
(LDC2003T05) and Chinese Gigaword (LDC2003T09) cor-
pora.
74
scores to other co-occurring transliteration pairs.
As we will show later, such a propagation strat-
egy can generally further improve the translitera-
tion accuracy; in particular, it can further improve
the already high performance from combining the
two scoring methods.
3.1 Candidate Selection
The English named entity candidate selection pro-
cess was already described above. Candidate Chi-
nese transliterations are generated by consulting
a list of characters that are frequently used for
transliterating foreign names. As discussed else-
where (Sproat et al, 1996), a subset of a few hun-
dred characters (out of several thousand) tends to
be used overwhelmingly for transliterating foreign
names into Chinese. We use a list of 495 such
characters, derived from various online dictionar-
ies. A sequence of three or more characters from
the list is taken as a possible name. If the character
??? occurs, which is frequently used to represent
the space between parts of an English name, then
at least one character to the left and right of this
character will be collected, even if the character in
question is not in the list of ?foreign? characters.
Armed with the English and Chinese candidate
lists, we then consider the pairing of every En-
glish candidate with every Chinese candidate. Ob-
viously it would be impractical to do this for all of
the candidates generated for, say, an entire year:
we consider as plausible pairings those candidates
that occur within a day of each other in the two
corpora.
3.2 Candidate scoring based on
pronunciation
We adopt a source-channel model for scoring
English-Chinese transliteration pairs. In general,
we seek to estimate P (e|c), where e is a word in
Roman script, and c is a word in Chinese script.
Since Chinese transliteration is mostly based on
pronunciation, we estimate P (e?|c?), where e? is
the pronunciation of e and c? is the pronunciation
of c. Again following standard practice, we de-
compose the estimate of P (e?|c?) as P (e?|c?) =
?
i P (e?i|c?i). Here, e?i is the ith subsequence of
the English phone string, and c?i is the ith subse-
quence of the Chinese phone string. Since Chi-
nese transliteration attempts to match the syllable-
sized characters to equivalent sounding spans of
the English language, we fix the c?i to be syllables,
and let the e?i range over all possible subsequences
of the English phone string. For training data we
have a small list of 721 names in Roman script and
their Chinese equivalent.3 Pronunciations for En-
glish words are obtained using the Festival text-to-
speech system (Taylor et al, 1998); for Chinese,
we use the standard pinyin transliteration of the
characters. English-Chinese pairs in our training
dictionary were aligned using the alignment algo-
rithm from (Kruskal, 1999), and a hand-derived
set of 21 rules-of-thumb: for example, we have
rules that encode the fact that Chinese /l/ can cor-
respond to English /r/, /n/ or /er/; and that Chinese
/w/ may be used to represent /v/. Given that there
are over 400 syllables in Mandarin (not count-
ing tone) and each of these syllables can match
a large number of potential English phone spans,
this is clearly not enough training data to cover all
the parameters, and so we use Good-Turing esti-
mation to estimate probabilities for unseen corre-
spondences. Since we would like to filter implau-
sible transliteration pairs we are less lenient than
standard estimation techniques in that we are will-
ing to assign zero probability to some correspon-
dences. Thus we set a hard rule that for an En-
glish phone span to correspond to a Chinese sylla-
ble, the initial phone of the English span must have
been seen in the training data as corresponding to
the initial of the Chinese syllable some minimum
number of times. For consonant-initial syllables
we set the minimum to 4. We omit further details
of our estimation technique for lack of space. This
phonetic correspondence model can then be used
to score putative transliteration pairs.
3.3 Candidate Scoring based on Frequency
Correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event.
Thus if we have comparable news articles over a
sufficiently long time period, it is possible to ex-
ploit such correlations to learn the associations of
names in different languages. The idea of exploit-
ing frequency correlation has been well studied.
(See the previous work section.) We adopt the
method proposed in (Tao and Zhai, 2005), which
3The LDC provides a much larger list of transliterated
Chinese-English names, but we did not use this here for two
reasons. First, we have found it it be quite noisy. Secondly,
we were interested in seeing how well one could do with a
limited resource of just a few hundred names, which is a more
realistic scenario for languages that have fewer resources than
English and Chinese.
75
works as follows: We pool all documents in a sin-
gle day to form a large pseudo-document. Then,
for each transliteration candidate (both Chinese
and English), we compute its frequency in each
of those pseudo-documents and obtain a raw fre-
quency vector. We further normalize the raw fre-
quency vector so that it becomes a frequency dis-
tribution over all the time points (days). In order
to compute the similarity between two distribution
vectors, The Pearson correlation coefficient was
used in (Tao and Zhai, 2005); here we also consid-
ered two other commonly used measures ? cosine
(Salton and McGill, 1983), and Jensen-Shannon
divergence (Lin, 1991), though our results show
that Pearson correlation coefficient performs bet-
ter than these two other methods.
3.4 Score Propagation
In both scoring methods described above, scoring
of each candidate transliteration pair is indepen-
dent of the other. As we have noted, document
pairs that contain lots of plausible transliteration
pairs should be viewed as more plausible docu-
ment pairs; at the same time, in such a situation we
should also trust the putative transliteration pairs
more. Thus these document pairs and translitera-
tion pairs mutually ?reinforce? each other, and this
can be exploited to further optimize our translit-
eration scores by allowing transliteration pairs to
propagate their scores to each other according to
their co-occurrence strengths.
Formally, suppose the current generation of
transliteration scores are (ei, ci, wi) i = 1, ..., n,
where (ei, ci) is a distinct pair of English and Chi-
nese names. Note that although for any i 6= j, we
have (ei, ci) 6= (ej , cj), it is possible that ei = ej
or ci = cj for some i 6= j. wi is the transliteration
score of (ei, ci).
These pairs along with their co-occurrence re-
lation computed based on our comparable cor-
pora can be formally represented by a graph as
shown in Figure 2. In such a graph, a node repre-
sents (ei, ci, wi). An edge between (ei, ci, wi) and
(ej , cj , wj) is constructed iff (ei, ci) and (ej , cj)
co-occur in a certain document pair (Et, Ct), i.e.
there exists a document pair (Et, Ct), such that
ei, ej ? Et and ci, cj ? Ct. Given a node
(ei, ci, wi), we refer to all its directly-connected
nodes as its ?neighbors?. The documents do not
appear explicitly in the graph, but they implicitly
affect the graph?s topology and the weight of each
edge. Our idea of score propagation can now be
formulated as the following recursive equation for
w1
w4
w2
w3
w5
w6
w7
(e4, c4)
(e3, c3)
(e5, c5)
(e5, c5)
(e2, c2)
(e7, c7)
(e6, c6)
Figure 2: Graph representing transliteration pairs
and cooccurence relations.
updating the scores of all the transliteration pairs.
w(k)i = ?? w
(k?1)
i + (1 ? ?) ?
n
?
j 6=i,j=1
(w(k?1)j ? P (j|i)),
where w(k)i is the new score of the pair (ei, ci)
after an iteration, while w(k?1)i is its old score
before updating; ? ? [0, 1] is a parameter to
control the overall amount of propagation (when
? = 1, no propagation occurs); P (j|i) is the con-
ditional probability of propagating a score from
node (ej , cj , wj) to node (ei, ci, wi).
We estimate P (j|i) in two different ways: 1)
The number of cooccurrences in the whole collec-
tion (Denote as CO). P (j|i) = C(i,j)?
j? C(i,j?)
, where
C(i, j) is the cooccurrence count of (ei, ci) and
(ej , cj); 2) A mutual information-based method
(Denote as MI). P (j|i) = MI(i,j)?
j? MI(i,j?)
, where
MI(i, j) is the mutual information of (ei, ci) and
(ej , cj). As we will show, the CO method works
better. Note that the transition probabilities be-
tween indirect neighbors are always 0. Thus prop-
agation only happens between direct neighbors.
This formulation is very similar to PageRank,
a link-based ranking algorithm for Web retrieval
(Brin and Page, 1998). However, our motivation
is propagating scores to exploit cooccurrences, so
we do not necessarily want the equation to con-
verge. Indeed, our results show that although the
initial iterations always help improve accuracy, too
many iterations actually would decrease the per-
formance.
4 Evaluation
We use a comparable English-Chinese corpus to
evaluate our methods for Chinese transliteration.
We take one day?s worth of comparable news arti-
cles (234 Chinese stories and 322 English stories),
generate about 600 English names with the entity
recognizer (Li et al, 2004) as described above, and
76
find potential Chinese transliterations also as pre-
viously described. We generated 627 Chinese can-
didates. In principle, all these 600 ? 627 pairs are
potential transliterations. We then apply the pho-
netic and time correlation methods to score and
rank all the candidate Chinese-English correspon-
dences.
To evaluate the proposed transliteration meth-
ods quantitatively, we measure the accuracy of the
ranked list by Mean Reciprocal Rank (MRR), a
measure commonly used in information retrieval
when there is precisely one correct answer (Kan-
tor and Voorhees, 2000). The reciprocal rank is
the reciprocal of the rank of the correct answer.
For example, if the correct answer is ranked as the
first, the reciprocal rank would be 1.0, whereas if
it is ranked the second, it would be 0.5, and so
forth. To evaluate the results for a set of English
names, we take the mean of the reciprocal rank of
each English name.
We attempted to create a complete set of an-
swers for all the English names in our test set,
but a small number of English names do not seem
to have any standard transliteration according to
the resources that we consulted. We ended up
with a list of about 490 out of the 600 English
names judged. We further notice that some an-
swers (about 20%) are not in our Chinese candi-
date set. This could be due to two reasons: (1) The
answer does not occur in the Chinese news articles
we look at. (2) The answer is there, but our candi-
date generation method has missed it. In order to
see more clearly how accurate each method is for
ranking the candidates, we also compute the MRR
for the subset of English names whose transliter-
ation answers are in our candidate list. We dis-
tinguish the MRRs computed on these two sets of
English names as ?AllMRR? and ?CoreMRR?.
Below we first discuss the results of each of the
two methods. We then compare the two methods
and discuss results from combining the two meth-
ods.
4.1 Phonetic Correspondence
We show sample results for the phonetic scoring
method in Table 1. This table shows the 10 high-
est scoring transliterations for each Chinese char-
acter sequence based on all texts in the Chinese
and English Xinhua newswire for the 13th of Au-
gust, 2001. 8 out of these 10 are correct. For all
the English names the MRR is 0.3, and for the
?paris ??? pei-lei-si 3.51
iraq ??? yi-la-ke 3.74
staub ??? si-ta-bo 4.45
canada ?? jia-na-da 4.85
belfast ????? bei-er-fa-si-te 4.90
fischer ??? fei-she-er 4.91
philippine ??? fei-lu?-bin 4.97
lesotho ?? lai-suo-two 5.12
?tirana ??? tye-lu-na 5.15
freeman ??? fu-li-man 5.26
Table 1: Ten highest-scoring matches for the Xin-
hua corpus for 8/13/01. The final column is the
?log P estimate for the transliteration. Starred
entries are incorrect.
core names it is 0.89. Thus on average, the cor-
rect answer, if it is included in our candidate list,
is ranked mostly as the first one.
4.2 Frequency correlation
Similarity AllMRR CoreMRR
Pearson 0.1360 0.3643
Cosine 0.1141 0.3015
JS-div 0.0785 0.2016
Table 2: MRRs of the frequency correlation meth-
ods.
We proposed three similarity measures for the
frequency correlation method, i.e., the Cosine,
Pearson coefficient, and Jensen-Shannon diver-
gence. In Table 2, we show their MRRs. Given
that the only resource the method needs is compa-
rable text documents over a sufficiently long pe-
riod, these results are quite encouraging. For ex-
ample, with Pearson correlation, when the Chinese
transliteration of an English name is included in
our candidate list, the correct answer is, on aver-
age, ranked at the 3rd place or better. The results
thus show that the idea of exploiting frequency
correlation does work. We also see that among
the three similarity measures, Pearson correlation
performs the best; it performs better than Cosine,
which is better than JS-divergence.
Compared with the phonetic correspondence
method, the performance of the frequency correla-
tion method is in general much worse, which is not
surprising, given the fact that terms may be corre-
lated merely because they are topically related.
77
4.3 Combination of phonetic correspondence
and frequency correlation
Method AllMRR CoreMRR
Phonetic 0.2999 0.8895
Freq 0.1360 0.3643
Freq+PhoneticFilter 0.3062 0.9083
Freq+PhoneticScore 0.3194 0.9474
Table 3: Effectiveness of combining the two scor-
ing methods.
Since the two methods exploit complementary
resources, it is natural to see if we can improve
performance by combining the two methods. In-
deed, intuitively the best candidate is the one that
has a good pronunciation alignment as well as a
correlated frequency distribution with the English
name. We evaluated two strategies for combining
the two methods. The first strategy is to use the
phonetic model to filter out (clearly impossible)
candidates and then use the frequency correlation
method to rank the candidates. The second is to
combine the scores of these two methods. Since
the correlation coefficient has a maximum value
of 1, we normalize the phonetic correspondence
score by dividing all scores by the maximum score
so that the maximum normalized value is also 1.
We then take the average of the two scores and
rank the candidates based on their average scores.
Note that the second strategy implies the applica-
tion of the first strategy.
The results of these two combination strategies
are shown in Table 3 along with the results of the
two individual methods. We see that both com-
bination strategies are effective and the MRRs of
the combined results are all better than those of the
two individual methods. It is interesting to see that
the benefit of applying the phonetic correspon-
dence model as a filter is quite significant. Indeed,
although the performance of the frequency corre-
lation method alone is much worse than that of the
phonetic correspondence method, when working
on the subset of candidates passing the phonetic
filter (i.e., those candidates that have a reasonable
phonetic alignment with the English name), it can
outperform the phonetic correspondence method.
This once again indicates that exploiting the fre-
quency correlation can be effective. When com-
bining the scores of these two methods, we not
only (implicitly) apply the phonetic filter, but also
exploit the discriminative power provided by the
phonetic correspondence scores and this is shown
to bring in additional benefit, giving the best per-
formance among all the methods.
4.4 Error Analysis
From the results above, we see that the MRRs for
the core English names are substantially higher
than those for all the English names. This means
that our methods perform very well whenever we
have the answer in our candidate list, but we have
also missed the answers for many English names.
The missing of an answer in the candidate list is
thus a major source of errors. To further under-
stand the upper bound of our method, we manu-
ally add the missing correct answers to our can-
didate set and apply all the methods to rank this
augmented set of candidates. The performance is
reported in Table 4 with the corresponding perfor-
mance on the original candidate set. We see that,
Method ALLMRR
Original Augmented
Phonetic 0.2999 0.7157
Freq 0.1360 0.3455
Freq+PhoneticFilter 0.3062 0.6232
Freq+PhoneticScore 0.3194 0.7338
Table 4: MRRs on the augmented candidate list.
as expected, the performance on the augmented
candidate list, which can be interpreted as an up-
per bound of our method, is indeed much better,
suggesting that if we can somehow improve the
candidate generation method to include the an-
swers in the list, we can expect to significantly im-
prove the performance for all the methods. This
is clearly an interesting topic for further research.
The relative performance of different methods on
this augmented candidate list is roughly the same
as on the original candidate list, except that the
?Freq+PhoneticFilter? is slightly worse than that
of the phonetic method alone, though it is still
much better than the performance of the frequency
correlation alone. One possible explanation may
be that since these names do not necessarily oc-
cur in our comparable corpora, we may not have
sufficient frequency observations for some of the
names.
78
Method AllMRR CoreMRR
init. CO MI init. CO MI
Freq+PhoneticFilter 0.3171 0.3255 0.3255 0.9058 0.9372 0.9372
Freq+PhoneticScore 0.3290 0.3373 0.3392 0.9422 0.9659 0.9573
Table 5: Effectiveness of score propagation.
4.5 Experiments on score propagation
To demonstrate that score propagation can further
help transliteration, we use the combination scores
in Table 3 as the initial scores, and apply our prop-
agation algorithm to iteratively update them. We
remove the entries when they do not co-occur with
others. There are 25 such English name candi-
dates. Thus, the initial scores are actually slightly
different from the values in Table 3. We show
the new scores and the best propagation scores in
Table 5. In the table, ?init.? refers to the initial
scores. and ?CO? and ?MI? stand for best scores
obtained using either the co-occurrence or mutual
information method. While both methods result
in gains, CO very slightly outperforms the MI ap-
proach. In the score propagation process, we in-
troduce two additional parameters: the interpola-
tion parameter ? and the number of iterations k.
Figure 3 and Figure 4 show the effects of these
parameters. Intuitively, we want to preserve the
initial score of a pair, but add a slight boost from
its neighbors. Thus, we set ? very close to 1 (0.9
and 0.95), and allow the system to perform 20 it-
erations. In both figures, the first few iterations
certainly leverage the transliteration, demonstrat-
ing that the propagation method works. However,
we observe that the performance drops when more
iterations are used, presumably due to noise intro-
duced from more distantly connected nodes. Thus,
a relatively conservative approach is to choose a
high ? value, and run only a few iterations. Note,
finally, that the CO method seems to be more sta-
ble than the MI method.
5 Conclusions and Future Work
In this paper we have discussed the problem of
Chinese-English name transliteration as one com-
ponent of a system to find matching names in com-
parable corpora. We have proposed two methods
for transliteration, one that is more traditional and
based on phonetic correspondences, and one that
is based on word distributions and adopts meth-
ods from information retrieval. We have shown
 0.76
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 0  2  4  6  8  10  12  14  16  18  20
M
RR
 v
al
ue
s
number of iterations
alpha=0.9, MI
alpha=0.9, CO
alpha=0.95, MI
alpha=0.95, CO
Figure 3: Propagation: Core items
that both methods yield good results, and that even
better results can be achieved by combining the
methods. We have further showed that one can
improve upon the combined model by using rein-
forcement via score propagation when translitera-
tion pairs cluster together in document pairs.
The work we report is ongoing. We are inves-
tigating transliterations among several language
pairs, and are extending these methods to Ko-
rean, Arabic, Russian and Hindi ? see (Tao et al,
2006).
6 Acknowledgments
This work was funded by Dept. of the Interior con-
tract NBCHC040176 (REFLEX). We also thank
three anonymous reviewers for ACL06.
References
Lisa Ballesteros and W. Bruce Croft. 1998. Resolv-
ing ambiguity for cross-language retrieval. In Re-
search and Development in Information Retrieval,
pages 64?71.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks and ISDN Systems, 30:107?
117.
79
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  2  4  6  8  10  12  14  16  18  20
M
RR
 v
al
ue
s
number of iterations
alpha=0.9, MI
alpha=0.9, CO
alpha=0.95, MI
alpha=0.95, CO
Figure 4: Propagation: All items
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC CS Dept.
Martin Franz, J. Scott McCarley, and Salim Roukos.
1998. Ad hoc and multilingual information retrieval
at IBM. In Text REtrieval Conference, pages 104?
115.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of ACL 1995,
pages 236?243.
W. Gao, K.-F. Wong, and W. Lam. 2004. Phoneme-
based transliteration of foreign names for OOV
problem. In IJCNLP, pages 374?381, Sanya,
Hainan.
P. Kantor and E. Voorhees. 2000. The TREC-5 confu-
sion track: Comparing retrieval methods for scanned
text. Information Retrieval, 2:165?176.
M. Kay and M. Roscheisen. 1993. Text translation
alignment. Computational Linguistics, 19(1):75?
102.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. CL, 24(4).
J. Kruskal. 1999. An overview of sequence compar-
ison. In D. Sankoff and J. Kruskal, editors, Time
Warps, String Edits, and Macromolecules, chapter 1,
pages 1?44. CSLI, 2nd edition.
X. Li, P. Morie, and D. Roth. 2004. Robust reading:
Identification and tracing of ambiguous names. In
NAACL-2004.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
H. Masuichi, R. Flournoy, S. Kaufmann, and S. Peters.
2000. A bootstrapping method for extracting bilin-
gual text pairs.
H.M. Meng, W.K Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named enti-
ties in English-Chinese cross-languge spoken doc-
ument retrieval. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of ACL 1995, pages
320?322.
Fatiha Sadat, Masatoshi Yoshikawa, and Shunsuke Ue-
mura. 2003. Bilingual terminology acquisition from
comparable corpora and phrasal translation to cross-
language information retrieval. In ACL ?03, pages
141?144.
G. Salton and M. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A
stochastic finite-state word-segmentation algorithm
for Chinese. CL, 22(3).
K. Tanaka and H. Iwasaki. 1996. Extraction of lexical
translation from non-aligned corpora. In Proceed-
ings of COLING 1996.
Tao Tao and ChengXiang Zhai. 2005. Mining compa-
rable bilingual text corpora for cross-language infor-
mation integration. In KDD?05, pages 691?696.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard
Sproat, and ChengXiang Zhai. 2006. Unsupervised
named entity transliteration using temporal and pho-
netic correlation. In EMNLP 2006, Sydney, July.
P. Taylor, A. Black, and R. Caley. 1998. The archi-
tecture of the Festival speech synthesis system. In
Proceedings of the Third ESCA Workshop on Speech
Synthesis, pages 147?151, Jenolan Caves, Australia.
Ying Zhang and Phil Vines. 2004. Using the web for
automated translation extraction in cross-language
information retrieval. In SIGIR ?04, pages 162?169.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264?271,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Instance Weighting for Domain Adaptation in NLP
Jing Jiang and ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{jiang4,czhai}@cs.uiuc.edu
Abstract
Domain adaptation is an important problem
in natural language processing (NLP) due to
the lack of labeled data in novel domains. In
this paper, we study the domain adaptation
problem from the instance weighting per-
spective. We formally analyze and charac-
terize the domain adaptation problem from
a distributional view, and show that there
are two distinct needs for adaptation, cor-
responding to the different distributions of
instances and classification functions in the
source and the target domains. We then
propose a general instance weighting frame-
work for domain adaptation. Our empir-
ical results on three NLP tasks show that
incorporating and exploiting more informa-
tion from the target domain through instance
weighting is effective.
1 Introduction
Many natural language processing (NLP) problems
such as part-of-speech (POS) tagging, named entity
(NE) recognition, relation extraction, and seman-
tic role labeling, are currently solved by supervised
learning from manually labeled data. A bottleneck
problem with this supervised learning approach is
the lack of annotated data. As a special case, we
often face the situation where we have a sufficient
amount of labeled data in one domain, but have little
or no labeled data in another related domain which
we are interested in. We thus face the domain adap-
tation problem. Following (Blitzer et al, 2006), we
call the first the source domain, and the second the
target domain.
The domain adaptation problem is commonly en-
countered in NLP. For example, in POS tagging, the
source domain may be tagged WSJ articles, and the
target domain may be scientific literature that con-
tains scientific terminology. In NE recognition, the
source domain may be annotated news articles, and
the target domain may be personal blogs. Another
example is personalized spam filtering, where we
may have many labeled spam and ham emails from
publicly available sources, but we need to adapt the
learned spam filter to an individual user?s inbox be-
cause the user has her own, and presumably very dif-
ferent, distribution of emails and notion of spams.
Despite the importance of domain adaptation in
NLP, currently there are no standard methods for
solving this problem. An immediate possible solu-
tion is semi-supervised learning, where we simply
treat the target instances as unlabeled data but do
not distinguish the two domains. However, given
that the source data and the target data are from dif-
ferent distributions, we should expect to do better
by exploiting the domain difference. Recently there
have been some studies addressing domain adapta-
tion from different perspectives (Roark and Bacchi-
ani, 2003; Chelba and Acero, 2004; Florian et al,
2004; Daume? III and Marcu, 2006; Blitzer et al,
2006). However, there have not been many studies
that focus on the difference between the instance dis-
tributions in the two domains. A detailed discussion
on related work is given in Section 5.
In this paper, we study the domain adaptation
problem from the instance weighting perspective.
264
In general, the domain adaptation problem arises
when the source instances and the target instances
are from two different, but related distributions.
We formally analyze and characterize the domain
adaptation problem from this distributional view.
Such an analysis reveals that there are two distinct
needs for adaptation, corresponding to the differ-
ent distributions of instances and the different clas-
sification functions in the source and the target do-
mains. Based on this analysis, we propose a gen-
eral instance weighting method for domain adapta-
tion, which can be regarded as a generalization of
an existing approach to semi-supervised learning.
The proposed method implements several adapta-
tion heuristics with a unified objective function: (1)
removing misleading training instances in the source
domain; (2) assigning more weights to labeled tar-
get instances than labeled source instances; (3) aug-
menting training instances with target instances with
predicted labels. We evaluated the proposed method
with three adaptation problems in NLP, including
POS tagging, NE type classification, and spam filter-
ing. The results show that regular semi-supervised
and supervised learning methods do not perform as
well as our new method, which explicitly captures
domain difference. Our results also show that in-
corporating and exploiting more information from
the target domain is much more useful for improv-
ing performance than excluding misleading training
examples from the source domain.
The rest of the paper is organized as follows. In
Section 2, we formally analyze the domain adapta-
tion problem and distinguish two types of adapta-
tion. In Section 3, we then propose a general in-
stance weighting framework for domain adaptation.
In Section 4, we present the experiment results. Fi-
nally, we compare our framework with related work
in Section 5 before we conclude in Section 6.
2 Domain Adaptation
In this section, we define and analyze domain adap-
tation from a theoretical point of view. We show that
the need for domain adaptation arises from two fac-
tors, and the solutions are different for each factor.
We restrict our attention to those NLP tasks that can
be cast into multiclass classification problems, and
we only consider discriminative models for classifi-
cation. Since both are common practice in NLP, our
analysis is applicable to many NLP tasks.
Let X be a feature space we choose to represent
the observed instances, and let Y be the set of class
labels. In the standard supervised learning setting,
we are given a set of labeled instances {(xi, yi)}Ni=1,
where xi ? X , yi ? Y , and (xi, yi) are drawn from
an unknown joint distribution p(x, y). Our goal is to
recover this unknown distribution so that we can pre-
dict unlabeled instances drawn from the same distri-
bution. In discriminative models, we are only con-
cerned with p(y|x). Following the maximum likeli-
hood estimation framework, we start with a parame-
terized model family p(y|x; ?), and then find the best
model parameter ?? that maximizes the expected log
likelihood of the data:
?? = argmax
?
?
X
?
y?Y
p(x, y) log p(y|x; ?)dx.
Since we do not know the distribution p(x, y), we
maximize the empirical log likelihood instead:
?? ? argmax
?
?
X
?
y?Y
p?(x, y) log p(y|x; ?)dx
= argmax
?
1
N
N?
i=1
log p(yi|xi; ?).
Note that since we use the empirical distribution
p?(x, y) to approximate p(x, y), the estimated ?? is
dependent on p?(x, y). In general, as long as we have
sufficient labeled data, this approximation is fine be-
cause the unlabeled instances we want to classify are
from the same p(x, y).
2.1 Two Factors for Domain Adaptation
Let us now turn to the case of domain adaptation
where the unlabeled instances we want to classify
are from a different distribution than the labeled in-
stances. Let ps(x, y) and pt(x, y) be the true un-
derlying distributions for the source and the target
domains, respectively. Our general idea is to use
ps(x, y) to approximate pt(x, y) so that we can ex-
ploit the labeled examples in the source domain.
If we factor p(x, y) into p(x, y) = p(y|x)p(x),
we can see that pt(x, y) can deviate from ps(x, y) in
two different ways, corresponding to two different
kinds of domain adaptation:
265
Case 1 (Labeling Adaptation): pt(y|x) deviates
from ps(y|x) to a certain extent. In this case, it is
clear that our estimation of ps(y|x) from the labeled
source domain instances will not be a good estima-
tion of pt(y|x), and therefore domain adaptation is
needed. We refer to this kind of adaptation as func-
tion/labeling adaptation.
Case 2 (Instance Adaptation): pt(y|x) is mostly
similar to ps(y|x), but pt(x) deviates from ps(x). In
this case, it may appear that our estimated ps(y|x)
can still be used in the target domain. However, as
we have pointed out, the estimation of ps(y|x) de-
pends on the empirical distribution p?s(x, y), which
deviates from pt(x, y) due to the deviation of ps(x)
from pt(x). In general, the estimation of ps(y|x)
would be more influenced by the instances with high
p?s(x, y) (i.e., high p?s(x)). If pt(x) is very differ-
ent from ps(x), then we should expect pt(x, y) to be
very different from ps(x, y), and therefore different
from p?s(x, y). We thus cannot expect the estimated
ps(y|x) to work well on the regions where pt(x, y)
is high, but ps(x, y) is low. Therefore, in this case,
we still need domain adaptation, which we refer to
as instance adaptation.
Because the need for domain adaptation arises
from two different factors, we need different solu-
tions for each factor.
2.2 Solutions for Labeling Adaptation
If pt(y|x) deviates from ps(y|x) to some extent, we
have one of the following choices:
Change of representation:
It may be the case that if we change the rep-
resentation of the instances, i.e., if we choose a
feature space X ? different from X , we can bridge
the gap between the two distributions ps(y|x) and
pt(y|x). For example, consider domain adaptive
NE recognition where the source domain contains
clean newswire data, while the target domain con-
tains broadcast news data that has been transcribed
by automatic speech recognition and lacks capital-
ization. Suppose we use a naive NE tagger that
only looks at the word itself. If we consider capi-
talization, then the instance Bush is represented dif-
ferently from the instance bush. In the source do-
main, ps(y = Person|x = Bush) is high while
ps(y = Person|x = bush) is low, but in the target
domain, pt(y = Person|x = bush) is high. If we
ignore the capitalization information, then in both
domains p(y = Person|x = bush) will be high pro-
vided that the source domain contains much fewer
instances of bush than Bush.
Adaptation through prior:
When we use a parameterized model p(y|x; ?)
to approximate p(y|x) and estimate ? based on the
source domain data, we can place some prior on the
model parameter ? so that the estimated distribution
p(y|x; ??) will be closer to pt(y|x). Consider again
the NE tagging example. If we use capitalization as
a feature, in the source domain where capitalization
information is available, this feature will be given a
large weight in the learned model because it is very
useful. If we place a prior on the weight for this fea-
ture so that a large weight will be penalized, then
we can prevent the learned model from relying too
much on this domain specific feature.
Instance pruning:
If we know the instances x for which pt(y|x) is
different from ps(y|x), we can actively remove these
instances from the training data because they are
?misleading?.
For all the three solutions given above, we need
either some prior knowledge about the target do-
main, or some labeled target domain instances;
from only the unlabeled target domain instances, we
would not know where and why pt(y|x) differs from
ps(y|x).
2.3 Solutions for Instance Adaptation
In the case where pt(y|x) is similar to ps(y|x), but
pt(x) deviates from ps(x), we may use the (unla-
beled) target domain instances to bias the estimate
of ps(x) toward a better approximation of pt(x), and
thus achieve domain adaptation. We explain the idea
below.
Our goal is to obtain a good estimate of ??t that is
optimized according to the target domain distribu-
tion pt(x, y). The exact objective function is thus
??t = argmax
?
?
X
?
y?Y
pt(x, y) log p(y|x; ?)dx
= argmax
?
?
X
pt(x)
?
y?Y
pt(y|x) log p(y|x; ?)dx.
266
Our idea of domain adaptation is to exploit the la-
beled instances in the source domain to help obtain
??t .
Let Ds = {(xsi , ysi )}Nsi=1 denote the set of la-
beled instances we have from the source domain.
Assume that we have a (small) set of labeled and
a (large) set of unlabeled instances from the tar-
get domain, denoted by Dt,l = {(xt,lj , yt,lj )}
Nt,l
j=1 and
Dt,u = {xt,uk }
Nt,u
k=1 , respectively. We now show three
ways to approximate the objective function above,
corresponding to using three different sets of in-
stances to approximate the instance space X .
Using Ds:
Using ps(y|x) to approximate pt(y|x), we obtain
??t ? argmax
?
?
X
pt(x)
ps(x)ps(x)
?
y?Y
ps(y|x) log p(y|x; ?)dx
? argmax
?
?
X
pt(x)
ps(x) p?s(x)
?
y?Y
p?s(y|x) log p(y|x; ?)dx
= argmax
?
1
Ns
Ns?
i=1
pt(xsi )
ps(xsi )
log p(ysi |xsi ; ?).
Here we use only the labeled instances in Ds but
we adjust the weight of each instance by pt(x)ps(x) . The
major difficulty is how to accurately estimate pt(x)ps(x) .
Using Dt,l:
??t ? argmax
?
?
X
p?t,l(x)
?
y?Y
p?t,l(y|x) log p(y|x; ?)dx
= argmax
?
1
Nt,l
Nt,l?
j=1
log p(yt,lj |xt,lj ; ?)
Note that this is the standard supervised learning
method using only the small amount of labeled tar-
get instances. The major weakness of this approxi-
mation is that when Nt,l is very small, the estimation
is not accurate.
Using Dt,u:
??t ? argmax
?
?
X
p?t,u(x)
?
y?Y
pt(y|x) log p(y|x; ?)dx
= argmax
?
1
Nt,u
Nt,u?
k=1
?
y?Y
pt(y|xt,uk ) log p(y|xt,uk ; ?),
The challenge here is that pt(y|xt,uk ; ?) is unknown
to us, thus we need to estimate it. One possibility
is to approximate it with a model ?? learned from
Ds and Dt,l. For example, we can set pt(y|x, ?) =
p(y|x; ??). Alternatively, we can also set pt(y|x, ?)
to 1 if y = argmaxy? p(y?|x; ??) and 0 otherwise.
3 A Framework of Instance Weighting for
Domain Adaptation
The theoretical analysis we give in Section 2 sug-
gests that one way to solve the domain adaptation
problem is through instance weighting. We propose
a framework that incorporates instance pruning in
Section 2.2 and the three approximations in Sec-
tion 2.3. Before we show the formal framework, we
first introduce some weighting parameters and ex-
plain the intuitions behind these parameters.
First, for each (xsi , ysi ) ? Ds, we introduce a pa-
rameter ?i to indicate how likely pt(ysi |xsi ) is close
to ps(ysi |xsi ). Large ?i means the two probabilities
are close, and therefore we can trust the labeled in-
stance (xsi , ysi ) for the purpose of learning a clas-
sifier for the target domain. Small ?i means these
two probabilities are very different, and therefore we
should probably discard the instance (xsi , ysi ) in the
learning process.
Second, again for each (xsi , ysi ) ? Ds, we intro-
duce another parameter ?i that ideally is equal to
pt(xsi )
ps(xsi ) . From the approximation in Section 2.3 thatuses only Ds, it is clear that such a parameter is use-
ful.
Next, for each xt,ui ? Dt,u, and for each possible
label y ? Y , we introduce a parameter ?i(y) that
indicates how likely we would like to assign y as a
tentative label to xt,ui and include (xt,ui , y) as a train-
ing example.
Finally, we introduce three global parameters ?s,
?t,l and ?t,u that are not instance-specific but are as-
sociated with Ds, Dt,l and Dt,u, respectively. These
three parameters allow us to control the contribution
of each of the three approximation methods in Sec-
tion 2.3 when we linearly combine them together.
We now formally define our instance weighting
framework. Given Ds, Dt,l and Dt,u, to learn a clas-
sifier for the target domain, we find a parameter ??
that optimizes the following objective function:
267
?? = argmax
?
[
?s ? 1Cs
Ns?
i=1
?i?i log p(ysi |xsi ; ?)
+?t,l ? 1Ct,l
Nt,l?
j=1
log p(yt,lj |xt,lj ; ?)
+?t,u ? 1Ct,u
Nt,u?
k=1
?
y?Y
?k(y) log p(y|xt,uk ; ?)
+ log p(?)
]
,
where Cs =
?Ns
i=1 ?i?i, Ct,l = Nt,l, Ct,u =?Nt,u
k=1
?
y?Y ?k(y), and ?s + ?t,l + ?t,u = 1. The
last term, log p(?), is the log of a Gaussian prior dis-
tribution of ?, commonly used to regularize the com-
plexity of the model.
In general, we do not know the optimal values of
these parameters for the target domain. Neverthe-
less, the intuitions behind these parameters serve as
guidelines for us to design heuristics to set these pa-
rameters. In the rest of this section, we introduce
several heuristics that we used in our experiments to
set these parameters.
3.1 Setting ?
Following the intuition that if pt(y|x) differs much
from ps(y|x), then (x, y) should be discarded from
the training set, we use the following heuristic to
set ?s. First, with standard supervised learning, we
train a model ??t,l from Dt,l. We consider p(y|x; ??t,l)
to be a crude approximation of pt(y|x). Then, we
classify {xsi}Nsi=1 using ??t,l. The top k instances
that are incorrectly predicted by ??t,l (ranked by their
prediction confidence) are discarded. In another
word, ?si of the top k instances for which ysi 6=
argmaxy p(y|xsi ; ??t,l) are set to 0, and ?i of all the
other source instances are set to 1.
3.2 Setting ?
Accurately setting ? involves accurately estimating
ps(x) and pt(x) from the empirical distributions.
For many NLP classification tasks, we do not have a
good parametric model for p(x). We thus need to re-
sort to non-parametric density estimation methods.
However, for many NLP tasks, x resides in a high
dimensional space, which makes it hard to apply
standard non-parametric density estimation meth-
ods. We have not explored this direction, and in our
experiments, we set ? to 1 for all source instances.
3.3 Setting ?
Setting ? is closely related to some semi-supervised
learning methods. One option is to set ?k(y) =
p(y|xt,uk ; ?). In this case, ? is no longer a constant
but is a function of ?. This way of setting ? corre-
sponds to the entropy minimization semi-supervised
learning method (Grandvalet and Bengio, 2005).
Another way to set ? corresponds to bootstrapping
semi-supervised learning. First, let ??(n) be a model
learned from the previous round of training. We then
select the top k instances from Dt,u that have the
highest prediction confidence. For these instances,
we set ?k(y) = 1 for y = argmaxy? p(y?|xt,uk ; ??(n)),
and ?k(y) = 0 for all other y. In another word, we
select the top k confidently predicted instances, and
include these instances together with their predicted
labels in the training set. All other instances in Dt,u
are not considered. In our experiments, we only con-
sidered this bootstrapping way of setting ?.
3.4 Setting ?
?s, ?t,l and ?t,u control the balance among the three
sets of instances. Using standard supervised learn-
ing, ?s and ?t,l are set proportionally to Cs and Ct,l,
that is, each instance is weighted the same whether
it is in Ds or in Dt,l, and ?t,u is set to 0. Similarly,
using standard bootstrapping, ?t,u is set proportion-
ally to Ct,u, that is, each target instance added to the
training set is also weighted the same as a source
instance. In neither case are the target instances em-
phasize more than source instances. However, for
domain adaptation, we want to focus more on the
target domain instances. So intuitively, we want to
make ?t,l and ?t,u somehow larger relative to ?s. As
we will show in Section 4, this is indeed beneficial.
In general, the framework provides great flexibil-
ity for implementing different adaptation strategies
through these instance weighting parameters.
4 Experiments
4.1 Tasks and Data Sets
We chose three different NLP tasks to evaluate our
instance weighting method for domain adaptation.
The first task is POS tagging, for which we used
268
6166 WSJ sentences from Sections 00 and 01 of
Penn Treebank as the source domain data, and 2730
PubMed sentences from the Oncology section of the
PennBioIE corpus as the target domain data. The
second task is entity type classification. The setup is
very similar to Daume? III and Marcu (2006). We
assume that the entity boundaries have been cor-
rectly identified, and we want to classify the types
of the entities. We used ACE 2005 training data
for this task. For the source domain, we used the
newswire collection, which contains 11256 exam-
ples, and for the target domains, we used the we-
blog (WL) collection (5164 examples) and the con-
versational telephone speech (CTS) collection (4868
examples). The third task is personalized spam fil-
tering. We used the ECML/PKDD 2006 discov-
ery challenge data set. The source domain contains
4000 spam and ham emails from publicly available
sources, and the target domains are three individual
users? inboxes, each containing 2500 emails.
For each task, we consider two experiment set-
tings. In the first setting, we assume there are a small
number of labeled target instances available. For
POS tagging, we used an additional 300 Oncology
sentences as labeled target instances. For NE typ-
ing, we used 500 labeled target instances and 2000
unlabeled target instances for each target domain.
For spam filtering, we used 200 labeled target in-
stances and 1800 unlabeled target instances. In the
second setting, we assume there is no labeled target
instance. We thus used all available target instances
for testing in all three tasks.
We used logistic regression as our model of
p(y|x; ?) because it is a robust learning algorithm
and widely used.
We now describe three sets of experiments, cor-
responding to three heuristic ways of setting ?, ?t,l
and ?t,u.
4.2 Removing ?Misleading? Source Domain
Instances
In the first set of experiments, we gradually remove
?misleading? labeled instances from the source do-
main, using the small number of labeled target in-
stances we have. We follow the heuristic we de-
scribed in Section 3.1, which sets the ? for the top
k misclassified source instances to 0, and the ? for
all the other source instances to 1. We also set ?t,l
and ?t,l to 0 in order to focus only on the effect of
removing ?misleading? instances. We compare with
a baseline method which uses all source instances
with equal weight but no target instances. The re-
sults are shown in Table 1.
From the table, we can see that in most exper-
iments, removing these predicted ?misleading? ex-
amples improved the performance over the baseline.
In some experiments (Oncology, CTS, u00, u01), the
largest improvement was achieved when all misclas-
sified source instances were removed. In the case of
weblog NE type classification, however, removing
the source instances hurt the performance. A pos-
sible reason for this is that the set of labeled target
instances we use is a biased sample from the target
domain, and therefore the model trained on these in-
stances is not always a good predictor of ?mislead-
ing? source instances.
4.3 Adding Labeled Target Domain Instances
with Higher Weights
The second set of experiments is to add the labeled
target domain instances into the training set. This
corresponds to setting ?t,l to some non-zero value,
but still keeping ?t,u as 0. If we ignore the do-
main difference, then each labeled target instance
is weighted the same as a labeled source instance
(?u,l?s =
Cu,l
Cs ), which is what happens in regular su-pervised learning. However, based on our theoret-
ical analysis, we can expect the labeled target in-
stances to be more representative of the target do-
main than the source instances. We can therefore
assign higher weights for the target instances, by ad-
justing the ratio between ?t,l and ?s. In our experi-
ments, we set ?t,l?s = a
Ct,l
Cs , where a ranges from 2 to20. The results are shown in Table 2.
As shown from the table, adding some labeled tar-
get instances can greatly improve the performance
for all tasks. And in almost all cases, weighting the
target instances more than the source instances per-
formed better than weighting them equally.
We also tested another setting where we first
removed the ?misleading? source examples as we
showed in Section 4.2, and then added the labeled
target instances. The results are shown in the last
row of Table 2. However, although both removing
?misleading? source instances and adding labeled
269
POS NE Type Spam
k Oncology k CTS k WL k u00 u01 u02
0 0.8630 0 0.7815 0 0.7045 0 0.6306 0.6950 0.7644
4000 0.8675 800 0.8245 600 0.7070 150 0.6417 0.7078 0.7950
8000 0.8709 1600 0.8640 1200 0.6975 300 0.6611 0.7228 0.8222
12000 0.8713 2400 0.8825 1800 0.6830 450 0.7106 0.7806 0.8239
16000 0.8714 3000 0.8825 2400 0.6795 600 0.7911 0.8322 0.8328
all 0.8720 all 0.8830 all 0.6600 all 0.8106 0.8517 0.8067
Table 1: Accuracy on the target domain after removing ?misleading? source domain instances.
POS NE Type Spam
method Oncology method CTS WL method u00 u01 u02
Ds only 0.8630 Ds only 0.7815 0.7045 Ds only 0.6306 0.6950 0.7644
Ds + Dt,l 0.9349 Ds + Dt,l 0.9340 0.7735 Ds + Dt,l 0.9572 0.9572 0.9461
Ds + 5Dt,l 0.9411 Ds + 2Dt,l 0.9355 0.7810 Ds + 2Dt,l 0.9606 0.9600 0.9533
Ds + 10Dt,l 0.9429 Ds + 5Dt,l 0.9360 0.7820 Ds + 5Dt,l 0.9628 09611 0.9601
Ds + 20Dt,l 0.9443 Ds + 10Dt,l 0.9355 0.7840 Ds + 10Dt,l 0.9639 0.9628 0.9633
D?s + 20Dt,l 0.9422 D?s + 10Dt,l 0.8950 0.6670 D?s + 10Dt,l 0.9717 0.9478 0.9494
Table 2: Accuracy on the unlabeled target instances after adding the labeled target instances.
target instances work well individually, when com-
bined, the performance in most cases is not as good
as when no source instances are removed. We hy-
pothesize that this is because after we added some
labeled target instances with large weights, we al-
ready gained a good balance between the source data
and the target data. Further removing source in-
stances would push the emphasis more on the set
of labeled target instances, which is only a biased
sample of the whole target domain.
The POS data set and the CTS data set have pre-
viously been used for testing other adaptation meth-
ods (Daume? III and Marcu, 2006; Blitzer et al,
2006), though the setup there is different from ours.
Our performance using instance weighting is com-
parable to their best performance (slightly worse for
POS and better for CTS).
4.4 Bootstrapping with Higher Weights
In the third set of experiments, we assume that we
do not have any labeled target instances. We tried
two bootstrapping methods. The first is a standard
bootstrapping method, in which we gradually added
the most confidently predicted unlabeled target in-
stances with their predicted labels to the training
set. Since we believe that the target instances should
in general be given more weight because they bet-
ter represent the target domain than the source in-
stances, in the second method, we gave the added
target instances more weight in the objective func-
tion. In particular, we set ?t,u = ?s such that the
total contribution of the added target instances is
equal to that of all the labeled source instances. We
call this second method the balanced bootstrapping
method. Table 3 shows the results.
As we can see, while bootstrapping can generally
improve the performance over the baseline where
no unlabeled data is used, the balanced bootstrap-
ping method performed slightly better than the stan-
dard bootstrapping method. This again shows that
weighting the target instances more is a right direc-
tion to go for domain adaptation.
5 Related Work
There have been several studies in NLP that address
domain adaptation, and most of them need labeled
data from both the source domain and the target do-
main. Here we highlight a few representative ones.
For generative syntactic parsing, Roark and Bac-
chiani (2003) have used the source domain data
to construct a Dirichlet prior for MAP estimation
of the PCFG for the target domain. Chelba and
Acero (2004) use the parameters of the maximum
entropy model learned from the source domain as
the means of a Gaussian prior when training a new
model on the target data. Florian et al (2004) first
train a NE tagger on the source domain, and then use
the tagger?s predictions as features for training and
testing on the target domain.
The only work we are aware of that directly mod-
270
POS NE Type Spam
method Oncology CTS WL u00 u01 u02
supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068
standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760
balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.9772
Table 3: Accuracy on the target domain without using labeled target instances. In balanced bootstrapping,
more weights are put on the target instances in the objective function than in standard bootstrapping.
els the different distributions in the source and the
target domains is by Daume? III and Marcu (2006).
They assume a ?truly source domain? distribution,
a ?truly target domain? distribution, and a ?general
domain? distribution. The source (target) domain
data is generated from a mixture of the ?truly source
(target) domain? distribution and the ?general do-
main? distribution. In contrast, we do not assume
such a mixture model.
None of the above methods would work if there
were no labeled target instances. Indeed, all the
above methods do not make use of the unlabeled
instances in the target domain. In contrast, our in-
stance weighting framework allows unlabeled target
instances to contribute to the model estimation.
Blitzer et al (2006) propose a domain adaptation
method that uses the unlabeled target instances to
infer a good feature representation, which can be re-
garded as weighting the features. In contrast, we
weight the instances. The idea of using pt(x)ps(x) toweight instances has been studied in statistics (Shi-
modaira, 2000), but has not been applied to NLP
tasks.
6 Conclusions and Future Work
Domain adaptation is a very important problem with
applications to many NLP tasks. In this paper,
we formally analyze the domain adaptation problem
and propose a general instance weighting framework
for domain adaptation. The framework is flexible to
support many different strategies for adaptation. In
particular, it can support adaptation with some target
domain labeled instances as well as that without any
labeled target instances. Experiment results on three
NLP tasks show that while regular semi-supervised
learning methods and supervised learning methods
can be applied to domain adaptation without con-
sidering domain difference, they do not perform as
well as our new method, which explicitly captures
domain difference. Our results also show that incor-
porating and exploiting more information from the
target domain is much more useful than excluding
misleading training examples from the source do-
main. The framework opens up many interesting
future research directions, especially those related to
how to more accurately set/estimate those weighting
parameters.
Acknowledgments
This work was in part supported by the National Sci-
ence Foundation under award numbers 0425852 and
0428472. We thank the anonymous reviewers for
their valuable comments.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120?128.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proc. of EMNLP, pages 285?292.
Hal Daume? III and Daniel Marcu. 2006. Domain adapta-
tion for statistical classifiers. J. Artificial Intelligence
Res., 26:101?126.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL, pages 1?8.
Y. Grandvalet and Y. Bengio. 2005. Semi-supervised
learning by entropy minimization. In NIPS.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptatin to novel domains.
In Proc. of HLT-NAACL, pages 126?133.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
271
Proceedings of ACL-08: HLT, pages 816?824,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generating Impact-Based Summaries for Scientific Literature
Qiaozhu Mei
University of Illinois at Urbana-
Champaign
qmei2@uiuc.edu
ChengXiang Zhai
University of Illinois at Urbana-
Champaign
czhai@cs.uiuc.edu
Abstract
In this paper, we present a study of a novel
summarization problem, i.e., summarizing the
impact of a scientific publication. Given a pa-
per and its citation context, we study how to
extract sentences that can represent the most
influential content of the paper. We propose
language modeling methods for solving this
problem, and study how to incorporate fea-
tures such as authority and proximity to ac-
curately estimate the impact language model.
Experiment results on a SIGIR publication
collection show that the proposed methods
are effective for generating impact-based sum-
maries.
1 Introduction
The volume of scientific literature has been growing
rapidly. From recent statistics, each year 400,000
new citations are added to MEDLINE, the major
biomedical literature database 1. This fast growth
of literature makes it difficult for researchers, espe-
cially beginning researchers, to keep track of the re-
search trends and find high impact papers on unfa-
miliar topics.
Impact factors (Kaplan and Nelson, 2000) are
useful, but they are just numerical values, so they
cannot tell researchers which aspects of a paper are
influential. On the other hand, a regular content-
based summary (e.g., the abstract or conclusion sec-
tion of a paper or an automatically generated topical
summary (Giles et al, 1998)) can help a user know
1http://www.nlm.nih.gov/bsd/history/tsld024.htm
about the main content of a paper, but not necessar-
ily the most influential content of the paper. Indeed,
the abstract of a paper mostly reflects the expected
impact of the paper as perceived by the author(s),
which could significantly deviate from the actual im-
pact of the paper in the research community. More-
over, the impact of a paper changes over time due to
the evolution and progress of research in a field. For
example, an algorithm published a decade ago may
be no longer the state of the art, but the problem def-
inition in the same paper can be still well accepted.
Although much work has been done on text sum-
marization (See Section 6 for a detailed survey), to
the best of our knowledge, the problem of impact
summarization has not been studied before. In this
paper, we study this novel summarization problem
and propose language modeling-based approaches
to solving the problem. By definition, the impact
of a paper has to be judged based on the consent of
research community, especially by people who cited
it. Thus in order to generate an impact-based sum-
mary, we must use not only the original content, but
also the descriptions of that paper provided in papers
which cited it, making it a challenging task and dif-
ferent from a regular summarization setup such as
news summarization. Indeed, unlike a regular sum-
marization system which identifies and interprets the
topic of a document, an impact summarization sys-
tem should identify and interpret the impact of a pa-
per.
We define the impact summarization problem in
the framework of extraction-based text summariza-
tion (Luhn, 1958; McKeown and Radev, 1995), and
cast the problem as an impact sentence retrieval
816
problem. We propose language models to exploit
both the citation context and original content of a
paper to generate an impact-based summary. We
study how to incorporate features such as author-
ity and proximity into the estimation of language
models. We propose and evaluate several different
strategies for estimating the impact language model,
which is key to impact summarization. No exist-
ing test collection is available for evaluating impact
summarization. We construct a test collection us-
ing 28 years of ACM SIGIR papers (1978 - 2005)
to evaluate the proposed methods. Experiment re-
sults on this collection show that the proposed ap-
proaches are effective for generating impact-based
summaries. The results also show that using both the
original document content and the citation contexts
is important and incorporating citation authority and
proximity is beneficial.
An impact-based summary is not only useful for
facilitating the exploration of literature, but also
helpful for suggesting query terms for literature
retrieval, understanding the evolution of research
trends, and identifying the interactions of different
research fields. The proposed methods are also ap-
plicable to summarizing the impact of documents in
other domains where citation context exists, such as
emails and weblogs.
The rest of the paper is organized as follows. In
Section 2 and 3, we define the impact-based summa-
rization problem and propose the general language
modeling approach. In Section 4, we present differ-
ent strategies and features for estimating an impact
language model, a key challenge in impact summa-
rization. We discuss our experiments and results in
Section 5. Finally, the related work and conclusions
are discussed in Section 6 and Section 7.
2 Impact Summarization
Following the existing work on topical summariza-
tion of scientific literature (Paice, 1981; Paice and
Jones, 1993), we define an impact-based summary
of a paper as a set of sentences extracted from
a paper that can reflect the impact of the paper,
where ?impact? is roughly defined as the influence
of the paper on research of similar or related top-
ics as reflected in the citations of the paper. Such
an extraction-based definition of summarization has
also been quite common in most existing general
summarization work (Radev et al, 2002).
By definition, in order to generate an impact sum-
mary of a paper, we must look at how other papers
cite the paper, use this information to infer the im-
pact of the paper, and select sentences from the orig-
inal paper that can reflect the inferred impact. Note
that we do not directly use the sentences from the ci-
tation context to form a summary. This is because in
citations, the discussion of the paper cited is usually
mixed with the content of the paper citing it, and
sometimes also with discussion about other papers
cited (Siddharthan and Teufel, 2007).
Formally, let d = (s0, s1, ..., sn) be a paper tobe summarized, where si is a sentence. We referto a sentence (in another paper) in which there is
an explicit citation of d as a citing sentence of d.
When a paper is cited, it is often discussed consec-
utively in more than one sentence near the citation,
thus intuitively we would like to consider a window
of sentences centered at a citing sentence; the win-
dow size would be a parameter to set. We call such
a window of sentences a citation context, and use C
to denote the union of all the citation contexts of d
in a collection of research papers. Thus C itself is
a set (more precisely bag) of sentences. The task
of impact-based summarization is thus to 1) con-
struct a representation of the impact of d, I , based
on d and C; 2) design a scoring function Score(.)
to rank sentences in d based on how well a sentence
reflects I . A user-defined number of top-ranked sen-
tences can then be selected as the impact summary
for d.
The formulation above immediately suggests that
we can cast the impact summarization problem as
a retrieval problem where each candidate sentence
in d is regarded as a ?document,? the impact of the
paper (i.e., I) as a ?query,? and our goal is to ?re-
trieve? sentences that can reflect the impact of the
paper as indicated by the citation context. Looking
at the problem in this way, we see that there are two
main challenges in impact summarization: first, we
must be able to infer the impact based on both the
citation contexts and the original document; second,
we should measure how well a sentence reflects this
inferred impact. To solve these challenges, in the
next section, we propose to model impact with un-
igram language models and score sentences using
817
Kullback-Leibler divergence. We further propose
methods for estimating the impact language model
based on several features including the authority of
citations, and the citation proximity.
3 Language Models for Impact
Summarization
3.1 Impact language models
From the retrieval perspective, our collection is the
paper to be summarized, and each sentence is a
?document? to be retrieved. However, unlike in the
case of ad hoc retrieval, we do not really have a
query describing the impact of the paper; instead,
we have a lot of citation contexts that can be used
to infer information about the query. Thus the main
challenge in impact summarization is to effectively
construct a ?virtual impact query? based on the cita-
tion contexts.
What should such a virtual impact query look
like? Intuitively, it should model the impact-
reflecting content of the paper. We thus propose to
represent such a virtual impact query with a unigram
language model. Such a model is expected to assign
high probabilities to those words that can describe
the impact of paper d, just as we expect a query
language model in ad hoc retrieval to assign high
probabilities to words that tend to occur in relevant
documents (Ponte and Croft, 1998). We call such a
language model the impact language model of paper
d (denoted as ?I ); it can be estimated based on both
d and its citation context C as will be discussed in
Section 4.
3.2 KL-divergence scoring
With the impact language model in place, we
can then adopt many existing probabilistic retrieval
models such as the classical probabilistic retrieval
models (Robertson and Sparck Jones, 1976) and the
Kullback-Leibler (KL) divergence retrieval model
(Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a),
to solve the problem of impact summarization by
scoring sentences based on the estimated impact lan-
guage model. In our study, we choose to use the KL-
divergence scoring method to score sentences as this
method has performed well for regular ad hoc re-
trieval tasks (Zhai and Lafferty, 2001a) and has an
information theoretic interpretation.
To apply the KL-divergence scoring method, we
assume that a candidate sentence s is generated from
a sentence language model ?s. Given s in d and thecitation context C , we would first estimate ?s basedon s and estimate ?I based on C , and then score swith the negative KL divergence of ?s and ?I . Thatis,
Score(s) = ?D(?I ||?s)
=
?
w?V
p(w|?I) log p(w|?s)?
?
w?V
p(w|?I) log p(w|?I)
where V is the set of words in our vocabulary and w
denotes a word.
From the information theoretic perspective, the
KL-divergence of ?s and ?I can be interpretedas measuring the average number of bits wasted
in compressing messages generated according to
?I (i.e., impact descriptions) with coding non-optimally designed based on ?s. If ?s and ?I arevery close, the KL-divergence would be small and
Score(s) would be high, which intuitively makes
sense. Note that the second term (entropy of ?I ) isindependent of s, so it can be ignored for ranking s.
We see that according to the KL-divergence scor-
ing method, our main tasks are to estimate ?s and
?I . Since s can be regarded as a short document, wecan use any standard method to estimate ?s. In thiswork, we use Dirichlet prior smoothing (Zhai and
Lafferty, 2001b) to estimate ?s as follows:
p(w|?s) =
c(w, s) + ?s ? P (w|D)
|s| + ?s
(1)
where |s| is the length of s, c(w, s) is the count of
word w in s, p(w|D) is a background model esti-
mated using c(w,D)P
w??V c(w?,D)
(D can be the set of all
the papers available to us) and ?s is a smoothing pa-rameter to be empirically set. Note that as the length
of a sentence is very short, smoothing is critical for
addressing the data sparseness problem.
The remaining challenge is to estimate ?I accu-rately based on d and its citation contexts.
4 Estimation of Impact Language Models
Intuitively, the impact of a paper is mostly reflected
in the citation context. Thus the estimation of the
impact language model should be primarily based
on the citation context C . However, we would like
818
our impact model to be able to help us select impact-
reflecting sentences from d, thus it is important for
the impact model to explain well the paper content
in general. To achieve this balance, we treat the ci-
tation context C as prior information and the current
document d as the observed data, and use Bayesian
estimation to estimate the impact language model.
Specifically, let p(w|C) be a citation context lan-
guage model estimated based on the citation con-
text C . We define Dirichlet prior with parameters
{?Cp(w|C)}w?V for the impact model, where ?Cencodes our confidence on this prior and effectively
serves as a weighting parameter for balancing the
contribution of C and d for estimating the impact
model. Given the observed document d, the poste-
rior mean estimate of the impact model would be
(MacKay and Peto, 1995; Zhai and Lafferty, 2001b)
P (w|?I) =
c(w, d) + ?cp(w|C)
|d| + ?c
(2)
?c can be interpreted as the equivalent sample size ofour prior. Thus setting ?c = |d| means that we putequal weights on the citation context and the doc-
ument itself. ?c = 0 yields p(w|?I) = p(w|d),which is to say that the impact is entirely captured
by the paper itself, and our impact summarization
problem would then become the standard single doc-
ument (topical) summarization. Intuitively though,
we would want to set ?c to a relatively large num-ber to exploit the citation context in our estimation,
which is confirmed in our experiments.
An alternative way is to simply interpolate p(w|d)
and p(w|C) with a constant coefficient:
p(w|?I) = (1 ? ?)p(w|d) + ?p(w|C) (3)
We will compare the two strategies in Section 5.
How do we estimate p(w|C)? Intuitively, words
occurring in C frequently should have high proba-
bilities. A simple way is to pool together all the sen-
tences in C and use the maximum likelihood estima-
tor,
p(w|C) =
?
s?C c(w, s)
?
w??V
?
s??C c(w?, s?)
(4)
where c(w, s) is the count of w in s.
One deficiency of this simple estimate is that we
treat all the (extended) citation sentences equally.
However, there are at least two reasons why we want
to assign unequal weights to different citation sen-
tences: (1) A sentence closer to the citation label
should contribute more than one far away. (2) A sen-
tence occurring in a highly authorative paper should
contribute more than that in a less authorative paper.
To capture these two heuristics, we define a weight
coefficient ?s for a sentence s in C as follows:
?s = pg(s)pr(s)
where pg(s) is an authority score of the paper con-
taining s and pr(s) is a proximity score that rewards
a sentence close to the citation label.
For example, pg(s) can be the PageRank value
(Brin and Page, 1998) of the document with s, which
measures the authority of the document based on a
citation graph, and is computed as follows: We con-
struct a directed graph from the collection of scien-
tific literature with each paper as a vertex and each
citation as a directed edge pointing from the citing
paper to the cited paper. We can then use the stan-
dard PageRank algorithm (Brin and Page, 1998) to
compute a PageRank value for each document. We
used this approach in our experiments.
We define pr(s) as pr(s) = 1?k , where k is thedistance (counted in terms of the number of sen-
tences) between sentence s and the center sentence
of the window containing s; by ?center sentence?,
we mean the citing sentence containing the citation
label. Thus the sentence with the citation label will
have a proximity of 1 (because k = 0), while the
sentences away from the citation label will have a
decaying weight controlled by parameter ?.
With ?s, we can then use the following?weighted? maximum likelihood estimate for the
impact language model:
p(w|C) =
?
s?C ?sc(w, s)
?
w??V
?
s??C ?s?c(w?, s?)
(5)
As we will show in Section 5, this weighted
maximum likelihood estimate performs better than
the simple maximum likelihood estimate, and both
pg(s) and pr(s) are useful.
819
5 Experiments and Results
5.1 Experiment Design
5.1.1 Test set construction
Because no existing test set is available for evalu-
ating impact summarization, we opt to create a test
set based on 28 years of ACM SIGIR papers (1978
- 2005) available through the ACM Digital Library2
and the SIGIR membership. Leveraging the explicit
citation information provided by ACM Digital Li-
brary, for each of the 1303 papers, we recorded all
other papers that cited the paper and extracted the
citation context from these citing papers. Each ci-
tation context contains 5 sentences with 2 sentences
before and after the citing sentence.
Since a low-impact paper would not be useful for
evaluating impact summarization, we took all the
14 papers from the SIGIR collection that have no
less than 20 citations by papers in the same col-
lection as candidate papers for evaluation. An ex-
pert in Information Retrieval field read each paper
and its citation context, and manually created an
impact-based summary by selecting all the ?impact-
capturing? sentences from the paper. Specifically,
the expert first attempted to understand the most in-
fluential content of a paper by reading the citation
contexts. The expert then read each sentence of
the paper and made a decision whether the sentence
covers some ?influential content? as indicated in the
citation contexts. The sentences that were decided
as covering some influential content were then col-
lected as the gold standard impact summary for the
paper.
We assume that the title of a paper will always
be included in the summary, so we excluded the ti-
tle both when constructing the gold standard and
when generating a summary. The gold standard
summaries have a minimum length of 5 sentences
and a maximum length of 18 sentences; the me-
dian length is 9 sentences. These 14 impact-based
summaries are used as gold standards for our exper-
iments, based on which all summaries generated by
the system are evaluated. This data set is available at
http://timan.cs.uiuc.edu/data/impact.html. We must
admit that using only 14 papers and only one expert
for evaluation is a limitation of our work. However,
2http://www.acm.org/dl
going beyond the 14 papers would risk reducing the
reliability of impact judgment due to the sparseness
of citations. How to develop a better test collection
is an important future direction.
5.1.2 Evaluation Metrics
Following the current practice in evaluating sum-
marization, particularly DUC3, we use the ROUGE
evaluation package (Lin and Hovy, 2003). Among
ROUGE metrics, ROUGE-N (models n-gram co-
occurrence, N = 1, 2) and ROUGE-L (models
longest common sequence) generally perform well
in evaluating both single-document summarization
and multi-document summarization (Lin and Hovy,
2003). Since they are general evaluation measures
for summarization, they are also applicable to eval-
uating the MEAD-Doc+Cite baseline method to be
described below. Thus although we evaluated our
methods with all the metrics provided by ROUGE,
we only report ROUGE-1 and ROUGE-L in this pa-
per (other metrics give very similar results).
5.1.3 Baseline methods
Since impact summarization has not been previ-
ously studied, there is no natural baseline method to
compare with. We thus adapt some state-of-the-art
conventional summarization methods implemented
in the MEAD toolkit (Radev et al, 2003)4 to obtain
three baseline methods: (1) LEAD: It simply ex-
tracts sentences from the beginning of a paper, i.e.,
sentences in the abstract or beginning of the intro-
duction section; we include LEAD to see if such
?leading sentences? reflect the impact of a paper as
authors presumably would expect to summarize a
paper?s contributions in the abstract. (2) MEAD-
Doc: It uses the single-document summarizer in
MEAD to generate a summary based solely on the
original paper; comparison with this baseline can
tell us how much better we can do than a conven-
tional topic-based summarizer that does not consider
the citation context. (3) MEAD-Doc+Cite: Here
we concatenate all the citation contexts in a paper to
form a ?citation document? and then use the MEAD
multidocument summarizer to generate a summary
from the original paper plus all its citation docu-
ments; this baseline represents a reasonable way
3http://duc.nist.gov/
4?http://www.summarization.com/mead/?
820
Sum. Length Metric Random LEAD MEAD-Doc MEAD-Doc+Cite KL-Divergence
3 ROUGE-1 0.163 0.167 0.301* 0.248 0.323
3 ROUGE-L 0.144 0.158 0.265 0.217 0.299
5 ROUGE-1 0.230 0.301 0.401 0.333 0.467
5 ROUGE-L 0.214 0.292 0.362 0.298 0.444
10 ROUGE-1 0.430 0.514 0.575 0.472 0.649
10 ROUGE-L 0.396 0.494 0.535 0.428 0.622
15 ROUGE-1 0.538 0.610 0.685 0.552 0.730
15 ROUGE-L 0.499 0.586 0.650 0.503 0.705
Table 1: Performance Comparison of Summarizers
of applying an existing summarization method to
generate an impact-based summary. Note that this
method may extract sentences in the citation con-
texts but not in the original paper.
5.2 Basic Results
We first show some basic results of impact sum-
marization in Table 1. They are generated us-
ing constant coefficient interpolation for the impact
language model (i.e., Equation 3) with ? = 0.8,
weighted maximum likelihood estimate for the ci-
tation context model (i.e., Equation 5) with ? = 3,
and ?s = 1, 000 for candidate sentence smoothing(Equation 1). These results are not necessarily opti-
mal as will be seen when we examine parameter and
method variations.
From Table 1, we see clearly that our method
consistently outperforms all the baselines. Among
the baselines, MEAD-Doc is consistently better than
both LEAD and MEAD-Doc+Cite. While MEAD-
Doc?s outperforming LEAD is not surprising, it is
a bit surprising that MEAD-Doc also outperforms
MEAD-Doc+Cite as the latter uses both the cita-
tion context and the original document. One possi-
ble explanation may be that MEAD is not designed
for impact summarization and it has been trapped
by the distracting content in the citation context 5.
Indeed, this can also explain why MEAD-Doc+Cite
tends to perform worse than LEAD by ROUGE-L
since if MEAD-Doc+Cite picks up sentences from
the citation context rather than the original papers,
it would not match as well with the gold standard
as LEAD which selects sentences from the origi-
5One anonymous reviewer suggested an interesting im-
provement to the MEAD-Doc+Cite baseline, in which we
would first extract sentences from the citation context and then
for each extracted sentence find a similar one in the original pa-
per. Unfortunately, we did not have time to test this approach
before the deadline for the camera-ready version of this paper.
nal papers. These results thus show that conven-
tional summarization techniques are inadequate for
impact summarization, and the proposed language
modeling methods are more effective for generating
impact-based summaries.
In Table 2, we show a sample impact-based sum-
mary and the corresponding MEAD-Doc regular
summary. We see that the regular summary tends
to have general sentences about the problem, back-
ground and techniques, not very informative in con-
veying specific contributions of the paper. None of
these sentences was selected by the human expert. In
contrast, the sentences in the impact summary cover
several details of the impact of the paper (i.e., spe-
cific smoothing methods especially Dirichlet prior,
sensitivity of performance to smoothing, and dual
role of smoothing), and sentences 4 and 6 are also
among the 8 sentences picked by the human expert.
Interestingly, neither sentence is in the abstract of
the original paper, suggesting a deviation of the ac-
tual impact of a paper and that perceived by the au-
thor(s).
5.3 Component analysis
We now turn to examine the effectiveness of each
component in the proposed methods and different
strategies for estimating ?I .
Effectiveness of interpolation: We hypothesized
that we need to use both the original document and
the citation context to estimate ?I . To test this hy-pothesis, we compare the results of using only d,
only the citation context, and interpolation of them
in Table 3. We show two different strategies of inter-
polation (i.e., constant coefficient with ? = 0.8 and
Dirichlet with ?c = 20, 000) as described in Sec-tion 4.
From Table 3, we see that both strategies of in-
terpolation indeed outperform using either the origi-
821
Impact-based summary:
1. Figure 5: Interpolation versus backoff for Jelinek-Mercer (top), Dirichlet smoothing (middle), and absolute discounting (bottom).
2. Second, one can de-couple the two different roles of smoothing by adopting a two stage smoothing strategy in which Dirichlet smoothing is
first applied to implement the estimation role and Jelinek-Mercer smoothing is then applied to implement the role of query modeling
3. We find that the backoff performance is more sensitive to the smoothing parameter than that of interpolation, especially in Jelinek-Mercer
and Dirichlet prior.
4. We then examined three popular interpolation-based smoothing methods (Jelinek-Mercer method, Dirichlet priors, and absolute discounting),
as well as their backoff versions, and evaluated them using several large and small TREC retrieval testing collections.
summary 5. By rewriting the query-likelihood retrieval model using a smoothed document language model, we derived a general retrieval
formula where the smoothing of the document language model can be interpreted in terms of several heuristics used intraditional models,
including TF-IDF weighting and document length normalization.
6. We find that the retrieval performance is generally sensitive to the smoothing parameters, suggesting that an understanding and appropriate
setting of smoothing parameters is very important in the language modeling approach.
Regular summary (generated using MEAD-Doc):
1. Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that
of language model estimation, which has been studied extensively in other application areas such as speech recognition.
2. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the
query according to the estimated language model.
3. On the one hand, theoretical studies of an underlying model have been developed; this direction is, for example, represented by the various
kinds of logic models and probabilistic models (e.g., [14, 3, 15, 22]).
4. After applying the Bayes? formula and dropping a document-independent constant (since we are only interested in ranking documents), we
have p(d|q) ? (q|d)p(d).
5. As discussed in [1], the righthand side of the above equation has an interesting interpretation, where, p(d) is our prior belief that d is relevant
to any query and p(q|d) is the query likelihood given the document, which captures how well the document ?fits? the particular query q.
6. The probability of an unseen word is typically taken as being proportional to the general frequency of the word, e.g., as computed using the
document collection.
Table 2: Impact-based summary vs. regular summary for the paper ?A study of smoothing methods for language
models applied to ad hoc information retrieval?.
nal document model (p(w|d)) or the citation context
model (p(w|C)) alone, which confirms that both the
original paper and the citation context are important
for estimating ?I . We also see that using the citationcontext alone is better than using the original paper
alone, which is expected. Between the two strate-
gies, Dirichlet dynamic coefficient is slightly better
than constant coefficient (CC), after optimizing the
interpolation parameter for both strategy.
Interpolation
Measure P (w|d) P (w|C) ConstCoef Dirichlet
ROUGE-1 0.529 0.635 0.643 0.647
ROUGE-L 0.501 0.607 0.619 0.623
Table 3: Effectiveness of interpolation
Citation authority and proximity: These heuris-
tics are very interesting to study as they are unique
to impact summarization and not well studied in the
existing summarization work.
pg(s) pr(s)=1/?k
pr(s) off ? = 2 ? = 3 ? = 4
Off 0.685 0.711 0.714 0.700
On 0.708 0.712 0.706 0.703
Table 4: Authority (pg(s)) and proximity (pr(s))
In Table 4, we show the ROUGE-L values for var-
ious combinations of these two heuristics (summary
length is 15). We turn off either pg(s) or pr(s) by
setting it to a constant; when both are turned off, we
have the unweighted MLE of p(w|C) (Equation 4).
Clearly, using weighted MLE with any of the two
heuristics is better than the unweighted MLE, indi-
cating that both heuristics are effective. However,
combining the two heuristics does not always im-
prove over using a single one. Since intuitively these
two heuristics are orthogonal, this may suggest that
our way of combining the two scores (i.e., taking a
product of them) may not be optimal; further study
is needed to better understand this. The ROUGE-1
results are similar.
Tuning of other parameters: There are three other
parameters which need to be tuned: (1) ?s for can-didate sentence smoothing (Equation 1); (2) ?c inDirichlet interpolation for impact model estimation
(Equation 2); and (3) ? in constant coefficient inter-
polation (Equation 3). We have examined the sen-
sitivity of performance to these parameters. In gen-
eral, for a wide range of values of these parameters,
the performance is relatively stable and near opti-
mal. Specifically, the performance is near optimal as
822
long as ?s and ?c are sufficiently large (?s ? 1000,
?c ? 20, 000), and the interpolation parameter ? isbetween 0.4 and 0.9.
6 Related Work
General text summarization, including single docu-
ment summarization (Luhn, 1958; Goldstein et al,
1999) and multi-document summarization (Kraaij et
al., 2001; Radev et al, 2003) has been well stud-
ied; our work is under the framework of extractive
summarization (Luhn, 1958; McKeown and Radev,
1995; Goldstein et al, 1999; Kraaij et al, 2001),
but our problem formulation differs from any exist-
ing formulation of the summarization problem. It
differs from regular single-document summarization
because we utilize extra information (i.e. citation
contexts) to summarize the impact of a paper. It also
differs from regular multi-document summarization
because the roles of original documents and cita-
tion contexts are not equivalent. Specifically, cita-
tion contexts serve as an indicator of the impact of
the paper, but the summary is generated by extract-
ing the sentences from the original paper.
Technical paper summarization has also been
studied (Paice, 1981; Paice and Jones, 1993; Sag-
gion and Lapalme, 2002; Teufel and Moens, 2002),
but the previous work did not explore citation con-
text to emphasize the impact of papers.
Citation context has been explored in several
studies (Nakov et al, 2004; Ritchie et al, 2006;
Schwartz et al, 2007; Siddharthan and Teufel,
2007). However, none of the previous studies has
used citation context in the same way as we did,
though the potential of directly using citation sen-
tences (called citances) to summarize a paper was
pointed out in (Nakov et al, 2004).
Recently, people have explored various types of
auxiliary knowledge such as hyperlinks (Delort et
al., 2003) and clickthrough data (Sun et al, 2005), to
summarize a webpage; such work is related to ours
as anchor text is similar to citation context, but it is
based on a standard formulation of multi-document
summarization and would contain only sentences
from anchor text.
Our work is also related to work on using lan-
guage models for retrieval (Ponte and Croft, 1998;
Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001)
and summarization (Kraaij et al, 2001). However,
we do not have an explicit query and constructing
the impact model is a novel exploration. We also
proposed new language models to capture the im-
pact.
7 Conclusions
We have defined and studied the novel problem of
summarizing the impact of a research paper. We cast
the problem as an impact sentence retrieval problem,
and proposed new language models to model the im-
pact of a paper based on both the original content
of the paper and its citation contexts in a literature
collection with consideration of citation autority and
proximity.
To evaluate impact summarization, we created a
test set based on ACM SIGIR papers. Experiment
results on this test set show that the proposed im-
pact summarization methods are effective and out-
perform several baselines that represent the existing
summarization methods.
An important future work is to construct larger
test sets (e.g., of biomedical literature) to facilitate
evaluation of impact summarization. Our formula-
tion of the impact summarization problem can be
further improved by going beyond sentence retrieval
and considering factors such as redundancy and co-
herency to better organize an impact summary. Fi-
nally, automatically generating impact-based sum-
maries can not only help users access and digest
influential research publications, but also facilitate
other literature mining tasks such as milestone min-
ing and research trend monitoring. It would be in-
teresting to explore all these applications.
Acknowledgments
We are grateful to the anonymous reviewers for their
constructive comments. This work is in part sup-
ported by a Yahoo! Graduate Fellowship and NSF
grants under award numbers 0713571, 0347933, and
0428472.
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
Proceedings of the Seventh International Conference
on World Wide Web, pages 107?117.
823
J.-Y. Delort, B. Bouchon-Meunier, and M. Rifqi. 2003.
Enhanced web document summarization using hyper-
links. In Proceedings of the Fourteenth ACM Confer-
ence on Hypertext and Hypermedia, pages 208?215.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998. Citeseer: an automatic citation indexing sys-
tem. In Proceedings of the Third ACM Conference on
Digital Libraries, pages 89?98.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of ACM SIGIR 99, pages 121?128.
Nancy R. Kaplan and Michael L. Nelson. 2000. Deter-
mining the publication impact of a digital library. J.
Am. Soc. Inf. Sci., 51(4):324?339.
W. Kraaij, M. Spitters, and M. van der Heijden. 2001.
Combining a mixture language model and naive bayes
for multi-document summarisation. In Proceedings of
the DUC2001 workshop.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of ACM
SIGIR 2001, pages 111?119.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 71?78.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Development,
2(2):159?165.
D. MacKay and L. Peto. 1995. A hierarchical Dirich-
let language model. Natural Language Engineering,
1(3):289?307.
Kathleen McKeown and Dragomir R. Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 74?82.
P. Nakov, A. Schwartz, and M. Hearst. 2004. Citances:
Citation sentences for semantic analysis of bioscience
text. In Proceedings of ACM SIGIR?04 Workshop on
Search and Discovery in Bioinformatics.
Chris D. Paice and Paul A. Jones. 1993. The identifi-
cation of important concepts in highly structured tech-
nical papers. In Proceedings of the 16th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 69?78.
C. D. Paice. 1981. The automatic generation of literature
abstracts: an approach based on the identification of
self-indicating phrases. In Proceedings of the 3rd An-
nual ACM Conference on Research and Development
in Information Retrieval, pages 172?191.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. Comput. Linguist., 28(4):399?408.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda Celebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization: the mead
project. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
375?382.
A. Ritchie, S. Teufel, and S. Robertson. 2006. Creating
a test collection for citation-based ir experiments. In
Proceedings of the HLT-NAACL 2006, pages 391?398.
S. Robertson and K. Sparck Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27:129?146.
Hpracop Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497?526.
A. S. Schwartz, A. Divoli, and M. A. Hearst. 2007. Mul-
tiple alignment of citation sentences with conditional
random fields and posterior decoding. In Proceedings
of the 2007 EMNLP-CoNLL, pages 847?857.
A. Siddharthan and S. Teufel. 2007. Whose idea was
this, and why does it matter? attributing scientific
work to citations. In Proceedings of NAACL/HLT-07,
pages 316?323.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In Proceedings
of the 28th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 194?201.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
ChengXiang Zhai and John Lafferty. 2001a. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management (CIKM 2001), pages 403?410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 334?342.
824
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 250?257,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Named Entity Transliteration Using Temporal and Phonetic
Correlation
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat and ChengXiang Zhai
University of Illinois at Urbana-Champaign
{syoon9,afister2,rws}@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate unsuper-
vised name transliteration using compara-
ble corpora, corpora where texts in the two
languages deal in some of the same top-
ics ? and therefore share references to
named entities ? but are not translations
of each other. We present two distinct
methods for transliteration, one approach
using an unsupervised phonetic translit-
eration method, and the other using the
temporal distribution of candidate pairs.
Each of these approaches works quite
well, but by combining the approaches
one can achieve even better results. We
believe that the novelty of our approach
lies in the phonetic-based scoring method,
which is based on a combination of care-
fully crafted phonetic features, and empiri-
cal results from the pronunciation errors of
second-language learners of English. Un-
like previous approaches to transliteration,
this method can in principle work with any
pair of languages in the absence of a train-
ing dictionary, provided one has an esti-
mate of the pronunciation of words in text.
1 Introduction
As a part of a on-going project on multilingual
named entity identification, we investigate unsu-
pervised methods for transliteration across lan-
guages that use different scripts. Starting from
paired comparable texts that are about the same
topic, but are not in general translations of each
other, we aim to find the transliteration correspon-
dences of the paired languages. For example, if
there were an English and Arabic newspaper on
the same day, each of the newspapers would likely
contain articles about the same important inter-
national events. From these comparable articles
across the two languages, the same named enti-
ties such as persons and locations would likely be
found. For at least some of the English named
entities, we would therefore expect to find Ara-
bic equivalents, many of which would in fact be
transliterations.
The characteristics of transliteration differ ac-
cording to the languages involved. In particular,
the exact transliteration of say, an English name
is highly dependent on the language since this will
be influenced by the difference in the phonological
systems of the language pairs. In order to show the
reliability of a multi-lingual transliteration model,
it should be tested with a variety of different lan-
guages. We have tested our transliteration meth-
ods with three unrelated target languages ? Ara-
bic, Chinese and Hindi, and a common source lan-
guage ? English. Transliteration from English to
Arabic and Chinese is complicated (Al-Onaizan
and Knight, 2002). For example, while Arabic or-
thography has a conventional way of writing long
vowels using selected consonant symbols ? ba-
sically <w>, <y> and <?>, in ordinary text
short vowels are rarely written. When transliter-
ating English names there is the option of repre-
senting the vowels as either short (i.e. unwrit-
ten) or long (i.e. written with one of the above
three mentioned consonant symbols). For exam-
ple London is transliterated as     lndn, with no
vowels; Washington often as  
	   wSnjTwn,
with <w> representing the final <o>. Transliter-
ations in Chinese are very different from the orig-
inal English pronunciation due to the limited syl-
lable structure and phoneme inventory of Chinese.
For example, Chinese does not allow consonant
clusters or coda consonants except [n, N], and this
results in deletion, substitution of consonants or
insertion of vowels. Thus while a syllable initial
/d/ may surface as in Baghdad  ba-ge-da,
note that the syllable final /d/ is not represented.
250
Hindi transliteration is not well-studied, but it is
in principle easier than Arabic and Chinese since
Hindi phonotactics is much more similar to that of
English.
2 Previous Work
Named entity transliteration is the problem of pro-
ducing, for a name in a source language, a set
of one or more transliteration candidates in a tar-
get language. Previous work ? e.g. (Knight and
Graehl, 1998; Meng et al, 2001; Al-Onaizan and
Knight, 2002; Gao et al, 2004) ? has mostly as-
sumed that one has a training lexicon of translit-
eration pairs, from which one can learn a model,
often a source-channel or MaxEnt-based model.
Comparable corpora have been studied exten-
sively in the literature ? e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et
al., 2000; Sadat et al, 2004), but transliteration
in the context of comparable corpora has not been
well addressed. The general idea of exploiting
time correlations to acquire word translations from
comparable corpora has been explored in several
previous studies ? e.g., (Fung, 1995; Rapp, 1995;
Tanaka and Iwasaki, 1996). Recently, a Pearson
correlation method was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005); this idea is similar to the method used in
(Kay and Roscheisen, 1993) for sentence align-
ment. In our work, we adopt the method proposed
in (Tao and Zhai, 2005) and apply it to the problem
of transliteration; note that (Tao and Zhai, 2005)
compares several different metrics for time corre-
lation, as we also note below ? and see (Sproat et
al., 2006).
3 Transliteration with Comparable
Corpora
We start from comparable corpora, consisting of
newspaper articles in English and the target lan-
guages for the same time period. In this paper, the
target languages are Arabic, Chinese and Hindi.
We then extract named-entities in the English text
using the named-entity recognizer described in (Li
et al, 2004), which is based on the SNoW machine
learning toolkit (Carlson et al, 1999). To perform
transliteration, we use the following general ap-
proach: 1 Extract named entities from the English
corpus for each day; 2 Extract candidates from the
same day?s newspapers in the target language; 3
For each English named entity, score and rank the
target-language candidates as potential transliter-
ations. We apply two unsupervised methods ?
time correlation and pronunciation-based methods
? independently, and in combination.
3.1 Candidate scoring based on
pronunciation
Our phonetic transliteration score uses a standard
string-alignment and alignment-scoring technique
based on (Kruskal, 1999) in that the distance is de-
termined by a combination of substitution, inser-
tion and deletion costs. These costs are computed
from a language-universal cost matrix based on
phonological features and the degree of phonetic
similarity. (Our technique is thus similar to other
work on phonetic similarity such as (Frisch, 1996)
though details differ.) We construct a single cost
matrix, and apply it to English and all target lan-
guages. This technique requires the knowledge of
the phonetics and the sound change patterns of the
language, but it does not require a transliteration-
pair training dictionary. In this paper we assume
the WorldBet transliteration system (Hieronymus,
1995), an ASCII-only version of the IPA.
The cost matrix is constructed in the following
way. All phonemes are decomposed into stan-
dard phonological features. However, phonolog-
ical features alone are not enough to model the
possible substution/insertion/deletion patterns of
languages. For example, /h/ is more frequently
deleted than other consonants, whereas no single
phonological feature allows us to distinguish /h/
from other consonants. Similarly, stop and frica-
tive consonants such as /p, t, k, b, d, g, s, z/ are
frequently deleted when they appear in the coda
position. This tendency is very salient when the
target languages do not allow coda consonants or
consonant clusters. So, Chinese only allows [n,
N] in coda position, and stop consonants in coda
position are frequently lost; Stanford is translit-
erated as sitanfu, with the final /d/ lost. Since
phonological features do not consider the posi-
tion in the syllable, this pattern cannot be cap-
tured by conventional phonological features alone.
To capture this, an additional feature ?deletion
of stop/fricative consonant in the coda position?
is added. We base these observations, and the
concomitant pseudofeatures on pronunciation er-
ror data of learners of English as a second lan-
guage, as reported in (Swan and Smith, 2002). Er-
251
rors in second language pronunciation are deter-
mined by the difference in the phonological sys-
tem of learner?s first and second language. The
same substitution/deletion/insertion patterns in the
second language learner?s errors appear also in
the transliteration of foreign names. For exam-
ple, if the learner?s first language does not have
a particular phoneme found in English, it is sub-
stituted by the most similar phoneme in their first
language. Since Chinese does not have /v/, it is
frequently substituted by /w/ or /f/. This sub-
stitution occurs frequently in the transliteration
of foreign names in Chinese. Swan & Smith?s
study covers 25 languages, and includes Asian
languages such as Thai, Korean, Chinese and
Japanese, European languages such as German,
Italian, French, and Polish and Middle Eastern
languages such as Arabic and Farsi. Frequent sub-
stitution/insertion/deletion patterns of phonemes
are collected from these data. Some examples are
presented in Table 1.
Twenty phonological features and 14 pseud-
ofeatures are used for the construction of the cost
matrix. All features are classified into 5 classes.
There are 4 classes of consonantal features ?
place, manner, laryngeality and major (conso-
nant, sonorant, syllabicity), and a separate class
of vocalic features. The purpose of these classes
is to define groups of features which share the
same substitution/insertion/deletion costs. For-
mally, given a class C, and a cost CC , for each
feature f ? C, CC defines the cost of substitut-
ing a different value for f than the one present in
the source phoneme. Among manner features, the
feature continuous is classified separately, since
the substitution between stop and fricative con-
sonants is very frequent; but between, say, nasals
and fricatives such substitution is much less com-
mon. The cost for frequent sound change pat-
terns should be low. Based on our intuitions, our
pseudofeatures are classified into one or another
of the above-mentioned five classes. The substitu-
tion/deletion/insertion cost for a pair of phonemes
is the sum of the individual costs of the features
which are different between the two phonemes.
For example, /n/ and /p/ are different in sonorant,
labial and coronal features. Therefore, the substi-
tution cost of /n/ for /p/ is the sum of the sonorant,
labial and coronal cost (20+10+10 = 40). Features
and associated costs are shown in Table 2. Sam-
ple substitution, insertion, and deletion costs for
/g/ are presented in Table 3.
The resulting cost matrix based on these prin-
ciples is then used to calculate the edit distance
between two phonetic strings. Pronunciations for
English words are obtained using the Festival text-
to-speech system (Taylor et al, 1998), and the tar-
get language words are automatically converted
into their phonemic level transcriptions by various
language-dependent means. In the case of Man-
darin Chinese this is based on the standard pinyin
transliteration system. For Arabic this is based
on the orthography, which works reasonably well
given that (apart from the fact that short vowels
are no represented) the script is fairly phonemic.
Similarly, the pronunciation of Hindi can be rea-
sonably well-approximated based on the standard
Devanagari orthographic representation. The edit
cost for the pair of strings is normalized by the
number of phonemes. The resulting score ranges
from zero upwards; the score is used to rank can-
didate transliterations, with the candidate having
the lowest cost being considered the most likely
transliteration. Some examples of English words
and the top three ranking candidates among all of
the potential target-language candidates are given
in Table 4.1 Starred entries are correct.
3.2 Candidate scoring based on time
correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event. For
example, the 2004 tsunami disaster was covered
in news articles in many different languages. We
would thus expect to see a peak of frequency of
names such as Sri Lanka, India, and Indonesia in
news articles published in multiple languages in
the same time period. In general, we may expect
topically related names in different languages to
tend to co-occur together over time. Thus if we
have comparable news articles over a sufficiently
long time period, it is possible to exploit such cor-
relations to learn the associations of names in dif-
ferent languages.
The idea of exploiting time correlation has been
well studied. We adopt the method proposed in
(Tao and Zhai, 2005) to represent the source name
and each name candidate with a frequency vector
and score each candidate by the similarity of the
1We describe candidate selection for each of the target
languages later.
252
Input Output Position
D D, d, z everywhere
T T, t, s everywhere
N N, n, g everywhere
p/t/k deletion coda
Table 1: Substitution/insertion/deletion patterns for phonemes based on English second-language
learner?s data reported in (Swan and Smith, 2002). Each row shows an input phoneme class, possi-
ble output phonemes (including null), and the positions where the substitution (or deletion) is likely to
occur.
Class Feature Cost
Major features and Consonant Del consonant 20
sonorant
consonant deletion
Place features and Vowel Del coronal 10
vowel del/ins
stop/fricative consonant del at coda position
h del/ins
Manner features nasal 5
dorsal feature for palatal consonants
Vowel features and Exceptions vowel height 3
vowel place
exceptional
Manner/ Laryngeal features continuous 1.5
voicing
Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional
denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between
these two sounds actually occur frequently in second-language error data.
two frequency vectors. This is very similar to the
case in information retrieval where a query and a
document are often represented by a term vector
and documents are ranked by the similarity be-
tween their vectors and the query vector (Salton
and McGill, 1983). But the vectors are very dif-
ferent and should be constructed in quite differ-
ent ways. Following (Tao and Zhai, 2005), we
also normalize the raw frequency vector so that
it becomes a frequency distribution over all the
time points. In order to compute the similarity be-
tween two distribution vectors ~x = (x1, ..., xT )
and ~y = (y1, ..., yT ), the Pearson correlation co-
efficient was used in (Tao and Zhai, 2005). We
also consider two other commonly used measures
? cosine (Salton and McGill, 1983), and Jensen-
Shannon divergence (Lin, 1991), though our re-
sults show that Pearson correlation coefficient per-
forms better than these two other methods. Since
the time correlation method and the phonetic cor-
respondence method exploit distinct resources, it
makes sense to combine them. We explore two ap-
proaches to combining these two methods, namely
score combination and rank combination. These
will be defined below in Section 4.2.
4 Experiments
We evaluate our algorithms on three compara-
ble corpora: English/Arabic, English/Chinese, and
English/Hindi. Data statistics are shown in Ta-
ble 5.
From each data set in Table 5, we picked out all
news articles from seven randomly selected days.
We identified about 6800 English names using the
entity recognizer from (Carlson et al, 1999), and
chose the most frequent 200 names as our English
named entity candidates. Note that we chose the
most frequent names because the reliability of the
statistical correlation depends on the size of sam-
ple data. When a name is rare in a collection,
253
Source Target Cost Target Cost
g g 0 r 40.5
kh 2.5 e 44.5
cCh 5.5 del 24
tsh 17.5 ins 20
N 26.5
Table 3: Substitution/deletion/insertion costs for /g/.
English Candidate
Script Worldbet
Philippines 1       
 
 f l b y n
*2     
	    
 
 f l b y n y t
3            f l b y n a
Megawati *1 
 
 
  m h a f th
2          m i j a w a t a
3        m a k w z a
English Candidate
Script Romanization Worldbet
Belgium *1 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 340?348,
Beijing, August 2010
Opinosis: A Graph-Based Approach to Abstractive Summarization of
Highly Redundant Opinions
Kavita Ganesan and ChengXiang Zhai and Jiawei Han
Department of Computer Science
University of Illinois at Urbana-Champaign
{kganes2,czhai,hanj}@cs.uiuc.edu
Abstract
We present a novel graph-based summa-
rization framework (Opinosis) that generates
concise abstractive summaries of highly re-
dundant opinions. Evaluation results on sum-
marizing user reviews show that Opinosis
summaries have better agreement with hu-
man summaries compared to the baseline ex-
tractive method. The summaries are readable,
reasonably well-formed and are informative
enough to convey the major opinions.
1 Introduction
Summarization is critically needed to help users
better digest the large amounts of opinions ex-
pressed on the web. Most existing work in Opin-
ion Summarization focus on predicting sentiment
orientation on an entity (Pang et al, 2002) (Pang
and Lee, 2004) or attempt to generate aspect-based
ratings for that entity (Snyder and Barzilay, 2007)
(Lu et al, 2009)(Lerman et al, 2009)(Titov and
Mcdonald, 2008). Such summaries are very infor-
mative, but it is still hard for a user to understand
why an aspect received a particular rating, forcing
a user to read many, often highly redundant sen-
tences about each aspect. To help users further di-
gest the opinions in each aspect, it is thus desirable
to generate a concise textual summary of such re-
dundant opinions.
Indeed, in many scenarios, we will face the
problem of summarizing a large number of highly
redundant opinions; other examples include sum-
marizing the ?tweets? on Twitter or comments
made about a blog or news article. Due to the sub-
tle variations of redundant opinions, typical extrac-
tive methods are often inadequate for summarizing
such opinions. Consider the following sentences:
1. The iPhone?s battery lasts long, only had to
charge it once every few days.
2. iPhone?s battery is bulky but it is cheap..
3. iPhone?s battery is bulky but it lasts long!
With extractive summarization, no matter which
single sentence of the three is chosen as a sum-
mary, the generated summary would be biased.
In such a case, an abstractive summary such as
?iPhone?s battery is cheap, lasts long but is bulky?
is a more complete summary, conveying all the
necessary information. Extractive methods also
tend to be verbose and this is especially problem-
atic when the summaries need to be viewed on
smaller screens like on a PDA. Thus, an informa-
tive and concise abstractive summary would be a
better solution.
Unfortunately, abstractive summarization is
known to be difficult. Existing work in abstractive
summarization has been quite limited and can be
categorized into two categories: (1) approaches us-
ing prior knowledge (Radev and McKeown, 1998)
(Finley and Harabagiu, 2002) (DeJong, 1982) and
(2) approaches using Natural Language Genera-
tion (NLG) systems (Saggion and Lapalme, 2002)
(Jing and McKeown, 2000). The first line of work
requires considerable amount of manual effort to
define schemas such as frames and templates that
can be filled with the use of information extraction
techniques. These systems were mainly used to
summarize news articles. The second category of
work uses deeper NLP analysis with special tech-
niques for text regeneration. Both approaches ei-
ther heavily rely on manual effort or are domain
dependent.
In this paper, we propose a novel flexible sum-
marization framework, Opinosis, that uses graphs
to produce abstractive summaries of highly redun-
dant opinions. In contrast with the previous work,
Opinosis assumes no domain knowledge and uses
shallow NLP, leveraging mostly the word order in
the existing text and its inherent redundancies to
generate informative abstractive summaries. The
key idea of Opinosis is to first construct a tex-
tual graph that represents the text to be summa-
rized. Then, three unique properties of this graph
are used to explore and score various subpaths
that help in generating candidate abstractive sum-
maries.
Evaluation results on a set of user reviews show
that Opinosis summaries have reasonable agree-
ment with human summaries. Also, the gener-
340
ated summaries are readable, concise and fairly
well-formed. Since Opinosis assumes no do-
main knowledge and is highly flexible, it can
be potentially used to summarize any highly re-
dundant content and could even be ported to
other languages. (All materials related to this
work including the dataset and demo software can
be found at http://timan.cs.uiuc.edu/
downloads.html.)
2 Opinosis-Graph
Our key idea is to use a graph data structure (called
Opinosis-Graph) to represent natural language text
and cast this abstractive summarization problem
as one of finding appropriate paths in the graph.
Graphs have been commonly used for extractive
summarization (e.g., LexRank (Erkan and Radev,
2004) and TextRank (Mihalcea and Tarau, 2004)),
but in these works the graph is often undirected
with sentences as nodes and similarity as edges.
Our graph data structure is different in that each
node represents a word unit with directed edges
representing the structure of sentences. Moreover,
we also attach positional information to nodes as
will be discussed later.
Algorithm 1 (A1): OpinosisGraph(Z)
1: Input: Topic related sentences to be summarized: Z = {zi}ni=12: Output: G = (V,E)
3: for i = 1 to n do4: w ? Tokenize(zi)5: sent size? SizeOf(w)
6: for j = 1 to sent size do
7: LABEL? wj8: PID ? j
9: SID ? i10: ifExistsNode(G,LABEL) then
11: vj ? GetExistingNode(G,LABEL)12: PRIvj ? PRIvj ? (SID, PID)13: else14: vj ? CreateNewNode(G,LABEL)15: PRIvj ? (SID, PID)16: end if
17: if notExistsEdge(vj?1 ? vj , G) then18: AddEdge(vj?1 ? vj , G)19: end if20: end for21: end for
Our graph representation is closer to that used by
Barzilay and Lee (Barzilay and Lee, 2003) for the
task of paraphrasing, wherein each node in the
graph represents a unique word. However, in their
work, such a graph is used to identify regions of
commonality and variability amongst similar sen-
tences. Thus, the positional information is not re-
quired nor is it maintained. In contrast, we main-
tain positional information at each node as this is
critical for the selection of candidate paths.
Algorithm A1 outlines the steps involved in
building an Opinosis-Graph. We start with a set
of sentences relevant to a specific topic, which can
be obtained in different ways depending on the ap-
plication. For example, they may be all sentences
related to the battery life of the iPod Nano. We de-
note these sentences as Z = {zi}ni=1 where each ziis a sentence containing part-of-speech (POS) an-
notations. (A1:4) Each zi ? Z is split into a set
of word units, where each unit, wj consists of a
word and its corresponding POS annotation (e.g.
?service:nn?, ?good:adj?). (A1:7-9) Each unique
wj will form a node, vj , in the Opinosis-Graph,
with wj being the label. Also, since we only have
one node per unique word unit, each node keeps
track of all sentences that it is a part of using a sen-
tence identifier (SID) along with its position of oc-
currence in that sentence (PID). (A1:10-16) Each
node will thus carry a Positional Reference Infor-
mation (PRI) which is a list of {SID:PID} pairs
representing the node?s membership in a sentence.
(A1:17-19) The original structure of a sentence is
recorded with the use of directed edges. Figure 1
shows a resulting Opinosis-Graph based on four
sentences.
The Opinosis-Graph has some unique proper-
ties that are crucial in generating abstractive sum-
maries. We highlight some of the core properties
by drawing examples from Figure 1:
Property 1. (Redundancy Capture). Highly re-
dundant discussions are naturally captured by sub-
graphs.
Figure 1 shows that although the phrase ?great de-
vice? was mentioned in different parts of sentences
(1) and (3), this phrase forms a relatively heavy
sub-path in the resulting graph. This is a good in-
dication of salience.
Property 2. (Gapped Subsequence Capture). Ex-
isting sentence structures introduce lexical links
that facilitate the discovery of new sentences or re-
inforce existing ones.
The main point conveyed by sentences (2) and (3)
in Figure 1 is that calls drop frequently. However,
this is expressed in slightly different ways and is
reflected in the resulting subgraph. Since sentence
(2) introduces a lexical link between ?drop? and
?frequently?, the word ?too? can be ignored for sen-
tence (3) as the same amount of information is re-
tained. This is analogous to capturing a repetitive
gapped subsequence where similar sequences with
minor variations are captured. With this, the sub-
graph calls drop frequently can be considered re-
dundant.
Property 3. (Collapsible Structures). Nodes that
resemble hubs are possibly collapsible.
In Figure 1 we see that the subgraph ?the iPhone
is?, is fairly heavy and the ?is? node acts like a
341
my
pho
ne
call
s
freq
uen
tly
too {3:8}
with
dro
p ipho
ne
is
a
my {2:1}
pho
ne
{2:2
}
call
s
{2:3
, 3:
6}
freq
uen
tly
{2:5
, 3:
9}
with {2:6
}
the
dro
p
{2:4
, 3:
7}
gre
at
{1:5
, 3:
1}
{1:2
, 2:
8, 4
:2}
{1:3
,4:3
}{1
:4}
.
{1:7
,2:
9,3
:10
}
{1:1
, 2:
7, 3
:5, 
4:1
,4:5
}
wor
th
pric
e
{4:6
}
{
,
}
, {3:3}
but {3:4}{1:7
,2:
9,3
:10
}
wor
th {4:4
} no
de 
labe
l
SID
:PID
 pa
irs
dev
ice
{1:6
, 3:
2}
Inp
ut:
SID
:1.
The
 iPh
one
is a
 gre
at d
evi
ce.
 
SID
:2.
 My
 ph
one
 ca
lls 
dro
p fr
equ
ent
ly w
ith 
the
 iPh
one
. 
SID
:3.
 Gr
eat
 de
vice
, bu
t th
e c
alls
 dro
p to
o fr
equ
ent
ly.
p ,
p
q
y
SID
:4.
 Th
e iP
hon
eis
 wo
rth
 the
 pri
ce.
Figure 1: Sample Opinosis-Graph. Thick edges
indicate salient paths.
?hub? where it connects to various other nodes.
Such a structure is naturally captured by the
Opinosis-Graph and is a good candidate for com-
pression to generate a summary such as ?The
iPhone is a great device and is worth the price?.
Also, certain word POS (e.g. linking verbs like
?is? and ?are?) often carry hub-like properties that
can be used in place of the outlink information.
3 Opinosis Summarization Framework
In this section, we describe a general framework
for generating abstractive summaries using the
Opinosis-Graph. We also describe our implemen-
tation of the components in this framework.
At a high level, we generate an abstractive sum-
mary by repeatedly searching the Opinosis graph
for appropriate subgraphs that both encode a valid
sentence (thus meaningful sentences) and have
high redundancy scores (thus representative of the
major opinions). The sentences encoded by these
subgraphs would then form an abstractive sum-
mary.
Going strictly by the definition of true abstrac-
tion (Radev et al, 2002), our problem formula-
tion is still more extractive than abstractive be-
cause the generated summary can only contain
words that occur in the text to be summarized;
our problem definition may be regarded as a word-
level (finer granularity) extractive summarization.
However, compared to the conventional sentence-
level extractive summarization, our formulation
has flavors of abstractive summarization wherein
we have elements of fusion (combining extracted
portions) and compression (squeezing out unim-
portant material from a sentence). Hence, the sen-
tences in the generated summary are generally not
the same as any original sentence. Such a ?shal-
low? abstractive summarization problem is more
tractable, enabling us to develop a general solution
to the problem. We now describe each component
in such a summarization framework.
3.1 Valid Path
A valid path intuitively refers to a path that corre-
sponds to a meaningful sentence.
Definition 1. (Valid Start Node - VSN). A node vq
is a valid start node if it is a natural starting point
of a sentence.
We use the positional information of a node to de-
termine if it is a VSN. Specifically, we check if
Average(PIDvq) ? ?vsn, where ?vsn is a pa-rameter to be empirically set. With this, we only
qualify nodes that tend to occur early on in a sen-
tence.
Definition 2. (Valid End Node - VEN). A node vs
is a valid end point if it completes a sentence.
We use the natural ending points in the text to be
summarized as hints to which node may be a valid
end point of a path (i.e., a sentence). Specifically,
a node is a valid end node if (1) the node is a
punctuation such as period and comma or (2) the
node is any coordinating conjunction (e.g., ?but?
and ?yet?).
Definition 3. (Valid Path). A path W = {vq...vs}
is valid if it is connected by a set of directed edges
such that (1) vq is a VSN, (2) vs is a VEN, and
(3) W satisfies a set of well-formedness POS con-
straints.
Since not every path starting with a VSN and end-
ing at a VEN encodes a meaningful sentence, we
further require a valid path to satisfy the following
POS constraints (expressed in regular-expression)
to ensure that a valid path encodes a well-formed
sentence:
1. . ? (/nn) + . ? (/vb) + . ? (/jj) + .?
2. . ? (/jj) + . ? (/to) + . ? (/vb).?
3. . ? (/rb) ? . ? (/jj) + . ? (/nn) + .?
4. . ? (/rb) + . ? (/in) + . ? (/nn) + .?
This also provides a way (if needed) for the appli-
cation to generate only specific type of sentences
like comparative sentences or strictly opinionated
sentences. These rules are thus application spe-
cific.
3.2 Path Scoring
Intuitively, to generate an abstractive summary, we
should select a valid path that can represent most of
the redundant opinions well. We would thus favor
a valid path with a high redundancy score.
Definition 4. (Path Redundancy). Let W =
{vq...vs} be a path from an Opinosis-Graph. The
path redundancy of W , r(q, s), is the number of
overlapping sentences covered by this path, i.e.,
342
r(q, s) = nq??nq+1...??ns,
where ni = PRIvi and ?? is the intersection be-tween two sets of SIDs such that the difference be-
tween the corresponding PIDs is no greater than
?gap, and ?gap > 0 is a parameter.
Path redundancies provide good indication of how
many sentences discuss something similar at each
point in the path. The ?gap parameter controls the
maximum allowed gaps in discovering these re-
dundancies. Thus, a common sentence X between
nodes vq and vr, will be considered a valid inter-
sect if (PIDvrx ? PIDvqx) ? ?gap.Based on path redundancy, we propose several
ways to score a path for the purpose of selecting a
good path to include in the summary:
1. Sbasic(W ) = 1|W |
?s
k=i+1,i r(i, k)
2. Swt len(W ) = 1|W |
?s
k=i+1,i |vi, vk| ? r(i, k)
3. Swt loglen(W ) = 1|W |(r(i, i+ 1) +?s
k=i+2,i+1 log2|vi, vk| ? r(i, k))
vi is the first node in the path being scored and vs
is the last node. |vi, vk| is the length from node vi
to vk. |W | is the length of the entire path being
scored. The Sbasic scoring function scores a path
purely based on the level of redundancy. One could
also argue that high redundancy on a longer path is
intuitively more valuable than high redundancy on
a shorter path as the former would provide better
coverage than the latter. This intuition is factored
in by the Swt len and Swt loglen scoring functions
where the level of redundancy is weighted by the
path length. Swt loglen is similar to Swt len only
that it scales down the path length so that it does
not entirely dominate.
3.3 Collapsed paths
In some cases, paths in the Opinosis-Graph may be
collapsible (as explained in Section 2). In such a
case, the collapse operation is performed and then
the path scores are computed. We will now ex-
plain a few concepts related to collapsible struc-
tures. Let W? = {vi...vk} be a path from the
Opinosis-Graph.
Definition 5. (Collapsible Node). Node vk is a
candidate for collapse if its POS is a verb.
We only attempt to collapse nodes that are verbs
due to the heavy usage of verbs in opinion text and
the ease with which the structures can be combined
to form a new sentence. However, as mentioned
earlier other properties like the outlink information
can be used to determine if a node is collapsible.
Definition 6. (Collapsed Candidates, Anchor).
Let vk be a collapsible node. The collapsed can-
didates of vk (denoted by CC = {cci}mi=1) are the
Canchor CC Connectora. the sound quality is cc1 : really good and
cc2 : clearb. the iphone is cc1 : great but
cc2 : expensive
Table 1: Example of anchors, collapsed candidates
and suitable connectors
remaining paths after vk in all the valid paths go-
ing through vi...vk. The prefix vi...vk is called the
anchor, denoted as Canchor = {vi...vk}. Each
path {vi...vn}, where vn is the last node in each
cci ? CC, is an individually valid path.
Table 1 shows a simplistic example of anchors and
corresponding collapsed candidates. Once the an-
chor and collapsed candidates have been identified,
the task is then to combine all of these to form a
new sentence.
Definition 7. (Stitched Sentence) A stitched sen-
tence is one that combines Canchor and CC to
form a combined, logical sentence.
We will now describe the stitching procedure that
we use, by drawing examples from Table 1. Since
we are dealing with verbs, Canchor can be com-
bined with the corresponding CC with commas
to separate each cci ? CC with one exception -
the correct sentence connector has to be used for
the last cci. For Canchora , the phrases really goodand clear can be connected by ?and? due to the
same sentiment orientation. For Canchorb , the col-lapsed candidate phrases are well connected by the
word ?but?. We use the existing Opinosis-Graph
to determine the most appropriate connector. We
do this by looking at all coordinating conjunction
(e.g. ?but?, ?yet?) nodes (vcconj) that are connected
to the first node of the last collapsed candidate,
ccm. This would be the node labeled ?clear? for
Canchora and ?expensive? for Canchorb . We denotethese nodes as v0,ccm . The vcconj , with the high-est path redundancy with v0,ccm , will be selectedas the connector.
Definition 8. (Collapsed Path Score) The final
path score after the entire collapse operation is the
average across path scores computed from vi to the
last node in each cci ? CC.
The collapsed path score essentially involves com-
puting the path scores of the individual sentences
assuming that they are not collapsed and then av-
eraging them.
3.4 Generation of summary
Once we can score all the valid paths as well as all
the collapsed paths, the generation of an abstrac-
tive summary can be done in two steps: First, we
rank all the paths (including the collapsed paths)
in descending order of their scores. Second, we
343
eliminate duplicated (or extremely similar) paths
by using a similarity measure (in our experiments,
we used Jaccard). We then take the top few re-
maining paths as the generated summary, with the
number of paths to be chosen controlled by a pa-
rameter ?ss, which represents summary size.
Although conceptually we enumerate all the
valid paths, in reality we can use a redundancy
score threshold, ?r to prune many non-promising
paths. This is reasonable because we are only in-
terested in paths with high redundancy scores.
4 Summarization Algorithm
Algorithms A2 and A3 describe the steps involved
in Opinosis Summarization. A2 is the starting
point of the Opinosis Summarization and A3 is a
subroutine where path finding takes place, invoked
from within A2.
Algorithm 2 (A2): OpinosisSummarization(Z)
1: Input: Topic related sentences to be summarized: Z = {zi}ni=12: Output: O ={Opinosis Summaries}
3: g ? OpinosisGraph(Z)
4: node size? SizeOf(g)
5: for j = 1 to node size do
6: if V SN(vj) then7: pathLen? 1
8: score? 09: cList? CreateNewList()
10: Traverse(cList, vj , score, PRIvj , labelvj , pathLen)11: candidates? {candidates ? cList}
12: end if13: end for14: C ? EliminateDuplicates(candidates)
15: C ? SortByPathScore(C)
16: for i = 1 to ?ss do17: O = {O ? PickNextBestCandidate(C)}
18: end for
(A2:3) Opinosis Summarization starts with the
construction of the Opinosis-Graph, described in
detail in Section 2. This is followed by the depth
first traversal of this graph to locate valid paths
that become candidate summaries. (A2:6-12) To
achieve this, each node vj in the Opinosis-Graph
is examined to determine if it is a VSN and, if it
is, path finding will start from this node by invok-
ing subroutine A3. A3 takes the following as in-
put: list - a list to hold candidate summaries; vi
- the node to continue traversal from; score - the
accumulated path score; PRIoverlap - the intersect
between PRIs of all nodes visited so far (see Defi-
nition 4); sentence - the summary sentence formed
so far; len - the current path length. (A2:7-10) Be-
fore invoking A3 from A2, the path length is set to
?1?, path score is set to ?0? and a new list is cre-
ated to store candidate summaries generated from
node vj . (A2:11) All candidate summaries gener-
ated from vj will be stored in a common pool of
candidate summaries.
Algorithm 3 (A3): Traverse(...)
1: Input: list, vk ? V , score, PRIoverlap, sentence, len2: Output: A set of candidate summaries
3: redundancy ? SizeOf(PRIoverlap)4: if redundancy ? ?r then5: if V EN(vk) then6: if V alidSentence(sentence) then
7: finalScore? scorelen8: AddCandidate(list, sentence, finalScore)
9: end if10: end if
11: for vn ? Neighborsvk do12: PRInew ? PRIoverlap ?? PRIvn13: redundancy ? SizeOf(PRInew)14: newSent? Concat(sentence, labelvn )15: L? len+ 116: newScore? score+ PathScore(redundancy, L)
17: if Collapsible(vn) then18: Canchor ? newSent19: tmp? CreateNewList()
20: for vx ? Neighborsvn do21: Traverse(tmp, vx, 0, PRInew, labelvx , L)22: CC ? EliminateDuplicates(tmp)
23: CCPathScore? AveragePathScore(CC)
24: finalScore? newScore+ CCPathScore
25: stitchedSent? Stitch(Canchor, CC)26: AddCandidate(list, stitchedSent, finalScore)
27: end for28: else29: Traverse(list, vn, newScore, PRInew, newSent, L)30: end if31: end for32: end if
(A3:3-4) Algorithm A3 starts with a check to
ensure that the minimum path redundancy require-
ment is satisfied (see definition 4). For the very
first node sent from A2, the path redundancy is the
size of the raw PRI . (A3:5-10) If the redundancy
requirement is satisfied, a few checks are done to
determine if a valid path has been found. If it has,
then the resulting sentence and its final score are
added to the list of candidate summaries.
(A3:11-31) Traversal proceeds recursively
through the exploration of all neighboring nodes
of the current node, vk. (A3:12-16) For every
neighboring node, vn the PRI overlap information,
path length, summary sentence and path score
are updated before the next recursion. (A3:29)
If a vn is not collapsible, then a regular traver-
sal takes place. (A3:17-27) However, if vn is
collapsible, the updated sentence in A3:14, will
now serve as an anchor in A3:18. (A3:21) A3
will then attempt to start a recursive traversal
from all neighboring nodes of vn in order to find
corresponding collapsed candidates. (A3:22-26)
After this, duplicates are eliminated from the
collapsed candidates and the collapsed path score
is computed. The resulting stitched sentence and
its final score are then added to the original list of
candidate summaries.
(A2:14-18) Once all paths have been explored
344
for candidate generation, duplicate candidates are
removed and the remaining are sorted in descend-
ing order of their path scores. The best ?ss candi-
dates are ?picked? as final Opinosis summaries.
5 Experimental Setup
We evaluate this abstractive summarization task
using reviews of hotels, cars and various prod-
ucts1. Based on these reviews, 2 humans were
asked to construct ?opinion seeking? queries which
would consist of an entity name and a topic of in-
terest. Example of such queries are: Amazon Kin-
dle:buttons, Holiday Inn, Chicago: staff, and so
on. We compiled a set of 51 such queries. We cre-
ate one review document per query by collecting
all review sentences that contain the query words
for the given entity. Each review document thus
consists of a set of unordered, redundant review
sentences related to the query. There are approxi-
mately 100 sentences per review document.
We use ROUGE (Lin, 2004b) to quantitatively
assess the agreement of Opinosis summaries with
human composed summaries. ROUGE is based on
an n-gram co-occurrence between machine sum-
maries and human summaries and is a widely ac-
cepted standard for evaluation of summarization
tasks. In our experiments, we use ROUGE-1,
ROUGE-2 and ROUGE-SU4 measures. ROUGE-
1 and ROUGE-2 have been shown to have most
correlation with human summaries (Lin and Hovy,
2003) and higher order ROUGE-N scores (N > 1)
estimate the fluency of summaries.
We use multiple reference (human) summaries
in our evaluation since it can achieve better cor-
relation with human judgment (LIN, 2004a). We
leverage Amazon?s Online Workforce2 to get 5 dif-
ferent human workers to summarize each review
document. The workers were asked to be concise
and were asked to summarize the major opinions in
the review document presented to them. We manu-
ally reviewed each set of reference summaries and
dropped summaries that had little or no correlation
with the majority. This left us with around 4 refer-
ence summaries for each review document.
To allow performance comparison between hu-
mans, Opinosis and the baseline method, we im-
plemented a Jackknifing procedure where, given K
references, the ROUGE score is computed over K
sets of K-1 references. With this, average human
performance is computed by treating each refer-
ence summary as a ?system? summary, computing
ROUGE scores over the remaining K-1 reference
1Reviews collected from Tripadvisor, Amazon, Edmunds
2https://www.mturk.com
summaries.
Due to the limited work in abstractive sum-
marization, no natural baseline could be used for
comparison. The existing work in this area is
mostly domain dependent and requires too much
manual effort (explained in Section 1). The next
best baseline is to use a state of the art extractive
method. Thus, we use MEAD (Radev et al, 2000)
as our baseline. MEAD is an extractive summa-
rizer based on cluster centroids. It uses a collection
of the most important words from the whole clus-
ter to select the best sentences for summarization.
By default, the scoring of sentences in MEAD is
based on 3 parameters - minimum sentence length,
centroid, and position in text. MEAD was ideal
for our task because a good summary in our case
would be one that could capture the most essential
information. This is exactly what centroid-based
summarization aims to achieve. Also, since the po-
sition in text parameter is irrelevant in our case, we
could easily turn this off with MEAD.
We introduce a readability test to understand if
Opinosis summaries are in fact readable. Suppose
we have N sentences from a system-generated
summary and M sentences from corresponding
human summaries. We mix all these sentences
and then ask a human assessor to pick at most N
sentences that are least readable as the prediction
of system summary.
readability(O) = 1? #CorrectPickN
If the human assessor often picks out system gen-
erated summaries as being least readable, then the
readability of system summaries is poor. If not,
then the system generated summaries are no dif-
ferent from human summaries.
6 Results
The baseline method (MEAD) selects 2 most rep-
resentative sentences as summaries. To give a fair
comparison, we fix the Opinosis summary size,
?ss = 2. We also fix ?vsn = 15. The best Opinosis
configuration with ?ss = 2 and ?vsn = 15 is
called Opinosisbest (?gap = 4, ?r = 2, Swt loglen).
ROUGE scores reported are with the use of stem-
ming and stopword removal.
Performance comparison between humans,
Opinosis and baseline. Table 2 shows the perfor-
mance comparison between humans, Opinosisbest
and the baseline method. First, we see that the
baseline method has very high recall scores com-
pared to Opinosis. This is because extractive meth-
ods that just ?select? sentences tend to be much
longer resulting in higher recall. However, these
summaries tend to carry information that may not
be significant and is clearly reflected by the poor
345
Recall
ROUGE-1 ROUGE-2 ROUGE-SU4 Avg # Words
Human 0.3184 0.1106 0.1293 17
Opinosis 0.2831 0.0853 0.0851 15
Baseline 0.4932 0.1058 0.2316 75
Precision
ROUGE-1 ROUGE-2 ROUGE-SU4 Avg # Words
Human 0.3434 0.1210 0.1596 17
Opinosis 0.4482 0.1416 0.2261 15
Baseline 0.0916 0.0184 0.0102 75
F-score
ROUGE-1 ROUGE-2 ROUGE-SU4 Avg # Words
Human 0.3088 0.1069 0.1142 17
Opinosis 0.3271 0.0998 0.1027 15
Baseline 0.1515 0.0308 0.0189 75
Table 2: Performance comparison between Hu-
mans, Opinosisbest and Baseline.
0.110
0330
0.1000.110
E-SU4
0.3100.330
GE-1
0.0900.1000.110
ROUG
basi
c
0.2900.3100.330
ROU
basi
c
00700.0800.0900.1000.110
basi
c
wt_l
ogle
n
wt_l
en
0.2700.2900.3100.330
basi
c
wt_l
ogle
n
wtl
en
0.0700.0800.0900.1000.110
1
2
3
4
5basi
c
wt_l
ogle
n
wt_l
en
0.2500.2700.2900.3100.330
1
2
3
4
5bas
ic wt_l
ogle
n
wt_l
en
?gap
?gap
0.0700.0800.0900.1000.110
1
2
3
4
5basi
c
wt_l
ogle
n
wt_l
en
0.2500.2700.2900.3100.330
1
2
3
4
5bas
ic wt_l
ogle
n
wt_l
en
?gap
?gap
Figure 2: ROUGE scores (f-measure) at different
levels of ?gap, ?r = 2.
precision scores.
Next, we see that humans have reasonable
agreement amongst themselves given that these are
independently composed summaries. This agree-
ment is especially clear with the ROUGE-2 re-
call score where the recall is better than Opinosis
but comparable to the baseline even though the
summaries are much shorter. It is also clear that
Opinosis is closer in performance to humans than
to the baseline method. The recall scores of
Opinosis summaries are slightly lower than that
achieved by humans, while the precision scores are
higher (Wilcoxon test shows that the increase in
precision is statistically more significant than the
decrease in recall). In terms of f-scores, Opinosis
has the best ROUGE-1 score and its ROUGE-2 and
ROUGE-SU4 scores are comparable with human
performance. The baseline method has the low-
est f-scores. The difference between the f-scores
of Opinosis and that of humans is statistically in-
significant.
Comparison of scoring functions. Next, we look
into the performance of the three scoring func-
tions, Sbasic, Swt len and Swt loglen described in
Section 3. Figure 2 shows ROUGE scores of these
scoring methods at varying levels of ?gap. First,
0.30
ROU
GE-
1
0.090.10
ROU
GE-
SU4
0.200.250.30
ROU
GE-
1
0.050.060.070.080.090.10
R2
R3
R4
ROU
GE-
SU4
0.200.250.30
R2
R3
R4
ROU
GE-
1
bas
ic
wt_
logl
en
wt_
bas
ic
0.050.060.070.080.090.10
R2
R3
R4
ROU
GE-
SU4
bas
ic
wt_
logl
en
wt_
bas
ic
Figure 3: ROUGE scores (f-measure) at different
levels of ?r averaged across ?gap ? [1, 5]
0.230.28
PRECISION
precis
ion-re
call cu
rve
ROUG
E-SU4
0.400.450.50
PRECISION
precis
ion-re
call cu
rve
ROUG
E-1
x coll
apse
x dup
elim x coll
apse
+dup
elim
Opino
sis
(base
line)
+ll
x coll
apse
x dup
elim x 
collap
se
+dup
elim
Opino
sis
(base
line)
0.180.230.28 0
.06
0.07
0.08
0.09
0.1
RECA
LL
precis
ion-re
call cu
rve
ROUG
E-SU4
0.350.400.450.50 0
.22
0.24
0.26
0.28
0.3
RECA
LL
precis
ion-re
call cu
rve
ROUG
E-1
x coll
apse
x dup
elim x coll
apse
+dup
elim
Opino
sis
(base
line)
+coll
apse
x dup
elim
x coll
apse
x dup
elim x 
collap
se
+dup
elim
Opino
sis
(base
line) +col
lapse x dup
elim
Figure 4: Precision-Recall comparison with differ-
ent Opinosis features turned off.
it can be observed that Swt basic which does not
use path length information, performs the worst.
This is due to the effect of heavily favoring re-
dundant paths over longer but reasonably redun-
dant ones that can provide more coverage. We also
see that Swt len and Swt loglen are similar in per-
formance with Swt loglen marginally outperform-
ing Swt len when ?gap > 2. Since Swt len uses
the raw path length in its scoring function, it may
be inflating the path scores of long but insignifi-
cant paths. Swt loglen scales down the path length,
thus providing a reasonable tradeoff between re-
dundancy and the length of the selected path. The
three scoring functions are not influenced by dif-
ferent levels of ?r as shown in Figure 3.
Effect of gap setting (?gap). Now, we will ex-
amine the effect of ?gap on the generated sum-
maries. Based on Figure 2, we see that setting
?gap=1 yields in relatively low performance. This
is because ?gap=1 implies immediate adjacency
between the PIDs of two nodes and such strict ad-
jacency enforcements prevent redundancies from
being discovered. When ?gap is increased to 2,
there is a big jump in performance, after which
improvements are observed in smaller amounts. A
very large gap setting could increase the possibility
of generating ill-formed sentences, thus we recom-
mend that ?gap is set between 2-5.
Effect of redundancy requirement (?r) . Fig-
ure 3 shows the ROUGE scores at different levels
of ?r. It is clear that when ?r > 2, the quality of
summaries is negatively impacted. Since we only
have about 100 sentences per review document,
?r > 2 severely restricts the number of paths that
can be explored, yielding in lower ROUGE scores.
Since the scoring function can account for the level
of redundancy, ?r should be set according to the
size of the input data. For our dataset, ?r = 2 was
ideal.
346
?A
bo
ut 
foo
da
t H
oli
da
y I
nn
, L
on
do
n?
Human
summaries:
[1]
Foo
d w
as
 exc
ell
en
t w
ith
 a 
wid
e r
an
ge
 of
 ch
oic
es
 an
d g
oo
d s
erv
ice
s.
[2]
The
 fo
od
 is
 go
od
, th
e s
erv
ice
 gr
ea
t. Ve
ry 
go
od
 se
lec
tio
n o
f fo
od
 fo
r b
rea
kfas
t 
bu
ffe
t.
?W
ha
t is
 fr
ee
at 
Be
stw
es
ter
nI
nn
, S
an
 Fr
an
cis
co
?
Human
summaries:
[1]
The
re 
is 
fre
e Wi
Fiin
ter
ne
t a
cc
es
s a
va
ila
ble
 in
 al
l th
e r
oo
ms
.. Fro
m 
5-6
 p.m
. th
ere
 is
 fre
e 
win
e t
as
tin
g a
nd
 ap
pe
tize
rs 
av
ail
ab
le 
to 
all
 th
e g
ue
sts
.
[2]
Eve
nin
g w
ine
 re
ce
pti
on
 an
d f
ree
 co
ffe
e i
n t
he
 m
orn
ing
. Fre
e i
nte
rne
t, f
ree
 pa
rkin
g a
nd
 
fre
em
as
sa
ge
Opinosis
abstractive summary:
The
 fo
od
 w
as
  e
xce
lle
nt,
  g
oo
d a
nd
  d
eli
cio
us
. Ver
y g
oo
d s
ele
cti
on
 of
 fo
od
.
Baseline
extractive summary:
With
in 
20
0 y
ard
s o
f le
av
ing
 th
e h
ote
l a
nd
 he
ad
ing
 to
 th
e Tu
be
 St
ati
on
 yo
u h
av
e a
 
nu
mb
er 
of 
fas
t fo
od
 ou
tle
ts,
 hi
gh
str
ee
tRe
sta
uta
nts
, P
as
try
 sh
op
s a
nd
 
su
pe
rm
arke
ts
so
ify
ou
did
wis
ht
ol
ive
in
yo
ur
ho
tel
roo
m
for
the
du
rat
ion
of
yo
ur
fre
e m
as
sa
ge
.
Opinosis
abstractive summary:
Free
 w
ine
 re
ce
pti
on
 in
 ev
en
ing
. Fre
e c
off
ee
 an
d b
isc
ott
i a
nd
 w
ine
.
Baseline
extractive summary:
The
 fre
e w
ine
 an
d n
ibb
les
 se
rve
d b
etw
ee
n 5
pm
 an
d 6
pm
 w
ere
 a 
lov
ely
 to
uc
h. Th
ere
's f
ree
 
co
ffe
e
tea
sa
tb
rea
kfas
tti
me
wit
hl
ittl
eb
isc
ott
ia
nd
be
st
of
all
fro
m
5t
ill6
pm
yo
ug
et
af
ree
su
pe
rm
arke
ts
, s
o i
f y
ou
 di
d w
ish
 to
 liv
e i
n y
ou
r h
ote
l ro
om
 fo
r th
e d
ura
tio
n o
f y
ou
r 
sta
y, 
yo
u c
ou
ld 
do
.......
co
ffe
e, 
tea
s a
t b
rea
kfas
t 
tim
e w
ith
 lit
tle
 bi
sc
ott
i a
nd
, b
es
t o
f a
ll, 
fro
m 
5 t
ill 6
pm
 yo
u g
et 
a f
ree
 
win
e 't
as
tin
g' r
ec
ep
tio
n w
hic
h, 
as
 lo
ng
 as
 yo
u d
on
't ta
ke??
Figure 5: Sample results comparing Opinosis summaries with human and baseline summaries.
Effect of collapsed structures and duplicate
elimination. So far, it has been assumed that all
features used in Opinosis are required to gener-
ate reasonable summaries. To test this hypothesis,
we use Opinosisbest as a baseline and then we turn
off different features of Opinosis. We turn off the
duplicate elimination feature, then the collapsi-
ble structure feature, and finally both. Figure 4
shows the resulting precision-recall curve. From
this graph, we see that without duplicate elimina-
tion and when collapsing is turned off, the preci-
sion is highest but recall is lowest. No collaps-
ing implies shorter sentences and thus lower recall,
which is clearly reflected in Figure 4. On top of
this, if duplicates are allowed, the overall informa-
tion coverage is low, further affecting the recall.
Notice that the presence of duplicates with the col-
lapse feature turned on results in very high recall
(even higher than the baseline). This is caused by
the presence of similar phrases that were not elim-
inated from the collapsed candidates, resulting in
long sentences that artificially boost recall. The
Opinosis baseline which uses duplicate elimina-
tion and the collapsible structure feature, offers a
reasonable tradeoff between precision and recall.
Readability of Summaries. To test the readability
of Opinosis summaries, we conducted a readabil-
ity test (described in Section 5) using summaries
generated from Opinosisbest. A human assessor
picked the 2 least readable sentences from each of
the 51 test sets (based on 51 summaries). Collec-
tively, there were 565 sentences out of which 102
were Opinosis generated. Out of these, the hu-
man assessor picked only 34 of the sentences as
being least readable, resulting in an average read-
ability score of 0.67. This shows that more than
60% of the generated sentences are indistinguish-
able from human composed sentences. Of the 34
sentences with problems, 11 contained no informa-
tion or were incomprehensible, 12 were incomplete
possibly due to false positives when the sentence
validity check was done, and 8 had conflicting in-
formation such as ?the hotel room is clean and
dirty?. This happens due to mixed feelings about
the same topic and can be resolved using sentiment
analysis. The remaining 3 sentences were found
to contain poor grammar, possibly caused by the
gaps allowed in finding redundant paths.
Sample Summaries. Finally, in Figure 5 we show
two sample summaries on two different topics.
Notice that the Opinosis summaries are concise,
fairly well-formed and have closer resemblance to
human summaries than to the baseline summaries.
7 Conclusion
In this paper, we described a novel summarization
framework (Opinosis) that uses textual graphs to
generate abstractive summaries of highly redun-
dant opinions. Evaluation results on a set of review
documents show that Opinosis summaries have
better agreement with human summaries com-
pared to the baseline extractive method. The
Opinosis summaries are concise, reasonably well-
formed and communicate essential information.
Our readability test shows that more than 60% of
the generated sentences are no different from hu-
man composed sentences.
Opinosis is a flexible framework in that many
of its modules can be easily improved or replaced
with other suitable implementation. Also, since
Opinosis is domain independent and relies on min-
imal external resources, it can be used with any
corpus containing high amounts of redundancies.
Our graph representation naturally ensures the
coherence of a summary, but such a graph empha-
sizes too much on the surface order of words. As a
result, it cannot group sentences at a deep seman-
tic level. To address this limitation, we can use a
similar idea to overlay parse trees and this would
be a very interesting future research.
8 Acknowledgments
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National Sci-
ence Foundation under grants IIS-0347933, IIS-
0713581, IIS-0713571, and CNS-0834709.
347
References
[Barzilay and Lee2003] Barzilay, Regina and Lillian
Lee. 2003. Learning to paraphrase: an unsuper-
vised approach using multiple-sequence alignment.
In NAACL ?03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 16?23, Morristown, NJ,
USA. Association for Computational Linguistics.
[DeJong1982] DeJong, Gerald F. 1982. An overview of
the FRUMP system. In Lehnert, Wendy G. and Mar-
tin H. Ringle, editors, Strategies for Natural Lan-
guage Processing, pages 149?176. Lawrence Erl-
baum, Hillsdale, NJ.
[Erkan and Radev2004] Erkan, Gu?nes and Dragomir R.
Radev. 2004. Lexrank: graph-based lexical central-
ity as salience in text summarization. J. Artif. Int.
Res., 22(1):457?479.
[Finley and Harabagiu2002] Finley, Sanda Harabagiu
and Sanda M. Harabagiu. 2002. Generating sin-
gle and multi-document summaries with gistexter. In
Proceedings of the workshop on automatic summa-
rization, pages 30?38.
[Jing and McKeown2000] Jing, Hongyan and Kath-
leen R. McKeown. 2000. Cut and paste based
text summarization. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 178?185, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
[Lerman et al2009] Lerman, Kevin, Sasha Blair-
Goldensohn, and Ryan Mcdonald. 2009. Sentiment
summarization: Evaluating and learning user prefer-
ences. In 12th Conference of the European Chapter
of the Association for Computational Linguistics
(EACL-09).
[Lin and Hovy2003] Lin, Chin-Yew and Eduard Hovy.
2003. Automatic evaluation of summaries using n-
gram co-occurrence statistics. In Proc. HLT-NAACL,
page 8 pages.
[LIN2004a] LIN, Chin-Yew. 2004a. Looking for a few
good metrics : Rouge and its evaluation. proc. of the
4th NTCIR Workshops, 2004.
[Lin2004b] Lin, Chin-Yew. 2004b. Rouge: a pack-
age for automatic evaluation of summaries. In Pro-
ceedings of the Workshop on Text Summarization
Branches Out (WAS 2004), Barcelona, Spain.
[Lu et al2009] Lu, Yue, ChengXiang Zhai, and Neel
Sundaresan. 2009. Rated aspect summarization of
short comments. In 18th International World Wide
Web Conference (WWW2009), April.
[Mihalcea and Tarau2004] Mihalcea, R. and P. Tarau.
2004. TextRank: Bringing order into texts. In Pro-
ceedings of EMNLP-04and the 2004 Conference on
Empirical Methods in Natural Language Processing,
July.
[Pang and Lee2004] Pang, Bo and Lillian Lee. 2004.
A sentimental education: Sentiment analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of the ACL, pages 271?278.
[Pang et al2002] Pang, Bo, Lillian Lee, and Shivaku-
mar Vaithyanathan. 2002. Thumbs up? Sentiment
classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 79?86.
[Radev and McKeown1998] Radev, DR and K. McKe-
own. 1998. Generating natural language summaries
from multiple on-line sources. Computational Lin-
guistics, 24(3):469?500.
[Radev et al2000] Radev, Dragomir, Hongyan Jing, and
Malgorzata Budzikowska. 2000. Centroid-based
summarization of multiple documents: Sentence ex-
traction, utility-based evaluation, and user studies.
In In ANLP/NAACL Workshop on Summarization,
pages 21?29.
[Radev et al2002] Radev, Dragomir R., Eduard Hovy,
and Kathleen McKeown. 2002. Introduction to the
special issue on summarization.
[Saggion and Lapalme2002] Saggion, Horacio and Guy
Lapalme. 2002. Generating indicative-informative
summaries with sumum. Computational Linguistics,
28(4):497?526.
[Snyder and Barzilay2007] Snyder, Benjamin and
Regina Barzilay. 2007. Multiple aspect ranking
using the good grief algorithm. In In Proceedings
of the Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL, pages
300?307.
[Titov and Mcdonald2008] Titov, Ivan and Ryan Mc-
donald. 2008. A joint model of text and aspect rat-
ings for sentiment summarization. In Proceedings
of ACL-08: HLT, pages 308?316, Columbus, Ohio,
June. Association for Computational Linguistics.
348
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 734?742,
Beijing, August 2010
Exploiting Structured Ontology to Organize Scattered Online Opinions
Yue Lu, Huizhong Duan, Hongning Wang, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
{yuelu2,duan9,wang296,czhai}@illinois.edu
Abstract
We study the problem of integrating scat-
tered online opinions. For this purpose,
we propose to exploit structured ontology
to obtain well-formed relevant aspects to
a topic and use them to organize scattered
opinions to generate a structured sum-
mary. Particularly, we focus on two main
challenges in implementing this idea, (1)
how to select the most useful aspects from
a large number of aspects in the ontology
and (2) how to order the selected aspects
to optimize the readability of the struc-
tured summary. We propose and explore
several methods for solving these chal-
lenges. Experimental results on two dif-
ferent data sets (US Presidents and Digital
Cameras) show that the proposed methods
are effective for selecting aspects that can
represent the major opinions and for gen-
erating coherent ordering of aspects.
1 Introduction
The explosive growth of online opinions raises in-
teresting challenges for opinion integration and
summarization. It is especially interesting to in-
tegrate and summarize scattered opinions in blog
articles and forums as they tend to represent the
general opinions of a large number of people and
get refreshed quickly as people dynamically gen-
erate new content, making them valuable for un-
derstanding the current views of a topic.
However, opinions in blogs and forums are
usually fragmental, scattered around, and buried
among other off-topic content, so it is quite chal-
lenging to organize them in a meaningful way.
Traditional text summarization techniques gener-
ate an unstructured list of sentences as a sum-
mary, which cannot reveal representative opinions
on different aspects of a topic or effectively facil-
itate navigation into the huge opinion space. To
address this limitation, recent work has shown the
usefulness of generating a structured summary of
opinions, in which related opinions are grouped
into topical aspects with explicit labeling of all the
aspects. A major challenge in producing such a
structured summary is how to generate these as-
pects for an arbitrary topic (e.g., products, politi-
cal figures, policies, etc.). Intuitively, the aspects
should be concise phrases that can both be easily
interpreted in the context of the topic under con-
sideration and capture the major opinions. How-
ever, where can we find such phrases and which
phrases should we select as aspects? Furthermore,
once we selected aspects, how should we order
them to improve the readability of a structured
summary? One way to generate aspects is to clus-
ter all the opinion sentences and then identify rep-
resentative phrases in each cluster. Although as-
pects selected in this way can effectively capture
the major opinions, a major limitation is that it is
generally hard to ensure that the selected phrases
are well connected with the given topic (Chen and
Dumais, 2000).
In this paper, we propose a novel approach
to generating aspects by leveraging the ontolo-
gies with structured information that are available
online, such as open domain knowledge base in
Freebase1. Such kind of ontology data is not in
small scale by any measure. For example, Free-
base alone contains more than 10 million topics,
3000 types, and 30,000 properties; moreover, it is
constantly growing as people collaboratively con-
tribute. Freebase provides different properties for
different types of topics such as personal infor-
mation for a ?US President? and product features
for a ?Digital Camera?. Since this kind of re-
sources can provide related entities/relations for a
1http://www.freebase.com
734
wide range of topics , our general idea is to lever-
age them as guidance for more informed organi-
zation of scattered online opinions, and in partic-
ular, to select the most important properties of a
topic from such structured ontology as aspects to
generate a structured opinion summary. A signif-
icant advantage of this approach to aspect genera-
tion is that the selected aspects are guaranteed to
be very well connected with the topic, but it also
raises an additional challenge in selecting the as-
pects to best capture the major opinions from a
large number of aspects provided for each topic in
the ontology. Different from some existing work
on exploiting ontologies, e.g., (Sauper and Barzi-
lay, 2009), which relies on training data, we focus
on exploring unsupervised approaches, which can
be applied to a larger scope of topics.
Specifically, given a topic with entries in an on-
tology and a collection of scattered online opin-
ions about the topic, our goal is to generate a
structured summary where representative major
opinions are organized with well aligned aspects
and in an order easy for human to follow. We
propose the following general approach: First, re-
trieval techniques are employed to align opinions
to relevant aspects. Second, a subset of most inter-
esting aspects are selected. Third, we will further
order the selected aspects to present them in a rea-
sonable order. Finally, for the opinions uncovered
by the selected aspects from the ontology, we use
a phrase ranking method to suggest new aspects to
add to the ontology for increasing its coverage.
Implementing the second and third steps in-
volves special challenges. In particular, without
any training data, it is unclear how we should
show the most interesting aspects in ontology with
major opinions aligned and which presentation
order of aspects is natural and intuitive for hu-
man. Solving these two challenges is the main
focus of this paper. We propose three meth-
ods for aspect selection, i.e., size-based, opinion
coverage-based, and conditional entropy-based
methods, and two methods for aspect ordering,
i.e., ontology-ordering and coherence ordering.
We evaluate our methods on two different types of
topics: US Presidents and Digital Cameras. Qual-
itative results demonstrate the utility of integrating
opinions based on structured ontology as well as
the generalizability of proposed methods. Quan-
titative evaluation is also conducted to show the
effectiveness of our methods.
Note that we use the term ?opinion? to broadly
refer to any discussion in opinionated sources
such as blogs and reviews. This allows us to for-
mulate and solve the problem in a general way.
Indeed, the main goal of our work is to extract
and organize the major opinions about a topic that
are buried in many scattered opinionated sources
rather than perform deeper understanding of opin-
ions (e.g., distinguishing positive from negative
opinions), which can be done by using any exist-
ing sentiment analysis technique as an orthogonal
post-processing step after applying our method.
2 Related Work
Aspect summarization, i.e., structured opinion
summarization over topical aspects, has attracted
much attention recently. Existing work iden-
tifies aspects using frequent-pattern/association-
rule mining, e.g. (Liu et al, 2005; Popescu and
Etzioni, 2005), sentence clustering, e.g. (Ga-
mon et al, 2005; Leouski and Croft, 1996), or
topic modeling, e.g. (Mei et al, 2006; Titov and
McDonald, 2008). After that, meaningful and
prominent phrases need to be selected to repre-
sent the aspects, e.g. (Zhao and He, 2006; Mei
et al, 2007). However, these methods suffer from
the problem of producing trivial aspects. Conse-
quently, some of the aspects generated are very
difficult to interpret (Chen and Dumais, 2000). In
this paper, we propose a different kind of approach
that is to use aspects provided by ontology which
are known to be relevant and easy to interpret.
Ontology is used in (Carenini et al, 2005) but
only for mapping product features. The closest
work to ours are (Lu and Zhai, 2008; Sauper and
Barzilay, 2009); both try to use well-written arti-
cles for summarization. However, (Lu and Zhai,
2008) assumes the well-written article is struc-
tured with explicit or implicit aspect information,
which does not always hold in practice, while
(Sauper and Barzilay, 2009) needs a relatively
large amount of training data in the given domain.
In comparison, our work only needs the ontology
information for the given topic which is much eas-
ier to obtain from resources such as Freebase.
735
3 Methods
Given (1) an input topic T , (2) a large number of
aspects/properties A = {A1, ..., Am} from an on-
tology that are related to T , and (3) a huge col-
lection of scattered opinion sentences about the
topic DT = {s1, . . . , sn}, our goal is to gener-
ate a structured organization of opinions that are
both aligned well with the interesting aspects and
representative of major opinions about the topic.
The envisioned structured organization consists
of a sequence of selected aspects from ontol-
ogy ordered to optimize readability and a set of
sentences matching each selected aspect. Once
we obtain a set of sentences in each aspect, we
can easily apply a standard text summarization
method to further summarize these sentences, thus
the unique challenges related to our main idea of
exploiting ontology are the following, which are
also the main focus of our study:
Aspect Selection: How can we select a subset of
aspects A? ? A to capture the major opinions in
our opinion set DT ?
Aspect Ordering: How can we order a subset of
selected aspects A? so as to present them in an or-
der pi(A?) that is most natural with respect to hu-
man perception?
New Aspects Suggestion: Can we exploit the
opinions in DT to suggest new aspects to be added
to the ontology?
3.1 Aspect Selection
In order to align the scattered opinions to the
most relevant aspects, we first use each aspect la-
bel Ai ? A as a query to retrieve a set of rel-
evant opinions in the collection Si ? DT with
a standard language modeling approach, i.e., the
KL-divergence retrieval model (Zhai and Lafferty,
2001). Up to 1000 opinion sentences are retrieved
for each aspect; each opinion sentence can be po-
tentially aligned to several aspects. In this way,
scattered online discussion are linked to the most
relevant aspects in the ontology, which enables a
user to use aspects as ?semantic bridges? to navi-
gate into the opinion space..
However, there are usually a lot of candidate
aspects in an ontology, and only some are heav-
ily commented in online discussions, so showing
all the aspects is not only unnecessary, but also
overwhelming for users. To solve this problem,
we propose to utilize the aligned opinions to fur-
ther select a subset of the most interesting aspects
A? ? A with size k. Several approaches are pos-
sible for this subset selection problem.
Size-based: Intuitively, the selected subset A?
should reflect the major opinions. So a straightfor-
ward method is to order the aspects Ai by the size
of the aligned opinion sentences Si, i.e., the num-
ber of relevant opinion sentences, and then select
the top k ones.
Opinion Coverage-based: The previous method
does not consider possible redundancy among the
aspects. A better approach is to select the subset
that covers as many distinct opinion sentences as
possible. This can be formulated as a maximum
coverage problem, for which a greedy algorithm
is known to be a good approximation: we select
one aspect at a time that is aligned with the largest
number of uncovered sentences.
Conditional Entropy-based: Aspects from a struc-
tured ontology are generally quite meaningful, but
they are not designed specifically for organizing
the opinions in our data set. Thus, they do not
necessarily correspond well to the natural clus-
ters in scattered opinions. To obtain aspects that
are aligned well with the natural clusters in scat-
tered opinions, we can first cluster DT into l
clusters C = {C1, . . . , Cl} using K-means with
TF ? IDF as features, and then choose the sub-
set of aspects that minimize Conditional Entropy
of the cluster label given the aspect:
A? = argminH(C|A?) = argmin?
??
?
Ai?A?,Ci?C
p(Ai, Ci) log
p(Ai, Ci)
p(Ai)
?
?
This Conditional Entropy measures the uncer-
tainty about the cluster label of a sentence given
the knowledge of its aspect. Intuitively, if the as-
pects are aligned well with the clusters, we would
be able to predict well the cluster label of a sen-
tence if we know its aspect, thus there would be
less uncertainty about the cluster label. In the
extreme case when the cluster label can be com-
pletely determined by the aspect, the conditional
entropy would reach its minimum (i.e., 0). Intu-
itively, the conditional entropy-based method es-
sentially selects the most appropriate aspects from
736
Algorithm 1 Greedy Algorithm for
Conditional Entropy Based Aspect Selection
Input: A = {A1, ..., Am}
Output: k-sized A? ? A
1: A? = {?mi=1Ai}2: for j=1 to k do
3: bestH = ?; bestA = A0
4: for each Ai in A do
5: tempA? = {Ai, A? \Ai}
6: if H(C|tempA?) < bestH then
7: bestH = H(C|tempA?)
8: bestA = Ai
9: A? = {bestA,A? \ bestA}
10: output A?
the ontology to label clusters of opinions.
The exact solution of this combinatorial optimiza-
tion problem is NP-complete, so we employ a
polynomial time greedy algorithm to approximate
it: in the i-th iteration, we select the aspect that
can minimize the conditional entropy given the
previous i ? 1 selected aspects. Pseudo code is
given in Algorithm 1.
3.2 Aspect Ordering
In order to present the selected aspects to users
in a most natural way, it is important to obtain a
coherent order of them, i.e., generating an order
consistent with human perception. To achieve this
goal, our idea is to use human written articles on
the topic to learn how to organize the aspects au-
tomatically. Specifically, we would order aspects
so that the relative order of the sentences in all the
aspects would be as consistent with their order in
the original online discussions as possible.
Formally, the input is a subset of selected as-
pects A?; each Ai ? A? is aligned with a set of
relevant opinion sentences Si = {Si,1, Si,2, ...}.
We define a coherence measurement function over
sentence pairs Co(Si,k, Sj,l), which is set to 1 iff
Si,k appears before Sj,l in the same article. Other-
wise, it is set to 0. Then a coherence measurement
function over an aspect pair can be calculated as
Co(Ai, Aj) =
?
Si,k?Si,Sj,l?Sj Co(Si,k, Sj,l)
|Si||Sj |
As an output, we would like to find a permutation
p?i(A?) that maximizes the coherence of all pair-
wise aspects, i.e.,
p?i(A?) = arg max
pi(A?)
?
Ai,Aj?A?,Ai?Aj
Co(Ai, Aj)
Algorithm 2 Greedy Algorithm for
Coherence Based Aspect Ordering
Input: A
Output: pi(A)
1: for each Ai, Aj in A do
2: calculate Co(Ai, Aj)
3: for p = 1 to len = A.size() do
4: Max = A[1]
5: for each aspect Ai in A do
6: Ai.coherence = 0
7: for each aspect Aj in pi(A) do
8: Ai.coherence+ = Co(Aj , Ai)
9: for each aspect Aj in A, j 6= i do
10: Ai.coherence+ = Co(Ai, Aj)
11: if Ai.coherence > Max.coherence then
12: Max = Ai
13: remove Max from A; add Max to pi(A)
14: output pi(A)
where Ai ? Aj means that Ai is before Aj . It
is easy to prove that the problem is NP-complete.
Therefore, we resort to greedy algorithms to find
approximations of the solution. Particularly we
view the problem as a ranking problem. The al-
gorithm proceeds by finding at each ranking po-
sition an aspect that can maximize the coherence
measurement, starting from the top of the rank list.
The detailed algorithm is given in Algorithm 2.
3.3 New Aspects Suggestion
Finally, if the opinions cover more aspects than in
the ontology, we also want to identify informative
phrases to label such extra aspects; such phrases
can also be used to further augment the ontology
with new aspects.
This problem is similar to existing work on gen-
erating labels for clusters (Zeng et al, 2004) or
topic models (Mei et al, 2007). Here we employ
a simple but representative technique to demon-
strate the feasibility of discovering interesting new
aspects for augmenting the ontology. We first ex-
tract named entities from scattered opinions DT
using Stanford Named Entity Recognizer (Finkel
et al, 2005). After that, we rank the phrases by
pointwise Mutual Information (MI):
MI(T, ph) = log P (T, ph)P (T )P (ph)
where T is the given topic and ph refers to a candi-
date entity phrase. P (T, ph) is proportional to the
number of opinion sentences they co-occur; P (T )
or P (ph) are proportional to the number of times
T or ph appears. A higher MI value indicates a
737
Statistics Category 1 Category 2
US president Digital Camera
Number of Topics 36 110
Number of Aspects 65?26 32?4
Number of Opinions 1001?1542 170?249
Table 1: Statistics of Data Sets
stronger association. We can then suggest the top
ranked entity phrases that are not in the selected
aspects as new aspects.
4 Experiments
4.1 Data Sets
To examine the generalizability of our methods,
we test on two very different categories of top-
ics: US Presidents and Digital Cameras.2 For the
ontology, we leverage Freebase, downloading the
structured ontology for each topic. For the opin-
ion corpus, we use blog data for US Presidents and
customer reviews for Digital Cameras. The blog
entries for US Presidents were collected by using
Google Blog Search3 with the name of a president
as the query. Customer reviews for Digital Cam-
eras were crawled from CNET4. The basic statis-
tics of our data sets is shown in Table 1. For all the
data collections, Porter stemmer (Porter, 1997) is
applied and stop words are removed.
4.2 Sample Results
We first show sample results of automatic orga-
nization of online opinions. We use the opin-
ion coverage-based algorithm to select 10 aspects
(10-20 aspects were found to be optimal in (Ka?ki,
2005)) and then apply the coherence-based aspect
ordering method. The number of clusters is set so
that there are on average 15 opinions per cluster.
Opinion Organization: Table 2 and Table 3
present sample results for President Ronald Rea-
gan and Sony Cybershot DSC-W200 camera re-
spectively5. We can see that (1) although Freebase
aspects provide objective and accurate informa-
tion about the given topics, extracted opinion sen-
tences offer additional subjective information; (2)
aligning scattered opinion sentences to most rel-
evant aspects in the ontology helps digestion and
2We have made our data sets available at http://
timan.cs.uiuc.edu/downloads.html .
3http://blogsearch.google.com
4http://www.cnet.com
5Due to space limit, we only show the first few aspects as
output by our methods.
navigation; and (3) the support number, which is
the number of opinion sentences aligned to an as-
pect, can show the popularity of the aspect in the
online discussions.
Adaptability of Aspect Selection: Being un-
supervised is a significant advantage of our meth-
ods over most existing work. It provides flexibil-
ity of applying the methods in different domains
without the requirement of training data, benefit-
ing from both the ontology based template guid-
ance as well as data-driven approaches. As a re-
sult, we can generate different results for differ-
ent topics even in the same domain. In Table 4,
we show the top three selected and ordered as-
pects for Abraham Lincoln and Richard Nixon.
Although they belong to the same category, differ-
ent aspects are picked up due to the differences in
online opinions. People talk a lot about Lincoln?s
role in American Civil War and his famous quo-
tation, but when talking about Nixon, people fo-
cus on ending the Vietnam war and the Watergate
scandal. ?Date of birth? and ?Government posi-
tion? are ranked first because people tend to start
talking from these aspects, which is more natural
than starting from aspects like ?Place of death?.
Baseline Comparison: We also show below the
aspects for Lincoln generated by a representative
approach using clustering method (e.g. (Gamon et
al., 2005)). i.e., we label the largest clusters by se-
lecting phrases with top mutual information. We
can see that although some phrases make sense,
not all are well connected with the given topic;
using aspects in ontology circumvents this prob-
lem. This example confirms the finding in pre-
vious work that the popular existing clustering-
based approach to aspects generation cannot gen-
erate meaningful labels (Chen and Dumais, 2000).
Vincent
New Salem State Historic Site
USS Abraham Lincoln
Martin Luther King Jr
Gettysburg
John F.
New Aspect Discovery: Finally, in Table 5 we
show some phrases ranked among top 10 using
the method described in Section 3.3. They reveal
additional aspects covered in online discussions
and serve as candidate new aspects to be added to
Freebase. Interestingly, John Wilkes Booth, who
assassinated President Lincoln, is not explicitly
738
FreeBase Aspects Supt Representative Opinion Sentences
Appointees: 897 Martin Feldstein, whose criticism of Reagan era deficits has not been forgotten.
- Martin Feldstein Reagan?s first National Security advisor was quoted as declaring...
- Chief Economic Advisor
Government Positions Held: 967 1981 Jan 20, Ronald Reagan was sworn in as president as 52 American hostages
- President of the United States boarded a plane in Tehran and headed toward freedom.
- Jan 20, 1981 to Jan 20, 1989 40th president of the US Ronald Reagan broke the so called ?20 year curse?...
Vice president: 847 8 years, 1981-1988 George H. W. Bush as vice president under Ronald Reagan...
- George H. W. Bush ...exception to the rule was in 1976, when George H W Bush beat Ronald.
Table 2: Opinion Organization Result for President Ronald Reagan
FreeBase Aspects Supt Representative Opinion Sentences
Format: 13 Quality pictures in a compact package.
- Compact ... amazing is that this is such a small and compact unit but packs so much power.
Supported Storage Types: 11 This camera can use Memory Stick Pro Duo up to 8 GB
- Memory Stick Duo Using a universal storage card and cable (c?mon Sony)
Sensor type: 10 I think the larger ccd makes a difference.
- CCD but remember this is a small CCD in a compact point-and-shoot.
Digital zoom: 47 once the digital :smart? zoom kicks in you get another 3x of zoom
-2? I would like a higher optical zoom, the W200 does a great digital zoom translation...
Table 3: Opinion Organization Result for Sony Cybershot DSC-W200 Camera
listed in Freebase, but we can find it in people?s
online discussion using mutual information.
4.3 Evaluation of Aspect Selection
Measures: Aspect selection is a new challenge,
so there is no standard way to evaluate it. It is also
very hard for human to read all of the aspects and
opinions and then select a gold standard subset.
Therefore, we opt to use indirect measures captur-
ing different characteristics of the aspect selection
problem (1) Aspect Coverage (AC): we first as-
sign each aspect Ai to the cluster Cj that has the
most overlapping sentences with Ai, approximat-
ing the cluster that would come into mind when
a reader sees Ai. Then AC is defined as the per-
centage of the clusters covered by at least one as-
pect. (2) Aspect Precision (AP ): for each cov-
ered cluster Ci, AP measures the Jaccard similar-
ity between Ci as a set of opinions and the union
of all aspects assigned to Ci. (3) Average Aspect
Precision (AAP ): defines averaged AP for all
clusters where an uncovered Ci has a zero AP ;
it essentially combines AC and AP . We also re-
port Sentence Coverage (SC), i.e., how many dis-
tinct opinion sentences can be covered by the se-
lected aspects and Conditional Entropy (H), i.e.,
how well the selected aspects align with the nat-
ural clusters in the opinions; a smaller H value
indicates a better alignment.
Results: We summarize the evaluation results in
Measures SC H AC AP AAP
PRESIDENTS
Random 503 1.9069 0.5140 0.0933 0.1223
Size-based 500 1.9656 0.3108 0.1508 0.0949
Opin Cover 746 1.8852 0.5463 0.0913 0.1316
Cond Ent. 479 1.7687 0.5770 0.0856 0.1552
CAMERAS
Random 55 1.6389 0.6554 0.0871 0.1271
Size-based 70 1.6463 0.6071 0.1077 0.1340
Opin Cover 82 1.5866 0.6998 0.0914 0.1564
Cond Ent. 70 1.5598 0.7497 0.0789 0.1574
Table 6: Evaluation Results for Aspect Selection
Table 6. In addition to the three methods de-
scribed in Section 3.1, we also include one base-
line of averaging 10 runs of random selection. The
best performance by each measure on each data
set is highlighted in bold font. Not surprisingly,
opinion coverage-based approach has the best
sentence coverage (SC) performance and condi-
tional entropy-based greedy algorithm achieves
the lowest H . Size-based approach is best in as-
pect precision but at the cost of lowest aspect cov-
erage. The trade-off between AP and AC is com-
parable to that between precision and recall as
in information retrieval while AAP summarizes
the combination of these two. The greedy algo-
rithm based on conditional entropy outperforms
all other approaches in AC and also in AAP , sug-
gesting that it can provide a good balance between
AP and AC.
739
Supt Richard-Nixon Supt Abraham-Lincoln
50 Date of birth: 419 Government Positions Held:
- Jan 9, 1913 - United States Representative Mar 4,1847-Mar 3,1849
108 Tracks Recorded: 558 Military Commands:
- 23-73 Broadcast: End of the Vietnam War - American Civil War - United States of America
120 Works Written About This Topic: 810 Quotations: - Nearly all men can stand adversity, but if
- Watergate you want to test a man?s character, give him power.
Table 4: Comparison of Aspect Selection for Two Presidents (aligned opinions are omitted here)
Suggested Phrases Supporting Opinion Sentences
Abraham Lincoln Presidential Library CDB projects include the Abraham Lincoln Presidential Library and Museum
Abraham Lincoln Memorial ..., eventually arriving at Abraham Lincoln Memorial.
John Wilkes Booth John Wilkes Booth shoots President Abraham Lincoln at Ford?s Theatre ...
Table 5: New Phrases for Abraham Lincoln
4.4 Evaluation of Aspect Ordering
Human Annotation: In order to quantitatively
evaluate the effectiveness of aspect ordering, we
conduct user studies to establish gold standard or-
dering. Three users were each given k selected as-
pects and asked to perform two tasks for each US
President: (1) identify clusters of aspects that are
more natural to be presented together (cluster con-
straints) and (2) identify aspect pairs where one
aspect is preferred to appear before the other from
the viewpoint of readability. (order constraints).
We did not ask them to provide a full order of
the k aspects, because we suspect that there are
usually more than one ?perfect? order. Instead,
identifying partial orders or constraints is easier
for human to perform, thus provides more robust
gold standard.
Human Agreement: After obtaining the human
annotation results, we first study human consen-
sus on the ordering task. For both types of human
identified constraints, we convert them into pair-
wise relations of aspects, e.g., ?Ai and Aj should
be presented together? or ?Ai should be displayed
before Aj?. Then we calculate the agreement per-
centage among the three users. In Table 7, we can
see that only a very small percentage of pair-wise
partial orders (15.92% of the cluster constraints
and none of the order constraints) are agreed by
all the three users, though the agreement of clus-
tering is much higher than that of ordering. This
indicates that ordering the aspects is a subjective
and difficult task.
Measures: Given the human generated gold stan-
dard of partial constraints, we use the follow-
ing measures to evaluate the automatically gen-
AgreedBy Cluster Constraint Order Constraint
1 37.14% 89.22%
2 46.95% 10.78%
3 15.92% 0.00%
Table 7: Human Agreement on Ordering
erated full ordering of aspects: (1) Cluster Pre-
cision (prc): for all the aspect pairs placed in
the same cluster by human, we calculate the per-
centage of them that are also placed together in
the system output. (2) Cluster Penalty (pc): for
each aspect pair placed in the same cluster by hu-
man, we give a linear penalty proportional to the
number of aspects in between the pair that the
system places; pc can be interpreted as the aver-
age number of aspects between aspect pairs that
should be presented together in the case of mis-
ordering. Smaller penalty corresponds to better
ordering performance. (3) Order Precision (pro):
the percentage of correctly predicted aspect pairs
compared with human specified order.
Results: In Table 8, we report the ordering
performance based on two selection algorithms:
opinion coverage-based and conditional entropy-
based. Different selection algorithms provide dif-
ferent subsets of aspects for the ordering algo-
rithms to operate on. For comparison with our
coherence-based ordering algorithm, we include a
random baseline and Freebase ontology ordering.
Note that Freebase order is a very strong baseline
because it is edited by human even though the pur-
pose was not for organizing opinions. To take into
account the variation of human annotation, we use
four versions of gold standard: three are from the
individual annotators and one from the union of
their annotation. We did not include the gold stan-
740
Selection Gold Cluster Precision (prc) Cluster Penalty (pc) Order Precision (pro)
Algo STD Random Freebase Coherence Random Freebase Coherence Random Freebase Coherence
Opin Cover 1 0.3290 0.9547 0.9505 1.8798 0.1547 0.1068 0.4804 0.7059 0.4510
Opin Cover 2 0.3266 0.9293 0.8838 1.7944 0.3283 0.1818 0.4600 0.4000 0.4000
Opin Cover 3 0.2038 0.4550 0.4417 2.5208 1.3628 1.7994 0.5202 0.4561 0.5263
Opin Cover union 0.3234 0.7859 0.7237 1.8378 0.6346 0.4609 0.4678 0.4635 0.4526
Cond Entropy 1 0.2540 0.9355 0.8978 2.0656 0.2957 0.2016 0.5106 0.7111 0.5444
Cond Entropy 2 0.2535 0.7758 0.8323 2.1790 0.7530 0.5222 0.4759 0.6759 0.5093
Cond Entropy 3 0.2523 0.4030 0.5545 2.3079 2.1328 1.1611 0.5294 0.7143 0.8175
Cond Entropy union 0.3067 0.7268 0.7488 1.9735 1.0720 0.7196 0.5006 0.6500 0.6833
Table 8: Evaluation Results on Aspect Ordering
dard that is the intersection of three annotators be-
cause that would leave us with too little overlap.
We have several observations: (1) In general, re-
sults show large variations when using different
versions of gold standard, indicating the subjec-
tive nature of the ordering task. (2) Coherence-
based ordering shows similar performance to
Freebase order-based in cluster precision (prc),
but when we take into consideration the distance-
based penalty (pc) of separating aspects pairs in
the same cluster, coherence-based ordering is al-
most always significantly better except in one
case. This shows that our method can effectively
learn the coherence of aspects based on how their
aligned opinion sentences are presented in online
discussions. (3) Order precision (pro) can hardly
distinguish different ordering algorithm. This in-
dicates that people vary a lot in their preferences
as which aspects should be presented first. How-
ever, in cases when the random baseline outper-
forms others the margin is fairly small, while
Freebase order and coherence-based order have a
much larger margin of improvement when show-
ing superior performance.
5 Conclusions and Future Work
A major challenge in automatic integration of
scattered online opinions is how to organize all
the diverse opinions in a meaningful way for any
given topic. In this paper, we propose to solve this
challenge by exploiting related aspects in struc-
tured ontology which are guaranteed to be mean-
ingful and well connected to the topic. We pro-
posed three different methods for selecting a sub-
set of aspects from the ontology that can best
capture the major opinions, including size-based,
opinion coverage-based, and conditional entropy-
based methods. We also explored two ways to
order aspects, i.e., ontology-order and coherence
optimization. In addition, we also proposed ap-
propriate measures for quantitative evaluation of
both aspect selection and ordering.
Experimental evaluation on two data sets (US
President and Digital Cameras) shows that by ex-
ploiting structured ontology, we can generate in-
teresting aspects to organize scattered opinions.
The conditional entropy method is shown to be
most effective for aspect selection, and the coher-
ence optimization method is more effective than
ontology-order in optimizing the coherence of the
aspect ordering, though ontology-order also ap-
pears to perform reasonably well. In addition, by
extracting salient phrases from the major opinions
that cannot be covered well by any aspect in an
existing ontology, we can also discover interest-
ing new aspects to extend the existing ontology.
Complementary with most existing summariza-
tion work, this work proposes a new direction of
using structured information to organize and sum-
marize unstructured opinions, opening up many
interesting future research directions. For in-
stance, in order to focus on studying aspect selec-
tion and ordering, we have not tried to optimize
sentences matching with aspects in the ontology;
it would be very interesting to further study how
to accurately retrieve sentences matching each as-
pect. Another promising future work is to orga-
nize opinions using both structured ontology in-
formation and well-written overview articles.
Acknowledgment
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National
Science Foundation under grants IIS-0347933,
IIS-0713581, IIS-0713571, and CNS-0834709.
741
References
Carenini, Giuseppe, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In K-CAP ?05: Proceedings of the 3rd international
conference on Knowledge capture, pages 11?18,
New York, NY, USA. ACM.
Chen, Hao and Susan Dumais. 2000. Bringing or-
der to the web: automatically categorizing search
results. In CHI ?00: Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 145?152, New York, NY, USA. ACM.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Gamon, Michael, Anthony Aue, Simon Corston-
Oliver, and Eric K. Ringger. 2005. Pulse: Min-
ing customer opinions from free text. In Famili,
A. Fazel, Joost N. Kok, Jose? Mar??a Pen?a, Arno
Siebes, and A. J. Feelders, editors, IDA, volume
3646 of Lecture Notes in Computer Science, pages
121?132. Springer.
Ka?ki, Mika. 2005. Optimizing the number of search
result categories. In CHI ?05: CHI ?05 extended
abstracts on Human factors in computing systems,
pages 1517?1520, New York, NY, USA. ACM.
Leouski, Anton V. and W. Bruce Croft. 1996. An eval-
uation of techniques for clustering search results.
Technical report.
Liu, Bing, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In WWW ?05: Proceedings of the
14th international conference on World Wide Web,
pages 342?351, New York, NY, USA. ACM.
Lu, Yue and Chengxiang Zhai. 2008. Opinion in-
tegration through semi-supervised topic modeling.
In Huai, Jinpeng, Robin Chen, Hsiao-Wuen Hon,
Yunhao Liu, Wei-Ying Ma, Andrew Tomkins, and
Xiaodong Zhang, editors, WWW, pages 121?130.
ACM.
Mei, Qiaozhu, Chao Liu, Hang Su, and ChengXiang
Zhai. 2006. A probabilistic approach to spatiotem-
poral theme pattern mining on weblogs. In WWW
?06: Proceedings of the 15th international confer-
ence on World Wide Web, pages 533?542.
Mei, Qiaozhu, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Berkhin, Pavel, Rich Caruana, and Xin-
dong Wu, editors, KDD, pages 490?499. ACM.
Pang, Bo and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In HLT ?05, pages 339?346, Morristown, NJ, USA.
Association for Computational Linguistics.
Porter, M. F. 1997. An algorithm for suffix stripping.
pages 313?316.
Sauper, Christina and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 208?216,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Titov, Ivan and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
WWW ?08: Proceeding of the 17th international
conference on World Wide Web, pages 111?120,
New York, NY, USA. ACM.
Zeng, Hua-Jun, Qi-Cai He, Zheng Chen, Wei-Ying
Ma, and Jinwen Ma. 2004. Learning to cluster
web search results. In SIGIR ?04: Proceedings
of the 27th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 210?217, New York, NY, USA.
ACM.
Zhai, Chengxiang and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of CIKM
2001, pages 403?410.
Zhao, Jing and Jing He. 2006. Learning to generate
labels for organizing search results from a domain-
specified corpus. In WI ?06: Proceedings of the
2006 IEEE/WIC/ACM International Conference on
Web Intelligence, pages 390?396, Washington, DC,
USA. IEEE Computer Society.
742
Coling 2010: Poster Volume, pages 1158?1166,
Beijing, August 2010
Shallow Information Extraction from Medical Forum Data
Parikshit Sondhi and Manish Gupta and ChengXiang Zhai and Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana Champaign
{sondhi1, gupta58, czhai, juliahmr}@illinois.edu
Abstract
We study a novel shallow information ex-
traction problem that involves extracting
sentences of a given set of topic cate-
gories from medical forum data. Given
a corpus of medical forum documents,
our goal is to extract two related types
of sentences that describe a biomedical
case (i.e., medical problem descriptions
and medical treatment descriptions). Such
an extraction task directly generates med-
ical case descriptions that can be useful
in many applications. We solve the prob-
lem using two popular machine learning
methods Support Vector Machines (SVM)
and Conditional Random Fields (CRF).
We propose novel features to improve the
accuracy of extraction. Experiment results
show that we can obtain an accuracy of up
to 75%.
1 Introduction
Conventional information extraction tasks gener-
ally aim at extracting finer granularity semantic
information units such as entities and relations.
While such detailed information is no doubt very
useful, extraction of such information also tends
to be difficult especially when the mentions of the
entities to be extracted do not conform to regular
syntactic patterns.
In this paper, we relax this conventional goal
of extraction and study an easier extraction task
where we aim at extracting sentences that belong
to a set of predefined semantic categories. That is,
we take a sentence as a unit for extraction. Specif-
ically, we study this problem in the context of ex-
tracting medical case description from medical fo-
rums.
A variety of medical health forums exist online.
People use them to post their problems, get ad-
vices from experienced patients, get second opin-
ions from other doctors, or merely to vent out their
frustration.
Compared with well-structured sources such as
Wikipedia, forums are more valuable in the sense
that they contain first hand patient experiences
with richer information in terms of what treat-
ments are better than others and why. Besides
this, on forums, patients explain their symptoms
much more freely than those mentioned on rela-
tively formal sources like Wikipedia. And hence,
forums are much more easier to understand for a
na??ve user.
However, even on targeted forums (which fo-
cus on a single disease), data is quite unstruc-
tured. There is therefore a need to structure out
this information and present it in a form that can
directly be used for a variety of other information
extraction applications like the collecting of med-
ical case studies pertaining to a particular disease,
mining frequently discussed symptoms, identify-
ing correlation between symptoms and treatments,
etc.
A typical medical case description tends to con-
sist of two aspects:
? Physical Examination/Symptoms (PE):
This covers current conditions and includes
any condition that is the focus of current
discussion. Note that if a drug causes an
allergy, then we consider it as a PE and
not a medication. Any condition that is the
focus of conversation, i.e. around which
1158
treatments are being proposed or questions
are being asked is considered PE even if the
user is recounting their past experience.
? Medications (MED): Includes medications
the person is currently taking, or is intend-
ing to take, or any medication on which the
question is targeted. Medications do not nec-
essarily mean drugs. Any measures (includ-
ing avoiding of substances) taken to treat or
avoid the symptoms are considered as medi-
cation. Sometimes, users also mention other
things like constituents of the drug, how
much of the drug to consume at a time, how
to get access to a medication, how much it
costs, side effects of medications, other qual-
ities of medications etc.
Figure 1 shows an example of PE and MED la-
belings.
Figure 1: Example of PE and MED labelings
We thus frame the problem of extracting med-
ical case descriptions as extracting sentences that
describe any of these two aspects. Specifically,
the task is to identify sentences in each of the two
related categories (i.e., PE and MED) from forum
posts. As an extraction task, this task is ?shal-
lower? than conventional information extraction
tasks such as entity extraction in the sense that
we extract a sentence as a unit, which makes the
extraction task more tractable. Indeed, the task
is more similar to sentence categorization. How-
ever, it also differs from a regular sentence cat-
egorization task (e.g., sentiment analysis) in that
the multiple categories are usually closely related
and categorization of multiple sentences may be
dependent in the sense that knowing the category
of one sentence may influence our decision about
the category of another sentence nearby. For ex-
ample, knowing that a sentence is in the category
PE should increase our belief that the next sen-
tence is of category of PE or MED.
We solve the problem using two popular ma-
chine learning methods, Support Vector Machines
(SVM) and Conditional Random Fields (CRF).
We define and study a large set of features, includ-
ing two kinds of novel features: (1) novel features
based on semantic generalization of terms, and (2)
novel features specific to forums.
Since this is a novel task, there is no existing
data set that we can use for evaluation. We thus
create a new data set for evaluation. Experiment
results show that both groups of novel features
are effective and can improve extraction accuracy.
With the best configurations, we can obtain an ac-
curacy of up to 75%, demonstrating feasibility of
automatic extraction of medical case descriptions
from forums.
2 Related work
Medical data mining has been looked atleast since
the early 2000s. Cios and Moore (2002) em-
phasize the uniqueness of medical data mining.
They stress that data mining in medicine is dis-
tinct from that in other fields, because the data
are heterogeneous, and special ethical, legal, and
social constraints apply to private medical infor-
mation. Treatment recommendation systems have
been built that use the structured data to diag-
nose based on symptoms (Lazarus et al, 2001)
and recommend treatments. Holt et al(2005) pro-
vide references to medical systems that use case
based reasoning methodologies for medical diag-
nosis. Huge amounts of medical data stored in
clinical data warehouses can be used to detect pat-
terns and relationships, which could provide new
medical knowledge (Lazarus et al, 2001). In con-
trast, we look at the problem of converting some
of the unstructured medical text data present in fo-
rum threads into structured symptoms and treat-
ments. This data can then be used by all of the
above mentioned applications.
Structuring of unstructured text has been stud-
ied by many works in the literature. Auto-
matic information extraction (Aone and Ramos-
Santacruz, 2000; Buttler et al, 2001) and wrap-
per induction techniques have been used for struc-
turing web data. Sarawagi (2008) and Laen-
1159
der et al (2002) offer comprehensive overviews
of information extraction and wrapper induction
techniques respectively. The main difference be-
tween our work and main stream work on extrac-
tion is that we extract sentences as units, which
is shallower but presumably more robust. Heinze
et al (2002) state that the current state-of-the-
art in NLP is suitable for mining information of
moderate content depth across a diverse collec-
tion of medical settings and specialties. Zhou
et al (2006), the authors perform information ex-
traction from clinical medical records using a de-
cision tree based classifier using resources such as
WordNet 1, UMLS 2 etc. They extract past medi-
cal history and social behaviour from the records.
In other related works, sentiment classifica-
tion (Pang et al, 2002; Prabowo and Thelwall,
2009; Cui et al, 2006; Dave et al, 2003) attempts
to categorize text based on polarity of sentiments
and is often applied at the sentence level (Kim and
Zhai, 2009). Some work has also been done on
extracting content from forum data. This includes
finding question answer pairs (Cong et al, 2008)
from online forums, auto-answering queries on a
technical forum (Feng et al, 2006), ranking an-
swers (Harabagiu and Hickl, 2006) etc. To the
best of our knowledge, this is the first work on
shallow extraction from medical forum data.
3 Problem formulation
Let P = (s1, ...sn) be a sequence of sentences
in a forum post. Given a set of interesting cate-
gories C = {c1, ..., ck} that describe a medical
case, our task is to extract sentences in each cat-
egory from the post P . That is, we would like to
classify each sentence si into one of the categories
ci or Background, which we treat as a special cat-
egory meaning that the sentence is irrelevant to
our extraction task. Depending on specific appli-
cations, a sentence may belong to more than one
category.
In this paper, we focus on extracting sen-
tences of two related categories describing a med-
ical case: (1) Physical Examination (PE), which
includes sentences describing the condition of
1http://wordnet.princeton.edu/
2http://www.nlm.nih.gov/research/umls
a patient (i.e., roughly symptoms) (2) Medica-
tions (MED), which includes sentences mention-
ing medications (i.e., roughly treatment). These
sentences provide a basic description of a medi-
cal case and can already be very useful if we can
extract them.
We chose to analyze at the sentence level be-
cause a sentence provides enough context to de-
tect the category accurately. For example, de-
tecting the categories at word level will not help
us to mark a sentence like ?I get very uncom-
fortable after eating cheese? as PE or mark a
sentence like ?It?s best to avoid cheese in that
case? as MED. Here the problem is loosely repre-
sented by a combination of ?uncomfortable eating
cheese? and the solution is represented loosely by
?avoid cheese?. Indeed, in preliminary analysis,
we found that most of the times, the postings con-
sist of PE and MED type sentences.
4 Methods
We use SVMs and CRFs to learn classifiers
to solve our problem. SVMs represent ap-
proaches that solve the problem as a classifi-
cation/categorization task while CRFs solve the
problem as a sequence labeling task. In this sec-
tion, we provide the basics of SVMs and CRFs.
4.1 Support Vector Machines
SVM first introduced in (Boser et al, 1992), are
a binary classifier that constructs a hyperplane
which separates the training instances belonging
to the two classes. SVMs maximize the separa-
tion margin between this hyperplane and the near-
est training datapoints of any class. The larger the
margin, the lower the generalization error of the
classifier. SVMs have been used to classify both
linearly and non-linearly seperable data, and have
been shown to outperform other popular classi-
fiers like decision trees, Na??ve Bayes classifiers,
k-nearest neighbor classifiers, etc. We use SVMs
as a representative classifier that does not consider
dependencies between the predictions on multiple
sentences.
4.2 Conditional Random Fields
Each of the sentences in the postings can itself
contain features which help us to categorize it.
1160
Besides this, statistical dependencies exist be-
tween sentences. Intuitively, a MED sentence will
follow a PE sentence with high probability, but the
probability of a PE sentence following an MED
sentence would be low. Conditional random fields
are graphical models that can capture such depen-
dencies among input sentences. A CRF model de-
fines a conditional distribution p(y|x) where y is
the predicted category (label) and x is the set of
sentences (observations). CRF is an undirected
graphical model in which each vertex represents
a random variable whose distribution is to be in-
ferred, and each edge represents a dependency be-
tween two random variables. The observation x
can be dependent on the current hidden label y,
previous n hidden labels and on any of the other
observations in a n order CRF. CRFs have been
shown to outperform other probabilistic graphical
models like Hidden Markov Models (HMMs) and
Maximum Entropy Markov Models (MeMMs).
Sutton and McCallum (2006) provide an excellent
tutorial on CRFs.
5 Features
To perform our categorization task, we use the fol-
lowing features.
? Word based features: This includes uni-
grams, bigrams and trigrams in the current
sentence. Each of the n-grams is mapped to a
separate boolean feature per sentence where
value is 1 if it appears in sentence and 0 oth-
erwise.
? Semantic features: This includes Unified
Medical Language System (UMLS3) seman-
tic groups of words in the current sentence.
UMLS is a prominent bio-medical domain
ontology. It contains approximately a mil-
lion bio-medical concepts grouped under 135
semantic groups. MMTX4 is a tool that al-
lows mapping of free text into UMLS con-
cepts and groups. We use these 135 semantic
groups as our semantic features. In order to
generate these features, we first process this
sentence through MMTX API which pro-
vides all the semantic groups that were found
3http://www.nlm.nih.gov/research/umls/
4http://mmtx.nlm.nih.gov/
in the sentence. Each of the semantic groups
becomes a boolean feature.
? Position based features: We define two
types of position based features: position of
the current sentence in the post and position
of the current post in the thread. These fea-
tures are specific to the forum data. We in-
clude these features based on the observa-
tions that first post usually contains condition
related sentences while subsequent posts of-
ten contain treatment measures for the cor-
responding condition. Each of the position
number of a sentence in a post and a post
in a thread is mapped to a boolean feature
which gets fired for a sentence at a partic-
ular position. E.g. For a sentence at po-
sition i in a post, POSITION IN POST i
would be set to 1 while other features PO-
SITION IN POST j where j 6= i would be
set to 0.
? User based features: We include a boolean
feature which gets fired when the sentence
is a part of a post by the thread creator.
This feature is important because most of the
posts by a thread creator have a high proba-
bility of being a PE.
? Tag based features(Edge features): We de-
fine features on tags (PE/MED/Backgnd) of
previous two sentences to capture local de-
pendencies between sentences. E.g., a set
of medication related tags often follow a de-
scription of a condition. We use these fea-
tures only for CRF based experiments.
? Morphological features: These include one
boolean feature each for presence of
? a capitalized word in the sentence
? an abbreviation in the sentence
? a number in the sentence
? a question mark in the sentence
? an exclamation mark in the sentence
? Length based features: We also consider the
number of words in a sentence as a separate
type of feature. Feature LENGTH i becomes
true for a sentence containing i words.
1161
Category Labeler 1 Labeler 2
PE 513 517
MED 286 280
Background 695 697
Table 1: Labeling results
6 Experiments
6.1 Dataset
Evaluation of this new extraction task is chal-
lenging as no test set is available. To solve
this problem, we opted to created our own test
set. HealthBoards5 is a medical forum web por-
tal that allows patients to discuss their ailments.
We scraped 175 posts contained in 50 threads on
allergy i.e., an average of 3.5 posts per thread
and around 2 posts per user with a maximum
of 9 posts by a particular user. Two humans
were asked to tag this corpus as conditions (i.e.,
PE category) or treatments (i.e., MED category)
or none on a per sentence basis. The corpus
consists of 1494 sentences. Table 1 shows the
labeling results. The data set is available at
(http://timan.cs.uiuc.edu/downloads.html). Also
the labeling results match quite well (82.86%)
with a Kappa statistic value of 0.73. Occasion-
ally (around 3%) PE and MED both occur in the
same sentence and the labelers chose to mark such
sentences as PE. In the case when the two label-
ers disagree, we manually analyzed the results and
further chose one of them for our experiments.
6.2 Evaluation methodology
For evaluation, we use 5-fold cross validation.
For CRFs, we used the Mallet6 toolkit and for
SVM, we used SVM-Light7. We experimented
by varying the size of the training set, with differ-
ent feature sets, using two machine learning mod-
els: SVMs and CRFs. Our aim is to accurately
classify any sentence in a post as PE or MED
or background. First we explore and identify the
feature sets that help us in attaining higher accu-
racy. Next, we identify the setting (sequence la-
beling by CRFs or independent classification by
SVMs) that works better to model our problem.
5http://www.healthboards.com
6http://mallet.cs.umass.edu/
7http://svmlight.joachims.org/
We present most of our results using four metrics:
precision, recall, F1 measure and average accu-
racy which is the ratio of correctly labeled sen-
tences to the total sentences.
We considered the following features: all the
2647 words in the vocabulary (no stop-word re-
moval or any other type of selection), 10858 bi-
grams, 135 semantic groups from UMLS, two po-
sition based features, one user based feature, two
tag based features, four morphological features
and one length based feature as described in the
previous section. Thus our feature set is quite
rich. Note that other than the usual features, se-
mantic, position-based and user-based features are
specific to the medical domain or to forum data.
6.3 Basic Results
First we considered word features, and learned a
linear chain CRF model. We added other sets of
features one by one, and observed variations in ac-
curacy. Table 2 shows the accuracy in terms of
precision, recall and F1. Note that these results are
for an Order 1 linear-chain CRF. Accuracy is mea-
sured as ratio of the number of correct labelings of
PE, MED and background to the total number of
sentences in our dataset. Notice that the MED ac-
curacy values are in general quite low compared
to those of PE. As we will discuss later, accuracy
is low for MED because our word-based features
are not discriminative enough for the MED cate-
gory.
From Table 2, we see that the accuracy keeps
increasing as we add semantic UMLS based fea-
tures, position based features and morphological
features. However, length based features (word
count), user-based faetures, and bigrams do not re-
sult in any improvements. We also tried trigrams,
but did not observe any accuracy gains. Thus we
find that semantic features and position-based fea-
tures which are specific to the medical domain
and the forum data respectively are helpful when
added on top of word features, while generic fea-
tures such as length-based features tend to not add
value.
We also trained an order 2 CRF using the same
set of features. Results obtained were similar to
order 1 CRFs and so we do not report them here.
This shows that local dependencies are more im-
1162
Feature set PE Prec MED Prec PE Recall MED Recall PE F1 MED F1 Accuracy %
Word 0.60 0.49 0.65 0.36 0.62 0.42 63.43
+Semantic 0.61 0.52 0.68 0.37 0.64 0.43 65.05?
+Position 0.63 0.54 0.7 0.34 0.66 0.42 65.45
+Morphological 0.64 0.52 0.69 0.36 0.66 0.42 65.70
+WordCount 0.62 0.51 0.70 0.33 0.66 0.40 65.23
+Thread Creator 0.62 0.51 0.71 0.34 0.66 0.41 65.49
+Bigrams 0.62 0.51 0.69 0.34 0.66 0.41 64.82
Table 2: Order 1 Linear Chain CRF. ?Improvement over only word features significant at 0.05-level,
using Wilcoxon?s signed-rank test
portant in medical forum data and global depen-
dencies do not add further signal.
Further, we perform experiments using SVMs
using the same set of features. Table 3 shows
accuracy results on SVM. Again PE is detected
with higher accuracy compared to MED. Unlike
CRFs, SVMs do not incorporate the notion of lo-
cal dependencies between sentences. However,
we observe that SVMs outperform CRFs, as is ev-
ident from the results in Table 3. This is interest-
ing, since it suggests that the SVM accuracy can
potentially be further enhanced by incorporating
such dependency information (e.g. in the form
of new features). We leave this as part of future
work.
Figure 2 shows an example of a forum post
(which talks about allergy to dogs) being tagged
using our CRF model.
Figure 2: Tagging example of a forum post
6.4 Feature selection
Incremental addition of different feature types did
not lead to substantial improvement in perfor-
mance. This suggests that none of the feature
classes contains all ?good? features. We there-
fore perform feature selection based on informa-
tion gain and choose the top 4253 features from
among all the features discussed earlier, based on
a threshold for the gain. This results in improve-
ment in the accuracy values over the previous best
results (Table 4).
Among the word feature set, we found that
important features were allergy, alergies, food,
hives, allergic, sinus, bread. Among bigrams, al-
lergic to, ear infections, my throat, are allergic,
to gluten, food allergies have high information
gain values. Among the UMLS based se-
mantic groups, we found that patf (Pathologic
Function), dsyn (Disease or Syndrome), orch
(Organic Chemical), phsu (Pharmacologic Sub-
stance), sosy (Sign or Symptom) have high in-
formation gain values. Also looking at the word
count feature, we notice that background sen-
tences are generally short sentences. All these fea-
tures are clearly highly discriminative.
6.5 Variation in training data size
We varied the amount of training data used for
learning the models to observe the variation in
performance with size of training data. Table 5
shows the variation in accuracy (PE F1, MED
F1 and average accuracy) for different sizes of
training data using CRFs. In general, we observe
that accuracy improves as we increase the training
data, but the degree varies with the feature sets
used. We see similar trends in SVM also. These
results show that it is possible to further improve
prediction accuracy by obtaining additional train-
ing data.
6.6 Probing into the low MED accuracy
As observed in Tables 2 and 3, MED accuracy
is quite low compared to PE accuracy. We wish
to gain a deeper insight into why the MED ac-
curacy suffers. Therefore, we plot the frequency
of words in sentences marked as PE or MED ver-
sus the rank of the word as shown in the figure 3.
We removed the stop words. Observe that for PE
the curve is quite steep. This indicates that there
1163
Feature set PE Prec MED Prec PE Recall MED Recall PE F1 MED F1 Accuracy %
Word 0.65 0.52 0.71 0.28 0.68 0.36 66.13
+Semantic 0.73 0.54 0.73 0.38 0.73 0.45 71.02?
+Position 0.71 0.52 0.71 0.35 0.71 0.42 69.61
+Morphological 0.72 0.53 0.72 0.38 0.72 0.44 70.28
+WordCount 0.74 0.54 0.72 0.37 0.73 0.44 71.55
+Thread Creator 0.74 0.56 0.72 0.39 0.73 0.46 72.02
+Bigrams 0.75 0.54 0.72 0.40 0.74 0.46 71.69
Table 3: SVM results. ?Improvement over only word features significant at 0.05-level, using
Wilcoxon?s signed-rank test
Classifier PE Prec PE Recall PE F1 MED Prec MED Recall MED F1 Accuracy %
SVM (all* features) 0.72 0.53 0.72 0.38 0.72 0.44 70.28
SVM (selected features) 0.75 0.75 0.75 0.61 0.33 0.44 75.08?
CRF (all* features) 0.64 0.52 0.69 0.36 0.66 0.42 65.70
CRF (selected features) 0.60 0.77 0.67 0.58 0.37 0.45 65.93?
Table 4: Accuracy using the best feature set. (*Word +Semantic +Position +Morphological features).
?Improvement over all* features significant at 0.05-level, using Wilcoxon?s signed-rank test
are some discriminative words which have very
high frequency and so the word features observed
in the training set alo get fired for sentences in
the test set with high probability. While for MED,
we observe that most of the words have very low
frequencies. This basically means that discrimi-
native words for MED may not occur with good
enough frequency. So, many of the word features
that show up in the training set may not appear in
the test data. Hence, MED accuracy suffers.
50
60
70
80
f t
he
 te
rm
PE
MED
0
10
20
30
40
Fr
eq
ue
nc
y 
of
1 31 61 91 12
1
15
1
18
1
21
1
24
1
27
1
30
1
33
1
36
1
39
1
42
1
45
1
Rank of the term
Figure 3: Freq of words vs rank for PE and MED
6.7 Multi-class vs Single class categorization
Note that our task is quite different from plain sen-
tence categorization task. We observe that there is
a dependence between the categories (PE/MED)
that we are trying to predict per sentence. For ex-
ample, considering 100% training data, Table 6
compares the precision, recall and F1 values when
PE MED Backgnd EOP
PE 0.54 0.13 0.28 0.05
MED 0.15 0.51 0.30 0.04
Backgnd 0.18 0.08 0.54 0.20
BOP 0.40 0.07 0.53 0.0
Table 7: Transition probability values
SVM and CRF are trained as single class classi-
fiers using word+semantic features with the multi-
class results obtained previously. Results are gen-
erally better when we do multi-class categoriza-
tion versus single-class categorization. This trend
was reflected for other featuresets also.
6.8 Analysis of transition probabilities
Table 7 shows the transition probabilities from
one category to another as calculated based on our
labelled dataset. BOP is beginning of posting and
EOP is end of posting. Note that posts often start
with a PE or a background sentence and often end
with a background sentence. Also, consecutive
sentences within a posting tend to belong to the
same category.
6.9 Error analysis
We also perform some error analysis on results us-
ing the best feature set. Table 8 shows the confu-
sion matrix for CRF/SVM. We observe many of
the MED errors are because an MED sentence of-
ten gets marked as PE. This basically happens be-
cause some sentences contain both PE and MED.
1164
Feature set 25% 50% 75% 100%
Word 0.59/0.21/0.57 0.6/0.36/0.60 0.61/0.39/0.62 0.62/0.42/0.63
+Semantic 0.61/0.17/0.59 0.63/0.32/0.61 0.64/0.38/0.63 0.64/0.43/0.65
+Position 0.59/0.18/0.56 0.64/0.29/0.60 0.65/0.33/0.62 0.66/0.42/0.65
+Morphological 0.6/0.19/0.57 0.64/0.32/0.61 0.65/0.37/0.63 0.66/0.42/0.65
Best 0.61/0.18/0.65 0.66/0.28/0.64 0.66/0.38/0.66 0.69/0.43/0.68
Table 5: Precision, recall, and F value for various sizes of training data set.
Classifier Type PE Prec PE Recall PE F1 MED Prec MED Recall MED F1
SVM PE vs BKG 0.79 0.64 0.71 - - -
SVM MED vs BKG - - - 0.6 0.28 0.39
SVM Multi-class 0.73 0.73 0.73 0.54 0.38 0.45
CRF PE vs BKG 0.68 0.64 0.66 - - -
CRF MED vs BKG - - - 0.53 0.3 0.39
CRF Multi-class 0.61 0.68 0.64 0.52 0.37 0.43
Table 6: Multi-class vs Single-class categorization with word+semantic features
PE MED Backgnd
PE 424/404 37/37 81/101
MED 102/70 107/95 81/125
Backgnd 164/62 55/21 618/754
Table 8: Confusion matrix showing counts of
actual vs predicted labels for (Best CRF Classi-
fier/Best SVM Classifier)
Other than that some of the PE keywords are also
present in MED sentences, and since the few dis-
criminative MED keywords are quite low in fre-
quency, MED accuracy suffers. E.g. The sen-
tence ?i?m still on antibiotics for the infection but
they don?t seem to be doing any good anymore.?
was labeled as MED but marked as PE by the
CRF. The sentence clearly talks about a medica-
tion. However, the keyword ?infection? is often
observed in PE sentences and so the CRF marks
the sentence as PE.
7 Conclusion
In this paper, we studied a novel shallow infor-
mation extraction task where the goal is to extract
relevant sentences to a predefined set of categories
that describe a medical case. We proposed to
solve the problem using supervised learning and
explored two representative approaches (i.e., CRF
and SVM). We proposed and studied two different
types of novel features for this task, including gen-
eralized terms and forum structure features. We
also created the first test set for evaluating this
problem. Our experiment results show that (1) the
proposed new features are effective for improving
the extraction accuracy, and (2) it is feasible to au-
tomatically extract medical cases in this way, with
the best prediction accuracy above 75%.
Our work can be further extended in several
ways. First, since constructing a test set is labor-
intensive, we could only afford experimenting
with a relatively small data set. It would be in-
teresting to further test the proposed features on
larger data set. Second, while in CRF, we have
shown adding dependency features improves per-
formance, it is unclear how to evaluate this po-
tential benefit with SVM. Since SVM generally
outperforms CRF for this task, it would be very
interesting to further explore how we can extend
SVM to incorporate dependency.
8 Acknowledgement
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National
Science Foundation under grants IIS-0347933,
IIS-0713581, IIS-0713571, and CNS-0834709.
References
Aone, Chinatsu and Mila Ramos-Santacruz. 2000.
Rees: a large-scale relation and event extraction sys-
tem. In ANLP.
Boser, Bernhard E., Isabelle Guyon, and Vladimir Vap-
nik. 1992. A training algorithm for optimal mar-
1165
gin classifiers. In Computational Learing Theory,
pages 144?152.
Buttler, David, Ling Liu, and Calton Pu. 2001. A fully
automated object extraction system for the world
wide web. In ICDCS.
Cios, Krzysztof J. and William Moore. 2002. Unique-
ness of medical data mining. Artificial Intelligence
in Medicine, 26:1?24.
Cong, Gao, Long Wang, Chin-Yew Lin, Young-In
Song, and Yueheng Sun. 2008. Finding question-
answer pairs from online forums. In SIGIR ?08:
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 467?474, New York,
NY, USA. ACM.
Cui, Hang, Vibhu Mittal, and Mayur Datar. 2006.
Comparative Experiments on Sentiment Classifica-
tion for Online Product Reviews. In Proc. of the Na-
tional Conf. on Artificial Intelligence, pages 1265?
1270.
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the Peanut Gallery: Opinion
Extraction and Semantic Classification of Product
Reviews. In Proc. of WWW, pages 519?528.
Feng, Donghui, Erin Shaw, Jihie Kim, and Eduard
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
IUI ?06: Proceedings of the 11th international con-
ference on Intelligent user interfaces, pages 171?
177, New York, NY, USA. ACM.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In ACL-44: Proceedings of the 21st
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 905?
912, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Heinze, Daniel T., Mark L. Morsch, and John Hol-
brook. 2002. Mining free-text medical records. In
Proceedings of the AMIA Annual Symposium.
Holt, Alec, Isabelle Bichindaritz, Rainer Schmidt, and
Petra Perner. 2005. Medical applications in case-
based reasoning. Knowl. Eng. Rev., 20(3):289?292.
Kim, Hyun Duk and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In CIKM, pages 385?394.
Laender, Alberto H. F., Berthier A. Ribeiro-neto, Alti-
gran S. da Silva, and Juliana S. Teixeira. 2002. A
brief survey of web data extraction tools. SIGMOD
Record.
Lazarus, R, K P Kleinman, I Dashevsky, A DeMaria,
and R Platt. 2001. Using automated medical
records for rapid identification of illness syndromes
(syndromic surveillance): the example of lower res-
piratory infection. BMC Public Health, 1:9.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment Classification using
Machine Learning techniques. In Proc. of EMNLP,
pages 79?86.
Prabowo, Rudy and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143?157, April.
Sarawagi, Sunita. 2008. Information extraction.
Foundations and Trends in Databases, 1.
Sutton, Charles and Andrew Mccallum, 2006. In-
troduction to Conditional Random Fields for Rela-
tional Learning. MIT Press.
Zhou, Xiaohua, Hyoil Han, Isaac Chankai, Ann Pre-
strud, and Ari Brooks. 2006. Approaches to text
mining for clinical medical records. In SAC ?06:
Proceedings of the 2006 ACM symposium on Ap-
plied computing, pages 235?239, New York, NY,
USA. ACM.
1166
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 66?76,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Summarizing Contrastive Viewpoints in Opinionated Text
Michael J. Paul?
University of Illinois
Urbana, IL 61801, USA
mjpaul2@illinois.edu
ChengXiang Zhai
University of Illinois
Urbana, IL 61801, USA
czhai@cs.uiuc.edu
Roxana Girju
University of Illinois
Urbana, IL 61801, USA
girju@illinois.edu
Abstract
This paper presents a two-stage approach to
summarizing multiple contrastive viewpoints
in opinionated text. In the first stage, we
use an unsupervised probabilistic approach to
model and extract multiple viewpoints in text.
We experiment with a variety of lexical and
syntactic features, yielding significant perfor-
mance gains over bag-of-words feature sets.
In the second stage, we introduce Compara-
tive LexRank, a novel random walk formula-
tion to score sentences and pairs of sentences
from opposite viewpoints based on both their
representativeness of the collection as well as
their contrastiveness with each other. Exper-
imental results show that the proposed ap-
proach can generate informative summaries of
viewpoints in opinionated text.
1 Introduction
The amount of opinionated text available online has
been growing rapidly, increasing the need for sys-
tems that can summarize opinions expressed in such
text so that a user can easily digest them. In this pa-
per, we study how to summarize opinionated text in
a such a way that highlights contrast between multi-
ple viewpoints, which is a little-studied task.
Usually, online opinionated text is generated by
multiple people, and thus often contains multi-
ple viewpoints about an issue or topic. A view-
point/perspective refers to ?a mental position from
which things are viewed? (cf. WordNet). An opin-
ion is usually expressed in association with a partic-
ular viewpoint, even though the viewpoint is usually
?Now at Johns Hopkins University (mpaul@cs.jhu.edu).
not explicitly given; for example, a blogger that is
in favor of a policy would likely look at the positive
aspects of the policy (i.e., positive viewpoint), while
someone against the policy would likely empha-
size the negative aspects (i.e., negative viewpoint).
Moreover, in an opinionated text with diverse opin-
ions, the multiple viewpoints taken by opinion hold-
ers are often ?contrastive?, leading to opposite po-
larities. Indeed, such contrast in opinions may be a
main driving force behind many online discussions.
Futhermore, opinions regarding news events and
other short-term issues may quickly emerge and dis-
appear. Such opinions may reflect many differ-
ent types of viewpoints which cannot be modeled
by current systems. For this reason, we believe
that a viewpoint summarization system would ben-
efit from the ability to extract unlabeled viewpoints
without supervision. Even if such clustering has in-
accuracies, it could still be a useful starting point for
human editors to select representative excerpts.
Thus, given a set of opinionated documents about
a topic, we aim at automatically extracting and sum-
marizing the multiple contrastive viewpoints implic-
itly expressed in the opinionated text to facilitate
digestion and comparison of different viewpoints.
Specifically, we will generate two types of multi-
view summaries: macro multi-view summary and
micro multi-view summary. A macro multi-view
summary would contain multiple sets of sentences,
each representing a different viewpoint; these differ-
ent sets of sentences can be compared to understand
the difference of multiple viewpoints at the ?macro
level.? A micro multi-view summary would con-
tain a set of pairs of contrastive sentences (each pair
66
consists of two sentences representing two different
viewpoints), making it easy to understand the differ-
ence between two viewpoints at the ?micro level.?
Although opinion summarization has been exten-
sively studied (e.g., (Liu et al, 2005; Hu and Liu,
2004; Hu and Liu, 2006; Zhuang et al, 2006)), ex-
isting work has not attempted to generate our envi-
sioned contrastive macro and micro multi-view sum-
maries in an unsupervised way, which is the goal of
our work. For example, Hu and Liu (2006) rank sen-
tences based on their dominant sentiment according
to the polarity of adjectives occuring near a product
feature in a sentence. A contradiction occurs when
two sentences are highly unlikely to be simultane-
ously true (cf. (Marneffe et al, 2008)). Although
little work has been done on contradiction detection,
there are a few notable approaches (Harabagiu et al,
2006; Marneffe et al, 2008; Kim and Zhai, 2009).
The closest work to ours is perhaps that of Ler-
man and McDonald (2009) who present an approach
to contrastive summarization. They add an objective
to their summarization model such that the summary
model for one set of text is different from the model
for the other set. The idea is to highlight the key
differences between the sets, however this is a dif-
ferent type of contrast than the one we study here ?
our goal is instead to make the summaries similar to
each other, to contrast how the same information is
conveyed through different viewpoints.
In this paper, we propose a two-stage approach
to solving this novel summarization problem, which
will be explained in the following two sections.
2 Modeling Viewpoints
The first challenge to be solved in order to generate
a contrastive summary of multiple viewpoints is to
model and extract these viewpoints which are hidden
in text. In this paper we propose to solve this chal-
lenge by employing the Topic-Aspect Model (TAM)
(Paul and Girju, 2010), which is an extension of the
Latent Dirichlet Allocation (LDA) model (Blei et al,
2003) for jointly modeling topics and viewpoints in
text. While most existing work on such topic models
(including TAM) has taken a topic model as a gen-
erative model for word tokens in text, we propose to
take TAM as a generative model for more complex
linguistic features extracted from text. These are
more discriminative than single word tokens and can
improve the accuracy of extracting multiple view-
points as we will show in the experimental results?
section. Below we first give a brief introduction to
TAM and then present the proposed set of features.
2.1 Topic-Aspect Model (TAM)
LDA-style probabilistic topic models of document
content (Blei et al, 2003) have been shown to offer
state-of-the-art summarization quality. Such mod-
els also provide a framework for adding additional
structure to a summarization model (Haghighi and
Vanderwende, 2009). In our case, we want to add
more structure to a model to incorporate the notion
of viewpoint/perspective into our summaries.
When it comes to extracting viewpoints, recent re-
search suggests that it may be beneficial to model
both topics and perspectives, as sentiment may be
expressed differently depending on the issue in-
volved (Brody and Elhadad, 2010; Paul and Girju,
2010). For example, let?s consider a set of product
reviews for a home theater system. Content topics
in this data might include things like sound qual-
ity, usability, etc., while the viewpoints might be
the positive and negative sentiments. A word like
speakers, for instance depends on the sound topic
but not a viewpoint, while good would be an exam-
ple of a word that depends on a viewpoint but not
any particular topic. A word like loud would depend
on both (since it would be considered positive senti-
ment only in the context of the sound quality topic),
while a word like think depends on neither.
We make use of a recent model, the Topic-Aspect
Model (Paul and Girju, 2010), which can model
such behavior with or without supervision. Under
this model, a document has a mixture over topics as
well as a mixture over viewpoints. The two mix-
tures are drawn independently of each other, and
thus can be thought of as two separate clustering di-
mensions. A word is associated with variables de-
noting its topic and viewpoint assignments, as well
as two binary variables to denote if the word de-
pends on the topic and if the word depends on the
viewpoint. A word may depend on the topic, the
viewpoint, both, or neither, as in the above example.
The generative process for a document d under
this model can be briefly described as follows. For
each word in a document:
67
1. Sample a topic z from P (z|d) and a viewpoint v
from P (v|d).
2. Sample a ?level? ` ? {0, 1} from P (`|d). This
determines if the word will depend on the topic (topical
level) or not (background level).
3. Sample a ?route? r ? {0, 1} from P (r|`, z). This
determines if the word will depend on the viewpoint.
4. Sample a word w from P (w|z, v, r, `).
The probabilities are multinomial/binomial dis-
tributions with Dirichlet/Beta priors, and thus this
model falls under the standard LDA framework. The
number of topics and number of viewpoints are pa-
rameters that must be specified. Inference can be
done with Gibbs sampling (Paul and Girju, 2010).
TAM naturally gives us a very rich output to
use in a viewpoint summarization application. If
we are doing unsupervised viewpoint extraction,
we can use the output of the model to compute
P (v|sentence) which could be used to generate
summaries that contain only excerpts that strongly
highlight one viewpoint over another. Similarly,
we could use the learned topic mixtures to generate
topic-specific summaries. Futhermore, the variables
r and ` tell us if a word is dependent on the view-
point and topic, and we could use this information
to focus on sentences that contain informative con-
tent words. Note that without supervision, TAM?s
clustering is based only on co-occurrences and the
patterns it captures may or may not correspond with
the viewpoints we wish to extract. Nonetheless, we
show in this research that it can indeed find mean-
ingful viewpoints with reasonable accuracy on cer-
tain data sets. Although we do not explore this in
this paper, additional information about the view-
points could be added to TAM by defining priors on
the distributions to further improve the accuracy of
viewpoint discovery.
2.2 Features
Previous work with TAM used only bag of words
features, which may not be the best features for cap-
turing viewpoints. For example, ?Israel attacked
Palestine? and ?Palestine attacked Israel? are iden-
tical excerpts in an exchangable bag of words rep-
resentation, yet one is more likely to come from the
perspective of a Palestinian and the other from an Is-
raeli. In this subsection, we will propose a variety of
feature sets. We evaluate the utility of these features
to the task of modeling viewpoints by measuring the
accuracy of unsupervised clustering.
2.2.1 Words
We have experimented with simple bag of words
features as baseline approaches, both with and with-
out removing stop words, and found that the accu-
racy of clustering by viewpoint is better when re-
taining all words. This supports the observation
that common function words may have important
psychological properties (Chung and Pennebaker,
2007). Thus, we do not do any stop word removal
for any of our other feature sets. We find that we get
better results by stemming the words, so we apply
Porter?s stemmer to all of our features described.
2.2.2 Dependency Relations
It has been shown that using syntactic information
can improve the accuracy of sentiment models (Joshi
and Rose?, 2009). Thus, instead of representing doc-
uments as a bag of words, we will experiment with
using features returned by a dependency parser. For
this, we used the Stanford parser1, which returns de-
pendency tuples of the form rel(a, b) where rel is
some dependency relation and a and b are tokens of
a sentence. We can use these specific tuples as fea-
tures, referred here as the full-tuple representation.
One problem with this representation is that we
are using very specific information and it is harder
for learning algorithms to find patterns due to the
lack of redundancy. One solution is to generalize
these features and rewrite a tuple rel(a, b) as two
tuples: rel(a, ?) and rel(?, b) (Greene and Resnik,
2009; Joshi and Rose?, 2009). We will refer to this as
the split-tuple representation.
2.2.3 Negation
If a word wi appears in the head of a neg rela-
tion, then we would like this to be reflected in other
dependency tuples in which wi occurs. For a tuple
rel(wi, wj), if either wi or wj is negated, then we
simply rewrite it as ?rel(wi, wj).
An alternative would be to rewrite the individual
word wi as ?wi. However in our experiments this
representation produced worse accuracies, perhaps
because this produces less redundancy.
1http://nlp.stanford.edu/software/
68
2.2.4 Polarity
We also hypothesize that lexical polarity informa-
tion may improve our model. If we are using the
full-tuple representation, then a tuple becomes more
general by replacing the specific word with a + or
?. In the case that both words are polarity words,
we use two tuples, replacing only one word at a
time rather than replacing both words with their po-
larity signs. To determine the polarity of a word,
we simply use the Subjectivity Clues lexicon (Wil-
son et al, 2005) and as polarity values, positive (+),
negative (-), and neutral (*). Under our split-tuple
representation, this becomes more specific by re-
placing the ? with the polarity sign. For example,
the tuple amod(idea, good) would be represented
as amod(idea,+) and amod(?, good). We collapse
negated features to flip the polarity sign such that
?rel(a,+) becomes rel(a,?).
2.2.5 Generalized Relations
We also experimented with backing off the rela-
tions themselves. Since the Stanford dependencies
can be organized in a hierarchy2, we will represent
the relations at more generalized levels in the hi-
erarchy. For example, both a direct object and an
indirect object are a type of object. For a relation
rel, we define Rrel as the relation above rel in
the hierarchy ? for example, Rdobj = obj. We
make an exception for neg which has its own im-
portant properties that we wish to retain, so we let
Rneg = neg. Thus, when using these features, we
rewrite rel(a, b) as Rrel(a, b).
3 Multi-Viewpoint Summarization
As a computation problem, extractive multi-
viewpoint summarization would take as input a set
of candidate excerpts3 X = {x1, x2, ..., x|X|} with
k viewpoints and generate two types of multi-view
contrastive summaries: 1) A macro contrastive sum-
mary Smacro consists of k disjoint sets of excerpts,
X1, X2, ..., Xk ? X with each Xi containing repre-
sentative sentences of the i-th view (i.e., Smacro =
(X1, ..., Xk)). The number of excerpts in each Xi
can be empirically set based on application needs.
2The complete hierarchy can be found in the Stanford de-
pendencies manual (Marneffe and Manning, 2008).
3An ?excerpt? refers to the smallest unit of text that will
make up our summary such as a sentence.
2) A micro contrastive summary Smicro consists
of a set of excerpt pairs, each containing two ex-
cerpts from two different viewpoints, i.e., Smicro =
{(s1, t1), ..., (sn, tn)} where si ? X and ti ? X are
two comparable excerpts representing two different
viewpoints. n is the length of the summary, which
can be set empirically based on application needs.
Note that both macro and micro summaries can re-
veal contrast between different viewpoints, though
at different granularity levels.
To generate macro and micro summaries based
on the probabilistic assignment of excerpts to view-
points given by TAM, we propose a novel extension
to the LexRank algorithm (Erkan and Radev, 2004),
a graph-based method for scoring representative ex-
cerpts to be used in a summary. Our key idea is to
modify the definition of the jumping probability in
the random walk model so that it would favor ex-
cerpts that represent a viewpoint well and encour-
age jumping to an excerpt comparable with the cur-
rent one but from a different viewpoint. As a re-
sult, the stationary distribution of the random walk
model would capture representative contrastive ex-
cerpts and allow us to generate both macro and mi-
cro contrastive summaries within a unified frame-
work. We now describe this novel summarization
algorithm (called Comparative LexRank) in detail.
3.1 Comparative LexRank
LexRank is a PageRank-like algorithm (Page et al,
1998), where we define a random walk model on
top of a graph that has sentences to be summarized
as nodes and edges placed between two sentences
that are similar to each other. We can then score
all the sentences based on the expected probability
of a random walker visiting each sentence. We use
the short-hand P (xj |xi) to denote the probability of
being at node xj at a time t given that the walker
was at xi at time t ? 1. The jumping probability
from node xi to node xj is given by:
P (xj |xi) =
sim(xi, xj)
?
j??X sim(xi, xj?)
(1)
where sim is a content similarity function defined
on two sentence/excerpt nodes.
Our extension is mainly to modify this jumping
probability in two ways so as to favor visiting con-
trastive representative opinions from multiple view-
69
points. The first modification is to make it favor
jumping to a good representative excerpt x of any
viewpoint v (i.e., with high probability p(v|x) ac-
cording to the TAM model). The second modifica-
tion is to further favor jumping between two excerpts
that can potentially form a good contrastive pair for
use in generating a micro contrastive summary.
Specifically, under our model, the random walker
first decides whether to jump to a sentence of the
same viewpoint or to a sentence of a different view-
point. We define this decision as a binary variable
z ? {0, 1}. Intuitively, if we can force the ran-
dom walker to move back and forth between view-
points, then the final scores will favor sentences that
are similar across both viewpoints.
We define two different modified similarity func-
tions for the two possible values of z. The first one,
sim0 (corresponding to z = 0) scales the similarity
by the likelihood that the two x?s represent the same
viewpoint, and the second one, sim1 (for z = 1)
scales the similarity by the likelihood that the x?s
come from different viewpoints.
sim0(xi, xj) = sim(xi, xj)
k?
m=1
P (v = m|xi)P (v = m|xj)
sim1(xi, xj) = sim(xi, xj)?
?
m1,m2?[1,k],m1 6=m2
P (v = m1|xi)P (v = m2|xj)
where P (v|x) denotes the probability that the ex-
cerpt x belongs to the viewpoint v, and in general,
can be obtained through any multi-viewpoint model.
A special case of this is when the labels for view-
points are known, in which case P (v|x) = 1 for the
correct label and 0 for the others.
In our experiments, P (v|x) comes from the out-
put of TAM, and we define sim(xi, xj) as the cosine
between the vectors xi and xj , although again any
similarity function could be used. The conditional
transition probability from xi to xj given z is then:
P (xj |xi, z) =
simz(xi, xj)
?
j??X simz(xi, xj?)
(2)
Using ? to denote P (z = 0) and marginalizing
across z, we have the transition probability:
P (xj |xi) = ?P (xj |xi, z = 0)+ (1??)P (xj |xi, z = 1)
The stationary distribution of the random walk
gives us a scoring of the excerpts to be used in our
summary. It is also possible to score pairs of ex-
cerpts that contrast each other. We define the score
for a pair (xi, xj) as the probability of being at xi
and transitioning to xj or vice versa, where xi and
xj are of opposite viewpoints. Specifically:
P (xi)P (xj |xi, z = 1) + P (xj)P (xi|xj , z = 1) (3)
3.2 Summary Generation
The final summary should be a set of excerpts that
have a high relevance score according to our scoring
algorithm, but are not redundant among each other.
Many techniques could be used to accomplish this
(Carbonell and Goldstein, 1998; McDonald, 2007),
but we use a simple greedy approach: at each step
of the summary generation algorithm, we add the
excerpt with the highest relevance score as long as
the excerpt?s redundancy score ? the cosine similar-
ity between the candidate and the current summary
? is under some threshold ?. This is repeated until
the summary reaches a user-supplied length limit.
Macro contrastive summarization: A macro-level
summary consists of independent summaries for
each viewpoint, which we generate by first using the
random walk stationary distribution across all of the
data to rank the excerpts. We then separate the top-
ranked excerpts into two disjoint sets according to
their viewpoint based on whichever gives a greater
value of P (v|x), and finally remove redundancy and
produce the summary according to our method de-
scribed above. We refer to this as macro contrastive
summarization, because the summaries will contrast
each other in that they have related content, but the
excerpts in the summaries are not explicitly aligned
with each other.
Micro contrastive summarization: A candidate
excerpt for a micro-level summary will consist of
a pair (xi, xj) with the pairwise relevance score de-
fined in Equation 3. We can then rank these pairs and
remove redundancy. It is possible that both xi and xj
in a high-scoring pair may belong to the same view-
point; such a case would be filtered out since we are
mainly interested in including contrastive pairs in
our summary. We refer to this as micro contrastive
summarization, because the summaries will allow
us to see contrast at the level of individual excerpts
from different viewpoints.
70
4 Experiments and Evaluation
4.1 Experimental Setup
Evaluation of multi-view summarization is challeng-
ing as there is no existing data set we can use. We
leverage the resources on the Web and created two
data sets in the domain of political opinion.
Our first dataset is a set of 948 verbatim responses
to a Gallup R? phone survey about the 2010 U.S.
healthcare bill (Jones, 2010), conducted March 4-7,
2010. Responses in this set tend to be short and of-
ten incomplete or otherwise ill-formed and informal
sentences. Respondants indicate if they are ?for? or
?against? the bill, and there is a roughly even mix of
the two viewpoints (45% for and 48% against).
We also use the Bitterlemons corpus, a collection
of 594 editorials about the Israel-Palestine conflict.
This dataset is fully described in (Lin et al, 2006)
and has been used in other perspective modeling lit-
erature (Lin et al, 2008; Greene and Resnik, 2009).
The style of this data differs substantially from the
healthcare data in that documents in this set tend to
be long and verbose articles with well-formed sen-
tences. It again contains a fairly even mixture of two
different perspectives: 312 articles from Israeli au-
thors and 282 articles from Palestinian authors.
Moreover, for the healthcare data set, manually
extracted opinion polls are available on the Web,
which we further leverage to construct gold stan-
dard summaries to evaluate our method quantita-
tively. The data and test sets are available at
http://apfel.ai.uiuc.edu/resources.html.
4.2 Stage One: Modeling Viewpoints
The main research question we want to answer in
modeling viewpoints is whether richer feature sets
would lead to better accuracy than word features.
We used our various feature sets as input to TAM
and measured the accuracy of clustering documents
by viewpoint. This evaluation serves both to mea-
sure how accurately this type of clustering can be
done, as well as to measure which types of features
are important for modeling viewpoints.
We found that the clustering accuracy is improved
if we measure the accuracy of only the subset of
documents such that P (v|doc) is greater than some
threshold (we used 0.8). Thus, the accuraries pre-
sented in this section are measured using this confi-
dence threshold. We will use this approach for the
summarization task as well, as it ensures we are only
summarizing documents where we have high confi-
dence about their viewpoint membership.
There are several parameters to set for TAM.
Since our focus is on comparing linguistic features
with word features, we simply set these parame-
ters to some reasonable values: We used Dirich-
let pseudo-counts of 80.0 for P (` = 0), 20.0 for
P (` = 1), uniform pseudo-counts of 5.0 for P (x),
0.1 for the topic and aspect mixtures, and 0.01 for
the word distributions. We tell the model to use 2
viewpoints as well as 5 topics for the healthcare cor-
pus and 8 topics for the Bitterlemons corpus.
There is high variance in the accuracies depend-
ing on how the Gibbs samplers were initialized. We
thus repeated the experiments many times to obtain
relatively confident measures ? 200 times for the
healthcare set and 50 times for the Bitterlemons set,
with 2000 iterations each time. A natural way to se-
lect a model is to choose the model that gives the
highest likelihood to its input. To evaluate how well
this selection strategy would work, we measured the
correlation between accuracy and likelihood.
The results are shown in Table 1. We can make
several observations. (1) In all cases, the proposed
linguistic features yield higher accuracy than the
word features, supporting our hypothesis that for
viewpoint modeling, applying TAM to these features
improves performance over using simple word fea-
tures. Since virtually all existing work on topic mod-
els assumes word tokens as data to be modeled, our
results suggest that it would be interesting to explore
applying generative topic models to complex fea-
tures for other tasks as well. This may be because by
adding additional complex features to the observed
data, we artificially inflate the data likelihood to em-
phasize modeling co-occurrences of such features,
which effectively biases the model to capture a cer-
tain perspective of co-occurrences.
(2) The increase is substantially greater for the
Bitterlemons corpus, which may be due to the fact
that the parsing accuracy is likely better because the
language is formal. The split-tuple representation
is very significantly better for the healthcare corpus,
but it is not clear which is better for the Bitterlemons
corpus. It is also not clear how the generalized rela-
tions affect the performance.
71
Healthcare Corpus Bitterlemons Corpus
Feature Set Mean Med Max MaxLL Corr Mean Med Max MaxLL Corr
bag of words 61.12 +/- 0.76% 61.01 72.17 52.92 0.187 68.22 +/- 3.31% 69.26 88.27 84.94 0.39
- no stopwords 60.58 +/- 0.79% 60.50 72.18 62.58 0.154 61.29 +/- 3.05% 57.69 91.34 82.91 0.33
full-tuples 62.42 +/- 0.88% 62.47 74.04 63.37 0.201 80.89 +/- 3.45% 85.40 94.07 92.10 0.34
+ negation 63.67 +/- 0.81% 64.54 74.07 69.25 0.338 80.60 +/- 3.88% 88.07 95.61 91.32 0.66
+ neg. + polarity 63.16 +/- 0.94% 64.46 74.05 67.8 0.455 82.53 +/- 3.55% 86.64 94.44 91.16 0.31
gen. full-tuples 63.80 +/- 0.73% 64.35 73.29 71.70 0.254 76.62 +/- 4.09% 84.56 94.53 84.56 0.25
split-tuples 68.32 +/- 0.90% 70.74 77.80 76.57 0.646 77.14 +/- 3.64% 81.29 92.99 88.13 0.30
+ negation 68.00 +/- 0.91% 69.11 79.73 76.14 0.187 83.53 +/- 3.05% 87.71 95.00 95.00 0.12
+ neg. + polarity 65.11 +/- 1.05% 65.35 78.59 67.22 0.159 81.24 +/- 3.37% 83.44 95.03 88.55 0.08
gen. split-tuples 69.31 +/- 0.83% 70.69 77.90 73.90 0.653 76.69 +/- 4.36% 83.78 93.60 91.67 0.09
Table 1: The clustering accuracy with TAM using a variety of feature sets. These results were averaged over 200 randomly-initialized Gibbs
sampling procedures for the healthcare set, and 50 procedures for the Bitterlemons set. The 95% confidence interval using a standard t-test is also
given. Max refers to the maximum accuracy obtained over the 200 or 50 instances. MaxLL refers to the clustering accuracy using the model that
yielded the highest corpus log-likelihood as defined by TAM. Corr refers to the Pearson correlation coefficient between accuracy and log-likelihood.
(3) It appears that adding polarity helps the full-
tuple features (by making them more general) but
hurts the split-tuple features (by making them more
specific). Negation significantly improves the full-
tuple features in the Bitterlemons corpus, but it is
not clear if it helps in the other cases. It should be
noted that capturing negation and polarity is a very
complex and difficult task, and it is not expected that
our simple approaches will accurately capture these
properties. Nonetheless, it seems that these simple
features may help in certain cases.
4.3 Stage Two: Summarizing Viewpoints
For the second stage (i.e., the Comparative LexRank
algorithm), we mainly want to evaluate the quality
of the generated contrastive multi-viewpoint sum-
mary and study the effectiveness of our extension
to the standard LexRank. Below we present exten-
sive evaluation of our summarization method on the
healthcare data. We do not have an evaluation set
with which to compute quantitative metrics on the
Bitterlemons corpus, so we will instead perform a
simple qualitative evaluation in the last subsection.
4.3.1 Gold Standard Summaries
The responses to the Gallup healthcare poll are
described in an article4 which gives a table of the
main responses found in the data along with their
prominence in the data. In a way, this represents an
expert human-generated summary of our database,
and we will use this as a gold standard macro con-
trastive summary against which the representative-
4http://www.gallup.com/poll/126521/Favor-Oppose-
Obama-Healthcare-Plan.aspx
ness of a multi-viewpoint contrastive summary can
be evaluated. The reasons given in this table will
be used verbatim as our reference set, excluding the
other/no-reason/no-opinion reasons. A sample of
this table is shown in Table 2.
We also want to develop a reference set for micro
contrastive summaries, where we are mainly inter-
ested in evaluating contrastiveness. To do this, we
asked 3 annotators to identify contrastive pairs in
the ?main reasons? table described above. Each pair
must contain one reason from the ?for? side and one
reason from the ?against? side, though we do not re-
quire a one-to-one alignment; that is, multiple pairs
may contain the same reason. We take the set of
pairs that were identified as being contrastive by at
least 2 annotators to be our gold set of contrastive
pairs. Because these pairs come from the gold sum-
mary, they are still representative of the collection as
a whole, rather than fine-grained contrasts.
The macro reference set contains 9 ?for? reasons
and 15 ?against? reasons. The micro reference set
contains 13 annotator-identified pairs composed of 9
unique ?for? reasons and 8 unique ?against? reasons.
4.3.2 Baseline Approaches
Graph-based algorithms: The standard LexRank
algorithm can also be used to score pairs of sen-
tences according to Equation 3. We will thus com-
pare our new LexRank extension to the unmodified
form of this algorithm. When ? = 1, the random
walk model only transitions to sentences within the
same viewpoint, and thus in this case our modified
algorithm produces the same ranking as the unmod-
ified LexRank. This will be our first baseline.
72
For Against
People need health insurance/Too many uninsured 29% Will raise costs of insurance/Make it less affordable 20%
System is broken/Needs to be fixed 18% Does not address real problems 19%
Costs are out of control/Would help control costs 12% Need more information/clarity on how system would work 8%
Moral responsibility to provide/Obligation/Fair 12% Against big government/Too much government involvement 8%
Table 2: Some of the top reasons given along with their prominence in the healthcare data, as analyzed by Gallup. This is a sample of what will
serve as our gold set. The highlighted cells show an example of a contrastive pair identified by our annotators.
Model-based algorithms: We will also compare
against the approach of Lerman and McDonald
(2009) who introduce their contrastiveness objective
into a model-based summarization algorithm. The
basic form of this algorithm is to select a set of sen-
tences Sm to minimize the KL-divergence between
the models of the summary Sm and the entire collec-
tion Xm for a viewpoint m. The objective function
is: ?
?k
m=1KL(L(Sm)||L(Xm)) where L is an ar-
bitrary language model. We define L(A) simply as
the unigram distribution over words in the collection
A, a method also evaluated by Haghighi and Vander-
wende (2009). This is the fairest comparison to our
LexRank experiments, where sentences are also rep-
resented as unigrams. (We do not do any modeling
with TAM in our quantitative evaluation.)
Lerman and McDonald introduce an additional
term to maximize the KL-divergence between the
summary of one viewpoint and the collection of the
opposite viewpoint, so that each viewpoint?s sum-
mary is dissimilar to the other viewpoints. We bor-
row this idea but instead do the opposite so that the
viewpoints? summaries are more (rather than less)
similar to each other. This contrastive version of our
model-based baseline is formulated as:
?
k?
m1=1
KL(L(Sm1)||L(Xm1)) +
(
1
k?1
?
m2?[1,k],m1 6=m2
KL(L(Sm1)||L(Xm2))
)
Our summary generation algorithm is to iteratively
add excerpts to the summary in a greedy fash-
ion, selecting the excerpt with the highest score in
each iteration. Note that this approach only gen-
erates macro-level summaries, leaving us with the
LexRank baseline for micro-level summaries.
4.3.3 Metrics
We will evaluate our summaries using a variant of
the standard ROUGE evaluation metric (Lin, 2004).
Recall that we have two different evaluation sets
? one that contains all of the reasons for each view-
point, and one that consists only of aligned pairs of
excerpts. Since the same excerpt may appear in mul-
tiple pairs, there would be significant redundancy in
our reference summary if we were to include every
pair. Thus, we will restrict a contrastive reference
summary to exclude overlapping pairs, and we will
have many reference sets for all possible combina-
tions of pairs. There is only one reference set for the
representativeness criterion.
Our reference summaries have a unique property
in that the summaries have already been annotated
with the prominence of the different reasons in the
data. A good summary should capture the more
prominent statements, so we will include this in our
scoring function. We thus augment the basic ROUGE
n-gram recall score by weighting the n-gram counts
in the reference summary according to this percent-
age. This is a generalization of the standard ROUGE
formula where this percentage would be uniform.
For evaluating the macro-level summaries, we
will score the summaries for the two viewpoints sep-
arately, given a reference set Refi and a candidate
summary Ci for a viewpoint v = i. The final score
is a combination of the scores for both viewpoints,
i.e. Srep = 0.5S(Refi, Ci)+0.5S(Refj , Cj) where
S(Ref,C) is our ROUGE-based scoring metric. It
would also be interesting to measure how well a
viewpoint?s summary matches the gold summary
of the opposite viewpoint, which will give insights
into how well the Comparative LexRank algorithm
makes the two summaries similar to each other. We
will measure this as the inverse of the above metric,
i.e. Sopp = 0.5S(Refi, Cj) + 0.5S(Refj , Ci).
Finally, to score the micro-level comparative sum-
maries (recall that this gives explicitly-aligned pairs
of excerpts), we will concatenate each pair (xi, xj)
as a single excerpt, and use these as the excerpts in
our reference and candidate summaries. The scor-
ing function is then Sp = S(Refpairs, Cpairs). Note
that we have multiple reference summaries for the
73
? Srep-1 Sopp-1 Srep-2 Sopp-2 Sp-1 Sp-2
0.0 .425 .416 .083 .060 .309 .036
0.2 .410 .423 .082 .065 .285 .044
0.5 .419 .434 .085 .072 .386 .044
0.8 .410 .324 .095 .028 .367 .062
1.0 .354 .240 .070 .006 .322 .057
MB .362 .246 .089 .003
MC .347 .350 .054 .059
Table 3: Our evaluation scores for various values of ?. Smaller val-
ues of ? favor greater contrastiveness. Note that ? = 1 should be con-
sidered a baseline, because at this value the algorithm ignores the con-
trastiveness and it becomes a standard summarization problem. MB and
MC refer to our model-based baselines described in Subsection 4.3.2.
Bold scores are significant over all baselines according to a paired t-test.
micro-level evaluation due to overlapping pairs in
the evaluation set. In this case, the ROUGE score
is defined as the maximum score among all possible
reference summaries (Lin, 2004).
We measure both unigram (removing stop words,
denoted S-1) and bigram (retaining stop words, de-
noted S-2) recall, stemming words in all cases.
4.3.4 Evaluation Results
In order to evaluate our Comparative LexRank
algorithm by itself, in this subsection we will not
use the output of TAM as part of our summariza-
tion input, and will assign excerpts fixed values of
P (v|x) = 1 for the correct label and 0 otherwise.
We constructed our sentence vectors with unigrams
(removing stop words) and no IDF weighting.
We set the PageRank damping factor (Erkan and
Radev, 2004) to 0.01 and tried combinations of
the redundancy threshold ? ? {0.01, 0.05, 0.1, 0.2}
with different values of ?, the parameter which con-
trols the level of contrastiveness. For each value of
?, we optimized ? on the original data set according
to Srep?Sopp so that we can directly compare these
scores, and then we tuned ? separately for Sp. The
summary length is 6 excerpts. To obtain more ro-
bust results, we repeated the experiment 100 times
on random half-size subsets of our data. The scores
shown in Table 3 are averaged across these trials.
In general, increasing ? increases Srep, which
suggests that tuning ? behaves as expected, and
high- and mid-range ? values indeed produce sum-
maries where the summaries of the two viewpoints
are more similar to each other. Similarly, mid-range
? values produce substantially higher values of Sp-1,
the unigram ROUGE scores for the micro contrastive
summary, although there is not a large difference be-
tween the bigram scores. An example of our micro-
level output is shown in Table 4.
As for our model-based baseline, we show results
for both the basic algorithm (denoted MB) in addi-
tion to the contrastive modification (denoted MC).
We see that the contrastive modification behaves
as expected and produces much higher scores for
Sopp, however, this method does not outperform our
LexRank algorithm. It is interesting to note that in
almost all cases where a contrastive objective is in-
troduced, the scores for the opposite viewpoint Sopp
increase without decreasing the Srep scores, sug-
gesting that contrastiveness can be introduced into a
multi-view summarization problem without dimin-
ishing the overall quality of the summary. It is
admittedly difficult to make generalizations about
these methods from experiments with only one data
set, but we have at least some evidence that our al-
gorithm works as intended.
4.4 Unsupervised Summarization
So far we have focused on evaluating our viewpoint
clustering models and our multi-view summariza-
tion algorithms separately. We will finally show how
these two stages might work in tandem in unsuper-
vised summarization of the Bitterlemons corpus.
Without a gold set, it is difficult to perform an
extensive automatic evaluation as we did with the
healthcare data. Instead we will perform a sim-
ple qualitative evaluation to see if the algorithm ap-
pears to achieve its goal. Thus, we asked 8 people
to guess if each viewpoint?s summary was written
by Israeli or Palestinian authors. To diversify the
summaries, for each annotator we randomly split
each summary into two equal-sized subsets of the
sentence set. Thus each person was asked to label
four different summaries, which were presented in
a random order. If humans can correctly identify
the viewpoints, then this would suggest both that the
TAM accurately clustered documents by viewpoint
and the summarization algorithm is selecting sen-
tences that coherently represent the viewpoints.
We first ran TAM on our data using the same pro-
cedure and parameters as in Subsection 4.2 using the
full-tuple features. We repeated this 10 times and
used the model that gave the highest data likelihood
as our model for summarization input. We then gen-
74
For the Healthcare Bill Against the Healthcare Bill
the government already provides half of the healthcare dollars in the government is too much involvement.
united states [...] [they] might as well spend their dollars smarter
my kids are uninsured. a lot of people will be getting it that should be getting it on their own,
and my kids will be paying a lot of taxes.
so everybody would have it and afford it. we cannot afford it.
because of my family. i don?t know enough about it and i don?t know where exactly
it?s going to put my family.
because i have no health insurance and i need it. because i have health insurance.
cost of healthcare is so high. high costs.
Table 4: An example of our micro-level contrastive summarization output on the healthcare data, using ? = 0.05 and ? = 0.5.
erated macro contrastive summaries of our data for
the two viewpoints with 6 sentences per viewpoint.
We used unigram sentence vectors with IDF weight-
ing. We used ? = 0.5 and ? = 0.1, which gave the
highest score at this ? value on the healthcare data.
Only one of these sentences was clustered incor-
rectly by TAM. The human judges correctly labeled
78% of the summary sets, suggesting that our sys-
tem accurately selected some sentences that could
be recognized as belonging to the viewpoints, but
is not perfect. Unsupervised micro-level summaries
were less coherent. Many of the sentences are mis-
labeled, and the ones that are correctly labeled are
not representative of the collection.
This is not surprising, and indeed exposes the
challenge inherent in our problem definition: clus-
tering documents based on similarity and then high-
lighting sentences with high similarity but opposite
cluster membership are almost conflicting objectives
for an unsupervised learner. Such contrastive pairs
are perhaps the most difficult data points to model.
A good test of a viewpoint model may be whether it
can capture the nuanced properties of the viewpoints
needed to contrast them at the micro level.
5 Discussion
The properties of the text which we attempt to sum-
marize in our work are related to the concept of
framing from political science (Chong and Druck-
man, 2010), which is defined as ?an interpretation or
evaluation of an issue, event, or person that empha-
sizes certain of its features or consequences? focus-
ing on ?certain features and implications of the issue
? rather than others.? For example, someone in favor
of the healthcare bill might focus on the benefits and
someone against the bill might focus on the cost.
However, our approach is different in that our
contrastive objective encourages the summaries to
include each point as addressed by all viewpoints,
rather than each viewpoint selectively emphasizing
only certain points. In a sense, this makes our sum-
mary more like a live debate, where one side must
directly respond to a point raised by the other side.
For example, someone in favor of healthcare reform
might cite the high cost of the current system, but
someone against this might counter-argue that the
proposed system in the new bill has its own high
costs (as seen in the last row of Table 4). The idea is
to show how both sides address the same issues.
Thus, we can say that we are summarizing the
key arguments/issues/points from different opinions.
Futhermore, our models and algorithms are defined
very generally, and while we tested their viability in
the domain of political opinion, they may also be
useful for many other comparative tasks.
In conclusion, we have presented steps toward a
two-stage system that can automatically extract and
summarize viewpoints in opinionated text. First, we
have shown that accuracy of clustering documents
by viewpoint can be enhanced by using simple but
rich dependency features. This can be done within
the framework of existing probabilistic topic models
without altering the models simply by using a ?bag
of features? representation of documents.
Second, we have introduced Comparative
LexRank, an extension of the LexRank algorithm
that aims to generate contrastive summaries both at
the macro and micro level. The algorithm presented
is general enough that it can be applied to any
number of viewpoints, and can accomodate input
where the viewpoints are either given fixed labels,
or given probabilistic assignments. The tradeoff
between contrast and representation can flexibly be
tuned to an application?s needs.
75
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
NAACL ?10.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR ?98, pages
335?336.
Dennis Chong and James N. Druckman. 2010. Identi-
fying frames in political news. In Erik P. Bucy and
R. Lance Holbert, editors, Sourcebook for Political
Communication Research: Methods, Measures, and
Analytical Techniques. Routledge.
Cindy Chung and James W. Pennebaker. 2007. The psy-
chological function of function words. Social Commu-
nication: Frontiers of Social Psychology, pages 343?
359.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479.
Stephan Greene and Philip Resnik. 2009. More than
words: syntactic packaging and implicit sentiment. In
NAACL ?09, pages 503?511.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
NAACL ?09, pages 362?370.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760.
Minqing Hu and Bing Liu. 2006. Opinion extraction and
summarization on the Web. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI-
2006), Nectar Paper Track, Boston, MA.
Jeffrey M. Jones. 2010. ?in u.s., 45% favor, 48% oppose
obama healthcare plan?, March.
Mahesh Joshi and Carolyn Penstein Rose?. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL-IJCNLP ?09: Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 313?316.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating
comparative summaries of contradictory opinions in
text. In CIKM ?09: Proceeding of the 18th ACM con-
ference on Information and knowledge management,
pages 385?394, New York, NY, USA. ACM.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
summarization: an experiment with consumer reviews.
In NAACL ?09, pages 113?116, Morristown, NJ, USA.
Association for Computational Linguistics.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In CoNLL-X ?06: Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 109?116.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In ECML PKDD ?08: Proceedings
of the European conference on Machine Learning and
Knowledge Discovery in Databases - Part II, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In WWW ?05: Proceedings of the 14th
international conference on World Wide Web, pages
342?351, New York, NY, USA. ACM Press.
Marie-Catherine De Marneffe and Christopher Manning.
2008. Stanford typed dependencies manual. Techni-
cal report, Stanford University.
Marie-Catherine De Marneffe, Anna Rafferty, and
Christopher Manning. 2008. Finding contradictions
in text. In Proceedings of the Association for Compu-
tational Linguistics Conference (ACL).
Ryan McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization. In
ECIR?07: Proceedings of the 29th European confer-
ence on IR research, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
Digital Library Technologies Project.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In AAAI-2010: Twenty-Fourth Confer-
ence on Artificial Intelligence.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347?354.
Li Zhuang, Feng Jing, Xiao-yan Zhu, and Lei Zhang.
2006. Movie review mining and summarization. In
Proceedings of the ACM SIGIR Conference on Infor-
mation and Knowledge Management (CIKM).
76
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1511?1521, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Discriminative Model for Query Spelling Correction with Latent
Structural SVM
Huizhong Duan, Yanen Li, ChengXiang Zhai and Dan Roth
University of Illinois at Urbana-Champaign
201 N Goodwin Ave
Urbana, IL 61801
{duan9, yanenli2, czhai, danr}@illinois.edu
Abstract
Discriminative training in query spelling cor-
rection is difficult due to the complex inter-
nal structures of the data. Recent work on
query spelling correction suggests a two stage
approach a noisy channel model that is used
to retrieve a number of candidate corrections,
followed by discriminatively trained ranker
applied to these candidates. The ranker, how-
ever, suffers from the fact the low recall of the
first, suboptimal, search stage.
This paper proposes to directly optimize the
search stage with a discriminative model
based on latent structural SVM. In this model,
we treat query spelling correction as a multi-
class classification problem with structured in-
put and output. The latent structural informa-
tion is used to model the alignment of words
in the spelling correction process. Experiment
results show that as a standalone speller, our
model outperforms all the baseline systems. It
also attains a higher recall compared with the
noisy channel model, and can therefore serve
as a better filtering stage when combined with
a ranker.
1 Introduction
Query spelling correction has become a crucial com-
ponent in modern information systems. Particularly,
search engine users rely heavily on the query cor-
rection mechanism to formulate effective queries.
Given a user query q, which is potentially mis-
spelled, the goal of query spelling correction is to
find a correction of the query c that could lead to a
better search experience. A typical query spelling
correction system employs a noisy channel model
(Kernighan et al 1990). The model assumes that
the correct query c is formed in the user?s mind be-
fore entering the noisy channels, e.g., typing, and
get misspelled. Formally, the model maximizes the
posterior probability p(c|q):
c? = arg max
c
p(c|q). (1)
Applying Bayes rule, the formulation can be
rewritten as:
c? = arg max
c
p(q|c)p(c)
= arg max
c
[log p(q|c) + log p(c)].
(2)
The model uses two probabilities. The prior prob-
ability p(c) represents how likely it is that c is the
original correct query in the user?s mind. The prob-
ability is usually modeled by a language model es-
timated from a sizable corpus. The transformation
probability p(q|c) measures how likely it is that q is
the output given that c has been formed by the user.
This probability can be either heuristic-based (edit
distance) or learned from samples of well aligned
corrections. One problem with the noisy channel
model is that there is no weighting for the two kinds
of probabilities, and since they are estimated from
different sources, there are usually issues regarding
their scale and comparability, resulting in subopti-
mal performance (Gao et al 2010). Another limita-
tion of this generative model is that it is not able to
take advantage of additional useful features.
1511
A discriminative model may solve these problems
by adding the flexibility of using features and apply-
ing weights. But training such a model is not easy.
The difficulty is that the output space of query cor-
rection is enormous, as the candidate corrections for
each a query term could be the entire vocabulary.
This is even worse when word boundary errors (i.e.
merging and splitting of words) exist. The problem
is intractable with standard discriminative models as
we cannot enumerate every candidate correction.
To solve the problem, (Gao et al 2010) proposed
a two stage approach. In this approach, a ranker is
trained to score each candidate correction of a query.
When a query is issued, the system first uses the
noisy channel model with a standard search algo-
rithm to find the 20 best candidates. Then the ranker
is used to re-rank these candidates and find the best
correction for the query. This ranker based system
has one critical limitation, though. Since the ranking
stage is decoupled from the search, it relies on the
outsourced search algorithm to find the candidates.
Because query spelling correction is an online oper-
ation, only a small number of candidates can enter
the ranker due to efficiency concerns, thus limiting
the ability of the ranker to the ceiling of recall set by
the suboptimal search phase.
The research question we address here is whether
we can directly optimize the search phase of query
spelling correction using a discriminative model
without loss of efficiency. More specifically, we
want 1) a learning process that is aware of the
search phase and interacts with its result; 2) an ef-
ficient search algorithm that is able to incorporate
the learned model and guide the search to the target
spelling correction.
In this paper, we propose a new discriminative
model for query correction that maintains the ad-
vantage of a discriminative model in accommodat-
ing flexible combination of features and naturally in-
corporates an efficient search algorithm in learning
and inference. Similarly to (Chang et al 2010) we
collapse a two stage process into a single discrim-
inatively trained process, by considering the output
of the first stage as an intermediate latent represen-
tation for the joint learning process. Specifically, we
make use of the latent structural SVM (LS-SVM)
(Yu and Joachims, 2009) formulation. We formu-
late the problem query spelling correction as a multi-
class classification problem on structured inputs and
outputs. The advantage of the structural SVM model
is that it allows task specific, customizable solutions
for the inference problem. This allows us to adapt
the model to make it work directly with the search
algorithm we use for finding the best correction of
the query. To account for word boundary errors, we
model the word alignment between the query and
the correction as a latent structural variable. The
LS-SVM model allows us to jointly search over the
output space and the latent structure space.
As the inference algorithm in the proposed dis-
criminative model we use an algorithm that resem-
bles a traditional noisy channel model. To adapt
the LS-SVM model to enable the efficient search of
query spelling correction, we study how features can
be designed. We analyze the properties of features
that can be used in the search algorithm and propose
a criteria for selecting and designing new features.
We demonstrate the use of the criteria by design-
ing separate features for different types of spelling
errors (e.g. splitting, merging). With the proposed
discriminative model, we can directly optimize the
search phase of query spelling correction without
loss of efficiency. Our model can be used not only as
a standalone speller with high accuracy, but also as
a high recall candidate generation stage for a ranker
based system.
Experiments verify the effectiveness of the dis-
criminative model, as the accuracy of correction can
be improved significantly over baseline systems in-
cluding an award winning query spelling system.
Even though the optimization is primarily based on
the top correction, the weights trained by LS-SVM
can be used to search for more candidate corrections.
The improvement in recall at different levels over the
noisy channel model demonstrates that our model is
superior even when used in the two-stage approach..
2 Related Work
Spelling correction has a long history (Levenshtein,
1966). Traditional techniques were on small scale
and depended on having a small trusted lexicons
(Kukich, 1992). Later, statistical generative mod-
els were shown to be effective in spelling correc-
tion, where a source language model and an er-
ror model were identified as two major components
1512
(Brill and Moore, 2000). Note that we are not deal-
ing here with the standard models in context sen-
sitive spelling (Golding and Roth, 1999) where the
set of candidate correction is a known ?confusion
set?. Query spelling correction, a special form of
the problem, has received much attention in recent
years. Compared with traditional spelling correc-
tion task, query spelling deals with more complex
types of misspellings and a much larger scale of lan-
guage. Research in this direction includes utiliz-
ing large web corpora and query log (Chen et al
2007; Cucerzan and Brill, 2004; Ahmad and Kon-
drak, 2005), employing large-scale n-gram models,
training phrase-based error model from clickthrough
data (Sun et al 2010) and developing additional fea-
tures (Gao et al 2010).
Query alteration/refinement is a very relevant
topic to query spelling correction. The goal of
query alteration/refinement is to modify the inef-
fective query so that it could . Researches on this
track include query expansion (Xu and Croft, 1996;
Qiu and Frei, 1993; Mitra et al 1998), query con-
traction(Kumaran and Allan, 2008; Bendersky and
Croft, 2008; Kumaran and Carvalho, 2009) and
other types of query reformulations for bridging the
vocabulary gap (Wang and Zhai, 2008). (Guo et al
2008) proposed a unified model to perform a broad
set of query refinements including correction, seg-
mentation and stemming. However, it has very lim-
ited ability in query correction. In this paper, we
study the discriminative training of query spelling
correction, which is potentially beneficial to many
existing studies.
Noisy channel model (or source channel model)
has been widely used in NLP. Many approaches have
been proposed to perform discriminative training of
the model (McCallum et al 2000; Lafferty, 2001).
However, these approaches mostly deal with a rela-
tively small search space where the number of can-
didates at each step is limited (e.g. POS tagging). A
typically used search algorithm is dynamic program-
ming. In spelling correction, however, the search
space is much bigger and the existing approaches
featuring dynamic programming are difficult to be
applied.
Structural learning and latent structural learning
has been studied a lot in NLP in recent years(Chang
et al 2010; Dyer et al 2011), and has been
shown to be useful in a range of NLP applications
from Textual Entailment, Paraphrasing and Translit-
eration (Chang et al 2010) to sentiment analysis
(Yessenalina et al 2010).
Work has also been done on integrating discrimi-
native learning in search. Freitag and Khadivi used a
perceptron algorithm to train for sequence alignment
problem. A beam search algorithm was utilized in
the search (Freitag and Khadivi, 2007). Daume et
al. proposed the Searn framework for search based
structural prediction (Daume et al 2009). Our
model differs from the Searn framework in that it
learns to make global decisions rather than accumu-
lating local decisions. The global decision was made
possible by an efficient search algorithm.
Query spelling correction also shares many sim-
ilarities with statistical machine translation (SMT).
Sun et al(2010) has formulated the problem within
an SMT framework. However, SMT usually in-
volves more complex alignments, while in query
spelling correction search is the more challenging
part. Our main contribution in this paper is a novel
unified way to directly optimize the search phase of
query spelling correction with the use of LS-SVM.
3 Discriminative Model for Query Spelling
Correction Based on LS-SVM
In this section, we first present the discriminative
formulation of the problem of query spelling correc-
tion. Then we introduce in detail the model we use
for solving the problem.
3.1 The Discriminative Form of Query Spelling
Correction
In query spelling correction, given a user entered
query q, which is potentially misspelled, the goal is
to find a correction c, such that it could be a more
effective query which improves the quality of search
results. A general discriminative formulation of the
problem is of the following form:
f(q) = arg max
c?V?
[w ??(q, c)], (3)
where ?(q, c) is a vector of features and w is the
model parameter. This discriminative formulation is
more general compared to the noisy channel model.
It has the flexibility of using features and applying
1513
weights. The noisy channel model is a special case
of the discriminative form where only two features,
the source probability and the transformation proba-
bility, are used and uniform weightings are applied.
However, this problem formulation does not give us
much insight on how to proceed to design the model.
Especially, it is unclear how ?(q, c) can be com-
puted.
To enhance the formulation, we explore the fact
that spelling correction follows a word-by-word pro-
cedure. Let us first consider a scenario where word
boundary errors does not exist. In this scenario,
each query term matches and only matches to a sin-
gle term in the correction. Formally, let us denote
q = q1, ..., qn and c = c1, ..., cm as structured ob-
jects from the space of V?, where V is our vocabu-
lary of words and V? is all possible phrases formed
by words in V . Both q and c have an intrinsic se-
quential structure. When no word boundary error
exists, |c| = |q| holds for any candidate correction
c. qi and ci establish a one-to-one mapping. In this
case, we have a more detailed discriminative form:
f(q) = arg max
c?V|q|
[w ? (?0 +
|q|?
i=1
?1(qi, ci))], (4)
where ?0 is a vector of normalizing factors,
?1(qi, ci) is the decomposed computation of ?(q, c)
for each query term qi and ci, for i = 1 to |q|.
Equation 4 is a clearer formulation. The major
challenge of solving this discriminative problem is
the complexity. Theoretically, each term has |V|
candidates and it is impossible to enumerate over
all possible combinations. To make it even worse,
merging and splitting errors are quite common in
misspelling. As a result, the assumption of one-to-
one mapping does not hold in practice.
To account for these word boundary errors and
enhance the discriminative formulation, we intro-
duce a latent variable a to model the unobserved
structural information. More specifically, a =
a1, a2, ...a|a| is the alignment between q and c. Each
alignment node at is a represented by a quadruple
(qstart, qend, cstart, cend). Figure 1 shows a com-
mon merge error and its best alignment. The phrase
?credit card?, in this case, is incorrectly merged into
one word ?creditcard? by the user. Figure 2 shows
Figure 1: Example of Merge Error and Alignment
Figure 2: Example of Split Error and Alignment
the best alignment for a common split error, where
the word ?gamespot? is incorrectly split into a two
word phrase ?game spot?.
Taking into consideration the latent variable, we
arrive at our final discriminative form of query
spelling correction:
f(q) = arg max(c,a)?Vn?A[w ??(q, c, a)]
= arg max(c,a)?V??A[w ? (?0
+
?|a|
t=0 ?1(qat , cat , at))],
(5)
The challenges of successfully applying a dis-
criminative model to this problem formulation are
1) how can we design a learning algorithm to learn
the model parameter w to directly optimize the max-
imization problem; 2) how can we solve the maxi-
mization efficiently without having to enumerate all
candidates; 3) how can we design features to guar-
antee the correctness of the search algorithm. In the
following subsections we introduce our solutions to
the three challenges in detail.
3.2 Latent Structural SVM
We employ the latent structural SVM (LS-SVM)
model for learning the discriminative model of query
spelling correction. LS-SVM is a large margin
method that deals with structured prediction prob-
lems with latent structural information (Yu and
Joachims, 2009). LS-SVM has the merit of allowing
1514
task specific, customizable solutions for the infer-
ence problem. This makes it easy to adapt to learn-
ing the model parameters for different problems.
The following is a brief introduction of LS-SVM
that largely mirrors the work by (Yu and Joachims,
2009).
Without loss of generality, let us aim at learning
a prediction function f : X ? Y that maps input
x ? X to an output y ? Y with latent structural
information h ? H. The decision function is of the
following form:
f(x) = arg max
(y,h)?Y?H
[w ??(x, y, h)], (6)
where ?(x, y, h) is the set of feature functions de-
fined jointly over the input x, the output y and the
latent variable h. w is the parameter of the model.
Given a set of training examples that consist of input
and output pairs {(x1, y1), ...(xn, yn)} ? (X ?Y)n,
the LS-SVM method solves the following optimiza-
tion problem:
minw
1
2
?w?2
+C
n?
i=1
max
(y?,h?)?Y?H
[w ??(xi, y?, h?) + ?(yi, y?)]
?C
n?
i=1
max
h?H
[w ??(xi, yi, h)],
(7)
where ?(yi, y?) is the loss function for the ith ex-
ample. The details of the derivation is omitted in
this paper. Readers who are interested can read more
from (Yu and Joachims, 2009).
There are two maximization problems that are es-
sential in Equation 7. The first one is the loss aug-
mented decision function:
max
(y?,h?)?Y?H
[w ??(xi, y?, h?) + ?(yi, y?)], (8)
and the second is the inference of latent variable
given the label of the training data:
max
h?H
[w ??(xi, yi, h)]. (9)
The Latent Structural SVM framework does not
specify how the maximization problems in Equation
8 and Equation 9 are solved, as well as the infer-
ence problem in 6. These maximization problems
are task dependent. Being able to efficiently solve
them is the key to successfully applying the Latent
Structural SVM method. We will show in detail how
we solve these maximization problems to make LS-
SVM work for query spelling correction in the fol-
lowing subsection.
For training the LS-SVM model, a Concave-
Convex Procedure (CCCP) was proposed to solve
this optimization problem (Yu and Joachims, 2009).
The method resembles the Expect-Maximization
(EM) training method as it updates the model by it-
eratively recomputing the latent variable. However,
rather than performing ?sum-product? training as in
EM where a distribution over the hidden variable is
maintained, the CCCP method used for LS-SVM is
more similar to the ?max-product? paradigm where
we ?guess? the best hidden variable in each iteration,
except here we ?guess? by minimizing a regularized
loss function instead of maximizing the likelihood.
3.3 Solving the Inference Problems
The essential inference problem is to find the correc-
tion that maximizes the scoring function according
to the model (i.e., the decision function in Equation
6). For this purpose we design a best first search al-
gorithm similar to the standard search algorithm in
the noisy channel model. The essence of the search
algorithm is to bound the score of each candidate
so that we could evaluate the most promising candi-
dates first. The algorithm is given in Algorithm 1.
Essentially, the algorithm maintains a priority
queue of all search paths. Each time the best path is
de-queued, it is expanded with up to m ? 1 words
in q by searching over a vocabulary trie of up to
m-gram. Each path is represented as a quadruple
(pos, str, sc, a), representing the current term posi-
tion in query, the string of the path, the path?s score
and the alignment so far. The priority queue is sorted
according to the score of each path in descending or-
der. The GetSuggestions() function retrieves the
top n similar words to the given word with a vocab-
ulary trie according to an error model.
Splitting errors are dealt with in Algorithm 1 by
?looking forward? m words in the query when gen-
erating candidate words. Merging errors are ac-
counted for by including up to m-gram in the vocab-
1515
ulary trie. It is worth mentioning that performance
of Algorithm 1 could be further improved by com-
puting heuristic scores for each path.
Algorithm 1: Best First Search Algorithm
Input: Vocabulary Trie V , query q, output size k,
max order m, candidate pool size n
Output: List l of top k corrections for q
1 Initialize List l;
2 Initialize PriorityQueue pq;
3 Enqueue to pq a start path with position set to 0,
string set to empty string, score set to w ??0, and
path alignment set to empty set;
4 while pq is not Empty do
5 Path pi ? pq.Dequeue();
6 if pi.pos < q.terms.length then
7 for i? 0 tom do
8 ph? q.terms[pi.pos+ 1...pi.pos+ i];
9 sug ? GetSuggestions(ph, V, n);
10 foreach s in sug do
11 pos? ? pi.pos+ i;
12 str? ? concat(pi.str, s.str);
13 a? ? pi.a ? s.a;
14 sc? ? pi.sc+w ??1(qs.a, cs.a, s.a);
15 Enqueue pq with the new path
(pos?, str?, sc?, a?);
16 else
17 Add suggestion string pi.str to l;
18 if l.Count > k then return l;
19 return l;
As Algorithm 1 originates from the noisy channel
model, the two known features that work with the
algorithm are log p(c) and log p(q|c) from the noisy
channel model. However, it is unknown whether
other features can work with the search algorithm
and how we can develop new features to ensure it.
After analyzing the properties of the features and the
search algorithm, we find that a feature ? has to sat-
isfy the following monotonicity constraint in order
to be used in Algorithm 1.
Monotonicity Property. Given query q, for
any alignment At = At?1 ? {at} at time t,
?(qAt , cAt , At) ? ?(qAt?1 , cAt?1 , At?1), where
qAt is the concatenation of qa0 to qat and cAt is the
concatenation of ca0 to cat .
That is, the value of the feature (which is com-
puted in an accumulative manner) cannot increase
as the candidate is extended with a new term at
any search step. This ensures that the score of the
best candidate at any search step is guaranteed to be
higher than the score of any future candidates. It
also implies ?t(qat , cat , at) ? 0 for any t ? T . The
monotonicity feature ensures the correctness of Al-
gorithm 1. We show how we design features with
the guidance of the monotonicity constraint in Sec-
tion 4.
The solution to to the loss augmented inference
depends on the loss function we use. In spelling cor-
rection, usually only one correction is valid for an
input query. Therefore, we apply the 0-1 loss to our
model:
?(c, c?) =
{
0 c = c?
1 c 6= c?
(10)
Given this loss function, the loss augmented infer-
ence problem can be solved easily with an algorithm
similar to Algorithm 1. This is done by initializing
the loss to be 1 at the beginning of each search path.
During the search procedure, we check if the loss
decreases to 0 given the correction string so far. If
this is the case, we decreases the score by 1 and add
the path back to the priority queue. More advanced
functions may also be used (Dreyer et al 2006),
which may lead to better training performance. We
plan to further study different loss functions in our
future work.
The inference of the latent alignment variable can
be solved with dynamic programming, as the num-
ber of possible alignments is limited given the query
and the correction.
4 Features
In the following discussions, we will describe how
the features in our discriminative model are devel-
oped under the guidance of the monotonicity con-
straint.
4.1 Source Probability and Transformation
Probability
We know from empirical experience that the source
probability and the transformation probability are
the two most important features in query spelling
correction. We include them in our model in a nor-
malized form. Taking the source probability for ex-
ample, we define the following feature:
1516
?(q, c, a) = ?+
?|a|
1 log p(c)
?
= 1 +
?|a|
1
log p(c)
? ,
(11)
where ? is a normalizing factor computed as:
? = ?|q| log pmin, (12)
where pmin is the smallest probability we use in
practice.
The formula fits the general form we define in 5
in that ?0 = 1 and ?1(qat , cat , at) =
log p(c)
? for any
t = 1 to |a|.
Similarly, we have the follow feature for the trans-
formation probability:
??(q, c, a) = ?+
?|a|
1 log p(q|c)
?
= 1 +
?|a|
1
log p(q|c)
? .
(13)
We use the web Microsoft n-gram model1 to com-
pute source model p(c). We train the unigram trans-
formation model for the transformation probability
p(q|c) according to (Duan and Hsu, 2011).
In generative models, we treat transformation
probabilities from merging and splitting errors in the
same way as single word errors. In our discrimi-
native model we can assign separate weight to the
transformation probabilities resulted from different
types of errors. This allows fine tuning of the query
spelling correction system, making it more adaptive
to environments where the ratio of different types of
errors may vary. Moreover, the model also allows
us to include language models trained over different
resources, such as query log, title of webpages or
anchor texts.
4.2 Local Heuristic Features
Despite the goal of query spelling correction is to
deal with misspellings, in real world most queries
are correctly spelled. A good query spelling correc-
tion system shall prevent as much as possible from
misjudging an correctly spelled query as misspelled.
With this idea in mind, we invent some heuristic
functions to avoid misjudging.
1http://research.microsoft.com/en-
us/collaboration/focus/cs/web-ngram.aspx
Local Heuristic 1. When a query term is matched
against trustable vocabulary, it increases the chance
that the term is already in its correct form. For ex-
ample, we extract a reliable vocabulary from the title
field of Wikipedia2. We therefore design the follow-
ing feature:
?(q, c, a) = 1 +
|a|?
t=1
?1(qat , cat , at), (14)
where ?1(qat , cat , at) is defined as:
?1(qat , cat , at) =
?
?
?
0 qat /? W
0 qat ? W, qat = ct
? 1|q| qat ? W, qat 6= cat
(15)
where W is the vocabulary of Wikipedia titles.
Since |q| > |a| always holds, the feature is normal-
ized between 0 and 1.
Local Heuristic 2. Another heuristic is that
words with numbers in it, despite usually not in-
cluded in any vocabulary, should be treated care-
fully as they tend to be correct words. Such words
could be a model, a serial number or a special en-
tity name. Since the number keys on keyboard are
away from the letter keys, they are more likely to be
intentionally typed in if found in user queries. Simi-
lar to Heuristic 1, we design the following feature to
capture this heuristic:
??(q, c, a) = 1 +
|a|?
t=1
??1(qat , cat , at), (16)
where ??1(qat , cat , at) is defined as:
??1(qat , cat , aat) =
?
?
?
0 [0...9] /? qat
0 [0...9] ? qat , qat = cat
? 1|q| [0...9] ? qat , qat 6= cat
(17)
4.3 Global Heuristic Features
Some global heuristics are also important in query
spelling correction. For instance, the total number
2http://www.wikipedia.org
1517
of words being corrected in the query may be an
indicator of whether the system has leaned towards
overcorrecting. To account for this global heuristic,
we design the following feature:
?(q, c, a) =
{
1 wc(q, c, a) < wcmax
0 otherwise
(18)
where wc(q, c, a) is the number of word changes
at step t, wcmax is the maximum number of word
changes we allow in our system (in a soft way). Sim-
ilarly, other thresholded features can be designed
such as the number of total edit operations. The use
of global features is similar to the use of loss func-
tion in the search algorithm.
5 Experiments
In order to test the effectiveness and efficiency of our
proposed discriminative training method, in this sec-
tion we conduct extensive experiments on two web
query spelling datasets. Below we first present the
dataset and evaluation metrics, followed by the ex-
periment results on query spelling correction.
5.1 Dataset Preparation
The experiments are conducted on two query
spelling correction datasets. One is the TREC
dataset based on the publicly available TREC
queries (2008 Million Query Track). This dataset
contains 5892 queries and the corresponding correc-
tions annotated by the MSR Speller Challenge 3 or-
ganizers. There could be more than one plausible
corrections for a query. In this dataset only 5.3% of
queries are judged as misspelled.
We have also annotated another dataset that con-
tains 4926 MSN queries, where for each query there
is at most one correction. Three experts are involved
in the annotation process. For each query, we con-
sult the speller from two major search engines (i.e.
Google and Bing). If they agree on the returned
results (including the case if the query is just un-
changed), we take it as the corrected form of the in-
put query. If the results are not the same from the
two, as least one human expert will manually anno-
tate the most likely corrected form of the query. Fi-
nally, about 13% of queries are judged as misspelled
3http://web-ngram.research.microsoft.com/spellerchallenge/
in this dataset, which is close to the error rate of real
web queries. We?ve made this dataset publicly avail-
able to all researchers4.
Both the two datasets are split randomly into two
equal subsets for training and testing.
5.2 Evaluation Metrics
We evaluate our system based on the evaluation met-
rics proposed in Microsoft Speller Challenge, in-
cluding expected precision, expected recall and ex-
pected F1 measure.
Let q be a user query and C(q) = (c1, c2, , ck)
be the set of system output with posterior probabil-
ities P (ci|q). Let S(q) denote the set of plausible
spelling variations annotated by the human experts
for q. Expected Precision is computed as:
Precision =
1
|Q|
?
q?Q
?
c?C(q)
Ip(c, q)P (c|q), (19)
where Ip(c, q) = 1 if c ? S(q), and 0 otherwise.
And expected recall is defined as:
Recall =
1
|Q|
?
q?Q
?
a?S(q)
Ir(C(q), a)/|S(q)|, (20)
where Ir(C(q), a) = 1 if a ? C(q) for a ? S(q),
and 0 otherwise. We use R@N to denote recall for
systems limited to output top N corrections.
Expected F1 measure can be computed as:
F1 =
2 ? precision ? recall
precision+ recall
(21)
5.3 Experiment Results
Table 1 compares the performance of our LS-SVM
based model with two strong baseline systems. The
first baseline system is an Echo system which sim-
ply echos the input. The echo system is usually con-
sidered as a strong baseline in query spelling cor-
rection as the majority of the queries are correctly
spelled queries. The second baseline Lueck-2011
we use is a award winning speller system5 (Luec,
2011), which was ranked at the first place in Mi-
crosoft Spelling Challenge 2011.
4http://times.cs.uiuc.edu/duan9/msn speller.tar.gz
5http://www.phraselink.com
1518
Table 1: LSSVM vs Baselines Serving as Standalone Speller
All Queries Misspelled Queries
Dataset Method Precision R@10 F1 Precision R@10 F1
Echo 0.949 0.876 0.911 0 0 0
TREC Lueck-2011 0.963 0.932 0.947 0.391 0.479 0.430
LS-SVM 0.955 0.944 0.949 0.331 0.678? 0.445?
Echo 0.869 0.869 0.869 0 0 0
MSN Lueck-2011 0.896 0.921 0.908 0.334 0.397 0.363
LS-SVM 0.903 0.953 0.928 0.353? 0.662? 0.461?
We show performances for the entire query sets
as well as the query sets consisting only the mis-
spelled queries. As we can see, our system out-
performs both baseline systems on almost all met-
rics, except the precision of Lueck-2011 is better
than ours on TREC dataset. We perform statistical
test and measures where our system shows statisti-
cal significant improvement over both baseline sys-
tems are noted by ?. It is theoretically impossible
to achieve statistical significance in the entire query
set as majority queries have almost identical perfor-
mance in different systems due to the large amount
of correct queries. But our method shows signifi-
cant improvement in the dealing with the misspelled
queries. This experiment verified the effectiveness
of our proposed discriminative model. As a stan-
dalone speller, our system achieves very high accu-
racy.
Despite we are primarily focused on optimizing
the top correction in our discriminative model, we
can also use the trained system to output multiple
candidate corrections. Table 2 compare our system
with the noisy channel model (N-C) in terms of re-
call at different levels of cutoff. For all levels, we see
that our system achieves higher recall than the noisy
channel model. This indicates that when used to-
gether with a secondary ranker, our system serves as
a better filtering method than the unoptimized noisy
channel model. Since the ranker makes use of arbi-
trary features, it has the potential of further improv-
ing the accuracy of query spelling correction. We
plan to further explore this idea as a future work.
In Table 3 we study the effect of treating the trans-
formation probability of merging and splitting er-
rors as separate features and including the local and
global heuristic features (rich features). We see that
Table 2: LS-SVM vs Noisy Channel Model Serving as
Filtering Method
Dataset Method R@5 R@10 R@20
TREC N-C 0.896 0.899 0.901
LS-SVM 0.923 0.944 0.955
MSN N-C 0.870 0.873 0.876
LS-SVM 0.950 0.953 0.960
the precision of query spelling correction can bene-
fits from the use of rich features. However, it does
not result in much improvement in recall. This is
reasonable as the additional features are primarily
designed to improve the accuracy of the top correc-
tion generated by the system. In doing so, it actu-
ally regularizes the ability of the system in retrieving
diversified results. For instance, the global heuris-
tic feature on the number of word change tries to
prevent the system from returning candidates hav-
ing more than a certain number of changed words.
For the TREC collection where more than one cor-
rections can be labeled for a query, this phenomena
is aggravated.
Table 3: LSSVM w/ and w/o Rich Features
Dataset Method Precision R@10 F1
TREC w/o 0.942 0.946 0.944
w/ 0.955 0.944 0.949
MSN w/o 0.898 0.952 0.924
w/ 0.903 0.953 0.928
6 Conclusions
In this paper, we present a novel discriminative
model for query spelling correction. The paper made
the following contributions:
1519
First, to the best of our knowledge, this is a novel
exploration of directly optimizing the search phase
in query spelling correction with a discriminative
model. By modeling word alignment as the latent
structural information, our formulation also deals
with word boundary errors. We propose to use LS-
SVM for learning the discriminative model which
naturally incorporates search in the learning process.
Second, we develop an efficient search algorithm
that solves the inference problems in the LS-SVM
based model. We analyze the criteria for selecting
and designing features to ensure the correctness and
efficiency of the search algorithm. Third, we explore
effective features to improve the accuracy of the
model. Finally, experiments are conducted to verify
the effectiveness of the proposed model. It is shown
that as a standalone speller our system achieves high
accuracy. When used in a two stage approach, it at-
tains higher recall than the noisy channel model and
can thus serve as a superior method for candidate
generation. We also verify that through the use of
rich features, we can further improve the accuracy
of our query spelling correction system.
7 Acknowledgments
This paper is based upon work supported in part by
MIAS, the Multimodal Information Access and Syn-
thesis center at UIUC, part of CCICADA, a DHS
Center of Excellence, and by the National Science
Foundation under grant CNS-1027965, and by a Mi-
crosoft grant.
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In HLT/EMNLP.
The Association for Computational Linguistics.
M. Bendersky and W. B. Croft. 2008. Discovering key
concepts in verbose queries. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?08. ACM, New York, NY, USA, 491-498.
E. Brill and R. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceed-
ings of the 38th Annual Meeting of the Association for
Computational Linguistics, Hong Kong.
M. Chang, D. Goldwasser, D. Roth and V. Srikumar.
2010. Discriminative Learning over Constrained La-
tent Representations. In Proceedings of NAACL.
Q. Chen, M. Li, and M. Zhou. 2007. Improving
query spelling correction using web search results. In
EMNLP-CoNLL, pages 181?189.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
H. Daume, J. Langford and D. Marcu. 2009. Search-
based Structured Prediction. Machine Learning Jour-
nal (MLJ).
M. Dreyer, D. Smith and N. Smith. 2006. Vine parsing
and minimum risk reranking for speed and precision.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning. 201-205.
H. Duan and B.-J. P. Hsu. 2011. Online spelling correc-
tion for query completion. In Proceedings of the 20th
international conference on World wide web, WWW
?11, pages 117?126, New York, NY, USA.
C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011.
Unsupervised Word Alignment with Arbitrary Fea-
tures. In Proceedings of ACL.
D. Freitag, S. Khadivi. 2007. A Sequence Alignment
Model Based on the Averaged Perceptron. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning. 238-247.
J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In COLING, pages 358?366.
A. R. Golding and D. Roth 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. In
Machine Learning, vol 34, pages 107?130.
J. Guo, G. Xu, H. Li, and X. Cheng. 2008. A unified and
discriminative model for query refinement. In Pro-
ceedings of the 31st annual international ACM SIGIR,
SIGIR ?08, pages 379?386, New York, NY, USA.
C. John Yu and T. Joachims. 2009. Learning structural
SVMs with latent variables. In Proceedings of the 26th
Annual International Conference on Machine Learn-
ing (ICML ?09). ACM, New York, NY, USA, 1169-
1176.
M. D. Kernighan , K. W. Church , W. A. Gale. 1990. A
spelling correction program based on a noisy channel
model. In Proceedings of the 13th conference on Com-
putational linguistics. 205-210. August 20-25, 1990,
Helsinki, Finland.
K. Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM computing surveys, 24(4).
G. Kumaran and J. Allan. 2008. Effective and efficient
user interaction for long queries. In Proceedings of
the 31st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?08. ACM, New York, NY, USA.
1520
G. Kumaran and V. R. Carvalho. 2009. Reducing long
queries using query quality predictors. In Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?09. ACM, New York, NY, USA, 564-571.
J. Lafferty. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML ?01). 282?289.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, 10(8), 707-710.
G. Luec. 2011. A data-driven approach for correcting
search quaries. In Spelling Alteration for Web Search
Workshop.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum Entropy Markov Models for Information Extrac-
tion and Segmentation. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML ?00). 591-598.
M. Mitra, A. Singhal, and C. Buckley. 1998. Improving
automatic query expansion. In Proceedings of the 21st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?98.
Y. Qiu and H. Frei. 1993. Concept based query expan-
sion. In Proceedings of the 16th annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?93. ACM, New York,
NY, USA, 160-169.
X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning
phrase-based spelling error models from clickthrough
data. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 266?274, Stroudsburg, PA, USA.
X. Wang, C. Zhai. 2008. Mining Term Association Pat-
terns from Search Logs for Effective Query Reformu-
lation. In Proceedings of the 17th ACM International
Conference on Information and Knowledge Manage-
ment 2008, CIKM?08. 479-488.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings of
the 19th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?96. ACM, New York, NY.
A. Yessenalina, Y. Yue, C. Cardie. 2010. Multi-
level Structured Models for Document-level Sentiment
Classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP ?10). 10461056.
1521
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1128?1137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Lingual Latent Topic Extraction
Duo Zhang
University of Illinois at
Urbana-Champaign
dzhang22@cs.uiuc.edu
Qiaozhu Mei
University of Michigan
qmei@umich.edu
ChengXiang Zhai
University of Illinois at
Urbana-Champaign
czhai@cs.uiuc.edu
Abstract
Probabilistic latent topic models have re-
cently enjoyed much success in extracting
and analyzing latent topics in text in an un-
supervised way. One common deficiency
of existing topic models, though, is that
they would not work well for extracting
cross-lingual latent topics simply because
words in different languages generally do
not co-occur with each other. In this paper,
we propose a way to incorporate a bilin-
gual dictionary into a probabilistic topic
model so that we can apply topic models to
extract shared latent topics in text data of
different languages. Specifically, we pro-
pose a new topic model called Probabilis-
tic Cross-Lingual Latent Semantic Anal-
ysis (PCLSA) which extends the Proba-
bilistic Latent Semantic Analysis (PLSA)
model by regularizing its likelihood func-
tion with soft constraints defined based on
a bilingual dictionary. Both qualitative and
quantitative experimental results show that
the PCLSA model can effectively extract
cross-lingual latent topics from multilin-
gual text data.
1 Introduction
As a robust unsupervised way to perform shallow
latent semantic analysis of topics in text, prob-
abilistic topic models (Hofmann, 1999a; Blei et
al., 2003b) have recently attracted much atten-
tion. The common idea behind these models is the
following. A topic is represented by a multino-
mial word distribution so that words characteriz-
ing a topic generally have higher probabilities than
other words. We can then hypothesize the exis-
tence of multiple topics in text and define a gener-
ative model based on the hypothesized topics. By
fitting the model to text data, we can obtain an es-
timate of all the word distributions corresponding
to the latent topics as well as the topic distributions
in text. Intuitively, the learned word distributions
capture clusters of words that co-occur with each
other probabilistically.
Although many topic models have been pro-
posed and shown to be useful (see Section 2 for
more detailed discussion of related work), most
of them share a common deficiency: they are de-
signed to work only for mono-lingual text data and
would not work well for extracting cross-lingual
latent topics, i.e. topics shared in text data in
two different natural languages. The deficiency
comes from the fact that all these models rely on
co-occurrences of words forming a topical cluster,
but words in different language generally do not
co-occur with each other. Thus with the existing
models, we can only extract topics from text in
each language, but cannot extract common topics
shared in multiple languages.
In this paper, we propose a novel topic model,
called Probabilistic Cross-Lingual Latent Seman-
tic Analysis (PCLSA) model, which can be used to
mine shared latent topics from unaligned text data
in different languages. PCLSA extends the Proba-
bilistic Latent Semantic Analysis (PLSA) model
by regularizing its likelihood function with soft
constraints defined based on a bilingual dictio-
nary. The dictionary-based constraints are key to
bridge the gap of different languages and would
force the captured co-occurrences of words in
each language by PCLSA to be ?synchronized?
so that related words in the two languages would
have similar probabilities. PCLSA can be esti-
mated efficiently using the General Expectation-
Maximization (GEM) algorithm. As a topic ex-
traction algorithm, PCLSA would take a pair of
unaligned document sets in different languages
and a bilingual dictionary as input, and output a
set of aligned word distributions in both languages
that can characterize the shared topics in the two
languages. In addition, it also outputs a topic cov-
1128
erage distribution for each language to indicate the
relative coverage of different shared topics in each
language.
To the best of our knowledge, no previous work
has attempted to solve this topic extraction prob-
lem and generate the same output. The closest
existing work to ours is the MuTo model pro-
posed in (Boyd-Graber and Blei, 2009) and the
JointLDA model published recently in (Jagarala-
mudi and Daume? III, 2010). Both used a bilingual
dictionary to bridge the language gap in a topic
model. However, the goals of their work are dif-
ferent from ours in that their models mainly focus
on mining cross-lingual topics of matching word
pairs and discovering the correspondence at the
vocabulary level. Therefore, the topics extracted
using their model cannot indicate how a common
topic is covered differently in the two languages,
because the words in each word pair share the
same probability in a common topic. Our work fo-
cuses on discovering correspondence at the topic
level. In our model, since we only add a soft con-
straint on word pairs in the dictionary, their prob-
abilities in common topics are generally different,
naturally capturing which shows the different vari-
ations of a common topic in different languages.
We use a cross-lingual news data set and a re-
view data set to evaluate PCLSA. We also propose
a ?cross-collection? likelihood measure to quanti-
tatively evaluate the quality of mined topics. Ex-
perimental results show that the PCLSA model
can effectively extract cross-lingual latent topics
from multilingual text data, and it outperforms a
baseline approach using the standard PLSA on text
data in each language.
2 Related Work
Many topic models have been proposed, and the
two basic models are the Probabilistic Latent Se-
mantic Analysis (PLSA) model (Hofmann, 1999a)
and the Latent Dirichlet Allocation (LDA) model
(Blei et al, 2003b). They and their extensions
have been successfully applied to many prob-
lems, including hierarchical topic extraction (Hof-
mann, 1999b; Blei et al, 2003a; Li and McCal-
lum, 2006), author-topic modeling (Steyvers et al,
2004), contextual topic analysis (Mei and Zhai,
2006), dynamic and correlated topic models (Blei
and Lafferty, 2005; Blei and Lafferty, 2006), and
opinion analysis (Mei et al, 2007; Branavan et al,
2008). Our work is an extension of PLSA by in-
corporating the knowledge of a bilingual dictio-
nary as soft constraints. Such an extension is sim-
ilar to the extension of PLSA for incorporating so-
cial network analysis (Mei et al, 2008a) but our
constraint is different.
Some previous work on multilingual topic mod-
els assume documents in multiple languages are
aligned either at the document level, sentence level
or by time stamps (Mimno et al, 2009; Zhao and
Xing, 2006; Kim and Khudanpur, 2004; Ni et al,
2009; Wang et al, 2007). However, in many ap-
plications, we need to mine topics from unaligned
text corpus. For example, mining topics from
search results in different languages can facilitate
summarization of multilingual search results.
Besides all the multilingual topic modeling
work discussed above, comparable corpora have
also been studied extensively (e.g. (Fung, 1995;
Franz et al, 1998; Masuichi et al, 2000; Sadat
et al, 2003; Gliozzo and Strapparava, 2006)), but
most previous work aims at acquiring word trans-
lation knowledge or cross-lingual text categoriza-
tion from comparable corpora. Our work differs
from this line of previous work in that our goal is
to discover shared latent topics from multi-lingual
text data that are weakly comparable (e.g. the data
does not have to be aligned by time).
3 Problem Formulation
In general, the problem of cross-lingual topic ex-
traction can be defined as to extract a set of com-
mon cross-lingual latent topics covered in text col-
lections in different natural languages. A cross-
lingual latent topic will be represented as a multi-
nomial word distribution over the words in all
the languages, i.e. a multilingual word distri-
bution. For example, given two collections of
news articles in English and Chinese, respectively,
we would like to extract common topics simul-
taneously from the two collections. A discov-
ered common topic, such as the terrorist attack
on September 11, 2001, would be characterized
by a word distribution that would assign relatively
high probabilities to words related to this event in
both English and Chinese (e.g. ?terror?, ?attack?,
?afghanistan?, ?taliban?, and their translations in
Chinese).
As a computational problem, our input is a
multi-lingual text corpus, and output is a set of
cross-lingual latent topics. We now define this
problem more formally.
1129
Definition 1 (Multi-Lingual Corpus) A multi-
lingual corpus C is a set of text collections
{C1, C2, . . . , Cs}, where Ci = {di1, di2, . . . , diMi}
is a collection of documents in language Li with
vocabulary Vi = {wi1, wi2, . . . , wiNi}. Here, Mi is
the total number of documents in Ci, Ni is the to-
tal number of words in Vi, and dij is a document in
collection Ci.
Following the common assumption of bag-of-
words representation, we represent document dij
with a bag of words {wij1 , w
i
j2 , . . . , w
i
jd}, and use
c(wik, dij) to denote the count of word wik in docu-
ment dij .
Definition 2 (Cross-Lingual Topic): A cross-
lingual topic ? is a semantically coherent multi-
nomial distribution over all the words in the vo-
cabularies of languages L1, ..., Ls. That is, p(w|?)
would give the probability of a word w which can
be in any of the s languages under consideration. ?
is semantically coherent if it assigns high probabil-
ities to words that are semantically related either in
the same language or across different languages.
Clearly, we have
?s
i=1
?
w?Vi p(w|?) = 1 for any
cross-lingual topic ?.
Definition 3 (Cross-Lingual Topic Extrac-
tion) Given a multi-lingual corpus C, the task of
cross-lingual topic extraction is to model and ex-
tract k major cross-lingual topics {?1, ?2, . . . , ?k}
from C, where ?i is a cross-lingual topic, and k is
a user specified parameter.
The extracted cross-lingual topics can be di-
rectly used as a summary of the common con-
tent of the multi-lingual data set. Note that once
a cross-lingual topic is extracted, we can eas-
ily obtain its representation in each language Li
by ?splitting? the cross-lingual topic into multi-
ple word distributions in different languages. For-
mally, the word distribution of a cross-lingual
topic ? in language Li is given by pi(wi|?) =
p(wi|?)
?
w?Vi
p(w|?) .
These aligned language-specific word distribu-
tions can directly review the variations of topics
in different languages. They can also be used to
analyze the difference of the coverage of the same
topic in different languages. Moreover, they are
also useful for retrieving relevant articles or pas-
sages in each language and aligning them to the
same common topic, thus essentially also allow-
ing us to integrate and align articles in multiple
languages.
4 Probabilistic Cross-Lingual Latent
Semantic Analysis
In this section, we present our probabilistic cross-
lingual latent semantic analysis (PCLSA) model
and discuss how it can be used to extract cross-
lingual topics from multi-lingual text data.
The main reason why existing topic models
can?t be used for cross-lingual topic extraction is
because they cannot cross the language barrier.
Intuitively, in order to cross the language barrier
and extract a common topic shared in articles in
different languages, we must rely on some kind
of linguistic knowledge. Our PCLSA model as-
sumes the availability of bi-lingual dictionaries for
at least some language pairs, which are generally
available for major language pairs. Specifically,
for text data in languages L1, ..., Ls, if we rep-
resent each language as a node in a graph and
connect those language pairs for which we have a
bilingual dictionary, the minimum requirement is
that the whole graph is connected. Thus, as a min-
imum, we will need s? 1 distinct bilingual dictio-
naries. This is so that we can potentially cross all
the language barriers.
Our key idea is to ?synchronize? the extraction
of monolingual ?component topics? of a cross-
lingual topic from individual languages by forcing
a cross-lingual topic word distribution to assign
similar probabilities to words that are potential
translations according to a Li-Lj bilingual dictio-
nary. We achieve this by adding such preferences
formally to the likelihood function of a probabilis-
tic topic model as ?soft constraints? so that when
we estimate the model, we would try to not only
fit the text data well (which is necessary to extract
coherent component topics from each language),
but also satisfy our specified preferences (which
would ensure the extracted component topics in
different languages are semantically related). Be-
low we present how we implement this idea in
more detail.
A bilingual dictionary for languages Li and Lj
generally would give us a many-to-many map-
ping between the vocabularies of the two lan-
guages. With such a mapping, we can construct
a bipartite graph Gij = (Vij , Eij) between the
two languages where if one word can be poten-
tially translated into another word, the two words
would be connected with an edge. An edge can
be weighted based on the probability of the cor-
responding translation. An example graph for
1130
Chinese-English dictionary is shown in Figure 1.
Figure 1: A Dictionary based Word Graph
With multiple bilingual dictionaries, we can
merge the graphs to generate a multi-partite graph
G = (V,E). Based on this graph, the PCLSA
model extends the standard PLSA by adding a
constraint to the likelihood function to ?smooth?
the word distributions of topics in PLSA on the
multi-partite graph so that we would encourage the
words that are connected in the graph (i.e. pos-
sible translations of each other) to be given simi-
lar probabilities by every cross-lingual topic. Thus
when a cross-lingual topic picks up words that co-
occur in mono-lingual text, it would prefer pick-
ing up word pairs whose translations in other lan-
guages also co-occur with each other, giving us a
coherent multilingual word distribution that char-
acterizes well the content of text in different lan-
guages.
Specifically, let ? = {?j} (j = 1, ..., k) be a set
of k cross-lingual topic models to be discovered
from a multilingual text data set with s languages
such that p(w|?i) is the probability of word w ac-
cording to the topic model ?i.
If we are to use the regular PLSA to model our
data, we would have the following log-likelihood
and we usually use a maximum likelihood estima-
tor to estimate parameters and discover topics.
L(C) =
s
?
i=1
?
d?Ci
?
w
c(w, d) log
k
?
j=1
p(?j |d)p(w|?j)
Our main extension is to add to L(C) a cross-
lingual constraint term R(C) to incorporate the
knowledge of bilingual dictionaries. R(C) is de-
fined as
R(C) = 12
?
?u,v??E
w(u, v)
k
?
j=1
(p(wu|?j)Deg(u) ?
p(wv|?j)
Deg(v) )
2
where w(u, v) is the weight on the edge between
u and v in the multi-partite graph G = (V,E),
which in our experiments is set to 1, and Deg(u)
is the degree of word u, i.e. the sum of the weights
of all the edges ending with u.
Intuitively, R(C) measures the difference be-
tween p(wu|?j) and p(wv|?j) for each pair (u, v)
in a bilingual dictionary; the more they differ, the
larger R(C) would be. So it can be regarded as
a ?loss function? to help us assess how well the
?component word distributions? in multiple lan-
guages are correlated semantically. Clearly, we
would like the extracted topics to have a small
R(C). We choose this specific form of loss func-
tion because it would make it convenient to solve
the optimization problem of maximizing the cor-
responding regularized maximum likelihood (Mei
et al, 2008b). The normalization with Deg(u)
and Deg(v) can be regarded as a way to compen-
sate for the potential ambiguity of u and v in their
translations.
Putting L(C) and R(C) together, we would
like to maximize the following objective function
which is a regularized log-likelihood:
O(C, G) = (1 ? ?)L(C)? ?R(C) (1)
where ? ? (0, 1) is a parameter to balance the
likelihood and the regularizer. When ? = 0, we
recover the standard PLSA.
Specifically, we will search for a set of values
for all our parameters that can maximize the ob-
jective function defined above. Our parameters
include all the cross-lingual topics and the cov-
erage distributions of the topics in all documents,
which we denote by ? = {p(w|?j), p(?j |d)}d,w,j
where j = 1, ..., k, w varies over the entire vo-
cabularies of all the languages , d varies over
all the documents in our collection. This opti-
mization problem can be solved using a General-
ized Expectation-Maximization (GEM) algorithm
as described in (Mei et al, 2008a).
Specifically, in the E-step of the algorithm, the
distribution of hidden variables is computed using
Eq. 2.
z(w, d, j) = p(?j |d)p(w|?j)?
j? p(?j? |d)p(w|?j?)
(2)
Then in the M-step, we need to maximize the
complete data likelihood Q(?;?n):
Q(?;?n) = (1? ?)L?(C)? ?R(C)
1131
where
L?(C) =
?
d
?
w
c(w, d)
?
j
z(w, d, j) log p(?j |d)p(w|?j), (3)
with the constraints that
?
j p(?j |d) = 1 and
?
w p(w|?j) = 1.
There is a closed form solution if we only want
to maximize the L?(C) part:
p(n+1)(?j |d) =
?
w c(w, d)z(w, d, j)
?
w
?
j? c(w, d)z(w, d, j?)
p(n+1)(w|?j) =
?
d c(w, d)z(w, d, j)
?
d
??
w c(w?, d)z(w?, d, j)
(4)
However, there is no closed form solution in the
M-step for the whole objective function. Fortu-
nately, according to GEM we do not need to find
the local maximum of Q(?;?n) in every M-step,
and we only need to find a new value ?n+1 to im-
prove the complete data likelihood, i.e. to make
sure Q(?n+1; ?n) ? Q(?n; ?n). So our method
is to first maximize the L?(C) part using Eq. 4 and
then use Eq. 5 to gradually increase the R(C) part.
p(t+1)(wu|?j) = (1? ?)p(t)(wu|?j) (5)
+ ?
?
?u,v??E
w(u, v)
Deg(v)
p(t)(wv|?j)
Here, parameter ? is the length of each smooth-
ing step. Obviously, after each smoothing step,
the sum of the probabilities of all the words in one
topic is still equal to 1. We smooth the parameters
until we cannot get a better parameter set ?n+1.
Then, we continue to the next E-step. If there is
no ?n+1 s.t. Q(?n+1; ?n) ? Q(?n; ?n), then
we consider ?n to be the local maximum point of
the objective function Eq. 1.
5 Experiment Design
5.1 Data Set
The data set we used in our experiment is collected
from news articles of Xinhua English and Chi-
nese newswires. The whole data set is quite big,
containing around 40,000 articles in Chinese and
35,000 articles in English. For different purpose of
our experiments, we randomly selected different
number of documents from the whole corpus, and
we will describe the concrete statistics in each ex-
periment. To process the Chinese corpus, we use
a simple segmenter1 to split the data into Chinese
phrases. Both Chinese and English stopwords are
removed from our data.
The dictionary file we used for our PCLSA
model is from mandarintools.com2. For each Chi-
nese phrase, if it has several English meanings, we
add an edge between it and each of its English
translation. If one English translation is an En-
glish phrase, we add an edge between the Chinese
phrase and each English word in the phrase.
5.2 Baseline Method
As a baseline method, we can apply the standard
PLSA (Hofmann, 1999a) directly to the multi-
lingual corpus. Since PLSA takes advantage of
the word co-occurrences in the document level to
find semantic topics, directly using it for a multi-
lingual corpus will result in finding topics mainly
reflecting a single language (because words in dif-
ferent languages would not co-occur in the same
document in general). That is, the discovered top-
ics are mostly monolingual. These monolingual
topics can then be aligned based on a bilingual dic-
tionary to suggest a possible cross-lingual topic.
6 Experimental Results
6.1 Qualitative Comparison
To qualitatively compare PCLSAwith the baseline
method, we compare the word distributions of top-
ics extracted by them. The data set we used in this
experiment is selected from the Xinhua News data
during the period from Jun. 8th, 2001 to Jun. 15th,
2001. There are totally 1799 English articles and
1485 Chinese articles in the data set. The num-
ber of topics to be extracted is set to 10 for both
methods.
Table 1 shows the experimental results. To
make it easier to understand, we add an English
translation to each Chinese phrase in our results.
The first ten rows show sample topics of the mod-
eling results of traditional PLSA model. We can
see that it only contains mono-language topics,
i.e. the topics are either in Chinese or in En-
glish. The next ten rows are the results from
our PCLSA model. Compared with the base-
line method, PCLSA can not only find coherent
topics from the cross-lingual corpus, but it can
also show the content about one topic from both
two language corpora. For example, in ?Topic 2?
1http://www.mandarintools.com/segmenter.html
2http://www.mandarintools.com/cedict.html
1132
Table 2: Synthetic Data Set from Xinhua News
English Shrine Olympic Championship
90 101 70
Chinese CPC Anniversary Afghan War Championship
95 206 72
which is about ?Israel? and ?Palestinian?, the Chi-
nese corpus mentions a lot about ?Arafat? who is
the leader of ?Palestinian?, while the English cor-
pus discusses more on topics such as ?cease fire?
and ?women?. Similarly, in ?Topic 9?, the topic
is related to Philippine, the Chinese corpus men-
tions some environmental situation in Philippine,
while the English corpus mentions a lot about
?Abu Sayyaf?.
6.2 Discovering Common Topics
To demonstrate the ability of PCLSA for finding
common topics in cross-lingual corpus, we use
some event names, e.g. ?Shrine? and ?Olympic?,
as queries and randomly select a certain number of
documents from the whole corpus, which are re-
lated to the queries. The number of documents for
each query in the synthetic data set is shown in Ta-
ble 2. In either the English corpus or the Chinese
corpus, we select a smaller number of documents
about topic ?Championship? combined with the
other two topics in the same corpus. In this way,
when we want to extract two topics from either En-
glish or Chinese corpus, the ?Championship? topic
may not be easy to extract, because the other two
topics have more documents in the corpus. How-
ever, when we use PCLSA to extract four topics
from the two corpora together, we expect that the
topic ?Championship? will be found, because now
the sum of English and Chinese documents related
to ?Championship? is larger than other topics. The
experimental result is shown in Table 3. The first
two columns are the two topics extracted from En-
gish corpus, the third and the forth columns are
two topics from Chinese corpus, and the other four
columns are the results from cross-lingual cor-
pus. We can see that in either the Chinese sub-
collection or the English sub-collection, the topic
?Championship? is not extracted as a significant
topic. But, as expected, the topic ?Championship?
is extracted from the cross-lingual corpus, while
the topic ?Olympic? and topic ?Shrine? are merged
together. This demonstrate that PCLSA is capable
of extracting common topics from a cross-lingual
corpus.
6.3 Quantitative Evaluation
We also quantitatively evaluate how well our
PCLSA model can discover common topics
among corpus in different languages. We pro-
pose a ?cross-collection? likelihood measure for
this purpose. The basic idea is: suppose we got
k cross-lingual topics from the whole corpus, then
for each topic, we split the topic into two sepa-
rate set of topics, English topics and Chinese top-
ics, using the splitting formula described before,
i.e. pi(wi|?) = p(w
i|?)
?
w?Vi
p(w|?) . Then, we use the
word distribution of the Chinese topics (translating
the words into English) to fit the English Corpus
and use the word distribution of the English top-
ics (translating the words into Chinese) to fit the
Chinese Corpus. If the topics mined are common
topics in the whole corpus, then such a ?cross-
collection? likelihood should be larger than those
topics which are not commonly shared by the En-
glish and the Chinese corpus. To calculate the
likelihood of fitness, we use the folding-in method
proposed in (Hofmann, 2001). To translate topics
from one language to another, e.g. Chinese to En-
glish, we look up the bilingual dictionary and do
word-to-word translation. If one Chinese word has
several English translations, we simply distribute
its probability mass equally to each English trans-
lation.
For comparison, we use the standard PLSA
model as the baseline. Basically, suppose PLSA
mined k semantic topics in the Chinese corpus and
k semantic topics in the English corpus. Then, we
also use the ?cross-collection? likelihood measure
to see how well those k semantic Chinese topics fit
the English corpus and those k semantic English
topics fit the Chinese corpus.
We totally collect three data sets to compare the
performance. For the first data set, (English 1,
Chinese 1), both the Chinese and English corpus
are chosen from the Xinhua News Data during
the period from 2001.06.08 to 2001.06.15, which
has 1799 English articles and 1485 Chinese ar-
ticles. For the second data set, (English 2, Chi-
nese 2), the Chinese corpus Chinese 2 is the same
as Chinese 1, but the English corpus is chosen
from 2001.06.14 to 2001.06.19 which has 1547
documents. For the third data set, (English 3, Chi-
nese 3), the Chinese corpus is the same as in data
set one, but the English corpus is chosen from
2001.10.02 to 2001.10.07 which contains 1530
documents. In other words, in the first data set,
1133
Table 1: Qualitative Evaluation
Topic 0 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9
j(party) +"(crime) ?C(athlete) ?(palestine) \*(collaboration) s?(education) israel bt dollar china
??j(communist) @(agriculture) 	(champion) ????(palestine) ?0(shanghai) E(ball) palestinian beat percent cooperate
??(revolution) @?(travel) ?)?(championship) 1??(israel) ?(relation) ??(league) eu final million shanghai
j?(party member) Qs(heathendom) ?(base) *?(cease fire) ?)(bilateral) E(soccer) police championship index develop
??(central) ??(public security) ??E(badminton) ?\)(UN) ?4(trade) I?(minute) report play stock beije
?B(ism) w(name) ?(sports) ??(mid east) :(president) ??(team member) secure champion point particulate
?\(cadre) ?(case) ??(final) ??(lebanon) )(country) s(teacher) kill win share matter
??(chairman mao) ?(law enforcement) E(women) j??(macedon) ?P(friendly) ?B?(school) europe olympic close sco
??(chinese communist) =(city) 6?(chess) ?B(conflict) ??(meet) E?(team) egypt game 0 invest
s(leader) ?(penalize) H?(fitness) ??(talk) [?(russia)  (grade A) treaty cup billion project
?)(bilateral) ??(league) israel cooperate ?C(athlete) party eu invest 0 ??(absorb)
\*(collaboration) w(name) 1??(israel) sco particulate j(party) khatami =?(investment) dollar ?
??(talk) E(ball) bt develop 	 communist ireland 7?(billion) percent ?Y?e(abu)
?P(friendly) ??(shenhua) palestinian country athlete revolution ?}(ireland) s?(education) index ?
?(palestine) ??(host) ceasefire president champion ?B(-ism) elect ??(environ. protect.) million (?(particle)country A ?n(arafat) apec ii ?(antiwar) vote ??(money) stock philippine
?\)(UN) ball women shanghai 6?(chess) 3?(comrade) presidential ?B?(school) billion abu
s|(leader) ?y(jinde) jerusalem africa competition ??(revolution) cpc market point ?(base)bilateral ?(season) mideast meet contestant j?(party) iran s(teacher) 7(billion) ?state E?(player) lebanon T?(zemin jiang) v(gymnastics) ideology referendum business share ?(object)
Table 3: Effectiveness of Extracting Common Topics
English 1 English 2 Chinese 1 Chinese 2 Cross 1 Cross 2 Cross 3 Cross 4
japan olympic ??j(CPC) ??F(afghan) koizumi ??(taliban) swim ?|(worker)
shrine ioc ?(championship) ?(taliban) yasukuni /(military) ?(championship) party
visit beije -(world) ??(taliban) ioc city ?y(free style) ??(three)
koizumi game ?.(thought) /(military) japan refugee !y(diving) j.?(marx)
yasukuni july ?X(theory) K?(attack) olympic side ?)?(championship) communist
war bid j.?(marx) ?(US army) beije ?(US army) ???(semi final) marx
august swim ?y(swim) [(laden) shrine q(bomb) competition theory
asia vote ?)?(championship) \?(army) visit 	Y(kabul) ?y(swim) Oj(found party)
criminal championship j(party) q(bomb) ???(olympic) 8?(attack) ?9(record) ??j(CPC)
ii committee Oj(found party) 	Y(kabul) ???.(olympic) 
?(refugee) [??(xuejuan luo) revolution
the English corpus and Chinese corpus are com-
parable with each other, because they cover simi-
lar events during the same period. In the second
data set, the English and Chinese corpora share
some common topics during the overlap period.
The third data is the most tough one since the two
corpora are from different periods. The purpose of
using these three different data sets for evaluation
is to test how well PCLSA can mine common top-
ics from either a data set where the English corpus
and the Chinese corpus are comparable or a data
set where the English corpus and the Chinese cor-
pus rarely share common topics.
The experimental results are shown in Table 4.
Each row shows the ?cross-collection? likelihood
of using the ?cross-collection? topics to fit the data
set named in the first column. For example, in
the first row, the values are the ?cross-collection?
likelihood of using Chinese topics found by differ-
ent methods from the first data set to fit English 1.
The last collum shows howmuch improvement we
got from PCLSA compared with PLSA. From the
results, we can see that in all the data sets, our
PCLSA has higher ?cross-collection? likelihood
value, which means it can find better common top-
ics compared to the baseline method. Notice that
the Chinese corpora are the same in all three data
sets. The results show that both PCLSA and PLSA
get lower ?cross-collection? likelihood for fitting
the Chinese corpora when the data set becomes
?tougher?, i.e. less topic overlapping, but the im-
Table 4: Quantitative Evaluation of Common
Topic Finding (?cross-collection? log-likelihood)
PCLSA PLSA Rel. Imprv.
English 1 -2.86294E+06 -3.03176E+06 5.6%
Chinese 1 -4.69989E+06 -4.85369E+06 3.2%
English 2 -2.48174E+06 -2.60805E+06 4.8%
Chinese 2 -4.73218E+06 -4.88906E+06 3.2%
English 3 -2.44714E+06 -2.60540E+06 6.1%
Chinese 3 -4.79639E+06 -4.94273E+06 3.0%
provement of PCLSA over PLSA does not drop
much. On the other hand, the improvement of
PCLSA over PLSA on the three English corpora
does not show any correlation with the difficulty
of the data set.
6.4 Extracting from Multi-Language Corpus
In the previous experiments, we have shown the
capability and effectiveness of the PCLSA model
in latent topic extraction from two language cor-
pora. In fact, the proposed model is general and
capable of extracting latent topics from multi-
language corpus. For example, if we have dic-
tionaries among multiple languages, we can con-
struct a multi-partite graph based on the corre-
spondence between those vocabularies, and then
smooth the PCLSA model with this graph.
To show the effectiveness of PCLSA in min-
ing multiple language corpus, we first construct a
simulated data set based on 1115 reviews of three
brands of laptops, namely IBM (303), Apple(468)
and DELL(344). To simulate a three language cor-
1134
Table 5: Effectiveness of Latent Topic Extraction from Multi-Language Corpus
Topic 0 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7
cd(apple) battery(dell) mouse(dell) print(apple) port(ibm) laptop(ibm) os(apple) port(dell)port(apple) drive(dell) button(dell) resolution(dell) card(ibm) t20(ibm) run(apple) 2(dell)drive(apple) 8200(dell) touchpad(dell) burn(apple) modem(ibm) thinkpad(ibm) 1(apple) usb(dell)airport(apple) inspiron(dell) pad(dell) normal(dell) display(ibm) battery(ibm) ram(apple) 1(dell)firewire(apple) system(dell) keyboard(dell) image(dell) built(ibm) notebook(ibm) mac(apple) 0(dell)dvd(apple) hour(dell) point(dell) digital(apple) swap(ibm) ibm(ibm) battery(apple) slot(dell)usb(apple) sound(dell) stick(dell) organize(apple) easy(ibm) 3(ibm) hour(apple) firewire(dell)rw(apple) dell(dell) rest(dell) cds(apple) connector(ibm) feel(ibm) 12(apple) display(dell)card(apple) service(dell) touch(dell) latch(apple) feature(ibm) hour(ibm) operate(apple) standard(dell)mouse(apple) life(dell) erase(dell) advertise(dell) cd(ibm) high(ibm) word(apple) fast(dell)
osx(apple) applework(apple) port(dell) battery(dell) lightest(ibm) uxga(dell) light(ibm) battery(apple)memory(dell) file(apple) port(apple) battery(ibm) quality(dell) ultrasharp(dell) ultrabay(ibm) point(dell)special(dell) bounce(apple) port(ibm) battery(apple) year(ibm) display(dell) connector(ibm) touchpad(dell)crucial(dell) quit(apple) firewire(apple) geforce4(dell) hassle(ibm) organize(apple) dvd(ibm) button(dell)memory(apple) word(apple) imac(apple) 100mhz(apple) bania(dell) learn(apple) nice(ibm) hour(apple)memory(ibm) file(ibm) firewire(dell) 440(dell) 800mhz(apple) logo(apple) modem(ibm) battery(ibm)netscape(apple) file(dell) firewire(ibm) bus(apple) trackpad(apple) postscript(apple) connector(dell) battery(dell)reseller(apple) microsoft(apple) jack(apple) 8200(dell) cover(ibm) ll(apple) light(apple) fan(dell)10(dell) ms(apple) playback(dell) 8100(dell) workmanship(dell) sxga(dell) light(dell) erase(dell)special(apple) excel(apple) jack(dell) chipset(dell) section(apple) warm(apple) floppy(ibm) point(apple)
2000(ibm) ram(apple) port(dell) itune(apple) uxga(dell) port(apple) pentium(dell) drive(ibm)window(ibm) ram(ibm) port(apple) applework(apple) screen(dell) port(ibm) processor(dell) drive(dell)2000(apple) ram(dell) port(ibm) imovie(apple) screen(ibm) port(dell) p4(dell) drive(apple)2000(dell) screen(apple) 2(dell) import(apple) screen(apple) usb(apple) power(dell) hard(ibm)window(apple) 1(apple) 2(apple) battery(apple) ultrasharp(dell) plug(apple) pentium(apple) osx(apple)window(dell) screen(ibm) 2(ibm) iphoto(apple) 1600x1200(dell) cord(apple) pentium(ibm) hard(dell)portege(ibm) screen(dell) speak(dell) battery(ibm) display(dell) usb(ibm) keyboard(dell) hard(apple)option(ibm) 1(ibm) toshiba(dell) battery(dell) display(apple) usb(dell) processor(ibm) card(ibm)hassle(ibm) 1(dell) speak(ibm) hour(apple) display(ibm) firewire(apple) processor(apple) dvd(ibm)device(ibm) maco(apple) toshiba(ibm) hour(ibm) view(dell) plug(ibm) power(apple) card(dell)
pus, we use an ?IBM? word, an ?Apple? word, and
a ?Dell? word to replace an English word in their
corpus. For example, we use ?IBM10?, ?Apple10?,
?Dell10? to replace the word ?CD? whenever it ap-
pears in an IBM?s, Apple?s, or Dell?s review. Af-
ter the replacement, the reviews about IBM, Ap-
ple, and Dell will not share vocabularies with each
other. On the other hand, for any three created
words which represent the same English word, we
add three edges among them, and therefore we
get a simulated dictionary graph for our PCLSA
model.
The experimental result is shown in Table 5, in
which we try to extract 8 topics from the cross-
lingual corpus. The first ten rows show the re-
sult of our PCLSA model, in which we set a very
small value to the weight parameter ? for the reg-
ularizer part. This can be used as an approxima-
tion of the result from the traditional PLSA model
on this three language corpus. We can see that
the extracted topics are mainly written in mono-
language. As we set the value of parameter ?
larger, the extracted topics become multi-lingual,
which is shown in the next ten rows. From this
result, we can see the difference between the re-
views of different brands about the similar topic.
In addition, if we set the ? even larger, we will
get topics that are mostly made of the same words
from the three different brands, which means the
extracted topics are very smooth on the dictionary
graph now.
7 Conclusion
In this paper, we study the problem of cross-
lingual latent topic extraction where the task is to
extract a set of common latent topics from multi-
lingual text data. We propose a novel probabilistic
topic model (i.e. the Probabilistic Cross-Lingual
Latent Semantic Analysis (PCLSA) model) that
can incorporate translation knowledge in bilingual
dictionaries as a regularizer to constrain the pa-
rameter estimation so that the learned topic models
would be synchronized in multiple languages. We
evaluated the model using several data sets. The
experimental results show that PCLSA is effec-
tive in extracting common latent topics from mul-
tilingual text data, and it outperforms the baseline
method which uses the standard PLSA to fit each
monolingual text data set.
Our work opens up some interesting future re-
search directions to further explore. First, in
this paper, we have only experimented with uni-
form weighting of edge in the bilingual graph.
It should be very interesting to explore how to
assign weights to the edges and study whether
weighted graphs can further improve performance.
Second, it would also be interesting to further
extend PCLSA to accommodate discovering top-
ics in each language that aren?t well-aligned with
other languages.
8 Acknowledgments
We sincerely thank the anonymous reviewers for
their comprehensive and constructive comments.
The work was supported in part by NASA grant
1135
NNX08AC35A, by the National Science Foun-
dation under Grant Numbers IIS-0713581, IIS-
0713571, and CNS-0834709, and by a Sloan Re-
search Fellowship.
References
David Blei and John Lafferty. 2005. Correlated topic
models. In NIPS ?05: Advances in Neural Informa-
tion Processing Systems 18.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd interna-
tional conference on Machine learning, pages 113?
120.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2003a. Hierarchical topic models and the nested
chinese restaurant process. In Neural Information
Processing Systems (NIPS) 16.
D. Blei, A. Ng, and M. Jordan. 2003b. Latent Dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
J. Boyd-Graber and D. Blei. 2009. Multilingual topic
models for unaligned text. In Uncertainty in Artifi-
cial Intelligence.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of ACL 2008.
Martin Franz, J. Scott McCarley, and Salim Roukos.
1998. Ad hoc and multilingual information retrieval
at IBM. In Text REtrieval Conference, pages 104?
115.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of ACL 1995,
pages 236?243.
Alfio Gliozzo and Carlo Strapparava. 2006. Exploit-
ing comparable corpora and bilingual dictionaries
for cross-language text categorization. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 553?560, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
T. Hofmann. 1999a. Probabilistic latent semantic anal-
ysis. In Proceedings of UAI 1999, pages 289?296.
Thomas Hofmann. 1999b. The cluster-abstraction
model: Unsupervised learning of topic hierarchies
from text data. In IJCAI? 99, pages 682?687.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Mach. Learn.,
42(1-2):177?196.
Jagadeesh Jagaralamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proceedings of the European Conference on In-
formation Retrieval (ECIR), Milton Keynes, United
Kingdom.
Woosung Kim and Sanjeev Khudanpur. 2004. Lex-
ical triggers and latent semantic analysis for cross-
lingual language model adaptation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 3(2):94?112.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In ICML ?06: Proceedings of the 23rd in-
ternational conference on Machine learning, pages
577?584.
H. Masuichi, R. Flournoy, S. Kaufmann, and S. Peters.
2000. A bootstrapping method for extracting bilin-
gual text pairs. In Proc. 18th COLINC, pages 1066?
1070.
Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture
model for contextual text mining. In Proceedings of
KDD ?06, pages 649?655.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of WWW ?07.
Qiaozhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008a. Topic modeling with network regular-
ization. In WWW, pages 101?110.
Qiaozhu Mei, Duo Zhang, and ChengXiang Zhai.
2008b. A general optimization framework for
smoothing language models on graph structures. In
SIGIR ?08: Proceedings of the 31st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 611?618,
New York, NY, USA. ACM.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew Mccallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880?889, Singapore,
August. Association for Computational Linguistics.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from wikipedia.
In WWW ?09: Proceedings of the 18th international
conference on World wide web, pages 1155?1156,
New York, NY, USA. ACM.
F. Sadat, M. Yoshikawa, and S. Uemura. 2003. Bilin-
gual terminology acquisition from comparable cor-
pora and phrasal translation to cross-language infor-
mation retrieval. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 141?144.
1136
Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi,
and Thomas Griffiths. 2004. Probabilistic author-
topic models for information discovery. In Proceed-
ings of KDD?04, pages 306?315.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and
Richard Sproat. 2007. Mining correlated bursty
topic patterns from coordinated text streams. In
KDD ?07: Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 784?793, New York, NY,
USA. ACM.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics.
1137
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1526?1535,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Structural Topic Model for Latent Topical Structure Analysis
Hongning Wang, Duo Zhang, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana IL, 61801 USA
{wang296, dzhang22, czhai}@cs.uiuc.edu
Abstract
Topic models have been successfully applied
to many document analysis tasks to discover
topics embedded in text. However, existing
topic models generally cannot capture the la-
tent topical structures in documents. Since
languages are intrinsically cohesive and coher-
ent, modeling and discovering latent topical
transition structures within documents would
be beneficial for many text analysis tasks.
In this work, we propose a new topic model,
Structural Topic Model, which simultaneously
discovers topics and reveals the latent topi-
cal structures in text through explicitly model-
ing topical transitions with a latent first-order
Markov chain. Experiment results show that
the proposed Structural Topic Model can ef-
fectively discover topical structures in text,
and the identified structures significantly im-
prove the performance of tasks such as sen-
tence annotation and sentence ordering.
1 Introduction
A great amount of effort has recently been made in
applying statistical topic models (Hofmann, 1999;
Blei et al, 2003) to explore word co-occurrence pat-
terns, i.e. topics, embedded in documents. Topic
models have become important building blocks of
many interesting applications (see e.g., (Blei and
Jordan, 2003; Blei and Lafferty, 2007; Mei et al,
2007; Lu and Zhai, 2008)).
In general, topic models can discover word clus-
tering patterns in documents and project each doc-
ument to a latent topic space formed by such word
clusters. However, the topical structure in a docu-
ment, i.e., the internal dependency between the top-
ics, is generally not captured due to the exchange-
ability assumption (Blei et al, 2003), i.e., the doc-
ument generation probabilities are invariant to con-
tent permutation. In reality, natural language text
rarely consists of isolated, unrelated sentences, but
rather collocated, structured and coherent groups of
sentences (Hovy, 1993). Ignoring such latent topi-
cal structures inside the documents means wasting
valuable clues about topics and thus would lead to
non-optimal topic modeling.
Taking apartment rental advertisements as an ex-
ample, when people write advertisements for their
apartments, it?s natural to first introduce ?size? and
?address? of the apartment, and then ?rent? and
?contact?. Few people would talk about ?restric-
tion? first. If this kind of topical structures are cap-
tured by a topic model, it would not only improve
the topic mining results, but, more importantly, also
help many other document analysis tasks, such as
sentence annotation and sentence ordering.
Nevertheless, very few existing topic models at-
tempted to model such structural dependency among
topics. The Aspect HMM model introduced in
(Blei and Moreno, 2001) combines pLSA (Hof-
mann, 1999) with HMM (Rabiner, 1989) to perform
document segmentation over text streams. However,
Aspect HMM separately estimates the topics in the
training set and depends on heuristics to infer the
transitional relations between topics. The Hidden
Topic Markov Model (HTMM) proposed by (Gru-
ber et al, 2007) extends the traditional topic models
by assuming words in each sentence share the same
topic assignment, and topics transit between adja-
cent sentences. However, the transitional structures
among topics, i.e., how likely one topic would fol-
low another topic, are not captured in this model.
1526
In this paper, we propose a new topic model,
named Structural Topic Model (strTM) to model and
analyze both latent topics and topical structures in
text documents. To do so, strTM assumes: 1) words
in a document are either drawn from a content topic
or a functional (i.e., background) topic; 2) words in
the same sentence share the same content topic; and
3) content topics in the adjacent sentences follow a
topic transition that satisfies the first order Markov
property. The first assumption distinguishes the se-
mantics of the occurrence of each word in the doc-
ument, the second requirement confines the unreal-
istic ?bag-of-word? assumption into a tighter unit,
and the third assumption exploits the connection be-
tween adjacent sentences.
To evaluate the usefulness of the identified top-
ical structures by strTM, we applied strTM to the
tasks of sentence annotation and sentence ordering,
where correctly modeling the document structure
is crucial. On the corpus of 8,031 apartment ad-
vertisements from craiglist (Grenager et al, 2005)
and 1,991 movie reviews from IMDB (Zhuang et
al., 2006), strTM achieved encouraging improve-
ment in both tasks compared with the baseline meth-
ods that don?t explicitly model the topical structure.
The results confirm the necessity of modeling the
latent topical structures inside documents, and also
demonstrate the advantages of the proposed strTM
over existing topic models.
2 Related Work
Topic models have been successfully applied to
many problems, e.g., sentiment analysis (Mei et
al., 2007), document summarization (Lu and Zhai,
2008) and image annotation (Blei and Jordan, 2003).
However, in most existing work, the dependency
among the topics is loosely governed by the prior
topic distribution, e.g., Dirichlet distribution.
Some work has attempted to capture the interre-
lationship among the latent topics. Correlated Topic
Model (Blei and Lafferty, 2007) replaces Dirichlet
prior with logistic Normal prior for topic distribu-
tion in each document in order to capture the cor-
relation between the topics. HMM-LDA (Griffiths
et al, 2005) distinguishes the short-range syntactic
dependencies from long-range semantic dependen-
cies among the words in each document. But in
HMM-LDA, only the latent variables for the syn-
tactic classes are treated as a locally dependent se-
quence, while latent topics are treated the same as in
other topic models. Chen et al introduced the gen-
eralized Mallows model to constrain the latent topic
assignments (Chen et al, 2009). In their model,
they assume there exists a canonical order among
the topics in the collection of related documents and
the same topics are forced not to appear in discon-
nected portions of the topic sequence in one docu-
ment (sampling without replacement). Our method
relaxes this assumption by only postulating transi-
tional dependency between topics in the adjacent
sentences (sampling with replacement) and thus po-
tentially allows a topic to appear multiple times in
disconnected segments. As discussed in the pre-
vious section, HTMM (Gruber et al, 2007) is the
most similar model to ours. HTMM models the
document structure by assuming words in the same
sentence share the same topic assignment and suc-
cessive sentences are more likely to share the same
topic. However, HTMM only loosely models the
transition between topics as a binary relation: the
same as the previous sentence?s assignment or draw
a new one with a certain probability. This simpli-
fied coarse modeling of dependency could not fully
capture the complex structure across different docu-
ments. In contrast, our strTM model explicitly cap-
tures the regular topic transitions by postulating the
first order Markov property over the topics.
Another line of related work is discourse analysis
in natural language processing: discourse segmen-
tation (Sun et al, 2007; Galley et al, 2003) splits a
document into a linear sequence of multi-paragraph
passages, where lexical cohesion is used to link to-
gether the textual units; discourse parsing (Soricut
and Marcu, 2003; Marcu, 1998) tries to uncover a
more sophisticated hierarchical coherence structure
from text to represent the entire discourse. One work
in this line that shares a similar goal as ours is the
content models (Barzilay and Lee, 2004), where an
HMM is defined over text spans to perform infor-
mation ordering and extractive summarization. A
deficiency of the content models is that the identi-
fication of clusters of text spans is done separately
from transition modeling. Our strTM addresses this
deficiency by defining a generative process to simul-
taneously capture the topics and the transitional re-
1527
lationship among topics: allowing topic modeling
and transition modeling to reinforce each other in a
principled framework.
3 Structural Topic Model
In this section, we formally define the Structural
Topic Model (strTM) and discuss how it captures the
latent topics and topical structures within the docu-
ments simultaneously. From the theory of linguistic
analysis (Kamp, 1981), we know that document ex-
hibits internal structures, where structural segments
encapsulate semantic units that are closely related.
In strTM, we treat a sentence as the basic structure
unit, and assume all the words in a sentence share the
same topical aspect. Besides, two adjacent segments
are assumed to be highly related (capturing cohesion
in text); specifically, in strTM we pose a strong tran-
sitional dependency assumption among the topics:
the choice of topic for each sentence directly de-
pends on the previous sentence?s topic assignment,
i.e., first order Markov property. Moveover, tak-
ing the insights from HMM-LDA that not all the
words are content conveying (some of them may
just be a result of syntactic requirement), we intro-
duce a dummy functional topic zB for every sen-
tence in the document. We use this functional topic
to capture the document-independent word distribu-
tion, i.e., corpus background (Zhai et al, 2004). As
a result, in strTM, every sentence is treated as a mix-
ture of content and functional topics.
Formally, we assume a corpus consists of D doc-
uments with a vocabulary of size V, and there are
k content topics embedded in the corpus. In a given
document d, there arem sentences and each sentence
i hasNi words. We assume the topic transition prob-
ability p(z|z?) is drawn from a Multinomial distribu-
tionMul(?z?), and the word emission probability un-
der each topic p(w|z) is drawn from a Multinomial
distribution Mul(?z).
To get a unified description of the generation
process, we add another dummy topic T-START in
strTM, which is the initial topic with position ?-1?
for every document but does not emit any words.
In addition, since our functional topic is assumed to
occur in all the sentences, we don?t need to model
its transition with other content topics. We use a
Binomial variable pi to control the proportion be-
tween content and functional topics in each sen-
tence. Therefore, there are k+1 topic transitions, one
for T-START and others for k content topics; and k
emission probabilities for the content topics, with an
additional one for the functional topic zB (in total
k+1 emission probability distributions).
Conditioned on the model parameters ? =
(?, ?, pi), the generative process of a document in
strTM can be described as follows:
1. For each sentence si in document d:
(a) Draw topic zi from Multinomial distribu-
tion conditioned on the previous sentence
si?1?s topic assignment zi?1:
zi ? Mul(?zi?1)
(b) Draw each word wij in sentence si from
the mixture of content topic zi and func-
tional topic zB:
wij ? pip(wij |?, zi)+(1?pi)p(wij |?, zB)
The joint probability of sentences and topics in
one document defined by strTM is thus given by:
p(S0, S1, . . . , Sm, z|?, ?, pi) =
m
?
i=1
p(zi|?, zi?1)p(Si|zi)
(1)
where the topic to sentence emission probability is
defined as:
p(Si|zi) =
Ni
?
j=0
[
pip(wij |?, zi) + (1? pi)p(wij |?, zB)
]
(2)
This process is graphically illustrated in Figure 1.
 
zmz0 ??..
wm??..
NmD
K+1
w0
N0
K+1
 
z1
w1
N1
Tstart
Figure 1: Graphical Representation of strTM.
From the definition of strTM, we can see that the
document structure is characterized by a document-
specific topic chain, and forcing the words in one
1528
sentence to share the same content topic ensures se-
mantic cohesion of the mined topics. Although we
do not directly model the topic mixture for each doc-
ument as the traditional topic models do, the word
co-occurrence patterns within the same document
are captured by topic propagation through the transi-
tions. This can be easily understood when we write
down the posterior probability of the topic assign-
ment for a particular sentence:
p(zi|S0, S1, . . . , Sm,?)
=p(S0, S1, . . . , Sm|zi,?)p(zi)
p(S0, S1, . . . , Sm)
? p(S0, S1, . . . , Si, zi)? p(Si+1, Si+2, . . . , Sm|zi)
=
?
zi?1
p(S0, . . . , Si?1, zi?1)p(zi|zi?1)p(Si|zi)
?
?
zi+1
p(Si+1, . . . , Sm|zi+1)p(zi+1|zi) (3)
The first part of Eq(3) describes the recursive in-
fluence on the choice of topic for the ith sentence
from its preceding sentences, while the second part
captures how the succeeding sentences affect the
current topic assignment. Intuitively, when we need
to decide a sentence?s topic, we will look ?back-
ward? and ?forward? over all the sentences in the
document to determine a ?suitable? one. In addition,
because of the first order Markov property, the local
topical dependency gets more emphasis, i.e., they
are interacting directly through the transition proba-
bilities p(zi|zi?1) and p(zi+1|zi). And such interac-
tion on sentences farther away would get damped by
the multiplication of such probabilities. This result
is reasonable, especially in a long document, since
neighboring sentences are more likely to cover sim-
ilar topics than two sentences far apart.
4 Posterior Inference and Parameter
Estimation
The chain structure in strTM enables us to perform
exact inference: posterior distribution can be ef-
ficiently calculated by the forward-backward algo-
rithm, the optimal topic sequence can be inferred
using the Viterbi algorithm, and parameter estima-
tion can be solved by the Expectation Maximization
(EM) algorithm. More technical details can be found
in (Rabiner, 1989). In this section, we only discuss
strTM-specific procedures.
In the E-Step of EM algorithm, we need to col-
lect the expected count of a sequential topic pair
(z, z?) and a topic-word pair (z, w) to update the
model parameters ? and ? in the M-Step. In strTM,
E[c(z, z?)] can be easily calculated by forward-
backward algorithm. But we have to go one step
further to fetch the required sufficient statistics for
E[c(z, w)], because our emission probabilities are
defined over sentences.
Through forward-backward algorithm, we can get
the posterior probability p(si, z|d,?). In strTM,
words in one sentence are independently drawn from
either a specific content topic z or functional topic
zB according to the mixture weight pi. Therefore,
we can accumulate the expected count of (z, w) over
all the sentences by:
E[c(z, w)] =
?
d,s?d
pip(w|z)p(s, z|d,?)c(w, s)
pip(w|z) + (1? pi)p(w|zB)
(4)
where c(w, s) indicates the frequency of word w in
sentence s.
Eq(4) can be easily explained as follows. Since
we already observe topic z and sentence s co-
occur with probability p(s, z|d,?), each word w
in s should share the same probability of be-
ing observed with content topic z. Thus the ex-
pected count of c(z, w) in this sentence would be
p(s, z|d,?)c(w, s). However, since each sentence
is also associated with the functional topic zB , the
word w may also be drawn from zB . By applying
the Bayes? rule, we can properly reallocate the ex-
pected count of c(z, w) by Eq(4). The same strategy
can be applied to obtain E[c(zB, w)].
As discussed in (Johnson, 2007), to avoid the
problem that EM algorithm tends to assign a uni-
form word/state distribution to each hidden state,
which deviates from the heavily skewed word/state
distributions empirically observed, we can apply a
Bayesian estimation approach for strTM. Thus we
introduce prior distributions over the topic transi-
tion Mul(?z?) and emission probabilities Mul(?z),
and use the Variational Bayesian (VB) (Jordan et al,
1999) estimator to obtain a model with more skewed
word/state distributions.
Since both the topic transition and emission prob-
abilities are Multinomial distributions in strTM,
the conjugate Dirichlet distribution is the natural
1529
choice for imposing a prior on them (Diaconis and
Ylvisaker, 1979). Thus, we further assume:
?z ? Dir(?) (5)
?z ? Dir(?) (6)
where we use exchangeable Dirichlet distributions
to control the sparsity of ?z and ?z . As ? and ? ap-
proach zero, the prior strongly favors the models in
which each hidden state emits as few words/states as
possible. In our experiments, we empirically tuned
? and ? on different training corpus to optimize log-
likelihood.
The resulting VB estimation only requires a mi-
nor modification to the M-Step in the original EM
algorithm:
??z =
?(E[c(z?, z)] + ?)
?(E[c(z)] + k?)
(7)
??z =
?(E[c(w, z)] + ?)
?(E[c(z)] + V ?)
(8)
where ?(x) is the exponential of the first derivative
of the log-gamma function.
The optimal setting of pi for the proportion of con-
tent topics in the documents is empirically tuned by
cross-validation over the training corpus to maxi-
mize the log-likelihood.
5 Experimental Results
In this section, we demonstrate the effectiveness
of strTM in identifying latent topical structures
from documents, and quantitatively evaluate how the
mined topic transitions can help the tasks of sen-
tence annotation and sentence ordering.
5.1 Data Set
We used two different data sets for evaluation: apart-
ment advertisements (Ads) from (Grenager et al,
2005) and movie reviews (Review) from (Zhuang et
al., 2006).
The Ads data consists of 8,767 advertisements for
apartment rentals crawled from Craigslist website.
302 of them have been labeled with 11 fields, in-
cluding size, feature, address, etc., on the sentence
level. The review data contains 2,000 movie reviews
discussing 11 different movies from IMDB. These
reviews are manually labeled with 12 movie feature
labels (We didn?t use the additional opinion anno-
tations in this data set.) , e.g., VP (vision effects),
MS (music and sound effects), etc., also on the sen-
tences, but the annotations in the review data set is
much sparser than that in the Ads data set (see in Ta-
ble 1). The sentence-level annotations make it pos-
sible to quantitatively evaluate the discovered topic
structures.
We performed simple preprocessing on these
two data sets: 1) removed a standard list of stop
words, terms occurring in less than 2 documents;
2) discarded the documents with less than 2 sen-
tences; 3) aggregated sentence-level annotations
into document-level labels (binary vector) for each
document. Table 1 gives a brief summary on these
two data sets after the processing.
Ads Review
Document Size 8,031 1,991
Vocabulary Size 21,993 14,507
Avg Stn/Doc 8.0 13.9
Avg Labeled Stn/Doc 7.1* 5.1
Avg Token/Stn 14.1 20.0
*Only in 302 labeled ads
Table 1: Summary of evaluation data set
5.2 Topic Transition Modeling
First, we qualitatively demonstrate the topical struc-
ture identified by strTM from Ads data1. We trained
strTM with 11 content topics in Ads data set, used
word distribution under each class (estimated by
maximum likelihood estimator on document-level
labels) as priors to initialize the emission probabil-
ity Mul(?z) in Eq(6), and treated document-level la-
bels as the prior for transition from T-START in each
document, so that the mined topics can be aligned
with the predefined class labels. Figure 2 shows the
identified topics and the transitions among them. To
get a clearer view, we discarded the transitions be-
low a threshold of 0.1 and removed all the isolated
nodes.
From Figure 2, we can find some interesting top-
ical structures. For example, people usually start
with ?size?, ?features? and ?address?, and end
with ?contact? information when they post an apart-
1Due to the page limit, we only show the result in Ads data
set.
1530
TELEPHONE
appointment
information
contact
email
parking
kitchen
room
laundry
storage
close
shopping
transportation
bart
location
http
photos
click
pictures
view
deposit
month
lease
rent
year
pets
kitchen
cat
negotiate
smoking
water
garbage
included
paid
utilities
NUM
bedroom
bath
room
large
Figure 2: Estimated topics and topical transitions in Ads data set
ment ads. Also, we can discover a strong transition
from ?size? to ?features?. This intuitively makes
sense because people usually write ?it?s a two bed-
rooms apartment? first, and then describe other ?fea-
tures? about the apartment. The mined topics are
also quite meaningful. For example, ?restrictions?
are usually put over pets and smoking, and parking
and laundry are always the major ?features? of an
apartment.
To further quantitatively evaluate the estimated
topic transitions, we used Kullback-Leibler (KL) di-
vergency between the estimated transition matrix
and the ?ground-truth? transition matrix as the met-
ric. Each element of the ?ground-truth? transition
matrix was calculated by Eq(9), where c(z, z?) de-
notes how many sentences annotated by z? immedi-
ately precede one annotated by z. ? is a smoothing
factor, and we fixed it to 0.01 in the experiment.
p?(z|z?) = c(z, z
?) + ?
c(z) + k?
(9)
The KL divergency between two transition matri-
ces is defined in Eq(10). Because we have a k ? k
transition matrix (Tstart is not included), we calcu-
lated the average KL divergency against the ground-
truth over all the topics:
avgKL=
?k
i=1 KL(p(z|z
?
i)||p?(z|z?i))+KL(p?(z|z?i)||p(z|z?i))
2k
(10)
where p?(z|z?) is the ground-truth transition proba-
bility estimated by Eq(9), and p(z|z?) is the transi-
tion probability given by the model.
We used pLSA (Hofmann, 1999), latent permuta-
tion model (lPerm) (Chen et al, 2009) and HTMM
(Gruber et al, 2007) as the baseline methods for the
comparison. Because none of these three methods
can generate a topic transition matrix directly, we
extended them a little bit to achieve this goal. For
pLSA, we used the document-level labels as priors
for the topic distribution in each document, so that
the estimated topics can be aligned with the prede-
fined class labels. After the topics were estimated,
for each sentence we selected the topic that had
the highest posterior probability to generate the sen-
tence as its class label. For lPerm and HTMM, we
used Kuhn-Munkres algorithm (Lova?sz and Plum-
mer, 1986) to find the optimal topic-to-class align-
ment based on the sentence-level annotations. Af-
ter the sentences were annotated with class labels,
we estimated the topic transition matrices for all of
these three methods by Eq(9).
1531
Since only a small portion of sentences are an-
notated in the Review data set, very few neighbor-
ing sentences are annotated at the same time, which
introduces many noisy transitions. As a result, we
only performed the comparison on the Ads data set.
The ?ground-truth? transition matrix was estimated
based on all the 302 annotated ads.
pLSA+prior lPerm HTMM strTM
avgKL 0.743 1.101 0.572 0.372
p-value 0.023 1e-4 0.007 ?
Table 2: Comparison of estimated topic transitions on
Ads data set
In Table 2, the p-value was calculated based on t-
test of the KL divergency between each topic?s tran-
sition probability against strTM. From the results,
we can see that avgKL of strTM is smaller than the
other three baseline methods, which means the esti-
mated transitional relation by strTM is much closer
to the ground-truth transition. This demonstrates
that strTM captures the topical structure well, com-
pared with other baseline methods.
5.3 Sentence Annotation
In this section, we demonstrate how the identified
topical structure can benefit the task of sentence an-
notation. Sentence annotation is one step beyond the
traditional document classification task: in sentence
annotation, we want to predict the class label for
each sentence in the document, and this will be help-
ful for other problems, including extractive summa-
rization and passage retrieval. However, the lack of
detailed annotations on sentences greatly limits the
effectiveness of the supervised classification meth-
ods, which have been proved successful on docu-
ment classifications.
In this experiment, we propose to use strTM to ad-
dress this annotation task. One advantage of strTM
is that it captures the topic transitions on the sen-
tence level within documents, which provides a reg-
ularization over the adjacent predictions.
To examine the effectiveness of such structural
regularization, we compared strTM with four base-
line methods: pLSA, lPerm, HTMM and Naive
Bayes model. The sentence labeling approaches for
strTM, pLSA, lPerm and HTMM have been dis-
cussed in the previous section. As for Naive Bayes
model, we used EM algorithm 2 with both labeled
and unlabeled data for the training purpose (we used
the same unigram features as in topics models). We
set weights for the unlabeled data to be 10?3 in
Naive Bayes with EM.
The comparison was performed on both data sets.
We set the size of topics in each topic model equal
to the number of classes in each data set accord-
ingly. To tackle the situation where some sentences
in the document are not strictly associated with any
classes, we introduced an additional NULL content
topic in all the topic models. During the training
phase, none of the methods used the sentence-level
annotations in the documents, so that we treated the
whole corpus as the training and testing set.
To evaluate the prediction performance, we cal-
culated accuracy, recall and precision based on the
correct predictions over the sentences, and averaged
over all the classes as the criterion.
Model Accuracy Recall Precison
pLSA+prior 0.432 0.649 0.457
lPerm 0.610 0.514 0.471
HTMM 0.606 0.588 0.443
NB+EM 0.528 0.337 0.612
strTM 0.747 0.674 0.620
Table 3: Sentence annotation performance on Ads data
set
Model Accuracy Recall Precison
pLSA+prior 0.342 0.278 0.250
lPerm 0.286 0.205 0.184
HTMM 0.369 0.131 0.149
NB+EM 0.341 0.354 0.431
strTM 0.541 0.398 0.323
Table 4: Sentence annotation performance on Review
data set
Annotation performance on the two data sets is
shown in Table 3 and Table 4. We can see that strTM
outperformed all the other baseline methods on most
of the metrics: strTM has the best accuracy and re-
call on both of the two data sets. The improvement
confirms our hypothesis that besides solely depend-
ing on the local word patterns to perform predic-
2Mallet package: http://mallet.cs.umass.edu/
1532
tions, adjacent sentences provide a structural reg-
ularization in strTM (see Eq(3)). Compared with
lPerm, which postulates a strong constrain over the
topic assignment (sampling without replacement),
strTM performed much better on both of these two
data sets. This validates the benefit of modeling lo-
cal transitional relation compared with the global or-
dering. Besides, strTM achieved over 46% accu-
racy improvement compared with the second best
HTMM in the review data set. This result shows
the advantage of explicitly modeling the topic tran-
sitions between neighbor sentences instead of using
a binary relation to do so as in HTMM.
To further testify how the identified topical struc-
ture can help the sentence annotation task, we first
randomly removed 100 annotated ads from the train-
ing corpus and used them as the testing set. Then,
we used the ground-truth topic transition matrix es-
timated from the training data to order those 100 ads
according to their fitness scores under the ground-
truth topic transition matrix, which is defined in
Eq(11). We tested the prediction accuracy of differ-
ent models over two different partitions, top 50 and
bottom 50, according to this order.
fitness(d) = 1
|d|
|d|
?
i=0
log p?(ti|ti?1) (11)
where ti is the class label for ith sentence in doc-
ument d, |d| is the number of sentences in docu-
ment d, and p?(ti|ti?1) is the transition probability
estimated by Eq(9).
Top 50 p-value Bot 50 p-value
pLSA+prior 0.496 4e-12 0.542 0.004
lPerm 0.669 0.003 0.505 8e-4
HTMM 0.683 0.004 0.579 0.003
NB + EM 0.492 1e-12 0.539 0.002
strTM 0.752 ? 0.644 ?
Table 5: Sentence annotation performance according to
structural fitness
The results are shown in Table 5. From this table,
we can find that when the testing documents follow
the regular patterns as in the training data, i.e., top
50 group, strTM performs significantly better than
the other methods; when the testing documents don?t
share such structure, i.e., bottom 50 group, strTM?s
performance drops. This comparison confirms that
when a testing document shares similar topic struc-
ture as the training data, the topical transitions cap-
tured by strTM can help the sentence annotation task
a lot. In contrast, because pLSA and Naive Bayes
don?t depend on the document?s structure, their per-
formance does not change much over these two par-
titions.
5.4 Sentence Ordering
In this experiment, we illustrate how the learned top-
ical structure can help us better arrange sentences in
a document. Sentence ordering, or text planning, is
essential to many text synthesis applications, includ-
ing multi-document summarization (Goldstein et al,
2000) and concept-to-text generation (Barzilay and
Lapata, 2005).
In strTM, we evaluate all the possible orderings
of the sentences in a given document and selected
the optimal one which gives the highest generation
probability:
??(m) = argmax
?(m)
?
z
p(S?[0], S?[1], . . . , S?[m], z|?)
(12)
where ?(m) is a permutation of 1 to m, and ?[i] is
the ith element in this permutation.
To quantitatively evaluate the ordering result, we
treated the original sentence order (OSO) as the per-
fect order and used Kendall?s ?(?) (Lapata, 2006) as
the evaluation metric to compute the divergency be-
tween the optimum ordering given by the model and
OSO. Kendall?s ?(?) is widely used in information
retrieval domain to measure the correlation between
two ranked lists and it indicates how much an order-
ing differs from OSO, which ranges from 1 (perfect
matching) to -1 (totally mismatching).
Since only the HTMM and lPerm take the order
of sentences in the document into consideration, we
used them as the baselines in this experiment. We
ranked OSO together with candidate permutations
according to the corresponding model?s generation
probability. However, when the size of documents
becomes larger, it?s infeasible to permutate all the
orderings, therefore we randomly permutated 200
possible orderings of sentences as candidates when
there were more than 200 possible candidates. The
1533
2bedroom 1bath in very nice complex! Pool,
carport, laundry facilities!! Call Don (650)207-
5769 to see! Great location!! Also available,
2bed.2bath for $1275 in same complex.
=?
2bedroom 1bath in very nice complex! Pool, car-
port, laundry facilities!! Great location!! Also
available, 2bed.2bath for $1275 in same complex.
Call Don (650)207-5769 to see!
2 bedrooms 1 bath + a famyly room in a cul-de-
sac location. Please drive by and call Marilyn for
appointment 650-652-5806. Address: 517 Price
Way, Vallejo. No Pets Please!
=?
2 bedrooms 1 bath + a famyly room in a cul-de-
sac location. Address: 517 Price Way, Vallejo. No
Pets Please! Please drive by and call Marilyn for
appointment 650-652-5806.
Table 6: Sample results for document ordering by strTM
experiment was performed on both data sets with
80% data for training and the other 20% for testing.
We calculated the ?(?) of all these models for
each document in the two data sets and visualized
the distribution of ?(?) in each data set with his-
togram in Figure 3. From the results, we could ob-
serve that strTM?s ?(?) is more skewed towards the
positive range (with mean 0.619 in Ads data set and
0.398 in review data set) than lPerm?s results (with
mean 0.566 in Ads data set and 0.08 in review data
set) and HTMM?s results (with mean 0.332 in Ads
data set and 0.286 in review data set). This indi-
cates that strTM better captures the internal structure
within the documents.
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 10
100200
300400
500600
700800
900
?(?)
# of Doc
uments
AdslPermHTMMstrTM
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 10
20
40
60
80
100
120
140
160
?(?)
# of Doc
uments
ReviewlPermHTMMstrTM
(a) Ads (b) Review
Figure 3: Document Ordering Performance in ?(?).
We see that all methods performed better on the
Ads data set than the review data set, suggesting
that the topical structures are more coherent in the
Ads data set than the review data. Indeed, in the
Ads data, strTM perfectly recovered 52.9% of the
original sentence order. When examining some mis-
matched results, we found that some of them were
due to an ?outlier? order given by the original docu-
ment (in comparison to the ?regular? patterns in the
set). In Table 6, we show two such examples where
we see the learned structure ?suggested? to move
the contact information to the end, which intuitively
gives us a more regular organization of the ads. It?s
hard to say that in this case, the system?s ordering is
inferior to that of the original; indeed, the system or-
der is arguably more natural than the original order.
6 Conclusions
In this paper, we proposed a new structural topic
model (strTM) to identify the latent topical struc-
ture in documents. Different from the traditional
topic models, in which exchangeability assumption
precludes them to capture the structure of a docu-
ment, strTM captures the topical structure explicitly
by introducing transitions among the topics. Experi-
ment results show that both the identified topics and
topical structure are intuitive and meaningful, and
they are helpful for improving the performance of
tasks such as sentence annotation and sentence or-
dering, where correctly recognizing the document
structure is crucial. Besides, strTM is shown to out-
perform not only the baseline topic models that fail
to model the dependency between the topics, but
also the semi-supervised Naive Bayes model for the
sentence annotation task.
Our work can be extended by incorporating richer
features, such as named entity and co-reference, to
enhance the model?s capability of structure finding.
Besides, advanced NLP techniques for document
analysis, e.g., shallow parsing, may also be used to
further improve structure finding.
7 Acknowledgments
We thank the anonymous reviewers for their use-
ful comments. This material is based upon work
supported by the National Science Foundation un-
der Grant Numbers IIS-0713581 and CNS-0834709,
and NASA grant NNX08AC35A.
1534
References
R. Barzilay and M. Lapata. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings
of the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 331?338.
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models, with applications to generation
and summarization. In Proceedings of HLT-NAACL,
pages 113?120.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of the 26th annual international
ACM SIGIR conference, pages 127?134.
D.M. Blei and J.D. Lafferty. 2007. A correlated topic
model of science. The Annals of Applied Statistics,
1(1):17?35.
D.M. Blei and P.J. Moreno. 2001. Topic segmentation
with an aspect hidden Markov model. In Proceedings
of the 24th annual international ACM SIGIR confer-
ence, page 348. ACM.
D.M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. The Journal of Machine
Learning Research, 3(2-3):993 ? 1022.
H. Chen, SRK Branavan, R. Barzilay, and D.R. Karger.
2009. Global models of document structure using la-
tent permutations. In Proceedings of HLT-NAACL,
pages 371?379.
P. Diaconis and D. Ylvisaker. 1979. Conjugate pri-
ors for exponential families. The Annals of statistics,
7(2):269?281.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1,
pages 562?569.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence ex-
traction. In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 40?48.
T. Grenager, D. Klein, and C.D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In Proceedings of the 43rd an-
nual meeting on association for computational linguis-
tics, pages 371?378.
T.L. Griffiths, M. Steyvers, D.M. Blei, and J.B. Tenen-
baum. 2005. Integrating topics and syntax. Advances
in neural information processing systems, 17:537?
544.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic markov models. volume 2, pages 163?
170.
T. Hofmann. 1999. Probabilistic latent semantic index-
ing. In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 50?57.
E.H. Hovy. 1993. Automated discourse generation using
discourse structure relations. Artificial intelligence,
63(1-2):341?385.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine learning, 37(2):183?
233.
H. Kamp. 1981. A theory of truth and semantic repre-
sentation. Formal methods in the study of language,
1:277?322.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
L. Lova?sz and M.D. Plummer. 1986. Matching theory.
Elsevier Science Ltd.
Y. Lu and C. Zhai. 2008. Opinion integration through
semi-supervised topic modeling. In Proceeding of
the 17th international conference on World Wide Web,
pages 121?130.
Daniel Marcu. 1998. The rhetorical parsing of natural
language texts. In ACL ?98, pages 96?103.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th interna-
tional conference on World Wide Web, pages 171?180.
L.R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257?286.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the NAACL-
HTC, pages 149?156.
B. Sun, P. Mitra, C.L. Giles, J. Yen, and H. Zha. 2007.
Topic segmentation with shared topic detection and
alignment of multiple documents. In Proceedings of
the 30th ACM SIGIR, pages 199?206.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
minning. In Proceeding of the 10th ACM SIGKDD
international conference on Knowledge discovery in
data mining, pages 743?748.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43?50.
1535
