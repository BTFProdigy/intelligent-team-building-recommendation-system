Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing Graphs with Hyperedge Replacement Grammars
David Chiang
Information Sciences Institute
University of Southern California
Jacob Andreas
Columbia University
University of Cambridge
Daniel Bauer
Department of Computer Science
Columbia University
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Bevan Jones
University of Edinburgh
Macquarie University
Kevin Knight
Information Sciences Institute
University of Southern California
Abstract
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm?s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
1 Introduction
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al, 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al, 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann?s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: ?As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.? Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
2 Hyperedge replacement grammars
We give a short example of how HRG works, fol-
lowed by formal definitions.
2.1 Example
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S . The first rewriting step replaces this edge
with a subgraph, which we might read as ?The
924
boy?girl?
want? arg0
arg1
boy?
believe? arg1
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 1: Sample members of a graph language,
representing the meanings of (clockwise from up-
per left): ?The girl wants the boy,? ?The boy is
believed,? and ?The boy wants the girl to believe
that he wants her.?
boy wants something (X) involving himself.? The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as ?The boy
wants the girl to believe something (Y) involving
both of them.? The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
2.2 Definitions
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = ?V, E, ??, where
? V is a finite set of nodes
? E ? V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
? ? : E ? C assigns a label (drawn from the
finite set C) to each edge.
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
Definition 2. A hypergraph fragment is a tuple
?V, E, ?, X?, where ?V, E, ?? is a hypergraph and
X ? V+ is a list of distinct nodes called the ex-
ternal nodes.
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = ?N,T, P, S ? where
? N and T are finite disjoint sets of nonterminal
and terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
A ? R, where A ? N and R is a graph frag-
ment over N ? T .
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ?G H? (or, H? is derived from H in one
step) as follows. Let e = (v1 ? ? ? vk) be an edge in
H with label A. Let (A? R) be a production ofG,
where R has external nodes XR = (u1 ? ? ? uk). Then
we write H ?G H? if H? is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1, . . . , k.
Let H ??G H? (or, H? is derived from H) be thereflexive, transitive closure of?G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
S ??G H.
When a HRG rule (A ? R) is applied to an
edge e, the mapping of external nodes in R to the
925
1
X ?
believe? arg1
girl?
arg0
1
Y
1 2
Y
?
12
want?
arg0
arg1
S
1
boy?
X
want? arg1
arg0
2 believe? arg1
want? arg1
girl?
arg0
boy?
arg0
Y
3
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?The
boy wants the girl to believe that he wants her.?
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = ?N,T, P, S , ??, where
?N, T, P, S ? is a HRG and ? : P ? K assigns a
weight in K to each production. The weight of a
derivation ofG is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
3 Parsing
Lautemann?s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
3.1 Representing subgraphs
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = ?V, E, ?? is, for some
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
926
subset E? ? E, the smallest subgraph containing
all edges in E?. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 ? ? ?w j. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple ?bn(I), be(I, v),m ? I?, where
? bn(I) is the set of boundary nodes of I
? be(I, v) be the set of boundary edges of v in I
? (m ? I) is a flag indicating whether the
marker node is in I.
The boundary representation of I suffices to
specify I compactly.
Proposition 1. If I and I? are two subgraphs of H
with the same boundary representation, then I =
I?.
Proof. Case 1: bn(I) is empty. If m ? I and m ? I?,
then all edges of H must belong to both I and I?,
that is, I = I? = H. Otherwise, if m < I and m < I?,
then no edges can belong to either I or I?, that is,
I = I? = ?.
Case 2: bn(I) is nonempty. Suppose I , I?;
without loss of generality, suppose that there is an
edge e that is in I \ I?. Let ? be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along ?
must be in I \ I?, or else there would be a boundary
node in the middle of ?, and ? would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of ?must be in I \ I?. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I?, which is a contradiction. 
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I ? J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I ? J iff it has a
boundary node of I ? J as an endpoint and is a
boundary edge of I or J.
Proof. (?) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(?) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I ? J, and an edge e? outside I, which
must also be outside J. For e < J (because I and
J are disjoint), and if e? ? J, then v would be a
boundary node of J. Therefore, e? < I ? J, so v is
a boundary node of I ? J. Case (ii): v has an edge
in I and therefore I ? J, and an edge not in either
I or J. 
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v ? bn(I) do
E ? be(I, v) ? be(J, v)
if v < bn(J) or v has an edge not in E then
add v to bn(I ? J)
be(I ? J, v)? E
for all v ? bn(J) do
if v < bn(I) then
add v to bn(I ? J)
be(I ? J, v)? be(I, v) ? be(J, v)
(m ? I ? J)? (m ? I) ? (m ? J)
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
3.2 Treewidth
Lautemann?s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
927
the notion of treewidth (Gildea, 2011), which we
review briefly here.
Definition 7. A tree decomposition of a graph
H = ?V, E? is a tree T , each of whose nodes ?
is associated with sets V? ? V and E? ? E, with
the following properties:
1. Vertex cover: For each v ? V , there is a node
? ? T such that v ? V?.
2. Edge cover: For each e = (v1 ? ? ? vk) ? E,
there is exactly one node ? ? T such that e ?
E?. We say that ? introduces e. Moreover,
v1, . . . , vk ? V?.
3. Running intersection: For each v ? V , the set
{? ? T | v ? V?} is connected.
The width of T is max |V?| ? 1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
?V, E, X? is a tree decomposition of ?V, E? that has
the additional property that all the external nodes
belong to V? for some ?. (Without loss of general-
ity, we assume that ? is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
K4 =
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al, 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a na??ve algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T . Then,
augment the V? as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SynSem algorithm (Jones et al, 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
method mean max
exact 1.491 2
approximate 1.494 3
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
(a) 0
a
believe? arg1
b
girl?
arg0
1
Y
(b) 0
1
0
b 1
0
a
b 1
arg1
a
b 1
Y
?
0
b
arg0
b
girl?
?
0believe?
?
Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al (2011)). Each tree node ?
must be one of:
? A leaf node, such that V? = ?.
? A unary node, which introduces exactly one
edge e.
? A binary node, which introduces no edges.
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A ?
R) ? G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
928
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let RD? be the subgraph of R in-
duced by the union of E?? for all ?? equal to or
dominated by ?. Then we can show the following.
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of RD? belong to V? ? Vparent(?).
Proof. Let v be a boundary node of RD?. Node v
must have an edge in RD? and therefore in R?? for
some ?? dominated by or equal to ?.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both V? and Vparent(?)
must contain v as well.
Case 2: v has an edge not in RD?. Therefore
there must be a tree node not dominated by or
equal to ? that contains this edge, and therefore
v. So by the running intersection property, ? and
its parent must contain v as well. 
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
3.3 Inference rules
We present the parsing algorithm as a deductive
system (Shieber et al, 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X ? V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A ??G I. Thus, the goalitem is [S ,H, ?]. An active item has the form
[A? R, ?, I, ?], where
? (A? R) is a production of G
? ? is a node of TR
? I is a subgraph of H
? ? is a bijection between the boundary nodes
of RD? and those of I.
The parser must ensure that ? is a bijection when
it creates a new item. Below, we use the notation
{e 7? e?} or {e 7? X} for the mapping that sends
each node of e to the corresponding node of e?
or X.
Passive items are generated by the following
rule:
? Root [B? Q, ?, J, ?]
[B, J, X]
where ? is the root of TQ, and X j = ?(XQ, j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
? Leaf
[A? R, ?, ?, ?]
where ? is a leaf node of TR.
? (Unary) Nonterminal
[A? R, ?1, I, ?] [B, J, X]
[A? R, ?, I ? J, ? ? {e 7? X}]
where ?1 is the only child of ?, and e is intro-
duced by ? and is labeled with nonterminal B.
? (Unary) Terminal
[A? R, ?1, I, ?]
[A? R, ?, I ? {e?}, ? ? {e 7? e?}]
where ?1 is the only child of ?, e is introduced
by ?, and e and e? are both labeled with ter-
minal a.
? Binary
[A? R, ?1, I, ?1] [A? R, ?2, J, ?2]
[A? R, ?, I ? J, ?1 ? ?2]
where ?1 and ?2 are the two children of ?.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key ?B, |bn(J)|?, so that when the next item on the
agenda is an active item [A ? R, ?1, I, ?], we
know that all possible matching passive items are
929
S ?
X
X
X
X ?
a
a a
a
a
(a) (b)
a
a a
a aa
(c)
Figure 4: Illustration of unsoundness in the recog-
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key ??(e), |e|?. Similarly, active items should
be indexed by key ??(e), |e|? so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (?1
or ?2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
3.4 The disjointness check
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H?, as well as a graph isomorphism
? : H? ? H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H? is isomorphic to the input graph H? as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
Condition (1) is enough to guarantee that ? is lo-
cally one-to-one in the sense that for all v ? H?, ?
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if ?I : I? ? H and
?J : J? ? H are locally one-to-one, then ?I ? ?J
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where ? changes (say) from being
one-to-one to two-to-one. So if ? is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that ? maps only
one node to the marker m. We can show this again
by induction: if ?I and ?J each map only one node
to m, then ?I??J must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that ?I , ?J , and ?I ? ?J
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H? are connected graphs,
? : H? ? H is locally one-to-one, and ??1 is de-
fined for some node of H, then ? is a bijection.
Proof. Suppose that ? is not a bijection. Then
there must be two nodes v?1, v?2 ? H? such that
?(v?1) = ?(v?2) = v ? H. We also know that thereis a node, namely, m, such that m? = ??1(m) is de-
fined.2 Choose a path ? (ignoring edge direction)
from v to m. Because ? is a local isomorphism,
we can construct a path from v?1 to m? that mapsto ?. Similarly, we can construct a path from v?2to m? that maps to ?. Let u? be the first node that
these two paths have in common. But u? must have
two edges that map to the same edge, which is a
contradiction. 
2If H were not connected, we would choose the marker in
the same connected component as v.
930
3.5 Complexity
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of RD? or RD?i , all
of which belong to V? (by Proposition 3), so there
are at most |V?| ? k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A? R, ?, I, ?], every boundary node of RD? must
belong to V??Vparent(?) (by Proposition 3). There-
fore the number of boundary nodes is at most k+1
(but typically less), and the number of possible
items is in O((2dn)k+1).
4 Synchronous Parsing
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
?N, T, T ?, P, S ?, where
? N is a finite set of nonterminal symbols
? T and T ? are finite sets of terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
(A? ?R,R?,??), where R is a graph fragment
over N ? T and R? is a graph fragment over
N ? T ?. The relation ? is a bijection linking
nonterminal mentions in R and R?, such that
if e ? e?, then they have the same label. We
call R the source side and R? the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A? ?R,R?,??), we con-
struct a nice tree decomposition of R?R? such that:
? All the external nodes of both R and R? be-
long to V? for some ?. (Without loss of gen-
erality, assume that ? is the root.)
? If e ? e?, then e and e? are introduced by the
same tree node.
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I?, X?] and active
items have the form [A? R : R?, ?, I, ?, I?, ??].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
[A? R : R?, ?1, I, ?, I?, ??] [B, J, X, J?, X?]
[A? R : R?, ?, I ? J, ? ? {e j 7? X j},
I? ? J?, ?? ? {e?j 7? X?j}]
where ?1 is the only child of ?, e and e? are both
introduced by ? and e ? e?, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
5 Conclusion
Although Lautemann?s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann?s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
3The Bolinas toolkit can be downloaded from
?http://www.isi.edu/licensed-sw/bolinas/?.
931
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
References
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ?97), pages 29?36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Micha? Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95?162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231?248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69?124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399?421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65?72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars?basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136?167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
932
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47?52,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Parsing as Machine Translation
Jacob Andreas
Computer Laboratory
University of Cambridge
jda33@cam.ac.uk
Andreas Vlachos
Computer Laboratory
University of Cambridge
av308@cam.ac.uk
Stephen Clark
Computer Laboratory
University of Cambridge
sc609@cam.ac.uk
Abstract
Semantic parsing is the problem of de-
riving a structured meaning representation
from a natural language utterance. Here
we approach it as a straightforward ma-
chine translation task, and demonstrate
that standard machine translation com-
ponents can be adapted into a semantic
parser. In experiments on the multilingual
GeoQuery corpus we find that our parser
is competitive with the state of the art,
and in some cases achieves higher accu-
racy than recently proposed purpose-built
systems. These results support the use of
machine translation methods as an infor-
mative baseline in semantic parsing evalu-
ations, and suggest that research in seman-
tic parsing could benefit from advances in
machine translation.
1 Introduction
Semantic parsing (SP) is the problem of trans-
forming a natural language (NL) utterance into
a machine-interpretable meaning representation
(MR). It is well-studied in NLP, and a wide va-
riety of methods have been proposed to tackle
it, e.g. rule-based (Popescu et al, 2003), super-
vised (Zelle, 1995), unsupervised (Goldwasser et
al., 2011), and response-based (Liang et al, 2011).
At least superficially, SP is simply a machine
translation (MT) task: we transform an NL ut-
terance in one language into a statement of an-
other (un-natural) meaning representation lan-
guage (MRL). Indeed, successful semantic parsers
often resemble MT systems in several impor-
tant respects, including the use of word align-
ment models as a starting point for rule extrac-
tion (Wong and Mooney, 2006; Kwiatkowski et
al., 2010) and the use of automata such as tree
transducers (Jones et al, 2012) to encode the re-
lationship between NL and MRL.
The key difference between the two tasks is that
in SP, the target language (the MRL) has very dif-
ferent properties to an NL. In particular, MRs must
conform strictly to a particular structure so that
they are machine-interpretable. Contrast this with
ordinary MT, where varying degrees of wrongness
are tolerated by human readers (and evaluation
metrics). To avoid producing malformed MRs, al-
most all of the existing research on SP has focused
on developing models with richer structure than
those commonly used for MT.
In this work we attempt to determine how ac-
curate a semantic parser we can build by treating
SP as a pure MT task, and describe pre- and post-
processing steps which allow structure to be pre-
served in the MT process.
Our contributions are as follows: We develop
a semantic parser using off-the-shelf MT compo-
nents, exploring phrase-based as well as hierarchi-
cal models. Experiments with four languages on
the popular GeoQuery corpus (Zelle, 1995) show
that our parser is competitve with the state-of-
the-art, in some cases achieving higher accuracy
than recently introduced purpose-built semantic
parsers. Our approach also appears to require
substantially less time to train than the two best-
performing semantic parsers. These results sup-
port the use of MT methods as an informative
baseline in SP evaluations and show that research
in SP could benefit from research advances in MT.
2 MT-based semantic parsing
The input is a corpus of NL utterances paired with
MRs. In order to learn a semantic parser using
MT we linearize the MRs, learn alignments be-
tween the MRL and the NL, extract translation
rules, and learn a language model for the MRL.
We also specify a decoding procedure that will re-
turn structured MRs for an utterance during pre-
diction.
47
states bordering Texas
state(next to(state(stateid(texas))))
? STEM & LINEARIZE
state border texa
state1 next to1 state1 stateid1 texas0
? ALIGN
state border texa
state1 next to1 state1 stateid1 texas0
? EXTRACT (PHRASE)
? state , state1 ?
? state border , state1 border1 ?
? texa , state1 stateid1 texas0 ?...
? EXTRACT (HIER)
[X] ? ?state , state1?
[X] ? ?state [X] texa ,
state1 [X] state1 stateid1 texas0?...
Figure 1: Illustration of preprocessing and rule ex-
traction.
Linearization We assume that the MRL is
variable-free (that is, the meaning representation
for each utterance is tree-shaped), noting that for-
malisms with variables, like the ?-calculus, can
be mapped onto variable-free logical forms with
combinatory logics (Curry et al, 1980).
In order to learn a semantic parser using MT
we begin by converting these MRs to a form more
similar to NL. To do so, we simply take a preorder
traversal of every functional form, and label every
function with the number of arguments it takes.
After translation, recovery of the function is easy:
if the arity of every function in the MRL is known,
then every traversal uniquely specifies its corre-
sponding tree. Using an example from GeoQuery,
given an input function of the form
answer(population(city(cityid(?seattle?, ?wa?))))
we produce a ?decorated? translation input of the
form
answer1 population1 city1 cityid2 seattle0 wa0
where each subscript indicates the symbol?s arity
(constants, including strings, are treated as zero-
argument functions). Explicit argument number
labeling serves two functions. Most importantly,
it eliminates any possible ambiguity from the tree
reconstruction which takes place during decod-
ing: given any sequence of decorated MRL to-
kens, we can always reconstruct the correspond-
ing tree structure (if one exists). Arity labeling ad-
ditionally allows functions with variable numbers
of arguments (e.g. cityid, which in some training
examples is unary) to align with different natural
language strings depending on context.
Alignment Following the linearization of the
MRs, we find alignments between the MR tokens
and the NL tokens using the IBM Model 4 (Brown
et al, 1993). Once the alignment algorithm is
run in both directions (NL to MRL, MRL to NL),
we symmetrize the resulting alignments to obtain
a consensus many-to-many alignment (Och and
Ney, 2000; Koehn et al, 2005).
Rule extraction From the many-to-many align-
ment we need to extract a translation rule ta-
ble, consisting of corresponding phrases in NL
and MRL. We consider a phrase-based transla-
tion model (Koehn et al, 2003) and a hierarchi-
cal translation model (Chiang, 2005). Rules for
the phrase-based model consist of pairs of aligned
source and target sequences, while hierarchical
rules are SCFG productions containing at most
two instances of a single nonterminal symbol.
Note that both extraction algorithms can learn
rules which a traditional tree-transducer-based ap-
proach cannot?for example the right hand side
[X] river1 all0 traverse1 [X]
corresponding to the pair of disconnected tree
fragments:
[X]

traverse

river

[X]
all
(where each X indicates a gap in the rule).
Language modeling In addition to translation
rules learned from a parallel corpus, MT systems
also rely on an n-gram language model for the tar-
get language, estimated from a (typically larger)
monolingual corpus. In the case of SP, such a
monolingual corpus is rarely available, and we in-
stead use the MRs available in the training data to
learn a language model of the MRL. This informa-
tion helps guide the decoder towards well-formed
48
structures; it encodes, for example, the preferences
of predicates of the MRL for certain arguments.
Prediction Given a new NL utterance, we need
to find the n best translations (i.e. sequences
of decorated MRL tokens) that maximize the
weighted sum of the translation score (the prob-
abilities of the translations according to the rule
translation table) and the language model score, a
process usually referred to as decoding. Standard
decoding procedures for MT produce an n-best list
of all possible translations, but here we need to
restrict ourselves to translations corresponding to
well-formed MRs. In principle this could be done
by re-writing the beam search algorithm used in
decoding to immediately discard malformed MRs;
for the experiments in this paper we simply filter
the regular n-best list until we find a well-formed
MR. This filtering can be done with time linear in
the length of the example by exploiting the argu-
ment label numbers introduced during lineariza-
tion. Finally, we insert the brackets according to
the tree structure specified by the argument num-
ber labels.
3 Experimental setup
Dataset We conduct experiments on the Geo-
Query data set. The corpus consists of a set of
880 natural-language questions about U.S. geog-
raphy in four languages (English, German, Greek
and Thai), and their representations in a variable-
free MRL that can be executed against a Prolog
database interface. Initial experimentation was
done using 10 fold cross-validation on the 600-
sentence development set and the final evaluation
on a held-out test set of 280 sentences. All seman-
tic parsers for GeoQuery we compare against also
makes use of NP lists (Jones et al, 2012), which
contain MRs for every noun phrase that appears in
the NL utterances of each language. In our exper-
iments, the NP list was included by appending all
entries as extra training sentences to the end of the
training corpus of each language with 50 times the
weight of regular training examples, to ensure that
they are learned as translation rules.
Evaluation for each utterance is performed by
executing both the predicted and the gold standard
MRs against the database and obtaining their re-
spective answers. An MR is correct if it obtains
the same answer as the gold standard MR, allow-
ing for a fair comparison between systems using
different learning paradigms. Following Jones et
al. (2012) we report accuracy, i.e. the percent-
age of NL questions with correct answers, and F1,
i.e. the harmonic mean of precision (percentage of
correct answers obtained).
Implementation In all experiments, we use the
IBM Model 4 implementation from the GIZA++
toolkit (Och and Ney, 2000) for alignment, and
the phrase-based and hierarchical models imple-
mented in the Moses toolkit (Koehn et al, 2007)
for rule extraction. The best symmetrization algo-
rithm, translation and language model weights for
each language are selected using cross-validation
on the development set. In the case of English and
German, we also found that stemming (Bird et al,
2009; Porter, 1980) was hepful in reducing data
sparsity.
4 Results
We first compare the results for the two translation
rule extraction models, phrase-based and hierar-
chical (?MT-phrase? and ?MT-hier? respectively
in Table 1). We find that the hierarchical model
performs better in all languages apart from Greek,
indicating that the long-range reorderings learned
by a hierarchical translation system are useful for
this task. These benefits are most pronounced in
the case of Thai, likely due to the the language?s
comparatively different word order.
We also present results for both models with-
out using the NP lists for training in Table 2. As
expected, the performances are almost uniformly
lower, but the parser still produces correct output
for the majority of examples.
As discussed above, one important modifica-
tion of the MT paradigm which allows us to pro-
duce structured output is the addition of structure-
checking to the beam search. It is not evident,
a priori, that this search procedure is guaran-
teed to find any well-formed outputs in reasonable
time; to test the effect of this extra requirement on
en de el th
MT-phrase 75.3 68.8 70.4 53.0
MT-phrase (-NP) 63.4 65.8 64.0 39.8
MT-hier 80.5 68.9 69.1 70.4
MT-hier (-NP) 62.5 69.9 62.9 62.1
Table 2: GeoQuery accuracies with and without
NPs. Rows with (-NP) did not use the NP list.
49
English [en] German [de] Greek [el] Thai [th]
Acc. F1 Acc. F1 Acc. F1 Acc. F1
WASP 71.1 77.7 65.7 74.9 70.7 78.6 71.4 75.0
UBL 82.1 82.1 75.0 75.0 73.6 73.7 66.4 66.4
tsVB 79.3 79.3 74.6 74.6 75.4 75.4 78.2 78.2
hybrid-tree 76.8 81.0 62.1 68.5 69.3 74.6 73.6 76.7
MT-phrase 75.3 75.8 68.8 70.8 70.4 73.0 53.0 54.4
MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7
Table 1: Accuracy and F1 scores for the multilingual GeoQuery test set. Results for other systems as
reported by Jones et al (2012).
the speed of SP, we investigate how many MRs
the decoder needs to generate before producing
one which is well-formed. In practice, increasing
search depth in the n-best list from 1 to 50 results
in a gain of no more than a percentage point or
two, and we conclude that our filtering method is
appropriate for the task.
We also compare the MT-based semantic
parsers to several recently published ones: WASP
(Wong and Mooney, 2006), which like the hier-
archical model described here learns a SCFG to
translate between NL and MRL; tsVB (Jones et
al., 2012), which uses variational Bayesian infer-
ence to learn weights for a tree transducer; UBL
(Kwiatkowski et al, 2010), which learns a CCG
lexicon with semantic annotations; and hybrid-
tree (Lu et al, 2008), which learns a synchronous
generative model over variable-free MRs and NL
strings.
In the results shown in Table 1 we observe that
on English GeoQuery data, the hierarchical trans-
lation model achieves scores competitive with the
state of the art, and in every language one of the
MT systems achieves accuracy at least as good as
a purpose-built semantic parser.
We conclude with an informal test of training
speeds. While differences in implementation and
factors like programming language choice make
a direct comparison of times necessarily impre-
cise, we note that the MT system takes less than
three minutes to train on the GeoQuery corpus,
while the publicly-available implementations of
tsVB and UBL require roughly twenty minutes and
five hours respectively on a 2.1 GHz CPU. So
in addition to competitive performance, the MT-
based parser also appears to be considerably more
efficient at training time than other parsers in the
literature.
5 Related Work
WASP, an early automatically-learned SP system,
was strongly influenced by MT techniques. Like
the present work, it uses GIZA++ alignments as
a starting point for the rule extraction procedure,
and algorithms reminiscent of those used in syn-
tactic MT to extract rules.
tsVB also uses a piece of standard MT ma-
chinery, specifically tree transducers, which have
been profitably employed for syntax-based ma-
chine translation (Maletti, 2010). In that work,
however, the usual MT parameter-estimation tech-
nique of simply counting the number of rule oc-
currences does not improve scores, and the au-
thors instead resort to a variational inference pro-
cedure to acquire rule weights. The present work
is also the first we are aware of which uses phrase-
based rather than tree-based machine translation
techniques to learn a semantic parser. hybrid-tree
(Lu et al, 2008) similarly describes a generative
model over derivations of MRL trees.
The remaining system discussed in this paper,
UBL (Kwiatkowski et al, 2010), leverages the fact
that the MRL does not simply encode trees, but
rather ?-calculus expressions. It employs resolu-
tion procedures specific to the ?-calculus such as
splitting and unification in order to generate rule
templates. Like other systems described, it uses
GIZA alignments for initialization. Other work
which generalizes from variable-free meaning rep-
resentations to ?-calculus expressions includes the
natural language generation procedure described
by Lu and Ng (2011).
UBL, like an MT system (and unlike most of the
other systems discussed in this section), extracts
rules at multiple levels of granularity by means of
this splitting and unification procedure. hybrid-
tree similarly benefits from the introduction of
50
multi-level rules composed from smaller rules, a
process similar to the one used for creating phrase
tables in a phrase-based MT system.
6 Discussion
Our results validate the hypothesis that it is possi-
ble to adapt an ordinary MT system into a work-
ing semantic parser. In spite of the compara-
tive simplicity of the approach, it achieves scores
comparable to (and sometimes better than) many
state-of-the-art systems. For this reason, we argue
for the use of a machine translation baseline as a
point of comparison for new methods. The results
also demonstrate the usefulness of two techniques
which are crucial for successful MT, but which are
not widely used in semantic parsing. The first is
the incorporation of a language model (or com-
parable long-distance structure-scoring model) to
assign scores to predicted parses independent of
the transformation model. The second is the
use of large, composed rules (rather than rules
which trigger on only one lexical item, or on tree
portions of limited depth (Lu et al, 2008)) in
order to ?memorize? frequently-occurring large-
scale structures.
7 Conclusions
We have presented a semantic parser which uses
techniques from machine translation to learn map-
pings from natural language to variable-free mean-
ing representations. The parser performs com-
parably to several recent purpose-built semantic
parsers on the GeoQuery dataset, while training
considerably faster than state-of-the-art systems.
Our experiments demonstrate the usefulness of
several techniques which might be broadly applied
to other semantic parsers, and provides an infor-
mative basis for future work.
Acknowledgments
Jacob Andreas is supported by a Churchill Schol-
arship. Andreas Vlachos is funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 270019 (SPACEBOOK project www.
spacebook-project.eu).
References
Steven Bird, Edward Loper, and Edward Klein.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Ann
Arbor, Michigan.
H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To
H.B. Curry: Essays on Combinatory Logic, Lambda
Calculus, and Formalism. Academic Press.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1486?1495, Portland, Oregon.
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesian tree transduc-
ers. In Proceedings of the 50th Annual Meeting of
the Association of Computational Linguistics, pages
488?496, Jeju, Korea.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
48?54, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch-
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180, Prague, Czech Republic.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, Mas-
sachusetts.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
51
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590?599, Port-
land, Oregon.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?11, pages 1611?
1622. Association for Computational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettle-
moyer. 2008. A generative model for parsing nat-
ural language to meaning representations. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 783?
792, Edinburgh, UK.
Andreas Maletti. 2010. Survey: Tree transducers
in machine translation. In Proceedings of the 2nd
Workshop on Non-Classical Models for Automata
and Applications, Jena, Germany.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th Inter-
national Conference on Intelligent User Interfaces,
pages 149?157, Santa Monica, CA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the 2006 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 439?446, New York.
John M. Zelle. 1995. Using Inductive Logic Program-
ming to Automate the Construction of Natural Lan-
guage Parsers. Ph.D. thesis, Department of Com-
puter Sciences, The University of Texas at Austin.
52
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 822?827,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How much do word embeddings encode about syntax?
Jacob Andreas and Dan Klein
Computer Science Division
University of California, Berkeley
{jda,klein}@cs.berkeley.edu
Abstract
Do continuous word embeddings encode
any useful information for constituency
parsing? We isolate three ways in which
word embeddings might augment a state-
of-the-art statistical parser: by connecting
out-of-vocabulary words to known ones,
by encouraging common behavior among
related in-vocabulary words, and by di-
rectly providing features for the lexicon.
We test each of these hypotheses with a
targeted change to a state-of-the-art base-
line. Despite small gains on extremely
small supervised training sets, we find
that extra information from embeddings
appears to make little or no difference
to a parser with adequate training data.
Our results support an overall hypothe-
sis that word embeddings import syntac-
tic information that is ultimately redun-
dant with distinctions learned from tree-
banks in other ways.
1 Introduction
This paper investigates a variety of ways in
which word embeddings might augment a con-
stituency parser with a discrete state space. Word
embeddings?representations of lexical items as
points in a real vector space?have a long history
in natural language processing, going back at least
as far as work on latent semantic analysis (LSA)
for information retrieval (Deerwester et al, 1990).
While word embeddings can be constructed di-
rectly from surface distributional statistics, as in
LSA, more sophisticated tools for unsupervised
extraction of word representations have recently
gained popularity (Collobert et al, 2011; Mikolov
et al, 2013a). Semi-supervised and unsupervised
models for a variety of core NLP tasks, includ-
ing named-entity recognition (Freitag, 2004), part-
of-speech tagging (Sch?utze, 1995), and chunking
(Turian et al, 2010) have been shown to benefit
from the inclusion of word embeddings as fea-
tures. In the other direction, access to a syntac-
tic parse has been shown to be useful for con-
structing word embeddings for phrases composi-
tionally (Hermann and Blunsom, 2013; Andreas
and Ghahramani, 2013). Dependency parsers have
seen gains from distributional statistics in the form
of discrete word clusters (Koo et al, 2008), and re-
cent work (Bansal et al, 2014) suggests that simi-
lar gains can be derived from embeddings like the
ones used in this paper.
It has been less clear how (and indeed whether)
word embeddings in and of themselves are use-
ful for constituency parsing. There certainly exist
competitive parsers that internally represent lexi-
cal items as real-valued vectors, such as the neural
network-based parser of Henderson (2004), and
even parsers which use pre-trained word embed-
dings to represent the lexicon, such as Socher et
al. (2013). In these parsers, however, use of word
vectors is a structural choice, rather than an added
feature, and it is difficult to disentangle whether
vector-space lexicons are actually more powerful
than their discrete analogs?perhaps the perfor-
mance of neural network parsers comes entirely
from the model?s extra-lexical syntactic structure.
In order to isolate the contribution from word em-
beddings, it is useful to demonstrate improvement
over a parser that already achieves state-of-the-art
performance without vector representations.
The fundamental question we want to explore
is whether embeddings provide any information
beyond what a conventional parser is able to in-
duce from labeled parse trees. It could be that
the distinctions between lexical items that embed-
dings capture are already modeled by parsers in
other ways and therefore provide no further bene-
fit. In this paper, we investigate this question em-
pirically, by isolating three potential mechanisms
for improvement from pre-trained word embed-
822
0 0.1 0.2 0.3 0.4 0.5 0.6
?0.4
?0.2
0
0.2
0.4
0.6
0.8
a
the
this
that
mostfew
each
every
Figure 1: Word representations of English de-
terminers, projected onto their first two principal
components. Embeddings from Collobert et al
(2011).
dings. Our result is mostly negative. With ex-
tremely limited training data, parser extensions us-
ing word embeddings give modest improvements
in accuracy (relative error reduction on the order
of 1.5%). However, with reasonably-sized training
corpora, performance does not improve even when
a wide variety of embedding methods, parser mod-
ifications, and parameter settings are considered.
The fact that word embedding features result
in nontrivial gains for discriminative dependency
parsing (Bansal et al, 2014), but do not appear to
be effective for constituency parsing, points to an
interesting structural difference between the two
tasks. We hypothesize that dependency parsers
benefit from the introduction of features (like clus-
ters and embeddings) that provide syntactic ab-
stractions; but that constituency parsers already
have access to such abstractions in the form of su-
pervised preterminal tags.
2 Three possible benefits of word
embeddings
We are interested in the question of whether
a state-of-the-art discrete-variable constituency
parser can be improved with word embeddings,
and, more precisely, what aspect (or aspects) of
the parser can be altered to make effective use of
embeddings.
It seems clear that word embeddings exhibit
some syntactic structure. Consider Figure 1,
which shows embeddings for a variety of English
determiners, projected onto their first two princi-
pal components. We can see that the quantifiers
each and every cluster together, as do few and
most. These are precisely the kinds of distinc-
tions between determiners that state-splitting in
the Berkeley parser has shown to be useful (Petrov
and Klein, 2007), and existing work (Mikolov et
al., 2013b) has observed that such regular em-
bedding structure extends to many other parts of
speech. But we don?t know how prevalent or
important such ?syntactic axes? are in practice.
Thus we have two questions: Are such groupings
(learned on large data sets but from less syntacti-
cally rich models) better than the ones the parser
finds on its own? How much data is needed to
learn them without word embeddings?
We consider three general hypotheses about
how embeddings might interact with a parser:
1. Vocabulary expansion hypothesis: Word
embeddings are useful for handling out-of-
vocabulary words, because they automati-
cally ensure that unknown words are treated
the same way as known words with similar
representations. Example: the infrequently-
occurring treebank tag UH dominates greet-
ings (among other interjections). Upon en-
countering the unknown word hey, the parser
assigns a low posterior probability of hav-
ing been generated from UH. But its distri-
butional representation is very close to the
known word hello, and a model capable of
mapping hey to its neighbor should be able to
assign the right tag.
2. Statistic sharing hypothesis: Word embed-
dings are useful for handling in-vocabulary
words, by making it possible to pool statistics
for related words. Example: individual first
names are also rare in the treebank, but tend
to cluster together in distributional represen-
tations. A parser which exploited this effect
could use this to acquire a robust model of
name behavior by sharing statistics from all
first names together, preventing low counts
from producing noisy models of names.
3. Embedding structure hypothesis: The
structure of the space used for the embed-
dings directly encodes syntactic information
in its coordinate axes. Example: with the
exception of a, the vertical axis in Figure 1
823
seems to group words by definiteness. We
would expect a feature corresponding to a
word?s position along this axis to be a useful
feature in a feature-based lexicon.
Note that these hypotheses are not all mutually
exclusive, and two or all of them might provide in-
dependent gains. Our first task is thus to design a
set of orthogonal experiments which make it pos-
sible to test each of the three hypotheses in isola-
tion. It is also possible that other mechanisms are
at play that are not covered by these three hypothe-
ses, but we consider these three to be likely central
effects.
3 Parser extensions
For the experiments in this paper, we will use
the Berkeley parser (Petrov and Klein, 2007) and
the related Maryland parser (Huang and Harper,
2011). The Berkeley parser induces a latent, state-
split PCFG in which each symbol V of the (ob-
served) X-bar grammar is refined into a set of
more specific symbols {V
1
, V
2
, . . .} which cap-
ture more detailed grammatical behavior. This
allows the parser to distinguish between words
which share the same tag but exhibit very differ-
ent syntactic behavior?for example, between ar-
ticles and demonstrative pronouns. The Maryland
parser builds on the state-splitting parser, replac-
ing its basic word emission model with a feature-
rich, log-linear representation of the lexicon.
The choice of this parser family has two moti-
vations. First, these parsers are among the best in
the literature, with a test performance of 90.7 F
1
for the baseline Berkeley parser on the Wall Street
Journal corpus (compared to 90.4 for Socher et al
(2013) and 90.1 for Henderson (2004)). Second,
and more importantly, the fact that they use no
continuous state representations internally makes
it easy to design experiments that isolate the con-
tributions of word vectors, without worrying about
effects from real-valued operators higher up in the
model. We consider the following extensions:
Vocabulary expansion ? OOV model
To evaluate the vocabulary expansion hypothe-
sis, we introduce a simple but targeted out-of-
vocabulary (OOV) model in which every unknown
word is simply replaced by its nearest neighbor in
the training set. For OOV words which are not in
the dictionary of embeddings, we back off to the
unknown word model for the underlying parser.
Statistic sharing ? Lexicon pooling model
To evaluate the statistic sharing hypothesis, we
propose a novel smoothing technique. The Berke-
ley lexicon stores, for each latent (tag, word) pair,
the probability p(w|t) directly in a lookup ta-
ble. If we want to encourage similarly-embedded
words to exhibit similar behavior in the generative
model, we need to ensure that the are preferen-
tially mapped onto the same latent preterminal tag.
In order to do this, we replace this direct lookup
with a smoothed, kernelized lexicon, where:
p(w|t) =
1
Z
?
w
?
?
t,w
?
e
??||?(w)??(w
?
)||
2
(1)
with Z a normalizing constant to ensure that p(?|t)
sums to one over the entire vocabulary. ?(w) is the
vector representation of the word w, ?
t,w
are per-
basis weights, and ? is an inverse radius parame-
ter which determines the strength of the smooth-
ing. Each ?
t,w
is learned in the same way as
its corresponding probability in the original parser
model?during each M step of the training proce-
dure, ?
w,t
is set to the expected number of times
the word w appears under the refined tag t. Intu-
itively, as ? grows small groups of related words
will be assigned increasingly similar probabilities
of being generated from the same tag (in the limit
where ? = 0, Equation 1 is a uniform distribu-
tion over the entire vocabulary). As ? grows large
words become more independent (and in the limit
where ? = ?, each summand in Equation 1 is
zero except where w
?
= w, and we recover the
original direct-lookup model).
There are computational concerns associated
with this approach: the original scoring procedure
for a (word, tag) pair was a single (constant-time)
lookup; here it might take time linear in the size
of the vocabulary. This causes parsing to become
unacceptably slow, so an approximation is neces-
sary. Luckily, the exponential decay of the kernel
ensures that each word shares most of its weight
with a small number of close neighbors, and al-
most none with words farther away. To exploit
this, we pre-compute the k-nearest-neighbor graph
of points in the embedding space, and take the sum
in Equation 1 only over this set of nearest neigh-
bors. Empirically, taking k = 20 gives adequate
performance, and increasing it does not seem to
alter the behavior of the parser.
As in the OOV model, we also need to worry
about how to handle words for which we have no
824
vector representation. In these cases, we simply
treat the words as if their vectors were so far away
from everything else they had no influence, and
report their weights as p(w|t) = ?
w
. This ensures
that our model continues to include the original
Berkeley parser model as a limiting case.
Embedding structure ? embedding features
To evaluate the embedding structure hypothesis,
we take the Maryland featured parser, and replace
the set of lexical template features used by that
parser with a set of indicator features on a dis-
cretized version of the embedding. For each di-
mension i, we create an indicator feature corre-
sponding to the linearly-bucketed value of the fea-
ture at that index. In order to focus specifically
on the effect of word embeddings, we remove the
morphological features from the parser, but retain
indicators on the identity of each lexical item.
The extensions we propose are certainly not
the only way to target the hypotheses described
above, but they have the advantage of being min-
imal and straightforwardly interpretable, and each
can be reasonably expected to improve parser per-
formance if its corresponding hypothesis is true.
4 Experimental setup
We use the Maryland implementation of the
Berkeley parser as our baseline for the kernel-
smoothed lexicon, and the Maryland featured
parser as our baseline for the embedding-featured
lexicon.
1
For all experiments, we use 50-
dimensional word embeddings. Embeddings la-
beled C&W are from Collobert et al (2011); em-
beddings labeled CBOW are from Mikolov et al
(2013a), trained with a context window of size 2.
Experiments are conducted on the Wall Street
Journal portion of the English Penn Treebank. We
prepare three training sets: the complete training
set of 39,832 sentences from the treebank (sec-
tions 2 through 21), a smaller training set, consist-
ing of the first 3000 sentences, and an even smaller
set of the first 300.
Per-corpus-size settings of the parameter ? are
set by searching over several possible settings on
the development set. For each training corpus size
we also choose a different setting of the number of
splitting iterations over which the Berkeley parser
is run; for 300 sentences this is two splits, and for
1
Both downloaded from https://code.google.
com/p/umd-featured-parser/
Model 300 3000 Full
Baseline 71.88 84.70 91.13
OOV (C&W) 72.20 84.77 91.22
OOV (CBOW) 72.20 84.78 91.22
Pooling (C&W) 72.21 84.55 91.11
Pooling (CBOW) 71.61 84.73 91.15
Features (ident) 67.27 82.77 90.65
Features (C&W) 70.32 83.78 91.08
Features (CBOW) 69.87 84.46 90.86
Table 1: Contributions from OOV, lexical pooling
and featured models, for two kinds of embeddings
(C&W and CBOW). For both choices of embed-
ding, the pooling and OOV models provide small
gains with very little training data, but no gains
on the full training set. The featured model never
achieves scores higher than the generative base-
line.
Model 300 3000 Full
Baseline 72.02 84.09 90.70
Pool + OOV (C&W) 72.43
?
84.36
?
90.11
Table 2: Test set experiments with the best com-
bination of models (based on development exper-
iments). Again, we observe small gains with re-
stricted training sets but no gains on the full train-
ing set. Entries marked
?
are statistically signifi-
cant (p < 0.05) under a paired bootstrap resam-
pling test.
3000 four splits. This is necessary to avoid over-
fitting on smaller training sets. Consistent with the
existing literature, we stop at six splits when using
the full training corpus.
5 Results
Various model-specific experiments are shown in
Table 1. We begin by investigating the OOV
model. As can be seen, this model alone achieves
small gains over the baseline for a 300-word train-
ing corpus, but these gains become statistically in-
significant with more training data. This behavior
is almost completely insensitive to the choice of
embedding.
Next we consider the lexicon pooling model.
We began by searching over exponentially-spaced
values of ? to determine an optimal setting for
825
Experiment WSJ ? Brown French
Baseline 86.36 74.84
Pool + OOV 86.42 75.18
Table 3: Experiments for other corpora, using the
same combined model (lexicon pooling and OOV)
as in Table 2. Again, we observe no significant
gains over the baseline.
each training set size; as expected, for small set-
tings of ? (corresponding to aggressive smooth-
ing) performance decreased; as we increased the
parameter, performance increased slightly before
tapering off to baseline parser performance. The
first block in Table 1 shows the best settings of ?
for each corpus size; as can be seen, this also gives
a small improvement on the 300-sentence training
corpus, but no discernible once the system has ac-
cess to a few thousand labeled sentences.
Last we consider a model with a featured lex-
icon, as described in Huang and Harper (2011).
A baseline featured model (?ident?) contains only
indicator features on word identity (and performs
considerably worse than its generative counter-
part on small data sets). As described above, the
full featured model adds indicator features on the
bucketed value of each dimension of the word em-
bedding. Here, the trend observed in the other two
models is even more prominent?embedding fea-
tures lead to improvements over the featured base-
line, but in no case outperform the standard base-
line with a generative lexicon.
We take the best-performing combination of all
of these models (based on development experi-
ments, a combination of the lexical pooling model
with ? = 0.3, and OOV, both using C&W word
embeddings), and evaluate this on the WSJ test
set (Table 2). We observe very small (but statis-
tically significant) gains with 300 and 3000 train
sentences, but a decrease in performance on the
full corpus.
To investigate the possibility that improvements
from embeddings are exceptionally difficult to
achieve on the Wall Street Journal corpus, or on
English generally, we perform (1) a domain adap-
tation experiment, in which we use the OOV and
lexicon pooling models to train on WSJ and test
on the first 4000 sentences of the Brown corpus
(the ?WSJ ? Brown? column in Table 3), and (2)
a multilingual experiment, in which we train and
test on the French treebank (the ?French? column).
Apparent gains from the OOV and lexicon pooling
models remain so small as to be statistically indis-
tinguishable.
6 Conclusion
With the goal of exploring how much useful syn-
tactic information is provided by unsupervised
word embeddings, we have presented three vari-
ations on a state-of-the-art parsing model, with
extensions to the out-of-vocabulary model, lexi-
con, and feature set. Evaluation of these modi-
fied parsers revealed modest gains on extremely
small training sets, which quickly vanish as train-
ing set size increases. Thus, at least restricted to
phenomena which can be explained by the exper-
iments described here, our results are consistent
with two claims: (1) unsupervised word embed-
dings do contain some syntactically useful infor-
mation, but (2) this information is redundant with
what the model is able to determine for itself from
only a small amount of labeled training data.
It is important to emphasize that these results
do not argue against the use of continuous repre-
sentations in a parser?s state space, nor argue more
generally that constituency parsers cannot possi-
bly benefit from word embeddings. However, the
failure to uncover gains when searching across a
variety of possible mechanisms for improvement,
training procedures for embeddings, hyperparam-
eter settings, tasks, and resource scenarios sug-
gests that these gains (if they do exist) are ex-
tremely sensitive to these training conditions, and
not nearly as accessible as they seem to be in de-
pendency parsers. Indeed, our results suggest a
hypothesis that word embeddings are useful for
dependency parsing (and perhaps other tasks) be-
cause they provide a level of syntactic abstrac-
tion which is explicitly annotated in constituency
parses. We leave explicit investigation of this hy-
pothesis for future work.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014. The first
author is supported by a National Science Foun-
dation Graduate Research Fellowship.
826
References
Jacob Andreas and Zoubin Ghahramani. 2013. A gen-
erative model of vector space semantics. In Pro-
ceedings of the ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, GeorgeW. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904, Sofia, Bulgaria, August.
Zhongqiang Huang and Mary P. Harper. 2011.
Feature-rich log-linear lexical model for latent vari-
able pcfg grammars. In Proceedings of the Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 219?227.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 595?
603.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 746?751.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics. Assocation for Computational
Linguistics.
Hinrich Sch?utze. 1995. Distributional part-of-speech
tagging. In Proceedings of the European Associa-
tion for Computational Linguistics, pages 141?148.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
827
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 13?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Corpus Creation for New Genres:
A Crowdsourced Approach to PP Attachment
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.edu
Abstract
This paper explores the task of building an ac-
curate prepositional phrase attachment corpus
for new genres while avoiding a large invest-
ment in terms of time and money by crowd-
sourcing judgments. We develop and present
a system to extract prepositional phrases and
their potential attachments from ungrammati-
cal and informal sentences and pose the subse-
quent disambiguation tasks as multiple choice
questions to workers from Amazon?s Mechan-
ical Turk service. Our analysis shows that
this two-step approach is capable of producing
reliable annotations on informal and poten-
tially noisy blog text, and this semi-automated
strategy holds promise for similar annotation
projects in new genres.
1 Introduction
Recent decades have seen rapid development in nat-
ural language processing tools for parsing, semantic
role-labeling, machine translation, etc., and much of
this success can be attributed to the study of statisti-
cal techniques and the availability of large annotated
corpora for training. However, the performance of
these systems is heavily dependent on the domain
and genre of their training data, i.e. systems trained
on data from a particular domain tend to perform
poorly when applied to other domains and adap-
tation techniques are not always able to compen-
sate (Dredze et al, 2007). For this reason, achiev-
ing high performance on new domains and genres
frequently necessitates the collection of annotated
training data from those domains and genres, a time-
consuming and frequently expensive process.
This paper examines the problem of collecting
high-quality annotations for new genres with a focus
on time and cost efficiency. We explore the well-
studied but non-trivial task of prepositional phrase
(PP) attachment and describe a semi-automated sys-
tem for identifying accurate attachments in blog
data, which is frequently noisy and difficult to parse.
PP attachment disambiguation involves finding a
correct attachment for a prepositional phrase in a
sentence. For example, in the sentence ?We went to
John?s house on Saturday?, the phrase ?on Satur-
day? attaches to the verb ?went?. In another exam-
ple, ?We went to John?s house on 12th Street?, the
PP ?on 12th street? attaches to the noun ?John?s
house?. This sort of disambiguation requires se-
mantic knowledge about sentences that is difficult
to glean from their surface form, a problem which
is compounded by the informal nature and irregular
vocabulary of blog text.
In this work, we investigate whether crowd-
sourced human judgments are capable of distin-
guishing appropriate attachments. We present a sys-
tem that simplifies the attachment problem and rep-
resents it in a format that can be intuitively tackled
by humans.
Our approach to this task makes use of a heuristic-
based system built on a shallow parser that identi-
fies the likely words or phrases to which a PP can
attach. To subsequently select the correct attach-
ment, we leverage human judgments from multi-
ple untrained annotators (referred to here as work-
ers) through Amazon?s Mechanical Turk 1, an online
marketplace for work. This two-step approach of-
1http://www.mturk.amazon.com
13
fers distinct advantages: the automated system cuts
down the space of potential attachments effectively
with little error, and the disambiguation task can be
reduced to small multiple choice questions which
can be tackled quickly and aggregated reliably.
The remainder of this paper focuses on the PP at-
tachment task over blog text and our analysis of the
resulting aggregate annotations. We note, however,
that this type of semi-automated approach is poten-
tially applicable to any task which can be reliably
decomposed into independent judgments that un-
trained annotators can tackle (e.g., quantifier scop-
ing, conjunction scope). This work is intended as
an initial step towards the development of efficient
hybrid annotation tools that seamlessly incorporate
aggregate human wisdom alongside effective algo-
rithms.
2 Related Work
Identifying PP attachments is an essential task for
building syntactic parse trees. While this task has
been studied using fully-automated systems, many
of them rely on parse tree output for predicting po-
tential attachments (Ratnaparkhi et al, 1994; Yeh
and Vilain, 1998; Stetina and Nagao, 1997; Zavrel
et al, 1997). However, systems that rely on good
parses are unlikely to perform well on new genres
such as blogs and machine translated texts for which
parse tree training data is not readily available.
Furthermore, the predominant dataset for eval-
uating PP attachment is the RRR dataset (Ratna-
parkhi et al, 1994) which consists of PP attach-
ment cases from the Wall Street Journal portion of
the Penn Treebank. Instead of complete sentences,
this dataset consists of sets of the form {V,N1,P,N2}
where {P,N2} is the PP and {V,N1} are the poten-
tial attachments. This simplification of the PP at-
tachment task to a choice between two alternatives
is unrealistic when considering the potential long-
distance attachments encountered in real-world text.
While blogs and other web text, such as discus-
sion forums and emails, have been studied for a va-
riety of tasks such as information extraction (Hong
and Davison, 2009), social networking (Gruhl et
al., 2004), and sentiment analysis (Leshed and
Kaye, 2006), we are not aware of any previous ef-
forts to gather syntactic data (such as PP attach-
ments) in the genre. Syntactic methods such as
POS tagging, parsing and structural disambiguation
are commonly used when analyzing well-structured
text. Including the use of syntactic information
has yielded improvements in accuracy in speech
recognition (Chelba and Jelenik, 1998; Collins et
al., 2005) and machine translation (DeNeefe and
Knight, 2009; Carreras and Collins, 2009). We an-
ticipate that datasets such as ours could be useful for
such tasks as well.
Amazon?s Mechanical Turk (MTurk) has become
very popular for manual annotation tasks and has
been shown to perform equally well over labeling
tasks such as affect recognition, word similarity, rec-
ognizing textual entailment, event temporal order-
ing and word sense disambiguation, when compared
to annotations from experts (Snow et al, 2008).
While these tasks were small in scale and intended to
demonstrate the viability of annotation via MTurk,
it has also proved effective in large-scale tasks in-
cluding the collection of accurate speech transcrip-
tions (Gruenstein et al, 2009). In this paper we ex-
plore a method for corpus building on a large scale
in order to extend annotation into new domains and
genres.
We previously evaluated crowdsourced PP attach-
ment annotation by using MTurk workers to repro-
duce PP attachments from the Wall Street Journal
corpus (Rosenthal et al, 2010). The results demon-
strated that MTurk workers are capable of identi-
fying PP attachments in newswire text, but the ap-
proach used to generate attachment options is de-
pendent on the existing gold-standard parse trees
and cannot be used on corpora where parse trees are
not available. In this paper, we build on the semi-
automated annotation principle while avoiding the
dependency on parsers, allowing us to apply this
technique to the noisy and informal text found in
blogs.
3 System Description
Our system must both identify PPs and generate a
list of potential attachments for each PP in this sec-
tion. Figure 1 illustrates the structure of the system.
First, the system extracts sentences from scraped
blog data. Text is preprocessed by stripping HTML
tags, advertisements, non-Latin and non-printable
14
PPs
PPs
Question 
 Builder
PP Identifier
Chunker
+
Preprocessor
sentences
Chunked 
Chunked 
sentences
point predictor
Attachment
attachments
Potential
Mechanical
     Turk
Questions
forNew domain
data (Blogs)
Figure 1: Overview of question generation system
characters. Emoticon symbols are removed using a
standard list. 2
The cleaned data is then partitioned into sentences
using the NLTK sentence splitter. 3 In order to
compensate for the common occurrence of informal
punctuation and web-specific symbols in blog text,
we replace all punctuation symbols between quo-
tation marks and parentheses with placeholder tags
(e.g. ?QuestionMark?) during the sentence splitting
process and do the same for website names, time
markers and referring phrases (e.g. @John). Ad-
ditionally, we attempt to re-split sentences at ellipsis
boundaries if they are longer than 80 words and dis-
card them if this fails.
As parsers trained on news corpora tend to per-
form poorly on unstructured texts like blogs, we
rely on a chunker to partition sentences into phrases.
Choosing a good chunker is essential to this ap-
proach: around 35% of the cases in which the cor-
rect attachment is not predicted by the system are
due to chunker error. We experimented with differ-
ent chunkers over a random sample of 50 sentences
before selecting a CRF-based chunker (Phan, 2006)
for its robust performance.
The chunker output is initially processed by fus-
ing together chunks in order to ensure that a single
chunk represents a complete attachment point. Two
consecutive NP chunks are fused if the first contains
an element with a possessive part of speech tag (e.g.
John?s book), while particle chunks (PRT) are fused
with the VP chunks that precede them (e.g. pack
up). These chunked sentences are then processed
to identify PPs and potential attachment points for
them, which can then be used to generate questions
2http://www.astro.umd.edu/?marshall/
smileys.html
3http://www.nltk.org
for MTurk workers.
3.1 PP Extraction
PPs can be classified into two broad categories based
on the number of chunks they contain. A simple
PP consists of only two chunks: a preposition and
one noun phrase, while a compound PP has multi-
ple simple PPs attached to its primary noun phrase.
For example, in the sentence ?I just made some last-
minute changes to the latest issue of our newsletter?,
the PP with preposition ?to? can be considered to be
either the simple PP ?to the latest issue? or the com-
pound PP ?to the latest issue of our newsletter?.
We handle compound PPs by breaking them down
into multiple simple PPs; compound PPs can be re-
covered by identifying the attachments of their con-
stituent simple PPs. Our simple PP extraction al-
gorithm identifies PPs as a sequence of chunks that
consist of one or more prepositions terminating in a
noun phrase or gerund.
3.2 Attachment Point Prediction
A PP usually attaches to the noun or verb phrase pre-
ceding it or, in some cases, can modify a following
clause by attaching to the head verb. We build a set
of rules based on this intuition to pick out the poten-
tial attachments in the sentence; these rules are de-
scribed in Table 1. The rules are applied separately
for each PP in a sentence and in the same sequence
as mentioned in the table (except for rule 4, which
is applied while choosing a chunk using any of the
other rules).
15
Rule Example
1 Choose closest NP and VP preceding the PP. I made modifications to our newsletter.
2 Choose next closest VP preceding the PP if the VP selected in (1)
contains a VBG.
He snatched the disk flying away with one hand.
3 Choose first VP following the PP. On his desk he has a photograph.
4 All chunks inside parentheses are skipped, unless the PP falls within
parentheses.
Please refer to the new book (second edition) for
more notes.
5 Choose anything immediately preceding the PP that is not out of
chunk and has not already been picked.
She is full of excitement.
6 If a selected NP contains the word and, expand it into two options,
one with the full expression and one with only the terms following
and.
He is president and chairman of the board.
7 For PPs in chains of the form P-NP-P-NP (PP-PP), choose all the
NPs in the chain preceding the PP and apply all the above rules
considering the whole chain as a single PP.
They found my pictures of them from the concert.
8 If there are fewer than four options after applying the above rules,
also select the VP preceding the last VP selected, the NP preceding
the last NP selected, and the VP following the last VP picked.
Table 1: List of rules for attachment point predictor. In the examples, PPs are denoted by boldfaced text and potential
attachment options are underlined.
4 Experiments
An experimental study was undertaken to test our
hypothesis that we could obtain reliable annotations
on informal genres using MTurk workers. Here we
describe the dataset and our methods.
4.1 Dataset and Interface
We used a corpus of blog posts made on LiveJour-
nal 4 for system development and evaluation. Only
posts from English-speaking countries (i.e. USA,
Canada, UK, Australia and New Zealand) were con-
sidered for this study.
The interface provided to MTurk workers showed
the sentence on a plain background with the PP high-
lighted and a statement prompting them to pick the
phrase in the sentence that the given PP modified.
The question was followed by a list of options. In
addition, we provided MTurk workers the option to
indicate problems with the given PP or the listed op-
tions. Workers could write in the correct attachment
if they determined that it wasn?t present in the list of
options, or the correct PP if the one they were pre-
sented with was malformed. This allowed them to
correct errors made by the chunker and automated
attachment point predictor. In all cases, workers
were forced to pick the best answer among the op-
tions regardless of errors. We also supplied a num-
4http://www.livejournal.com
ber of examples covering both well-formed and er-
roneous cases to aid them in identifying appropriate
attachments.
4.2 Experimental Setup
For our experiment, we randomly selected 1000
questions from the output produced by the system
and provided each question to five different MTurk
workers, thereby obtaining five different judgments
for each PP attachment case. Workers were paid four
cents per question and the average completion time
per task was 48 seconds. In total $225 was spent
on the full study with $200 spent on the workers and
$25 on MTurk fees.The total time taken for the study
was approximately 16 hours.
A pilot study was carried out with 50 sentences
before the full study to test the annotation interface
and experiment with different ways of presenting the
PP and attachment options to workers. During this
study, we observed that while workers were will-
ing to suggest correct answers or PPs when faced
with erroneous questions, they often opted to not
pick any of the options provided unless the question
was well-formed. This was problematic because, in
many cases, expert annotators were able to identify
the most appropriate attachment option. Therefore,
in the final study we forced them to pick the most
suitable option from the given choices before indi-
cating errors and writing in alternatives.
16
Workers in agreement Number of questions Accuracy Coverage
5 (unanimity) 389 97.43% 41.33%
? 4 (majority) 689 94.63% 73.22%
? 3 (majority) 887 88.61% 94.26%
? 2 (plurality) 906 87.75% 96.28%
Total 941 84.48% 100%
Table 2: Accuracy and coverage over agreement thresholds
5 Evaluation corpus
In order to determine if the MTurk results were re-
liable, worker responses had to be validated by hav-
ing expert annotators perform the same task. For
this purpose, two of the authors annotated the 1000
questions used for the experiment independently and
compared their judgments. Disagreements were ob-
served in 127 cases; these were then resolved by a
pool of non-author annotators. If all three annota-
tors on a case disagreed with each other the question
was discarded; this situation occured 43 times. An
additional 16 questions were discarded because they
did not have a valid PP. For example, ?I am painting
with my blanket on today?. Here ?on today? is in-
correctly extracted as a PP because the particle ?on?
is tagged as a preposition. The rest of the analysis
presented in this section was performed on the re-
maining 941 sentences.
The annotators? judgments were compared to the
answers provided by the MTurk workers and, in
the case of disagreement between the experts and
the majority of workers, the sentences were man-
ually inspected to determine the reason. In five
cases, more than one valid attachment was possi-
ble; for example, in the sentence ?The video below is
of my favourite song on the album - A Real Woman?,
the PP ?of my favourite song? could attach to either
the noun phrase ?the video? or the verb ?is? and con-
veys the same meaning. In such cases, both the ex-
perts and the workers were considered to have cho-
sen the correct answer.
In 149 cases, the workers also augmented their
choices by providing corrections to incomplete an-
swers and badly constructed PPs. For example,
the PP ?of the Rings and Mikey? in the sentence
?Samwise from Lord of the Rings and Mikey from
The Goonies are the same actor ?? was corrected to
?of the Rings?. In 34/39 of the cases where the cor-
rect answer was not present in the options provided,
at least one worker indicated correct attachment for
the PP.
5.1 Attachment Prediction Evaluation
We measure the recall for our attachment point pre-
dictor as the number of questions for which the cor-
rect attachment appeared among the generated op-
tions divided by the total number of questions. The
system achieves a recall of 95.85% (902/941 ques-
tions). We observed that in many cases where the
correct attachment point was not predicted, it was
due to a chunker error. For example, in the following
sentence, ?Stop all the clocks , cut off the telephone
, Prevent the dog from barking with a juicy bone...?,
the PP ?from barking? attaches to the verb ?Pre-
vent?; however, due to an error in chunking ?Pre-
vent? is tagged as a noun phrase and hence is not
picked by our system. The correct attachment was
also occasionally missed when the attachment point
was too far from the PP. For example, in the sentence
?Fitting as many people as possible on one sofa and
under many many covers and getting intimate?, the
correct attachment for the PP ?under many many
covers? is the verb ?Fitting? but it is not picked by
our system.
Even though the correct attachment was not al-
ways given, the workers could still provide their own
correct answer. In the first example above, 3/5 work-
ers indicated that the correct attachment was not in
the list of options and wrote it in.
6 Results
Table 2 summarizes the results of the experiment.
We assess both the coverage and reliability of
worker predictions at various levels of worker agree-
ment. This serves as an indicator of the effective-
ness of the MTurk results: the accuracy can be taken
17
Figure 2: The number of questions in which exactly x
workers provided the correct answer
as a general confidence measure for worker predic-
tions; when five workers agree we can be 97.43%
confident in the correctness of their prediction, when
at least four workers agree we can be 94.63% con-
fident, etc. Unanimity indicates that all workers
agreed on an answer, majority indicates that more
than half of workers agreed on an answer, and plu-
rality indicates that two workers agreed on a single
answer, while the remaining three workers each se-
lected different answers. We observe that at high
levels of worker agreement, we get extremely high
accuracy but limited coverage of the data set; as
we decrease our standard for agreement, coverage
increases rapidly while accuracy remains relatively
high.
Figure 2 shows the number of workers providing
the correct answer on a per-question basis. This
illustrates the distribution of worker agreements
across questions. Note that in the majority of cases
(69.2%), at least four workers provided the correct
answer; in only 3.6% of cases were no workers able
to select the correct attachment.
Figure 3 shows the distribution of worker agree-
ments. Unlike Table 2, these figures are not cumu-
lative and include non-plurality two-worker agree-
ments. Note that the number of agreements dis-
cussed in this figure is greater than the 941 evaluated
because in some cases there were multiple agree-
ments on a single question. As an example, three
workers may choose one answer while the remain-
ing two workers choose another; this question then
produces both a three-worker agreement as well as a
two-worker agreement.
Figure 3: The number of cases in which exactly x work-
ers agreed on an answer
No. of options No. of cases Accuracy
< 4 179 86.59%
4 718 84.26%
> 4 44 79.55%
Table 3: Variation in worker performance with the num-
ber of attachment options presented
All questions on which there is agreement also
produce a majority vote, with one exception: the
2/2/1 agreement. Although the correct answer was
selected by one set of two workers in every case of
2/2/1 agreement, this is not particularly useful for
corpus-building as we have no way to identify a pri-
ori which set is correct. Fortunately, 2/2/1 agree-
ments were also quite rare and occurred in only 3%
of cases.
Figure 3 appears to indicate that instances of
agreement between two workers are unlikely to pro-
duce good attachments; they have a an average ac-
curacy of 37.2%. However, this is due in large part
to cases of 3/2 agreement, in which the two workers
in the minority are usually wrong, as well as cases of
2/2/1 agreement which contain at least one incorrect
instance of two-worker agreement. However, if we
only consider cases in which the two-worker agree-
ment forms a plurality (i.e. all other workers dis-
agree amongst themselves), we observe an average
accuracy of 64.3% which is similar to that of cases
of three-worker agreement (67.7%).
We also attempted to study the variation in worker
performance based on the complexity of the task;
specifically looking at how response accuracy var-
ied depending on the number of options that workers
were presented with. Although our system aimed to
18
Figure 4: Variation in accuracy with sentence length.
generate four attachment options per case, fewer op-
tions were produced for small sentences and opening
PPs while additional options were generated in sen-
tences containing PP-NP chains (see Table 1 for the
complete list of rules). Table 3 shows the variation in
accuracy with the number of options provided to the
workers. We might expect that an increased number
of options may be correlated with decreased accu-
racy and the data does indeed seem to suggest this
trend; however, we do not have enough datapoints
for the cases with fewer or more than four options to
verify whether this effect is significant.
We also analyzed the relationship between the
length of the sentence (in terms of number of words)
and the accuracy. Figure 4 indicates that as the
length of the sentence increases, the average accu-
racy decreases. This is not entirely unexpected as
lengthy sentences tend to be more complicated and
therefore harder for human readers to parse.
7 Conclusions and Future Work
We have shown that by working in conjunction
with automated attachment point prediction sys-
tems, MTurk workers are capable of annotating PP
attachment problems with high accuracy, even when
working with unstructured and informal blog text.
This work provides an immediate framework for the
building of PP attachment corpora for new genres
without a dependency on full parsing.
More broadly, the semi-automated framework
outlined in this paper is not limited to the task of
annotating PP attachments; indeed, it is suitable for
almost any syntactic or semantic annotation task
where untrained human workers can be presented
with a limited number of options for selection. By
dividing the desired annotation task into smaller
sub-tasks that can be tackled independently or in a
pipelined manner, we anticipate that more syntac-
tic information can be extracted from unstructured
text in new domains and genres without the sizable
investment of time and money normally associated
with hiring trained linguists to build new corpora.
To this end, we intend to further leverage the advent
of crowdsourcing resources in order to tackle more
sophisticated annotation tasks.
Acknowledgements
The authors would like to thank Kevin Lerman for
his help in formulating the original ideas for this
work. This material is based on research supported
in part by the U.S. National Science Foundation
(NSF) under IIS-05-34871. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of EMNLP, pages 200?209.
Ciprian Chelba and Frederick Jelenik. 1998. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of ACL, pages
507?514.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
EMNLP, pages 727?736.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL, pages 1051?1055,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Gruenstein, Ian McGraw, and Andrew Sutherland.
2009. A self-transcribing speech corpus: collecting
continuous speech with an online educational game.
In Proceedings of the Speech and Language Technol-
ogy in Education (SLaTE) Workshop.
19
Figure 5: HIT Interface for PP attachment task
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of WWW, pages 491?501.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering in
discussion boards. In Proceedings of SIGIR, pages
171?178.
Gilly Leshed and Joseph ?Jofish? Kaye. 2006. Under-
standing how bloggers feel: recognizing affect in blog
posts. In CHI ?06 extended abstracts on Human fac-
tors in computing systems, pages 1019?1024.
Xuan-Hieu Phan. 2006. CRFChunker: CRF
English phrase chunker. http://crfchunker.
sourceforge.net.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of HLT, pages 250?
255.
Sara Rosenthal, William J. Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. Semi-
automated annotation for prepositional phrase attach-
ment. In Proceedings of LREC, Valletta, Malta.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP
attachment ambiguity resolution with a semantic dic-
tionary. In Proceedings of the Workshop on Very Large
Corpora, pages 66?80.
Alexander S. Yeh and Marc B. Vilain. 1998. Some prop-
erties of preposition and subordinate conjunction at-
tachments. In Proceedings of COLING, pages 1436?
1442.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997. Resolving PP attachment ambiguities with
memory-based learning. In Proceedings of the Work-
shop on Computational Language Learning (CoNLL),
pages 136?144.
Appendix A: Mechanical Turk Interface
Figure 5 shows a screenshot of the interface pro-
vided to the Mechanical Turk workers for the PP at-
tachment task. By default, examples and additional
options are hidden but can be viewed using the links
provided. The screenshot illustrates a case in which
a worker is confronted with an incorrect PP and uses
the additional options to correct it.
20
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 227?236,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Fuzzy Syntactic Reordering
for Phrase-based Statistical Machine Translation
Jacob Andreas and Nizar Habash and Owen Rambow
Center for Computational Learning Systems
Columbia University
jda2129@columbia.edu
{habash,rambow}@ccls.columbia.edu
Abstract
The quality of Arabic-English statistical ma-
chine translation often suffers as a result of
standard phrase-based SMT systems? inabil-
ity to perform long-range re-orderings, specif-
ically those needed to translate VSO-ordered
Arabic sentences. This problem is further ex-
acerbated by the low performance of Arabic
parsers on subject and subject span detection.
In this paper, we present two parse ?fuzzi-
fication? techniques which allow the transla-
tion system to select among a range of pos-
sible S?V re-orderings. With this approach,
we demonstrate a 0.3-point improvement in
BLEU score (69% of the maximum possible
using gold parses), and a corresponding im-
provement in the percentage of syntactically
well-formed subjects under a manual evalua-
tion.
1 Introduction
The question of how to effectively use phrase-based
statistical machine translation (PSMT) to translate
between language pairs which require long-range re-
ordering has attracted a great deal of interest in re-
cent years. The inability to capture long-range re-
ordering behaviors is a weakness inherent in PSMT
systems, which typically have only two mechanisms
to control the reordering between source and tar-
get language: (1) distortion penalties, which penal-
ize or forbid long-distance re-orderings in order to
reduce the search space explored by the decoder,
and (2) lexicalized reordering models, which cap-
ture the preferences of individual phrases to orient
themselves monotonically, reversed with their pre-
ceding phrases or discontinuously. Because both
of these mechanisms work at the phrase level, they
have proven very effective at capturing short-range
reordering behaviors, but unable to describe long
range movements; in fact, the distortion penalty ef-
fectively causes the translation system to not pre-
fer long-range re-orderings, even when they are as-
signed significantly higher probability by the lan-
guage model.
The problem is particularly acute in translating
from Arabic to English: Arabic sentences frequently
exhibit a VSO ordering (both VSO and SVO are
permitted in Arabic), while English permits only
an SVO order. Past research has shown that verb
anticipation and subject-span detection is a ma-
jor source of error when translating from Arabic
to English (Green et al, 2009; Bisazza and Fed-
erico, 2010). Unable to perform long-range reorder-
ing, PSMT frequently produces English sentences in
which verbs precede their subjects (sometimes with
?hallucinated? pronouns in front of them) or do not
appear at all. Intuitively, better handling of these re-
orderings has the potential to improve both accuracy
and fluency of translation.
In this paper, we present two parse fuzzification
techniques which allow the translation system to se-
lect among a range of possible S?V re-orderings.
With this approach, we demonstrate a 0.3-point im-
provement in BLEU score (69% of the maximum
possible using gold parses), and a corresponding im-
provement in the percentage of syntactically well-
formed subjects under a manual evaluation.
The rest of the paper is structured as follows. Sec-
tion 2 gives a review of research on this topic. Sec-
tion 3 motivates the approach discussed in Section 4.
227
Section 5 presents the results of a set of machine
translation experiments using the automatic metrics
BLEU (Papineni et al, 2002) and METEOR (Baner-
jee and Lavie, 2005), and a manual-evaluation of
subject integrity. Section 6 discusses our conclu-
sions and future plans.
2 Related Work
The general approach pursued in this paper?that
of using pre-ordering to improve translation output?
has been explored by many researchers. Most
work has focused on automatically learning reorder-
ing rules (Xia and McCord, 2004; Habash, 2007b;
Elming, 2008; Elming and Habash, 2009; Dyer
and Resnik, 2010). Xia and McCord (2004) de-
scribe an approach for translation from French to
English, where context-free constituency reordering
rules are acquired automatically using source and
target parses and word alignment. Elming (2008)
and Elming and Habash (2009) use a large set of
linguistic features to automatically learn reordering
rules for English-Danish and English-Arabic; the
rules are used to pre-order the input into a lattice
of variant orders. Habash (2007b) learns syntactic
reordering rules targeting Arabic-English word or-
der differences and integrated them as deterministic
preprocessing. He reports improvements in BLEU
compared to phrase-based SMT limited to mono-
tonic decoding, but these improvements do not hold
with distortion. He hypothesizes that parse errors
are responsible for lack of improvement. Dyer and
Resnik (2010) use an input forest structure to rep-
resent word-order alternatives and learn models for
long-range source reordering that maximize trans-
lation quality. Their results for Arabic-English are
negative.
In contrast to these approaches, Collins et al
(2005) apply six manually defined transformations
to German parse trees which yield an improvement
on a German-English translation task. In this paper,
we follow Collins et al (2005) and restrict ourselves
to handcrafted rules (in our case, actually a single
over-generating rule) motivated by linguistic under-
standing.
One major concern not addressed in any of the
aforementioned research on syntax-based reordering
is the fact that the quality of parsers for many lan-
guages is still quite poor. Collins et al (2005), for
example, assume that the parse trees they use are
correct. While the state-of-the-art in English pars-
ing is fairly good (though far from perfect), this
is not the case in other languages, where parsing
shows substantial error rates. Moreover, when at-
tempting to reorder so as to bring the source text
more grammatically in line with the target language,
a bad parse can be disastrous: moving parts of the
sentence that shouldn?t be moved, and introducing
more distortion error than it is able to correct. To ad-
dress the problem of noisy parse data, Bisazza and
Federico (2010) identify the subject using a chunker,
then fuzzify it, creating a lattice in which the transla-
tion system has a choice of several different paths,
corresponding to re-orderings of different subject
spans.
In investigating syntax-based reordering for Ara-
bic specifically, Carpuat et al (2010) show that a
syntax-driven reordering of the training data only
for the purpose of alignment improvement leads to
a substantial improvement in translation quality, but
do not report a corresponding improvement when re-
ordering test data in a similar fashion. Interestingly,
Bisazza and Federico (2010) report that fuzzy re-
ordering the test data improves MT output, suggest-
ing that fuzzification may be the mechanism neces-
sary to render reordering on test data useful. To the
best of our knowledge, nobody has yet used fuzzifi-
cation to correct the identified subject span of com-
plete Arabic dependency parses. Green et al (2009)
use a conditional random field sequence classifier
to detect Arabic noun phrase subjects in verb-initial
clauses achieving an F-score of 61.3%. They in-
tegrate their classifier?s decisions as additional fea-
tures in the Moses decoder (Koehn et al, 2007), but
do not show any gains.
The present work may be thought of as extending
the fuzzification explored by Bisazza and Federico
(2010) to the domain of full parsing?a combina-
tion, in some sense, of their approach with the work
of Carpuat et al (2010). The approach examined in
this paper differs from Collins et al (2005) in its use
of fuzzification, from Bisazza and Federico (2010)
in its use of a complete dependency parse, and from
Carpuat et al (2010) in its use of a reordered test set.
228
Figure 1: An example of a dependency tree of a Verb-Object-Subject Arabic sentence: 	?A??j. ? ??J
? @ ZA?? 	?AK
Q? @ 	Q?	?
J 	j 	j 	?? 	?
KPAJ
? +H. hz AlryAD msA? Alywm hjwmAn b+ syArtyn mfxxtyn ?Two car bombs shook Riyadh thisevening?. The predicted tree (on the left) shows an incorrect subject span (words 5-8).
Figure 1: An example of a dependency tree of a Verb-Object-Subject Arabic sentence: ??????? ? ????? ? ?? ? ??????? ? ????????? ?? ?? ??? ??????????? +?? hz AlryAD msA? Alywm hjwmAn b+ syArtyn mfxxtyn ?Two car bombs shook Riyadh thisevening?. The predicted tree (on the left) shows an incorrect subject span (words 5-8).
Gold Tree
VRB
??? hz1
?shook?
OBJ
PROP??????? ? AlryAD2
?RiyAdh?
MOD
NOM
?? ? msA?3
?evening?
IDF
NOM
????? ? Alywm4
?today?
SBJ
NOM??????? ? hjwmAn5
?two attacks?
MOD
PRT
+?? b+6
?with?
OBJ
NOM??????????? syArtyn7
?two cars?
MOD
NOM?????? ?? ?? ??? mfxxtyn8
?booby-trapped?
Predicted Tree
VRB
??? hz1
?shook?
MOD
PROP??????? ? AlryAD2
?RiyAdh?
MOD
NOM
?? ? msA?3
?evening?
IDF
NOM
????? ? Alywm4
?today?
SBJ
NOM??????? ? hjwmAn5
?two attacks?
MOD
PRT
+?? b+6
?with?
OBJ
NOM??????????? syArtyn7
?two cars?
MOD
NOM?????? ?? ?? ??? mfxxtyn8
?booby-trapped?
We focused on correcting the largest sources of er-
ror: incorrect span and false-positive subjects. As
false-positive subject corrections were already cap-
tured by providing a no-reorder option in the lattice,
only span errors needed additional correction.
In principle, spans can be marked incorrectly both
on their front and back ends; however, because left-
dependency is fairly uncommon in Arabic and hap-
pens in a limited number of predictable cases, the
system made so few errors in identifying the left
boundary of spans (1.8%) that it is not worth try-
ing to correct them. [A note on terminology: ?left?
and ?right? are used throughout this paper with ref-
erence to English word order. ?Left? should be un-
derstood to mean ?towards the beginning of the sen-
tence?, and ?right? to mean ?towards the end of the
sentence.?]
The question is thus how to correct the right edge
of spans assuming that label and attachment have
been predicted correctly. Span classifications can be
broken into three categories: those that are too long
(i.e. that have too many right descendants), too short
(i.e. that have too few right descendants), or correct
(so that the predicted tree has all the same descen-
dants as the gold tree). A comparison of gold and
predicted trees for MT05 was conducted, revealing
the following breakdown:
Type # %
Long 260 12.4%
Short 293 14.0%
Correct 1538 73.6%
Total 2091 100%
Table 1: Distribution of span errors
These numbers are quite low: roughly 3 out of
every 10 subjects identified in the corpus have their
spans incorrectly marked. This suggest that fuzzifi-
cation will provide room for improvement. But what
technique should we use to fuzzify the subjects?
To answer this question, we examined more
3 Motivation
While the VSO order is common at both the matrix
and non-matrix level in Arabic newswire text, ma-
trix VSO constructions are almost always reordered
in translation, while non-matrix VSO constructions
are frequently translated monotonically (they are in-
stead passivized or otherwise transformed in a fash-
ion that leaves them parallel to the source Arabic
text) (Carpuat et al, 2010). This reordering, as
noted in the introduction, is notoriously difficult for
phrase-based statistical machine translation systems
to capture. It is further exacerbated by the low
quality of Arabic parsing especially for subject span
identification (Green et al, 2009).
3.1 Reordering
We began by performing a series of reordering ex-
periments using gold-standard parses of the NIST
MT05 data set:1 (a) a baseline experiment with no
reordering, (b) an experiment which forced reorder-
ing on all matrix subjects, and (c) an experiment in
which the translation system was presented with a
lattice, in which one path contained the original sen-
tence and the other path contained the sentence with
the matrix subject reordered. The baseline system
produced a BLEU score of 47.13, forced reorder-
ing produced a BLEU score of 47.43, and optional
reordering produced a BLEU score of 47.55. These
results indicate that, given correct reordering bound-
aries, the translation quality can indeed be improved
with reordered test data. Furthermore, the improve-
ment noted above between the orced eordering and
optional reordering experiments, while small, indi-
cates that even with correct parses it is sometimes
preferable to leave the input sentence un-reordered.
This is consistent with Carpuat et al (2010)?s ob-
1The gold parses for NIST MT05 are part of the Columbia
Arabic Treeebank (CATiB) (Habash and Roth, 2009).
229
servation that even VS-ordered matrix verbs in Ara-
bic are sometimes translated monotonically into En-
glish (as, for example, in passive constructions). An
alternative explanation may be that since the train-
ing data itself is not re-ordered, it is plausible that
some re-ordering may cause otherwise good possi-
ble matches in the phrase table to not match any
more.
3.2 Parser Error
The problem of finding correct subject span bound-
aries for reordering, however, is a particularly dif-
ficult one. Both Habash (2007b) and Green et al
(2009) have noted previously that even state-of-
the-art Arabic dependency parsers tend to perform
poorly, and we would expect that incorrect bound-
aries would do more harm than good for translation.
In order to determine how to ?fix? these spans, it is
first necessary to understand the kinds of errors that
the parser makes. A set of predicted parses of the
NIST MT05 data was compared to the gold parses
of the same data set.
There are three categories of error the parser can
make in identifying subjects: labeling errors, attach-
ment errors and span errors. In labeling errors, the
parser either incorrectly marks a node SBJ when no
such label appears in the gold tree, or fails to identify
one of the gold-labeled SBJs. In attachment error,
the identified subject is marked as depending on the
wrong node. Finally, in span error, the descendants
assigned to a labeled SBJ are wrong. The distribu-
tion of parser errors in the NIST MT05 data is as
follows:
? Label errors: 19.8% of predicted subjects are
not gold subjects, and 19.1% of gold subjects
are not identified as predicted subjects.
? Attachment errors: 16.92% of gold subjects are
incorrectly attached in the predicted tree.
? Span errors: 26.4% of predicted subject spans
are incorrect.
In this paper, we focus on correcting the largest
sources of error: incorrect span and false-positive
subjects. We now provide further analysis of the
span errors.
In principle, spans can be marked incorrectly both
on their front and back ends; however, because left-
dependency is fairly uncommon in Arabic and hap-
pens in a limited number of predictable cases, the
parser made so few errors in identifying the left
boundary of spans (1.8%) that it is not worth trying
to correct them.2
The question is thus how to correct the right edge
of spans assuming that label and attachment have
been predicted correctly. Span classifications can
be broken into three categories: those that are too
long (i.e. that have too many right descendants), too
short (i.e. that have too few right descendants), or
correct (so that the predicted tree has all the same
descendants as the gold tree, without regard to their
syntactic structure). A comparison of gold and pre-
dicted trees for MT05 was conducted, revealing the
distribution shown in Table 1. We see that the 26.4%
of subjects with incorrect spans are roughly equally
divided between subjects that are too short and sub-
jects that are too long.
Type # %
Long 260 12.4%
Short 293 14.0%
Correct 1538 73.6%
Total 2091 100%
Table 1: Distribution of span errors in NIST MT05
To gain further insight into the nature of the sub-
ject span errors, we examined more closely the
26.4% of cases where the span is incorrectly labeled,
looking specifically at the ?difference box?: the set
of contiguous nodes that must be added to or re-
moved from the predicted span to bring it into agree-
ment with the gold span (see Fig. 1).3 Specifically,
we wished to know how many top-level constituents
required addition or removal to cover the entire dif-
ference. The smaller the number of top-level con-
stituents that needs to be added, the fewer reorder-
ing variations possible, and the better the expected
performance of the system.
Roughly 2% of these difference boxes are what
we might call ?pathological? cases: due to some se-
2A note on terminology: ?left? and ?right? are used through-
out this paper with reference to word order when using the Latin
alphabet. ?Left? should be understood to mean ?towards the be-
ginning of the sentence?, and ?right? to mean ?towards the end
of the sentence.?
3Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007).
230
r2 
r1 
a1 
a2 sbj 
r2 
r1 
a1 
a2 
original 
verb 
+ 
+ 
_ 
_ 
Figure 2: A schematic representation of the fuzzification algorithm. The black node is the matrix subject, + indicates
that a node (and its descendants) can be added, ? indicates that a node (and its descendants) can be removed, and the
black brackets denote the boundaries of the candidate spans.
rious error in parsing, there is a constituent inside
the difference box with descendants outside the box.
These are algorithmically very difficult to correct as
they require us to either add a constituent and then
prune it, or remove a constituent and then reattach
some of its children; attempting to correct for this
possibility in all sentences will lead to a combinato-
rial explosion of possible parses. Fortunately, these
pathological cases make up a small enough portion
of the data set that they can be safely disregarded.
More promisingly, 66.5% of incorrect spans can
be corrected with the addition or removal of a single
constituent; in other words, the recall of span iden-
tification can be improved from 73.6% to 91.2% by
adding or removing at most one constituent at the
end of the parser?s identified span.
4 Approach
To improve translation of matrix subjects, we im-
plement fuzzy reordering by using a lattice-based
approach similar to Bisazza and Federico (2010) to
correct the matrix subject spans identified by a state-
of-the-art dependency parser (Marton et al, 2010).
Specifically, we take a twofold approach to fuzzy
reordering. First, we present the translation system
with both un-reordered and reordered options. This
is motivated by the observation that on gold parses,
optional reordering outperformed forced reordering
(Section 3.1). Second, we apply a fuzzification algo-
rithm to the reordered subject span, adding yet more
options to the lattice. This is motivated by the ob-
servation that the greatest source of parsing errors
in subjects is span errors (Section 3.2). We discuss
these two techniques in turn.
4.1 Optional Reordering
In keeping with results from the initial gold experi-
ments, we decided to generate a lattice identical to
that used for the optional-reordering experiment, in
which the translation system was presented with the
input sentence both un-reordered and reordered, us-
ing a predicted parse to perform the reordering.
4.2 Subject Span Fuzzification
The observation that 91.2% of spans can be recalled
with single-constituent modifications led very natu-
rally to the following fuzzification algorithm, which
is illustrated in Fig. 2:
1. For each matrix subject in the parse tree4, cre-
ate an empty list to hold fuzzified boundaries.
2. Original span: Add to the list the tuple (l, r, v),
where l is the index of the predicted span?s left-
most descendant, r is the index of the predicted
span?s rightmost descendant and v is the verb
4Allowance must be made for parsers which incorrectly
identify multiple subjects for the matrix verb.
231
that the predicted span attaches to. (This step
produces the span labeled ?original? in Fig. 2.)
3. Expansion: Add to the list all tuples of the form
(l, r+, v), where r+ is the index of the right-
most descendant of a node whose leftmost de-
scendant has index r + 1. (This step produces
the spans labeled ?a1? and ?a2? in Fig. 2.)
4. Contraction: Add to the list all tuples of the
form (l, r??1, v), where r? is the index of the
leftmost descendant of a node whose rightmost
descendant has index r. (This step produces the
spans labeled ?r1? and ?r2? in Fig. 2.)
5. Create the list of all valid combinations of
spans by taking the Cartesian product of all
the per-subject span lists, and rejecting all en-
tries in which two spans overlap. (This step ac-
counts for multiple subject cases.)
The result of this algorithm is a list of lists of tuples,
where each tuple defines a single reordering, and
each list of tuples defines a set of spans that must be
moved to the left of the matrix verb for one reorder-
ing. These re-orderings are then joined together to
form the final lattice. If a single-constituent correc-
tion to the span exists (except in the aforementioned
pathological and left-attachment cases), it is guaran-
teed to appear as one path through the lattice.
5 Evaluation
5.1 Experimental Setup
We used the open-source Moses PSMT toolkit
(Koehn et al, 2007). Training data was a newswire
(MSA-English) parallel text with 12M words on the
Arabic side (LDC2007E103)5 Sentences were re-
ordered only for alignment, following the approach
of Carpuat et al (2010). Parses were obtained using
a publicly available parser for Arabic (Marton et al,
2010). GIZA++ was used for word alignment (Och
and Ney, 2003) and phrase translations of up to 10
words are extracted in the Moses phrase table. The
same baseline phrase table was used in all experi-
ments.
The system?s language model was trained both on
the English portion of the training corpus and En-
glish Gigaword (Graff and Cieri, 2003). We used a
5All data is available from the Linguistic Data Consortium:
http://www.ldc.upenn.edu.
5-gram language model with modified Kneser-Ney
smoothing implemented using the SRILM toolkit
(Stolcke, 2002). Feature weights were tuned with
MERT (Och, 2003) to maximize BLEU on the NIST
MT06 corpus. MERT was done only for the baseline
system; these same weights were used for all exper-
iments to control for the effect of MERT instability.
In the future, we plan to experiment with approach-
specific optimization and to use recent published
suggestions on controlling for optimizer instability
(Clark et al, 2011).
English data was tokenized using simple
punctuation-based rules. Arabic data was seg-
mented with to the Arabic Treebank tokeniza-
tion scheme (Maamouri et al, 2004) using the
MADA+TOKAN morphological disambiguator and
tokenizer (Habash and Rambow, 2005; Habash,
2007a; Roth et al, 2008). The Arabic text was
also Alif/Ya normalized (Habash, 2010). MADA-
produced Arabic lemmas were used for word
alignment.
We compare four settings with predicted parses
(as opposed to the gold parse experiments discussed
in Section 3):
? BASE An un-reordered test set;
? FORCE A test set which forced reordering on
matrix verbs;
? OPT A test set with fuzzification through op-
tional reordering on matrix verbs; and
? SPAN A test set with fuzzification through op-
tional reordering on matrix verbs and through
fuzzification of the subject span according to
the algorithm shown in Section 4.2.
Each reordering corpus used Moses? lattice input
format (Dyer et al, 2008) (including the baselines,
which had only one path). Results are presented in
terms of the standard BLEU metric (Papineni et al,
2002), METEOR metric (Banerjee and Lavie, 2005)
and a manual evaluation targeting subject span trans-
lation correctness.
5.2 Automatic Evaluation Results
Table 2 presents the results for the experiments dis-
cussed above. Columns three and Four (Prec-1g
and Prec-4g) indicate the corresponding 1-gram and
4-gram (sub-BLEU) precision scores, respectively.
232
System BLEU Prec-1g Prec-4g METEOR
BASE 47.13 81.91 29.52 53.09
FORCE 47.03 81.78 29.52 53.11
OPT 47.42 81.88 30.04 53.22
SPAN 47.41 81.92 30.03 53.21
Table 2: Automatic evaluation results
Both OPT and SPAN showed a statistically signif-
icant improvement in BLEU score over BASE and
FORCE above the 95% level. Statistical signifi-
cance is computed using paired bootstrap resam-
pling (Koehn, 2004). The difference between OPT
and SPAN, however, was not statistically significant.
The relatively small difference in BLEU score be-
tween the baseline and gold reordering (Section 3:
baseline 47.13 and optional reordering 47.55) sug-
gests that we should expect at most a modest in-
crease in BLEU from improving the predicted trees.
The first key observation in these results is that
with a noisy parser, translation quality actually goes
down with forced reordering?the opposite of what
was observed in the gold experiment. By introduc-
ing either optional reordering or complete fuzzifi-
cation, however, BLEU score increases .3 past the
baseline to achieve nearly three quarters of the gain
obtained by optional reordering using the gold parse
(Section 3: baseline 47.13 and optional reordering
47.55). In other words, it is possible to compensate
for the parser noisiness without actually attempting
to correct spans: simply allowing the translation sys-
tem to fall back on an un-reordered input leads to a
significant gain in BLEU.
One possible explanation for this fact is that we
only ever correct for parses on the right-hand side?
the left sides are virtually always correct. Thus,
when we perform any reordering, even if the subject
span is not entirely perfect, we guarantee that we
bring at least one word from the sentence (and usu-
ally more) into alignment where it was out of align-
ment before; this obviously leads to better BLEU
n-gram scores along that boundary.
The general trend in these results is confirmed by
the results of a METEOR analysis, also provided in
Tab. 2. Again, both the OPT and SPAN systems
result exhibit comparable performance, and demon-
strate an improvement over the baseline.
The second observation is that introducing span
fuzzification did not improve over simple optional
reordering. There are a several reasons this could be
happening:
? The increased fluency and introduction of un-
seen phrases cancel each other out.
? All the gains that come from reordering occur
at the left; the presence or absence of correct
words at the right end is less important.
? Better sentences are proposed during the trans-
lation process, but they are not selected during
the final filtering stage.
? The sentences being output are actually better,
but the improvement is not captured by the au-
tomatic evaluation.
Further experiments will be necessary to determine
whether any of the first three possibilities is the case.
We next consider the fourth possibility in more de-
tail.
5.3 Manual Evaluation
We additionally conducted a manual evaluation to
examine how subject quality differed in fuzzified vs.
unfuzzified parses. Each sentence examined was as-
signed one of the six labels below. Examples are
with respect to the reference sentence ?Recep Tayyip
Erdogan announced that Turkey is strong.?
? MM: both verb and subject missing. ?Turkey
is strong.?
? MV: verb missing. ?Recep Tayyip Erdogan
Turkey is strong.?
? MS: subject missing. ?announced that Turkey
is strong.?
? SO: subject overlaps with verb. ?Recep an-
nounced Tayyip Erdogan Turkey is strong.?
? SI: verb precedes subject (as in Arabic). ?an-
nounced Recep Tayyip Erdogan that Turkey is
strong.?
? C: verb follows subject (as in English), i.e. the
correct ordering. ?Recep Tayyip Erdogan an-
nounced that Turkey is strong.? We also include
in this category sentences where the English
reference contains no verb (e.g. in newspaper
headlines).
233
System MM MS MV SI SO C M* S* C
BASE 8 13 11 9 3 53 33 12 53
OPT 7 11 10 5 5 61 28 10 61
SPAN 8 10 09 5 2 64 27 7 64
Table 3: Subject integrity analysis results. All numbers are %s.
By grouping some of these categories together, we
obtained the following label scheme:
? M*: MM, MV or MS, i.e. verb or subject is
missing.
? S*: SO or SI, i.e. word order is incorrect.
? C: as above.
280 sentences selected randomly from our test set
were evaluated, generating 461 unique output sen-
tences. Annotation was performed by two English
speakers, with 40 input sentences (68 unique out-
puts) annotated by both authors to collect agreement
statistics. For the complete label scheme, the an-
notators agreed on 86.8% of labels, with Cohen?s
? = 0.811. For the simple label scheme, the an-
notators agreed on 92.6% of labels, with ? = .883.
Results for the BASE, OPT and SPAN systems are
shown in Table 3. Each annotator?s labels were as-
signed a weight of .5 in the section that was jointly
annotated.
Again, both the OPT and SPAN systems display
statistically significant improvements over the base-
line system (p < 0.001). While the SPAN system
consistently displays better results than the OPT sys-
tem, the significance is low (p < .3). Statistical sig-
nificance was measured using the McNemar test of
statistical significance (McNemar, 1947).
These results thus agree with the BLEU score in
indicating that the OPT and SPAN systems are sub-
stantially better than the baseline, but statistically in-
distinguishable from each other. They further in-
dicate that most of the improvements in the OPT
system come from preventing dropped subjects or
verbs, while the improvements in the SPAN system
result in roughly equal proportion from preventing
word-dropping and ensuring correct ordering.
6 Conclusion & Future Work
We presented an approach for improving Arabic-
English PSMT using syntactic information from a
noisy parser. We demonstrated that translation qual-
ity goes down with forced reordering, but improves
with the introduction of either optional reordering
and subject span fuzzification. The BLEU score in-
creases by 0.3% absolute past the baseline achieve
nearly three quarters of the maximum possible gain
starting with gold parses. A detailed manual eval-
uation produces results generally consistent with
BLEU, but highlights the small improvements that
can be gained by subject span fuzzification.
In the future, we plan to explore a more sophis-
ticated approach to the lattice of re-orderings pre-
sented here. We would take into account the fact that
it is possible to suggest to the system that certain
re-orderings are less likely than others without re-
moving them from the search space completely. The
same can be done for the fuzzification task: while
we might wish to add additional fuzzification op-
tions, we also don?t want the correct choice to be
crowded out by too many alternatives.
Acknowledgements
We would like to thank Marine Carpuat, Yuval Mar-
ton, Amit Abbi and Ahmed El Kholy for helpful dis-
cussions and feedback. The second and third authors
were supported by the Defense Advanced Research
Projects Agency (DARPA) under GALE Contract
No HR0011-08-C-0110. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
234
Arianna Bisazza and Marcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Proceedings
of ACL 2010: Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, Uppsala, Swe-
den.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English Statistical Ma-
chine Translation by Reordering Post-Verbal Subjects
for Alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178?183, Uppsala, Swe-
den, July.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 858?866, Los Angeles,
California.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, Columbus, Ohio.
Jakob Elming and Nizar Habash. 2009. Syntactic Re-
ordering for English-Arabic Phrase-Based Machine
Translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69?77, Athens, Greece, March.
J. Elming. 2008. Syntactic reordering integrated with
phrase-based smt. In Proceedings of the ACL Work-
shop on Syntax and Structure in Statistical Translation
(SSST-2).
David Graff and Christopher Cieri. 2003. English Gi-
gaword, LDC Catalog No.: LDC2003T05. Linguistic
Data Consortium, University of Pennsylvania.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP Subject Detection in Verb-initial Ara-
bic Clauses. In Proceedings of the Third Workshop
on Computational Approaches to Arabic Script-based
Languages (CAASL3).
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
221?224, Suntec, Singapore.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash. 2007a. Arabic Morphological Repre-
sentations for Machine Translation. In A. van den
Bosch and A. Soudi, editors, Arabic Computational
Morphology: Knowledge-based and Empirical Meth-
ods. Springer.
Nizar Habash. 2007b. Syntactic preprocessing for statis-
tical machine translation. In Proceedings of the 11th
MT Summit.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of the
Empirical Methods in Natural Language Processing
Conference (EMNLP?04), Barcelona, Spain.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic Dependency Parsing with Lexical
and Inflectional Morphological Features. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 13?21, Los Angeles, CA, USA, June.
Quinn McNemar. 1947. Note on the sampling error of
the difference between correlated proportions or per-
centages. Psychometrika, 12(2):153?157.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
235
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL-08: HLT, Short Papers, pages 117?120, Colum-
bus, Ohio.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Fei Xia and Michael McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004),
pages 508?514, Geneva, Switzerland.
236
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 37?45,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Detecting Influencers in Written Online Conversations
Or Biran1* Sara Rosenthal1* Jacob Andreas1**
Kathleen McKeown1* Owen Rambow2?
1 Department of Computer Science, Columbia University, New York, NY 10027
2 Center for Computational Learning Systems, Columbia University, New York, NY 10027
* {orb, sara, kathy}@cs.columbia.edu
** jda2129@columbia.edu ?rambow@ccls.columbia.edu
Abstract
It has long been established that there is a cor-
relation between the dialog behavior of a par-
ticipant and how influential he or she is per-
ceived to be by other discourse participants.
In this paper we explore the characteristics of
communication that make someone an opinion
leader and develop a machine learning based
approach for the automatic identification of
discourse participants that are likely to be in-
fluencers in online communication. Our ap-
proach relies on identification of three types
of conversational behavior: persuasion, agree-
ment/disagreement, and dialog patterns.
1 Introduction
In any communicative setting where beliefs are ex-
pressed, some are more influential than others. An
influencer can alter the opinions of their audience,
resolve disagreements where no one else can, be rec-
ognized by others as one who makes important con-
tributions, and often continue to influence a group
even when not present. Other conversational par-
ticipants often adopt their ideas and even the words
they use to express their ideas. These forms of per-
sonal influence (Katz and Lazarsfeld, 1955) are part
of what makes someone an opinion leader. In this
paper, we explore the characteristics of communica-
tion that make someone an opinion leader and de-
velop a machine learning based approach for the au-
tomatic identification of discourse participants who
are likely to be influencers in online communication.
Detecting influential people in online conversa-
tional situations has relevance to online advertising
strategies which exploit the power of peer influence
on sites such as Facebook. It has relevance to analy-
sis of political postings, in order to determine which
candidate has more appeal or which campaign strat-
egy is most successful. It is also relevant for design-
ing automatic discourse participants for online dis-
cussions (?chatbots?) as it can provide insight into
effective communication. Despite potential applica-
tions, analysis of influence in online communication
is a new field of study in part because of the rela-
tively recent explosion of social media. Thus, there
is not an established body of theoretical literature in
this area, nor are there established implementations
on which to improve. Given this new direction for
research, our approach draws on theories that have
been developed for identifying influence in spoken
dialog and extends them for online, written dialog.
We hypothesize that an influencer, or an influencer?s
conversational partner, is likely to engage in the fol-
lowing conversational behaviors:
Persuasion: An influencer is more likely to express
personal opinions with follow-up (e.g., justification,
reiteration) in order to convince others.
Agreement/disagreement: A conversational partner
is more likely to agree with an influencer, thus im-
plicitly adopting his opinions.
Dialog Patterns: An influencer is more likely to par-
ticipate in certain patterns of dialog, for example
initiating new topics of conversation, contributing
more to dialog than others, and engendering longer
dialog threads on the same topic.
Our implementation of this approach comprises
a system component for each of these conversa-
tional behaviors. These components in turn provide
37
the features that are the basis of a machine learn-
ing approach for the detection of likely influencers.
We test this approach on two different datasets, one
drawn from Wikipedia discussion threads and the
other drawn from LiveJournal weblogs. Our results
show that the system performs better for detection
of influencer on LiveJournal and that there are in-
teresting differences across genres for detecting the
different forms of conversational behavior.
The paper is structured as follows. After review-
ing related work, we define influence, present our
data and methods. We present a short overview of
the black box components we use for persuasion and
detection of agreement/disagreement, but our focus
is on the development of the influencer system as a
whole and thus we spend most time exploring the
results of experimentation with the system on dif-
ferent data sets, analyzing which components have
most impact. We first review related work.
2 Related Work
It has long been established that there is a correlation
between the conversational behavior of a discourse
participant and how influential he or she is perceived
to be by the other discourse participants (Bales et al,
1951; Scherer, 1979; Brook and Ng, 1986; Ng et al,
1993; Ng et al, 1995). Specifically, factors such as
frequency of contribution, proportion of turns, and
number of successful interruptions have been identi-
fied as being important indicators of influence. Reid
and Ng (2000) explain this correlation by saying that
?conversational turns function as a resource for es-
tablishing influence?: discourse participants can ma-
nipulate the dialog structure in order to gain influ-
ence. This echoes a starker formulation by Bales
(1970): ?To take up time speaking in a small group
is to exercise power over the other members for at
least the duration of the time taken, regardless of the
content.? Simply claiming the conversational floor is
a feat of power. This previous work presents two is-
sues for a study aimed at detecting influence in writ-
ten online conversations.
First, we expect the basic insight ? conversation
as a resource for influence ? to carry over to written
dialog: we expect to be able to detect influence in
written dialog as well. However, some of the charac-
teristics of spoken dialog do not carry over straight-
forwardly to written dialog, most prominently the
important issue of interruptions: there is no interrup-
tion in written dialog. Our work draws on findings
for spoken dialog, but we identify characteristics of
written dialog which are relevant to influence.
Second, the insistence of Bales (1970) that power
is exercised through turn taking ?regardless of con-
tent? may be too strong. Reid and Ng (2000) discuss
experiments which address not just discourse struc-
ture features, but also a content feature which repre-
sents how closely a turn is aligned with the overall
discourse goal of one of two opposing groups (with
opposing opinions on a specific issue) participating
in the conversation. They show that interruptions are
more successful if aligned with the discourse goal.
They propose a model in which such utterances
?lead to participation which in turn predicts social
influence?, so that the correlation between discourse
structure and influence is really a secondary phe-
nomenon. However, transferring such results to
other types of interactions (for example, in which
there are not two well-defined groups) is challeng-
ing. In this study, we therefore examine two types of
features as they relate to influence: content-related
(persuasion and agreement/disagreement), and dis-
course structure-related.
So far, there has been little work in NLP related
to influencers. Quercia et al (2011) look at influ-
encers? language use in Twitter contrasted to other
users? groups and find some significant differences.
However, their analysis and definition relies quite
heavily on the particular nature of social activity
on Twitter. Rienks (2007) discusses detecting influ-
encers in a corpus of conversations. While he fo-
cuses entirely on non-linguistic behavior, he does
look at (verbal) interruptions and topic initiations
which can be seen as corresponding to some of our
Dialog Patterns Language Uses.
3 What is an Influencer?
Our definition of an influencer was collectively for-
mulated by a community of researchers involved in
the IARPA funded project on Socio Cultural Content
in Language (SCIL).
This group defines an influencer to be someone
who:
38
P1 by Arcadian <pc1>There seems to be a much better list at the National Cancer Institute than the one we?ve
got.</pc1><pa1>It ties much better to the actual publication (the same 11 sections, in the same order).</pa1>
I?d like to replace that section in this article. Any objections?
P2 by JFW <pc2><a1>Not a problem.</a1></pc2>Perhaps we can also insert the relative incidence as
published in this month?s wiki Blood journal
P3 by Arcadian I?ve made the update. I?ve included template links to a source that supports looking up
information by ICD-O code.
P4 by Emmanuelm Can Arcadian tell me why he/she included the leukemia classification to this lymphoma
page? It is not even listed in the Wikipedia leukemia page! <pc3>I vote for dividing the WHO classification
into 4 parts in 4 distinct pages: leukemia, lymphoma, histocytic and mastocytic neoplasms.</pc3><pa3>
Remember, Wikipedia is meant to be readable </pa3>by all. Let me know what you think before I delete
the non-lymphoma parts.
P5 by Arcadian Emmanuelm, aren?t you the person who added those other categories on 6 July 2005?
P6 by Emmanuelm <d1>Arcadian, I added only the lymphoma portion of the WHO classification.
You added the leukemias on Dec 29th.</d1>Would you mind moving the leukemia portion to the
leukemia page?
P7 by Emmanuelm <pc4>Oh, and please note that I would be very comfortable with a ?cross-coverage?
of lymphocytic leukemias in both pages.</pc4>My comment is really about myeloid, histiocytic and
mast cell neoplasms who share no real relationship with lymphomas.
P8 by Arcadian <pa5><a2>To simplify the discussion, I have restored that section to your version.
</a2></pa5>You may make any further edits, and <pc6>I will have no objection.</pc6>
P9 by JFW The full list should be on the hematological malignancy page, and the lymphoma part can be here.
<pc7>It would be defendable to list ALL and CLL here.</pc7><pa7>They fall under the lymphoproliferative
disorders.</pa7>
Table 1: Influence Example: A Wikipedia discussion thread displaying Emmanuelm as the influencer. Replies are
indicated by indentation (for example, P2 is a response to P1). All Language Uses are visible in this example: Attempt
to Persuade ({pci, pai}), Claims (pci), Argumentation (pai), Agreement (ai), Disagreement (di), and the five Dialog
Patterns Language Uses (eg. Arcadian has positive Initiative).
1. Has credibility in the group.
2. Persists in attempting to convince others, even if
some disagreement occurs
3. Introduces topics/ideas that others pick up on or
support.
By credibility, we mean someone whose ideas are
adopted by others or whose authority is explicitly
recognized. We hypothesize that this shows up
through agreement by other conversants. By per-
sists, we mean someone who is able to eventually
convince others and often takes the time to do so,
even if it is not quick. This aspect of our definition
corresponds to earlier work in spoken dialog which
shows that frequency of contributions and propor-
tion of turns is a method people use to gain influence
(Reid and Ng, 2000; Bales, 1970). By point 3, we
see that the influencer may be influential even in di-
recting where the conversation goes, discussing top-
ics that are of interest to others. This latter feature
can be measured through the discourse structure of
the interaction. The influencer must be a group par-
ticipant but need not be active in the discussion(s)
where others support/credit him.
The instructions that we provided to annotators
included this definition as well as examples of who
is not an influencer. We told annotators that if some-
one is in a hierarchical power relation (e.g., a boss),
then that person is not an influencer to sub-ordinates
(or, that is not the type of influencer we are look-
ing for). We also included someone with situational
power (e.g., authority to approve other?s actions) or
power in directing the communication (e.g., a mod-
erator) as negative examples.
We also gave positive examples of influencers. In-
fluencers include an active participant who argues
against a disorganized group and resolves a discus-
sion is an influencer, a person who provides an an-
swer to a posted question and the answer is accepted
after discussion, and a person who brings knowledge
to a discussion. We also provided positive and neg-
39
ative examples for some of these cases.
Table 1 shows an example of a dialog where there
is evidence of influence, drawn from a Wikipedia
Talk page. A participant (Arcadian) starts the thread
with a proposal and a request for support from other
participants. The influencer (Emmanuelm) later
joins the conversation arguing against Arcadian?s
proposal. There is a short discussion, and Arcadian
defers to Emmanuelm?s position. This is one piece
of dialog within this group where Emmanuelm may
demonstrate influence. The goal of our system is to
find evidence for situations like this, which suggests
that a person is more likely to be an influencer.
Since we attempt to find local influence (a per-
son who is influential in a particular thread, as op-
posed to influential in general), our notion of influ-
encer is consistent with diverse views on social in-
fluence. It is consistent with the definition of influ-
encer proposed by Gladwell (2001) and Katz (1957):
an exceptionally convincing and influential person,
set apart from everyone else by his or her ability to
spread opinions. While it superficially seems incon-
sistent with Duncan Watts? concept of ?accidental
influentials? (Watts, 2007), that view does not make
the assertion that a person cannot be influential in
a particular situation (in fact, it predicts that some-
one will) - only that one cannot in general identify
people who are always more likely to be influencers.
4 Data and Annotation
Our data set consists of documents from two differ-
ent online sources: weblogs from LiveJournal and
discussion forums from Wikipedia.
LiveJournal is a virtual community in which peo-
ple write about their personal experiences in a we-
blog. A LiveJournal entry is composed of a post
(the top-level content written by the author) and a
set of comments (written by other users and the au-
thor). Every comment structurally descends either
from the post or from another comment.
Each article on Wikipedia has a discussion forum
(called a Talk page) associated with it that is used
to discuss edits for the page. Each forum is com-
posed of a number of threads with explicit topics,
and each thread is composed of a set of posts made
by contributors. The posts in a Wikipedia discussion
thread may or may not structurally descend from
other posts: direct replies to a post typically descend
from it. Other posts can be seen as descending from
the topic of the thread.
For consistency of terms, from here on we refer to
each weblog or discussion forum thread as a thread
and to each post or comment as a post.
We have a total of 333 threads: 245 from Live-
Journal and 88 from Wikipedia. All were annotated
for influencers. The threads were annotated by two
undergraduate students of liberal arts. These stu-
dents had no prior training or linguistic background.
The annotators were given the full definition from
section 3 and asked to list the participants that they
thought were influencers. Each thread may in princi-
ple have any number of influencers, but one or zero
influencers per thread is the common case and the
maximal number of influencers found in our dataset
was two. The inter-annotator agreement on whether
or not a participant is an influencer (given by Co-
hen?s Kappa) is 0.72.
5 Method
Our approach is based on three conversational be-
haviors which are identified by separate system
components described in the following three sec-
tions. Figure 1 shows the pipeline of the Influencer
system and Table 1 displays a Wikipedia discussion
thread where there is evidence of an influencer and
in which we have indicated the conversational be-
haviors as they occur. Motivated by our definition,
each component is concerned with an aspect of the
likely influencer?s discourse behavior:
Persuasion examines the participant?s language to
identify attempts to persuade, such as {pc1, pa1} in
Table 1, which consist of claims (e.g. pc1) made
by the participant and supported by argumentations
(e.g. pa1). It also identifies claims and argumenta-
tions independently of one another (pc4 and pa5).
Agreement/Disagreement examines the other par-
ticipants? language to find how often they agree or
disagree with the participant?s statements. Examples
are a1 and d1 in Table 1.
Dialog Patterns examines how the participant inter-
acts in the discussion structurally, independently of
the content and the language used. An example of
this is Arcadian being the first poster and contribut-
ing the most posts in the thread in Table 1.
40
Figure 1: The influencer pipeline. Solid lines indicate
black-box components, which we only summarize in this
paper. Dashed lines indicate components described here.
Each component contributes a number of Lan-
guage Uses which fall into that category of conver-
sational behavior and these Language Uses are used
directly as features in a supervised machine learn-
ing model to predict whether or not a participant is
an influencer. For example, Dialog Patterns con-
tributes the Language Uses Initiative, Irrelevance,
Incitation, Investment and Interjection.
The Language Uses of the Persuasion and Agree-
ment/Disagreement components are not described in
detail in this paper, and instead are treated as black
boxes (indicated by solid boxes in Figure 1). We
have previously published work on some of these
(Biran and Rambow, 2011; Andreas et al, 2012).
The remainder of this section describes them briefly
and provides the results of evaluations of their per-
formance (in Table 2). The next section describes
the features of the Dialog Patterns component.
5.1 Persuasion
This component identifies three Language Uses: At-
tempt to Persuade, Claims and Argumentation.
We define an attempt to persuade as a set of con-
tributions made by a single participant which may
be made anywhere within the thread, and which are
all concerned with stating and supporting a single
claim. The subject of the claim does not matter:
an opinion may seem trivial, but the argument could
still have the structure of a persuasion.
Our entire data set was annotated for attempts to
persuade. The annotators labeled the text partici-
pating in each instance with either claim, the stated
opinion of which the author is trying to persuade
others or argumentation, an argument or evidence
that supports that claim. An attempt to persuade
must contain exactly one claim and at least one in-
stance of argumentation, like the {claim, argumen-
tation} pairs {pc1, pa1} and {pc3, pj3} in Table 1.
In addition to the complete attempt to persuade
Language Use, we also define the less strict Lan-
guage Uses claims and argumentation, which use
only the subcomponents as stand-alones.
Our work on argumentation, which builds on
Rhetorical Structure Theory (Mann and Thompson,
1988), is described in (Biran and Rambow, 2011).
5.2 Agreement/Disagreement
Agreement and disagreement are two Language
Uses that model others? acceptance of the partici-
pant?s statements. Annotation (Andreas et al, 2012)
is performed on pairs of phrases, {p1, p2}. A phrase
is a substring of a post or comment in a thread. The
annotations are directed since each post or comment
has a time stamp associated with it. This means that
p1 and p2 are not interchangeable. p1 is called the
?target phrase?, and p2 is called the ?subject phrase?.
A person cannot agree with him- or herself, so the
author of p1 and p2 cannot be the same. Each anno-
tation is also labeled with a type: either ?agreement?
or ?disagreement?.
6 Dialog Patterns
The Dialog Patterns component extracts features
based on the structure of the thread. Blogs and dis-
cussion threads have a tree structure, with a blog
post or a topic of discussion as the root and a set of
41
Component Wikipedia LiveJournal
P R F P R F
Attempt 79.1 69.6 74 57.5 48.2 52.4
to persuade
Claims 83.6 74.5 78.8 53.7 13.8 22
Argumentation 23.3 91.7 37.1 30.9 48.9 37.8
Agreement 12 31.9 17.4 20 50 28.6
Disagreement 8.7 9.5 9.1 6.3 14.3 8.7
Table 2: Performance of the black-box Language Uses in
terms of Precision (P), Recall (R), and F-measure(F).
Conversational
Behavior
Language Use
(Feature)
Users
Component A J E
Persuasion Claims 2/6 2/6 2/6
Argumentation Y Y Y
Attempt to Per-
suade
Y Y Y
Agreement/ Agreement 1/1 0/1 0/1
Disagreement Disagreement 1/1 0/1 0/1
Dialog Initiative Y N N
Patterns Irrelevance 2/4 1/2 1/3
Incitation 4 1 3
Interjection 1/9 2/9 4/9
Investment 4/9 2/9 3/9
Table 3: The feature values for each of the partici-
pants, Arcadian (A), JFW (J), and Emmanuelm (E), in
the Wikipedia discussion thread shown in Table 1.
comments or posts which are marked as a reply - ei-
ther to the root or to an earlier post. The hypothesis
behind Dialog Patterns is that influencers have typ-
ical ways in which they participate in a thread and
which are visible from the structure alone.
The Dialog Patterns component contains five sim-
ple Language Uses:
Initiative The participant is or is not the first poster
of the thread.
Irrelevance The percentage of the participant?s
posts that are not replied to by anyone.
Incitation The length of the longest branch of
posts which follows one of the participant?s posts.
Intuitively, the longest discussion started directly by
the participant.
Investment The participant?s percentage of all posts
in the thread.
Interjection The point in the thread, represented
as percentage of posts already posted, at which the
participant enters the discussion.
7 System and Evaluation
The task of the system is to decide for each partici-
pant in a thread whether or not he or she is an influ-
encer in that particular thread. It is realized with a
supervised learning model: we train an SVM with a
small number of features, namely the ten Language
Uses. One of our goals in this work is to evaluate
which Language Uses allow us to more accurately
classify someone as an influencer. Table 3 shows
the full feature set and feature values for the sample
discussion thread in Table 1. We experimented with
a number of different classification methods, includ-
ing bayesian and rule-based models, and found that
SVM produced the best results.
7.1 Evaluation
We evaluated on Wikipedia and LiveJournal sepa-
rately. The data set for each corpus consists of all
participants in all threads for which there was at least
one influencer. We exclude threads for which no in-
fluencer was found, narrowing our task to finding the
influencers where they exist. For each participant X
in each thread Y, the system answers the following
question: Is X an influencer in Y?
We used a stratified 10-fold cross validation of
each data set for evaluation, ensuring that the same
participant (from two different threads) never ap-
peared in both training and test at each fold, to elim-
inate potential bias from fitting to a particular partic-
ipant?s style. The system components were identical
when evaluating both data sets, except for the claims
system which was trained on sentiment-annotated
data from the corpus on which it was evaluated.
Table 4 shows the performance of the full system
and of systems using only one Language Use feature
compared against a baseline which always answers
positively (X is always an influencer in Y). It also
shows the performance for the best system, which
was found for each data set by looking at all possible
combinations of the features. The best system for
the Wikipedia data set is composed of four features:
Claims, Argumentation, Agreement and Investment.
The best LiveJournal system is composed of all five
Dialog Patterns features, Attempt to Persuade and
Argumentation. We found our results to be statis-
42
System Wikipedia LiveJournal
P R F P R F
Baseline: all-
yes
16.2 100 27.9 19.2 19.2 32.2
Full 40.5 80.5 53.9 61.7 82 70.4
Initiative 31.6 31.2 31.4 73.5 72.7 73.1
Irrelevance 21.7 77.9 34 19.2 100 32.2
Incitation 28.3 77.9 41.5 49.5 73.8 59.2
Investment 43 71.4 53.7 50.2 75.4 60.3
Interjection 24.7 88.3 38.6 36.9 91.3 52.5
Agreement 36 46.8 40.7 45.1 82.5 58.3
Disagreement 35.3 70.1 47 19.2 100 32.2
Claims 40 72.7 51.6 54.3 76 63.3
Argumentation 19 98.7 31.8 31.1 85.2 45.6
Attempt 23.7 79.2 36.5 37.4 48.1 42.1
to persuade
Best system 47 80.5 59.3 66.2 84.7 74.3
Table 4: Performance in terms of Precision (P), Recall
(R), and F-measure (F) using the baseline (everyone is an
influencer), all features (full), individual features one at a
time, and the best feature combination for each data set.
tically significant (with the Bonferroni adjustment)
in paired permutation tests between the best system,
the full system and the baseline of each data set.
When we first performed these experiments, we
used all threads in the data set. The performance on
this full set was lower, as shown in Table 5 due to
the presence of threads with no influencers. Threads
in which the annotators could not find a clear influ-
encer tend to be of a different nature: there is either
no clear topic of discussion, or no argument (every-
one is in agreement). We leave the task of distin-
guishing these threads from those which are likely
to have an influencer to future work.
7.2 Evaluating with Perfect Components
In a hierarchical system such as ours, errors can
be attributed to imperfect components or to a bad
choice of features, so it is important to look at the
potential contribution of the components. As an ex-
ample, Table 6 shows the difference between our
Attempt to Persuade system and a hypothetical per-
fect Attempt to Persuade component, simulated by
using the gold annotations, when predicting influ-
encer directly (i.e., a participant is an influencer iff
she makes an attempt to persuade).
Clearly, when predicting influencers, Attempt to
System Wikipedia LiveJournal
P R F P R F
Baseline 13.9 100 24.5 14.2 100 24.9
Full 36.7 79.2 50.2 46.3 79.8 58.6
Best 40.1 76.6 52.7 48.2 81.4 60.6
Table 5: Performance on the data set of all threads, in-
cluding those with no influencers. The ?Best System? is
the system that performed best on the filtered data set.
Data Set Our System Gold Answers
P R F P R F
Wikipedia 23.6 69.4 35.2 23.8 81.6 36.9
LiveJournal 37.5 48.1 42.1 40.7 61.8 49
Table 6: Performance of the Attempt to Persuade compo-
nent in directly predicting influencers. A comparison of
our system and the component?s gold annotation. These
experiments were run on the full data set, which is why
the system results are not exactly those of Table 4.
Persuade is a stronger indicator in LiveJournal than
it is in Wikipedia. However, as shown in Table 2,
our Attempt to Persuade system performs better on
Wikipedia. This situation is reflected in Table 6,
where the lower quality of the system component in
LiveJournal corresponds to a significantly lower per-
formance when applied to the influencer task. These
results demonstrate that Attempt to Persuade is a
good feature: a more precise feature value means
higher predictability of influencer. In the future we
will perform similar analyses for the other features.
8 Discussion
We evaluated our system on two corpora - Live-
Journal and Wikipedia discussions - which differ in
structure, context and discussion topics. As our re-
sults show, they also differ in the way influencers
behave and the way others respond to them. To
illustrate the differences, we contrast the sample
Wikipedia thread (Table 1) with an example from
LiveJournal (Table 7).
It is common in LiveJournal for the blogger to be
an influencer, as is the case in our example thread,
because the topic of the thread is set by the blog-
ger and comments are typically made by her friends.
This fact is reflected in our results: Initiative is a
very strong indicator in LiveJournal, but not so in
43
P1 by poconell <pc1>He really does make good on his promises! </pc1><pa1>Day three in office, and the
Global Gag Rule (A.K.A?The Mexico City Policy?) is gone!</pa1>I was holding my breath, hoping it
wouldn?t be left forgotte. He didn?t wait. <pc2>He can see the danger and risk in this policy, and the damage
it has caused to women and families.</pc2><pc3>I love that man!</pc3>
P2 by thalialunacy <a1>I literally shrieked ?HELL YES!? in my car when I heard. :D:D:D</a1>
P3 by poconell <a2>Yeah, me too</a2>
P4 by lunalovepotter <pc4><a3>He is SO AWESOME!</a3></pc4><pa4>Right down to business, no
ifs, ands, or buts! :D</pa4>
P5 by poconell <pc5>It?s amazing to see him so serious too!</pc5><pa5>This is one tough,
no-nonsense man!</pa5>
P6 by penny sieve My icon says it all :)
P7 by poconell <pc6>And I?m jealous of you with that President!</pc6><pa6>We tried to overthrow
our Prime Minister, but he went crying to the Governor General. </pa6>
Table 7: Influence Example: A LiveJournal discussion thread displaying poconell as the influencer. All the Language
Uses are visible in this example: agreement/disagreement (ai/di), persuasion ({pci, pai}, pci, pai), and dialog patterns
(eg. poconell has positive Initiative). This example is very different from the Wikipedia example in Table 1.
Wikipedia, where the discussion is between a group
of editors, all of whom are equally interested in the
topic. In general, the Dialog Patterns features are
stronger in LiveJournal. We believe this is due to the
fact that the tree structure in LiveJournal is strictly
enforced. In Wikipedia, people do not always reply
directly to the relevant post. Investment is the excep-
tion: it does not make use of the tree structure, and
is therefore an important indicator in Wikipedia.
Attempt to Persuade is useful in LiveJournal (the
influencer poconell makes three attempts to per-
suade in Table 7) but less so in Wikipedia. This is
explained by the precision of the gold system in Ta-
ble 6. Only 23.8% of those who attempt to persuade
in Wikipedia are influencers, compared with 40.7%
in LiveJournal. Attempts to Persuade are more com-
mon in Wikipedia (all participants attempt to per-
suade in Table 1), since people write there specifi-
cally to argue their opinion on how the article should
be edited. Conversely, agreement is a stronger pre-
dictor of influence in Wikipedia than in LiveJournal;
we believe that is because of a similar phenomenon,
that people in LiveJournal (who tend to know each
other) agree with each other more often. Disagree-
ment is not a strong indicator for either corpus which
may say something about influencers in general -
they can be disagreed with as often as anyone else.
9 Conclusion and Future Work
We have studied the relevance of content-related
conversational behavior (persuasion and agree-
ment/disagreement), and discourse structure-related
conversational behavior to detection of influence.
Identifying influencers is a hard task, but we are
able to show good results on the LiveJournal corpus
where we achieve an F-measure of 74.3%. Despite
a lower performance on Wikipedia, we are still able
to significantly outperform the baseline which yields
only 28.2%. Differences in performance between
the two seem to be attributable in part to the more
straightforward dialog structure in LiveJournal.
There are several areas for future work. In our
current work, we train and evaluate separately for
our two corpora. Alternatively, we could investigate
different training and testing combinations: train on
one corpus and evaluate on the other; a mixed cor-
pus for training and testing; genre-independent cri-
teria for developing different systems (e.g. length of
thread). We will also evaluate on new genres (such
as the Enron emails) in order to gain an appreciation
of how different genres of written dialog are.
Acknowledgment
This work has been supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0141. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon.
44
References
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagreement
in threaded discussion. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Computation (LREC), Istanbul, Turkey, May.
R. F. Bales, Strodtbeck, Mills F. L., T. M., and M. Rose-
borough. 1951. Channels of communication in small
groups. American Sociological Review, pages 16(4),
461?468.
R. F. Bales. 1970. Personality and interpersonal be-
haviour.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog. In Proceedings of the Fifth
IEEE International Conference on Semantic Comput-
ing.
M.E. Brook and S. H. Ng. 1986. Language and social
influence in small conversational groups. Journal of
Language and Social Psychology, pages 5(3), 201?
210.
Malcolm Gladwell. 2001. The tipping point: how little
things can make a big difference. Abacus.
Elihu Katz and Paul F. Lazarsfeld. 1955. Personal in-
fluence. Free Press, Glencoe, IL. by Elihu Katz and
Paul F. Lazarsfeld. With a foreword by Elmo Roper.
?A report of the Bureau of Applied Social Research,
Columbia University.? Bibliography: p. 381-393.
E. Katz. 1957. The Two-Step Flow of Communication:
An Up-To-Date Report on an Hypothesis. Bobbs-
Merrill Reprint Series in the Social Sciences, S137.
Ardent Media.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
S. H. Ng, D. Bell, and M. Brooke. 1993. Gaining turns
and achieving high in influence ranking in small con-
versational groups. British Journal of Social Psychol-
ogy, pages 32, 265?275.
S. H. Ng, M Brooke, and M. Dunne. 1995. Interruption
and in influence in discussion groups. Journal of Lan-
guage and Social Psychology, pages 14(4),369?381.
Daniele Quercia, Jonathan Ellis, Licia Capra, and Jon
Crowcroft. 2011. In the mood for being influential on
twitter. In SocialCom/PASSAT, pages 307?314. IEEE.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as a
resource for in influence: evidence for prototypical ar-
guments and social identification processes. European
Journal of Social Psychology, pages 30, 83?100.
Rutger Joeri Rienks. 2007. Meetings in smart environ-
ments : implications of progressing technology. Ph.D.
thesis, Enschede, the Netherlands, July.
K. R. Scherer. 1979. Voice and speech correlates of per-
ceived social influence in simulated juries. In H. Giles
and R. St Clair (Eds), Language and social psychol-
ogy, pages 88?120. Oxford: Blackwell.
Duncan Watts. 2007. The accidental influentials. Har-
vard Business Review.
45
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 91?99,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
A Generative Model of Vector Space Semantics
Jacob Andreas
Computer Laboratory
University of Cambridge
jda33@cam.ac.uk
Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk
Abstract
We present a novel compositional, gener-
ative model for vector space representa-
tions of meaning. This model reformulates
earlier tensor-based approaches to vector
space semantics as a top-down process,
and provides efficient algorithms for trans-
formation from natural language to vectors
and from vectors to natural language. We
describe procedures for estimating the pa-
rameters of the model from positive exam-
ples of similar phrases, and from distribu-
tional representations, then use these pro-
cedures to obtain similarity judgments for
a set of adjective-noun pairs. The model?s
estimation of the similarity of these pairs
correlates well with human annotations,
demonstrating a substantial improvement
over several existing compositional ap-
proaches in both settings.
1 Introduction
Vector-based word representations have gained
enormous popularity in recent years as a basic tool
for natural language processing. Various models
of linguistic phenomena benefit from the ability to
represent words as vectors, and vector space word
representations allow many problems in NLP to be
reformulated as standard machine learning tasks
(Blei et al, 2003; Deerwester et al, 1990).
Most research to date has focused on only one
means of obtaining vectorial representations of
words: namely, by representing them distribution-
ally. The meaning of a word is assumed to be
fully specified by ?the company it keeps? (Firth,
1957), and word co-occurrence (or occasionally
term-document) matrices are taken to encode this
context adequately. Distributional representations
have been shown to work well for a variety of dif-
ferent tasks (Schu?tze and Pedersen, 1993; Baker
and McCallum, 1998).
The problem becomes more complicated when
we attempt represent larger linguistic structures?
multiword constituents or entire sentences?
within the same vector space model. The most ba-
sic issue is one of sparsity: the larger a phrase, the
less frequently we expect it to occur in a corpus,
and the less data we will have from which to es-
timate a distributional representation. To resolve
this problem, recent work has focused on compo-
sitional vector space models of semantics. Based
on the Fregean observation that the meaning of a
sentence is composed from the individual mean-
ings of its parts (Frege, 1892), research in com-
positional distributional semantics focuses on de-
scribing procedures for combining vectors for in-
dividual words in order to obtain an appropriate
representation of larger syntactic constituents.
But various aspects of this account remain un-
satisfying. We have a continuous semantic space
in which finitely many vectors are associated with
words, but no way (other than crude approxima-
tions like nearest-neighbor) to interpret the ?mean-
ing? of all the other points in the space. More gen-
erally, it?s not clear that it even makes sense to talk
about the meaning of sentences or large phrases in
distributional terms, when there is no natural con-
text to represent.
We can begin to address these concerns by turn-
ing the conventional account of composition in
vector space semantics on its head, and describ-
ing a model for generating language from vectors
in semantic space. Our approach is still composi-
tional, in the sense that a sentence?s meaning can
be inferred from the meanings of its parts, but we
relax the requirement that lexical items correspond
to single vectors by allowing any vector. In the
process, we acquire algorithms for both meaning
inference and natural language generation.
Our contributions in this paper are as follows:
? A new generative, compositional model of
91
phrase meaning in vector space.
? A convex optimization procedure for map-
ping words onto their vector representations.
? A training algorithm which requires only
positive examples of phrases with the same
meaning.
? Another training algorithm which requires
only distributional representations of phrases.
? A set of preliminary experimental results in-
dicating that the model performs well on real-
world data in both training settings.
2 Model overview
2.1 Motivations
The most basic requirement for a vector space
model of meaning is that whatever distance metric
it is equipped with accurately model human judg-
ments of semantic similarity. That is: sequences
of words which are judged to ?mean the same
thing? should cluster close together in the seman-
tic space, and totally unrelated sequences of words
should be spread far apart.
Beyond this, of course, inference of vector
space representations should be tractable: we re-
quire efficient algorithms for analyzing natural
language strings into their corresponding vectors,
and for estimating the parameters of the model that
does the mapping. For some tasks, it is also useful
to have an algorithm for the opposite problem?
given a vector in the semantic space, it should be
possible to produce a natural-language string en-
coding the meaning of that vector; and, in keep-
ing with our earlier requirements, if we choose
a vector close to the vector corresponding to a
known string, the resulting interpretation should
be judged by a human to mean the same thing,
and perhaps with some probability be exactly the
same.
It is these three requirements?the use of human
similarity judgments as the measure of the seman-
tic space?s quality, and the existence of efficient al-
gorithms for both generation and inference?that
motivate the remainder of this work.
We take as our starting point the general pro-
gram of Coecke et al (2010) which suggests that
the task of analyzing into a vector space should
be driven by syntax. In this framework, the com-
positional process consists of repeatedly combin-
ing vector space word representations according to
linguistic rules, in a bottom-up process for trans-
lating a natural language string to a vector in
space.
But our requirement that all vectors be trans-
latable into meanings?that we have both analy-
sis and generation algorithms?suggests that we
should take the opposite approach, working with a
top down model of vector space semantics.
For simplicity, our initial presentation of this
model, and the accompanying experiments, will
be restricted to the case of adjective-noun pairs.
Section 5 will then describe how this framework
can be extended to full sentences.
2.2 Preliminaries
We want to specify a procedure for mapping a
natural language noun-adjective pair (a, n) into
a vector space which we will take to be Rp.
We assume that our input sentence has already
been assigned a single CCG parse (Steedman and
Baldridge, 2011), which for noun-adjective pairs
has the form
blue orangutans
N/N N >
N
(1)
Here, the parser has assigned each token a cate-
gory of the form N, N/N, etc. Categories are ei-
ther simple, drawn from a set of base types (here
just N for ?noun?), or complex, formed by com-
bining simple categories. A category of the form
X/Y ?looks right? for a category of the form Y,
and can combine with other constituents by appli-
cation (we write X/Y Y ? X) or composition
(X/Y Y/Z ? X/Z) to form higher-level con-
stituents.
To this model we add a vector space seman-
tics. We begin with a brief review the work of
Coecke et al (2010). Having assigned simple cat-
egories to vector spaces (in this case, N to Rp),
complex categories correspond to spaces of ten-
sors. A category of the form X/Y is recursively
associated with SX ? SY, where SX and SY are
the tensor spaces associated with the categories X
and Y respectively. So the space of adjectives (of
type N/N) is just Rq?Rq, understood as the set of
q? q matrices. To find the meaning of a adjective-
noun pair, we simply multiply the adjective matrix
and noun vector as specified by the CCG deriva-
tion. The result is another vector in the same se-
mantic space as the noun, as desired.
92
To turn this into a top-down process, we need
to describe a procedure for splitting meanings and
their associated categories.
2.3 Generation
Our goal in this subsection is to describe a proba-
bilistic generative process by which a vector in a
semantic space is realized in natural language.
Given a constituent of category X, and a corre-
sponding vector x residing in some SX , we can ei-
ther generate a lexical item of the appropriate type
or probabilistically draw a CCG derivation rooted
in X, then independently generate the leaves. For
noun-adjective pairs, this can only be done in one
way, namely as in (1) (for a detailed account of
generative models for CCG see Hockenmaier and
Steedman (2002)). We will assume that this CCG
derivation tree is observed, and concern ourselves
with filling in the appropriate vectors and lexical
items. This is a strong independence assumption!
It effectively says ?the grammatical realization of
a concept is independent of its meaning?. We will
return to it in Section 6.
The adjective-noun model has four groups of
parameters: (1) a collection ?N/N of weight vec-
tors ?a for adjectives a, (2) a collection ?N of
weight vectors ?n for nouns n, (3) a collection
EN/N of adjective matrices Ea for adjectives a, and
finally (4) a noise parameter ?2. For compactness
of notation we will denote this complete set of pa-
rameters ?.
Now we can describe how to generate an
adjective-noun pair from a vector x. The CCG
derivation tells us to produce a noun and an ad-
jective, and the type information further informs
us that the adjective acts as a functor (here a ma-
trix) and the noun as an argument. We begin by
choosing an adjective a conditional on x. Having
made our lexical choice, we deterministically se-
lect the corresponding matrix Ea from EN/N. Next
we noisily generate a new vector y = Eax + ?,
a vector in the same space as x, corresponding
to the meaning of x without the semantic content
of a. Finally, we select a noun n conditional on
y, and output the noun-adjective pair (a, n). To
use the previous example, suppose x means blue
orangutans. First we choose an adjective a =
?blue? (or with some probability ?azure? or ?ultra-
marine?), and select a corresponding adjectiveEa.
Then the vector y = Eax should mean orangutan,
and when we generate a noun conditional on y we
should have n = ?orangutan? (or perhaps ?mon-
key?, ?primate?, etc.).
This process can be summarized with the graph-
ical model in Figure 1. In particular, we draw a
?N/N
a
x
Ea y n
EN/N ?2 ?N
Figure 1: Graphical model of the generative pro-
cess.
from a log-linear distribution over all words of the
appropriate category, and use the corresponding
Ea with Gaussian noise to map x onto y:
p(a|x; ?N/N) =
exp(?>a x)?
????N/N
exp(??>x)
(2)
p(y|x,Ea;?
2) = N (Eax, ?
2)(y) (3)
Last we choose n as
p(n|y; ?N) =
exp(?>n y)?
????N
exp(??>z)
(4)
Some high-level intuition about this model: in
the bottom-up account, operators (drawn from ten-
sor spaces associated with complex categories)
can be thought of as acting on simple objects and
?adding? information to them. (Suppose, for ex-
ample, that the dimensions of the vector space cor-
respond to actual perceptual dimensions; in the
bottom-up account the matrix corresponding to the
adjective ?red? should increase the component of
an input vector that lies in dimension correspond-
ing to redness.) In our account, by contrast, ma-
trices remove information, and the ?red? matrix
should act to reduce the vector component corre-
sponding to redness.
2.4 Analysis
Now we must solve the opposite problem: given
an input pair (a, n), we wish to map it to an ap-
propriate vector in Rp. We assume, as before, that
we already have a CCG parse of the input. Then,
analysis corresponds to solving the following op-
timization problem:
arg min
x
? log p(x|a, n;?)
93
By Bayes? rule,
p(x|a, n;?) ? p(a, n|x;?)p(x)
so it suffices to minimize
? log p(x)? log p(a, n|x;?)
To find the single best complete derivation of an
input pair (equivalent to the Viterbi parse tree in
syntactic parsing), we can rewrite this as
arg min
x,y
? log p(x)? log p(a, b, y|x;?) (5)
where, as before, y corresponds to the vector space
semantics representation of the noun alone. We
take our prior log p(x) to be a standard normal.
We have:
? log p(a, n, y|x)
= ? log p(a|x;?)? log p(y|a, x;?)
? log p(n|y;?)
? ??>a x+ log
?
????N/N
exp ??>x
+
1
?2
||Eax? y||
2
? ?>n y + log
?
????N
exp ??>y
Observe that this probability is convex: it con-
sists of a sum of linear terms, Euclidean norms,
and log-normalizers, all convex functions. Conse-
quently, Equation 5 can be solved exactly and ef-
ficiently using standard convex optimization tools
(Boyd and Vandenberghe, 2004).
3 Relation to existing work
The approach perhaps most closely related to the
present work is the bottom-up account given by
Coecke et al (2010), which has already been dis-
cussed in some detail in the preceding section. A
regression-based training procedure for a similar
model is given by Grefenstette et al (2013). Other
work which takes as its starting point the decision
to endow some (or all) lexical items with matrix-
like operator semantics include that of Socher et
al. (2012) and Baroni and Zamparelli (2010). In-
deed, it is possible to think of the model in Ba-
roni and Zamparelli?s paper as corresponding to
a training procedure for a special case of this
model, in which the positions of both nouns and
noun-adjective vectors are fixed in advance, and in
which no lexical generation step takes place. The
adjective matrices learned in that paper correspond
to the inverses of the E matrices used above.
Also relevant here is the work of Mitchell and
Lapata (2008) and Zanzotto et al (2010), which
provide several alternative procedures for compos-
ing distributional representations of words, and
Wu et al (2011), which describes a compositional
vector space semantics with an integrated syntac-
tic model. Our work differs from these approaches
in requiring only positive examples for training,
and in providing a mechanism for generation as
well as parsing. Other generative work on vec-
tor space semantics includes that of Hermann et
al. (2012), which models the distribution of noun-
noun compounds. This work differs from the
model that paper in attempting to generate com-
plete natural language strings, rather than simply
recover distributional representations.
In training settings where we allow all posi-
tional vectors to be free parameters, it?s possible
to view this work as a kind of linear relational em-
bedding (Paccanaro and Hinton, 2002). It differs
from that work, obviously, in that we are interested
in modeling natural language syntax and seman-
tics rather than arbitrary hierarchical models, and
provide a mechanism for realization of the embed-
ded structures as natural language sentences.
4 Experiments
Since our goal is to ensure that the distance be-
tween natural language expressions in the vector
space correlates with human judgments of their
relatedness, it makes sense to validate this model
by measuring precisely that correlation. In the re-
mainder of this section, we provide evidence of the
usefulness of our approach by focusing on mea-
surements of the similarity of adjective-noun pairs
(ANs). We describe two different parameter esti-
mation procedures for different kinds of training
data.
4.1 Learning from matching pairs
We begin by training the model on matching
pairs. In this setting, we start with a collec-
tion N sets of up to M adjective-noun pairs
(ai1, ni1), (ai2, ni2), . . . which mean the same
thing. We fix the vector space representation yi of
each noun ni distributionally, as described below,
and find optimal settings for the lexical choice pa-
rameters ?N/N and ?N, matrices (here all q ? q)
94
EN/N, and, for each group of adjective-noun pairs
in the training set, a latent representation xi. The
fact that the vectors yi are tied to their distribu-
tional vectors does not mean we have committed
to the distributional representation of the corre-
sponding nouns! The final model represents lexi-
cal choice only with the weight vectors ??fixing
the vectors just reduces the dimensionality of the
parameter estimation problem and helps steer the
training algorithm toward a good solution. The
noise parameter then acts as a kind of slack vari-
able, modeling the fact that there may be no pa-
rameter setting which reproduces these fixed dis-
tributional representations through exact linear op-
erations alone.
We find a maximum-likelihood estimate for
these parameters by minimizing
L(?, x) = ?
N?
i=1
M?
i=1
log p(aij , nij |xi;?) (6)
The latent vectors xi are initialized to one of their
corresponding nouns, adjective matricesE are ini-
tialized to the identity. The components of ?N are
initialized identically to the nouns they select, and
the components of ?N/N initialized randomly. We
additionally place an L2 regularization penalty on
the scoring vectors in both ? (to prevent weights
from going to infinity) and E (to encourage adjec-
tives to behave roughly like the identity). These
penalties, as well as the noise parameter, are ini-
tially set to 0.1.
Note that the training objective, unlike the
analysis objective, is non-convex. We use L-
BFGS (Liu and Nocedal, 1989) on the likeli-
hood function described above with ten such ran-
dom restarts, and choose the parameter setting
which assigns the best score to a held-out cross-
validation set. Computation of the objective and
its gradient at each step is linear in the number of
training examples and quadratic in the dimension-
ality of the vector space.
Final evaluation is performed by taking a set of
pairs of ANs which have been assigned a similar-
ity score from 1?6 by human annotators. For each
pair, we map it into the vector space as described
in Section 2.4 above. and finally compute the co-
sine similarity of the two pair vectors. Perfor-
mance is measured in the correlation (Spearman?s
?) between these cosine similarity scores and the
human similarity judgments.
4.1.1 Setup details
Noun vectors yi are estimated distributionally
from a corpus of approximately 10 million tokens
of English-language Wikipedia data (Wikimedia
Foundation, 2013). A training set of adjective-
noun pairs are collected automatically from a col-
lection of reference translations originally pre-
pared for a machine translation task. For each
foreign sentence we have four reference transla-
tions produced by different translators. We as-
sign POS tags to each reference (Loper and Bird,
2002) then add to the training data any adjec-
tive that appears exactly once in multiple refer-
ence translations, with all the nouns that follow it
(e.g. ?great success?, ?great victory?, ?great ac-
complishment?). We then do the same for repeated
nouns and the adjectives that precede them (e.g.
?great success?, ?huge success?, ?tremendous suc-
cess?). This approach is crude, and the data col-
lected are noisy, featuring such ?synonym pairs?
as (?incomplete myths?, ?incomplete autumns?)
and (?similar training?, ?province-level training?),
as well as occasional pairs which are not adjective-
noun pairs at all (e.g. ?first parliamentary?). Nev-
ertheless, as results below suggest, they appear to
be good enough for purposes of learning an appro-
priate representation.
For the experiments described in this section,
we use 500 sets of such adjective-noun pairs,
corresponding to 1104 total training examples.
Testing data consists of the subset of entries in
the dataset from (Mitchell and Lapata, 2010) for
which both the adjective and noun appear at least
once (not necessarily together) in the training set,
a total of 396 pairs. None of the pairs in this test
set appears in training. We additionally withhold
from this set the ten pairs assigned a score of 6
(indicating exact similarity), setting these aside for
cross-validation.
In addition to the model discussed in the first
section of this paper (referred to here as ?GEN?),
we consider a model in which there is only one
adjective matrix E used regardless of the lexical
item (referred to as ?GEN-1?).
The NP space is taken to be R20, and we re-
duce distributional vectors to 20 dimensions using
a singular value decomposition.
95
4.2 Learning from distributional
representations
While the model does not require distributional
representations of latent vectors, it?s useful to con-
sider whether it can also provide a generative ana-
log to recent models aimed explicitly at produc-
ing vectorial representations of phrases given only
distributional representations of their constituent
words. To do this, we take as our training data a
set of N single ANs, paired with a distributional
representation of each AN. In the new model, the
meaning vectors x are no longer free parameters,
but fully determined by these distributional repre-
sentations. We must still obtain estimates for each
? and EN/N, which we do by minimizing
L(?) = ?
N?
i=1
log p(ai,j , ni,j |xi;?) (7)
4.2.1 Experimental setup
Experimental setup is similar to the previous sec-
tion; however, instead of same-meaning pairs col-
lected from a reference corpus, our training data
is a set of distributional vectors. We use the same
noun vectors, and obtain these new latent pair vec-
tors by estimating them in the same fashion from
the same corpus.
In order to facilitate comparison with the other
experiment, we collect all pairs (ai, ni) such that
both ai and ni appear in the training set used in
Section 4.1 (although, once again, not necessar-
ily together). Initialization of ? and E , regular-
ization and noise parameters, as well as the cross-
validation procedure, all proceed as in the previ-
ous section. We also use the same restricted eval-
uation set, again to allow the results of the two
experiments to be compared. We evaluate by mea-
suring the correlation of cosine similarities in the
learned model with human similarity judgments,
and as before consider a variant of the model in
which a single adjective matrix is shared.
4.3 Results
Experimental results are displayed in Table 1. For
comparison, we also provide results for a base-
line which uses a distributional representation of
the noun only, the Adjective-Specific Linear Map
(ALM) model of Baroni and Zamparelli (2010) and
two vector-based compositional models discussed
in (Mitchell and Lapata, 2008): , which takes
the Hadamard (elementwise) product of the distri-
butional representations of the adjective and noun,
and +, which adds the distributions. As before,
we use SVD to project these distributional repre-
sentations onto a 20-dimensional subspace.
We observe that in both matrix-based learn-
ing settings, the GEN model or its parameter-
tied variant achieves the highest score (though
the distributionally-trained GEN-1 doesn?t per-
form as well as the summing approach). The pair-
trained model performs best overall. All corre-
lations except  and the distributionally-trained
GEN are statistically significant (p < 0.05), as
are the differences in correlation between the
matching-pairs-trained GEN and all other mod-
els, and between the distributionally-trained GEN-
1 and ALM. Readers familiar with other papers
employing the similarity-judgment evaluation will
note that scores here are uniformly lower than re-
ported elsewhere; we attribute this to the compar-
atively small training set (with hundreds, instead
of thousands or tens of thousands of examples).
This is particularly notable in the case of the ALM
model, which Baroni and Zamparelli report out-
performs the noun baseline when given a training
set of sufficient size.
Training data Model ?
Word distributions Noun .185
+ .239
 .000
Matching pairs GEN-1 .130
GEN .365
Word and phrase ALM .136
distributions GEN-1 .201
GEN .097
Table 1: Results for the similarity judgment exper-
iment.
We also give a brief demonstration of the gen-
eration capability of this model as shown in Fig-
ure 2. We demonstrate generation from three dif-
ferent vectors: one inferred as the latent represen-
tation of ?basic principles? during training, one
obtained by computing a vectorial representation
of ?economic development? as described in Sec-
tion 2.4 and one selected randomly from within
vector space. We observe that the model cor-
rectly identifies the adjectives ?fundamental? and
?main? as synonymous with ?basic? (at least when
applied to ?principles?). It is also able to cor-
rectly map the vector associated with ?economic
96
Input Realization
Training vector tyrannical principles
(?basic principles?) fundamental principles
main principles
Test vector economic development
(?economic development?) economic development
economic development
Random vector vital turning
further obligations
bad negotiations
Figure 2: Generation examples using the GEN
model trained with matching pairs.
development? back onto the correct lexical real-
ization. Words generated from the random vector
appear completely unrelated; this suggests that we
are sampling a portion of the space which does not
correspond to any well-defined concept.
4.4 Discussion
These experimental results demonstrate, first and
foremost, the usefulness of a model that is not tied
to distributional representations of meaning vec-
tors: as the comparatively poor performance of
the distribution-trained models shows, with only
a small number of training examples it is better to
let the model invent its own latent representations
of the adjective-noun pairs.
It is somewhat surprising, in the experi-
ments with distributional training data, that the
single-adjective model outperforms the multiple-
adjective model by so much. We hypothesize that
this is due to a search error?the significantly ex-
panded parameter space of the multiple-adjective
model makes it considerably harder to estimate
parameters; in the case of the distribution-only
model it is evidently so hard the model is unable
to identify an adequate solution even over multiple
training runs.
5 Extending the model
Having described and demonstrated the usefulness
of this model for capturing noun-adjective similar-
ity, we now describe how to extend it to capture
arbitrary syntax. While appropriate experimental
evaluation is reserved for future work, we outline
the formal properties of the model here. We?ll take
as our example the following CCG derivation:
sister Cecilia has blue orangutans
N/N N (S\N)/N N/N N
> >
N N
>
S\N
<
S
Observe that ?blue orangutans? is generated ac-
cording to the noun-adjective model already de-
scribed.
5.1 Generation
To handle general syntax, we must first extend the
set EN/N of adjective matrices to sets EX for all
functor categories X, and create an additional set
of weight vectors ?X for every category X.
When describing how to generate one split in
the CCG derivation (e.g. a constituent of type S
into constituents of type NP and S\NP), we can
identify three cases. The first, ?fully-lexicalized?
case is the one already described, and is the gen-
erative process by which the a vector meaning
blue orangutans is transformed into ?blue? and
?orangutans?, or sister Cecilia into ?sister? and
?Cecilia?. But how do we get from the top-level
sentence meaning to a pair of vectors meaning
sister Cecilia and has blue orangutans (an ?un-
lexicalized? split), and from has blue orangutans
to the word ?has? and a vector meaning blue
orangutans (a ?half-lexicalized? split)?
Unlexicalized split We have a vector xwith cat-
egory X, from which we wish to obtain a vector y
with category Y, and z with category Z. For this we
further augment the sets E with matrices indexed
by category rather than lexical item. Then we pro-
duce y = EYx + ?, z = EZ + ? where, as in the
previous case, ? is Gaussian noise with variance
?2. We then recursively generate subtrees from y
and z.
Half-lexicalized split This proceeds much as in
the fully lexicalized case. We have a vector x from
which we wish to obtain a vector y with category
Y, and a lexical item w with category Z.
We choose w according to Equation 2, select
a matrix Ew and produce y = Ewx + ? as be-
fore, and then recursively generate a subtree from
y without immediately generating another lexical
item for y.
5.2 Analysis
As before, it suffices to minimize
? log p(x) ? log p(W,P |x) for a sentence
97
W = (w1, w2, ? ? ? , wn) and a set of internal
vectors P . We select our prior p(x) exactly as
before, and can define p(W,P |x) recursively. The
fully-lexicalized case is exactly as above. For the
remaining cases, we have:
Unlexicalized split Given a subsequence
Wi:j = (wi, ? ? ? , wj), if the CCG parse splits Wi:j
into constituents Wi:k and Wk:j , with categories
Y and Z, we have:
? log p(Wi:j |x) =? log p(Wi:k, P |EY x)
? log p(Wk:j , P |EZx)
Half-lexicalized split If the parse splits Wi:j
into wi and Wi+1:j with categories Y and Z, and
y ? P is the intermediate vector used at this step
of the derivation, we have:
? log p(Wi:j , y|x)
= ? log p(wi|x)? log p(y|x,wi)
? log p(Wi+1:j |y)
? ??Twix+ log
?
w??LY
exp ?Tw?x
+
1
?2
||Ewix? y||
2
? log p(Wi+1:j , P |y)
Finally, observe that the complete expression
of the log probability of any derivation is, as be-
fore, a sum of linear and convex terms, so the
optimization problem remains convex for general
parse trees.
6 Future work
Various extensions to the model proposed in this
paper are possible. The fact that relaxing the
distributional requirement for phrases led to per-
formance gains suggests that something similar
might be gained from nouns. If a reliable train-
ing procedure could be devised with noun vectors
as free parameters, it might learn an even better
model of phrase similarity?and, in the process,
simultaneously perform unsupervised word sense
disambiguation on the training corpus.
Unlike the work of Coecke et al (2010), the
structure of the types appearing in the CCG deriva-
tions used here are neither necessary nor sufficient
to specify the form of the matrices used in this
paper. Instead, the function of the CCG deriva-
tion is simply to determine which words should
be assigned matrices, and which nouns. While
CCG provides a very natural way to do this, it
is by no means the only way, and future work
might focus on providing an analog using a differ-
ent grammar?all we need is a binary-branching
grammar with a natural functor-argument distinc-
tion.
Finally, as mentioned in Section 2.3, we have
made a significant independence assumption in re-
quiring that the entire CCG derivation be gener-
ated in advance. This assumption was necessary
to ensure that the probability of a vector in mean-
ing space given its natural language representation
would be a convex program. We suspect, however,
that it is possible to express a similar probabil-
ity for an entire packed forest of derivations, and
optimize it globally by means of a CKY-like dy-
namic programming approach. This would make
it possible to optimize simultaneously over all pos-
sible derivations of a sentence, and allow positions
in meaning space to influence the form of those
derivations.
7 Conclusion
We have introduced a new model for vector
space representations of word and phrase mean-
ing, by providing an explicit probabilistic process
by which natural language expressions are gener-
ated from vectors in a continuous space of mean-
ings. We?ve given efficient algorithms for both
analysis into and generation out of this meaning
space, and described two different training proce-
dures for estimating the parameters of the model.
Experimental results demonstrate that these al-
gorithms are capable of modeling graded human
judgments of phrase similarity given only positive
examples of matching pairs, or distributional rep-
resentations of pairs as training data; when trained
in this fashion, the model outperforms several
other compositional approaches to vector space
semantics. We have concluded by suggesting how
syntactic information might be more closely inte-
grated into this model. While the results presented
here are preliminary, we believe they present com-
pelling evidence of representational power, and
motivate further study of related models for this
problem.
Acknowledgments
We would like to thank Stephen Clark and An-
dreas Vlachos for feedback on a draft of this pa-
per.
98
References
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of the 21st annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 96?103.
ACM.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of Ma-
chine Learning Research, 3:993?1022.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex optimization. Cambridge university press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. arXiv
preprint arXiv:1003.4394.
Scott Deerwester, Susan T. Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391?407.
John Rupert Firth. 1957. A synopsis of linguistic the-
ory, 1930-1955.
Gottlob Frege. 1892. Uber Sinn und Bedeutung.
Zeitschrift fur Philosophie und philosophische Kri-
tik, pages 25?50. English Translation: em On Sense
and Meaning, in Brian McGuinness (ed), em Frege:
collected works, pp. 157?177, Basil Blackwell, Ox-
ford.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Karl Moritz Hermann, Phil Blunsom, and Stephen Pul-
man. 2012. An unsupervised ranking model for
noun-noun compositionality. In Proceedings of the
First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 132?141. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 335?342. Association for Com-
putational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Edward Loper and Steven Bird. 2002. Nltk: the nat-
ural language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodologies
for teaching natural language processing and com-
putational linguistics - Volume 1, ETMTNLP ?02,
pages 63?70, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of
ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Alberto Paccanaro and Jefferey Hinton. 2002. Learn-
ing hierarchical structures with linear relational em-
bedding. In Advances in Neural Information Pro-
cessing Systems 14: Proceedings of the 2001 Neural
Information Processing Systems (NIPS) Conference,
volume 14, page 857. MIT Press.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector
model for syntagmatic and paradigmatic relatedness.
Making sense of words, pages 104?113.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. Non-Transformational
Syntax Oxford: Blackwell, pages 181?224.
Wikimedia Foundation. 2013. Wikipedia.
http://dumps.wikimedia.org/enwiki/. Accessed:
2013-04-20.
Stephen Wu, William Schuler, et al 2011. Struc-
tured composition of semantic vectors. In Proceed-
ings of the Ninth International Conference on Com-
putational Semantics (IWCS 2011), pages 295?304.
Citeseer.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
99
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58?67,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Grounding Language with Points and Paths in Continuous Spaces
Jacob Andreas and Dan Klein
Computer Science Division
University of California, Berkeley
{jda,klein}@cs.berkeley.edu
Abstract
We present a model for generating path-
valued interpretations of natural language
text. Our model encodes a map from
natural language descriptions to paths,
mediated by segmentation variables which
break the language into a discrete set of
events, and alignment variables which
reorder those events. Within an event,
lexical weights capture the contribution of
each word to the aligned path segment.
We demonstrate the applicability of our
model on three diverse tasks: a new color
description task, a new financial news task
and an established direction-following
task. On all three, the model outperforms
strong baselines, and on a hard variant of
the direction-following task it achieves
results close to the state-of-the-art system
described in Vogel and Jurafsky (2010).
1 Introduction
This paper introduces a probabilistic model for
predicting grounded, real-valued trajectories from
natural language text. A long tradition of re-
search in compositional semantics has focused on
discrete representations of meaning. The origi-
nal focus of such work was on logical translation:
mapping statements of natural language to a for-
mal language like first-order logic (Zettlemoyer
and Collins, 2005) or database queries (Zelle and
Mooney, 1996). Subsequent work has integrated
this logical translation with interpretation against
a symbolic database (Liang et al., 2013).
There has been a recent increase in interest
in perceptual grounding, where lexical semantics
anchor in perceptual variables (points, distances,
etc.) derived from images or video. Bruni et al.
(2014) describe a procedure for constructing word
representations using text- and image-based dis-
% Chan
ge
0.98
0.99
1.00
1.01
1.02
Hour of day10 12 14 10 12 14
U.S. stocks rebound after bruising two-day swoon
Figure 1: Example stock data. The chart displays
index value over a two-day period (divided by the
dotted line), while the accompanying headline de-
scribes the observed behavior.
tributional information. Yu and Siskind (2013)
describe a model for identifying scenes given de-
scriptions, and Golland et al. (2010), Kollar et al.
(2010), and Krishnamurthy and Kollar (2013) de-
scribe models for identifying individual compo-
nents of scenes described by text. These all have
the form of matching problems between text and
observed groundings?what has been missing so
far is the ability to generate grounded interpreta-
tions from scratch, given only text.
Our work continues in the tradition of this per-
ceptual grounding work, but makes two contribu-
tions. First, our approach is able to predict simple
world states (and their evolution): for a general
class of continuous domains, we produce a repre-
sentation of p(world | text) that admits easy sam-
pling and maximization. This makes it possible to
produce grounded interpretations of text without
reference to a pre-existing scene. Simultaneously,
we extend the range of temporal phenomena that
can be modeled?unlike the aforementioned spa-
tial semantics work, we consider language that de-
58
scribes time-evolving trajectories, and unlike Yu
and Siskind (2013), we allow these trajectories to
have event substructure, and model temporal or-
dering. Our class of models generalizes to a vari-
ety of different domains: a new color-picking task,
a new financial news task, and a more challenging
variant of the direction-following task established
by Vogel and Jurafsky (2010).
As an example of the kinds of phenomena we
want to model, consider Figure 1, which shows
the value of the Dow Jones Industrial Average
over June 3rd and 4th 2008, along with a finan-
cial news headline from June 4th. There are sev-
eral effects of interest here. One phenomenon we
want to capture is that the lexical semantics of in-
dividual words must be combined: swoon roughly
describes a drop while bruising indicates that the
drop was severe. We isolate this lexical combi-
nation in Section 4, where we consider a limited
model of color descriptions (Figure 2). A second
phenomenon is that the description is composed
of two separate events, a swoon and a rebound;
moreover, those events do not occur in their tex-
tual order, as revealed by after. In Section 5, we
extend the model to include segmentation and or-
dering variables and apply it to this stock data.
The situation where language describes a
path through some continuous space?literal or
metaphorical?is more general than stock head-
lines. Our claim is that a variety of problems
in language share these same characteristics. To
demonstrate generality of the model, we also ap-
ply it in Section 6 to a challenging variant of the
direction-following task described by Vogel and
Jurafsky (2010) (Figure 3), where we achieve re-
sults close to a state-of-the-art system that makes
stronger assumptions about the task.
2 Three tasks in grounded semantics
The problem of inferring a structured state repre-
sentation from sensory input is a hard one, but we
can begin to tackle grounded semantics by restrict-
ing ourselves to cases where we have sequences
of real-valued observations directly described by
text. In this paper we?ll consider the problems
of recognizing colors, describing time series, and
following navigational instructions. While these
tasks have been independently studied, we believe
that this is the first work which presents them in
a unified framework, and carries them out with a
single family of models.
.
dark pastel blue
(a) (b)
Figure 2: Example color data: (a) a named color;
(b) its coordinates in color space.
Colors Figure 2 shows a color called dark pas-
tel blue. English speakers, even if unfamiliar with
the specific color, can identify roughly what the
name signifies because of prior knowledge of the
meanings of the individual words.
Because the color domain exhibits lexical com-
positionality but not event structure, we present it
here to isolate the non-temporal compositional ef-
fects in our model. Any color visible to the human
eye can be identified with three coordinates, which
we?ll take to be hue, saturation and value (HSV).
As can be seen in Figure 2 the ?hue? axis corre-
sponds to the differentiation made by basic color
names in most languages. Other modifiers act on
the saturation and value axes: either simple ones
like dark (which decreases value), or more compli-
cated ones like pastel (which increases value and
decreases saturation). Given a set of named colors
and their HSV coordinates, a learning algorithm
should be able to identify the effects of each word
in the vocabulary and predict the appearance of
new colors with previously-unseen combinations
of modifiers.
Compositional interpretations of color have re-
ceived attention in linguistics and philosophy of
language (Kennedy andMcNally, 2010), but while
work in grounded computational semantics like
that of Krishnamurthy and Kollar (2013) has suc-
ceeded in learning simple color predicates, our
model is the first to capture the machine learning
of color in a fine-grained, compositional way.
Time series As a first step into temporal struc-
ture, we?ll consider language describing the be-
havior of stock market indices. Here, again, there
is a simple parameterization?in this case just a
single number describing the total value of the
index?but as shown by the headline example in
Figure 1, the language used to describe changes
in the stock market can be quite complex. Head-
59
right round the white water [. . . ] but stay quite close ?cause
you don?t otherwise you?re going to be in that stone creek
Figure 3: Example map data: a portion of a map,
and a single line from a dialog which describes
navigation relative to the two visible landmarks.
lines may describe multiple events, or multi-part
events like rebound or extend; stocks do not sim-
ply rise or fall, but stagger, stumble, swoon, and
so on. There are compositional effects here as
well: distinction is made between falling and
falling sharply; gradual trends are distinguished
from those which occur suddenly, at the beginning
or end of the trading day. Along with temporal
structure, the problem requires a more sophisti-
cated treatment of syntax than the colors case?
now we have to identify which subspans of the
sentence are associated with each event observed,
and determine the correspondence between sur-
face order and actual order in time.
The learning of correspondences between text
and time series has attracted more interest in nat-
ural language generation than in semantics (Yu et
al., 2007). Research on natural language process-
ing and stock data, meanwhile, has largely focused
on prediction of future events (Kogan et al., 2009).
Direction following We?ll conclude by apply-
ing our model to the well-studied problem of
following navigational directions. A variety of
reinforcement-learning approaches for following
directions on a map were previously investigated
by Vogel and Jurafsky (2010) using a corpus as-
sembled by Anderson et al. (1991). An example
portion of a path and its accompanying instruction
is shown in Figure 3. While also representable as
a set of real valued coordinates, here 2-d, this data
set looks very different?a typical example con-
sists of more than a hundred sentences of the kind
shown in Figure 3, accompanying a long path. The
language, a transcript of a spoken dialog, is also
considerably less formal than the language found
in the Wall Street Journal examples, involving dis-
fluency, redundancy and occasionally errors. Nev-
ertheless the underlying structure of this problem
and the stock problem are fundamentally similar.
In addition to Vogel and Jurafsky, Tellex et al.
(2011) give a weakly-supervised model for map-
ping single sentences to commands, and Brana-
van et al. (2009) give an alternative reinforcement-
learning approach for following long command se-
quences. An intermediate between this approach
and ours is the work of Chen and Mooney (2011)
and Artzi and Zettlemoyer (2013), which boot-
strap a semantic parser to generate logical forms
specifying the output path, rather than predicting
the path directly.
Between them, these tasks span a wide range of
linguistic phenomena relevant to grounded seman-
tics, and provide a demonstration of the useful-
ness and general applicability of our model. While
development of the perceptual groundwork neces-
sary to generalize these results to more complex
state spaces remains a major problem, our three
examples provide a starting point for studying the
relationship between perception, time and the se-
mantics of natural language.
3 Preliminaries
In the experiments that follow, each training ex-
ample will consist of:
? Natural language text, consisting of a con-
stituency parse tree or trees. For a given ex-
ample, we will denote the associated trees
(T
1
, T
2
, . . .). These are also observed at test
time, and used to predict new groundings.
? A vector-valued, grounded observation, or
a sequence of observations (a path), which
we will denote V for a given example. We
will further assume that each of these paths
has been pre-segmented (discussed in detail
in Section 5) into a sequence (V
1
,V
2
, . . .).
These are only observed during training.
The probabilistic backbone of our model is a
collection of linear and log-linear predictors. Thus
it will be useful to work with vector-valued rep-
resentations of both the language and the path,
which we accomplish with a pair of feature func-
tions ?
t
and ?
v
. As the model is defined only
in terms of these linear representations, we can
60
?t
(T )
?
Label at root of T
?
Lemmatized leaves of T
?
v
(V )
?
Last element of V
?
Curvature of quadratic
approx. to V (stocks only)
?
a
(T,A
i
, A
i?1
)
Cartesian prod. of ?
t
(T ) with:
?
I[A
i
is aligned]
?
I[A
i?1
is aligned]
?
A
1
?A
i?1
(if both aligned)
Table 1: Features used for linear parameterization
of the grounding model.
simplify notation by writing T
i
= ?
t
(T
i
) and
V
i
= ?
v
(V
i
). As the ultimate prediction task is to
produce paths, and not their featurized representa-
tions, we will assume that it is also straightforward
to compute ?
?1
v
, which projects path features back
into the original grounding domain.
All parse trees are predicted from input text us-
ing the Berkeley Parser (Petrov and Klein, 2007).
Feature representations for both trees and paths are
simple and largely domain-independent; they are
explicitly enumerated in Table 1.
The general framework presented here leaves
one significant problem unaddressed: given a large
state vector encoding properties of multiple ob-
jects, how do we resolve an utterance about a sin-
gle object to the correct subset of indices in the
vector? While none of the tasks considered in this
paper require an argument resolution step of this
kind, interpretation of noun phrases is one of the
better-studied problems in compositional seman-
tics (Zelle and Mooney (1996), inter alia), and
we expect generalization of this approach to be
straightforward using these tools.
We will consider the color, stock, and naviga-
tion tasks in turn. It is possible to view the models
we give for all three as instantiations of the same
graphical model, but for ease of presentation we
will introduce this model incrementally.
4 Predicting vectors
Prediction of a color variable from text has the
form of a regression problem: given a vector of
lexical features extracted from the name, we wish
to predict the entries of a vector in color space. It
seems linguistically plausible that this regression
is sparse and linear: that most words, if they pro-
vide any constraints at all, tend to express prefer-
ences about a subset of the available dimensions;
and that composition within the domain of a sin-
gle event largely consists of words additively pre-
dicting that event?s parameters, without complex
nonlinear interactions. This is motivated by the
observation that pragmatic concerns force linguis-
tic descriptors to orient themselves along a small
set of perceptual bases: once we have words for
north and east, we tend to describe intermediates
as northeast rather than inventing an additional
word which means ?a little of both?.
As discussed above, we can represent a color as
a point in a three-dimensional HSV space. Let T
denote features on the parse tree of the color name,
and V its representation in color space (consistent
with the definition of ?
v
given in Table 1). Linear-
ity suggests the following model:
p(T, V ) ? e
?
?
?
?
t
T?V
?
2
2
(1)
The learning problem is then:
argmin
?
t
?
T,V
?
?
?
?
?
t
T ? V
?
?
?
2
2
(2)
which, with a sparse prior on ?
t
, is the proba-
bilistic formulation of Lasso regression (Tibshi-
rani, 1996), for which standard tools are available
in the optimization literature.
To predict color space values from a new (fea-
turized) name T , we output:
argmax
V
p(T, V ) = ?
?
t
T
4.1 Evaluation
We collect a set of color names and their
corresponding HSV triples from the English
Wikipedia?s List of Colors, retaining only those
color names in which every word appears at least
three times in the training corpus. This leaves a
set of 419 colors, which we randomly divide into
a 377-item training set and 42-item test set. The
model?s goal will be to learn to identify new col-
ors given only their names.
We consider two evaluations: one which mea-
sures the model?s ability to distinguish the named
color from a random alternative?analogous to the
evaluation in Yu and Siskind (2013)?and one
which measures the absolute difference between
predicted and true color values. In particular, in
the first evaluation the model is presented with
the name of a color and a pair of candidates, one
61
Method Sel. ? H ? S ? V ?
Random 0.50 0.30 0.38 0.39
Last word 0.78 0.05 0.26 0.17
Full model 0.81 0.07 0.21 0.13
Human 0.86 - - -
Table 2: Results for the color selection task.
Sel(ection accuracy) is frequency with which the
system was able to correctly identify the color de-
scribed when paired with a random alternative.
Other columns are the magnitude of the average
prediction error along the axes of the color space.
Full model selection accuracy is a statistically sig-
nificant (p < 0.05) improvement over the baseline
using a paired sign test.
the color corresponding to the name and another
drawn randomly from the test set, and report the
fraction of times the true color is assigned a higher
probability than the random alternative. In the sec-
ond, we report the absolute value of the difference
between true and predicted hue, saturation, and lu-
minosity.
We compare against two baselines: one which
looks only at the last word in the color name (al-
most always a hue category), and so captures no
compositional effects, and another which outputs
random values for all three coordinates. Results
are shown in Table 2. The model with all lexical
features outperforms both baselines on selection
and all but one absolute error metric.
4.2 Error analysis
An informal experiment in which the color selec-
tion task was repeated on one of the authors? col-
leagues (the ?Human? row in Table 2) yielded an
accuracy of 86%, only 5% better than the system.
While not intended as a rigorous upper bound on
performance, this suggests that the model capac-
ity and training data are sufficient to capture most
interesting color behavior. The errors that do oc-
cur appear to mostly be of two kinds. In one case,
a base color is seen only with a small (or related)
set of modifiers, from which the system is unable
to infer the meaning of the base color (e.g. from
Japanese indigo, lavender indigo, and electric in-
digo, the learning algorithm infers that indigo is
bright purple). In the other, no part of the color
word is seen in training, and the system outputs an
unrelated ?default? color (teal is predicted to be
bright red).
5 Predicting paths
The idea that a sentence?s meaning is fundamen-
tally described by a set of events, each associated
with a set of predicates, is well-developed in neo-
Davidsonian formal semantics (Parsons, 1990).
We adopt the skeleton of this formal approach by
tying our model to (latent) partitions of the in-
put sentence into disjoint events. Rather than at-
tempting to pass through a symbolic meaning rep-
resentation, however, this event structure will be
used to map text directly into the grounding do-
main. We assume that this domain has pre-existing
structure?in particular, that in our input paths V ,
the boundaries of events have already been iden-
tified, and that the problem of aligning text to
portions of the segment only requires aligning to
segment indices rather than fine-grained time in-
dices. This is a strong assumption, and one that
future work may wish to revisit, but there exist
both computational tools from the changepoint de-
tection literature (Basseville and Nikiforov, 1995)
and pieces of evidence from cognitive science (Za-
cks and Swallow, 2007) which suggest that assum-
ing a pre-linguistic structuring of events is a rea-
sonable starting point.
In the text domain, we make the corresponding
assumption that each of these events is syntacti-
cally local?that a given span of the input sentence
provides information about at most one of these
segmented events.
The main structural difference between the
color example in Figure 2 and the stock market ex-
ample in Figure 1 is the introduction of a time di-
mension orthogonal to the dimensions of the state
space. To accommodate this change, we extend
the model described in the previous subsection in
the following way: Instead of a single vector, each
tree representation T is paired with a sequence of
path featuresV = (V
1
, V
2
, . . . , V
M
). For the time
being we continue to assume that there is only
one input tree per training example. As before,
we wish to model the probability p(T,V), but the
problem becomes harder: a single sentence might
describe multiple events, but we don?t know what
the correspondence is between regions of the sen-
tence and segmentsV.
Though the ultimate goal is still prediction of V
vectors from novel T instances, we cannot do this
without also inferring a set of latent alignments be-
tween portions of the path and input sentence dur-
ing training. To allow a sentence to explain mul-
62
? ? ?
A1 A2C1 C2
V1 V2
T
1
T
2
[Stocks rose] [Stocks rose, then fell]
 acv
 a?
 ta  tc
Figure 4: Factor graph for stocks grounding
model. Only a subset of the alignment candidates
are shown. ?
tc
maps text to constraints, ?
acv
maps
constraints to grounded segments, and ?
ta
deter-
mines which constraints act on which segments.
tiple events, we?ll break each T apart into a set of
alignment candidates T
i
. We?ll allow as an align-
ment candidate any subtree of T , and additionally
any subtree from which a single constituent has
been deleted.
We then introduce two groups of latent vari-
ables: alignment variables A = (A
1
, A
2
, . . .),
which together describe a mapping from pieces
of the input sentence to segments of the ob-
served path, and what we?ll call ?constraint? vari-
ables C = (C
1
, C
2
, . . .), which express each
aligned tree segment?s prediction about what its
corresponding path should look like (so that the
possibly-numerous parts of the tree aligned to a
single segment can independently express prefer-
ences about the segment?s path features).
In addition to ensuring that the alignment is
consistent with the bracketing of the tree, it might
be desirable to impose additional global con-
straints on the alignment. There are various ways
to do this in a graphical modeling framework; the
most straightforward is to add a combinatorial fac-
tor touching all alignment variables which checks
for satisfaction of the global constraint. In gen-
eral this makes alignment intractable. If the total
number of alignments licensed by this combina-
torial factor is small (i.e. if acceptable alignments
are sparse within the exponentially-large set of all
possible assignments to A), it is possible to di-
rectly sum them out during inference. Otherwise
approximate techniques (as discussed in the fol-
lowing section) will be necessary.
As discussed in Section 2, our financial time-
lines cover two-day periods, and it seems natural
to treat each day as a separate event. Then
the simple regression model described in the
preceding section, extended to include alignment
and constraint variables, has the form of the factor
graph shown in Figure 4. In particular, the joint
distribution p(T,V) is the product of four groups
of factors:
Alignment factors ?
ta
, which use a log-linear
model to score neighboring pairs of factors with
a feature function ?
a
:
?
ta
(T
i
, A
i
, A
i?1
) =
e
?
?
a
?
a
(T
i
,A
i
,A
i?1
)
?
A
?
i
,A
?
i?1
e
?
?
a
?
a
(T
i
,A
?
i
,A
?
i?1
)
(3)
Constraint factors ?
tc
, which map text features
onto constraint values:
?
tc
(T
i
, C
i
) = e
?||?
?
t
T
i
?C
i
||
2
2
(4)
Prediction factors ?
acv
which encourage pre-
dicted constraints and path features to agree:
?
acv
(A
i
, C
i
, V
j
) =
{
1 if A
i
?= j
e
?||C
i
?V
j
||
2
2
o.w.
(5)
A global factor ?
a
?
(A
1
, A
2
, ? ? ? ) which places
an arbitrary combinatorial constraint on the
alignment.
Note the essential similarity between Equations 1
and 4?in general, it can be shown that this factor
model reduces to the regression model we gave for
colors when there is only one of each T
i
and V
j
.
5.1 Learning
In order to make learning in the stocks domain
tractable, we introduce the following global
constraints on alignment: every terminal must be
aligned, and two constituents cannot be aligned
to the same segment. Together, these simplify
learning by ensuring that the number of terms
in the sum over A and C is polynomial (in fact
O(n
2
)) in the length of the input sentence. We
wish to find the maximum a posteriori estimate
p(?
t
, ?
a
|T,V) for ?
t
and ?
a
, which we can do
63
using the Expectation?Maximization algorithm.
To find regression scoring weights ?
t
, we have:
E step:
M = E
[
?
i
T
i
(T
i
)
?
]
; N = E
[
?
i
T
i
V
?
A
i
]
(6)
M step:
?
t
= M
?1
N (7)
To find alignment scoring weights ?
a
, we must
maximize:
?
i
E
?
?
log
?
?
e
?
?
a
?
a
(A
i
,A
i?1
,T
i
)
?
A
?
i
,A
?
i?1
e
?
?
a
?
a
(A
?
i
,A
?
i?1
,T
i
)
?
?
?
?
(8)
which can be done using a variety of convex op-
timization tools; we used L-BFGS (Liu and No-
cedal, 1989).
The predictive distribution p(V|T ) can also be
straightforwardly computed using the standard in-
ference procedures for graphical models.
5.2 Evaluation
Our stocks dataset consists of a set of headlines
from the ?Market Snapshot? column of the Wall
Street Journal?s MarketWatch website,
1
paired
with hourly stock charts for each day described
in a headline. Data is collected over a roughly
decade-long period between 2001 and 2012; af-
ter removing weekends and days with incomplete
stock data, we have a total of 2218 headline/time
series pairs. As headlines most often discuss a
single day or a short multi-day period, each train-
ing example consists of two days? worth of stock
data concatenated together. We use a 90%/10%
train/test split, with all test examples following all
training examples chronologically.
We compare against two baselines: one which
uses no text (and so learns only the overall mar-
ket trend during the training period), and another
which uses a fixed alignment instead of summing,
aligning the entire tree to the second day?s time se-
ries. Prediction error is the sum of squared errors
between the predicted and gold time series.
We report both the magnitude of the prediction
error, and the model?s ability to distinguish be-
tween the described path and a randomly-selected
alternative. The system scores poorly on squared
1
http://www.marketwatch.com/Search?m=
Column&mp=Market%20Snapshot
% Chan
ge
0.98
0.99
1.00
1.01
1.02
Hour of day10 12 14 10 12 14
[U.S. stocks end lower]
2
[as economic worries persist]
1
Figure 5: Example output from the stocks task.
The model prediction is given in blue (solid), and
the reference time series in green (dashed). Brack-
ets indicate the predicted boundaries of event-
introducing spans, and subscripts their order in the
sentence. The model correctly identifies that end
lower refers to the current day, and persist pro-
vides information about the previous day.
Method Sel. acc. ? Pred. err. ?
No text 0.51 0.0012
Fixed alignment 0.59 0.0011
Full model 0.61 0.0018
Human 0.72 ?
Table 3: Results for the stocks task. Sel(ection
accuracy) measures the frequency with which the
system correctly identifies the stock described in
the headline when paired with a random alterna-
tive. Pred(iction error) is the mean sum of squared
errors between the real and predicted paths. Full
model selection accuracy is a statistically signif-
icant improvement (p < 0.05) over the baseline
using a paired sign test.
error (which disproportionately penalizes large de-
viations from the correct answer, preferring con-
servative models), but outperforms both base-
lines on the task of choosing the described stock
history?when it is wrong, its errors are often
large in magnitude, but its predictions more fre-
quently resemble the correct time series than the
other systems.
Figure 5 shows example system output for an
example sentence. The model correctly identifies
the two events, orders them in time and gets their
approximate trend correct. Table 4 shows some
64
% Chan
ge
0.98
0.99
1.00
1.01
1.02
Hour of day10 12 14 10 12 14
[U.S. stocks extend losing stretch]
1
Figure 6: Example error from the stocks task. The
system?s prediction, in blue (solid), fails to seg-
ment the input into two events, and thus incor-
rectly extends the losing trend to the entire output
time span.
features learned by the model?as desired, it cor-
rectly interprets a variety of different expressions
used to describe stock behavior.
5.3 Error analysis
As suggested by Table 4, learned weights for the
trajectory-grounded features ?
t
are largely correct.
Thus, most incorrect outputs from the system in-
volve alignment to time. Many multipart events
(like rebound) can be reasonably explained using
the curvature feature without splitting the text into
two segments; as a result, the system tends to be
fairly conservative about segmentation and often
under-segments. This results in examples like Fig-
ure 6, in which the downward trend suggested by
losing is incorrectly extended to the entire out-
put curve. Here, another informal experiment us-
ing humans as the predictors indicates that pre-
dictions are farther from human-level performance
Word Sign Magnitude ?10
3
rise 0.27 ?0.78
swoon ?0.57 0
sharply ?0.22 0.28
slammed ?0.36 0
lifted 0.66 0
Table 4: Learned parameter settings for overall
daily change, which the path featurization decom-
poses into a sign and a magnitude.
than they are on the colors task.
6 Generalizing the model
Last we consider the problem of following navi-
gational directions. The difference between this
and the previous task is largely one of scale: rather
than attempting to predict the values of only two
segments, we have a long string of them. The text,
rather than a single tree, consists of a sequence of
tens or hundreds of pre-segmented utterances.
There is one additional complication?rather
than being defined in an absolute space, as they are
in the case of stocks, constraints in the maps do-
main are provided relative to a set of known land-
marks (like the white water and stone creek in Fig-
ure 3). We resolve landmarks automatically based
on string matching, in a manner similar to Vogel
and Jurafsky (2010), and assign each sentence in
the discourse with a single referred-to landmark l
i
.
If no landmark is explicitly named, it inherits from
the previous utterance. We continue to score con-
straints as before, but update the prediction factor:
?
acv
(A
i
, C
i
, V
j
) =
{
1 if A
i
?= j
e
?||l
i
+C
i
?V
j
||
2
2
o.w.
(9)
The factor graph is shown in Figure 7; ob-
serve that this is simply an unrolled version of
Figure 4?the basic structure of the model is un-
changed. While pre-segmentation of the discourse
means we can avoid aligning internal constituents
of trees, we still need to treat every utterance as an
alignment candidate, without a sparse combinato-
rial constraint. As a result, the sum over A and
C is no longer tractable to compute explicitly, and
approximate inference will be necessary.
For the experiments described in this paper, we
do this with a sequence of Monte Carlo approxi-
mations. We run a Gibbs sampler, iteratively re-
sampling each A
i
and C
i
as well as the parameter
vectors ?
t
and ?
a
to obtain estimates of E?
t
and
E?
a
. The resampling steps for ?
t
and ?
a
are them-
selves difficult to perform exactly, so we perform
an internal Metropolis-Hastings run (with a Gaus-
sian proposal distribution) to obtain samples from
the marginal distributions over ?
t
and ?
a
.
We approximate the mode of the posterior dis-
tribution by its mean. To follow a new set of direc-
tions in the prediction phase, we fix the parameter
vectors and instead sample overA, C andV, and
output EV. To complete the prediction process
65
? ? ?
? ? ?
? ? ?
CNANA1 A2 A3C1 C2 C3
V1 V2 VM
T 1 T
2 T 3 T
N
 ta  tc
 a?
 acv
Figure 7: Factor graph for the general grounding model. Note that Figure 4 is a subgraph.
we must invert ?
v
, which we do by producing the
shortest path licensed by the features.
6.1 Evaluation
The Map Task Corpus consists of 128 dia-
logues describing paths on 16 maps, accompa-
nied by transcriptions of spoken instructions, pre-
segmented using prosodic cues. See Vogel and Ju-
rafsky (2010) for a more detailed description of the
corpus in a language learning setting. For com-
parability, we?ll use the same evaluation as Vogel
and Jurafsky, which rewards the system for mov-
ing between pairs of landmarks that also appear in
the reference path, and penalizes it for additional
superfluous movement. Note that we are solv-
ing a significantly harder problem: the version ad-
dressed by Vogel and Jurafsky is a discrete search
problem, and the system has hard-coded knowl-
edge that all paths pass along one of the four sides
of each landmark. Our system, by contrast, can
navigate to any point in R
2
, and must learn that
most paths stay close to a named landmark.
At test time, the system is given a new sequence
of text instructions, and must output the corre-
sponding path. It is scored on the fraction of
correct transitions in its output path (precision),
and the fraction of transitions in the gold path
recovered (recall). Vogel and Jurafsky compare
their system to a policy-gradient algorithm for us-
ing language to follow natural language instruc-
tions described by Branavan et al. (2009), and we
present both systems for comparison.
Results are shown in Table 5. Our system sub-
stantially outperforms the policy gradient baseline
of Branavan et al., and performs close (particularly
with respect to transition recall) to the system of
Vogel and Jurafsky, with fewer assumptions.
System Prec. Recall F
1
Branavan et al. (09) 0.31 0.44 0.36
Vogel & Jurafsky (10) 0.46 0.51 0.48
This work 0.43 0.51 0.45
Table 5: Results for the navigation task. Higher is
better for all of precision, recall and F
1
.
6.2 Error analysis
As in the case of stocks, most of the prediction
errors on this task are a result of misalignment.
In particular, many of the dialogues make passing
reference to already-visited landmarks, or define
destinations in empty regions of the map in terms
of multiple landmarks simultaneously. In each of
these cases, the system is prone to directly visit-
ing the named landmark or landmarks instead of
ignoring or interpolating as necessary.
7 Conclusion
We have presented a probabilistic model for
grounding natural language text in vector-valued
state sequences. The model is capable of seg-
menting text into a series of events, ordering these
events in time, and compositionally determining
their internal structure. We have evaluated on a va-
riety of new and established applications involving
colors, time series and navigation, demonstrating
improvements over strong baselines in all cases.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014. The first
author is supported by a National Science Foun-
dation Graduate Research Fellowship.
66
References
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
JimMiller, et al. 1991. The HCRCmap task corpus.
Language and speech, 34(4):351?366.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michele Basseville and Igor V Nikiforov. 1995. De-
tection of abrupt changes: theory and applications.
Journal of the Royal Statistical Society-Series A
Statistics in Society, 158(1):185.
SRK Branavan, Harr Chen, Luke S Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 82?90. Association for Com-
putational Linguistics.
Elia Bruni, NamKhanh Tran, andMarco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In AAAI, volume 2.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference on
Empirical Methods in Natural Language Process-
ing, pages 410?419. Association for Computational
Linguistics.
Christopher Kennedy and Louise McNally. 2010.
Color, context, and compositionality. Synthese,
174(1):79?98.
Shimon Kogan, Dimitry Levin, Bryan R Routledge,
Jacob S Sagi, and Noah A Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 272?280. Association for Computa-
tional Linguistics.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Grounding verbs of motion in natu-
ral language commands to robots. In International
Symposium on Experimental Robotics.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Terence Parsons. 1990. Events in the semantics of En-
glish. MIT Press.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of Human
Language Technologies: The 2007 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics. Assocation for
Computational Linguistics.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267?288.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 806?814. Association for
Computational Linguistics.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from videos described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics. Association for Computational Linguistics.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
2007. Choosing the content of textual summaries of
large time-series data sets. Natural Language Engi-
neering, 13(1):25?49.
Jeffrey M Zacks and Khena M Swallow. 2007. Event
segmentation. Current Directions in Psychological
Science, 16(2):80?84.
John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050?1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence, pages 658?
666.
67
