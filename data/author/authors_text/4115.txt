Two-Phase Shift-Reduce Deterministic Dependency Parser of Chinese 
Meixun Jin, Mi-Young Kim and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering,  
Pohang University of Science and Technology (POSTECH) 
Advanced Information Technology Research Center (AITrc) 
{Meixunj, colorful, jhlee}@postech.ac.kr 
Abstract 
In the Chinese language, a verb may 
have its dependents on its left, right or on 
both sides. The ambiguity resolution of 
right-side dependencies is essential for de-
pendency parsing of sentences with two or 
more verbs. Previous works on shift-
reduce dependency parsers may not guar-
antee the connectivity of a dependency tree 
due to their weakness at resolving the 
right-side dependencies. This paper pro-
poses a two-phase shift-reduce dependency 
parser based on SVM learning. The left-
side dependents and right-side nominal de-
pendents are detected in Phase I, and right-
side verbal dependents are decided in 
Phase II. In experimental evaluation, our 
proposed method outperforms previous 
shift-reduce dependency parsers for the 
Chine language, showing improvement of 
dependency accuracy by 10.08%.  
1 Introduction 
Dependency parsing describes syntactic struc-
ture of a sentence in terms of links between in-
dividual words rather than constituency trees. 
The fundamental relation in dependency parsing 
is between head and dependent. Robinson[1] 
formulates four axioms to the well-formed de-
pendency structures, known as single headed, 
acyclic, connective and projective. 
In this paper, we present a dependency pars-
ing strategy that produces one dependency struc-
ture that satisfies all these constraints.  
This paper is organized as follows. Related 
works are introduced in section 2. In section 3, 
detailed analysis of the work of Nivre[2] and 
Yamada[3] are given. Then our parsing strategy 
is introduced. In section 4, experiments and re-
sults are delivered. Finally a conclusion will be 
given in section 5. 
2 Overview of Related Works 
Most nature language grammars tend to as-
sign many possible syntactic structures to the 
same input utterance. A parser should output a 
single analysis for each sentence. The task of 
selecting one single analysis for a given sen-
tence is known as disambiguation.  
Some of the parsing strategies first produce 
all possible trees for a sentence. The disam-
biguation work is done in the end by searching 
the most probable one through parsing tree for-
est. Statistical parsers employ probability as a 
disambiguation measure and output the tree with 
the highest probability[4,5]. However, in the 
work of Collins [6], 42% of the correct parse 
trees were not in the candidate pool of ~30-best 
parses. Disambiguation work by searching 
throughout the parsing tree forest has limitations. 
The alternative way is to disambiguate at each 
parsing step and output the parsing result deter-
ministically. Nivre[2] and Yamada[3] suggest a 
shift-reduce like dependency parsing strategy. In 
section 3.1 we give a detailed analysis of their 
approach.  
There are several approaches for dependency 
parsing on Chinese text. Ma[5] and Cheng[18] 
are examples of these approaches. The training 
and test set Ma[5] used, are not sufficient to 
prove the reliability of Ma?s[5] approach. On the 
frame of parsing Chinese with CFG, there are 
several approaches to apply the original English 
parsing strategies to Chinese [7,8,9]. The poten-
tial purposes of these works are to take advan-
tage of state-of-art English parsing strategy and 
to find a way to apply it to Chinese text. Due to 
the differences between Chinese and English, 
256
the performance of the system on Chinese is 
about 10% lower comparing the performance of 
the original system.  
3 Two-Phase Dependency Parsing 
3.1  Review of Previous Shift-Reduce Dependency 
Parsers 
Nivre[3] presented a shift-reduce dependency 
parsing algorithm which can parse in linear time. 
The Nivre?s parser was represented by a triples 
<S, I, A>, where S is a stack, I is a list of (re-
maining) input tokens, and A is the set of deter-
mined dependency relations. Nivre defined four 
transitions: Left-Arc, Right-Arc, Reduce, and 
Shift. If there is a dependency relation between 
the top word of the stack and the input word, 
according to the direction of the dependency arc, 
it can be either Left-Arc or Right-Arc. Otherwise, 
the transition can be either shift or reduce. If the 
head of the top word of the stack is already de-
termined, then the transition is reduce, otherwise 
shift. The action of each transition is shown in 
Fig.1. For details, please refer to Nivre[3,10]. 
Fig.2 gives an example1 of parsing a Chinese 
sentence using Nivre?s algorithm. 
Nivre?s[3,10] approach has several advan-
tages. First, the dependency structure produced 
by the algorithm is projective and acyclic[3]. 
Second, the algorithm performs very well for 
deciding short-distance dependences. Third, at 
each parsing step, all of the dependency rela-
tions on the left side of the input word are de-
termined. Also as the author emphasizes, the 
time complexity is linear.  
However, wrong decision of reduce transition, 
like early reduce, cause the word at the top of 
the stack loses the chance to be the head of oth-
ers. Some words lose the chance to be the head 
of other following words. As a result, the de-
pendents of this word will have a wrong head or 
may have no head. 
The parsing steps of a Chinese sentence using 
Nivre?s[3] algorithm are given in Fig.2. At step-
5 of Fig.2, after reduce, the top of the stack was 
popped. The algorithm doesn?t give a chance for 
the word ?? to be the head of other words. 
Therefore, word ???? cannot have word ??
?? as its head. In the final dependency tree of 
example-1 in Fig.2, the arc from ?? to ?? is 
wrong. Fig.3 gives the correct dependency tree. 
Here, ?? is the head of word ??. 
                                                          
1 All the example sentences are from CTB. 
 
If there is a dependency relation between top.stack and input 
If the dependency relation is Left_arc 
   Insert (input, top.stack) pair into set A 
   pop(stack);   
Else  
   Insert (top.stack, input) pair  into set A 
   push(input);   
Else 
If the head of top of the stack is determined 
   pop(stack); 
    Else 
      push(input); 
Fig. 1. Transitions defined  by Nivre[3] 
 
?? ?     ?? ??  ??         ??? 
This  province  plan extend  attract merchants  attract investments. 
The province plans to expand attracting merchants and investments. 
                           stack ,  input                      relation set A   
 
Step-0:     <nil,???????????,{}> 
Step-1:?S  <??,?????????,{}> 
Step-2:?LA <?,????????,{(?,??)}> 
Step-3:?LA <??,??????,{(?,??),(??,?)}> 
Step-4:?RA <?? ??,????,{(?,??),(??,?), 
(?????)}> 
Step-5:?R <??,????,{(?,??),(??,?), 
(??,??)}> 
Step-6:?S <?? ??,??,{(?,??),(??,?), 
(??,??)}> 
Step-7:?LA <??,??,{(?,??),(??,?), 
(??,??),(??,??)}> 
Step-8:?LA <??,nil,{(?,??),(??,?), 
(??,??),(??,??),(?????)}> 
The dependency structure of the output: 
 
 
?? ?     ?? ??  ??         ??? 
 
S:Shift LA:Left-arc RA:Right-arc R:reduce 
Fig. 2.  Example-1: Parsing using Nivre?s algorithm 
 
 
 
?? ?     ?? ??  ??         ??? 
Fig. 3. The correct parse tree of Example-1 
 
Fig.4. gives the parsing step of another example. 
As the final dependency tree in Fig.4 shows, 
there is no head for word ???After Step-5, 
the top of the stack is word ? and input word is 
? . There is no dependency relation between 
these two words. Since the head of the word ? 
is already determined in step-2?the next transi-
tion is R(educe). As a result, word ? loses the 
chance to be the head of word ??. So, there is 
no head assigned to word ?? in Fig.4. There-
fore, Nivre?s algorithm causes some errors for 
determining the right-side dependents. 
Yamada?s[4] approach is similar to Nivre?s[3]. 
Reduce
shift
Right_arc
Left_arc 
257
Yamada?s algorithm define three actions: left, 
right and shift, which were similar to those of 
Nivre?s. Yamada parsed a sentence by scanning 
the sentence word by word from left to right, 
during the meantime, left or right or shift actions 
were decided. For short dependents, Yamada?s 
algorithm can cope with it easily. For long de-
pendents, Yamada tried to solve by increasing 
the iteration of scanning the sentences. As Ya-
mada pointed out, ?shift? transition was executed 
for two kinds of structure. This may cause 
wrong decision while deciding the action of 
transition. Yamada tried to resolve it by looking 
ahead for more information on the right side of 
the target word.  
 
??    ? ??? ? ?        ???? ? ??? 
declare to teachers a  piece      exciting   of  news. 
Declare a piece of exciting news to teachers. 
? ? ? 
Step-2 :?S  <????????????????,{}> 
Step-3 :?RA <??????????????,{(??,?)}>
Step-4 :?RA <?????????????,{(??,?), 
(?,???)}> 
Step-5 :?R <???????????,{(??,?), 
(?,???)}> 
Step-6 :?R <????????????,{(??,?), 
(?,???)}> 
? ? ? 
Step-n:?RA <??,nil,{(??,?),(?,???),(?,?)? 
(?,????),(??,?),(??,?)}> 
 
The dependency structure of the output: 
 
 
??    ? ??? ? ?        ???? ? ??? 
Fig. 4. Example-2: Parsing with Nivre?s algorithm 
 
 
??  ? ??? ??        ??                   ??           ?   ??. 
report _  200       attract  foreign country     investment   of   plan. 
Report 200 plans in attracting foreign investment. 
? ? ? 
step-i : ?RA < ??, ?????????,{( ??,?)} >
Fig. 5. Example-3: Parsing with Nivre?s algorithm 
 
When applying to Chinese parsing, the deter-
mination of dependency relation between two 
verbs is not effective. In the example-3 of Fig.5, 
at step-i, the parser decides whether the depend-
ency relation between ?? and ?? is either 
Left-arc or Right-arc. The actual head of  the 
verb ?? is ?, which is distant. By looking 
only two or three right side words ahead, to de-
cide the dependency relation between these 
verbs at this moment is not reliable. Yamada?s 
algorithm is not a clear solution to determine the 
right side dependents either. 
3.2 Two-Phase Dependency Parsing 
For the head-final languages like Korean or 
Japanese, Nivre[3] and Yamada?s[4] approaches 
are efficient. However, being applied to Chinese 
text, the existing methods cannot correctly de-
tect various kinds of right-side dependents in-
volved in verbs. All wrong decisions of reduce 
transition mainly occur if the right dependent of 
a verb is also a verb, which may have right-side 
dependents.  
For the correct detection of the right-side de-
pendents, we divide the parsing procedure into 
two-phase. Phase I is to detect the left-side de-
pendents and right-side nominal dependents. 
Although some nominal dependents are right-
side, they don?t have dependents on the right 
side, and will not cause any ambiguities related 
to right-side dependents. In Phase II, the detec-
tion of right-side verbal dependents, are per-
formed.  
3.2.1 Phase I  
In Phase I, we determine the left-side depend-
ents and right-side nominal dependents. We de-
fine three transitions for Phase I: Shift, Left-Arc, 
Right-Arc. The actions of transition shift and 
Left-Arc are the same as Nivre[3] defines. How-
ever, in our method, the transition of Right-Arc 
does not push the input token to the stack. The 
original purpose for pushing input to stack after 
right-arc, is to give a chance for the input to be 
a potential head of the following words. In Chi-
nese, only verbs and prepositions have right-side 
dependents. For other POS categories, the action 
of pushing into stack is nonsense.  In case that 
the input word is a preposition, there is no am-
biguities we describe. Only the words belong to 
various verbal categories may cause problems. 
The method that we use is as follows. When the 
top word of the stack and the next input word 
are verbs, like VV, VE, VC or VA2 [11], the 
detection of the dependency relation between 
these two verbs is delayed by transition of shift. 
To differentiate this shift from original shift, we 
call this verbal-shift. The determination of the 
dependency relation between these two verbs 
will be postponed until phase II. The transitions 
are summarized as Fig.6. 
If there is no more input word, phase I termi-
nates. The output of the phase I is a stack, which 
                                                          
2 VV, VE, VC and VA are Penn Chinese Treebank POS 
categories related to verbs. For details, please refer to [11]. 
258
contains verbs in reverse order of the original 
appearance of the verbs in the sentence. Each 
verb in the stack may have their partial depend-
ents, which are determined in Phase I.  
    
If the action is Verbal-shift 
: push the input to the stack 
else if the action is Shift 
  push the input to the stack 
else if the action is Left-arc 
  set the dependency relation for two words; pop 
the top of the stack 
else if the action is Right-arc 
  set the dependency relation for two words 
Fig. 6.  Types of transitions in the phase I 
 
The type of transition is determined by the top 
word of the stack, input word and their context. 
Most of the previous parsing models[4,12,13] 
use lexical words as features. Compared to Penn 
English Treebank, the size of Penn Chinese 
Treebank (version 4.0, abbreviated as CTB) is 
rather small. Considering the data sparseness 
problem, we use POS tags instead of lexical 
words itself. As Fig.7. shows, the window for 
feature extraction is the top word of the stack, 
input word, previous word of the top of the 
stack, next word of the input. The left-side 
nearest dependent of these is also taken into 
consideration. Besides, we use two more fea-
tures, if_adjoin, and Punc. The feature vector for 
Phase I is shown in Fig.7.  
3.2.2 Phase II  
    After Phase I, only verbs remain in the stack. 
In Phase II, we determine the right-side verbal 
dependents.  We take the output stack of Phase I 
as input. Some words in the stack will have 
right-side dependents as shown in Fig.8. For 
Phase II, we also define three transitions: shift, 
left-arc, right-arc. The operations of these three 
transitions are the same as Phase I, but there are 
no verbal-shifts. Fig.9 shows the output of Phase 
I and parsing at Phase II of example given in 
Fig.8.  
The window for feature extraction is the same 
as that of Phase I. The right-side nearest de-
pendent is newly taken as features for Phase II. 
The feature vector for Phase II is shown in 
Fig.10. 
The two-phase parsing will output a projec-
tive, acyclic and connective dependency struc-
ture. Nivre[10] said that the time complexity of 
his parser is 2 times the size of the sentence. Our 
algorithm is 4 times the size of the sentence, so 
the time complexity of our parser is still linear to 
the size of the sentence. 
 
Windows for feature extraction : 
t.stack :  top word of the stack 
p.stack:  previous word of top of the stack 
input   :  input word 
n.input:  next word of the input word 
 
x.pos : POS tag of word x 
x.left.child : the left-side nearest dependent of word x 
 
punc : the surface form of punctuation between top word of the 
stack and input word, if there is any 
if_adjoin : a binary indicator to show if the top word of the 
stack and input word are adjoined  
 
The feature vector for Phase I is : 
<p.stack.pos t.stack.pos input.pos n.input.pos p.stack.left.child.pos 
t.stack.left.child.pos input.left.child.pos punc if_adjoin> 
Fig. 7. Feature vector for Phase I 
 
????????????????????????????
????????????????????? 
(The official said that Sichuan will pursue a more open door policy, 
continuously improve the investment environments and attract more 
capitals from overseas, advanced techniques and experiences of ad-
ministration.) 
 
The contents of stack after Phase I: <??????????>.  
(attract, improve, pursue, said ) 
 
The dependents  of each verb in the stack 
 
 
 
 
 
 
 
 
 
Fig. 8. Dependents of each verb after Phase I 
 
step-0      <nil, ?? ?? ?? ??{}> 
step-1?S   < ??, ?? ?? ??{}> 
step-2?RA  < ??, ?? ??{(??,??)}> 
step-3?RA  < ??, ??{(??,??),(??,??)}> 
step-4?LA  < nil, ??{(??,??),(??,??), 
(?,??)}> 
step-5 ?S   < ?, nil?{(??,??),(??,??), 
(?,??)}> 
Fig. 9. Example of parsing at Phase II 
 
The feature vector for Phase II is : 
<p.stack.pos t.stack.pos input.pos n.input.pos 
p.stack.left.child.pos t.stack.left.child.pos  input.left.child.pos 
p.stack.right.child.pos t.stack.right.child.pos in-
put.right.child.pos n.input.right.child.pos punc if_adjoin> 
Fig. 10. Feature vector for Phase II. 
4 Experiments and Evaluation  
Our parsing procedure is sequentially per-
formed from left to right. The feature vectors for 
?
?
?
? ?
?
?
? 
?
? 
?
right-side right-side right-sideleft-side left-side 
left-side 
left-side
259
Phase I and Phase II are used as the input for the 
parsing model. The model outputs a parsing ac-
tion, left-arc, right-arc or shift. We use SVM as 
the model to obtain a parsing action, and use 
CTB for training and test the model. 
4.1 Conversion of Penn Chinese Treebank to 
Dependency Trees 
Annotating a Treebank is a tedious task. To 
take the advantage of CTB, we made some heu-
ristic rules to convert CTB into dependency 
Treebank. This kind of conversion task has been 
done on English Treebank[14,10,4]. We use the 
dependency formalism as Zhou[15] defined. 
CTB contains 15,162 newswire sentences (in-
cluding titles, fragments and headlines). The 
contents of CTB are from Xinhua of mainland, 
information services department of HKSAR and 
Sinorama magazine of Taiwan. For experiments, 
12,142 sentences are extracted, excluding all the 
titles, headlines and fragments.  
For the conversion task, we made some heu-
ristic rules. CTB defines total 23 syntactic 
phrases and verb compounds[11]. A phrase is 
composed of several words accompanied to a 
head word. The head word of each phrase is 
used as an important resource for PCFG pars-
ing[12,13]. According to the position of the head 
word with respect to other words, a phrase3 can 
be categorized into head-final, head-initial or 
head-middle set. Table.1 shows the head-initial, 
head-final and head-middle groups.  
For VP, IP and CP, these phrases have a verb 
as its head word. So we find a main verb and 
regard the verb the head word of the phrase. If 
the head word for each phrase is determined, 
other words composing the phrase simply take 
the head word of the phrase as its head. In the 
case of BA/LB4, we take a different view from 
what is done in CTB. Zhou[15] regards BA/LB 
as the dependent of the following verb. We fol-
low Zhou?s[15] thought. For sentences contain-
ing BA/LB, we converted them into dependency 
trees manually. With above heuristics, we con-
verted the original CTB into dependency Tree-
bank.  
                                                          
3 We use the label of phrases as CTB has defined. We ex-
clude FRAG, LST, PRN. For each definition of the phrase 
please refer to [11]. 
4 BA, LB are two POS categories of CTB. For details, see 
[11]. 
4.2 Experiments 
SVM is one of the binary classifiers based on 
maximum margin strategy introduced by Vap-
nik[16]. SVM has been used for various NLP 
tasks, and gives reasonable outputs. For the ex-
periments reported in this paper, we used the 
software package SVMlight [17]. 
For evaluation matrix, we use Dependency 
Accuracy and Root Accuracy defined by Ya-
mada[4]. An additional evaluation measure, 
None Head is defined as following. 
 
None Head: the proportion of words whose 
head is not determined. 
 
GROUP PHRASES 
Head-initial PP; VRD; VPT; 
Head-final ADJP; ADVP; CLP; DNP; DVP; DP; 
LCP; NP; QP; VCD; VCP; UCP; VSB; 
VNV; 
Head-
middle 
CP; IP; VP; 
Table 1. Cluster of CTB syntactic phrases 
 
Table 2. Comparison of dependency accuracy with Nivre?s 
 
We construct two SVM binary classifiers, 
Dep vs. N_Dep and LA vs. RA, to output the 
transition action of Left-arc, Right-arc or Shift. 
Dep vs. N_Dep classifier determines if two 
words have a dependency relation. If two words 
have no dependency relation, the transition ac-
tion is simply Shift. If there is a dependency re-
lation, the second classifier will decide the 
direction of it, and the transition action is either 
Left-arc or Right-arc.  
We first train a model along the algorithm of 
Nivre[10]. The training and test sentences are 
randomly selected. Table.2 shows that 1.53% of 
the words cannot find their head after parsing. 
This result means that the original Nivre?s algo-
rithm cannot guarantee a connective dependency 
structure.  
With our two-phase parsing algorithm, there 
is no none head. Then, the dependency accuracy 
and root accuracy are increased by 10.08% and 
13.35% respectively.  
 Dependency 
accuracy 
Root ac-
curacy 
None 
head 
Nivre?s algorithm[10] 73.34% 69.98% 1.53% 
Ours  84.42% 83.33% ---- 
260
4.3 Comparison with Related Works 
Compared to the original works of Nivre[10] 
and Yamada[4], the performance of our system 
is lower. We think that is because the target lan-
guage is different.  
 
 Average 
sentence 
length 
Dependency 
accuracy 
Root 
accuracy
Ma[5] 9 80.25% 83.22%
Cheng[18] 5.27 94.44% -- 
Ours 34 84.42% 83.33%
Table 3 Comparison of the parsing performances 
between Ma[5], Cheng[18] and ours 
 
Table 3 gives the comparison of the perform-
ances between Ma[5], Chen[18] and ours. The 
training and test domain of Ma[5] is not clear. 
Cheng[18] used CKIP corpus in his experiments. 
The average length of sentence in our test set is 
34, which is much longer than that in Ma[5] and 
Cheng[18]. The performance of our system is 
still better than Ma[5] and less than Cheng[8]. 
5 Conclusion 
To resolve the right-side long distance de-
pendencies, we propose two-phase shift-reduce 
parsing strategy. The parsing strategy not only 
guarantees the connectivity of dependency tree, 
but also improves the parsing performance. As 
the length of sentences increases, the ambigui-
ties for parsing increase drastically. With our 
two-phase shift-reduce parsing strategy, the per-
formance of syntactic parsing of long sentences 
is also reasonable. 
The motivation of this paper is to design a 
well-formed dependency parser for Chinese. We 
believe that there?re rooms to improve the per-
formance. We plan to work further to explore 
the optimal features. We also plan to parse Eng-
lish text with our algorithm to see if it can com-
pete with the state-of-art dependency parsers on 
English. We believe that our parsing strategy 
can apply to other languages, in which head po-
sition is mixed, as Chinese language. We think 
that it is the main contribution of our approach. 
References 
1. Robinson, J.J.: Dependency structures and 
transformation rules. Language 46 (1970) 259-285 
2. Nivre, J.: An efficient algorithm for projective 
dependency parsing. In Proceedings of IWPT 
(2003) 149-160 
3. Yamada, H. and Matsumoto, Y.: Statistical de-
pendency analysis with support vector machines. 
In Proceedings of IWPT (2003) 195-206 
4. Eisner, J.M.:Three new probabilistic models for 
dependency parsing: An exploration. In Proceed-
ings of ACL.( 1996) 340-345 
5. Ma,J., Zhang,Y. and Li,S.: A statistical depend-
ency parser of Chinese under small training data. 
IJCNLP-04 Workshop : Beyond Shallow Analy-
ese-Formalisms and Statistical Modeling for Deep 
Analyses (2004) 
6. Collins,M.: Discriminative reranking for natural 
language parsing. In proceedings of ICML 
17.(2000) 175-182 
7. Fung,P., Ngai,G, Yang,Y.S and Chen,B.: A maxi-
mum-entropy Chinese parser augmented by trans-
formation-based learning. ACM transactions on 
Asian language information processing. Volume 
3. Number 2.(2004) 159-168 
8. Levy,R. and Manning,C.: Is it harder to parse Chi-
nese, or the Chinese Treebank? In Proceedings of 
ACL. (2003) 439-446  
9. Bikel, D.M. and.Chiang, D.: Two Statistical Pars-
ing models applied to the Chinese Treebank. In 
proceedings of  the second Chinese language 
processing workshop. (2000)  
10.Nivre,J, Hall,J and Nilsson,J.: Deterministic de-
pendency parsing of English text. In Proceedings 
of COLING. (2004) 23?27  
11.Xue,N and Xia,F.: The bracketing guidelines for 
the Penn Chinese Treebank(3.0). IRCS Report 00-
08, University of Pennsylvania (2000) 
12.Collins,M.: Three generative lexicalised models 
for statistical parsing. In Proceedings of the 35th  
Annual Meeting of the Association for Computa-
tional Linguistics, Madrid (1997) 16-23 
13.Charniak,E.: A maximum-entropy-inspired parser. 
In Proceedings of NAACL. Seattle (2000) 132?
139,  
14.Collins,M.: A new statistical parser based on bi-
gram lexical dependencies. In Proceedings of the 
Thirty-Fourth Annual Meeting of the Association 
for Computational Linguistics, philladelphia 
(1996) 184?191 
15.Zhou,M. and Huang,C.: Approach to the Chinese 
dependency formalism for the tagging of corpus. 
Journal of Chinese information processing.(in 
Chinese), Vol. 8(3) (1994) 35-52 
16.Joachims,T.: Making large-scale SVM learning 
practical. Advances in Kernel Methods-Support 
Vector Learning, B.Scholkopf and C.Burges and 
A.Smola(Eds.), MIT-Press (1999) 
17. Vapnik, V.N.: The nature of statistical learning 
theory. Springer, New York. (1995) 
18. Cheng, Y.C, Asahara,M and Matsumoto Y.: De-
terministic dependency structure analyzer for Chi-
nese. In proceedings of the first IJCNLP(2004) 
135-140 
261
Segmentation of Chinese Long Sentences Using Commas 
Mei xun Jin1, Mi-Young Kim2, Dongil Kim3 and Jong-Hyeok Lee4 
Graduate School for Informa-
tion Technology1, 
Div. of Electrical and 
Computer Engineering24,
Pohang University of Science and Technology 
Advanced Information Technology Research Center(Altrc)
{meixunj1,colorful2,jhlee4}@ 
postech.ac.kr 
Language Engineering Institute3
Div. of Computer, Electronics 
and Telecommunications 
Yanbian University of Science 
and Technology 
dongil@ybust.edu.cn 
 
 
 
 
Abstract 
The comma is the most common form of 
punctuation. As such, it may have the 
greatest effect on the syntactic analysis of a 
sentence. As an isolate language, Chinese 
sentences have fewer cues for parsing. The 
clues for segmentation of a long Chinese 
sentence are even fewer. However, the av-
erage frequency of comma usage in Chi-
nese is higher than other languages. The 
comma plays an important role in long 
Chinese sentence segmentation. This paper 
proposes a method for classifying commas 
in Chinese sentences by their context, then 
segments a long sentence according to the 
classification results. Experimental results 
show that accuracy for the comma classifi-
cation reaches 87.1 percent, and with our 
segmentation model, our parser?s depend-
ency parsing accuracy improves by 9.6 per-
cent.  
 
1 Introduction 
Chinese is a language with less morphology and 
no case marker. In Chinese, a subordinate clause or 
coordinate clause is sometimes connected without 
any conjunctions in a sentence. Because of these 
characteristics, Chinese has a rather different set of 
salient ambiguities from the perspective of statisti-
cal parsing (Levy and Manning, 2003). In addition, 
the work for clause segmentation is also rather dif-
ferent compared with other languages.  
    However, in written Chinese, the comma is used 
more frequently (Lin, 2000). In English, the average 
use of comma per sentence is 0.869 (Jones, 1996a)1 
~1.04(Hill, 1996), and in Chinese it is 1.792, which 
is one and a half to two more times as it is used in 
English. In Korean, the comma is used even less 
than it is in English (Lin, 2000). 
   Since Chinese has less morphology and no case 
marker, and the comma is frequently used, the 
comma becomes an important cue for long Chinese 
sentence parsing. Because more commas may ap-
pear in longer sentences, the necessity of analyzing 
the comma also increases.  
    Some handbooks about standard Chinese gram-
mars list ten to twenty uses of the comma, accord-
ing to the context. Among these uses, is occurrence 
at the end of a clause3 in a sentence (Lin, 2000). 
About 30% of commas are used to separate the 
clause from its main sentence or neighbor clause(s). 
If the comma appears at the end of a clause, the 
position can naturally be set as the clause segmen-
tation point.  
     This paper proposes Chinese long sentence 
segmentation by classifying the comma. In section 
2, related work in clause segmentation and punc-
tuation processing is presented. Comma classifica-
tion criteria are then introduced, and the 
classification model follows. Afterwards, some 
experimental results show how the proposed 
comma classification and long sentence segmenta-
tion are effective in Chinese parsing. Finally, a 
conclusion will be given.  
                                                          
1 The frequency of comma per sentence is calculated as = Total frequency of 
commas/(Total frequency of full stop + Total frequency of Question mark),  
based on the punctuation statistics of Jone?s Phd. thesis P56,57.  
2 The calculation is based on People?s Daily Corpus?98. 
3 Clause in this paper, is a predicate with its full complements, subject, object(s). 
According to the type of a predicate and the context, subject or object may or 
may not appear. Adjunct of the predicate may or may not be included in the 
clause.  
 
 
 
 
?????????????  
Figure1: example of a dependency relation  
 
 
 
??????????????????? 
 
 
 
Figure2: example of a dependency relation  
 
 
 
 
???????????????? 
 
 
Figure 3: example of a dependency relation 
Examples:  
(1) ????????????? 
They have class in the morning and do experiments in 
the afternoon. 
(2) ??????????????????? 
Several years ago,  BeiHai City was only an unknown 
small fishing village. 
(3) ???????????????? 
The students prefer young and beautiful teachers.  
(4) ?????????????? 
Xiao Ming is doing homework and his mom is 
knitting. 
(5) ?????????????? 
Though he studies very hard, his score is not satisfiable.   
(6) ??????????????????????
???? 
The change of domestic economic development in 
Russia has promoted the trade exchange between two 
countries. 
(7) ???????????????????? 
Bank of China invited a Japanese company as its con-
soler last October.  
(8) ????????????????????? 
He is a good leader in the company as well as a good 
daddy at home. 
(9) ?????????????????????
?? 
The quick transfer of the scientific research achieve-
ment to industry is the characteristic of this develop-
ment district. 
(10) ??????????????? 
The students happily come to the playground. 
(11) ??????????????????????
?????????? 
The investment from Korea to DaLian city has grown 
for three years, and all Korean investment companies 
in DaLian  receive preferential treatment. 
(12) ????????????????????   
The statistics show that the exportation from DaLian to 
Korea is reach to USD100,000,000. 
(13) ??????????????????????
?? 
In 1994, TongYong Company purchased goods worthy 
of more than  USD40,000,000.  
(14) ???????????????? 
She gets up early, and does physical exercise every 
morning. 
(15) ?????????????????????
?? 
The occupation of the first products is less than 3/10 
and the portion of the second ones is more than 7/10.   
2 Related Work  
2.1 Related Work for Clause Segmenta-
tion 
Syntactic ambiguity problems increase drasti-
cally as the input sentence becomes longer. Long 
sentence segmentation is a way to avoid the prob-
lem. Many studies have been made on clause seg-
mentation (Carreras and Marquez, 2002, Leffa, 1998, 
Sang and Dejean,2001). In addition, many studies 
also have been done on long sentences segmenta-
tion by certain patterns (Kim and Zhang, 2001, Li and 
Pei, 1990, Palmer and Hearst, 1997).  
However, some researchers merely ignore punc-
tuation, including the comma, and some research-
ers use a comma as one feature to detect the 
segmentation point, not fully using the information 
from the comma.  
2.2 Related Work for Punctuation Proc-
essing  
Several researchers have provided descriptive 
treatment of the role of punctuations: Jones (1996b) 
determined the syntactic function of the punctua-
tion mark. Bayraktar and Akman (1998) classified 
commas by means of the syntax-patterns in which 
they occur. However, theoretical forays into the 
syntactic roles of punctuation were limited. 
Many researchers have used the punctuation 
mark for syntactic analysis and insist that punctua-
tion indicates useful information. Jones (1994) suc-
cessfully shows that grammar with punctuation 
outperforms one without punctuation. Briscoe and 
Carroll 1995) also show the importance of punc-
tuation in reducing syntactic ambiguity. Collins 
(1999), in his statistical parser, treats a comma as an 
important feature. Shiuan and Ann (1996) separate 
complex sentences with respect to the link word, 
including the comma. As a result, their syntactic 
parser performs an error reduction of 21.2% in its 
accuracy.  
(
                                                          
Say (1997) provides a detailed introduction to us-
ing punctuation for a variety of other natural lan-
guage processing tasks.  
All of these approaches prove that punctuation 
analyses improve various natural language process-
ing performance, especially in complex sentence 
segmentation. 
3 Types of Commas  
The comma is the most common punctuation, 
and the one that might be expected to have the 
greatest effect on syntactic parsing. Also, it seems 
natural to break a sentence at the comma position 
in Chinese sentences. The procedure for syntactic 
analysis of a sentence, including the segmentation 
part, is as follows: 
1st step: segment the sentence at a comma 
2nd step: do the dependency analysis for each 
segment 
3rd step: set the dependency relation between  
segment pairs  
In Chinese dependency parsing, not all commas 
are proper as segmentation points.  
First, segmentation at comma in some sentences, 
will cause some of the words fail to find their heads. 
Figure 2 shows, in example (2), there are two 
words, ??  (BeiHai City)  and ?  (preposition) 
from the left segment have dependency relation 
with the word ?(is) of the right segment. So, the 
segmentation at comma , will cause two of  words 
?? (BeiHai City)  and ? (preposition) in the left 
segment, cannot find their head in the second step 
of syntactic parsing stage.  
Second, segmentation at commas can cause 
some words to find the wrong head. Example (3) of 
figure 3 shows two pairs of words with dependency 
relations. For each pair, one word is from the left 
segment, and one word is from the right segment :
?? (like) from the left segment and ??(teacher)  
from the right, ?? (young) from the left and ? 
(of) from the right. Segmentation at the comma 
will cause the word ??(young) to get the word ?
? (like) as its head, which is wrong. 
Example (2) and (3) demonstrate improper sen-
tence segmentation at commas. In figure 2 and fig-
ure 3, there are two dependency lines that cross 
over the commas for both sentences. We call these 
kinds of commas mul_dep_lines_cross comma 
(multiple lines cross comma). In figure 1, there is 
only one dependency line cross over the comma. 
We call these kinds of commas one_dep_line_cross 
comma.    
Segmentation at one_dep_line_cross comma is 
helpful for reducing parsing complexity and can 
contribute to accurate parsing results. However, we 
should avoid segmenting at the position of 
mul_dep_lines_cross comma. It is necessary to 
check each comma according to its context.  
3.1 Delimiter Comma and Separator 
Comma 
Nunberg (1990) classified commas in English 
into two categories, as a delimiter comma and a 
separator comma, by whether the comma is used to 
separate the elements of the same type 4  or not. 
While a delimiter comma is used to separate differ-
ent syntactic types, a separator comma is used to 
separate members of conjoined elements. The 
commas in Chinese can also be classified into these 
two categories. The commas in example (3) and (4) 
are separators, while those in (2) and (5), are 
delimiters. 
However, both delimiter comma and separator 
commas can be mul_dep_line_cross commas. In 
example (2), the comma is a delimiter comma as 
well as a mul_dep_line_cross comma. As a separa-
tor comma, the comma in example (3), is also a 
mul_dep_line_cross comma. Nunberg?s classifica-
tion cannot help to identify mul_dep_line_cross 
commas. 
We therefore need a different kind of classifica-
tion of comma. Both delimiter comma and separa-
tor comma can occur within a clause or at the end 
of a clause. Commas that appear at the end of a 
clause are clearly one_dep_line_cross commas. 
The segmentation at these kinds of comma is valid. 
4 Same type means that it has the same syntactic role in the sentence, it can be a 
coordinate phrase or coordinate clause. 
3.2 Inter-clause Comma and Intra-clause 
Comma 
Commas occurring within a clause are here 
called intra-clause commas. Similarly, commas at 
the end of a clause will be called inter-clause 
commas. Example  (2), (3) include intra-clause 
commas, and example (4), (5) include inter-clause 
commas. 
3.2.1 Constituents of the Two Segments 
Adjoining a Comma 
A segment is a group of words between two 
commas or a group of words from the beginning 
(or end) of a sentence to its nearest comma. 
To identify whether a comma is an inter-clause 
comma or an intra-clause comma, we assign values 
to each comma. These values reflect the nature of 
the two segments next to the comma. Either the left 
or right segment of a comma, can be deduced as a 
phrase5, or several non-overlapped phrases, or a 
clause.(see examples (6)~(15)). The value we as-
sign to a comma is a two-dimensional value 
(left_seg, right_seg). The value of left_seg and 
right_seg can be p(hrase) or c(lause), therefore the 
assigned value for each comma can be (p,p), (p,c), 
(c,p) or (c,c).  
 Commas with (p,p) as the assigned value, in-
clude the case when the left and right segment of 
the comma can be deduced as one phrase, as shown 
in example (6) or several non-overlapped phrases, 
as described in example (7).  
We can assign the value of (c,p) to commas in 
example (8), (9) and (10),  indicating the left ad-
joining segment is a clause and the right one is a 
phrase or several non-overlapped phrases. In a 
similar way, commas in example (11)~(13) are 
case of (p,c). 
 If a comma has (c,c) as the assigned value, both 
the left segment and the right segment can be de-
duced as a clause. The relation between the two 
clauses can be coordinate (example (14)) or subor-
dinate (example (15)).  
                                                          
5 Phrase is the group of words that can be deduced as the phrase in Chinese Penn 
Tree Bank 2.0. A phrase may contain an embedded clause as its adjunct or 
complement.  
(a), ??????????? 
(b) ????????? 
In example (a) ,the PP has the embedded clause as its complement. And in 
example (b), the embedded clause is the adjunct of the NP. 
3.2.2 Syntactic Relation between Two Ad-
joining Segments 
A word (some words) in the left segment and a 
word (some words) in the right segment of a 
comma may or may not have a dependency rela-
tion(s). For a comma, if at least one word from the 
left segment has a dependency relation with a word 
from the right segment, we say the left segment and 
the right segment have a syntactic relation. Other-
wise the two segments adjoining the comma have 
no syntactic relations. Rel() functions are defined 
in table-1. 
Table 1: functions Rel(), Dir() and Head() 
Rel() 
? To check if any words of the left segment has a 
dependency relation with the word of the right 
segment. 
? If there is, Rel()=1  
Otherwise Rel()=0. 
Dir() 
? To indicate how many direction(s) of the de-
pendency relations the left and right segment 
have. when Rel()=1. 
? For one_dep_line_cross comma, Dir()=1. 
? For mul_dep_line_cross comma, if the directions 
of the dependency relations are the same, 
Dir()=1, else Dir()=2. 
Head() 
? To indicate which side of segment contains the 
head of any words of the other side, when 
Rel()=1. 
? When Dir()=1, if the left segment contains any 
word as the head of a word of the right, Head() = 
left; Otherwise Head()=right. 
? When Dir()=2,   
1. According to the direction of dependency 
relation of these two segments, to find the 
word which has no head. 
2. If the word is on the left, Head()=left, other-
wise, Head()=right.  
 
For the one_dep_line_cross comma, the left and 
right segments have syntactic relation, and only 
one word from a segment has a dependency rela-
tion with a word from the other segment. For 
mul_dep_line_cross comma, at least two pairs of 
words from each segment have dependency rela-
tions. We then say that the left and right segments 
adjacent to the comma have multiple dependency 
relations. The directions of each relation may differ 
or not. We define a function Dir() as follows : if all 
the directions of the relations are the same, get 1 as 
its value, else 2 for its value. This is in table-1. We 
also define function Head() to indicate whether the 
left segment or the right segment contains the head 
word of the other when the two segments have syn-
tactic relation. This is also shown in table 1. 
In example (3) as figure 3 shows, Rel()=1, 
Dir()=2 and Head()=left. 
3.2.3 Inter-clause Comma and Intra-
clause Comma  
For commas assigned values  (p,p) or (c,c), the 
function Rel() is always 1. Commas with values (c, 
p) or (p,c) can be further divided into two sub-cases. 
Table 2 shows the sub-case of (c,p), and table 3 
shows the sub-cases of (p,c). 
 
Rel() =0 The 2nd comma of Example (8); 
Example (9); 
Head() =right = p 
(c,p)-
I 
Rel()=1 Example (10); 
Head() = left=c 
(c,p)-
II 
Table 2: sub-cases of commas with value of (c,p) 
 
Rel()=0 Example (11); 
Example (12); 
Head() =left = p 
(p,c)-
I 
Rel()=1 Example (13); 
Head() = right=c 
(p,c)-
II 
Table3: sub-cases of commas with value of (p,c) 
 
Commas with the value of (p,p), (c,p)-II and 
(p,c)-II are used to connect coordinate phrases or to 
separate two constituents of a clause. These com-
mas are intra-clause commas. 
Commas with (c,c), (c,p)-I and (p,c)-I are used 
as a clause boundaries. These are inter-clause 
commas. 
An inter-clause comma joins the clauses together 
to form a sentence. The commas that belong to an 
inter-clause category are safe as segmentation 
points (Kim, 2001).   
4 Feature Selection  
To identify the inter-clause or intra-clause role 
of a comma, we need to estimate the right and left 
segment conjuncts to the comma, using informa-
tion from both segments. Any information to iden-
tify a segment as a clause or a phrase or phrases is 
useful. Carreras and Marquez (2001) prove that 
using features containing relevant information 
about a clause leads to more efficient clause identi-
fication. Their system outperforms all other sys-
tems in CoNLL?01 clause identification shared task 
(Sang & Dejean, 2001). Given this consideration, we 
select two categories of features as follows. 
 (1) Direct relevant feature category: predicate 
and its complements. 
 (2) Indirect relevant feature category: auxiliary 
words or adverbials or prepositions or clausal 
conjunctions. 
Directly relevant features 
VC: if a copula ? appears 
VA: if an adjective appears 
VE: if ? as the main verb appears 
VV: if a verb appears 
CS: if a subordinate conjunction appears 
Table 4: feature types for classification 
Indirectly relevant features 
AD: if an adverb appears 
AS: if an aspect marker appears 
P: if a preposition appears 
DE: if ? appears 
DEV:if ? appears  
DER: if ? appears 
BA_BEI: if ? or ? appears 
LC: if a localizer appears 
FIR_PR : if the first word is a pronoun 
LAS_LO: if the last word is a localizer 
LAS_T : if the last word is a time 
LAS_DE_N : if the last word is a noun that follows ? 
No_word : if the length of a word is more than 5 
no_verb: if no verb(including VA)  
DEC: if there is relative clause 
ONE: if the segment has only one word 
 
To detect whether a segment is a clause or 
phrase, the verbs are important. However, Chinese 
has no morphological paradigms and a verb takes 
various syntactic roles besides the predicate, with-
out any change of its surface form. This means that  
information about the verb is not sufficient, in itself, 
to determine whether segment is a clause.  
When the verb takes other syntactic roles besides 
the predicate, it?s frequently accompanied by func-
tion words. For example, a verb can be used as the 
complement of the auxiliary word ? or ?(Xia, 
2000), to modify the following verb or noun. In 
these cases, the auxiliary words are helpful for de-
ciding the syntactic role of the verb. Other function 
words around the verb also help us to estimate the 
syntactic role of the verb. Under this consideration, 
we employ all the function words as features, 
where they are composed as the indirect relevant 
feature category.  
Table 4 gives the entire feature set. The label of 
each feature type is same as the tag set of Chinese 
Penn Treebank 2.0 (see Xia (2000) for more de-
tailed description). If the feature appears at the left 
segment, we label it as L_feature type, and if it is 
on the right, it?s labeled as R_ feature type, where 
feature type is the feature that is shown on table 4.  
The value for each feature is either 0 or 1. When 
extracting features of a sentence, if any feature in 
the table 4, appears in the sentence, we assign the 
value as 1 otherwise 0. The features of example (12) 
are extracted as table 5 describes. All of these val-
ues are composed as an input feature vector for 
comma classification. 
Table 5: the extracted features of example (12)  
5 Experiments 
For training and testing, we use the Chinese 
Penn Treebank 2.0 corpus based on 10-fold valida-
tion. First, using bracket information, we extract 
the type (inter-clause comma or intra-clause 
comma) for each comma, as we defined. The ex-
tracted information is used as the standard answer 
sheet for training and testing. 
 We extract the feature vector for each comma, 
and use support vector machines (SVM) to perform 
the classification work.  
Performances are evaluated by the following 
four types of measures: accuracy, recall, F?=1/2 for 
inter-clause and intra-clause comma respectively, 
and total accuracy. Each evaluation measure is cal-
culated as follows. 
Inter(or intra)-clause comma accuracy6 =  
 
identifiedofnumberthe
identifiedcorrectlyofnumberthe  
Inter(or intra)-clause comma recall  = 
 
classtheofnumbertotal
identifiedcorrectlyofnumberthe  
Inter(or intra)-clause comma F ?=1/2 = 
recall) comma clauseintra)inter(or              
precision comma clauseintra)(inter(or 
)recallcommaclauseintra)inter(or               
precisioncommaclauseintra)inter(or (2
?
+?
?
???
 
Total accuracy = 
 
commasofnumbertotal
identifiedcorrectlyofnumbertotal  
5.1 Classification Using SVM 
 Support vector machines (SVM) are one of the 
binary classifiers based on maximum margin strat-
egy introduced by Vapnik (Vapnik, 1995). For many 
classification works, SVM outputs a state of the art 
performance.  
L_VC =0 L_VA =0 L_VE =0  
R_VC = 0 R_VA =0 R_VE =0 
L_VV = 0 L_CS =0 L_AD =0 
R_VV =1 R_CS =0 R_AD =0 
L_AS =0 L_P =0  L_DE =0 
R_AS =1 R_P =0 R_DE =1 
L_DEV =0 L_DER = 0 L_BA_BEI =0 
R_DEV =0 R_DER =0 R_BA_BEI=0 
L_LC =0 L_DEC = 0 L_FIR_PR  =0 
R_LC =0 R_DEC =0 R_FIR_PR=0  
L_LAS_LO =0 L_LAS_T =1 L_LAS_DE_N=0 
R_LAS_LO=0 R_LAS_T=0 R_LAS_DE_N=1 
L_No_word=0 L_no_verb =1 L_ONE = 0 
R_No_word =1 R_no_verb =0 R_ONE =0 
There are two advantages in using SVM for clas-
sification: 
(1) High generalization performance in high di-
mensional feature spaces. 
(2) Learning with combination of multiple fea-
tures is possible via various kernel functions.  
Because of these characteristics, many research-
ers use SVM for natural language processing and 
obtain satisfactory experimental results (Yamada, 
2003). 
In our experiments, we use SVMlight (Joachims, 
1999) as a classification tool. 
5.2 Experimental Results 
First, we set the entire left segment and right 
segment as an input window. Table 6 gives the per-
formance with different kernel functions. The RBF 
kernel function with ? =1.5 outputs the best per-
formance. Therefore, in the following experiments, 
we use this kernel function only.  
Next, we perform several experiments on how 
the selection of word window affects performance. 
First, we select the adjoining 3 words of the right 
and left segment each, indicated as win-3 in table 7. 
                                                          
6 The inter-clause comma precision is abbreviated  as inter-P. Same way, Inter-R 
for inter-clause comma recall, ..etc. 
Second, we select the first 2 words and last 3 words 
of the left segment and the first 3 and last 2 of the 
right segment, indicated as win 2-3 in table 7. Fi-
nally, we use the part of speech sequence as input. 
As the experimental results show, the part of 
speech sequence is not a good feature. The features 
with clausal relevant information obtain a better 
output. We also find that the word window of first 
2-last 3 obtains the best total precision, better than 
using the entire left and right segments. From this, 
we conclude that the words at the beginning and 
end of the segment reveal segment clausal informa-
tion more effectively than other words in the seg-
ment. 
5.3 Comparison of Parsing Accuracy with 
and without Segmentation Model 
The next experiment tests how the segmentation 
model contributes to parsing performance. We use 
a Chinese dependency parser, which was imple-
mented with the architecture presented by Kim 
(2001) presents.  
After integrating the segmentation model, the 
parsing procedure is as follows: 
- Part of speech tagging. 
- Long sentence segmentation by comma. 
- Parsing based on segmentation. 
Table 9 gives a comparison of the results of the 
original parser with the integrated parser.  
5.4 Comparison with Related Work  
Shiuan and Ann?s (1996) system obtains the 
clues for segmenting a complex sentence in Eng-
lish by disambiguating the link words, including 
the comma. The approach to find the segmentation 
point by analyzing the specific role of the comma 
in the sentence seems similar with our approach. 
However, our system differs from theirs as follows: 
(1) Shiuan and Ann?s system sieves out just two 
roles for the comma, while ours gives an 
analysis for the complete usages of the 
comma. 
(2) Shiuan and Ann?s system also analyzes the 
clausal conjunction or subordinating preposi-
tion as the segmentation point. 
Although the language for analysis is different, 
and the training and testing data also differ, the 
motivation of the two systems is the same. In addi-
tion, both systems are evaluated by integrating the 
original parser. The average accuracy of comma 
disambiguation in Shiuan and Ann?s is 93.3% that 
is higher than ours by 6.2%. However, for parsing 
accuracy, Shiuan and Ann?s system improves by 
4%(error reduction of 21.2%), while ours improves 
by 9.6 percent. 
 
Kernel 
function Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
linear 
74.22
% 
77.87
% 
72.52
% 
70.61
% 
76.00
% 
71.56
% 
73.14
% 
Polynomial 
d=2 
79.84
% 
81.15
% 
84.51
% 
83.77
% 
80.49
% 
84.14
% 
82.86
% 
Polynomial 
d=3 
78.57
% 
81.15
% 
88.39
% 
86.84
% 
79.84
% 
87.61
% 
84.86
% 
RBF    ? = 0.5 78.46% 83.61% 88.64% 85.53% 80.95% 87.05% 84.86% 
RBF  ? = 1.5 78.69% 78.69% 89.04% 89.04% 78.69% 89.04% 85.43% 
RBF  ? = 2.5 80.62% 85.25% 88.24% 85.53% 82.87% 86.86% 85.43% 
RBF ? = 3.5 79.41% 88.52% 85.05% 79.82% 83.72% 82.35% 82.86% 
Table 6: experimental results with  
different kernel functions 
 
Word Win-
dow Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
Win3 
80.45
% 
87.70
% 
84.33
% 
80.26
% 
83.92
% 
82.25
% 
82.86
% 
Win2-3 
85.60
% 
87.70
% 
88.00
% 
86.84
% 
86.64
% 
87.42
% 
87.14
% 
Table 7: experimental results for  
word window size  
 
 Inter-P Inter-R Intra-P Intra-R Inter-F Intra-F Total-P
POS 
sequence 
75.42
% 
72.95
% 
80.60
% 
82.02
% 
74.17
% 
81.30
% 
78.86
% 
Table 8: experimental results for using  
part of speech sequence  
 
 Original 
parser 
Integrated 
parser 
Average dependency pars-
ing accuracy7 
73.8% 83.4% 
Average complete sentence 
accuracy 
23.8% 25.4% 
Table 9: comparison of parsing accuracy of the 
original parser with the integrated parser 
                                                          
7 The evaluation measures are used as it is defined in Kim (2001). 
6 Conclusion 
In this paper, we propose a method to segment a 
Chinese sentence by classification of the comma.  
We define the criteria for classification, and ac-
cording to the criteria, a model for classification of 
the comma is given. The segmentation at the 
comma position seems to be efficient for improv-
ing the accuracy of dependency parsing by 
9.6percent. Moreover, since commas more fre-
quently appear in Chinese language, we expect our 
approach including salient and refined analysis of 
comma usages provides feasible solutions for seg-
mentation. 
However, the accuracy for the segmentation is 
not yet satisfactory. Since erroneous segmentation 
may cause a parsing failure for the entire sentence, 
errors can be serious. Further research should be 
done to improve the performance and reduce side 
effects for parsing the entire sentence.  
Acknowledgments 
This work was supported by the KOSEF through 
the Advanced Information Technology Research 
Center (AITrc), and by the BK21 Project. 
References 
M. Bayparktar,  B. Say and V. Akman 1998, An analysis 
of English punctuation: the special case of comma, 
International Journal of Corpus Linguistics, 1998 
X. Carreras, L. Marquez, V. Punyakanok, and D. Roth 
2002, Learning and inference for clause identification, 
Proceeding of 13th European Conference on Machine 
Learning, Finland, 2002 
R.L. Hill 1996, A comma in parsing: A study into the 
influence of punctuation (commas) on contextually 
isolated "garden-path" sentences.  M.Phil disseration, 
Dundee University, 1996 
T.Joachims 1999, Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press, 1999 
B. Jones 1994, Exploring the role of punctuation in pars-
ing natural text, Proceedings of COLING-94, pages 
421-425 
B. Jones 1996a, What?s the point? A (computational)  
theory of punctuation, PhD Thesis, Centre for Cogni-
tive Science, University of Edinburgh, Edinburgh, 
UK, 1996 
B. Jones 1996b, Towards testing the syntax of punctua-
tion, Proceeding of 34th ACL, 1996 
M.Y.Kim, S.J. Kang, J.H. Lee 2001, Resolving ambigu-
ity in Inter-chunk dependency parsing, Proceedings 
of the sixth Natural Language Processing Pacific Rim 
Symposium, Tokyo, Japan, 2001 
S. Kim, B.Zhang and Y. Kim 2001, Learning-based 
intrasentence segmentation for efficient translation of 
long sentences, Machine Translation, Vol.16, no.3, 
2001 
Roger Levy and Christopher Manning. 2003. Is it harder 
to parse Chinese, or the Chinese Treebank? In 
Proceeding of ACL-2003. 
V.J. Leffa 1998, clause processing in complex sentences, 
Proceeding of 1st International Conference on Lan-
guage Resources and Evaluation, Spain,1998 
W.C. Li, T.Pei, B.H. Lee and Chiou, C.F. 1990, Parsing 
long English sentences with pattern rules, Proceeding 
of 13th International Conference on Computational 
Linguistics, Finland, 1990 
Shui-fang Lin 2000. study and application of punctua-
tion(??????????). People?s Publisher, 
P.R.China. (in Chinese) 
Geoffrey Nunberg 1990. the linguistics of punctua-
tion .CSLI lecture notes. 18, Stanford, California. 
D.D. Palmer and M.A. Hearst 1997, Adaptive multilin-
gual sentence boundary disambiguation, Computa-
tional Linguistics, Vol.27, 1997 
E.F.T.K. Sang and H.Dejean. 2001, Introduction to the 
CoNLL-2001 shared task: clause identification, Pro-
ceeding of CoNLL-2001 
B. Say and V. Akman 1997, current approaches to 
punctuation in computational linguistics, Computers 
and the Humanities, 1997 
P.L. Shiuan and C.T.H. Ann 1996, A divide-and-
conquer strategy for parsing, Proceedings of the 
ACL/SIGPARSE 5th international workshop on pars-
ing technologies, Santa Cruz, USA, pp57-66 
Fei Xia 2000, The bracketing Guidelines for the Penn 
Chinese Treebank(3.0) 
Vladimir N Vapnik 1995 The nature of statistical learn-
ing theory. New York, 1995 
High Efficiency Realization for a
Wide-Coverage Unification Grammar
John Carroll1 and Stephan Oepen2
1 University of Sussex
2 University of Oslo and Stanford University
Abstract. We give a detailed account of an algorithm for efficient tactical gener-
ation from underspecified logical-form semantics, using a wide-coverage gram-
mar and a corpus of real-world target utterances. Some earlier claims about chart
realization are critically reviewed and corrected in the light of a series of practical
experiments. As well as a set of algorithmic refinements, we present two novel
techniques: the integration of subsumption-based local ambiguity factoring, and
a procedure to selectively unpack the generation forest according to a probability
distribution given by a conditional, discriminative model.
1 Introduction
A number of wide-coverage precise bi-directional NL grammars have been developed
over the past few years. One example is the LinGO English Resource Grammar (ERG)
[1], couched in the HPSG framework. Other grammars of similar size and coverage also
exist, notable examples using the LFG and the CCG formalisms [2,3]. These grammars
are used for generation from logical form input (also termed tactical generation or real-
ization) in circumscribed domains, as part of applications such as spoken dialog systems
[4] and machine translation [5].
Grammars like the ERG are lexicalist, in that the majority of information is encoded
in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e.
rules operating on phrases). The semantic input to the generator for such grammars,
often, is a bag of lexical predicates with semantic relationships captured by appropriate
instantiation of variables associated with predicates and their semantic roles. For these
sorts of grammars and ?flat? semantic inputs, lexically-driven approaches to realization
? such as Shake-and-Bake [6], bag generation from logical form [7], chart generation
[8], and constraint-based generation [9] ? are highly suitable. Alternative approaches
based on semantic head-driven generation and more recent variants [10,11] would work
less well for lexicalist grammars since these approaches assume a hierarchically struc-
tured input logical form.
Similarly to parsing with large scale grammars, realization can be computation-
ally expensive. In his presentation of chart generation, Kay [8] describes one source
of potential inefficiency and proposes an approach for tackling it. However, Kay does
not report on a verification of his approach with an actual grammar. Carroll et al [12]
 Dan Flickinger and Ann Copestake contributed a lot to the work described in this paper. We
also thank Berthold Crysmann, Jan Tore L?nning and Bob Moore for useful discussions. Fund-
ing is from the projects COGENT (UK EPSRC) and LOGON (Norwegian Research Council).
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 165?176, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
166 J. Carroll and S. Oepen
? h1,
{ h1:proposition m(h2), h3: run v(e4, x5), h3:past(e4),
h6: the q(x5, h7, h8), h9: athlete n(x5), h9: young a(x5), h9: polish a(x5) },
{ h2 =q h3, h8 =q h9 } ?
Fig. 1. Simplified MRS for an utterance like the young Polish athlete ran (and variants). Elements
from the bag of EPs are linked through both scopal and ?standard? logical variables.
present a practical evaluation of chart generation efficiency with a large-scale HPSG
grammar, and describe a different approach to the problem which becomes necessary
when using a wide-coverage grammar. White [3] identifies further inefficiencies, and
describes and evaluates strategies for addressing them, albeit using what appears to be
a somewhat task-specific rather than genuine wide-coverage grammar. In this paper,
we revisit this previous work and present new, improved algorithms for efficient chart
generation; taken together these result in (i) practical performance that improves over
a previous implementation by two orders of magnitude, and (ii) throughput that is near
linear in the size of the input semantics.
In Section 2, we give an overview of the grammar and the semantic formalism
we use, recap the basic chart generation procedure, and discuss the various sources of
potential inefficiency in the basic approach. We then describe the algorithmic improve-
ments we have made to tackle these problems (Section 3), and conclude with the results
of evaluating these improvements (Section 4).
2 Background
2.1 Minimal Recursion Semantics and the LinGO ERG
Minimal Recursion Semantics (MRS) [13] is a popular member of a family of flat, un-
derspecified, event-based (neo-Davidsonian) frameworks for computational semantics
that have been in wide use since the mid-1990s. MRS allows both underspecification of
scope relations and generalization over classes of predicates (e.g. two-place temporal
relations corresponding to distinct lexical prepositions: English in May vs. on Monday,
say), which renders it an attractive input representation for tactical generation. While an
in-depth introduction to MRS is beyond the scope of this paper, Figure 1 shows an ex-
ample semantics that we will use in the following sections. The truth-conditional core is
captured as a flat multi-set (or ?bag?) of elementary predications (EPs), combined with
generalized quantifiers and designated handle variables to account for scopal relations.
The bag of EPs is complemented by the handle of the top-scoping EP (h1 in our exam-
ple) and a set of ?handle constraints? recording restrictions on scope relations in terms
of dominance relations.
The LinGO ERG [1] is a general-purpose, open-source HPSG implementation with
fairly comprehensive lexical and grammatical coverage over a variety of domains and
genres. The grammar has been deployed for diverse NLP tasks, including machine
translation of spoken and edited language, email auto response, consumer opinion track-
ing (from newsgroup data), and some question answering work.1 The ERG uses MRS
1 See http://www.delph-in.net/erg/ for background information on the ERG.
High Efficiency Realization for a Wide-Coverage Unification Grammar 167
as its meaning representation layer, and the grammar distribution includes treebanked
versions of several reference corpora ? providing disambiguated and hand-inspected
?gold? standard MRS formulae for each input utterance ? of which we chose one of the
more complex sets for our empirical investigations of realization performance using the
ERG (see Section 4 below).
2.2 The Basic Procedure
Briefly, the basic chart generation procedure works as follows. A preprocessing phase
indexes lexical entries, lexical rules and grammar rules by the semantics they contain.
In order to find the lexical entries with which to initialize the chart, the input semantics
is checked against the indexed lexicon. When a lexical entry is retrieved, the variable
positions in its relations are instantiated in one-to-one correspondence with the variables
in the input semantics (a process we term Skolemization, in loose analogy to the more
general technique in theorem proving; see Section 3.1 below). For instance, for the MRS
in Figure 1, the lookup process would retrieve one or more instantiated lexical entries
for run containing h3: run v(e4, x5). Lexical and morphological rules are applied to the
instantiated lexical entries. If the lexical rules introduce relations, their application is
only allowed if these relations correspond to parts of the input semantics (h3:past(e4),
say, in our example). We treat a number of special cases (lexical items containing more
than one relation, grammar rules which introduce relations, and semantically vacuous
lexical items) in the same way as Carroll et al [12].
After initializing the chart (with inactive edges), active edges are created from in-
active ones by instantiating the head daughter of a rule; the resulting edges are then
combined with other inactive edges. Chart generation is very similar to chart parsing,
but what an edge covers is defined in terms of semantics, rather than orthography. Each
edge is associated with the set of relations it covers. Before combining two edges a
check is made to ensure that edges do not overlap: i.e. that they do not cover the same
relation(s). The goal is to find all possible inactive edges covering the full input MRS.
2.3 Complexity
The worst-case time complexity of chart generation is exponential (even though chart
parsing is polynomial). The main reason for this is that in theory a grammar could allow
any pair of edges to combine (subject to the restriction described above that the edges
cover non-overlapping bags of EPs). For an input semantics containing n EPs, and
assuming each EP retrieves a single lexical item, there could in the worst case be O(2n)
edges, each covering a different subset of the input semantics. Although in the general
case we cannot improve the complexity, we can make the processing steps involved
cheaper, for instance efficiently checking whether two edges are candidates for being
combined (see Section 3.1 below). We can also minimize the number of edges covering
each subset of EPs by ?packing? locally equivalent edges (Section 3.2).
A particular, identifiable source of complexity is that, as Kay [8] notes, when a word
has more than one intersective modifier an indefinite number of its modifiers may be
applied. For instance, when generating from the MRS in Figure 1, edges corresponding
to the partial realizations athlete, young athlete, Polish athlete, and young Polish athlete
will all be constructed. Even if a grammar constrains modifiers so there is only one valid
168 J. Carroll and S. Oepen
ordering, or the generator is able to pack equivalent edges covering the same EPs, the
number of edges built will still be 2n, because all possible complete and incomplete
phrases will be built. Using the example MRS, ultimately useless edges such as the
young athlete ran (omitting Polish) will be created.
Kay proposes an approach to this problem in which edges are checked before they
are created to see if they would ?seal off? access to a semantic index (x5 in this case) for
which there is still an unincorporated modifier. Although individual sets of modifiers still
result in exponential numbers of edges, the exponentiality is prevented from propagating
further. However, Carroll et al [12] argue that this check works only in limited circum-
stances, since for example in (1) the grammar must allow the index for ran to be available
all the way up the tree to How, and simultaneously also make available the indexes for
newspapers, say, and athlete at appropriate points so these words could be modified2.
(1) How quickly did the newspapers say the athlete ran?
Carroll et al describe an alternative technique which adjoins intersective modifiers into
edges in a second phase, after all possible edges that do not involve intersective modi-
fication have been constructed by chart generation. This overcomes the multiple index
problem described above and reduces the worst-case complexity of intersective modi-
fication in the chart generation phase to polynomial, but unfortunately the subsequent
phase which attempts to adjoin sets of modifiers into partial realizations is still expo-
nential. We describe below (Section 3.3) a related technique which delays processing of
intersective modifiers by inserting them into the generation forest, taking advantage of
dynamic programming to reduce the complexity of the second phase. We also present
a different approach which filters out edges based on accessibility of sets of seman-
tic indices (Section 3.4), which covers a wider variety of cases than just intersective
modification, and in practice is even more efficient.
Exponential numbers of edges imply exponential numbers of realizations. For an
application task we would usually want only one (the most natural or fluent) realization,
or a fixed small number of good realizations that the application could then itself select
from. In Section 3.5 we present an efficient algorithm for selectively unpacking the
generation forest to produce the n-best realizations according to a statistical model.
3 Efficient Wide-Coverage Realization
3.1 Relating Chart Edges and Semantic Components
Once lexical lookup is complete and up until a final, post-generation comparison of
results to the input MRS, the core phases of our generator exclusively operate on typed
feature structures (which are associated to chart edges). For efficiency reasons, our algo-
rithm avoids any complex operations on the original logical-form input MRS. In order
to best guide the search from the input semantics, however, we employ two techniques
that relate components of the logical form to corresponding sub-structures in the feature
2 White [3] describes an approach to dealing with intersective modifiers which requires the
grammarian to write a collection of rules that ?chunk? the input semantics into separate modi-
fier groups which are processed separately; this involves extra manual work, and also appears
to suffer from the same multiple index problem.
High Efficiency Realization for a Wide-Coverage Unification Grammar 169
structure (FS) universe: (i) Skolemization of variables and (ii) indexing by EP cover-
age. Of these, only the latter we find commonly discussed in the literature, but we expect
some equivalent of making variables ground to be present in most implementations.
As part of the process of looking up lexical items and grammar rules introducing se-
mantics in order to initialize the generator chart, all FS correspondences to logical vari-
ables from the input MRS are made ?ground? by specializing the relevant sub-structure
with Skolem constants uniquely reflecting the underlying variable, for example adding
constraints like [SKOLEM ?x5?] for all occurrences of x5 from our example MRS.
Skolemization, thus, assumes that distinct variables from the input MRS, where supplied,
cannot become co-referential during generation. Enforcing variable identity at the FS
level makes sure that composition (by means of FS unification) during rule applications
is compatible to the input semantics. In addition, it enables efficient pre-unification fil-
tering (see ?quick-check? below), and is a prerequisite for our index accessibility test
described in Section 3.4 below.
In chart parsing, edges are stored into and retrieved from the chart data structure
on the basis of their string start and end positions. This ensures that the parser will
only retrieve pairs of chart edges that cover compatible segments of the input string (i.e.
that are adjacent with respect to string position). In chart generation, Kay [8] proposed
indexing the chart on the basis of logical variables, where each variable denotes an
individual entity in the input semantics, and making the edge coverage compatibility
check a filter. Edge coverage (with respect to the EPs in the input semantics) would be
encoded as a bit vector, and for a pair of edges to be combined their corresponding bit
vectors would have to be disjoint.
We implement Kay?s edge coverage approach, using it not only when combining
active and inactive edges, but also for two further tasks in our approach to realization:
? in the second phase of chart generation to determine which intersective modifier(s)
can be adjoined into a partially incomplete subtree; and
? as part of the test for whether one edge subsumes another, for local ambiguity
factoring (see Section 3.2 below)3.
In our testing with the LinGO ERG, many hundreds or thousands of edges may be
produced for non-trivial input semantics, but there are only a relatively small number
of logical variables. Indexing edges on these variables involves bookkeeping that turns
out not to be worthwhile in practice; logical bit vector operations on edge coverage
take negligible time, and these serve to filter out the majority of edge combinations
with incompatible indices. The remainder are filtered out efficiently before unification
is attempted by a check on which rules can dominate which others, and the quick-check,
as developed for unification-based parsing [14]. For the quick-check, it turns out that
the same set of feature paths that most frequently lead to unification failure in parsing
also work well in generation.
3 We therefore have four operations on bit vectors representing EP coverage (C) in chart edges:
? concatenation of edges e1 and e2 ? e3: C(e3) = OR(C(e1), C(e2));
? can edges e1 and e2 combine? AND(C(e1), C(e2)) = 0;
? do edges e1 and e2 cover the same EPs? C(e1) = C(e2);
? do edges e1, . . . , en cover all input EPs? NOT(OR(C(e1), . . . , C(en)) = 0.
170 J. Carroll and S. Oepen
3.2 Local Ambiguity Factoring
In chart parsing with context free grammars, the parse forest (a compact representation
of the full set of parses) can only be computed in polynomial time if sub-analyses dom-
inated by the same non-terminal and covering the same segment of the input string are
?packed?, or factored into a single unitary representation [15]. Similar benefits accrue
for unification grammars without a context free backbone such as the LinGO ERG,
if the category equality test is replaced by feature structure subsumption [16]4; also,
feature structures representing the derivation history need to be restricted out when ap-
plying a rule [17]. The technique can be applied to chart realization if the input span is
expressed as coverage of the input semantics. For example, with the input of Figure 1,
the two phrases in (2) below would have equivalent feature structures, and we pack the
one found second into the one found first, which then acts as the representative edge for
all subsequent processing.
(2) young Polish athlete | Polish young athlete
We have found that packing is crucial to efficiency: realization time is improved by more
than an order of magnitude for inputs with more than 500 realizations (see Section 4).
Changing packing to operate with respect just to feature structure equality rather than
subsumption degrades throughput significantly, resulting in worse overall performance
than with packing disabled completely: in other words, equivalence-only packing fails
to recoup the cost of the feature structure comparisons involved.
A further technique we use is to postpone the creation of feature structures for active
edges until they are actually required for a unification operation, since many end up as
dead ends. Oepen and Carroll [18] do a similar thing in their ?hyper-active? parsing
strategy, for the same reason.
3.3 Delayed Modifier Insertion
As discussed in Section 2.3, Carroll et al [12] adjoin intersective modifiers into each
partial tree extracted from the forest; their algorithm searches for partitions of modifier
phrases to adjoin, and tries all combinations. This process adds an exponential (in the
number of modifiers) factor to the complexity of extracting each partial realization.
This is obviously unsatisfactory, and in practice is slow for larger problems when
there are many possible modifiers. We have devised a better approach which delays
processing of intersective modifiers by inserting them into the generation forest at ap-
propriate locations before the forest is unpacked. By doing this, we take advantage of
the dynamic programming-based procedure for unpacking the forest to reduce the com-
plexity of the second phase. The procedure is even more efficient if realizations are
unpacked selectively (section 3.5).
3.4 Index Accessibility Filtering
Kay?s original proposal for dealing efficiently with modifiers founders because more
than one semantic index may need to be accessible at any one time (leading to the
4 Using subsumption-based packing means that the parse forest may represent some globally
inconsistent analyses, so these must be filtered out when the forest is unpacked.
High Efficiency Realization for a Wide-Coverage Unification Grammar 171
alternative solutions of modifier adjunction, and of chunking the input semantics ? see
Sections 2.3 and 3.3).
However, it turns out that Kay?s proposal can form the basis of a more generally
applicable approach to the problem. We assume that we have available an operation
collect-semantic-vars() that traverses a feature structure and returns the set of semantic
indices that it makes available5. We store in each chart edge two sets: one of semantic
variables in the feature structure that are accessible (that is, they are present in the
feature structure and could potentially be picked by another edge when it is combined
with this one), and a second set of inaccessible semantic variables (ones that were once
accessible but no longer are). Then,
? when an active edge is combined with an inactive edge, the accessible sets and
inaccessible sets in the resulting edge are the union of the corresponding sets in the
original edges;
? when an inactive edge is created, its accessible set is computed to be the semantic
indices available in its feature structure, and the variables that used to be accessible
but are no longer in the accessible set are added to its inaccessible set, i.e.
1 tmp ? edge.accessible;
2 edge.accessible ? collect-semantic-vars(edge.fs)
3 edge.inaccessible ? (tmp \ edge.accessible) ? edge.inaccessible
? immediately after creating an inactive edge, each EP in the input semantics that
the edge does not (yet) cover is inspected, and if the EP?s index is in the edge?s
inaccessible set then the edge is discarded (since there is no way in the future that
the EP could be integrated with any extension of the edge?s semantics).
A nice property of this new technique is that it applies more widely than to just
intersective modification: for instance, if the input semantics were to indicate that a
phrase should be negated, no edges would be created that extended that phrase without
the negation being present. Section 4 shows this technique results in dramatic improve-
ments in realization efficiency.
3.5 Selective Unpacking
The selective unpacking procedure outlined in this section allows us to extract a small
set of n-best realizations from the generation forest at minimal cost. The global rank
order is determined by a conditional Maximum Entropy (ME) model ? essentially an
adaptation of recent HPSG parse selection work to the realization ranking task [19]. We
use a similar set of features to Toutanova and Manning [20], but our procedure dif-
fers from theirs in that it applies the stochastic model before unpacking, in a guided
search through the generation forest. Thus, we avoid enumerating all candidate realiza-
tions. Unlike Malouf and van Noord [21], on the other hand, we avoid an approximative
beam search during forest creation and guarantee to produce exactly the n-best realiza-
tions (according to the ME model). Further looking at related parse selection work, our
procedure is probably most similar to those of Geman and Johnson [22] and Miyao and
5 Implementing collect-semantic-vars() can be efficient: searching for Skolem constants through-
out the full structure, it does a similar amount of computation as a single unification.
172 J. Carroll and S. Oepen
1 ?
?
2 3
? ?
4 3
?
2 ?
?
5 6
? ?
5 7
?
4 ?
?
8 6
? ?
8 7
? ?
9 6
? ?
9 7
?
6 ?
?
10
? ?
11
?
Fig. 2. Sample generator forest and sub-node decompositions: ovals in the forest (on the left)
indicate packing of edges under subsumption, i.e. edges 4 , 7 , 9 , and 11 are not in the gen-
erator chart proper. During unpacking, there will be multiple ways of instantiating a chart edge,
each obtained from cross-multiplying alternate daughter sequences locally. The elements of this
cross-product we call decomposition, and they are pivotal points both for stochastic scoring and
dynamic programming in selective unpacking. The table on the right shows all non-leaf decom-
positions for our example generator forest: given two ways of decomposing 6 , there will be three
candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees.
Tsujii [23], but neither provide a detailed discussion of the dependencies between local-
ity of ME features and the complexity of the read-out procedure from a packed forest.
Two key notions in our selective unpacking procedure are the concepts of (i) decom-
posing an edge locally into candidate ways of instantiating it and of (ii) nested contexts
of ?horizontal? search for ranked hypotheses (i.e. uninstantiated edges) about candidate
subtrees. See Figure 2 for examples of edge decomposition, but note that the ?depth?
of each local cross-product needs to correspond to the maximum required context size
of ME features; for ease of exposition, our examples assume a context size of no more
than depth one (but the algorithm straightforwardly generalizes to larger contexts). Given
one decomposition ? i.e. a vector of candidate daughters to a token construction ? there
can be multiple ways of instantiating each daughter: a parallel index vector ?i0 . . . in?
serves to keep track of ?vertical? search among daughter hypotheses, where each index ij
denotes the i-th instantiation (hypothesis) of the daughter at position j. Hypotheses are
associated with ME scores and ordered within each nested context by means of a local
agenda (stored in the original representative edge, for convenience). Given the additive
nature of ME scores on complete derivations, it can be guaranteed that larger derivations
including an edge e as a sub-constituent on the fringe of their local context of optimiza-
tion will use the best instantiation of e in their own best instantiation. The second-best
larger instantiation, in turn, will be obtained from moving to the second-best hypothesis
for one of the elements in the (right-hand side of the) decomposition. Therefore, nested
local optimizations result in a top-down, exact n-best search through the generation for-
est, and matching the ?depth? of local decompositions to the maximum required ME
feature context effectively prevents exhaustive cross-multiplication of packed nodes.
The main function hypothesize-edge() in Figure 3 controls both the ?horizontal? and
?vertical? search, initializing the set of decompositions and pushing initial hypothe-
ses onto the local agenda when called on an edge for the first time (lines 11 ? 17).
Furthermore, the procedure retrieves the current next-best hypothesis from the agenda
(line 18), generates new hypotheses by advancing daughter indices (while skipping over
High Efficiency Realization for a Wide-Coverage Unification Grammar 173
1 procedure selectively-unpack-edge(edge , n) ?
2 results ? ? ?; i ? 0;
3 do
4 hypothesis ? hypothesize-edge(edge , i); i ? i + 1;
5 if (new ? instantiate-hypothesis(hypothesis)) then
6 n ? n ? 1; results ? results ? ?new?;
7 while (hypothesis and n ? 1)
8 return results;
9 procedure hypothesize-edge(edge , i) ?
10 if (edge.hypotheses[i]) return edge.hypotheses[i];
11 if (i = 0) then
12 for each (decomposition in decompose-edge(edge)) do
13 daughters ? ? ?; indices ? ? ?
14 for each (edge in decomposition.rhs) do
15 daughters ? daughters ? ?hypothesize-edge(edge, 0)?;
16 indices ? indices ? ?0?;
17 new-hypothesis(edge, decomposition, daughters, indices);
18 if (hypothesis ? edge.agenda.pop()) then
19 for each (indices in advance-indices(hypothesis.indices)) do
20 if (indices ? edge.indices) then continue
21 daughters ? ? ?;
22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do
23 daughter ? hypothesize-edge(edge, i);
24 if (not daughter) then
25 daughters ? ? ?; break
26 daughters ? daughters ? ?daughter?;
27 if (daughters) then new-hypothesis(edge, decomposition, daughters, indices)
28 edge.hypotheses[i] ? hypothesis;
29 return hypothesis;
30 procedure new-hypothesis(edge , decomposition , daughters , indices) ?
31 hypothesis ? new hypothesis(decomposition, daughters, indices);
32 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);
33 edge.indices ? edge.indices ? {indices};
Fig. 3. Selective unpacking procedure, enumerating the n best realizations for a top-level result
edge from the generation forest. An auxiliary function decompose-edge() performs local cross-
multiplication as shown in the examples in Figure 2. Another utility function not shown in pseudo-
code is advance-indices(), another ?driver? routine searching for alternate instantiations of daughter
edges, e.g. advance-indices(?0 2 1?) ? {?1 2 1? ?0 3 1? ?0 2 2?}. Finally, instantiate-hypothesis() is
the function that actually builds result trees, replaying the unifications of constructions from the
grammar (as identified by chart edges) with the feature structures of daughter constituents.
configurations seen earlier) and calling itself recursively for each new index (lines 19 ?
27), and, finally, arranges for the resulting hypothesis to be cached for later invocations
on the same edge and i values (line 28). Note that we only invoke instantiate-hypothesis()
on complete, top-level hypotheses, as the ME features of Toutanova and Manning [20]
can actually be evaluated prior to building each full feature structure. However, the
procedure could be adapted to perform instantiation of sub-hypotheses within each lo-
cal search, should additional features require it. For better efficiency, our instantiate-
hypothesis() routine already uses dynamic programming for intermediate results.
4 Evaluation and Summary
Below we present an empirical evaluation of each of the refinements discussed in Sec-
tions 3.2 through 3.5. Using the LinGO ERG and its ?hike? treebank ? a 330-sentence
174 J. Carroll and S. Oepen
Table 1. Realization efficiency for various instantiations of our algorithm. The table is broken
down by average ambiguity rates, the first two columns showing the number of items per aggre-
gate and average string length. Subsequent columns show relative cpu time of one- and two-phase
realization with or without packing and filtering, shown as a relative multiplier of the baseline
performance in the 1p+f+ column. The rightmost column is for selective unpacking of up to 10
trees from the forest produced by the baseline configuration, again as a factor of the baseline. (The
quality of the selected trees depends on the statistical model and the degree of overgeneration in
the grammar, and is a completely separate issue which we do not address in this paper).
items length 1p?f? 2p?f? 1p?f+ 1p+f? 2p+f? 1p+f+ n=10Aggregate  ? ? ? ? ? ? s ?
500 < trees 9 23.9 31.76 20.95 11.98 9.49 3.69 31.49 0.33
100 < trees ? 500 22 17.4 53.95 36.80 3.80 8.70 4.66 5.61 0.42
50 < trees ? 100 21 18.1 51.53 13.12 1.79 8.09 2.81 3.74 0.62
10 < trees ? 50 80 14.6 35.50 18.55 1.82 6.38 3.67 1.77 0.89
0 ? trees ? 10 185 10.5 9.62 6.83 1.19 6.86 3.62 0.58 0.95
Overall 317 12.9 35.03 20.22 5.97 8.21 3.74 2.32 0.58
Coverage 95% 97% 99% 99% 100% 100% 100%
collection of instructional text taken from Norwegian tourism brochures ? we bench-
marked various generator configurations, starting from the ?gold? standard MRS formula
recorded for each utterance in the treebank. At 12.8 words, average sentence length in
the original ?hike? corpus is almost exactly what we see as the average length of all
paraphrases obtained from the generator (see Table 1); from the available reference
treebanks for the ERG, ?hike? appears to be among the more complex data sets.
Table 1 summarizes relative generator efficiency for various configurations, where
we use the best-performing exhaustive procedure 1p+f+ (one-phase generation with
packing and index accessibility filtering) as a baseline. The configuration 1p?f? (one-
phase, no packing or filtering) corresponds to the basic procedure suggested by Kay [8],
while 2p?f? (two-phase processing of modifiers without packing and filtering) imple-
ments the algorithm presented by Carroll et al [12]. Combining packing and filter-
ing clearly outperforms both these earlier configurations, i.e. giving an up to 50 times
speed-up for inputs with large numbers of realizations. Additional columns contrast the
various techniques in isolation, thus allowing an assessment of the individual strengths
of our proposals. On low- to medium-ambiguity items, for example, filtering gives rise
to a bigger improvement than packing, but packing appears to flatten the curve more.
Both with and without packing, filtering improves significantly over the Carroll et al
two-phase approach to intersective modifiers (i.e. comparing columns 2p?f? and 2p+f?
to 1p?f+ and 1p+f+, respectively), thus confirming the increased generality of our solu-
tion to the modification problem. Finally, the benefits of packing and filtering combine
more than merely multiplicatively: compared to 1p?f?, just filtering gives a speed-up of
5.9, and just packing a speed-up of 4.3. At 25, the product of these factors is well below
the overall reduction of 35 that we obtain from the combination of both techniques.
While the rightmost column in Table 1 already indicates that 10-best selective un-
packing further improves generator performance by close to a factor of two, Figure 4
breaks down generation time with respect to forest creation vs. unpacking time. When
plotted against increasing input complexity (in terms of the ?size? of the input MRS),
forest creation appears to be a low-order polynomial (or better), whereas exhaustive
High Efficiency Realization for a Wide-Coverage Unification Grammar 175
0 5 10 15 20 25 30 35
Input Complexity (Number of EPs in MRS)
0
2
4
6
8
10
12
14
s
(generated by [incr tsdb()] at 15-apr-2005 (00:55 h))
? packed forest creation
? selective unpacking
? exhaustive unpacking
Fig. 4. Break-down of generation times (in seconds) according to realization phases and input
complexity (approximated in the number of EPs in the original MRS used for generation). The
three curves are, from ?bottom? to ?top?, the average time for constructing the packed generation
forest, selective unpacking time (using n = 10), and exhaustive unpacking time. Note that both
unpacking times are shown as increments on top of the forest creation time.
unpacking (necessarily) results in an exponential explosion of generation time: with
more than 25 EPs, it clearly dominates total processing time. Selective unpacking, in
contrast, appears only mildly sensitive to input complexity and even on complex inputs
adds no more than a minor cost to total generation time. Thus, we obtain an over-
all observed run-time performance of our wide-coverage generator that is bounded (at
least) polynomially. Practical generation times using the LinGO ERG average below or
around one second for outputs of fifteen words in length, i.e. time comparable to human
production.
References
1. Flickinger, D.: On building a more efficient grammar by exploiting types. Natural Language
Engineering 6 (1) (2000) 15 ? 28
2. Butt, M., Dyvik, H., King, T.H., Masuichi, H., Rohrer, C.: The Parallel Grammar project.
In: Proceedings of the COLING Workshop on Grammar Engineering and Evaluation, Taipei,
Taiwan (2002) 1 ? 7
3. White, M.: Reining in CCG chart realization. In: Proceedings of the 3rd International Con-
ference on Natural Language Generation, Hampshire, UK (2004)
4. Moore, J., Foster, M.E., Lemon, O., White, M.: Generating tailored, comparative descriptions
in spoken dialogue. In: Proceedings of the 17th International FLAIRS Conference, Miami
Beach, FL (2004)
5. Oepen, S., Dyvik, H., L?nning, J.T., Velldal, E., Beermann, D., Carroll, J., Flickinger, D.,
Hellan, L., Johannessen, J.B., Meurer, P., Nordga?rd, T., Rose?n, V.: Som a? kapp-ete med
trollet? Towards MRS-based Norwegian ? English Machine Translation. In: Proceedings
of the 10th International Conference on Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD (2004)
6. Whitelock, P.: Shake-and-bake translation. In: Proceedings of the 14th International Confer-
ence on Computational Linguistics, Nantes, France (1992) 610 ? 616
7. Phillips, J.: Generation of text from logical formulae. Machine Translation 8 (1993) 209 ?
235
176 J. Carroll and S. Oepen
8. Kay, M.: Chart generation. In: Proceedings of the 34th Meeting of the Association for
Computational Linguistics, Santa Cruz, CA (1996) 200 ? 204
9. Gardent, C., Thater, S.: Generating with a grammar based on tree descriptions. A constraint-
based approach. In: Proceedings of the 39th Meeting of the Association for Computational
Linguistics, Toulouse, France (2001)
10. Shieber, S., van Noord, G., Pereira, F., Moore, R.: Semantic head-driven generation. Com-
putational Linguistics 16 (1990) 30 ? 43
11. Moore, R.: A complete, efficient sentence-realization algorithm for unification grammar. In:
Proceedings of the 2nd International Natural Language Generation Conference, Harriman,
NY (2002) 41 ? 48
12. Carroll, J., Copestake, A., Flickinger, D., Poznanski, V.: An efficient chart generator for
(semi-)lexicalist grammars. In: Proceedings of the 7th European Workshop on Natural Lan-
guage Generation, Toulouse, France (1999) 86 ? 95
13. Copestake, A., Flickinger, D., Sag, I., Pollard, C.: Minimal Recursion Semantics. An intro-
duction. (1999)
14. Kiefer, B., Krieger, H.U., Carroll, J., Malouf, R.: A bag of useful techniques for efficient and
robust parsing. In: Proceedings of the 37th Meeting of the Association for Computational
Linguistics, College Park, MD (1999) 473 ? 480
15. Billot, S., Lang, B.: The structure of shared forests in ambiguous parsing. In: Proceedings of
the 27th Meeting of the Association for Computational Linguistics, Vancouver, BC (1989)
143 ? 151
16. Oepen, S., Carroll, J.: Ambiguity packing in constraint-based parsing. Practical results. In:
Proceedings of the 1st Conference of the North American Chapter of the ACL, Seattle, WA
(2000) 162 ? 169
17. Shieber, S.: Using restriction to extend parsing algorithms for complex feature-based for-
malisms. In: Proceedings of the 23rd Meeting of the Association for Computational Linguis-
tics, Chicago, IL (1985) 145 ? 152
18. Oepen, S., Carroll, J.: Performance profiling for parser engineering. Natural Language
Engineering 6 (1) (2000) 81 ? 97
19. Velldall, E., Oepen, S., Flickinger, D.: Paraphrasing treebanks for stochastic realization rank-
ing. In: Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories, Tu?bingen,
Germany (2004)
20. Toutanova, K., Manning, C.: Feature selection for a rich HPSG grammar using decision
trees. In: Proceedings of the 6th Conference on Natural Language Learning, Taipei, Taiwan
(2002)
21. Malouf, R., van Noord, G.: Wide coverage parsing with stochastic attribute value grammars.
In: Proceedings of the IJCNLP workshop Beyond Shallow Analysis, Hainan, China (2004)
22. Geman, S., Johnson, M.: Dynamic programming for parsing and estimation of stochastic
unification-based grammars. In: Proceedings of the 40th Meeting of the Association for
Computational Linguistics, Philadelphia, PA (2002)
23. Miyao, Y., Tsujii, J.: Maximum entropy estimation for feature forests. In: Proceedings of the
Human Language Technology Conference, San Diego, CA (2002)
Proceedings of NAACL-HLT 2013, pages 868?877,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Open Information Extraction with Tree Kernels
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel and Denilson Barbosa
Department of Computing Science
University of Alberta
{yx2,miyoung2,kfjquinn,goebel,denilson}@cs.ualberta.ca
Abstract
Traditional relation extraction seeks to iden-
tify pre-specified semantic relations within
natural language text, while open Information
Extraction (Open IE) takes a more general ap-
proach, and looks for a variety of relations
without restriction to a fixed relation set. With
this generalization comes the question, what
is a relation? For example, should the more
general task be restricted to relations medi-
ated by verbs, nouns, or both? To help answer
this question, we propose two levels of sub-
tasks for Open IE. One task is to determine if
a sentence potentially contains a relation be-
tween two entities? The other task looks to
confirm explicit relation words for two enti-
ties. We propose multiple SVM models with
dependency tree kernels for both tasks. For
explicit relation extraction, our system can ex-
tract both noun and verb relations. Our results
on three datasets show that our system is su-
perior when compared to state-of-the-art sys-
tems like REVERB and OLLIE for both tasks.
For example, in some experiments our system
achieves 33% improvement on nominal rela-
tion extraction over OLLIE. In addition we
propose an unsupervised rule-based approach
which can serve as a strong baseline for Open
IE systems.
1 Introduction
Relation Extraction (RE) systems are designed to
discover various semantic relations (e.g. <Obama,
president, the United States>) from natural lan-
guage text. Traditional RE systems extract spe-
cific relations for prespecified name-entity types
(Bunescu and Mooney, 2005; Chan and Dan, 2011;
Zhou and Zhu, 2011). To train such systems, ev-
ery relation needs manually annotated training ex-
amples, which supports limited scope and is diffi-
cult to extend. For this reason, Banko et al (2007)
proposed Open Information Extraction (Open IE),
whose goal is to extract general relations for two en-
tities. The idea is to avoid the need for specific train-
ing examples, and to extract a diverse range of rela-
tions. This generalized form has received significant
attention, e.g., (Banko et al, 2007; Akbik, 2009; Wu
and Weld, 2010; Fader et al, 2011; Mausam et al,
2012).
Because Open IE is not guided by or not restricted
to a prespecified list of relations, the immediate chal-
lenge is determining about what counts as a relation?
Most recent Open IE systems have targeted verbal
relations (Banko et al, 2007; Mausam et al, 2012),
claiming that these are the majority. However, Chan
and Dan (2011) show that only 20% of relations in
the ACE programs Relation Detection and Charac-
terization (RDC) are verbal. Our manually extracted
relation triple set from the Penn Treebank shows that
there are more nominal relations than verbal ones,
3 to 2. This difference arises because of the ambi-
guity of what constitutes a relation in Open IE. It
is often difficult even for humans to agree on what
constitutes a relation, and which words in the sen-
tence establish a relation between a pair of entities.
For example, in the sentence ?Olivetti broke Cocom
rules? is there a relation between Olivetti and Co-
com? This ambiguity in the problem definition leads
to significant challenges and confusion when eval-
uating and comparing the performance of different
methods and systems. An example are the results
in Fader et al (2011) and Mausam et al (2012). In
Fader et al (2011), REVERB ?is reported? as su-
868
perior to WOEparse, a system proposed in Wu and
Weld (2010); while in Mausam et al (2012), it is
reported the opposite.
To better answer the question, what counts as a
relation? we propose two tasks for Open IE. The
first task seeks to determine whether there is a re-
lation between two entities (called ?Binary task?).
The other is to confirm whether the relation words
extracted for the two entities are appropriate (the
?Triple task?). The Binary task does not restrict re-
lation word forms, whether they are mediated by
nouns, verbs, prepositions, or even implicit rela-
tions. The Triple task requires an abstract repre-
sentation of relation word forms, which we develop
here. We assume that relation words are nouns or
verbs; in our data, these two types comprise 71% of
explicit relations.
We adapt an SVM dependency tree kernel model
(Moschitti, 2006) for both tasks. The input to our
tasks is a dependency parse, created by Stanford
Parser. Selecting relevant features from a parse tree
for semantic tasks is difficult. SVM tree kernels
avoid extracting explicit features from parse trees
by calculating the inner product of the two trees.
For the Binary task, our dependency path is the path
between two entities. For the Triple task, the path
is among entities and relation words (i.e. relation
triples). Tree kernels have been used in traditional
RE and have helped achieve state of the art perfor-
mance (Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Wang, 2008; Nguyen et al, 2009;
Zhou and Zhu, 2011). But one challenge of using
tree kernels on Open IE is that the lexicon of re-
lations is much larger than those of traditional RE,
making it difficult to include the lexical information
as features. Here we proposed an unlexicalized tree
structure for Open IE. As far as we know, this is the
first time an SVM tree kernel has been applied in
Open IE. Experimental results on multiple datasets
show our system outperforms state-of-the-art sys-
tems REVERB and OLLIE. Typically an Open IE
system is tested on one dataset. However, because
the definition of relation is ambiguous, we believe
that is necessary to test with multiple datasets.
In addition to the supervised model, we also pro-
pose an unsupervised model which relies on several
heuristic rules. Results with this approach show that
this simple unsupervised model provides a robust
strong baseline for other approaches.
In summary, our main contributions are:
? Use SVM tree kernels for Open IE. Our sys-
tem is robust comparing with other Open IE
systems, achieving superior scores in two test
sets and comparative scores in another set.
? Extend beyond verbal relations, which are
prevalent in current systems. Analyze implicit
relation problem in Open IE, which is ignored
by other work.
? Propose an unsupervised model for Open IE,
which can be a strong baseline for other ap-
proaches.
The rest of this paper is organized as follows. Sec-
tion 2 provides the problem description and system
structure, before summarizing previous work in Sec-
tion 3. Section 4 defines our representation of rela-
tion word patterns crucial to our task two, and Sec-
tion 5 describes tree kernels for SVM. Section 6 de-
scribes the unsupervised model, and Section 7 ex-
plains our experiment design and results. Section 8
concludes with a summary, and anticipation of fu-
ture work.
2 Problem Definition and System
Structure
The common definition of the Open IE task is a
function from a sentence, s, to a set of triples,
{< E1, R,E2 >}, where E1 and E2 are entities
(noun phrases) and R is a textual fragment indicat-
ing a semantic relation between the two entities. Our
?Triple task? is within this definition. However it is
often difficult to determine which textual fragments
to extract. In addition, semantic relations can be im-
plicit, e.g., consider the located in relation in the sen-
tence fragment ?Washington, US.? To illustrate how
much information is lost when restricting the rela-
tion forms, we add another task (the ?Binary task?),
determining if there is a relation between the two en-
tities. It is a function from s, to a set of binary rela-
tions over entities, {< E1, E2 >}. This binary task
is designed to overcome the disadvantage of current
Open IE systems, which suffer because of restricting
the relation form, e.g., to only verbs, or only nouns.
The two tasks are independent to each other.
869
	

	

		

		
		Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 39?47,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transliteration Generation and Mining with Limited Training Resources
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma, Aditya Bhargava,
Qing Dou, Mi-Young Kim, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,dwyer,bergsma,abhargava,qdou,miyoung2,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL+: an online dis-
criminative sequence prediction model
based on many-to-many alignments,
which is further augmented by the in-
corporation of joint n-gram features.
Experimental results show improvement
over the results achieved by DIRECTL in
2009. We also explore a number of diverse
resource-free and language-independent
approaches to transliteration mining,
which range from simple to sophisticated.
1 Introduction
Many out-of-vocabulary words in statistical ma-
chine translation and cross-language information
retrieval are named entities. If the languages in
question use different writing scripts, such names
must be transliterated. Transliteration can be de-
fined as the conversion of a word from one writ-
ing script to another, which is usually based on the
phonetics of the original word.
DIRECTL+ is our current approach to name
transliteration which is an extension of the DI-
RECTL system (Jiampojamarn et al, 2009). We
augmented the feature set with joint n-gram fea-
tures which allow the discriminative model to uti-
lize long dependencies of joint information of
source and target substrings (Jiampojamarn et al,
2010). Experimental results suggest an improve-
ment over the results achieved by DIRECTL in
2009.
Transliteration mining aims at automatically
obtaining bilingual lists of names written in differ-
ent scripts. We explore a number of different ap-
proaches to transliteration mining in the context of
the NEWS 2010 Shared Task.1 The sole resource
that is provided for each language pair is a ?seed?
1http://translit.i2r.a-star.edu.sg/
news2010
dataset that contains 1K transliteration word pairs.
The objective is then to mine transliteration pairs
from a collection of Wikipedia titles/topics that are
given in both languages.
We explore a number of diverse resource-free
and language-independent approaches to translit-
eration mining. One approach is to bootstrap the
seed data by generating pseudo-negative exam-
ples, which are combined with the positives to
form a dataset that can be used to train a clas-
sifier. We are particularly interested in achiev-
ing good performance without utilizing language-
specific resources, so that the same approach can
be applied with minimal or no modifications to an
array of diverse language pairs.
This paper is divided in two main parts that cor-
respond to the two tasks of transliteration genera-
tion and transliteration mining.
2 Transliteration generation
The structure of this section is as follows. In Sec-
tion 2.1, we describe the pre-processing steps that
were applied to all datasets. Section 2.2 reviews
two methods for aligning the source and target
symbols in the training data. We provide details
on the DIRECTL+ systems in Section 2.3. In Sec-
tion 2.4, we discuss extensions of DIRECTL+ that
incorporate language-specific information. Sec-
tion 2.5 summarizes our results.
2.1 Pre-processing
For all generation tasks, we pre-process the pro-
vided data as follows. First, we convert all char-
acters in the source word to lower case. Then,
we remove non-alphabetic characters unless they
appear in both the source and target words. We
normalize whitespace that surrounds a comma, so
that there are no spaces before the comma and ex-
actly one space following the comma. Finally, we
separate multi-word titles into single words, using
whitespace as the separator. We assume a mono-
39
tonic matching and ignore the titles that have a dif-
ferent number of words on both sides.
We observed that in the ArAe task there are
cases where an extra space is added to the target
when transliterating from Arabic names to their
English equivalents; e.g., ?Al Riyad?, ?El Sayed?,
etc. In order to prevent the pre-processing from
removing too many title pairs, we allow non-equal
matching if the source title is a single word.
For the English-Chinese (EnCh) task, we con-
vert the English letter ?x? to ?ks? to facilitate bet-
ter matching with its Chinese targets.
During testing, we pre-process test data in the
same manner, except that we do not remove non-
alphabetic characters. After the pre-processing
steps, our system proposes 10-best lists for single
word titles in the test data. For multi-word titles,
we construct 10-best lists by ranking the combina-
tion scores of single words that make up the test
titles.
2.2 Alignment
In the transliteration tasks, training data consist
of pairs of names written in source and target
scripts without explicit character-level alignment.
In our experiments, we applied two different algo-
rithms to automatically generate alignments in the
training data. The generated alignments provide
hypotheses of substring mappings in the training
data. Given aligned training data, a transliteration
model is trained to generate names in the target
language given names in the source language.
The M2M-aligner (Jiampojamarn et al, 2007)
is based on the expectation maximization (EM)
algorithm. It allows us to create alignments be-
tween substrings of various lengths. We opti-
mized the maximum substring sizes for the source
and target based on the performance of the end
task on the development sets. We allowed empty
strings (nulls) only on the target side. We used the
M2M-aligner for all alignment tasks, except for
English-Pinyin alignment. The source code of the
M2M-aligner is publicly available.2
An alternative alignment algorithm is based on
the phonetic similarity of graphemes. The key idea
of this approach is to represent each grapheme by a
phoneme or a sequence of phonemes that is likely
to be represented by the grapheme. The sequences
of phonemes on the source side and the target
side can then be aligned on the basis of phonetic
2http://code.google.com/p/m2m-aligner/
b a r c - l a y
| | | | | | | |
b a - k u r - i
Figure 1: An alignment example.
similarity between phonemes. The main advan-
tage of the phonetic alignment is that it requires
no training data. We use the ALINE phonetic
aligner (Kondrak, 2000), which aligns two strings
of phonemes. The example in Figure 1 shows
the alignment of the word Barclay to its Katakana
transliteration ba-ku-ri. The one-to-one alignment
can then be converted to a many-to-many align-
ment by grouping the Japanese phonemes that cor-
respond to individual Katakana symbols.
2.3 DIRECTL+
We refer to our present approach to transliteration
as DIRECTL+. It is an extension of our DIRECTL
system (Jiampojamarn et al, 2009). It includes ad-
ditional ?joint n-gram? features that allow the dis-
criminative model to correlate longer source and
target substrings. The additional features allow
our discriminative model to train on information
that is present in generative joint n-gram models,
and additionally train on rich source-side context,
transition, and linear-chain features that have been
demonstrated to be important in the transliteration
task (Jiampojamarn et al, 2010).
Our model is based on an online discriminative
framework. At each training iteration, the model
generates an m-best list for each given source
name based on the current feature weights. The
feature weights are updated according to the gold-
standard answers and the generated m-best an-
swer lists using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003). This
training process iterates over the training examples
until the model converges. For m-best and n-gram
parameters, we set m = 10 and n = 6 for all lan-
guage pairs. These parameters as well as others
were optimized on the development sets.
We trained our models directly on the data
that were provided by the organizers, with three
exceptions. In order to improve performance,
we gave special treatment to English-Korean
(EnKo), English-Chinese (EnCh), and English-
Hindi (EnHi). These special cases are described
in the next section.
40
2.4 Beyond DIRECTL+
2.4.1 Korean Jaso
A Korean syllable can be decomposed into two
or three components called Jaso: an initial con-
sonant, a middle vowel, and optionally a final con-
sonant. The Korean generation for EnKo involves
the following three steps: (1) English-to-Jaso gen-
eration, (2) correction of illegal Jaso sequences,
and (3) Jaso-to-Korean conversion.
In order to correct illegal Jaso sequences that
cannot be combined into Korean syllables in step
2, we consider both vowel and consonant rules.
A Korean vowel can be either a simple vowel or
a complex vowel that combines two simple vow-
els. We can use this information in order to replace
double vowels with one complex vowel. We also
use the silent consonant o (i-eung) when we need
to insert a consonant between double vowels. A
Korean vowel - (eu) is most commonly inserted
between two English consonants in transliteration.
In order to resolve three consecutive consonants, it
can be placed into the most probable position ac-
cording to the probability distribution of the train-
ing data.
2.4.2 Japanese Katakana
In the Japanese Katakana generation task, we re-
place each Katakana symbol with one or two let-
ters using standard romanization tables. This has
the effect of expressing the target side in Latin let-
ters, which facilitates the alignment. DIRECTL+
is trained on the converted data to generate the tar-
get from the source. A post-processing program
then attempts to convert the generated letters back
into Katakana symbols. Sequences of letters that
cannot be converted into Katakana are removed
from the output m-best lists and replaced by lower
scoring sequences that pass the back-conversion
filter. Otherwise, there is usually a single valid
mapping because most Katakana symbols are rep-
resented by single vowels or a consonant-vowel
pair. The only apparent ambiguity involves the
letter n, which can either stand by itself or clus-
ter with the following vowel letter. We resolve the
ambiguity by always assuming the latter case un-
less the letter n occurs at the end of the word.
2.4.3 Chinese Pinyin
Following (Jiampojamarn et al, 2009), we experi-
mented with converting the original Chinese char-
acters to Pinyin as an intermediate representation.
Pinyin is the most commonly known romanization
system for Standard Mandarin and many free tools
are available for converting Chinese characters to
Pinyin. Its alphabet contains the same 26 letters
as English. Each Chinese character can be tran-
scribed phonetically into Pinyin. A small percent-
age of Chinese characters have multiple pronunci-
ations, and are thus represented by different Pinyin
sequences. For those characters, we manually se-
lected the pronunciations that are normally used
for names. This pre-processing step significantly
reduces the size of the target symbols: from 370
distinct Chinese characters to 26 Pinyin symbols.
This allows our system to produce better align-
ments.
We developed three models: (1) trained on the
original Chinese characters, (2) trained on Pinyin,
and (3) the model that incorporates the phonetic
alignment described in Section 2.2. The combi-
nation of the predictions of the different systems
was performed using the following simple algo-
rithm (Jiampojamarn et al, 2009). First, we rank
the individual systems according to their top-1 ac-
curacy on the development set. To obtain the top-
1 prediction for each input word, we use simple
voting, with ties broken according to the ranking
of the systems. We generalize this approach to
handle n-best lists by first ordering the candidate
transliterations according to the rank assigned by
each individual system, and then similarly break-
ing ties by voting and using the ranking of the sys-
tems.
2.4.4 Language identification for Hindi
Bhargava and Kondrak (2010) apply support vec-
tor machines (SVMs) to the task of identifying
the language of names. The intuition here is that
language information can inform transliteration.
Bhargava and Kondrak (2010) test this hypothe-
sis on the NEWS 2009 English-Hindi transliter-
ation data by training language identification on
data manually tagged as being of either Indian or
non-Indian origin. It was found that splitting the
data disjointly into two sets and training separate
transliteration models yields no performance in-
crease due to the decreased size of the data for the
models.
We adopt this approach for the NEWS 2010
task, but here we do not use disjoint splits. In-
stead, we use the SVMs to generate probabilities,
and then we apply a threshold to these probabili-
ties to generate two datasets. For example, if we
set the threshold to be 0.05, then we determine the
41
probabilities of a given name being of Indian ori-
gin (phi) and of being of non-Indian origin (pen).
If phi < 0.05 then the name is excluded from the
Indian set, and if pen < 0.05 then the name is
excluded from the non-Indian set. Using the two
obtained non-disjoint sets, we then train a translit-
eration model for each set using DIRECTL+.
Since the two sets are not disjoint, we must de-
cide how to combine the two results. Given that a
name occurs in both sets, and both models provide
a ranked list of possible targets for that name, we
obtain a combined ranking using a linear combi-
nation over the mean reciprocal ranks (MRRs) of
the two lists. The weights used are phi and pen so
that the more likely a name is considered to be of
Indian origin, the more strongly the result from the
Indian set is considered relative to the result from
the non-Indian set.
2.5 Evaluation
In the context of the NEWS 2010 Machine
Transliteration Shared Task we tested our sys-
tem on all twelve datasets: from English to Chi-
nese (EnCh), Thai (EnTh), Hindi (EnHi), Tamil
(EnTa), Bangla (EnBa), Kannada (EnKa), Ko-
rean Hangul (EnKo), Japanese Katakana (EnJa),
Japanese Kanji (JnJk); and, in the opposite di-
rection, to English from Arabic (ArAe), Chi-
nese (ChEn), and Thai (ThEn). For all datasets,
we trained transliteration models on the provided
training and development sets without additional
resources.
Table 1 shows our best results obtained on the
datasets in terms of top-1 accuracy and mean F-
score. We also include the rank in standard runs
ordered by top-1 word accuracy. The EnCh re-
sult presented in the table refers to the output of
the three-system combination, using the combi-
nation algorithm described in Section 2.4.3. The
respective results for the three component EnCh
systems were: 0.357, 0.360, and 0.363. The
EnJa result in the table refers the system described
in Section 2.4.2 that applied specific treatment
to Japanese Katakana. Based on our develop-
ment results, this specific treatment improves as
much as 2% top-1 accuracy over the language-
independent model. The EnHi system that in-
corporates language identification obtained ex-
actly the same top-1 accuracy as the language-
independent model. However, the EnKo system
with Jaso correction produced the top-1 accu-
Task top-1 F-score Rank
EnCh 0.363 0.707 2
ChEn 0.137 0.740 1
EnTh 0.378 0.866 2
ThEn 0.352 0.861 2
EnHi 0.456 0.884 1
EnTa 0.390 0.891 2
EnKa 0.341 0.867 2
EnJa 0.398 0.791 1
EnKo 0.554 0.770 1
JnJk 0.126 0.426 1
ArAe 0.464 0.924 1
EnBa 0.395 0.877 2
Table 1: Transliteration generation results
racy of 0.554, which is a significant improvement
over 0.387 achieved by the language-independent
model.
3 Transliteration mining
This section is structured as follows. In Sec-
tion 3.1, we describe the method of extracting
transliteration candidates that serves as the input
to the subsequently presented mining approaches.
Two techniques for generating negative exam-
ples are discussed in Section 3.2. Our language-
independent approaches to transliteration mining
are described in Section 3.3, and a technique for
mining English-Chinese pairs is proposed in Sec-
tion 3.4. In Section 3.5, we address the issue of
overlapping predictions. Finally, Section 3.6 and
Section 3.7 summarize our results.
3.1 Extracting transliteration candidates
We cast the transliteration mining task as a bi-
nary classification problem. That is, given a word
in the source language and a word in the target
language, a classifier predicts whether or not the
pair constitutes a valid transliteration. As a pre-
processing step, we extract candidate translitera-
tions from the pairs of Wikipedia titles. Word seg-
mentation is performed based on sequences of one
or more spaces and/or punctuation symbols, which
include hyphens, underscores, brackets, and sev-
eral other non-alphanumeric characters. Apostro-
phes and single quotes are not used for segmenta-
tion (and therefore remain in a given word); how-
ever, all single quote-like characters are converted
into a generic apostrophe. Once an English ti-
tle and its target language counterpart have been
42
segmented into words, we form the candidate set
for this title as the cross product of the two sets
of words after discarding any words that contain
fewer than two characters.
After the candidates have been extracted, indi-
vidual words are flagged for certain attributes that
may be used by our supervised learner as addi-
tional features. Alternatively, the flags may serve
as criteria for filtering the list of candidate pairs
prior to classification. We identify words that are
capitalized, consist of all lowercase (or all capital)
letters, and/or contain one or more digits. We also
attempt to encode each word in the target language
as an ASCII string, and flag that word if the opera-
tion succeeds. This can be used to filter out words
that are written in English on both the source and
target side, which are not transliterations by defi-
nition.
3.2 Generating negative training examples
The main issue with applying a supervised learn-
ing approach to the NEWS 2010 Shared Task is
that annotated task-specific data is not available
to train the system. However, the seed pairs do
provide example transliterations, and these can be
used as positive training examples. The remaining
issue is how to select the negative examples.
We adopt two approaches for selecting nega-
tives. First, we generate all possible source-target
pairs in the seed data, and take as negatives those
pairs which are not transliterations but have a
longest common subsequence ratio (LCSR) above
0.58; this mirrors the approach used by Bergsma
and Kondrak (2007). The method assumes that
the source and target words are written in the same
script (e.g., the foreign word has been romanized).
A second possibility is to generate all seed pair-
ings as above, but then randomly select negative
examples, thus mirroring the approach in Klemen-
tiev and Roth (2006). In this case, the source and
target scripts do not need to be the same. Com-
pared with the LCSR technique, random sampling
in this manner has the potential to produce nega-
tive examples that are very ?easy? (i.e., clearly not
transliterations), and which may be of limited util-
ity when training a classifier. On the other hand, at
test time, the set of candidates extracted from the
Wikipedia data will include pairs that have very
low LCSR scores; hence, it can be argued that dis-
similar pairs should also appear as negative exam-
ples in the training set.
3.3 Language-independent approaches
In this section, we describe methods for transliter-
ation mining that can, in principle, be applied to a
wide variety of language pairs without additional
modification. For the purposes of the Shared Task,
however, we convert all source (English) words to
ASCII by removing diacritics and making appro-
priate substitutions for foreign letters. This is done
to mitigate sparsity in the relatively small seed sets
when training our classifiers.
3.3.1 Alignment-derived romanization
We developed a simple method of performing ro-
manization of foreign scripts. Initially, the seed set
of transliterations is aligned using the one-to-one
option of the M2M-aligner approach (Jiampoja-
marn et al, 2007). We allow nulls on both the
source and target sides. The resulting alignment
model contains pairs of Latin letters and foreign
script symbols (graphemes) sorted by their con-
ditional probability. Then, for each grapheme,
we select a letter (or a null symbol) that has the
highest conditional probability. The process pro-
duces an approximate romanization table that can
be obtained without any knowledge of the target
script. This method of romanization was used by
all methods described in the remainder of Sec-
tion 3.3.
3.3.2 Normalized edit distance
Normalized edit distance (NED) is a measure of
the similarity of two strings. We define a uniform
edit cost for each of the three operations: substitu-
tion, insertion, and deletion. NED is computed by
dividing the minimum edit distance by the length
of the longer string, and subtracting the resulting
fraction from 1. Thus, the extreme values of NED
are 1 for identical strings, and 0 for strings that
have no characters in common.
Our baseline method, NED+ is simply the NED
measure augmented with filtering of the candidate
pairs described in Section 3.1. In order to address
the issue of morphological variants, we also fil-
ter out the pairs in which the English word ends
in a consonant and the foreign word ends with a
vowel. With no development set provided, we set
the similarity thresholds for individual languages
on the basis of the average word length in the seed
sets. The values were 0.38, 0.48, 0.52, and 0.58
for Hindi, Arabic, Tamil, and Russian, respec-
tively, with the last number taken from Bergsma
and Kondrak (2007).
43
3.3.3 Alignment-based string similarity
NED selects transliteration candidates when the
romanized foreign strings have high character
overlap with their English counterparts. The mea-
sure is independent of the language pair. This
is suboptimal for several reasons. First of all,
phonetically unrelated words can share many in-
cidental character matches. For example, the
French word ?recettes? and the English word
?proceeds? share the letters r,c,e,e,s as a com-
mon subsequence, but the words are phonetically
unrelated. Secondly, many reliable, recurrent,
language-specific substring matches are prevalent
in true transliterations. These pairings may or may
not involve matching characters. NED can not
learn or adapt to these language-specific patterns.
In light of these drawbacks, researchers have
proposed string similarity measures that can learn
from provided example pairs and adapt the simi-
larity function to a specific task (Ristad and Yiani-
los, 1998; Bilenko and Mooney, 2003; McCallum
et al, 2005; Klementiev and Roth, 2006).
One particularly successful approach is by
Bergsma and Kondrak (2007), who use discrim-
inative learning with an improved feature repre-
sentation. The features are substring pairs that are
consistent with a character-level alignment of the
two strings. This approach strongly improved per-
formance on cognate identification, while varia-
tions of it have also proven successful in transliter-
ation discovery (Goldwasser and Roth, 2008). We
therefore adopted this approach for the translitera-
tion mining task.
We produce negative training examples using
the LCSR threshold approach described in Sec-
tion 3.2. For features, we extract from the aligned
word pairs all substring pairs up to a maximum
length of three. We also append characters mark-
ing the beginning and end of words, as described
in Bergsma and Kondrak (2007). For our clas-
sifier, we use a Support Vector Machine (SVM)
training with the very efficient LIBLINEAR pack-
age (Fan et al, 2008). We optimize the SVM?s
regularization parameter using 10-fold cross vali-
dation on the generated training data. At test time,
we apply our classifier to all the transliteration
candidates extracted from the Wikipedia titles,
generating transliteration pairs whenever there is
a positive classification.
3.3.4 String kernel classifier
The alignment-based classifier described in the
preceding section is limited to using substring fea-
tures that are up to (roughly) three or four letters
in length, due to the combinatorial explosion in the
number of unique features as the substring length
increases. It is natural to ask whether longer sub-
strings can be utilized to learn a more accurate pre-
dictor.
This question inspired the development of a sec-
ond SVM-based learner that uses a string kernel,
and therefore does not have to explicitly repre-
sent feature vectors. Our kernel is a standard n-
gram (or spectrum) kernel that implicitly embeds
a string in a feature space that has one co-ordinate
for each unique n-gram (see, e.g., (Shawe-Taylor
and Cristianini, 2004)). Let us denote the alphabet
over input strings as A. Given two input strings x
and x?, this kernel function computes:
k(x, x?) =
?
s?An
#(s, x)#(s, x?)
where s is an n-gram and #(a, b) counts the num-
ber of times a appears as a substring of b.
An extension of the n-gram kernel that we em-
ploy here is to consider all n-grams of length
1 ? n ? k, and weight each n-gram as a func-
tion of its length. In particular, we specify a value
? and weight each n-gram by a factor of ?n. We
implemented this kernel in the LIBSVM software
package (Chang and Lin, 2001). Optimal values
for k, ?, and the SVM?s regularization parame-
ter were estimated for each dataset using 5-fold
cross-validation. The values of (k, ?) that we ul-
timately used were: EnAr (3, 0.8), EnHi (8, 0.8),
EnRu (5, 1.2), and EnTa (5, 1.0).
Our input string representation for a candidate
pair is formed by first aligning the source and tar-
get words using M2M-aligner (Jiampojamarn et
al., 2007). Specifically, an alignment model is
trained on the seed examples, which are subse-
quently aligned and used as positive training ex-
amples. We then generate 20K negative examples
by random sampling (cf. Section 3.2) and apply
the alignment model to this set. Not all of these
20K word pairs will necessarily be aligned; we
randomly select 10K of the successfully aligned
pairs to use as negative examples in the training
set.
Each aligned pair is converted into an ?align-
ment string? by placing the letters that appear in
44
Word pair zubtsov z u b ov
Aligned pair z|u|b|t|s|o|v| z|u|b|| |o|v|
Align?t string zz|uu|bb|t|s |oo|vv
Table 2: An example showing how an alignment
string (the input representation for the string ker-
nel) is created from a word pair.
the same position in the source and target next to
one another, while retaining the separator charac-
ters (see Table 2). We also appended beginning
and end of word markers. Note that no romaniza-
tion of the target words is necessary for this pro-
cedure.
At test time, we apply the alignment model to
the candidate word pairs that have been extracted
from the train data, and retain all the successfully
aligned pairs. Here, M2M-aligner also acts as a
filter, since we cannot form alignment strings from
unaligned pairs ? these yield negative predictions
by default. We also filter out pairs that met any of
the following conditions: 1) the English word con-
sists of all all capital or lowercase letters, 2) the
target word can be converted to ASCII (cf. Sec-
tion 3.1), or 3) either word contains a digit.
3.3.5 Generation-based approach
In the mining tasks, we are interested in whether a
candidate pair (x, y) is a transliteration pair. One
approach is to determine if the generated translit-
erations of a source word y? = ?(x) and a target
word x? = ?(y) are similar to the given candi-
date pair. We applied DIRECTL+ to the mining
tasks by training transliteration generation models
on the provided seed data in forward and back-
ward transliteration directions, creating ?(x) and
?(y) models. We now define a transliteration
score function in Eq. 1. N(x?, x) is the normal-
ized edit distance between string x? and x, and w1
and w2 are combination weights to favor forward
and backward transliteration models.
S(x, y) = w1 ? N(y?, y) + w2 ? N(x?, x)w1 + w2
(1)
A candidate pair is considered a transliteration
pair if its S(x, y) > ? . Ideally, we would like
to optimize these parameters, ?, w1, w2 based on
a development set for each language pair. Unfor-
tunately, no development sets were provided for
the Shared Task. Therefore, following Bergsma
and Kondrak (2007), we adopt the threshold of
? = 0.58. We experimented with three sets of val-
ues for w1 and w2: (1, 0), (0.5, 0.5), and (0, 1).
Our final predictions were made using w0 = 0
and w1 = 1, which appeared to produce the best
results. Thus, only the backward transliteration
model was ultimately employed.
3.4 English-Chinese string matching
Due to the fact that names transliterated into Chi-
nese consist of multiple Chinese characters and
that the Chinese text provided in this shared task
is not segmented, we have to adopt a different ap-
proach to the English-Chinese mining task (Unlike
many other languages, there are no clear bound-
aries between Chinese words). We first train a
generation model using the seed data and then ap-
ply a greedy string matching algorithm to extract
transliteration pairs.
The generation model is built using the discrim-
inative training framework described in (Jiampoja-
marn et al, 2008). Two models are learned: one
is trained using English and Chinese characters,
while the other is trained on English and Pinyin (a
standard phonetic representation of Chinese char-
acters). In order to mine transliteration pairs from
Wikipedia titles, we first use the generation model
to produce transliterations for each English token
on the source side as both Chinese characters and
Pinyin. The generated Chinese characters are ul-
timately converted to Pinyin during string match-
ing. We also convert all the Chinese characters on
the target side to their Pinyin representations when
performing string matching.
The transliteration pairs are then mined by com-
bining two different strategies. First of all, we ob-
serve that most of the titles that contain a separa-
tion symbol ? ? ? on the target side are translit-
erations. In this case, the number of tokens on
both sides is often equal. Therefore, the mining
task can be formulated as a matching problem.
We use a competitive linking approach (Melamed,
2000) to find the best match. First, we select
links between all possible pairs if similarity of
strings on both sides is above a threshold (0.6 ?
length(Pinyin)). We then greedily extract the
pairs with highest similarity until the number of
unextracted segments on either side becomes zero.
The problem becomes harder when there is no
indication of word segmentation for Chinese. In-
stead of trying to segment the Chinese characters
first, we use an incremental string matching strat-
45
egy. For each token on the source side, the algo-
rithm calculates its similarity with all possible n-
grams (2 ? n ? L) on the target side, where L
is the length of the Chinese title (i.e., the number
of characters). If the similarity score of n-gram
with the highest similarity surpasses a threshold
(0.5 ? length(Pinyin)), the n-gram sequence is
proposed as a possible transliteration for the cur-
rent source token.
3.5 Resolving overlapping predictions
Given a set of candidate word pairs that have been
extracted from a given Wikipedia title according to
the procedure described in Section 3.1, our clas-
sifiers predict a class label for each pair inde-
pendently of the others. Pairs that receive neg-
ative predictions are discarded immediately and
are never reported as mined pairs. However, it
is sometimes necessary to arbitrate between pos-
itive predictions, since it is possible for a classifier
to mark as transliterations two or more pairs that
involve the same English word or the same target
word in the title. Clearly, mining multiple overlap-
ping pairs will lower the system?s precision, since
there is (presumably) at most one correct translit-
eration in the target language version of the title
for each English word.3
Our solution is to apply a greedy algorithm that
sorts the word pair predictions for a given title
in descending order according to the scores that
were assigned by the classifier. We make one pass
through the sorted list and report a pair of words as
a mined pair unless the English word or the target
language word has already been reported (for this
particular title).4
3.6 Results
In the context of the NEWS 2010 Shared Task
on Transliteration Generation we tested our sys-
tem on all five data sets: from English to Rus-
sian (EnRu), Hindi (EnHi), Tamil (EnTa), Arabic
(EnAr), and Chinese (EnCh). The EnCh set dif-
fers from the remaining sets in the lack of transpar-
ent word segmentation on the Chinese side. There
were no development sets provided for any of the
language pairs.
3On the other hand, mining all such pairs might improve
recall.
4A bug was later discovered in our implementation of this
algorithm, which had failed to add the words in a title?s first
mined pair to the ?already reported? list. This sometimes
caused up to two additional mined pairs per title to be re-
ported in the prediction files that were submitted.
Task System F P R
EnRu NED+ .875 .880 .869
BK-2007 .778 .684 .902
StringKernel* .811 .746 .889
DIRECTL+ .786 .778 .795
EnHi NED+ .907 .875 .941
BK-2007 .882 .883 .880
StringKernel .924 .954 .895
DIRECTL+ .904 .945 .866
EnTa NED+ .791 .916 .696
BK-2007 .829 .808 .852
StringKernel .914 .923 .906
DIRECTL+ .801 .919 .710
EnAr NED+ .800 .818 .783
BK-2007 .816 .834 .798
StringKernel* .827 .917 .753
DIRECTL+ .742 .861 .652
EnCh GreedyMatch .530 .698 .427
DIRECTL+ .009 .045 .005
Table 3: Transliteration mining results. An aster-
isk (*) indicates an unofficial result.
Table 3 shows the results obtained by our var-
ious systems on the final test sets, measured in
terms of F-score (F), precision (P), and recall
(R). The systems referred to as NED+, BK-2007,
StringKernel, DIRECTL+, and GreedyMatch are
described in Section 3.3.2, Section 3.3.3, Sec-
tion 3.3.4, Section 3.3.5, and Section 3.4 respec-
tively. The runs marked with an asterisk (*)
were produced after the Shared Task deadline, and
therefore are not included in the official results.
3.7 Discussion
No fixed ranking of the four approaches emerges
across the four alphabetic language pairs (all ex-
cept EnCh). However, StringKernel appears to be
the most robust, achieving the highest F-score on
three language pairs. This suggests that longer
substring features are indeed useful for classifying
candidate transliteration pairs. The simple NED+
method is a clear winner on EnRu, and obtains de-
cent scores on the remaining alphabetic language
pairs. The generation-based DIRECTL+ approach
ranks no higher than third on any language pair,
and it fails spectacularly on EnCh because of the
word segmentation ambiguity.
Finally, we observe that there are a number of
cases where the results for our discriminatively
trained classifiers, BK-2007 and StringKernel, are
46
not significantly better than those of the simple
NED+ approach. We conjecture that automatically
generating training examples is suboptimal for this
task. A more effective strategy may be to filter all
possible word pairs in the seed data to only those
with NED above a fixed threshold. We would then
apply the same threshold to the Wikipedia candi-
dates, only passing to the classifier those pairs that
surpass the threshold. This would enable a better
match between the training and test operation of
the system.
4 Conclusion
The results obtained in the context of the NEWS
2010 Machine Transliteration Shared Task con-
firm the effectiveness of our discriminative ap-
proaches to transliteration generation and mining.
Acknowledgments
This research was supported by the Alberta Inge-
nuity Fund, Informatics Circle of Research Excel-
lence (iCORE), and the Natural Sciences and En-
gineering Research Council of Canada (NSERC).
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proc. ACL.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
NAACL-HLT.
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proc. KDD.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Dan Goldwasser and Dan Roth. 2008. Transliteration
as constrained optimization. In Proc. EMNLP.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In NEWS ?09: Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration, pages 28?31.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features
into a discriminative training framework. In Proc.
NAACL-HLT.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In Proc. HLT-NAACL.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proc. UAI.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, 20(5).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
47
