Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 136?143,
New York, June 2006. c?2006 Association for Computational Linguistics
Understanding Temporal Expressions in Emails
Benjamin Han, Donna Gates and Lori Levin
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh PA 15213
{benhdj|dmg|lsl}@cs.cmu.edu
Abstract
Recent years have seen increasing re-
search on extracting and using temporal
information in natural language applica-
tions. However most of the works found
in the literature have focused on identi-
fying and understanding temporal expres-
sions in newswire texts. In this paper
we report our work on anchoring tempo-
ral expressions in a novel genre, emails.
The highly under-specified nature of these
expressions fits well with our constraint-
based representation of time, Time Cal-
culus for Natural Language (TCNL). We
have developed and evaluated a Tempo-
ral Expression Anchoror (TEA), and the
result shows that it performs significantly
better than the baseline, and compares fa-
vorably with some of the closely related
work.
1 Introduction
With increasing demand from ever more sophisti-
cated NLP applications, interest in extracting and
understanding temporal information from texts has
seen much growth in recent years. Several works
have addressed the problems of representing tem-
poral information in natural language (Setzer, 2001;
Hobbs and Pan, 2004; Saur?? et al, 2006), extracting
and/or anchoring (normalizing) temporal and event
related expressions (Wiebe et al, 1998; Mani and
Wilson, 2000; Schilder and Habel, 2001; Vazov,
2001; Filatova and Hovy, 2001), and discovering the
ordering of events (Mani et al, 2003). Most of these
works have focused on capturing temporal informa-
tion contained in newswire texts, and whenever both
recognition and normalization tasks of temporal ex-
pressions were attempted, the latter almost always
fell far behind from the former in terms of perfor-
mance.
In this paper we will focus on a different combi-
nation of the problems: anchoring temporal expres-
sions in scheduling-related emails. In our project
work of building personal agents capable of schedul-
ing meetings among different users1, understand-
ing temporal expressions is a crucial step. We have
therefore developed and evaluated our system Tem-
poral Expression Anchorer (TEA) that is capable of
normalizing such expressions in texts. As input TEA
takes English text with temporal expressions al-
ready identified, and transduces the expressions into
their representations using Time Calculus for Nat-
ural Language (TCNL) (Han and Kohlhase, 2003).
These representations, or TCNL formulae, are then
evaluated by incorporating the contextual informa-
tion to give the final normalized output. TCNL has
the following characteristics: (1) a human calendar
(e.g., the Gregorian calendar) is explicitly modeled
as a constraint system to deal with the highly under-
specified nature of many temporal expressions, and
it allows easy extension to include new temporal
primitives; (2) a set of NL-motivated operators with
a granularity-enriched type system facilitates the
representation of the intensional meaning of a tem-
1Project RADAR,
http://www.radar.cs.cmu.edu/external.asp
136
poral expression in a compositional way; and (3) the
use of temporal references such as ?focus? in the
representation cleanly separates the core meaning of
an expression from its contextual dependency.
The rest of this paper is organized as follows.
Sec. 2 first surveys the characteristics of temporal
expressions in emails compared to those in newswire
texts, and motivates the design of our representation.
Sec 3 then introduces the formalism TCNL. The sys-
tem TEA and the anchoring process is detailed in
Sec. 4, and the evaluation of the system is reported
in Sec. 5. Finally Sec. 6 concludes this paper and
outlines the future work.
2 Temporal Expressions in Emails
The extent of temporal expressions considered in
this paper includes most of the expressions using
temporal terms such as 2005, summer, evening,
1:30pm, tomorrow, etc. These expressions can be
classified into the following categories:
? Explicit: These expressions can be immedi-
ately anchored, i.e., positioned on a timeline.
E.g., June 2005, 1998 Summer, etc.
? Deictic: These expressions form a specific re-
lation with the speech time (timestamp of an
email). E.g., tomorrow, last year, two weeks
from today.
? Relative: These include the other expressions
that form a specific relation with a temporal fo-
cus, i.e., the implicit time central to the discus-
sion. E.g., from 5 to 7, on Wednesday, etc. Dif-
ferent from the speech time, a temporal focus
can shift freely during the discourse.
? Durational: These are the expressions that de-
scribe certain length in time. E.g., for about
an hour, less than 20 minutes. This is differ-
ent from an interval expression where both the
starting point and the ending point are given
(e.g., from 5 to 7). Most durational expres-
sions are used to build more complex expres-
sions, e.g., for the next 20-30 minutes.
It is worth emphasizing the crucial difference be-
tween deictic expressions and relative expressions:
anchoring the former only relies on the fixed speech
time while normalizing the latter requires the usually
hidden focus. As illustrated below the latter task can
be much more challenging:
?I?m free next week. Let?s meet on
Wednesday.?
?Are you free on Wednesday??
In the first example the ?Wednesday? denotes a dif-
ferent date since the first sentence sets up a different
focus. To make things even more interesting, ver-
bal tense can also play a role, e.g., ?He finished the
report on Wednesday.?
There are other types of temporal expressions
such as recurrence (?every Tuesday?) and rate ex-
pressions (?twice on Wednesday?) that are not sup-
ported in our system, although they are planned in
our future work (Sec. 6).
To appreciate the different nature of emails as a
genre, an interesting observation can be made by
comparing the distributions of temporal expressions
in emails and in newswire texts. The email cor-
pora we used for development and testing were col-
lected from MBA students of Carnegie Mellon Uni-
versity over the year 1997 and 1998. The 277 stu-
dents, organized in approximately 50 teams of 4 to
6 members, were participating in a 14-week course
and running simulated companies in a variety of
market scenarios (Kraut et al, 2004). The original
dataset, the CSpace email corpus, contains approx-
imately 15,000 emails. We manually picked 1,196
emails that are related to scheduling - these include
scheduling meetings, presentations, or general plan-
ning for the groups. The emails are then randomly
divided into five sets (email1 to email5), and only
four of them are used in this work: email1 was used
to establish our baseline, email2 and email5 were
used for development, and part of email4 was used
for testing. Table 1 shows some basic statistics of
these three datasets2, and an edited sample email is
shown in Fig. 1 (names altered). The most appar-
ent difference comparing these emails to newswire
texts is in the percentage of explicit expressions oc-
curring in the two different genres. In (Mani et al,
2003) it was reported that the proportion of such ex-
pressions is about 25% in the newswire corpus they
2The percentages in some rows do not add up to 100% be-
cause some expressions like coordination can be classified into
more than one type.
137
Date: Thu, 11 Sep 1997 00:14:36 -0500
I have put an outline out in the n10f1 OpReview directory...
(omitted)
We have very little time for this. Please call me Thursday
night to get clarification. I will need graphs and prose in
files by Saturday Noon.
? Mary
ps. Mark and John , I waited until AFTER midnight to
send this .
Figure 1: A sample email (edited)
used3. In contrast, explicit expressions on average
only account for around 9.5% in the three email
datasets. This is not surprising given that people
tend to use under-specified expressions in emails for
economic reasons. Another thing to note is that there
are roughly the same number of relative expressions
and non-relative expressions. Since non-relative ex-
pressions (including deictic expressions) can be an-
chored without tracking the temporal focus over a
discourse and therefore can be dealt with in a fairly
straightforward way, we may assign 50% as a some-
what generous baseline performance of any anchor-
ing system4.
Another difference between emails and newswire
texts is that the former is a medium for communi-
cation: an email can be used as a reply, or can be
attached within another email, or even be used to
address to multiple recipients. All of this compli-
cates a great deal of our task. Other notable dif-
ferences are that in emails hour ambiguity tend to
appear more often (?I?ll be home at 2.?), and peo-
ple tend to be more creative when they compose
short messages such as using tables (e.g., an entire
column of numbers to denote the number of min-
utes alloted for each presenter), bullet lists, abbrevi-
ations, and different month/day formats (?1/9? can
mean January 9 or September 1), etc. Emails also
contain more ?human errors? such as misspellings
(?Thusday? to mean Thursday) and confusion about
dates (e.g., using ?tomorrow? when sending emails
3Using the North American News Corpus.
4This is a bit generous since solving simple calendric arith-
metics such as anchoring last summer still requires a non-trivial
modeling of human calendars; see Sec. 3.
around midnight), etc. Overall it is very difficult to
recover from this type of errors.
3 Representing Times in Natural
Language
This section provides a concise overview of TCNL;
readers are referred to (Han and Kohlhase, 2003;
Han et al, 2006) for more detail.
TCNL has two major components: a constraint-
based model for human calendars and a represen-
tational language built on top of the model. Dif-
ferent from the other representations such as Zeit-
Gram (Stede and Haas, 1998), TOP (Androut-
sopoulos, 1999), and TimeML/Timex3 (Saur?? et al,
2006), the language component of TCNL is essen-
tially ?calendar-agnostic? - any temporal unit can be
plugged in a formula once it is defined in the cal-
endar model, i.e., the calendar model serves as the
lexicon for the TCNL language.
Fig. 2 shows a partial model for the Gregorian cal-
endar used in TEA. The entire calendar model is ba-
sically a constraint graph with partial ordering. The
nodes labeled with ?year? etc. represent temporal
units (or variables when viewed as a constraint sat-
isfaction problem (CSP) (Ruttkay, 1998)), and each
unit can take on a set of possible values. The undi-
rected edges represent constraints among the units,
e.g., the constraint between month and day man-
dates that February cannot have more than 29 days.
A temporal expression in NL is then viewed as if
it assigns values to some of the units, e.g., ?Friday
the 13th? assigns values to only units dow (day-
of-week) and day. An interval-based AC-3 algo-
rithm with a chronological backtracking mechanism
is used to derive at the consistent assignments to the
other units, therefore allowing us to iterate to any
one of the possible Friday the 13th.
The ordering among the units is designated by two
relations: measurement and periodicity (arrows in
Fig. 2). These relations are essential for supporting
various operations provided by the TCNL language
such as determining temporal ordering of two time
points, performing arithmetic, and changing tempo-
ral granularity, etc. For example, to interpret the ex-
pression ?early July?, we identify that July is a value
of unit month, and month is measured by day. We
then obtain the size of July in terms of day (31) and
138
Table 1: Basic statistics of the email corpora
# of
emails
# of
tempex
explicit deictic relative durational
email1 253 300 3 (1%) 139 (46.33%) 158 (52.67%) N/A
email2 253 344 19 (5.5%) 112 (32.6%) 187 (54.4%) 27 (7.8%)
email4 (part.) 149 279 71 (25.4%) 77 (27.6%) 108 (38.7%) 22 (7.9%)
email5 126 213 14 (6.6%) 105 (49.3%) 92 (43.2%) 3 (1.4%)
Year
Month Day
Hour
Minute
Second
Week
Day-of-week
Time-of-day
Time-of-week
Year component Week component
?
X component
unit constraints
alignment constraints
is-measured-by relation
is-periodic-in relation
*
*
*
*
*
*
*
(* marks a representative)
*
temporal unit
Figure 2: A partial model of the Gregorian calendar
designate the first 10 days (31/3) as the ?early? part
of July.
Internally the calendar model is further parti-
tioned into several components, and different com-
ponents are aligned using non-binary constraints
(e.g., in Fig. 2 the year component and the week
component are aligned at the day and dow units).
This is necessary because the top units in these com-
ponent are not periodic within one another. All of
the operations are then extended to deal with multi-
ple calendar components.
Built on top of the calendar model is the typed
TCNL language. The three major types are coor-
dinates (time points; e.g., {sep,6day} for Septem-
ber 6), quantities (durations; e.g., |1hour| for one
hour) and enumerations (sets of points, including
intervals; e.g., [{wed},{fri}] for Wednesday and
Friday). More complex expressions can be rep-
resented by using various operators, relations and
temporal references; e.g., {now?|1day|} for yes-
terday, {|1mon|@{>= }} for the coming Monday
(or the first coming Monday in the future; the
? ? represents the temporal focus), | < |1hour|| for
less than one hour, [{wed}:{fri}] for Wednes-
day to Friday, [f {sat, noon}] for by Saturday
noon5, and [[{15hour}:{17hour}]&{wed}] for 3-5pm
on Wednesday. The TCNL language is designed
in such a way that syntactically different formu-
lae can be evaluated to denote the same date;
e.g., {tue, now+|1week|} (?Tuesday next week?) and
{now+|1tue|} (?next Tuesday?) can denote the same
date.
Associated with the operators are type and granu-
larity requirements. For example, when a focus is
specified down to second granularity, the formula
{now+|1day|} will return a coordinate at the day
granularity - essentially stripping away information
finer than day. This is because the operator ?+?
(called fuzzy forward shifting) requires the left-hand
side operand to have the same granularity as that of
the right-hand side operand. Type coercion can also
happen automatically if it is required by an operator.
For example, the operator ?@? (ordinal selection) re-
quires that the right-hand side operand to be of type
enumeration. When presenting a coordinate such as
{>= } (some point in the future), it will be coerced
5The f denotes the relation ?finishes? (Allen, 1984); the for-
mula denotes a set of coordinates no later than a Saturday noon.
139
Table 2: Summary of operators in TCNL; LHS/RHS is the left/right operand, g(e) returns the granularity of
e and min(s) returns the set of minimal units among s.
operator Type requirement Granularity requirement Semantics Example
+ and ? C ? Q ? C g(LHS) ? g(RHS) fuzzy forward/backward
shifting
{now+|1day|}
(?tomorrow?)
++ and ?? C ? Q ? C g(LHS) ?
min(g(LHS)?g(RHS))
exact forward/backward
shifting
{now++|2hour|}
(?2 hours from now?)
@ Q ? E ? C g(RHS) ? g(LHS) ordinal {|2{sun}|@{may}}
(?the 2nd Sunday in May?)
& C ? C ? C
C ? E ? E
E ? C ? E
E ? E ? E
g(LHS) ?
min(g(LHS)?g(RHS))
distribution {now &{now+|1year|}}
(?this time next year?)
[{15hour}&[{wed}:{fri}]]
(?3pm from Wednesday to
Friday?)
into an enumeration so that the ordinal operator can
select a requested element out of it. These designs
make granularity change and re-interpretation part
of a transparent process. Table 2 lists the operators
in the TCNL language.
Most of under-specified temporal expressions still
lack necessary information in themselves in order to
be anchored. For example, it is not clear what to
make out of ?on Wednesday? with no context. In
TCNL more information can be supplied by using
one of the coordinate prefixes: the ?+?/??? prefix
signifies the relation of a coordinate with the fo-
cus (after/before the focus), and the ?f?/?p? indicates
the relation of a coordinate with the speech time
(future/past). For example, the Wednesday in ?the
company will announce on Wednesday? is repre-
sented as +f{wed}, while ?the company announced
on Wednesday? is represented as ?p{wed}. When
evaluating these formulae, TEA will rewrite the for-
mer into {|1wed|@{>= , >= now}} and the latter
into {?|1wed|@{<= , <= now}} if necessary, es-
sentially trying to find the nearest Wednesday ei-
ther in the future or in the past. Since TCNL for-
mulae can be embedded, prefixed coordinates can
also appear inside a more complex formula; e.g.,
{{|2{sun}|@f{may}}+|2day|} represents ?2 days af-
ter a future Mother?s day?6.
Note that TCNL itself does not provide a mecha-
nism to instantiate the temporal focus (? ?). The re-
sponsibility of shifting a focus whenever necessary
(focus tracking) is up to TEA, which is described in
the next section.
6This denotes a possible range of dates, but it is still different
from an enumeration.
4 TEA: Temporal Expression Anchorer
The input to our system TEA is English texts with
temporal expression markups, and the output is a
time string for each temporal expression. The format
of a time string is similar to the ISO 8601 scheme:
for a time point the format is YYYYMMDDTHHMMSS
(T is a separator), for an interval it is a pair of points
separated by ?/? (slash). Also whenever there are
slots that lack information, we use ??? (question
mark) in its place. If a points can reside at any place
between two bounds, we use (lower..upper)
to represent it. Table. 3 shows the TEA output over
the example email given in Fig. 1 (min and max are
the minimal and the maximal time points TEA can
reason with).
TEA uses the following procedure to anchor each
temporal expression:
1. The speech time (variable now) and the focus
(? ?) is first assigned to a timestamp (e.g., the
received date of an email).
2. For each temporal expression, its nearest verb
chunk is identified using the part-of-speech
tags of the sentence. Expressions associated
with a verb of past tense or present imperfective
will be given prefix ??p? to its TCNL formula,
otherwise it is given ?+f?7.
3. A finite-state parser is then used to transduce an
expression into its TCNL formula. At the pars-
ing stage the tense and granularity information
is available to the parser.
7This is of course a simplification; future work needs to be
done to explore other possibilities.
140
Table 3: Anchoring example for the email in Fig. 1
Expression TCNL formula Temporal focus (f ) Anchored time string
(timestamp) 19970911T001436
Thursday night +f{thu,night} 19970911T001436 (19970911T18????..
19970911T23????)
by Saturday Noon [f +f{sat,noon}] (19970911T18????..
19970911T23????)
min/19970913T12????
until AFTER mid-
night
[f{>= ?p{midnight}}] 19970911T001436 min/(19970911..max)
4. The produced TCNL formula (or formulae
when ambiguity arises) is then evaluated with
the speech time and the current focus. In case
of ambiguity, one formula will be chosen based
on certain heuristics (below). The result of the
evaluation is the final output for the expression.
5. Recency-based focus tracking: we use the fol-
lowing procedure to determine if the result ob-
tained above can replace the current focus (be-
low). In cases where the result is an ambigu-
ous coordinate (i.e., it denotes a possible range
of points), if one of the bounds is min or max,
we use the other to be the new focus; if it is
not possible, we choose to keep the focus un-
changed. On the other hand, if the result is
an enumeration, we go through a similar pro-
cedure to avoid using an enumeration with a
min/max bound as the new focus. Finally no
quantity can become a focus.
Note that in Step 3 the decision to make partial
semantics of a temporal expression available to our
parser is based on the following observation: con-
sider the two expressions below
?Tuesday before Christmas?
= {tue, < {|25day|@{dec}}}
?Tuesday before 6pm?
= {< {tue,18hour}, de {tue}}
Both expressions share the same ?X before Y ? pat-
tern, but their interpretations are different8. The key
to discriminate the two is to compare the granulari-
ties of X and Y : if Y if at a coarser granularity then
the first interpretation should be adopted.
In Step 4 we use the following procedure to dis-
ambiguate the result:
8de denotes a relation ?during or equal? (Allen, 1984).
1. Remove any candidate that resulted in an in-
consistency when solving for a solution in the
calendar CSP.
2. If the result is meant to be a coordinate, pick
the one that is closest to the focus.
3. If the result is supposed to be an enumeration,
pick the one whose starting point is closest to
the focus, and whose length is the shortest one.
4. Otherwise pick the first one as the result.
For example, if the current time is 2:00 pm, for ex-
pression ?at 3? with a present/future tense, the best
answer is 15:00. For expression ?from 3 to 5?, the
best answer is from 3 pm to 5 pm.
When deciding whether a temporal expression
can become the next focus, we use simple heuris-
tics to rule out any expression that behaves like a
noun modifier. This is motivated by the following
example (timestamp: 19970919T103315):
IT basically analyses the breakdown on
labor costs and compares our 1998 labor
costs with their demands for 1999-2000.
...
I will check mail on Sunday and see any
feedback.
Without blocking the expression 1999-2000 from
becoming the focus, the last expression will be in-
correctly anchored in year 2000. The key obser-
vation here is that a noun-modifying temporal ex-
pression usually serves as a temporal co-reference
instead of representing a new temporal entity in the
discourse. These references tend to have a more con-
fined effect in anchoring the subsequent expressions.
141
Table 4: Development and testing results
Accuracy Parsing errors Human errors Anchoring errors
email2 (dev) 78.2% 10.47% 1.7% 9.63%
email5 (dev) 85.45% 5.16% 1% 8.39%
email4 (test-
ing)
76.34% 17.92% < 1% 5.74%
5 Evaluation
The temporal expressions in all of the datasets were
initially tagged using rules developed for Minor-
Third9, and subsequently corrected manually by two
of the authors. We then developed a prototype sys-
tem and established our baseline over email1 (50%).
The system at that time did not have any focus track-
ing mechanism (i.e., it always used the timestamp
as the focus), and it did not use any tense infor-
mation. The result confirms our estimate given in
Sec. 2. We then gradually developed TEA to its cur-
rent form using email1, email2 and email5. Dur-
ing the four-month development we added the focus
tracking mechanism, incorporating the tense infor-
mation into each TCNL formula via the coordinate
prefixes, and introduced several representational im-
provements. Finally we tested the system on the un-
seen dataset email4, and obtained the results shown
in Table 4. Note that the percentages reported in
the table are accuracies, i.e., the number of cor-
rectly anchored expressions over the total number
of temporal expressions over a dataset, since we are
assuming correct tagging of all of the expressions.
Our best result was achieved in the dev set email5
(85.45%), and the accuracy over the test set email4
was 76.34%.
Table 4 also lists the types of the errors made by
our system. The parsing errors are mistakes made
at transducing temporal expressions using the finite-
state parser into their TCNL formulae, the human
errors are described in Sec. 2, and the rest are the
anchoring errors. The accuracy numbers are all
compared favorably to the baseline (50%). To put
this performance in perspective, in (Wiebe et al,
1998) a similar task was performed over transcribed
scheduling-related phone conversations. They re-
ported an average accuracy 80.9% over the CMU
9http://minorthird.sourceforge.net/
test set and 68.9% over the NMSU test set. Although
strictly speaking the two results cannot be compared
due to differences in the nature of the corpora (tran-
scription vs. typing), we nevertheless believe it rep-
resents a closer match compared to the other works
done on newswire genre.
It should also be noted that we adopted a simi-
lar recency-based focus model as in (Wiebe et al,
1998). Although simple to implement, this naive
approach proved to be one major contributor to the
anchoring errors in our experiments. An example is
given below (the anchored times are shown in sub-
script):
This research can not proceed until the
trade-offs are known on Monday19970818 .
...
Mary will perform this by
Friday(min..19970822) using the data
from Monday19970825 .
The last expression received an incorrect date: it
should be the same date the expression ?on Mon-
day? refers to. Our system made this error because
it blindly used the most recently mentioned time
((min..19970822)) as the focus to anchor the
formula +f{mon}. This error later also propagated
to the anchoring of the subsequent expressions.
6 Conclusion and Future Work
In this paper we have adopted a constraint-based
representation of time, Time Calculus for Natural
Language (TCNL), to tackle the task of anchoring
temporal expressions in a novel genre, emails. We
believe that the genre is sufficiently different from
newswire texts, and its highly under-specified nature
fits well with a constraint-based modeling of human
calendars. TCNL also allows for an explicit repre-
sentation of temporal focus, and many of our intu-
itions about granularity change and temporal arithe-
142
matics are encapsulated in its type system and oper-
ators. The performance of our anchoring system is
significantly better than baseline, and compares fa-
vorably with some of the closely related work.
In the future we will re-examine our focus track-
ing mechanism (being the most significant source of
errors), and possibly treat it as a classification prob-
lem (similar to (Mani et al, 2003)). We also need to
investigate the disambiguation procedure and pos-
sibly migrate the functionality into a separate dis-
course module. In addition, the co-referencing ten-
dency of noun-modifying expressions could lead to
a better way to anchoring this particular type of tem-
poral expressions. Finally we would like to ex-
pand our coverage of temporal expressions to in-
clude other types of expressions such as recurrence
expressions10.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. NBCHD030010.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA), or the Department of Interior-National
Business Center (DOI-NBC).
References
J. F. Allen. 1984. Towards a General Theory of Action
and Time. Artificial Intelligence, 23:123?154.
I. Androutsopoulos. 1999. Temporal Meaning Rep-
resentations in a Natural Language Front-end. In
M. Gergatsoulis and P. Rondogiannis, editors, Inten-
sional Programming II (Proceedings of the 12th In-
ternational Symposium on Languages for Intensional
Programming, Athens, Greece.
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps To Event-Clauses. In Proceedings of ACL-
2001: Workshop on Temporal and Spatial Information
Processing, Toulouse, France, 7.
10The current design of TCNL allows for a more restricted
type of recurrence: e.g., ?3pm from Wednesday to Friday? is
represented as [{15hour}&[{wed}:{fri}]]. However this is in-
sufficient to represent expressions such as ?every 4 years?.
Benjamin Han and Michael Kohlhase. 2003. A Time
Calculus for Natural Language. In The 4th Work-
shop on Inference in Computational Semantics, Nancy,
France, September.
B. Han, D. Gates, and L. Levin. 2006. From Language to
Time: A Temporal Expression Anchorer. In Proceed-
ings of the 13th International Symposium on Tempo-
ral Representation and Reasoning (TIME 2006), Bu-
dapest, Hungary.
J. R. Hobbs and Feng. Pan. 2004. An ontology of time
for the semantic web. TALIP Special Issue on Spa-
tial and Temporal Information Processing, 3(1):66?
85, March.
R. E. Kraut, S. R. Fussell, F. J. Lerch, and A Espinosa.
2004. Coordination in teams: Evidence from a sim-
ulated management game. Journal of Organizational
Behavior, to appear.
I. Mani and G. Wilson. 2000. Robust Temporal Process-
ing of News. In Proceedings of ACL-2000.
I. Mani, B. Schiffman, and J. Zhang. 2003. Inferring
Temporal Ordering of Events in News. In Proceedings
of the Human Language Technology Conference (HLT-
NAACL?03).
Zso?fia Ruttkay. 1998. Constraint Satisfaction - a Survey.
Technical Report 11(2-3), CWI.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2006. TimeML Annotation Guidelines, Version 1.2.1,
January 31.
F. Schilder and C. Habel. 2001. From Temporal Expres-
sions To Temporal Information: Semantic Tagging Of
News Messages. In Proceedings of ACL-2001: Work-
shop on Temporal and Spatial Information Processing,
Toulouse, France, 7.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield.
M. Stede and S. Haas. 1998. Understanding and track-
ing temporal descriptions in dialogue. In B. Schro?der,
W. Lenders, W. Hess, and T. Portele, editors, Proceed-
ings of the 4th Conference on Natural Language Pro-
cessing - KONVENS ?98.
N. Vazov. 2001. A System for Extraction of Tempo-
ral Expressions from French Texts Based on Syntac-
tic and Semantic Constraints. In Proceedings of ACL-
2001: Workshop on Temporal and Spatial Information
Processing, Toulouse, France, 7.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and
K. J. McKeever. 1998. An Empirical Approach to
Temporal Reference Resolution. Journal of Artificial
Intelligence Research, 9:247?293.
143
