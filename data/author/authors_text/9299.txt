Proceedings of NAACL HLT 2009: Short Papers, pages 165?168,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Search Result Re-ranking by Feedback Control Adjustment for
Time-sensitive Query
Ruiqiang Zhang? and Yi Chang? and Zhaohui Zheng?
Donald Metzler? and Jian-yun Nie?
?Yahoo! Labs, 701 First Avenue, Sunnyvale, CA94089
?University of Montreal, Montreal, Quebec,H3C 3J7, Canada
?{ruiqiang,yichang,zhaohui,metzler}@yahoo-inc.com
?nie@iro.umontreal.ca
Abstract
We propose a new method to rank a special
category of time-sensitive queries that are year
qualified. The method adjusts the retrieval
scores of a base ranking function according
to time-stamps of web documents so that the
freshest documents are ranked higher. Our
method, which is based on feedback control
theory, uses ranking errors to adjust the search
engine behavior. For this purpose, we use
a simple but effective method to extract year
qualified queries by mining query logs and a
time-stamp recognition method that considers
titles and urls of web documents. Our method
was tested on a commercial search engine. The
experiments show that our approach can sig-
nificantly improve relevance ranking for year
qualified queries even if all the existing meth-
ods for comparison failed.
1 Introduction
Relevance ranking plays a crucial role in search
engines. There are many proposed machine learn-
ing based ranking algorithms such as language
modeling-based methods (Zhai and Lafferty, 2004),
RankSVM (Joachims, 2002), RankBoost (Freund et al,
1998) and GBrank (Zheng et al, 2007). The input to
these algorithms is a set of feature vectors extracted from
queries and documents. The goal is to find the parameter
setting that optimizes some relevance metric given
training data. While these machine learning algorithms
can improve average relevance, they may be ineffctive
for certain special cases. Time-sensitive queries are one
such special case that machine-learned ranking functions
may have a hard time learning, due to the small number
of such queries.
Consider the query ?sigir? (the name of a conference),
which is time sensitive. Table 1 shows two example
search result pages for the query, SERP1 and SERP2. The
query: sigir
SERP1 url1: http://www.sigir.org
url2: http://www.sigir2008.org
url3: http://www.sigir2004.org
url4: http://www.sigir2009.org
url5: http://www.sigir2009.org/schedule
SERP2 url1: http://www.sigir.org
url2: http://www.sigir2009.org
url3: http://www.sigir2009.org/schedule
url4: http://www.sigir2008.org
url5: http://www.sigir2004.org
Table 1: Two contrived search engine result pages
ranking of SERP2 is clearly better than that of SERP1 be-
cause the most recent event, ?sigir2009?, is ranked higher
than other years.
Time is an important dimension of relevance in web
search, since users tend to prefer recent documents to old
documents. At the time of this writing (February 2009),
none of the major commercial search engines ranked the
homepage for SIGIR 2009 higher than previous SIGIR
homepages for the query ?sigir?. One possible reason for
this is that ranking algorithms are typically based on an-
chor text features, hyperlink induced features, and click-
through rate features. However, these features tend to fa-
vor old pages more than recent ones. For example, ?si-
gir2008? has more links and clicks than ?sigir2009? be-
cause ?sigir2008? has existed longer time and therefore
has been visited more. It is less likely that newer web
pages from ?sigir2009? can be ranked higher using fea-
tures that implicitly favor old pages.
However, the fundamental problem is that current ap-
proaches have focused on improving general ranking al-
gorithms. Methods for improving ranking of specific
types of query like temporal queries are often overlooked.
Aiming to improve ranking results, some methods of
re-ranking search results are proposed, such as the work
by (Agichtein et al, 2006) and (Teevan et al, 2005).
165
Search Engine
Detector 
Controller
error R(q, yn)R(q, yo)
_
+
Figure 1: Feedback control for search engine
These work uses user search behavior information or per-
sonalization information as features that are integrated
into an enhanced ranking model. We propose a novel
method of re-ranking search results. This new method
is based on feedback control theory, as illustrated in 1.
We make a Detector to monitor search engine (SE) out-
put and compare it with the input, which is the desired
search engine ranking. If an error is found, we design
the controller that uses the error to adjust the search en-
gine output, such that the search engine output tracks the
input. We will detail the algorithm in Section 4.1.
Our method was applied to a special class of time-
sensitive query, year qualified queries (YQQs). For this
category, we found users either attached a year with the
query explicitly, like ?sigir 2009?, or used the query only
without a year attached,like ?sigir?. We call the former
explicit YQQs, and the latter implicit YQQs. Using query
log analysis, we found these types of queries made up
about 10% of the total query volume. We focus exclu-
sively on implicit YQQs by translating the user?s im-
plicit intention as the most recent year. Explicit YQQs
are less interesting, because the user?s temporal inten-
tion is clearly specified in the query. Therefore, rank-
ing for these types of queries is relatively straightfor-
ward. Throughout the remainder of this paper, we use
the ?YQQ? to refer to implicit YQQs, unless otherwise
stated.
2 Adaptive score adjustment
Our proposed re-ranking model is shown in Eq. 1, as be-
low.
F(q, d) =
{
R(q, d) if q < YQQ
R(q, d) + Q(q, d) otherwise
Q(q, d) =
{ (e(do, dn) + k)e??(q) if y(d) = yn
0 otherwise
e(do, dn) = R(q, do) ? R(q, dn)
(1)
This work assumes that a base ranking function is used
to rank documents with respect to an incoming query. We
denote this base ranking function as R(q, d). This ranking
function is conditioned on a query q and a document d. It
is assumed to model the relevance between q and d.
Our proposed method is flexible for all YQQ queries.
Suppose the current base ranking function gives the re-
sults as SERP1 of Table 1. To correct the ranking, we
propose making an adjustment to R(q, d).
In Eq. 1, F(q, d) is the final ranking function. If the
query is not an YQQ, the base ranking function is used.
Otherwise, we propose an adjustment function, Q(q, d) ,
to adjust the base ranking function. Q(q, d) is controlled
by the ranking error, e(do, dn), signifying the base func-
tion ranking error if the newest web page dn is ranked
lower than the oldest web page do. y(d) is the year that
the event described by d has occurred or will occur. If
yo and yn indicate the oldest year and the newest year,
then y(do) = yo, y(dn) = yn. R(q, do) and R(q, dn) are the
base ranking function scores for the oldest and the newest
documents.
k is a small shift value for direction control. When
k < 0, the newest document is adjusted slightly under the
old one. Otherwise, it is adjusted slightly over the old
one. Experiments show k > 0 gave better results. The
value of k is determined in training.
?(q) is the confidence score of a YQQ query, mean-
ing the likelihood of a query to be YQQ. The confidence
score is bigger if a query is more likely to be YQQ. More
details are given in next section. ? is a weighting param-
eter for adjusting ?(q).
The exp function e??(q) is a weighting to control boost-
ing value. A higher value, confidence ?, a larger boosting
value, Q(q, d).
Our method can be understood by feedback control
theory, as illustrated in Fig. 1. The ideal input is R(q, yo)
representing the desired ranking score for the newest
Web page, R(q, yn). But the search engine real output
is R(q, yn). Because search engine is a dynamic system,
its ranking is changing over time. This results in ranking
errors, e(do, dn) = R(q, do) ? R(q, dn). The function of
?Controller? is to design a function to adjust the search
engine ranking so that the error approximates to zero,
e(do, dn) = 0. For this work, ?Controller? is Q(q, d).
?Detector? is a document year-stamp recognizer, which
will be described more in the next section. ?Detector?
is used to detect the newest Web pages and their ranking
scores. Fig. 1 is an ideal implementation of our methods.
We cannot carry out real-time experiments in this work.
Therefore, the calculation of ranking errors was made in
offline training.
3 YQQ detection and year-stamp
recognition
To implement Eq. 1, we need to find YQQ queries and to
identify the year-stamp of web documents.
Our YQQ detection method is simple, efficient, and
relies only on having access to a query log with frequency
information. First, we extracted all explicit YQQs from
166
query log. Then, we removed all the years from explicit
YQQs. Thus, implicit YQQs are obtained from explicit
YQQs. The implicit YQQs are saved in a dictionary. In
online test, we match input queries with each of implicit
YQQs in the dictionary. If an exact match is found, we
regard the input query as YQQ, and apply Eq. 1 to re-rank
search results.
After analyzing samples of the extracted YQQs, we
group them into three classes. One is recurring-event
query, like ?sigir?, ?us open tennis?; the second is news-
worthy query, like ?steve ballmer?, ?china foreign re-
serves?; And the class not belong to any of the above
two, like ?christmas?, ?youtube?. We found our proposed
methods were the most effective for the first category. In
Eq. 1, we can use confidence ?(q) to distinguish the three
categories and their change of ranking as shown in Eq.1,
that is defined as below.
?(q) =
?
y w(q, y)
#(q) +?y w(q, y)
(2)
where w(q, y) = #(q.y)+#(y.q). #(q.y) denotes the num-
ber of times that the base query q is post-qualified with
the year y in the query log. Similarly, #(y.q) is the num-
ber of times that q is pre-qualified with the year y. This
weight measures how likely q is to be qualified with y,
which forms the basis of our mining and analysis. #(q) is
the counts of independent query, without associating with
any other terms.
We also need to know the year-stamp y(d) for each
web document so that the ranking score of a document
is updated if y(d) = yn is satisfied. We can do this
from a few sources such as title, url, anchar text, and
extract date from documents that is possible for many
news pages. For example, from url of the web page,
?www.sigir2009.org?, we detect its year-stamp is 2009.
We have also tried to use some machine generated
dates. However, in the end we found such dates are in-
accurate and cannot be trusted. For example, discovery
time is the time when the document was found by the
crawler. But a web document may exist several years be-
fore a crawler found it. We show the worse effect of using
discovery time in the experiments.
4 Experiments
We will describe the implementation methods and experi-
mental results in this section. Our methods include offline
dictionary building and online test. In offline training, our
first step is to mine YQQs. A commercial search engine
company provided us with six months of query logs. We
extracted a list of YQQs using Section 3?s method. For
each of the YQQs, we run the search engine and output
the top N results. For each document, we used the method
described in Section 3 to recognize the year-stamp and
find the oldest and the newest page. If there are multiple
urls with the same yearstamp, we choose the first oldest
and the first most recent. Next,we calculated the boost-
ing value according to Eq. 1. Each query has a boosting
value. For online test, a user?s query is matched with each
of the YQQs in the dictionary. If an exact match is found,
the boosting value will be added to the base ranking score
iff the document has the newest yearstamp.
For evaluating our methods, we randomly extracted
600 YQQs from the dictionary. We extracted the top-5
search results for each of queries using the base ranking
function and the proposed ranking function. We asked
human editors to judge all the scraped results. We used
five judgment grades: Perfect, Excellent, Good, Fair,
and Bad. Editors were instructed to consider temporal
issues when judging. For example, sigir2004 is given
a worse grade than sigir2009. To avoid bias, we ad-
vised editors to retain relevance as their primary judg-
ment criteria. Our evaluation metric is relative change
in DCG, %?dcg = DCGproposed?DCGbaselineDCGbaseline , where DCG is
the traditional Discounted Cumulative Gain (Jarvelin and
Kekalainen, 2002).
4.1 Effect of the proposed boosting method
Our experimental results are shown in Table 2, where we
compared our work with the existing methods. While we
cannot apply (Li and Croft, 2003)?s approach directly be-
cause first, our search engine is not based on language
modeling; second, it is impossible to obtain exact times-
tamp for web pages as (Li and Croft, 2003) did in the
track evaluation. However, we tried to simulate (Li and
Croft, 2003)?s approach in web search by using the linear
integration method exactly as the same as(Li and Croft,
2003) by adding a time-based function with our base
ranking function. For the timestamp, we used discovery
time in the time-based function. The parameters (?, ?)
have the exact same meaning as in (Li and Croft, 2003)
but were tuned according to our base ranking function.
With regards to the approach by (Diaz and Jones, 2004),
we ranked the web pages in decreasing order of discov-
ery time. Our own approaches were tested under options
with and without using adaptation. For no adaption, we
let the e of Eq.1 equal to 0, meaning no score difference
between the oldest document and the newest document
was captured, but a constant value was used. It is equiv-
alent to an open loop in Fig.1. For adaption, we used the
ranking errors to adjust the base ranking. In the Table we
used multiple ks to show the effect of changing k. Using
different k can have a big impact on the performance. The
best value we found was k = 0.3. In this experiment, we
let ?(q) = 0 so that the result responds to k only.
Our approach is significantly better than the existing
methods. Both of the two existing methods produced
worse results than the baseline, which shows the ap-
167
Li & Croft (?, ?)=(0.2,2.0) -0.5
(?, ?)=(0.2,4.0) -1.2
Diaz & Jones -4.5?
No adaptation (e = 0, k=0.3 1.2
open loop) k=0.4 0.8
Adaptation (closed loop) k=0.3 6.6?
k=0.4 6.2?
Table 2: %?dcg of proposed method comparing with
existing methods.A sign ??? indicates statistical signifi-
cance (p-value<0.05)
? 0 0.2 0.4 0.6 0.8 1.0
%?dcg 6.6? 7.8? 8.4? 4.5 2.1 -0.2?
Table 3: Effect of confidence as changing ?.
proaches may be inappropriate for Web search. Not sur-
prisingly, using adaption achieved much better results
than without using adaption. Thus, these experiments
prove the effectiveness of our proposed methods.
Another important parameter in the Eq.1 is the confi-
dence score ?(q), which indicates the confidence of query
to be YQQ. In Eq. 1, ? is used to adjusting ?(q). We
observed dcg gain for each different ?. The results are
shown in Table 3. The value of ? needs to be tuned for
different base ranking functions. A higher ? can hurt per-
formance. In our experiments, the best value of 0.4 gave a
8.4% statistically significant gain in DCG. The ? = 0 set-
ting means we turn off confidence, which results in lower
performance. Thus, using YQQ confidence is effective.
5 Discussions and conclusions
In this paper, we proposed a novel approach to solve
YQQ ranking problem, which is a problem that seems
to plague most major commercial search engines. Our
approach for handling YQQs does not involve any query
expansion that adds a year to the query. Instead, keeping
the user?s query intact, we re-rank search results by ad-
justing the base ranking function. Our work assumes the
intent of YQQs is to find documents about the most recent
year. For this reason, we use YQQ confidence to measure
the probability of this intent. As our results showed, our
proposed method is highly effective. A real example is
given in Fig. 2 to show the significant improvement by
our method.
Our adaptive methods are not limited to YQQs only.
We believe this framework can be applied to any category
of queries once a query classification and a score detector
have been implemented.
Figure 2: Ranking improvement for query ICML by our
method: before re-rank(left) and after(right)
References
Eugene Agichtein, Eric Brill, and Susan Dumais. 2006.
Improving web search ranking by incorporating user
behavior information. In SIGIR ?06, pages 19?26.
Fernando Diaz and Rosie Jones. 2004. Using temporal
profiles of queries for precision prediction. In Proc.
27th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, pages 18?24, New
York, NY, USA. ACM.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In ICML ?98: Proceedings
of the Fifteenth International Conference on Machine
Learning, pages 170?178.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 133?
142.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based
language models. In Proc. 12th Intl. Conf. on Infor-
mation and Knowledge Management, pages 469?475,
New York, NY, USA. ACM.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005.
Personalizing search via automated analysis of inter-
ests and activities. In SIGIR ?05, pages 449?456.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Trans. Inf. Syst., 22(2):179?
214.
Zhaohui Zheng, Keke Chen, Gordon Sun, and Hongyuan
Zha. 2007. A regression framework for learning rank-
ing functions using relative relevance judgments. In
SIGIR ?07, pages 287?294.
168
A Unified Approach in Speech-to-Speech Translation: Integrating
Features of Speech Recognition and Machine Translation
Ruiqiang Zhang and Genichiro Kikui and Hirofumi Yamamoto
Taro Watanabe and Frank Soong and Wai Kit Lo
ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang, genichiro.kikui}@atr.jp
Abstract
Based upon a statistically trained speech
translation system, in this study, we try
to combine distinctive features derived from
the two modules: speech recognition and
statistical machine translation, in a log-
linear model. The translation hypotheses
are then rescored and translation perfor-
mance is improved. The standard trans-
lation evaluation metrics, including BLEU,
NIST, multiple reference word error rate
and its position independent counterpart,
were optimized to solve the weights of the
features in the log-linear model. The exper-
imental results have shown significant im-
provement over the baseline IBM model 4
in all automatic translation evaluation met-
rics. The largest was for BLEU, by 7.9%
absolute.
1 Introduction
Current translation systems are typically of a
cascaded structure: speech recognition followed
by machine translation. This structure, while
explicit, lacks some joint optimality in per-
formance since the speech recognition module
and translation module are running rather in-
dependently. Moreover, the translation module
of a speech translation system, a natural off-
spring of text-input based translation system,
usually takes a single-best recognition hypoth-
esis transcribed in text and performs standard
text-based translation. Lots of supplementary
information available from speech recognition,
such as N -best recognition recognition hypothe-
ses, likelihoods of acoustic and language models,
is not well utilized in the translation process.
The information can be effective for improving
translation quality if employed properly.
The supplementary information can be ex-
ploited by a tight coupling of speech recognition
and machine translation (Ney, 1999) or keeping
the cascaded structure unchanged but using an
integration model, log-linear model, to rescore
the translation hypotheses. In this study the
last approach was used due to its explicitness.
In this paper we intended to improve speech
translation by exploiting these information.
Moreover, a number of advanced features from
the machine translation module were also added
in the models. All the features from the speech
recognition and machine translation module
were combined by the log-linear models seam-
lessly.
In order to test our results broadly, we used
four automatic translation evaluation metrics:
BLEU, NIST, multiple word error rate and po-
sition independent word error rate, to measure
the translation improvement.
In the following, in section 2 we introduce the
speech translation system. In section 3, we de-
scribe the optimization algorithm used to find
the weight parameters in the log-linear model.
In section 4 we demonstrate the effectiveness
of our technique in speech translation experi-
ments. In the final two sections we discuss the
results and present our conclusions.
2 Feature-based Log-linear Models
in Speech Translation
The speech translation experimental system
used in this study illustrated in Fig. 1 is a typi-
cal, statistics-based one. It consists of two ma-
jor cascaded components: an automatic speech
recognition (ASR) module and a statistical ma-
chine translation (SMT) module. Additionally,
a third module, ?Rescore?, has been added to the
system and it forms a key component in the sys-
tem. Features derived from ASR and SMT are
combined in this module to rescore translation
candidates.
Without loss of its generality, in this paper
we use Japanese-to-English translation to ex-
plain the generic speech translation process. Let
X denote acoustic observations of a Japanese
X
utterance
recognized
text
target
translation
E
best
translation
J N1
ASR SMT
E
NxK
1
Rescore
Figure 1: Current framework of speech transla-
tion
utterance, typically a sequence of short-time
spectral vectors received at a frame rate of ev-
ery centi-second. It is first recognized as a
Japanese sentence, J . The recognized sentence
is then translated into a corresponding English
sentence, E.
The conversion from X to J is performed in
the ASR module. Based on Bayes? rule, P (J |X)
can be written as
P (J |X) = Pam(X|J)Plm(J)/P (X)
where Pam(X|J) is the acoustic model likeli-
hood of the observations given the recognized
sentence J ; Plm(J), the source language model
probability; and P (X), the probability of all
acoustic observations.
In the experiment we generated a set of N -
best hypotheses, JN1 = {J1, J2, ? ? ? , JN} 1 and
each Ji is determined by
Ji = arg maxJ??i
Pam(X|J)Plm(J)
where ?i is the set of all possible source sen-
tences excluding all higher ranked Jk?s, 1 ? k ?
i ? 1.
The conversion from J to E in Fig. 1 is
the machine translation process. According
to the statistical machine translation formal-
ism (Brown et al, 1993), the translation process
is to search for the best sentence E? such that
E? = arg max
E
P (E|J) = arg max
E
P (J |E)P (E)
where P (J |E) is a translation model charac-
terizing the correspondence between E and J ;
P (E), the English language model probability.
In the IBM model 4, the translation model
P (J |E) is further decomposed into four sub-
models:
? Lexicon Model ? t(j|e): probability of a
word j in the Japanese language being
translated into a word e in the English lan-
guage.
1Hereafter, J1 is called the single-best hypothesis of
speech recognition; JN1 , the N -best hypotheses.
? Fertility model ? n(?|e): probability of
a English language word e generating ?
words.
? Distortion model ? d: probability of distor-
tion, which is decomposed into the distor-
tion probabilities of head words and non-
head words.
? NULL translation model ? p1: a fixed prob-
ability of inserting a NULL word after de-
termining each English word.
In the above we listed seven features: two
from ASR (Pam(X|J), Plm(J)) and five from
SMT (P (E), t(j|e), n(?|e), d, p1).
The third module in Fig. 1 is to rescore trans-
lation hypotheses from SMT by using a feature-
based log-linear model. All translation can-
didates output through the speech recognition
and translation modules are re-evaluated by us-
ing all relevant features and searching for the
best translation candidate of the highest score.
The log-linear model used in our speech trans-
lation process, P (E|X), is
P?(E|X) =
exp(?Mi=1 ?ifi(X,E))?
E? exp(
?M
i=1 ?ifi(X,E?))
? = {?M1 }
(1)
In the Eq. 1, fi(X,E) is the logarithm value
of the i-th feature; ?i is the weight of the i-
th feature. Integrating different features in the
equation results in different models. In the ex-
periments performed in section 4, four different
models will be trained by increasing the number
of features successively to investigate the effect
of different features for improving speech trans-
lation.
In addition to the above seven features, the
following features are also incorporated.
? Part-of-speech language models: English
part-of-speech language models were used.
POS dependence of a translated English
sentence is an effective constraint in prun-
ing English sentence candidates. In our ex-
periments 81 part-of-speech tags and a 5-
gram POS language model were used.
? Length model P (l|E, J): l is the length
(number of words) of a translated English
sentence.
? Jump weight: Jump width for adjacent
cepts in Model 4 (Marcu and Wong, 2002).
? Example matching score: The translated
English sentence is matched with phrase
translation examples. A score is derived
based on the count of matches (Watanabe
and Sumita, 2003).
? Dynamic example matching score: Similar
to the example matching score but phrases
were extracted dynamically from sentence
examples (Watanabe and Sumita, 2003).
Altogether, we used M(=12) different fea-
tures. In section 3, we review Powell?s algo-
rithm (Press et al, 2000) as our tool to opti-
mize model parameters, ?M1 , based on different
objective translation metrics.
3 Parameter Optimization Based
on Translation Metrics
The denominator in Eq. 1 can be ignored since
the normalization is applied equally to every hy-
pothesis. Hence, the choice of the best transla-
tion, E?, out of all possible translations, E, is
independent of the denominator,
E? = arg max
E
M?
i=1
?ilogPi(X,E) (2)
where we write features, fi(X,E), explicitly in
logarithm, logPi(X,E).
The effectiveness of the model in Eq. 2 de-
pends upon the parameter optimization of the
parameter set ?M1 , with respect to some objec-
tively measurable but subjectively relevant met-
rics.
Suppose we have L speech utterances and
for each utterance, we generate N best speech
recognition hypotheses. For each recogni-
tion hypothesis, K English language transla-
tion hypotheses are generated. For the l-th
input speech utterance, there are then Cl =
{El1 , ? ? ? , ElN?K} translations. All L speech ut-
terances generate L?N?K translations in to-
tal.
Our goal is to minimize the translation ?dis-
tortion? between the reference translations, R,
and the translated sentences, E? .
?M1 = optimize D(E? ,R) (3)
where E? = {E?1, ? ? ? , E?L} is a set of translations
of all utterances. The translation E?l of the l-
th utterance is produced by the (Eq. 2), where
E ? Cl.
Let R = {R1, ? ? ? , RL} be the set of transla-
tion references for all utterances. Human trans-
lators paraphrased 16 reference sentences for
each utterance, i.e., Rl contains 16 reference
candidates for the l-th utterance.
D(E? ,R) is a translation ?distortion? or an
objective translation assessment. The following
four metrics were used specifically in this study:
? BLEU (Papineni et al, 2002): A weighted
geometric mean of the n-gram matches be-
tween test and reference sentences multi-
plied by a brevity penalty that penalizes
short translation sentences.
? NIST : An arithmetic mean of the n-gram
matches between test and reference sen-
tences multiplied by a length factor which
again penalizes short translation sentences.
? mWER (Niessen et al, 2000): Multiple ref-
erence word error rate, which computes the
edit distance (minimum number of inser-
tions, deletions, and substitutions) between
test and reference sentences.
? mPER: Multiple reference position inde-
pendent word error rate, which computes
the edit distance without considering the
word order.
The BLEU score and NIST score are calcu-
lated by the tool downloadable 2.
Because the objective function in the model
(Eq. 3) is not smoothed function, we used Pow-
ell?s search method to find a solution. The Pow-
ell?s algorithm used in this work is similar as the
one from (Press et al, 2000) but we modified the
line optimization codes, a subroutine of Powell?s
algorithm, with reference to (Och, 2003).
Finding a global optimum is usually difficult
in a high dimensional vector space. To make
sure that we had found a good local optimum,
we restarted the algorithm by using various ini-
tializations and used the best local optimum as
the final solution.
4 Experiments
4.1 Corpus & System
The data used in this study was the Basic
Travel Expression Corpus (BTEC) (Kikui et al,
2003), consisting of commonly used sentences
listed in travel guidebooks and tour conversa-
tions. The corpus were designed for developing
multiple language speech-to-speech translation
systems. It contains four different languages:
Chinese, Japanese, Korean and English. Only
Japanese-English parallel data was used in this
2http://www.nist.gov/speech/tests/mt/
Table 1: Training, development and test data
from Basic Travel Expression Corpus(BTEC)
Japanese English
Train Sentences 162,318
Words 1,288,767 949,377
Dev. Sentences 510
Words 4015 2983
Test Sentences 508
Words 4112 2951
study. The speech data was recorded by multi-
ple speakers and was used to train the acoustic
models, while the text database was used for
training the language and translation models.
The standard BTEC training corpus, the first
file and the second file from BTEC standard test
corpus #01 were used for training, development
and test respectively. The statistics of corpus is
shown in table 1.
The speech recognition engine used in the ex-
periments was an HMM-based, large vocabu-
lary continuous speech recognizer. The acoustic
HMMs were triphone models with 2,100 states
in total, using 25 dimensional, short-time spec-
trum features. In the first and second pass of
decoding, a multiclass word bigram of a lexicon
of 37,000 words plus 10,000 compound words
was used. A word trigram was used in rescor-
ing the results.
The machine translation system is a graph-
based decoder (Ueffing et al, 2002). The first
pass of the decoder generates a word-graph, a
compact representation of alternative transla-
tion candidates, using a beam search based on
the scores of the lexicon and language mod-
els. In the second pass an A* search traverses
the graph. The edges of the word-graph, or
the phrase translation candidates, are gener-
ated by the list of word translations obtained
from the inverted lexicon model. The phrase
translations extracted from the Viterbi align-
ments of the training corpus also constitute the
edges. Similarly, the edges are also created from
dynamically extracted phrase translations from
the bilingual sentences (Watanabe and Sumita,
2003). The decoder used the IBM Model 4
with a trigram language model and a 5-gram
part-of-speech language model. The training of
IBM model 4 was implemented by the GIZA++
package (Och and Ney, 2003).
4.2 Model Training
In order to quantify translation improvement by
features from speech recognition and machine
translation respectively, we built four log-linear
models by adding features successively. The
four models are:
? Standard translation model(stm): Only
features from the IBM model 4 (M=5) de-
scribed in section 2 were used in the log-
linear models. We did not perform parame-
ter optimization on this model. It is equiv-
alent to setting all the ?M1 to 1. This model
was the standard model used in most sta-
tistical machine translation system. It is
referred to as the baseline model.
? Optimized standard translation models
(ostm): This model consists of the same
features as the previous model ?stm? but
the parameters were optimized by Powell?s
algorithm. We intended to exhibit the ef-
fect of parameter optimization by compar-
ing this model with the baseline ?stm?.
? Optimized enhanced translation models
(oetm): We incorporated additional trans-
lation features described in section 2 to
enrich the model ?ostm?. In this model
the number of the total features, M , is 10.
Model parameters were optimized. We in-
tended to show how much the enhanced
features can improve translation quality.
? Optimized enhanced speech translation
models (oestm): Features from speech
recognition, likelihood scores of acoustic
and language models, were incorporated
additionally into the model ?oetm?. All
the 12 features described in section 2 were
used. Model parameters were optimized.
To optimize ? parameters of the log-linear
models, we used the development data of 510
speech utterances. We adopted an N -best
hypothesis approach (Och, 2003) to train ?.
For each input speech utterance, N?K candi-
date translations were generated, where N is
the number of generated recognition hypothe-
ses and K is the number of translation hypothe-
ses. A vector of dimension M , corresponding to
multiple features used in the translation model,
was generated for each translation candidate.
The Powell?s algorithm was used to optimize
these parameters. We used a large K to ensure
that promising translation candidates were not
Table 2: Comparisons of single-best and N -best
hypotheses of speech recognition performance
in terms of word accuracy, sentence accuracy,
insertion, deletion and substitution error rates
word sent ins del sub
acc(%) acc(%) (%) (%) (%)
single-best 93.5 78.7 2.0 0.8 3.6
N -best 96.1 87.0 1.2 0.3 2.2
pruned out. In the training, we set N=100 and
K=1, 000.
By using different objective translation eval-
uation metrics described in section 3, for each
model we obtained four sets of optimized pa-
rameters with respect to BLEU, NIST, mWER
and mPER metrics, respectively.
4.3 Translation Improvement by
Additional Features
All 508 utterances in the test data were used to
evaluate the models. Similar to processing the
development data, the speech recognizer gen-
erated N -best (N=100) recognition hypothe-
ses for each test speech utterance. Table 2
shows speech recognition results of the test data
set in single-best and N -best hypotheses. We
observed that over 8% sentence accuracy im-
provement was obtained from the single-best to
the N -best recognition hypotheses. The recog-
nized sentences were then translated into corre-
sponding English sentences. 1,000 such trans-
lation candidates were produced for each recog-
nition hypothesis. These candidates were then
rescored by each of the four models with four
sets of optimized parameters obtained in the
training respectively. The candidates with the
best score were chosen.
The best translations generated by a model
were evaluated by the translation assessment
metrics used to optimize the model parameters
in the development. The experimental results
are shown in Table 3.
In the experiments we changed the number
of speech recognition hypotheses, N , to see how
translation performance is changed as N . We
found that the best translation was achieved
when a relatively smaller set of hypotheses,
N=5, was used. Hence, the values in Table 3
were obtained when N was set to 5.
We test each model by employing the single-
best recognition hypothesis translations and
the N -best recognition hypothesis translations.
Table 3: Translation improvement from the
baseline model(stm) to the optimized enhanced
speech translation model(oestm): Models are
optimized using the same metric as shown in
the columns. Numbers are in percentage except
NIST score.
BLEU NIST mWER mPER
Single-best recognition hypothesis translation
stm 54.2 7.5 39.8 34.8
ostm 59.0 8.9 36.2 34.0
oetm 59.2 9.9 34.3 31.5
N -best recognition hypothesis translation
stm 55.5 7.3 39.8 35.4
ostm 61.1 8.8 36.4 33.9
oetm 61.1 10.0 34.0 31.1
oestm 62.1 10.2 33.7 29.4
The single-best translation was from the trans-
lation of the single best hypotheses of the speech
recognition and the N -best hypothesis trans-
lation was from the translations of all the hy-
potheses produced by speech recognition.
In Table 3, we observe that a large improve-
ment is achieved from the baseline model ?stm?
to the final model ?oestm?. The BLEU, NIST,
mWER, mPER scores are improved by 7.9%,
2.7, 6.1%, 5.4% respectively. Note that a high
value of BLEU and NIST score means a good
translation while a worse translation for mWER
and mPER. Consistent performance improve-
ment was achieved in the single-best and N -
best recognition hypotheses translations. We
observed that the improvement were due to the
following reasons:
? Optimization. Models with optimized pa-
rameters yielded a better translation than
the models with unoptimized parameters.
It can be seen by comparing the model
?stm? with the model ?ostm? for both the
single-best and the N -best results.
? N -best recognition hypotheses. In major-
ity of the cells in Table 3, translation per-
formance of the N -best recognition is bet-
ter than of the corresponding single-best
recognition. N -best BLEU score of ?ostm?
improved over the single-best of ?ostm? by
2.1%. However, NIST score is indifferent
to the change. It appears that NIST score
is insensitive to detect slight translation
changes.
Table 4: Translation improvement of incorrectly
recognized utterances from single-best(oetm) to
N -best(oestm)
BLEU NIST mWER mPER
single-best 29.0 6.1 59.7 51.8
N -best 36.3 7.2 54.4 47.9
? Enhanced features. Translation perfor-
mance is improved steadily when more fea-
tures are incorporated into the log-linear
models. Translation performance of model
?oetm? is better than model ?ostm? be-
cause more effective translation features
are used. Model ?oestm? is better than
model ?oetm? due to its enhanced speech
recognition features. It confirms that our
approach to integrate features from speech
recognition and translation features works
very well.
4.4 Recognition Improvement of
Incorrectly Recognized Sentences
In previous experiments we demonstrated that
speech translation performance was improved
by the proposed enhanced speech translation
model ?oestm?. In this section we want to show
that this improvement is because of the signifi-
cant improvement of incorrectly recognized sen-
tences when N -best recognition hypotheses are
used.
We carried out the following experiments.
Only incorrectly recognized sentences were ex-
tracted for translation and re-scored by the
model ?oetm? for the single-best case and the
model ?oestm? for the N -best case. The trans-
lation results are shown in Table 4. Translation
of incorrectly recognized sentences are improved
significantly as shown in the table.
Because we used N -best recognition hypothe-
ses, the log-linear model chose the recogni-
tion hypothesis among the N hypotheses which
yielded the best translation. As a result, speech
recognition could be improved if the higher ac-
curate recognition hypotheses was chosen for
translation. This effect can be observed clearly
if we extracted the chosen recognition hypothe-
ses of incorrectly recognized sentences. Table 5
shows the word accuracy and sentence accuracy
of the recognition hypotheses selected by the
translation module. The sentence accuracy of
incorrectly recognized sentences was improved
by 7.5%. The word accuracy was also improved.
Table 5: Recognition accuracy of incorrectly
recognized utterance improved by N -best hy-
pothesis translation.
word acc. (%) sent. acc. (%)
single-best 74.6 0
N -best BLEU 76.4 7.5
mWER 75.9 6.5
5 Discussions
As regards to integrating speech recognition
with translation, a coupling structure (Ney,
1999) was proposed as a speech translation in-
frastructure that multiplies acoustic probabili-
ties with translation probabilities in a one-step
decoding procedure. But no experimental re-
sults have been given on whether and how this
coupling structure improved speech translation.
(Casacuberta et al, 2002) used a finite-state
transducer where scores from acoustic infor-
mation sources and lexicon translation models
were integrated together. Word pairs of source
and target languages were tied in the decoding
graph. However, this method was only tested
for a pair of similar languages, i.e., Spanish to
English. For translating between languages of
different families where the syntactic structures
can be quite different, like Japanese and En-
glish, rigid tying of word pair still remains to be
shown its effectiveness for translation.
Our approach is rather general, easy to imple-
ment and flexible to expand. In the experiments
we incorporated features from acoustic models
and language models. But this framework is
flexible to include more effective features. In-
deed, the proposed speech translation paradigm
of log-linear models have been shown effective in
many applications (Beyerlein, 1998) (Vergyri,
2000) (Och, 2003).
In order to use speech recognition features,
the N -best speech recognition hypotheses were
needed. Using N -best could bear computing
burden. However, our experiments have shown
a smaller N seems to be adequate to achieve
most of the translation improvement without
significant increasing of computations.
6 Conclusion
In this paper we presented our approach of in-
corporating both speech recognition and ma-
chine translation features into a log-linear
speech translation model to improve speech
translation.
Under this new approach, translation perfor-
mance was significantly improved. The perfor-
mance improvement was confirmed by consis-
tent experimental results and measured by us-
ing various objective translation metrics. In
particular, BLEU score was improved by 7.9%
absolute.
We show that features derived from speech
recognition: likelihood of acoustic and language
models, helped improve speech translation. The
N -best recognition hypotheses are better than
the single-best ones when they are used in trans-
lation. We also show that N -best recogni-
tion hypothesis translation can improve speech
recognition accuracy of incorrectly recognized
sentences.
The success of the experiments owes to the
use of statistical machine translation and log-
linear models so that various of effective fea-
tures can be jointed and balanced to output the
optimal translation results.
Acknowledgments
We would like to thank for assistance from Ei-
ichiro Sumita, Yoshinori Sagisaka, Seiichi Ya-
mamoto and the anonymous reviewers.
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy of Japan entitled ?A study of speech dia-
logue translation technology based on a large
corpus?.
References
Peter Beyerlein. 1998. Discriminative model
combination. In Proc.of ICASSP?1998, vol-
ume 1, pages 481?484.
Peter F. Brown, Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta, Enrique Vidal, and
Juan M. Vilar. 2002. Architectures for
speech-to-speech translation using finite-state
models. In Proc. of speech-to-speech trans-
lation workshop, pages 39?44, Philadelphia,
PA, July.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech transla-
tion. In Proc.of EUROSPEECH?2003, pages
381?384, Geneva.
Daniel Marcu and William Wong. 2002. A
phrase-based, joint probability model for sta-
tistical machine translation. In Proc. of
EMNLP-2002, Philadelphia, PA, July.
Hermann Ney. 1999. Speech translation: Cou-
pling of recognition and translation. In Proc.
of ICASSP?1999, volume 1, pages 517?520,
Phoenix, AR, March.
Sonja Niessen, Franz J. Och, Gregor Leusch,
and Hermann Ney. 2000. An evaluation tool
for machine translation: Fast evaluation for
machine translation research. In Proc.of the
LREC (2000), pages 39?45, Athens, Greece,
May.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proc. of ACL?2003, pages 160?167.
Kishore A. Papineni, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of machine
translation. In Proc. of ACL?2002, pages
311?318, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky,
William T. Vetterling, and Brian P. Flan-
nery. 2000. Numerical Recipes in C++.
Cambridge University Press, Cambridge,
UK.
Nicola Ueffing, Franz Josef Och, and Hermann
Ney. 2002. Generation of word graphs in sta-
tistical machine translation. In Proc. of the
Conference on Empirical Methods for Natu-
ral Language Processing (EMNLP02), pages
156?163, Philadelphia, PA, July.
Dimitra Vergyri. 2000. Use of word level side
information to improve speech recognition. In
Proc. of the IEEE International Conference
on Acoustics, Speech and Signal Processing,
2000.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical ma-
chine translation. In Machine Translation
Summit IX, pages 410?417, New Orleans,
Louisiana.
Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan
Bakeoff
Ruiqiang Zhang1,2 and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
We created a new Chinese morpholog-
ical analyzer, Achilles, by integrating
rule-based, dictionary-based, and statis-
tical machine learning method, condi-
tional random fields (CRF). The rule-
based method is used to recognize regular
expressions: numbers, time and alphabets.
The dictionary-based method is used to
find in-vocabulary (IV) words while out-
of-vocabulary (OOV) words are detected
by the CRFs. At last, confidence measure
based approach is used to weigh all the re-
sults and output the best ones. Achilles
was used and evaluated in the bakeoff.
We participated the closed tracks of word
segmentation and part-of-speech tagging
for all the provided corpus. In spite of
an unexpected file encoding errors, the
system exhibited a top level performance.
A higher word segmentation accuracy for
the corpus ckip and ncc were achieved.
We are ranked at the fifth and eighth po-
sition out of all 19 and 26 submissions
respectively for the two corpus. Achilles
uses a feature combined approach for part-
of-speech tagging. Our post-evaluation re-
sults prove the effectiveness of this ap-
proach for POS tagging.
1 Introduction
Many approaches have been proposed in Chinese
word segmentation in the past decades. Segmen-
tation performance has been improved significantly,
from the earliest maximal match (dictionary-based)
approaches to HMM-based (Zhang et al, 2003) ap-
proaches and recent state-of-the-art machine learn-
ing approaches such as maximum entropy (Max-
Ent) (Xue and Shen, 2003), support vector ma-
chine (SVM) (Kudo and Matsumoto, 2001), con-
ditional random fields (CRF) (Peng and McCallum,
2004), and minimum error rate training (Gao et al,
2004). After analyzing the results presented in the
first and second Bakeoffs, (Sproat and Emerson,
2003) and (Emerson, 2005), we created a new Chi-
nese word segmentation system named as ?Achilles?
that consists of four modules mainly: Regular ex-
pression extractor, dictionary-based Ngram segmen-
tation, CRF-based subword tagging (Zhang et al,
2006), and confidence-based segmentation. Of the
four modules, the subword-based tagging, differing
from the existing character-based tagging, was pro-
posed in our work recently. We will give a detail de-
scription to this approach in the following sections.
In the followings, we illustrate our word seg-
mentation process in Section 2, where the subword-
based tagging is implemented by the CRFs method.
Section 3 illustrates our feature-based part-of-
speech tagging approach. Section 4 presents our ex-
perimental results. Section 5 describes current state-
of-the-art methods for Chinese word segmentation.
Section 6 provides the concluding remarks.
2 Introduction of main modules in
Achilles
The process of Achilles is illustrated in Fig. 1, where
three modules of Achilles are shown: a dictionary-
178
Sixth SIGHAN Workshop on Chinese Language Processing
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
input
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
Dictionary-based word segmentation
?%?,?,?2?2??%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Subword-based IOB tagging
?%?,?,?2?2??%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Confidence-based segmentation
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
output
Figure 1: Outline of word segmentation process
based N-gram word segmentation for segmenting IV
words, a subword-based tagging by the CRF for rec-
ognizing OOVs, and a confidence-dependent word
segmentation used for merging the results of both
the dictionary-based and the IOB tagging. An ex-
ample exhibiting each step?s results is also given in
the figure.
The rule-based regular expression is not shown in
the figure because this module interweaves with the
other modules. This module can be called if needed
at any time. The function of this module is to recog-
nize numerical, temporal expression and others like
product number, telephone number, credit number
or alphabets. For example, ?????(35,000)?,
???(August)?, ?0774731301?, ?George Bush?.
2.1 Dictionary-based N-gram word
segmentation
Dictionary-based N-gram word segmentation is an
important module for Achilles. This module can
achieve a very high R-iv, but no OOV detection.
We combined with it the N-gram language model
(LM) to solve segmentation ambiguities. For a given
Chinese character sequence, C = c0c1c2 . . . cN , the
problem of word segmentation can be formalized
as finding a word sequence, W = wt0wt1wt2 . . .wtM ,
which satisfies
wt0 = c0 . . . ct0 , wt1 = ct0+1 . . . ct1
wti = cti?1+1 . . . cti , wtM = ctM?1+1 . . . ctM
ti > ti?1, 0 ? ti ? N, 0 ? i ? M
such that
W = arg max
W
P(W |C) = arg max
W
P(W)P(C|W)
= arg max
W
P(wt0wt1 . . .wtM )?(c0 . . . ct0 ,wt0)
?(ct0+1 . . . ct1 ,wt1) . . . ?(ctM?1+1 . . . cM,wtM )
(1)
We applied Bayes? law in the above derivation.
Because the word sequence must keep consistent
with the character sequence, P(C|W) is expanded
to be a multiplication of a Kronecker delta function
series, ?(u, v), equal to 1 if both arguments are the
same and 0 otherwise.
Equation 1 indicates the process of dictionary-
based word segmentation. We looked up the lexicon
to find all the IVs, and evaluated the word sequences
with the LMs.
2.2 Subword-based IOB tagging using CRFs
If dictionary-based module recognizes IVs success-
fully, the subword-based IOB tagging can recog-
nize OOVs. Before the subword-based tagging,
the character-based ?IOB? tagging approach has
been widely used in Chinese word segmentation
recently (Xue and Shen, 2003; Peng and McCal-
lum, 2004; Tseng et al, 2005). Under the scheme,
each character of a word is labeled as ?B? if it is
the first character of a multiple-character word, or
?O? if the character functions as an independent
word, or ?I? otherwise.? For example, ??(whole)
???(Beijing city)? is labeled as ??(whole)/O
?(north)/B?(capital)/I?(city)/I?.
We proposed the subword-based tagging (Zhang
et al, 2006) to improve the existing character-based
tagging. The subword-based IOB tagging assigns
tags to a pre-defined lexicon subset consisting of
the most frequent multiple-character words in addi-
tion to single Chinese characters. If only Chinese
characters are used, the subword-based IOB tagging
is downgraded into a character-based one. Taking
the same example mentioned above, ??(whole)?
??(Beijing city)? is labeled as ??(whole)/O ?
?(Beijing)/B?(city)/I? in the subword-based tag-
ging, where ???(Beijing)/B? is labeled as one
unit.
We used the CRFs approach to train the IOB tag-
ger (Lafferty et al, 2001) on the training data. We
179
Sixth SIGHAN Workshop on Chinese Language Processing
downloaded and used the package ?CRF++? from
the site ?http://www.chasen.org/t?aku/software.? Ac-
cording to the CRFs, the probability of an IOB tag
sequence, T = t0t1 ? ? ? tM, given the word sequence,
W = w0w1 ? ? ?wM, is defined by
p(T |W) =
exp
????????
M?
i=1
????????
?
k
?k fk(ti?1, ti,W) +
?
k
?kgk(ti,W)
????????
???????? /Z,
Z =
?
T=t0t1???tM
p(T |W)
(2)
where we call fk(ti?1, ti,W) bigram feature functions
because the features trigger the previous observa-
tion ti?1 and current observation ti simultaneously;
gk(ti,W), the unigram feature functions because they
trigger only current observation ti. ?k and ?k are
the model parameters corresponding to feature func-
tions fk and gk respectively.
The model parameters were trained by maximiz-
ing the log-likelihood of the training data using L-
BFGS gradient descent optimization method. In
order to overcome overfitting, a gaussian prior was
imposed in the training.
The types of unigram features used in our experi-
ments included the following types:
w0,w?1,w1,w?2,w2,w0w?1,w0w1,w?1w1,
w?2w?1,w2w0
where w stands for word. The subscripts are po-
sition indicators. 0 means the current word; ?1,?2,
the first or second word to the left; 1, 2, the first or
second word to the right.
For the bigram features, we only used the previ-
ous and the current observations, t?1t0.
As to feature selection, we simply used absolute
counts for each feature in the training data. We de-
fined a cutoff value for each feature type and se-
lected the features with occurrence counts over the
cutoff.
A forward-backward algorithm was used in the
training and viterbi algorithm was used in the de-
coding.
2.3 Confidence-dependent word segmentation
Before moving to this step in Figure 1, we produced
two segmentation results: the one by the dictionary-
based approach and the one by the IOB tagging.
However, neither was perfect. The dictionary-based
segmentation produced results with higher R-ivs but
lower R-oovs while the IOB tagging yielded the con-
trary results. In this section we introduce a con-
fidence measure approach to combine the two re-
sults. We define a confidence measure, CM(tiob|w),
to measure the confidence of the results produced
by the IOB tagging by using the results from
the dictionary-based segmentation. The confidence
measure comes from two sources: IOB tagging and
dictionary-based word segmentation. Its calculation
is defined as:
CM(tiob|w) = ?CMiob(tiob|w) + (1 ? ?)?(tw, tiob)ng
(3)
where tiob is the word w?s IOB tag assigned by the
IOB tagging; tw, a prior IOB tag determined by the
results of the dictionary-based segmentation. After
the dictionary-based word segmentation, the words
are re-segmented into subwords by FMM before be-
ing fed to IOB tagging. Each subword is given a
prior IOB tag, tw. CMiob(t|w), a confidence probabil-
ity derived in the process of IOB tagging, is defined
as
CMiob(t|wi) =
?
T=t0t1???tM ,ti=t P(T |W,wi)?
T=t0t1???tM P(T |W)
where the numerator is a sum of all the observation
sequences with word wi labeled as t.
?(tw, tiob)ng denotes the contribution of the
dictionary-based segmentation. It is a Kronecker
delta function defined as
?(tw, tiob)ng = { 1 if tw = tiob0 otherwise
In Eq. 3, ? is a weighting between the IOB tag-
ging and the dictionary-based word segmentation.
We found the value 0.7 for ?, empirically.
By Eq. 3 the results of IOB tagging were re-
evaluated. A confidence measure threshold, t, was
defined for making a decision based on the value.
If the value was lower than t, the IOB tag was re-
jected and the dictionary-based segmentation was
used; otherwise, the IOB tagging segmentation was
used. A new OOV was thus created. For the two
extreme cases, t = 0 is the case of the IOB tagging
while t = 1 is that of the dictionary-based approach.
In a real application, a satisfactory tradeoff between
180
Sixth SIGHAN Workshop on Chinese Language Processing
R-ivs and R-oovs could find through tuning the con-
fidence threshold.
3 Part-of-speech Tagging
Our POS tagging is a traditional maximum entropy
tagging (A.Ratnaparkhi, 1996) as follows,
p(t|h) = 1Z(h)exp(
M?
i=1
?i fi(h, t)) (4)
where Z(h) is a normalizing factor determined by
requirement ?t p(t|h) = 1 over all t:
Z(h) =
?
t
exp(
M?
i=1
?i fi(h, t)) (5)
In the evaluation, 17 categories of triggers were
used, which include:
(w, t) , (w?2w?1w, t) , (w?1ww1, t) , (ww1w2, t) ,
(w?1w, t) , (ww1, t) , (t?1, t) , (t?2t?1, t) , (t?1w1, t),
(t?1ww1, t), (w?1w1, t) , (w?1, t) , (w1, t) , (t?1w, t),
(t?2t?1w, t) , (w?2w?1, t) , (w1w2, t)
where:
w is the word whose tag we are predicting; t is the
tag we are predicting; t?1 is the tag to the left of tag
t; t?2 is the tag to the left of tag t?1; w?1 is the word
to the left of word w; w?2 is the word to the left of
word w?1; w1 is the word to the right of word w; w2
is the word to the right of word w1 ;
In addition to the ME based POS tagging ap-
proach, we also combined a N-gram based POS tag-
ging.
N-gram tagger is the most widely used tagger in
part-of-speech tagging methods. The basic idea is
to maximize a posterior probability p(T |W) given a
word sequence in order to find its tag sequence. By
using Bayes rule, this can be transformed as to max-
imize p(T ) ? p(W |T ). Prior probability p(T ) is a N-
gram language model of tag sequence. p(W |T ) is
thought as an unigram model. In this experiment we
used trigram to model p(T ).
Differing from the interpolation smoothing al-
gorithm used in(Merialdo, 1994), both p(T ) and
p(W |T ) were smoothed by back-off methods(Katz,
1987). Because a N-gram backoff model P(T ) is
well-known, a backoff implementation of p(W |T )
was given here only. It is of the following equation.
R P F R-oov R-iv
CKIP 0.938 0.931 0.935 0.640 0.966
CITYU 0.943 0.933 0.938 0.686 0.965
CTB 0.941 0.943 0.942 0.663 0.961
NCC 0.931 0.933 0.932 0.592 0.950
SXU 0.932 0.929 0.930 0.487 0.971
Table 1: Post evaluation of word segmentation.
p(w|t) =
{ p?(w|t) if p?(w|t) , 0
?(t) p?(w) otherwise (6)
where:
- p?(w|t) and p?(w) are discounting relative fre-
quencies of p(w|t) and p(w), calculated by
back-off discounting algorithm. The discount
thresholds of p?(w|t) and p?(w) in present exper-
iment were 12 and 1 respectively. A new word
?UNK? was added to the vocabulary, whose
probability p?(w) represents that of all the un-
seen words.
- ?(t) is a normalizing value to ensure ?w p(w|t) =
1.
4 Experiments
We participated all the closed evaluation of word
segmentation and part-of-speech tagging. Our
scores should have achieved better than the official
numbers if we had submitted the results in the right
format. Achilles outputs results in GBK/BIG5 for-
mat. However, the format determined by bakeoff
organizers is Unicode-16. We made a lethal er-
ror when we converted the files from GBK/BIG5
to Unicode-16. Hence, the official results display
wrong scores for our system?s results.
We evaluated our results again in the post-
evaluation. The results for word segmentation is
shown in Table 1. The results for POS tagging is
shown in Table 2.
Table 1 and Table 2 represent the real perfor-
mance of Achilles in this evaluation. The official
data do not.
5 Discussion
Achilles achieved good word segmentation results
as shown in Table 1. Achilles was designed through
181
Sixth SIGHAN Workshop on Chinese Language Processing
Acc. R-oov R-iv
CKIP 0.913 0.530 0.946
CITYU 0.881 0.470 0.914
CTB 0.934 0.709 0.947
NCC 0.945 0.575 0.963
PKU 0.937 0.646 0.952
Table 2: Post evaluation of part-of-speech tagging.
three perspectives: IV recognition, OOV recogni-
tion and regular expression recognition. IV recogni-
tion can be solved at higher accuracy by dictionary-
based approach. OOV recognition can be solved
by IOB tagging. However, the flexible numerical
and temporal expression cannot be solved by the
above two methods. Hence, we used regular expres-
sion. Finally, the inconsistency of the above meth-
ods are resolved by confidence measure approach.
These features causes higher performance achieved
by Achilles.
6 Conclusions
This paper described systematically the main fea-
tures of our Chinese morphological analyzer,
Achilles. Because of its delicate design and state-of-
the-art technological integration, Achilles achieved
better or comparable segmentation results when it
was compared with the world best segmenter.
You can get Achilles from the site
?http://www.slc.atr.jp/?rzhang/Achilles.html?.
References
A.Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In ACL-2004,
Barcelona, July.
S. Katz. 1987. Estimation of probabilities for sparse data
for the language model component of a speech rec-
ognizer. IEEE Transactions on Acoustics Speech and
Signal Processing, 35:400?401.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machine. In Proc. of NAACL-2001,
pages 192?199.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 591?598.
B. Merialdo. 1994. Tagging english text with a proba-
bilistic model. Computational Linguistics, 20(2):155?
172.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562?568, Geneva, Switzerland.
Richard Sproat and Tom Emerson. 2003. The first inter-
national chinese word segmentation bakeoff. In Pro-
ceedings of the Second SIGHAN Workshop on Chinese
Language Processing, Sapporo, Japan, July.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, Jeju, Korea.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language Pro-
cessing.
Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, pages 184?
187.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proc. of HLT-
NAACL.
182
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193?196,
New York, June 2006. c?2006 Association for Computational Linguistics
Subword-based Tagging by Conditional Random Fields for Chinese Word
Segmentation
Ruiqiang Zhang1,2 and Genichiro Kikui? and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
We proposed two approaches to improve Chi-
nese word segmentation: a subword-based tag-
ging and a confidence measure approach. We
found the former achieved better performance
than the existing character-based tagging, and
the latter improved segmentation further by
combining the former with a dictionary-based
segmentation. In addition, the latter can be
used to balance out-of-vocabulary rates and
in-vocabulary rates. By these techniques we
achieved higher F-scores in CITYU, PKU and
MSR corpora than the best results from Sighan
Bakeoff 2005.
1 Introduction
The character-based ?IOB? tagging approach has been
widely used in Chinese word segmentation recently (Xue
and Shen, 2003; Peng and McCallum, 2004; Tseng
et al, 2005). Under the scheme, each character of a
word is labeled as ?B? if it is the first character of a
multiple-character word, or ?O? if the character func-
tions as an independent word, or ?I? otherwise.? For ex-
ample, ? (whole) (Beijing city)? is labeled as
? (whole)/O (north)/B (capital)/I (city)/I?.
We found that so far all the existing implementations
were using character-based IOB tagging. In this work
we propose a subword-based IOB tagging, which as-
signs tags to a pre-defined lexicon subset consisting of
the most frequent multiple-character words in addition to
single Chinese characters. If only Chinese characters are
used, the subword-based IOB tagging is downgraded into
a character-based one. Taking the same example men-
tioned above, ? (whole) (Beijing city)? is la-
beled as ? (whole)/O (Beijing)/B (city)/I? in the
subword-based tagging, where ? (Beijing)/B? is la-
beled as one unit. We will give a detailed description of
this approach in Section 2.
? Now the second author is affiliated with NTT.
In addition, we found a clear weakness with the IOB
tagging approach: It yields a very low in-vocabulary (IV)
rate (R-iv) in return for a higher out-of-vocabulary (OOV)
rate (R-oov). In the results of the closed test in Bakeoff
2005 (Emerson, 2005), the work of (Tseng et al, 2005),
using conditional random fields (CRF) for the IOB tag-
ging, yielded very high R-oovs in all of the four corpora
used, but the R-iv rates were lower. While OOV recog-
nition is very important in word segmentation, a higher
IV rate is also desired. In this work we propose a confi-
dence measure approach to lessen the weakness. By this
approach we can change R-oovs and R-ivs and find an
optimal tradeoff. This approach will be described in Sec-
tion 2.2.
In the followings, we illustrate our word segmentation
process in Section 2, where the subword-based tagging is
implemented by the CRFs method. Section 3 presents our
experimental results. Section 4 describes current state-
of-the-art methods for Chinese word segmentation, with
which our results were compared. Section 5 provides the
concluding remarks.
2 Our Chinese word segmentation process
Our word segmentation process is illustrated in Fig. 1. It
is composed of three parts: a dictionary-based N-gram
word segmentation for segmenting IV words, a subword-
based tagging by the CRF for recognizing OOVs, and a
confidence-dependent word segmentation used for merg-
ing the results of both the dictionary-based and the IOB
tagging. An example exhibiting each step?s results is also
given in the figure.
Since the dictionary-based approach is a well-known
method, we skip its technical descriptions. However,
keep in mind that the dictionary-based approach can pro-
duce a higher R-iv rate. We will use this advantage in the
confidence measure approach.
2.1 Subword-based IOB tagging using CRFs
There are several steps to train a subword-based IOB tag-
ger. First, we extracted a word list from the training data
sorted in decreasing order by their counts in the training
193
????????
+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
input
????????
+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
Dictionary-based word segmentation
?%?,?,?2?2??%?,
+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Subword-based IOB tagging
?%?,?,?2?2??%?,
+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Confidence-based segmentation
????????
+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
output
Figure 1: Outline of word segmentation process
data. We chose all the single characters and the top multi-
character words as a lexicon subset for the IOB tagging.
If the subset consists of Chinese characters only, it is a
character-based IOB tagger. We regard the words in the
subset as the subwords for the IOB tagging.
Second, we re-segmented the words in the training
data into subwords belonging to the subset, and assigned
IOB tags to them. For a character-based IOB tagger,
there is only one possibility of re-segmentation. How-
ever, there are multiple choices for a subword-based
IOB tagger. For example, ? (Beijing-city)? can
be segmented as ? (Beijing-city)/O,? or ?
(Beijing)/B (city)/I,? or ? (north)/B (capital)/I
(city)/I.? In this work we used forward maximal match
(FMM) for disambiguation. Of course, backward max-
imal match (BMM) or other approaches are also appli-
cable. We did not conduct comparative experiments be-
cause trivial differences of these approaches may not re-
sult in significant consequences to the subword-based ap-
proach.
In the third step, we used the CRFs approach to train
the IOB tagger (Lafferty et al, 2001) on the training data.
We downloaded and used the package ?CRF++? from the
site ?http://www.chasen.org/t?aku/software.? According to
the CRFs, the probability of an IOB tag sequence, T =
t0t1 ? ? ? tM , given the word sequence, W = w0w1 ? ? ?wM , is
defined by
p(T |W) =
exp
?
?
?
?
?
?
?
M
?
i=1
?
?
?
?
?
?
?
?
k
?k fk(ti?1, ti,W) +
?
k
?kgk(ti,W)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
/Z,
Z =
?
T=t0t1???tM
p(T |W)
(1)
where we call fk(ti?1, ti,W) bigram feature functions be-
cause the features trigger the previous observation ti?1
and current observation ti simultaneously; gk(ti,W), the
unigram feature functions because they trigger only cur-
rent observation ti. ?k and ?k are the model parameters
corresponding to feature functions fk and gk respectively.
The model parameters were trained by maximizing the
log-likelihood of the training data using L-BFGS gradi-
ent descent optimization method. In order to overcome
overfitting, a gaussian prior was imposed in the training.
The types of unigram features used in our experiments
included the following types:
w0,w?1,w1,w?2,w2,w0w?1,w0w1,w?1w1,w?2w?1,w2w0
where w stands for word. The subscripts are position in-
dicators. 0 means the current word; ?1, ?2, the first or
second word to the left; 1, 2, the first or second word to
the right.
For the bigram features, we only used the previous and
the current observations, t?1t0.
As to feature selection, we simply used absolute counts
for each feature in the training data. We defined a cutoff
value for each feature type and selected the features with
occurrence counts over the cutoff.
A forward-backward algorithm was used in the train-
ing and viterbi algorithm was used in the decoding.
2.2 Confidence-dependent word segmentation
Before moving to this step in Figure 1, we produced two
segmentation results: the one by the dictionary-based ap-
proach and the one by the IOB tagging. However, nei-
ther was perfect. The dictionary-based segmentation pro-
duced results with higher R-ivs but lower R-oovs while
the IOB tagging yielded the contrary results. In this sec-
tion we introduce a confidence measure approach to com-
bine the two results. We define a confidence measure,
CM(tiob|w), to measure the confidence of the results pro-
duced by the IOB tagging by using the results from the
dictionary-based segmentation. The confidence measure
comes from two sources: IOB tagging and dictionary-
based word segmentation. Its calculation is defined as:
CM(tiob|w) = ?CMiob(tiob|w) + (1 ? ?)?(tw, tiob)ng (2)
where tiob is the word w?s IOB tag assigned by the IOB
tagging; tw, a prior IOB tag determined by the results of
the dictionary-based segmentation. After the dictionary-
based word segmentation, the words are re-segmented
into subwords by FMM before being fed to IOB tagging.
Each subword is given a prior IOB tag, tw. CMiob(t|w), a
confidence probability derived in the process of IOB tag-
ging, is defined as
CMiob(t|wi) =
?
T=t0t1???tM ,ti=t P(T |W,wi)
?
T=t0t1???tM P(T |W)
where the numerator is a sum of all the observation se-
quences with word wi labeled as t.
194
?(tw, tiob)ng denotes the contribution of the dictionary-
based segmentation. It is a Kronecker delta function de-
fined as
?(tw, tiob)ng = {
1 if tw = tiob
0 otherwise
In Eq. 2, ? is a weighting between the IOB tagging
and the dictionary-based word segmentation. We found
the value 0.7 for ?, empirically.
By Eq. 2 the results of IOB tagging were re-evaluated.
A confidence measure threshold, t, was defined for mak-
ing a decision based on the value. If the value was lower
than t, the IOB tag was rejected and the dictionary-based
segmentation was used; otherwise, the IOB tagging seg-
mentation was used. A new OOV was thus created. For
the two extreme cases, t = 0 is the case of the IOB tag-
ging while t = 1 is that of the dictionary-based approach.
In a real application, a satisfactory tradeoff between R-
ivs and R-oovs could find through tuning the confidence
threshold. In Section 3.2 we will present the experimental
segmentation results of the confidence measure approach.
3 Experiments
We used the data provided by Sighan Bakeoff 2005 to
test our approaches described in the previous sections.
The data contain four corpora from different sources:
Academia Sinica (AS), City University of Hong Kong
(CITYU), Peking University (PKU) and Microsoft Re-
search in Beijing (MSR). Since this work was to evaluate
the proposed subword-based IOB tagging, we carried out
the closed test only. Five metrics were used to evaluate
segmentation results: recall(R), precision(P), F-score(F),
OOV rate(R-oov) and IV rate(R-iv). For detailed info. of
the corpora and these scores, refer to (Emerson, 2005).
For the dictionary-based approach, we extracted a
word list from the training data as the vocabulary. Tri-
gram LMs were generated using the SRI LM toolkit for
disambiguation. Table 1 shows the performance of the
dictionary-based segmentation. Since there were some
single-character words present in the test data but not in
the training data, the R-oov rates were not zero in this
experiment. In fact, there were no OOV recognition.
Hence, this approach produced lower F-scores. However,
the R-ivs were very high.
3.1 Effects of the Character-based and the
subword-based tagger
The main difference between the character-based and the
word-based is the contents of the lexicon subset used
for re-segmentation. For the character-based tagging, we
used all the Chinese characters. For the subword-based
tagging, we added another 2000 most frequent multiple-
character words to the lexicons for tagging. The segmen-
tation results of the dictionary-based were re-segmented
R P F R-oov R-iv
AS 0.941 0.881 0.910 0.038 0.982
CITYU 0.928 0.851 0.888 0.164 0.989
PKU 0.948 0.912 0.930 0.408 0.981
MSR 0.968 0.927 0.947 0.048 0.993
Table 1: Our segmentation results by the dictionary-
based approach for the closed test of Bakeoff 2005, very
low R-oov rates due to no OOV recognition applied.
R P F R-oov R-iv
AS 0.951 0.942 0.947 0.678 0.964
0.953 0.940 0.947 0.647 0.967
CITYU 0.939 0.943 0.941 0.700 0.958
0.950 0.942 0.946 0.736 0.967
PKU 0.940 0.950 0.945 0.783 0.949
0.943 0.946 0.945 0.754 0.955
MSR 0.957 0.960 0.959 0.710 0.964
0.965 0.963 0.964 0.716 0.972
Table 2: Segmentation results by a pure subword-based
IOB tagging. The upper numbers are of the character-
based and the lower ones, the subword-based.
using the FMM, and then labeled with ?IOB? tags by the
CRFs. The segmentation results using CRF tagging are
shown in Table 2, where the upper numbers of each slot
were produced by the character-based approach while the
lower numbers were of the subword-based. We found
that the proposed subword-based approaches were effec-
tive in CITYU and MSR corpora, raising the F-scores
from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for
MSR corpus. There were no F-score changes for AS and
PKU corpora, but the recall rates were improved. Com-
paring Table 1 and 2, we found the CRF-modeled IOB
tagging yielded better segmentation than the dictionary-
based approach. However, the R-iv rates were getting
worse in return for higher R-oov rates. We will tackle
this problem by the confidence measure approach.
3.2 Effect of the confidence measure
In section 2.2, we proposed a confidence measure ap-
proach to re-evaluate the results of IOB tagging by com-
binations of the results of the dictionary-based segmen-
tation. The effect of the confidence measure is shown in
Table 3, where we used ? = 0.7 and confidence threshold
t = 0.8. In each slot, the numbers on the top were of the
character-based approach while the numbers on the bot-
tom were the subword-based. We found the results in Ta-
ble 3 were better than those in Table 2 and Table 1, which
prove that using confidence measure approach achieved
the best performance over the dictionary-based segmen-
tation and the IOB tagging approach. The act of con-
fidence measure made a tradeoff between R-ivs and R-
oovs, yielding higher R-oovs than Table 1 and higher R-
195
R P F R-oov R-iv
AS 0.953 0.944 0.948 0.607 0.969
0.956 0.947 0.951 0.649 0.969
CITYU 0.943 0.948 0.946 0.682 0.964
0.952 0.949 0.951 0.741 0.969
PKU 0.942 0.957 0.949 0.775 0.952
0.947 0.955 0.951 0.748 0.959
MSR 0.960 0.966 0.963 0.674 0.967
0.972 0.969 0.971 0.712 0.976
Table 3: Effects of combination using the confidence
measure. The upper numbers and the lower numbers are
of the character-based and the subword-based, respec-
tively
AS CITYU MSR PKU
Bakeoff-best 0.952 0.943 0.964 0.950
Ours 0.951 0.951 0.971 0.951
Table 4: Comparison our results with the best ones from
Sighan Bakeoff 2005 in terms of F-score
ivs than Table 2.
Even with the use of confidence measure, the word-
based IOB tagging still outperformed the character-based
IOB tagging. It proves the proposed word-based IOB tag-
ging was very effective.
4 Discussion and Related works
The IOB tagging approach adopted in this work is not a
new idea. It was first used in Chinese word segmentation
by (Xue and Shen, 2003), where maximum entropy meth-
ods were used. Later, this approach was implemented
by the CRF-based method (Peng and McCallum, 2004),
which was proved to achieve better results than the maxi-
mum entropy approach because it can solve the label bias
problem (Lafferty et al, 2001).
Our main contribution is to extend the IOB tagging ap-
proach from being a character-based to a subword-based.
We proved the new approach enhanced the word segmen-
tation significantly. Our results are listed together with
the best results from Bakeoff 2005 in Table 4 in terms
of F-scores. We achieved the highest F-scores in CITYU,
PKU and MSR corpora. We think our proposed subword-
based tagging played an important role for the good re-
sults. Since it was a closed test, some information such
as Arabic and Chinese number and alphabetical letters
cannot be used. We could yield a better results than those
shown in Table 4 using such information. For example,
inconsistent errors of foreign names can be fixed if al-
phabetical characters are known. For AS corpus, ?Adam
Smith? are two words in the training but become a one-
word in the test, ?AdamSmith?. Our approaches pro-
duced wrong segmentations for labeling inconsistency.
Another advantage of the word-based IOB tagging
over the character-based is its speed. The subword-based
approach is faster because fewer words than characters
were labeled. We found a speed up both in training and
test.
The idea of using the confidence measure has appeared
in (Peng and McCallum, 2004), where it was used to rec-
ognize the OOVs. In this work we used it more delicately.
By way of the confidence measure we combined results
from the dictionary-based and the IOB-tagging-based and
as a result, we could achieve the optimal performance.
5 Conclusions
In this work, we proposed a subword-based IOB tagging
method for Chinese word segmentation. Using the CRFs
approaches, we prove that it outperformed the character-
based method using the CRF approaches. We also suc-
cessfully employed the confidence measure to make a
confidence-dependent word segmentation. This approach
is effective for performing desired segmentation based on
users? requirements to R-oov and R-iv.
Acknowledgements
The authors appreciate the reviewers? effort and good ad-
vice for improving the paper.
References
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 591?598.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562?568, Geneva, Switzerland.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bakeoff
2005. In Proceedings of the Fourth SIGHANWorkshop
on Chinese Language Processing, Jeju, Korea.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language Pro-
cessing.
196
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961?968,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Subword-based Tagging for Confidence-dependent Chinese Word
Segmentation
Ruiqiang Zhang1,2 and Genichiro Kikui? and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
We proposed a subword-based tagging for
Chinese word segmentation to improve
the existing character-based tagging. The
subword-based tagging was implemented
using the maximum entropy (MaxEnt)
and the conditional random fields (CRF)
methods. We found that the proposed
subword-based tagging outperformed the
character-based tagging in all compara-
tive experiments. In addition, we pro-
posed a confidence measure approach to
combine the results of a dictionary-based
and a subword-tagging-based segmenta-
tion. This approach can produce an
ideal tradeoff between the in-vocaulary
rate and out-of-vocabulary rate. Our tech-
niques were evaluated using the test data
from Sighan Bakeoff 2005. We achieved
higher F-scores than the best results in
three of the four corpora: PKU(0.951),
CITYU(0.950) and MSR(0.971).
1 Introduction
Many approaches have been proposed in Chinese
word segmentation in the past decades. Segmen-
tation performance has been improved significantly,
from the earliest maximal match (dictionary-based)
approaches to HMM-based (Zhang et al, 2003) ap-
proaches and recent state-of-the-art machine learn-
ing approaches such as maximum entropy (Max-
Ent) (Xue and Shen, 2003), support vector machine
?Now the second author is affiliated with NTT.
(SVM) (Kudo and Matsumoto, 2001), conditional
random fields (CRF) (Peng and McCallum, 2004),
and minimum error rate training (Gao et al, 2004).
By analyzing the top results in the first and second
Bakeoffs, (Sproat and Emerson, 2003) and (Emer-
son, 2005), we found the top results were produced
by direct or indirect use of so-called ?IOB? tagging,
which converts the problem of word segmentation
into one of character tagging so that part-of-speech
tagging approaches can be used for word segmen-
tation. This approach was also called ?LMR? (Xue
and Shen, 2003) or ?BIES? (Asahara et al, 2005)
tagging. Under the scheme, each character of a
word is labeled as ?B? if it is the first character of a
multiple-character word, or ?I? otherwise, and ?O?
if the character functioned as an independent word.
For example, ??(whole) ???(Beijing city)? is
labeled as ??/O?/B?/I?/I?. Thus, the training
data in word sequences are turned into IOB-labeled
data in character sequences, which are then used as
the training data for tagging. For new test data, word
boundaries are determined based on the results of
tagging.
While the IOB tagging approach has been widely
used in Chinese word segmentation, we found that
so far all the existing implementations were using
character-based IOB tagging. In this work we pro-
pose a subword-based IOB tagging, which assigns
tags to a pre-defined lexicon subset consisting of the
most frequent multiple-character words in addition
to single Chinese characters. If only Chinese char-
acters are used, the subword-based IOB tagging is
downgraded to a character-based one. Taking the
same example mentioned above, ?????? is la-
961
beled as ??/O ??/B ?/I? in the subword-based
tagging, where ???/B? is labeled as one unit. We
will give a detailed description of this approach in
Section 2.
There exists a clear weakness with the IOB tag-
ging approach: It yields a very low in-vocabulary
rate (R-iv) in return for a higher out-of-vocabulary
(OOV) rate (R-oov). In the results of the closed
test in Bakeoff 2005 (Emerson, 2005), the work
of (Tseng et al, 2005), using CRFs for the IOB tag-
ging, yielded a very high R-oov in all of the four
corpora used, but the R-iv rates were lower. While
OOV recognition is very important in word segmen-
tation, a higher IV rate is also desired. In this work
we propose a confidence measure approach to lessen
this weakness. By this approach we can change the
R-oov and R-iv and find an optimal tradeoff. This
approach will be described in Section 2.3.
In addition, we illustrate our word segmentation
process in Section 2, where the subword-based tag-
ging is described by the MaxEnt method. Section 3
presents our experimental results. The effects using
the MaxEnts and CRFs are shown in this section.
Section 4 describes current state-of-the-art methods
with Chinese word segmentation, with which our re-
sults were compared. Section 5 provides the con-
cluding remarks and outlines future goals.
2 Chinese word segmentation framework
Our word segmentation process is illustrated in
Fig. 1. It is composed of three parts: a dictionary-
based N-gram word segmentation for segmenting IV
words, a maximum entropy subword-based tagger
for recognizing OOVs, and a confidence-dependent
word disambiguation used for merging the results
of both the dictionary-based and the IOB-tagging-
based. An example exhibiting each step?s results is
also given in the figure.
2.1 Dictionary-based N-gram word
segmentation
This approach can achieve a very high R-iv, but no
OOV detection. We combined with it the N-gram
language model (LM) to solve segmentation ambi-
guities. For a given Chinese character sequence,
C = c0c1c2 . . . cN , the problem of word segmenta-
tion can be formalized as finding a word sequence,
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
input
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
Dictionary-based word segmentation
?%?,?,?2?2??%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Subword-based IOB tagging
?%Proceedings of the ACL 2007 Demo and Poster Sessions, pages 181?184,
Prague, June 2007. c?2007 Association for Computational Linguistics
Boosting Statistical Machine Translation by Lemmatization and Linear
Interpolation
Ruiqiang Zhang1,2 and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
Data sparseness is one of the factors that de-
grade statistical machine translation (SMT).
Existing work has shown that using morpho-
syntactic information is an effective solu-
tion to data sparseness. However, fewer ef-
forts have been made for Chinese-to-English
SMT with using English morpho-syntactic
analysis. We found that while English is
a language with less inflection, using En-
glish lemmas in training can significantly
improve the quality of word alignment that
leads to yield better translation performance.
We carried out comprehensive experiments
on multiple training data of varied sizes to
prove this. We also proposed a new effec-
tive linear interpolation method to integrate
multiple homologous features of translation
models.
1 Introduction
Raw parallel data need to be preprocessed in the
modern phrase-based SMT before they are aligned
by alignment algorithms, one of which is the well-
known tool, GIZA++ (Och and Ney, 2003), for
training IBM models (1-4). Morphological analy-
sis (MA) is used in data preprocessing, by which the
surface words of the raw data are converted into a
new format. This new format can be lemmas, stems,
parts-of-speech and morphemes or mixes of these.
One benefit of using MA is to ease data sparseness
that can reduce the translation quality significantly,
especially for tasks with small amounts of training
data.
Some published work has shown that apply-
ing morphological analysis improved the quality of
SMT (Lee, 2004; Goldwater and McClosky, 2005).
We found that all this earlier work involved exper-
iments conducted on translations from highly in-
flected languages, such as Czech, Arabic, and Span-
ish, to English. These researchers also provided de-
tailed descriptions of the effects of foreign language
morpho-syntactic analysis but presented no specific
results to show the effect of English morphologi-
cal analysis. To the best of our knowledge, there
have been no papers related to English morpholog-
ical analysis for Chinese-to-English (CE) transla-
tions even though the CE translation has been the
main track for many evaluation campaigns includ-
ing NIST MT, IWSLT and TC-STAR, where only
simple tokenization or lower-case capitalization has
been applied to English preprocessing. One possi-
ble reason why English morphological analysis has
been neglected may be that English is less inflected
to the extent that MA may not be effective. How-
ever, we found this assumption should not be taken-
for-granted.
We studied what effect English lemmatization had
on CE translation. Lemmatization is shallow mor-
phological analysis, which uses a lexical entry to re-
place inflected words. For example, the three words,
doing, did and done, are replaced by one word, do.
They are all mapped to the same Chinese transla-
tions. As a result, it eases the problem with sparse
data, and retains word meanings unchanged. It is
not impossible to improve word alignment by using
English lemmatization.
We determined what effect lemmatization had in
experiments using data from the BTEC (Paul, 2006)
CSTAR track. We collected a relatively large cor-
pus of more than 678,000 sentences. We conducted
comprehensive evaluations and used multiple trans-
181
lation metrics to evaluate the results. We found that
our approach of using lemmatization improved both
the word alignment and the quality of SMT with
a small amounts of training data, and, while much
work indicates that MA is useless in training large
amounts of data (Lee, 2004), our intensive exper-
iments proved that the chance to get a better MT
quality using lemmatization is higher than that with-
out it for large amounts of training data.
On the basis of successful use of lemmatization
translation, we propose a new linear interpolation
method by which we integrate the homologous fea-
tures of translation models of the lemmatization and
non-lemmatization system. We found the integrated
model improved all the components? performance in
the translation.
2 Moses training for system with
lemmatization and without
We used Moses to carry out the expriments. Moses
is the state of the art decoder for SMT. It is an ex-
tension of Pharaoh (Koehn et al, 2003), and sup-
ports factor training and decoding. Our idea can
be easily implemented by Moses. We feed Moses
English words with two factors: surface word and
lemma. The only difference in training with lemma-
tization from that without is the alignment factor.
The former uses Chinese surface words and English
lemmas as the alignment factor, but the latter uses
Chinese surface words and English surface words.
Therefore, the lemmatized English is only used in
word alignment. All the other options of Moses are
same for both the lemmatization translation and non-
lemmatization translation.
We use the tool created by (Minnen et al, 2001) to
complete the morphological analysis of English. We
had to make an English part-of-speech (POS) tag-
ger that is compatible with the CLAWS-5 tagset to
use this tool. We use our in-house tagset and En-
glish tagged corpus to train a statistical POS tagger
by using the maximum entropy principle. Our tagset
contains over 200 POS tags, most of which are con-
sistent to the CLAWS-5. The tagger achieved 93.7%
accuracy for our test set.
We use the default features defined by Pharaoh
in the phrase-based log-linear models i.e., a target
language model, five translation models, and one
distance-based distortion model. The weighting pa-
rameters of these features were optimized in terms
of BLEU by the approach of minimum error rate
training (Och, 2003).
The data for training and test are from the
IWSLT06 CSTAR track that uses the Basic Travel
Expression Corpus (BTEC). The BTEC corpus are
relatively larger corpus for travel domain. We use
678,748 Chinese/English parallel sentences as the
training data in the experiments. The number of
words are about 3.9M and 4.4M for Chinese and En-
glish respectively. The number of unique words for
English is 28,709 before lemmatization and 24,635
after lemmatization. A 15%-20% reduction in vo-
cabulary is obtained by the lemmatization. The test
data are the one used in IWSLT06 evaluation. It
contains 500 Chinese sentences. The test data of
IWSLT05 are the development data for tuning the
weighting parameters. Multiple references are used
for computing the automatic metrics.
3 Experiments
3.1 Regular test
The purpose of the regular tests is to find what ef-
fect lemmatization has as the amount of training
data increases. We used the data from the IWSLT06
CSTAR track. We started with 50,000 (50 K) of
data, and gradually added more training data from
a 678 K corpus to this. We applied the methods
in Section 2 to train the non-lemmatized translation
and lemmatized translation systems. The results are
listed in Table 1. We use the alignment error rate
(AER) to measure the alignment performance, and
the two popular automatic metric, BLEU1 and ME-
TEOR2 to evaluate the translations. To measure the
word alignment, we manually aligned 100 parallel
sentences from the BTEC as the reference file. We
use the ?sure? links and the ?possible? links to de-
note the alignments. As shown in Table 1, we found
our approach improved word alignment uniformly
from small amounts to large amounts of training
data. The maximal AER reduction is up to 27.4%
for the 600K. However, we found some mixed trans-
lation results in terms of BLEU. The lemmatized
1http://domino.watson.ibm.com/library/CyberDig.nsf (key-
word=RC22176)
2http://www.cs.cmu.edu/?alavie/METEOR
182
Table 1: Translation results as increasing amount of training
data in IWSLT06 CSTAR track
System AER BLEU METEOR
50K nonlem 0.217 0.158 0.427
lemma 0.199 0.167 0.431
100K nonlem 0.178 0.182 0.457
lemma 0.177 0.188 0.463
300K nonlem 0.150 0.223 0.501
lemma 0.132 0.217 0.505
400K nonlem 0.136 0.231 0.509
lemma 0.102 0.224 0.507
500K nonlem 0.119 0.235 0.519
lemma 0.104 0.241 0.522
600K nonlem 0.095 0.238 0.535
lemma 0.069 0.248 0.536
Table 2: Statistical significance test in terms of BLEU:
sys1=non-lemma, sys2=lemma
Data size Diff(sys1-sys2)
50K -0.092 [-0.0176,-0.0012]
100K -0.006 [-0.0155,0.0039]
300K 0.0057 [-0.0046,0.0161]
400K 0.0074 [-0.0023,0.0174]
500K -0.0054 [-0.0139,0.0035]
600K -0.0103 [-0.0201,-0.0006]
translations did not outperform the non-lemmatized
ones uniformly. They did for small amounts of data,
i.e., 50 K and 100 K, and for large amounts, 500 K
and 600 K. However, they failed for 300 K and 400
K.
The translations were under the statistical signif-
icance test by using the bootStrap scripts3. The re-
sults giving the medians and confidence intervals are
shown in Table 2, where the numbers indicate the
median, the lower and higher boundary at 95% con-
fidence interval. we found the lemma systems were
confidently better than the nonlem systems for the
50K and 600K, but didn?t for other data sizes.
This experiments proved that our proposed ap-
proach improved the qualities of word alignments
that lead to the translation improvement for the 50K,
100K, 500K and 600K. In particular, our results
revealed large amounts of data of 500 K and 600
3http://projectile.is.cs.cmu.edu/research/public/tools/bootStrap
/tutorial.htm
Table 3: Competitive scores (BLEU) for non-lemmatization and
lemmatization using randomly extracted corpora
System 100K 300K 400K 600K total
lemma 10/11 5.5/11 6.5/11 5/7 27/40
nonlem 1/11 5.5/11 4.5/11 2/7 13/40
K was improved by the lemmatization while it has
been found impossible in most published results.
However, data of 300 K and 400 K worsen trans-
lations achieved by the lemmatization4. In what fol-
lows, we discuss a method of random sampling of
creating multiple corpora of varied sizes to see ro-
bustness of our approach and re-investigate the re-
sults of the 300K and 400K.
3.2 Random sampling test
In this section, we use a method of random extrac-
tion to generate new multiple training data for each
corpus of one definite size. The new data are ex-
tracted from the whole corpus of 678 K randomly.
We generate ten new corpora for 100 K, 300 K,
and 400 K data and six new corpora for the 678 K
data. Thus, we create eleven and seven corpora of
varied sizes if the corpora in the last experiments
are counted. We use the same method as in Sec-
tion 2 for each generated corpus to construct sys-
tems to compare non-lemmatization and lemmati-
zation. The systems are evaluated again using the
same test data. The results are listed in Table 3
and Figure 1. Table 3 shows the ?scoreboard? of
non-lemmatized and lemmatized results in terms of
BLEU. If its score for the lemma system is higher
than that for the nonlem system, the former earns
one point; if equal, each earns 0.5; otherwise, the
nonlem earns one point. As we can see from the ta-
ble, the results for the lemma system are better than
those for the nonlem system for the 100K in 10 of
the total 11 corpora. Of the total 40 random corpora,
the lemma systems outperform the nonlem systems
in 27 times.
By analyzing the results from Tables 1 and 3, we
can arrive at some conclusions. The lemma systems
outperform the nonlem for training corpora less than
4while the results was not confident by statistical signifi-
cance test, the medians of 300K and 400K were lowered by
the lemmatization
183
0.16
0.25 NL-600K
L-600K
NL-400K
L-400K
NL-300K
L-300K
NL-100K
L-100K
1110987654321
0.169
0.178
0.187
0.196
0.205
0.214
0.223
0.232
0.241
BLEU
Number of randomly extracted corpora
Figure 1: Bleu scores for randomly extracted corpora
100 K. The BLEU score favors the lemma system
overwhelmingly for this size. When the amount of
training data is increased up to 600 K, the lemma
still beat the nonlem system in most tests while the
number of success by the nonlem system increases.
This random test, as a complement to the last ex-
periment, reveals that the lemma either performs the
same or better than the nonlem system for training
data of any size. Therefore, the lemma system is
slightly better than the nonlem in general.
Figure 1 illustrates the BLEU scores for the
?lemma(L)? and ?nonlem(NL)? systems for ran-
domly extracted corpora. A higher number of points
is obtained by the lemma system than the nonlem for
each corpus.
4 Effect of linear interpolation of features
We generated translation models for lemmatization
translation and non-lemmatization translation. We
found some features of the translation models could
be added linearly. For example, phrase translation
model p(e| f ) can be calculated as,
p(e| f ) = ?1 pl(e| f ) + ?2 pnl(e| f )
where pl(e| f ) and pnl(e| f ) is the phrase translation
models corresponding to the lemmatization system
and non-lemma system. ?1 + ?2 = 1. ?s can be
obtained by maximizing likelihood or BLEU scores
of a development data. But we used the same val-
ues for all the ?. p(e| f ) is the phrase translation
model after linear interpolation. Besides the phrase
translation model, we used this approach to integrate
Table 4: Effect of linear interpolation
lemma nonlemma interpolation
open track 0.1938 0.1993 0.2054
the three other features: phrase inverse probability,
lexical probability, and lexical inverse probability.
We tested this integration using the open track of
IWSLT 2006, a small task track. The BLEU scores
are shown in Table 4. An improvement over both of
the systems were observed.
5 Conclusions
We proposed a new approach of using lemmatiza-
tion and linear interpolation of homologous features
in SMT. The principal idea is to use lemmatized En-
glish for the word alignment. Our approach was
proved effective for the BTEC Chinese to English
translation. It is significant in particular that we
have target language, English, as the lemmatized ob-
ject because it is less usual in SMT. Nevertheless,
we found our approach significantly improved word
alignment and qualities of translations.
References
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proceedings of HLT/EMNLP, pages 676?683,
Vancouver, British Columbia, Canada, October.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127?133.
Young-Suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL 2004: Short
Papers, pages 57?60, Boston, Massachusetts, USA.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Natural
Language Engineering, 7(3):207?223.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL 2003, pages
160?167.
Michael Paul. 2006. Overview of the IWSLT 2006 Eval-
uation Campaign. In Proc. of the IWSLT, pages 1?15,
Kyoto, Japan.
184
Proceedings of the Third Workshop on Statistical Machine Translation, pages 216?223,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improved Statistical Machine Translation by Multiple Chinese Word
Segmentation
Ruiqiang Zhang1,2 and Keiji Yasuda1,2 and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Science City, Kyoto, 619-0288, Japan
{ruiqiang.zhang,keiji.yasuda,eiichiro.sumita}@atr.jp
Abstract
Chinese word segmentation (CWS) is a
necessary step in Chinese-English statisti-
cal machine translation (SMT) and its per-
formance has an impact on the results of
SMT. However, there are many settings in-
volved in creating a CWS system such as
various specifications and CWS methods.
This paper investigates the effect of these
settings to SMT. We tested dictionary-
based and CRF-based approaches and
found there was no significant difference
between the two in the qualty of the result-
ing translations. We also found the corre-
lation between the CWS F-score and SMT
BLEU score was very weak. This paper
also proposes two methods of combining
advantages of different specifications: a
simple concatenation of training data and
a feature interpolation approach in which
the same types of features of translation
models from various CWS schemes are
linearly interpolated. We found these ap-
proaches were very effective in improving
quality of translations.
1 Introduction
Chinese word segmentation (CWS) is a necessary
step in Chinese-English statistical machine transla-
tion (SMT). The research on CWS independently
from SMT has been conducted for decades. As an
evidence, the CWS evaluation campaign, the Sighan
Bakeoff (Emerson, 2005),1, has been held four times
since 2004. However, works on relations between
CWS and SMT are scarce.
Generally, two factors need to be considered in
constructing a CWS system. The first one is the
specifications for CWS, i.e., the rules or guidelines
for word segmentation, and the second one is the
CWS methods. There are many CWS specifications
used by different organizations. Unfortunately, these
organizations do not seem to have any intention of
reaching a unified specification. More than five or
six specifications have been used in the four Sighan
Bakeoffs. There is also significant disagreement on
the specifications, although much of their contents is
the same. One of the aims of this work was therefore
to establish whether inconsistencies in specifications
significantly affect the quality of SMT.
The second factor is CWS methods. We grouped
all of the CWS methods into two classes: the class
without out-of-vocabulary (OOV) recognition and
the class with OOV recognition, represented by the
dictionary-based CWS and the CRF-based CWS, re-
spectively. Out-of-vocabulary recognition may have
two-sided effects on SMT performance. The CRF-
based CWS that supports OOV recognition produces
word segmentations with a higher F-score, but a
huge number of new words recognized correctly and
incorrectly that can incur data sparseness in training
the SMT models. On the other hand, the dictionary-
based approach that does not support OOV recogni-
tion produced a lower F-score, but with a relatively
weak data spareness problem. Which approach pro-
1A CWS competition organized by the ACL special interest
group on Chinese.
216
Table 1: Examples of disagreement in segmentation guidelines
ChineseName EnglishName Time
AS DENGXIAOPING GEORGE BUSH 1997YEAR 7MONTH 1DAY
CITYU DENGXIAOPING GEORGEBUSH 1997 YEAR 7 MONTH 1 DAY
MSR DENGXIAOPING GEORGEBUSH 1997YEAR7MONTH1DAY
PKU DENG XIAOPING GEORGEBUSH 1997YEAR 7MONTH 1DAY
Table 2: A second example of disagreement in segmentation guidelines
Composite words Composite words
AS FUJITSUCOMPANY EUROZONE
CITYU FUJITSU COMPANY EUROZONE
MSR FUJITSUCOMPANY EURO ZONE
PKU FUJITSU COMPANY EUROZONE
duces a better SMT result is our research interest in
this work.
The performance of CWS is usually measured by
the F-score, while that of SMT is measured using
the BLEU score. Does a CWS with a higher F-
score produce a better translation? In this paper
we answer this question by comparing F-scores with
BLEU scores.
In this work, we also propose approaches to make
use of all the Sighan training data regardless of the
specifications. Two methods are proposed: (1) a
simple combination of all the training data, and (2)
implementing linear interpolation of multiple trans-
lation models. Linear interpolation is widely used in
language modeling for speech recognition. We in-
terpolated multiple translation models generated by
the CWS schemes and found our approaches were
very effective in improving the translations.
2 CWS specifications and corpora from
the second Sighan Bakeoff
A Chinese word is composed of one or more char-
acters. There are no spaces between the words.
Automatic word segmentation is required for ma-
chine translation. Usually a specification is needed
to carry out word segmentation. Unfortunately, there
are many different versions of specifications. Differ-
ent tasks give rise to different requirements and the
CWS specifications must be adjusted accordingly.
For example, shorter segmentation has been shown
to be better for speech recognition. A composite
word (numbers, dates, times, etc.) is split into char-
acters even if it is one word defined by linguists. In
contrast, longer segmentation is preferred for named
entity recognition consisting of longer character se-
quences, such as the name of people, places, and or-
ganizations.
This work investigated four well-known spec-
ifications created by four different organizations:
Academia Sinica (AS), City University of Hong
Kong (CITYU), Microsoft Research (Beijing)
(MSR), and Beijing University (PKU). These specs
were used in the second Sighan Bakeoff (Emerson,
2005). When we compared the four specifications
and the manual segmentations in the Sighan Bakeoff
training data, we found there were many inconsis-
tencies among the four specifications. Some exam-
ples are shown in Table 1 and 2. For instance, the
AS and PKU specifications are distinct in splitting
both Chinese and English names. We also found the
MSR specification generated more composite words
and grouped longer character sequences into a word.
Using this specification could generate tens of thou-
sands of new words, which can cause data sparse-
ness for SMT.
In addition to using the four specifications, we
also downloaded the training and test corpora of the
second Sighan Bakeoff. We used each of the train-
ing corpora provided to create a CWS scheme and
evaluated the performance of the schemes on our test
217
data. This enabled us to examine the effect of CWS
specifications on SMT.
We used a Chinese word segmentation tool,
Achilles, to implement word segmentation. Part of
the work using this tool was described by (Zhang
et al, 2006). The approach was reported to achieve
the highest word segmentation accuracy using the
data from the second Sighan Bakeoff. Moreover,
this tool meets our need to test the effect of the two
kinds of CWS approaches for SMT. We can easily
train a dictionary-based and a CRF-based CWS by
using this tool. By turning the program?s option for
the CRF model on and off, we can use the Achilles
as a dictionary-based approach and as a CRF-based
CWS. In fact, the dictionary-based approach is the
default approach for Achilles.
3 Experiments
3.1 SMT resources
We followed the instructions for the 2005 NIST MT
evaluation campaign. Training the translation mod-
els for our SMT system used the available LDC par-
allel data except the UN corpus. To train the lan-
guage models for English, we used all the avail-
able English parallel data plus Xinhua News of the
LDC Gigaword English corpus, LDC2005T12. In
summary, we used 2.4 million parallel sentences for
training the translation model. We used the test data
defined in the NIST MT05 evaluation which is de-
fined in the LDC corpus as LDC2006E38. We used
the corpus, LDC2006E43, as the development data
for loglinear model optimization.
We used a phrase-based SMT system that is based
on a log-linear model incorporating multiple fea-
tures. The training and decoding system of our SMT
used the publicly available Pharaoh (Koehn et al,
2003)2. GIZA++ was used for word alignment.
The Pharaoh decoder was used exclusively in
all the experiments. No additional features but
the defaults defined by Pharaoh were used. The
feature weights were optimized against the BLEU
scores (Och, 2003).
We chose automatic metrics to evaluate CWS and
SMT. We used the F-score for CWS and BLEU for
SMT. The BLEU is BLEU4, computed using the
NIST-provided ?mt-eval? script.
2http://www.iccs.informatics.ed.ac.uk/?pkoehn
3.2 Implementation of CWS schemes
To determine the effect of CWS on SMT, we cre-
ated 14 CWS schemes which are shown in Ta-
ble 3. Schemes 1 to 12 were implemented using
the in-house tool, Achilles, and schemes 13 and 14
using off-the-shelf tools. The CWS schemes are
named according to the specifications (AS, CITYU,
MSR, PKU), implementing methods (CRF-based or
dictionary-based), and lexicon sources (Sighan or
LDC corpus). The table also shows the results of
segmentation on the SMT training and test data, i.e.,
number of total tokens, unique words, and OOV
words.
We divided the schemes into two groups for sim-
plicity. The first group includes schemes 1 to 12,
which were trained using a specific Sighan corpus.
For example, schemes 1 to 3 were trained using the
AS corpus, schemes 4 to 6 using the CITYU cor-
pus, and so on. The meaning of the name of the
CWS scheme can be derived from the table ? the
name is defined by specifications, methods and lexi-
con sources. For example, the CRF-AS scheme per-
forms CRF-based segmentation; and its lexicon is
from the AS corpus provided by the Sighan. The
CRF-AS segmenter can be easily trained, as de-
scribed by Achilles.
The second group contains two schemes 13 and
14. The ICTCLAS is a HHMM-based hierarchical
HMM segmenter (Zhang et al, 2003) that uses the
specifications of PKU. This segmenter incorporates
parts-of-speech information in the probability mod-
els and generates multiple HMM models for solving
segmentation ambiguities. The MSRSEG was de-
veloped by Gao et al (Gao et al, 2004). This seg-
menter is based on the MSR specifications. It uses a
log-linear model that integrates multiple features.
The segmenters of the first group, dict-AS
and dict-LDC-AS, are two dictionary-based CWS
schemes. They differ in lexicon size and lexicon
extracting source. The former used a lexicon ex-
tracted directly from the Sighan AS training data
while the latter used a lexicon from LDC parallel
corpora. It took some efforts to get the lexicon. First,
we used the CRF-AS to segment the LDC corpora.
We extracted a unique word list from the segmented
data and sorted it in decreasing order according to
word frequency. Because OOV was recognized by
218
Table 3: Analysis of results of segmentation on LDC training and test data for all CWS schemes
No. CWS schemes Specifications Methods Lexicon Tokens Unique words OOVs
1 CRF-AS AS CRF Sighan 47,934,088 413,588 1,193
2 dict-AS AS Dict Sighan 51,664,675 89,346 237
3 dict-LDC-AS AS Dict LDC 48,665,364 102,919 273
4 CRF-CITYU CITYU CRF Sighan 47,963,541 426,273 1,155
5 dict-CITYU CITYU Dict Sighan 51,251,729 56,996 362
6 dict-LDC-CITYU CITYU Dict LDC 48,787,154 102,754 217
7 CRF-MSR MSR CRF Sighan 46,483,923 523,788 1,297
8 dict-MSR MSR Dict Sighan 51,302,509 60,247 248
9 dict-LDC-MSR MSR Dict LDC 47,469,271 102,390 217
10 CRF-PKU PKU CRF Sighan 48,022,697 440,114 1,136
11 dict-PKU PKU Dict Sighan 52,721,809 47,176 211
12 dict-LDC-PKU PKU Dict LDC 48,721,795 102,213 256
13 ICTCLAS PKU HHMM - 50,751,402 162,222 835
14 MSRSEG MSR - - ?48,734,113 274,411 1,443
the CRF-AS, a huge word list was generated(see Ta-
ble 3). We chose the most frequent 100,000 words
as the dictionary for the dict-LDC-AS 3. The LM for
the dict-AS was trained using the AS corpus while
the LM for the dict-LDC-AS was trained using the
segmented SMT training corpus.
Therefore, the dict-LDC-AS used a larger lexicon
than the dict-AS. This lexicon contained the most
frequent OOV words recognized by the CRF-AS.
Our aim was to investigate whether the dict-LDC-
AS, whose lexicon consisted of the lexicon of dict-
AS and new words recognized by CRF-AS, could
improve SMT.
As shown in Table 3, using CRF-AS generated a
huge number of unique words for the training data
and OOV words for the test data. We found that
the CRF-AS generated three times more OOVs for
the test data than the dictionary-based CWS,dict-AS
(see OOVs in Table 3).
Other schemes in the first group were imple-
mented similarly to the ?AS?.
Table 3 lists the segmentation statistics for the
training and test data of all the tested CWS schemes,
where ?Tokens? indicates the total number of words
in the training data. ?Unique words? and ?OOVs?
3Only those words that appeared at least five times in the
lexicon were considered.
Table 4: BLEU scores for CWS schemes
CWS AS CITYU MSR PKU
CRF 23.70 23.55 22.50 23.61
dict 23.46 23.72 23.33 23.61
dict-LDC 23.52 23.36 23.16 23.74
ICTCLAS - - - 24.12
MSRSEG - - 19.72 -
BEST 23.70 23.72 23.33 23.74 (24.12)
mean the lexicon size of the segmented training data
and the unknown words in the test data, respectively.
3.3 Effect of CWS specifications on SMT
Our first concern was the effect of CWS specifica-
tions on SMT. The results in Table 4 show the rela-
tionships that were found. The last row gives the
best BLEU scores obtained for each of the CWS
specifications. The scores for AS, CITYU, MSR and
PKU were 23.70 (CRF-AS), 23.72 (dict-CITYU),
23.33 (dict-MSR) and 23.74 (dict-PKU-LDC), re-
spectively. We found there were no observable dif-
ferences between AS, CITYU, and PKU. However,
the specification that produced the worst transla-
tions was the MSR. The MSR specification appears
219
to have been designed for recognizing named enti-
ties (NE) (See the examples of segmentation in Ta-
ble 1). Many NEs are regarded as words by MSR,
while they are more appropriately split into sepa-
rate words by other specifications. For example, the
long word, ?1997YEAR7MONTH1DAY? (?July 1,
1997?). As a result, the CRF-MSR generated 20%
more words in the vocabulary than the other CWS
schemes in segmenting the SMT training data. The
larger vocabulary can trigger data sparseness prob-
lems and result in SMT degradation. The segmenter,
MSRSEG, produced an even lower BLEU score
(19.72) than the Achilles.
The results were verified by significance
test (Zhang et al, 2004). We found the systems
with the BLEU scores higher than 23.70 were
significantly better than those lower than 23.70.
3.4 Correlation between BLEU score and
F-score
The values of the F-scores and BLEU scores are
listed in parallel in Table 5. We tied the F-scores
and specifications together because comparing the
value of the F-score across specs is meaningless. We
separated the F-score and BLEU score for different
corpus. The F-score was calculated using the Sighan
test data. The CRF-based approach usually gives a
higher F-score, but its corresponding BLEU scores
were not always higher. The F-score and BLEU
score correlated well for ICTCLAS and CRF-AS
but less well for CRF-CITYU, CRF-PKU and CRF-
MSR. Obviously, there is no strong correlation be-
tween the F-score and BLEU score.
4 Effect of combining multiple CWS
schemes
We used the Sighan Bakeoff corpora of different
CWS specifications separately in the previous ex-
periments. Here, we propose two approaches to us-
ing all the resources combined. The first approach
is to concatenate all the training data of the Sighan
Bakeoff, regardless of the specifications and train-
ing a new CWS for segmenting SMT training data.
The second approach involves linear integration of
translation models. We found that both approaches
produced an improvement in translation quality.
4.1 Effect of combining training data from
multiple CWS specifications
The CWS specifications are very different and the
corresponding Sighan training data are segmented
in different ways. We used these data separately
in the previous work as if they were incompatible.
However, creating data manually is laborious and
costly. It would therefore be a significant advan-
tage if all the data could be used, regardless of the
different specifications. We therefore created a new
CWS scheme, called ?dict-hybrid?. This CWS was
trained by concatenating all the Sighan Bakeoff cor-
pora regardless of the different specifications. The
?dict-hybrid? was trained using Achilles. It uses a
dictionary-based approach, and its lexicon and lan-
guage model were obtained as follows.
First, we created a hybrid corpus by combining
all the Sighan training corpora: AS, CITYU, MSR,
PKU. The hybrid corpus was used to train a CRF-
based CWS. This CWS was then used to segment
the SMT training corpus and then we extracted a
lexicon of 100,000 from the top frequent words of
the segmented SMT corpus. This lexicon was used
as the lexicon of the ?dict-hybrid.? The LM of ?dict-
hybrid? was also trained on the segmented corpus.
Note a lexicon and a LM are the only needed re-
sources for building a dictionary-based CWS, like
the ?dict-hybrid.? (Zhang et al, 2006)
We used the ?dict-hybrid? to segment the SMT
training corpus and test data. This segmentation
generated 49,546,231 tokens, 112,072 unique words
for the training data and 693 OOVs for the test data.
The segmentation data were used for training a
new SMT model. We tested the model using the
same approach and found the BLEU score obtained
by this CWS scheme was 23.91. This score was
better than those in Table 4 obtained by any of the
Achilles CWS schemes except ICTCLAS. There-
fore, the CWS scheme ?dict-hybrid? produced better
translations than other schemes implemented using
Achilles, indicating that using multiple CWS cor-
pora can improve SMT even if their specifications
are different.
Significance testing also showed that the results
for ICTCLAS and ?dict-hybrid? were not signifi-
cantly different. The results of ?dict-hybrid? are sig-
nificantly better than those in the Table 4 which have
220
Table 5: Correlation between F-score and BLEU
PKU MSR
F-score BLEU F-score BLEU
CRF 0.939 23.61 CRF 0.954 22.50
dict 0.930 23.61 dict 0.947 23.22
dict-LDC 0.931 23.74 dict-LDC 0.928 23.16
ICTCLAS 0.948 24.12 MSRSEG 0.969 19.72
CITYU AS
F-score BLEU F-score BLEU
CRF 0.920 23.55 CRF 0.922 23.70
dict 0.873 23.72 dict 0.896 23.46
dict-LDC 0.886 23.36 dict-LDC 0.878 23.52
a BLEU score lower than 23.70.
4.2 Effect of feature interpolation of
translation models
We investigated the effect of linearly integrating
multiple features of the same type. We generated
multiple translation models by using different word
segmenters. Each translation model corresponded to
a word segmenter. The same type of features as in
the log-linear model were added linearly. For exam-
ple, the phrase translation model p(e| f ) can be lin-
early interpolated as, p(e| f ) = ?Si=1 ?i pi(e| f ) where
pi(e| f ) is the phrase translation model correspond-
ing to the i-th CWSs. ?i is the weight, and S is the
total number of models. ?Si=1 ?i = 1.
?s can be obtained by maximizing the likelihood
or BLEU scores of the development data. Optimiz-
ing the ? has been described elsewhere (Foster and
Kuhn, 2007). p(e| f ) is the phrase translation model
generated.
In addition to the phrase translation model, we
used the same approach to integrate three other
features: phrase inverse probability p( f |e), lexical
probability lex(e| f , a), and lexical inverse probabil-
ity lex( f |e, a).
We integrated the CWS schemes ranked in the
top five in Table 4: ICTCLAS, dict-hybrid, dict-
LDC-PKU, dict-CITYU, and CRF-AS. We labeled
the five schemes A, B, C, D, and E, respectively,
as shown in Table 6. The first line of Table 6 rep-
resents the test data segmented by the five CWS
schemes. ?tst-A? means the test data was segmented
by ICTCLAS. ?tst-B? means the test data segmented
by ?dict-hybrid?, and so on. The second line gives
baseline results showing the original results with-
out the use of feature integration. For different test
data, the baseline is different. The baseline of ICT-
CLAS was tested on ?tst-A? only. The baseline of
?dict-hybrid? was tested on ?tst-B? only. From the
third line we gradually added a translation model
to the models used in the baseline. For example,
?A+B? integrates models made using ICTCLAS and
?dict-hybrid.? Each integration models were tested
only on the test data participated in the integration.
Hence, some slots in Table 6 are blank.
We did not carry out parameter optimization with
regards to the ?s. Instead, we used equal ?s for all
the features. For example, all ?s equal 0.5 for A+B,
and 0.25 for A+B+C+D. Each cell in Table 6 indi-
cates the BLEU score of the integration in relation
to the test data. We found our approach improved
the baseline results significantly. The more models
integrated, the better the results. The improvement
was positive for all of the test data. With regards to
the integration, if a phrase pair exists in one model
only, we suppose the values of probabilities are zero
in other models.
To better understand the effects of feature inter-
polation, we blended the features of the translation
models, as shown in Table 7, by simply combining
the phrase pairs without probability interpolation.
When we merged two models, we defined one model
as the master model and the other as the supple-
mentary model. Only phrase pairs that were in the
221
supplementary models but not in the master model
were appended to the master model. Their feature
probabilities were not changed. Hence, the com-
bined model was a blend of phrase pairs from the
master model and supplementary model. There was
no probability integration, that was significantly dif-
ferent from the feature interpolation approach. For
each set of test data in Table 7, the master model
was the model using the same CWS as the test data.
While there was one row for each type of combina-
tion, the cells in the row contained different models.
For example, ?A+B? for test data ?A? uses ?A? as the
master model and ?B? as the supplementary model,
while the opposite holds for test data ?B?.
Comparing Table 6 and 7 showed that feature
interpolation outperformed feature blending. Fea-
ture interpolation yielded surprisingly good results.
The performance consistently improved when more
models were integrated, but this was not the case
for feature blending. This shows that probability
integration is very effective. Increasing the size of
phrase pairs, as feature blending does, is not as ef-
fective.
We used equal values for the ?s. Optimal values
may be obtained using the optimization approach
of maximizing BLEU or the likelihood of develop-
ment data as has been reported previously (Foster
and Kuhn, 2007). However, optimization is compu-
tationally expensive and the effect was not satisfac-
tory. Therefore, we decided not optimizing the ?s in
this work.
5 Related work and Discussions
CWS has been the subject of intensive research
in recent years, as is evident from the last
four international evaluations, the Sighan Bake-
offs, and many approaches have been proposed
over the past decade. Segmentation performance
has been improved significantly, from the earli-
est maximal match (dictionary-based) approaches to
CRF (Peng and McCallum, 2004) approach. We
used dictionary-based and CRF-based CWS ap-
proaches to demonstrate the effect of CWS on SMT,
both without and with OOV recognition.
SMT is a very complicated system to study. Its
response to CWS schemes is intractable and it is
very hard to use one or two measures to describe
the relationship between CWS and SMT, in a similar
way to describing the relationship between the align-
ment error rate (AER) and SMT (Fraser and Marcu,
2007). The CWS and SMT are related by a series of
factors such as the specifications, OOVs, lexicons,
and F-scores. None of these factors can be directly
related to the SMT. While we have completed many
experiments, based on changing the CWS specifica-
tions and methods used, to determine the relation-
ship between CWS and SMT, we have not estab-
lished any overwhelming rules. However, we be-
lieve the following guidelines are appropriate in con-
sidering a CWS system for SMT. Firstly, the F-score
is not a reliable guide to SMT quality. A very high
F-score may produce the lowest quality translations,
as was found for the MSRSEG. Secondly, it is better
to design a specification with smaller word units to
reduce data sparseness. Specifications like those for
MSR will produce an inferior translation. Thirdly,
do not use a huge lexicon for word segmentation.
A huge lexicon will result in data sparseness and
segmentation complexity. And lastly, using multi-
ple word segmentation results and approaches does
work. We used two approaches that combined mul-
tiple word segmentation - dict-hybrid and feature in-
tegration - and both improved the translations signif-
icantly.
The BLEU scores in our experiments were rela-
tively low in comparison with current state-of-the art
results. However, our system was very similar to the
system (Koehn et al, 2005) that gave a BLEU score
of 24.3, comparable to ours. The BLEU score can
be raised if we do post-editing, use more data for
language modeling and other methods.
6 Conclusions
We investigated the effect of CWS on SMT from
two points of view. Firstly, we analyzed multiple
CWS specifications and built a CWS for each one to
examine how they affected translations. Secondly,
we investigated the advantages and disadvantages of
various CWS approaches, both dictionary-based and
CRF-based, and built CWSs using these approaches
to examine their effect on translations.
We proposed a new approach to linear interpo-
lation of translation features. This approach pro-
duced a significant improvement in translation and
222
Table 6: Feature interpolation of translation models: A=ICTCLAS, B=dict-hybrid, C=dict-PKU-LDC, D=dict-CITYU, E=CRF-AS
Model tst-A tst-B tst-C tst-D tst-E
Baseline 24.12 23.91 23.74 23.72 23.70
A+B 24.25 24.20
A+B+C 24.49 24.31 23.84
A+B+C+D 24.60 24.43 24.05 24.27
A+B+C+D+E 24.61 24.55 24.16 24.39 24.17
Table 7: Feature blending of translation models
Model tst-A tst-B tst-C tst-D tst-E
Baseline 24.12 23.91 23.74 23.72 23.70
A+B 24.20 24.24
A+B+C 24.27 24.14 23.69
A+B+C+D 23.92 24.29 23.61 24.00
A+B+C+D+E 23.86 24.31 23.69 24.05 23.76
achieved the best BLEU score of all the CWS
schemes.
We have published a much more detailed pa-
per (Zhang et al, 2008) to describe the relations be-
tween CWS and SMT.
References
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. In Computational linguistics, Squibs Discussion,
volume 33 of 3, pages 293?303, September.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In ACL-2004,
pages 462?469, Barcelona, July.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Miles Osborne Chris Callison-Burch, David
Talbot, and Michael White. 2005. Edinburgh sys-
tem description for the 2005 nist mt evaluation. In
Proceedings of Machine Translation Evaluation Work-
shop.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562?568, Geneva, Switzerland.
Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, pages 184?
187.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of the LREC.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the HLT-NAACL, Companion Volume: Short Pa-
pers, pages 193?196, New York City, USA, June.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Chinese word segmentation and statistical ma-
chine translation. ACM Trans. Speech Lang. Process.,
5(2), May.
223
Chinese Unknown Word Translation by Subword Re-segmentation
Ruiqiang Zhang1,2 and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang, eiichiro.sumita}@{nict.go.jp, atr.jp}
Abstract
We propose a general approach for trans-
lating Chinese unknown words (UNK) for
SMT. This approach takes advantage of
the properties of Chinese word composition
rules, i.e., all Chinese words are formed
by sequential characters. According to the
proposed approach, the unknown word is
re-split into a subword sequence followed
by subword translation with a subword-
based translation model. ?Subword? is a
unit between character and long word. We
found the proposed approach significantly
improved translation quality on the test data
of NIST MT04 and MT05. We also found
that the translation quality was further im-
proved if we applied named entity transla-
tion to translate parts of unknown words be-
fore using the subword-based translation.
1 Introduction
The use of phrase-based translation has led to great
progress in statistical machine translation (SMT).
Basically, the mechanism of this approach is re-
alized by two steps:training and decoding. In the
training phase, bilingual parallel sentences are pre-
processed and aligned using alignment algorithms or
tools such as GIZA++ (Och and Ney, 2003). Phrase
pairs are then extracted to be a phrase translation ta-
ble. Probabilities of a few pre-defined features are
computed and assigned to the phrase pairs. The fi-
nal outcome of the training is a translation table con-
sisting of source phrases, target phrases, and lists
of probabilities of features. In the decoding phase,
the translation of a test source sentence is made by
reordering the target phrases corresponding to the
source phrases, and searching for the best hypothesis
that yields the highest scores defined by the search
criterion.
However, this mechanism cannot solve unknown
word translation problems. Unknown words (UNK)
point to those unseen words in the training or non-
existing words in the translation table. One strat-
egy to deal with translating unknown words is to re-
move them from the target sentence without transla-
tion on assumption of fewer UNKs in the test data.
Of course, this simple way produces a lower quality
of translations if there are a lot of UNKs in the test
data, especially for using a Chinese word segmenter
that produces many UNKs. The translation of UNKs
need to be solved by a special method.
The translation of Chinese unknown words seems
more difficult than other languages because Chinese
language is a non-inflected language. Unlike other
languages (Yang and Kirchhoff, 2006; Nie?len and
Ney, 2000; Goldwater and McClosky, 2005), Chi-
nese UNK translation cannot use information from
stem and inflection analysis. Using machine translit-
eration can resolve part of UNK translation (Knight
and Graehl, 1997). But this approach is effective for
translating phonetically related unknown words, not
for other types. No unified approach for translating
Chinese unknown words has been proposed.
In this paper we propose a novel statistics-based
approach for unknown word translation. This ap-
proach uses the properties of Chinese word compo-
sition rules ? Chinese words are composed of one
or more Chinese characters. We can split longer un-
known words into a sequence of smaller units: char-
acters or subwords. We train a subword based trans-
lation model and use the model to translate the sub-
225
word sequence. Thus we get the translation of the
UNKs. We call this approach ?subword-based un-
known word translation?.
In what follows, section 2 reviews phrase-based
SMT. section 3 describes the dictionary-based CWS,
that is the main CWS in this work. Section 4 de-
scribes our named entity recognition approach. Sec-
tion 5 describes the subword-based approach for
UNK translation. Section 7 describes the experi-
ments we conducted to evaluate our subword ap-
proach for translating Chinese unknown words. Sec-
tion 8 describes existing methods for UNK transla-
tions for other languages than Chinese. Section 9
briefly summarizes the main points of this work.
2 Phrase-based statistical machine
translation
Phrase-based SMT uses a framework of log-linear
models (Och, 2003) to integrate multiple features.
For Chinese to English translation, source sentence
C is translated into target sentence E using a proba-
bility model:
P?(E|C) =
exp(?Mi=1 ?i fi(C, E))?
E? exp(
?M
i=1 ?i fi(C, E?))
? = {?M1 , }
(1)
where fi(C, E) is the logarithmic value of the i-th
feature, and ?i is the weight of the i-th feature. The
candidate target sentence that maximizes P(E|C) is
the solution.
Obviously, the performance of such a model de-
pends on the qualities of its features. We used the
following features in this work.
? Target language model: an N-gram language
model is used.
? Phrase translation model p(e| f ): gives the
probability of the target phrases for each source
phrase.
? Phrase inverse probability p( f |e): the probabil-
ity of a source phrase for a given target phrase.
It is the coupled feature of the last one.
? Lexical probability lex(e| f , a): the sum of the
target word probabilities for the given source
words and the alignment of the phrase pairs.
? Lexical inverse probability lex( f |e, a): the sum
of the source word probabilities for the given
target words and alignment.
? Target phrase length model #(p): the number of
phrases included in the translation hypothesis.
? Target word penalty model: the number of
words included in the translation hypothesis.
? Distance model #(w): the number of words be-
tween the tail word of one source phrase and
the head word of the next source phrase.
In general, the following steps are used to get the
above features.
1. Data processing: segment Chinese words and
tokenize the English.
2. Word alignment: apply two-way word align-
ment using GIZA++.
3. Lexical translation: calculate word lexical
probabilities.
4. Phrase extraction: extract source target bilin-
gual pairs by means of union, intersection, et.
al.
5. Phrase probability calculation: calculate phrase
translation probability.
6. Lexical probability: generate word lexical
probabilities for phrase pairs.
7. Minimal error rate training: find a solution to
the ??s in the log-linear models.
3 Dictionary-based Chinese word
segmentation
For a given Chinese character sequence,
C = c0c1c2 . . . cN , the problem of word seg-
mentation is addressed as finding a word se-
quence, W = wt0wt1wt2 . . .wtM , where the words,
wt0 ,wt1 ,wt2 , . . . ,wtM , are pre-defined by a provided
lexicon/dictionary, which satisfy
wt0 = c0 . . . ct0 , wt1 = ct0+1 . . . ct1
wti = cti?1+1 . . . cti , wtM = ctM?1+1 . . . ctM
ti > ti?1, 0 ? ti ? N, 0 ? i ? M
226
This word sequence is found by maximizing the
function below,
W = arg max
W
P(W |C)
= arg max
W
P(wt0wt1 . . .wtM )
(2)
We applied Bayes? law in the above derivation.
P(wt0wt1 . . .wtM ) is a language model that can be ex-
panded by the chain rule. If trigram LMs are used,
it is approximated as
P(w0)P(w1|w0)P(w2|w0w1) ? ? ? P(wM |wM?2wM?1)
where wi is a shorthand for wti .
Equation 2 indicates the process of the dictionary-
based word segmentation. Our CWS is based on it.
We used a beam search algorithm because we found
that it can speed up the decoding. Trigram LMs were
used to score all the hypotheses, of which the one
with the highest LM scores is the final output.
As the name indicates, the word segmentation re-
sults by the dictionary-based CWS are dependent
on the size and contents of the lexicon. We will
use three lexicons in order to compare effects of
lexicon size to the translations. The three lexicons
denoted as Character, Subword and Hyperword are
listed below. An example sentence, ?????
???(HuangYingChun lives in Beijing City), is
given to show the segmentation results of using the
lexicons.
? Character: Only Chinese single charac-
ters are included in the lexicon. The
sentence is split character by character.
?/?/?/?/?/?/?/?
? Subword: A small amount of most frequent
words (10,000) are added to the lexicon.
Choosing the subwords are described in sec-
tion 5. ?/?/?/?/?/??/?
? Hyperword: A big size of lexicon is used, con-
sisting of 100,000 words. ?/?/?/?/?/??
?
4 Named entity recognition (NER)
Named entities in the test data need to be treated
separately. Otherwise, a poor translation quality
was found by our experiments. We define four
Table 1: NER accuracy
type Recall Precision F-score
nr 85.32% 93.41% 89.18%
ns 87.80% 90.46% 89.11%
nt 84.50% 87.54% 85.99%
all 84.58% 90.97% 87.66%
types of named entities: people names (nr), orga-
nization names (nt), location names (ns), and nu-
merical expressions (nc) such as calendar, time, and
money. Our NER model is built according to con-
ditional random fields (CRF) methods (Lafferty et
al., 2001), by which we convert the problem of NER
into that of sequence labeling. For example, we can
label the last section?s example as, ??/B nr?/I nr
?/I nr ?/O ?/O ?/B nt ?/I nt ?/I nt?, where
?B? stands for the first character of a NE; ?I?, other
than the first character of a NE; ?O?, isolated char-
acter. ?nr? and ?nt? are two labels of NE.
We use the CRF++ tools to train the models for
named entity recognition1. The performance of our
NER model was shown in Table 4. We use the
Peking University (PKU) named entity corpus to
train the models. Part of the data was used as test
data.
We stick to the results of CWS if there are ambi-
guities in the segmentation boundary between CWS
and NER.
The NER was used only on the test data in transla-
tions. It was not used on the training data due to the
consideration of data sparseness. Using NER will
generate more unknown words that cannot be found
a translation in the translation table. That is why we
use a subword-based translation approach.
5 Subword-based translation model for
UNK translation
We found there were two reasons accounting for
producing untranslatable words. The first is the
size of lexicon. We proposed three size of lexi-
cons in section 3, of which the Hyperword type uses
100,000 words. Because of a huge lexical size, some
of the words cannot be learned by SMT training be-
cause of limited training data. The CWS chooses
only one candidate segmentation from thousands in
1http://chasen.org/?taku/software/CRF++/
227
splitting a sentence into word sequences. Therefore,
the use of a candidate will block other candidates.
Hence, many words in the lexicon cannot be fully
trained if a large lexicon is used. The second is our
NER module. The NER groups a longer sequence of
characters into one entity that cannot be translated.
We have analyzed this points in the last section.
Therefore, in order to translate unknown words,
our approach is to split longer unknown words into
smaller pieces, and then translate the smaller pieces
by using Character or Subword models. Finally, we
put the translations back to the Hyperword models.
We call this method subword-based unknown word
translation regardless of whether a Character model
or Subword model is used.
As described in Section 3, Characters CWS uses
only characters in the lexicon. So there is no tricks
for it. But for the Subword CWS, its lexicon is a
small subset of the Hyperword CWS. In fact, we use
the following steps for generating the lexicon. In the
beginning, we use the Hyperword CWS to segment
the training data. Then, we extract a list of unique
tokens and calculate their counts from the results of
segmentation. Next, we sort the list as the decreas-
ing order of the counts, and choose N most frequent
words from the top of the list. We restrict the length
of subwords to three. We use the N words as the
lexicon for the subword CWS. N can be changed.
Section 7.4 shows its effect to translations. The sub-
word CWS uses a trigram language model to disam-
biguate. Refer to (Zhang et al, 2006) for details
about selecting the subwords.
We applied Subword CWS to re-segment the
training data. Finally, we can train a subword-based
SMT translation model used for translating the un-
known words. Training this subword translation
model was done in the same way as for the Hyper-
word translation model that uses the main CWS, as
described in the beginning of Section 2.
6 Named entity translation
The subword-based UNK translation approach can
be applied to all the UNKs indiscriminately. How-
ever, if we know an UNK is a named entity, we
can translate this UNK more accurately than using
the subword-based approach. Some unknown words
can be translated by named entity translation if they
are correctly recognized as named entity and fit a
translation pattern. For example, the same words
with different named entities are translated differ-
ently in the context. The word, ???, is translated
into ?nine? for measures and money, ?September?
for calendar, and ?jiu? for Chinese names.
As stated in Section 4, we use NER to recognize
four types of named entities. Correspondingly, we
created the translation patterns to translate each type
of the named entities. These patterns include pat-
terns for translating numerical expressions, patterns
for translating Chinese and Japanese names, and pat-
terns for translating English alphabet words. The us-
ages are described as follows.
Numerical expressions are the largest proportion
of unknown words. They include calendar-related
terms (days, months, years), money terms, mea-
sures, telephone numbers, times, and addresses.
These words are translated using a rule-based ap-
proach. For example, ???????, is translated
into ?at 3:15?.
Chinese and Japanese names are composed of
two, three, or four characters. They are translated
into English by simply replacing each character with
its spelling. The Japanese name, ??????, is
translated into ?Shinzo Abe?.
English alphabets are encoded in different Chi-
nese characters. They are translated by replacing the
Chinese characters with the corresponding English
letters.
We use the above translation patterns to translate
the named entities. Using translation patterns pro-
duce almost correct translation. Hence, we put the
named entity translation to work before we apply the
subword translation model. The subword translation
model is used when the unknown words cannot be
translated by named entity translation.
7 SMT experiments
7.1 Data
We used LDC Chinese/English data for training. We
used two test data of NIST MT04 and NIST MT05.
The statistics of the data are shown in Table 6. We
used about 2.4 million parallel sentences extracted
from LDC data for training. Experiments on both
the MT04 and MT05 test data used the same transla-
tion models on the same training data, but the min-
228
Table 2: Statistics of data for MT experiments
Chinese English
MT Training Sentences 2,399,753
words 49,546,231 52,746,558
MT04 LDC2006E43 Test Sentences 1,788
Words 49,860
MT05 LDC2006E38 Test Sentences 1,082
Words 30,816
Table 3: Statistics of unknown words of test data using different CWS
Hyperword+Named entities Hyperword Subwords Characters
Numerics People Org. Loc. other
MT04 460 146 250 230 219 650 18 2
MT05 414 271 311 146 323 680 23 2
imum error rate training was different. The MT04
and MT05 test data were also used as development
data for cross experiments.
We used a Chinese word segmentation tool,
Achilles, for doing word segmentation. Its word
segmentation accuracy was higher than the stanford
word segmenter (Tseng et al, 2005) in our labora-
tory test (Zhang et al, 2006).
The average length of a sentence for the test data
MT04 and MT05 after word segmentation is 37.5
by using the Subword CWS, and 27.9 by using the
Hyperword CWS.
Table 6 shows statistics of unknown words in
MT04 and MT05 using different word segmenta-
tion. Obviously, character-based and subword-based
CWS generated much fewer unknown words, but
sentences are over-segmented. The CWS of Hy-
perword generated many UNKs because of using
a large size of lexicon. However, if named entity
recognition was applied upon the segmented results
of the Hyperword, more UNKs were produced. Take
an example for MT04. There are 1,305 UNKs in
which numeric expressions amount to 35.2%, peo-
ple names at 11.2%, organization names at 19.2%,
location names at 17.6%, and others at 16.8%. Anal-
ysis of these numbers helps to understand the distri-
bution of unknown words.
7.2 Effect of the various CWS
As described in section 3, we used three lexicon
size for the dictionary-based CWS. Therefore, we
had three CWS denoted as: Character, Subword and
Hyperword. We used the three CWS in turn to do
word segmentation to the training data, and then
built the translation models respectively. We tested
the performance of each of the translation models
on the test data. The results are shown on Table 4.
The translations are evaluated in terms of BLEU
score (Papineni et al, 2002). This experiment was
just testing the effect of the three CWS. Therefore,
all the UNKs of the test data were not translated,
simply removed from the results.
We found the character-based CWS yielded the
lowest BLEU scores, indicating the translation qual-
ity of this type is the worst. The Hyperword CWS
achieved the best results. If we relate it to Ta-
ble 6, we found while the Hyperword CWS pro-
duced many more UNKs than the Character and
Subword CWS, its translation quality was improved
instead. The fact proves the quality of transla-
tion models play a more important role than the
amount of unknown word translation. Using the
Hyperword CWS can generate a higher quality of
translation models than the Character and Subword
CWS. Therefore, we cannot use the character and
subword-based CWS in Chinese SMT system due to
their overall poor performance. But we found their
229
Table 4: Compare the translations by different CWS (BLEU
scores)
MT04 MT05
Character 0.253 0.215
Subword 0.265 0.229
Hyperword 0.280 0.236
Table 5: Effect of subword and named entity translation
(BLEU)
MT04 MT05
Baseline(Hyperword) 0.280 0.236
Baseline+Subword 0.283 0.244
Baseline+NER 0.283 0.242
Baseline+NER+Subword 0.285 0.246
usage for UNK translation.
7.3 Effect of subword translation for UNKs
The experiments in this section show the effect of
using the subword translation model for UNKs. We
compared the results of using subword translation
with those of without using it. We also used named
entity translation together with the subword trans-
lation. Thus, we could compare the effect of sub-
word translation under conditions of with or without
named entity translation. We listed four kinds of re-
sults to evaluate the performance of our approach in
Table 5 where the symbols indicate:
? Baseline: this is the results made by the Hyper-
word CWS of Table 4. No subword translation
for UNKs and named entity translations were
used. Unknown words were simply removed
from the output.
? Baseline+Subword: the results were made un-
der the same conditions as the first except all of
the UNKs were extracted, re-segmented by the
subword CWS and translated by the subword
translation models. However, the named entity
translation was not used.
? Baseline+NER: this experiment did not use
subword-based translation for UNKs. But we
used named entity translation. Part of UNKs
was labeled with named entities and translated
by pattern match of section 6.
? Baseline+NER+Subword: this experiment
used the named entity translation and the
subword-based translation. The difference
from the second one is that some UNKs were
translated by the translation patterns of sec-
tion 6 at first and the remaining UNKs were
translated using the subword model (the sec-
ond one translated all of the UNKs using the
subword model).
The results of our experiments are shown in Ta-
ble 5. We found the subword models improved
translations in all of the experiments. Using the
subword models on the MT04 test data improved
translations in terms of BLEU scores from 0.280
to 0.283, and from 0.236 to 0.244 on the MT05
test data. While only small gains of BLEU were
achieved by UNK translation, this improvement is
sufficient to prove the effectiveness of the subword
models, given that the test data had only a low pro-
portion of UNKs.
The BLEU scores of ?Baseline+NER? is higher
than that of ?Baseline?, that proves using named en-
tity translation improved translations, but the effect
of using named entity translation was worse than us-
ing the subword-based translation. This is because
the named entity translation is applicable for the
named entities only. However, the subword-based
translation is used for all the UNKs.
When we applied named entity translation to
translate some of recognized named entities fol-
lowed by using the subword models, we found
BLEU gains over using the subword models
uniquely, 0.2% for MT04 and 0.2% for MT05. This
experiment proves that the best way of using the
subword models is to separate the UNKs that can
be translated by named entity translation from those
that cannot, and let the subword models handle
translations of those not translated.
Analysis using the bootstrap tool created by
Zhang et al (Zhang et al, 2004) showed that the
results made by the subword translations were sig-
nificantly better than the ones not using it.
7.4 Effect of changing the size of subword
lexicon
We have found a significant improvement by using
the subword models. The essence of the approach
230
Table 6: BLEU scores for changing the subword lexicon size
subword size MT04 MT05
character 0.280 0.237
10K 0.283 0.244
20K 0.283 0.240
is to split unknown words into subword sequences
and use subword models to translate the subword
sequences. The choices are flexible in choosing the
number of subwords in the subword lexicon. If a
different subword list is used, the results of the sub-
word re-segmentation will be changed. Will choos-
ing a different subword list have a large impact on
the translation of UNKs? As shown in Table 6, we
used three classes of subword lists: character, 10K
subwords and 20K subwords. The ?character? class
used only single-character words, about 5,000 char-
acters. The other two classes, ?10K? and ?20K?,
used 10,000 and 20,000 subwords. The method for
choosing the subwords was described in Section 5.
We have used ?10K? in the previous experiments.
We did not use named entity translation for this ex-
periment.
We found that using ?character? as the subword
unit brought in nearly no improvement over the
baseline results. Using 20K subwords yielded bet-
ter results than the baseline but smaller gains than
that of using the 10K subwords for MT05 data. It
proves that using subword translation is an effective
approach but choosing a right size of subword lexi-
con is important. We cannot propose a better method
for finding the size. We can do more experiments
repeatedly to find this value. We found the size of
10,000 subwords achieved the best results for our
experiments.
8 Related work
Unknown word translation is an important problem
for SMT. As we showed in the experiments, appro-
priate handling of this problem results in a signifi-
cant improvement of translation quality. As we have
known, there exists some methods for solving this
problem. While these approaches were not proposed
in aim to unknown word translation, they can be
used for UNK translations indirectly.
Most existing work focuses on named entity
translation (Carpuat et al, 2006) because named en-
tities are the large proportion of unknown words. We
also used similar methods for translating named en-
tities in this work.
Some used stem and morphological analysis for
UNKs such as (Goldwater and McClosky, 2005).
Morphological analysis is effective for inflective
languages but not for Chinese. Using unknown
word modeling such as backoff models was pro-
posed by (Yang and Kirchhoff, 2006).
Other proposed methods include paraphras-
ing (Callison-Burch et al, 2006) and translitera-
tion (Knight and Graehl, 1997) that uses the feature
of phonetic similarity. However, This approach does
not work if no phonetic relationship is found.
Splitting compound words into translatable sub-
words as we did in this work have been used
by (Nie?len and Ney, 2000) and (Koehn and Knight,
2003) for languages other than Chinese where de-
tailed splitting methods are proposed. We used
forward maximum match method to split unknown
words. This splitting method is relatively simple but
works well for Chinese. The splitting for Chinese is
not as complicated as those languages with alphabet.
9 Discussion and conclusion
We made use of the specific property of Chinese lan-
guage and proposed a subword re-segmentation to
solve the translation of unknown words. Our ap-
proach was tested under various conditions such as
using named entity translation and varied subword
lexicons. We found this approach was very effective.
We are hopeful that this approach can be applied into
languages that have similar features as Chinese, for
example, Japanese.
While the work was done on a SMT system
which is not the state-of-the-art 2, the idea of using
subword-based translation for UNKs is applicable to
any systems because the problem of UNK transla-
tion has to be faced by any system.
Acknowledgement
The authors would like to thank Dr.Michael Paul for
his assistance in this work, especially for evaluating
methods and statistical significance test.
2The BLEU score of the top one system is about 0.35 for
MT05 (http://www.nist.gov/speech/tests/mt/).
231
References
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In HLT-NAACL-2006.
Marine Carpuat, Yihai Shen, Xiaofeng Yu, and Dekai
Wu. 2006. Toward Integrating Word Sense and Entity
Disambiguation into Statistical Machine Translation.
In Proc. of the IWSLT.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proceedings of the HLT/EMNLP.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proc. of the ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In EACL-2003.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 591?598.
Sonja Nie?len and Hermann Ney. 2000. Improving smt
quality with morpho-syntactic analysis. In Proc. of
COLING.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine trainslation. In Proc. ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of the 40th ACL, pages 311?318,
Philadelphia, USA.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, Jeju, Korea.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In EACL-2006.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores: How much improvement
do we need to have a better system? In Proceedings of
the LREC.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the HLT-NAACL.
232
Method of Selecting Training Data to Build a Compact and Efficient
Translation Model
Keiji Yasuda?,?, Ruiqiang Zhang?,?, Hirofumi Yamamoto?,? and Eiichiro Sumita?,?
?National Institute of Communications Technology
?ATR Spoken Language Translation Research Laboratories
2?2?2, Hikaridai, ?Keihanna Science City?, Kyoto, 619?0288 Japan
{keiji.yasuda,ruiqiang.zhang}@nict.go.jp
{hirofumi.yamamoto,eiichiro.sumita}@nict.go.jp
Abstract
Target task matched parallel corpora are re-
quired for statistical translation model train-
ing. However, training corpora sometimes
include both target task matched and un-
matched sentences. In such a case, train-
ing set selection can reduce the size of the
translation model. In this paper, we propose
a training set selection method for transla-
tion model training using linear translation
model interpolation and a language model
technique. According to the experimental
results, the proposed method reduces the
translation model size by 50% and improves
BLEU score by 1.76% in comparison with a
baseline training corpus usage.
1 Introduction
Parallel corpus is one of the most important compo-
nents in statistical machine translation (SMT), and
there are two main factors contributing to its perfor-
mance. The first is the quality of the parallel corpus,
and the second is its quantity.
A parallel corpus that has similar statistical char-
acteristics to the target domain should yield a more
efficient translation model. However, domain-
mismatched training data might reduce the transla-
tion model?s performance. A large training corpus
obviously produces better quality than a small one.
However, increasing the size of the training corpus
causes another problem, which is increased compu-
tational processing load. This problem not only af-
fects the training of the translation model, but also
its applications. The reason for this is that a large
amount of training data tends to yield a large trans-
lation model and applications then have to deal with
this model.
We propose a method of selecting translation
pairs as the training set from a training parallel
corpus to solve the problem of an expanded trans-
lation model with increased training load. This
method enables an adequate training set to be se-
lected from a large parallel corpus by using a small
in-domain parallel corpus. We can make the transla-
tion model compact without degrading performance
because this method effectively reduces the size of
the set for training the translation model. This com-
pact translation model can outperform a translation
model trained on the entire original corpus.
This method is especially effective for domains
where it is difficult to enlarge the corpus, such as
in spoken language parallel corpora (Kikui et al,
2006). The main approach to recovering from an un-
dersupply of the in-domain corpus has been to use
a very large domain-close or out-of-domain paral-
lel corpus for the translation model training (NIST,
2006). In such case, the proposed method effectively
reduces the size of the training set and translation
model.
Section 2 describes the method of selecting the
training set. Section 3 details the experimental re-
sults for selecting the training set and actual trans-
lation from the International Workshop on Spoken
Language Translation 2006 (IWSLT2006). Section
4 compares the results of the proposed method with
those of the conventional method. Section 5 con-
cludes the paper.
655
Target language
Targetlanguage Sourcelanguage
Source language
Large out-of-domain parallel corpus 
Small in-domain parallel corpus
1. Train translationmodel
3. Calculate perplexity
LMTarget LMSource
2. Train languagemodels
TMin-domain
4. Select translation pairs based on the perplexity
6. Integrate TMs using linear interpolation
5. Train translationmodel TMselected
TMfinal
Figure 1: Framework of method.
2 Method
Our method use a small in-domain parallel corpus
and a large out-of-domain parallel corpus, and it
selects a number of appropriate training translation
pairs from the out-of-domain parallel corpus. Fig-
ure 1 is a flow diagram of the method. The proce-
dure is as follows:
1. Train a translation model using the in-domain
parallel corpus.
2. Train a language model using the source lan-
guage side or/and target language side of the
in-domain corpus.
3. Calculate the word perplexity for each sentence
(in source language side or/and target language
side) in the out-of-domain corpus by using the
following formulas.
PPe = Pe(Se)?
1
ne (1)
where PPe is the target language side perplex-
ity, and Pe is the probability given by the target
side language model. Se is the target language
sentence in the parallel corpus, and ne is the
number of words in the sentence.
We can also calculate the perplexity in the
source language (PPf ) in the same way.
PPf = Pf (Sf )
? 1nf (2)
If we use perplexities in both languages, we can
calculate average perplexity (PPe+f ) by using
the following formula.
PPe+f = (PPe ? PPf )
1
2 (3)
656
Table 1: Size of parallel corpora
English Chinese English Chinese
In-domain
parallel corpus
40 K 40 K 320 K 301 K Basic Travel Expressions Corpus
Out-of-domain
parallel corpus
2.5 M 2.5 M 62 M 54 M
LDC corpus (LDC 2002T01, LDC2003T17, LDC2004T07,
LDC2004T08, LDC2005T06 and LDC2005T10)
# of sentences # of words
Explanation
4. Select translation pairs from the out-of-domain
parallel corpus. If the perplexity is smaller than
the threshold, use translation pairs as the train-
ing set. Otherwise, discard the translation pairs.
5. Train a translation model by using the selected
translation pairs.
6. Integrate the translation model obtained in 1
and 6 by linear interpolation.
3 Experiments
We carried out statistical machine translation experi-
ments using the translation models obtained with the
proposed method.
3.1 Framework of SMT
We employed a log-linear model as a phrase-based
statistical machine translation framework. This
model expresses the probability of a target-language
word sequence (e) of a given source language word
sequence (f ) given by
P (e|f) =
exp
(?M
i=1 ?ihi(e, f)
)
?
e? exp
(?M
i=1 ?ihi(e?, f)
) (4)
where hi(e, f) is the feature function, ?i is the fea-
ture function?s weight, and M is the number of fea-
tures. We can approximate Eq. 4 by regarding its
denominator as constant. The translation results (e?)
are then obtained by
e?(f, ?M1 ) = argmaxe
M?
i=1
?ihi(e, f) (5)
3.2 Experimental conditions
3.2.1 Corpus
We used data from the Chinese-to-English trans-
lation track of the IWSLT 2006(IWSLT, 2006) for
the experiments. The small in-domain parallel cor-
pus was from the IWSLT workshop. This corpus
was part of the ATR Bilingual Travel Expression
Corpus (ATR-BTEC) (Kikui et al, 2006). The large
out-of-domain parallel corpus was from the LDC
corpus (LDC, 2007). Details on the data are listed
in Table 1. We used the test set of the IWSLT2006
workshop for the evaluation. This test set consisted
of 500 Chinese sentences with eight English refer-
ence translations per Chinese sentence.
For the statistical machine-translation experi-
ments, we first aligned the bilingual sentences
for preprocessing using the Champollion tool (Ma,
2006). We then segmented the Chinese words us-
ing Achilles (Zhang et al, 2006). After the seg-
mentation, we removed all punctuation from both
English and Chinese corpuses and decapitalized the
English corpus. We used the preprocessed data to
train the phrase-based translation model by using
GIZA++ (Och and Ney, 2003) and the Pharaoh tool
kit (Koehn et al, 2003).
3.2.2 Features
We used eight features (Och and Ney, 2003;
Koehn et al, 2003) and their weights for the transla-
tions.
1. Phrase translation probability from source lan-
guage to target language (weight = 0.2)
2. Phrase translation probability from target lan-
guage to source language (weight = 0.2)
3. Lexical weighting probability from source lan-
guage to target language (weight = 0.2)
4. Lexical weighting probability from source tar-
get to language weight = 0.2)
5. Phrase penalty (weight = 0.2)
657
6. Word penalty (weight = ?1.0)
7. Distortion weight (weight = 0.5)
8. Target language model probability (weight =
0.5)
According to a previous study, the minimum er-
ror rate training (MERT) (Och, 2003), which is the
optimization of feature weights by maximizing the
BLEU score on the development set, can improve
the performance of a system. However, the range
of improvement is not stable because the MERT al-
gorithm uses random numbers while searching for
the optimum weights. As previously mentioned, we
used fixed weights instead of weights optimized by
MERT to remove its unstable effects and simplify
the evaluation.
3.2.3 Linear interpolation of translation
models
The experiments used four features (Feature # 1
to 4 in 3.2.2) as targets for integration. For each fea-
ture, we applied linear interpolation by using the fol-
lowing formula.
h(e, f) = ?outhout(e, f)+(1??out)hin(e, f) (6)
Here, hin(e, f) and hout(e, f) are features trained
on the in-domain parallel corpus and out-of-domain
corpus, respectively. ?out is the weight for the fea-
ture trained on the out-of-domain parallel corpus.
3.2.4 Language model
We used a Good-Turing (Good, 1953) 3-gram lan-
guage model for data selection.
For the actual translation, we used a modi-
fied Kneser-Ney (Chen and Goodman, 1998) 3-
gram language model because modified Kneser-Ney
smoothing tended to perform better than the Good-
Turing language model in this translation task. For
training of the language model, only the English side
of the in-domain corpus was used. We used the
same language model for the entire translation ex-
periment.
3.3 Experimental results
3.3.1 Translation performance
Figure 2 and 3 plot the results of the experiments.
The horizontal axis represents the weight for the out-
of-domain translation model, and the vertical axis
15%
16%
17%
18%
19%
20%
21%
22%
23%
24%
25%
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Weight for out-of-domain translation model
B
L
E
U
 s
c
o
re
400 K
800 K
1.2 M
1.6 M
2.5 M
Figure 2: Results of data selection and linear inter-
polation (BLEU)
represents the automatic metric of translation qual-
ity (BLEU score (Papineni et al, 2002) in Fig. 2,
and NIST score (NIST, 2002) in Fig. 3). Thick
straight broken lines in the figures indicate auto-
matic scores of a baseline system. This base line sys-
tem was trained on the in-domain and all of the out-
of-domain corpus (2.5M sentence pairs). These data
were concatenated before training; then one model
was trained without linear interpolation. The five
symbols in the figures represent the sizes (# of sen-
tence pairs) of the selected parallel corpus. Here,
the selection was carried out by using Eq. 1. For
automatic evaluation, we used the reference transla-
tion with a case unsensitive and no-punctuation set-
ting. Hence, higher automatic scores indicate better
translations; the selected corpus size of 1.2M (?)
indicates the best translation quality in Fig. 2 at the
point where the weight for the out-of-domain trans-
lation model is 0.7.
In contrast to Fig. 2, Fig. 3 shows no improve-
ments to the NIST score by using the baseline out-
of-domain usage. The optimal weights for each cor-
pus size are different from those in Fig. 2. How-
ever, there is no difference in optimal corpus size;
i.e., the selected corpus size of 1.2M gives the best
NIST score.
658
5.4
5.5
5.6
5.7
5.8
5.9
6
6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Weight for out-of-domain translation model
N
IS
T
 s
c
o
re
400 K
800 K
1.2 M
1.6 M
2.5 M
Figure 3: Results of data selection and linear inter-
polation (BLEU)
Table 2: Size of integrated phrase tables
In-domain Out-of-domain
40 K 0 14 M
40 K 1.2 M 917 M
40 K 2.5 M 1.8 G
Size of phrase table
(Bytes)
Corpus size
(Sentence pairs)
3.3.2 Size of the translation models
Table 2 lists the sizes of the translation models
of the baseline and optimum-size training corpus.
The size of the phrase table is the uncompressed file
size of the phrase table trained by the Pharaoh tool
kit. As the table indicates, our method reduced the
model sizes by 50%.
This reduction had a positive effect on the com-
putational load of decoding.
3.3.3 Equations for the selection
The experiments described above used only target
language side information, i.e., Eq. 1, for the data
selection. Here, we compare selection performances
of Eqs. 1, 2, and 3. Table 3 shows the results.
The first row shows the results of using only the in-
domain parallel corpus. The second row shows re-
sults of the baseline. The third row shows the results
of using linear interpolation without data selection.
Comparing the results for the three equations, we
see that Eq. 1 gives the best performance. It out-
performs not only the baseline but also the results
obtained by using all of the (2.5M) out-of-domain
data and linear interpolation.
The results of using source language side infor-
mation (Eq. 2) and information from both language
sides (Eq. 3) still showed better performance than
the baseline system did.
4 Comparison with conventional method
There are few studies on data selection for trans-
lation model training. Most successful and recent
study was that of (Lu et al, 2007). They applied
the TF*IDF framework to translation model train-
ing corpus selection. According to their study, they
obtained a 28% translation model size reduction (A
2.41G byte model was reduced to a 1.74G byte
model) and 1% BLEU score improvement (BLEU
score increased from 23.63% to 24.63%). Although
there results are not directly comparable to ours [??]
because of the differences in the experimental set-
ting, our method outperforms theirs for both aspects
of model size reduction and translation performance
improvement (50% model size reduction and 1.76%
BLEU score improvement).
5 Conclusions
We proposed a method of selecting training sets for
training translation models that dramatically reduces
the sizes of the training set and translation models.
We carried out experiments using data from the
Chinese-to-English translation track of the IWSLT
evaluation campaign. The experimental results indi-
cated that our method reduced the size of the training
set by 48%. The obtained translation models were
half the size of the baseline.
The proposed method also had good translation
performance. Our experimental results demon-
strated that an SMT system with a half-size transla-
tion model obtained with our method improved the
BLEU score by 1.76%. (Linear interpolation im-
proved BLEU score by 1.61% and data selection im-
proved BLEU score by an additional 0.15%.)
659
Table 3: Results of data selection by using Eqs. 1, 2, and 3
In-domain Out-of-domain
40 K 0 N/A N/A 21.68%
40 K 2.5 M N/A N/A 23.16%
40 K 2.5 M N/A 0.7 24.77%
40 K 1.2 M Eq. 1 0.7 24.92%
40 K 1.2 M Eq. 2 0.8 24.76%
40 K 1.2 M Eq. 3 0.6 24.56%
Optimal weight for
out-of-domain model
BLEU score
Corpus size (Sentence pairs)
Selection method
We also compared the selections using source lan-
guage side information, target language side infor-
mation and information from both language sides.
The experimental results show that target language
side information gives the best performance in the
experimental setting. However, there are no large
differences among the different selection results.
The results are encouraging because they show that
the in-domain mono-lingual corpus is sufficient to
select training data from the out-of-domain parallel
corpus.
References
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. In Tech-
nical report TR-10-98, Center for Research in Com-
puting Technology (Harvard University).
I. J Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3):237?264.
IWSLT. 2006. IWSLT: International Work-
shop on Spoken Language Translation.
http://www.slc.atr.jp/IWSLT2006/.
G. Kikui, S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech trans-
lation. In IEEE Transactions on Audio, Speech and
Language Processing, volume 14(5), pages 1674?
1682.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. Proc. of Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), pages 127?133.
LDC. 2007. Linguistic Data Consortium.
http://www.ldc.upenn.edu/.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by training
data selection and optimization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 343?
350.
X Ma. 2006. Champollion: A Robust Parallel Text Sen-
tence Aligner. In Proc. of international conference on
Language Resources and Evaluation (LREC), pages
489?492.
NIST. 2002. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt/
mt2001/resource/.
NIST. 2006. The 2006 NIST Machine
Translation Evaluation Plan (MT06).
http://www.nist.gov/speech/tests/mt/
doc/mt06 evalplan.v3.pdf.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum Error Rate Training for Sta-
tistical Machine Translation. Proc. of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318.
R. Zhang, G. Kikui, and E. Sumita. 2006. Subword-
based Tagging by Conditional Random Fields for Chi-
nese Word Segmentation. Proc. of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL), Short Paper:193?196.
660
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 33?41, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Unsupervised SVM Classifier for Answer Selection in Web
Question Answering
Youzheng Wu, Ruiqiang Zhang, Xinhui Hu, and Hideki Kashioka
National Institute of Information and Communications Technology (NICT),
ATR Spoken Language Communication Research Labs.
2-2-2 Hikaridai ?Keihanna Science City? Kyoto 619-0288 Japan
{Youzheng.wu,Ruiqiang.zhang,Xinhui.hu,Hideki.kashioka}@atr.jp
Abstract
Previous machine learning techniques for
answer selection in question answering
(QA) have required question-answer train-
ing pairs. It has been too expensive and
labor-intensive, however, to collect these
training pairs. This paper presents a novel
unsupervised support vector machine (U-
SVM) classifier for answer selection, which
is independent of language and does not re-
quire hand-tagged training pairs. The key
ideas are the following: 1. unsupervised
learning of training data for the classifier by
clustering web search results; and 2. select-
ing the correct answer from the candidates
by classifying the question. The compara-
tive experiments demonstrate that the pro-
posed approach significantly outperforms
the retrieval-based model (Retrieval-M), the
supervised SVM classifier (S-SVM), and the
pattern-based model (Pattern-M) for answer
selection. Moreover, the cross-model com-
parison showed that the performance rank-
ing of these models was: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
1 Introduction
The purpose of answer selection in QA is to se-
lect the exact answer to the question from the ex-
tracted candidate answers. In recent years, many
supervised machine learning techniques for answer
selection in open-domain question answering have
been investigated in some pioneering studies [Itty-
cheriah et al 2001; Ng et al 2001; Suzuki et al
2002; Sasaki, et al 2005; and Echihabi et al 2003].
Compared with retrieval-based [Yang et al 2003],
pattern-based [Ravichandran et al 2002 and Soub-
botin et al 2002], and deep NLP-based [Moldovan
et al 2002, Hovy et al 2001; and Pasca et al 2001]
answer selection, machine learning techniques are
more effective in constructing QA components from
scratch. These techniques suffer, however, from the
problem of requiring an adequate number of hand-
tagged question-answer training pairs. It is too ex-
pensive and labor intensive to collect such training
pairs for supervised machine learning techniques.
To tackle this knowledge acquisition bottleneck,
this paper presents an unsupervised SVM classifier
for answer selection, which is independent of lan-
guage and question type, and avoids the need for
hand-tagged question-answer pairs. The key ideas
are as follows:
1. Regarding answer selection as a kind of classi-
fication task and adopting an SVM classifier;
2. Applying unsupervised learning of pseudo-
training data for the SVM classifier by cluster-
ing web search results;
3. Training the SVM classifier by using three
types of features extracted from the pseudo-
training data; and
4. Selecting the correct answer from the candidate
answers by classifying the question. Note that
this means classifying a question into one of
the clusters learned by clustering web search
results. Therefore, our classifying the question
33
Figure 1: Web Question Answering Architecture
is different from conventional question classifi-
cation (QC) [Li et al 2002] that determines the
answer type of the question.
The proposed approach is fully unsupervised and
starts only from a user question. It does not require
richly annotated corpora or any deep linguistic tools.
To the best of our knowledge, no research on this
kind of study we discuss here has been reported.
Figure 1 illustrates the architecture of our web QA
approach. The S-SVM and Pattern-M models are
included for comparison.
Because the focus of this paper just evaluates the
answer selection part, our approach requires knowl-
edge of the answer type to the question in order to
find candidate answers, and that the answer must be
a NE for convenience in candidate extraction. Ex-
periments using Chinese versions of the TREC 2004
and 2005 test data sets show that our approach sig-
nificantly outperforms the S-SVM for answer selec-
tion, with a top 1 score improvement of more than
20%. Results obtained with the test data set in [Wu
et al 2004] show that the U-SVM increases the
top 1/mrr 5/top 5 scores by 5.95%/6.06%/8.68%
as compared with the Pattern-M. Moreover, our
cross-model comparison demonstrates that the per-
formance ranking of all models considered is: U-
SVM > Pattern-M > S-SVM > Retrieval-M.
2 Comparison among Models
Related researches on answer selection in QA can be
classified into four categories. The retrieval-based
model [Yang et al 2003] selects a correct answer
from the candidates according to the distance be-
tween a candidate and all question keywords. This
model does not work, however, if the question and
the answer-bearing sentences do not match on the
surface. The pattern-based model [Ravichandran
et al 2002 and Soubbotin et al 2002] first clas-
sifies the question into predefined categories, and
then extracts the exact answer by using answer pat-
terns learned off-line. Although the pattern-based
model can obtain high precision for some prede-
fined types of questions, it is difficult to define ques-
tion types in advance for open-domain question an-
swering. Furthermore, this model is not suitable for
all types of questions. The deep NLP-based model
[Moldovan et al 2002; Hovy et al 2001; and Pasca
et al 2001] usually parses the user question and an
answer-bearing sentence into a semantic represen-
tation, and then semantically matches them to find
the answer. This model has performed very well at
TREC workshops, but it heavily depends on high-
performance NLP tools, which are time consuming
and labor intensive for many languages. Finally, the
machine learning-based model has also been inves-
tigated. current models of this type are based on su-
pervised approaches [Ittycheriah et al 2001; Ng et
al. 2001; Suzuki et al 2002; and Sasaki et al 2005]
that are heavily dependent on hand-tagged question-
answer training pairs, which not readily available.
In response to this situation, this paper presents
the U-SVM for answer selection in open-domain
web question answering system. Our U-SVM has
the following advantages over supervised machine
learning techniques. First, the U-SVM classifies
questions into a question-dependent set of clusters,
and the answer is the name of a question cluster.
In contrast, most previous models have classified
candidates into positive and negative. Second, the
U-SVM automatically learns the unique question-
dependent clusters and the pseudo-training for each
34
Table 1: Comparison of Various Machine Learning Techniques
System Model Key Idea Training Data
[Ittycheriah et al 2001] ME Classifier Classifying candidates into positive
and negative
5,000 English
Q-A pairs
[Suzuki et al 2002] SVM Classifier Classifying candidates into positive
and negative
1358 Japanese
Q-A pairs
[Echihabi et al 2003] N-C Model Selecting correct answer by aligning
question with sentences
90,000 English
Q-A pairs
[Sasaki et al 2005] ME Classifier Classifying words in sentences into an-
swer and non-answer words
2,000 Japanese
Q-A pairs
Our U-SVM Model SVM Classifier Classifying question into a set of
question-dependent clusters
No Q-A pairs
question. This differs from the supervised tech-
niques, in which a large number of hand-tagged
training pairs are shared by all of the test ques-
tions. In addition, supervised techniques indepen-
dently process the answer-bearing sentences, so the
answers to the questions may not always be ex-
tractable because of algorithmic limitations. On the
other hand, the U-SVM can use the interdependence
between answer-bearing sentences to select the an-
swer to a question.
Table 1 compares the key idea and training data
used in the U-SVM with those used in the supervised
machine learning techniques. Here, ME means the
maximum entropy model, and N-C means the noisy-
channel model.
3 The U-SVM
The essence of the U-SVM is to regard answer selec-
tion as a kind of text categorization-like classifica-
tion task, but with no training data available. In the
U-SVM, the steps of ?clustering web search results?,
?classifying the question?, and ?training SVM clas-
sifier? play very important roles.
3.1 Clustering Web Search Results
Web search results, such as snippets returned by
Google, usually include a mixture of multiple
subtopics (called clusters in this paper) related to
the user question. To group the web search results
into clusters, we assume that the candidate answer in
each Google snippet can represent the ?signature? of
its cluster. In other words, the Google snippets con-
taining the same candidate are regarded as aligned
snippets, and thus belong to the same cluster. Web
search results are clustered in two phases.
? A first-stage Google search (FGS) is ap-
plied to extract n candidate answers
{c1, c2, . . . , cn} from the top m Google
snippets {s1, s2, . . . , sm} by a NER tool
[Wu et al 2005]. Those snippets containing
the candidates {ci} and at least one ques-
tion keyword {qi} are retained. Finally,
the retained snippets {s1, s2, . . . , sm} are
clustered into n clusters {C1, C2, . . . , Cn}
by clustering web search results, that is,
If a snippet includes L different candidates,
the snippet belongs to L different clusters.
If the candidates of different snippets are
the same, these snippets belong to the same
clusters.
Consequently, the number of clusters {Ci} is
fully determined by the number of candidates
{ci}, and the cluster name of a cluster Ci is the
candidate answer ci. Up to this point, we have
obtained clusters and sample snippets for each
cluster that will be used as training data for the
SVM classifier. Because this training data is
learned automatically, rather than hand-tagged,
we call it pseudo-training data.
? A second-stage Google search (SGS) is ap-
plied to resolve data sparseness in the pseudo-
training samples learned through the FGS. The
FGS data may have very few training snip-
pets in some clusters, so more snippets must
be collected. Note that this step just learns new
35
Google snippets into the clusters learned by the
FGS, but does not add new clusters.
For each candidate answer ci:
Combine the original query q = {qi} and
the candidate ci to form a new query q? =
{q, ci}.
Submit q? to Google and download the top
50 Google snippets.
Retain the snippets containing the candi-
date ci and at least one keyword qi.
Group the retained snippets into n clusters
to form the new pseudo-training data.
End
Here, we give an example illustrating the prin-
ciple of clustering web search results in the
FGS. In submitting TREC 2004 test question 1.1
?when was the first Crip gang started?? to Google
(http://www.google.com/apis), we extract n(= 8)
different candidates from the top m(= 30) Google
snippets. The Google snippets containing the same
candidates are aligned snippets, and thus the 12 re-
tained snippets are grouped into 8 clusters, as listed
in Table 2. This table roughly indicates that the snip-
pets with the same candidate answers contain the
same sub-meanings, so these snippets are considered
as aligned snippets. For example, all Google snip-
pets that contain the candidate answer 1969 express
the time of establishment of ?the first Crip gang?.
In summary, the U-SVM uses the result of ?clus-
tering web search results? as the pseudo-training
data of the SVM classifier, and then classifies user
question into one of the clusters for answer selec-
tion. On the one hand, the clusters and their names
are based on candidate answers to question; on the
other hand, candidates are dependent on question.
Therefore, the clusters are question-dependent.
3.2 Classifying Question
Using the pseudo-training data obtained by cluster-
ing web search results to train the SVM classifier,
we classify user questions into a set of question-
dependent clusters and assume that the correct an-
swer is the name of the question cluster that is as-
signed by the trained U-SVM classifier. For the
above example, if the U-SVM classifier, trained on
the pseudo-training data listed in Table 2, classifies
the above test question into a cluster whose name is
1969, then the cluster name 1969 is the answer to
the question.
This paper selects LIBSVM toolkit1 to implement
the SVM classifier. The kernel is the radical basis
function with the parameter ? = 0.001 in the exper-
iments.
3.3 Feature Extraction
To classify the question into a question-dependent
set of clusters, the U-SVM classifier extracts three
types of features.
? A similarity-based feature set (SBFS) is
extracted from the Google snippets. The SBFS
attempts to capture the word overlap between
a question and a snippet. The possible values
range from 0 to 1.
SBFS Features
percentage of matched keywords (KWs)
percentage of mismatched KWs
percentage of matched bi-grams of KWs
percentage of matched thesauruses
normalized distance between candidate and
KWs
To compute the matched thesaurus feature, we
adopt TONGYICICILIN 2 in the experiments.
? A Boolean match-based feature set (BMFS) is
also extracted from the Google snippets. The
BMFS attempts to capture the specific key-
word Boolean matches between a question and
a snippet. The possible values are true or false.
BMFS Features
person names are matched or not
location names are matched or not
organization names are matched or not
time words are matched or not
number words are matched or not
root verb is matched or not
candidate has or does not have bi-gram in
snippet matching bi-gram in question
candidate has or does not have desired
named entity type
? A window-based word feature set (WWFS)
is a set of words consisting of the words
1http://www.csie.ntu.edu.tw/ cjlin/libsvm/
2A Chinese Thesaurus Lexicon
36
Table 2: Clustering Web Search Results
Cluster Name Google Snippet
1969 It is believed that the first Crip gang was formed in late 1969. During this time in
Los Angeles there were ...
... the first Bloods and Crips gangs started forming in Los Angeles in late 1969, the
Island Bloods sprung up in north Pomona ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
August 8, 2005 High Country News ? August 8, 2005: The Gangs of Zion
2004 2004 main 1 Crips 1.1 FACTOID When was the first Crip gang started? 1.2 FAC-
TOID What does the name mean or come...
1972 One of the first-known and publicized killings by Crip gang members occurred at
the Hollywood Bowl in March 1972.
1971 Williams joined Washington in 1971, forming the westside faction of what had
come to be called the Crips.
The Crips gang formed as a kind of community watchdog group in 1971 after the
demise of the Black Panthers. ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
1982 Oceanside police first started documenting gangs in 1982, when five known gangs
were operating in the city: the Posole Locos...
mid-1990s Street Locos; Deep Valley Bloods and Deep Valley Crips. By the mid-1990s, gang
violence had ...
1970s The Blood gangs started up as opposition to the Crips gangs, also in the 1970s, and
the rivalry stands to this day ...
preceding {wi?5, . . . , wi?1} and following
{wi+1, . . . , wi+5} the candidate answer. The
WWFS features can be regarded as a kind of
relevant snippets-based question keywords ex-
pansion. By extracting the WWFS feature set,
the feature space in the U-SVM becomes ques-
tion dependent, which may be more suitable for
classifying the question. The number of classi-
fication features in the S-SVM must be fixed,
however, because all questions share the same
training data. This is one difference between
the U-SVM and the supervised SVM classifier
for answer selection. Each word feature in the
WWFS is weighted using its ISF value.
ISF (wj , Ci) =
N(wj , Ci) + 0.5
N(wj) + 0.5
(1)
where N(wj) is the total number of the
snippets containing word feature wj , and
N(wj , Ci) is the number of snippets in cluster
Ci containing word feature wj .
When constructing question vector, we assume
that the question is an ideal question that con-
tains all the extracted WWFS words. There-
fore, the values of the WWFS word features in
question vector are 1. Similarly, the values of
the SBFS and BMFS features in question vec-
tor are also estimated by self-similarity calcu-
lation.
4 Experiments
4.1 Data Sets
For the experiments, no English named entity recog-
nition (NER) tool is in our hand at the time of
the experiments; therefore, we validate the U-SVM
37
in terms of Chinese web QA using three test data
sets, which will be published with this paper3. Al-
though the U-SVM is independent of the question
types, for convenience in candidate extraction, only
those questions whose answers are named entities
are selected. The three test data sets are CTREC04,
CTREC05 and CTEST05. CTREC04 is a set of
178 Chinese questions translated from TREC 2004
FACTOID testing questions. CTREC05 is a set of
279 Chinese questions translated from TREC 2005
FACTOID testing questions. CTEST05 is a set of
178 Chinese questions found in [Wu et al 2004]
that are similar to TREC testing questions except
that they are written in Chinese. Figure 2 breaks
down the types of questions (manually assigned) in
the CTREC04 and CTREC05 data sets. Here, PER,
LOC, ORG, TIM, NUM, and CR refer to questions
whose answers are a person, location, organization,
time, number, and book or movie, respectively.
Figure 2: Statistics of CTEST05
To collect the question-answer training data for
the S-SVM, we submitted 807 Chinese questions to
Google and extracted the candidates for each ques-
tion from the top 50 Google snippets. We then man-
ually selected the snippets containing the correct
answers as positive snippets, and designated all of
the other snippets as negative snippets. Finally, we
collected 807 hand-tagged Chinese question-answer
pairs as the training data of S-SVM called CTRAIN-
DATA.
4.2 Evaluation Method
In the experiments, the top m(= 50) Google snip-
pets are adopted to extract candidates by using a
3Currently no public testing question set for simplified Chi-
nese QA is available.
Chinese NER tool [Wu et al 2005]. The number of
the candidates extracted from the top m(= 50) snip-
pets, n, is adaptive for different questions but it does
not exceed 30. The results are evaluated in terms
of two scores, top n and mrr 5. Here, top n is the
rate at which at least one correct answer is included
in the top n answers, while mrr 5 is the average re-
ciprocal rank (1/n) of the highest rank n(n ? 5) of
a correct answer to each question.
4.3 U-SVM vs. Retrieval-M
The Retrieval-M selects the candidate with the short-
est distances to all question keywords as the cor-
rect answer. In this experiment, the Retrieval-M
is implemented based on the snippets returned by
Google, while the U-SVM is based on the SGS data,
the SBFS and BMFS feature. Table 3 summarizes
the comparative performance.
Table 3: Comparison of Retrieval-M and U-SVM
Retrieval-M U-SVM
top 1 27.84% 53.61%
CTREC04 mrr 5 43.67% 66.25%
top 5 71.13% 88.66%
top 1 34.00% 50.00%
CTREC05 mrr 5 48.20% 62.38%
top 5 71.33% 82.67%
The table shows that the U-SVM greatly improves
the performance of the Retrieval-M: the top 1 im-
provements for CTREC04 and CTREC05 are about
25.8% and 16.0%, respectively. This experiment
demonstrates that the assumptions used here in clus-
tering web search results and in classifying the ques-
tion are effective in many cases, and that the U-SVM
benefits from these assumptions.
4.4 U-SVM vs. S-SVM
To explore the effectiveness of our unsupervised
model as compared with the supervised model, we
conduct a cross-model comparison of the S-SVM
and the U-SVM with the SBFS and BMFS feature
sets. The U-SVM results are compared with the S-
SVM results for CTREC04 and CTREC05 in Ta-
bles 4 and 5, respectively. The S-SVM is trained
on CTRAINDATA.
These tables show the following:
38
Table 4: Comparison of U-SVM and S-SVM on
CTREC04
FGS SGS
top 1 S-SVM 30.93% 39.18%
U-SVM 45.36% 53.61%
mrr 1 S-SVM 45.36% 53.54%
U-SVM 57.44% 66.25%
top 5 S-SVM 71.13% 79.38%
U-SVM 76.29% 88.66%
Table 5: Comparison of U-SVM and S-SVM on
CTREC05
FGS SGS
top 1 S-SVM 30.00% 33.33%
U-SVM 48.00% 50.00%
mrr 1 S-SVM 45.59% 48.67%
U-SVM 58.01% 62.38%
top 5 S-SVM 72.00% 74.67%
U-SVM 75.33% 82.67%
? The proposed U-SVM significantly outper-
forms the S-SVM for all measurements and
all test data sets. For the CTREC04 test data
set, the top1 improvements for the FGS and
SGS data are about 14.5% and 14.4%, respec-
tively. For the CTREC05 test data set, the top1
score for the FGS data increases from 30.0%
to 48.0%, and the top 1 score for the SGS data
increases from 33.3% to 50.0%. Note that the
SBFS and BMFS features here is fewer than the
features in [Ittycheriah et al 2001; Suzuki et
al. 2002], but the comparison is still effective
because the models are compared in terms of
the same features. In the S-SVM, all questions
share the same training data, while the U-SVM
uses the unique pseudo-training data for each
question. This is the main reason why the U-
SVM performs better than the S-SVM does.
? The SGS data is greatly helpful for both
the U-SVM and the S-SVM. Compared with
the FGS data, the top 1/mrr 5/top 5 im-
provements for the S-SVM and the U-SVM
on CTREC04 are 8.25%/8.18%/8.25% and
7.25%/8.81%/12.37%. The SGS can be re-
garded as a kind of query expansion. The rea-
sons for this improvement are: the data sparse-
ness in FGS data is partially resolved; and the
use of the Web to introduce data redundancy
is helpful. [Clarke et al 2001; Magnini et al
2002; and Dumais et al 2002].
In the S-SVM, all of the test questions share the
same hand-tagged training data, so the WWFS fea-
tures cannot be easily used [Ittycheriah et al 2002;
Suzuki, et al 2002]. Tables 6 and 7 compare
the performances of the U-SVM with the (SBFS +
BMFS) features, the WWFS features, and combina-
tion of three types of features for the CTREC04 and
CTREC05 test data sets, respectively.
Table 6: Performances of U-SVM for Different Fea-
tures on CTREC04
SBFS+BMFS WWFS Combination
top 1 53.61% 46.39% 60.82%
mrr 5 66.25% 59.19% 71.31%
top 5 88.66% 81.44% 88.66%
Table 7: Performances of U-SVM for Different Fea-
tures on CTREC05
SBFS+BMFS WWFS Combination
top 1 50.00% 49.33% 57.33%
mrr 5 62.38% 59.26% 65.61%
top 5 82.67% 74.00% 80.00%
These tables report that combining three types
of features can improve the performance of
the U-SVM. Using a combination of features
with the CTREC04 test data set results in the
best performances: 60.82%/71.31%/88.66% for
top 1/mrr 5/top 5. Similarly, as compared with
using the (SBFS + BMFS) and WWFS features, the
improvements from using a combination of features
with the CTREC05 test data set are 7.33%/3.23%/-
2.67% and 8.00%/6.35%/6.00%, respectively. The
results also demonstrate that the (SBFS + BMFS)
features are more important than the WWFS fea-
tures.
These comparative experiments indicate that the
U-SVM performs better than the S-SVM does, even
though the U-SVM is an unsupervised technique and
no hand-tagged training data is provided. The aver-
39
age top 1 improvements for both test data sets are
both more than 20%.
4.5 U-SVM vs. Pattern-M vs. S-SVM
To compare the U-SVM with the Pattern-M and
the S-SVM, we use the CTEST05 data set, shown
in Figure 3. The CTEST05 includes 14 different
question types, for example, Inventor Stuff (with
question like ?Who invented telephone??), Event-
Day (with question like ?when is World Day for Wa-
ter??), and so on. The Pattern-M uses the depen-
dency syntactic answer patterns learned in [Wu et
al. 2007] to extract the answer, and named entities
are also used to filter noise from the candidates.
Figure 3: Statistics of CTEST05
Table 8 summarizes the performances of the U-
SVM, Pattern-M, and S-SVM models on CTEST05.
Table 8: Comparison of U-SVM, Pattern-M and S-
SVM on CTEST05
S-SVM Pattern-M U-SVM
top 1 44.89% 53.14% 59.09%
mrr 5 56.49% 61.28% 67.34%
top 5 74.43% 73.14% 81.82%
The results in the table show that the U-SVM
significantly outperforms the S-SVM and Pattern-
M, while the S-SVM underperforms the Pattern-
M. Compared with the Pattern-M, the U-SVM in-
creases the top 1/mrr 5/top 5 scores by 5.95%/
6.06%/8.68%, respectively. The reasons may lie in
the following:
? The Chinese dependency parser influences de-
pendency syntactic answer-pattern extraction,
and thus degrades the performance of the
Pattern-M model.
? The imperfection of Google snippets affects
pattern matching, and thus adversely influences
the Pattern-M model. From the cross-model
comparison, we conclude that the performance
ranking of these models is: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
5 Conclusion and Future Work
This paper presents an unsupervised machine learn-
ing technique (called the U-SVM) for answer selec-
tion that is validated in Chinese open-domain web
QA. Regarding answer selection as a kind of classifi-
cation task, the U-SVM automatically learns clusters
and pseudo-training data for each cluster by cluster-
ing web search results. It then selects the correct
answer from the candidates according to classifying
the question. The contribution of this paper is that
it presents an unsupervised machine learning tech-
nique for web QA that starts with only a user ques-
tion. The results of our experiments with three test
data sets are encouraging. As compared with the
S-SVM, the top 1 performances of the U-SVM for
the CTREC04 and CTREC05 data sets are signifi-
cantly improved, at more than 20%. Moreover, the
U-SVM performs better than the Retrieval-M and
the Pattern-M.
These experiments have only validated the U-
SVM on named entity types of questions that ac-
count for about 82% of all TREC2004 and 2005
FACTOID test questions. In fact, our technique is
independent of question types only if the candidates
can be extracted. In the future, we will explore the
effectiveness of our technique for the other types of
questions. The web search results clustering in the
U-SVM defines that a candidate in a Google snip-
pet can represent the ?signature? of its cluster. This
definition, however, is not always effective. To fil-
ter noise in the pseudo-training data, we will extract
relations between the candidates and the keywords
as the cluster signatures of Google snippets. More-
over, applying the U-SVM to QA systems in other
languages, like English and Japanese, will also be
included in our future work.
40
References
Abdessamad Echihabi, and Daniel Marcu. 2003. A
Noisy-Channel Approach to Question Answering. In
Proc. of ACL-2003, Japan.
Abraham Ittycheriah, Salim Roukos. 2002. IBM?s Sta-
tistical Question Answering System-TREC 11. In Proc.
of TREC-11, Gaithersburg, Maryland.
Bernardo Magnini, Matteo Negri, Roberto Prevete,
Hristo Tanev. 2002. Is It the Right Answer? Exploit-
ing Web Redundancy for Answer Validation. In Proc.
of ACL-2002, Philadelphia, pp. 425 432.
Charles L. A. Clarke, Gordon V. Cormack, Thomas R.
Lynam. Exploiting Redundancy in Question Answer-
ing In Proc. of SIGIR-2001, pp 358?365, 2001.
Christopher Pinchak, Dekang Lin. 2006. A Probabilistic
Answer Type Model. In Proc. of EACL-2006, Trento,
Italy, pp. 393-400.
Dan Moldovan, Sanda Harabagiu, Roxana Girju, et al
2002. LCC Tools for Question Answering. NIST Spe-
cial Publication: SP 500-251, TREC-2002.
Deepak Ravichandran, Eduard Hovy. 2002. Learning
Surface Text Patterns for a Question Answering Sys-
tem. In Proc. of the 40th ACL, Philadelphia, July
2002.
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin. 2001. The
Use of External Knowledge of Factoid QA. In Proc.
of TREC 2001, Gaithersburg, MD, U.S.A., November
13-16, 2001.
Hui Yang, Tat-Seng Chua. 2003. QUALIFIER: Question
Answering by Lexical Fabric and External Resources.
In Proc. of EACL-2003, page 363-370.
Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia. 2001.
Question Answering Using a Large Text Database: A
Machine Learning Approach. In Proc. of EMNLP-
2001, pp66-73 (2001).
Jun Suzuki, Yutaka Sasaki, Eisaku Maeda. 2002. SVM
Answer Selection for Open-Domain Question Answer-
ing. In Proc. of Coling-2002, pp. 974 980 (2002).
Marius Pasca. 2001. A Relational and Logic Represen-
tation for Open-Domain Textual Question Answering.
In Proc. of ACL (Companion Volume) 2001: 37-42.
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use of
Patterns for Detection of Likely Answer Strings: A Sys-
tematic Approach. In Proc. of TREC-2002, Gaithers-
burg, Maryland, November 2002.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andre Ng. Web Question Answering: Is More
Always Better?. In Proc. SIGIR-2002, pp 291?298,
2002.
Xin Li, and Dan Roth. 2002. Learning Question Classi-
fication. In Proc. of the 19th International Conference
on Computational Linguistics, Taibai, 2002.
Youzheng Wu, Hideki Kashioka, Jun Zhao. 2007. Us-
ing Clustering Approaches to Open-domain Question
Answering. In Proc. of CICLING-2007, Mexico City,
Mexico, pp506 517, 2007.
Youzheng Wu, Jun Zhao and Bo Xu. 2005. Chinese
Named Entity Recognition Model Based on Multiple
Features. In Proc. of HLT/EMNLP-2005, Vancouver,
Canada, pp.427-434.
Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu.
2004. Building an Evaluation Platform for Chinese
Question Answering Systems. In Proc. of the First
NCIRCS, China, December, 2004.
Yutaka Sasaki. 2005. Question Answering as Question-
Biased Term Extraction: A New Approach toward
Multilingual QA. In Proc. of ACL-2005, pp.215-222.
41
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129?1139,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning Recurrent Event Queries for Web Search
Ruiqiang Zhang and Yuki Konda and Anlei Dong
Pranam Kolari and Yi Chang and ZhaohuiZheng
Yahoo! Inc
701 First Avenue, Sunnyvale, CA94089
Abstract
Recurrent event queries (REQ) constitute a
special class of search queries occurring at
regular, predictable time intervals. The fresh-
ness of documents ranked for such queries is
generally of critical importance. REQ forms a
significant volume, as much as 6% of query
traffic received by search engines. In this
work, we develop an improved REQ classi-
fier that could provide significant improve-
ments in addressing this problem. We ana-
lyze REQ queries, and develop novel features
from multiple sources, and evaluate them us-
ing machine learning techniques. From histor-
ical query logs, we develop features utilizing
query frequency, click information, and user
intent dynamics within a search session. We
also develop temporal features by time series
analysis from query frequency. Other gener-
ated features include word matching with re-
current event seed words and time sensitiv-
ity of search result set. We use Naive Bayes,
SVM and decision tree based logistic regres-
sion model to train REQ classifier. The re-
sults on test data show that our models outper-
formed baseline approach significantly. Ex-
periments on a commercial Web search en-
gine also show significant gains in overall rel-
evance, and thus overall user experience.
1 Introduction
REQ pertains to queries about events which oc-
cur at regular, predictable time intervals, most often
weekly, monthly, annually, bi-annually, etc. Natu-
rally, users issue REQ periodically. REQ usually re-
fer to:
Organized public events such as festivals, confer-
ences, expos, sports competitions, elections: winter
olympics, boston marathon, the International Ocean
Research Conference, oscar night.
Public holidays and other noteworthy dates: labor day,
date of Good Friday, Thanksgiving, black friday.
Products with annual model releases, such as car models:
ford explorer, prius.
Lottery drawings: California lotto results.
TV shows and programs which are currently running:
American idol, Inside Edition.
Cultural related activities: presidential election, tax re-
turn, 1040 form.
Our interest in studying REQ arises from the chal-
lenge imposed on Web search ranking. To illustrate
this, we show an example in Fig. 1 that snapshots
the real ranking results of the query, EMNLP, is-
sued in 2010 when the authors composed this pa-
per, on Google search engine. It is obvious the
ranking is not satisfactory because the page about
EMNLP2008 is on the first position in 2010. Ide-
ally, the page about EMNLP2010 on the 6th position
should be on the first position even if users don?t
explicitly issue the query, EMNLP 2010, because
EMNLP is a REQ. The query, ?EMNLP?, implic-
itly, without a year qualifier, needs to be served the
most recent pages about ?EMNLP?.
A better search ranking result cannot be achieved
if we do not categorize ?EMNLP? as a REQ, and
provide special ranking treatment to such queries.
Existing search engines adopt a fairly involved rank-
ing algorithm to order Web search results by con-
sidering many factors. Time is an important fac-
tor but not the most critical. The page?s rank-
ing score mostly depends on other features such
as tf-idf (Salton and McGill, 1983), BM25 (Jones
1129
Figure 1: A real problematic ranking result by Google for
a REQ query, ?EMNLP?. The EMNLP2010 page should
be on the 1st position.
et al, 2000), anchor text, historical clicks, pager-
ank (Brin and Page, 1998), and overall page qual-
ity. New pages about EMNLP2010 obtain less fa-
vorable feature values than the pages of 2009 earlier
in terms of anchor text, click or pagerank because
they have existed for a shorter time and haven?t ac-
cumulated sufficient popularity to make them stand
out. Without special treatment, the new pages about
?EMNLP2010? will typically not be ranked appro-
priately for the users.
Typically, a recurrent event is associated with a
root, and spawns a large set of queries. Oscar,
for instance, is a recurrent event about the annual
Academy Award. Based on this, queries like ?oscar
best actress?, ?oscar best dress?, ?oscar best movie
award?, are all recurrent event queries. As such,
REQ is a highly frequent category of query in Web
search. By Web search query log analysis, we ob-
serve that there about 5-6% queries of total query
volume belongs to this category.
In this work, we learn if a query is in the REQ
class, by effectively combining multiple features.
Our features are developed through analysis of his-
torical query logs. We discuss our approaches in de-
tail in Section 3. We then develop a REQ classi-
fier where all the features are integrated by machine
learning models. We use Naive Bayes, SVM and de-
cision tree based logistic regression models. These
models are described in Section 4. Our experiments
for REQ classifier and Web search ranking are de-
tailed in Section 5 and 6.
2 Related Work
We found our work were related to two other prob-
lems: general query classification and time-sensitive
query classification. For general query classifica-
tion, the task is to assign a Web search query to
one or more predefined categories based on its top-
ics. In the query classification contest in KDD-
CUP 2005 (Li et al, 2005), seven categories and
67 sub-categories were defined. The winning so-
lution (Shen et al, 2005) used multiple classifiers
integrated by ensemble method. The difficulties for
query classification are from short queries, lack of
labeled data, and query sense ambiguity. Most pop-
ular studies use query log, web search results, unla-
beled data to enrich query classification (Shen et al,
2006; Beitzel et al, 2005), or use document classifi-
cation to predict query classification (Broder et al, ).
General query classification is also studied for query
intent detection by (Li et al, 2008).
There are many prior works to study the time sen-
sitivity issue in web search. For example, Baeza-
Yates et al (Baeza-Yates et al, 2002) studied the re-
lation between the web dynamics, structure and page
quality, and demonstrated that PageRank is biased
against new pages. In T-Rank Light and T-Rank al-
gorithms (Berberich et al, 2005), both activity (i.e.,
update rates) and freshness (i.e., timestamps of most
recent updates) of pages and links are taken into ac-
count for link analysis. Cho et al (Cho et al, 2005)
proposed a page quality ranking function in order to
alleviate the problem of popularity-based ranking,
and they used the derivatives of PageRank to fore-
cast future PageRank values for new pages. Pandey
et al (Pandey et al, 2005) studied the tradeoff be-
tween new page exploration and high-quality page
exploitation, which is based on a ranking method to
randomly promote some new pages so that they can
accumulate links quickly.
More recently, Dong et al (Dong et al, 2010a)
1130
proposed a machine-learned framework to improve
ranking result freshness, in which novel features,
modeling algorithms and editorial guideline are used
to deal with time sensitivities of queries and doc-
uments. In another work (Dong et al, 2010b), they
use micro-blogging data (e.g., Twitter data) to detect
fresh URLs. Novel and effective features are also
extracted for fresh URLs so that ranking recency in
web search is improved.
Perhaps the most related work to this paper is
the query classification approach used in (Zhang
et al, 2009) and (Metzler et al, 2009), in which
year qualified queries (YQQs) are detected based
on heuristic rules. For example, a query contain-
ing a year stamp is an explicit YQQ; if the year
stamp is removed from this YQQ, the remaining part
of this query is also a YQQ, which is called im-
plicit YQQ. Different ranking approaches were used
in (Zhang et al, 2009) and (Metzler et al, 2009)
where (Zhang et al, 2009) boosted pages of the most
latest year while (Metzler et al, 2009) promoted
pages of the most influential years. Similarly, Nunes
et al (Nunes, 2007) applied information extraction
techniques to identify temporal expression in web
search queries, and found 1.5% of queries contain-
ing temporal expression.
Dong et al (Dong et al, 2010a) proposed a
breaking-news query classifier with high accuracy
and reasonable coverage, which works not by mod-
eling each individual topic and tracking it over time,
but by modeling each discrete time slot, and compar-
ing the models representing different time slots. The
buzziness of a query is computed as the language
model likelihood difference between different time
slots. In this approach, both query log and news
contents are exploited to compute language model
likelihood.
Diaz (Diaz, 2009) determined the newsworthiness
of a query by predicting the probability of a user
clicks on the news display of a query. In this frame-
work, the data sources of both query log and news
corpus are leveraged to compute contextual features.
Furthermore, the online click feedback also plays a
critical role for future click prediction.
Konig et al (Knig et al, 2009) estimated the
click-through rate for dedicated news search result
with a supervised model, which is to satisfy the
requirement of adapting quickly to emerging news
event. Some additional corpora such as blog crawl
and Wikipedia is used for buzziness inference. Com-
pared with (Diaz, 2009), different feature and learn-
ing algorithms are used.
Elsas et al (Elsas and Dumais, 2010) studied
improving relevance ranking by detecting document
content change to leverage temporal information.
3 Feature Generation
To better understand our work, we first introduce
three terms. We subdivide all raw queries in query
log into three categories: Explicit Timestamp, Im-
plicit Timestamp, and No Timestamp. An Explicit
Timestamp query contains at least one token being a
time indicator. For example, emnlp 2010, 2007 De-
cember holiday calendar, amsterdam weather sum-
mer 2009, Google Q1 reports 2010. These queries
are considered to conatin time indicators, because
we can regard {2010, 2007, 2009} as year indica-
tor, december as month indicator, {summer, Q1(first
quarter)} as seasonal indicator. To simplify our
work, we only consider the year indicators, 2010,
2007, 2009. Such year indicators are also the most
important and most popular indicators, as noted in
(Zhang et al, 2009). Any query containing at least
one year indicator is an Explicit Timestamp query.
Due to word sense ambiguity, some queries labeled
as Explicit Timestamp by this method may have no
connection with time such as Windows Office 2007,
2010 Sunset Boulevard, or call number 2008. In this
work, we tolerate this type of error because word
sense disambiguation is a peripheral problem for this
task.
Implicit Timestamp queries are resulted by re-
moving all year indicators from the corresponding
Explicit Timestamp queries. For example, the Im-
plicit Timestamp query of emnlp 2010 is emnlp.
All other queries are No Timestamp queries because
they have never been found together with a year in-
dicator.
Classifying queries into the above three cate-
gories depends on the used query log. A search
engine company partner provided us a query log
from 08/01/2009 to 02/29/2010 for this research.
We found the proportions of the three categories
in this query log are 13.8% (Explicit), 17.1% (Im-
plicit) and 69.1% (No Timestamp). These numbers
1131
could be slightly different depending on the source
of query logs. Note that 17.1% of Implicit Times-
tamp queries in the query log is a significant num-
ber. However, not all Implicit Timestamp queries
are REQ. Many Implicit Timestamp queries have no
time sense. They belong to Implicit Timestamp just
because users issued the query with a year indica-
tor through varied intents. For example, ?google? is
found to be an Implicit Timestamp query since there
were many ?google 2008? or ?google 2009? in the
query log.
The next few sections introduce our work in rec-
ognizing recurrent event time sense for Implicit
Timestamp queries. We first focus on features.
There are many features that were exploited in REQ
classifier. We extract these features from query log,
query session log, click log, search results, time se-
ries and NLP morphological analysis.
3.1 Query log analysis
The following features are extracted from query log
analysis:
QueryDailyFrequency: the total counts of the
query divided by the number of the days in the pe-
riod.
ExplicitQueryRatio: Ratio of number of counts
query was issued with year and number of counts
query was issued with or without year. This feature
is the method used by (Zhang et al, 2009).
UniqExplicitQueryCount: Number of uniq Ex-
plicit Timestamp queries associated with query. For
example, if a query was issued with query+2009 and
query+2008, this feature?s value is two.
ChiSquareYearDist: this feature is the distance be-
tween two distributions: one is frequency distribu-
tion over years for all REQ queries. The other is
that for single REQ query. It is calculated through
following steps: (a) Aggregate the frequencies for
all queries for all years. Suppose we observe all
years from 2001 to 2010. So we can get vector,
E = ( a f10
sum1 ,
a f09
sum1 , ...,
a f01
sum1 ) where a fi is the frequency
sum of year 20i for all REQ queries. sum1 =
a f10 + a f09 + ... + a f01, the sum of all year fre-
quency. (b) Given a query, suppose we observe
this query?s yearly frequency distribution is , Oq =
(q f10, q f09, , ..., q f01). q fi is this query?s frequency
for the year 20i. Pad the slot with zeros if no fre-
quency found. The expected distribution for this
query is, Eq = ( sum2?a f10
sum1 ,
sum2?a f09
sum1 , ...,
sum2?a f01
sum1 ),
where sum2 = q f10 + q f09 + ... + q f01 is sum of
all year frequency for the query. (d) Calculate CHI-
squared value to represent the different yearly fre-
quency distribution between Eq and Oq according to
?2 =
?N
i=1
(Oqi ?E
q
i )2
Eqi
. Using CHI square distance as a
method is widely used for statistical hypothesis test.
We found it to be a useful feature for REQ classifier.
3.2 Query reformulation
If users cannot find the newest page by issuing Im-
plicit Timestamp query, they may re-issue the query
using an Explicit Timestamp query. We can detect
this change in a search session (a 30 minutes period
for each query). By finding this kind of behavior
from users, we next extract three features.
UserSwitch: Number of unique users that switched
from Implicit Timestamp queries to Explicit Times-
tamp queries.
YearSwitch: Number of unique year-like tokens
switched by users in a query session.
NormalizedUserSwitch: Feature UserSwitch di-
vided by QueryDailyFrequency.
3.3 Click log analysis
If a query is time sensitive, users may click a
page that displays the year indicator on title or
url. An example that shows year indicator on
url is www.lsi.upc.edu/events/emnlp2010/call.html.
Search engine click log saves all users? click infor-
mation. We used click log to derive the following
features.
YearUrlTop5CTR: Aggregated click through rate
(CTR) of all top five URLs containing a year in-
dicator. CTR of an URL is defined as the number
of clicks of an URL divided by the number of page
views.
YearUrlFPCTR: Aggregated click through rate
(CTR) of all first page URLs containing a year in-
dicator.
3.4 Search engine result set
For each Implicited Timestamp query, we can scrape
the search engine to get search results. We count the
number of titles and urls that contain year indicator.
We use this number as a feature, and generate 6 fea-
tures.
1132
TitleYearTop5: the number of titles containing a
year indication on the top 5 results. This value is
4 in Fig. 1.
TitleYearTop10: the number of titles containing a
year indication on the top 10 results. This value is 6
in Fig. 1.
TitleYearTop30: the number of titles containing a
year indication on the top 30 results.
UrlYearTop5: the number of urls containing a year
indication on the top 5 results. This value is 1 in
Fig. 1.
UrlYearTop10: the number of urls containing a year
indication on the top 10 results.
UrlYearTop30: the number of titles containing a
year indication on the top 30 results.
3.5 Time series analysis
Recurrent event query has periodic occurrence pat-
tern in time series. Top graph of Figure 2 shows the
frequency change of the query, ?Oscar?. The annual
event usually starts from Oscar nomination as ear-
lier as last year December to award announcement
of February this year. So a small spike and a big
spike are observed in the graph to indicate nomina-
tion period and ceremony period. There are a period
of silence between the two periods. The frequency
pattern keeps unchanged each year. We show three
years (2007,8,9) in the graph. By making use of re-
current event queries? periodic properties, we calcu-
lated the query period as a new feature.
We use autocorrelation to calculate the period.
R(?) =
?N??
t=1 (xt ? ?)(xt+? ? ?)
{
?N??
t=1 (xt ? ?)2(xt+? ? ?)2}1/2
where x(t) is query daily frequency. N is the num-
ber of days used for this query. We can get maxi-
mum of 3 years data for some queries but only a few
months for others. R(?) is autocorrelation function.
Peaks (the local biggest R(?) given a time window)
can be detected from R(?) plot. The period T is cal-
culated as the duration between two neighbor peaks.
T = 365 for the query, ?Oscar?. The bottom graph
of Fig. 2 shows the autocorrelation function plot for
the query Oscar.
3.6 Recurrent event seed word list
Many recurrent event queries share some common
words that have recurrent time sense. We list most
new results top schedule
football festival movie world
show day best tax
result calendar honda ford
download exam nfl miss
awards toyota tour sale
american fair list pictures
election game basketball cup
Table 1: Top recurrent event seed words
frequently used recurrent seeds in Table 1. Those
seeds are likely combined with other words to form
new recurrent event queries. For example, the seed,
?new?, can be used by queries ?new bmw cars?,
?whitney houston new songs?, ?apple new iphone?,
or ?hairstyle new?.
To generate the seed list, we tokenized all the
queries from Implicit Timestamp queries and split
all the tokens. We then sort and unique all the to-
kens, and submit top tokens to professional editors
who are asked to pick 8,000 seeds from the top fre-
quent tokens. Some top tokens were removed if they
are not qualified to form recurrent event queries. The
editors took about four days to do the judgment ac-
cording to the token?s time sense and examples of
recurrent event queries. However, this is a one-time
effort. A token will be in the seed if there are many
recurrent event examples formed by this token, by
editors? judgment.
Table 1 shows 32 top seeds. Some seeds connect
with time such as, ?new, schedule, day, best, calen-
dar?; some relate to sports, ?football, game, nfl, tour,
basketball, cup?; some about cars, ?honda, ford, toy-
ota?. The reason why ?miss? is in the seeds is that
there are many annual events about beauty contest
such as ?miss america, miss california, miss korea?.
We use the seed list to generate the following
three features:
AveNumberTokensSeeds: number of tokens that is
in the seed list divided by number of tokens in the
query.
AveNumberTokensNotSeeds: number of tokens
that is not in the seed list divided by number of to-
kens in the query.
DiffNumberTokensSeeds: The difference of the
above two values.
1133
-0 .2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1 15 29 43 57 71 85 99 113 127 141 155 169 183 197 211 225 239 253 267 281 295 309 323 337 351 365 379 393 407 421 435 449 463 477 491 505 519 533 547 561 575 589 603 617 631 645 659 673 687 701 715 729 743 757 771 785 799 813 827 841 855 869 883 897
Figure 2: Frequency waveform(top) and corresponding autocorrelation curve (bottom) for query Oscar.
4 Learning Approach for REQ
The REQ classification is a typical machine learn-
ing task. Given M observed samples used for train-
ing data, {(x0, y0), (x1, y1), ? ? ? , (xM, yM)} where xi is
a feature vector we developed in last section for a
given query. yi is the observation value, {+1,?1},
indicating the class of REQ and non-REQ. The task
is to find the class probability given an unknown fea-
ture vector, x?, that is,
p(y = c|x?), c = +1,?1. (1)
There are a lot of machine learning methods ap-
plicable to implement Eq. 1. In this work, we
adopted three representative methods.
The first method is Naive Bayes method. This
method treats features independent. If x is enx-
tended into feature vector, x = {x0, x1, ? ? ? , xN} then,
p(y = c|x) = 1
Z
p(c)
i=N
?
i=0
p(xi|c)
The second method is SVM. In this work we used
the tool for our experiments, LIBSVM (Chang and
Lin, 2001). Because SVM is a well known approach
and widely used in many classification task, we skip
to describe how to use this tool. Readers can turn to
the reference for more details.
The third method is based on decision tree based
logistic regression model. The probability is given
by the formula below,
p(y = c|x) = 1
1 + e? f (x)
(2)
We employ Gradient Boosted Decision Tree algo-
rithm (Friedman, 2001) to learn the function f (X).
Gradient Boosted Decision Tree is an additive re-
gression algorithm consisting of an ensemble of
trees, fitted to current residuals, gradients of the loss
function, in a forward step-wise manner. It itera-
tively fits an additive model as
ft(x) = Tt(x;?) + ?
T
?
t=1
?tTt(x;?t)
such that certain loss function L(yi, fT (x + i)) is
minimized, where Tt(x;?t) is a tree at iteration t,
weighted by parameter ?t, with a finite number of
parameters, ?t and ? is the learning rate. At iteration
t, tree Tt(x; ?) is induced to fit the negative gradient
by least squares.
The optimal weights of trees ?t are determined by
?t = argmin?
N
?
i
L(yi, ft?1(xi) + ?T (xi, ?))
Each node in the trees represents a split on a fea-
ture. The tuneable parameters in such a machine-
learnt model include the number of leaf nodes in
each tree, the relative contribution of score from
each tree called the shrinkage, and total number of
shallow decision trees.
The relative importance of a feature S i, in such
forests of decision trees, is aggregated over all the
1134
m shallow decision trees (Breiman et al, 1984) as
follows:
S 2i =
1
M
M
?
m=1
L?1
?
n=1
wl ? wr
wl + wr
(ylyr)2I(vt = i) (3)
where vt is the feature on which a split occurs, yl
and yr are the mean regression responses from the
right, and left sub-tree, and wl and wr are the corre-
sponding weights to the means, as measured by the
number of training examples traversing the left and
right sub-trees.
5 REQ Learner Evaluation
We collected 6,000 queries labeled as either Recur-
rent or Non-recurrent by professional human edi-
tors. The 6,000 queries were sampled from Implicit
Timestamp queries according to frequency distribu-
tion to be representative. We split the queries into
5,000 for training and 1,000 for test. For each query,
we calculated features? values as described in Sec-
tion 3.
The Naive Bayes method used single Gaussian
function for each independent feature. Mean and
variance were calculated from the training data.
As for LIBSVM, we used C-SVC, linear function
as kernel and 1.0 of shrinkage.
The parameters used in the regression model were
20 of trees, 20 of nodes and 0.8 of learning rate
(shrinkage).
The test results are shown in Fig. 3, recall-
precision curve. We set a series of threshold to the
probability of c = +1 calculated by Eq. 1 so that
we can get the point values of recall and precision in
Fig. 3. For example, if we set a threshold of 0.6, a
query with a probability larger than 0.6 is classified
as REQ. Otherwise, it is non-REQ. The precision
is a measure of correctly classified REQ queries di-
vided by all classified REQ queries. The recall is a
measure of correctly classified REQ queries divided
by all REQ queries in test data.
In addition to the three plots, we also show the
results using only one feature, ExplicitQueryRatio,
for comparison with the classification method used
by (Zhang et al, 2009).All the three models us-
ing all features performed better than the existing
method using ExplicitQueryRatio. The highest im-
provement was achieved by GBDT regression tree
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
ExplicitQueryRatio
GBDTree
Naive Bayes
SVM
Figure 3: Comparison of precision and recall rate be-
tween our method and the existing method.
model. The results of Naive Bayes were lower than
SVM and GBDTree. This model is weaker because
it treats features independently. Typically SVMs and
GBDT gives comparable results on a large class of
problems. Since for this task we use features from
different sources, the feature values are designed to
have larger dynamic range, which is better handled
by GBDT.
The features? importance ranked by Equation 3
is shown in Table 2. We list the top 10 features.
The No.1 important feature is ExplicitQueryRatio.
The second and seventh features are from search ses-
sion analysis by counting users who changed queries
from Implicit Timestamp to Explicit Timestamp.
This is a strong source of features. The time se-
ries analysis feature is ranked No.3. Calculation of
this feature needs two years query log to be much
more effective, but we didn?t get so large data for
many queries. One of the features from recurrent
event seed list is ranked No.4. This is also an impor-
tant feature source. The ChiSquareYearDist feature
is ranked 5th, that proves the recurrent event query
frequency has a statistical distribution pattern over
years. TitleYearTop30 and TitleYearTop10 that are
derived from scraping results are ranked the 9th and
10th important.
Fig. 4 shows the distribution of feature values for
1135
Feature Rank Score
ExplicitQueryRatio 1 100
NormalizedUserSwitch 2 71.7
AutoCorrelation 3 54.0
AveNumberTokenSeeds 4 48.7
ChiSquareYearDist 5 36.3
YearUrlFPCTR 6 19.1
UserSwitch 7 11.7
QueryDailyFreq 8 10.7
TitleYearTop30 9 10.6
TitleYearTop10 10 5.8
Table 2: Top 10 most important features: rank and im-
portance score (100 is maximum)
1
2
3
4
5
6
7
8
9
10
Figure 4: Feature value distribution of all data
(blue=REQ, red=non-REQ)
each sample of the 6,000 data, where each point rep-
resents a query and each line represents a feature?s
value for all queries. One point is a query. The fea-
tures are ordered according to feature importance of
Table 2. The ?blue? points indicate REQ queries and
the ?red? points, non-REQ queries. Some features
are continuous like the 1st and 2nd. Some feature
values are discrete like the last two indicating Ti-
tleYearTop30 and TitleYearTop10. There are ?red?
samples in the 4th feature but overlapped with and
covered by ?blue? samples visually.
In the Table 3, we show F-Measure values as we
gradually added features from the feature, Explicit-
QueryRatio, according to feature importance in Ta-
ble 2. We listed the F-Measure values under three
threshold, 0.6, 0.7 and 0.8. Higher threshold will in-
crease classifier precision rate but reduce recall rate.
F-Measure is a metric combining precision rate and
recall rate. It is clearly observed that the classifier
performance is improved as more features are used.
Threshold
Feature 0.6 0.7 0.8
ExplicitQueryRatio 0.833 0.833 0.752
+NormalizedUserSwitch 0.840 0.837 0.791
+AutoCorrelation 0.850 0.839 0.823
+AveNumberTokenSeeds 0.857 0.854 0.834
+ChiSquareYearDist 0.857 0.864 0.839
+YearUrlFPCTR 0.869 0.867 0.837
+UserSwitch 0.862 0.862 0.846
+QueryDailyFreq 0.860 0.852 0.847
+TitleYearTop30 0.854 0.853 0.843
+TitleYearTop10 0.858 0.861 0.852
+All 0.876 0.867 0.862
Table 3: F-Measures as varying thresholds by adding top
features.
Query Probability
ncaa men?s basketball tournament 0.999
bmw 328i sedan reviews 0.999
new apple iphone release 0.932
sigir 0.920
new york weather in april 0.717
academy awards reviews 0.404
google ipo 0.120
adidas jp 0.082
Table 4: Probabilities of example queries by GBDT tree
classifier
Some query examples, and their scores from our
model are listed in Table 4. The last two exam-
ples, google ipo and adidas jp, have very low values,
and are not REQs. The first four queries are typical
REQs. They have higher values of features Explicit-
QueryRatio,Normalized UserSwitch and YearUrlF-
PCTR. Although both new apple iphone release re-
views and academy awards reviews are about re-
views, academy awards reviews has lower value
of NormalizedUserSwitch and ChiSquareYearDist
could be the reason for a lower score.
6 Web Search Ranking
In this section, we use the approach proposed
by (Zhang et al, 2009) to test the REQ classifier
for Web search ranking. In their approach, search
ranking is altered by boosting pages with most re-
cent year if the query is a REQ. The year indicator
1136
DCG@5 DCG@1
bucket #(query) Organic Our?s % over Organic Organic Ours % over Organic
[0.0,0.1] 59 6.87 6.96 1.48(-2.3) 4.08 4.19 2.69(-1.07)
[0.1,0.2] 76 5.86 6.01 2.52(0.98) 2.88 2.91 1.14(1.69)
[0.2,0.3] 85 6.33 6.41 1.24(2.12) 3.7 3.7 0.0(0.8)
[0.3,0.4] 75 5.18 5.24 1.18(-0.7) 2.92 2.95 1.14(1.37)
[0.4,0.5] 78 4.96 4.82 -2.84(-1.35) 2.5 2.42 -3.06(0)
[0.5,0.6] 84 5.4 5.37 -0.45(-0.3) 2.82 2.85 1.05(-1.5)
[0.6,0.7] 78 4.78) 5.19) 8.42(3.64) 2.56 2.83 10.75(4.1)
[0.7,0.8] 80 4.45 4.60 3.41(3.19) 2.21 2.26 1.98(2.8)
[0.8,0.9] 78 4.81 4.96 3.15(4.79) 2.32 2.33 0.55(0.65)
[0.9,1.0] 107 5.08 5.50 8.41*(4.41) 2.64 3.09 16.78*(1.36)
[0.0,1.0] 800 5.33 5.47 2.74*(2.17) 2.83 2.93 3.6*(1.26)
Table 5: REQ learner improves search engine organic results. The numbers in the brackets are by Zhang?s methods.
Direct comparison with Zhang?s method is valid only in the last line, using all queries. A sign ??? indicates statistical
significance (p-value<0.05)
can be detected either from title or URL of the re-
sult. For clarity, we re-write their ranking function
as below,
F(q, d) = R(q, d) + [e(do, dn) + k]e??(q)
where the ranking function, F(q, d), consists of
two parts: the base function R(q, d) plus boosting.
If the query q is not a REQ, boosting is set to zero.
Otherwise, boosting is decided by e(do, dn), k, ? and
?(q). e(do, dn) is the difference of base ranking score
between the oldest page and the newest page. If the
newest page has a lower ranking score than the old-
est page, then the difference is added to the newest
page to promote the ranking of the newest page.
?(q) is the confidence score of a REQ query. It is
the value of Eq. 1. ? and k are two empirical param-
eters. (Zhang et al, 2009)?s work has experimented
the effects of using different value of ? and k (? = 0
equals to no discounts for ranking adjustment). We
used ? = 0.4 and k = 0.3 which were the best con-
figuration in (Zhang et al, 2009).
For evaluating our methods, we randomly ex-
tracted 800 queries from the Implicit Timestamp
queries. We scraped a commercial search engine us-
ing the 800 queries. We extracted the top five search
results for each query under three configures: or-
ganic search engine results, (Zhang et al, 2009)?s
method and ours using REQ classifier. We asked
human editors to judge all the scraped (query, url)
pairs. Editors assign five grades according to rel-
evance between query and articles: Perfect, Excel-
lent, Good, Fair, and Bad. For example, a ?Perfect?
grade means the content of the url match exactly the
query intent.
We use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2002) at rank k as
our primary evaluation metrics to measure retrieval
performance. DCG is defined as,
DCG@k =
k
?
i=1
2r(i) ? 1
log2(1 + i)
where r(i) ? {0 . . . 4} is the relevance grade of the ith
ranked document.
The Web search ranking results are shown in Ta-
ble 5. We used GBDT tree learning methods be-
cause it achieved the best results. We divided 800
test queries into 10 buckets according to the classi-
fier probability. The bucket, [0.0,0.1], contains the
query with a classifier probability greater than 0 but
less than 0.1. Our results are compared with organic
search results, but we also show the improvements
over search organic by (Zhang et al, 2009) in the
brackets. Because Zhang?s approach output differ-
ent classifier values from Ours for the same query,
buckets of the same range in the Table contain dif-
ferent queries. Hence, it is inappropriate to compare
1137
Zhang?s with Ours for the same buckets except the
last row where we used all the queries.
Our classifier?s overall performance is much bet-
ter than the organic search results. We achieved
2.74% DCG@5 gain and 3.6% DCG@1 gain over
organic search for all queries. The gains are higher
than (Zhang et al, 2009)?s results with regards to
improvement over organic results. By direct com-
parison, Ours was 2.7% better than Zhangs signif-
icantly in terms of DCG@1 by Wilcoxon signifi-
cant test. DCG@5 is 1.1% better, but not signifi-
cant. The table also show that the higher buckets
with higher probability achieved higher DCG gain
than the lower buckets overall. Our approach ob-
served 16.78% DCG@1 gain for bucket [0.9,1.0].
This shows that our methods are very effective.
7 Conclusions
We found most of REQ are long tail queries that
pose a major challenge to Web search. We have
demonstrated learning REQ is important for Web
search. this type of queries can?t be solved in tra-
ditional ranking method. We found building a REQ
classifier was a good solution. Our work described
using machine learning method to build REQ clas-
sifier. Our proposed methods are novel compar-
ing with traditional query classification methods.
We identified and developed features from query
log, search session, click and time series analysis.
We applied several ML approaches including Naive
Bayes, SVM and GBDT tree to implement REQ
learner. Finally, we show through ranking experi-
ments that the methods we proposed are very effec-
tive and beneficial for search engine ranking.
Acknowledgements
We express our thanks to who have assisted us
to complete this work, especially, to Fumiaki Ya-
maoka, Toru Shimizu, Yoshinori Kobayashi, Mit-
suharu Makita, Garrett Kaminaga, Zhuoran Chen.
References
R. Baeza-Yates, F. Saint-Jean, and C. Castillo. 2002.
Web dynamics, age and page qualit. String Process-
ing and Information Retrieval, pages 453?461.
Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, David
Grossman, David D. Lewis, Abdur Chowdhury, and
Aleksandr Kolcz. 2005. Automatic web query classi-
fication using labeled and unlabeled training data. In
SIGIR ?05, pages 581?582.
K. Berberich, M. Vazirgiannis, and G. Weikum. 2005.
Time-aware authority rankings. Internet Math,
2(3):301?332.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. 1984.
Classification and Regression Trees. Wadsworth and
Brooks, Monterey, CA.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Proceedings of
International Conference on World Wide Web.
Andrei Z. Broder, Marcus Fontoura, Evgeniy
Gabrilovich, Amruta Joshi, Vanja Josifovski, and
Tong Zhang. Robust classification of rare queries
using web knowledge. In SIGIR ?07, pages 231?238.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
J. Cho, S. Roy, and R. Adams. 2005. Page quality: In
search of an unbiased web ranking. Proc. of ACM SIG-
MOD Conference.
F. Diaz. 2009. Integration of news content into web re-
sults. Proceedings of the Second ACM International
Conference on Web Search and Data Mining (WSDM),
pages 182?191.
Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne,
Jing Bai, Ruiqiang Zhang, Karolina Buchner, Ciya
Liao, and Fernando Diaz. 2010a. Towards recency
ranking in web search. Proceedings of the Third ACM
International Conference on Web Search and Data
Mining (WSDM), pages 11?20.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010b. Time is of the essence: im-
proving recency ranking using twitter data. 19th Inter-
national World Wide Web Conference (WWW), pages
331?340.
Jonathan L. Elsas and Susan T. Dumais. 2010. Lever-
aging temporal dynamics of document content in rele-
vance ranking. In WSDM, pages 1?10.
J. H. Friedman. 2001. Greedy function approximation:
A gradient boosting machine. Annals of Statistics,
29(5):1189?1232.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: devel-
opment and comparative experiments. Inf. Process.
Manage., 36(6):779?808.
A. C. Knig, M. Gamon, and Q. Wu. 2009. Click-through
prediction for news queries. Proc. of SIGIR, pages
347?354.
1138
Ying Li, Zijian Zheng, and Honghua (Kathy) Dai.
2005. Kdd cup-2005 report: facing a great challenge.
SIGKDD Explor. Newsl., 7(2):91?99.
Xiao Li, Ye yi Wang, and Alex Acero. 2008. Learning
query intent from regularized click graphs. In In SI-
GIR 2008, pages 339?346. ACM.
Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang
Zhang. 2009. Improving search relevance for im-
plicitly temporal queries. In SIGIR ?09: Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
pages 700?701.
S. Nunes. 2007. Exploring temporal evidence in web
information retrieval. BCS IRSG Symposium: Future
Directions in Information Access.
S. Pandey, S. Roy, C. Olston, J. Cho, and S. Chakrabarti.
2005. Shuffling a stacked deck: The case for partially
randomized ranking of search engine results. VLDB.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, NY.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng
Pan, Kangheng Wu, Jie Yin, and Qiang Yang. 2005.
Q2c@ust: our winning solution to query classification
in kddcup 2005. SIGKDD Explor. Newsl., 7(2):100?
110.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,
Kangheng Wu, Jie Yin, and Qiang Yang. 2006. Query
enrichment for web-query classification. ACM Trans.
Inf. Syst., 24(3):320?352.
Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, Donald
Metzler, and Jian-yun Nie. 2009. Search result
re-ranking by feedback control adjustment for time-
sensitive query. In HLT-NAACL ?09, pages 165?168.
1139
