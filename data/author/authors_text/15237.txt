Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1612?1623, Dublin, Ireland, August 23-29 2014.
Minimally Supervised Classification to Semantic Categories using
Automatically Acquired Symmetric Patterns
Roy Schwartz
1
Roi Reichart
2
1
Institute of Computer Science, The Hebrew University
{roys02|arir}@cs.huji.ac.il
2
Technion IIT
roiri@ie.technion.ac.il
Ari Rappoport
1
Abstract
Classifying nouns into semantic categories (e.g., animals, food) is an important line of research
in both cognitive science and natural language processing. We present a minimally supervised
model for noun classification, which uses symmetric patterns (e.g., ?X and Y?) and an iterative
variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a
predefined set of symmetric patterns, but extract them automatically from plain text, in an unsu-
pervised manner. We experiment with four semantic categories and show that symmetric patterns
constitute much better classification features compared to leading word embedding methods. We
further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-
the-art label propagation alternatives for this task. In experiments, our model obtains 82%-94%
accuracy using as few as four labeled examples per category, emphasizing the effectiveness of
simple search and representation techniques for this task.
1 Introduction
The role of language is to express meaning. In the field of NLP, there has been an increasingly grow-
ing number of approaches that deal with semantics. Among these are vector space models (Turney and
Pantel, 2010; Baroni and Lenci, 2010), lexical acquisition (Hearst, 1992; Dorow et al., 2005; Davidov
and Rappoport, 2006), universal cognitive conceptual annotation (Abend and Rappoport, 2013) and au-
tomatic induction of feature representations (Collobert et al., 2011). In this paper, we utilize extremely
weak supervision to classify words into fundamental cognitive semantic categories.
There are several types of semantic categories expressed by languages, e.g., objects, actions, and
properties. We follow human development, acquiring coarse-grained categories and distinctions before
detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete ?things? (Langacker,
2008, Ch. 4), roughly corresponding to nouns ? the main participants in linguistic clauses ? that are
universally present in the semantics of virtually all languages (Dixon, 2005).
Most works on noun classification to semantic categories require large amounts of human annotation
to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or
rely on language-specific resources such as WordNet (Evans and Or?asan, 2000; Or?asan and Evans, 2007).
Such heavy supervision is labor intensive and makes these models domain and language dependent.
Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can com-
pensate for the lack of input from the senses in text corpora. Our model therefore performs semantic
category classification using only a small number of labeled seed words per category. The experiments
we conduct show that such weak supervision is sufficient to construct a high quality classifier.
A key component of our model is the application of symmetric patterns. We define patterns to be
sequences of words and wildcards (e.g., ?X is a dog?, ?both X and Y?, etc.). Accordingly, symmet-
ric patterns are patterns that contain exactly two wildcards, where both wildcards are interchangeable.
Examples of symmetric patterns include ?X and Y?, ?X as well as Y? and ?neither X nor Y?.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/.
1612
Works that apply symmetric patterns in their model generally require expert knowledge in the form of a
pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract
symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This
algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about
high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain
and language independent.
Our model addresses semantic classification in a transductive setup. It takes advantage of word sim-
ilarity scores that are computed based on symmetric pattern features, and propagates information from
concepts with known classes to the rest of the concepts. For this aim we apply an iterative variant of the
k-Nearest Neighbors algorithm (denoted with I-k-NN) to a graph in which vertices correspond to nouns
and word pairs are connected with edges based on their participation in symmetric patterns.
We experiment with a subset of 450 nouns from the CSLB dataset (Devereux et al., 2013), which were
annotated with semantic categories by thirty human subjects. From the set of semantic categories in this
dataset, we select categories that are both frequent and have a high inter-annotator agreement (Section 2).
This results in a set of four semantic categories ? animacy, edibility, is a tool and is worn.
Our experiments show that our model performs very well even when only a small number of labeled
seed words are available. For example, on the task of binary classification with respect to a single
category, when using as few as four labeled seed words, classification accuracy reaches 82%-94%.
Furthermore, our model outperforms several strong baselines for this task. First, we compare our
model against a model that uses a deep neural network word embedding baseline (Collobert et al., 2011)
instead of our symmetric pattern based features, and applies the exact same I-k-NN algorithm. In recent
years, deep networks word embeddings obtained state-of-the-art results in several NLP tasks (Collobert
and Weston, 2008; Socher et al., 2013). However, in our task, features based on simple, intuitive and
easy to compute symmetric patterns, lead to substantially better performance (average improvement of
0.15 F1 points). Second, our model outperforms two baseline models that utilize the same symmetric
pattern classification features as in our model, but replace our simple I-k-NN algorithm with two leading
label propagation alternatives (the normalized graph cut (N-Cut) algorithm (Yu and Shi, 2003) and the
Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)). The average improvement over
these two baselines is 0.21 and 0.03 F1 points .
The rest of the paper is organized as follows. Section 2 describes our semantic classification task
and, particularly, the semantic classes that we aim to learn. Section 3 presents our method for automatic
symmetric patterns acquisition. Sections 4, 5 and 6 describe our model, experimental setup and results,
respectively. Related work is finally surveyed in Section 7.
2 Task Definition
The task we tackle in this paper is the classification of nouns into semantic categories. This section
defines the categories we address and the dataset we use.
Semantic Categorization of Concrete Nouns. We focus on concrete ?things? (Langacker, 2008),
which correspond to noun categories. Nouns are interesting because they are the most basic lexical
semantic categories. Specifically, children acquire nouns before any other category (Clark, 2009). More-
over, noun categories are generally not subjective. For example, it is hard to argue that a dog is not
an animal, or that an apple is inedible, in most reasonable contexts. The context independent nature of
nouns makes them appropriate for a type level classification task, such as the one we tackle. In order to
provide a better description of the categories we aim to predict, we now turn to discuss the CSLB dataset,
with which we experiment.
Dataset. We experiment with the CSLB property norms dataset (Devereux et al., 2013). In order to
prepare this data set, thirty human subjects were presented with 638 concrete nouns and were asked to
write the categories associated with each concept. Table 1 presents the top five categories for the nouns
apple and horse.
1613
Noun Categories
Apple is a fruit, does grow on trees, is green, is red, has pips seeds
Horse is ridden, is an animal, has four legs, has legs, has hooves
Table 1: Five most frequent semantic categories for the words apple and horse in the CSLB dataset.
Category Selection. The CSLB dataset consists of a total of 2725 semantic categories. We apply
a selection mechanism that provides us with a dataset in which (1) only noun categories (things) are
included; and (2) only semantic categories that are prominent across humans are considered. For this,
we apply the following filtering stages. First, since the vast majority of annotated categories are rare (for
example, 1691 categories are assigned to a single noun only), we set a minimum threshold of 35 nouns
per category (5% of the nouns). After removing highly infrequent categories, 28 are left. We then apply
an inter-annotator agreement criterion: for each semantic category c, we compute the average number
of human annotators that associated this category with a given noun, across the nouns annotated with c.
We select the category c only if the value of this statistic is higher than 10 subjects (1/3 of the subjects),
which results in a semantic category set of size 18. Finally, we discard categories, such as color and size,
that do not correspond to things. We are left with four noun semantic categories: animacy (animals),
edibility (food items), is a tool (tools), and is worn (clothes).
Interestingly, the resulting semantic categories can also be justified from a cognitive perspective. There
is a large body of work indicating that our categories relate to brain organization principles. For example,
Just et al. (2010) showed that food products and tools arouse different brain activation patterns. More-
over, a number of works showed that both animate objects and tools are represented in specific brain re-
gions. These works used neuroimaging methods such as functional magnetic resonance imaging (fMRI)
(Naselaris et al., 2012), electroencephalography (EEG) (Chan et al., 2011) and magnetoencephalogra-
phy (MEG) (Sudre et al., 2012). See (Martin, 2007) for a detailed survey. This parallel evidence to the
prominence of our categories provides substance for intriguing future research.
3 Symmetric Patterns
Patterns. In this work, patterns are combinations of words and wildcards, which provide a structural
phrase representation. Examples of patterns include ?X and Y?, ?X such as Y?, ?X is a country?, etc.
Patterns can be used to extract various relations between words. For example, patterns such as ?X of a
Y? (?basement of a building?) can be useful for detecting the meronymy (part-of) relation (Berland and
Charniak, 1999). Symmetric patterns (e.g., ?X and Y?, ?France and Holland?), which we use in this
paper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002).
Symmetric Patterns. Symmetric patterns are patterns that contain exactly two wildcards, and where
these wildcards are interchangeable. Examples of symmetric patterns include ?X and Y?, ?X or Y? and
?X as well as Y?. Previous works have shown that word pairs that participate in symmetric patterns bare
strong semantic resemblance, and consequently, that these patterns can be used to cluster words into
semantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorow
et al., 2005; Davidov and Rappoport, 2006). A key observation of this paper is that symmetric patterns
can be also used for semantic classification, where recall is as important as precision.
Flexible Patterns. It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008;
Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsu-
pervised manner. The key idea that makes this procedure possible is the concept of ?flexible patterns?,
which are composed of high frequency words (HFW) and content words (CW). Every word in the lan-
guage is defined as either HFW or CW, based on the number of times this word appears in a large corpus.
This clustering procedure is applied by traversing a large corpus, and marking words that appear with
corpus frequency higher than a predefined threshold t
1
as HFWs, and words with corpus frequency lower
than t
2
as CWs.
1
1
We follow (Davidov and Rappoport, 2006) and set t
1
= 10
?5
, t
2
= 10
?3
. Note that some words are marked both as HFW
and as CW. See (Davidov and Rappoport, 2008) for discussion.
1614
The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepo-
sitions, determiners, etc.) while CWs are comprised mostly of content words (nouns, verbs, adjectives
and adverbs). This coarse grained clustering is useful for pattern extraction from plain text, since lan-
guage patterns tend to use fixed function words, while content words change from one instance of the
pattern to another (Davidov and Rappoport, 2006).
Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words to
CWs and HFWs, extracting all pattern instances. An extracted pattern instance consists of CW wildcards
and the actual words replacing the HFWs in the pattern type. Consider the sentence ?The boy is happy
and joyful?. Replacing the content words with the CW wildcard results in ?The CW is CW and CW?.
From this intermediate representation, we extract word sequences of a given length constraint and denote
them as flexible patterns.
2
The flexible patterns of length 5 extracted from this sentence are ?The CW is
CW and? and ?CW is CW and CW?. The reader is referred to (Davidov and Rappoport, 2006) for more
details.
Automatically Extracted Symmetric Patterns. Most models that incorporate symmetric patterns use
a predefined set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we apply
an automatic, completely unsupervised procedure for symmetric pattern extraction. This procedure,
described in Algorithm 1, is adopted from (Davidov and Rappoport, 2006).
The procedure first extracts flexible patterns that contain exactly two CW wildcards. It then selects
those flexible patterns in which both CWs are interchangeable. That is, it selects a pattern p if every
word pair CW
1
, CW
2
that participates in p indicates with high probability that the word pair C
2
, C
1
also participates in p. For example, for the symmetric pattern ?CW and CW?, both ?cats and dogs?
and ?dogs and cats? are semantically plausible expressions, and are therefore likely to appear in a large
corpus. On the other hand, the flexible pattern ?CW such as CW? is asymmetric, as exemplified in
expressions like ?countries such as France?, where replacing the CWs does not result in a semantically
plausible expression (# ?France such as countries?). The selection process is done by computing the
proportion of CW
1
, CW
2
pairs that participate in p for which CW
2
, CW
1
also participates in p. Patterns
for which this proportion exceed a certain threshold are selected.
We apply Algorithm 1 on the google books 5-gram corpus (Michel et al., 2011)
3
and extract 20 sym-
metric patterns. Some of the more interesting symmetric patterns extracted using this algorithm include
?CW and the CW?, ?from CW to CW?, ?CW rather than CW? and ?CW versus CW?. In the next section
we present our approach to semantic classification, which makes use of automatically acquired symmet-
ric patterns for word similarity computations.
4 Model
In this section we present our model for binary word classification according to a single semantic category
in a minimally-supervised, transductive setup. Given a set of words, we label a small number of words
with their correct label according to the category at hand (+1 for words that belong to the category, -1
for words that do not belong to it). Our model is based on an undirected weighted graph, in which
vertices correspond to words, and edges correspond to relations between words. Our goal is to classify
the unlabeled words (vertices) in the graph through a label propagation process. We now turn to describe
our model in detail.
Graph Construction. We construct our graph such that an edge is added between two words (vertices)
if both words participate in a symmetric pattern. The edge generation process is performed as follows.
We first apply our symmetric pattern extraction procedure (Algorithm 1), and denote the set of selected
symmetric patterns with P . We then traverse a large corpus
4
and extract all word pairs that participate
in any pattern p ? P . We denote the number of occurrences of a word pair (w
1
, w
2
) in such patterns
with f
w
1
,w
2
. Finally, we select all word pairs (w
1
, w
2
) for which min(f
w
1
,w
2
, f
w
2
,w
1
) > ?. Each such
2
We set the maximal flexible pattern length to be 5.
3
https://books.google.com/ngrams
4
We use google books 5-grams (Michel et al., 2011).
1615
Algorithm 1 Symmetric pattern extraction
1: procedure EXTRACT SYMMETRIC PATTERNS(C,W )
2: . C is a large corpus, W is a lexicon
3: . Traverse C and extract all flexible patterns of length 3-5 that appear in C and contain exactly two content words
4: P ? extract flexible patterns(C,W )
5: for p ? P do
6: if p appears in <10
?6
of the sentences in C then
7: Discard p and continue
8: end if
9: G
p
? a directed graph s.t. V (G
p
)?W ,E(G
p
)?{(w
1
, w
2
)?W
2
:w
1
,w
2
participate in at least one instance of p}
10: . An undirected graph based on the bidirectional edges of the G
p
11: symG
p
? an undirected graph: {(w
1
), (w
1
,w
2
) : (w
1
,w
2
) ? E(G
p
) ? (w
2
, w
1
) ? E(G
p
)}
12: . Two measures of symmetry
13: M
1
?
|
V (symG
p
)
|
|
V (G
p
)
|
,M
2
?
|
E(symG
p
)
|
|
E(G
p
)
|
14: . Symmetric pattern candidates are those with high M
1
and M
2
values
15: if min (M
1
,M
2
) < 0.05 then
16: Discard p
17: end if
18: end for
19: for p ? P do
20: . E.g., ?CW and CW? is contained in ?both CW and CW?
21: if ?p
?
? P s.t. p
?
is contained in p then
22: Discard p
23: end if
24: end for
25: return The top 20 members of P with the highest M
1
value
26: end procedure
pair is connected with an edge e
w
1
,w
2
in the graph, where the edge weight (denoted with w
w
1
,w
2
) is the
geometric mean between f
w
1
,w
2
and f
w
2
,w
1
.
Label Propagation. Given a small number of annotated words (vertices), our goal is to propagate the
information these words convey to other words in the graph. To do so, we apply an iterative variant of the
k-Nearest Neighbors algorithm (I-k-NN). This iterative variant is required due to graph sparsity; when
starting with a small set of labeled vertices, most unlabeled vertices do not have any labeled neighbor, and
thus running the standard k-NN algorithm would result in classifying a very small number of vertices.
Our approach is to run iterations of the k-NN algorithm, and thus propagate information to additional
vertices at each iteration. At each k-NN step, the algorithm selects words that have at least one labeled
neighbor. From this set, only the words that have the highest ratio of neighbors with the same label are
selected, and are assigned with this label.
Consider a simple example. Say we have three candidate vertices a, b and c, where a has one neighbor
with label +1 (ratio(a) = 1/1 = 1.0), b has two neighbors with label -1 (ratio(b) = 2/2 = 1.0) and
c has three neighbors with label +1 and one neighbor with label -1 (ratio(c) = max(3, 1)/4 = 3/4).
Then, a and b are selected and are assigned with +1 and ?1, respectively.
Seed Expansion. In minimally supervised setups like ours, the model is initialized with a small set of
labeled seed examples. A natural approach in such settings is to apply a seed expansion step, in order to
obtain a larger set of labeled seeds. Our method uses the same graph construction procedure described
above, but uses a larger edge generation threshold ? >> ?.
5
We then apply an iterative procedure that
labels a vertex v with a label l if either (a) v is directly connected to ? of the vertices labeled with l or (b)
v is connected to ?
l
of the neighbors of vertices labeled with l.
6
This procedure is run iteratively until no
more vertices meet any of the criteria (a) or (b).
5
Using a larger threshold results in a sparser graph. Nevertheless, each edge in this graph is more likely to represent a real
semantic relation.
6
? and ?
l
are hyperparameters tuned on our development set (see Section 5.2).
1616
5 Experimental Setup
5.1 Baselines
We compare our model to two types of baselines. The first (Classification Features Baselines) utilizes
the I-k-NN algorithm, along with a different set of classification features. The second (Label Propa-
gation Baselines) utilizes the same classification features as we do, but replaces I-k-NN with a more
sophisticated label propagation algorithm.
5.1.1 Classification Features Baselines
In this set of baselines, we use different methods for building our graph. Concretely, instead of adding
edges for pairs of words that appear in the same symmetric pattern, we use word similarity measures
based on different feature sets as described below. The process of building the graph using the baseline
word similarity measures is described in Section 5.2.
SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word
representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,
7
a deep network based
word embedding method, which has been used to produce state-of-the-art results in several NLP tasks,
including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine
similarity between two word embeddings as a word similarity measure.
Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al.,
1992).
8
This clustering, in which words share a cluster if they tend to appear in the same lexical con-
text, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al.,
2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a
simple contextual preference similarity correlates with similarity in semantic categorization better than
symmetric pattern features.
The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph
distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word simi-
larity measure for building our graph.
5.1.2 Label Propagation Baselines
In this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still using
the symmetric pattern features for word similarity computations.
N-Cut. This baseline applies the normalized graph cut algorithm (Yu and Shi, 2003)
9
for label propa-
gation. Given a graphG = (V,E) and two sets of verticesA,B ? V , this algorithm defines links(A,B)
to be the sum of edge weights between A and B. The objective of the algorithm is to find the clusters
A, V \ A that minimize
links(A,V \A)
links(A,V )
. The algorithm of (Yu and Shi, 2003) is particularly efficient for
this problem as it avoids eigenvector computations which may become computationally prohibitive for
large graphs (for more details, see their paper). In order to encode information about our labeled seed
words, we hard-code a large negative value (-100000) to the weights of edges between seed words with
different labels (positive and negative).
MAD. The Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)
10
is an extension
of the Adsorption algorithm (Baluja et al., 2008). MAD is a stochastic graph-based label propagation
algorithm which has shown to have a number of attractive theoretical properties and demonstrated good
experimental results.
7
The word embeddings were downloaded from http://ml.nec-labs.com/senna/
8
We use the clusters induced by (Koo et al., 2008), who applied the Brown algorithm implementation of (Liang,
2005) to the BLLIP corpus (Charniak et al., 2000). http://www.people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
9
http://www.cis.upenn.edu/
?
jshi/software/Ncut_9.zip
10
http://github.com/parthatalukdar/junto
1617
5.2 Experiments
Graph Construction. We experiment with the CSLB dataset (Devereux et al., 2013), consisting of 638
nouns, annotated with their semantic categories by thirty human subjects. We first omit all nouns that
are annotated as having more than one sense, and use the remaining 603 nouns to build our graph. From
these nouns, 146 nouns are annotated as animate, 115 as edible, 50 as wearable and 35 as tools.
11
We
then discard nouns that have less than two neighbors, which results in a final set of 450 nouns (vertices).
The graphs used in the classification features baselines are different than those used by the models that
use our symmetric pattern classification features, since the features define the graph structure (Section 4).
In order to provide a meaningful comparison, we build graphs with the same number of vertices for each
of these baselines. We do so by selecting the n edges with the highest weight, together with the set of
vertices connected by these edges, such that the resulting graph has 450 vertices. Working with these
sets of vertices is the optimal setting for these baselines, as the resulting graphs are the ones with the
highest possible edge weights for graphs with 450 vertices.
12
Parameter Tuning. In order to avoid adding additional labeled examples for the sake of parameter
tuning, we set the hyperparameter values to the ones for which each model performs best on an auxiliary
semantic classification task. Concretely, we experiment with a fifth semantic category (audibility),
13
which is not part of our evaluation setting, for parameter tuning. Note that this results in our model
having the same hyperparamter values for all four classification tasks.
In order to ensure that the models assign all participating words with labels, we set ?=3, where ? is
the minimal number of times a word pair should appear in the same symmetric pattern in order to have
an edge in our graph (See Section 4). In our seed expansion procedure, where we search for seeds whose
label is predicted with high confidence, only word pairs that appear at least ?=50 times in the same
symmetric pattern are assigned an edge in the graph. We set the seed expansion procedure parameters to
be ? = 0.6, ?
+1
= 0.5, ?
?1
= 0.2.
Evaluation. For each classification task, we run experiments with 4, 10, 20 and 40 labeled seed words.
In each setting, half of the labeled seed words are assigned a positive label and the other half are assigned
a negative label. For each semantic category and labeled seed set size, we repeat our experiment 1000
times, each of which with a different set of randomly selected labeled seed examples, and report the
average results. We report both accuracy (number of correct labels divided by number of vertices in
the graph) and F1 score, which is the harmonic mean of p (the average precision across labels) and r
(average recall across labels).
These two measures represent complementary aspects of our results. On the one hand, accuracy is
the most natural classification performance measure. On the other hand, the number of positive labels is
substantially smaller than the number of negative labels,
14
and thus this measure can be manipulated: a
dummy model that always assigns the negative label gets a high accuracy. The F1 score controls against
such models by assigning them low scores.
6 Results
Our experiments are designed to explore two main questions: (a) the value of symmetric patterns as
semantic classification features, compared to state-of-the-art word clustering and embedding methods;
and (b) the required complexity of an algorithm that can propagate information about semantic simi-
larity. Particularly, we test the value of our simple I-k-NN algorithm compared to more sophisticated
alternatives.
A Minimally Supervised Setting. Our first set of experiments is in a minimally supervised setting
where only two positive and two negative examples are available for each binary classification task. This
11
Some words are classified as belonging to more than one category (e.g., ?chicken? is both animate and edible).
12
The resulting graphs are actually denser than the symmetric patterns-based graph: 14K and 9K edges for the Brown and
SENNA graphs, respectively, compared to < 5K edges in the symmetric patterns graph.
13
We used four labeled seed words in these experiments.
14
Only 6-25% of the nouns have a positive label.
1618
Animacy Edibility is worn is a tool
SP SENNA Brown SP SENNA Brown SP SENNA Brown SP SENNA Brown
Acc.
MAD 80.4% 77.7% 12.0% 75.0% 56.5% 14.8% 82.7% 66.8% 14.7% 73.3% 67.7% 12.2%
N-Cut 71.4% 60.4% 51.2% 75.5% 59.4% 50.9% 83.3% 71.5% 51.4% 82.7% 77.1% 52.0%
I-k-NN 85.1% 76.0% 55.5% 82.2% 56.8% 68.0% 94.1% 70.9% 66.7% 82.0% 75.7% 65.0%
F1
MAD 0.77 0.76 0.18 0.69 0.55 0.24 0.71 0.56 0.22 0.58 0.47 0.17
N-Cut 0.49 0.45 0.46 0.51 0.44 0.45 0.61 0.56 0.41 0.56 0.50 0.38
I-k-NN 0.78 0.70 0.48 0.71 0.53 0.62 0.86 0.59 0.55 0.64 0.52 0.51
Table 2: Accuracy and F1 score comparison between our model and the baselines. The columns cor-
respond to the type of classification features used by the model: SP ? symmetric patterns, SENNA ?
word embeddings extracted using deep networks (Collobert et al., 2011), Brown ? Brown word clus-
tering (Brown et al., 1992). The rows correspond to the algorithms applied by the model: N-Cut ? the
normalized graph cut algorithm (Yu and Shi, 2003), MAD ? the modified adsorption algorithm (Talukdar
and Crammer, 2009), I-k-NN ? our iterative k-NN algorithm. Our model (I-k-NN + SP) is superior in all
cases, except for the accuracy of the ?is a tool? semantic category, where it is second only to N-Cut+SP.
5 10 15 200.55
0.6
0.65
0.7
0.75
0.8
0.85
#training_samples
F1 Sco
re
 
 
SENNA (MAD)Brown (I?k?NN)Symmetric Patterns (I?k?NN)
(a) Classification Features Comparison
5 10 15 200.45
0.50.55
0.60.65
0.70.75
0.80.85
#training_samples
F1 Sco
re
 
 
N?Cut (Symmetric Patterns)MAD (Symmetric Patterns)I?k?NN (Symmetric Patterns)
(b) Algorithm Comparison
5 10 15 200.5
0.550.6
0.650.7
0.750.8
0.85
#training_samples
F1 Sco
re
 
 
I?k?NN, SENNAMAD, SENNAMAD, Symmetric PatternsI?k?NN, Symmetric Patterns
(c) Top four Best Models
Figure 1: (a) Comparison of the different classification features. The figure shows the F1 scores of the
best model that uses each of the feature sets (the label propagation algorithm used in each model appears
in parentheses). (b) Comparison of the different label propagation algorithms. The figure shows the F1
scores of the best model that uses each of the algorithms (the classification feature sets used in each model
appears in parentheses. It is always symmetric patterns). (c) The four best overall models (algorithm +
classification feature set). The figures show that the symmetric pattern feature set is superior to the other
feature sets, and that I-k-NN is superior or comparable to the other label propagation algorithms.
setup enables us to explore the performance of our model when the amounts of labeled training data is
taken to the possible minimum.
Table 2 presents our results. With respect to objective (a), the table clearly demonstrates that symmetric
patterns lead to much better results compared to the alternatives. Particularly, for all four semantic
categories, and across both evaluation measures, it is a model that utilizes symmetric pattern classification
features that achieves the best results. The average difference between the best model that uses symmetric
patterns and the best model that does not is 12.5% accuracy and 0.13 F1 points. The dominance of
symmetric pattern classification features is further demonstrated by the fact that a model that uses these
features always performs better than a model that uses the same algorithm but different features.
With respect to objective (b) the table shows that I-k-NN provides a large improvement in seven out
of eight (category ? evaluation measure) settings. The average difference between the best model that
utilizes I-k-NN and the best model that applies a different algorithm is 5.4% accuracy and 0.06 F1 points.
Analysis of Labeled Seed Set Size. In order to get a wider perspective on our model, we repeated our
experiments with various sizes of the labeled seed set: 5,10 and 20 positive and negative labeled examples
per semantic category. For brevity, only the F1 score results of the edibility category are presented. The
trends observed on the other semantic categories (as well as when using the accuracy measure) are very
similar.
Figure 1a compares the different classification features. For each feature f , results of the best per-
forming model that uses f are shown. The results reveal that symmetric patterns clearly outperform the
other features. The average differences between the best symmetric patterns-based model and the best
1619
models that use the other features are 0.15 (SENNA) and 0.16 (Brown) F1 points.
Figure 1b compares the different label propagation algorithms. For each algorithm a, results for the
best performing model that uses a are presented. The results reveal that the I-k-NN algorithm outper-
forms both algorithms by 0.03 (MAD) and 0.21 (N-Cut) F1 points. The results also show that for all
algorithms, the best performing model uses symmetric patterns classification features, which further
demonstrates the dominance of these features.
Finally, Figure 1c presents the four top performing models (algorithm + classification feature). In
accordance with the other findings presented in this section, the top two models, which outperform the
other models by a large margin, apply symmetric pattern classification features.
Seed Expansion Effect. Our model uses a seed expansion procedure in order to expand a small set of
labeled seed words to a larger set (see Section 4). In order to assess the quality of this procedure we
compute, for each semantic category, the average size of the expanded set and the accuracy of the new
seeds (i.e., the proportion of new seeds that are labeled correctly). Results show that the initial set is
increased from four seeds (two positive + two negative) to 48-52, and that the accuracy of the new seeds
is as high as 88-99%. Our experiments also show that this procedure provides a substantial performance
boost to our I-k-NN algorithm, which obtains a 7.2% accuracy and 0.05 F1 points improvement (averaged
over the four semantic categories) when applied with the expanded set of labeled seed words compared
to the original set of size four.
7 Related Work
Classification into Semantic Categories. Several works tackled the task of semantic classification,
mostly focusing on animacy, concreteness and countability. The vast majority of these works are either
supervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005;
?vrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) or
make use of external, language-specific resources such as WordNet (Or?asan and Evans, 2001; Or?asan
and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a
small set of labeled seed words.
Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences
in instances of hand-crafted patterns such as ?X who Y? and ?X and his Y?. While their model uses
patterns that are tailored to the animacy and gender categories, our model uses automatically induced
patterns and is thus applicable to a range of semantic categories.
Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais,
1997) based classification features. They used their model to classify nouns into the concrete/abstract
category using 40 labeled seed words . Unlike our model, which requires only a small set of labeled seeds,
their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting the
seed set of labeled words that are used for propagation. Our model, on the other hand, does not require
any seed selection procedure, and utilizes a randomly selected set of labeled seed words.
Lexical Acquisition. Another line of work focused on the acquisition of semantic categories. In this
setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for
precision. Our model tackles a different task, namely the classification of words according to a given
category where both recall and precision are to be optimized.
Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of
symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language
specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et
al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised
parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model
automatically induces symmetric patterns, obtaining high quality results without relying on any type of
language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and
Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label
1620
seeds to achieve good performance; in contrast, our work performs very well with a randomly selected
set of labeled seed words.
8 Conclusion
We presented a minimally supervised model for noun classification into coarse grained semantic cate-
gories. Our model obtains 82%-94% accuracy on four semantic categories even when using only four
labeled seed words per category. We showed that our modeling decisions ? using symmetric patterns as
classification features and a simple iterative k-NN algorithm for label propagation ? lead to a substantial
performance gain compared to state-of-the-art, more sophisticated, alternatives. Our results demonstrate
the applicability of minimally supervised methods for semantic classification tasks. Future work will
include modifying our model to support other, more fine-grained types of semantic categories, includ-
ing adjectival categories (properties). We also plan to work on token-level word classification, and thus
support multi-sense words, as well as demonstrate the power of unsupervised patterns acquisition for
multilingual setups.
Acknowledgments
This research was funded (in part) by the Harry and Sylvia Hoffman leadership and responsibility pro-
gram (for the first author), the Google Faculty research award (for the second author), the Intel Collab-
orative Research Institute for Computational Intelligence (ICRI-CI) and the Israel Ministry of Science
and Technology Center of Knowledge in Machine Learning and Artificial Intelligence (Grant number
3-9243).
References
O. Abend and A. Rappoport. 2013. Universal conceptual cognitive annotation (UCCA). In Proc. of ACL.
T. Baldwin and F. Bond. 2003. A plethora of methods for learning English countability. In Proc. of EMNLP.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random walks through the view graph. In Proc. of WWW, pages
895?904. ACM.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Proc. of ACL.
S. R. Bowman and H. Chopra. 2012. Automatic Animacy Classification. In Proc. of NAACL-HLT Student
Research Workshop.
P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4):467?479.
A. M. Chan, J. M. Baker, E. Eskandar, D. Schomer, I. Ulbert, K. Marinkovic, S. S. Cash, and E. Halgren. 2011.
First-pass selectivity for semantic categories in human anteroventral temporal lobe. The Journal of Neuro-
science, 31(49):18119?18129.
E. Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and M. Johnson. 2000. BLLIP 198789 WSJ Corpus Release 1,
LDC No. LDC2000T43. Linguistic Data Consortium.
A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proc. of CoNLL.
E. V. Clark. 2009. First language acquisition. Cambridge University Press.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks
with multitask learning. In Proc. of ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing
(almost) from scratch. JMLR, 12:2493?2537.
1621
D. Davidov and A. Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric pat-
terns and high frequency words. In Proc. of ACL-Coling.
D. Davidov and A. Rappoport. 2008. Unsupervised discovery of generic relationships using pattern clusters and
its evaluation by automatically generated SAT analogy questions. In Proc. of ACL-HLT.
B. J. Devereux, L. K. Tyler, J. Geertzen, and B. Randall. 2013. The centre for speech, language and the brain
(CSLB) concept property norms. Behavior research methods, pages 1?9.
R. M. Dixon. 2005. A semantic approach to English grammar. Oxford University Press.
B. Dorow, D. Widdows, K. Ling, J. P. Eckmann, D. Sergi, and E. Moses. 2005. Using Curvature and Markov
Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination.
R. Evans and C. Or?asan. 2000. Improving anaphora resolution by identifying animate entities in texts. In Proc. of
DAARC.
V. Hatzivassiloglou and K. R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of ACL.
M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of Coling ? Volume 2.
H. Ji and D. Lin. 2009. Gender and Animacy Knowledge Discovery from Web-Scale N-Grams for Unsupervised
Person Mention Detection. In Proc. of PACLIC.
M. A. Just, V. L. Cherkassky, S. Aryal, and T. M. Mitchell. 2010. A neurosemantic theory of concrete noun
representation based on the underlying brain codes. PloS one, 5(1):e8622.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL-HLT.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage
graphs. In Proc. of ACL-HLT.
O. Y. Kwong. 2011. Measuring concept concreteness from the lexicographic perspective. In Proc. of PACLIC.
T. K. Landauer and S. T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.
R. W. Langacker. 2008. Cognitive grammar: A basic introduction. Oxford University Press.
P. Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, Massachusetts Institute of Tech-
nology.
J. M. Mandler. 2004. The foundations of mind: Origins of conceptual thought. Oxford University Press New
York.
A. Martin. 2007. The representation of object concepts in the brain. Annual Review of Psychology, 58:25?45.
J. B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Or-
want, et al. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176?
182.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In
Proc. of NAACL.
J. L. Moore, C. J. Burges, E. Renshaw, and W.-t. Yih. 2013. Animacy Detection with Voting Models. In Proc. of
EMNLP.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. Reinforcing English countability prediction with one
countability per discourse property. Proc. of ACL-Coling.
T. Naselaris, D. E. Stansbury, and J. L. Gallant. 2012. Cortical representation of animate and inanimate objects in
complex natural scenes. Journal of Physiology-Paris, 106(5):239?249.
C. Or?asan and R. Evans. 2001. Learning to identify animate references. In Proc. of the Workshop on Computa-
tional Natural Language.
C. Or?asan and R. Evans. 2007. NP Animacy Identification for Anaphora Resolution. JAIR, 29:79?103.
1622
L. ?vrelid. 2005. Animacy classification based on morphosyntactic corpus frequencies: some experiments with
Norwegian nouns. In Proc. of the Workshop on Exploring Syntactically Annotated Corpora, pages 1?11.
J. Peng and K. Araki. 2005. Detecting the countability of english compound nouns using web-based models. In
Proc. of IJCNLP.
E. Riloff and J. Shepherd. 1997. A corpus-based approach for building semantic lexicons. In Proc. of EMNLP.
R. Schwartz, O. Tsur, A. Rappoport, and M. Koppel. 2013. Authorship Attribution of Micro-Messages. In Proc.
of EMNLP.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proc.
of ACL-Coling.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank. In Proc. of EMNLP.
G. Sudre, D. Pomerleau, M. Palatucci, L. Wehbe, A. Fyshe, R. Salmelin, and T. Mitchell. 2012. Tracking neural
coding of perceptual and semantic features of concrete nouns. NeuroImage, 62(1):451?463.
P. P. Talukdar and K. Crammer. 2009. New regularized algorithms for transductive learning. In ECML-PKDD,
pages 442?457. Springer.
O. Tsur, D. Davidov, and A. Rappoport. 2010. ICWSM ? a great catchy name: Semi-supervised recognition of
sarcastic sentences in online product reviews. In Proc. of ICWSM.
P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of
artificial intelligence research, 37(1):141?188.
P. Turney, Y. Neuman, D. Assaf, and Y. Cohen. 2011. Literal and metaphorical sense identification through
concrete and abstract context. In Proc. of EMNLP.
P. D. Turney. 2008. The latent relation mapping engine: Algorithm and experiments. Journal of Artificial Intelli-
gence Research, 33:615?655.
R. C. Wang and W. W. Cohen. 2009. Automatic set instance extraction using the web. In Proc. of ACL-IJCNLP.
D. Widdows and B. Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. of Coling.
X. Xing, Y. Zhang, and M. Han. 2010. Query difficulty prediction for contextual image retrieval. In Advances in
Information Retrieval, pages 581?585. Springer.
S. X. Yu and J. Shi. 2003. Multiclass spectral clustering. In Proc. of ICCV.
1623
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880?1891,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Authorship Attribution of Micro-Messages
Roy Schwartz Oren Tsur Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
{roys02|oren|arir}@cs.huji.ac.il
Moshe Koppel
Department of Computer Science
Bar Ilan University
koppel@macs.biu.ac.il
Abstract
Work on authorship attribution has tradition-
ally focused on long texts. In this work, we
tackle the question of whether the author of
a very short text can be successfully iden-
tified. We use Twitter as an experimental
testbed. We introduce the concept of an au-
thor?s unique ?signature?, and show that such
signatures are typical of many authors when
writing very short texts. We also present a new
authorship attribution feature (?flexible pat-
terns?) and demonstrate a significant improve-
ment over our baselines. Our results show that
the author of a single tweet can be identified
with good accuracy in an array of flavors of
the authorship attribution task.
1 Introduction
Research in authorship attribution has developed
substantially over the last decade (Stamatatos,
2009). The vast majority of such research has been
dedicated towards finding the author of long texts,
ranging from single passages to book chapters. In
recent years, the growing popularity of social me-
dia has created special interest, both theoretical and
computational, in short texts. This has led to many
recent authorship attribution projects that experi-
mented with web data such as emails (Abbasi and
Chen, 2008), web forum messages (Solorio et al,
2011) and blogs (Koppel et al, 2011b). This paper
addresses the question to what extent the authors of
very short texts can be identified. To answer this
question, we experiment with Twitter tweets.
Twitter messages (tweets) are limited to 140 char-
acters. This restriction imposes major difficulties on
authorship attribution systems, since authorship at-
tribution methods that work well on long texts are
often not as useful when applied to short texts (Bur-
rows, 2002; Sanderson and Guenter, 2006).
Nonetheless, tweets are relatively self-contained
and have smaller sentence length variance com-
pared to excerpts from longer texts (see Section 3).
These characteristics make Twitter data appealing as
a testbed when focusing on short texts. Moreover,
an authorship attribution system of tweets may have
various applications. Specifically, a range of cyber-
crimes can be addressed using such a system, includ-
ing identity fraud and phishing.
In this paper, we introduce the concept of k-
signatures. We denote the k-signatures of an author
a as the features that appear in at least k% of a?s
training samples, while not appearing in the training
set of any other author. When k is large, such signa-
tures capture a unique style used by a. An analysis
of our training set reveals that unique k-signatures
are typical of many authors. Moreover, a substantial
portion of the tweets in our training set contain at
least one such signature. These findings suggest that
a single tweet, although short and sparse, often con-
tains sufficient information for identifying its author.
Our results show that this is indeed the case.
We train an SVM classifier with a set of features
that include character n-grams and word n-grams.
We use a rigorous experimental setup, with varying
number of authors (values between 50-1,000) and
various sizes of the training set, ranging from 50 to
1,000 tweets per author. In all our experiments, a
single tweet is used as test document. We also use
a setting in which the system is allowed to respond
don?t know in cases of uncertainty. Applying this
option results in higher precision, at the expense of
1880
lower recall.
Our results show that the author of a tweet can be
successfully identified. For example, when using a
dataset of as many as 1,000 authors with 200 train-
ing tweets per author, we are able to obtain 30.3%
accuracy (as opposed to a random baseline of only
0.1%). Using a dataset of 50 authors with as few
as 50 training tweets per author, we obtain 50.7%
accuracy. Using a dataset of 50 authors with 1,000
training tweets per author, our results reach as high
as 71.2% in the standard classification setting, and
exceed 91% accuracy with 60% recall in the don?t
know setting.
We also apply a new set of features, never previ-
ously used for this task ? flexible patterns. Flexi-
ble patterns essentially capture the context in which
function words are used. The effectiveness of func-
tion words as authorship attribution features (Koppel
et al, 2009) suggests using flexible pattern features.
The fact that flexible patterns are learned from plain
text in a fully unsupervised manner makes them
domain and language independent. We demon-
strate that using flexible patterns gives significant
improvement over our baseline system. Further-
more, using flexible patterns, our system obtains a
6.1% improvement over current state-of-the-art re-
sults in authorship attribution on Twitter.
To summarize, the contribution of this paper is
threefold.
? We provide the most extensive research to date
on authorship attribution of micro-messages,
and show that authors of very short texts can
be successfully identified.
? We introduce the concept of an author?s unique
k-signature, and demonstrate that such signa-
tures are used by many authors in their writing
of micro-messages.
? We present a new feature for authorship attri-
bution ? flexible patterns ? and show its sig-
nificant added value over other methods. Us-
ing this feature, our system obtains a 6.1% im-
provement over the current state-of-the-art.
The rest of the paper is organized as follows. Sec-
tions 2 and 3 describe our methods and our experi-
mental testbed (Twitter). Section 4 presents the con-
cept of k-signatures. Sections 5 and 6 present our
experiments and results. Flexible patterns are pre-
sented in Section 7 and related work is presented in
Section 8.
2 Methodology
In the following we briefly describe the main fea-
tures employed by our system. The features below
are binary features.
Character n-grams. Character n-gram features
are especially useful for authorship attribution on
micro-messages since they are relatively tolerant
to typos and non-standard use of punctuation (Sta-
matatos, 2009). These are common in the non-
formal style generally applied in social media ser-
vices. Consider the example of misspelling ?Brit-
ney? as ?Brittney?. The misspelled name shares the
4-grams ?Brit? and ?tney? with the correct name. As
a result, these features provide information about the
author?s style (or at least her topic of interest), which
is not available through lexical features.
Following standard practice, we use 4-grams
(Sanderson and Guenter, 2006; Layton et al, 2010;
Koppel et al, 2011b). White spaces are considered
characters (i.e., a character n-gram may be com-
posed of letters from two different words). A sin-
gle white-space is appended to the beginning and
the end of each tweet. For efficiency, we consider
only character n-gram features that appear at least
tcng times in the training set of at least one author
(see Section 5).
Word n-grams. We hypothesize that word n-gram
features would be useful for authorship attribution
on micro-messages. We assume that under a strict
length restriction, many authors would prefer using
short, repeating phrases (word n-grams).
In our experiments, we consider 2 ? n ? 5.1
We regard sequences of punctuation marks as words.
Two special words are added to each tweet to indi-
cate the beginning and the end of the tweet. For effi-
ciency, we consider only word n-gram features that
appear at least twng times in the training set of at
least one author (see Section 5).
Model. We use libsvm?s Matlab implementation
of a multi-class SVM classifier with a linear kernel
1We skip unigrams as they are generally captured by the
character n-gram features.
1881
(Chang and Lin, 2011). We use ten-fold cross vali-
dation on the training set to select the best regular-
ization factor between 0.5 and 0.005.2
3 Experimental Testbed
Our main research question in this paper is to deter-
mine the extent to which authors of very short texts
can be identified. A major issue in working with
short texts is selecting the right dataset. One ap-
proach is breaking longer texts into shorter chunks
(Sanderson and Guenter, 2006). We take a differ-
ent approach and experiment with micro-messages
(specifically, tweets).
Tweets have several properties making them an
ideal testbed for authorship attribution of short texts.
First, tweets are posted as single units and do not
necessarily refer to each other. As a result, they tend
to be self contained. Second, tweets have more stan-
dardized length distribution compared to other types
of web data. We compared the mean and standard
deviation of sentence length in our Twitter dataset
and in a corpus of English web data (Ferraresi et al,
2008).3 We found that (a) tweets are shorter than
standard web data (14.2 words compared to 20.9),
and (b) the standard deviation of the length of tweets
is much smaller (6.4 vs. 21.4).
Pre-Processing. We use a Twitter corpus that in-
cludes approximately 5 ? 108 tweets.4 All non-
English tweets and tweets that contain fewer than
3 words are removed from the dataset. We also re-
move tweets marked as retweets (using the RT sign,
a standard Twitter symbol to indicate that this tweet
was written by a different user). As some users
retweet without using the RT sign, we also remove
tweets that are an exact copy of an existing tweet
posted in the previous seven days.
Apart from plain text, some tweets contain ref-
erences to other Twitter users (in the format of
@<user>). Since using reference information
makes this task substantially easier (Layton et al,
2010), we replace each user reference with the spe-
cial meta tag REF. For sparsity reasons, we also re-
place web addresses with the meta tag URL, num-
2In practice, 0.05 or 0.1 are selected in almost all cases.
3http://wacky.sslmit.unibo.it
4These comprise ?15% of all public tweets created from
May 2009 to March 2010.
0 5 10 15 20 25 30 35 40 45 >50
0
10
20
30
40
50
60
70
80
90
Number of k?signatures per user
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 1: Number of users with at least x k-signatures
(100 authors, 180 training tweets per author).
bers with the meta tag NUM, time of day with the
meta tag TIME and dates with the meta tag DATE.
4 k-Signatures
In this section, we show that many authors adopt
a unique style when writing micro-messages. This
style can be detected by a strong classification algo-
rithm (such as SVM), and be sufficient to correctly
identify the author of a single tweet.
We define the concept of the k-signature of an au-
thor a to be a feature that appears in at least k% of
a?s training set, while not appearing in the training
set of any other user. Such signatures can be useful
for identifying future (unlabeled) tweets written by
a.
To validate our hypothesis, we use a dataset of
100 authors with 180 tweets per author. We com-
pute the number of k-signatures used by each of
the authors in our dataset. Figure 1 shows our re-
sults for a range of k values (2%, 5%, 10%, 20%
and 50%). Results demonstrate that 81 users use
at least one 2%-signature, 43 users use at least one
5%-signature, and 17 users use at least one 10%-
signature. These results indicate that a large portion
of the users adopt a unique signature (or set of sig-
natures) when writing short texts. Table 1 provides
examples of 10%-signatures.
1882
Signature Type 10%-signature Examples
Character n-grams
? ? ??
REF oh ok ? ? Glad you found it!
Hope everyone is having a good afternoon ? ?
REF Smirnoff lol keeping the goose in the freezer ? ?
?yew ?
gurl yew serving me tea nooch
REF about wen yew and ronnie see each other
REF lol so yew goin to check out tini?s tonight huh???
Word n-grams
.. lal
REF aww those are cool where u get those.. how do ppl react.. lal
Ludas album is gone be hott.. lal
Dayum refs don?t get injury timeouts.. lal.. get him off the field..
smoochies , e3
I?m just back after takin? a very long, icy cold
shower........Shivering smoochies,E3 http://bit.ly/4CzzP9
A blue stout or two would be nice as well, Purr!Blue smooth
smoochies,E3 http://bit.ly/75D4fO
That is sooooooooooooooooooo unfair!Double smoochies,E3
http://bit.ly/07sXRGX
Table 1: Examples of 10%-signatures.
Results also show that seven users use one or
more 20%-signatures, and five users even use one
or more 50%-signatures. Looking carefully at these
users, we find that they write very structured mes-
sages, and are probably bots, such as news feeds,
bidding systems, etc. Table 2 provides examples of
tweets posted by such users.5
Another interesting question is how many tweets
contain at least one k-signature. Figure 2 shows
for each user the number of tweets in her training
set for which at least one k-signature is found. Re-
sults demonstrate that a total of 18.6% of the train-
ing tweets contain at least one 2%-signature, 10.3%
the training tweets contain at least one 5%-signature
and 6.5% of the training tweets contain at least one
10%-signature. These findings validate our assump-
tion that many users use k-signatures in short texts.
These findings also have direct implications on
authorship attribution of micro-messages, since k-
signatures are reliable classification features. As
a result, texts written by authors that tend to use
k-signatures are likely to be easily identified by a
reasonable classification algorithm. Consequently,
k-signatures provide a possible explanation for the
high quality results presented in this paper.
In the broader context, the presence (and contri-
5Our k-signature method can actually be useful for automat-
ically identifying such users. We defer this to future work.
0 20 40 60 80 100 120 140 160 180
0
10
20
30
40
50
60
70
80
90
Number of Tweets with at least one k?Signature
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 2: Number of users with at least x training tweets
that contain at least one k-signature (100 authors, 180
training tweets per author).
bution) of k-signatures is in line with the hypothesis
proposed by (Davidov et al, 2010a): while still us-
ing an informal and unstructured (grammatical) lan-
guage, authors tend to use typical and unique struc-
tures in order to allow a short message to stand alone
without a clear conversational context.
1883
User 20%-signature Examples
1 I?m listening to :
I?m listening to: Sigur R?s ? Intro:
http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb
I?m listening to: Tina Arena ? In Command:
http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25
I?m listening to: Midnight Oil ? Under the Overpass:
http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg
2 news now ( str )
#Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks of
the HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging
#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-
resentatives press to reneg... http://bit.ly/bHPn2L
#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-
try?s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging
3
( NUM bids )
end date :
NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +
CASE: &#163;66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..
http://bit.ly/7uPt6V
Microsoft Xbox 360 Game System - Console Only - Working: US $51.99
(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTv
Microsoft Sony Playstation 3 (80 GB) Console 6 Months Old:
&#163;190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..
http://bit.ly/7kwtDS
Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-
signatures.
5 Experiments
We report of three different experimental configu-
rations. In the experiments described below, each
dataset is divided into training and test sets using
ten-fold cross validation. On the test phase, each
document contains a single tweet.
Experimenting with varying Training Set Sizes.
In order to test the affect of the training set size,
we experiment with an increasingly larger number
of tweets per author. Experimenting with a range of
training set sizes serves two purposes: (a) to check
whether the author of a tweet can be identified us-
ing a very small number of (short) training samples,
and (b) check howmuch our system can benefit from
training on a larger corpus.
In our experiments we only consider users who
posted between 1,000?2,000 tweets6 (a total of
6This range is selected since on one hand we want at least
1,000 tweets per author for our experiments, and on the other
hand we noticed that users with a larger number of tweets in
corpus tend to be spammers or bots that are very easy to identify,
so we limit this number to 2,000.
10,183 users), and randomly select 1,000 tweets per
user. From these users, we select 10 groups of 50
users each.7 We perform a set of classification ex-
periments, selecting for each author an increasingly
larger subset of her 1,000 tweets as training set. Sub-
set sizes are (50, 100, 200, 500, 1,000). Thresh-
old values for our features in each setting (see Sec-
tion 2) are (2, 2, 4, 10, 20) for tcng and (2, 2, 2, 3, 5)
for twng, respectively.
Experimenting with varying Numbers of Au-
thors. In a second set of experiments, we use an
increasingly larger number of authors (values be-
tween 100-1,000), in order to check whether the au-
thor of a very short text can be identified in a ?needle
in a haystack? type of setting.
Due to complexity issues, we only experiment
with 200 tweets per author as training set. We se-
lect groups of size 100, 200, 500 and 1,000 users
(one group per size). We use the same threshold val-
ues as the 200 tweets per author setting previously
described (tcng = 4, twng = 2).
7An eleventh group is selected as development set.
1884
0 100 200 300 400 500 600 700 800 900 1000
45
50
55
60
65
70
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 3: Authorship attribution accuracy for 50 authors
with various training set sizes. The values are averaged
over 10 groups. The random baseline is 2%.
Recall-Precision Tradeoff. Another aspect of our
research question is the level of certainty our system
has when suggesting an author for a given tweet.
In cases of uncertainty, many real life applications
would prefer not to get any response instead of get-
ting a response with low certainty. Moreover, in real
life applications we are often not even sure that the
real author is part of our training set. Consequently,
we allow our system to respond ?don?t know? in
cases of low confidence (Koppel et al, 2006; Kop-
pel et al, 2011b). This allows our system to obtain
higher precision, at the expense of lower recall.
To implement this feature, we use SVM?s proba-
bility estimates, as implemented in libsvm. These
estimates give a score to each potential author.
These scores reflect the probability that this author
is the correct author, as decided by the prediction
model. The selected author is always the one with
the highest probability estimate.
As selection criterion, we use a set of increasingly
larger thresholds (0.05-0.9) for the probability of the
selected author. This means that we do not select test
samples for which the selected author has a proba-
bility estimate value lower than the threshold.
0 100 200 300 400 500 600 700 800 900 1000
25
30
35
40
45
50
55
60
Number of Candidate Authors
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 4: Authorship attribution accuracy with varying
number of candidate authors, using 200 training tweets
per author. The random baselines for 509, 100, 200, 500
and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,
respectively.
6 Basic Results
Experimenting with varying Training Set Sizes.
Figure 3 shows results for our experiments with
50 authors and various training set sizes. Results
demonstrate that authors of very short texts can be
successfully identified, even with as few as 50 tweets
per author (49.5%). When given more training sam-
ples, authors are identified much more accurately
(up to 69.7%). Results also show that, according to
our hypothesis, word n-gram features substantially
improve over character n-grams features only (3%
averaged improvement over all settings).
Experimenting with varying Numbers of Au-
thors. Figure 4 shows our results for various num-
bers of authors, using 200 tweets per author as train-
ing set. Results demonstrate that authors of an
unknown tweet can be identified to a large extent
even when there are as many as 1,000 candidate au-
thors (30.3%, as opposed to a random baseline of
only 0.1%). Results further validate that word n-
gram features substantially improve over character
9Results for 50 authors with 200 tweets per author are taken
from Figure 3.
1885
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
1,000 tweets/author
500 tweets/author
200 tweets/author
100 tweets/author
50 tweets/author
Figure 5: Recall-precision curves for 50 authors with
varying training set sizes.
n-grams features (2.6% averaged improvement).
Recall-Precision Tradeoff. Figure 5 shows the
recall-precision curves for our experiments with 50
authors and varying training set sizes. Results
demonstrate that we are able to obtain very high pre-
cision (over 90%) while still maintaining a relatively
high recall (from ?35% recall for 50 tweets per au-
thor up to> 60% recall for 1,000 tweets per author).
Figure 6 shows the recall-precision curves for our
experiments with varying number of authors. Re-
sults demonstrate that even in the 1,000 authors set-
ting, we are able to obtain high precision values
(90% and 70%) with reasonable recall values (18%
and ?30%, respectively).
7 Flexible Patterns
In previous sections we provided strong evidence
that authors of micro-messages can be successfully
identified using standard methods. In this section we
present a new feature, never previously used for this
task ? flexible patterns. We show that flexible pat-
terns can be used to improve classification results.
Flexible patterns are a generalization of word n-
grams, in the sense that they capture potentially un-
seen word n-grams. As a result, flexible patterns
can pick up fine-grained differences between au-
thors? styles. Unlike other types of pattern features,
0 10 20 30 40 50 60 70 80 90 100
30
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
50 authors
100 authors
200 authors
500 authors
1,000 authors
Figure 6: Recall-precision curves for varying number of
authors.
flexible patterns are computed automatically from
plain text. As such, they can be applied to various
tasks, independently of domain and language. We
describe them in detail.
Word Frequency. Flexible patterns are composed
of high frequency words (HFW) and content words
(CW). Every word in the corpus is defined as either
HFW or CW. This clustering is performed by count-
ing the number of times each word appears in the
corpus of size s. A word that appears more than
10?4?s times in a corpus is considered HFW. A
word that appears less than 10?3?s times in a cor-
pus is considered CW. Some words may serve both
as HFWs and CWs (see Davidov and Rappoport
(2008b) for discussion).
Structure of a Flexible Pattern. Flexible patterns
start and end with an HFW. A sequence of zero or
more CWs separates consecutive HFWs. At least
one CW must appear in every pattern.10 For effi-
ciency, at most six HFWs (and as a result, five CW
sequences) may appear in a flexible pattern. Exam-
ples of flexible patterns include
1. ?theHFW CW ofHFW theHFW?
10Omitting this treats word n-grams as flexible patterns.
1886
Flexible Pattern Features. Flexible patterns can
serve as binary classification features; a tweet
matches a given flexible pattern if it contains the
flexible pattern sequence. For example, (1) is
matched by (2).
2. ?Go to theHFW houseCW ofHFW theHFW rising sun?
Partial Flexible Patterns. A flexible pattern may
appear in a given tweet with additional words not
originally found in the flexible pattern, and/or with
only a subset of the HFWs (Davidov et al, 2010a).
For example, (3) is a partial match of (1), since the
word ?great? is not part of the original flexible pat-
tern. Similarly, (4) is another partial match of (1),
since (a) the word ?good? is not part of the original
flexible pattern and (b) the second occurrence of the
word ?the? does not appear in (4) (missing word is
marked by ).
3. ?TheHFW greatHFW kingCW ofHFW theHFW ring?
4. ?TheHFW goodHFW kingCW ofHFW Spain?
We use such cases as features with lower weight,
proportional to the number of found HFWs in the
tweet (w = 0.5?nfoundnexpected ). For example, (1) receives a
weight of 1 (complete match) against (2). Against
(3), it receives a weight of 0.5 (= 0.5?33 , partial
match with no missing HFWs). Against (4) it re-
ceives a weight of 1/3 (= 0.5?23 , partial match with
only 2/3 HFWs found).
Experimenting with Flexible Pattern Features.
We repeat our experiments with varying training set
sizes (see Section 5) with two more systems: one
that uses character n-grams and flexible pattern fea-
tures, and another that uses character n-grams, word
n-grams and flexible patterns. High frequency word
counts are computed separately for each author us-
ing her training set. We only consider flexible pat-
tern features that appear at least tfp times in the
training set of at least one author. Values of tfp for
training set sizes (50, 100, 200, 500, 1,000) are (2,
3, 7, 7, 8), respectively.
Results. Figure 7 shows our results. Results
demonstrate that flexible pattern features have an
added value over both character n-grams alone (av-
eraged 2.9% improvement) and over character n-
grams and word n-grams together (averaged 1.5%
0 100 200 300 400 500 600 700 800 900 1000
35
40
45
50
55
60
65
70
75
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams, Word N?grams &
Flex. Patt. Feats.
Char. N?grams + Flex. Patt. Feats.
Char. N?grams + Word N?grams
Char. N?grams
SCAP
Naive Bayes
Figure 7: Authorship attribution accuracy for 50 authors
with various training set sizes and various feature sets.
The values are averaged over 10 groups. The random
baseline is 2%.
Comparison to previous work: SCAP ? SCAP algo-
rithm results, as reported by (Layton et al, 2010), Naive
Bayes ? Naive Bayes algorithm results, as reported by
(Boutwell, 2011).
improvement). We perform t-tests on each of our
training set sizes to check whether the latter im-
provement is significant. Results demonstrate that
it is highly significant in all settings, with p-values
smaller than values between 10?3 (for 50 tweets per
author) and 10?8 (1,000 tweets per author).
Comparison to Previous Works. Figure 7 also
shows results for the only two works that experi-
mented in some of the settings we experimented in:
Layton et al (2010) and Boutwell (2011) (see Sec-
tion 8). Our system substantially outperforms these
two systems, by margins of 5.9% to 19%. These
margins are explained by the choice of algorithm
(SVM and not SCAP/naive Bayes) and our set of
features (character n-grams + word n-grams + flex-
ible patterns compared to character n-grams only).
In order to rule out the possibility that these mar-
gins stem from using different datasets, we tested
our system on the dataset used in (Layton et al,
2010). Our system obtains even higher results on
this dataset than on our datasets (61.6%, a total im-
1887
provement of 6.1% over (Layton et al, 2010)).
Discussion. To illustrate the additional contribu-
tion of flexible patterns over word n-grams, consider
the following tweets, written by the same author.
5. ?. . . theHFW wayCW IHFW treatedCW herHFW?
6. ?. . . half of theHFW thingsCW IHFW have seen?
7. ?. . . theHFW friendsCW IHFW have had for years?
8. ?. . . in theHFW neighborhoodCW IHFW grew up in?
Consider a case where (5) is part of the test set,
while (6-8) appear in the training set. As (5) shares
no sequence of words with (6-8), no word n-gram
feature is able to identify the author?s style in (5).
However, this style can be successfully identified us-
ing the flexible pattern (9), shared by (5-8).
9. theHFW CW IHFW
This demonstrates the added value flexible pattern
features have over word n-gram features.
8 Related Work
Authorship attribution dates back to the end of 19th
century, when (Mendenhall, 1887) applied sentence
length and word length features to plays of Shake-
speare. Ever since, many methods have been devel-
oped for this task. For recent surveys, see (Koppel
et al, 2009; Stamatatos, 2009; Juola, 2012).
Authorship attribution methods can be generally
divided into two categories (Stamatatos, 2009). In
similarity-based methods, an anonymous text is at-
tributed to some author whose writing style is most
similar (by some distance metric). In machine learn-
ing methods, which we follow in this paper, anony-
mous texts are classified, using machine learning al-
gorithms, into different categories (in this case, dif-
ferent authors).
Machine learning papers differ from each other by
the features and machine learning algorithm. Exam-
ples of features include HFWs (Mosteller and Wal-
lace, 1964; Argamon et al, 2007), character n-gram
(Kjell, 1994; Hoorn et al, 1999; Stamatatos, 2008),
word n-grams (Peng et al, 2004), part-of-speech
n-grams (Koppel and Schler, 2003; Koppel et al,
2005) and vocabulary richness (Abbasi and Chen,
2005).
The various machine learning algorithms used in-
clude naive Bayes (Mosteller and Wallace, 1964;
Kjell, 1994), neural networks (Matthews and Mer-
riam, 1993; Kjell, 1994), K-nearest neighbors (Kjell
et al, 1995; Hoorn et al, 1999) and SVM (De Vel et
al., 2001; Diederich et al, 2003; Koppel and Schler,
2003).
Traditionally, authorship attribution systems have
mainly been evaluated against long texts such as
theater plays (Mendenhall, 1887), essays (Yule,
1939; Mosteller and Wallace, 1964), biblical books
(Mealand, 1995; Koppel et al, 2011a) and book
chapters (Argamon et al, 2007; Koppel et al, 2007).
In recent year, many works focused on web data
such as emails (De Vel et al, 2001; Koppel and
Schler, 2003; Abbasi and Chen, 2008), web forum
messages (Abbasi and Chen, 2005; Solorio et al,
2011), blogs (Koppel et al, 2006; Koppel et al,
2011b) and chat messages (Abbasi and Chen, 2008).
Some works focused on SMS messages (Mohan et
al., 2010; Ishihara, 2011).
Authorship Attribution on Twitter. The perfor-
mance of authorship attribution systems on short
texts is affected by several factors (Stamatatos,
2009). These factors include the number of candi-
date authors, the training set size and the size of the
test document.
Very few authorship attribution works experi-
mented with Twitter. Unlike our work, all used a
single group of authors (group sizes varied between
3-50). Layton et al (2010) used the SCAP method-
ology (Frantzeskou et al, 2007) with character n-
gram features. They experimented with 50 authors
and compared different numbers of tweets per au-
thor (values between 20-200). Surprisingly, they
showed that their system does not improve when
given more training tweets. In our work, we no-
ticed a different trend, and showed that more data
can be extremely valuable for authorship attribution
systems on micro-messages (see Section 6). Silva
et al (2011) trained an SVM classifier with various
features (e.g., punctuation and vocabulary features)
on a small dataset of three authors only, with vary-
ing training set size. Although their work used a
set of Twitter-specific features that we do not explic-
itly use, our features implicitly cover a large portion
of their features (such as punctuation and emoticon
1888
features, which are largely covered by character n-
grams).
Boutwell (2011) used a naive Bayes classifier
with character n-gram features. She experimented
with 50 authors and two training size values (120
and 230). She also provided a set of experiments that
studied the effect of joining several tweets into a sin-
gle document. Mikros and Perifanos (2013) trained
an SVM classifier with character n-gram and word
n-grams. They experimented with 10 authors of
Greek text, and also joined several tweets into a sin-
gle document. Joining several tweets into a longer
document is appealing since it can lead to substantial
improvement of the classification results, as demon-
strated by the works above. However, this approach
requires the test data to contain several tweets that
are known a-priori to be written by the same author.
This assumption is not always realistic. In our paper,
we intentionally focus on a single tweet as document
size.
Flexible Patterns. Patterns were introduced by
(Hearst, 1992), who used hand crafted patterns
to discover hyponyms. Hard coded patterns
were used for many tasks, such as discovering
meronymy (Berland and Charniak, 1999), noun cat-
egories (Widdows and Dorow, 2002), verb relations
(Chklovski and Pantel, 2004) and semantic class
learning (Kozareva et al, 2008).
Patterns were first extracted in a fully unsuper-
vised manner (?flexible patterns?) by (Davidov and
Rappoport, 2006), who used flexible patterns in or-
der to establish noun categories, and (Bicic?i and
Yuret, 2006) who used them for analogy question
answering. Ever since, flexible patterns were used
as features for various tasks such as extraction of
semantic relationships (Davidov et al, 2007; Tur-
ney, 2008b; Bollegala et al, 2009), detection of
synonyms (Turney, 2008a), disambiguation of nom-
inal compound relations (Davidov and Rappoport,
2008a), sentiment analysis (Davidov et al, 2010b)
and detection of sarcasm (Tsur et al, 2010).
9 Conclusion
The main goal of this paper is to measure to what
extent authors of micro-messages can be identified.
We have shown that authors of very short texts
can be successfully identified in an array of au-
thorship attribution settings reported for long doc-
uments. This is the first work on micro-messages
to address some of these settings. We introduced
the concept of k-signature. Using this concept, we
proposed an interpretation of our results. Last, we
presented the first authorship attribution system that
uses flexible patterns, and demonstrated that using
these features significantly improves over other sys-
tems. Our system obtains 6.1% improvement over
the current state-of-the-art.
Acknowledgments
We would like to thank Elad Eban and Susan Good-
man for their helpful advice, as well as Robert Lay-
ton for providing us with his dataset. This research
was funded (in part) by the Harry and Sylvia Hoff-
man leadership and responsibility program (for the
first author) and the Intel Collaborative Research In-
stitute for Computational Intelligence (ICRI-CI).
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-
thorship analysis to extremist-group web forum mes-
sages. IEEE Intelligent Systems, 20:67?75.
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:
A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Transac-
tions on Information Systems, 26(2):7:1?7:29.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features: Research articles. J. Am. Soc. Inf. Sci.
Technol., 58(6):802?822.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proc. of ACL, pages
57?64, College Park, Maryland, USA.
Ergun Bicic?i and Deniz Yuret. 2006. Clustering word
pairs to answer analogy questions. In Proc. of TAINN,
pages 1?8.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2009. Measuring the similarity between
implicit semantic relations from the web. In Proc. of
WWW, New York, New York, USA. ACM Press.
Sarah R. Boutwell. 2011. Authorship Attribution of
Short Messages Using Multimodal Features. Master?s
thesis, Naval Postgraduate School.
John Burrows. 2002. ?Delta?: a Measure of Stylistic
Difference and a Guide to Likely Authorship. Literary
and Linguistic Computing, 17(3):267?287.
1889
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Dekang Lin and Dekai Wu, editors, Proc.
of EMNLP, pages 33?40, Barcelona, Spain.
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proc.
of ACL-Coling, pages 297?304, Sydney, Australia.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of ACL-08: HLT,
pages 227?235, Columbus, Ohio, June. Association
for Computational Linguistics.
Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
SAT analogy questions. In Proc. of ACL-HLT, pages
692?700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of ACL,
pages 232?239, Prague, Czech Republic.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proc. of CoNLL, pages 107?
116, Uppsala, Sweden.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proc. of Coling, pages 241?249, Bei-
jing, China.
Olivier De Vel, Alison Anderson, Malcolm Corney, and
George Mohay. 2001. Mining e-mail content for au-
thor identification forensics. ACM Sigmod Record,
30(4):55?64.
JoachimDiederich, Jo?rg Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied intelligence, 19(1-
2):109?123.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english. In
Proc. of the 4th Web as Corpus Workshop, WAC-4.
Georgia Frantzeskou, Efstathios Stamatatos, Stefanos
Gritzalis, and Carole E Chaski. 2007. Identifying au-
thorship by byte-level n-grams: The source code au-
thor profile (scap) method. Int Journal of Digital Evi-
dence, 6(1):1?18.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of Coling
? Volume 2, pages 539?545, Stroudsburg, PA, USA.
Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and
Floor van der Ham. 1999. Neural network identifi-
cation of poets using letter sequences. Literary and
Linguistic Computing, 14(3):311?338.
Shunichi Ishihara. 2011. A forensic authorship clas-
sification in sms messages: A likelihood ratio based
approach using n-gram. In Proc. of the Australasian
Language Technology Association Workshop 2011,
pages 47?56, Canberra, Australia.
Patrick Juola. 2012. Large-scale experiments in author-
ship attribution. English Studies, 93(3):275?283.
Bradley Kjell, W Addison Woods, and Ophir Frieder.
1995. Information retrieval using letter tuples with
neural network and nearest neighbor classifiers. In
IEEE International Conference on Systems, Man and
Cybernetics, volume 2, pages 1222?1226. IEEE.
Bradley Kjell. 1994. Authorship determination using let-
ter pair frequency features with neural network classi-
fiers. Literary and Linguistic Computing, 9(2):119?
124.
Moshe Koppel and Jonathan Schler. 2003. Exploiting
stylistic idiosyncrasies for authorship attribution. In
Proc. of IJCAI?03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis, volume 69,
page 72.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, KDD ?05, pages 624?628, New York,
NY, USA.
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and
EranMesseri. 2006. Authorship attribution with thou-
sands of candidate authors. In SIGIR, pages 659?660.
Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-
Dokow. 2007. Measuring differentiability: Unmask-
ing pseudonymous authors. JMLR, 8:1261?1276.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9?26.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011a. Unsupervised decom-
position of a document into authorial components. In
Proc. of ACL-HLT, pages 1356?1364, Portland, Ore-
gon, USA.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011b. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
1890
pattern linkage graphs. In Proc. of ACL-HLT, pages
1048?1056, Columbus, Ohio.
Robert Layton, PaulWatters, and Richard Dazeley. 2010.
Authorship attribution for twitter in 140 characters or
less. In Proc. of the 2010 Second Cybercrime and
Trustworthy Computing Workshop, CTC ?10, pages 1?
8, Washington, DC, USA. IEEE Computer Society.
Robert AJ Matthews and Thomas VN Merriam. 1993.
Neural computation in stylometry i: An application to
the works of shakespeare and fletcher. Literary and
Linguistic Computing, 8(4):203?209.
DL Mealand. 1995. Correspondence analysis of luke.
Literary and linguistic computing, 10(3):171?182.
Thomas Corwin Mendenhall. 1887. The characteristic
curves of composition. Science, ns-9(214S):237?246.
George K Mikros and Kostas Perifanos. 2013. Author-
ship attribution in greek tweets using authors multi-
level n-gram profiles. In 2013 AAAI Spring Sympo-
sium Series.
Ashwin Mohan, Ibrahim M Baggili, and Marcus K
Rogers. 2010. Authorship attribution of sms mes-
sages using an n-grams approach. Technical report,
CERIAS Tech Report 2011.
Frederick Mosteller and David Lee Wallace. 1964.
Inference and disputed authorship: The Federalist.
Addison-Wesley.
Fuchun Peng, Dale Schuurmans, and Shaojun Wang.
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Information Retrieval, 7(3-
4):317?345.
Conrad Sanderson and Simon Guenter. 2006. Short text
authorship attribution via sequence kernels, markov
chains and author unmasking: An investigation. In
Proc. of EMNLP, pages 482?491, Sydney, Australia.
Rui Sousa Silva, Gustavo Laboreiro, Lu??s Sarmento, Tim
Grant, Euge?nio Oliveira, and Belinda Maia. 2011.
?twazn me!!! ;(? automatic authorship analysis of
micro-blogging messages. In Proc. of the 16th inter-
national conference on Natural language processing
and information systems, NLDB?11, pages 161?168,
Berlin, Heidelberg. Springer-Verlag.
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-Gomez. 2011. Modality specific
meta features for authorship attribution in web forum
posts. In Proc. of IJCNLP, pages 156?164, Chiang
Mai, Thailand, November.
Efstathios Stamatatos. 2008. Author identification: Us-
ing text sampling to handle the class imbalance prob-
lem. Inf. Process. Manage., 44(2):790?799.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm?a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product reviews.
In Proc. of ICWSM.
Peter Turney. 2008a. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proc. of
Coling, pages 905?912,Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Peter D. Turney. 2008b. The latent relation mapping en-
gine: Algorithm and experiments. Journal of Artificial
Intelligence Research, 33:615?655.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of Coling, pages 1?7, Stroudsburg, PA, USA.
George Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: with application
to two cases of disputed authorship. Biometrika, 30(3-
4):363?390.
1891
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 663?672,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Neutralizing Linguistically Problematic Annotations
in Unsupervised Dependency Parsing Evaluation
Roy Schwartz1 Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem
{roys02|omria01|arir}@cs.huji.ac.il
2Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
roiri@csail.mit.edu
Abstract
Dependency parsing is a central NLP task. In
this paper we show that the common eval-
uation for unsupervised dependency parsing
is highly sensitive to problematic annotations.
We show that for three leading unsupervised
parsers (Klein and Manning, 2004; Cohen and
Smith, 2009; Spitkovsky et al, 2010a), a small
set of parameters can be found whose mod-
ification yields a significant improvement in
standard evaluation measures. These param-
eters correspond to local cases where no lin-
guistic consensus exists as to the proper gold
annotation. Therefore, the standard evaluation
does not provide a true indication of algorithm
quality. We present a new measure, Neutral
Edge Direction (NED), and show that it greatly
reduces this undesired phenomenon.
1 Introduction
Unsupervised induction of dependency parsers is a
major NLP task that attracts a substantial amount
of research (Klein and Manning, 2004; Cohen et
al., 2008; Headden et al, 2009; Spitkovsky et al,
2010a; Gillenwater et al, 2010; Berg-Kirkpatrick
et al, 2010; Blunsom and Cohn, 2010, inter alia).
Parser quality is usually evaluated by comparing its
output to a gold standard whose annotations are lin-
guistically motivated. However, there are cases in
which there is no linguistic consensus as to what the
correct annotation is (Ku?bler et al, 2009). Examples
include which verb is the head in a verb group struc-
ture (e.g., ?can? or ?eat? in ?can eat?), and which
? Omri Abend is grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship.
noun is the head in a sequence of proper nouns (e.g.,
?John? or ?Doe? in ?John Doe?). We refer to such
annotations as (linguistically) problematic. For such
cases, evaluation measures should not punish the al-
gorithm for deviating from the gold standard.
In this paper we show that the evaluation mea-
sures reported in current works are highly sensitive
to the annotation in problematic cases, and propose
a simple new measure that greatly neutralizes the
problem.
We start from the following observation: for three
leading algorithms (Klein and Manning, 2004; Co-
hen and Smith, 2009; Spitkovsky et al, 2010a), a
small set (at most 18 out of a few thousands) of pa-
rameters can be found whose modification dramati-
cally improves the standard evaluation measures (the
attachment score measure by 9.3-15.1%, and the
undirected measure by a smaller but still significant
1.3-7.7%). The phenomenon is implementation in-
dependent, occurring with several algorithms based
on a fundamental probabilistic dependency model1.
We show that these parameter changes can be
mapped to edge direction changes in local structures
in the dependency graph, and that these correspond
to problematic annotations. Thus, the standard eval-
uation measures do not reflect the true quality of the
evaluated algorithm.
We explain why the standard undirected evalua-
tion measure is in fact sensitive to such edge direc-
1It is also language-independent; we have produced it in five
different languages: English, Czech, Japanese, Portuguese, and
Turkish. Due to space considerations, in this paper we focus
on English, because it is the most studied language for this task
and the most practically useful one at present.
663
tion changes, and present a new evaluation measure,
Neutral Edge Direction (NED), which greatly allevi-
ates the problem by ignoring the edge direction in lo-
cal structures. Using NED, manual modifications of
model parameters always yields small performance
differences. Moreover, NED sometimes punishes
such manual parameter tweaking by yielding worse
results. We explain this behavior using an exper-
iment revealing that NED always prefers the struc-
tures that are more consistent with the modeling as-
sumptions lying in the basis of the algorithm. When
manual parameter modification is done against this
preference, the NED results decrease.
The contributions of this paper are as follows.
First, we show the impact of a small number of an-
notation decisions on the performance of unsuper-
vised dependency parsers. Second, we observe that
often these decisions are linguistically controversial
and therefore this impact is misleading. This reveals
a problem in the common evaluation of unsuper-
vised dependency parsing. This is further demon-
strated by noting that recent papers evaluate the task
using three gold standards which differ in such deci-
sions and which yield substantially different results.
Third, we present the NED measure, which is agnos-
tic to errors arising from choosing the non-gold di-
rection in such cases.
Section 2 reviews related work. Section 3 de-
scribes the performed parameter modifications. Sec-
tion 4 discusses the linguistic controversies in anno-
tating problematic dependency structures. Section 5
presents NED. Section 6 describes experiments with
it. A discussion is given in Section 7.
2 Related Work
Grammar induction received considerable attention
over the years (see (Clark, 2001; Klein, 2005) for
reviews). For unsupervised dependency parsing, the
Dependency Model with Valence (DMV) (Klein and
Manning, 2004) was the first to beat the simple
right-branching baseline. A technical description of
DMV is given at the end of this section.
The great majority of recent works, including
those experimented with in this paper, are elabora-
tions of DMV. Smith and Eisner (2005) improved
the DMV results by generalizing the function maxi-
mized by DMV?s EM training algorithm. Smith and
Eisner (2006) used a structural locality bias, experi-
menting on five languages. Cohen et al (2008) ex-
tended DMV by using a variational EM training al-
gorithm and adding logistic normal priors. Cohen
and Smith (2009, 2010) further extended it by us-
ing a shared logistic normal prior which provided a
new way to encode the knowledge that some POS
tags are more similar than others. A bilingual joint
learning further improved their performance.
Headden et al (2009) obtained the best reported
results on WSJ10 by using a lexical extension of
DMV. Gillenwater et al (2010) used posterior reg-
ularization to bias the training towards a small num-
ber of parent-child combinations. Berg-Kirkpatrick
et al (2010) added new features to the M step of the
DMV EM procedure. Berg-Kirkpatrick and Klein
(2010) used a phylogenetic tree to model parame-
ter drift between different languages. Spitkovsky
et al (2010a) explored several training protocols
for DMV. Spitkovsky et al (2010c) showed the
benefits of Viterbi (?hard?) EM to DMV training.
Spitkovsky et al (2010b) presented a novel lightly-
supervised approach that used hyper-text mark-up
annotation of web-pages to train DMV.
A few non-DMV-based works were recently pre-
sented. Daume? III (2009) used shift-reduce tech-
niques. Blunsom and Cohn (2010) used tree sub-
stitution grammar to achieve best results on WSJ?.
Druck et al (2009) took a semi-supervised ap-
proach, using a set of rules such as ?A noun is usu-
ally the parent of a determiner which is to its left?,
experimenting on several languages. Naseem et al
(2010) further extended this idea by using a single
set of rules which globally applies to six different
languages. The latter used a model similar to DMV.
The controversial nature of some dependency
structures was discussed in (Nivre, 2006; Ku?bler
et al, 2009). Klein (2005) discussed controversial
constituency structures and the evaluation problems
stemming from them, stressing the importance of a
consistent standard of evaluation.
A few works explored the effects of annotation
conventions on parsing performance. Nilsson et
al. (2006) transformed the dependency annotations
of coordinations and verb groups in the Prague
TreeBank. They trained the supervised MaltParser
(Nivre et al, 2006) on the transformed data, parsed
the test data and re-transformed the resulting parse,
664
w3 w2 w1
(a)
w3 w2 w1
(b)
Figure 1: A dependency structure on the words
w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b))
an edge-flip of w2?w1.
thus improving performance. Klein and Manning
(2004) observed that a large portion of their errors is
caused by predicting the wrong direction of the edge
between a noun and its determiner. Ku?bler (2005)
compared two different conversion schemes in Ger-
man supervised constituency parsing and found one
to have positive influence on parsing quality.
Dependency Model with Valence (DMV). DMV
(Klein and Manning, 2004) defines a probabilistic
grammar for unlabeled dependency structures. It is
defined as follows: the root of the sentence is first
generated, and then each head recursively generates
its right and left dependents. The parameters of the
model are of two types: PSTOP and PATTACH .
PSTOP (dir, h, adj) determines the probability to
stop generating arguments, and is conditioned on 3
arguments: the head h, the direction dir ((L)eft
or (R)ight) and adjacency adj (whether the head
already has dependents ((Y )es) in direction dir or
not ((N)o)). PATTACH(arg|h, dir) determines the
probability to generate arg as head h?s dependent in
direction dir.
3 Significant Effects of Edge Flipping
In this section we present recurring error patterns
in some of the leading unsupervised dependency
parsers. These patterns are all local, confined to a
sequence of up to three words (but mainly of just
two consecutive words). They can often be mended
by changing the directions of a few types of edges.
The modified parameters described in this section
were handpicked to improve performance: we ex-
amined the local parser errors occurring the largest
number of times, and found the corresponding pa-
rameters. Note that this is a valid methodology,
since our goal is not to design a new algorithm but
to demonstrate that modifying a small set of param-
eters can yield a major performance boost and even-
tually discover problems with evaluation methods or
algorithms.
I
PRP
want
VBP
to
TO
eat
VB
.
ROOT
Figure 2: A parse of the sentence ?I want to eat?, before
(straight line) and after (dashed line) an edge-flip of the
edge ?to???eat?.
We start with a few definitions. Consider Fig-
ure 1(a) that shows a dependency structure on the
words w1, w2, w3. Edge flipping (henceforth, edge-
flip) the edge w2?w1 is the following modification
of a parse tree: (1) setting w2?s parent as w1 (instead
of the other way around), and (2) setting w1?s par-
ent as w3 (instead of the edge w3?w2). Figure 1(b)
shows the dependency structure after the edge-flip.
Note that (1) imposes setting a new parent to w2,
as otherwise it would have had no parent. Setting
this parent to be w3 is the minimal modification of
the original parse, since it does not change the at-
tachment of the structure [w2, w1] to the rest of the
sentence, but only the direction of the internal edge.
Figure 2 presents a parse of the sentence ?I want
to eat?, before and after an edge-flip of the edge
?to???eat?.
Since unsupervised dependency parsers are gen-
erally structure prediction models, the predictions
of the parse edges are not independent. Therefore,
there is no single parameter which completely con-
trols the edge direction, and hence there is no direct
way to perform an edge-flip by parameter modifica-
tion. However, setting extreme values for the param-
eters controlling the direction of a certain edge type
creates a strong preference towards one of the direc-
tions, and effectively determines the edge direction.
This procedure is henceforth termed parameter-flip.
We show that by performing a few parameter-
flips, a substantial improvement in the attachment
score can be obtained. Results are reported for three
algorithms.
Parameter Changes. All the works experimented
with in this paper are not lexical and use sequences
of POS tags as their input. In addition, they all use
the DMV parameter set (PSTOP and PATTACH) for
parsing. We will henceforth refer to this set, condi-
tioned on POS tags, as the model parameter set.
We show how an edge in the dependency graph
is encoded using the DMV parameters. Say the
665
model prefers setting ?to? (POS tag: TO) as a de-
pendent of the infinitive verb (POS tag: V B) to its
right (e.g., ?to eat?). This is reflected by a high
value of PATTACH(TO|V B,L), a low value of
PATTACH(V B|TO,R), since ?to? tends to be a left
dependent of the verb and not the other way around,
and a low value of PSTOP (V B,L,N), as the verb
usually has at least one left argument (i.e., ?to?).
A parameter-flip of w1?w2 is hence performed
by setting PATTACH(w2|w1, R) to a very low
value and PATTACH(w1|w2, L) to a very high
value. When the modifications to PATTACH
are insufficient to modify the edge direction,
PSTOP (w2, L,N) is set to a very low value and
PSTOP (w1, R,N) to a very high value2.
Table 1 describes the changes made for the three
algorithms. The ?+? signs in the table correspond to
edges in which the algorithm disagreed with the gold
standard, and were thus modified. Similarly, the ???
signs in the table correspond to edges in which the
algorithm agreed with the gold standard, and were
thus not modified. The number of modified param-
eters does not exceed 18 (out of a few thousands).
The Freq. column in the table shows the percent-
age of the tokens in sections 2-21 of PTB WSJ that
participate in each structure. Equivalently, the per-
centage of edges in the corpus which are of either
of the types appearing in the Orig. Edge column.
As the table shows, the modified structures cover a
significant portion of the tokens. Indeed, 42.9% of
the tokens in the corpus participate in at least one of
them3.
Experimenting with Edge Flipping. We experi-
mented with three DMV-based algorithms: a repli-
cation of (Klein and Manning, 2004), as appears in
(Cohen et al, 2008) (henceforth, km04), Cohen and
Smith (2009) (henceforth, cs09), and Spitkovsky et
al. (2010a) (henceforth, saj10a). Decoding is done
using the Viterbi algorithm4. For each of these algo-
rithms we present the performance gain when com-
pared to the original parameters.
The training set is sections 2-21 of the Wall Street
2Note that this yields unnormalized models. Again, this is
justified since the resulting model is only used as a basis for
discussion and is not a fully fledged algorithm.
3Some tokens participate in more than one structure.
4http://www.cs.cmu.edu/?scohen/parser.html.
Structure Freq. Orig. Edge km04 cs09 saj10a
Coordination
(?John & Mary?) 2.9% CC?NNP ? + ?
Prepositional
Phrase (?in
the house?)
32.7%
DT?NN + + +
DT?NNP ? + +
DT?NNS ? ? +
IN?DT + + ?
IN?NN + + ?
IN?NNP + ? ?
IN?NNS ? + ?
PRP$?NN ? ? +
Modal Verb
(?can eat?) 2.4% MD?V B ? + ?
Infinitive Verb
(?to eat?) 4.5% TO?V B ? + +
Proper Name
Sequence
(?John Doe?)
18.5% NNP?NNP + ? ?
Table 1: Parameter changes for the three algorithms. The
Freq. column shows what percentage of the tokens in sec-
tions 2-21 of PTB WSJ participate in each structure. The
Orig. column indicates the original edge. The modified
edge is of the opposite direction. The other columns show
the different algorithms: km04: basic DMV model (repli-
cation of (Klein and Manning, 2004)); cs09; (Cohen and
Smith, 2009); saj10a: (Spitkovsky et al, 2010a).
Journal Penn TreeBank (Marcus et al, 1993). Test-
ing is done on section 23. The constituency annota-
tion was converted to dependencies using the rules
of (Yamada and Matsumoto, 2003)5.
Following standard practice, we present the at-
tachment score (i.e., percentage of words that have a
correct head) of each algorithm, with both the origi-
nal parameters and the modified ones. We present
results both on all sentences and on sentences of
length ? 10, excluding punctuation.
Table 2 shows results for all algorithms6. The
performance difference between the original and the
modified parameter set is considerable for all data
sets, where differences exceed 9.3%, and go up to
15.1%. These are enormous differences from the
perspective of current algorithm evaluation results.
4 Linguistically Problematic Annotations
In this section, we discuss the controversial nature
of the annotation in the modified structures (Ku?bler
5http://www.jaist.ac.jp/?h-yamada/
6Results are slightly worse than the ones published in the
original papers due to the different decoding algorithms (cs09
use MBR while we used Viterbi) and a different conversion pro-
cedure (saj10a used (Collins, 1999) and not (Yamada and Mat-
sumoto, 2003)) ; see Section 5.
666
Algo. ? 10 ? ?
Orig. Mod. ? Orig. Mod. ?
km04 45.8 59.8 14 34.6 43.9 9.3
cs09 60.9 72.9 12 39.9 54.6 14.7
saj10a 54.7 69.8 15.1 41.6 54.3 12.7
Table 2: Results of the original (Orig. columns), the
modified (Mod. columns) parameter sets and their dif-
ference (? columns) for the three algorithms.
et al, 2009). We remind the reader that structures
for which no linguistic consensus exists as to their
correct annotation are referred to as (linguistically)
problematic.
We begin by showing that all the structures mod-
ified are indeed linguistically problematic. We then
note that these controversies are reflected in the eval-
uation of this task, resulting in three, significantly
different, gold standards currently in use.
Coordination Structures are composed of two
proper nouns, separated by a conjunctor (e.g., ?John
and Mary?). It is not clear which token should be the
head of this structure, if any (Nilsson et al, 2006).
Prepositional Phrases (e.g., ?in the house? or ?in
Rome?), where every word is a reasonable candidate
to head this structure. For example, in the annotation
scheme used by (Collins, 1999) the preposition is the
head, in the scheme used by (Johansson and Nugues,
2007) the noun is the head, while TUT annotation,
presented in (Bosco and Lombardo, 2004), takes the
determiner to be the noun?s head.
Verb Groups are composed of a verb and an aux-
iliary or a modal verb (e.g., ?can eat?). Some
schemes choose the modal as the head (Collins,
1999), others choose the verb (Rambow et al, 2002).
Infinitive Verbs (e.g., ?to eat?) are also in contro-
versy, as in (Yamada and Matsumoto, 2003) the verb
is the head while in (Collins, 1999; Bosco and Lom-
bardo, 2004) the ?to? token is the head.
Sequences of Proper Nouns (e.g., ?John Doe?)
are also subject to debate, as PTB?s scheme takes the
last proper noun as the head, and BIO?s scheme de-
fines a more complex scheme (Dredze et al, 2007).
Evaluation Inconsistency Across Papers. A fact
that may not be recognized by some readers is that
comparing the results of unsupervised dependency
parsers across different papers is not directly pos-
sible, since different papers use different gold stan-
dard annotations even when they are all derived from
the Penn Treebank constituency annotation. This
happens because they use different rules for con-
verting constituency annotation to dependency an-
notation. A probable explanation for this fact is that
people have tried to correct linguistically problem-
atic annotations in different ways, which is why we
note this issue here7.
There are three different annotation schemes
in current use: (1) Collins head rules (Collins,
1999), used in e.g., (Berg-Kirkpatrick et al, 2010;
Spitkovsky et al, 2010a); (2) Conversion rules of
(Yamada and Matsumoto, 2003), used in e.g., (Co-
hen and Smith, 2009; Gillenwater et al, 2010); (3)
Conversion rules of (Johansson and Nugues, 2007)
used, e.g., in the CoNLL shared task 2007 (Nivre et
al., 2007) and in (Blunsom and Cohn, 2010).
The differences between the schemes are substan-
tial. For instance, 14.4% of section 23 is tagged dif-
ferently by (1) and (2)8.
5 The Neutral Edge Direction (NED)
Measure
As shown in the previous sections, the annotation
of problematic edges can substantially affect perfor-
mance. This was briefly discussed in (Klein and
Manning, 2004), which used undirected evaluation
as a measure which is less sensitive to alternative
annotations. Undirected accuracy was commonly
used since to assess the performance of unsuper-
vised parsers (e.g., (Smith and Eisner, 2006; Head-
den et al, 2008; Spitkovsky et al, 2010a)) but also
of supervised ones (Wang et al, 2005; Wang et al,
2006). In this section we discuss why this measure
is in fact not indifferent to edge-flips and propose a
new measure, Neutral Edge Direction (NED).
7Indeed, half a dozen flags in the LTH Constituent-to-
Dependency Conversion Tool (Johansson and Nugues, 2007)
are used to control the conversion in problematic cases.
8In our experiments we used the scheme of (Yamada and
Matsumoto, 2003), see Section 3. The significant effects of
edge flipping were observed with the other two schemes as well.
667
w1
w2
w3
(a)
w1
w3
w2
(b)
w4
w3
w2
(c)
Figure 3: A dependency structure on the words
w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an
edge-flip of w2?w3, and when the direction of the edge
between w2 and w3 is switched and the new parent of w3
is set to be some other word, w4 (Figure 3(c)).
Undirected Evaluation. The measure is defined
as follows: traverse over the tokens and mark a cor-
rect attachment if the token?s induced parent is either
(1) its gold parent or (2) its gold child. The score is
the ratio of correct attachments and the number of
tokens.
We show that this measure does not ignore edge-
flips. Consider Figure 3 that shows a depen-
dency structure on the words w1, w2, w3 before (Fig-
ure 3(a)) and after (Figure 3(b)) an edge-flip of
w2?w3. Assume that 3(a) is the gold standard and
that 3(b) is the induced parse. Consider w2. Its
induced parent (w3) is its gold child, and thus undi-
rected evaluation does not consider it an error. On
the other hand, w3 is assigned w2?s gold parent, w1.
This is considered an error, since w1 is neither w3?s
gold parent (as it is w2), nor its gold child9. There-
fore, one of the two tokens involved in the edge-flip
is penalized by the measure.
Recall the example ?I want to eat? and the edge-
flip of the edge ?to???eat? (Figure 2). As ?to??s
parent in the induced graph (?want?) is neither its
gold parent nor its gold child, the undirected evalu-
ation measure marks it as an error. This is an exam-
ple where an edge-flip in a problematic edge, which
should not be considered an error, was in fact con-
sidered an error by undirected evaluation.
Neutral Edge Direction (NED). The NED measure
is a simple extension of the undirected evaluation
measure10. Unlike undirected evaluation, NED ig-
nores all errors directly resulting from an edge-flip.
9Otherwise, the gold parse would have contained a
w1?w2?w3?w1 cycle.
10An implementation of NED is available at
http://www.cs.huji.ac.il/?roys02/software/ned.html
NED is defined as follows: traverse over the to-
kens and mark a correct attachment if the token?s in-
duced parent is either (1) its gold parent (2) its gold
child or (3) its gold grandparent. The score is the ra-
tio of correct attachments and the number of tokens.
NED, by its definition, ignores edge-flips. Con-
sider again Figure 3, where we assume that 3(a) is
the gold standard and that 3(b) is the induced parse.
Much like undirected evaluation, NED will mark the
attachment of w2 as correct, since its induced parent
is its gold child. However, unlike undirected evalua-
tion, w3?s induced attachment will also be marked as
correct, as its induced parent is its gold grandparent.
Now consider another induced parse in which the
direction of the edge between w2 and w3 is switched
and the w3?s parent is set to be some other word,
w4 (Figure 3(c)). This should be marked as an er-
ror, even if the direction of the edge between w2 and
w3 is controversial, since the structure [w2, w3] is no
longer a dependent of w1. It is indeed a NED error.
Note that undirected evaluation gives the parses in
Figure 3(b) and Figure 3(c) the same score, while if
the structure [w2, w3] is problematic, there is a major
difference in their correctness.
Discussion. Problematic structures are ubiquitous,
with more than 40% of the tokens in PTB WSJ
appearing in at least one of them (see Section 3).
Therefore, even a substantial difference in the at-
tachment between two parsers is not necessarily in-
dicative of a true quality difference. However, an at-
tachment score difference that persists under NED is
an indication of a true quality difference, since gen-
erally problematic structures are local (i.e., obtained
by an edge-flip) and NED ignores such errors.
Reporting NED alone is insufficient, as obviously
the edge direction does matter in some cases. For
example, in adjective?noun structures (e.g., ?big
house?), the correct edge direction is widely agreed
upon (?big???house?) (Ku?bler et al, 2009), and
thus choosing the wrong direction should be con-
sidered an error. Therefore, we suggest evaluating
using both NED and attachment score in order to get
a full picture of the parser?s performance.
A possible criticism on NED is that it is only in-
different to alternative annotations in structures of
size 2 (e.g., ?to eat?) and does not necessarily handle
larger problematic structures, such as coordinations
668
ROOT
John
and Mary
(a)
ROOT
John
and
Mary
(b)
ROOT
in
house
the
(c)
ROOT
in
the
house
(d)
ROOT
house
in
the
(e)
Figure 4: Alternative parses of ?John and Mary? and ?in
the house?. Figure 4(a) follows (Collins, 1999), Fig-
ure 4(b) follows (Johansson and Nugues, 2007). Fig-
ure 4(c) follows (Collins, 1999; Yamada and Matsumoto,
2003). Figure 4(d) and Figure 4(e) show induced parses
made by (km04,saj10a) and cs09, respectively.
(see Section 4). For example, Figure 4(a) and Fig-
ure 4(b) present two alternative annotations of the
sentence ?John and Mary?. Assume the parse in Fig-
ure 4(a) is the gold parse and that in Figure 4(b) is
the induced parse. The word ?Mary? is a NED error,
since its induced parent (?and?) is neither its gold
child nor its gold grandparent. Thus, NED does not
accept all possible annotations of structures of size
3. On the other hand, using a method which accepts
all possible annotations of structures of size 3 seems
too permissive. A better solution may be to modify
the gold standard annotation, so to explicitly anno-
tate problematic structures as such. We defer this
line of research to future work.
NED is therefore an evaluation measure which is
indifferent to edge-flips, and is consequently less
sensitive to alternative annotations. We now show
that NED is indifferent to the differences between the
structures originally learned by the algorithms men-
tioned in Section 3 and the gold standard annotation
in all the problematic cases we consider.
Most of the modifications made are edge-flips,
and are therefore ignored by NED. The exceptions
are coordinations and prepositional phrases which
are structures of size 3. In the former, the alter-
native annotations differ only in a single edge-flip
(i.e., CC?NNP ), and are thus not NED errors. Re-
garding prepositional phrases, Figure 4(c) presents
the gold standard of ?in the house?, Figure 4(d) the
parse induced by km04 and saj10a and Figure 4(e)
the parse induced by cs09. As the reader can verify,
both induced parses receive a perfect NED score.
In order to further demonstrate NED?s insensitiv-
ity to alternative annotations, we took two of the
three common gold standard annotations (see Sec-
tion 4) and evaluated them one against the other. We
considered section 23 of WSJ following the scheme
of (Yamada and Matsumoto, 2003) as the gold stan-
dard and of (Collins, 1999) as the evaluated set. Re-
sults show that the attachment score is only 85.6%,
the undirected accuracy is improved to 90.3%, while
the NED score is 95.3%. This shows that NED is sig-
nificantly less sensitive to the differences between
the different annotation schemes, compared to the
other evaluation measures.
6 Experimenting with NED
In this section we show that NED indeed reduces
the performance difference between the original and
the modified parameter sets, thus providing empiri-
cal evidence for its validity. For brevity, we present
results only for the entire WSJ corpus. Results on
WSJ10 are similar. The datasets and decoding algo-
rithms are the same as those used in Section 3.
Table 3 shows the score differences between the
parameter sets using attachment score, undirected
evaluation and NED. A substantial difference per-
sists under undirected evaluation: a gap of 7.7% in
cs09, of 3.5% in saj10a and of 1.3% in km04.
The differences are further reduced using NED.
This is consistent with our discussion in Section 5,
and shows that undirected evaluation only ignores
some of the errors inflicted by edge-flips.
For cs09, the difference is substantially reduced,
but a 4.2% performance gap remains. For km04 and
saj10a, the original parameters outperform the new
ones by 3.6% and 1% respectively.
We can see that even when ignoring edge-flips,
some difference remains, albeit not necessarily in
the favor of the modified models. This is because
we did not directly perform edge-flips, but rather
parameter-flips. The difference is thus a result of
second-order effects stemming from the parameter-
flips. In the next section, we explain why the remain-
ing difference is positive for some algorithms (cs09)
and negative for others (km04, saj10a).
For completeness, Table 4 shows a comparison of
some of the current state-of-the-art algorithms, using
attachment score, undirected evaluation and NED.
The training and test sets are those used in Section 3.
The table shows that the relative orderings of the al-
gorithms under NED is different than under the other
669
Algo. Mod. ? Orig.Attach. Undir. NED
km04 9.3 (43.9?34.6) 1.3 (54.2?52.9) ?3.6 (63?66.6)
cs09 14.7 (54.6?39.9) 7.7 (56.9?49.2) 4.2 (66.8?62.6)
saj10a 12.7 (54.3?41.6) 3.5 (59.4?55.9) ?1 (66.8?67.8)
Table 3: Differences between the modified and original
parameter sets when evaluated using attachment score
(Attach.), undirected evaluation (Undir.), and NED.
measures. This is an indication that NED provides a
different perspective on algorithm quality11 .
Algo. Att10 Att? Un10 Un? NED10 NED?
bbdk10 66.1 49.6 70.1 56.0 75.5 61.8
bc10 67.2 53.6 73 61.7 81.6 70.2
cs09 61.5 42 66.9 50.4 81.5 62.9
gggtp10 57.1 45 62.5 53.2 80.4 65.1
km04 45.8 34.6 60.3 52.9 78.4 66.6
saj10a 54.7 41.6 66.5 55.9 78.9 67.8
saj10c 63.8 46.1 72.6 58.8 84.2 70.8
saj10b? 67.9 48.2 74.0 57.7 86.0 70.7
Table 4: A comparison of recent works, using Att (at-
tachment score) Un (undirected evaluation) and NED, on
sentences of length ? 10 (excluding punctuation) and
on all sentences. The gold standard is obtained using
the rules of (Yamada and Matsumoto, 2003). bbdk10:
(Berg-Kirkpatrick et al, 2010), bc10: (Blunsom and
Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10:
(Gillenwater et al, 2010), km04: A replication of (Klein
and Manning, 2004), saj10a: (Spitkovsky et al, 2010a),
saj10c: (Spitkovsky et al, 2010c), saj10b?: A lightly-
supervised algorithm (Spitkovsky et al, 2010b).
7 Discussion
In this paper we explored two ways of dealing with
cases in which there is no clear theoretical justifi-
cation to prefer one dependency structure over an-
other. Our experiments suggest that it is crucial to
deal with such structures if we would like to have
a proper evaluation of unsupervised parsing algo-
rithms against a gold standard.
The first way was to modify the parameters of the
parsing algorithms so that in cases where such prob-
lematic decisions are to be made they follow the gold
standard annotation. Indeed, this modification leads
to a substantial improvement in the attachment score
of the algorithms.
11Results may be different than the ones published in the
original papers due to the different conversion procedures used
in each work. See Section 4 for discussion.
The second way was to change the evaluation.
The NED measure we proposed does not punish for
differences between gold and induced structures in
the problematic cases. Indeed, in Section 6 (Table 3)
we show that the differences between the original
and modified models are much smaller when eval-
uating with NED compared to when evaluating with
the traditional attachment score.
As Table 3 reveals, however, even when evaluat-
ing with NED, there is still some difference between
the original and the modified model, for each of the
algorithms we consider. Moreover, for two of the al-
gorithms (km04 and saj10a) NED prefers the original
model while for one (cs09) it prefers the modified
version. In this section we explain these patterns and
show that they are both consistent and predictable.
Our hypothesis, for which we provide empirical
justification, is that in cases where there is no theo-
retically preferred annotation, NED prefers the struc-
tures that are more learnable by DMV. That is, NED
gives higher scores to the annotations that better fit
the assumptions and modeling decisions of DMV,
the model that lies in the basis of the parsing algo-
rithms.
To support our hypothesis we perform an experi-
ment requiring two preparatory steps for each algo-
rithm. First, we construct a supervised version of
the algorithm. This supervised version consists of
the same statistical model as the original unsuper-
vised algorithm, but the parameters are estimated to
maximize the likelihood of a syntactically annotated
training corpus, rather than of a plain text corpus.
Second, we construct two corpora for the algo-
rithm, both consist of the same text and differ only
in their syntactic annotation. The first is annotated
with the gold standard annotation. The second is
similarly annotated except in the linguistically prob-
lematic structures. We replace these structures with
the ones that would have been created with the un-
supervised version of the algorithm (see Table 1 for
the relevant structures for each algorithm)12. Each
12In cases the structures are comprised of a single edge, the
second corpus is obtained from the gold standard by an edge-
flip. The only exceptions are the cases of the prepositional
phrases. Their gold standard and the learned structures for each
of the algorithms are shown in Figure 4. In this case, the sec-
ond corpus is obtained from the gold standard by replacing each
prepositional phrase in the gold standard with the corresponding
670
corpus is divided into a training and a test set.
We then train the supervised version of the algo-
rithms on each of the training sets. We parse the test
data twice, once with each of the resulting models.
We evaluate both parsed corpora against the corpus
annotation from which they originated.
The training set of each corpus consists of sec-
tions 2?21 of WSJ20 (i.e., WSJ sentences of length
?20, excluding punctuation)13 and the test set is sec-
tion 23 of WSJ?. Evaluation is performed using
both NED and attachment score. The patterns we
observed are very similar for both. For brevity, we
report only attachment score results.
km04 cs09 saj10a
Orig. Gold Orig. Gold Orig. Gold
NED,
Unsup. 66.6 63 62.6 66.8 67.8 66.8
Sup. 71.3 69.9 63.3 69.9 71.8 69.9
Table 5: The first line shows the NED results from
Section 6, when using the original parameters (Orig.
columns) and the modified parameters (Gold columns).
The second line shows the results of the supervised ver-
sions of the algorithms using the corpus which agrees
with the unsupervised model in the problematic cases
(Orig.) and the gold standard (Gold).
The results of our experiment are presented in Ta-
ble 5 along with a comparison to the NED scores
from Section 6. The table clearly demonstrates that a
set of parameters (original or modified) is preferred
by NED in the unsupervised experiments reported in
Section 6 (top line) if and only if the structures pro-
duced by this set are better learned by the supervised
version of the algorithm (bottom line).
This observation supports our hypothesis that in
cases where there is no theoretical preference for
one structure over the other, NED (unlike the other
measures) prefers the structures that are more con-
sistent with the modeling assumptions lying in the
basis of the algorithm. We consider this to be a de-
sired property of a measure since a more consistent
model should be preferred where no theoretical pref-
erence exists.
learned structure.
13In using WSJ20, we follow (Spitkovsky et al, 2010a),
which showed that training the DMV on sentences of bounded
length yields a higher score than using the entire corpus. We
use it as we aim to use an optimal setting.
8 Conclusion
In this paper we showed that the standard evalua-
tion of unsupervised dependency parsers is highly
sensitive to problematic annotations. We modified a
small set of parameters that controls the annotation
in such problematic cases in three leading parsers.
This resulted in a major performance boost, which
is unindicative of a true difference in quality.
We presented Neutral Edge Direction (NED), a
measure that is less sensitive to the annotation of
local structures. As the problematic structures are
generally local, NED is less sensitive to their alterna-
tive annotations. In the future, we suggest reporting
NED along with the current measures.
Acknowledgements. We would like to thank Shay
Cohen for his assistance with his implementation of
the DMV parser and Taylor Berg-Kirkpatrick, Phil
Blunsom and Jennifer Gillenwater for providing us
with their data sets. We would also like to thank
Valentin I. Spitkovsky for his comments and for pro-
viding us with his data sets.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero and Dan Klein, 2010. Painless unsu-
pervised learning with features. In Proc. of NAACL.
Taylor Berg-Kirkpatrick and Dan Klein, 2010. Phyloge-
netic Grammar Induction. In Proc. of ACL.
Cristina Bosco and Vincenzo Lombardo, 2004. Depen-
dency and relational structure in treebank annotation.
In Proc. of the Workshop on Recent Advances in De-
pendency Grammar at COLING?04.
Phil Blunsom and Trevor Cohn, 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP.
Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008.
Logistic Normal Priors for Unsupervised Probabilistic
Grammar Induction. In Proc. of NIPS.
Shay B. Cohen and Noah A. Smith, 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying. In
Proc. of HLT-NAACL.
Michael J. Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Hal Daume? III, 2009. Unsupervised search-based struc-
tured prediction. In Proc. of ICML.
671
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o V. Grac?a and Fernando Pereira,
2007. Frustratingly Hard Domain Adaptation for De-
pendency Parsing. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
Gregory Druck, Gideon Mann and Andrew McCal-
lum, 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In
Proc. of ACL.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o V. Grac?a,
Ben Taskar and Fernando Preira, 2010. Sparsity in
dependency grammar induction. In Proc. of ACL.
William P. Headden III, David McClosky, and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. In Proc. of
COLING.
William P. Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Proc.
of HLT-NAACL.
Richard Johansson and Pierre Nugues, 2007. Ex-
tended Constituent-to-Dependency Conversion for En-
glish. In Proc. of NODALIDA.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL.
Sandra Ku?bler, 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proc. of RANLP.
Sandra Ku?bler, R. McDonald and Joakim Nivre, 2009.
Dependency Parsing. Morgan And Claypool Publish-
ers.
Mitchell Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics 19:313-330.
Tahira Naseem, Harr Chen, Regina Barzilay and Mark
Johnson, 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. of EMNLP.
Joakim Nivre, 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre, Johan Hall and Jens Nilsson, 2006. Malt-
Parser: A data-driven parser-generator for depen-
dency parsing. In Proc. of LREC-2006.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel and Deniz Yuret,
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of the CoNLL Shared Task, EMNLP-
CoNLL, 2007.
Jens Nilsson, Joakim Nivre and Johan Hall, 2006. Graph
transformations in data-driven dependency parsing.
In Proc. of ACL.
Owen Rambow, Cassandre Creswell, Rachel Szekely,
Harriet Tauber and Marilyn Walker, 2002. A depen-
dency treebank for English. In Proc. of LREC.
Noah A. Smith and Jason Eisner, 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proc. of IJCAI.
Noah A. Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induc-
tion. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010a. From Baby Steps to Leapfrog: How ?Less
is More? in Unsupervised Dependency Parsing. In
Proc. of NAACL-HLT.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010b. Profiting from Mark-Up: Hyper-Text An-
notations for Guided Parsing. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky
and Christopher D. Manning, 2010c. Viterbi training
improves unsupervised dependency parsing. In Proc.
of CoNLL.
Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005.
Strictly Lexical Dependency Parsing. In IWPT.
Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schu-
urmans, 2006. Improved Large Margin Dependency
Parsing via Local Constraints and Laplacian Regular-
ization. In Proc. of CoNLL.
Hiroyasu Yamada and Yuji Matsumoto, 2003. Statistical
dependency analysis with support vector machines. In
Proc. of the International Workshop on Parsing Tech-
nologies.
672
