Acquiring the Meaning of Discourse Markers
Ben Hutchinson
School of Informatics
University of Edinburgh
B.Hutchinson@sms.ed.ac.uk
Abstract
This paper applies machine learning techniques to
acquiring aspects of the meaning of discourse mark-
ers. Three subtasks of acquiring the meaning of a
discourse marker are considered: learning its polar-
ity, veridicality, and type (i.e. causal, temporal or
additive). Accuracy of over 90% is achieved for all
three tasks, well above the baselines.
1 Introduction
This paper is concerned with automatically acquir-
ing the meaning of discourse markers. By con-
sidering the distributions of individual tokens of
discourse markers, we classify discourse markers
along three dimensions upon which there is substan-
tial agreement in the literature: polarity, veridical-
ity and type. This approach of classifying linguistic
types by the distribution of linguistic tokens makes
this research similar in spirit to that of Baldwin and
Bond (2003) and Stevenson and Merlo (1999).
Discourse markers signal relations between dis-
course units. As such, discourse markers play an
important role in the parsing of natural language
discourse (Forbes et al, 2001; Marcu, 2000), and
their correspondence with discourse relations can
be exploited for the unsupervised learning of dis-
course relations (Marcu and Echihabi, 2002). In
addition, generating natural language discourse re-
quires the appropriate selection and placement of
discourse markers (Moser and Moore, 1995; Grote
and Stede, 1998). It follows that a detailed account
of the semantics and pragmatics of discourse mark-
ers would be a useful resource for natural language
processing.
Rather than looking at the finer subtleties in
meaning of particular discourse markers (e.g. Best-
gen et al (2003)), this paper aims at a broad scale
classification of a subclass of discourse markers:
structural connectives. This breadth of coverage
is of particular importance for discourse parsing,
where a wide range of linguistic realisations must be
catered for. This work can be seen as orthogonal to
that of Di Eugenio et al (1997), which addresses the
problem of learning if and where discourse markers
should be generated.
Unfortunately, the manual classification of large
numbers of discourse markers has proven to be a
difficult task, and no complete classification yet ex-
ists. For example, Knott (1996) presents a list of
around 350 discourse markers, but his taxonomic
classification, perhaps the largest classification in
the literature, accounts for only around 150 of these.
A general method of automatically classifying dis-
course markers would therefore be of great utility,
both for English and for languages with fewer man-
ually created resources. This paper constitutes a
step in that direction. It attempts to classify dis-
course markers whose classes are already known,
and this allows the classifier to be evaluated empiri-
cally.
The proposed task of learning automatically the
meaning of discourse markers raises several ques-
tions which we hope to answer:
Q1. Difficulty How hard is it to acquire the mean-
ing of discourse markers? Are some aspects of
meaning harder to acquire than others?
Q2. Choice of features What features are useful
for acquiring the meaning of discourse mark-
ers? Does the optimal choice of features de-
pend on the aspect of meaning being learnt?
Q3. Classifiers Which machine learning algo-
rithms work best for this task? Can the right
choice of empirical features make the classifi-
cation problems linearly separable?
Q4. Evidence Can corpus evidence be found for
the existing classifications of discourse mark-
ers? Is there empirical evidence for a separate
class of TEMPORAL markers?
We proceed by first introducing the classes of dis-
course markers that we use in our experiments. Sec-
tion 3 discusses the database of discourse markers
used as our corpus. In Section 4 we describe our ex-
periments, including choice of features. The results
are presented in Section 5. Finally, we conclude and
discuss future work in Section 6.
2 Discourse markers
Discourse markers are lexical items (possibly multi-
word) that signal relations between propositions,
events or speech acts. Examples of discourse mark-
ers are given in Tables 1, 2 and 3. In this paper
we will focus on a subclass of discourse markers
known as structural connectives. These markers,
even though they may be multiword expressions,
function syntactically as if they were coordinating
or subordinating conjunctions (Webber et al, 2003).
The literature contains many different classi-
fications of discourse markers, drawing upon a
wide range of evidence including textual co-
hesion (Halliday and Hasan, 1976), hypotactic
conjunctions (Martin, 1992), cognitive plausibil-
ity (Sanders et al, 1992), substitutability (Knott,
1996), and psycholinguistic experiments (Louw-
erse, 2001). Nevertheless there is also considerable
agreement. Three dimensions of classification that
recur, albeit under a variety of names, are polarity,
veridicality and type. We now discuss each of these
in turn.
2.1 Polarity
Many discourse markers signal a concession, a con-
trast or the denial of an expectation. These mark-
ers have been described as having the feature polar-
ity=NEG-POL. An example is given in (1).
(1) Suzy?s part-time, but she does more work
than the rest of us put together. (Taken from
Knott (1996, p. 185))
This sentence is true if and only if Suzy both is part-
time and does more work than the rest of them put
together. In addition, it has the additional effect of
signalling that the fact Suzy does more work is sur-
prising ? it denies an expectation. A similar effect
can be obtained by using the connective and and
adding more context, as in (2)
(2) Suzy?s efficiency is astounding. She?s
part-time, and she does more work than the
rest of us put together.
The difference is that although it is possible for
and to co-occur with a negative polarity discourse
relation, it need not. Discourse markers like and are
said to have the feature polarity=POS-POL. 1 On
1An alternative view is that discourse markers like and are
underspecified with respect to polarity (Knott, 1996). In this
the other hand, a NEG-POL discourse marker like
but always co-occurs with a negative polarity dis-
course relation.
The gold standard classes of POS-POL and NEG-
POL discourse markers used in the learning exper-
iments are shown in Table 1. The gold standards
for all three experiments were compiled by consult-
ing a range of previous classifications (Knott, 1996;
Knott and Dale, 1994; Louwerse, 2001). 2
POS-POL NEG-POL
after, and, as, as soon as,
because, before, considering
that, ever since, for, given that,
if, in case, in order that, in that,
insofar as, now, now that, on
the grounds that, once, seeing
as, since, so, so that, the in-
stant, the moment, then, to the
extent that, when, whenever
although,
but, even if,
even though,
even when,
only if, only
when, or, or
else, though,
unless, until,
whereas, yet
Table 1: Discourse markers used in the polarity ex-
periment
2.2 Veridicality
A discourse relation is veridical if it implies the
truth of both its arguments (Asher and Lascarides,
2003), otherwise it is not. For example, in (3) it is
not necessarily true either that David can stay up or
that he promises, or will promise, to be quiet. For
this reason we will say if has the feature veridical-
ity=NON-VERIDICAL.
(3) David can stay up if he promises to be quiet.
The disjunctive discourse marker or is also NON-
VERIDICAL, because it does not imply that both
of its arguments are true. On the other hand, and
does imply this, and so has the feature veridical-
ity=VERIDICAL.
The VERIDICAL and NON-VERIDICAL discourse
markers used in the learning experiments are shown
in Table 2. Note that the polarity and veridicality
are independent, for example even if is both NEG-
POL and NON-VERIDICAL.
2.3 Type
Discourse markers like because signal a CAUSAL
relation, for example in (4).
account, discourse markers have positive polarity only if they
can never be paraphrased using a discourse marker with nega-
tive polarity. Interpreted in these terms, our experiment aims to
distinguish negative polarity discourse markers from all others.
2An effort was made to exclude discourse markers whose
classification could be contentious, as well as ones which
showed ambiguity across classes. Some level of judgement was
therefore exercised by the author.
VERIDICAL NON-
VERIDICAL
after, although, and, as, as soon
as, because, but, considering
that, even though, even when,
ever since, for, given that, in or-
der that, in that, insofar as, now,
now that, on the grounds that,
once, only when, seeing as,
since, so, so that, the instant,
the moment, then, though, to
the extent that, until, when,
whenever, whereas, while, yet
assuming
that, even if,
if, if ever, if
only, in case,
on condition
that, on the
assumption
that, only if,
or, or else,
supposing
that, unless
Table 2: Discourse markers used in the veridicality
experiment
(4) The tension in the boardroom rose sharply
because the chairman arrived.
As a result, because has the feature
type=CAUSAL. Other discourse markers that
express a temporal relation, such as after, have
the feature type=TEMPORAL. Just as a POS-POL
discourse marker can occur with a negative polarity
discourse relation, the context can also supply a
causal relation even when a TEMPORAL discourse
marker is used, as in (5).
(5) The tension in the boardroom rose sharply
after the chairman arrived.
If the relation a discourse marker signals is nei-
ther CAUSAL or TEMPORAL it has the feature
type=ADDITIVE.
The need for a distinct class of TEMPORAL dis-
course relations is disputed in the literature. On
the one hand, it has been suggested that TEMPO-
RAL relations are a subclass of ADDITIVE ones on
the grounds that the temporal reference inherent
in the marking of tense and aspect ?more or less?
fixes the temporal ordering of events (Sanders et al,
1992). This contrasts with arguments that resolv-
ing discourse relations and temporal order occur as
distinct but inter-related processes (Lascarides and
Asher, 1993). On the other hand, several of the dis-
course markers we count as TEMPORAL, such as as
soon as, might be described as CAUSAL (Oberlan-
der and Knott, 1995). One of the results of the ex-
periments described below is that corpus evidence
suggests ADDITIVE, TEMPORAL and CAUSAL dis-
course markers have distinct distributions.
The ADDITIVE, TEMPORAL and CAUSAL dis-
course markers used in the learning experiments are
shown in Table 3. These features are independent
of the previous ones, for example even though is
CAUSAL, VERIDICAL and NEG-POL.
ADDITIVE TEMPORAL CAUSAL
and, but,
whereas
after, as
soon as,
before,
ever
since,
now, now
that, once,
until,
when,
whenever
although, because,
even though, for, given
that, if, if ever, in case,
on condition that, on
the assumption that,
on the grounds that,
provided that, provid-
ing that, so, so that,
supposing that, though,
unless
Table 3: Discourse markers used in the type exper-
iment
3 Corpus
The data for the experiments comes from a
database of sentences collected automatically from
the British National Corpus and the world wide
web (Hutchinson, 2004). The database contains ex-
ample sentences for each of 140 discourse structural
connectives.
Many discourse markers have surface forms with
other usages, e.g. before in the phrase before noon.
The following procedure was therefore used to se-
lect sentences for inclusion in the database. First,
sentences containing a string matching the sur-
face form of a structural connective were extracted.
These sentences were then parsed using a statistical
parser (Charniak, 2000). Potential structural con-
nectives were then classified on the basis of their
syntactic context, in particular their proximity to S
nodes. Figure 1 shows example syntactic contexts
which were used to identify discourse markers.
(S ...) (CC and) (S...)
(SBAR (IN after) (S...))
(PP (IN after) (S...))
(PP (VBN given) (SBAR (IN that) (S...)))
(NP (DT the) (NN moment) (SBAR...))
(ADVP (RB as) (RB long)
(SBAR (IN as) (S...)))
(PP (IN in) (SBAR (IN that) (S...)))
Figure 1: Identifying structural connectives
It is because structural connectives are easy to
identify in this manner that the experiments use only
this subclass of discourse markers. Due to both
parser errors, and the fact that the syntactic heuris-
tics are not foolproof, the database contains noise.
Manual analysis of a sample of 500 sentences re-
vealed about 12% of sentences do not contain the
discourse marker they are supposed to.
Of the discourse markers used in the experiments,
their frequencies in the database ranged from 270
for the instant to 331,701 for and. The mean num-
ber of instances was 32,770, while the median was
4,948.
4 Experiments
This section presents three machine learning ex-
periments into automatically classifying discourse
markers according to their polarity, veridicality
and type. We begin in Section 4.1 by describing
the features we extract for each discourse marker
token. Then in Section 4.2 we describe the differ-
ent classifiers we use. The results are presented in
Section 4.3.
4.1 Features used
We only used structural connectives in the experi-
ments. This meant that the clauses linked syntacti-
cally were also related at the discourse level (Web-
ber et al, 2003). Two types of features were ex-
tracted from the conjoined clauses. Firstly, we used
lexical co-occurrences with words of various parts
of speech. Secondly, we used a range of linguisti-
cally motivated syntactic, semantic, and discourse
features.
4.1.1 Lexical co-occurrences
Lexical co-occurrences have previously been shown
to be useful for discourse level learning tasks (La-
pata and Lascarides, 2004; Marcu and Echihabi,
2002). For each discourse marker, the words occur-
ring in their superordinate (main) and subordinate
clauses were recorded,3 along with their parts of
speech. We manually clustered the Penn Treebank
parts of speech together to obtain coarser grained
syntactic categories, as shown in Table 4.
We then lemmatised each word and excluded all
lemmas with a frequency of less than 1000 per mil-
lion in the BNC. Finally, words were attached a pre-
fix of either SUB or SUPER according to whether
they occurred in the sub- or superordinate clause
linked by the marker. This distinguished, for exam-
ple, between occurrences of then in the antecedent
(subordinate) and consequent (main) clauses linked
by if.
We also recorded the presence of other discourse
markers in the two clauses, as these had previously
3For coordinating conjunctions, the left clause was taken to
be superordinate/main clause, the right, the subordinate clause.
New label Penn Treebank labels
vb vb vbd vbg vbn vbp vbz
nn nn nns nnp
jj jj jjr jjs
rb rb rbr rbs
aux aux auxg md
prp prp prp$
in in
Table 4: Clustering of POS labels
been found to be useful on a related classification
task (Hutchinson, 2003). The discourse markers
used for this are based on the list of 350 markers
given by Knott (1996), and include multiword ex-
pressions. Due to the sparser nature of discourse
markers, compared to verbs for example, no fre-
quency cutoffs were used.
4.1.2 Linguistically motivated features
These included a range of one and two dimensional
features representing more abstract linguistic infor-
mation, and were extracted through automatic anal-
ysis of the parse trees.
One dimensional features
Two one dimensional features recorded the location
of discourse markers. POSITION indicated whether
a discourse marker occurred between the clauses it
linked, or before both of them. It thus relates to
information structuring. EMBEDDING indicated the
level of embedding, in number of clauses, of the dis-
course marker beneath the sentence?s highest level
clause. We were interested to see if some types of
discourse relations are more often deeply embed-
ded.
The remaining features recorded the presence of
linguistic features that are localised to a particu-
lar clause. Like the lexical co-occurrence features,
these were indexed by the clause they occurred in:
either SUPER or SUB.
We expected negation to correlate with nega-
tive polarity discourse markers, and approximated
negation using four features. NEG-SUBJ and NEG-
VERB indicated the presence of subject negation
(e.g. nothing) or verbal negation (e.g. n?t). We also
recorded the occurrence of a set of negative polar-
ity items (NPI), such as any and ever. The features
NPI-AND-NEG and NPI-WO-NEG indicated whether
an NPI occurred in a clause with or without verbal
or subject negation.
Eventualities can be placed or ordered in time us-
ing not just discourse markers but also temporal ex-
pressions. The feature TEMPEX recorded the num-
ber of temporal expressions in each clause, as re-
turned by a temporal expression tagger (Mani and
Wilson, 2000).
If the main verb was an inflection of to be or to do
we recorded this using the features BE and DO. Our
motivation was to capture any correlation of these
verbs with states and events respectively.
If the final verb was a modal auxiliary, this el-
lipsis was evidence of strong cohesion in the text
(Halliday and Hasan, 1976). We recorded this with
the feature VP-ELLIPSIS. Pronouns also indicate co-
hesion, and have been shown to correlate with sub-
jectivity (Bestgen et al, 2003). A class of features
PRONOUNS
 
represented pronouns, with  denot-
ing either 1st person, 2nd person, or 3rd person ani-
mate, inanimate or plural.
The syntactic structure of each clause was cap-
tured using two features, one finer grained and one
coarser grained. STRUCTURAL-SKELETON identi-
fied the major constituents under the S or VP nodes,
e.g. a simple double object construction gives ?NP
VB NP NP?. ARGS identified whether the clause
contained an (overt) object, an (overt) subject, or
both, or neither.
The overall size of a clause was represented us-
ing four features. WORDS, NPS and PPS recorded
the numbers of words, NPs and PPs in a clause (not
counting embedded clauses). The feature CLAUSES
counted the number of clauses embedded beneath a
clause.
Two dimensional features
These features all recorded combinations of linguis-
tic features across the two clauses linked by the
discourse marker. For example the MOOD feature
would take the value  DECL,IMP  for the sentence
John is coming, but don?t tell anyone!
These features were all determined automatically
by analysing the auxiliary verbs and the main verbs?
POS tags. The features and the possible values for
each clause were as follows: MODALITY: one of
FUTURE, ABILITY or NULL; MOOD: one of DECL,
IMP or INTERR; PERFECT: either YES or NO; PRO-
GRESSIVE: either YES or NO; TENSE: either PAST
or PRESENT.
4.2 Classifier architectures
Two different classifiers, based on local and global
methods of comparison, were used in the experi-
ments. The first, 1 Nearest Neighbour (1NN), is an
instance based classifier which assigns each marker
to the same class as that of the marker nearest to
it. For this, three different distance metrics were
explored. The first metric was the Euclidean dis-
tance function  , shown in (6), applied to proba-
bility distributions.
	
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 149?156,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Modelling the substitutability of discourse connectives
Ben Hutchinson
School of Informatics
University of Edinburgh
B.Hutchinson@sms.ed.ac.uk
Abstract
Processing discourse connectives is im-
portant for tasks such as discourse parsing
and generation. For these tasks, it is use-
ful to know which connectives can signal
the same coherence relations. This paper
presents experiments into modelling the
substitutability of discourse connectives.
It shows that substitutability effects dis-
tributional similarity. A novel variance-
based function for comparing probability
distributions is found to assist in predict-
ing substitutability.
1 Introduction
Discourse coherence relations contribute to the
meaning of texts, by specifying the relationships be-
tween semantic objects such as events and propo-
sitions. They also assist in the interpretation of
anaphora, verb phrase ellipsis and lexical ambigu-
ities (Hobbs, 1985; Kehler, 2002; Asher and Las-
carides, 2003). Coherence relations can be implicit,
or they can be signalled explicitly through the use of
discourse connectives, e.g. because, even though.
For a machine to interpret a text, it is impor-
tant that it recognises coherence relations, and so as
explicit markers discourse connectives are of great
assistance (Marcu, 2000). When discourse con-
nectives are not present, the task is more difficult.
For such cases, unsupervised approaches have been
developed for predicting relations, by using sen-
tences containing discourse connectives as training
data (Marcu and Echihabi, 2002; Lapata and Las-
carides, 2004). However the nature of the relation-
ship between the coherence relations signalled by
discourse connectives and their empirical distribu-
tions has to date been poorly understood. In par-
ticular, one might wonder whether connectives with
similar meanings also have similar distributions.
Concerning natural language generation, texts are
easier for humans to understand if they are coher-
ently structured. Addressing this, a body of research
has considered the problems of generating appropri-
ate discourse connectives (for example (Moser and
Moore, 1995; Grote and Stede, 1998)). One such
problem involves choosing which connective to gen-
erate, as the mapping between connectives and re-
lations is not one-to-one, but rather many-to-many.
Siddharthan (2003) considers the task of paraphras-
ing a text while preserving its rhetorical relations.
Clauses conjoined by but, or and when are sepa-
rated to form distinct orthographic sentences, and
these conjunctions are replaced by the discourse ad-
verbials however, otherwise and then, respectively.
The idea underlying Siddharthan?s work is that
one connective can be substituted for another while
preserving the meaning of a text. Knott (1996)
studies the substitutability of discourse connectives,
and proposes that substitutability can motivate the-
ories of discourse coherence. Knott uses an empiri-
cal methodology to determine the substitutability of
pairs of connectives. However this methodology is
manually intensive, and Knott derives relationships
for only about 18% of pairs of connectives. It would
thus be useful if substitutability could be predicted
automatically.
149
This paper proposes that substitutability can be
predicted through statistical analysis of the contexts
in which connectives appear. Similar methods have
been developed for predicting the similarity of nouns
and verbs on the basis of their distributional similar-
ity, and many distributional similarity functions have
been proposed for these tasks (Lee, 1999). However
substitutability is a more complex notion than simi-
larity, and we propose a novel variance-based func-
tion for assisting in this task.
This paper constitutes a first step towards predict-
ing substitutability of cnonectives automatically. We
demonstrate that the substitutability of connectives
has significant effects on both distributional similar-
ity and the new variance-based function. We then at-
tempt to predict substitutability of connectives using
a simplified task that factors out the prior likelihood
of being substitutable.
2 Relationships between connectives
Two types of relationships between connectives are
of interest: similarity and substitutability.
2.1 Similarity
The concept of lexical similarity occupies an impor-
tant role in psychology, artificial intelligence, and
computational linguistics. For example, in psychol-
ogy, Miller and Charles (1991) report that psycholo-
gists ?have largely abandoned ?synonymy? in favour
of ?semantic similarity?.? In addition, work in au-
tomatic lexical acquisition is based on the proposi-
tion that distributional similarity correlates with se-
mantic similarity (Grefenstette, 1994; Curran and
Moens, 2002; Weeds and Weir, 2003).
Several studies have found subjects? judge-
ments of semantic similarity to be robust. For
example, Miller and Charles (1991) elicit similar-
ity judgements for 30 pairs of nouns such as
cord?smile, and found a high correlation with
judgements of the same data obtained over 25
years previously (Rubenstein and Goodenough,
1965). Resnik (1999) repeated the experiment,
and calculated an inter-rater agreement of 0.90.
Resnik and Diab (2000) also performed a similar
experiment with pairs of verbs (e.g. bathe?kneel).
The level of inter-rater agreement was again signifi-
cant (r = 0.76).
1. Take an instance of a discourse connective
in a corpus. Imagine you are the writer
that produced this text, but that you need to
choose an alternative connective.
2. Remove the connective from the text, and
insert another connective in its place.
3. If the new connective achieves the same dis-
course goals as the original one, it is consid-
ered substitutable in this context.
Figure 1: Knott?s Test for Substitutability
Given two words, it has been suggested that if
words have the similar meanings, then they can be
expected to have similar contextual distributions.
The studies listed above have also found evidence
that similarity ratings correlate positively with the
distributional similarity of the lexical items.
2.2 Substitutability
The notion of substitutability has played an impor-
tant role in theories of lexical relations. A defini-
tion of synonymy attributed to Leibniz states that
two words are synonyms if one word can be used in
place of the other without affecting truth conditions.
Unlike similarity, the substitutability of dis-
course connectives has been previously studied.
Halliday and Hasan (1976) note that in certain con-
texts otherwise can be paraphrased by if not, as in
(1) It?s the way I like to go to work.
One person and one line of enquiry at a time.
Otherwise/if not, there?s a muddle.
They also suggest some other extended paraphrases
of otherwise, such as under other circumstances.
Knott (1996) systematises the study of the substi-
tutability of discourse connectives. His first step is
to propose a Test for Substitutability for connectives,
which is summarised in Figure 1. An application of
the Test is illustrated by (2). Here seeing as was
the connective originally used by the writer, how-
ever because can be used instead.
150
w1
w2
(a) w1 and w2 are
SYNONYMS
w1 w2
(b) w1 is a
HYPONYM of w2
w1
w2
(c) w1 and w2 are
CONTINGENTLY
SUBSTITUTABLE
w1
w2
(d) w1 and w2 are
EXCLUSIVE
Figure 2: Venn diagrams representing relationships between distributions
(2) Seeing as/because we?ve got nothing but
circumstantial evidence, it?s going to be
difficult to get a conviction. (Knott, p. 177)
However the ability to substitute is sensitive to the
context. In other contexts, for example (3), the sub-
stitution of because for seeing as is not valid.
(3) It?s a fairly good piece of work, seeing
as/#because you have been under a lot of
pressure recently. (Knott, p. 177)
Similarly, there are contexts in which because can
be used, but seeing as cannot be substituted for it:
(4) That proposal is useful, because/#seeing as it
gives us a fallback position if the negotiations
collapse. (Knott, p. 177)
Knott?s next step is to generalise over all contexts
a connective appears in, and to define four substi-
tutability relationships that can hold between a pair
of connectives w1 and w2. These relationships are
illustrated graphically through the use of Venn dia-
grams in Figure 2, and defined below.
? w1 is a SYNONYM of w2 if w1 can always be
substituted for w2, and vice versa.
? w1 and w2 are EXCLUSIVE if neither can ever
be substituted for the other.
? w1 is a HYPONYM of w2 if w2 can always be
substituted for w1, but not vice versa.
? w1 and w2 are CONTINGENTLY SUBSTI-
TUTABLE if each can sometimes, but not al-
ways, be substituted for the other.
Given examples (2)?(4) we can conclude that be-
cause and seeing as are CONTINGENTLY SUBSTI-
TUTABLE (henceforth ?CONT. SUBS.?). However
this is the only relationship that can be established
using a finite number of linguistic examples. The
other relationships all involve generalisations over
all contexts, and so rely to some degree on the judge-
ment of the analyst. Examples of each relationship
given by Knott (1996) include: given that and see-
ing as are SYNONYMS, on the grounds that is a HY-
PONYM of because, and because and now that are
EXCLUSIVE.
Although substitutability is inherently a more
complex notion than similarity, distributional simi-
larity is expected to be of some use in predicting sub-
stitutability relationships. For example, if two dis-
course connectives are SYNONYMS then we would
expect them to have similar distributions. On the
other hand, if two connectives are EXCLUSIVE, then
we would expect them to have dissimilar distribu-
tions. However if the relationship between two con-
nectives is HYPONYMY or CONT. SUBS. then we
expect to have partial overlap between their distribu-
tions (consider Figure 2), and so distributional simi-
larity might not distinguish these relationships.
The Kullback-Leibler (KL) divergence function
is a distributional similarity function that is of par-
ticular relevance here since it can be described in-
formally in terms of substitutability. Given co-
occurrence distributions p and q, its mathematical
definition can be written as:
D(p||q) =
?
x
p(x)(log
1
q(x)
? log
1
p(x)
) (5)
151
w1
w2
(a) w1 and w2
are SYNONYMS
w2
w1
(b) w2 is a HY-
PONYM of w1
w1 w2
(c) w1 is a HY-
PONYM of w2
w1
w2
(d) w1 and w2 are
CONT. SUBS.
w2w1
(e) w1 and w2 are
EXCLUSIVE
Figure 3: Surprise in substituting w2 for w1 (darker shading indicates higher surprise)
The value log 1p(x) has an informal interpretation as
a measure of how surprised an observer would be
to see event x, given prior likelihood expectations
defined by p. Thus, if p and q are the distributions of
words w1 and w2 then
D(p||q) = Ep(surprise in seeing w2
? surprise in seeing w1) (6)
where Ep is the expectation function over the distri-
bution of w1 (i.e. p). That is, KL divergence mea-
sures how much more surprised we would be, on
average, to see word w2 rather than w1, where the
averaging is weighted by the distribution of w1.
3 A variance-based function for
distributional analysis
A distributional similarity function provides only
a one-dimensional comparison of two distributions,
namely how similar they are. However we can ob-
tain an additional perspective by using a variance-
based function. We now introduce a new function V
by taking the variance of the surprise in seeing w2,
over the contexts in which w1 appears:
V (p, q) = V ar(surprise in seeing w2)
= Ep((Ep(log
1
q(x)
) ? log
1
q(x)
)2) (7)
Note that like KL divergence, V (p, q) is asymmetric.
We now consider how the substitutability of con-
nectives affects our expectations of the value of V .
If two connectives are SYNONYMS then each can
always be used in place of other. Thus we would
always expect a low level of surprise in seeing one
Relationship Function
of w1 to w2 D(p||q) D(q||p) V (p, q) V (q, p)
SYNONYM Low Low Low Low
HYPONYM Low Medium Low High
CONT. SUBS. Medium Medium High High
EXCLUSIVE High High Low Low
Table 1: Expectations for distributional functions
connective in place of the other, and this low level of
surprise is indicated via light shading in Figure 3a.
It follows that the variance in surprise is low. On the
other hand, if two connectives are EXCLUSIVE then
there would always be a high degree of surprise in
seeing one in place of the other. This is indicated
using dark shading in Figure 3e. Only one set is
shaded because we need only consider the contexts
in which w1 is appropriate. In this case, the vari-
ance in surprise is again low. The situation is more
interesting when we consider two connectives that
are CONT. SUBS.. In this case substitutability (and
hence surprise) is dependent on the context. This
is illustrated using light and dark shading in Fig-
ure 3d. As a result, the variance in surprise is high.
Finally, with HYPONYMY, the variance in surprise
depends on whether the original connective was the
HYPONYM or the HYPERNYM.
Table 1 summarises our expectations of the val-
ues of KL divergence and V , for the various sub-
stitutability relationships. (KL divergence, unlike
most similarity functions, is sensitive to the order of
arguments related by hyponymy (Lee, 1999).) The
152
Something happened and something else happened.
Something happened or something else happened.
? 0 ? 1 ? 2 ? 3 ? 4 ? 5
Figure 4: Example experimental item
experiments described below test these expectations
using empirical data.
4 Experiments
We now describe our empirical experiments which
investigate the connections between a) subjects? rat-
ings of the similarity of discourse connectives, b)
the substitutability of discourse connectives, and c)
KL divergence and the new function V applied to
the distributions of connectives. Our motivation is
to explore how distributional properties of words
might be used to predict substitutability. The ex-
periments are restricted to connectives which relate
clauses within a sentence. These include coordinat-
ing conjunctions (e.g. but) and a range of subordina-
tors including conjunctions (e.g. because) as well as
phrases introducing adverbial clauses (e.g. now that,
given that, for the reason that). Adverbial discourse
connectives are therefore not considered.
4.1 Experiment 1: Subject ratings of similarity
This experiment tests the hypotheses that 1) subjects
agree on the degree of similarity between pairs of
discourse connectives, and 2) similarity ratings cor-
relate with the degree of substitutability.
4.1.1 Methodology
We randomly selected 48 pairs of discourse con-
nectives such that there were 12 pairs standing in
each of the four substitutability relationships.To do
this, we used substitutability judgements made by
Knott (1996), supplemented with some judgements
of our own. Each experimental item consisted of
the two discourse connectives along with dummy
clauses, as illustrated in Figure 4. The format of the
experimental items was designed to indicate how a
phrase could be used as a discourse connective (e.g.
it may not be obvious to a subject that the phrase
the moment is a discourse connective), but without
Mean HYP CONT. SUBS. EXCL
SYNONYM 3.97 * * *
HYPONYM 3.43 * *
CONT. SUBS. 1.79 *
EXCLUSIVE 1.08
Table 2: Similarity by substitutability relationship
providing complete semantics for the clauses, which
might bias the subjects? ratings. Forty native speak-
ers of English participated in the experiment, which
was conducted remotely via the internet.
4.1.2 Results
Leave-one-out resampling was used to compare
each subject?s ratings are with the means of their
peers? (Weiss and Kulikowski, 1991). The average
inter-subject correlation was 0.75 (Min = 0.49, Max
= 0.86, StdDev = 0.09), which is comparable to pre-
vious results on verb similarity ratings (Resnik and
Diab, 2000). The effect of substitutability on simi-
larity ratings can be seen in Table 2. Post-hoc Tukey
tests revealed all differences between means in Ta-
ble 2 to be significant.
The results demonstrate that subjects? ratings of
connective similarity show significant agreement
and are robust enough for effects of substitutability
to be found.
4.2 Experiment 2: Modelling similarity
This experiment compares subjects? ratings of sim-
ilarity with lexical co-occurrence data. It hypothe-
sises that similarity ratings correlate with distribu-
tional similarity, but that neither correlates with the
new variance in surprise function.
4.2.1 Methodology
Sentences containing discourse connectives were
gathered from the British National Corpus and the
world wide web, with discourse connectives identi-
fied on the basis of their syntactic contexts (for de-
tails, see Hutchinson (2004b)). The mean number
of sentences per connective was about 32, 000, al-
though about 12% of these are estimated to be er-
rors. From these sentences, lexical co-occurrence
data were collected. Only co-occurrences with dis-
153
 0
 0.5
 1
 1.5
 2
 2.5
 0  1  2  3  4  5D
i
v
e
r
g
e
n
c
e
 
o
f
 
D
M
 
c
o
-
o
c
c
u
r
r
e
n
c
e
s
Similarity judgements
best fitSYNONYMHYPONYMCONT SUBSEXCLUSIVE
Figure 5: Similarity versus distributional divergence
course adverbials and other structural discourse con-
nectives were stored, as these had previously been
found to be useful for predicting semantic features
of connectives (Hutchinson, 2004a).
4.2.2 Results
A skewed variant of the Kullback-Leibler diver-
gence function was used to compare co-occurrence
distributions (Lee, 1999, with ? = 0.95). Spear-
man?s correlation coefficient for ranked data showed
a significant correlation (r = ?0.51, p < 0.001).
(The correlation is negative because KL divergence
is lower when distributions are more similar.) The
strength of this correlation is comparable with sim-
ilar results achieved for verbs (Resnik and Diab,
2000), but not as great as has been observed for
nouns (McDonald, 2000). Figure 5 plots the mean
similarity judgements against the distributional di-
vergence obtained using discourse markers, and also
indicates the substitutability relationship for each
item. (Two outliers can be observed in the upper left
corner; these were excluded from the calculations.)
The ?variance in surprise? function introduced in
the previous section was applied to the same co-
occurrence data.1 These variances were compared
to distributional divergence and the subjects? simi-
larity ratings, but in both cases Spearman?s correla-
tion coefficient was not significant.
In combination with the previous experiment,
1In practice, the skewed variant V (p, 0.95q + 0.05p) was
used, in order to avoid problems arising when q(x) = 0.
these results demonstrate a three way correspon-
dence between the human ratings of the similar-
ity of a pair of connectives, their substitutabil-
ity relationship, and their distributional similarity.
Hutchinson (2005) presents further experiments on
modelling connective similarity, and discusses their
implications. This experiment also provides empiri-
cal evidence that the new variance in surprise func-
tion is not a measure of similarity.
4.3 Experiment 3: Predicting substitutability
The previous experiments provide hope that sub-
stitutability of connectives might be predicted on
the basis of their empirical distributions. However
one complicating factor is that EXCLUSIVE is by far
the most likely relationship, holding between about
70% of pairs. Preliminary experiments showed
that the empirical evidence for other relationships
was not strong enough to overcome this prior bias.
We therefore attempted two pseudodisambiguation
tasks which eliminated the effects of prior likeli-
hoods. The first task involved distinguishing be-
tween the relationships whose connectives subjects
rated as most similar, namely SYNONYMY and HY-
PONYMY. Triples of connectives ?p, q, q?? were
collected such that SYNONYM(p, q) and either HY-
PONYM(p, q?) or HYPONYM(q?, p) (we were not at-
tempting to predict the order of HYPONYMY). The
task was then to decide automatically which of q and
q? is the SYNONYM of p.
The second task was identical in nature to the first,
however here the relationship between p and q was
either SYNONYMY or HYPONYMY, while p and q?
were either CONT. SUBS. or EXCLUSIVE. These
two sets of relationships are those corresponding to
high and low similarity, respectively. In combina-
tion, the two tasks are equivalent to predicting SYN-
ONYMY or HYPONYMY from the set of all four rela-
tionships, by first distinguishing the high similarity
relationships from the other two, and then making a
finer-grained distinction between the two.
4.3.1 Methodology
Substitutability relationships between 49 struc-
tural discourse connectives were extracted from
Knott?s (1996) classification. In order to obtain more
evaluation data, we used Knott?s methodology to ob-
tain relationships between an additional 32 connec-
154
max(D1, D2) max(V1, V2) (V1 ? V2)2
SYN 0.627 4.44 3.29
HYP 0.720 5.16 8.02
CONT 1.057 4.85 7.81
EXCL 1.069 4.79 7.27
Table 3: Distributional analysis by substitutability
tives. This resulted in 46 triples ?p, q, q?? for the first
task, and 10,912 triples for the second task.
The co-occurrence data from the previous section
were re-used. These were used to calculate D(p||q)
and V (p, q). Both of these are asymmetric, so for
our purposes we took the maximum of applying
their arguments in both orders. Recall from Table 1
that when two connectives are in a HYPONYMY re-
lation we expect V to be sensitive to the order in
which the connectives are given as arguments. To
test this, we also calculated (V (p, q) ? V (q, p))2,
i.e. the square of the difference of applying the argu-
ments to V in both orders. The average values are
summarised in Table 3, with D1 and D2 (and V1 and
V2) denoting different orderings of the arguments to
D (and V ), and max denoting the function which
selects the larger of two numbers.
These statistics show that our theoretically moti-
vated expectations are supported. In particular, (1)
SYNONYMOUS connectives have the least distribu-
tional divergence and EXCLUSIVE connectives the
most, (2) CONT. SUBS. and HYPONYMOUS connec-
tives have the greatest values for V , and (3) V shows
the greatest sensitivity to the order of its arguments
in the case of HYPONYMY.
The co-occurrence data were used to construct a
Gaussian classifier, by assuming the values for D
and V are generated by Gaussians.2 First, normal
functions were used to calculate the likelihood ratio
of p and q being in the two relationships:
P (syn|data)
P (hyp|data)
=
P (syn)
P (hyp)
?
P (data|syn)
P (data|hyp)
(8)
= 1?
n(max(D1, D2);?syn, ?syn)
n(max(D1, D2);?hyp, ?hyp)
(9)
2KL divergence is right skewed, so a log-normal model was
used to model D, whereas a normal model used for V .
Input to Gaussian SYN vs SYN/HYP vs
Model HYP EX/CONT
max(D1, D2) 50.0% 76.1%
max(V1, V2) 84.8% 60.6%
Table 4: Accuracy on pseudodisambiguation task
where n(x;?, ?) is the normal function with mean
? and standard deviation ?, and where ?syn, for ex-
ample, denotes the mean of the Gaussian model for
SYNONYMY. Next the likelihood ratio for p and
q was divided by that for p and q?. If this value
was greater than 1, the model predicted p and q
were SYNONYMS, otherwise HYPONYMS. The same
technique was used for the second task.
4.3.2 Results
A leave-one-out cross validation procedure was
used. For each triple ?p, q, q??, the data concern-
ing the pairs p, q and p, q? were held back, and the
remaining data used to construct the models. The
results are shown in Table 4. For comparison, a ran-
dom baseline classifier achieves 50% accuracy.
The results demonstrate the utility of the new
variance-based function V . The new variance-based
function V is better than KL divergence at dis-
tinguishing HYPONYMY from SYNONYMY (?2 =
11.13, df = 1, p < 0.001), although it performs
worse on the coarser grained task. This is consis-
tent with the expectations of Table 1. The two clas-
sifiers were also combined by making a naive Bayes
assumption. This gave an accuracy of 76.1% on the
first task, which is significantly better than just us-
ing KL divergence (?2 = 5.65, df = 1, p < 0.05),
and not significantly worse than using V . The com-
bination?s accuracy on the second task was 76.2%,
which is about the same as using KL divergence.
This shows that combining similarity- and variance-
based measures can be useful can improve overall
performance.
5 Conclusions
The concepts of lexical similarity and substitutabil-
ity are of central importance to psychology, ar-
tificial intelligence and computational linguistics.
155
To our knowledge this is the first modelling study
of how these concepts relate to lexical items in-
volved in discourse-level phenomena. We found a
three way correspondence between data sources of
quite distinct types: distributional similarity scores
obtained from lexical co-occurrence data, substi-
tutability judgements made by linguists, and the
similarity ratings of naive subjects.
The substitutability of lexical items is important
for applications such as text simplification, where it
can be desirable to paraphrase one discourse con-
nective using another. Ultimately we would like to
automatically predict substitutability for individual
tokens. However predicting whether one connective
can either a) always, b) sometimes or c) never be
substituted for another is a step towards this goal.
Our results demonstrate that these general substi-
tutability relationships have empirical correlates.
We have introduced a novel variance-based func-
tion of two distributions which complements distri-
butional similarity. We demonstrated the new func-
tion?s utility in helping to predict the substitutabil-
ity of connectives, and it can be expected to have
wider applicability to lexical acquisition tasks. In
particular, it is expected to be useful for learning
relationships which cannot be characterised purely
in terms of similarity, such as hyponymy. In future
work we will analyse further the empirical proper-
ties of the new function, and investigate its applica-
bility to learning relationships between other classes
of lexical items such as nouns.
Acknowledgements
I would like to thank Mirella Lapata, Alex Las-
carides, Alistair Knott, and the anonymous ACL re-
viewers for their helpful comments. This research
was supported by EPSRC Grant GR/R40036/01 and
a University of Sydney Travelling Scholarship.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of Conver-
sation. Cambridge University Press.
James R. Curran and M. Moens. 2002. Improvements in auto-
matic thesaurus extraction. In Proceedings of the Workshop
on Unsupervised Lexical Acquisition, Philadelphia, USA.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Brigitte Grote and Manfred Stede. 1998. Discourse marker
choice in sentence planning. In Eduard Hovy, editor, Pro-
ceedings of the Ninth International Workshop on Natural
Language Generation, pages 128?137, New Brunswick,
New Jersey. Association for Computational Linguistics.
M. Halliday and R. Hasan. 1976. Cohesion in English. Long-
man.
Jerry A Hobbs. 1985. On the coherence and structure of dis-
course. Technical Report CSLI-85-37, Center for the Study
of Language and Information, Stanford University.
Ben Hutchinson. 2004a. Acquiring the meaning of discourse
markers. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL 2004),
pages 685?692.
Ben Hutchinson. 2004b. Mining the web for discourse mark-
ers. In Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC 2004), pages
407?410, Lisbon, Portugal.
Ben Hutchinson. 2005. Modelling the similarity of discourse
connectives. To appear in Proceedings of the the 27th An-
nual Meeting of the Cognitive Science Society (CogSci2005).
Andrew Kehler. 2002. Coherence, Reference and the Theory of
Grammar. CSLI publications.
Alistair Knott. 1996. A data-driven methodology for motivat-
ing a set of coherence relations. Ph.D. thesis, University of
Edinburgh.
Mirella Lapata and Alex Lascarides. 2004. Inferring sentence-
internal temporal relations. In In Proceedings of the Human
Language Technology Conference and the North American
Chapter of the Association for Computational Linguistics
Annual Meeting, Boston, MA.
Lillian Lee. 1999. Measures of distributional similarity. In
Proceedings of ACL 1999.
Daniel Marcu and Abdessamad Echihabi. 2002. An unsuper-
vised approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), Philadelphia, PA.
Daniel Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. The MIT Press.
Scott McDonald. 2000. Environmental determinants of lexical
processing effort. Ph.D. thesis, University of Edinburgh.
George A. Miller and William G. Charles. 1991. Contextual
correlates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
M. Moser and J. Moore. 1995. Using discourse analysis and
automatic text generation to study discourse cue usage. In
Proceedings of the AAAI 1995 Spring Symposium on Empir-
ical Methods in Discourse Interpretation and Generation.
Philip Resnik and Mona Diab. 2000. Measuring verb similarity.
In Proceedings of the Twenty Second Annual Meeting of the
Cognitive Science Society, Philadelphia, US, August.
Philip Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to problems
of ambiguity in natural language. Journal of Artificial Intel-
ligence Research, 11:95?130.
H. Rubenstein and J. B. Goodenough. 1965. Contextual corre-
lates of synonymy. Computational Linguistics, 8:627?633.
Advaith Siddharthan. 2003. Preserving discourse structure
when simplifying text. In Proceedings of the 2003 European
Natural Language Generation Workshop.
Julie Weeds and David Weir. 2003. A general framework
for distributional similarity. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP 2003), Sapporo, Japan, July.
Sholom M. Weiss and Casimir A. Kulikowski. 1991. Computer
systems that learn. Morgan Kaufmann, San Mateo, CA.
156
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 890?899,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Using the Web for Language Independent Spellchecking and
Autocorrection
Casey Whitelaw and Ben Hutchinson and Grace Y Chung and Gerard Ellis
Google Inc.
Level 5, 48 Pirrama Rd, Pyrmont NSW 2009, Australia
{whitelaw,benhutch,gracec,ged}@google.com
Abstract
We have designed, implemented and eval-
uated an end-to-end system spellcheck-
ing and autocorrection system that does
not require any manually annotated train-
ing data. The World Wide Web is used
as a large noisy corpus from which we
infer knowledge about misspellings and
word usage. This is used to build an er-
ror model and an n-gram language model.
A small secondary set of news texts with
artificially inserted misspellings are used
to tune confidence classifiers. Because
no manual annotation is required, our sys-
tem can easily be instantiated for new lan-
guages. When evaluated on human typed
data with real misspellings in English and
German, our web-based systems outper-
form baselines which use candidate cor-
rections based on hand-curated dictionar-
ies. Our system achieves 3.8% total error
rate in English. We show similar improve-
ments in preliminary results on artificial
data for Russian and Arabic.
1 Introduction
Spellchecking is the task of predicting which
words in a document are misspelled. These pre-
dictions might be presented to a user by under-
lining the misspelled words. Correction is the
task of substituting the well-spelled hypotheses
for misspellings. Spellchecking and autocorrec-
tion are widely applicable for tasks such as word-
processing and postprocessing Optical Character
Recognition. We have designed, implemented
and evaluated an end-to-end system that performs
spellchecking and autocorrection.
The key novelty of our work is that the sys-
tem was developed entirely without the use of
manually annotated resources or any explicitly
compiled dictionaries of well-spelled words. Our
multi-stage system integrates knowledge from sta-
tistical error models and language models (LMs)
with a statistical machine learning classifier. At
each stage, data are required for training models
and determining weights on the classifiers. The
models and classifiers are all automatically trained
from frequency counts derived from the Web and
from news data. System performance has been
validated on a set of human typed data. We have
also shown that the system can be rapidly ported
across languages with very little manual effort.
Most spelling systems today require some hand-
crafted language-specific resources, such as lex-
ica, lists of misspellings, or rule bases. Sys-
tems using statistical models require large anno-
tated corpora of spelling errors for training. Our
statistical models require no annotated data. In-
stead, we rely on the Web as a large noisy corpus
in the following ways. 1) We infer information
about misspellings from term usage observed on
the Web, and use this to build an error model. 2)
The most frequently observed terms are taken as
a noisy list of potential candidate corrections. 3)
Token n-grams are used to build an LM, which
we use to make context-appropriate corrections.
Because our error model is based on scoring sub-
strings, there is no fixed lexicon of well-spelled
words to determine misspellings. Hence, both
novel misspelled or well-spelled words are allow-
able. Moreover, in combination with an n-gram
LM component, our system can detect and correct
real-word substitutions, ie, word usage and gram-
matical errors.
Confidence classifiers determine the thresholds
for spelling error detection and autocorrection,
given error and LM scores. In order to train these
classifiers, we require some textual content with
some misspellings and corresponding well-spelled
words. A small subset of the Web data from news
pages are used because we assume they contain
890
relatively few misspellings. We show that con-
fidence classifiers can be adequately trained and
tuned without real-world spelling errors, but rather
with clean news data injected with artificial mis-
spellings.
This paper will proceed as follows. In Section 2,
we survey related prior research. Section 3 de-
scribes our approach, and how we use data at each
stage of the spelling system. In experiments (Sec-
tion 4), we first verify our system on data with ar-
tificial misspellings. Then we report performance
on data with real typing errors in English and Ger-
man. We also show preliminary results from port-
ing our system to Russian and Arabic.
2 Related Work
Spellchecking and correction are among the oldest
text processing problems, and many different so-
lutions have been proposed (Kukich, 1992). Most
approaches are based upon the use of one or more
manually compiled resources. Like most areas
of natural language processing, spelling systems
have been increasingly empirical, a trend that our
system continues.
The most direct approach is to model the
causes of spelling errors directly, and encode them
in an algorithm or an error model. Damerau-
Levenshtein edit distance was introduced as a
way to detect spelling errors (Damerau, 1964).
Phonetic indexing algorithms such as Metaphone,
used by GNU Aspell (Atkinson, 2009), repesent
words by their approximate ?soundslike? pronun-
ciation, and allow correction of words that ap-
pear orthographically dissimilar. Metaphone relies
upon data files containing phonetic information.
Linguistic intuition about the different causes of
spelling errors can also be represented explicitly in
the spelling system (Deorowicz and Ciura, 2005).
Almost every spelling system to date makes use
of a lexicon: a list of terms which are treated as
?well-spelled?. Lexicons are used as a source of
corrections, and also to filter words that should
be ignored by the system. Using lexicons in-
troduces the distinction between ?non-word? and
?real-word? errors, where the misspelled word is
another word in the lexicon. This has led to
the two sub-tasks being approached separately
(Golding and Schabes, 1996). Lexicon-based ap-
proaches have trouble handling terms that do not
appear in the lexicon, such as proper nouns, for-
eign terms, and neologisms, which can account for
a large proportion of ?non-dictionary? terms (Ah-
mad and Kondrak, 2005).
A word?s context provides useful evidence as
to its correctness. Contextual information can be
represented by rules (Mangu and Brill, 1997) or
more commonly in an n-gram LM. Mays et al
(1991) used a trigram LM and a lexicon, which
was shown to be competitive despite only allow-
ing for a single correction per sentence (Wilcox-
O?Hearn et al, 2008). Cucerzan and Brill (2004)
claim that an LM is much more important than
the channel model when correcting Web search
queries. In place of an error-free corpus, the Web
has been successfully used to correct real-word
errors using bigram features (Lapata and Keller,
2004). This work uses pre-defined confusion sets.
The largest step towards an automatically train-
able spelling system was the statistical model for
spelling errors (Brill and Moore, 2000). This re-
places intuition or linguistic knowledge with a
training corpus of misspelling errors, which was
compiled by hand. This approach has also been
extended to incorporate a pronunciation model
(Toutanova and Moore, 2002).
There has been recent attention on using Web
search query data as a source of training data, and
as a target for spelling correction (Yang Zhang and
Li, 2007; Cucerzan and Brill, 2004). While query
data is a rich source of misspelling information in
the form of query-revision pairs, it is not available
for general use, and is not used in our approach.
The dependence upon manual resources has
created a bottleneck in the development of
spelling systems. There have been few language-
independent, multi-lingual systems, or even sys-
tems for languages other than English. Language-
independent systems have been evaluated on Per-
sian (Barari and QasemiZadeh, 2005) and on Ara-
bic and English (Hassan et al, 2008). To our
knowledge, there are no previous evaluations of
a language-independent system across many lan-
guages, for the full spelling correction task, and
indeed, there are no pre-existing standard test sets
for typed data with real errors and language con-
text.
3 Approach
Our spelling system follows a noisy channel
model of spelling errors (Kernighan et al, 1990).
For an observed word w and a candidate correc-
tion s, we compute P (s|w) as P (w|s)? P (s).
891
confidence
classifiers
Web
News data 
with artificial 
misspellings
corrected textinput text
language model
error model
term list
scored
suggestions
Figure 1: Spelling process, and knowledge sources used.
The text processing workflow and the data used
in building the system are outlined in Figure 1 and
detailed in this section. For each token in the in-
put text, candidate suggestions are drawn from the
term list (Section 3.1), and scored using an error
model (Section 3.2). These candidates are eval-
uated in context using an LM (Section 3.3) and
re-ranked. For each token, we use classifiers (Sec-
tion 3.4) to determine our confidence in whether
a word has been misspelled and if so, whether it
should be autocorrected to the best-scoring sug-
gestion available.
3.1 Term List
We require a list of terms to use as candidate cor-
rections. Rather than attempt to build a lexicon
of words that are well-spelled, we instead take the
most frequent tokens observed on the Web. We
used a large (> 1 billion) sample of Web pages,
tokenized them, and took the most frequently oc-
curring ten million tokens, with very simple filters
for non-words (too much punctuation, too short or
long). This term list is so large that it should con-
tain most well-spelled words, but also a large num-
ber of non-words or misspellings.
3.2 Error Model
We use a substring error model to estimate
P (w|s). To derive the error model, let R be
a partitioning of s into adjacent substrings, and
similarly let T be a partitioning of w, such that
|T | = |R|. The partitions are thus in one-to-one
alignment, and by allowing partitions to be empty,
the alignment models insertions and deletions of
substrings. Brill and Moore estimate P (w|s) as
follows:
P (w|s) ? max
R, T s.t. |T |=|R|
|R|
?
i=1
P (T
i
|R
i
) (1)
Our system restricts partitionings that have sub-
strings of length at most 2.
To train the error model, we require triples of
(intended word, observed word, count), which are
described below. We use maximum likelihood es-
timates of P (T
i
|R
i
).
3.2.1 Using the Web to Infer Misspellings
To build the error model, we require as train-
ing data a set of (intended word, observed word,
count) triples, which is compiled from the World
Wide Web. Essentially the triples are built by start-
ing with the term list, and a process that auto-
matically discovers, from that list, putative pairs
of spelled and misspelled words, along with their
counts.
We believe the Web is ideal for compiling this
set of triples because with a vast amount of user-
generated content, we believe that the Web con-
tains a representative sample of both well-spelled
and misspelled text. The triples are not used di-
rectly for proposing corrections, and since we have
a substring model, they do not need to be an ex-
haustive list of spelling mistakes.
The procedure for finding and updating counts
for these triples also assumes that 1) misspellings
tend to be orthographically similar to the intended
word; Mays et al(1991) observed that 80% of
892
misspellings derived from single instances of in-
sertion, deletion, or substitution; and 2) words are
usually spelled as intended.
For the error model, we use a large corpus (up to
3.7?10
8
pages) of crawled public Web pages. An
automatic language-identification system is used
to identify and filter pages for the desired lan-
guage. As we only require a small window of con-
text, it would also be possible to use an n-gram
collection such as the Google Web 1T dataset.
Finding Close Words. For each term in the
term list (defined in Section 3.1), we find all
other terms in the list that are ?close? to it. We
define closeness using Levenshtein-Damerau edit
distance, with a conservative upper bound that in-
creases with word length (one edit for words of
up to four characters, two edits for up to twelve
characters, and three for longer words). We com-
pile the term list into a trie-based data structure
which allows for efficient searching for all terms
within a maximum edit distance. The computa-
tion is ?embarassingly parallel? and hence easily
distributable. In practice, we find that this stage
takes tens to hundreds of CPU-hours.
Filtering Triples. At this stage, for each
term we have a cluster of orthographically similar
terms, which we posit are potential misspellings.
The set of pairs is reflexive and symmetric, e.g. it
contains both (recieve, receive) and (receive, re-
cieve). The pairs will also include e.g. (deceive,
receive). On the assumption that words are spelled
correctly more often than they are misspelled, we
next filter the set such that the first term?s fre-
quency is at least 10 times that of the second term.
This ratio was chosen as a conservative heuristic
filter.
Using Language Context. Finally, we use the
contexts in which a term occurs to gather direc-
tional weightings for misspellings. Consider a
term w; from our source corpus, we collect the
set of contexts {c
i
} in which w occurs. The defi-
nition of a context is relatively arbitrary; we chose
to use a single word on each side, discarding con-
texts with fewer than a total of ten observed occur-
rences. For each context c
i
, candidate ?intended?
terms arew andw?s close terms (which are at least
10 times as frequent as w). The candidate which
appears in context c
i
the most number of times is
deemed to be the term intended by the user in that
context.
The resulting dataset consists of triples of the
original observed term, one of the ?intended?
terms as determined by the above algorithm, and
the number of times this term was intended. For
a single term, it is possible (and common) to have
multiple possible triples, due to the context-based
assignment.
Inspecting the output of this training process
shows some interesting patterns. Overall, the
dataset is still noisy; there are many instances
where an obviously misspelled word is not as-
signed a correction, or only some of its instances
are. The dataset contains around 100 million
triples, orders of magnitude larger than any man-
ually compiled list of misspellings . The kinds of
errors captured in the dataset include stereotypi-
cal spelling errors, such as acomodation, but also
OCR-style errors. computationaUy was detected
as a misspelling of computationally where the ?U?
is an OCR error for ?ll?; similarly, Postmodem was
detected as a misspelling of Postmodern (an exam-
ple of ?keming?).
The data also includes examples of ?real-word?
errors. For example, 13% of occurrences of
occidental are considered misspellings of acci-
dental; contrasting with 89% of occurrences of
the non-word accidential. There are many ex-
amples of terms that would not be in a normal
lexicon, including neologisms (mulitplayer for
multiplayer), companies and products (Playsta-
ton for Playstation), proper nouns (Schwarznegger
for Schwarzenegger) and internet domain names
(mysapce.com for myspace.com).
3.3 Language Model
We estimate P (s) using n-gram LMs trained on
data from the Web, using Stupid Backoff (Brants
et al, 2007). We use both forward and back-
ward context, when available. Contrary to Brill
and Moore (2000), we observe that user edits of-
ten have both left and right context, when editing
a document.
When combining the error model scores with
the LM scores, we weight the latter by taking their
??th power, that is
P (w|s) ? P (s)
?
(2)
The parameter ? reflects the relative degrees to
which the LM and the error model should be
trusted. The parameter ? also plays the additional
role of correcting our error model?s misestimation
of the rate at which people make errors. For exam-
ple, if errors are common then by increasing ? we
893
can reduce the value of P (w|w) ? P (w)
?
relative
to
?
s 6=w
P (s|w) ? P (s)
?
.
We train ? by optimizing the average inverse
rank of the correct word on our training corpus,
where the rank is calculated over all suggestions
that we have for each token.
During initial experimentation, it was noticed
that our system predicted many spurious autocor-
rections at the beginnings and ends of sentences
(or in the case of sentence fragments, the end of
the fragment). We hypothesized that we were
weighting the LM scores too highly in such cases.
We therefore conditioned ? on how much context
was available, obtaining values ?
i,j
where i, j rep-
resent the amount of context available to the LM
to the left and right of the current word. i and j are
capped at n, the order of the LM.
While conditioning ? in this way might at first
appear ad hoc, it has a natural interpretation in
terms of our confidence in the LM. When there is
no context to either side of a word, the LM simply
uses unigram probabilities, and this is a less trust-
worthy signal than when more context is available.
To train ?
i,j
we partition our data into bins cor-
responding to pairs i, j and optimize each ?
i,j
in-
dependently.
Training a constant ?, a value of 5.77 was ob-
tained. The conditioned weights ?
i,j
increased
with the values of i and j, ranging from ?
0,0
=
0.82 to ?
4,4
= 6.89. This confirmed our hypoth-
esis that the greater the available context the more
confident our system should be in using the LM
scores.
3.4 Confidence Classifiers for Checking and
Correction
Spellchecking and autocorrection were imple-
mented as a three stage process. These em-
ploy confidence classifiers whereby precision-
recall tradeoffs could be tuned to desirable levels
for both spellchecking and autocorrection.
First, all suggestions s for a word w are ranked
according to their P (s|w) scores. Second, a
spellchecking classifier is used to predict whether
w is misspelled. Third, if w is both predicted to be
misspelled and s is non-empty, an autocorrection
classifier is used to predict whether the top-ranked
suggestion is correct.
The spellchecking classifier is implemented us-
ing two embedded classifiers, one of which is used
when s is empty, and the other when it is non-
empty. This design was chosen because the use-
ful signals for predicting whether a word is mis-
spelled might be quite different when there are no
suggestions available, and because certain features
are only applicable when there are suggestions.
Our experiments will compare two classifier
types. Both rely on training data to determine
threshold values and training weights.
A ?simple? classifier which compares the value
of log(P (s|w)) ? log(P (w|w)), for the original
word w and the top-ranked suggestion s, with a
threshold value. If there are no suggestions other
than w, then the log(P (s|w)) term is ignored.
A logistic regression classifier that uses five
feature sets. The first set is a scores feature
that combines the following scoring information
(i) log(P (s|w)) ? log(P (w|w)) for top-ranked
suggestion s. (ii) LM score difference between
the original word w and the top suggestion s.
(iii) log(P (s|w)) ? log(P (w|w)) for second top-
ranked suggestion s. (iv) LM score difference be-
tween w and second top-ranked s. The other four
feature sets encode information about case signa-
tures, number of suggestions available, the token
length, and the amount of left and right context.
Certain categories of tokens are blacklisted, and
so never predicted to be misspelled. These are
numbers, punctuation and symbols, and single-
character tokens.
The training process has three stages. (1) The
context score weighting is trained, as described
in Section 3.3. (2) The spellchecking classifier is
trained, and tuned on held-out development data.
(3) The autocorrection classifier is trained on the
instances with suggestions that the spellchecking
classifier predicts to be misspelled, and it too is
tuned on held-out development data.
In the experiments reported in this paper, we
trained classifiers so as to maximize the F
1
-score
on the development data. We note that the desired
behaviour of the spellchecking and autocorrection
classifiers will differ depending upon the applica-
tion, and that it is a strength of our system that
these can be tuned independently.
3.4.1 Training Using Artificial Data
Training and tuning the confidence classifiers re-
quire supervised data, in the form of pairs of mis-
spelled and well-spelled documents. And indeed
we posit that relatively noiseless data are needed
to train robust classifiers. Since these data are
894
Language Sentences
Train Test
English 116k 58k
German 87k 44k
Arabic 8k 4k
Russian 8k 4k
Table 1: Artificial data set sizes. The development
set is approximately the same size as the training
set.
not generally available, we instead use a clean
corpus into which we artificially introduce mis-
spellings. While this data is not ideal, we show
that in practice it is sufficient, and removes the
need for manually-annotated gold-standard data.
We chose data from news pages crawled from
the Web as the original, well-spelled documents.
We chose news pages as an easily identifiable
source of text which we assume is almost entirely
well-spelled. Any source of clean text could be
used. For each language the news data were di-
vided into three non-overlapping data sets: the
training and development sets were used for train-
ing and tuning the confidence classifiers, and a test
set was used to report evaluation results. The data
set sizes, for the languages used in this paper, are
summarized in Table 1.
Misspelled documents were created by artifi-
cially introducing misspelling errors into the well-
spelled text. For all data sets, spelling errors
were randomly inserted at an average rate of 2 per
hundred characters, resulting in an average word
misspelling rate of 9.2%. With equal likelihood,
errors were either character deletions, transposi-
tions, or insertions of randomly selected charac-
ters from within the same document.
4 Experiments
4.1 Typed Data with Real Errors
In the absence of user data from a real application,
we attempted our initial evaluation with typed data
via a data collection process. Typed data with real
errors produced by humans were collected. We
recruited subjects from our coworkers, and asked
them to use an online tool customized for data
collection. Subjects were asked to randomly se-
lect a Wikipedia article, copy and paste several
text-only paragraphs into a form, and retype those
paragraphs into a subsequent form field. The sub-
jects were asked to pick an article about a favorite
city or town. The subjects were asked to type
at a normal pace avoiding the use of backspace
or delete buttons. The data were tokenized, au-
tomatically segmented into sentences, and manu-
ally preprocessed to remove certain gross typing
errors. For instance, if the typist omitted entire
phrases/sentences by mistake, the sentence was re-
moved. We collected data for English from 25
subjects, resulting in a test set of 11.6k tokens, and
495 sentences. There were 1251 misspelled tokens
(10.8% misspelling rate.)
Data were collected for German Wikipedia arti-
cles. We asked 5 coworkers who were German na-
tive speakers to each select a German article about
a favorite city or town, and use the same online
tool to input their typing. For some typists who
used English keyboards, they typed ASCII equiva-
lents to non-ASCII characters in the articles. This
was accounted for in the preprocessing of the ar-
ticles to prevent misalignment. Our German test
set contains 118 sentences, 2306 tokens with 288
misspelled tokens (12.5% misspelling rate.)
4.2 System Configurations
We compare several system configurations to in-
vestigate each component?s contribution.
4.2.1 Baseline Systems Using Aspell
Systems 1 to 4 have been implemented as base-
lines. These use GNU Aspell, an open source spell
checker (Atkinson, 2009), as a suggester compo-
nent plugged into our system instead of our own
Web-based suggester. Thus, with Aspell, the sug-
gestions and error scores proposed by the system
would all derive from Aspell?s handcrafted custom
dictionary and error model. (We report results us-
ing the best combination of Aspell?s parameters
that we found.)
System 1 uses Aspell tuned with the logistic
regression classifier. System 2 adds a context-
weighted LM, as per Section 3.3, and uses the
?simple? classifier described in Section 3.4. Sys-
tem 3 replaces the simple classifier with the logis-
tic regression classifier. System 4 is the same but
does not perform blacklisting.
4.2.2 Systems Using Web-based Suggestions
The Web-based suggester proposes suggestions
and error scores from among the ten million most
frequent terms on the Web. It suggests the 20
terms with the highest values of P (w|s) ? f(s)
using the Web-derived error model.
895
Systems 5 to 8 correspond with Systems 1 to
4, but use the Web-based suggestions instead of
Aspell.
4.3 Evaluation Metrics
In our evaluation, we aimed to select metrics that
we hypothesize would correlate well with real per-
formance in a word-processing application. In
our intended system, misspelled words are auto-
corrected when confidence is high and misspelled
words are flagged when a highly confident sug-
gestion is absent. This could be cast as a simple
classification or retrieval task (Reynaert, 2008),
where traditional measures of precision, recall and
F metrics are used. However we wanted to fo-
cus on metrics that reflect the quality of end-to-
end behavior, that account for the combined ef-
fects of flagging and automatic correction. Es-
sentially, there are three states: a word could be
unchanged, flagged or corrected to a suggested
word. Hence, we report on error rates that mea-
sure the errors that a user would encounter if the
spellchecking/autocorrection were deployed in a
word-processor. We have identified 5 types of er-
rors that a system could produce:
1. E
1
: A misspelled word is wrongly corrected.
2. E
2
: A misspelled word is not corrected but is
flagged.
3. E
3
: A misspelled word is not corrected or
flagged.
4. E
4
: A well spelled word is wrongly cor-
rected.
5. E
5
: A well spelled word is wrongly flagged.
It can be argued that these errors have varying
impact on user experience. For instance, a well
spelled word that is wrongly corrected is more
frustrating than a misspelled word that is not cor-
rected but is flagged. However, in this paper, we
treat each error equally.
E
1
, E
2
, E
3
and E
4
pertain to the correction
task. Hence we can define Correction Error Rate
(CER):
CER =
E
1
+ E
2
+ E
3
+ E
4
T
where T is the total number of tokens. E
3
and E
5
pertain to the nature of flagging. We define Flag-
ging Error Rate (FER) and Total Error Rate (TER):
FER =
E
3
+ E
5
T
TER =
E
1
+ E
2
+ E
3
+ E
4
+ E
5
T
For each system, we computed a No Good Sugges-
tion Rate (NGS) which represents the proportion
of misspelled words for which the suggestions list
did not contain the correct word.
5 Results and Discussion
5.1 Experiments with Artificial Errors
System TER CER FER NGS
1. Aspell, no LM, LR 17.65 6.38 12.35 18.3
2. Aspell, LM, Sim 4.82 2.98 2.86 18.3
3. Aspell, LM, LR 4.83 2.87 2.84 18.3
4. Aspell, LM, LR 22.23 2.79 19.89 16.3
(no blacklist)
5. WS, no LM, LR 9.06 7.64 6.09 10.1
6. WS, LM, Sim 2.62 2.26 1.43 10.1
7. WS, LM, LR 2.55 2.21 1.29 10.1
8. WS, LM, LR 21.48 2.21 19.75 8.9
(no blacklist)
Table 2: Results for English news data on an in-
dependent test set with artificial spelling errors.
Numbers are given in percentages. LM: Language
Model, Sim: Simple, LR: Logistic Regression,
WS: Web-based suggestions. NGS: No good sug-
gestion rate.
Results on English news data with artificial
spelling errors are displayed in Table 2. The sys-
tems which do not employ the LM scores per-
form substantially poorer that the ones with LM
scores. The Aspell system yields a total error rate
of 17.65% and our system with Web-based sug-
gestions yields TER of 9.06%.
When comparing the simple scorer with the lo-
gistic regression classifier, the Aspell Systems 2
and 3 generate similar performances while the
confidence classifier afforded some gains in our
Web-based suggestions system, with total error re-
duced from 2.62% to 2.55%. The ability to tune
each phase during development has so far proven
more useful than the specific features or classifier
used. Blacklisting is crucial as seen by our results
for Systems 4 and 8. When the blacklisting mech-
anism is not used, performance steeply declines.
When comparing overall performance for the
data between the Aspell systems and the Web-
based suggestions systems, our Web-based sug-
gestions fare better across the board for the news
data with artificial misspellings. Performance
896
gains are evident for each error metric that was ex-
amined. Total error rate for our best system (Sys-
tem 7) reduces the error of the best Aspell sys-
tem (System 3) by 45.7% (from 4.83% to 2.62%).
In addition, our no good suggestion rate is only
10% compared to 18% in the Aspell system. Even
where no LM scores are used, our Web-based sug-
gestions system outperforms the Aspell system.
The above results suggest that the Web-based
suggestions system performs at least as well as
the Aspell system. However, it must be high-
lighted that results on the test set with artificial
errors does not guarantee similar performance on
real user data. The artificial errors were generated
at a systematically uniform rate, and are not mod-
eled after real human errors made in real word-
processing applications. We attempt to consider
the impact of real human errors on our systems in
the next section.
5.2 Experiments with Human Errors
System TER CER FER NGS
English Aspell 4.58 3.33 2.86 23.0
English WS 3.80 3.41 2.24 17.2
German Aspell 14.09 10.23 5.94 44.4
German WS 9.80 7.89 4.55 32.3
Table 3: Results for Data with Real Errors in En-
glish and German.
Results for our system evaluated on data with
real misspellings in English and in German are
shown in Table 3. We used the systems that per-
formed best on the artificial data (System 3 for As-
pell, and System 7 for Web suggestions). The mis-
spelling error rates of the test sets were 10.8% and
12.5% respectively, higher than those of the arti-
ficial data which were used during development.
For English, the Web-based suggestions resulted
in a 17% improvement (from 4.58% to 3.80%) in
total error rate, but the correction error rate was
slightly (2.4%) higher.
By contrast, in German our system improved to-
tal error by 30%, from 14.09% to 9.80%. Correc-
tion error rate was also much lower in our Ger-
man system, comparing 7.89% with 10.23% for
the Aspell system. The no good suggestion rates
for the real misspelling data are also higher than
that of the news data. Our suggestions are lim-
ited to an edit distance of 2 with the original, and
it was found that in real human errors, the aver-
age edit distance of misspelled words is 1.38 but
for our small data, the maximum edit distance is
4 in English and 7 in German. Nonetheless, our
no good suggestion rates (17.2% and 32.3%) are
much lower than those of the Aspell system (23%
and 44%), highlighting the advantage of not using
a hand-crafted lexicon.
Our results on real typed data were slightly
worse than those for the news data. Several fac-
tors may account for this. (1) While the news data
test set does not overlap with the classifier train-
ing set, the nature of the content is similar to the
train and dev sets in that they are all news articles
from a one week period. This differs substantially
from Wikipedia article topics that were generally
about the history and sights a city. (2) Second,
the method for inserting character errors (random
generation) was the same for the news data sets
while the real typed test set differed from the ar-
tificial errors in the training set. Typed errors are
less consistent and error rates differed across sub-
jects. More in depth study is needed to understand
the nature of real typed errors.
 1
 2
 3
 4
 5
 6
 100  1000  10000  100000  1e+06  1e+07  1e+08  1e+09
Er
ro
r r
at
e
Corpus size
Typed TER
Typed CER
Typed FER
Artificial TER
Artificial CER
Artificial FER
Figure 2: Effect of corpus size used to train the
error model.
5.3 Effect of Web Corpus Size
To determine the effects of the corpus size on our
automated training, we evaluated System 7 using
error models trained on different corpus sizes. We
used corpora containing 10
3
, 10
4
, . . . , 10
9
Web
pages. We evaluated on the data set with real er-
rors. On average, about 37% of the pages in our
corpus were in English. So the number of pages
we used ranged from about 370 to about 3.7?10
8
.
As shown in Figure 2, the gains are small after
about 10
6
documents.
897
5.4 Correlation across data sets
We wanted to establish that performance improve-
ment on the news data with artificial errors are
likely to lead to improvement on typed data with
real errors. The seventeen English systems re-
ported in Table 3, Table 2 and Figure 2 were each
evaluated on both English test sets. The rank cor-
relation coefficient between total error rates on the
two data sets was high (? = 0.92; p < 5? 10
?6
).
That is, if one system performs better than another
on our artificial spelling errors, then the first sys-
tem is very likely to also perform better on real
typing errors.
5.5 Experiments with More Languages
System TER CER FER NGS
German Aspell 8.64 4.28 5.25 29.4
German WS 4.62 3.35 2.27 16.5
Arabic Aspell 11.67 4.66 8.51 25.3
Arabic WS 4.64 3.97 2.30 15.9
Russian Aspell 16.75 4.40 13.11 40.5
Russian WS 3.53 2.45 1.93 15.2
Table 4: Results for German, Russian, Arabic
news data.
Our system can be trained on many languages
with almost no manual effort. Results for German,
Arabic and Russian news data are shown in Ta-
ble 4. Performance improvements by the Web sug-
gester over Aspell are greater for these languages
than for English. Relative performance improve-
ments in total error rates are 47% in German, 60%
in Arabic and 79% in Russian. Differences in no
good suggestion rates are also very pronounced
between Aspell and the Web suggester.
It cannot be assumed that the Arabic and Rus-
sian systems would perform as well on real data.
However the correlation between data sets re-
ported in Section 5.4 lead us to hypothesize that
a comparison between the Web suggester and As-
pell on real data would be favourable.
6 Conclusions
We have implemented a spellchecking and au-
tocorrection system and evaluated it on typed
data. The main contribution of our work is that
while this system incorporates several knowledge
sources, an error model, LM and confidence clas-
sifiers, it does not require any manually annotated
resources, and infers its linguistic knowledge en-
tirely from the Web. Our approach begins with a
very large term list that is noisy, containing both
spelled and misspelled words, and derived auto-
matically with no human checking for whether
words are valid or not.
We believe this is the first published system
to obviate the need for any hand labeled data.
We have shown that system performance improves
from a system that embeds handcrafted knowl-
edge, yielding a 3.8% total error rate on human
typed data that originally had a 10.8% error rate.
News data with artificially inserted spellings were
sufficient to train confidence classifiers to a sat-
isfactory level. This was shown for both Ger-
man and English. These innovations enable the
rapid development of a spellchecking and correc-
tion system for any language for which tokeniz-
ers exist and string edit distances make sense. We
have done so for Arabic and Russian.
In this paper, our results were obtained without
any optimization of the parameters used in the pro-
cess of gathering data from the Web. We wanted to
minimize manual tweaking particularly if it were
necessary for every language. Thus heuristics such
as the number of terms in the term list, the criteria
for filtering triples, and the edit distance for defin-
ing close words were crude, and could easily be
improved upon. It may be beneficial to perform
more tuning in future. Furthermore, future work
will involve evaluating the performance of the sys-
tem for these language on real typed data.
7 Acknowledgment
We would like to thank the anonymous reviewers
for their useful feedback and suggestions. We also
thank our colleagues who participated in the data
collection.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs.
In HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 955?962,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
K. Atkinson. 2009. Gnu aspell. In Available at
http://aspell.net.
Loghman Barari and Behrang QasemiZadeh. 2005.
Clonizer spell checker adaptive, language indepen-
dent spell checker. In Ashraf Aboshosha et al, ed-
itor, Proc. of the first ICGST International Confer-
898
ence on Artificial Intelligence and Machine Learn-
ing AIML 05, volume 05, pages 65?71, Cairo, Egypt,
Dec. ICGST, ICGST.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction.
In ACL ?00: Proceedings of the 38th Annual Meet-
ing on Association for Computational Linguistics,
pages 286?293. Association for Computational Lin-
guistics.
S. Cucerzan and E. Brill. 2004. Spelling correction
as an iterative process that exploits the collective
knowledge of web users. In Proceedings of EMNLP
2004, pages 293?300.
F.J. Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tions of the ACM 7, pages 171?176.
S. Deorowicz and M.G. Ciura. 2005. Correcting
spelling errors by modelling their causes. Interna-
tional Journal of Applied Mathematics and Com-
puter Science, 15(2):275?285.
Andrew R. Golding and Yves Schabes. 1996. Com-
bining trigram-based and feature-based methods for
context-sensitive spelling correction. In In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 71?78.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language independent text correction using
finite state automata. In Proceedings of the 2008 In-
ternational Joint Conference on Natural Language
Processing (IJCNLP, 2008).
Mark D. Kernighan, Kenneth W. Church, and
William A. Gale. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proceed-
ings of the 13th conference on Computational lin-
guistics, pages 205?210. Association for Computa-
tional Linguistics.
K. Kukich. 1992. Techniques for automatically cor-
recting words in texts. ACM Computing Surveys 24,
pages 377?439.
Mirella Lapata and Frank Keller. 2004. The web as
a baseline: Evaluating the performance of unsuper-
vised web-based models for a range of nlp tasks. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
121?128, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Lidia Mangu and Eric Brill. 1997. Automatic rule
acquisition for spelling correction. In Douglas H.
Fisher, editor, ICML, pages 187?194. Morgan Kauf-
mann.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Informa-
tion Processing and Management, 27(5):517.
M.W.C. Reynaert. 2008. All, and only, the errors:
More complete and consistent spelling and ocr-error
correction evaluation. In Proceedings of the sixth
international language resources and evaluation.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 144?151.
L. Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-
der Budanitsky. 2008. Real-word spelling cor-
rection with trigrams: A reconsideration of the
mays, damerau, and mercer model. In Alexan-
der F. Gelbukh, editor, CICLing, volume 4919 of
Lecture Notes in Computer Science, pages 605?616.
Springer.
Wei Xiang Yang Zhang, Pilian He and Mu Li. 2007.
Discriminative reranking for spelling correction. In
The 20th Pacific Asia Conference on Language, In-
formation and Computation.
899
Intrinsic versus Extrinsic Evaluations of Parsing Systems
Diego Molla?
Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2109, Australia
diego@ics.mq.edu.au
Ben Hutchinson
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW, United Kingdom
B.Hutchinson@sms.ed.ac.uk
Abstract
A wide range of parser and/or grammar
evaluation methods have been reported
in the literature. However, in most cases
these evaluations take the parsers in-
dependently (intrinsic evaluations), and
only in a few cases has the effect
of different parsers in real applications
been measured (extrinsic evaluations).
This paper compares two evaluations
of the Link Grammar parser and the
Conexor Functional Dependency Gram-
mar parser. The parsing systems, de-
spite both being dependency-based, re-
turn different types of dependencies,
making a direct comparison impossi-
ble. In the intrinsic evaluation, the accu-
racy of the parsers is compared indepen-
dently by converting the dependencies
into grammatical relations and using the
methodology of Carroll et al (1998) for
parser comparison. In the extrinsic eval-
uation, the parsers? impact in a practi-
cal application is compared within the
context of answer extraction. The dif-
ferences in the results are significant.
1 Introduction
Parsing is a principal stage in many natural lan-
guage processing (NLP) systems. A good parser is
expected to return an accurate syntactic structure
of a sentence. This structure is typically forwarded
to other modules so that they can work with un-
ambiguous and well-defined structures represent-
ing the sentences. It is to be expected that the
performance of an NLP system quickly degrades
if the parsing system returns incorrect syntactic
structures, and therefore an evaluation of parsing
coverage and accuracy is important.
According to Galliers and Sparck Jones (1993),
there are two main criteria in performance evalua-
tion: ?Intrinsic criteria are those relating to a sys-
tem?s objective, extrinsic criteria those relating to
its function i.e. to its role in relation to its setup?s
purpose.? (Galliers and Sparck Jones, 1993, p22).
Thus, an intrinsic evaluation of a parser would
analyse the accuracy of the results returned by the
parser as a stand-alone system, whereas an ex-
trinsic evaluation would analyse the impact of the
parser within the context of a broader NLP appli-
cation.
There are currently several parsing
systems that attempt to achieve a wide
coverage of the English language (such
as those developed by Collins (1996),
Ja?rvinen and Tapanainen (1997), and
Sleator and Temperley (1993)). There is also
substantial literature on parsing evaluation (see,
for example, work by Sutcliffe et al (1996),
Black (1996), Carroll et al (1998), and
Bangalore et al (1998)). Recently there has
been a shift from constituency-based (e.g. count-
ing crossing brackets (Black et al, 1991)) to
dependency-based evaluation (Lin, 1995; Carroll
et al, 1998). Those evaluation methodologies
typically focus on comparisons of stand-alone
parsers (intrinsic evaluations). In this paper we
report on the comparison between an intrinsic
evaluation and an evaluation of the impact of
the parser in a real application (an extrinsic
evaluation).
We have chosen answer extraction as an exam-
ple of a practical application within which to test
the parsing systems. In particular, the extrinsic
evaluation uses ExtrAns, an answer extraction sys-
tem that operates over Unix manual pages (Molla?
et al, 2000). The two grammar systems to com-
pare are Link Grammar (Sleator and Temperley,
1993) and the Conexor Functional Dependency
Grammar parser (Tapanainen and Ja?rvinen, 1997)
(henceforth referred to as Conexor FDG). These
parsing systems were chosen because both include
a dependency-based parser and a comprehensive
grammar of English. However, the structures re-
turned are so different that a direct comparison be-
tween them is not straightforward. In Section 2 we
review the main differences between Link Gram-
mar and Conexor FDG. In Section 3 we present
the intrinsic comparison of parsers, and in Sec-
tion 4 we comment on the extrinsic comparison
within the context of answer extraction. The re-
sults of the evaluations are discussed in Section 5.
2 Link Grammar and Conexor FDG
Link Grammar (Sleator and Temperley, 1993) is
a grammar theory that is strongly dependency-
based. A freely available parsing system that im-
plements the Link Grammar theory has been de-
veloped at Carnegie Mellon University. The pars-
ing system includes an extensive grammar and lex-
icon and has a wide coverage of the English lan-
guage. Conexor FDG (Tapanainen and Ja?rvinen,
1997) is a commercial parser and grammar, based
on the theory of Functional Dependency Gram-
mar, and was originally developed at the Univer-
sity of Helsinki.
Despite both being dependency-based, there are
substantial differences between the structures re-
turned by the two parsers. Figure 1 shows Link
Grammar?s output for a sample sentence, and Fig-
ure 2 shows the dependency structure returned
by Conexor FDG for comparison. Table 1 ex-
plains the dependency types used in the depen-
dency structures of the figures.
The differences between the dependency struc-
tures returned by Link Grammar 2.1 and Conexor
FDG 3.6 can be summarised as follows.
Direction of dependency: Link Grammar?s
?links?, although similar to true dependencies, do
not state which participant is the head and which
is the dependent. However, Link Grammar uses
different link types for head-right links and head-
left links, so this information can be recovered.
Conexor FDG always indicates the direction of the
dependence.
Clausal heads: Link Grammar generally
chooses the front-most element to be the head
of a clause, rather than the main verb. This is
true of both matrix and subordinate clauses, as
exemplified by the Wd and R links in Figure 1.
Conexor FDG follows the orthodox convention of
choosing the main verb as the head of the clause.
Graph structures: Link Grammar?s links com-
bine dependencies at the surface-syntactic and
deep-syntactic levels (e.g., the link Bs, which
links a noun modified by a subject-type relative
clause to the relative clause?s head verb, in Fig-
ure 1 indicates a deep-syntactic dependency). The
resulting structures are graphs rather than trees.
An example is shown in Figure 1, where the noun
man modified by a relative clause is linked to both
the complementiser and the head verb of the rela-
tive clause.
Conjunctions: Our version of Link Grammar
analyses a coordinating conjunction as the head of
a coordinated phrase (Figure 1). This is a modifi-
cation of Link Grammar?s default behaviour which
returns a list of parses, one parse per conjunct.
However in Conexor FDG?s analyses the head will
be either the first or the last conjunct, depending
on whether the coordinated phrase?s head lies to
the left or to the right (Figure 2).
Dependency types: Link Grammar uses a set of
about 90 link types and many subtypes, which ad-
dress very specific syntactic constructions (e.g. the
link type EB connects adverbs to forms of be be-
fore a noun phrase or prepositional phrase: He
is APPARENTLY a good programmer). On the
other hand, Conexor FDG uses a set of 32 de-
///// the man.n that came.v ate.v bananas.n and apples.n with a fork.n1
Wd
Ds
Ss
Bs
R RS
MVp
O^ Js
Ds
Figure 1: Output of Link Grammar.
///// the man that came ate bananas and apples with a fork
 main <
>det
> subj 
 mod<
>subj
 ins <
obj< cc<
 cc <  pcomp<
>det
Figure 2: Dependency structure returned by Conexor FDG.
pendency relations, ranging from traditional gram-
matical functions (e.g. subject, object), to specific
types of modifiers (e.g. frequency, duration, loca-
tion).
Both Conexor FDG and Link Grammar also
return non-dependency information. For Link
Grammar, this consists of some word class in-
formation, shown as suffixes in Figure 1. For
Conexor FDG, the base form morphological in-
formation of each word is returned, along with a
?functional? tag or morpho-syntactic function and
a ?surface syntactic? tag for each word.1
3 Intrinsic Evaluations
Given that both parses are dependency-based, in-
trinsic evaluations that are based on constituency
structures (e.g. (Black et al, 1991)) are hard
to perform. Dependency-based evaluations are
not easy either: directly comparing dependency
graphs (as suggested by Lin (1995), for exam-
ple) becomes difficult given the differences be-
tween the structures returned by the Link Gram-
mar parser and Conexor FDG. We there-
fore need an approach that is independent from
the format of the parser output. Following
Carroll et al (1998) we use grammatical relations
to compare the accuracy of Link Grammar and
Conexor FDG. Carroll et al (1998) propose a set
of twenty parser-independent grammatical rela-
tions arranged in a hierarchy representing differ-
ent degrees of specificity. Four relations from the
hierarchy are shown in Table 2. The arguments to
1See (Ja?rvinen and Tapanainen, 1997) for more informa-
tion on the output from Conexor FDG.
each relation specify a head, a dependent, and pos-
sibly an initial grammatical relation (in the case
of SUBJ in passive sentences, for example) or the
?type?, which specifies the word introducing the
dependent (in the case of XCOMP).
For example, the grammatical relations of the
sentence the man that came ate bananas and ap-
ples with a fork without asking has the following
relations:
SUBJ(eat,man, ),
OBJ(eat,banana),
OBJ(eat,apple),
MOD(fork,eat,with),
SUBJ(come,man, ),
MOD(that,man,come),
XCOMP(without,eat,ask)
The terms ?head? and ?dependent? used
by Carroll et al (1998) to refer to the arguments
of grammatical relations should not be con-
fused with the similar terms in the theory of
dependency grammar. Grammatical relations
and dependency arcs represent different phe-
nomena. An example should suffice to illustrate
the difference; consider The man that came ate
bananas and apples with a fork. In dependency
grammar a unique head is assigned to each word,
for example the head of man is ate. However
man is the dependent of more than one gram-
matical relation, namely SUBJ(eat,man, )
and SUBJ(come,man, ). Furthermore, in
dependency grammar a word can have at most
one dependent of each argument type, and so ate
can have at most one object, for example. But
Link Grammar Conexor FDG
Name Description Name Description
Bs Singular external object of relative clause cc Coordination
Ds Singular determiner det Determiner
Js Singular object of a preposition ins <not documented>
MVp Verb-modifying preposition main Main element
O? Object mod General post-modifier
R Relative clause obj Object
RS Part of subject-type relative clause pcomp Prepositional complement
Ss Singular subject subj Subject
Wd Declarative sentence
Table 1: Some of the dependency types used by Link Grammar and Conexor FDG.
Relation Description
SUBJ(head, dependent, initial gr) Subject
OBJ(head, dependent) Object
XCOMP(type, head, dependent) Clausal complement without an overt subject
MOD(type, head, dependent) Modifier
Table 2: Grammatical relations used in the intrinsic evaluation.
the same is not true for grammatical relations,
and we get both OBJ(eat,banana) and
OBJ(eat,apple).
3.1 Accuracy
Our intrinsic evaluation began on the assumption
that grammatical relations could be deduced from
the dependency structures returned by the parsers.
In practise, however, this deduction process is not
always straightforward; for example complexity
arises when arguments are shared across clauses.
In addition, Link Grammar?s analysis of the front-
most elements as clausal heads complicates the
grammatical relation deduction when there are
modifying clauses.
An existing corpus of 500 sentences/10,000
words annotated with grammatical relations was
used for the evaluation (Carroll et al, 1999). We
restricted the evaluation to just the four relations
shown in Table 2. This decision had two motiva-
tions. Firstly, since the dependency parsers? out-
put did not recognise some distinctions made in
the hierarchy of relations, it did not make sense to
test these distinctions. Secondly, we wanted the
deduction of grammatical relations to be as simple
a process as possible, to minimise the chance of
introducing errors. This second consideration also
led us to purposefully ignore the sharing of argu-
ments induced by control verbs, as this could not
always be deduced reliably. Since this was done
for both parsers the comparison remains meaning-
ful.
Algorithms for producing grammatical relations
from Link Grammar and Conexor FDG output
were developed and implemented. The results of
parsing the corpus are shown in Table 3. Since
Conexor FDG returns one parse per sentence only
and Link Grammar returns all parses ranked, the
first (i.e. the best) parse returned by Link Gram-
mar was used in the intrinsic evaluation.
The table shows significantly lower values of
recall and precision for Link Grammar. This is
partly due to the fact that Link Grammar?s links
often do not connect the head of the clause, as we
have seen with the Wd link in Figure 1.
3.2 Speed
Link Grammar took 1,212 seconds to parse the
10,000 word corpus, while Conexor FDG took
20.5 seconds. This difference is due partly to the
fact that Link Grammar finds and returns multiple
(and often many) alternative parses. For example,
With Link
Grammar
With
Conexor
FDG
Precision SUBJ 50.3% 73.6%
OBJ 48.5% 84.8%
XCOMP 62.2% 76.2%
MOD 57.2% 63.7%
Average 54.6% 74.6%
Recall SUBJ 39.1% 64.5%
OBJ 50% 53.4%
XCOMP 32.1% 64.7%
MOD 53.7% 56.2%
Average 43.7% 59.7%
Table 3: Accuracy of identification of grammatical
relations.
Link Grammar found a total of 410,509 parses of
the 505 corpus sentences.
4 Extrinsic Evaluations
It is important to know not only the accuracy of
a parser but how possible parsing errors affect the
success of an NLP application. This is the goal of
an extrinsic evaluation, where the system is eval-
uated in relation to the embedding setup. Using
answer extraction as an example of an NLP appli-
cation, we compared the performance of the Link
Grammar system and Conexor FDG.
4.1 Answer Extraction and ExtrAns
The fundamental goal of Answer Extraction (AE)
is to locate those exact phrases of unedited text
documents that answer a query worded in nat-
ural language. AE has received much attention
recently, as the increasingly active Question An-
swering track in TREC demonstrates (Voorhees,
2001b; Voorhees, 2001a).
ExtrAns is an answer extraction system that
operates over UNIX manual pages (Molla? et al,
2000). A core process in ExtrAns is the produc-
tion of semantic information in the shape of logi-
cal forms for each sentence of each manual page,
as well as the user query. These logical forms are
designed so that they can be derived from any sen-
tence (using robust approaches to treat very com-
plex or ungrammatical sentences), and they are op-
timised for NLP tasks that involve the semantic
comparison of sentences, such as AE.
ExtrAns? logical forms are called minimal log-
ical forms (MLFs) because they encode the mini-
mum information required for effective answer ex-
traction. In particular, only the main dependencies
between the verb and arguments are expressed,
plus modifier and adjunct relations. Thus, com-
plex quantification, tense and aspect, temporal re-
lations, plurality, and modality are not expressed.
The MLFs use reification to achieve flat expres-
sions, very much in the line of Davidson (1967),
Hobbs (1985), and Copestake et al (1997). In the
current implementation only reification to objects,
eventualities (events or states), and properties is
applied. For example, the MLF of the sentence cp
will quickly copy files is:
holds(e4),
object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type command;2 there is an
entity x6 (a file); there is an entity e4, which rep-
resents a copying event where the first argument
is x1 and the second argument is x6; there is an
entity p3which states that e4 is done quickly, and
the event e4, that is, the copying, holds.
ExtrAns finds the answers to the questions by
converting the MLFs of the questions into Prolog
queries and then running Prolog?s default resolu-
tion mechanism to find those MLFs that can prove
the question.
This default search procedure is called the syn-
onym mode since ExtrAns uses a small WordNet-
style thesaurus (Fellbaum, 1998) to convert all the
synonyms into a synonym representative. Extr-
Ans also has an approximate mode which, be-
sides normalising all synonyms, scores all docu-
ment sentences on the basis of the maximum num-
ber of predicates that unify between the MLFs of
the query and the answer candidate (Molla? et al,
2000). If all query predicates can be matched then
2ExtrAns uses additional domain knowledge to infer that
cp is a command.
the approximate mode returns exactly the same an-
swers as the synonym mode.
4.2 The Comparison
Ideally, answer extraction systems should be eval-
uated according to how successful they are in help-
ing users to complete their tasks. The use of the
system will therefore depend on such factors as
how many potential answers the user is presented
with at a time, the way these potential answers are
ranked, how many potential answers the user is
prepared to read while searching for an actual an-
swer, and so on. These issues, though important,
are beyond the scope of the present evaluation. In
this evaluation we focus solely on the relevance of
the set of results returned by ExtrAns.
4.2.1 Method
Resources from a previous evaluation of Extr-
Ans (Molla? et al, 2000) were re-used for this eval-
uation. These resources were: a) a collection of
500 man pages, and b) a test set of 26 queries and
relevant answers found in the 500 manual pages.
The careful and labour-intensive construction of
the test set gives us confidence that practically all
relevant answers to each query are present in the
test set. The queries themselves were selected ac-
cording to the following criteria:
? There must be at least one answer in the man-
ual page collection.
? The query asks how to perform a particular
action, or how a particular command works.
? The query is simple, i.e. it asks only one
question.
The manual pages were parsed using Conexor
FDG and Link Grammar. The latter has a param-
eter for outputting either all parses found, or just
the best parse found, and both parameter settings
were used. The queries were then parsed by both
parsers and their logical forms were used to search
the respective databases. The experiment was re-
peated using both the synonym and approximate
search modes.
Parser Precision4 Recall F-score
Conexor FDG 55.8% 8.9% 0.074
LG?best 49.7% 11.4% 0.099
LG?all 50.9% 13.1% 0.120
Table 4: Averages per query in synonym mode.
Parser Precision4 Recall F-score
Conexor FDG 28.3% 21.9% 0.177
LG?best 31.8% 15.8% 0.150
LG?all 40.5% 20.5% 0.183
Table 5: Averages per query in approximate mode.
4.2.2 Results
Precision, Recall and the F-score (with Preci-
sion and Recall equally weighted) for each query
were calculated.3 When no results were returned
for a query the precision could not be calculated,
but the F-score is equal to zero. The results are
shown in Tables 4 and 5. The number of times the
results for a query contained no relevant answers
are shown in Table 6.
The tables show that the approximate mode
gives better results than the synonym mode. This
is to be expected, since the synonym mode returns
exact matches only and therefore some questions
may not produce any results. For those questions,
recall and F would be zero. In fact, the number of
questions without answers in the synonym mode
is so large that the comparison between Conexor
FDG and Link Grammar becomes unreliable in
this mode. In this discussion, therefore, we will
focus on the approximate mode.
The results returned by Link Grammar when all
parses are considered are significantly better than
when only the first (i.e. the best) parse is consid-
3F was calculated using the expression
F = 2 ? |returned and relevant||returned| + |relevant|
which is equivalent to the usual formulation (with ? = 1):
F = (?2 + 1) ? Precision ? Recall?2Precision + Recall
4Average over queries for which precision is defined, i.e.
when the number of returns is non-zero.
Parser Search mode No
results
returned
Nothing
relevant
returned
Con. FDG Synonym 20 20
Con. FDG Approximate 0 8
LG?best Synonym 16 18
LG?best Approximate 1 11
LG?all Synonym 15 18
LG?all Approximate 4 12
Table 6: Numbers of times no relevant answers
were found.
ered. This shows that, in the answer extraction
task, it is better to use the logical forms of all
possible sentence interpretations. Recall increases
and, remarkably, precision increases as well. This
means that the system is more likely to include
new relevant answers when all parses are consid-
ered.
In many applications it is more practical to con-
sider one parse only. Conexor FDG, for example,
returns one parse only, and the parsing speed com-
parison (Section 3.2) shows an important differ-
ence in parsing time. If we compare Conexor FDG
with Link Grammar set to return just the best parse
? since Conexor FDG returns one parse only, this
is the fairest comparison ? we can see that recall
of the system using Conexor FDG is higher than
that of the system using Link Grammar, while re-
taining similar precision.
5 Discussion
The fairest extrinsic comparison between Conexor
FDG and Link Grammar is the one that uses the
best parse returned by Link Grammar, and the an-
swer extraction method follows the approximate
mode. With these settings, Conexor FDG pro-
duces better results than Link Grammar. However,
the results of the extrinsic comparison are far less
dramatic than those of the intrinsic comparison,
specially in the precision figures.
One reason for the difference in the results is
that the intrinsic evaluation compares grammatical
relation accuracy, whereas the answer extraction
system used in the extrinsic evaluation uses logi-
cal forms. A preliminary inspection of the gram-
matical relations and logical forms of questions
and correct answers shows that high overlap of
grammatical relations does not translate into high
overlap of logical forms. A reason for this differ-
ence is that the semantic interpreters used in the
extrinsic evaluation explore exhaustively the de-
pendency structures returned by both parsing sys-
tems and they try to recover as much information
as possible. In contrast with this, the generators of
grammatical relations used in the intrinsic evalua-
tion provide the most direct mapping from depen-
dency structures to grammatical relations. For ex-
ample, typically a dependency structure would not
show a long dependency like the subject of come
in the sentence John wanted Mary to come:
John wanted.v Mary to.o come.v
Ss
TOo
Os I
As a result, the grammatical relations would not
show the subject of come. However, the subject
of come can be traced by following several de-
pendencies (I, TOo and Os above) and ExtrAns?
semantic interpreters do follow these dependen-
cies. In other words, the semantic interpreters
use more information than what is directly en-
coded in the dependency structures. Therefore,
the logical forms contain richer information than
the grammatical relations. We decided not to op-
timise the grammatical relations used in our eval-
uation because we wanted to test the expressivity
of the inherent grammars. It would be question-
able whether we should recover more information
than what is directly expressed. After all, provided
that the parse contains all the words in the origi-
nal order, we can theoretically ignore the sentence
structure and still recover all the information.
6 Summary and Further Work
We have performed intrinsic evaluations of
parsers and extrinsic evaluations within the
context of answer extraction. These evaluations
strengthen Galliers and Sparck Jones (1993)?s
claim that intrinsic evaluations are of very limited
value. In particular, our evaluations show that
intrinsic evaluations may provide results that
are distorted with respect to the most intuitive
purpose of a parsing system: to deliver syntactic
structures to subsequent modules of practical NLP
systems. There is a clear need for frameworks for
extrinsic evaluations of parsers for different NLP
applications.
Further research to confirm this conclusion will
be to try and minimise the occurrence of vari-
ables in the experiments by using the same corpus
for both the intrinsic and the extrinsic evaluations
and/or by using an answer extraction system that
operates on the level of grammatical relations in-
stead of MLFs. Additional further research will
be the use of other intrinsic evaluation methodolo-
gies and extrinsic evaluations within the context of
various other embedding setups.
Acknowledgement
This research is supported by the Macquarie Uni-
versity New Staff grant MUNS?9601/0069.
References
Srinivas Bangalore, Anoop Sarkar, Christine Doran,
and Beth Ann Hockey. 1998. Grammar & parser
evaluation in the XTAG project. In Proc. Workshop
on the Evaluation of Parsing Systems, LREC98.
Ezra Black, S.P. Abney, D. Flickinger, C. Gdaniec,
R. Grisham, P. Harrison, D. Hindle, R. Ingria,
F. Jelinek, J. Klavans, M. Liberman, M.P. Mar-
cus, S. Roukos, B. Santorini, and T. Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proc.
DARPA Speech and Natural Language Workshop,
pages 306?311, Pacific Grove, CA. Morgan Kauf-
mann.
Ezra Black. 1996. Evaluation of broad-coverage
natural-language parsers. In Ronald A. Cole, Joseph
Mariani, Hans Uszkoreit, Annie Zaenen, and Victor
Zue, editors, Survey of the State of the Art in Hu-
man Language Technology, pages 488?490. CSLU,
Oregon Graduate Institute.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proc. LREC98.
John Carroll, G. Minnen, and T. Briscoe. 1999. Corpus
annotation for parser evaluation.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc.
ACL. Santa Cruz.
Ann Copestake, Dan Flickinger, and Ivan A. Sag.
1997. Minimal recursion semantics: an introduc-
tion. Technical report, CSLI, Stanford University,
Stanford, CA.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81?120. Univ. of Pitts-
burgh Press.
Christiane Fellbaum. 1998. Wordnet: Introduction. In
Christiane Fellbaum, editor, WordNet: an electronic
lexical database, Language, Speech, and Communi-
cation, pages 1?19. MIT Press, Cambrige, MA.
Julia R. Galliers and Karen Sparck Jones. 1993. Evalu-
ating natural language processing systems. Techni-
cal Report TR-291, Computer Laboratory, Univer-
sity of Cambridge.
Jerry R. Hobbs. 1985. Ontological promiscuity. In
Proc. ACL?85, pages 61?69. University of Chicago,
Association for Computational Linguistics.
Timo Ja?rvinen and Pasi Tapanainen. 1997. A depen-
dency parser for english. Technical Report TR-1,
Department of Linguistics, University of Helsinki,
Helsinki.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proc. IJCAI-
95, pages 1420?1425, Montreal, Canada.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000. Extrans, an answer extrac-
tion system. T.A.L., 41(2):495?522.
Daniel D. Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Proc. Third Inter-
national Workshop on Parsing Technologies, pages
277?292.
Richard F. E. Sutcliffe, Heinz-Detlev Koch, and An-
nette McElligott, editors. 1996. Industrial Parsing
of Software Manuals. Rodopi, Amsterdam.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Procs. ANLP-97.
ACL.
Ellen M. Voorhees. 2001a. Overview of the TREC
2001 question answering track. In Ellen M.
Voorhees and Donna K. Harman, editors, Proc.
TREC-10, number 500-250 in NIST Special Publi-
cation. NIST.
Ellen M. Voorhees. 2001b. The TREC question
answering track. Natural Language Engineering,
7(4):361?378.
