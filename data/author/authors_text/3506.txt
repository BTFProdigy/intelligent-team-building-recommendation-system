Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 537?544,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Study on Automatically Extracted Keywords in Text Categorization
Anette Hulth and Bea?ta B. Megyesi
Department of Linguistics and Philology
Uppsala University, Sweden
anette.hulth@gmail.com bea@stp.lingfil.uu.se
Abstract
This paper presents a study on if and how
automatically extracted keywords can be
used to improve text categorization. In
summary we show that a higher perfor-
mance ? as measured by micro-averaged
F-measure on a standard text categoriza-
tion collection ? is achieved when the
full-text representation is combined with
the automatically extracted keywords. The
combination is obtained by giving higher
weights to words in the full-texts that
are also extracted as keywords. We also
present results for experiments in which
the keywords are the only input to the cat-
egorizer, either represented as unigrams
or intact. Of these two experiments, the
unigrams have the best performance, al-
though neither performs as well as head-
lines only.
1 Introduction
Automatic text categorization is the task of assign-
ing any of a set of predefined categories to a doc-
ument. The prevailing approach is that of super-
vised machine learning, in which an algorithm is
trained on documents with known categories. Be-
fore any learning can take place, the documents
must be represented in a form that is understand-
able to the learning algorithm. A trained predic-
tion model is subsequently applied to previously
unseen documents, to assign the categories. In
order to perform a text categorization task, there
are two major decisions to make: how to repre-
sent the text, and what learning algorithm to use
to create the prediction model. The decision about
the representation is in turn divided into two sub-
questions: what features to select as input and
which type of value to assign to these features.
In most studies, the best performing representa-
tion consists of the full length text, keeping the
tokens in the document separate, that is as uni-
grams. In recent years, however, a number of ex-
periments have been performed in which richer
representations have been evaluated. For exam-
ple, Caropreso et al (2001) compare unigrams
and bigrams; Moschitti et al (2004) add com-
plex nominals to their bag-of-words representa-
tion, while Kotcz et al (2001), and Mihalcea and
Hassan (2005) present experiments where auto-
matically extracted sentences constitute the input
to the representation. Of these three examples,
only the sentence extraction seems to have had any
positive impact on the performance of the auto-
matic text categorization.
In this paper, we present experiments in which
keywords, that have been automatically extracted,
are used as input to the learning, both on their own
and in combination with a full-text representation.
That the keywords are extracted means that the se-
lected terms are present verbatim in the document.
A keyword may consist of one or several tokens.
In addition, a keyword may well be a whole ex-
pression or phrase, such as snakes and ladders.
The main goal of the study presented in this pa-
per is to investigate if automatically extracted key-
words can improve automatic text categorization.
We investigate what impact keywords have on the
task by predicting text categories on the basis of
keywords only, and by combining full-text repre-
sentations with automatically extracted keywords.
We also experiment with different ways of rep-
resenting keywords, either as unigrams or intact.
In addition, we investigate the effect of using the
headlines ? represented as unigrams ? as input,
537
to compare their performance to that of the key-
words.
The outline of the paper is as follows: in Section
2, we present the algorithm used to automatically
extract the keywords. In Section 3, we present the
corpus, the learning algorithm, and the experimen-
tal setup for the performed text categorization ex-
periments. In Section 4, the results are described.
An overview of related studies is given in Section
5, and Section 6 concludes the paper.
2 Selecting the Keywords
This section describes the method that was used to
extract the keywords for the text categorization ex-
periments discussed in this paper. One reason why
this method, developed by Hulth (2003; 2004),
was chosen is because it is tuned for short texts
(more specifically for scientific journal abstracts).
It was thus suitable for the corpus used in the de-
scribed text categorization experiments.
The approach taken to the automatic keyword
extraction is that of supervised machine learning,
and the prediction models were trained on man-
ually annotated data. No new training was done
on the text categorization documents, but models
trained on other data were used. As a first step
to extract keywords from a document, candidate
terms are selected from the document in three dif-
ferent manners. One term selection approach is
statistically oriented. This approach extracts all
uni-, bi-, and trigrams from a document. The two
other approaches are of a more linguistic charac-
ter, utilizing the words? parts-of-speech (PoS), that
is, the word class assigned to a word. One ap-
proach extracts all noun phrase (NP) chunks, and
the other all terms matching any of a set of empir-
ically defined PoS patterns (frequently occurring
patterns of manual keywords). All candidate terms
are stemmed.
Four features are calculated for each candi-
date term: term frequency; inverse document fre-
quency; relative position of the first occurrence;
and the PoS tag or tags assigned to the candidate
term. To make the final selection of keywords,
the three predictions models are combined. Terms
that are subsumed by another keyword selected
for the document are removed. For each selected
stem, the most frequently occurring unstemmed
form in the document is presented as a keyword.
Each document is assigned at the most twelve key-
words, provided that the added regression value
Assign. Corr.
mean mean P R F
8.6 3.6 41.5 46.9 44.0
Table 1: The number of assigned (Assign.) key-
words in mean per document; the number of cor-
rect (Corr.) keywords in mean per document; pre-
cision (P); recall (R); and F-measure (F), when 3?
12 keywords are extracted per document.
(given by the prediction models) is higher than an
empirically defined threshold value. To avoid that
a document gets no keywords, at least three key-
words are assigned although the added regression
value is below the threshold (provided that there
are at least three candidate terms).
In Hulth (2004) an evaluation on 500 abstracts
in English is presented. For the evaluation, key-
words assigned to the test documents by profes-
sional indexers are used as a gold standard, that
is, the manual keywords are treated as the one
and only truth. The evaluation measures are preci-
sion (how many of the automatically assigned key-
words that are also manually assigned keywords)
and recall (how many of the manually assigned
keywords that are found by the automatic indexer).
The third measure used for the evaluations is the
F-measure (the harmonic mean of precision and
recall). Table 1 shows the result on that particu-
lar test set. This result may be considered to be
state-of-the-art.
3 Text Categorization Experiments
This section describes in detail the four experi-
mental settings for the text categorization exper-
iments.
3.1 Corpus
For the text categorization experiments we used
the Reuters-21578 corpus, which contains 20 000
newswire articles in English with multiple cate-
gories (Lewis, 1997). More specifically, we used
the ModApte split, containing 9 603 documents for
training and 3 299 documents in the fixed test set,
and the 90 categories that are present in both train-
ing and test sets.
As a first pre-processing step, we extracted the
texts contained in the TITLE and BODY tags. The
pre-processed documents were then given as in-
put to the keyword extraction algorithm. In Ta-
ble 2, the number of keywords assigned to the doc-
538
uments in the training set and the test set are dis-
played. As can be seen in this table, three is the
number of keywords that is most often extracted.
In the training data set, 9 549 documents are as-
signed keywords, while 54 are empty, as they have
no text in the TITLE or BODY tags. Of the 3 299
documents in the test set, 3 285 are assigned key-
words, and the remaining fourteen are those that
are empty. The empty documents are included in
the result calculations for the fixed test set, in or-
der to enable comparisons with other experiments.
The mean number of keyword extracted per docu-
ment in the training set is 6.4 and in the test set 6.1
(not counting the empty documents).
Keywords Training docs Test docs
0 54 14
1 68 36
2 829 272
3 2 016 838
4 868 328
5 813 259
6 770 252
7 640 184
8 527 184
9 486 177
10 688 206
11 975 310
12 869 239
Table 2: Number of automatically extracted key-
words per document in training set and test set re-
spectively.
3.2 Learning Method
The focus of the experiments described in this pa-
per was the text representation. For this reason, we
used only one learning algorithm, namely an im-
plementation of Linear Support Vector Machines
(Joachims, 1999). This is the learning method that
has obtained the best results in text categorization
experiments (Dumais et al, 1998; Yang and Liu,
1999).
3.3 Representations
This section describes in detail the input repre-
sentations that we experimented with. An impor-
tant step for the feature selection is the dimen-
sionality reduction, that is reducing the number
of features. This can be done by removing words
that are rare (that occur in too few documents or
have too low term frequency), or very common
(by applying a stop-word list). Also, terms may
be stemmed, meaning that they are merged into a
common form. In addition, any of a number of
feature selection metrics may be applied to further
reduce the space, for example chi-square, or infor-
mation gain (see for example Forman (2003) for a
survey).
Once that the features have been set, the final
decision to make is what feature value to assign.
There are to this end three common possibilities:
a boolean representation (that is, the term exists in
the document or not), term frequency, or tf*idf.
Two sets of experiments were run in which the
automatically extracted keywords were the only
input to the representation. In the first set, key-
words that contained several tokens were kept in-
tact. For example a keyword such as paradise fruit
was represented as paradise fruit and was
? from the point of view of the classifier ? just as
distinct from the single token fruit as from meat-
packers. No stemming was performed in this set
of experiments.
In the second set of keywords-only experiments,
the keywords were split up into unigrams, and also
stemmed. For this purpose, we used Porter?s stem-
mer (Porter, 1980). Thereafter the experiments
were performed identically for the two keyword
representations.
In a third set of experiments, we extracted only
the content in the TITLE tags, that is, the head-
lines. The tokens in the headlines were stemmed
and represented as unigrams. The main motiva-
tion for the title experiments was to compare their
performance to that of the keywords.
For all of these three feature inputs, we first
evaluated which one of the three possible feature
values to use (boolean, tf, or tf*idf). Thereafter,
we reduced the space by varying the minimum
number of occurrences in the training data, for a
feature to be kept.
The starting point for the fourth set of exper-
iments was a full-text representation, where all
stemmed unigrams occurring three or more times
in the training data were selected, with the feature
value tf*idf. Assuming that extracted keywords
convey information about a document?s gist, the
feature values in the full-text representation were
given higher weights if the feature was identical to
a keyword token. This was achieved by adding the
term frequency of a full-text unigram to the term
539
frequency of an identical keyword unigram. Note
that this does not mean that the term frequency
value was necessarily doubled, as a keyword often
contains more than one token, and it was the term
frequency of the whole keyword that was added.
3.4 Training and Validation
This section describes the parameter tuning, for
which we used the training data set. This set
was divided into five equally sized folds, to de-
cide which setting of the following two parameters
that resulted in the best performing classifier: what
feature value to use, and the threshold for the min-
imum number of occurrence in the training data
(in this particular order).
To obtain a baseline, we made a full-text uni-
gram run with boolean as well as with tf*idf fea-
ture values, setting the occurrence threshold to
three.
As stated previously, in this study, we were
concerned only with the representation, and more
specifically with the feature input. As we did not
tune any other parameters than the two mentioned
above, the results can be expected to be lower than
the state-of-the art, even for the full-text run with
unigrams.
The number of input features for the full-text
unigram representation for the whole training set
was 10 676, after stemming and removing all to-
kens that contained only digits, as well as those
tokens that occurred less than three times. The
total number of keywords assigned to the 9 603
documents in the training data was 61 034. Of
these were 29 393 unique. When splitting up the
keywords into unigrams, the number of unique
stemmed tokens was 11 273.
3.5 Test
As a last step, we tested the best performing rep-
resentations in the four different experimental set-
tings on the independent test set.
The number of input features for the full-text
unigram representation was 10 676. The total
number of features for the intact keyword repre-
sentation was 4 450 with the occurrence thresh-
old set to three, while the number of stemmed
keyword unigrams was 6 478, with an occurrence
threshold of two. The total number of keywords
extracted from the 3 299 documents in the test set
was 19 904.
Next, we present the results for the validation
and test procedures.
4 Results
To evaluate the performance, we used precision,
recall, and micro-averaged F-measure, and we let
the F-measure be decisive. The results for the 5-
fold cross validation runs are shown in Table 3,
where the values given are the average of the five
runs made for each experiment. As can be seen
in this table, the full-text run with a boolean fea-
ture value gave 92.3% precision, 69.4% recall, and
79.2% F-measure. The full-text run with tf*idf
gave a better result as it yielded 92.9% precision,
71.3% recall, and 80.7% F-measure. Therefore we
defined the latter as baseline.
In the first type of the experiment where each
keyword was treated as a feature independently
of the number of tokens contained, the recall
rates were considerably lower (between 32.0%
and 42.3%) and the precision rates were somewhat
lower (between 85.8% and 90.5%) compared to
the baseline. The best performance was obtained
when using a boolean feature value, and setting the
minimum number of occurrence in training data to
three (giving an F-measure of 56.9%).
In the second type of experiments, where
the keywords were split up into unigrams and
stemmed, recall was higher but still low (between
60.2% and 64.8%) and precision was somewhat
lower (88.9?90.2%) when compared to the base-
line. The best results were achieved with a boolean
representation (similar to the first experiment) and
the minimum number of occurrence in the training
data set to two (giving an F-measure of 75.0%)
In the third type of experiments, where only the
text in the TITLE tags was used and was repre-
sented as unigrams and stemmed, precision rates
increased above the baseline to 93.3?94.5%. Here,
the best representation was tf*idf with a token oc-
curring at least four times in the training data (with
an F-measure of 79.9%).
In the fourth and last set of experiments, we
gave higher weights to full-text tokens if the same
token was present in an automatically extracted
keyword. Here we obtained the best results. In
these experiments, the term frequency of a key-
word unigram was added to the term frequency
for the full-text features, whenever the stems were
identical. For this representation, we experi-
mented with setting the minimum number of oc-
currence in training data both before and after that
the term frequency for the keyword token was
added to the term frequency of the unigram. The
540
Input feature Feature value Min. occurrence Precision Recall F-measure
full-text unigram bool 3 92.31 69.40 79.22
full-text unigram tf*idf 3 92.89 71.30 80.67
keywords-only intact bool 1 90.54 36.64 52.16
keywords-only intact tf 1 88.68 33.74 48.86
keywords-only intact tf*idf 1 89.41 32.05 47.18
keywords-only intact bool 2 89.27 40.43 55.64
keywords-only intact bool 3 87.11 42.28 56.90
keywords-only intact bool 4 85.81 41.97 56.35
keywords-only unigram bool 1 89.12 64.61 74.91
keywords-only unigram tf 1 89.89 60.23 72.13
keywords-only unigram tf*idf 1 90.17 60.36 72.31
keywords-only unigram bool 2 89.02 64.83 75.02
keywords-only unigram bool 3 88.90 64.82 74.97
title bool 1 94.17 68.17 79.08
title tf 1 94.37 67.89 78.96
title tf*idf 1 94.46 68.49 79.40
title tf*idf 2 93.92 69.19 79.67
title tf*idf 3 93.75 69.65 79.91
title tf*idf 4 93.60 69.74 79.92
title tf*idf 5 93.31 69.40 79.59
keywords+full tf*idf 3 (before adding) 92.73 72.02 81.07
keywords+full tf*idf 3 (after adding) 92.75 71.94 81.02
Table 3: The average results from 5-fold cross validations for the baseline candidates and the four types
of experiments, with various parameter settings.
highest recall (72.0%) and F-measure (81.1%) for
all validation runs were achieved when the occur-
rence threshold was set before the addition of the
keywords.
Next, the results on the fixed test data set for
the four experimental settings with the best per-
formance on the validation runs are presented.
Table 4 shows the results obtained on the fixed
test data set for the baseline and for those experi-
ments that obtained the highest F-measure for each
one of the four experiment types.
We can see that the baseline ? where the full-
text is represented as unigrams with tf*idf as fea-
ture value ? yields 93.0% precision, 71.7% re-
call, and 81.0% F-measure. When the intact key-
words are used as feature input with a boolean fea-
ture value and at least three occurrences in train-
ing data, the performance decreases greatly both
considering the correctness of predicted categories
and the number of categories that are found.
When the keywords are represented as uni-
grams, a better performance is achieved than when
they are kept intact. This is in line with the find-
ings on n-grams by Caropreso et al (2001). How-
ever, the results are still not satisfactory since both
the precision and recall rates are lower than the
baseline.
Titles, on the other hand, represented as uni-
grams and stemmed, are shown to be a useful in-
formation source when it comes to correctly pre-
dicting the text categories. Here, we achieve the
highest precision rate of 94.2% although the recall
rate and the F-measure are lower than the baseline.
Full-texts combined with keywords result in the
highest recall value, 72.9%, as well as the highest
F-measure, 81.7%, both above the baseline.
Our results clearly show that automatically ex-
tracted keywords can be a valuable supplement to
full-text representations and that the combination
of them yields the best performance, measured as
both recall and micro-averaged F-measure. Our
experiments also show that it is possible to do a
satisfactory categorization having only keywords,
given that we treat them as unigrams. Lastly, for
higher precision in text classification, we can use
the stemmed tokens in the headlines as features
541
Input feature Feature value Min. occurrence Precision Recall F-measure
full-text unigram tf*idf 3 93.03 71.69 80.98
keywords-only intact bool 3 89.56 41.48 56.70
keywords-only unigram bool 2 90.23 64.16 74.99
title tf*idf 4 94.23 68.43 79.28
keywords+full tf*idf 3 92.89 72.94 81.72
Table 4: Results on the fixed test set.
with tf*idf values.
As discussed in Section 2 and also presented in
Table 2, the number of keywords assigned per doc-
ument varies from zero to twelve. In Figure 1, we
have plotted how the precision, the recall, and the
F-measure for the test set vary with the number of
assigned keywords for the keywords-only unigram
representation.
 100
 90
 80
 70
 60
 50
 40
 30
 
12(239)11(310)10(206)9(177)8(184)7(184)6(252)5(259)4(328)3(838)2(272)1(36)
Pe
r c
en
t
Number of assigned keywords (number of documents)
Precision
F-measure
Recall
Figure 1: Precision, recall, and F-measure for
each number of assigned keywords. The values
in brackets denote the number of documents.
We can see that the F-measure and the recall reach
their highest points when three keywords are ex-
tracted. The highest precision (100%) is obtained
when the classification is performed on a single
extracted keyword, but then there are only 36 doc-
uments present in this group, and the recall is low.
Further experiments are needed in order to estab-
lish the optimal number of keywords to extract.
5 Related Work
For the work presented in this paper, there are two
aspects that are of interest in previous work. These
are in how the alternative input features (that is, al-
ternative from unigrams) are selected and in how
this alternative representation is used in combina-
tion with a bag-of-words representation (if it is).
An early work on linguistic phrases is done by
Fu?rnkranz et al (1998), where all noun phrases
matching any of a number of syntactic heuristics
are used as features. This approach leads to a
higher precision at the low recall end, when eval-
uated on a corpus of Web pages. Aizawa (2001)
extracts PoS-tagged compounds, matching pre-
defined PoS patterns. The representation contains
both the compounds and their constituents, and
a small improvement is shown in the results on
Reuters-21578. Moschitti and Basili (2004) add
complex nominals as input features to their bag-
of-words representation. The phrases are extracted
by a system for terminology extraction1. The more
complex representation leads to a small decrease
on the Reuters corpus. In these studies, it is un-
clear how many phrases that are extracted and
added to the representations.
Li et al (2003) map documents (e-mail mes-
sages) that are to be classified into a vector space
of keywords with associated probabilities. The
mapping is based on a training phase requiring
both texts and their corresponding summaries.
Another approach to combine different repre-
sentations is taken by Sahlgren and Co?ster (2004),
where the full-text representation is combined
with a concept-based representation by selecting
one or the other for each category. They show
that concept-based representations can outperform
traditional word-based representations, and that a
combination of the two different types of represen-
tations improves the performance of the classifier
over all categories.
Keywords assigned to a particular text can be
seen as a dense summary of the same. Some
reports on how automatic summarization can be
used to improve text categorization exist. For ex-
1In terminology extraction all terms describing a domain
are to be extracted. The aim of automatic keyword indexing,
on the other hand, is to find a small set of terms that describes
a specific document, independently of the domain it belongs
to. Thus, the set of terms must be limited to contain only the
most salient ones.
542
ample, Ko et al (2004) use methods from text
summarization to find the sentences containing the
important words. The words in these sentences are
then given a higher weight in the feature vectors,
by modifying the term frequency value with the
sentence?s score. The F-measure increases from
85.8 to 86.3 on the Newsgroups data set using Sup-
port vector machines.
Mihalcea and Hassan (2004) use an unsuper-
vised method2 to extract summaries, which in turn
are used to categorize the documents. In their ex-
periments on a sub-set of Reuters-21578 (among
others), Mihalcea and Hassan show that the preci-
sion is increased when using the summaries rather
than the full length documents. ?Ozgu?r et al (2005)
have shown that limiting the representation to
2 000 features leads to a better performance, as
evaluated on Reuters-21578. There is thus evi-
dence that using only a sub-set of a document can
give a more accurate classification. The question,
though, is which sub-set to use.
In summary, the work presented in this paper
has the most resemblance with the work by Ko et
al. (2004), who also use a more dense version of
a document to alter the feature values of a bag-of-
words representation of a full-length document.
6 Concluding Remarks
In the experiments described in this paper, we
investigated if automatically extracted keywords
can improve automatic text categorization. More
specifically, we investigated what impact key-
words have on the task of text categorization by
making predictions on the basis of keywords only,
represented either as unigrams or intact, and by
combining the full-text representation with auto-
matically extracted keywords. The combination
was obtained by giving higher weights to words in
the full-texts that were also extracted as keywords.
Throughout the study, we were concerned with
the data representation and feature selection pro-
cedure. We investigated what feature value should
be used (boolean, tf, or tf*idf) and the minimum
number of occurrence of the tokens in the training
data.
We showed that keywords can improve the per-
formance of the text categorization. When key-
words were used as a complement to the full-text
representation an F-measure of 81.7% was ob-
2This method has also been used to extract keywords (Mi-
halcea and Tarau, 2004).
tained, higher than without the keywords (81.0%).
Our results also clearly indicate that keywords
alone can be used for the text categorization task
when treated as unigrams, obtaining an F-measure
of 75.0%. Lastly, for higher precision (94.2%) in
text classification, we can use the stemmed tokens
in the headlines.
The results presented in this study are lower
than the state-of-the-art, even for the full-text run
with unigrams, as we did not tune any other pa-
rameters than the feature values (boolean, term
frequency, or tf*idf) and the threshold for the min-
imum number of occurrence in the training data.
There are, of course, possibilities for further
improvements. One possibility could be to com-
bine the tokens in the headlines and keywords in
the same way as the full-text representation was
combined with the keywords. Another possible
improvement concerns the automatic keyword ex-
traction process. The keywords are presented in
order of their estimated ?keywordness?, based on
the added regression value given by the three pre-
diction models. This means that one alternative
experiment would be to give different weights de-
pending on which rank the keyword has achieved
from the keyword extraction system. Another al-
ternative would be to use the actual regression
value.
We would like to emphasize that the automati-
cally extracted keywords used in our experiments
are not statistical phrases, such as bigrams or tri-
grams, but meaningful phrases selected by includ-
ing linguistic analysis in the extraction procedure.
One insight that we can get from these ex-
periments is that the automatically extracted key-
words, which themselves have an F-measure of
44.0, can yield an F-measure of 75.0 in the cat-
egorization task. One reason for this is that the
keywords have been evaluated using manually as-
signed keywords as the gold standard, meaning
that paraphrasing and synonyms are severely pun-
ished. Kotcz et al (2001) propose to use text cate-
gorization as a way to more objectively judge au-
tomatic text summarization techniques, by com-
paring how well an automatic summary fares on
the task compared to other automatic summaries
(that is, as an extrinsic evaluation method). The
same would be valuable for automatic keyword in-
dexing. Also, such an approach would facilitate
comparisons between different systems, as com-
mon test-beds are lacking.
543
In this study, we showed that automatic text
categorization can benefit from automatically ex-
tracted keywords, although the bag-of-words rep-
resentation is competitive with the best perfor-
mance. Automatic keyword extraction as well as
automatic text categorization are research areas
where further improvements are needed in order to
be useful for more efficient information retrieval.
Acknowledgments
The authors are grateful to the anonymous review-
ers for their valuable suggestions on how to im-
prove the paper.
References
Akiko Aizawa. 2001. Linguistic techniques to im-
prove the performance of automatic text categoriza-
tion. In Proceedings of NLPRS-01, 6th Natural
Language Processing Pacific Rim Symposium, pages
307?314.
Maria Fernanda Caropreso, Stan Matwin, and Fabrizio
Sebastiani. 2001. A learner-independent evaluation
of the usefulness of statistical phrases for automated
text categorization. In Text Databases and Docu-
ment Management: Theory and Practice, pages 78?
102.
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algo-
rithms and representations for text categorization.
In Proceedings of the Seventh International Confer-
ence on Information and Knowledge Management
(CIKM?98), pages 148?155.
George Forman. 2003. An extensive empirical study
of feature selection metrics for text classification.
Journal of Machine Learning Research, 3:1289?
1305, March.
Johannes Fu?rnkranz, Tom Mitchell, and Ellen Riloff.
1998. A case study using linguistic phrases for text
categorization on the WWW. In AAAI-98 Workshop
on Learning for Text Categorization.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2003),
pages 216?223.
Anette Hulth. 2004. Combining Machine Learn-
ing and Natural Language Processing for Automatic
Keyword Extraction. Ph.D. thesis, Department of
Computer and Systems Sciences, Stockholm Uni-
versity.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods:
Support Vector Learning. MIT-Press.
Youngjoong Ko, Jinwoo Park, and Jungyun Seo. 2004.
Improving text categorization using the importance
of sentences. Information Processing and Manage-
ment, 40(1):65?79.
Aleksander Kolcz, Vidya Prabakarmurthi, and Jugal
Kalita. 2001. Summarization as feature selec-
tion for text categorization. In Proceedings of the
Tenth International Conference on Information and
Knowledge Management (CIKM?01), pages 365?
370.
David D. Lewis. 1997. Reuters-21578 text categoriza-
tion test collection, Distribution 1.0. AT&T Labs Re-
search.
Cong Li, Ji-Rong Wen, and Hang Li. 2003. Text clas-
sification using stochastic keyword generation. In
Proceedings of the 20th International Conference on
Machine Learning (ICML-2003).
Rada Mihalcea and Samer Hassan. 2005. Using the
essence of texts to improve document classifica-
tion. In Proceedings of the Conference on Recent
Advances in Natural Language Processing (RANLP
2005).
Rada Mihalcea and Paul Tarau. 2004. TextRank:
bringing order into texts. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2004).
Alessandro Moschitti and Roberto Basili. 2004. Com-
plex linguistic features for text classification: A
comprehensive study. In Sharon McDonald and
John Tait, editors, Proceedings of ECIR-04, 26th
European Conference on Information Retrieval Re-
search, pages 181?196. Springer-Verlag.
Arzucan ?Ozgu?r, Levent ?Ozgu?r, and Tunga Gu?ngo?r.
2005. Text categorization with class-based and
corpus-based keyword selection. In Proceedings
of the 20th International Symposium on Computer
and Information Sciences, volume 3733 of Lec-
ture Notes in Computer Science, pages 607?616.
Springer-Verlag.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Magnus Sahlgren and Rickard Co?ster. 2004. Using
bag-of-concepts to improve the performance of sup-
port vector machines in text categorization. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING 2004), pages
487?493.
Yiming Yang and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
the 22nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 42?49.
544
Enhancing Linguistically Oriented Automatic Keyword Extraction
Anette Hulth
Dept. of Computer and Systems Sciences
Stockholm University
SE-164 40 Kista, Sweden
hulth@dsv.su.se
Abstract
This paper presents experiments on how the
performance of automatic keyword extraction
can be improved, as measured by keywords
previously assigned by professional indexers.
The keyword extraction algorithm consists of
three prediction models that are combined to
decide what words or sequences of words in
the documents are suitable as keywords. The
models, in turn, are built using different defi-
nitions of what constitutes a term in a written
document.
1 Introduction
Automatic keyword indexing is the task of finding a small
set of terms that describes the content of a specific doc-
ument. If the keywords are chosen from the document
at hand, it is referred to as keyword extraction, and this
is the approach taken for the work presented in this pa-
per. Once a document has a set of keywords, they can
be useful for several tasks. For example, they can be the
entrance to a document collection, similar to a back-of-
the-book index; they can be used to refine a query to a
search engine; or they may serve as a dense summary for
a specific document.
In the presented research, the decision of what words
or sequences of words in the documents that are suitable
as keywords are made by prediction models trained on
documents with manually assigned keywords. This paper
presents a number of modifications to an existing key-
word extraction algorithm, as well as results of the em-
pirical verifications.
2 Background
The approach taken to the keyword extraction task is that
of supervised machine learning. This means that a set
of documents with known keywords is used to train a
model, which in turn is applied to select keywords to and
from previously unseen documents. The keyword extrac-
tion discussed in this paper is based on work presented in
Hulth (2003a) and Hulth (2003b).
In Hulth (2003a) an evaluation of three different meth-
ods to extract candidate terms from documents is pre-
sented. The methods are:
  extracting all uni-, bi, and trigrams that do not begin
or end with a stopword.
  extracting all noun phrase (NP) chunks as judged by
a partial parser.
  extracting all part-of-speech (PoS) tagged words or
sequences of words that match any of a set of empir-
ically defined PoS patterns.
The best performing models use four attributes. These
are:
  term frequency
  collection frequency
  relative position of the first occurrence
  the POS tag or tags assigned to the term
All terms are stemmed using Porter?s stemmer (Porter,
1980), and an automatically selected keyword is consid-
ered correct if it is equivalent to a stemmed manually as-
signed keyword. The performance of the classifiers is
evaluated by calculating the F-measure for the selected
keywords, with equal weight given to the precision and
the recall.
In Hulth (2003b), experiments on how the performance
of the keyword extraction can be improved by combining
the judgement of three classifiers are presented. The clas-
sifiers differ in how the data are represented, and more
specifically in how the candidate terms are selected from
the documents. By only assigning keywords that are se-
lected by at least two term selection approaches?that
is by taking the majority vote?a better performance is
achieved. In addition, by removing the subsumed key-
words (keywords that are substrings of other selected
keywords) the performance is yet higher.
The classifiers are constructed by Rule Discovery Sys-
tem (RDS), a system for rule induction1. This means that
the models consist of rules. The applied strategy is that
of recursive partitioning, where the resulting rules are hi-
erarchically organised (i.e., decision trees).
The data set on which the models are trained and tested
originates from the Inspec database2, and consists of ab-
stracts in English from scientific journal papers. The set
of 2 000 documents is divided into three sets: a training
set of 1 000 documents (to train the models), a validation
set consisting of 500 documents (to select the best per-
forming model, e.g., for setting the threshold value for
the regression runs), and the remaining 500 documents
are saved for testing (to get unbiased results). Each ab-
stract has two sets of keywords?assigned by a profes-
sional indexer?associated to them: a set of controlled
terms (keywords restricted to the Inspec thesaurus); and
a set of uncontrolled terms that can be any suitable terms.
Both the controlled terms and the uncontrolled terms may
or may not be present in the abstracts. However, the in-
dexers had access to the full-length documents when as-
signing the keywords, and not only to the abstracts. For
the experiments presented in this paper, only the uncon-
trolled terms are considered, as these to a larger extent
are present in the abstracts (76.2% as opposed to 18.1%
for the controlled terms). The performance is evaluated
using the uncontrolled keywords as the gold standard.
In the paper, three minor improvements to the keyword
extraction algorithm are presented. These concern how
one of the term selection approaches extract candidate
terms; how the collection frequency is calculated; and
how the weights are set to the positive examples. The
major focus of the paper is how the learning task is de-
fined. For these experiments, the same machine learning
system?RDS?is used as for the experiments presented
by Hulth (2003a). Also the same data are used to train the
models and to tune the parameters. The results of the ex-
periments are presented in Tables 1?5, which show: the
average number of keywords assigned per document (As-
sign.); the average number of correct keywords per docu-
ment (Corr.); precision (P); recall (R); and F-measure (F).
On average, 7.6 manually assigned keywords are present
per document. The total number of manual keywords
present in the abstracts in the test data set is 3 816, and is
the number on which the recall is calculated.
1http://www.compumine.com
2http://www.iee.org/publish/inspec/
3 Refinements
In this section, three minor modifications made to the
keyword extraction algorithm are presented. The first one
concerns how the NP-chunks are extracted from the doc-
uments: By removing the initial determiner of the NP-
chunks, a better performance is achieved. The second al-
teration is to use a general corpus for calculating the col-
lection frequency value. Also the weights for the positive
examples are set in a more systematic way, to maximise
the performance of the combined model.
3.1 Refining the NP-chunk Approach
It was noted in Hulth (2003b) that when extracting NP-
chunks, the accompanying determiners are also extracted
(per definition), but that determiners are rarely found at
the initial position of keywords. This means that the au-
tomatic evaluation treats such keywords as misclassified,
although they might have been correct without the deter-
miner. For this reason the determiners a, an, and the are
removed when occurring in the beginning of an extracted
NP-chunks. The results for the runs when extracting NP-
chunks with and without these determiners are found in
Table 1. As can be seen in this table, the recall increases
while the precision decreases. However, the high increase
in recall leads to an increase in the F-measure from 33.0
to 36.8.
Assign. Corr. P R F
With det. 9.6 2.8 29.7 37.2 33.0
Without det. 15.0 4.2 27.7 54.6 36.8
Table 1: Extracting NP-chunks with and without the ini-
tial determiners a, an, and the.
3.2 Using a General Corpus
In the experiments presented in Hulth (2003a), only the
documents present in the training, validation, and test set
respectively are used for calculating the collection fre-
quency. This means that the collection is rather homoge-
nous. For this reason, the collection frequency is instead
calculated on a set of 200 arbitrarily chosen documents
from the British National Corpus (BNC). In Table 2, the
performance of two runs when taking the majority vote of
the three classifiers removing the subsumed terms is pre-
sented. The first run (?Abstracts?) is when the collection
frequency is calculated from the abstracts. The second
run (?Gen. Corp.?) is when the BNC documents are used
for this calculation. If comparing these two runs, the F-
measure increases. In other words, using a more general
corpus for this calculation leads to a better performance
of the automatic keyword extraction.
Assign. Corr. P R F
Abstracts 11.1 3.8 33.9 49.2 40.1
Gen. Corp. 12.9 4.2 33.0 55.6 41.4
Table 2: Calculating the collection frequency from the
abstracts, and from a general corpus (Gen. Corp.).
3.3 Setting the Weights
As the data set is unbalanced?there is a larger number
of negative than positive examples?the positive exam-
ples are given a higher weight when training the predic-
tion models. In the experiments discussed so far, the
weights given to the positive examples are those result-
ing in the best performance for each individual classifier
(as described in Hulth (2003a)). For the results presented
further, the weights are instead set according to which
individual weight that maximises the F-measure for the
combined model on the validation set. The weight given
to the positive examples for each term selection approach
has in a (rather large) number of runs been altered sys-
tematically for each classifier, and the combination that
results in the best performance is selected. The results
on the test set are presented in Table 3. As can be seen
in this table, the recall decreases, while the precision and
the F-measure increase.
Assign. Corr. P R F
Individual best 12.9 4.2 33.0 55.6 41.4
Best combined 8.2 3.3 40.0 43.2 41.6
Table 3: Combining the classifiers with the best individ-
ual weight and with the best combination, respectively.
4 Regression vs. Classification
In the experiments presented in Hulth (2003a), the auto-
matic keyword indexing task is treated as a binary classi-
fication task, where each candidate term is classified ei-
ther as a keyword or a non-keyword. RDS allows for the
prediction to be treated as a regression task (Breiman et
al., 1984). This means that the prediction is given as a
numerical value, instead of a category. When training the
regression models, the candidate terms being manually
assigned keywords are given the value one, and all other
candidate terms are assigned the value zero. In this fash-
ion, the prediction is a value between zero and one, and
the higher the value, the more likely a candidate term is
to be a keyword (according to the model).
To combine the results from the three models, there
are two alternatives. Either the prediction value can be
added for all candidate terms, or it can be added only if
it is over a certain threshold set for each model, depend-
ing on the model?s individual performance. Regardless, a
candidate term may be selected as a keyword even if it is
extracted by only one method, provided that the value is
high enough. The threshold values are defined based on
the performance of the models on the validation data.
In Table 4, results for two regression runs on the test
data are presented. These two runs are in Table 4 com-
pared to the best performing classification run. The first
regression run (?Regression?) is when all candidate terms
having an added value over a certain threshold are se-
lected. The second presented regression run (Regression
with individual threshold: ?Reg. ind. thresh.?) is when a
threshold is set for each individual model: If a predic-
tion value is below this threshold it does not contribute
to the added value for a candidate term. In this case, the
threshold for the total score is slightly lower than when
no individual threshold is set. Both regression runs have
a higher F-measure than the classification run, due to the
fact that recall increases, more than what the precision
decreases. The run without individual thresholds results
in the highest F-measure.
Assign. Corr. P R F
Classification 8.2 3.3 40.0 43.2 41.6
Regression 10.8 4.2 38.9 54.8 45.5
Reg. ind. thresh. 11.3 4.2 37.1 54.7 44.2
Table 4: Using classification and regression. ?Reg. ind.
thesh.? refers to a run where the regression value from
each model contributes only if it is over a certain thresh-
old.
4.1 Defining the Number of Keywords
If closer inspecting the best regression run, this combined
model assigns on average 10.8 keywords per document.
The actual distribution varies from 3 documents with 0 to
1 document with 32 keywords. As mentioned, the predic-
tion value from a regression model is numeric, and indi-
cates how likely a candidate term is to be a keyword. It is
thus possible to rank the output, and consequently to limit
the number of keywords assigned per document. A set
of experiments has been performed with the aim to find
what number of keywords per document that results in the
highest F-measure, by varying the number of keywords
assigned. In these experiments, only terms with an added
value over the threshold are considered, and the candidate
terms with the highest values are selected first. The best
performance is when the maximum of twelve keywords
is selected for each document. (The subsumed terms are
removed after that the maximum number of keywords is
selected.) As can be seen in Table 5 (?All? compared to
?Max. 12?), the F-measure decreases as does the recall,
although the precision increases, when limiting the num-
ber of keywords.
There are, however, still some documents that do not
get any selected keywords. To overcome this, three
terms are assigned to each document even if the added
regression value is below the threshold. Doing this
gives a slightly lower precision, while the recall increases
slightly. The F-measure is unaffected (see Table 5: 3?12).
Assign. Corr. P R F
All 10.8 4.2 38.9 54.8 45.5
Max. 12 8.6 3.6 41.6 46.8 44.0
3?12 8.6 3.6 41.5 46.9 44.0
Table 5: Assigning all terms over the threshold (All),
and limiting the number of terms assigned per document
(Max. 12, and 3?12 respectively).
5 Concluding Remarks
In this paper, a number of experiments leading to a bet-
ter performance of a keyword extraction algorithm has
been presented. One improvement concerns how the NP-
chunks are extracted, where the results are improved by
excluding the initial determiners a, an, and the. Possi-
bly, this improvement could be yet higher if all initial de-
terminers were removed from the NP. Another improve-
ment concerns how the collection frequency is calculated,
where the F-measure of the extraction increases when a
general corpus is used. A third improvement concerns
how the weights to the positive examples are set. By ad-
justing the weights to maximise the performance of the
combined model, the F-measure increases. Also, one ma-
jor change is made to the algorithm, as the learning task
is redefined. This is done by using regression instead
of classification for the machine learning. Apart from
an increase in performance by regression, this enables a
ranked output of the keywords. This in turn makes it easy
to vary the number of keywords selected per document,
in case necessary for some types of applications. In ad-
dition, compared to classification, regression resembles
reality in the sense that some words are definitely key-
words, some are definitely not, but there are also many
candidate terms that are keywords to a certain extent.
Thus, there is a continuum of the candidate terms? ?key-
wordness?.
Evaluating automatically extracted keywords is not
trivial, as different persons may prefer different terms at
different occasions. This is also true for professional in-
dexers, where the consistency also depends on how ex-
perienced an indexer is. For example, Bureau van Dijk
(1995) has shown that the index consistency between ex-
perienced indexers may be up to 60?80 per cent, while it
is not unusual that it is as low as 20?30 between inexpe-
rienced indexers. The approach taken to the evaluation of
the experiments presented in this paper is that of using
keywords previously assigned by professional indexers
as a gold standard for calculating the precision, the re-
call, and the F-measure. If looking at the inter-judgement
agreement between the keywords selected by the com-
bined model assigning no more than twelve keywords per
document and the manually assigned keywords for the
documents in the test set, it is 28.2%. Thus the perfor-
mance of the keyword extraction algorithm is at least as
consistent as that of inexperienced professional indexers.
This is, however, only true to a certain extent, as some of
the keywords selected by the automatic extractor would
never have been considered by a human?not even a non-
professional3.
Acknowledgements
Thanks to Henrik Bostro?m for valuable discussions and
comments.
References
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Re-
gression Trees. Chapman & Hall, New York.
Bureau van Dijk. 1995. Parlement Europe?en, Evaluation
des ope?rations pilotes d?indexation automatique (Con-
vention spe?cifique no 52556), Rapport d?e?valution fi-
nale.
Anette Hulth. 2003a. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2003), pages 216?
223, Sapporo, Japan. Association for Computational
Linguistics.
Anette Hulth. 2003b. Reducing false positives by expert
combination in automatic keyword indexing. In Pro-
ceedings of the Conference on Recent Advances in Nat-
ural Language Processing (RANLP 2003), pages 197?
203, Borovets, Bulgaria.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
3Two examples of such keywords from the test data would
be ?As luck? and ?Comprehension goes?.
Improved Automatic Keyword Extraction
Given More Linguistic Knowledge
Anette Hulth
Department of Computer and Systems Sciences
Stockholm University
Sweden
hulth@dsv.su.se
Abstract
In this paper, experiments on automatic
extraction of keywords from abstracts us-
ing a supervised machine learning algo-
rithm are discussed. The main point of this
paper is that by adding linguistic know-
ledge to the representation (such as syn-
tactic features), rather than relying only on
statistics (such as term frequency and n-
grams), a better result is obtained as mea-
sured by keywords previously assigned by
professional indexers. In more detail, ex-
tracting NP-chunks gives a better preci-
sion than n-grams, and by adding the POS
tag(s) assigned to the term as a feature, a
dramatic improvement of the results is ob-
tained, independent of the term selection
approach applied.
1 Introduction
Automatic keyword assignment is a research topic
that has received less attention than it deserves, con-
sidering keywords? potential usefulness. Keywords
may, for example, serve as a dense summary for a
document, lead to improved information retrieval, or
be the entrance to a document collection. However,
relatively few documents have keywords assigned,
and therefore finding methods to automate the as-
signment is desirable.
A related research area is that of terminology ex-
traction (see e.g., Bourigault et al (2001)), where
all terms describing a domain are to be extracted.
The aim of keyword assignment is to find a small
set of terms that describes a specific document, in-
dependently of the domain it belongs to. However,
the latter may very well benefit from the results of
the former, as appropriate keywords often are of a
terminological character.
In this work, the automatic keyword extraction is
treated as a supervised machine learning task, an ap-
proach first proposed by Turney (2000). Two im-
portant issues are how to define the potential terms,
and what features of these terms are considered dis-
criminative, i.e., how to represent the data, and con-
sequently what is given as input to the learning al-
gorithm. In this paper, experiments with three term
selection approaches are presented: n-grams; noun
phrase (NP) chunks; and terms matching any of a
set of part-of-speech (POS) tag sequences. Four dif-
ferent features are used: term frequency, collection
frequency, relative position of the first occurrence,
and the POS tag(s) assigned to the term.
2 Points of Departure
Treating the automatic keyword extraction as a su-
pervised machine learning task means that a clas-
sifier is trained by using documents with known
keywords. The trained model is subsequently ap-
plied to documents for which no keywords are as-
signed: each defined term from these documents
is classified either as a keyword or a non-keyword;
or?if a probabilistic model is used?the probabil-
ity of the defined term being a keyword is given.
Turney (2000) presents results for a comparison be-
tween an extraction model based on a genetic algo-
rithm and an implementation of bagged C4.5 deci-
sion trees for the task. The terms are all stemmed uni-
grams, bigrams, and trigrams from the documents,
after stopword removal. The features used are, for
example, the frequency of the most frequent phrase
component; the relative number of characters of the
phrase; the first relative occurrence of a phrase com-
ponent; and whether the last word is an adjective,
as judged by the unstemmed suffix. Turney reports
that the genetic algorithm outputs better keywords
than the decision trees. Part of the same training and
test material is later used by Frank et al (1999) for
evaluating their algorithm in relation to Turney?s al-
gorithm. This algorithm, which is based on naive
Bayes, uses a smaller and simpler set of features?
term frequency, collection frequency (idf), and rel-
ative position?although it performs equally well.
Frank et al also discuss the addition of a fourth fea-
ture that significantly improves the algorithm, when
trained and tested on domain-specific documents.
This feature is the number of times a term is assigned
as a keyword to other documents in the collection.
It should be noted that the performance of the
state-of-the-art keyword extraction is much lower
than for many other NLP-tasks, such as tagging and
parsing, and there is plenty of room for improve-
ments. To give an idea of this, the results obtained
by the genetic algorithm trained by Turney (2000),
and the naive Bayes approach by Frank et al (1999)
are presented. The number of terms assigned must
be explicitly limited by the user for these algorithms.
Turney and Frank et al report the precision for five
and fifteen keywords per document. Recall is not re-
ported in their studies. In Table 1 their results when
training and testing on journal articles are shown,
and the highest values for the two algorithms are pre-
sented.
Prec. Corr. mean
5 terms* 29.0 1.45
15 terms** 18.3 2.75
Table 1: Precision, and the average number of
correct terms for Turney (2000)* and Frank et al
(1999)**, for five and fifteen extracted terms.
There are two drawbacks in common with
the approaches proposed by Turney (2000) and
Frank et al (1999). First, the number of tokens in a
keyword is limited to three. In the data used to train
the classifiers evaluated in this paper, 9.1% of the
manually assigned keywords consist of four tokens
or more, and the longest keywords have eight tokens.
Secondly, the user must state how many keywords
to extract from each document, as both algorithms,
for each potential keyword, output the probability of
the term being a keyword. This could be solved by
manually setting a threshold value for the probabil-
ity, but this decision should preferably be made by
the extraction system.
Finding potential terms?when no machine learn-
ing is involved in the process?by means of POS
patterns is a common approach. For exam-
ple, Barker and Cornacchia (2000) discuss an al-
gorithm where the number of words and the fre-
quency of a noun phrase, as well as the fre-
quency of the head noun is used to determine
what terms are keywords. An extraction sys-
tem called LinkIT (see e.g., Evans et al (2000))
compiles the phrases having a noun as the head,
and then ranks these according to the heads? fre-
quency. Boguraev and Kennedy (1999) extract tech-
nical terms based on the noun phrase patterns sug-
gested by Justeson and Katz (1995); these terms are
then the basis for a headline-like characterisation of
a document. The final example given in this paper
is Daille et al (1994) who apply statistical filters on
the extracted noun phrases. In that study it is con-
cluded that term frequency is the best filter candi-
date of the scores investigated. When POS patterns
are used to extract potential terms, the problem lies
in how to restrict the number of terms, and only keep
the ones that are relevant.
In the case of professional indexing, the terms are
normally limited to a domain-specific thesaurus, but
not to those present only in the document to which
they are assigned. For example, Steinberger (2001)
presents work where as a first step, all lemmas after
stop word removal in a document are ranked accord-
ing to the log-likelihood ratio, thus a list of content
descriptors is obtained. These terms are then used
to assign thesaurus terms, that have been automati-
cally assigned associating lemmas during a training
phase. In this paper, however, the concern is not to
limit the terms to a set of allowed terms.
As opposed to Turney (2000) and Frank et al
(1999), who experiment with keyword extraction
from full-length texts, this work concerns keyword
extraction from abstracts. The reason for this is that
many journal papers are not available as full-length
texts, but as abstracts only, as is the case for example
on the Internet.
The starting point for this work was to examine
whether the data representation suggested by Frank
et al was adequate for constructing a keyword ex-
traction model from and for abstracts. As the results
were poor, two alternatives to extracting n-grams
as the potential terms were explored. The first ap-
proach was to extract all noun phrases in the docu-
ments as judged by an NP-chunker. The second se-
lection approach was to define a set of POS tag se-
quences, and extract all words or sequences of words
that matched any of these, relying on a PoS tag-
ger. These two different approaches mean that the
length of the potential terms is not limited to some-
thing arbitrary, but reflects a linguistic property. The
solution to limiting the number of terms?as the
majority of the extracted words or phrases are not
keywords?was to apply a machine learning algo-
rithm to decide which terms are keywords and which
are not. The output from the machine learning algo-
rithm is binary (a term is either a keyword or not),
consequently the system itself limits the amount of
extracted keywords per document. As for the fea-
tures, a fourth feature was added to the ones used
by Frank et al, namely the POS tag(s) assigned to
the term. This feature turned out to dramatically im-
prove the results.
3 The Corpus
The collection used for the experiments described
in this paper consists of 2 000 abstracts in En-
glish, with their corresponding title and keywords
from the Inspec database. The abstracts are from
the years 1998 to 2002, from journal papers, and
from the disciplines Computers and Control, and In-
formation Technology. Each abstract has two sets
of keywords?assigned by a professional indexer?
associated to them: a set of controlled terms, i.e.,
terms restricted to the Inspec thesaurus; and a set
of uncontrolled terms that can be any suitable terms.
Both the controlled terms and the uncontrolled terms
may or may not be present in the abstracts. However,
the indexers had access to the full-length documents
when assigning the keywords. For the experiments
described here, only the uncontrolled terms were
considered, as these to a larger extent are present in
the abstracts (76.2% as opposed to 18.1%).
The set of abstracts was arbitrarily divided into
three sets: a training set (to construct the model)
consisting of 1 000 documents, a validation set (to
evaluate the models, and select the best perform-
ing one) consisting of 500 documents, and a test
set (to get unbiased results) with the remaining 500
abstracts. The set of manually assigned keywords
were then removed from the documents. For all ex-
periments the same training, validation, and test sets
were used.
4 Building the Classifiers
This section begins with a discussion on the differ-
ent ways the data were represented: in Section 4.1
the term selection approaches are described, and in
Section 4.2 the features are discussed. Thereafter, a
brief description of the machine learning approach
is given. Finally in Section 4.4, the training and the
evaluation of the classifiers are discussed.
4.1 Three Term Selection Approaches
In this section, the three different term selection ap-
proaches, in other words, the three definitions of
what constitutes a term in a document, are described.
n-grams
In a first set of runs, the terms were de-
fined in a manner similar to Turney (2000) and
Frank et al (1999). (Their studies were introduced
in Section 2.) All unigrams, bigrams, and trigrams
were extracted. Thereafter a stoplist was used (from
Fox (1992)), where all terms beginning or ending
with a stopword were removed. Finally all remain-
ing tokens were stemmed using Porter?s stemmer
(Porter, 1980). In this paper, this manner of selecting
terms is referred to as the n-gram approach.
The implementation differs from Frank et al
(1999) in the following aspects:
  Only non-alphanumeric characters that were
not present in any keyword in the training set
were removed (keeping e.g., C++).
  Numbers were removed only if they stood sep-
arately (keeping e.g., 4YourSoul.com).
  Proper nouns were kept.
  The stemming and the stoplist applied were dif-
ferent.
  The stems were kept even if they appeared only
once (which is true for 80.0% of the keywords
present in the training set).
NP-chunks
That nouns are appropriate as content descrip-
tors seems to be something that most agree upon.
When inspecting manually assigned keywords, the
vast majority turn out to be nouns or noun phrases
with adjectives, and as discussed in Section 2, the re-
search on term extraction focuses on noun patterns.
To not let the selection of potential terms be an ar-
bitrary process?which is the case when extracting
n-grams?and better capture the idea of keywords
having a certain linguistic property, I decided to ex-
periment with noun phrases.
In the next set of experiments a partial parser1 was
used to select all NP-chunks from the documents.
Experiments with both unstemmed and stemmed
terms were performed. This way of defining the
terms is in this paper called the chunking approach.
POS Tag Patterns
As about half of the manual keywords present in
the training data were lost using the chunking ap-
proach, I decided to define another term selection
approach. This still captures the idea of keywords
having a certain syntactic property, but is based on
empirical evidence in the training data.
A set of POS tag patterns?in total 56?were de-
fined, and all (part-of-speech tagged) words or se-
quences of words that matched any of these were
extracted. The patterns were those tag sequences
of the manually assigned keywords, present in the
training data, that occurred ten or more times. This
way of defining the terms is here called the pattern
approach. As with the chunking approach, exper-
iments with both unstemmed and stemmed terms
were performed.
Out of the 56 patterns, 51 contain one or more
noun tags. To give an idea of the patterns, the
1LT CHUNK, available at http://www.ltg.ed.-
ac.uk/software/pos/index.html (without the hy-
phen).
five most frequently occurring ones of the keywords
present in the training data are
  ADJECTIVE NOUN (singular or mass)
  NOUN NOUN (both sing. or mass)
  ADJECTIVE NOUN (plural)
  NOUN (sing. or mass) NOUN (pl.)
  NOUN (sing. or mass)
4.2 Four Features
Initially, the same features that Frank et al (1999)
used for their domain-independent experiments
were used. These were
  Within-document frequency
  Collection frequency
  Relative position of the first occurrence (the
proportion of the document preceding the first
occurrence).
The representation differed in that the term fre-
quency and the collection frequency were not
weighted together, but kept as two distinct features.
In addition, the real values were not discretised, only
rounded off to two decimals, thus more decision-
making was handed over to the algorithm. The col-
lection frequency was calculated for the three data
sets separately.
In addition, experiments with a fourth feature
were performed. This is the POS tag or tags as-
signed to the term by the same partial parser used
for finding the chunks and the tag patterns. When a
term consists of several tokens, the tags are treated
like a sequence. As an example, an extracted phrase
like random JJ excitations NNS gets the atomic fea-
ture value JJ NNS. In case a term occurs more than
once in the document, the tag or tag sequence as-
signed is the most frequently occurring one for that
term in the entire document. In case of a draw, the
first occurring one is assigned.
4.3 Rule Induction
As usual in machine learning, the input to the learn-
ing algorithm consists of examples, where an exam-
ple refers to the feature value vector for each, in
this case, potential keyword. An example that is a
manual keyword is assigned the class positive, and
those that are not are given the class negative. The
machine learning approach used for the experiments
is that of rule induction, i.e., the model that is con-
structed from the given examples, consists of a set of
rules2. The strategy used to construct the rules is re-
cursive partitioning (or divide-and-conquer), which
has as the goal to maximise the separation between
the classes for each rule.
The system used allows for different ensemble
techniques to be applied, meaning that a number of
classifiers are generated and then combined to pre-
dict the class. The one used for these experiments
is bagging (Breiman, 1996). In bagging, examples
from the training data are drawn randomly with re-
placement until a set of the original size is obtained.
This new set is then used to train a classifier. This
procedure is repeated n times to generate n classi-
fiers that then vote to classify an instance.
It should be noted that my intention is not to ar-
gue for this machine learning approach in favour of
any other. However, one advantage with rules is that
they may be inspected, and thus might give an in-
sight into how the learning component makes its de-
cisions, although this is less applicable when apply-
ing ensemble techniques.
4.4 The Training and the Evaluation
The feature values were calculated for each ex-
tracted unit in the training and the validation sets,
that is for the n-grams, NP-chunks, stemmed NP-
chunks, patterns, and the stemmed patterns respec-
tively. In other words, the within-document fre-
quency, the collection frequency, and the proportion
of the document preceding the first appearance for
each potential term were calculated. Also, the POS
tag(s) for each term were extracted. In addition,
as the machine learning approach is supervised, the
class was added, i.e., whether the term is a manually
assigned keyword or not. For the stemmed terms,
a unit was considered a keyword if it was equal to
a stemmed manual keyword. For the unstemmed
terms, the term had to match exactly.
The measure used to evaluate the results on the
validation set was the F-score, defined as
2The system is Rule Discovery System from Compumine
AB. www.compumine.com.
 	
ff