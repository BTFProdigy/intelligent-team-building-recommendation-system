Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 135?139,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Tweet Conversation Annotation Tool
with a Focus on an Arabic Dialect, Moroccan Darija
Stephen Tratz?, Douglas Briesch?, Jamal Laoudi?, and Clare Voss?
?Army Research Laboratory, Adelphi, MD 20783
?ArtisTech, Inc., Fairfax, VA 22030
{stephen.c.tratz.civ,douglas.m.briesch.civ,jamal.laoudi.ctr,clare.r.voss.civ}@mail.mil
Abstract
This paper presents the DATOOL, a graph-
ical tool for annotating conversations con-
sisting of short messages (i.e., tweets), and
the results we obtain in using it to annotate
tweets for Darija, an historically unwritten
Arabic dialect spoken by millions but not
taught in schools and lacking standardiza-
tion and linguistic resources.
With the DATOOL, a native-Darija
speaker annotated hundreds of mixed-
language and mixed-script conversations
at approximately 250 tweets per hour. The
resulting corpus was used in developing
and evaluating Arabic dialect classifiers
described briefly herein.
The DATOOL supports downstream dis-
course analysis of tweeted ?conversations?
by mapping extracted relations such as,
who tweets to whom in which language,
into graph markup formats for analysis in
network visualization tools.
1 Overview
For historically unwritten languages, few textual
resources exist for developing NLP applications
such as machine translation engines. Even when
audio resources are available, difficulties arise
when converting sound to text (Robinson and
Gadelii, 2003). Increasingly, however, with the
widespread use of mobile phones, these languages
are being written in social media such as Twitter.
Not only can these languages be written in multi-
ple scripts, but conversations, and even individual
messages, often involve multiple languages. To
build useful textual resources for documenting and
translating these languages (e.g., bilingual dictio-
naries), tools are needed to assist in language an-
notation for this noisy, multiscript, multilingual
form of communication.
This paper presents the Dialect Annotation Tool
(DATOOL), a graphical tool for annotating conver-
sations consisting of short messages (i.e., tweets),
and the results we obtain in using it to annotate
tweets for Darija, an historically unwritten North
African Arabic dialect spoken by millions but not
taught in schools and lacking in standardardiza-
tion and linguistic resources. The DATOOL can
retrieve the conversation for each tweet on a user?s
timeline or via Apollo (Le et al, 2011) and display
the discourse, enabling annotators to make more
informed decisions. It has integrated classifiers for
automatically annotating data so a user can either
verify or alter the automatically-generated annota-
tions rather than start from scratch. The tool can
also export annotated data to GEPHI (Bastian et
al., 2009), an open source network visualization
tool with many layout algorithms, which will fa-
cilitate future ?code-switching? research.
2 Tool Description
2.1 Version 1.0
The first version of the tool is depicted in Figure
1. It is capable of loading a collection of tweets
and extracting the full conversations they belong
to. Each conversation is displayed within its own
block in the conversation display table. An anno-
tator can mark multiple tweets as Darija (or other
language) by selecting multiple checkboxes in the
lefthand side of the table. Also, if a tweet is writ-
ten in multiple languages, the annotator can anno-
tate the different sections using the Message text
box below the conversation display table.
The tool also calculates user and collection level
summary statistics, which it displays below the
main annotation section.
We worked with a Darija-speaking annotator
during the tool?s development, who provided
valuable feedback, helping to shape the overall
design of the tool and improve its functionality.
135
Figure 1: The Dialect Annotation Tool (DATOOL) displaying a possible Twitter conversation.
Data Annotation Using version 1.0, the annotator
marked up 3013 tweets from 3 users for the pres-
ence of the Darija (approximately 1,000 per user),
averaging about 250 tweets per hour. Of the 1,400
tweets with Arabic script, 1,013 contained Darija.
This annotated data is used to evaluate the Arabic
dialect classifier discussed in Section 3.
2.2 Version 2.0
The second version of the tool contains the ad-
ditional ability to invoke pre-trained classification
models to automatically annotate tweets. The tool
displays the classifier?s judgment confidence next
to each tweet, and the user can set a minimal con-
fidence threshold, below which automatic annota-
tions are hidden. Figure 2 illustrates the new clas-
sification functionality.
2.3 XML Output
The DATOOL stores data in an XML-based for-
mat that can be reloaded for continuing or re-
vising annotation. It can also export four differ-
ent views of the data in Graph Exchange XML
Format (GEXF), a format that can be read by
GEPHI. In the social network view, users are
represented by nodes, and tweets are represented
as directed edges between the nodes. The in-
formation network view displays tweets as nodes
with directed edges between time-ordered tweets
within a conversation. In the social-information
network view, both users and tweets are repre-
sented by nodes, and there are directed edges both
from tweet senders to their tweets and from tweets
to recipients. The social-information network plus
view provides all the information of both the so-
cial network and the information network.
3 Classifier
For the second version of the DATOOL, we inte-
grated an Arabic dialect classifier capable of dis-
tinguishing among Darija, Egyptian, Gulf, Lev-
antine and MSA with the goal of improving the
speed and consistency of the annotation process.
Though language classification is sometimes
viewed as a solved problem (McNamee, 2005),
with some experiments achieving over 99% ac-
curacy (Cavnar and Trenkle, 1994), it is signifi-
cantly more difficult when distinguishing closely-
related languages or short texts (Vatanen et al,
2010; da Silva and Lopes, 2006). The only lan-
guage classification work for distinguishing be-
tween these closely-related Arabic dialects that
we are aware of was performed by Zaidan and
Callison-Burch (2013). They collected web com-
mentary data written in MSA, Egyptian, Levan-
tine, and Gulf and performed dialect identifica-
tion experiments, their strongest classifier achiev-
136
Figure 2: Screenshot showcasing the automatic classification output, including confidence values.
ing 81.0% accuracy.
3.1 Training Data
Since Zaidan and Callison-Burch?s dataset in-
cludes no Darija, we collected Darija exam-
ples from the following sources to augment their
dataset: Moroccan jokes from noktazwina.
com, web pages collected using Darija-specific
query terms with a popular search engine, and
37,538 Arabic script commentary entries from
hespress.com (a Moroccan news website).
Nearly all the joke (N=399) and query term
(N=874) data contained Darija. By contrast, the
commentary data was mostly MSA. To extract
a subset of the commentary entries most likely
to contain Darija, we applied an iterative, semi-
supervised approach similar to that described by
Tratz and Sanfilippo (2007), in which the joke and
query term data were treated as initial seeds and,
in each iteration, a small portion of commentary
data with the highest Darija scores were added to
the training set. After having run this process to
its completion, we examined 131 examples at in-
tervals of 45 from the resulting ranked list of com-
mentary. The 62nd example was the first of these
to have been incorrectly classified as containing
Darija. We thus elected to assume all examples up
to the 61st of the 131 contain Darija, for a total of
2,745 examples (61*45=2,745). As an additional
check, we examined two more commentary entries
from each of the 61 blocks, finding that 118 of 122
contain Darija.
3.2 Initial Classifier
The integrated dialect classifier is a Maximum En-
tropy model (Berger et al, 1996) that we train us-
ing the LIBLINEAR (Fan et al, 2008) toolkit.
In preprocessing, Arabic diacritics are removed,
all non-alphabetic and non-Arabic script charac-
ters are converted to whitespace, and sequences of
any repeating character are collapsed to a single
character. The following set of feature templates
are applied to each of the resulting whitespace-
separated tokens:
? The full token
? ?Shape? of the token?all consonants are replaced by
the letter C, alefs by A, and waws and yehs by W
? First character plus the last character (if length ? 2)
? Character unigrams, bigrams, and trigrams
? The last character of the token plus the first character
of the next token
? Prefixes of length 1, 2, and 3
? Indicators that token starts with mA and
? ends with $
? the next token ends with $
? is length 5 or greater
3.3 LDA Model
As an exploratory effort, we investigated using La-
tent Direchlet Allocation (LDA) (Blei et al, 2003)
as a method of language identification. Unfor-
tunately, using the aforementioned feature tem-
plates, LDA produced topics that corresponded
poorly with the training data labels. But, after
several iterations of feature engineering, the topics
began to reflect the dialect distinctions. Our final
LDA model feature templates are listed below.
? The full token
? Indicators that the token contains
? theh; thal; zah; theh, thal, or zah
? Indicators the token is of length 5+ and starts with
? hah plus yeh, teh, noon, or alef
? seen plus yeh, teh, noon, or alef
? beh plus yeh, teh, noon, or alef
? ghain plus yeh, teh, or noon
? or kaf plus yeh, teh, or noon
? Indicators that token starts with mA and
? ends with $
? the next token ends with $
? is length 5 or greater
The following features produced using the LDA
model for each document are given to the Maxi-
mum Entropy classifier: 1) indicator of the most-
likely cluster, 2) product of scores for each pair of
clusters.
3.4 Classifier Evaluation
We evaluated the versions of the classifier by ap-
plying them to the annotated data discussed in
137
Section 2.1. The initial classifier without the
LDA-derived features achieved 96.9% precision
and 24.1% recall. The version with LDA-derived
features achieved 97.2% precision and 44.1% re-
call, a substantial improvement. Upon review, we
concluded that most cases where the classifier ?in-
correctly? selected the Darija label were due to er-
rors in the gold standard.
4 Analysis of Annotated Conversations
Visualization of Darija in Conversations
The DATOOL may recover the conversation in
which a tweet occurs, providing the annotator with
the tweet?s full, potentially-multilingual context.
To visualize the distribution of Darija1 by script
in ?1K tweets from each user?s conversations, the
DATOOL transforms and exports annotated data
into a GEXF information network (cf. Figure 3),
which can be displayed in GEPHI.2 Currently,
GEPHI displays at most one edge between any two
nodes?GEPHI automatically augments the edge?s
weight for each additional copy of the edge.
The Darija in this user?s conversations, unlike
our two other users, is predominantly Romanized.
With more data, we plan to assess the impact of
one user?s script and language choice on others.
Figure 3: Information network visualization.
Red?contains Romanized Darija; green?
contains Arabic-script Darija; blue?no Darija.
Code-Switching
The alternation of Darija with non-Darija in the
1In our initial annotation work, words and tweets in lan-
guages other than Darija received no markup.
2GEPHI?s Force Atlas layout automatically positions sub-
graphs by size, with larger ones further away from the center.
information network (red and green nodes vs.
blue nodes) within conversations is consistent with
well-known code-switching among Arabic speak-
ers, extending spoken discourse into informal
writing (Bentahila and Davies, 1983; Redouane,
2005). Code-switching also appears within our
tweet corpus where Romanized Darija frequently
alternates with French. Given the prevalence of
code-switching within tweets, future work will en-
tail training a Roman-script classifier at the to-
ken level.3 Since our DATOOL already supports
token-level as well as multi-token, tweet-internal
annotation in the mid-screen Message box, our
current corpus provides a seed set for this effort.
5 Conclusion and Future Work
The DATOOL now supports semi-automated an-
notation of tweet conversations for Darija. As
we scale the process of building low-resource lan-
guage corpora, we will document its impact on an-
notation time when few native speakers are avail-
able, a condition also relevant and critical to pre-
serving endangered languages. We have begun ex-
tending the classifier to support additional Arabic
script languages (e.g., Farsi, Urdu), leveraging re-
sources from others (Bergsma et al, 2012).
Many other open questions remain regarding
the annotation process, the visualizations, and the
human expert. Which classified examples should
the language expert review? When should an an-
notator adjust the confidence threshold in the DA-
TOOL? For deeper linguistic analysis and code-
switching prediction, would seeing participants
and tweets, turn by turn, in network diagrams such
as Figure 4 help experts understand new patterns
emerging in tweet conversations?
Figure 4: Social-Information Network Plus.
3As described in Section 3, our current classifier works at
the tweet level and only on Arabic-script tweets.
138
Acknowledgments
We would like to thank Tarek Abdelzaher for all
his feedback regarding our work and guidance in
using Apollo. We would also like to thank our re-
viewers for their valuable comments and sugges-
tions.
References
Mathieu Bastian, Sebastien Heymann, and Mathieu Ja-
comy. 2009. Gephi: An Open Source Software for
Exploring and Manipulating Networks. In Interna-
tional AAAI Conference on Weblogs and Social Me-
dia.
Abdelali Bentahila and Eirlys E Davies. 1983. The
Syntax of Arabic-French Code-Switching. Lingua,
59(4):301?330.
Adam L. Berger, Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A Maximum En-
tropy Approach to Natural Language Processing.
Computational Linguistics, 22(1):39?71.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
Identification for Creating Language-Specific Twit-
ter Collections. In Proceedings of the 2012 Work-
shop on Language in Social Media (LSM 2012),
pages 65?74.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. The Journal of Machine
Learning Research, 3:993?1022.
William B Cavnar and John M Trenkle. 1994. N-
gram-based text categorization. Ann Arbor MI,
48113(2):161?175.
Joaquim Ferreira da Silva and Gabriel Pereira Lopes.
2006. Identification of document language is not yet
a completely solved problem. In Computational In-
telligence for Modelling, Control and Automation,
2006 and International Conference on Intelligent
Agents, Web Technologies and Internet Commerce,
International Conference on, pages 212?212. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Hieu Khac Le, Jeff Pasternack, Hossein Ahmadi,
M. Gupta, Y. Sun, Tarek F. Abdelzaher, Jiawei Han,
Dan Roth, Boleslaw K. Szymanski, and Sibel Adali.
2011. Apollo: Towards factfinding in participatory
sensing. In IPSN, pages 129?130.
Paul McNamee. 2005. Language identification: A
solved problem suitable for undergraduate instruc-
tion. Journal of Computing Sciences in Colleges,
20(3):94?101.
Rabia Redouane. 2005. Linguistic constraints on
codeswitching and codemixing of bilingual Moroc-
can Arabic-French speakers in Canada. In ISB4:
Proceedings of the 4th International Symposium on
Bilingualism, pages 1921?1933.
Clinton Robinson and Karl Gadelii. 2003. Writing
Unwritten Languages, A Guide to the Process.
http://portal.unesco.org/education/en/ev.php-URL
ID=28300&URL DO=DO TOPIC&URL SECTIO
N=201.html, UNESCO, Paris, France. December.
Stephen Tratz and Antonio Sanfilippo. 2007. A
High Accuracy Method for Semi-supervised Infor-
mation Extraction. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
169?172.
Tommi Vatanen, Jaakko J Va?yrynen, and Sami Virpi-
oja. 2010. Language identification of short text seg-
ments with n-gram models. In Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation LREC?10.
Omar Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguistics
(To Appear).
139
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47,
Gothenburg, Sweden, April 27, 2014.
c
?2014 Association for Computational Linguistics
Resumptive Pronoun Detection
for Modern Standard Arabic to English MT
Stephen Tratz
?
Clare Voss
?
Jamal Laoudi
?
?
Army Research Laboratory, Adelphi, MD 20783
?
Advanced Resource Technologies, Inc. Alexandria, VA 22314
{stephen.c.tratz.civ,clare.r.voss.civ,jamal.laoudi.ctr}@mail.mil
Abstract
Many languages, including Modern Stan-
dard Arabic (MSA), insert resumptive pro-
nouns in relative clauses, whereas many
others, such as English, do not, using
empty categories instead. This discrep-
ancy is a source of difficulty when trans-
lating between these languages because
there are words in one language that cor-
respond to empty categories in the other,
and these words must either be inserted
or deleted?depending on translation di-
rection. In this paper, we first examine
challenges presented by resumptive pro-
nouns in MSA-English translations and re-
view resumptive pronoun translations gen-
erated by a popular online MSA-English
MT engine. We then present what is, to
the best of our knowledge, the first system
for automatic identification of resumptive
pronouns. The system achieves 91.9 F1
and 77.8 F1 on Arabic Treebank data when
using gold standard parses and automatic
parses, respectively.
1 Introduction
One of the challenges for modern machine trans-
lation (MT) is the need to systematically insert
or delete information that is overtly expressed
in only one of the languages in order to main-
tain intelligibility and/or fluency. For example,
word alignment between pro-drop and non-pro-
drop languages can be negatively impacted by the
systematic dropping of pronouns in only one of the
languages (Xiang et al., 2013). A similar type of
linguistic phenomenon of great interest to linguists
that has not yet received significant attention in
MT research is the mismatch between languages
in their usage of resumptive pronouns. Some lan-
guages, such as Modern Standard Arabic (MSA),
require the insertion of resumptive pronouns in
many relative clauses, whereas other languages,
including English, rarely permit them. An exam-
ple of an MSA sentence is given below, with its
English gloss showing the resumptive pronoun in
bold, its reference translation (RT), and an MT
system output where the roles of patient and doc-
tor are incorrectly reversed:

?J
.
fi


J
.
??@ ?

K
	
Y

?
	
K

@ ?


	
Y?@
	
?fl


Q?
?
@

IK



@P
Gloss: I.saw the.patient who rescued.him the.doctor.
RT: I saw the patient whom the doctor rescued.
MT: I saw a patient who rescued the doctor.
In this paper, we examine translations pro-
duced by a popular online translation system for
MSA resumptive pronouns occurring in several
different syntactic positions to gain insight into
the types of errors generated by current MT en-
gines. In a test suite of 300 MSA sentences with
resumptive pronouns, over 30% of the relative
clauses with resumptive pronouns were translated
inaccurately. We then present an automatic classi-
fier that we built for identifying MSA resumptive
pronouns and the results obtained from using it in
experiments with the Arabic Treebank (Maamouri
et al., 2004; Maamouri and Bies, 2004). The
system achieves 91.9 F1 and 77.8 F1 on Arabic
Treebank data when using gold standard parses
and automatic parses, respectively. To the best
of our knowledge, this is the first attempt to
automatically identify resumptive pronouns in any
language.
2 Relevant MSA Linguistics
MSA and English relative clauses differ in struc-
ture, with one of the most prominent differences
being in regard to resumptive pronouns. Resump-
tive pronouns are required in many MSA rela-
tive clauses but are almost never grammatical in
English. In MSA, like English, if the external
42
Arabic (. . .
	
?Q?

@) Gloss (I know...) English RT (I know...) MT Output (I know...)
1a @
Q
ffi



J? ???

Jfi
.

K ?



?? @

?YJ


??@ the+lady who
i

i
smiles a lot the lady who
i

i
smiles a lot the lady who smiles a lot
1b @
Q
ffi



J? ???

Jfi
.

K

?YJ


? lady ?
i
smiles 
i
a lot a lady who
i

i
smiles a lot a lot lady smiling
1c @
Q
ffi



J? ???

Jfi
.
K


	
?? who
i
smiles 
i
a lot who
i

i
smiles a lot a lot of smiles
2a ?g
.
Q?@ A???? ?



?? @

??Q?

?? @ the+company that
i
financed+it
i
the+man the company that
i
the man financed 
i
the company that financed the man
2b ?g
.
Q?@ A????

??Q?

? company ?
i
financed+it
i
the+man a company ?
i
the man financed 
i
a company funded by the man
2c ?g
.
Q?@ ???? A? what
i
financed+it
i
the+man what
i
the man financed 
i
what the man-funded
3a ???

?A

J
	
?? @

I???

K ?


	
Y?@ Y???@ the+boy whom
i
talked the+girl with+him
i
the boy whom
i
the girl talked with 
i
the boy who spoke with the girl
3b ???

?A

J
	
?? @

I???

K @Y?? boy ?
i
talked the+girl with+him
i
a boy ?
i
the girl talked with 
i
the girl was born I spoke with him
3c

?A

J
	
?? @

I???

K
	
?? ?? [with whom]
i
talked the+girl 
i
[with whom]
i
the girl talked 
i
from speaking with the girl
4a ??
	
Q
	
ffi? PA?
	
E @ ?


	
Y?@ ?g
.
Q?@ the+man who
i
collapsed house+his
i
the man [whose house]
i

i
collapsed a man who collapsed home
4b ??
	
Q
	
ffi? PA?
	
E @ Cg
.
P man ?
i
collapsed house+his
i
a man [whose house]
i

i
collapsed a man of his house collapsed
4c ??
	
Q
	
ffi? PA?
	
E @
	
?? who
i
collapsed house+his
i
[whose house]
i

i
collapsed of his house collapsed
5 ?



??
	
J? ?? A? what
i
it
i
logical what
i

i
is logical what is logical
Table 1: A list of MSA sentences starting with relative clauses
	
?Q?

@ (translation: I know) along with their
English glosses, English reference translation (RT), and the output of MT system X. Empty categories
are indicated with  and empty WH nodes are indicated with ?. Subscripts indicate coreference. To
avoid clutter, the glosses do not explicitly indicate person, number, or gender.
antecedent plays the role of the subject, no re-
sumptive pronoun is inserted
1
; instead, MSA in-
flects the verb to agree with the subject in number
and gender by attaching an affix
2
. A second sig-
nificant difference between the two languages is
that, in MSA, relative pronouns are required for
relative clauses modifying definite noun phrases
but are prohibited when modifying indefinite noun
phrases; in English, definitiveness neither prevents
nor necessitates the inclusion of a relative pro-
noun. A third significant difference is that, for free
relative clauses?that is, relative clauses that are
not attached to an external antecedent?MSA has
a different set of relative pronouns for introducing
the clause
3
. A fourth challenge is that MSA has no
equivalent word for the English word ?whose? and,
to convey a similar meaning, employs resumptive
pronouns as possessive modifiers. Examples illus-
trating these differences are provided in Table 1.
For further background on MSA relative clauses
and MSA grammar, we refer readers to books by
Ryding (2005) and Badawi et al. (2004).
1
A notable exception to this rule is for equational sen-
tences. MSA lacks an overt copula corresponding to the En-
glish word ?is? and, to convey a similar meaning, resumptive
subject pronouns must be inserted in these contexts.
2
In standard VSO and VOS constructions, the verbs in-
flect as singular regardless of the number of the subject.
3
These pronouns are also employed to introduce ques-
tions.
3 Data
In our research, we rely on the conversion of con-
stituent into dependency structures and the train-
ing/dev/test splits of the Arabic Treebank (ATB)
parts 1, 2, & 3 (Maamouri et al., 2004; Maamouri
and Bies, 2004) as presented by Tratz (2013).
We extract features from labeled dependency trees
(rather than constituent trees) generated by Tratz?s
(2013) Arabic NLP system, which separates cli-
tics, labels parts-of-speech, produces dependency
parses, and identifies and labels affixes.
The original ATB dependency conversion does
not mark pronouns for resumptiveness, so we
modify the conversion process to obtain this infor-
mation. The original ATB constituent trees mark
this by labeling WHNP nodes and NP nodes with
identical indices. If the NP node corresponds to a
null subject and the head of the S under the SBAR
is a verb, we mark the inflectional affix on the
verb, which agrees with the subject in gender and
number, as resumptive. These inflectional affixes
are included as their own category within our anal-
yses since their presence precludes the appearance
of another resumptive pronoun within the relative
clause (e.g., as a direct object).
The total number of resumptive pronouns and
?resumptive? inflectional affixes in the training,
dev, and test sections are presented in Table 2. In
43
Training Dev Test
Pronouns 5775 794 796
Inflectional affixes 6161 807 845
Table 2: Number of resumptive pronouns and ?re-
sumptive? inflectional affixes by data section.
the training data, the four most likely positions
4
for the resumptive pronouns are:
i) direct object of relative clause?s main verb (33.9%)
ii) object of a preposition attached to the verb (20.8%)
iii) possessive modifier of the subject of the verb (5.4%)
iv) subject pronoun in an equational sentence (4.2%).
4 Translation Error Analysis
As an exploratory exercise to gain insight into the
types of errors generated by current MT engines
when translating from a language that inserts re-
sumptive pronouns (i.e., MSA) to one that doesn?t
(i.e., English), we worked with a native Arabic
speaker to produce a list of Arabic sentences that
vary in terms of definitiveness (and existence, as
with free relatives) of the external antecedent, and
the syntactic position of the resumptive pronoun,
along with English glosses and reference transla-
tions for these sentences. This set was then pro-
cessed using a popular online translation system,
which we refer to as system X. The sentences,
their glosses, reference translations, and automatic
translations are presented in Table 1.
Although system X did not typically produce
English pronouns corresponding to the resumptive
pronouns in the source, most of the translations
proved problematic, with many of the issues be-
ing related to reordering. Thus, while system X
appears to be good at not translating resumptive
pronouns, its performance on the relative clauses
that contain them has ample room for improve-
ment. Our working hypothesis is that system X?s
English language model is effective in discount-
ing candidate translations that keep the resumptive
pronoun.
As a second exploratory exercise, we automat-
ically extracted all the resumptive pronoun exam-
ples in the training section of the data described
in Section 3 and grouped them based upon the se-
quence of dependency arc labels from the resump-
tive pronoun up to the head of the relative clause
4
Examples of these frequent configurations are in Table 1.
and the first letter of the POS tag of the interven-
ing words (e.g., ?N? for noun, ?A? for adjective).
For each of the thirty most common configura-
tions, we took ten examples (for a total of 300), ran
them through system X?s Arabic-English model
and gave both the translation and the source text
to our native Arabic expert. Our expert examined
whether 1) the translation engine generated a pro-
noun corresponding to the source side resumptive
pronoun and 2) whether the translation was correct
locally within the relative clause (whether the pro-
noun was retained or not)
5
. The results for these
two judgments are presented in Table 3.
Corresponding Pronoun?
Yes No
Correct?
Yes 17 189
No 20 74
Table 3: Expert judgments
Our expert concluded that a corresponding En-
glish pronoun was produced in only 37 of the
300 examples (12.3%). Seventeen of these were
judged correct, although in many of these cases a
significant portion of the relative clause was trans-
lated incorrectly even though a small portion in-
cluding the pronoun was translated properly, mak-
ing judgment difficult. Our expert noted that many
of the correct translations involved switching the
voice of the verb in the relative clause from ac-
tive to passive voice using a past participle. Of
the 189 that had no corresponding pronoun and
were judged correct, 46 (24.3%) involved switch-
ing to passive voice. In general, it appears that
system X does a good job at not generating En-
glish pronouns corresponding to MSA resumptive
pronouns, although it makes numerous mistakes
with the data we presented to it.
5 System Description
Our MSA resumptive pronoun identification sys-
tem processes one sentence at a time and relies
upon the (averaged) structured perceptron algo-
rithm (Collins, 2002) to rank the feasible actions.
When processing a sentence containing n pro-
nouns and affixes, a total of n iterations are per-
formed. During each processing iteration, the
system considers two actions for every unlabeled
5
This latter task was challenging, but permitted, as in-
tended, lenient judgment of the MT output.
44
Function Definitions:
path(x) ? returns a list of dependency arcs from x up through the first ?ripcmp?, ?rcmod?, or ?ROOT? arc (link from affix to the
core word is also treated as an arc)
rDescendants(x) ? returns a list of paths (dependency arc lists) from x to each descendant already marked as resumptive
pDescendants(x) ? returns a list of paths (dependency arc lists) from x to each pronoun / verbal inflectional affix, not following
?cc?, ?ripcmp?, or ?rcmod? arcs
hasDepArc(x,y) ? returns a Boolean value indicating if an arc with label y descends from x
pathToString(x) ? concatenates the labels of the arcs in a list to create a string
last(x) ? returns the last element in the list x
split(x, y) ? splits a string x apart wherever it contains substring y, returning these pieces
deps(x), parent(x) ? return dependency arc(s) of which x is the {head, child}
head(x), child(x) ? returns the {head, child} of arc x
pro(x) ? if x is an affix, the word attached to it is returned, otherwise x is returned
l(x) ? return the label/part-of-speech for a dependency arc, affix, or word
T(x), t(x), suffixes(x) ? return the {type (?affix? or ?pro?), written text, suffixes} for x
n(x,y) ? returns the word node that is y words after pro(x)
Given: p ? pronoun or inflectional affix
Pseudocode:
?0:?+T(p), ?1:?+t(p), ?2:?+l(p), ?3:?+l(parent(p)), for(s in split(l(p),? ?)) { ?4:?+s }
if(T(p)=?affix?) { for(a in deps(pro(p))) { ?5:?+l(a) }, if(T(p)=?pro? or not(hasDepArc(pro(p), ?subj?))) { ?6? }
for(i in {-3,-2,-1,0,+1,+2,+3,+4}) { ?7:?+i+t(n(pro(p),i)), ?8:?+i+l(n(pro(p),i)), ?9:?+i+l(parent(n(pro(p),i))) }
?10:?+pathToString(path(p)), end := last(path(p)), resumptives := rDescendants(child(end))
if(l(end) != ?ROOT?) {
if(size(resumptives) > 0) {?11a? } else {?11b?+(size(pDescendants(child(end))) > 0)}
for(s in split(l(head(end)), ? ?)) ?12:?+s, for(arc in path(p)) { ?13?+l(arc) }
?14:?+t(head(end)), ?15:?+l(head(end)), ?16:?+l(parent(head(end)))
?17:?+t(child(end)), ?18:?+l(child(end)), ?19:?+l(parent(child(end)))
if(l(child(end)) = ?VB PV? and size(suffixes(child(end)))=0) { ?20? }
for(suff in suffixes(head(end))) { for(s in split(l(suff), ? ?)) { ?21:?+suff }} }
Figure 1: Pseudocode for feature production. Statements in bold font produce strings that are used to
identify features. The feature set consists of all pairwise combinations of these strings.
personal pronoun and inflectional verbal affix
6
within a given sentence, these actions being label-
as-?resumptive? and label-as-?not-resumptive?.
The highest scored action is performed and the
newly-labeled pronoun or affix is removed from
further processing.
The system scores each action by computing the
dot product between the feature vector derived for
the pronoun/inflectional affix and the weight vec-
tor. The feature vectors consist entirely of Boolean
values, each of which indicates the presence or ab-
sence of a particular feature. Each feature is iden-
tified by a unique string and these strings are gen-
erated using the pseudocode presented in Figure
1. (All pairwise combinations of the strings gen-
erated by the pseudocode are included as features.)
For space reasons, we omit a review of the train-
ing procedure for the structured perceptron and re-
fer the interested reader to work by Goldberg and
Elhadad (2010).
6
Occasionally an imperfect verb will have both a written
inflectional prefix and a written inflectional suffix. For these
cases, the system only considers the prefix as there is no need
to make two separate judgments.
6 Experiments
We trained our system on the training data us-
ing the gold standard clitic segmentation, parse,
and part-of-speech information and optimized it
for overall F1 (pronouns and inflectional affixes
combined) on the development data. Performance
peaked on training iteration 8, and we applied the
resulting model to two treatments of the test data,
once using the gold standard annotation and once
using the Tratz (2013) Arabic NLP system to au-
tomatically pre-process the data.
6.1 Results and Discussion
The scores for the development and test sections,
both for gold and automatic annotation, are pre-
sented in Table 4.
The system performs well when given input
with gold standard clitic segmentation, POS tags,
and dependency parses, achieving 91.9 F1 for re-
sumptive pronouns on the test set and 95.4 F1 for
the affixes. Performance however degrades sub-
stantially when automatic pre-processing of the
source is input instead. Some of this drop can
be explained by the use of gold standard markup
in training?more weight was likely assigned to
45
Pronoun Inflectional Affix
P R F1 P R F1
Dev
Gold 92.5 92.8 92.6 96.7 96.4 96.5
Auto 88.0 81.0 84.4 86.1 77.3 81.5
Test
Gold 92.1 91.7 91.9 95.0 95.9 95.4
Auto 83.6 72.8 77.8 86.6 76.0 81.0
Table 4: Precision, recall, and F1 results for the
?is-resumptive? label on the development and test
sets for gold standard clitic separation/POS tag-
ging/parsing and automatic preprocessing.
parse and POS tag-related features than would
have if automatic pre-processing of the source had
been used in training.
Having examined the classification system er-
rors on the development data, we conclude that
the main source of this drop is due to poor iden-
tification and attachment of bare relatives
7
by the
Tratz (2013) NLP system. While the NLP system
achieves 88.5 UAS and 86.1 LAS on the develop-
ment section,
8
its performance on identifying bare
relatives is comparatively low, with 70.0 precision
and 60.5 recall. For the test section, the NLP sys-
tem performance on bare relatives is even lower at
69.6 precision and 52.7 recall. This helps to ex-
plain why our resumptive pronoun classifier per-
forms worse on the test data than on the devel-
opment data when using automatic pre-processing
but not when using gold standard markup.
7 Related Work
The computational linguistics research most rele-
vant to ours is the work on identifying empty cat-
egories for several languages, including English,
Chinese, Korean, and Hindi. Empty categories
are nodes in a parse tree that do not correspond
to any written morpheme; these are used to han-
dle several linguistic phenomena, including pro-
drop. Recent research demonstrates that recovery
of empty categories can lead to improved transla-
tion quality for some language pairs (Chung and
Gildea, 2010; Xiang et al., 2013). For more in-
formation on the recovery of empty categories, we
refer the interested reader to work by Kukkadapu
and Mannem (2013), Cai et al. (2011), Yang and
Xue (2010), Gabbard et al. (2006), Schmid (2006),
Dienes and Dubey (2003), and Johnson (2002).
7
Relative clauses lacking a relative pronoun. As explained
in Section 2, MSA lacks relative pronouns for relative clauses
modifying indefinite noun phrases.
8
UAS and LAS stand for unlabeled and labeled attach-
ment scores.
8 Conclusion
In this paper, we present the challenge of translat-
ing MSA relative clauses, which often contain re-
sumptive pronouns, into English, which relies on
(inferred) empty categories instead. We examine
errors made by a popular online translation service
on MSA relative clauses and present an automatic
system for identifying MSA resumptive pronouns.
The online translation service occasionally gen-
erates English pronouns corresponding to MSA
resumptive pronouns, producing resumptive pro-
nouns for only 37 of 300 examples that cover a
variety of frequent MSA relative clause structures.
Our MSA resumptive pronoun identification
system achieves high levels of precision (92.1)
and recall (91.7) on resumptive pronoun identifi-
cation when using gold standard markup. Perfor-
mance drops significantly when using automatic
pre-processing, with precision and recall falling to
83.6 and 72.8, respectively. One of the sources
of the drop appears to be the weak performance
of the Tratz (2013) Arabic NLP system in identi-
fying and attaching bare relative clauses?that is,
relative clauses that lack a relative pronoun.
This work is the first attempt we are aware of to
automatically identify resumptive pronouns in any
language, and it presents a baseline for compari-
son for future research efforts.
9 Future Work
Going forward, we plan to experiment with apply-
ing our resumptive pronoun identifier to enhance
MT performance, likely by deleting all resumptive
pronouns during alignment and, again, at transla-
tion time. Another natural next step is to train the
system using automatically generated parse, part-
of-speech tag, and clitic segmentation information
instead of gold standard annotation to see if this
produces a similar drop in performance. We also
plan to investigate the use of frame information of
Arabic VerbNet (Mousser, 2010) as features, and
we would like to focus in greater detail on the dif-
ficulties in generating resumptive pronouns when
translating from English into MSA.
References
Elsaid Badawi, Michael G. Carter, and Adrian Gully.
2004. Modern Wrtitten Arabic: A Comprehensive
Grammar. Psychology Press.
46
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In ACL (Short Papers), pages 212?216.
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636?
645.
Michael J. Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory and
experiments with Perceptron Algorithms. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing.
P?eter Dienes and Amit Dubey. 2003. Antecedent re-
covery: Experiments with a trace tagger. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 33?40.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the penn treebank. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 184?191.
Y. Goldberg and M. Elhadad. 2010. An efficient
algorithm for easy-first non-directional dependency
parsing. In HLT-NAACL 2010.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 136?143.
Puneeth Kukkadapu and Prashanth Mannem. 2013. A
statistical approach to prediction of empty categories
in hindi dependency treebank. In Fourth Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages, page 91.
Mohamed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, Guidelines, Proce-
dures, and Tools. In Proceedings of the Workshop on
Computational Approaches to Arabic Script-based
languages, pages 2?9.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109.
Jaouad Mousser. 2010. A Large Coverage Verb Taxon-
omy for Arabic. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Eval-
uation.
Karin C. Ryding. 2005. A Reference Grammar of
Modern Standard Arabic. Cambridge University
Press.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized pcfgs and slash features. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 177?184.
Stephen Tratz. 2013. A cross-task flexible transition
model for arabic tokenization, affix detection, affix
labeling, pos tagging, and dependency parsing. In
Fourth Workshop on Statistical Parsing of Morpho-
logically Rich Languages.
Bing Xiang, Xiaoqiang Luo, and Bowen Zhou. 2013.
Enlisting the Ghost: Modeling Empty Categories for
Machine Translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistic.
Yaqin Yang and Nianwen Xue. 2010. Chasing the
ghost: recovering empty categories in the chinese
treebank. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1382?1390.
47
