Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 33?36,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Part of Speech Tagger for Assamese Text
Navanath Saharia
Department of CSE
Tezpur University
India - 784028
Dhrubajyoti Das
Department of CSE
Tezpur University
India - 784028
{nava tu,dhruba it06,utpal}@tezu.ernet.in
Utpal Sharma
Department of CSE
Tezpur University
India - 784028
Jugal Kalita
Department of CS
University of Colorado
Colorado Springs - 80918
kalita@eas.uccs.edu
Abstract
Assamese is
a morphologically rich, agglutinative and
relatively free word order Indic language.
Although spoken by nearly 30 million
people, very little computational linguistic
work has been done for this language. In
this paper, we present our work on part
of speech (POS) tagging for Assamese
using the well-known Hidden Markov
Model. Since no well-defined suitable
tagset was available, we develop a tagset
of 172 tags in consultation with experts
in linguistics. For successful tagging,
we examine relevant linguistic issues in
Assamese. For unknown words, we
perform simple morphological analysis
to determine probable tags. Using a
manually tagged corpus of about 10000
words for training, we obtain a tagging
accuracy of nearly 87% for test inputs.
1 Introduction
Part of Speech (POS) tagging is the process of
marking up words and punctuation characters in
a text with appropriate POS labels. The problems
faced in POS tagging are many. Many words that
occur in natural language texts are not listed in any
catalog or lexicon. A large percentage of words
also show ambiguity regarding lexical category.
The challenges of our work on POS tagging
for Assamese, an Indo-European language, are
compounded by the fact that very little prior
computational linguistic exists for the language,
though it is a national language of India and
spoken by over 30 million people. Assamese is a
morphologically rich, free word order, inflectional
language. Although POS tagged annotated
corpus for some of the Indian languages such as
Hindi, Bengali, and Telegu (SPSAL, 2007) have
become available lately, a POS tagged corpus for
Assamese was unavailable till we started creating
one for the work presented in this paper. Another
problem was that a clearly defined POS tagset for
Assamese was unavailable to us. As a part of the
work reported in this paper, we have developed
a tagset consisting of 172 tags, using this tagset
we have manually tagged a corpus of about ten
thousand Assamese words.
In the next section we provide a brief relevant
linguistic background of Assamese. Section 3
contains an overview of work on POS tagging.
Section 4 describes our experimental setup. In
Section 5, we analyse the result of our work
and compare the performance with other models.
Section 6 concludes this paper.
2 Linguistic Characteristics of Assamese
In Assamese, secondary forms of words are
formed through three processes: affixation,
derivation and compounding. Affixes play a very
important role in word formation. Affixes are used
in the formation of relational nouns and pronouns,
and in the inflection of verbs with respect to
number, person, tense, aspect and mood. For
example, Table 1 shows how a relational noun
edtA (deutA: father) is inflected depending on
number and person (Goswami, 2003). Though
Assamese is relatively free word order, yet the
predominant word order is subject-object-verb
(SOV).
The following paragraphs describe just a few
of the many characteristics of Assamese text that
make the tagging task complex.
? Depending on the context, even a common
word may have different
POS tags. For example: If kAreN (kArane),
der (dare), inime? (nimitte), ehtu (hetu), etc.,
are placed after pronominal adjective, they
are considered conjunction and if placed after
33
Table 1: Personal definitives are inflected on
person and number
Person Singular Plural
1
st
My father Our father
pzm emAr edtA aAmAr edtA
mor deutA aAmAr deutA
2
nd
Your father Your father
mAn mxm etAmAr edtArA etAmAelAkr edtArA
tomAr deutArA tomAlokar deutArA
2
nd
, Familiar Your father Your father
tu? mxm etAr edtAr thtwr edtAr
tor deutAr tahator deutAr
3
rd
Her father Their father
ttIy tAr edtAk ishwtr edtAk
tAir deutAk sihator deutAk
noun or personal pronoun they are considered
particle. For example,
 kAreN m ngelwA.
TF
1
: ei kArane moi nagalo.
This + why + I+ did not go.
ET
2
: This is why I did not go.
rAmr kAreN m ngelwA.
TF : rAmar kArane moi nagalo.
Ram?s + because of + I + did not go
ET : I did not go because of Ram.
In the first sentence kAreN (kArne) is placed
after pronominal adjective  (ei); so kArne
is considered conjunction. But in the
second sentence kArne is placed after noun
rAm (RAm), and hence kArne is considered
particle.
? Some prepositions or particles are used as
suffix if they occur after noun, personal
pronoun or verb. For example,
iseh EgiCl. TF: sihe goisil.
ET : Only he went.
Actually eh (he : only) is a particle, but it is
merged with the personal pronoun is (si).
? An affix denoting number, gender or person,
can be added to an adjective or other category
word to create a noun word. For example,
xunIyAjnI Eh aAihCA.
TF : dhuniyAjoni hoi aAhisA.
ET : You are looking beautiful.
Here xunIyA (dhuniyA : beautiful) is an
adjective, but after adding feminine suffix jnI
the whole constituent becomes a noun word.
1
TF : Transliterated Assamese Form
2
ET : Aproximate English Translation
? Even conjunctions can be used as other part
of speech.
hir aA Ydu vAeyk kkAeyk.
TF : Hari aAru Jadu bhAyek kokAyek.
ET : Hari and Jadu are brothers.
eYAWAkAil rAitr GTnAeTAeW ibFyeTAk aA aixk
rhsjnk kir tuilel.
TF : JowAkAli rAtir ghotonAtowe bishoitok
aAru adhik rahashyajanak kori tulile.
ET : The last night incident has made the
matter more mysterious.
The word aA (aAru : and) shows ambiguity
in these two sentences. In the first, it is used
as conjunction (i.e. Hari and Jadu) and in the
second, it is used as adjective of adjective.
3 Related Work
Several approaches have been used for building
POS taggers. Two main approaches are
supervised and unsupervised. Both supervised and
unsupervised tagging can be of three sub-types.
They are rule based, stochastic based and neural
network based. There are number of pros and cons
for each of these methods. The most common
stochastic tagging technique is Hidden Markov
Model (HMM).
During the last two
decades, many different types of taggers have been
developed, especially for corpus rich languages
such as English. Nevertheless, due to relatively
free word order, agglutinative nature, lack of
resources and the general lateness in entering the
computational linguistics field in India, reported
tagger development work on Indian languages
is relatively scanty. Among reported works,
Dandapat (2007) developed a hybrid model of
POS tagging by combining both supervised and
unsupervised stochastic techniques. Avinesh and
Karthik (2007) used conditional random field and
transformation based learning. The heart of the
system developed by Singh et al (2006) for Hindi
was the detailed linguistic analysis of morpho-
syntactic phenomena, adroit handling of suffixes,
accurate verb group identification and learning
of disambiguation rules. Saha et al (2004)
developed a system for machine assisted POS
tagging of Bangla corpora. Pammi and Prahllad
(2007) developed a POS tagger and chunker
using Decision Forests. This work explored
different methods for POS tagging of Indian
languages using sub-words as units. Generally,
most POS taggers for Indian langauages use
34
morphological analyzer as a module. However,
building morphological analyzer of a particular
Indian language is a very difficult task.
4 Our Approach
We have used a Assamese text corpus (Corpus
Asm) of nearly 300,000 words from the online
version of the Assamese daily Asomiya Pratidin
(Sharma et al, 2008). The downloaded articles
use a font-based encoding called Luit. For
our experiments we transliterate the texts to a
normalised Roman encoding using transliteration
software developed by us. We manually tag a
part of this corpus, Tr, consisting of nearly 10,000
words for training. We use other portions of
Corpus Asm for testing the tagger.
There was no tagset for Assamese before we
started the project reported in this paper. Due to
the morphological richness of the language, many
words of Assamese occur in secondary forms in
texts. This increases the number of POS tags
that needed for the language. Also, often there
are differences of opinion among linguists on the
tags that may be associated with certain words
in texts. We developed a tagset after in-depth
consultation with linguists and manually tagged
text segments of nearly 10,000 words according to
their guidance. To make the tagging process easier
we have subcategorised each category of noun
and personal pronoun based on six case endings
(viz, nominative, accussative, instumental, dative,
genitive and locative) and two numbers.
We have used HMM
(Dermatas and Kokkinakis, 1995) and the Viterbi
algorithm (1967) in developing our POS tagger.
HMM/Viterbi approach is the most useful method,
when pretagged corpus is not available. First, in
the training phase, we have manually tagged the
Tr part of the corpus using the tagset discussed
above. Then, we build four database tables
using probabilities extracted from the manually
tagged corpus- word-probability table, previous-
tag-probability table, starting-tag-probability table
and affix-probability table.
For testing, we consider three text segments, A,
B and C, each of about 1000 words. First the input
text is segmented into sentences. Each sentence
is parsed individually. Each word of a sentence
is stored in an array. After that, each word is
searched in the word-probability table. If the
word is unknown, its possible affixes are extracted
Table 2: POS tagging results with small corpora.
Size of training words : 10000, UWH : Unknown word
handling, UPH : Unknown proper noun handling
Test Size Average UDH UPH
set accuracy accuracy accuracy
A 992 84.68% 62.8% 42.0%
B 1074 89.94% 67.54% 53.96%
C 1241 86.05% 85.64% 26.47%
Table 3: Comparison of our result with other
HMM based model.
Author Language Average
accuracy
Toutanova et al(2003) English 97.24%
Banko and Moore(2004) English 96.55%
Dandapat and Sarkar(2006) Bengali 84.37%
Rao et al(2007)
Hindi 76.34%
Bengali 72.17%
Telegu 53.17%
Rao and Yarowsky(2007)
Hindi 70.67%
Bengali 65.47%
Telegu 65.85%
Sastry et al(2007)
Hindi 69.98%
Bengali 67.52%
Telegu 68.32%
Ekbal et al(2007)
Hindi 71.65%
Bengali 80.63%
Telegu 53.15%
Ours Assamese 85.64%
and searched in the affix-probability table. From
this search, we obtain the probable tags and
their corresponding probabilities for each word.
All these probable tags and the corresponding
probabilities are stored in a two dimensional array
which we call the lattice of the sentence. If we
do not get probable tags and probabilities for a
certain word from these two tables we assign tag
CN (Common Noun) and probability 1 to the
word since occurrence of CN is highest in the
manually tagged corpus. After forming the lattice,
the Viterbi algorithm is applied to the lattice that
yields the most probable tag sequence for that
sentence. After that next sentence is taken and the
same procedure is repeated.
5 Experimental Evaluation
The results using the three test segments are
summarised in Table 2. The evaluation of the
results require intensive manual verification effort.
Larger training corpora is likely to produce more
accurate results. More reliable results can be
obtained using larger test corpora. Table 3
compares our result with other HMM based
reported work. Form the table it is clear that
35
Toutanova et al (2003) obtained the best result
for English (97.24%). Among HMM based
experiments reported on Indian languages, we
have obtained the best result (86.89%). This work
is ongoing and the corpus size and the amount of
tagged text are being increased on a regular basis.
The accuracy of a tagger depends on the size of
tagset used, vocabulary used, and size, genre and
quality of the corpus used. Our tagset containing
172 tags is rather big compared to other Indian
language tagsets. A smaller tagset is likely to
give more accurate result, but may give less
information about word structure and ambiguity.
The corpora for training and testing our tagger are
taken form an Assamese daily newspaper Asomiya
Pratidin, thus they are of the same genre.
6 Conclusion & Future work
We have achieved good POS tagging results for
Assamese, a fairly widely spoken language which
had very little prior computational linguistic work.
We have obtained an average tagging accuracy
of 87% using a training corpus of just 10000
words. Our main achievement is the creation of
the Assamese tagset that was not available before
starting this project. We have implemented an
existing method for POS tagging but our work is
for a new language where an annotated corpora
and a pre-defined tagset were not available.
We are currently working on developing a
small and more compact tagset. We propose
the following additional work for improved
performance. First, the size of the manually
tagged part of the corpus will have to be
increased. Second, a suitable procedure for
handling unknown proper nouns will have to be
developed. Third, if this system can be expanded
to trigrams or even n-grams using a larger training
corpus, we believe that the tagging accuracy will
increase.
Acknowledgemnt
We would like to thank Dr. Jyotiprakash Tamuli,
Dr. Runima Chowdhary and Dr. Madhumita
Barbora for their help, specially in making the
Assamese tagset.
References
Avinesh PVS & Karthik G. POS tagging and chunking using
Conditional Random Field and Transformation based
learning. IJCAI-07 workshop on Shallow Parsing for
South Asian Languages. 2007.
Banko, M., & Robert Moore, R. Part of speech tagging in
context. 20th International Conference on Computational
Linguistics. 2004.
Dandapat, S. Part-of-Speech Tagging and Chunking with
Maximum Entropy Model. Workshop on Shallow Parsing
for South Asian Languages. 2007.
Dandapat, S., & Sarkar, S. Part-of-Speech Tagging for
Bengali with Hidden Markov Model. NLPAI ML
workshop on Part of speech tagging and Chunking for
Indian language. 2006.
Dermatas, S., & Kokkinakis, G. Automatic stochastic
tagging of natural language text. Computational
Linguistics 21 : 137-163. 1995.
Ekbal, A., Mandal, S., & Bandyopadhyay, S. POS tagging
using HMM and rule based chunking . Workshop on
Shallow Parsing for South Asian Languages. 2007.
Goswami, G. C. Asam
?
iy?a Vy?akaran
.
Pravesh, Second edition.
Bina Library, Guwahati. 2003.
http://shiva.iiit.ac.in/SPSAL2007. IJCAI-07 workshop on
Shallow Parsing for South Asian Languages. Hyderabad,
India.
Pammi, S.C., & Prahallad, K. POS tagging and chunking
using Decision Forests. Workshop on Shallow Parsing for
South Asian Languages. 2007.
Rao, D., & Yarowsky, D.. Part of speech tagging and
shallow parsing of Indian languages. IJCAI-07 workshop
on Shallow Parsing for South Asian Languages. 2007.
Rao, P.T., & Ram, S.R., Vijaykrishna, R. & Sobha L. A
text chunker and hybrid pos tagger for Indian languages.
IJCAI-07 workshop on Shallow Parsing for South Asian
Languages. 2007.
Saha, G.K., Saha, A.B., & Debnath, S. Computer
Assisted Bangla Words POS Tagging. Proc. International
Symposium on Machine Translation NLP & TSS. 2004.
Sastry, G.M.R., Chaudhuri, S., & Reddy, P.N. A HMM
based part-of-speech and statistical chunker for 3 Indian
languages. IJCAI-07 workshop on Shallow Parsing for
South Asian Languages. 2007.
Sharma, U., Kalita, J. & Das, R. K. Acquisition of
Morphology of an Indic language from text corpus. ACM
TALIP 2008.
Singh, S., Gupta K., Shrivastava, M., & Bhattacharyya,
P. Morphological richness offsets resource demand-
experiences in constructing a POS tagger for Hindi.
COLING/ACL. 2006.
Toutanova, K., Klein, D., Manning, C.D. & Singer,
Y. Feature-Rich part-of-speech tagging with a Cyclic
Dependency Network. HLT-NAACL. 2003.
Viterbi, A.J. Error bounds for convolutional codes and
an asymptotically optimum decoding algorithm. IEEE
Transaction on Information Theory 61(3) : 268-278.
1967.
36
 	

ffHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 685?688,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Summarizing Microblogs Automatically 
 
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita 
University of Colorado at Colorado Springs 
1420 Austin Bluffs Parkway 
Colorado Springs, CO 80918, USA 
{bsharifi, mhutton, jkalita}@uccs.edu 
 
  
 
Abstract 
In this paper, we focus on a recent Web trend 
called microblogging, and in particular a site 
called Twitter.  The content of such a site is an 
extraordinarily large number of small textual 
messages, posted by millions of users, at ran-
dom or in response to perceived events or sit-
uations.  We have developed an algorithm that 
takes a trending phrase or any phrase specified 
by a user, collects a large number of posts 
containing the phrase, and provides an auto-
matically created summary of the posts related 
to the term.  We present examples of summa-
ries we produce along with initial evaluation. 
1 Introduction 
Since Twitter?s inception in 2006, it has grown at 
an unprecedented rate.  In just four years, the ser-
vice has grown to approximately 20 million unique 
visitors each month with users sending short 140-
character messages (known as ?tweets?) approx-
imately 40 million times a day.  While the majority 
of these tweets are pointless babble or conversa-
tional, approximately 3.6% of these posts are top-
ics of mainstream news (Pear Analytics, 2009).  
For example, Twitter has been cited as breaking 
many important events before traditional media, 
such as the attacks in Mumbai and the crash of the 
US Airways flight into the Hudson River. 
In order to help users sort through the vast num-
ber of tweets that occur each day, Twitter.com has 
added a number of tools.  For instance, Twitter?s 
homepage displays important topics for three dif-
ferent ranges of time in order to see what topics are 
popular.  For most topics, users are forced to read 
through related posts in order to try and understand 
why a topic is trending.  In order to help users fur-
ther, Twitter has partnered with the third-party 
website WhatTheTrend1 in order to provide defini-
tions of trending topics.  WhatTheTrend allows 
users to manually enter descriptions of why a topic 
is trending.  Unfortunately, WhatTheTrend suffers 
with spam and rants as well as lag time before a 
new trending topic is defined by a user. 
While WhatTheTrend is a step in the right direc-
tion, a better approach is to automatically summar-
ize important events as they occur in real time.  We 
have developed such a method.  Our method can 
automatically summarize a collection of micro-
blogging posts that are all related to a topic into a 
short, one-line summary.  Our results show that our 
automated summarizer produces summaries that 
are close to human-generated summaries for the 
same set of posts.  For example, Table 1 below 
contains a sample of automatically produced sum-
maries for some recently trending topics on Twit-
ter. 
2 Related Work  
Some early work focused on summarizing results 
of database queries for presentation during natural 
language interactions (e.g., Kalita et al, 1986).  
Most summaries are generated for the purposes of 
providing a ?gist? of a document or a set of docu-
ments to human readers (e.g., Luhn, 1958; Bran-
dow et al, 1995).  Summaries are sometimes also 
used as inputs to machine learning approaches, say 
for categorization.  Kolcz et al (2001) summarize 
textual documents in order to classify them, using 
the summaries as a feature to be input to a classifi-
er.  Most early studies used a news corpus like the 
Reuters dataset.  As the Web started growing in 
size, the focus moved to Web pages. 
                                                        
1
 http://www.whatthetrend.com 
685
Table 1. Example Summaries Produced by the Phrase Reinforcement Algorithm. 
 
For example, Mahesh (1997) examines the effec-
tiveness of Web document summarization by sen-
tence extraction.  Recently, there has been work on 
summarizing blogs (e.g. Zhou and Hovy, 2006; Hu 
et al, 2007).  Most techniques focus on extraction: 
the selecting of salient pieces of documents in or-
der to generate a summary.  Applying extraction 
on microblogs at first appears irrelevant since a 
microblog post is already shorter than most sum-
maries.  However, extraction is possible when one 
considers extracting from multiple microblogs 
posts that are all related to a central theme. 
3 Approach  
3.1 Twitter API 
Through an entirely HTTP-based API provided by 
Twitter, users can programmatically perform al-
most any task that can be performed via Twitter?s 
web interface.  For non-whitelisted users, Twitter 
restricts a user to 150 requests/hour.  Furthermore, 
searches are limited to returning 1500 posts for a 
given request.  Our summarizer has been shown to 
produce comparable automated summaries to hu-
man summaries with as few as 100 posts. 
3.2 Phrase Reinforcement Algorithm 
Given a trending topic, one can query Twitter.com 
for posts that contain the topic phrase.  Presently, 
users would have to read these posts in order to 
comprehend and manually summarize their con-
tent.  Instead, we automate this process using our 
Phrase Reinforcement Algorithm. 
The central idea of the Phrase Reinforcement 
(PR) algorithm is to find the most commonly used 
phrase that encompasses the topic phrase.  This 
phrase is then used as a summary.  The algorithm 
was inspired from two simple observations: (1) 
users will often use the same word or sets of words 
adjacent to the topic phrase when describing a key 
idea and (2) users will often ?re-tweet? (a Twitter 
form of quoting) the most relevant content for a 
trending topic.  These two patterns create highly 
overlapping sequences of words when considering 
a large number of posts for a single topic.  The PR 
algorithm capitalizes on these behaviors in order to 
generate a summary. 
The Phrase Reinforcement algorithm begins 
with a starting phrase.  This is typically a trending 
topic, but can be non-trending as well.  Given the 
starting phrase, the PR algorithm submits a query 
to Twitter.com for a list of posts that each contains 
the phrase.  Once the posts are retrieved, the algo-
rithm filters the posts to remove any spam or other 
sources of irrelevant data (e.g. hyperlinks).  Filter-
ing is an important step in order to focus the algo-
rithm on the most relevant content.  We filter any 
spam by using a Na?ve Bayes classifier which we 
trained using previously gathered spam content 
from Twitter.com.  Next, non-English posts as well 
as duplicate posts are removed since we are con-
cerned with English summaries only and want to 
prevent a single user from dominating a topic.  Fi-
nally, given a set of relevant posts, we isolate the 
longest sentence from each post that contains the 
topic phrase.  These sentences form the input into 
the PR algorithm. 
Once we have the set of input sentences, the PR 
algorithm formally begins.  The algorithm starts by 
building a graph representing the common se-
quences of words (i.e. phrases) that occur both be-
Topic Automated Summary Date 
Ice Dancing Canadians Tessa Virtue and Scott Moir clinch the gold in Olympic ice 
dancing; U.S. pair Davis and White win silver 
2/22/2010 
Dodgers Phillies defeat Dodgers to take the National League Championship series. 10/21/2009 
Limbaugh Limbaugh dropped from group bidding for St. Louis Rams 10/14/2009 
Dow Jones The Dow Jones Industrial Average passes 10,000 for the first time since 
October 7th, 2008. 
10/14/2009 
Captain Lou Wrestler, personality Captain Lou Albano dies at 76 10/14/2009 
Bloomberg Bloomberg Acquires Businessweek for Less Than $5 million 10/13/2009 
G20 Trouble breaks out at G20 summit: Protesters and riot police have clashed 
ahead of the G20 summit in Pittsburgh 
09/24/2009 
AT&T AT&T plans for iPhone MMS to arrive Friday 09/23/2009 
686
fore and after the topic phrase.  The graph is gen-
erated such that it centers about a common root 
node representing the topic phrase.  Adjacent to the 
root node are chains of common sequences of 
words found within the input sentences.  In par-
ticular, each word is represented by a node and an 
associated count that indicates how many times the 
node?s phrase occurs within the set of input sen-
tences.  The phrase of a node is simply the se-
quence of words generated by following the path 
from the node to the root node.  To illustrate, con-
sider the following set of input sentences for the 
topic ?Ted Kennedy?. 
 
1. A tragedy: Ted Kennedy died today of 
cancer 
2. Ted Kennedy died today 
3. Ted Kennedy was a leader 
4. Ted Kennedy died at age 77 
   
Using these sentences, the PR algorithm would 
generate a graph similar to the one shown below in 
Figure 1. 
 
Figure 1. Example Phrase Reinforcement Graph. 
 
In Figure 1, we see the node ?today? has a count 
of two.  This indicates that the phrase ?Ted Kenne-
dy died today? occurs exactly two times within the 
set of input sentences (in sentences 1 and 2).  
Likewise, the node ?tradegy? has a count of one 
indicating the phrase ?tragedy Ted Kennedy? only 
occurs one time (in sentence 1).  In actuality, the 
PR algorithm would only add nodes to the graph 
with a count of at least two since it is looking for 
the most common phrase.  These are shown as the 
black nodes in Figure 1.  However, Figure 1 also 
includes unique nodes (shown in white) for helping 
illustrate the graph?s structure. 
After the graph is constructed, the PR algorithm 
assigns a weight to every node in order to prevent 
longer phrases from dominating the output.  In par-
ticular, stop words are given a weight of zero while 
remaining words are given weights that are both 
proportional to their count and penalized the farth-
er they are from the root node: 
  
	
  		

 			

? log2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 631?635,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Summarization of Historical Articles Using Temporal Event Clustering
James Gung
Department of Computer Science
Miami University
Oxford, Ohio 45056
gungjm@muohio.edu
Jugal Kalita
Department of Computer Science
University of Colorado
Colorado Springs CO 80920
jkalita@uccs.edu
Abstract
In this paper, we investigate the use of tempo-
ral information for improving extractive sum-
marization of historical articles. Our method
clusters sentences based on their timestamps
and temporal similarity. Each resulting clus-
ter is assigned an importance score which
can then be used as a weight in traditional
sentence ranking techniques. Temporal im-
portance weighting offers consistent improve-
ments over baseline systems.
1 Introduction
Extensive research has gone into determining
which features of text documents are useful for cal-
culating the importance of sentences for extractive
summarization, as well as how to use these features
(Gupta and Lehal, 2010). Little work, however, has
considered the importance of temporal information
towards single document summarization. This is
likely because many text documents have very few
explicit time features and do not necessarily describe
topics in chronological order.
Historical articles, such as Wikipedia articles de-
scribing wars, battles, or other major events, tend to
contain many explicit time features. Historical arti-
cles also tend to describe events in chronological or-
der. In addition, historical articles tend to focus on a
single central event. The importance of other events
can then be judged by their temporal distance from
this central event. Finally, important events in an ar-
ticle will be described in greater detail, employing
more sentences than less important events.
This paper investigates the value of a temporal-
based score towards automatic summarization,
specifically focusing on historical articles. We in-
vestigate whether or not such a score can be used as
a weight in traditional sentence ranking techniques
to improve summarization quality.
2 Related Work
Event-based summarization is a recent approach
to summary generation. (Filatova and Hatzivas-
siloglou, 2004) introduced atomic events, which are
named entities connected by a relation such as a verb
or action noun. Events are selected for summary by
applying a maximum coverage algorithm to mini-
mize redundancy while maintaining coverage of the
major concepts of the document. (Vanderwende et
al., 2004) identify events as triples consisting of two
nodes and a relation. PageRank is then used to de-
termine the relative importance of these triples rep-
resented in a graph. Sentence generation techniques
are applied towards summarization.
Limited work has explored the use of temporal
information for summarization. (Lim et al, 2005)
use the explicit time information in the context of
multi-document summarization for sentence extrac-
tion and detection of redundant sentences, ordering
input documents by time. They observe that impor-
tant sentences tend to occur in in time slots contain-
ing more documents and time slots occurring at the
end and beginning of the documents set. They se-
lect topic sentences for each time slot, giving higher
weights based on the above observation.
(Wu et al, 2007) extract event elements, the ar-
guments in an event, and event terms, the actions.
Each event is placed on a timeline divided into in-
tervals consistent with the timespan of the article.
Each element and event term receives a weight cor-
responding to the total number of elements and event
terms located in each time interval the event element
or term occupies. Each sentence is scored by the to-
tal weight of event elements and terms it contains.
Clustering of events based on time has also re-
ceived little attention. (Foote and Cooper, 2003) in-
vestigate clustering towards organizing timestamped
digital photographs. They present a method that first
631
calculates the temporal similarity between all pairs
of photographs at multiple time scales. These values
are stored in a chronologically ordered matrix. Clus-
ter boundaries are determined by calculating novelty
scores for each set of similarity matrices. These are
used to form the final clusters. We adopt this clus-
tering method for clustering timestamped sentences.
3 Approach
The goal of our method is to give each sentence
in an article a temporal importance score that can
be used as a weight in traditional sentence ranking
techniques. To do this, we need to gain an idea
of the temporal structure of events in an article. A
score must then be assigned to each group corre-
sponding to the importance of the group?s timespan
to the article as a whole. Each sentence in a partic-
ular group will be assigned the same temporal im-
portance score, necessitating the use of a sentence
ranking technique to find a complete summary.
3.1 Temporal Information Extraction
We use Heideltime, a rule-based system that
uses sets of regular expressions, to extract explicit
time expressions in the article and normalize them
(Stro?tgen and Gertz, 2010). Events that occur be-
tween each Heideltime-extracted timestamp are as-
signed timestamps consisting of when the prior
timestamp ends and the subsequent timestamp be-
gins. The approach is naive and is described in
(Chasin et al, 2011). This method of temporal ex-
traction is not reliable, but serves the purposes of
testing as a reasonable baseline for temporal extrac-
tion systems. As the precision increases, the perfor-
mance of our system should also improve.
3.2 Temporal Clustering
To cluster sentences into temporally-related
groups, we adopt a clustering method proposed by
Foote et al to group digital photograph collections.
Inter-sentence similarity is calculated between ev-
ery pair of sentences using Equation (1).
SK(i, j) = exp
(
?
|ti ? tj |
K
)
(1)
The similarity measure is based inversely on the
distance between the central time of the sentences.
Similarity scores are calculated at varying granular-
ities. If the article focuses on a central event that
Figure 1: Similarity matrices at varying k displayed as
heat maps, darker representing more similar entries
occurs over only a few hours, such as the assassi-
nation of John F. Kennedy, the best clustering will
generally be found from similarities calculated us-
ing a smaller time granularity. Conversely, articles
with central events spanning several years, such as
the American Civil War, will be clustered using sim-
ilarities calculated at larger time granularities.
The similarities are placed in a matrix and orga-
nized chronologically in order of event occurrence
time. In this matrix, entries close to the diagonal
are among the most similar and the actual diagonal
entries are maximally similar (diagonal entries cor-
respond to similarities between the same sentences).
To identify temporal event boundaries, (Foote and
Cooper, 2003) calculate novelty scores. A checker-
board kernel in which diagonal regions contain all
positive weights and off-diagonal regions contain all
negative weights is correlated along the diagonal of
the similarity matrix. The weights of each entry in
the kernel are calculated from a Gaussian function
such that the most central entries have the highest (or
lowest in the off-diagonal regions) values. The result
is maximized when the kernel is located on tempo-
ral event boundaries. In relatively uniform regions,
the positive and negative weights cancel each other
out, resulting in small novelty scores. Where there
is a gap in similarity, presumably at an event bound-
ary, off diagonal squares are dissimilar, increasing
the novelty score. In calculating novelty scores with
each set of similarity scores, we obtain a hierarchi-
cal set of boundaries. With each time granularity, we
have a potential clustering option.
In order to choose the best clustering, we calcu-
late a confidence score C for each boundary set,
then choose the clustering with the highest score, as
suggested in (Foote and Cooper, 2003). This score
is the sum of intercluster similarities (IntraS) be-
tween adjacent clusters subtracted from the sum of
intracluster (InterS) similarities as seen in Equa-
tion (4). A high confidence score suggests low inter-
632
cluster similarity and high intracluster similarity.
IntraS(BK)S =
|Bk|?1?
l=1
bl+1?
i,j=bl
SK(i, j)
(bl+1 ? bl)2
(2)
InterS(BK)S =
|Bk|?2?
l=1
bl+1?
i=bl
bl+2?
j=bl+1
SK(i, j)
(bl+1 ? bl)(bl+2 ? bl+1)
(3)
CS(BK) = IntraS(BK)S ? InterSBK)S (4)
3.3 Estimating Clustering Paramaters
Historical articles describing wars generally have
much larger timespans than articles describing bat-
tles. Looking at battles at a broad time granularity
applicable to wars may not produce a meaningful
clustering. Thus, we should estimate the temporal
structure of each article before clustering. The time
granularity for each clustering is controlled by the
k parameter in the similarity function between sen-
tences. To find multiple clusterings, we start at a
base k, then increment k by a multiplier for each new
clustering. We calculate the base k using the stan-
dard deviation for event times in the article. Mea-
suring the spread of events in the article gives us an
estimate of what time scale we should use.
3.4 Calculating Temporal Importance
We use three novel metrics to calculate the impor-
tance of a cluster towards a summary. The first met-
ric is based on the size of the cluster (Eqn 5). This
is motivated by the assumption that more important
events will be described in greater detail, thus pro-
ducing larger clusters. The second metric (Eqn 6) is
based on the distance from the cluster?s centroid to
the centroid of the largest cluster, corresponding to
the central event of the article. This metric is moti-
vated by the assumption that historical articles have
a central event which is described in the greatest de-
tail. The third metric is based on the spread of the
cluster (Eqn 7). Clusters with large spreads are un-
likely to pertain to the same event, and should there-
fore be penalized.
Size(Ci) =
|Ci|
|Cmax|
(5)
Sim(Ci) = exp
(
?
|tCiCentroid ? tMaxClusterCentroid|
m
)
(6)
Spread(Ci) = exp
(
?
?Ci
n ? (tmax ? tmin)
)
(7)
The parameters m and n serve to weight the impor-
tance of these measures and are assigned based on
the spread of events in an article. For n, we used the
standard deviation of event times in the article. For
m, we used the cluster similarity score from Equa-
tion (4). The three measures work in tandem to en-
sure that the importance measure will be valid even
if the largest cluster does not correspond to the cen-
tral event of the article.
3.5 Final Sentence Ranking
Each sentence is assigned a temporal importance
weight equal to the importance score of the clus-
ter to which it belongs. To find a complete ranking
of the sentences, we apply a sentence ranking tech-
nique. Any automatic summarization technique that
ranks its sentences with numerical scores can poten-
tially be augmented with our temporal importance
weight. We multiply the base scores from the rank-
ing by the associated temporal importance weights
for each sentence to find the final ranking.
WS(Vi) = (1? d) (8)
+d ?
?
Vj?In(Vi)
wj,i
?
vk?Out(Vj)
wj,k
WS(Vj)
Like several graph-based methods for sentence rank-
ing for summarization (e.g., (Erkan and Radev,
2004)), we use Google?s PageRank algorithm
(Equation 8) with a damping factor d of 0.85.
Similarity(Si, Sj) =
|{wk|wk ? Si&wk ? Sj}|
log(|Si|) + log(|Sj |)
(9)
We use TextRank (Mihalcea and Tarau, 2004) in
our experiments. Our similarity measure is calcu-
lated using the number of shared named entities and
nouns between sentences as seen in equation 9. For
identification of named entities, we use Stanford
NER (Finkel et al, 2005). It is straightforward to
weight the resulting TextRank scores for each sen-
tence using their cluster?s temporal importance.
4 Experimental Results
We test on a set of 13 Wikipedia articles describ-
ing historical battles. The average article length is
189 sentences and 4,367 words. The longest ar-
ticle is 545 sentences and contains 11,563 words.
The shortest article is 51 sentences and contains
633
1,476 words. Each article has at least two human-
annotated gold standard summaries. Volunteers
were asked to choose the most important sentences
from each article. We evaluate using ROUGE-2 bi-
gram matching (Lin, 2004).
4.1 Clustering
Each Wikipedia article contains a topic sentence
stating the timespan of the main event in the article.
This provides an easy way to determine whether a
clustering is successful. If the largest cluster con-
tains the timespan of the main event described by
the topic sentence, we consider the clustering to be
successful. The articles vary greatly in length. Also,
the ratio of sentences with time features to sentences
without is considerably varied. In 92% of the arti-
cles, there were successful clusterings. An exam-
ple of an article that didn?t cluster is Nickel Grass,
where the main event was divided into two clusters.
It is of interest to note that this article had one of
lowest time feature to sentence ratios, which possi-
bly explains the poor clustering.
4.2 Temporal Importance Weighting
We test our TextRank implementation with and
without temporal importance weighting.
We observe improvements in general using the
TextRank system with temporal importance weight-
ing. The ROUGE-2 score increased by 15.72%
across all the articles. The lowest increase was
0% and the highest was 128.86%. The average
ROUGE-2 scores were 0.2575 weighted and 0.2362
unweighted, a statistically significant increase with
a 95% confidence interval of 0.0066 to 0.0360.
In particular, we see significant improvements
in articles that contain sentences TextRank ranked
highly but have events occurring at significantly dif-
ferent times than the central event of the article. Al-
though the content of these sentences is highly re-
lated to the rest of the article, they should not be
included in the summary since their events happen
nowhere near the main event temporally.
Our random ranking system, which randomly
assigns base importance scores to each sentence,
observed only small improvements, of 4.27% on
average, when augmented with temporal impor-
tance weighting. It is likely that additional human-
annotated summaries are necessary for conclusive
results.
5 Conclusions and Future Work
The novelty-based clustering method worked ex-
tremely well for our purposes. These results can
likely be improved upon using more advanced tem-
poral extraction and interpolation methods, since we
used a naive method for interpolating between time
features prone to error. The temporal importance
weighting worked very well with TextRank and rea-
sonably well with random ranking.
It may also be fairly easy to predict the success of
using this temporal weight a priori to summarization
of an article. A small ratio of explicit time features to
sentences (less than 0.15) indicates that the temporal
interpolation process may not be very accurate. The
linearity of time features is also a good indication
of the success of temporal extraction. Finally, the
spread of time features in an article is a clue to the
success of our weighting method.
Acknowledgements
Research reported here has been funded partially
by NSF grants CNS-0958576 and CNS-0851783.
References
R. Chasin, D. Woodward, and J. Kalita, 2011. Machine
Intelligence: Recent Advances, chapter Extracting and
Displaying Temporal Entities from Historical Articles.
Narosa Publishing, Delhi.
G. Erkan and D.R. Radev. 2004. Lexrank: Graph-based
lexical centrality as salience in text summarization. J.
Artif. Intell. Res., 22:457?479.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In ACL Workshop on Sum-
marization.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL, pages
363?370.
J. Foote and M. Cooper. 2003. Media segmentation us-
ing self-similarity decomposition. In SPIE, volume
5021, pages 167?175.
V. Gupta and G.S. Lehal. 2010. A survey of text sum-
marization extractive techniques. Journal of Emerging
Technologies in Web Intelligence, 2(3):258?268.
J.M. Lim, I.S. Kang, J.H. Bae, and J.H. Lee. 2005. Sen-
tence extraction using time features in multi-document
summarization. Information Retrieval Technology,
pages 82?93.
634
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Workshop on text summariza-
tion, pages 25?26.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In EMNLP, pages 404?411.
J. Stro?tgen and M. Gertz. 2010. Heideltime: High qual-
ity rule-based extraction and normalization of tempo-
ral expressions. In 5th International Workshop on Se-
mantic Evaluation, pages 321?324.
L. Vanderwende, M. Banko, and A. Menezes. 2004.
Event-centric summary generation. Working notes of
DUC.
M. Wu, W. Li, Q. Lu, and K.F. Wong. 2007. Event-
based summarization using time features. Compu-
tational Linguistics and Intelligent Text Processing,
pages 563?574.
635
Proceedings of NAACL-HLT 2013, pages 445?449,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Better Twitter Summaries?
Joel Judd & Jugal Kalita
Department of Computer Science
University of Colorado
Colorado Springs, Colorado
Email: {jjudd2,jkalita}@uccs.edu
Abstract
This paper describes an approach to improve
summaries for a collection of Twitter posts cre-
ated using the Phrase Reinforcement (PR) Al-
gorithm (Sharifi et al, 2010a). The PR algo-
rithm often generates summaries with excess
text and noisy speech. We parse these sum-
maries using a dependency parser and use the
dependencies to eliminate some of the excess
text and build better-formed summaries. We
compare the results to those obtained using the
PR Algorithm.
1 Introduction
Millions of people use the Web to express themselves
and share ideas. Twitter is a very popular micro
blogging site. According to a recent study approxi-
mately 340 million Tweets are sent out every day1.
People mostly upload daily routines, fun activities
and other words of wisdom for readers. There is also
plenty of serious information beyond the personal;
according to a study approximately 4% of posts on
Twitter have relevant news data2. Topics that may
be covered by reputable new sources like CNN (Ca-
ble News Network) were considered relevant. A topic
is simply a keyword or key phrase that one may use
to search for Twitter posts containing it. It is pos-
sible to gather large amounts of posts from Twitter
on many different topics in short amounts of time.
Obviously, processing all this information by human
hands is impossible. One way to extract information
from Twitter posts on a certain topic is to automat-
ically summarize them. (Sharifi et al, 2010a; Sharifi
et al, 2010b; Sharifi et al, 2010c) present an al-
gorithm called the Phrase Reinforcement Algorithm
to produces summaries of a set of Twitter posts on
1http://blog.twitter.com/2012/03/
twitter-turns-six.htm
2http://www.pearanalytics.com/blog/wp-content/
uploads/2010/05/Twitter-Study-August-2009.pdf
a certain topic. The PR algorithm produces good
summaries for many topics, but for sets of posts on
certain topics, the summaries become syntactically
malformed or too wordy. This is because the PR
Algorithm does not pay much attention to syntactic
well-formedness as it constructs a summary sentence
from phrases that occur frequently in the posts it
summarizes. In this paper, we attempt to improve
Twitter summaries produced by the PR algorithm.
2 The PR Algorithm Revisited
Given a number of Twitter posts on a certain topic,
the PR algorithm starts construction of what is
called a word graph with a root node containing the
topic phrase. It builds a graph showing how words
occur before and after the phrase in the root node,
considering all the posts on the topic. It builds a
subgraph to the left of the topic phrase and another
subgraph to its right in a similar manner. To con-
struct the left graph, the algorithm starts with the
root node and obtains the set of words that occur
immediately before the current node?s phrase. For
each of these unique words, the algorithm adds them
to the graph as nodes with their associated counts
to the left of the current node. The algorithm con-
tinues this process recursively for each node added
to the graph until all the potential words have been
added to the left-hand side of the graph. The al-
gorithm repeats these steps symmetrically to con-
struct the right subgraph. Once the full graph is
there, the algorithm weights individual nodes. The
weights are initialized to the same values as their
frequency counts. Then, to account for the fact that
some phrases are naturally longer than others, they
penalize nodes that occur farther from the root node
by an amount that is proportional to their distance.
To generate a summary, the algorithm looks for the
most overlapping phrases within the graph. Since
the nodes? weights are proportional to their overlap,
the algorithm searches for the path within the graph
445
with the highest cumulative weight. The sequence of
words in this path becomes the summary.
3 Problem Description
We start by making some observations on the phrase-
reinforcement algorithm. Certain topics do not pro-
duce well-formed summaries, while others yield very
good summaries. For the posts that have a well-
centered topic without a huge amount of variation
among the posts, the algorithm works well and cre-
ates good summaries. Here is an example summary
produced by the PR algorithm.
Phillies defeat Dodgers to take the National
League Championship series.
(Sharifi et al, 2010a; Sharifi et al, 2010b; Sharifi
et al, 2010c) provide additional examples. The PR
algorithm limits the length of the summary to ap-
proximately 140 characters, the maximum length of
a Twitter post. However, often the summary sen-
tence produced has extraneous parts that appear due
to the fact that they appear frequently in the posts
being summarized, but these parts make the sum-
mary malformed or too wordy. An example with
some wordiness is given below.
today is day for vote obama this election day
Some ?raw? PR summaries are a lot more wordy
than the one above. The goal we address in this
paper is to create grammatically better formed sum-
maries by processing the ?raw? summaries formed by
the PR Algorithm. We drop this excess text and the
phrases or extract pieces of text which make sense
grammatically to form the final summary. This usu-
ally produces a summary with more grammatical ac-
curacy and less noise in between the words. This gets
the main point of the summary across better.
4 Approach
The idea behind creating the desired summary is
to parse the ?raw? summary and build dependen-
cies between the dependent and governor words in
each summary. We perform parts of speech tagging
and obtain lists of governing and dependent words.
This data forms the basis for creating a valid sum-
mary. For example given the Twitter post, today
is day for vote obama this election day, a depen-
dency parser produces the governor-dependent rela-
tionships as given in Table 1. Figure 1 also shows
the same grammatical dependencies between words
in the phrases.
We believe that a word which governs many words
is key to the phrase as a whole, and dependent words
Table 1: Governor and Dependent Words for today is
day1 for vote obama this election day2
Governor Dependent
day1 today
is
for
day2
obama vote
for obama
day2 this
election
Algorithm 1 Algorithm to Fix ?Raw? PRA Sum-
maries
I. For each word, check grammatical compatibil-
ity with words before and after the word being
checked.
II. If a word has no dependencies immediately be-
fore or after it, drop the word.
III. After each word has been checked, check for
the words that form a grammatical phrase.
IV. Write out the summary without the dropped
words and without phrases with only two words.
V. If needed, go back to step III, because there
shouldn?t be any more single words with no de-
pendencies to check, and repeat as many times as
necessary.
which are closely related, or in other words, lay close
to each other in the phrase should be left in the or-
der they appear. Conceptually, our approach works
as follows: look at every word and see if it makes
sense with the word before and after it. This builds
dependencies between the word in question with the
words around it. If a word before or after the word
being analyzed does not make sense grammatically,
it can be removed from that grammatically correct
phrase. Dependent words that are not close to each
other may not be as important as words that lay
close to each other and have more dependencies, and
thus may be thrown out of the summaries. Through
this process grammatically correct phrases can be
formed.
The dependencies are built by tagging each word
as a part of speech and seeing if it relates to other
words. For example, it checks whether or not the
conjunction ?and? is serving its purpose of combin-
ing a set of words or ideas, in other words, if those
dependencies exist. If dependencies exist with the
nearby words, that given collection of words can be
set aside as a grammatically correct phrase until it
reaches words with no dependencies, and the process
446
Figure 1: Dependency Parse for today is day1 for vote obama this election day2
today is day for vote obama this election day
dep
c o p
p r e p
nsubj
pobj
n n
det
n n
can continue. The phrases with few words can be
dropped, as well as single words. These new phrases
can be checked for grammatical accuracy in the same
way as the previous phrases, and if they pass, can
remain combined forming a longer summary that
should be grammatically correct. The main steps
are given in Algorithm 1.
Now, take the example summary produced by the
PR Algorithm for the election Twitter posts. Look-
ing at this summary, we, as humans, may make
changes and make the summary grammatically cor-
rect. Two potential ideal summaries would be the
following.
today is the day to vote for obama
vote for obama this election day
The actual process used in the making of the gram-
matical summaries is as follows. Two main lists are
created from lists of governor and dependent words,
one with the governor words and another with the
dependent words. The governor words are checked
to see how many dependent words are linked to them.
The governing words with the highest number of de-
pendent words are kept for later. For example using
the above phrase about the elections, the word ?day?
was the governing word with the highest amount of
dependent words and was thus kept for the final sum-
mary. The superscripts on the word ?day? differen-
tiate its two occurrences. The dependent words are
kept in groups of closely linked dependent words.
Using the same example about the election, an in-
termediate list of closely related dependent words is
?today,? ?is,? ?for,? ?vote,? ?obama,? ?this,? ?elec-
tion,? and ?day.? And the final list of closely related
dependent words is ?for,? ?vote,? ?obama,? ?this,?
?election? and ?day.? After these two lists are in the
final stages the lists are merged placing the words in
proper order.
5 Experiments and Results
To begin, the Twitter posts were collected manu-
ally and stored in text files. The topics we chose to
Table 2: ROUGE-L without Stopwords, Before
Task Recall Precision F-score
Task 1 0.667 0.343 0.453
Task 2 1.000 0.227 0.370
Task 3 0.353 0.240 0.286
Task 4 0.800 0.154 0.258
Task 5 1.000 0.185 0.313
Task 6 0.667 0.150 0.245
Task 7 0.889 0.125 0.219
Task 8 0.636 0.125 0.209
Task 9 0.500 0.300 0.375
Task 10 0.455 0.100 0.164
Average 0.696 0.195 0.289
focus on important current events and some pop cul-
ture. Approximately 100 posts were collected on ten
different topics. These topics are ?The Avengers,?
?Avril Lavigne,? ?Christmas,? ?the election,? ?Elec-
tion Day,? ?Iron Man 3,? ?president 2012,? ?Hurri-
cane Sandy,? ?Thanksgiving,? and ?vote.?
The collections of posts were passed on to three
volunteers to produce short accurate summaries that
capture the main idea from the posts. The collections
of posts were also first run through the PR Algorithm
and then through the process described in this paper
to try and refine the summaries output by the PR
Algorithm. The Stanford CoreNLP parser3 was used
to build the lists of governor and dependent words.
We use ROUGE evaluation metrics (Lin 2004)
just like (Sharifi et al, 2010a; Sharifi et al, 2010b;
Sharifi et al, 2010c), who evaluated summaries ob-
tained with the PR Algorithm. Specifically, we use
ROUGE-L, which uses the longest common subse-
quence (LCS) to compare summaries. As the LCS of
the two summaries in comparison increases in length,
so does the similarity of the two summaries.
We now discuss results using ROUGE-L on the
summaries we produce. Tables 2 through 5 show
the results of four different ROUGE-L evaluations,
comparing them to the results found using the PR
3http://nlp.stanford.edu/software/corenlp.shtml
447
Table 3: ROUGE-L without Stopwords, After
Task Recall Precision F-score
Task 1 0.667 0.480 0.558
Task 2 0.400 0.500 0.444
Task 3 0.000 0.000 0.000
Task 4 0.400 0.333 0.363
Task 5 0.900 0.600 0.720
Task 6 0.389 0.350 0.368
Task 7 0.556 0.250 0.345
Task 8 0.545 0.500 0.522
Task 9 0.417 0.417 0.417
Task 10 0.363 0.200 0.258
Average 0.464 0.363 0.400
Algorithm, and Table 6 shows the comparisons of the
averaged scores to the scores (Sharifi et al, 2010a)
obtained using the PR Algorithm. Table 2 shows the
regular ROUGE-L scores, meaning the recall, pre-
cision and F-scores for each task and the average
overall scores, for the collection of posts before using
the dependency parser to refine the summaries. Ta-
ble 3 displays the results after using the dependency
parser on the summaries formed by the PR Algo-
rithm. One of the options in ROUGE is to show the
?best? result, for each task. Table 4 has this result
for the PR Algorithm results. Table 5 shows the re-
sults of the ?best? scores, after running it through
the dependency parser. Table 6 shows the averages
from Tables 3 and 5, using the dependency parser,
compared to Sharifi et al?s results using the PR Al-
gorithm. Stopwords were not removed in our exper-
iments.
Table 4: ROUGE-L Best without Stopwords, Before
Recall Precision F-score
Task 1 1.000 0.429 0.600
Task 2 1.000 0.227 0.370
Task 3 0.500 0.200 0.286
Task 4 1.000 0.154 0.267
Task 5 1.000 0.167 0.286
Task 6 1.000 0.200 0.333
Task 7 1.000 0.125 0.222
Task 8 1.000 0.071 0.133
Task 9 1.000 0.400 0.571
Task 10 1.000 0.100 0.182
Average 0.950 0.207 0.325
As one can see, the use of our algorithm on the
summaries produced by the PR Algorithm improves
the F-score values, at least in the example cases we
tried. In almost every case, there is substantial rise
in the F-score. As previously mentioned, some col-
Table 5: ROUGE-L Best without Stopwords, After
Recall Precision F-score
Task 1 1.000 0.600 0.750
Task 2 0.400 0.500 0.444
Task 3 0.000 0.000 0.000
Task 4 0.500 0.333 0.400
Task 5 1.000 0.600 0.750
Task 6 0.600 0.600 0.600
Task 7 0.667 0.400 0.500
Task 8 1.000 0.333 0.500
Task 9 1.000 0.667 0.800
Task 10 1.000 0.250 0.400
Average 0.718 0.428 0.515
Table 6: ROUGE-L Averages after applying our algo-
rithm vs. Sharifi et al
Recall Precision F-score
Sharifi (PRA) 0.31 0.34 0.33
Rouge-L after re-
construction
0.46 0.36 0.40
Rouge-L best after
reconstruction
0.72 0.43 0.52
lections of Tweets do not produce good summaries.
Task 3 had some poor scores in all cases, so one can
deduce that the posts on that topic (Christmas) were
widely spread, or they did not have a central theme.
6 Conclusion
The PR Algorithm is not a pure extractive algo-
rithm. It creates summaries of Twitter posts by piec-
ing together the most commonly occurring words and
phrases in the entire set of tweets, but keeping the
order of constituents as close to the order in which
they occur in the posts, collectively speaking. As
we noted in this paper, the heuristic method using
which the PR Algorithm composes a summary sen-
tence out of the phrases sometimes leads to ungram-
matical sentences or wordy sentences. This paper
shows that the ?raw? summaries produced by the
PR Algorithm can be improved by taking into ac-
count governor-dependency relationships among the
constituents. There is nothing in this clean-up algo-
rithm that says that it works only with summaries of
tweets. The same approach can potentially be used
to improve grammaticality of sentences written by
humans in a sloppy manner. In addition, given sev-
eral sentences with overlapping content (from mul-
tiple sources), the same process can potentially be
used to construct a grammatical sentence out of all
the input sentences. This problem often arises in
general multi-document summarization. We believe
448
that a corrective approach like ours can be used to-
gether with a sentence compression approach, such
as (Knight and Marcu 2002), to produce even bet-
ter summaries in conjunction with the PR or other
summarization algorithms that work with socially-
generated texts which are often malformed and short.
We have shown in this paper that simply focusing
on grammatical dependency tends to make the fi-
nal summaries more grammatical and readable com-
pared to the raw summaries. However, we believe
that more complex restructuring of the words and
constituents would be necessary to improve the qual-
ity of the raw summaries, in general.
References
Knight, K. and Marcu, D. 2004. Summarization beyond
sentence extraction: A probabilistic approach to sen-
tence compression, Artificial Intelligence, Vol. 139, No.
1, pp. 91?107.
Lin, C.Y. 2004. Rouge: A package for automatic evalua-
tion of summaries, Text Summarization Branches Out:
Proceedings of the ACL-04 Workshop, pp. 74?81.
Sharifi, Beaux, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing Microblogs Automatically, Annual
Conference of the National Association for Advance-
ment of Computational Linguistics-Human Language
Technology (NAACL-HLT), pp. 685-688, Los Angeles.
Sharifi, Beaux, Mark-Anthony Hutton, and Jugal Kalita.
2010. Experiments in Microblog Summarization, Sec-
ond IEEE International Conference on Social Comput-
ing (SocialCom 2010), pp. 49-56, Minneapolis.
Sharifi, Beaux, Mark-Anthony Hutton and Jugal Kalita.
2010. Automatic Summarization of Twitter Topics,
National Workshop on Design and Analysis of Algo-
rithms, NWDAA 10, Tezpur University, Assam, India.
449
Proceedings of NAACL-HLT 2013, pages 524?528,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Creating Reverse Bilingual Dictionaries
Khang Nhut Lam
Department of Computer Science
University of Colorado
Colorado Springs, USA
klam2@uccs.edu
Jugal Kalita
Department of Computer Science
University of Colorado
Colorado Springs, USA
jkalita@uccs.edu
Abstract
Bilingual dictionaries are expensive resources
and not many are available when one of the
languages is resource-poor. In this paper, we
propose algorithms for creation of new reverse
bilingual dictionaries from existing bilingual
dictionaries in which English is one of the two
languages. Our algorithms exploit the simi-
larity between word-concept pairs using the
English Wordnet to produce reverse dictionary
entries. Since our algorithms rely on available
bilingual dictionaries, they are applicable to
any bilingual dictionary as long as one of the
two languages has Wordnet type lexical ontol-
ogy.
1 Introduction
The Ethnologue organization1 lists 6,809 distinct
languages in the world, most of which are resource-
poor. Most existing online bilingual dictionaries are
between two resource-rich languages (e.g., English,
Spanish, French or German) or between a resource-
rich language and a resource-poor language. There
are languages for which we are lucky to find a single
bilingual dictionary online. For example, the Uni-
versity of Chicago hosts bilingual dictionaries from
29 Southeast Asian languages2, but many of these
languages have only one bilingual dictionary online.
Existing algorithms for creating new bilingual
dictionaries use intermediate languages or interme-
diate dictionaries to find chains of words with the
same meaning. For example, (Gollins and Sander-
son, 2001) use lexical triangulation to translate in
parallel across multiple intermediate languages and
1http://www.ethnologue.com/
2http://dsal.uchicago.edu/dictionaries/list.html
fuse the results. They query several existing dictio-
naries and then merge results to maximize accuracy.
They use four pivot languages, German, Spanish,
Dutch and Italian, as intermediate languages. An-
other existing approach for creating bilingual dictio-
naries is using probabilistic inference (Mausam et
al., 2010). They organize dictionaries in a graph
topology and use random walks and probabilistic
graph sampling. (Shaw et al, 2011) propose a set
of algorithms to create a reverse dictionary in the
context of single language by using converse map-
ping. In particular, given an English-English dictio-
nary, they attempt to find the original words or terms
given a synonymous word or phrase describing the
meaning of a word.
The goal of this research is to study the feasibility
of creating a reverse dictionary by using only one ex-
isting dictionary and Wordnet lexical ontology. For
example, given a Karbi3-English dictionary, we will
construct an ENG-AJZ dictionary. The remainder of
this paper is organized as follows. In Section 2, we
discuss the nature of bilingual dictionaries. Section
3 describes the algorithms we propose to create new
bilingual dictionaries from existing dictionaries. Re-
sults of our experiments are presented in Section 4.
Section 5 concludes the paper.
2 Existing Online Bilingual Dictionaries
Powerful online translators developed by Google
and Bing provide pairwise translations (including
for individual words) for 65 and 40 languages, re-
spectively. Wiktionary, a dictionary created by vol-
unteers, supports over 170 languages. We find a
3Karbi is an endangered language spoken by 492,000 peo-
ple (2007 Ethnologue data) in Northeast India, ISO 639-3 code
AJZ. ISO 693-3 code for English is ENG.
524
large number of bilingual dictionaries at PanLex4
including an ENG-Hindi5 and a Vietnamese6-ENG
dictionary. The University of Chicago has a number
of bilingual dictionaries for South Asian languages.
Xobdo7 has a number of dictionaries, focused on
Northeast India.
We classify the many freely available dictionaries
into three main kinds.
? Word to word dictionaries: These are dictionar-
ies that translate one word in one language to
one word or a phrase in another language. An
example is an ENG-HIN dictionary at Panlex.
? Definition dictionaries: One word in one lan-
guage has one or more meanings in the second
language. It also may have pronunciation, parts
of speech, synonyms and examples. An exam-
ple is the VIE-ENG dictionary, also at Panlex.
? One language dictionaries: A dictionary of this
kind is found at dictionary.com.
We have examined several hundred online dictionar-
ies and found that they occur in many different for-
mats. Extracting information from these dictionaries
is arduous. We have experimented with five existing
bilingual dictionaries: VIE-ENG, ENG-HIN, and a
dictionary supported by Xobdo with 4 languages:
Assamese8, ENG, AJZ, and Dimasa9. We consider
the last one to be a collection of 3 bilingual dictio-
naries: ASM-ENG, AJZ-ENG, and DIS-ENG. We
choose these languages since one of our goals is to
work with resource-poor languages to enhance the
quantity and quality of resources available.
3 Proposed Solution Approach
A dictionary entry, called LexicalEntry, is a 2-tuple
<LexicalUnit, Definition>. A LexicalUnit is a
word or a phrase being defined, also called definien-
dum (Landau, 1984). A list of entries sorted by
the LexicalUnit is called a lexicon or a dictionary.
Given a LexicalUnit, the Definition associated with
it usually contains its class and pronunciation, its
4http://panlex.org/
5ISO 693-3 code HIN
6ISO 693-3 code VIE
7http://www.xobdo.org/
8Assamese is an Indo-European language spoken by about
30 million people, but it is resource-poor, ISO 693-3 code ASM.
9Dimasa is another endangered language from Northeast In-
dia, spoken by about 115,000 people, ISO 693-3 code DIS.
meaning, and possibly additional information. The
meaning associated with it can have several Senses.
A Sense is a discrete representation of a single aspect
of the meaning of a word. Thus, a dictionary entry
is of the form <LexicalUnit, Sense1, Sense2, ? ? ?>.
In this section, we propose a series of algorithms,
each one of which automatically creates a reverse
dictionary, or ReverseDictionary, from a dictio-
nary that translates a word in language L1 to a word
or phrase in language L2. We require that at least
one of two these languages has a Wordnet type lexi-
cal ontology (Miller, 1995). Our algorithms are used
to create reverse dictionaries from them at various
levels of accuracy and sophistication.
3.1 Direct Reversal (DR)
The existing dictionary has alphabetically sorted
LexicalUnits in L1 and each of them has one or
more Senses in L2. To create ReverseDictionary,
we simply take every pair <LexicalUnit, Sense>
in SourceDictionary and swap the positions of the
two.
Algorithm 1 DR Algorithm
ReverseDictionary := ?
for allLexicalEntryi ? SourceDictionary do
for all Sensej ? LexicalEntryi do
Add tuple <Sensej ,
LexicalEntryi.LexicalUnit> to
ReverseDictionary
end for
end for
This is a baseline algorithm so that we can com-
pare improvements as we create new algorithms.
If in our input dictionary, the sense definitions
are mostly single words, and occasionally a sim-
ple phrase, even such a simple algorithm gives
fairly good results. In case there are long or com-
plex phrases in senses, we skip them. The ap-
proach is easy to implement, and produces a high-
accuracy ReverseDictionary. However, the num-
ber of entries in the created dictionaries are lim-
ited because this algorithm just swaps the posi-
tions of LexicalUnit and Sense of each entry in the
SourceDictionary and does not have any method
to find the additional words having the same mean-
ings.
525
3.2 Direct Reversal with Distance (DRwD)
To increase the number of entries in the output dic-
tionary, we compute the distance between words
in the Wordnet hierarchy. For example, the words
"hasta-lipi" and "likhavat" in HIN have the meanings
"handwriting" and "script", respectively. The dis-
tance between "handwriting" and "script" in Word-
net hierarchy is 0.0, so that "handwriting" and
"script" likely have the same meaning. Thus, each of
"hasta-lipi" and "likhavat" should have both mean-
ings "handwriting" and "script". This approach
helps us find additional words having the same
meanings and possibly increase the number of lexi-
cal entries in the reverse dictionaries.
To create a ReverseDictionary, for every
LexicalEntryi in the existing dictionary,
we find all LexicalEntryj , i 6= j with dis-
tance to LexicalEntryi equal to or smaller
than a threshold ?. As results, we have new
pairs of entries <LexicalEntryi.LexicalUnit,
LexicalEntryj .Sense> ; then we swap positions
in the two-tuples, and add them into the Reverse-
Dictionary. The value of ? affects the number of
entries and the quality of created dictionaries. The
greater the value of ?, the larger the number of
lexical entries, but the smaller the accuracy of the
ReverseDictionary.
The distance between the two LexicalEntrys is the
distance between the two LexicalUnits if the Lexi-
calUnits occur in Wordnet ontology; otherwise, it is
the distance between the two Senses. The distance
between each phrase pair is the average of the to-
tal distances between every word pair in the phrases
(Wu and Palmer, 1994). If the distance between two
words or phrases is 1.00, there is no similarity be-
tween these words or phrases, but if they have the
same meaning, the distance is 0.00.
We find that aReverseDictionary created using
the value 0.0 for ? has the highest accuracy. This ap-
proach significantly increases the number of entries
in the ReverseDictionary. However, there is an is-
sue in this approach. For instance, the word "tuhbi"
in DIS means "crowded", "compact", "dense", or
"packed". Because the distance between the En-
glish words "slow" and "dense" in Wordnet is 0.0,
this algorithm concludes that "slow" has the mean-
ing "tuhbi" also, which is wrong.
Algorithm 2 DRwD Algorithm
ReverseDictionary := ?
for allLexicalEntryi ? SourceDictionary do
for all Sensej ? LexicalEntryi do
for all LexicalEntryu ?
SourceDictionary do
for all Sensev ? LexicalEntryu do
if distance(<LexicalEntryi.LexicalUnit,
Sensej> ,<LexicalEntryu.LexicalUnit,
Sensev> ) 6 ? then
Add tuple <Sensej ,
LexicalEntryu.LexicalUnit>
to ReverseDictionary
end if
end for
end for
end for
end for
3.3 Direct Reversal with Similarly (DRwS)
The DRwD approach computes simply the dis-
tance between two senses, but does not look at
the meanings of the senses in any depth. The
DRwS approach represents a concept in terms of
its Wordnet synset10, synonyms, hyponyms and
hypernyms. This approach is like the DRwD
approach, but instead of computing the distance
between lexical entries in each pair, we calcu-
late the similarity, called simValue. If the sim-
Value of a <LexicalEntryi,LexicalEntryj>, i 6=
j pair is equal or larger than ?, we conclude
that the LexicalEntryi has the same meaning as
LexicalEntryj .
To calculate simValue between two phrases, we
obtain the ExpansionSet for every word in each
phrase from the WordNet database. An Expansion-
Set of a phrase is a union of synset, and/or synonym,
and/or hyponym, and/or hypernym of every word in
it. We compare the similarity between the Expan-
sionSets. The value of ? and the kinds of Expan-
sionSets are changed to create different ReverseDic-
tionarys. Based on experiments, we find that the best
value of ? is 0.9, and the best ExpansionSet is the
union of synset, synonyms, hyponyms, and hyper-
nyms. The algorithm for computing the simValue of
entries is shown in Algorithm 3.
10Synset is a set of cognitive synonyms.
526
Algorithm 3 simValue(LexicalEntryi,
LexicalEntryj)
simWords := ?
if LexicalEntryi.LexicalUnit &
LexicalEntryj .LexicalUnit have a Word-
net lexical ontology then
for all (LexicalUnitu ? LexicalEntryi) &
(LexicalUnitv ? LexicalEntryj) do
Find ExpansionSet of every
LexicalEntry based on LexicalUnit
end for
else
for all (Senseu ? LexicalEntryi) &
(Sensev ? LexicalEntryj) do
Find ExpansionSet of every
LexicalEntry based on Sense
end for
end if
simWords ? ExpansionSet (LexicalEntryi) ?
ExpansionSet(LexicalEntryj)
n?ExpansionSet(LexicalEntryi).length
m?ExpansionSet(LexicalEntryj).length
simValue?min{ simWords.lengthn ,
simWords.length
m }
4 Experimental results
The goals of our study are to create the high-
precision reverse dictionaries, and to increase the
numbers of lexical entries in the created dictio-
naries. Evaluations were performed by volunteers
who are fluent in both source and destination lan-
guages. To achieve reliable judgment, we use the
same set of 100 non-stop word ENG words, ran-
domly chosen from a list of the most common
words11. We pick randomly 50 words from each
created ReverseDictionary for evaluation. Each
volunteer was requested to evaluate using a 5-point
scale, 5: excellent, 4: good, 3: average, 2: fair, and
1: bad. The average scores of entries in the Reverse-
Dictionarys is presented in Figure 1. The DRwS dic-
tionaries are the best in each case. The percentage of
agreements between raters is in all cases is around
70%.
The dictionaries we work with frequently have
several meanings for a word. Some of these mean-
ings are unusual, rare or very infrequently used. The
11http://www.world-english.org/english500.htm
DR algorithm creates entries for the rare or unusual
meanings by direct reversal. We noticed that our
evaluators do not like such entries in the reversed
dictionaries and mark them low. This results in
lower average scores in the DR algorithm compar-
ing to averages cores in the DRwS algorithm. The
DRwS algorithm seems to have removed a number
of such unusual or rare meanings (and entries simi-
lar to the rare meanings, recursively) improving the
average score
Our proposed approaches do not work well for
dictionaries containing an abundance of complex
phrases. The original dictionaries, except the VIE-
ENG dictionary, do not contain many long phrases
or complex words. In Vietnamese, most words
we find in the dictionary can be considered com-
pound words composed of simpler words put to-
gether. However, the component words are sepa-
rated by space. For example, "b?i th?n gi?o" means
"idolatry". The component words are "b?i" mean-
ing "bow low"; "th?n" meaning "deity"; and "gi?o"
meaning "lance", "spear", "to teach", or "to edu-
cate". The presence of a large number of compound
words written in this manner causes problems with
the ENG-VIE dictionary. If we look closely at Fig-
ure 1, all language pairs, except ENG-VIE show
substantial improvement in score when we compare
the DR algorithm with DRwS algorithm.
Figure 1: Average entry score in ReverseDictionary
The DRwD approach significantly increases the
number of entries, but the accuracy of the created
dictionaries is much lower. The DRwS approach us-
ing a union of synset, synonyms, hyponyms, and hy-
pernyms of words, and ? ? 0.9 produces the best re-
verse dictionaries for each language pair. The DRwS
approach increases the number of entries in the cre-
ated dictionaries compared to the DR algorithm as
527
shown in Figure 2.
Figure 2: Number of lexical entries in
ReverseDictionarys generated from 100 common
words
We also create the entire reverse dictionary for
the AJZ-ENG dictionary. The total number of en-
tries in the ENG-AJZ dictionaries created by us-
ing the DR algorithm and DRwS algorithm are
4677 and 5941, respectively. Then, we pick 100
random words from the ENG-AJZ created by us-
ing the DRwS algorithm for evaluation. The av-
erage score of every entry in this created dictio-
nary is 4.07. Some of the reversal bilingual dictio-
naries can be downloaded at http://cs.uccs.edu/ lin-
clab/creatingBilingualLexicalResource.html.
5 Conclusion
We proposed approaches to create a reverse dic-
tionary from an existing bilingual dictionary using
Wordnet. We show that a high precision reverse dic-
tionary can be created without using any other inter-
mediate dictionaries or languages. Using the Word-
net hierarchy increases the number of entries in the
created dictionaries. We perform experiments with
several resource-poor languages including two that
are in the UNESCO?s list of endangered languages.
Acknowledgements
We would like to thank the volunteers evaluating
the dictionaries we create: Morningkeey Phangcho,
Dharamsing Teron, Navanath Saharia, Arnab Phon-
glosa, Abhijit Bendale, and Lalit Prithviraj Jain. We
also thank all friends in the Xobdo project who pro-
vided us with the ASM-ENG-DIS-AJZ dictionaries.
References
Mausam, S. Soderlan, O. Etzioni, D.S. Weld, K. Reiter,
M. Skinner, M. Sammer, and J. Bilmers 2010. Pan-
lingual lexical translation via probabilistic inference,
Artificial Intelligence, 174:619?637.
R. Shaw, A. Datta, D. VanderMeer, and K. Datta 2011.
Building a scalable database - Driven Reverse Dic-
tionary, IEEE Transactions on Knowledge and Data
Engineering, volume 99.
T. Gollins and M. Sanderson. 2001. Improving cross lan-
guage information retrieval with triangulated transla-
tion, SIGIR ?01 Proceedings of the 24th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, New York, 90?
95.
S.I. Landau. 1984. Dictionaries, Cambridge Univ Press.
G.A. Miller. 1995. Wordnet: a lexical database
for English, Communications of the ACM, vol-
ume 38(11):39?41.
Z. Wu and P. Palmer. 1994. Verbs semantics and lexical
selection, In proceeding of the 32nd annual meeting
on Association for computaional linguistics, Strouds-
burg, 133?138.
528
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 106?111,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automatically constructing Wordnet synsets
Khang Nhut Lam, Feras Al Tarouti and Jugal Kalita
Computer Science department
University of Colorado
1420 Austin Bluffs Pkwy, Colorado Springs, CO 80918, USA
{klam2,faltarou,jkalita}@uccs.edu
Abstract
Manually constructing a Wordnet is a dif-
ficult task, needing years of experts? time.
As a first step to automatically construct
full Wordnets, we propose approaches to
generate Wordnet synsets for languages
both resource-rich and resource-poor, us-
ing publicly available Wordnets, a ma-
chine translator and/or a single bilin-
gual dictionary. Our algorithms translate
synsets of existing Wordnets to a target
language T, then apply a ranking method
on the translation candidates to find best
translations in T. Our approaches are ap-
plicable to any language which has at least
one existing bilingual dictionary translat-
ing from English to it.
1 Introduction
Wordnets are intricate and substantive reposito-
ries of lexical knowledge and have become im-
portant resources for computational processing of
natural languages and for information retrieval.
Good quality Wordnets are available only for a
few "resource-rich" languages such as English and
Japanese. Published approaches to automatically
build new Wordnets are manual or semi-automatic
and can be used only for languages that already
possess some lexical resources.
The Princeton Wordnet (PWN) (Fellbaum,
1998) was painstakingly constructed manually
over many decades. Wordnets, except the PWN,
have been usually constructed by one of two ap-
proaches. The first approach translates the PWN
to T (Bilgin et al, 2004), (Barbu and Mititelu,
2005), (Kaji and Watanabe, 2006), (Sagot and
Fi?er, 2008), (Saveski and Trajkovsk, 2010) and
(Oliver and Climent, 2012); while the second ap-
proach builds a Wordnet in T, and then aligns
it with the PWN by generating translations (Gu-
nawan and Saputra, 2010). In terms of popular-
ity, the first approach dominates over the second
approach. Wordnets generated using the second
approach have different structures from the PWN;
however, the complex agglutinative morphology,
culture specific meanings and usages of words and
phrases of target languages can be maintained. In
contrast, Wordnets created using the first approach
have the same structure as the PWN.
One of our goals is to automatically gener-
ate high quality synsets, each of which is a set
of cognitive synonyms, for Wordnets having the
same structure as the PWN in several languages.
Therefore, we use the first approach to construct
Wordnets. This paper discusses the first step of a
project to automatically build core Wordnets for
languages with low amounts of resources (viz.,
Arabic and Vietnamese), resource-poor languages
(viz., Assamese) or endangered languages (viz.,
Dimasa and Karbi)
1
. The sizes and the qualities
of freely existing resources, if any, for these lan-
guages vary, but are not usually high. Hence, our
second goal is to use a limited number of freely
available resources in the target languages as in-
put to our algorithms to ensure that our methods
can be felicitously used with languages that lack
much resource. In addition, our approaches need
to have a capability to reduce noise coming from
the existing resources that we use. For transla-
tion, we use a free machine translator (MT) and
restrict ourselves to using it as the only "dictio-
nary" we can have. For research purposes, we have
obtained free access to the Microsoft Translator,
which supports translations among 44 languages.
In particular, given public Wordnets aligned to the
PWN ( such as the FinnWordNet (FWN) (Lind?n,
2010) and the JapaneseWordNet (JWN) (Isahara et
al., 2008) ) and the Microsoft Translator, we build
Wordnet synsets for arb, asm, dis, ajz and vie.
1
ISO 693-3 codes of Arabic, Assamese, Dimasa, Karbi
and Vietnamese are arb, asm, dis, ajz and vie, respectively.
106
2 Proposed approaches
In this section, we propose approaches to create
Wordnet synsets for a target languages T using ex-
isting Wordnets and the MT and/or a single bilin-
gual dictionary. We take advantage of the fact
that every synset in PWN has a unique offset-POS,
referring to the offset for a synset with a partic-
ular part-of-speech (POS) from the beginning of
its data file. Each synset may have one or more
words, each of which may be in one or more
synsets. Words in a synset have the same sense.
The basic idea is to extract corresponding synsets
for each offset-POS from existing Wordnets linked
to PWN, in several languages. Next, we translate
extracted synsets in each language to T to produce
so-called synset candidates using MT. Then, we
apply a ranking method on these candidates to find
the correct words for a specific offset-POS in T.
2.1 Generating synset candidates
We propose three approaches to generate synset
candidates for each offset-POS in T.
2.1.1 The direct translation (DR) approach
The first approach directly translates synsets in
PWN to T as in Figure 1.
Figure 1: The DR approach to construct Wordnet
synsets in a target language T.
For each offset-POS, we extract words in that
synset from the PWN and translate them to the tar-
get language to generate translation candidates.
2.1.2 Approach using intermediate Wordnets
(IW)
To handle ambiguities in synset translation, we
propose the IW approach as in Figure 2. Publicly
available Wordnets in various languages, which
we call intermediate Wordnets, are used as re-
sources to create synsets for Wordnets. For each
offset-POS, we extract its corresponding synsets
from intermediate Wordnets. Then, the extracted
synsets, which are in different languages, are
translated to T using MT to generate synset candi-
dates. Depending on which Wordnets are used and
the number of intermediate Wordnets, the num-
ber of candidates in each synset and the number
of synsets in the new Wordnets change.
Figure 2: The IW approach to construct Wordnet
synsets in a target language T
2.1.3 Approach using intermediate Wordnets
and a dictionary (IWND)
The IW approach for creating Wordnet synsets de-
creases ambiguities in translations. However, we
need more than one bilingual dictionary from each
intermediate languages to T. Such dictionaries are
not always available for many languages, espe-
cially the ones that are resource poor. The IWND
approach is like the IW approach, but instead of
translating immediately from the intermediate lan-
guages to the target language, we translate synsets
extracted from intermediate Wordnets to English
(eng), then translate them to the target language.
The IWND approach is presented in Figure 3.
Figure 3: The IWND approach to construct Word-
net synsets
107
2.2 Ranking method
For each of offset-POS, we have many translation
candidates. A translation candidate with a higher
rank is more likely to become a word belonging to
the corresponding offset-POS of the new Wordnet
in the target language. Candidates having the same
ranks are treated similarly. The rank value in the
range 0.00 to 1.00. The rank of a word w, the so-
called rank
w
, is computed as below.
rank
w
=
occur
w
numCandidates
?
numDstWordnets
numWordnets
where:
- numCandidates is the total number of trans-
lation candidates of an offset-POS
- occur
w
is the occurrence count of the word w
in the numCandidates
- numWordnets is the number of intermediate
Wordnets used, and
- numDstWordnets is the number of distinct in-
termediate Wordnets that have words trans-
lated to the word w in the target language.
Our motivation for this rank formula is the fol-
lowing. If a candidate has a higher occurrence
count, it has a greater chance to become a cor-
rect translation. Therefore, the occurrence count
of each candidate needs to be taken into account.
We normalize the occurrence count of a word by
dividing it by numCandidates. In addition, if a
candidate is translated from different words hav-
ing the same sense in different languages, this can-
didate is more likely to be a correct translation.
Hence, we multiply the first fraction by numDst-
Wordnets. To normalize, we divide results by the
number of intermediate Wordnet used.
For instance, in our experiments we use 4 in-
termediate Wordnets, viz., PWN, FWN, JWN and
WOLF Wordnet (WWN) (Sagot and Fi?er, 2008).
The words in the offset-POS "00006802-v" ob-
tained from all 4 Wordnets, their translations to
arb, the occurrence count and the rank of each
translation are presented in the second, the fourth
and the fifth columns, respectively, of Figure 4.
2.3 Selecting candidates based on ranks
We separate candidates based on three cases as be-
low.
Case 1: A candidate w has the highest chance
to become a correct word belonging to a specific
synset in the target language if its rank is 1.0. This
means that all intermediate Wordnets contain the
synset having a specific offset-POS and all words
belonging to these synsets are translated to the
Figure 4: Example of calculating the ranks of
candidates translated from words belonging to the
offset-POS "00006802-v" in 4 Wordnets: PWN,
FWN, JWN and WWN. The word
A
, word
B
and
word
C
are obtained from PWN, FWN and WWN,
respectively. The JWN does not contain this offset-
POS. TL presents transliterations of the words in
arb. The numWordnets is 4 and the numCandi-
dates is 7. The rank of each candidate is shown in
the last column of Figure 4.
same word w. The more the number of intermedi-
ate Wordnets used, the higher the chance the can-
didate with the rank of 1.0 has to become the cor-
rect translation. Therefore, we accept all transla-
tions that satisfy this criterion. An example of this
scenario is presented in Figure 5.
Figure 5: Example of Case 1: Using the IW ap-
proach with four intermediate Wordnets, PWN,
FWN, JWN and WWN. All words belonging to
the offSet-POS "00952615-n" in all 4 Wordnets are
translated to the same word "?i?n" in vie. The
word "?i?n" is accepted as the correct word be-
longing to the offSet-POS "00952615-n" in the
Vietnamese Wordnet we create.
Case 2: If an offSet-POS does not have candi-
dates having the rank of 1.0, we accept the candi-
dates having the greatest rank. Figure 6 shows the
example of the second scenario.
Case 3: If all candidates of an offSet-POS has
the same rank which is also the greatest rank, we
108
Figure 6: Example of Case 2: Using the IW ap-
proach with three intermediate Wordnets, PWN,
FWN and WWN. For the offSet-POS "01437254-
v", there is no candidate with the rank of 1.0.
The highest rank of the candidates in "vie" is 0.67
which is the word g?i. We accept "g?i" as the cor-
rect word in the offSet-POS "01437254-v" in the
Vietnamese Wordnet we create.
skip these candidates. Table 1 gives an example of
the last scenario.
Wordnet Words Cand. Rank
PWN act h?nh ??ng 0.33
PWN behave ho?t ??ng 0.33
FWN do l?m 0.33
Table 1: Example of Case 3: Using the DR ap-
proach. For the offSet-POS "00010435-v", there
is no candidate with the rank of 1.0. The highest
rank of the candidates in vie is 0.33. All of 3 can-
didates have the rank as same as the highest rank.
Therefore, we do not accept any candidate as the
correct word in the offSet-POS "00010435-v" in
the Vietnamese Wordnet we create.
3 Experiments
3.1 Publicly available Wordnets
The PWN is the oldest and the biggest available
Wordnet. It is also free. Wordnets in many
languages are being constructed and developed
2
.
However, only a few of these Wordnets are of high
quality and free for downloading. The EuroWord-
net (Vossen, 1998) is a multilingual database with
Wordnets in European languages (e.g., Dutch, Ital-
ian and Spanish). The AsianWordnet
3
provides
a platform for building and sharing Wordnets for
Asian languages (e.g., Mongolian, Thai and Viet-
namese). Unfortunately, the progress in building
most of these Wordnets is slow and they are far
from being finished.
2
http://www.globalWordnet.org/gwa/Wordnet_table.html
3
http://www.asianWordnet.org/progress
In our current experiments as mentioned ear-
lier, we use the PWN and other Wordnets linked
to the PWN 3.0 provided by the Open Multilingual
Wordnet
4
project (Bond and Foster, 2013): WWN,
FWN and JWN. Table 2 provides some details of
the Wordnets used.
Wordnet Synsets Core
JWN 57,179 95%
FWN 116,763 100%
PWN 117,659 100%
WWN 59,091 92%
Table 2: The number of synsets in the Wordnets
linked to the PWN 3.0 are obtained from the Open
Multilingual Wordnet, along with the percentage
of synsets covered from the semi-automatically
compiled list of 5,000 "core" word senses in PWN.
Note that synsets which are not linked to the PWN
are not taken into account.
For languages not supported by MT, we use
three additional bilingual dictionaries: two dictio-
naries Dict(eng,ajz) and Dict(eng,dis) provided by
Xobdo
5
; one Dict(eng,asm) created by integrat-
ing two dictionaries Dict(eng,asm) provided by
Xobdo and Panlex
6
. The dictionaries are of vary-
ing qualities and sizes. The total number of entries
in Dict(eng,ajz), Dict(eng,asm) and Dict(eng,dis)
are 4682, 76634 and 6628, respectively.
3.2 Experimental results and discussion
As previously mentioned, our primary goal is to
build high quality synsets for Wordnets in lan-
guages with low amount of resources: ajz, asm,
arb, dis and vie. The number of Wordnet synsets
we create for arb and vie using the DR approach
and the coverage percentage compared to the
PWN synsets are 4813 (4.10%) and 2983 (2.54%),
respectively. The number of synsets for each
Wordnet we create using the IW approach with
different numbers of intermediate Wordnets and
the coverage percentage compared to the PWN
synsets are presented in Table 3.
For the IWND approach, we use all 4 Wordnets
as intermediate resources. The number of Wordnet
synsets we create using the IWND approach are
presented in Table 4. We only construct Wordnet
synsets for ajz, asm and dis using the IWND ap-
4
http://compling.hss.ntu.edu.sg/omw/
5
http://www.xobdo.org/
6
http://panlex.org/
109
App. Lang. WNs Synsets % coverage
IW arb 2 48,245 41.00%
IW vie 2 42,938 36.49%
IW arb 3 61,354 52.15%
IW vie 3 57,439 48.82%
IW arb 4 75,234 63.94%
IW vie 4 72,010 61.20%
Table 3: The number of Wordnet synsets we create
using the IW approach. WNs is the number of in-
termediate Wordnets used: 2: PWN and FWN, 3:
PWN, FWN and JWN and 4: PWN, FWN, JWN
and WWN.
proach because these languages are not supported
by MT.
App. Lang. Synsets % coverage
IWND ajz 21,882 18.60%
IWND arb 70,536 59.95%
IWND asm 43,479 36.95%
IWND dis 24,131 20.51%
IWND vie 42,592 36.20%
Table 4: The number of Wordnets synsets we cre-
ate using the IWND approach.
Finally, we combine all of the Wordnet synsets
we create using different approaches to generate
the final Wordnet synsets. Table 5 presents the fi-
nal number of Wordnet synsets we create and their
coverage percentage.
Lang. Synsets % coverage
ajz 21,882 18.60%
arb 76,322 64.87%
asm 43,479 36.95%
dis 24,131 20.51%
vie 98,210 83.47%
Table 5: The number and the average score of
Wordnets synsets we create.
Evaluations were performed by volunteers who
use the language of the Wordnet as mother tongue.
To achieve reliable judgment, we use the same
set of 500 offSet-POSs, randomly chosen from the
synsets we create. Each volunteer was requested
to evaluate using a 5-point scale ? 5: excellent, 4:
good, 3: average, 2: fair and 1: bad. The aver-
age score of Wordnet synsets for arb, asm and vie
are 3.82, 3.78 and 3.75, respectively. We notice
that the Wordnet synsets generated using the IW
approach with all 4 intermediate Wordnets have
the highest average score: 4.16/5.00 for arb and
4.26/5.00 for vie. We are in the process of finding
volunteers to evaluate the Wordnet synsets for ajz
and dis.
It is difficult to compare Wordnets because the
languages involved in different papers are differ-
ent, the number and quality of input resources vary
and the evaluation methods are not standard. How-
ever, for the sake of completeness, we make an at-
tempt at comparing our results with published pa-
pers. Although our score is not in terms of percent-
age, we obtain the average score of 3.78/5.00 (or
informally and possibly incorrectly, 75.60% preci-
sion) which we believe it is better than 55.30% ob-
tained by (Bond et al, 2008) and 43.20% obtained
by (Charoenporn et al, 2008). In addition, the av-
erage coverage percentage of all Wordnet synsets
we create is 44.85% which is better than 12% in
(Charoenporn et al, 2008) and 33276 synsets ('
28.28%) in (Saveski and Trajkovsk, 2010) .
The previous studies need more than one dic-
tionary to translate between a target language
and intermediate-helper languages. For exam-
ple, to create the JWN, (Bond et al, 2008) needs
the Japanese-Multilingual dictionary, Japanese-
English lexicon and Japanese-English life sci-
ence dictionary. For asm, there are a number
of Dict(eng,asm); to the best of our knowledge
only two online dictionaries, both between eng
and asm, are available. The IWND approach re-
quires only one input dictionary between a pair of
languages. This is a strength of our method.
4 Conclusion and future work
We present approaches to create Wordnet synsets
for languages using available Wordnets, a public
MT and a single bilingual dictionary. We create
Wordnet synsets with good accuracy and high cov-
erage for languages with low resources (arb and
vie), resource-poor (asm) and endangered (ajz and
dis). We believe that our work has the potential
to construct full Wordnets for languages which do
not have many existing resources. We are in the
process of creating a Website where all Wordnet
synsets we create will be available, along with a
user friendly interface to give feedback on individ-
ual entries. We will solicit feedback from commu-
nities that use these languages as mother-tongue.
Our goal is to use this feedback to improve the
quality of the Wordnet synsets. Some of Word-
net synsets we created can be downloaded from
http://cs.uccs.edu/?linclab/projects.html.
110
References
Antoni Oliver and Salvador Climent. 2012. Parallel
corpora for Wordnet construction: Machine trans-
lation vs. automatic sense tagging. In Proceed-
ings of the 13th International Conference on Com-
putational Linguistics and Intelligent Text Process-
ing (CICLing), volume part II, pages 110-121, New
Delhi, India, March.
Beno?t Sagot and Darja Fi?er. 2008. Building a free
French Wordnet from multilingual resources. In
Proceedings of the Ontolex 2008 Workshop, Mar-
rakech, Morocco, May.
Fellbaum, Christiane. 1998. Wordnet: An electronic
lexical database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Francis Bond and Ryan Foster. 2013. Linking and ex-
tending an open multilingual Wordnet. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1352?
1362, Sofia, Bulgaria, August.
Francis Bond, Hitoshi Isahara, Kyoko Kanzaki and
Kiyotaka Uchimoto. 2008. Boot-strapping a Word-
net using multiple existing Wordnets. In Proceed-
ings of the 6th International Conference on Lan-
guage Resources and Evaluation (LREC), pages
1619?1624, Genoa, Italy, May.
Eduard Barbu and Verginica Barbu Mititelu. 2005.
Automatic building of Wordnets. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP), Borovets,
Bulgaria, September.
Gunawan and Andy Saputra. 2010. Building synsets
for Indonesian Wordnet with monolingual lexical re-
sources. In Proceedings of the International Confer-
ence on Asian Language Processing (IALP), pages
297?300, Harbin, China, December.
Hiroyuki Kaji and Mariko Watanabe. 2006. Auto-
matic construction of Japanese Wordnet. In Pro-
ceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
1262?1267, Genoa, Italy, May.
Hitoshi Isahara, Francis Bond, Kiyotaka Uchimoto,
Masao Utiyama and Kyoko Kanzaki. 2008. De-
velopment of Japanese Wordnet. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC), pages 2420?2423,
Marrakech, Morocco, May.
Krister Lind?n and Laur Carlson. 2010. FinnWordnet -
WordNet p?finska via ?vers?ttning, LexicoNordica.
Nordic Journal of Lexicography, 17:119?140.
Martin Saveski and Igor Trajkovsk. 2010. Automatic
construction of Wordnets by using machine transla-
tion and language modeling. In Proceedings of the
13th Multiconference Information Society, Ljubl-
jana, Slovenia.
Orhan Bilgin, ?zlem ?entino?glu and Kemal Oflazer.
2004. Building a Wordnet for Turkish. Romanian
Journal of Information Science and Technology, 7(1-
2): 163?172.
Piek Vossen. 1998. A multilingual database with lex-
ical semantic networks. Kluwer Academic Publish-
ers, Dordrecht, Netherlands.
Thatsanee Charoenporn, Virach Sornlertlamvanich,
Chumpol Mokarat and Hitoshi Isahara. 2008. Semi-
automatic compilation of Asian Wordnet, In Pro-
ceedings of the 14th Annual Meeting of the Associa-
tion for Natural Language Processing, pages 1041?
1044, Tokyo, Japan.
111
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 54?62,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Creating Lexical Resources for Endangered Languages
Khang Nhut Lam, Feras Al Tarouti and Jugal Kalita
Computer Science department
University of Colorado
1420 Austin Bluffs Pkwy, Colorado Springs, CO 80918, USA
{klam2,faltarou,jkalita}@uccs.edu
Abstract
This paper examines approaches to gener-
ate lexical resources for endangered lan-
guages. Our algorithms construct bilin-
gual dictionaries and multilingual the-
sauruses using public Wordnets and a ma-
chine translator (MT). Since our work re-
lies on only one bilingual dictionary be-
tween an endangered language and an ?in-
termediate helper? language, it is applica-
ble to languages that lack many existing
resources.
1 Introduction
Languages around the world are becoming extinct
at a record rate. The Ethnologue organization
1
re-
ports 424 languages as nearly extinct and 203 lan-
guages as dormant, out a total of 7,106 recorded
languages. Many other languages are becoming
endangered, a state which is likely to lead to their
extinction, without determined intervention. Ac-
cording to UNESCO, ?a language is endangered
when its speakers cease to use it, use it in fewer
and fewer domains, use fewer of its registers and
speaking styles, and/or stop passing it on to the
next generation...?. In America, UNESCO reports
134 endangered languages, e.g., Arapaho, Chero-
kee, Cheyenne, Potawatomi and Ute.
One of the hallmarks of a living and thriving
language is the existence and continued produc-
tion of ?printed? (now extended to online pres-
ence) resources such as books, magazines and ed-
ucational materials in addition to oral traditions.
There is some effort afoot to document record and
archive endangered languages. Documentation
may involve creation of dictionaries, thesauruses,
text and speech corpora. One possible way to re-
suscitate these languages is to make them more
easily learnable for the younger generation. To
1
http://www.ethnologue.com/
learn languages and use them well, tools such as
dictionaries and thesauruses are essential. Dictio-
naries are resources that empower the users and
learners of a language. Dictionaries play a more
substantial role than usual for endangered lan-
guages and are ?an instrument of language main-
tenance? (Gippert et al., 2006). Thesauruses are
resources that group words according to similarity
(Kilgarriff, 2003). For speakers and students of an
endangered language, multilingual thesauruses are
also likely to be very helpful.
This study focuses on examining techniques
that leverage existing resources for ?resource-
rich? languages to build lexical resources for low-
resource languages, especially endangered lan-
guages. The only resource we need is a single
available bilingual dictionary translating the given
endangered language to English. First, we create a
reverse dictionary from the input dictionary using
the approach in (Lam and Kalita, 2013). Then, we
generate additional bilingual dictionaries translat-
ing from the given endangered language to sev-
eral additional languages. Finally, we discuss the
first steps to constructing multilingual thesauruses
encompassing endangered and resources-rich lan-
guages. To handle the word sense ambiguity prob-
lems, we exploit Wordnets in several languages.
We experiment with two endangered languages:
Cherokee and Cheyenne, and some resource-rich
languages such as English, Finnish, French and
Japanese
2
. Cherokee is the Iroquoian language
spoken by 16,000 Cherokee people in Oklahoma
and North Carolina. Cheyenne is a Native Ameri-
can language spoken by 2,100 Cheyenne people in
Montana and Oklahoma.
The remainder of this paper is organized as fol-
lows. Dictionaries and thesauruses are introduced
in Section 2. Section 3 discusses related work. In
2
ISO 693-3 codes for Cherokee, Cheyenne, English,
Finnish, French and Japanese are chr, chy, eng, fin, fra and
jpn, respectively.
54
Section 4 and Section 5, we present approaches
for creating new bilingual dictionaries and multi-
lingual thesauruses, respectively. Experiments are
described in Section 6. Section 7 concludes the
paper.
2 Dictionaries vs. Thesauruses
A dictionary or a lexicon is a book (now, in elec-
tronic database formats as well) that consists of a
list of entries sorted by the lexical unit. A lexical
unit is a word or phrase being defined, also called
definiendum. A dictionary entry or a lexical en-
try simply contains a lexical unit and a definition
(Landau, 1984). Given a lexical unit, the defini-
tion associated with it usually contains parts-of-
speech (POS), pronunciations, meanings, exam-
ple sentences showing the use of the source words
and possibly additional information. A monolin-
gual dictionary contains only one language such
as The Oxford English Dictionary
3
while a bilin-
gual dictionary consists of two languages such as
the English-Cheyenne dictionary
4
. A lexical entry
in the bilingual dictionary contains a lexical unit in
a source language and equivalent words or multi-
word expressions in the target language along with
optional additional information. A bilingual dic-
tionary may be unidirectional or bidirectional.
Thesauruses are specialized dictionaries that
store synonyms and antonyms of selected words
in a language. Thus, a thesaurus is a resource
that groups words according to similarity (Kilgar-
riff, 2003). However, a thesaurus is different from
a dictionary. (Roget, 1911) describes the orga-
nizes of words in a thesaurus as ?... not in alpha-
betical order as they are in a dictionary, but ac-
cording to the ideas which they express.... The
idea being given, to find the word, or words, by
which that idea may be most fitly and aptly ex-
pressed. For this purpose, the words and phrases
of the language are here classed, not according to
their sound or their orthography, but strictly ac-
cording to their signification?. Particularly, a the-
saurus contains a set of descriptors, an indexing
language, a classification scheme or a system vo-
cabulary (Soergel, 1974). A thesaurus also con-
sists of relationships among descriptors. Each de-
scriptor is a term, a notation or another string of
symbols used to designate the concept. Examples
3
http://www.oed.com/
4
http://cdkc.edu/cheyennedictionary/index-
english/index.htm
of thesauruses are Roget?s international Thesaurus
(Roget, 2008), the Open Thesaurus
5
or the one at
thesaurus.com.
We believe that the lexical resources we create
are likely to help endangered languages in sev-
eral ways. These can be educational tools for lan-
guage learning within and outside the community
of speakers of the language. The dictionaries and
thesauruses we create can be of help in developing
parsers for these languages, in addition to assisting
machine or human translators to translate rich oral
or possibly limited written traditions of these lan-
guages into other languages. We may be also able
to construct mini pocket dictionaries for travelers
and students.
3 Related work
Previous approaches to create new bilingual dic-
tionaries use intermediate dictionaries to find
chains of words with the same meaning. Then,
several approaches are used to mitigate the ef-
fect of ambiguity. These include consulting the
dictionary in the reverse direction (Tanaka and
Umemura, 1994) and computing ranking scores,
variously called a semantic score (Bond and
Ogura, 2008), an overlapping constraint score, a
similarity score (Paik et al., 2004) and a con-
verse mapping score (Shaw et al., 2013). Other
techniques to handle the ambiguity problem are
merging results from several approaches: merging
candidates from lexical triangulation (Gollins and
Sanderson, 2001), creating a link structure among
words (Ahn and Frampton, 2006) and building
graphs connecting translations of words in sev-
eral languages (Mausam et al., 2010). Researchers
also merge information from several sources such
as bilingual dictionaries and corpora (Otero and
Campos, 2010) or a Wordnet (Istv?n and Shoichi,
2009) and (Lam and Kalita, 2013). Some re-
searchers also extract bilingual dictionaries from
corpora (Ljube?i
?
c and Fi?er, 2011) and (Bouamor
et al., 2013). The primary similarity among these
methods is that either they work with languages
that already possess several lexical resources or
these approaches take advantage of related lan-
guages (that have some lexical resources) by using
such languages as intermediary. The accuracies of
bilingual dictionaries created from several avail-
able dictionaries and Wordnets are usually high.
However, it is expensive to create such original
5
http://www.openthesaurus.de/
55
lexical resources and they do not always exist for
many languages. For instance, we cannot find any
Wordnet for chr or chy. In addition, these exist-
ing approaches can only generate one or just a few
new bilingual dictionaries from at least two exist-
ing bilingual dictionaries.
(Crouch, 1990) clusters documents first using
a complete link clustering algorithm and gener-
ates thesaurus classes or synonym lists based on
user-supplied parameters such as a threshold sim-
ilarity value, number of documents in a cluster,
minimum document frequency and specification
of a class formation method. (Curran and Moens,
2002a) and (Curran and Moens, 2002b) evaluate
performance and efficiency of thesaurus extrac-
tion methods and also propose an approximation
method that provides for better time complexity
with little loss in performance accuracy. (Ram?rez
et al., 2013) develop a multilingual Japanese-
English-Spanish thesaurus using freely available
resources: Wikipedia and Wordnet. They extract
translation tuples from Wikipedia from articles in
these languages, disambiguate them by mapping
to Wordnet senses, and extract a multilingual the-
saurus with a total of 25,375 entries.
One thing to note about all these approaches is
that they are resource hungry. For example, (Lin,
1998) works with a 64-million word English cor-
pus to produce a high quality thesaurus with about
10,000 entries. (Ram?rez et al., 2013) has the en-
tire Wikipedia at their disposal with millions of
articles in three languages, although for experi-
ments they use only about 13,000 articles in total.
When we work with endangered or low-resource
languages, we do not have the luxury of collecting
such big corpora or accessing even a few thousand
articles from Wikipedia or the entire Web. Many
such languages have no or very limited Web pres-
ence. As a result, we have to work with whatever
limited resources are available.
4 Creating new bilingual dictionaries
A dictionary Dict(S,T) between a source language
S and a target language T has a list of entries. Each
entry contains a word s in the source language S,
part-of-speech (POS) and one or more translations
in the target language T. We call such a transla-
tion t. Thus, a dictionary entry is of the form
<s
i
,POS,t
i1
>, <s
i
,POS,t
i2
>, ....
This section examines approaches to create new
bilingual dictionaries for endangered languages
from just one dictionary Dict(S,I), where S is the
endangered source language and I is an ?inter-
mediate helper? language. We require that the
language I has an available Wordnet linked to
the Princeton Wordnet (PWN) (Fellbaum, 1998).
Many endangered languages have a bilingual dic-
tionary, usually to or from a resource-rich lan-
guage like French or English which is the inter-
mediate helper language in our experiments. We
make an assumption that we can find only one uni-
directional bilingual dictionary translating from a
given endangered language to English.
4.1 Generating a reverse bilingual dictionary
Given a unidirectional dictionary Dict(S,I) or
Dict(I,S), we reverse the direction of the entries
to produce Dict(I,S) or Dict(S,I), respectively. We
apply an approach called Direct Reversal with
Similarity (DRwS), proposed in (Lam and Kalita,
2013) to create a reverse bilingual dictionary from
an input dictionary.
The DRwS approach computes the distance be-
tween translations of entries by measuring their se-
mantic similarity, the so-called simValue. The sim-
Value between two phrases is calculated by com-
paring the similarity of the ExpansionSet for ev-
ery word in one phrase with ExpansionSet of ev-
ery word in the other phrase. An ExpansionSet of
a phrase is a union of the synset, synonym set, hy-
ponym set, and/or hypernym set of every word in
it. The synset, synonym, hyponym and hypernym
sets of a word are obtained from PWN. The greater
is the simValue between two phrases, the more se-
mantically similar are these phrases. According to
(Lam and Kalita, 2013), if the simValue is equal to
or greater than 0.9, the DRwS approach produces
the ?best? reverse dictionary.
For creating a reverse dictionary, we skip en-
tries with multiword expression in the translation.
Based on our experiments, we have found that ap-
proach is successful and hence, it may be an effec-
tive way to automatically create a new bilingual
dictionary from an existing one. Figure 1 presents
an example of generating entries for the reverse
dictionary.
4.2 Building bilingual dictionaries to/from
additional languages
We propose an approach using public Word-
nets and MT to create new bilingual dictionaries
Dict(S,T) from an input dictionary Dict(S,I). As
previously mentioned, I is English in our exper-
56
Figure 1: Example of creating entries for a reverse
dictionary Dict(eng,chr) from Dict(chr,eng). The
simValue between the words "ocean" and "sea" is
0.98, which is greater than the threshold of 0.90.
Therefore, the words "ocean" and "sea" in English
are hypothesized to have both meanings "ame-
quohi" and "ustalanali" in Cherokee. We add these
entries to Dict(eng, chr).
iments. Dict(S,T) translates a word in an endan-
gered language S to a word or multiword expres-
sion in a target language T. In particular, we create
bilingual dictionaries for an endangered language
S from a given dictionary Dict(S,eng). Figure 2
presents the approach to create new bilingual dic-
tionaries.
Figure 2: The approach for creating new bilin-
gual dictionaries from intermediate Wordnets and
a MT.
For each entry pair (s,e) in a given dictionary
Dict(S,eng), we find all synonym words of the
word e to create a list of synonym words in En-
glish: SY N
eng
. SY N
eng
of the word eng is
obtained from the PWN. Then, we find all syn-
onyms of words belonging to SY N
eng
in sev-
eral non-English languages to generate SY N
L
,
L ? {fin, fra, jpn}. SY N
L
in the language L is
extracted from the publicly available Wordnet in
language L linked to the PWN. Next, translation
candidates are generated by translating all words
in SY N
L
, L ? {eng, fin, fra, jpn} to the target
language T using an MT. A translation candidate is
considered a correct translation of the source word
in the target language if its rank is greater than a
threshold. For each word s, we may have many
candidates. A translation candidate with a higher
rank is more likely to become a correct translation
in the target language. The rank of a candidate is
computed by dividing its occurrence count by the
total number of candidates. Figure 3 shows an ex-
ample of creating entries for Dict(chr,vie), where
vie is Vietnamese, from Dict(chr,eng).
Figure 3: Example of generating new entries for
Dict(chr,vie) from Dict(chr,eng). The word "ayvt-
seni" in chr is translated to "throat" in eng. We
find all synonym words for "throat" in English to
generate SY N
eng
and all synonyms in fin, fra and
jpn for all words in SY N
eng
. Then, we translate
all words in all SY N
L
s to vie and rank them. Ac-
cording to rank calculations, the best translations
of "ayvtseni" in chr are the words "c? h?ng" and
"h?ng" in vie.
57
5 Constructing thesauruses
As previously mentioned, we want to generate a
multilingual thesaurus THS composed of endan-
gered and resource-rich languages. For example,
we build the thesaurus encompassing an endan-
gered language S and eng, fin, fra and jpn. Our
thesaurus contains a list of entries. Every entry has
a unique ID. Each entry is a 7-tuple: ID, SY N
S
,
SY N
eng
, SY N
fin
, SY N
fra
, SY N
jpn
and POS.
Each SY N
L
contains words that have the same
sense in language L. All SY N
L
, L ? {S, eng, fin,
fra, jpn} with the same ID have the same sense.
This section presents the initial steps in con-
structing multilingual thesauruses using Wordnets
and the bilingual dictionaries we create. The
approach to create a multilingual thesaurus en-
compassing an endangered language and several
resource-rich languages is presented in Figure 4
and Algorithm 1.
Figure 4: The approach to construct a multilingual
thesaurus encompassing an endangered language
S and resource-rich language.
First, we extract SY N
L
in resource-rich lan-
guages from Wordnets. To extract SY N
eng
,
SY N
fin
, SY N
fra
and SY N
jpn
, we use PWN
and Wordnets linked to the PWN provided by
the Open Multilingual Wordnet
6
project (Bond
and Foster, 2013): FinnWordnet (FWN) (Lind?n,
2010), WOLF (WWN) (Sagot and Fi?er, 2008)
and JapaneseWordnet (JWN) (Isahara et al.,
2008). For each Offset-POS, we extract its cor-
responding synsets from PWN, FWN, WWN and
6
http://compling.hss.ntu.edu.sg/omw/
JWN to generate SY N
eng
, SY N
fin
, SY N
fra
and
SY N
jpn
(lines 7-10). The POS of the entry is
the POS extracted from the Offset-POS (line 5).
Since these Wordnets are aligned, a specific offset-
POS retrieves synsets that are equivalent sense-
wise. Then, we translate all SY N
L
s to the given
endangered language S using bilingual dictionar-
ies we created in the previous section (lines 11-
14). Finally, we rank translation candidates and
add the correct translations to SY N
S
(lines 15-
19). The rank of a candidate is computed by di-
viding its occurrence count by the total number of
candidates. If a candidate has a rank value greater
than a threshold, we accept it as a correct transla-
tion and add it to SY N
S
.
Algorithm 1
Input: Endangered language S, PWN, FWN,
WWN, JWN, Dict(eng,S), Dict(fin,S), Dict(fra,S)
and Dict(jpn,S)
Output: thesaurus THS
1: ID:=0
2: for all offset-POSs in PWN do
3: ID++
4: candidates := ?
5: POS=extract(offset-POS)
6: SY N
S
:= ?
7: SY N
eng
=extract(offset-POS, PWN)
8: SY N
fin
=extract(offset-POS, FWN)
9: SY N
fra
=extract(offset-POS, WWN)
10: SY N
jpn
=extract(offset-POS, JWN)
11: candidates+=translate(SY N
eng
,S)
12: candidates+=translate(SY N
fin
,S)
13: candidates+=translate(SY N
fra
,S)
14: candidates+=translate(SY N
jpn
,S)
15: for all candidate in candidates do
16: if rank(candidate) > ? then
17: add(candidate,SY N
S
)
18: end if
19: end for
20: add ID, POS and all SY N
L
into THS
21: end for
Figure 5 presents an example of creating an en-
try for the thesaurus. We generate entries for the
multilingual thesaurus encompassing of Cherokee,
English, Finnish, French and Japanese.
We extract words belonging to offset-POS
"09426788-n" in PWN, FWN, WWN and JWN
and add them into corresponding SY N
L
. The
POS of this entry is "n", which is a "noun".
Next, we use the bilingual dictionaries we cre-
58
Figure 5: Example of generating an entry in the
multilingual thesaurus encompassing Cherokee,
English, Finnish, French and Japanese.
ated to translate all words in SY N
eng
, SY N
fin
,
SY N
fra
, SY N
jpn
to the given endangered lan-
guage, Cherokee, and rank them. According to the
rank calculations, the best Cherokee translation is
the word ?ustalanali?. The new entry added to the
multilingual thesaurus is presented in Figure 6.
Figure 6: An entry of the multilingual thesaurus
encompassing Cherokee, English, Finnish, French
and Japanese.
6 Experimental results
Ideally, evaluation should be performed by volun-
teers who are fluent in both source and destination
languages. However, for evaluating created dic-
tionaries and thesauruses, we could not recruit any
individuals who are experts in two corresponding
languages. We are in the process of finding vol-
unteers who are fluent in both languages for some
selected resources we create.
6.1 Datasets used
We start with two bilingual dictionaries:
Dict(chr,eng)
7
and Dict(chy,eng)
8
that we
obtain from Web pages. These are unidirectional
bilingual dictionaries. The numbers of entries
in Dict(chr,eng) and Dict(chy,eng) are 3,199
and 28,097, respectively. For entries in these
input dictionaries without POS information, our
algorithm chooses the best POS of the English
word, which may lead to wrong translations. The
Microsoft Translator Java API
9
is used as another
main resource. We were given free access to this
API. We could not obtain free access to the API
for the Google Translator.
The synonym lexicons are the synsets of PWN,
FWN, JWN and WWN. Table 1 provides some de-
tails of the Wordnets used.
Wordnet Synsets Core
JWN 57,179 95%
FWN 116,763 100%
PWN 117,659 100%
WWN 59,091 92%
Table 1: The number of synsets in the Wordnets
linked to PWN 3.0 are obtained from the Open
Multilingual Wordnet, along with the percentage
of synsets covered from the semi-automatically
compiled list of 5,000 "core" word senses in PWN.
Note that synsets which are not linked to the PWN
are not taken into account.
6.2 Creating reverse bilingual dictionaries
From Dict(chr,eng) and Dict(chy,eng), we create
two reverse bilingual dictionaries Dict(eng,chr)
with 3,538 entries and Dict(eng,chy) with 28,072
entries
Next, we reverse the reverse dictionaries we
produce to generate new reverse of the reverse
(RR) dictionaries, then integrate the RR dictio-
naries with the input dictionaries to improve the
sizes of dictionaries. During the process of gen-
erating new reverse dictionaries, we already com-
puted the semantic similarity values among words
to find words with the same meanings. We use a
simple approach called the Direct Reversal (DR)
approach in (Lam and Kalita, 2013) to create
7
http://www.manataka.org/page122.html
8
http://www.cdkc.edu/cheyennedictionary/index-
english/index.htm
9
https://datamarket.azure.com/dataset/bing/microsofttranslator
59
these RR dictionaries. To create a reverse dictio-
nary Dict(T,S), the DR approach takes each entry
<s,POS,t> in the input dictionary Dict(S,T) and
simply swaps the positions of s and t. The new
entry <t,POS,s> is added into Dict(T,S). Figure 7
presents an example.
Figure 7: Given a dictionary Dict(chy,eng), we
create a new Dict(eng,chy) using the DRwS ap-
proach of (Lam and Kalita, 2013). Then, we create
a new Dict(chy,eng) using the DR approach from
the created dictionary Dict(eng,chy). Finally, we
integrate the generated dictionary Dict(chy,eng)
with the input dictionary Dict(chy,eng) to create a
new dictionary Dict(chy,eng) with a greater num-
ber of entries
The number of entries in the integrated dictio-
naries Dict(chr,eng) and Dict(chy,eng) are 3,618
and 47,529, respectively. Thus, the number of en-
tries in the original dictionaries have "magically"
increased by 13.1% and 69.21%, respectively.
6.3 Creating additional bilingual dictionaries
We can create dictionaries from chr or chy to
any non-eng language supported by the Microsoft
Translator, e.g., Arabic (arb), Chinese (cht), Cata-
lan (cat), Danish (dan), German (deu), Hmong
Daw (mww), Indonesian (ind), Malay (zlm), Thai
(tha), Spanish (spa) and vie. Table 2 presents the
number of entries in the dictionaries we create.
These dictionaries contain translations only with
the highest ranks for each word.
Although we have not evaluated entries in the
particular dictionaries in Table 1, evaluation of
dictionaries with non-endangered languages, but
using the same approach, we have confidence that
these dictionaries are of acceptable, if not very
good quality.
Dictionary Entries Dictionary Entries
chr-arb 2,623 chr-cat 2,639
chr-cht 2,607 chr-dan 2,655
chr-deu 2,629 chr-mww 2,694
chr-ind 2,580 chr-zlm 2,633
chr-spa 2,607 chr-tha 2,645
chr-vie 2,618 chy-arb 10,604
chy-cat 10,748 chy-cht 10,538
chy-dan 10,654 chy-deu 10,708
chy-mww 10,790 chy-ind 10,434
chy-zlm 10,690 chy-spa 10,580
chy-tha 10,696 chy-vie 10,848
Table 2: The number of entries in some dictionar-
ies we create.
6.4 Creating multilingual thesauruses
We construct two multilingual thesauruses:
THS
1
(chr, eng, fin, fra, jpn) and THS
2
(chy, eng,
fin, fra, jpn). The number of entries in THS
1
and THS
2
are 5,073 and 10,046, respectively.
These thesauruses we construct contain words
with rank values above the average. A similar
approach used to create Wordnet synsets (Lam
et al., 2014) has produced excellent results. We
believe that our thesauruses reported in this paper
are of acceptable quality.
6.5 How to evaluate
Currently, we are not able to evaluate the dictio-
naries and thesauruses we create. In the future, we
expect to evaluate our work using two methods.
First, we will use the standard approach which is
human evaluation to evaluate resources as previ-
ously mentioned. Second, we will try to find an
additional bilingual dictionary translating from an
endangered language S (viz., chr or chy) to another
?resource-rich? non-English language (viz., fin or
fra), then, create a new dictionary translating from
S to English using the approaches we have intro-
duced. We plan to evaluate the new dictionary we
create, say Dict(chr,eng) against the existing dic-
tionary Dict(chr,eng).
7 Conclusion and future work
We examine approaches to create bilingual dictio-
naries and thesauruses for endangered languages
from only one input dictionary, publicly avail-
able Wordnets and an MT. Taking advantage of
available Wordnets linked to the PWN helps re-
duce ambiguities in dictionaries we create. We
60
run experiments with two endangered languages:
Cherokee and Cheyenne. We have also experi-
mented with two additional endangered languages
from Northeast India: Dimasa and Karbi, spo-
ken by about 115,000 and 492,000 people, respec-
tively. We believe that our research has the po-
tential to increase the number of lexical resources
for languages which do not have many existing re-
sources to begin with. We are in the process of
creating reverse dictionaries from bilingual dictio-
naries we have already created. We are also in
the process of creating a Website where all dic-
tionaries and thesauruses we create will be avail-
able, along with a user friendly interface to dis-
seminate these resources to the wider public as
well as to obtain feedback on individual entries.
We will solicit feedback from communities that
use the languages as mother-tongues. Our goal
will be to use this feedback to improve the qual-
ity of the dictionaries and thesauruses. Some of
resources we created can be downloaded from
http://cs.uccs.edu/?linclab/projects.html
References
Adam Kilgarriff. 2003. Thesauruses for natu-
ral language processing. In Proceedings of the
Joint Conference on Natural Language Processing
and Knowledge Engineering, pages 5?13, Beijing,
China, October.
Benoit Sagot and Darja Fi?er. 2008. Building a free
French Wordnet from multilingual resources. In
Proceedings of OntoLex, Marrakech, Morocco.
Carolyn J. Crouch 1990. An approach to the auto-
matic construction of global thesauri, Information
Processing & Management, 26(5): 629?640.
Christiane Fellbaum. 1998. Wordnet: An Electronic
Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Dagobert Soergel. 1974. Indexing languages and the-
sauri: construction and maintenance. Melville Pub-
lishing Company, Los Angeles, California.
Dhouha Bouamor, Nasredine Semmar and Pierre
Zweigenbaum. 2013 Using Wordnet and Semantic
Similarity for Bilingual Terminology Mining from
Comparable Corpora. In Proceedings of the 6th
Workshop on Building and Using Comparable Cor-
pora, pages 16?23, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th In-
ternational Conference on Computational Linguis-
tics (Volume 2), pages 768?774, Montreal, Quebec,
Canada.
Francis Bond and Kentaro Ogura. 2008 Combin-
ing linguistic resources to create a machine-tractable
Japanese-Malay dictionary. Language Resources
and Evaluation, 42(2): 127?136.
Francis Bond and Ryan Foster. 2013. Linking and
extending an open multilingual Wordnet. In Pro-
ceedings of 51st Annual Meeting of the Association
for Computational Linguistics (ACL 2013), pages
1352?1362, Sofia, Bulgaria, August.
Hitoshi Isahara, Francis Bond, Kiyotaka Uchimoto,
Masao Utiyama and Kyoko Kanzaki. 2008. De-
velopment of Japanese Wordnet. In Proceedings
of 6th International Conference on Language Re-
sources and Evaluation (LREC 2008), pages 2420?
2423, Marrakech, Moroco, May.
James R. Curran and Marc Moens. 2002a. Scaling
context space. In Proceedings of the 40th Annual
Meeting of Association for Computational Linguis-
tics (ACL 2002), pages 231?238, Philadelphia, USA,
July.
James R. Curran and Marc Moens. 2002b. Improve-
ments in automatic thesaurus extraction, In Pro-
ceedings of the Workshop on Unsupervised lexical
acquisition (Volume 9), pages 59?66, Philadelphia,
USA, July. Association for Computational Linguis-
tics.
Jessica Ram?rez, Masayuki Asahara and Yuji Mat-
sumoto. 2013. Japanese-Spanish thesaurus con-
struction using English as a pivot. arXiv preprint
arXiv:1303.1232.
Jost Gippert, Nikolaus Himmelmann and Ulrike Mosel,
eds. 2006. Essentials of Lnguage Documenta-
tion. Vol. 178, Walter de Gruyter GmbH & Co. KG,
Berlin, Germany.
Khang N. Lam and Jugal Kalita. 2013. Creating re-
verse bilingual dictionaries. In Proceedings of the
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT), pages 524?
528, Atlanta, USA, June.
Khang N. Lam, Feras A. Tarouti and Jugal Kalita.
2014. Automatically constructing Wordnet synsets.
To appear at the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2014),
Baltimore, USA, June.
Kisuh Ahn and Matthew Frampton. 2006. Automatic
generation of translation dictionaries using interme-
diary languages. In Proceedings of the Interna-
tional Workshop on Cross-Language Knowledge In-
duction, pages 41?44, Trento, Italy, April. European
Chapter of the Association for Computational Lin-
guistics.
Krister Lind?n and Lauri Carlson 2010. FinnWordnet -
WordNet p?finska via ?vers?ttning, LexicoNordica.
Nordic Journal of Lexicography (Volume 17), pages
119?140.
61
Kumiko Tanaka and Kyoji Umemura. 1994. Construc-
tion of bilingual dictionary intermediated by a third
language. In Proceedings of the 15th Conference on
Computational linguistics (COLING 1994), Volume
1, pages 297?303, Kyoto, Japan, August. Associa-
tion for Computational Linguistics.
Kyonghee Paik, Satoshi Shirai and Hiromi Nakaiwa.
2004. Automatic construction of a transfer dictio-
nary considering directionality. In Proceedings of
the Workshop on Multilingual Linguistic Resources,
pages 31?38, Geneva, Switzerland, August . Asso-
ciation for Computational Linguistics.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer and Jeff Bilmes 2010. Panlingual lexical trans-
lation via probabilistic inference. Artificial Intelli-
gence, 174(2010): 619?637.
Nikola Ljube?i?c and Darja Fi?er. 2011. Bootstrap-
ping bilingual lexicons from comparable corpora for
closely related languages. In Proceedings of the
14th International Conference on Text, Speech and
Dialogue (TSD 2011), pages 91?98. Plze?n, Czech
Republic, September.
Pablo G. Otero and Jos? R.P. Campos. 2010. Auto-
matic generation of bilingual dictionaries using in-
termediate languages and comparable corpora. In
Proceedings of the 11th International Conference on
Computational Linguistic and Intelligent Text Pro-
cessing (CICLing?10 ), pages 473?483, Ias?i, Roma-
nia, March.
Peter M. Roget. 1911. Roget?s Thesaurus of English
Words and Phrases.... Thomas Y. Crowell Com-
pany, New York, USA.
Peter M. Roget. 2008. Roget?s International The-
saurus, 3rd Edition. Oxford & IBH Publishing
Company Pvt, New Delhi, India.
Ryan Shaw, Anindya Datta, Debra VanderMeer and
Kaushik Datta. 2013. Building a scalable database
- Driven Reverse Dictionary. IEEE Transactions on
Knowledge and Data Engineering, 25(3): 528?540.
Sidney I. Landau 1984. Dictionaries: the art and
craft of lexicography. Charles Scribner?s Sons, New
York, USA.
Tim Gollins and Mark Sanderson. 2001. Improving
cross language information retrieval with triangu-
lated translation. In Proceedings of the 24th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
90?95, New Orleans, Louisiana, USA, September.
Varga Istv?n and Yokoyama Shoichi. 2009. Bilin-
gual dictionary generation for low-resourced lan-
guage pairs. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing (Volume 2), pages 862?870, Singapore,
August. Association for Computational Linguistics.
62
