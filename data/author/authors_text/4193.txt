R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 507 ? 518, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Exploring Syntactic Relation Patterns  
for Question Answering 
Dan Shen1,2, Geert-Jan M. Kruijff 1, and Dietrich Klakow2 
1
 Department of Computational Linguistics, Saarland University, 
Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany 
{dshen, gj}@coli.uni-sb.de 
2
 Lehrstuhl Sprach Signal Verarbeitung,Saarland University, 
Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany 
{dietrich.klakow}@lsv.uni-saarland.de 
Abstract. In this paper, we explore the syntactic relation patterns for open-
domain factoid question answering.  We propose a pattern extraction method to 
extract the various relations between the proper answers and different types of 
question words, including target words, head words, subject words and verbs, 
from syntactic trees.  We further propose a QA-specific tree kernel to partially 
match the syntactic relation patterns.  It makes the more tolerant matching be-
tween two patterns and helps to solve the data sparseness problem.  Lastly, we 
incorporate the patterns into a Maximum Entropy Model to rank the answer 
candidates.  The experiment on TREC questions shows that the syntactic rela-
tion patterns help to improve the performance by 6.91 MRR based on the com-
mon features. 
1   Introduction 
Question answering is to find answers for open-domain natural language questions in 
a large document collection.  A typical QA system usually consists of three basic 
modules: 1. Question Processing (QP) Module, which finds some useful information 
from questions, such as expected answer type and key words; 2. Information Retrieval 
(IR) Module, which searches a document collection to retrieve a set of relevant sen-
tences using the key words; 3. Answer Extraction (AE) Module, which analyzes the 
relevant sentences using the information provided by the QP module and identify the 
proper answer.  In this paper, we will focus on the AE module. 
In order to find the answers, some evidences, such as expected answer types and 
surface text patterns, are extracted from answer sentences and incorporated in the AE 
module using a pipelined structure, a scoring function or some statistical-based meth-
ods.  However, the evidences extracted from plain texts are not sufficient to identify a 
proper answer.  For examples, for ?Q1910: What are pennies made of??, the expected 
answer type is unknown; for ?Q21: Who was the first American in space??, the sur-
face patterns may not detect the long-distance relations between the question key 
phrase ?the first American in space? and the answer ?Alan Shepard? in ?? that car-
ried Alan Shepard on a 15 - minute suborbital flight in 1961 , making him the first 
508 D. Shen, G.-J.M. Kruijff, and D. Klakow 
American in space.?  To solve these problems, more evidences need to be extracted 
from the more complex data representations, such as parse trees. 
In this paper, we explore the syntactic relation patterns (SRP) for the AE module.  
An SRP is defined as a kind of relation between a question word and an answer can-
didate in the syntactic tree.  Different from the textual patterns, the SRPs capture the 
relations based on the sentence syntactic structure rather than the sentence surface.  
Therefore, they may get the deeper understanding of the relations and capture the long 
range dependency between words regardless of their ordering and distance in the 
surface text.  Based on the observation of the task, we find that the syntactic relations 
between different types of question words and answers vary a lot with each other.  We 
classify the question words into four classes, including target words, head words, 
subject phrases and verbs, and generate the SRPs for them respectively.  Firstly, we 
generate the SRPs from the training data and score them based on the support and 
confidence measures.  Next, we propose a QA-specific tree kernel to calculate the 
similarity between two SRPs in order to match the patterns from the unseen data into 
the pattern set.  The tree kernel makes the partial matching between two patterns and 
helps to solve the data sparseness problem.  Lastly, we incorporate the SRPs into a 
Maximum Entropy Model along with some common features to classify the answer 
candidates.  The experiment on TREC questions shows that the syntactic relation 
patterns improve the performance by 6.91 MRR based on the common features. 
Although several syntactic relations, such as subject-verb and verb-object, have 
been also considered in some other systems, they are basically extracted using a small 
number of hand-built rules.  As a result, they are limited and costly.  In our task, we 
automatically extract the various relations between different question words and an-
swers and more tolerantly match the relation patterns using the tree kernel. 
2   Related Work 
The relations between answers and question words have been explored by many suc-
cessful QA systems based on certain sentence representations, such as word sequence, 
logic form, parse tree, etc. 
In the simplest case, a sentence is represented as a sequence of words.  It is as-
sumed that, for certain type of questions, the proper answers always have certain 
surface relations with the question words.  For example, ?Q: When was X born??, the 
proper answers often have such relation ?<X> ( <Answer>--? with the question 
phrase X .  [14] first used a predefined pattern set in QA and achieved a good per-
formance at TREC10.  [13] further developed a bootstrapping method to learn the 
surface patterns automatically.  When testing, most of them make the partial matching 
using regular expression.  However, such surface patterns strongly depend on the 
word ordering and distance in the text and are too specific to the question type. 
LCC [9] explored the syntactic relations, such as subject, object, prepositional at-
tachment and adjectival/adverbial adjuncts, based on the logic form transformation.  
Furthermore they used a logic prover to justify the answer candidates.  The prover is 
accurate but costly. 
Most of the QA systems explored the syntactic relations on the parse tree.  Since 
such relations do not depend on the word ordering and distance in the sentence, they 
may cope with the various surface expressions of the sentence.  ISI [7] extracted the 
 Exploring Syntactic Relation Patterns for Question Answering 509 
relations, such as ?subject-verb? and ?verb-object?, in the answer sentence tree and 
compared with those in the question tree.  IBM?s Maximum Entropy-based model 
[10] integrated a rich feature set, including words co-occurrence scores, named entity, 
dependency relations, etc.  For the dependency relations, they considered some prede-
fined relations in trees by partial matching.  BBN [15] also considered the verb-
argument relations. 
However, most of the current QA systems only focus on certain relation types, 
such as verb-argument relations, and extract them from the syntactic tree using some 
heuristic rules.  Therefore, extracting such relations is limited in a very local context 
of the answer node, such as its parent or sibling nodes, and does not involve long 
range dependencies.  Furthermore, most of the current systems only concern the rela-
tions to certain type of question words, such as verb.  In fact, different types of ques-
tion words may have different indicative relations with the proper answers.  In this 
paper, we will automatically extract more comprehensive syntactic relation patterns 
for all types of question words, partially match them using a QA-specific tree kernel 
and evaluate their contributions by integrating them into a Maximum Entropy Model. 
3   Syntactic Relation Pattern Generating 
In this section, we will discuss how to extract the syntactic relation patterns.  Firstly, 
we briefly introduce the question processing module which provides some necessary 
information to the answer extraction module.  Secondly, we generate the dependency 
tree of the answer sentence and map the question words into the tree using a Modified 
Edit Distance (MED) algorithm.  Thirdly, we define and extract the syntactic relation 
patterns in the mapped dependency tree.  Lastly, we score and filter the patterns. 
3.1   Question Processing Module 
The key words are extracted from the questions.  Considering that different key words 
may have different syntactic relations with the answers, we divide the key words into 
the following four types: 
1. Target Words, which are extracted from what / which questions.  Such words indi-
cate the expected answer types, such as ?party? in ?Q1967: What party led ???. 
2. Head Words, which are extracted from how questions.  Such words indicate the 
expected answer heads, such as ?dog? in the ?Q210: How many dogs pull ??? 
3. Subject Phrases, which are extracted from all types of questions.  They are the base 
noun phrases of the questions except the target words and the head words. 
4. Verbs, which are the main verbs extracted from non-definition questions. 
The key words described above are identified and classified based on the question 
parse tree.  We employ the Collins Parser [2] to parse the questions and the answer 
sentences. 
3.2   Question Key Words Mapping 
From this section, we start to introduce the AE module.  Firstly, the answer sentences 
are tagged with named entities and parsed.  Secondly, the parse trees are transformed 
510 D. Shen, G.-J.M. Kruijff, and D. Klakow 
to the dependency trees based on a set of rules.  To simplify a dependency tree, some 
special rules are used to remove the non-useful nodes and dependency information.  
The rules include 
1. Since the question key words are always NPs and verbs, only the syntactic rela-
tions between NP and NP / NP and verb are considered. 
2. The original form of Base Noun Phrase (BNP) is kept and the dependency relations 
within the BNPs are not considered, such as adjective-noun.  A base noun phrase is 
defined as the smallest noun phrase in which there are no noun phrases embedded. 
An example of the dependency tree is shown in Figure 1.  We regard all BNP 
nodes and leaf nodes as answer candidates. 
 
Fig. 1. Dependency tree and Tagged dependency tree 
Next, we map the question key words into the simplified dependency trees.  We 
propose a weighted edit distance (WED) algorithm, which is to find the similarity 
between two phrases by computing the minimal cost of operations needed to transform 
one phrase into the other, where an operation is an insertion, deletion, or substitution. 
Different from the commonly-used edit distance algorithm [11], the WED defines 
the more flexible cost function which incorporates the morphological and semantic 
alternations of the words.  The morphological alternations indicate the inflections of 
noun/verb.  For example, for Q2149: How many Olympic gold medals did Carl Lewis 
win?   We map the verb win to the nominal winner in the answer sentence ?Carl 
Lewis, winner of nine Olympic gold medals, thinks that ??.  The morphological alter-
nations are found based on a stemming algorithm and the ?derivationally related 
forms? in WordNet [8].  The semantic alternations consider the synonyms of the 
words.  Some types of the semantic relations in WordNet enable the retrieval of syno-
nyms, such as hypernym, hyponym, etc.  For example, for Q212: Who invented the 
electric guitar?  We may map the verb invent to its direct hypernym create in answer 
sentences.  Based on the observation of the task, we set the substitution costs of the 
alternations as follows: Identical words have cost 0; Words with the same morpho-
logical root have cost 0.2; Words with the hypernym or hyponym relations have cost 
tagged dependency tree dependency tree 
live 
BNP 
NER_PER 
Ellington 
BNP 
NER_LOC 
BNP 
Washington his early NNP 20s
NER_DAT 
VER
VER: the verb of the question 
SUB: the subject words of the question 
TGT_HYP: the hypernym of the target word of the question  
live 
BNP 
NER_PER 
SUB 
Ellington 
BNP 
NER_LOC 
TGT_HYP 
BNP 
Washington his early NNP 20s 
NER_DAT 
Q1916: What city did Duke Ellington live in?  
A: Ellington lived in Washington until his early 20s. 
 Exploring Syntactic Relation Patterns for Question Answering 511 
0.4; Words in the same SynSet have cost 0.6; Words with subsequence relations have 
cost 0.8; otherwise, words have cost 1.  Figure 1 also shows an example of the tagged 
dependency tree. 
3.3   Syntactic Relation Pattern Extraction 
A syntactic relation pattern is defined as the smallest subtree which covers an answer 
candidate node and one question key word node in the dependency tree.  To capture 
different relations between answer candidates and different types of question words, 
we generate four pattern sets, called PSet_target, PSet_head, PSet_subject and 
PSet_verb, for the answer candidates.  The patterns are extracted from the training 
data.  Some pattern examples are shown in Table 1.  For a question Q, there are a set 
of relevant sentences SentSet.  The extraction process is as follows: 
1. for each question Q in the training data 
2. question processing model extract the key words of Q 
3. for each sentence s in SentSet 
a) parse s  
b) map the question key words into the parse tree 
c) tag all BNP nodes in the parse tree as answer candidates. 
d) for each answer candidate (ac) node 
 for each question word (qw) node 
      extract the syntactic relation pattern (srp) for ac and qw  
 add srp to PSet_target, PSet_head, PSet_subject or 
PSet_verb based on the types of qw. 
Table 1. Examples of the patterns in the four pattern sets 
PatternSet  Patterns Sup. Conf. 
(NPB~AC~TGT) 0.55 0.22 
(NPB~AC~null (NPB~null~TGT)) 0.08 0.06 PSet_target 
(NPB~null~null (NPB~AC~null) (NPB~null~TGT)) 0.02 0.09 
PSet_head (NPB~null~null (CD~AC~null) (NPB~null~HEAD)) 0.59 0.67 
(VP~null~null (NPB~null~SUB) (NPB~null~null 
(NPB~AC~null))) 
0.04 0.33 
PSet_subject 
(NPB~null~null (NPB~null~SUB) (NPB~AC~null)) 0.02 0.18 
PSet_verb (VP~null~VERB (NPB~AC~null)) 0.18 0.16 
3.4   Syntactic Relation Pattern Scoring 
The patterns extracted in section 3.3 are scored by support and confidence measures.  
Support and confidence measures are most commonly used to evaluate the association 
rules in the data mining area.  The support of a rule is the proportion of times the rule 
applies.  The confidence of a rule is the proportion of times the rule is correct.  In our 
task, we score a pattern by measuring the strength of the association rule from the 
pattern to the proper answer (the pattern is matched => the answer is correct). Let pi 
be any pattern in the pattern set PSet , 
512 D. Shen, G.-J.M. Kruijff, and D. Klakow 
the number of  in which  is correct
support( )
the size of 
p acipi PSet
=  
the number of  in which  is correct
confidence( )
the number of  
p acipi pi
=  
We score the patterns in the PSet_target, PSet_head, PSet_subject and PSet_verb 
respectively.  If the support value is less than the threshold supt or the confidence 
value is less than the threshold conft , the pattern is removed from the set.  In the ex-
periment, we set supt 0.01 and conft  0.5.  Table 1 lists the support and confidence of 
the patterns. 
4   Syntactic Relation Pattern Matching 
Since we build the pattern sets based on the training data in the current experiment, 
the pattern sets may not be large enough to cover all of the unseen cases.  If we make 
the exact match between two patterns, we will suffer from the data sparseness prob-
lem.  So a partial matching method is required.  In this section, we will propose a QA-
specific tree kernel to match the patterns. 
A kernel function 1 2( , ) : [0, ]K x x ? ?X X R , is a similarity measure between 
two objects 1x and 2x with some constraints.  It is the most important component of 
kernel methods [16].  Tree kernels are the structure-driven kernels used to calculate 
the similarity between two trees.  They have been successfully accepted in the natural 
language processing applications, such as parsing [4], part of speech tagging and 
named entity extraction [3], and information extraction [5, 17].  To our knowledge, 
tree kernels have not been explored in answer extraction. 
Suppose that a pattern is defined as a tree T with nodes 0 1{ , , ..., }nt t t  and each node 
it is attached with a set of attributes 0 1{ , , ..., }ma a a , which represent the local charac-
teristics of ti .  In our task, the set of the attributes include Type attributes, Ortho-
graphic attributes and Relation Role attributes, as shown in Table 2.  Figure 2 shows 
an example of the pattern tree T_ac#target. 
The core idea of the tree kernel ( , )1 2K T T  is that the similarity between two trees 
T1 and T2 is the sum of the similarity between their subtrees.  It can be calculated by 
dynamic programming and can capture the long-range relations between two nodes.  
The kernel we use is similar to [17] except that we define a task-specific matching 
function and similarity function, which are two primitive functions to calculate the 
similarity between two nodes in terms of their attributes.  
Matching function 
1 if . .  and . .   
( , )
0 otherwise                                           
i j i j
i j
t type t type t role t role
m t t
= =
=
???  
 Exploring Syntactic Relation Patterns for Question Answering 513 
Similarity function 
0{ ,..., }
( , ) ( . , . )i j i j
ma a a
s t t f t a t a
?
= ?  
where, ( . , . )i jf t a t a  is a compatibility function between two feature values 
. .
( . , . )
1   if 
0   otherwise
i j
i j
t a t a
f t a t a =
=???  
Table 2. Attributes of the nodes 
Attributes Examples 
POS tag CD, NNP, NN? Type 
syntactic tag NP, VP, ? 
Is Digit? DIG, DIGALL 
Is Capitalized? CAP, CAPALL 
Orthographic  
length of phrase LNG1, LNG2#3, LNGgt3 
Role1 Is answer candidate? true, false 
Role2 Is question key words? true, false 
 
Fig. 2. An example of the pattern tree T_ac#target 
5   ME-Based Answer Extraction 
In addition to the syntactic relation patterns, many other evidences, such as named 
entity tags, may help to detect the proper answers.  Therefore, we use maximum en-
tropy to integrate the syntactic relation patterns and the common features. 
5.1   Maximum Entropy Model 
[1] gave a good description of the core idea of maximum entropy model.  In our task, 
we use the maximum entropy model to rank the answer candidates for a question, 
T_ac#target 
Q1897: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort Worth Inter-
national Airport was 81 degrees. 
t4 t3 t2 
T: BNP 
O: null  
R1: true 
R2: false 
t1 
Dallas-Fort 
T: NNP 
O: CAPALL  
R1: false 
R2: false 
International 
T: JJ 
O: CAPALL  
R1: false 
R2: false 
Airport 
T: NNP 
O: CAPALL  
R1: false 
R2: true 
t0 
Worth 
T: NNP 
O: CAPALL 
R1: false 
R2: false 
 
514 D. Shen, G.-J.M. Kruijff, and D. Klakow 
which is similar to [12].  Given a question q and a set of possible answer candi-
dates 1 2{ , ... }nac ac ac , the model outputs the answer 1 2{ , ... }nac ac ac ac? with the 
maximal probability from the answer candidate set.  We define M feature func-
tions 1 2( ,{ , ... }, ),  m=1,...,Mm nf ac ac ac ac q .  The probability is modeled as  
1 2
1
1 2
1 2
' 1
exp[ ( ,{ , ... }, ))]
( | { , ... }, )
exp[ ( ',{ , ... }, )]
M
m m n
m
n M
m m n
ac m
f ac ac ac ac q
P ac ac ac ac q
f ac ac ac ac q
?
?
=
=
?
=
? ?
 
where, (m=1,...,M)
m
? are the model parameters, which are trained with General-
ized Iterative Scaling [6].  A Gaussian Prior is used to smooth the ME model. 
Table 3. Examples of the common features 
Features Examples Explanation 
NE#DAT_QT_DAT ac is NE (DATE) and qtarget is DATE NE  
NE#PER_QW_WHO ac is NE (PERSON) and qword is WHO 
SSEQ_Q ac is a subsequence of question 
CAP_QT_LOC ac is capitalized and qtarget is LOCATION 
Ortho-
graphic  
LNGlt3_QT_PER the length of ac ? 3 and qtarget is PERSON 
CD_QT_NUM syn. tag of ac is CD and qtarget is NUM Syntactic 
Tag  NNP_QT_PER syn. tag of ac is NNP and qtarget is PERSON 
Triggers TRG_HOW_DIST ac matches the trigger words for HOW questions which 
ask for distance  
5.2   Features 
For the baseline maximum entropy model, we use four types of common features: 
1. Named Entity Features: For certain question target, if the answer candidate is 
tagged as certain type of named entity, one feature fires. 
2. Orthographic Features: They capture the surface format of the answer candi-
dates, such as capitalizations, digits and lengths, etc.  
3. Syntactic Tag Features: For certain question target, if the word in the answer 
candidate belongs to a certain syntactic / POS type, one feature fires. 
4. Triggers: For some how questions, there are always some trigger words which are 
indicative for the answers.  For example, for ?Q2156: How fast does Randy John-
son throw??, the word ?mph? may help to identify the answer ?98-mph? in ?John-
son throws a 98-mph fastball.? 
Table 3 shows some examples of the common features.  All of the features are the 
binary features.  In addition, many other features, such as the answer candidate fre-
quency, can be extracted based on the IR output and are thought as the indicative 
evidences for the answer extraction [10].  However, in this paper, we are to evaluate 
the answer extraction module independently, so we do not incorporate such features 
in the current model. 
 Exploring Syntactic Relation Patterns for Question Answering 515 
In order to evaluate the effectiveness of the automatically generated syntactic rela-
tion patterns, we also manually build some heuristic rules to extract the relation fea-
tures from the trees and incorporate them into the baseline model.  The baseline 
model uses 20 rules.  Some examples of the hand-extracted relation features are 
listed as follows, 
z If the ac node is the same of the qtarget node, one feature fires. 
z If the ac node is the sibling of the qtarget node, one feature fires. 
z If the ac node is the child of the qsubject node, one feature fires. 
z ? 
Next, we will discuss the use of the syntactic relation features.  Firstly, for each 
answer candidate, we extract the syntactic relations between it and all mapped ques-
tion key words in the sentence tree.  Then for each extracted relation, we match it in 
the pattern set PSet_target, PSet_head, PSet_subject or PSet_verb.  A tree kernel 
discussed in Section 4 is used to calculate the similarity between two patterns.  Fi-
nally, if the maximal similarity is above a threshold ? , the pattern with the maximal 
similarity is chosen and the corresponding feature fires.  The experiments will evalu-
ate the performance and the coverage of the pattern sets based on different ?  values. 
6   Experiment 
We apply the AE module to the TREC QA task.  Since this paper focuses on the AE 
module alone, we only present those sentences containing the proper answers to the 
AE module based on the assumption that the IR module has got 100% precision.  The 
AE module is to identify the proper answers from the given sentence collection. 
We use the questions of TREC8, 9, 2001 and 2002 for training and the questions of 
TREC2003 for testing.  The following steps are used to generate the data: 
1. Retrieve the relevant documents for each question based on the TREC judgments. 
2. Extract the sentences, which match both the proper answer and at least one ques-
tion key word, from these documents.   
3. Tag the proper answer in the sentences based on the TREC answer patterns. 
In TREC 2003, there are 413 factoid questions in which 51 questions (NIL ques-
tions) are not returned with the proper answers by TREC.  According to our data 
generation process, we cannot provide data for those NIL questions because we can-
not get the sentence collections.  Therefore, the AE module will fail on all of the NIL 
questions and the number of the valid questions should be 362 (413 ? 51).  In the 
experiment, we still test the module on the whole question set (413 questions) to keep 
consistent with the other?s work.  The training set contains 1252 questions.  The per-
formance of our system is evaluated using the mean reciprocal rank (MRR).  Fur-
thermore, we also list the percentages of the correct answers respectively in terms of 
the top 5 answers and the top 1 answer returned.  No post-processes are used to adjust 
the answers in the experiments. 
In order to evaluate the effectiveness of the syntactic relation patterns in the answer 
extraction, we compare the modules based on different feature sets.  The first ME 
module ME1 uses the common features including NE features, Orthographic features, 
516 D. Shen, G.-J.M. Kruijff, and D. Klakow 
Syntactic Tag features and Triggers.  The second ME module ME2 uses the common 
features and some hand-extracted relation features, described in Section 5.2.  The 
third module ME3 uses the common features and the syntactic relation patterns which 
are automatically extracted and partial matched with the methods proposed in Section 
3 and 4.  Table 4 shows the overall performance of the modules.  Both ME2 and ME3 
outperform ME1 by 3.15 MRR and 6.91 MRR respectively.  This may indicate that 
the syntactic relations between the question words and the answers are useful for the 
answer extraction.  Furthermore, ME3 got the higher performance (+3.76 MRR) than 
ME2.  The probable reason may be that the relations extracted by some heuristic rules 
in ME2 are limited in the very local contexts of the nodes and they may not be suffi-
cient.  On the contrary, the pattern extraction methods we proposed can explore the 
larger relation space in the dependency trees. 
Table 4. Overall performance 
 ME1 ME2 ME3 
Top1  44.06 47.70 51.81 
Top5 53.27 55.45 58.85 
MRR 47.75 50.90 54.66 
Table 5. Performances for two pattern matching methods 
PartialMatch  ExactMatch 
( ? =1) ? =0.8 ? =0.6 ? =0.4 ? =0.2 ? =0 
Top1 50.12 51.33 51.81 51.57 50.12 50.12 
Top5 57.87 58.37 58.85 58.60 57.16 57.16 
MRR 53.18 54.18 54.66 54.41 52.97 52.97 
Furthermore, we evaluate the effectiveness of the pattern matching method in Sec-
tion 4.  We compare two pattern matching methods: the exact matching (ExactMatch) 
and the partial matching (PartialMatch) using the tree kernel.  Table 5 shows the 
performances for the two pattern matching methods.  For PartialMatch, we also 
evaluate the effect of the parameter ?  (described in Section 5.2) on the performance.  
In Table 5, the best PartialMatch ( ?  = 0.6) outperforms ExactMatch by 1.48 MRR.  
Since the pattern sets extracted from the training data is not large enough to cover the 
unseen cases, ExactMatch may have too low coverage and suffer with the data sparse-
ness problem when testing, especially for PSet_subject (24.32% coverage using Ex-
actMatch vs. 49.94% coverage using PartialMatch).  In addition, even the model with 
ExactMatch is better than ME2 (common features + hand-extracted relations) by 2.28 
MRR.  It indicates that the relation patterns explored with the method proposed in 
Section 3 are more effective than the relations extracted by the heuristic rules. 
Table 6 shows the size of the pattern sets PSet_target, PSet_head, PSet_subject 
and PSet_verb and their coverage for the test data based on different ?  values.  
PSet_verb gets the low coverage (<5% coverage).  The probable reason is that the 
verbs in the answer sentences are often different from those in the questions, therefore 
only a few question verbs can be matched in the answer sentences.  PSet_head also 
gets the relatively low coverage since the head words are only exacted from how 
questions and there are only 49/413 how questions with head words in the test data.  
 Exploring Syntactic Relation Patterns for Question Answering 517 
Table 6. Size and coverage of the pattern sets 
coverage (*%)  size 
? =1 ? =0.8 ? =0.6 ? =0.4 ? =0.2 ? =0 
PSet_target 45 49.85 53.73 57.01 58.14 58.46 58.46 
PSet_head 42 5.82 6.48 6.69 6.80 6.80 6.80 
PSet_subject 123 24.32 44.82 49.94 51.29 51.84 51.84 
PSet_verb 125 2.21 3.49 3.58 3.58 3.58 3.58 
We further evaluate the contributions of different types of patterns.  We respec-
tively combine the pattern features in different pattern set and the common features.  
Some findings can be concluded from Table 7: All of the patterns have the positive 
effects based on the common features, which indicates that all of the four types of the 
relations are helpful for answer extraction.  Furthermore, P_target (+4.21 MRR) and 
P_subject (+2.47 MRR) are more beneficial than P_head (+1.25 MRR) and P_verb 
(+0.19 MRR).  This may be explained that the target and subject patterns may have 
the effect on the more test data than the head and verb patterns since PSet_target and 
PSet_subject have the higher coverage for the test data than PSet_head and 
PSet_verb, as shown in Table 6.  
Table 7. Performance on feature combination 
Combination of features MRR 
common features 47.75 
common features + P_target 51.96 
common features + P_head 49.00 
common features + P_subject 50.22 
common features + P_verb 47.94 
7   Conclusion 
In this paper, we study the syntactic relation patterns for question answering.  We 
extract the various syntactic relations between the answers and different types of 
question words, including target words, head words, subject words and verbs and 
score the extracted relations based on support and confidence measures.  We further 
propose a QA-specific tree kernel to partially match the relation patterns from the 
unseen data to the pattern sets.  Lastly, we incorporate the patterns and some com-
mon features into a Maximum Entropy Model to rank the answer candidates.  The 
experiment shows that the syntactic relation patterns improve the performance by 
6.91 MRR based on the common features. Moreover, the contributions of the pat-
tern matching methods are evaluated.  The results show that the tree kernel-based 
partial matching outperforms the exact matching by 1.48 MRR.  In the future, we 
are to further explore the syntactic relations using the web data rather than the  
training data. 
518 D. Shen, G.-J.M. Kruijff, and D. Klakow 
References 
1. Berger, A., Della Pietra, S., Della Pietra, V.: A maximum entropy approach to natural lan-
guage processing. Computational Linguistics (1996), vol. 22, no. 1, pp. 39-71 
2. Collins, M.: A New Statistical Parser Based on Bigram Lexical Dependencies. In: Pro-
ceedings of ACL-96 (1996) 184-191 
3. Collins, M.: New Ranking Algorithms for Parsing and Tagging: Kernel over Discrete 
Structures, and the Voted Perceptron. In: Proceeings of ACL-2002 (2002). 
4. Collins, M., Duffy, N.: Convolution Kernels for Natural Language. Advances in Neural 
Information Processing Systems 14, Cambridge, MA.  MIT Press (2002) 
5. Culotta, A., Sorensen, J.: Dependency Tree Kernels for Relation Extraction. In: Proceed-
ings of ACL-2004 (2004) 
6. Darroch, J., Ratcliff, D.: Generalized iterative scaling for log-linear models. The annuals 
of Mathematical Statistics (1972), vol. 43, pp. 1470-1480 
7. Echihabi, A., Hermjakob, U., Hovy, E., Marcu, D., Melz, E., Ravichandran, D.: Multiple-
Engine Question Answering in TextMap. In: Proceedings of the TREC-2003 Conference, 
NIST (2003) 
8. Fellbaum, C.: WordNet - An Electronic Lexical Database. MIT Press, Cambridge, MA 
(1998) 
9. Harabagiu, S., Moldovan, D., Clark, C., Bowden, M., Williams, J., Bensley, J.: Answer 
Mining by Combining Extraction Techniques with Abductive Reasoning. In: Proceedings 
of the TREC-2003 Conference, NIST (2003) 
10. Ittycheriah, A., Roukos, S.: IBM's Statistical Question Answering System - TREC 11. In: 
Proceedings of the TREC-2002 Conference, NIST (2002) 
11. Levenshtein, V. I.: Binary Codes Capable of Correcting Deletions, Insertions and Rever-
sals. Doklady Akademii Nauk SSSR 163(4) (1965) 845-848 
12. Ravichandran, D., Hovy, E., Och, F. J.: Statistical QA - Classifier vs. Re-ranker: What's 
the difference? In: Proceedings of Workshop on Multilingual Summarization and Question 
Answering, ACL (2003) 
13. Ravichandran, D., Hovy, E.: Learning Surface Text Patterns for a Question Answering 
System. In: Proceedings of ACL-2002 (2002) 41-47 
14. Soubbotin, M. M., Soubbotin, S. M.: Patterns of Potential Answer Expressions as Clues to 
the Right Answer. In: Proceedings of the TREC-10 Conference, NIST (2001) 
15. Xu, J., Licuanan, A., May, J., Miller, S., Weischedel, R.: TREC 2002 QA at BBN: Answer 
Selection and Confidence Estimation. In: Proceedings of the TREC-2002 Conference, 
NIST (2002) 
16. Vapnik, V.: Statistical Learning Theory, John Wiley, NY, (1998) 732. 
17. Zelenko, D., Aone, C., Richardella, A.: Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research (2003) 1083-1106. 
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 65?72,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Studying Feature Generation from Various Data Representations for 
Answer Extraction 
 
Dan Shen?? Geert-Jan M. Kruijff? Dietrich Klakow? 
? Department of Computational Linguistics 
Saarland University 
Building 17,Postfach 15 11 50 
66041 Saarbruecken, Germany 
? Lehrstuhl Sprach Signal Verarbeitung 
Saarland University 
Building 17, Postfach 15 11 50 
66041 Saarbruecken, Germany 
{dshen,gj}@coli.uni-sb.de 
{dietrich.klakow}@lsv.uni-saarland.de 
 
 
Abstract 
In this paper, we study how to generate 
features from various data representations, 
such as surface texts and parse trees, for 
answer extraction.  Besides the features 
generated from the surface texts, we 
mainly discuss the feature generation in 
the parse trees.  We propose and compare 
three methods, including feature vector, 
string kernel and tree kernel, to represent 
the syntactic features in Support Vector 
Machines.  The experiment on the TREC 
question answering task shows that the 
features generated from the more struc-
tured data representations significantly 
improve the performance based on the 
features generated from the surface texts.  
Furthermore, the contribution of the indi-
vidual feature will be discussed in detail. 
1 Introduction 
Open domain question answering (QA), as defined 
by the TREC competitions (Voorhees, 2003), 
represents an advanced application of natural lan-
guage processing (NLP).  It aims to find exact an-
swers to open-domain natural language questions 
in a large document collection.  For example: 
Q2131: Who is the mayor of San Francisco? 
Answer: Willie Brown 
A typical QA system usually consists of three 
basic modules: 1. Question Processing (QP) Mod-
ule, which finds some useful information from the 
questions, such as expected answer type and key 
words.  2. Information Retrieval (IR) Module, 
which searches a document collection to retrieve a 
set of relevant sentences using the question key 
words.  3. Answer Extraction (AE) Module, which 
analyzes the relevant sentences using the informa-
tion provided by the QP module and identify the 
answer phrase. 
In recent years, QA systems trend to be more 
and more complex, since many other NLP tech-
niques, such as named entity recognition, parsing, 
semantic analysis, reasoning, and external re-
sources, such as WordNet, web, databases, are in-
corporated.  The various techniques and resources 
may provide the indicative evidences to find the 
correct answers.  These evidences are further com-
bined by using a pipeline structure, a scoring func-
tion or a machine learning method. 
In the machine learning framework, it is critical 
but not trivial to generate the features from the 
various resources which may be represented as 
surface texts, syntactic structures and logic forms, 
etc.  The complexity of feature generation strongly 
depends on the complexity of data representation.  
Many previous QA systems (Echihabi et al, 2003; 
Ravichandran, et al, 2003; Ittycheriah and Roukos, 
2002; Ittycheriah, 2001; Xu et al, 2002) have well 
studied the features in the surface texts.  In this 
paper, we will use the answer extraction module of 
QA as a case study to further explore how to gen-
erate the features for the more complex sentence 
representations, such as parse tree.  Since parsing 
gives the deeper understanding of the sentence, the 
features generated from the parse tree are expected 
to improve the performance based on the features 
generated from the surface text.  The answer ex-
65
traction module is built using Support Vector Ma-
chines (SVM).  We propose three methods to rep-
resent the features in the parse tree: 1. features are 
designed by domain experts, extracted from the 
parse tree and represented as a feature vector; 2. 
the parse tree is transformed to a node sequence 
and a string kernel is employed; 3. the parse tree is 
retained as the original representation and a tree 
kernel is employed. 
Although many textual features have been used 
in the others? AE modules, it is not clear that how 
much contribution the individual feature makes.  In 
this paper, we will discuss the effectiveness of 
each individual textual feature in detail.  We fur-
ther evaluate the effectiveness of the syntactic fea-
tures we proposed.  Our experiments using TREC 
questions show that the syntactic features improve 
the performance by 7.57 MRR based on the textual 
features.  It indicates that the new features based 
on a deeper language understanding are necessary 
to push further the machine learning-based QA 
technology.  Furthermore, the three representations 
of the syntactic features are compared.  We find 
that keeping the original data representation by 
using the data-specific kernel function in SVM 
may capture the more comprehensive evidences 
than the predefined features.  Although the features 
we generated are specific to the answer extraction 
task, the comparison between the different feature 
representations may be helpful to explore the syn-
tactic features for the other NLP applications. 
2 Related Work 
In the machine learning framework, it is crucial to 
capture the useful evidences for the task and inte-
grate them effectively in the model.  Many re-
searchers have explored the rich textual features 
for the answer extraction. 
IBM (Ittycheriah and Roukos, 2002; Ittycheriah, 
2001) used a Maximum Entropy model to integrate 
the rich features, including query expansion fea-
tures, focus matching features, answer candidate 
co-occurrence features, certain word frequency 
features, named entity features, dependency rela-
tion features, linguistic motivated features and sur-
face patterns.  ISI?s (Echihabi et al 2003; Echihabi 
and Marcu, 2003) statistical-based AE module im-
plemented a noisy-channel model to explain how a 
given sentence tagged with an answer can be re-
written into a question through a sequence of sto-
chastic operations.  (Ravichandran et al, 2003) 
compared two maximum entropy-based QA sys-
tems, which view the AE as a classification prob-
lem and a re-ranking problem respectively, based 
on the word frequency features, expected answer 
class features, question word absent features and 
word match features.  BBN (Xu et al 2002) used a 
HMM-based IR system to score the answer candi-
dates based on the answer contexts.  They further 
re-ranked the scored answer candidates using the 
constraint features, such as whether a numerical 
answer quantifies the correct noun, whether the 
answer is of the correct location sub-type and 
whether the answer satisfies the verb arguments of 
the questions.  (Suzuki et al 2002) explored the 
answer extraction using SVM. 
However, in the previous statistical-based AE 
modules, most of the features were extracted from 
the surface texts which are mainly based on the 
key words/phrases matching and the key word fre-
quency statistics.  These features only capture the 
surface-based information for the proper answers 
and may not provide the deeper understanding of 
the sentences.  In addition, the contribution of the 
individual feature has not been evaluated by them.  
As for the features extracted from the structured 
texts, such as parse trees, only a few works ex-
plored some predefined syntactic relation features 
by partial matching.  In this paper, we will explore 
the syntactic features in the parse trees and com-
pare the different feature representations in SVM.  
Moreover, the contributions of the different fea-
tures will be discussed in detail. 
3 Answer Extraction 
Given a question Q and a set of relevant sentences 
SentSet which is returned by the IR module, we 
consider all of the base noun phrases and the words 
in the base noun phrases as answer candidates aci.  
For example, for the question ?Q1956: What coun-
try is the largest in square miles??, we extract the 
answer candidates { Russia, largest country, larg-
est, country, world, Canada, No.2.} in the sentence 
?I recently read that Russia is the largest country 
in the world, with Canada No. 2.?  The goal of the 
AE module is to choose the most probable answer 
from a set of answer candidates 1 2{ , ,... }mac ac ac  
for the question Q. 
We regard the answer extraction as a classifica-
tion problem, which classify each question and 
66
answer candidate pair <Q, aci> into the positive 
class (the correct answer) and the negative class 
(the incorrect answer), based on some features.  
The predication for each <Q, aci> is made inde-
pendently by the classifier, then, the ac with the 
most confident positive prediction is chosen as the 
answer for Q.  SVM have shown the excellent per-
formance for the binary classification, therefore, 
we employ it to classify the answer candidates.   
Answer extraction is not a trivial task, since it 
involves several components each of which is 
fairly sophisticated, including named entity recog-
nition, syntactic / semantic parsing, question analy-
sis, etc.  These components may provide some 
indicative evidences for the proper answers.  Be-
fore generating the features, we process the sen-
tences as follows: 
1. tag the answer sentences with named entities. 
2. parse the question and the answer sentences us-
ing the Collins? parser (Collin, 1996). 
3. extract the key words from the questions, such 
as the target words, query words and verbs. 
In the following sections, we will briefly intro-
duce the machine learning algorithm.  Then, we 
will discuss the features in detail, including the 
motivations and representations of the features. 
4 Support Vector Machines  
Support Vector Machines (SVM) (Vapnik, 1995) 
have strong theoretical motivation in statistical 
learning theory and achieve excellent generaliza-
tion performance in many language processing 
applications, such as text classification (Joachims, 
1998). 
SVM constructs a binary classifier that predict 
whether an instance x ( n?w R ) is positive 
( ( ) 1f =x ) or negative ( ( ) 1f = ?x ), where, an 
instance may be represented as a feature vector or 
a structure like sequence of characters or tree.  In 
the simplest case (linearly separable instances), the 
decision f( ) sgn( b )? +x = w x is made based 
on a separating hyperplane 0b? + =w x  ( n?w R , 
b?R ).  All instances lying on one side of the hy-
perplane are classified to a positive class, while 
others are classified to a negative class. 
Given a set of labeled training instances 
( ) ( ) ( ){ }1 1 2 2, , , ,..., ,m mD y y y= x x x , where ni ?x R  
and { }1, 1iy = ? , SVM is to find the optimal hy-
perplane that separates the positive and negative 
training instances with a maximal margin.  The 
margin is defined as the distance from the separat-
ing hyperplane to the closest positive (negative) 
training instances.  SVM is trained by solving a 
dual quadratic programming problem. 
Practically, the instances are non-linearly sepa-
rable.  For this case, we need project the instances 
in the original space Rn to a higher dimensional 
space RN based on the kernel function 
1 2 1 2( , ) ( ), ( )K =<? ? >x x x x ,where, ( ): n N? ?x R R  is 
a project function of the instance.  By this means, a 
linear separation will be made in the new space.  
Corresponding to the original space Rn, a non-
linear separating surface is found.  The kernel 
function has to be defined based on the Mercer?s 
condition.  Generally, the following kernel func-
tions are widely used. 
Polynomial kernel: ( , ) ( 1) pi j i jk = ? +x x x x  
Gaussian RBF kernel:  
2 22( , ) i j-i jk e
??= x xx x  
5 Textual Features 
Since the features extracted from the surface texts 
have been well explored by many QA systems 
(Echihabi et al, 2003; Ravichandran, et al, 2003; 
Ittycheriah and Roukos, 2002; Ittycheriah, 2001; 
Xu et al, 2002), we will not focus on the textual 
feature generation in this paper.  Only four types of 
the basic features are used: 
1. Syntactic Tag Features: the features capture 
the syntactic/POS information of the words in 
the answer candidates.  For the certain ques-
tion, such as ?Q1903: How many time zones 
are there in the world??, if the answer candi-
date consists of the words with the syntactic 
tags ?CD NN?, it is more likely to be the 
proper answer. 
2. Orthographic Features: the features capture 
the surface format of the answer candidates, 
such as capitalization, digits and lengths, etc.  
These features are motivated by the observa-
tions, such as, the length  of the answers are 
often less than 3 words for the factoid ques-
tions; the answers may not be the subse-
quences of the questions; the answers often 
contain digits for the certain questions. 
3. Named Entity Features: the features capture 
the named entity information of the answer 
67
candidates.  They are very effective for the 
who, when and where questions, such as, For 
?Q1950: Who created the literary character 
Phineas Fogg??, the answer ?Jules Verne? is 
tagged as a PERSON name in the sentences 
?Jules Verne 's Phileas Fogg made literary 
history when he traveled around the world in 
80 days in 1873.?.  For the certain question tar-
get, if the answer candidate is tagged as the 
certain type of named entity, one feature fires. 
4. Triggers: some trigger words are collected for 
the certain questions.  For examples, for 
?Q2156: How fast does Randy Johnson 
throw??, the trigger word ?mph? for the ques-
tion words ?how fast? may help to identify the 
answer ?98-mph? in ?Johnson throws a 98-
mph fastball?. 
6 Syntactic Features 
In this section, we will discuss the feature genera-
tion in the parse trees.  Since parsing outputs the 
highly structured data representation of the sen-
tence, the features generated from the parse trees 
may provide the more linguistic-motivated expla-
nation for the proper answers.  However, it is not 
trivial to find the informative evidences from a 
parse tree. 
The motivation of the syntactic features in our 
task is that the proper answers often have the cer-
tain syntactic relations with the question key words.  
Table 1 shows some examples of the typical syn-
tactic relations between the proper answers (a) and 
the question target words (qtarget).  Furthermore, 
the syntactic relations between the answers and the 
different types of question key words vary a lot.  
Therefore, we capture the relation features for the 
different types of question words respectively.  The 
question words are divided into four types: 
z Target word, which indicates the expected an-
swer type, such as ?city? in ?Q: What city is 
Disneyland in??. 
z Head word, which is extracted from how ques-
tions and indicates the expected answer head, 
such as ?dog? in ?Q210: How many dogs 
pull ??? 
z Subject words, which are the base noun phrases 
of the question except the target word and the 
head word. 
z Verb, which is the main verb of the question. 
To our knowledge, the syntactic relation fea-
tures between the answers and the question key 
words haven?t been explored in the previous ma-
chine learning-based QA systems.  Next, we will 
propose three methods to represent the syntactic 
relation features in SVM. 
6.1 Feature Vector 
It is the commonly used feature representation in 
most of the machine learning algorithms.  We pre-
define a set of syntactic relation features, which is 
an enumeration of some useful evidences of the 
answer candidates (ac) and the question key words 
in the parse trees.  20 syntactic features are manu-
ally designed in the task.  Some examples of the 
features are listed as follows, 
z if the ac node is the same of the qtarget node, 
one feature fires. 
z if the ac node is the sibling of the qtarget node, 
one feature fires. 
z if the ac node the child of the qsubject node, 
one feature fires. 
The limitation of the manually designed features is 
that they only capture the evidences in the local 
context of the answer candidates and the question 
key words.  However, some question words, such 
as subject words, often have the long range syntac-
1. a node is the same as the qtarget node and qtarget is the hypernym of a. 
Q: What city is Disneyland in? 
S: Not bad for a struggling actor who was working at Tokyo Disneyland a few years ago. 
2. a node is the parent of qtarget node. 
Q: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort Worth International Airport was 81 degrees. 
3. a node is the sibling of the qtarget node. 
Q: What book did Rachel Carson write in 1962? 
S: In her 1962 book Silent Spring, Rachel Carson, a marine biologist, chronicled DDT 's poisonous effects, ?. 
Table 1: Examples of the typical relations between answer and question target word.  In Q, the italic word is 
question target word.  In S, the italic word is the question target word which is mapped in the answer sentence; 
the underlined word is the proper answer for the question Q. 
68
Figure 1: An example of the path from the answer 
candidate node to the question subject word node 
tic relations with the answers.  To overcome the 
limitation, we will propose some special kernels 
which may keep the original data representation 
instead of explicitly enumerate the features, to ex-
plore a much larger feature space. 
6.2 String Kernel 
The second method represents the syntactic rela-
tion as a linked node sequence and incorporates a 
string kernel in SVM to handle the sequence. 
We extract a path from the node of the answer 
candidate to the node of the question key word in 
the parse tree.  The path is represented as a node 
sequence linked by symbols indicating upward or 
downward movement through the tree. For exam-
ple, in Figure 1, the path from the answer candi-
date node ?211,456 miles? to the question subject 
word node ?the moon? is 
? NPB ADVP VP S NPB? ? ? ? ?, where ? ? ? and 
? ? ? indicate upward movement and downward 
movement in the parse tree.  By this means, we 
represent the object from the original parse tree to 
the node sequence.  Each character of the sequence 
is a syntactic/POS tag of the node.  Next, a string 
kernel will be adapted to our task to calculate the 
similarity between two node sequences. 
 
 
 
 
 
(Haussler, 1999) first described a convolution 
kernel over the strings.  (Lodhi et al, 2000) applied 
the string kernel to the text classification.  (Leslie 
et al, 2002) further proposed a spectrum kernel, 
which is simpler and more efficient than the previ-
ous string kernels, for protein classification prob-
lem.  In their tasks, the string kernels achieved the 
better performance compared with the human-
defined features. 
The string kernel is to calculate the similarity 
between two strings.  It is based on the observation 
that the more common substrings the strings have, 
the more similar they are.  The string kernel we 
used is similar to (Leslie et al, 2002).  It is defined 
as the sum of the weighted common substrings.  
The substring is weighted by an exponentially de-
caying factor ? (set 0.5 in the experiment) of its 
length k.  For efficiency, we only consider the sub-
strings which length are less than 3.  Different 
from (Leslie et al, 2002), the characters (syntac-
tic/POS tag) of the string are linked with each 
other.  Therefore, the matching between two sub-
strings will consider the linking information.  Two 
identical substrings will not only have the same 
syntactic tag sequences but also have the same 
linking symbols.  For example, for the node se-
quences NP VP VP S NP? ? ? ?  and NP NP VP NP? ? ? , 
there is a matched substring (k = 2): NP VP? . 
6.3 Tree Kernel 
The third method keeps the original representation 
of the syntactic relation in the parse tree and incor-
porates a tree kernel in SVM. 
Tree kernels are the structure-driven kernels to 
calculate the similarity between two trees.  They 
have been successfully accepted in the NLP appli-
cations.  (Collins and Duffy, 2002) defined a ker-
nel on parse tree and used it to improve parsing.  
(Collins, 2002) extended the approach to POS tag-
ging and named entity recognition.  (Zelenko et al, 
2003; Culotta and Sorensen, 2004) further ex-
plored tree kernels for relation extraction. 
We define an object (a relation tree) as the 
smallest tree which covers one answer candidate 
node and one question key word node.  Suppose 
that a relation tree T has nodes 0 1{ , , ..., }nt t t  and 
each node it is attached with a set of attrib-
utes 0 1{ , , ..., }ma a a , which represents the local char-
acteristics of ti .  In our task, the set of the 
attributes includes Type attributes, Orthographic 
attributes and Relation Role attributes, as shown in 
Table 2.  The core idea of the tree kernel ( , )1 2K T T  
is that the similarity between two trees T1 and T2 is 
PUNC
. away 221,456 miles 
S 
PP NPB VP 
VBZ ADVP
NPB RB 
the moon 
is 
Q1980: How far is the moon from Earth in miles? 
S: At its perigee, the closest approach to Earth , the 
moon is 221,456 miles away. 
?? 
69
T1_ac#target 
T2_ac#target 
Q1897: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort 
Worth International Airport was 81 degrees. 
t4t3 t2
T: BNP 
O: null  
R1: true 
R2: false 
t1
Dallas-Fort
T: NNP 
O: CAPALL 
R1: false 
R2: false
International 
T: JJ 
O: CAPALL  
R1: false 
R2: false 
Airport 
T: NNP 
O: CAPALL 
R1: false 
R2: true
Q35: What is the name of the highest mountain in Africa? 
S: Mount Kilimanjaro, at 19,342 feet, is Africa's highest moun-
tain, and is 5,000 feet higher than ?. 
Mount 
T: NNP 
O: CAPALL 
R1: false 
R2: true
Kilimanjaro
T: NNP 
O: CAPALL 
R1: false 
R2: false 
T: BNP 
O: null  
R1: true 
R2: false 
t0 
w0 
w1 w2 
Worth 
T: NNP 
O: CAPALL 
R1: false 
R2: false
the sum of the similarity between their subtrees.  It 
is calculated by dynamic programming and cap-
tures the long-range syntactic relations between 
two nodes.  The kernel we use is similar to (Ze-
lenko et al, 2003) except that we define a task-
specific matching function and similarity function, 
which are two primitive functions to calculate the 
similarity between two nodes in terms of their at-
tributes. 
Matching function 
1 if . .  and . .   
( , )
0 otherwise                                           
i j i j
i j
t type t type t role t role
m t t
= == ???  
Similarity function 
0{ ,..., }
( , ) ( . , . )i j i j
ma a a
s t t f t a t a
?
= ?  
where, ( . , . )i jf t a t a  is a compatibility function be-
tween two feature values 
. .
( . , . )
1   if 
0   otherwise
i j
i j
t a t a
f t a t a =
=???  
Figure 2 shows two examples of the relation tree 
T1_ac#targetword and T2_ac#targetword.  The 
kernel we used matches the following pairs of the 
nodes <t0, w0>, <t1, w2>, <t2, w2> and <t4, w1>. 
 
Attributes Examples 
POS tag CD, NNP, NN?Type 
syntactic tag NP, VP, ? 
Is Digit? DIG, DIGALL 
Is Capitalized? CAP, CAPALL 
Ortho-
graphic  
length of phrase LNG1, LNG2#3, 
LNGgt3 
Role1 Is answer candidate? true, false 
Role2 Is question key words? true, false 
Table 2: Attributes of the nodes 
7 Experiments 
We apply the AE module to the TREC QA task.  
To evaluate the features in the AE module inde-
pendently, we suppose that the IR module has got 
100% precision and only passes those sentences 
containing the proper answers to the AE module.  
The AE module is to identify the proper answers 
from the given sentence collection. 
We use the questions of TREC8, 9, 2001 and 
2002 for training and the questions of TREC2003 
for testing.  The following steps are used to gener-
ate the data: 
1. Retrieve the relevant documents for each ques-
tion based on the TREC judgments. 
2. Extract the sentences, which match both the 
proper answer and at least one question key word, 
from these documents. 
3. Tag the proper answer in the sentences based on 
the TREC answer patterns 
 
Figure 2: Two objects representing the relations be-
tween answer candidates and target words. 
 
In TREC 2003, there are 413 factoid questions 
in which 51 questions (NIL questions) are not re-
turned with the proper answers by TREC.  Accord-
ing to our data generation process, we cannot 
provide data for those NIL questions because we 
cannot get the sentence collections.  Therefore, the 
AE module will fail on all of the NIL questions 
and the number of the valid questions should be 
362 (413 ? 51).  In the experiment, we still test the 
module on the whole question set (413 questions) 
to keep consistent with the other?s work.  The 
training set contains 1252 questions.  The perform-
ance of our system is evaluated using the mean 
reciprocal rank (MRR).  Furthermore, we also list 
the percentages of the correct answers respectively 
70
in terms of the top 5 answers and the top 1 answer 
returned.  We employ the SVMLight (Joachims, 
1999) to incorporate the features and classify the 
answer candidates.  No post-processes are used to 
adjust the answers in the experiments. 
Firstly, we evaluate the effectiveness of the tex-
tual features, described in Section 5.  We incorpo-
rate them into SVM using the three kernel 
functions: linear kernel, polynomial kernel and 
RBF kernel, which are introduced in Section 4.  
Table 3 shows the performance for the different 
kernels.  The RBF kernel (46.24 MRR) signifi-
cantly outperforms the linear kernel (33.72 MRR) 
and the polynomial kernel (40.45 MRR).  There-
fore, we will use the RBF kernel in the rest ex-
periments. 
 Top1 Top5 MRR 
linear 31.28 37.91 33.72 
polynomial 37.91 44.55 40.45 
RBF 42.67 51.58 46.24 
Table 3: Performance for kernels 
 
In order to evaluate the contribution of the indi-
vidual feature, we test out module using different 
feature combinations, as shown in Table 4.  Sev-
eral findings are concluded: 
1. With only the syntactic tag features Fsyn., the 
module achieves a basic level MRR of 31.38.  The 
questions ?Q1903: How many time zones are there 
in the world?? is correctly answered from the sen-
tence ?The world is divided into 24 time zones.?.  
2. The orthographic features Forth. show the posi-
tive effect with 7.12 MRR improvement based on 
Fsyn..  They help to find the proper answer ?Grover 
Cleveland? for the question ?Q2049: What presi-
dent served 2 nonconsecutive terms?? from the 
sentence ?Grover Cleveland is the forgotten two-
term American president.?, while Fsyn. wrongly 
identify ?president? as the answer. 
3. The named entity features Fne are also benefi-
cial as they make the 4.46 MRR increase based on 
Fsyn.+Forth.   For the question ?Q2076: What com-
pany owns the soft drink brand "Gatorade"??, Fne 
find the proper answer ?Quaker Oats? in the sen-
tence ?Marineau , 53 , had distinguished himself 
by turning the sports drink Gatorade into a mass 
consumer brand while an executive at Quaker Oats 
During his 18-month??, while Fsyn.+Forth. return 
the wrong answer ?Marineau?. 
4. The trigger features Ftrg lead to an improve-
ment of 3.28 MRR based on Fsyn.+Forth+Fne.  They 
correctly answer more questions.  For the question 
?Q1937: How fast can a nuclear submarine 
travel??, Ftrg return the proper answer ?25 knots? 
from the sentence ?The submarine , 360 feet 
( 109.8 meters ) long , has 129 crew members and 
travels at 25 knots.?, but the previous features fail 
on it. 
Fsyn Forth. Fne Ftrg Top1 Top5 MRR
?    26.50 38.92 31.38
? ?   34.69 43.61 38.50
? ? ?  39.85 47.82 42.96
? ? ? ? 42.67 51.58 46.24
Table 4: Performance for feature combinations 
 
Next, we will evaluate the effectiveness of the syn-
tactic features, described in Section 6.  Table 5 
compares the three feature representation methods, 
FeatureVector, StringKernel and TreeKernel.   
z FeatureVector (Section 6.1).  We predefine 
some features in the syntactic tree and present 
them as a feature vector.  The syntactic fea-
tures are added with the textual features and 
the RBF kernel is used to cope with them. 
z StringKernel (Section 6.2).  No features are 
predefined.  We transform the syntactic rela-
tions between answer candidates and question 
key words to node sequences and a string ker-
nel is proposed to cope with the sequences.  
Then we add the string kernel for the syntactic 
relations and the RBF kernel for the textual 
features. 
z TreeKernel (Section 6.3).  No features are 
predefined.  We keep the original representa-
tions of the syntactic relations and propose a 
tree kernel to cope with the relation trees.  
Then we add the tree kernel and the RBF ker-
nel. 
 Top1 Top2 MRR
Fsyn.+Forth.+Fne+Ftrg 42.67 51.58 46.24
FeatureVector 46.19 53.69 49.28
StringKernel 48.99 55.83 52.29
TreeKernel 50.41 57.46 53.81
Table 5: Performance for syntactic feature repre-
sentations 
 
Table 5 shows the performances of FeatureVec-
tor, StringKernel and TreeKernel.  All of them im-
prove the performance based on the textual 
features (Fsyn.+Forth.+Fne+Ftrg) by 3.04 MRR, 6.05 
MRR and 7.57 MRR respectively.  The probable 
reason may be that the features generated from the 
structured data representation may capture the 
71
more linguistic-motivated evidences for the proper 
answers.  For example, the syntactic features help 
to find the answer ?nitrogen? for the question 
?Q2139: What gas is 78 percent of the earth 's at-
mosphere?? in the sentence ?One thing they have-
n't found in the moon's atmosphere so far is 
nitrogen, the gas that makes up more than three-
quarters of the Earth's atmosphere.?, while the 
textual features fail on it.  Furthermore, the String-
Kernel (+3.01MRR) and TreeKernel (+4.53MRR) 
achieve the higher performance than FeatureVec-
tor, which may be explained that keeping the 
original data representations by incorporating the 
data-specific kernels in SVM may capture the 
more comprehensive evidences than the predefined 
features.  Moreover, TreeKernel slightly outper-
forms StringKernel by 1.52 MRR.  The reason may 
be that when we transform the representation of the 
syntactic relation from the tree to the node se-
quence, some information may be lost, such as the 
sibling node of the answer candidates.  Sometimes 
the information is useful to find the proper answers. 
8 Conclusion  
In this paper, we study the feature generation based 
on the various data representations, such as surface 
text and parse tree, for the answer extraction.  We 
generate the syntactic tag features, orthographic 
features, named entity features and trigger features 
from the surface texts.  We further explore the fea-
ture generation from the parse trees which provide 
the more linguistic-motivated evidences for the 
task.  We propose three methods, including feature 
vector, string kernel and tree kernel, to represent 
the syntactic features in Support Vector Machines.  
The experiment on the TREC question answering 
task shows that the syntactic features significantly 
improve the performance by 7.57MRR based on 
the textual features.  Furthermore, keeping the 
original data representation using a data-specific 
kernel achieves the better performance than the 
explicitly enumerated features in SVM. 
References  
M. Collins.  1996.  A New Statistical Parser Based on 
Bigram Lexical Dependencies.  In Proceedings of 
ACL-96, pages 184-191. 
M. Collins. 2002.  New Ranking Algorithms for Parsing 
and Tagging: Kernel over Discrete Structures, and 
the Voted Perceptron.  In Proceedings of ACL-2002. 
M. Collins and N. Duffy.  2002.  Convolution Kernels 
for Natural Language.  Advances in Neural Informa-
tion Processing Systems 14, Cambridge, MA.  MIT 
Press. 
A. Culotta and J. Sorensen.  2004.  Dependency Tree 
Kernels for Relation Extraction.  In Proceedings of 
ACL-2004. 
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. 
Melz, D. Ravichandran.  2003.  Multiple-Engine 
Question Answering in TextMap.  In Proceedings of 
the TREC-2003 Conference, NIST. 
A. Echihabi, D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proceedings of the 
ACL-2003. 
D. Haussler. 1999.  Convolution Kernels on Discrete 
Structures.  Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
A. Ittycheriah and S. Roukos.  2002.  IBM?s Statistical 
Question Answering System ? TREC 11.  In Pro-
ceedings of the TREC-2002 Conference, NIST. 
A. Ittycheriah. 2001. Trainable Question Answering 
System.  Ph.D. Dissertation, Rutgers, The State Uni-
versity of New Jersey, New Brunswick, NJ. 
T. Joachims.  1999.  Making large-Scale SVM Learn-
ing Practical.  Advances in Kernel Methods - Sup-
port Vector Learning, MIT-Press, 1999. 
T. Joachims.  1998.  Text Categorization with Support 
Vector Machines: Learning with Many Relevant Fea-
tures.  In Proceedings of the European Conference on 
Machine Learning, Springer. 
C. Leslie, E. Eskin and W. S. Noble.  2002.  The spec-
trum kernel: A string kernel for SVM protein classi-
fication.  Proceedings of the Pacific Biocomputing 
Symposium. 
H. Lodhi, J. S. Taylor, N. Cristianini and C. J. C. H. 
Watkins.  2000.  Text Classification using String 
Kernels.  In NIPS, pages 563-569. 
D. Ravichandran, E. Hovy and F. J. Och.  2003.  Statis-
tical QA ? Classifier vs. Re-ranker: What?s the dif-
ference?  In Proceedings of Workshop on Mulingual 
Summarization and Question Answering, ACL 2003. 
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. SVM Answer 
Selection for Open-domain Question Answering. In 
Proc. of COLING 2002, pages 974?980. 
V. N. Vapnik.  1998.  Statistical Learning Theory.  
Springer. 
E.M. Voorhees.  2003. Overview of the TREC 2003 
Question Answering Track.  In Proceedings of the 
TREC-2003 Conference, NIST. 
J. Xu, A. Licuanan, J. May, S. Miller and R. Weischedel.  
2002.  TREC 2002 QA at BBN: Answer Selection 
and Confidence Estimation.  In Proceedings of the 
TREC-2002 Conference, NIST. 
D. Zelenko, C. Aone and A. Richardella.  2003.  Kernel 
Methods for Relation Extraction.  Journal of Ma-
chine Learning Research, pages 1083-1106. 
72
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 889?896,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploring Correlation of Dependency Relation Paths
for Answer Extraction
Dan Shen
Department of Computational Linguistics
Saarland University
Saarbruecken, Germany
dshen@coli.uni-sb.de
Dietrich Klakow
Spoken Language Systems
Saarland University
Saarbruecken, Germany
klakow@lsv.uni-saarland.de
Abstract
In this paper, we explore correlation of
dependency relation paths to rank candi-
date answers in answer extraction. Using
the correlation measure, we compare de-
pendency relations of a candidate answer
and mapped question phrases in sentence
with the corresponding relations in ques-
tion. Different from previous studies, we
propose an approximate phrase mapping
algorithm and incorporate the mapping
score into the correlation measure. The
correlations are further incorporated into
a Maximum Entropy-based ranking model
which estimates path weights from train-
ing. Experimental results show that our
method significantly outperforms state-of-
the-art syntactic relation-based methods
by up to 20% in MRR.
1 Introduction
Answer Extraction is one of basic modules in open
domain Question Answering (QA). It is to further
process relevant sentences extracted with Passage /
Sentence Retrieval and pinpoint exact answers us-
ing more linguistic-motivated analysis. Since QA
turns to find exact answers rather than text snippets
in recent years, answer extraction becomes more
and more crucial.
Typically, answer extraction works in the fol-
lowing steps:
? Recognize expected answer type of a ques-
tion.
? Annotate relevant sentences with various
types of named entities.
? Regard the phrases annotated with the ex-
pected answer type as candidate answers.
? Rank candidate answers.
In the above work flow, answer extraction heav-
ily relies on named entity recognition (NER). On
one hand, NER reduces the number of candidate
answers and eases answer ranking. On the other
hand, the errors from NER directly degrade an-
swer extraction performance. To our knowledge,
most top ranked QA systems in TREC are sup-
ported by effective NER modules which may iden-
tify and classify more than 20 types of named en-
tities (NE), such as abbreviation, music, movie,
etc. However, developing such named entity rec-
ognizer is not trivial. Up to now, we haven?t found
any paper relevant to QA-specific NER develop-
ment. So, it is hard to follow their work. In this pa-
per, we just use a general MUC-based NER, which
makes our results reproducible.
A general MUC-based NER can?t annotate a
large number of NE classes. In this case, all
noun phrases in sentences are regarded as candi-
date answers, which makes candidate answer sets
much larger than those filtered by a well devel-
oped NER. The larger candidate answer sets result
in the more difficult answer extraction. Previous
methods working on surface word level, such as
density-based ranking and pattern matching, may
not perform well. Deeper linguistic analysis has
to be conducted. This paper proposes a statisti-
cal method which exploring correlation of depen-
dency relation paths to rank candidate answers. It
is motivated by the observation that relations be-
tween proper answers and question phrases in can-
didate sentences are always similar to the corre-
sponding relations in question. For example, the
question ?What did Alfred Nobel invent?? and the
889
candidate sentence ?... in the will of Swedish in-
dustrialist Alfred Nobel, who invented dynamite.?
For each question, firstly, dependency relation
paths are defined and extracted from the question
and each of its candidate sentences. Secondly,
the paths from the question and the candidate sen-
tence are paired according to question phrase map-
ping score. Thirdly, correlation between two paths
of each pair is calculated by employing Dynamic
Time Warping algorithm. The input of the cal-
culation is correlations between dependency re-
lations, which are estimated from a set of train-
ing path pairs. Lastly, a Maximum Entropy-based
ranking model is proposed to incorporate the path
correlations and rank candidate answers. Further-
more, sentence supportive measure are presented
according to correlations of relation paths among
question phrases. It is applied to re-rank the can-
didate answers extracted from the different candi-
date sentences. Considering phrases may provide
more accurate information than individual words,
we extract dependency relations on phrase level
instead of word level.
The experiment on TREC questions shows that
our method significantly outperforms a density-
based method by 50% in MRR and three state-
of-the-art syntactic-based methods by up to 20%
in MRR. Furthermore, we classify questions by
judging whether NER is used. We investigate
how these methods perform on the two question
sets. The results indicate that our method achieves
better performance than the other syntactic-based
methods on both question sets. Especially for
more difficult questions, for which NER may not
help, our method improves MRR by up to 31%.
The paper is organized as follows. Section 2
discusses related work and clarifies what is new in
this paper. Section 3 presents relation path corre-
lation in detail. Section 4 and 5 discuss how to in-
corporate the correlations for answer ranking and
re-ranking. Section 6 reports experiment and re-
sults.
2 Related Work
In recent years? TREC Evaluation, most top
ranked QA systems use syntactic information in
answer extraction. Next, we will briefly discuss
the main usages.
(Kaisser and Becker, 2004) match a question
into one of predefined patterns, such as ?When
did Jack Welch retire from GE?? to the pattern
?When+did+NP+Verb+NPorPP?. For each ques-
tion pattern, there is a set of syntactic structures for
potential answer. Candidate answers are ranked
by matching the syntactic structures. This method
worked well on TREC questions. However, it
is costing to manually construct question patterns
and syntactic structures of the patterns.
(Shen et al, 2005) classify question words into
four classes target word, head word, subject word
and verb. For each class, syntactic relation pat-
terns which contain one question word and one
proper answer are automatically extracted and
scored from training sentences. Then, candidate
answers are ranked by partial matching to the syn-
tactic relation patterns using tree kernel. However,
the criterion to classify the question words is not
clear in their paper. Proper answers may have ab-
solutely different relations with different subject
words in sentences. They don?t consider the cor-
responding relations in questions.
(Tanev et al, 2004; Wu et al, 2005) compare
syntactic relations in questions and those in an-
swer sentences. (Tanev et al, 2004) reconstruct
a basic syntactic template tree for a question, in
which one of the nodes denotes expected answer
position. Then, answer candidates for this ques-
tion are ranked by matching sentence syntactic
tree to the question template tree. Furthermore, the
matching is weighted by lexical variations. (Wu et
al., 2005) combine n-gram proximity search and
syntactic relation matching. For syntactic rela-
tion matching, question tree and sentence subtree
around a candidate answer are matched from node
to node.
Although the above systems apply the different
methods to compare relations in question and an-
swer sentences, they follow the same hypothesis
that proper answers are more likely to have same
relations in question and answer sentences. For
example, in question ?Who founded the Black Pan-
thers organization??, where, the question word
?who? has the dependency relations ?subj? with
?found? and ?subj obj nn? with ?Black Panthers
organization?, in sentence ?Hilliard introduced
Bobby Seale, who co-founded the Black Panther
Party here ...?, the proper answer ?Bobby Seale?
has the same relations with most question phrases.
These methods achieve high precision, but poor
recall due to relation variations. One meaning
is often represented as different relation combi-
nations. In the above example, appositive rela-
890
tion frequently appears in answer sentences, such
as ?Black Panther Party co-founder Bobby Seale
is ordered bound and gagged ...? and indicates
proper answer Bobby Seale although it is asked in
different way in the question.
(Cui et al, 2004) propose an approximate de-
pendency relation matching method for both pas-
sage retrieval and answer extraction. The simi-
larity between two relations is measured by their
co-occurrence rather than exact matching. They
state that their method effectively overcomes the
limitation of the previous exact matching meth-
ods. Lastly, they use the sum of similarities of
all path pairs to rank candidate answers, which is
based on the assumption that all paths have equal
weights. However, it might not be true. For ex-
ample, in question ?What book did Rachel Carson
write in 1962??, the phrase ?Rachel Carson? looks
like more important than ?1962? since the former
is question topic and the latter is a constraint for
expected answer. In addition, lexical variations
are not well considered and a weak relation path
alignment algorithm is used in their work.
Based on the previous works, this paper ex-
plores correlation of dependency relation paths be-
tween questions and candidate sentences. Dy-
namic time warping algorithm is adapted to cal-
culate path correlations and approximate phrase
mapping is proposed to cope with phrase varia-
tions. Finally, maximum entropy-based ranking
model is developed to incorporate the correlations
and rank candidate answers.
3 Dependency Relation Path Correlation
In this section, we discuss how the method per-
forms in detail.
3.1 Dependency Relation Path Extraction
We parse questions and candidate sentences with
MiniPar (Lin, 1994), a fast and robust parser for
grammatical dependency relations. Then, we ex-
tract relation paths from dependency trees.
Dependency relation path is defined as a struc-
ture P =< N1, R,N2 > where, N1, N2 are
two phrases and R is a relation sequence R =<
r1, ..., ri > in which ri is one of the predefined de-
pendency relations. Totally, there are 42 relations
defined in MiniPar. A relation sequence R be-
tween two phrases N1, N2 is extracted by travers-
ing from the N1 node to the N2 node in a depen-
dency tree.
Q: What book did Rachel Carson write in 1962?
Paths for Answer Ranking
N1 (EAP)           R                               N2
What               det                             book
What               det obj subj                 Rachel Carson
What               det obj                        write
What               det obj mod pcomp-n    1962
Paths for Answer Re-rankingbook                obj subj                       Rachel Carsonbook                obj                              writebook                obj mod pcomp-n          1962
...
S: Rachel Carson ?s 1962 book " Silent Spring " said 
    dieldrin causes mania.
Paths for Answer Ranking
N1 (CA)              R                              N2Silent Spring      title                           bookSilent Spring      title gen                     Rachel CarsonSilent Spring      title num                    1962
Paths for Answer Re-rankingbook                    gen                         Rachel Carsonbook                    num                        1962         
...
Figure 1: Relation Paths for sample question and
sentence. EAP indicates expected answer position;
CA indicates candidate answer
For each question, we extract relation paths
among noun phrases, main verb and question
word. The question word is further replaced with
?EAP?, which indicates the expected answer po-
sition. For each candidate sentence, we firstly
extract relation paths between answer candidates
and mapped question phrases. These paths will
be used for answer ranking (Section 4). Secondly,
we extract relation paths among mapped question
phrases. These paths will be used for answer re-
ranking (Section 5). Question phrase mapping will
be discussed in Section 3.4. Figure 1 shows some
relation paths extracted for an example question
and candidate sentence.
Next, the relation paths in a question and each
of its candidate sentences are paired according to
their phrase similarity. For any two relation path
Pi and Pj which are extracted from the ques-
tion and the candidate sentence respectively, if
Sim(Ni1, Nj1) > 0 and Sim(Ni2, Nj2) > 0,
Pi and Pj are paired as < Pi, Pj >. The ques-
tion phrase ?EAP? is mapped to candidate answer
phrase in the sentence. The similarity between two
891
Path Pairs for Answer RankingN1 (EAP / CA)     Rq                               Rs               N2Silent Spring      det                              title             bookSilent Spring      det obj subj                  title gen       Rachel CarsonSilent Spring      det obj mod pcomp-n     title num      1962
Path Pairs for Answer Re-rankingN1                     Rq                               Rs               N2book                  obj subj                        gen             Rachel Carsonbook                  obj mod pcomp-n           num            1962
...
Figure 2: Paired Relation Path
phrases will be discussed in Section 3.4. Figure 2
further shows the paired relation paths which are
presented in Figure 1.
3.2 Dependency Relation Path Correlation
Comparing a proper answer and other wrong can-
didate answers in each sentence, we assume that
relation paths between the proper answer and
question phrases in the sentence are more corre-
lated to the corresponding paths in question. So,
for each path pair < P1, P2 >, we measure the
correlation between its two paths P1 and P2.
We derive the correlations between paths by
adapting dynamic time warping (DTW) algorithm
(Rabiner et al, 1978). DTW is to find an optimal
alignment between two sequences which maxi-
mizes the accumulated correlation between two
sequences. A sketch of the adapted algorithm is
as follows.
Let R1 =< r11, ..., r1n >, (n = 1, ..., N)
and R2 =< r21, ..., r2m >, (m = 1, ...,M) de-
note two relation sequences. R1 and R2 consist
of N and M relations respectively. R1(n) =
r1n and R2(m) = r2m. Cor(r1, r2) denotes
the correlation between two individual relations
r1, r2, which is estimated by a statistical model
during training (Section 3.3). Given the corre-
lations Cor(r1n, r2m) for each pair of relations
(r1n, r2m) within R1 and R2, the goal of DTW is
to find a path, m = map(n), which map n onto the
corresponding m such that the accumulated corre-
lation Cor? along the path is maximized.
Cor? = max
map(n)
{ N?
n=1
Cor(R1(n), R2(map(n))
}
A dynamic programming method is used to de-
termine the optimum path map(n). The accumu-
lated correlation CorA to any grid point (n,m)
can be recursively calculated as
CorA(n,m) = Cor(r1n, r2m) + maxq?m CorA(n? 1, q)
Cor? = CorA(N,M)
The overall correlation measure has to be nor-
malized as longer sequences normally give higher
correlation value. So, the correlation between two
sequences R1 and R2 is calculated as
Cor(R1, R2) = Cor?/max(N,M)
Finally, we define the correlation between two
relation paths P1 and P2 as
Cor(P1, P2) = Cor(R1, R2)? Sim(N11, N21)
? Sim(N12, N22)
Where, Sim(N11, N21) and Sim(N12, N22)
are the phrase mapping score when pairing
two paths, which will be described in Section
3.4. If two phrases are absolutely different
Cor(N11, N21) = 0 or Cor(N12, N22) = 0, the
paths may not be paired since Cor(P1, P2) = 0.
3.3 Relation Correlation Estimation
In the above section, we have described how to
measure path correlations. The measure requires
relation correlations Cor(r1, r2) as inputs. We
apply a statistical method to estimate the relation
correlations from a set of training path pairs. The
training data collecting will be described in Sec-
tion 6.1.
For each question and its answer sentences in
training data, we extract relation paths between
?EAP? and other phrases in the question and
paths between proper answer and mapped ques-
tion phrases in the sentences. After pairing the
question paths and the corresponding sentence
paths, correlation of two relations is measured by
their bipartite co-occurrence in all training path
pairs. Mutual information-based measure (Cui et
al., 2004) is employed to calculate the relation cor-
relations.
Cor(rQi , rSj ) = log
??? ?(rQi , rSj )
fQ(rQi )? fS(rSj )
where, rQi and rSj are two relations in question
paths and sentence paths respectively. fQ(rQi ) and
fS(rSj ) are the numbers of occurrences of rQi in
question paths and rSj in sentence paths respec-
tively. ?(rQi , rSj ) is 1 when rQi and rSj co-occur
in a path pair, and 0 otherwise. ? is a factor to
discount the co-occurrence value for long paths. It
is set to the inverse proportion of the sum of path
lengths of the path pair.
892
3.4 Approximate Question Phrase Mapping
Basic noun phrases (BNP) and verbs in questions
are mapped to their candidate sentences. A BNP
is defined as the smallest noun phrase in which
there are no noun phrases embedded. To address
lexical and format variations between phrases, we
propose an approximate phrase mapping strategy.
A BNP is separated into a set of heads
H = {h1, ..., hi} and a set of modifiers M =
{m1, ...mj}. Some heuristic rules are applied to
judge heads and modifiers: 1. If BNP is a named
entity, all words are heads. 2. The last word of
BNP is head. 3. Rest words are modifiers.
The similarity between two BNPs
Sim(BNPq, BNPs) is defined as:
Sim(BNPq, BNPs) = ?Sim(Hq, Hs)
+ (1? ?)Sim(Mq,Ms)
Sim(Hq, Hs) =
?
hi?Hq
?
hj?Hs
Sim(hi,hj)
|Hq
?
Hs|
Sim(Mq,Ms) =
?
mi?Mq
?
mj?Ms
Sim(mi,mj)
|Mq
?
Ms|
Furthermore, the similarity between two heads
Sim(hi, hj) are defined as:
? Sim = 1, if hi = hj after stemming;
? Sim = 1, if hi = hj after format alternation;
? Sim = SemSim(hi, hj)
These items consider morphological, format
and semantic variations respectively. 1. The mor-
phological variations match words after stemming,
such as ?Rhodes scholars? and ?Rhodes scholar-
ships?. 2. The format alternations cope with
special characters, such as ?-? for ?Ice-T? and
?Ice T?, ?&? for ?Abercrombie and Fitch? and
?Abercrombie & Fitch?. 3. The semantic simi-
larity SemSim(hi, hj) is measured using Word-
Net and eXtended WordNet. We use the same
semantic path finding algorithm, relation weights
and semantic similarity measure as (Moldovan and
Novischi, 2002). For efficiency, only hypernym,
hyponym and entailment relations are considered
and search depth is set to 2 in our experiments.
Particularly, the semantic variations are not con-
sidered for NE heads and modifiers. Modifier sim-
ilarity Sim(mi,mj) only consider the morpho-
logical and format variations. Moreover, verb sim-
ilarity measure Sim(v1, v2) is the same as head
similarity measure Sim(hi, hj).
4 Candidate Answer Ranking
According to path correlations of candidate an-
swers, a Maximum Entropy (ME)-based model is
applied to rank candidate answers. Unlike (Cui et
al., 2004), who rank candidate answers with the
sum of the path correlations, ME model may es-
timate the optimal weights of the paths based on
a training data set. (Berger et al, 1996) gave a
good description of ME model. The model we
use is similar to (Shen et al, 2005; Ravichandran
et al, 2003), which regard answer extraction as a
ranking problem instead of a classification prob-
lem. We apply Generalized Iterative Scaling for
model parameter estimation and Gaussian Prior
for smoothing.
If expected answer type is unknown during
question processing or corresponding type of
named entities isn?t recognized in candidate sen-
tences, we regard all basic noun phrases as can-
didate answers. Since a MUC-based NER loses
many types of named entities, we have to handle
larger candidate answer sets. Orthographic fea-
tures, similar to (Shen et al, 2005), are extracted to
capture word format information of candidate an-
swers, such as capitalizations, digits and lengths,
etc. We expect they may help to judge what proper
answers look like since most NER systems work
on these features.
Next, we will discuss how to incorporate path
correlations. Two facts are considered to affect
path weights: question phrase type and path
length. For each question, we divide question
phrases into four types: target, topic, constraint
and verb. Target is a kind of word which indicates
the expected answer type of the question, such as
?party? in ?What party led Australia from 1983 to
1996??. Topic is the event/person that the ques-
tion talks about, such as ?Australia?. Intuitively, it
is the most important phrase of the question. Con-
straint are the other phrases of the question ex-
cept topic, such as ?1983? and ?1996?. Verb is
the main verb of the question, such as ?lead?. Fur-
thermore, since shorter path indicates closer rela-
tion between two phrases, we discount path corre-
lation in long question path by dividing the corre-
lation by the length of the question path. Lastly,
we sum the discounted path correlations for each
type of question phrases and fire it as a feature,
such as ?Target Cor=c, where c is the correla-
tion value for question target. ME-based rank-
ing model incorporate the orthographic and path
893
correlation features to rank candidate answers for
each of candidate sentences.
5 Candidate Answer Re-ranking
After ranking candidate answers, we select the
highest ranked one from each candidate sentence.
In this section, we are to re-rank them according
to sentence supportive degree. We assume that a
candidate sentence supports an answer if relations
between mapped question phrases in the candidate
sentence are similar to the corresponding ones in
question. Relation paths between any two ques-
tion phrases are extracted and paired. Then, corre-
lation of each pair is calculated. Re-rank formula
is defined as follows:
Score(answer) = ??
?
i
Cor(Pi1, Pi2)
where, ? is answer ranking score. It is the nor-
malized prediction value of the ME-based ranking
model described in Section 4. ?
i
Cor(Pi1, Pi2) is
the sum of correlations of all path pairs. Finally,
the answer with the highest score is returned.
6 Experiments
In this section, we set up experiments on TREC
factoid questions and report evaluation results.
6.1 Experiment Setup
The goal of answer extraction is to identify ex-
act answers from given candidate sentence col-
lections for questions. The candidate sentences
are regarded as the most relevant sentences to the
questions and retrieved by IR techniques. Quali-
ties of the candidate sentences have a strong im-
pact on answer extraction. It is meaningless to
evaluate the questions of which none candidate
sentences contain proper answer in answer extrac-
tion experiment. To our knowledge, most of cur-
rent QA systems lose about half of questions in
sentence retrieval stage. To make more questions
evaluated in our experiments, for each of ques-
tions, we automatically build a candidate sentence
set from TREC judgements rather than use sen-
tence retrieval output.
We use TREC99-03 questions for training and
TREC04 questions for testing. As to build training
data, we retrieve all of the sentences which con-
tain proper answers from relevant documents ac-
cording to TREC judgements and answer patterns.
Then, We manually check the sentences and re-
move those in which answers cannot be supported.
As to build candidate sentence sets for testing, we
retrieve all of the sentences from relevant docu-
ments in judgements and keep those which contain
at least one question key word. Therefore, each
question has at least one proper candidate sentence
which contains proper answer in its candidate sen-
tence set.
There are 230 factoid questions (27 NIL ques-
tions) in TREC04. NIL questions are excluded
from our test set because TREC doesn?t supply
relevant documents and answer patterns for them.
Therefore, we will evaluate 203 TREC04 ques-
tions. Five answer extraction methods are evalu-
ated for comparison:
? Density: Density-based method is used as
baseline, in which we choose candidate an-
swer with the shortest surface distance to
question phrases.
? SynPattern: Syntactic relation patterns
(Shen et al, 2005) are automatically ex-
tracted from training set and are partially
matched using tree kernel.
? StrictMatch: Strict relation matching fol-
lows the assumption in (Tanev et al, 2004;
Wu et al, 2005). We implement it by adapt-
ing relation correlation score. In stead of
learning relation correlations during training,
we predefine them as: Cor(r1, r2) = 1 if
r1 = r2; 0, otherwise.
? ApprMatch: Approximate relation match-
ing (Cui et al, 2004) aligns two relation paths
using fuzzy matching and ranks candidates
according to the sum of all path similarities.
? CorME: It is the method proposed in this pa-
per. Different from ApprMatch, ME-based
ranking model is implemented to incorpo-
rate path correlations which assigns different
weights for different paths respectively. Fur-
thermore, phrase mapping score is incorpo-
rated into the path correlation measure.
These methods are briefly described in Section
2. Performance is evaluated with Mean Reciprocal
Rank (MRR). Furthermore, we list percentages of
questions correctly answered in terms of top 5 an-
swers and top 1 answer returned respectively. No
answer validations are used to adjust answers.
894
Table 1: Overall performance
Density SynPattern StrictMatch ApprMatch CorME
MRR 0.45 0.56 0.57 0.60 0.67
Top1 0.36 0.53 0.49 0.53 0.62
Top5 0.56 0.60 0.67 0.70 0.74
6.2 Results
Table 1 shows the overall performance of the five
methods. The main observations from the table
are as follows:
1. The methods SynPattern, StrictMatch, Ap-
prMatch and CorME significantly improve
MRR by 25.0%, 26.8%, 34.5% and 50.1%
over the baseline method Density. The im-
provements may benefit from the various ex-
plorations of syntactic relations.
2. The performance of SynPattern (0.56MRR)
and StrictMatch (0.57MRR) are close. Syn-
Pattern matches relation sequences of can-
didate answers with the predefined relation
sequences extracted from a training data
set, while StrictMatch matches relation se-
quences of candidate answers with the cor-
responding relation sequences in questions.
But, both of them are based on the assump-
tion that the more number of same rela-
tions between two sequences, the more sim-
ilar the sequences are. Furthermore, since
most TREC04 questions only have one or two
phrases and many questions have similar ex-
pressions, SynPattern and StrictMatch don?t
make essential difference.
3. ApprMatch and CorME outperform SynPat-
tern and StrictMatch by about 6.1% and
18.4% improvement in MRR. Strict matching
often fails due to various relation representa-
tions in syntactic trees. However, such vari-
ations of syntactic relations may be captured
by ApprMatch and CorME using a MI-based
statistical method.
4. CorME achieves the better performance by
11.6% than ApprMatch. The improvement
may benefit from two aspects: 1) ApprMatch
assigns equal weights to the paths of a can-
didate answer and question phrases, while
CorME estimate the weights according to
phrase type and path length. After training a
ME model, the weights are assigned, such as
5.72 for topic path ; 3.44 for constraints path
and 1.76 for target path. 2) CorME incorpo-
rates approximate phrase mapping scores into
path correlation measure.
We further divide the questions into two classes
according to whether NER is used in answer ex-
traction. If the expected answer type of a ques-
tion is unknown, such as ?How did James Dean
die?? or the type cannot be annotated by NER,
such as ?What ethnic group/race are Crip mem-
bers??, we put the question in Qw/oNE set, oth-
erwise, we put it in QwNE. For the questions in
Qw/oNE, we extract all basic noun phrases and
verb phrases as candidate answers. Then, answer
extraction module has to work on the larger can-
didate sets. Using a MUC-based NER, the rec-
ognized types include person, location, organiza-
tion, date, time and money. In TREC04 questions,
123 questions are put in QwNE and 80 questions
in Qw/oNE.
Table 2: Performance on two question sets QwNE
and Qw/oNE
QwNE Qw/oNE
Density 0.66 0.11
SynPattern 0.71 0.36
StrictMatch 0.70 0.36
ApprMatch 0.72 0.42
CorME 0.79 0.47
We evaluate the performance on QwNE and
Qw/oNE respectively, as shown in Table 2.
The density-based method Density (0.11MRR)
loses many questions in Qw/oNE, which indi-
cates that using only surface word information
is not sufficient for large candidate answer sets.
On the contrary, SynPattern(0.36MRR), Strict-
Pattern(0.36MRR), ApprMatch(0.42MRR) and
CorME (0.47MRR) which capture syntactic infor-
mation, perform much better than Density. Our
method CorME outperforms the other syntactic-
based methods on both QwNE and Qw/oNE. Es-
895
pecially for more difficult questions Qw/oNE, the
improvements (up to 31% in MRR) are more ob-
vious. It indicates that our method can be used to
further enhance state-of-the-art QA systems even
if they have a good NER.
In addition, we evaluate component contribu-
tions of our method based on the main idea of
relation path correlation. Three components are
tested: 1. Appr. Mapping (Section 3.4). We re-
place approximate question phrase mapping with
exact phrase mapping and withdraw the phrase
mapping scores from path correlation measure. 2.
Answer Ranking (Section 4). Instead of using
ME model, we sum all of the path correlations to
rank candidate answers, which is similar to (Cui
et al, 2004). 3. Answer Re-ranking (Section
5). We disable this component and select top 5
answers according to answer ranking scores.
Table 3: Component Contributions
MRR
Overall 0.67
- Appr. Mapping 0.63
- Answer Ranking 0.62
- Answer Re-ranking 0.66
The contribution of each component is evalu-
ated with the overall performance degradation af-
ter it is removed or replaced. Some findings are
concluded from Table 3. Performances degrade
when replacing approximate phrase mapping or
ME-based answer ranking, which indicates that
both of them have positive effects on the systems.
This may be also used to explain why CorME out-
performs ApprMatch in Table 1. However, remov-
ing answer re-ranking doesn?t affect much. Since
short questions, such as ?What does AARP stand
for??, frequently occur in TREC04, exploring the
phrase relations for such questions isn?t helpful.
7 Conclusion
In this paper, we propose a relation path
correlation-based method to rank candidate an-
swers in answer extraction. We extract and pair
relation paths from questions and candidate sen-
tences. Next, we measure the relation path cor-
relation in each pair based on approximate phrase
mapping score and relation sequence alignment,
which is calculated by DTW algorithm. Lastly,
a ME-based ranking model is proposed to incor-
porate the path correlations and rank candidate
answers. The experiment on TREC questions
shows that our method significantly outperforms
a density-based method by 50% in MRR and three
state-of-the-art syntactic-based methods by up to
20% in MRR. Furthermore, the method is espe-
cially effective for difficult questions, for which
NER may not help. Therefore, it may be used to
further enhance state-of-the-art QA systems even
if they have a good NER. In the future, we are to
further evaluate the method based on the overall
performance of a QA system and adapt it to sen-
tence retrieval task.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguisitics, 22:39?71.
Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and
Min-Yen Kan. 2004. National university of singa-
pore at the trec-13 question answering. In Proceed-
ings of TREC2004, NIST.
M. Kaisser and T. Becker. 2004. Question answering
by searching large corpora with linguistic methods.
In Proceedings of TREC2004, NIST.
Dekang Lin. 1994. Principar?an efficient, broad-
coverage, principle-based parser. In Proceedings of
COLING1994, pages 42?488.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING2002.
L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson.
1978. Considerations in dynamic time warping al-
gorithms for discrete word recognition. In Proceed-
ings of IEEE Transactions on acoustics, speech and
signal processing.
Deepak Ravichandran, Eduard Hovy, and Franz Josef
Och. 2003. Statistical qa - classifier vs. re-ranker:
What?s the difference? In Proceedings of ACL2003
workshop on Multilingual Summarization and Ques-
tion Answering.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Proceedings of IJCNLP2005.
H. Tanev, M. Kouylekov, and B. Magnini. 2004. Com-
bining linguisitic processing and web mining for
question answering: Itc-irst at trec-2004. In Pro-
ceedings of TREC2004, NIST.
M. Wu, M. Y. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. University at albany?s ilqua in trec
2005. In Proceedings of TREC2005, NIST.
896
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2291?2302, Dublin, Ireland, August 23-29 2014.
Separating Brands from Types: an Investigation of Different Features for
the Food Domain
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We examine the task of separating types from brands in the food domain. Framing the problem
as a ranking task, we convert simple textual features extracted from a domain-specific corpus into
a ranker without the need of labeled training data. Such method should rank brands (e.g. sprite)
higher than types (e.g. lemonade). Apart from that, we also exploit knowledge induced by semi-
supervised graph-based clustering for two different purposes. On the one hand, we produce an
auxiliary categorization of food items according to the Food Guide Pyramid, and assume that a
food item is a type when it belongs to a category unlikely to contain brands. On the other hand,
we directly model the task of brand detection using seeds provided by the output of the textual
ranking features. We also harness Wikipedia articles as an additional knowledge source.
1 Introduction
Brands play a significant role in social life. They are the subject matter of many discussions in social me-
dia. Their automatic detection for information extraction tasks is a pressing problem since, despite their
unique property to refer to commercial products of specific companies, in everyday language they often
occur in similar contexts as common nouns. A typical domain where such behaviour can be observed is
the food domain, where food brands (e.g. nutella or sprite) are often used synonymously with the food
type1 of which the brand is a prototypical instance (e.g. chocolate spread or lemonade). Such usage is
illustrated in (1) and (2).
(1) In the evening, I eat a slice of bread with either nutella or marmalade.
(2) I prepare my pancakes with baking soda, water and a lacing of sprite instead of sugar.
This particular phenomenon of metonymy (Lakoff and Johnson, 1980), commonly referred to as generi-
cized trademarks, of course, has consequences on automatic lexicon induction methods. If one automat-
ically extracts food types, one also obtains food brands.
In this paper, we examine features to detect brands automatically. Solving the issue with the help of
a manually-compiled list of brands neglects parts of the nature of brands. Brands come and go. Some
products may be discontinued after a certain amount of time (e.g. due to limited popularity) while, on the
other hand, new products constantly enter the market. For instance, popular food brands, such as sierra
mist or kazoozles, did not exist a decade ago. Therefore, a list of brands that is manually created today
may not reflect the predominant food brands that will be available in a decade.
The features we introduce to detect brands consider both the intrinsic properties of brands and their
contextual environment. Even though in many contexts, brands are used as ordinary type expressions
(1), there might be specific contexts that are only observed with brands. We also consider distributional
properties: brands may co-occur with other brands. Moreover, they may be biased towards certain
categories, e.g. sweets, beverages etc. For the latter, we actually exploit the usage of food brands to be
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1We define food type as common nouns that denote a particular type of food, e.g. apple, chocolate, cheese etc.
2291
Method Corpus Corpus Type P@10 P@100 P@500
ranking by frequency chefkoch.de domain specific 0.00 22.00 25.60
induction based on coordination Wikipedia open domain 90.00 60.00 47.80
induction based on coordination chefkoch.de domain specific 100.00 98.00 92.00
Table 1: Precision at rank n (P@n) of different food induction methods.
Label Items Examples
Food Types 1745 apple, baguette, beer, corn flakes, crisps, basmati rice, broccoli, choco-
late spread, gouda, orange juice, pork, potato, steak, sugar
Food Brands 221 activia, babybel, becel, butterfinger, kit kat, nutella, pepsi, philadelphia,
smacks, smarties, sprite, ramazzotti, tuborg, volvic
Table 2: Gold standard of the food vocabulary.
used as genericized trademarks, allowing food categorization methods for types to be easily extended to
brands. Moreover, we examine how external knowledge resources, such as Wikipedia, can be harnessed
as a means to separate brands from types. Our task is lexicon construction rather than contextual entity
classification, that is, we are interested in what a food item generally conveys and not what it conveys in
a specific context.
We consider the food domain as a target domain since there are large, unlabeled domain-specific
corpora available gathered from social media which are vital for the methods we explore. It is also a
domain for which there has already been done research in the area of natural language processing (NLP),
and there are common applications, such as virtual customer advice or product recommendation, that
may exploit such NLP technology.
The methods we consider require no, or hardly any human supervision. Thus, we imagine that they
can also be applied to other domains at a low cost. In particular, other life-style domains, such as fashion,
cosmetics or electronics show parallels, since comparable textual web data from which to extract domain-
specific knowledge are available.
Our experiments are carried out on German data, but our findings should carry over to other languages
since the issues we address are (mostly) language universal. All examples are given as English transla-
tions. We use the term food item to refer to the union of food brands and food types. All food items will
be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both
written uppercase. In English, both types and brands can be written uppercase or lowercase2, however,
there is a tendency in user-generated content/social media to write mostly lowercase.
2 Motivation & Data
Previous research on lexicon induction proposed a widely applicable method based on coordination
(Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First,
a set of seed expressions that are typical of the categories one wants to induce are defined. Then, addi-
tional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all
expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study
of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the
great semantic coherence of conjuncts.
This method can also be applied to the food domain. As a domain-specific dataset for all our experi-
ments, we use a crawl of chefkoch.de3 (Wiegand et al., 2012) consisting of 418, 558 webpages of forum
entries. chefkoch.de is the largest German web portal for food-related issues. Table 1 shows the effec-
tiveness of coordination as a means of extracting food items from our domain-specific corpus. Given a
seed set of 10 frequent food items (we use: water, salt, sugar, salad, bread, meat, cake, flour, butter and
2There are plenty of food types that are written uppercase, e.g. Jaffa Cakes, Beef Wellington, BLT, Hoppin? John etc.
3www.chefkoch.de
2292
Properties Type of Property Example Brands Types
nonwords general ebly, sprite, twix 41.63 -NA-
derived from proper noun general cheddar, evian, jim beam 31.22 2.29
foreign words general camembert, merci, wasabi 27.15 12.37
length general average no. of characters 7.97 10.53
word initial plosives stylistic p,t,k,b,d,g (attract attention) 31.22 35.81
assonance stylistic fanta, kiwi (fruit), papaya 11.76 11.06
alliteration stylistic babybel, blueberry, tic tac 6.79 3.78
onomatopoeia stylistic crunchips, popcorn 2.71 0.52
rhyme stylistic jelly belly, hubba bubba 1.35 0.06
Table 3: Comparison of intrinsic properties between brands and types; brands are always underlined; all
numbers (except for length) are the proportion with the respective property.
potato), we compute all conjuncts and rank them according to frequency. We do this on our domain-
specific corpus and on Wikipedia. As a baseline, we simply sort all nouns according to frequency in our
domain-specific corpus. The table shows that ranking by frequency is no effective method. Conjuncts
produce good results provided that they are extracted from a domain-specific corpus.
Even though coordination is a very reliable method to induce food items, it fails to distinguish between
food types and food brands. We produced a labeled food vocabulary to be used for all our subsequent
experiments consisting of food types and food brands (see Table 2). The food types exclusively comprise
the food vocabulary from Wiegand et al. (2014). The food brands were manually selected with the help
of the web. We only include food items that occur at least 5 times in our corpus. In our food vocabulary,
87% of our food brands occur as a conjunct of a food type. Therefore, the problem of confusing brands
with types is inherent to induction based on coordination.
3 Intrinsic Properties
Table 3 provides some statistics on intrinsic properties of our food items giving some indication which
feature types might be used for this task. We also include some stylistic properties of brands that have
been addressed in previous marketing research and applied psychology. We focus on fairly straightfor-
ward features from desirable brand name characteristics (Robertson, 1989), since we assume that there
is more general agreement on the underlying concepts than there is on the concepts underlying complex
sound symbolism (Klink, 2000; Yorkston and Menon, 2004). For the statistics in Table 3, most proper-
ties (i.e. all except length and word-initial plosives) have been detected manually. The reason for this is
that their automatic detection is not trivial (e.g. there is no established algorithm to detect onomatopoeia;
even the detection of rhyme or assonance is not straightforward given the low grapheme-phoneme corre-
spondence of English). We did not want the statistics for this exploratory experiment to be distorted by
error-proneness of the detection methods.
Table 3 shows that a large part of brands are nonwords indicating that this task is hard to be solved with
intrinsic features only. Since there is a high number of brands that are derived from some existing proper
noun being either a person or a location, named-entity recognition might be applied to this task. Many
brands are also foreign words. Unfortunately, applying language checking software on our food items
turned out to perform poorly. (These tools are only effective on longer texts, e.g. sentences or entire
documents, and do not work on isolated words, as in our problem setting.) We also noticed a difference
in average word length between brands and types which is consistent with Robertson (1989) who claims
that brand names should be simple. Most stylistic features seem to be less relevant to our task as they
are either too infrequent or not discriminative. Therefore, we do not consider them as features for the
detection of brands in our forthcoming experiments.
2293
Figure 1: Processing Pipeline for ranking.
Feature Type Features
ranking feature LENGTH, COMMERCE, NER
target
, NER
context
, DIVERS, PAT
mod
, PAT
pp
reset feature GRAPH
pyramid
bootstrapping feature GRAPH
brand
, WIKI, VSM
Table 4: Feature classification.
4 Method
Our aim is to determine predictive features for the detection of brands. Rather than employing some
supervised learner that requires manually labeled training data, we want to convert these features directly
into a classifier without costly labeled data. We conceive this task as a ranking task. The reason for using
a ranking is that our features can be translated into a ranking score in a very straightforward manner.
For the evaluation, we do not have to determine some empirical threshold separating the category brand
from the category type. Instead, the evaluation measures we employ for ranking implicitly assume highly
ranked instances as brands and instances ranked at the bottom as types.
For the ranking task, we employ the processing pipeline as illustrated in Figure 1. Most of our features
are designed in such a way that they assign a ranking score to each of our food items by counting how
often a feature is observed with a food item; that is why we call these features ranking features. The
resulting ranking should assign high scores to food brands and low scores to food types. If we want to
combine several features into one ranking, we simply average for each food item the different ranking
scores of the individual ranking features. This is possible since they have the same range [0; 1]. We
obtain such range by normalizing the number of occurrences of a feature with a particular food item by
the total number of occurrences of that food item. The combination by averaging is unbiased as it treats
all features equally.
We also introduce a reset feature which is applied on top of an existing ranking provided by ranking
features. A reset feature is a negative feature in the sense that it is usually a reliable cue that a food item
is not a brand. If it fires for a particular food item, then its ranking score is reset to 0.
Finally, we add bootstrapping features. These features produce an output similar to the ranking fea-
tures (i.e. another ranking). However, unlike the ranking features, the bootstrapping features produce
their output based on a weakly-supervised method which requires some labeled input. Rather than manu-
ally providing that input, we derive it from the combined output that is provided by the ranking and reset
features. We restrict ourselves to instances with a high-confidence prediction, which translates to the top
and bottom end of a ranking. (Since the instances are not manually labeled, of course, not every label
assignment will be correct. We hope, however, that by restricting to instances with a high-confidence
prediction, we can reduce the amount of errors to a minimum.) The output of a bootstrapping feature is
combined with the set of ranking features to a new ranking onto which again a reset feature is applied.
Table 4 shows which feature (each will be discussed below) belongs to which of the above feature
types (i.e. ranking, reset or bootstrapping features). Most features (i.e. all except WIKI) are extracted
from our domain-specific corpus introduced in ?2.
2294
4.1 Length
Since we established that brands tend to be shorter than types (?3), we add one feature that ranks each
food item according to its number of characters.
4.2 Target Named-Entity Recognition (NER
target
)
Brands can be considered a special kind of named entities. We apply a part-of-speech tagger to count
how often a food item has been tagged as a proper noun. We decided against a named-entity recognizer
as it usually only recognizes persons, locations and organizations, while part-of-speech taggers employ
a general tag for all proper nouns (that may go well beyond the three afore-mentioned common types).
We use a statistical tagger, i.e. TreeTagger (Schmid, 1994), that also employs features below the word
level. As many of our food items will be unknown words, a character-level analysis may still be able to
make useful predictions.
4.3 Contextual Named-Entity Recognition (NER
context
)
We also count the number of other named entities that co-occur with the target food brand within the
same sentence. We are only interested in organizations; an organization co-occurring with a brand is
likely to be the company producing that brand (e.g. He loves Kellogg?s
company
frosties
brand
.) For this
feature, we rely on the output of a named-entity recognizer for German (Chrupa?a and Klakow, 2010).
4.4 Diversification (DIVERS)
Once a product has established itself on the market for a substantial amount of time, many companies
introduce variants of their brand to further consolidate their market position. The purpose of this diversi-
fication is to appeal to customers with special needs. A typical variant of food brands are light products.
In many cases, the names of variants consist of the name of the original brand with some prefix or suffix
indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled
11 affixes and check for each food item how often it is accompanied by one of them.
4.5 Commerce Cues (COMMERCE)
Presumably, brands are more likely to be mentioned in the context of commercial transaction events than
types. Therefore, we created a list of words that indicate these types of events. The list was created
ad hoc. We used external resources, such as FrameNet (Baker et al., 1998) or GermaNet (Hamp and
Feldweg, 1997) (the German version of WordNet (Miller et al., 1990)), and made no attempt to tune that
list to our domain-specific food corpus. The final list (85 cues in total) comprises: verbs (and deverbal
nouns) that convey the event of a commercial transaction (e.g. buy, purchase or sell), persons involved in
a commercial transaction (e.g. customer or shop assistant), means of purchase (e.g. money, credit card
or bill), places of purchase (e.g. supermarket or shop) and judgment of price (e.g. cheap or expensive).
4.6 Food Modifier (PAT
mod
)
Even though many mentions of brands are similar to those of types, there exist some particular contexts
that are mostly observed with brands. If the food item to be classified often occurs as a modifier of
another food item, then the target item is likely to be some brand. This is due to the fact that many
brands are often mentioned in combination with the food type that they represent, e.g. volvic mineral
water, nutella chocolate spread.
4.7 Prepositional Phrase Embedding (PAT
pp
)
Instead of appearing as a modifier (?4.6), a brand may also be embedded in some prepositional phrase
that has a similar meaning, e.g. We only buy the chocolate spread [by nutella]
PP
.
4.8 Graph-based Methods (GRAPH)
We also employ some semi-supervised graph clustering method in order to assign semantic types to food
items as introduced in Wiegand et al. (2014). The underlying data structure is a food graph that is gener-
ated automatically from our domain-specific corpus where nodes represent food items and edge weights
2295
Category Description General Brands
MEAT meat and fish (products) 19.48 1.31
BEVERAGE beverages (incl. alcoholic drinks) 17.19 23.96
SWEET sweets, pastries and snack mixes 14.90 25.60
SPICE spices and sauces 10.53 2.42
VEGE vegetables (incl. salads) 10.38 0.00
STARCH starch-based side dishes 9.21 4.42
MILK milk products 6.71 23.48
FRUIT fruits 4.48 1.14
GRAIN grains, nuts and seeds 3.41 0.00
FAT fat 2.54 20.00
EGG eggs 0.92 0.00
Table 5: Proportion of categories in the entire food vocabulary (General) and among brands (Brands).
represent the similarity between different items. The weights are computed based on the frequency of
co-occurrence within a similarity pattern (e.g. X instead of Y). Food items that cluster with each other
in such a graph (i.e. food items that often co-occur in a similarity pattern) are most likely to belong to
the same class. For the detection of brands, we examine two different types of food categorization. We
always use the same clustering method (Wiegand et al., 2014) and the same graph. Depending on the
specific type of categorization, we only change the seeds to fit the categories to be induced.
4.8.1 Categories of the Food Guide Pyramid (GRAPH
pyramid
)
The first categorization we consider is the categorization of food items according to the Food Guide
Pyramid (U.S. Department of Agriculture, 1992) as examined in Wiegand et al. (2014). We observed
that food brands are not equally distributed throughout the entire range of food items. There is a notable
bias of food brands towards beverages (mostly soft drinks and alcoholic drinks), sweets, snack mixes,
dairy products and fat. Other categories, e.g. nuts, vegetables or meat, hardly contain brands.4 The
category inventory and the proportion among types and brands are displayed in Table 5.
We use the category information as a negative feature, that is, we re-set the ranking score to 0 if the
category of the food item is either MEAT, SPICE, VEGE, STARCH, FRUIT, GRAIN or EGG. In order
to obtain a category assignment to our food vocabulary, we re-run the best configuration from Wiegand et
al. (2014) including the choice of category seeds. We just extend the graph that formerly only contained
food types by nodes representing brands. We use no manually-compiled knowledge regarding food
brands. Even though the seed food items are exclusively food types, we hope to be also able to make
inferences regarding food brands. This is illustrated in Figure 2(a): The brand mars can be grouped with
food types that are sweets, therefore, we conclude that mars is also some sweet. (Brands can be grouped
with food types of their food category, since food brands are often used as if they were types (?1)). Since
sweets are plausible candidates for brands (Table 5), mars is likely to be some brand.
We think that such bias of brands towards certain subcategories is also present in other domains. For
example, in the electronic domain laptops will have a much larger variety of brands than network cables.
Similarly, in the fashion domain there exist much more shoe brands than sock brands.
4.8.2 Direct Graph Clustering Separating Brands from Types (GRAPH
brand
)
We also apply graph clustering directly for the separation of brands from types, i.e. we assign some
brand and type seeds and then run graph-based clustering (Figure 2(b)). In order to combine the output
of this clustering with that of the previous methods, we interpret the confidence of the output as a ranking
score. As we pursue an unsupervised approach, we do not manually label the seeds but rely on the output
of a ranker using a combination of above features (Figure 1). Instances at the top of the ranking are
considered brand seeds, while instances at the bottom are considered type seeds.
4There may be companies which, among other things, also sell these food types, but we do not want to extract the names of
organizations (as in traditional named-entity recognition), e.g. Kraft Foods, but specific product names, e.g. philadelphia.
2296
(a) food type categorization (b) brand detection
Figure 2: Similarity graphs; bold items are seeds; line width of edges represents strength of similarity.
4.9 Wikipedia Bootstrapping (WIKI)
For many information extraction tasks, the usage of collaboratively-edited resources is increasingly be-
coming popular. One of the largest resources of that type is Wikipedia. For our vocabulary of food items,
we could match 57% of the food brands and 53% of the food types with a Wikipedia article.
Even though Wikipedia may hold some useful information for the detection of brands, this information
is not readily available in a structured format, such as infoboxes. This is illustrated by (3)-(5) which
display the first sentence of three Wikipedia articles, where (3) and (4) are food brands and (5) is a food
type. There is some thematic overlap across the two categories (e.g. (4) and (5) describe the ingredients
of the food item). However, if one also considers the entire articles, some notable topical differences
between brands and types become obvious. The articles of food brands typically focus on commercial
aspects (i.e. market situation and product history) while articles of food types describe the actual food
item (e.g. by distinguishing it from other food items or naming its origin). Therefore, a binary topic
classification based on the entire document should be a suitable approach. In the light of the diversified
language employed for articles on brands (cp. (3)-(4)), we consider a bag-of-words classifier more
effective than applying some textual patterns on those texts.
(3) BRAND: Twix is a chocolate bar made by Mars, Inc.
(4) BRAND: Smarties is a brand under which Nestle? produces colour-varied sugar-coated chocolate
lentils.
(5) TYPE: Milk chocolate is a type of chocolate made from cocoa produce (cocoa bean, cocoa butter),
sugar, milk or dairy products.
Similar to GRAPH
brand
(?4.8.2), we harness Wikipedia via a bootstrapping method. We generate a
labeled training set of Wikipedia articles representing brands and types using the combined output of
the ranking features (+ reset feature). We then train a supervised classifier on these data and classify all
articles representing food items of our food vocabulary. We use the output score of the classifier for the
article of each food item (which amounts to some confidence score) and thus obtain a ranking score. For
those food items for which no Wikipedia entry exists, we produce a score of 0.
4.10 Vector Space Model (VSM)
While GRAPH
brand
(?4.8.2) determines similar food items by means of highly weighted edges in a sim-
ilarity graph (that represent the frequency of co-occurrences with a similarity pattern), we also examine
whether distributional similarity can be harnessed for the same purpose. We represent each food item
as a vector, where the vector components encode the frequency of words that co-occur with mentions of
the food item in a fixed window of 5 words (in our domain-specific corpus). Similar to GRAPH
brand
(?4.8.2) and WIKI (?4.9), we consider the n highest and m lowest ranked food items provided by rank-
ing features (+ reset feature) as labeled brand and type instances for a supervised classifier. For testing,
we apply this classifier on each food item in our vocabulary, or more precisely, its vector representation.
Thus we obtain another ranking score (again, the output amounts to some confidence score).
2297
Plain +Graph
pyramid
(reset feature)
Feature P@10 P@50 P@100 P@200 AP P@10 P@50 P@100 P@200 AP
RANDOM 10.00 18.00 14.00 14.00 0.119 20.00 22.00 22.00 21.50 0.167
LENGTH 10.00 20.00 22.00 21.50 0.163 10.00 32.00 41.00 40.00 0.230
DIVERS 60.00 46.00 37.00 25.00 0.207 60.00 50.00 39.00 30.50 0.240
COMMERCE 30.00 28.00 31.00 27.00 0.220 40.00 38.00 39.00 35.00 0.294
NER
context
70.00 72.00 52.00 43.50 0.401 80.00 72.00 51.00 46.50 0.425
PAT
pp
90.00 78.00 64.00 50.00 0.439 100.00 78.00 69.00 53.00 0.476
PAT
mod
60.00 68.00 69.00 58.00 0.460 90.00 76.00 76.00 58.00 0.507
NER
target
80.00 70.00 60.00 52.50 0.479 80.00 78.00 72.00 61.50 0.525
combined 100.00 88.00 66.00 59.00 0.612 100.00 86.00 76.00 62.50 0.626
Table 6: Precision at rank n (P@n) and average precision (AP) of the different ranking features.
Partition Prec Rec F
Food Types 70.49 72.82 71.04
Food Brands 69.09 66.21 64.93
Table 7: Performance of food categorization according to the Food Guide Pyramid (auxiliary classifica-
tion).
5 Experiments
In the following experiments, we mostly evaluate rankings. For that we employ precision at rank n and
average precision. The former computes precision at a predefined rank n, whereas the latter provides
an average of the precisions measured at every possible rank. While average precision provides a score
that evaluates the ranking as a whole, precision at rank n typically focuses on the correctness of higher
ranks.5
5.1 Evaluation of Ranking Features
Table 6 (left half) displays the results of the individual and combined ranking features. As a trivial base-
line, we also include RANDOM which is randomized ranking of the food items. The table shows that all
features except LENGTH produce a notably better ranking than RANDOM. Following the inspection of
intrinsic properties of brands in ?3, it does not come as a surprise that NER
target
is the strongest feature.
However, also the contextual features NER
context
, PAT
pp
and PAT
mod
produce reasonable results. If we
combine all features (except the poorly performing LENGTH), we obtain a notable improvement over
NER
target
which proves that those different features are complementary to a certain extent.
5.2 Evaluation of the Reset Feature
In Table 7, we examine the food categorization according to the Food Guide Pyramid as such. For
this evaluation, we partition the output of automatic categorization into (actual) types and brands. Thus
we can compare the performance between those two different types of food items, and can quantify
the loss on the categorization on brands against the categorization on types. (Due to the fact that the
seeds exclusively comprise types, we must assume that performance on brands will be lower.)6 Even
though there is a slight loss on brands (mostly recall), we still consider this categorization useful for our
purposes.
5The manually labeled food vocabulary is available at:
www.lsv.uni-saarland.de/personalPages/michael/relFood.html
6Since the categories to indicate unlikely brands (?4.8.1) are extremely sparse (Table 5), we conflate them for this evaluation
as one large category NEGATIVE. Because of this and due to the fact that the food type vocabulary is slightly smaller than
the one used in Wiegand et al. (2014) (since we only consider food items mentioned at least 5 times in our corpus (?2)), the
performance scores of food categorization in Table 7 and the one reported in Wiegand et al. (2014) differ accordingly.
2298
Classifier Acc Prec Rec F
Baselines
Majority-Class Classifier 88.76 44.38 50.00 47.02
seeds only: 50 top+150 bottom 9.51 91.00 13.85 23.47
seeds only: 100 top+300 bottom 18.57 86.17 25.48 37.81
Bootstrap. Features
WIKI (seeds: 50 top+150 bottom) 43.95 87.68 43.33 57.91
VSM (seeds: 100 top+300 bottom) 77.87 64.93 81.61 66.39
GRAPH
brand
(seeds: 100 top+300 bottom) 82.91 81.36 67.27 73.53
Table 8: Bootstrapping features in isolation compared with baselines (i.e. reference classifiers).
Table 6 (right half) shows the performance of the corresponding reset feature on the brand detection
task. We observe a systematic increase in performance when added on top of the ranking features.
5.3 Evaluation of Bootstrapping Features
Table 9 displays the performance of the bootstrapping features. For the labeled training data, we empir-
ically determined the optimal class ratio (1:3) and the optimal number of seeds (the top 100 and bottom
300 items for VSM and GRAPH
brand
, and top 50 and bottom 150 items for WIKI). As a supervised
classifier for VSM and WIKI, we chose Support Vector Machines using SVMlight (Joachims, 1999).
The table shows that only GRAPH
brand
and WIKI improve the ranking, whereas WIKI is notably
stronger. These results suggest that Wikipedia is a good resource from which to learn whether a food
item is a brand or not. However, this task could not be completely solved byWIKI since not all food items
are covered by Wikipedia (?4.9). To further prove this, we also evaluate an upper bound of Wikipedia,
WIKI
oracle
(exclusively using that resource), in which we pretend to correctly interpret every Wikipedia
page as an article for either a food brand or a food type. We rank all brands having a Wikipedia article
highest. They are followed by those food items having no article (ordered randomly) and, finally, by the
food types having a Wikipedia article. Table 9 shows that we are able to outperform WIKI
oracle
.
Our pipeline (Figure 1) applies the reset feature at two stages. We also examine whether it is necessary
to apply that feature for a second time. Presumably, the bootstrapping feature is so effective that we do
not have to apply further type filtering. After all, the reset feature will also downweight some correct
food items (Table 5). Table 9 confirms that when the reset feature is applied only once, we obtain a better
performance (according to average precision) for all bootstrapping features (even for VSM).
Finally, Table 8 evaluates the bootstrapping features in isolation. Since, unlike the ranking features,
the bootstrapping features provide a definite classification for each food item (in addition to a prediction
score evaluated as a ranking score), we consider the output for a binary classification task. In this setting,
we make use of the four evaluation measures accuracy, precision, recall and F-score. For the last three
measures, we always compute the macro average score.
As a baseline, we also include a majority-class classifier that always predicts the class food type.
Interestingly, in terms of F-score, GRAPH
brand
is the best method rather than WIKI, i.e. the best method
from the previous evaluation in Table 9. The reason for this is that we evaluate in isolation rather than in
combination with other features (i.e. parts of the additional benefit included in GRAPH
brand
may already
be contained in ranking and reset features). Secondly, in a ranking task (Table 9), good performance is
usually achieved by classifiers biased towards a high precision. Indeed, the best ranker in Table 9, i.e.
WIKI, achieves the highest precision in Table 8.
6 Related Work
Ling and Weld (2012) examine named-entity recognition on data that also include brands, however, the
class of brands is not explicitly discussed. Putthividhya and Hu (2011) explore brands in the context
of product attribute extraction. Entities are extracted from eBay?s clothing and shoe category. Nadeau
et al. (2006) explicitly generate gazetteers of car brands obtained from corresponding websites. Those
textual data are very restrictive in that they do not represent sentences but category listings or tables. In
this paper, we consider as textual source a more general text type, i.e. forum entries, that comprise full
2299
-2nd reset
Feature P@200 AP P@200 AP
WIKI
oracle
66.00 0.429 -N/A- -N/A-
ranking+GRAPH
pyramid
62.50 0.626 -N/A- -N/A-
ranking+GRAPH
pyramid
+VSM 60.00 0.619 63.00 0.661
ranking+GRAPH
pyramid
+GRAPH
brand
67.50 0.638 65.50 0.662
ranking+GRAPH
pyramid
+WIKI 70.00 0.688 73.00 0.718
Table 9: Impact of bootstrapping; -2nd reset: does not apply reset feature for a second time (Figure 1).
sentences. Previous work also focuses on traditional (semi-)supervised algorithms. Hence, there are only
few additional insights as to the specific properties of brand names. Min and Park (2012) examine the
aspect of product instance distinction on the use case of product reviews on jeans from Amazon. Their
work focuses on temporal features to identify distinct product instances (these may also include brand
names).
The food domain has also recently received some attention. Different types of classification have
been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et
al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according
to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined.
While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013;
Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand
et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to
food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang
(2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that
explicitly addresses the detection of brands in the food domain. While brands as such present an addi-
tional dimension to previously examined types of categorization, we also show that the categorization
according to the Food Guide Pyramid helps to decide whether a food item is a brand or not.
7 Conclusion
We examined the task of separating types from brands in the food domain. Framing the problem as a
ranking task, we directly converted predictive features extracted from a domain-specific corpus into a
ranker without the need of labeled training data. Apart from those ranking features, we also exploited
knowledge induced by semi-supervised graph-based clustering for two different purposes. On the one
hand, we produced an auxiliary categorization of food items according to the Food Guide Pyramid, and
assumed that a food item is a type when it belongs to a category that is unlikely to contain brands. On
the other hand, we directly modelled the task of brand detection by using seeds provided by the output
of the textual ranking features. We also learned additional high-precision knowledge from Wikipedia
webpages using a similar bootstrapping scheme.
Acknowledgements
This work was performed in the context of the Software-Cluster project SINNODIUM. Michael Wie-
gand was funded by the German Federal Ministry of Education and Research (BMBF) under grant no.
01IC10S01. The authors would like to thank Melanie Reiplinger for proofreading the paper.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceed-
ings of the International Conference on Computational Linguistics and Annual Meeting of the Association for
Computational Linguistics (COLING/ACL), pages 86?90, Montre?al, Quebec, Canada.
Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge, Lily Scherlis, and Noah A. Smith. 2012. Word Salad:
Relating Food Prices and Descriptions. In Proceedings of the Joint Conference on Empirical Methods in Natural
2300
Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 1357?1367,
Jeju Island, Korea.
Grzegorz Chrupa?a and Dietrich Klakow. 2010. A Named Entity Labeler for German: Exploiting Wikipedia and
Distributional Clusters. In Proceedings of the Conference on Language Resources and Evaluation (LREC),
pages 552?556, La Valletta, Malta.
Gregory Druck and Bo Pang. 2012. Spice it up? Mining Refinements to Online Instructions from User Generated
Content. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages
545?553, Jeju, Republic of Korea.
Gregory Druck. 2013. Recipe Attribute Detection Using Review Text as Supervision. In Proceedings of the
IJCAI-Workshop on Cooking with Computers (CWC), Beijing, China.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a Lexical-Semantic Net for German. In Proceedings of ACL
workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,
pages 9?15, Madrid, Spain.
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European Chapter of the Association for Computational Linguistics
(EACL), pages 174?181, Madrid, Spain.
Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 169?184. MIT Press.
Jun Seok Kang, Polina Kuznetsova, Michael Luca, and Yejin Choi. 2013. Where Not to Eat? Improving Public
Policy by Predicting Hygiene Inspections Using Online Reviews. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP), pages 1443?1448, Seattle, WA, USA.
Richard R. Klink. 2000. Creating Brand Names with Meaning: The Use of Sound Symbolism. Marketing Letters,
11(1):5?20.
George Lakoff and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press.
Xiao Ling and Daniel S. Weld. 2012. Fine-Grained Entity Recognition. In Proceedings of the National Conference
on Artificial Intelligence (AAAI), pages 94?100, Toronto, Canada.
Qingliang Miao, Shu Zhang, Bo Zhang, Yao Meng, and Hao Yu. 2012. Extracting and Visualizing Semantic
Relationships from Chinese Biomedical Text. In Proceedings of the Pacific Asia Conference on Language,
Information and Compuation (PACLIC), pages 99?107, Bali, Indonesia.
George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction to
WordNet: An On-line Lexical Database. International Journal of Lexicography, 3:235?244.
Hye-Jin Min and Jong C. Park. 2012. Product Name Classification for Product Instance Distinction. In Proceed-
ings of the Pacific Asia Conference on Language, Information and Compuation (PACLIC), pages 289?298, Bali,
Indonesia.
David Nadeau, Peter D. Turney, and Stan Matwin. 2006. Unsupervised Named-Entity Recognition: Generating
Gazetteers and Resolving Ambiguity. In Proceedings of the Canadian Conference on Artificial Intelligence,
pages 266?277, Que?bec City, Que?bec, Canada.
Duangmanee Putthividhya and Junling Hu. 2011. Bootstrapped Named Entity Recognition for Product Attribute
Extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 1557?1567, Edinburgh, Scotland, UK.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 117?124,
Providence, RI, USA.
Brian Roark and Eugene Charniak. 1998. Noun-phrase co-occurrence statistics for semi-automatic semantic
lexicon construction. In Proceedings of the International Conference on Computational Linguistics (COLING),
pages 1110?1116, Montreal, Quebec, Canada.
Kim Robertson. 1989. Strategically Desirable Brand Name Characteristics. Journal of Comsumer Marketing,
6(4):61?71.
2301
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Processing, pages 44?49, Manchester, United Kingdom.
Human Nutrition Information Service U.S. Department of Agriculture. 1992. The Food Guide Pyramid. Home
and Garden Bulletin 252, Washington, D.C., USA.
Willem Robert van Hage, Sophia Katrenko, and Guus Schreiber. 2005. A Method to Combine Linguistic
Ontology-Mapping Techniques. In Proceedings of International Semantic Web Conference (ISWC), pages 732
? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber. 2006. A Method for Learning Part-Whole Relations. In
Proceedings of International Semantic Web Conference (ISWC), pages 723 ? 735, Athens, GA, USA. Springer.
Michael Wiegand and Dietrich Klakow. 2013. Towards Contextual Healthiness Classification of Food Items ? A
Linguistic Approach. In Proceedings of the International Joint Conference on Natural Language Processing
(IJCNLP), pages 19?27, Nagoya, Japan.
Michael Wiegand, Benjamin Roth, and Dietrich Klakow. 2012. Web-based Relation Extraction for the Food
Domain. In Proceedings of the International Conference on Applications of Natural Language Processing to
Information Systems (NLDB), pages 222?227, Groningen, the Netherlands. Springer.
Michael Wiegand, Benjamin Roth, and Dietrich Klakow. 2014. Automatic Food Categorization from Large
Unlabeled Corpora and Its Impact on Relation Extraction. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguistics (EACL), pages 673?682, Gothenburg, Sweden.
Hui Yang, Rajesh Swaminathan, Abhishek Sharma, Vilas Ketkar, and Jason D?Silva, 2011. Learning Structure and
Schemas from Documents, volume 375 of Studies in Computational Intelligence, chapter Mining Biomedical
Text Towards Building a Quantitative Food-disease-geneNetwork, pages 205?225. Springer Berlin Heidelberg.
Eric Yorkston and Geeta Menon. 2004. A Sound Idea: Phonetic Effects of Brand Names on Consumer Judgments.
Journal of Consumer Research, 31:43?51.
Patrick Ziering, Lonneke van der Plas, and Hinrich Schuetze. 2013. Bootstrapping Semantic Lexicons for Techni-
cal Domains. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP),
Nagoya, Japan, 1321?1329.
2302
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 24?29,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Combining Generative and Discriminative Model Scores for Distant
Supervision
Benjamin Roth, Dietrich Klakow
Saarland University
Spoken Language Systems
Saarbru?cken, Germany
{benjamin.roth|dietrich.klakow}@lsv.uni-saarland.de
Abstract
Distant supervision is a scheme to generate
noisy training data for relation extraction by
aligning entities of a knowledge base with
text. In this work we combine the output of
a discriminative at-least-one learner with that
of a generative hierarchical topic model to re-
duce the noise in distant supervision data. The
combination significantly increases the rank-
ing quality of extracted facts and achieves
state-of-the-art extraction performance in an
end-to-end setting. A simple linear interpo-
lation of the model scores performs better
than a parameter-free scheme based on non-
dominated sorting.
1 Introduction
Relation extraction is the task of finding relational
facts in unstructured text and putting them into a
structured (tabularized) knowledge base. Training
machine learning algorithms for relation extraction
requires training data. If the set of relations is pre-
specified, the training data needs to be labeled with
those relations.
Manual annotation of training data is laborious
and costly, however, the knowledge base may al-
ready partially be filled with instances from the rela-
tions. This is utilized by a scheme known as distant
supervision (DS) (Mintz et al, 2009): text is au-
tomatically labeled by aligning (matching) pairs of
entities that are contained in a knowledge base with
their textual occurrences. Whenever such a match is
encountered, the surrounding context (sentence) is
assumed to express the relation.
This assumption, however, can fail.
Consider the example given in (Taka-
matsu et al, 2012): If the tuple
place_of_birth(Michael Jackson, Gary)
is contained in the knowledge base, one matching
context could be:
Michael Jackson was born in Gary ...
And another possible context:
Michael Jackson moved from Gary ...
Clearly, only the first context indeed expresses the
relation and should be labeled accordingly.
Three basic approaches have been proposed to
deal with noisy distant supervision instances: The
discriminative at-least-one approach (Riedel et al,
2010), that requires that at least one of the matches
for a relation-entity tuple indeed expresses the
relation; The generative approach (Alfonseca et
al., 2012) that separates relation-specific distribu-
tions from noise distributions by using hierarchical
topic models; And the pattern correlation approach
(Takamatsu et al, 2012) that assumes that contexts
which match argument pairs have a high overlap in
argument pairs with other patterns expressing the re-
lation.
In this work we combine 1) a discriminative at-
least-one learner, that requires high scores for both
a dedicated noise label and the matched relation, and
2) a generative topic model that uses a feature-based
representation to separate relation-specific patterns
from background or pair-specific noise. We score
surface patterns and show that combining the two
approaches results in a better ranking quality of re-
lational facts. In an end-to-end evaluation we set a
threshold on the pattern scores and apply the pat-
24
Figure 1: Hierarchical topic models. Intertext model
(left) and feature model (right).
terns in a TAC KBP-style evaluation. Although
the surface patterns are very simple (only strings of
tokens), they achieve state-of-the-art extraction re-
sults.
2 Related Work
2.1 At-Least-One Models
The original form of distant supervision (Mintz et
al., 2009) assumes all sentences containing an entity
pair to be potential patterns for the relation holding
between the entities. A variety of models relax this
assumption and only presume that at least one of the
entity pair occurrences is a textual manifestation of
the relation. The first proposed model with an at-
least-one learner is that of Riedel et al (2010) and
Yao et al (2010). It consists of a factor graph that
includes binary variables for contexts, and groups
contexts together for each entity pair. MultiR (Hoff-
mann et al, 2011) can be viewed as a multi-label
extension of (Riedel et al, 2010). A further exten-
sion is MIMLRE (Surdeanu et al, 2012), a jointly
trained two-stage classification model.
2.2 Hierarchical Topic Model
The hierarchical topic model (HierTopics) by Alfon-
seca et al (2012) models the distant supervision data
by a generative model. For each corpus match of an
entity pair in the knowledge base, the corresponding
surface pattern is assumed to be typical for either the
entity pair, the relation, or neither. This principle is
then used to infer distributions over patterns of one
of the following types:
1. For every entity pair, a pair-specific distribu-
tion.
2. For every relation, a relation-specific distribu-
tion.
3. A general background distribution.
The generative process assumes that for each ar-
gument pair in the knowledge base, all patterns
are generated by first choosing a hidden variable z
which can take on three values,B for background,R
for relation and P for pair. Corresponding vocabu-
lary distributions (?bg, ?rel, ?pair) for generating the
context patterns are chosen according to the value of
z. The Dirichlet-smoothed vocabulary distributions
are shared on the respective levels. Figure 1 shows
the plate diagram of the HierTopics model.
3 Model Extensions and Combination
3.1 Generative Model
We use a feature-based extension (Roth and Klakow,
2013) of Alfonseca et al (2012) to include bigrams
for a more fine-grained representation of the pat-
terns. For including features in the model, the model
is extended with a second layer of hidden variables.
A variable x represents a choice of B,R or P for
every pattern, i.e. there is one variable x for every
pattern. Each feature is generated conditioned on
a second variable z ? {B,R, P}, i.e. there are as
many variables z for a pattern as there are features
for it. First, the hidden variable x is generated, then
all z variables are generated for the corresponding
features (see Figure 1). The values B,R or P of z
depend on the corresponding x by a transition distri-
bution:
P (Zi = z|Xj(i) = x) =
{
psame, if z = x
1?psame
2 , otherwise
where features at indices i are mapped to the corre-
sponding pattern indices by a function j(i); psame
is set to .99 to enforce the correspondence between
pattern and feature topics. 1
3.2 Discriminative Model
As a second feature-based model, we employ a per-
ceptron model that enforces constraints on the labels
for patterns (Roth and Klakow, 2013). The model
consists of log-linear factors for the set of relations
1The hyper-parameters used for the feature-based topic
model are ? = (1, 1, 1) and ? = (.1, .001, .001).
25
Algorithm 1 At-Least-One Perceptron Training
? ? 0
for r ? R do
for pair ? kb pairs(r) do
for s ? sentences(pair) do
for r? ? R \ r do
if P (r|s, ?) ? P (r?|s, ?) then
? ? ? + ?(s, r)? ?(s, r?)
if P (NIL|s, ?) ? P (r?|s, ?) then
? ? ? + ?(s,NIL)? ?(s, r?)
if ?s?sentences(pair) : P (r|s, ?) ? P (NIL|s, ?) then
s? = argmaxs
P (r|s,?)
P (NIL|s,?)
? ? ? + ?(s?, r)? ?(s?, NIL)
R as well as a factor for the NIL label (no relation).
Probabilities for a relation r given a sentence pat-
tern s are calculated by normalizing over log-linear
factors defined as fr(s) = exp (
?
i ?i(s, r)?i), with
?(s, r) the feature vector for sentence s and label
assignment r, and ?r the feature weight vector.
The learner is directed by the following se-
mantics: First, for a sentence s that has a distant
supervision match for relation r, relation r should
have a higher probability than any other relation
r? ? R \ r. As extractions are expected to be
noisy, high probabilities for NIL are enforced
by a second constraint: NIL must have a higher
probability than any relation r? ? R \ r. Third, at
least one DS sentence for an argument pair is ex-
pected to express the corresponding relation r. For
sentences s for an entity pair belonging to relation
r, this can be written as the following constraints:
?s,r? : P (r|s) > P (r?|s) ? P (NIL|s) > P (r?|s)
?s : P (r|s) > P (NIL|s)
The violation of any of the above constraints
triggers a perceptron update. The basic algorithm is
sketched in Algorithm 1.2
3.3 Model Combination
The per-pattern probabilities P (r|pat) are calcu-
lated as in Alfonseca et al (2012) and aggregated
over all pattern occurrences: For the topic model,
the number of times the relation-specific topic has
been sampled for a pattern is divided by n(pat), the
number of times the same pattern has been observed.
Analogously for the perceptron, the number of times
a pattern co-occurs with entity pairs for r is multi-
plied by the perceptron score and divided by n(pat).
2The weight vectors are averaged over 20 iterations.
Figure 2: Score combination by non-dominated sorting:
Circles indicate patterns on the Pareto-frontier, which are
ranked highest. They are followed by the triangles, the
square indicates the lowest ranked pattern in this exam-
ple.
For the patterns of the form [ARG1] context
[ARG2], we compute the following scores:
? Maximum Likelihood (MLE):
n(pat,r)
n(pat)
? Topic Model:
n(pat,topic(r))
n(pat)
? Perceptron:
n(pat,r)
n(pat) ?
P (r|s,?)
P (r|s,?)+P (NIL|s,?)
? Interpolation:
0.5?n(pat,topic(r))
n(pat) +
0.5?n(pat,r)?P (r|s,?)
n(pat)?(P (r|s,?)+P (NIL|s,?))
The topic model and perceptron approaches are
based on plausible yet fundamentally different prin-
ciples of modeling noise without direct supervision.
It is therefore an interesting question how comple-
mentary the models are and how much can be gained
from a combination. As the two models do not use
direct supervision, we also avoid tuning parameters
for their combination.
We use two schemes to obtain a combined rank-
ing from the two model scores: The first is a rank-
ing based on non-dominated sorting by successively
computing the Pareto-frontier of the 2-dimensional
score vectors (Borzsony et al, 2001; Godfrey et
al., 2007). The underlying principle is that all data
points (patterns in our case) that are not dominated
by another point3 build the frontier and are ranked
highest (see Figure 2), with ties broken by linear
3A data point h1 dominates a data point h2 if h1 ? h2 in all
metrics and h1 > h2 in at least one metric.
26
combination. Sorting by computing the Pareto-
frontier has been applied to training machine transla-
tion systems (Duh et al, 2012) to combine the trans-
lation quality metrics BLEU, RIBES and NTER,
each of which is based on different principles. In the
context of machine translation it has been found to
outperform a linear interpolation of the metrics and
to be more stable to non-smooth metrics and non-
comparable scalings. We compare non-dominated
sorting with a simple linear interpolation with uni-
form weights.
4 Evaluation
4.1 Ranking-Based Evaluation
Evaluation is done on the ranking quality according
to TAC KBP gold annotations (Ji et al, 2010) of ex-
tracted facts from all TAC KBP queries from 2009-
2011 and the TAC KBP 2009-2011 corpora. First,
candidate sentences are retrieved in which the query
entity and a second entity with the appropriate type
are contained. Candidate sentences are then used
to provide answer candidates if one of the extracted
patterns matches. The answer candidates are ranked
according to the score of the matching pattern.
The basis for pattern extraction is the noisy DS
training data of a top-3 ranked system in TAC KBP
2012 (Roth et al, 2012). The retrieval component
of this system is used to obtain sentence and an-
swer candidates (ranked according to their respec-
tive pattern scores). Evaluation results are reported
as averages over per-relation results of the standard
ranking metrics mean average precision (map), geo-
metric map (gmap), precision at 5 and at 10 (p@5,
p@10).
The maximum-likelihood estimator (MLE) base-
line scores patterns by the relative frequency they
occur with a certain relation. The hierarchical topic
(hier orig) as described in Alfonseca et al (2012)
increases the scores under most metrics, however
the increase is only significant for p@5 and p@10.
The feature-based extension of the topic model
(hier feat) has significantly better ranking quality.
Slightly better scores are obtained by the at-least-
one perceptron learner. It is interesting to see that the
model combinations both by non-dominated sorting
perc+hier (pareto) as well as uniform interpolation
perc+hier (itpl) give a further increase in ranking
method map gmap p@5 p@10
MLE .253 .142 .263 .232
hier orig .270 .158 .353* .297*
hier feature .318?* .205?* .363* .321*
perceptron .330?* .210?* .379* .337*
perc+hier (pareto) .340?* .220?* .400* .340*
perc+hier (itpl) .344?* .220?* .426?* .353?*
Table 1: Ranking quality of extracted facts. Significance
(paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig(?).
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.2  0.4  0.6  0.8  1
Prec
ision
Recall
Interpolated Precision/Recall
MLEhier orighier featperceptronperc+hier (itpl)
Figure 3: Precision at recall levels.
quality. The simpler interpolation scheme gener-
ally works best. Figure 3 shows the Precision/Recall
curves of the basic models and the linear interpola-
tion. On the P/R curve, the linear interpolation is
equal or better than the single methods on all recall
levels.
4.2 End-To-End Evaluation
We evaluate the extraction quality of the induced
perc+hier (itpl) patterns in an end-to-end setting.
We use the evaluation setting of (Surdeanu et al,
2012) and the results obtained with their pipeline for
MIMLRE and their re-implementation of MultiR as
a point of reference.
In Surdeanu et al (2012) evaluation is done us-
ing a subset of queries from the TAC KBP 2010 and
2011 evaluation. The source corpus is the TAC KBP
source corpus and a 2010 Wikipedia dump. Only
those answers are considered in scoring that are con-
tained in a list of possible answers from their can-
didates (reducing the number of gold answers from
1601 to 576 and thereby considerably increasing the
value of reported recall).
For evaluating our patterns, we take the same
27
queries for testing as Surdeanu et al (2012). As the
document collection, we use the TAC KBP source
collection and a Wikipedia dump from 07/2009 that
was available to us. From this document collec-
tion, we use our retrieval pipeline of Roth et al
(2012) and take those sentences that contain query
entities and slot filler candidates according to NE-
tags. We filter out all candidates that are not con-
tained in the list of candidates considered in (Sur-
deanu et al, 2012), and use the same reduced set
of 576 gold answers as the key. We tune a single
threshold parameter t = .3 on held-out development
data and take all patterns with higher scores. Ta-
ble 2 shows that results obtained with the induced
patterns compare well with state-of-the-art relation
extraction systems.
method Recall Precision F1
MultiR .200 .306 .242
MIMLRE .314 .247 .277
perc+hier (itpl) .248 .401 .307
Table 2: TAC Scores on (Surdeanu et al, 2012) queries.
4.3 Illustration: Top-Ranked Patterns
Figure 4 shows top-ranked patterns for per:title
and org:top members employees, the two rela-
tions with most answers in the gold annotations. For
maximum likelihood estimation the score is 1.0 if
the patterns occurs only with the relation in question
? this includes all cases where the pattern is only
found once in the corpus. While this could be cir-
cumvented by frequency thresholding, we leave the
long tail of the data as it is and let the algorithm deal
with both frequent and infrequent patterns.
One can see that while the maximum likelihood
patterns contain some reasonable relational con-
texts, they are less prototypical and more prone to
distant supervision errors. The patterns scored high
by the proposed combination generalize better, vari-
ation at the top is achieved by re-combining ele-
ments that carry relational meaning (?is an?, ?vice
president?, ?president director?) or are closely cor-
related to the particular relation.
5 Conclusion
We have combined two models based on distinct
principles for noise reduction in distant supervision:
per:title, MLE
[ARG1] , a singing [ARG2]
*[ARG1] Best film : Capote ( as [ARG2]
[ARG1] Nunn ( born October 7 , 1957 in Little Rock , Arkansas
) is an American jazz [ARG2]
*[ARG2] Kevin Weekes , subbing for a rarely rested [ARG1]
[ARG1] Butterfill FRICS ( born February 14 , 1941 , Surrey ) is
a British [ARG2]
per:title, perc+hier (itpl)
[ARG1] , is a Canadian [ARG2]
[ARG1] Hilligoss is an American [ARG2]
[ARG1] , is an American film [ARG2]
[ARG1] , is an American film and television [ARG2]
*[ARG1] for Best [ARG2]
org:top members employees, MLE
[ARG2] remained chairman of [ARG1]
*[ARG2] asks the ball whether he and [ARG1]
[ARG2] was chairman of the [ARG1]
*[ARG1] , Joe Lieberman and [ARG2]
*[ARG1] ?s responsibility to pin down just how the government
decided to front $ 30 billion in taxpayer dollars for the Bear
Stearns deal , ? Chairman [ARG2]
org:top members employees, perc+hier (itpl)
[ARG2] , Vice President of the [ARG1]
[ARG1] Vice president [ARG2]
[ARG1] president director [ARG2]
[ARG1] vice president director [ARG2]
[ARG1] Board member [ARG2]
Figure 4: Top-scored patterns for maximum likelihood
(MLE) and the interpolation (perc+hier itpl) method. In-
exact patterns are marked by *.
a feature-based extension of a hierarchical topic
model, and an at-least-one perceptron. Interpola-
tion increases the quality of extractions and achieves
state-of-the-art extraction performance. A combina-
tion scheme based on non-dominated sorting, that
was inspired by work on combining machine trans-
lation metrics, was not as good as a simple linear
combination of scores. We think that the good re-
sults motivate research into more integrated combi-
nations of noise reduction approaches.
Acknowledgment
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and this
research is supported in part by this Google Fellow-
ship.
28
References
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort,
and Guillermo Garrido. 2012. Pattern learning for
relation extraction with a hierarchical topic model. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short Papers-
Volume 2, pages 54?59. Association for Computa-
tional Linguistics.
S Borzsony, Donald Kossmann, and Konrad Stocker.
2001. The skyline operator. In Data Engineering,
2001. Proceedings. 17th International Conference on,
pages 421?430. IEEE.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1, pages
1?10. Association for Computational Linguistics.
Parke Godfrey, Ryan Shipley, and Jarek Gryz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. The VLDB JournalThe International Journal on
Very Large Data Bases, 16(1):5?28.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 541?550.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Third Text Anal-
ysis Conference (TAC 2010).
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume 2,
pages 1003?1011. Association for Computational Lin-
guistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowledge
Discovery in Databases, pages 148?163. Springer.
Benjamin Roth and Dietrich Klakow. 2013. Feature-
based models for improving the quality of noisy train-
ing data for relation extraction. In Proceedings of the
22nd ACM International Conference on Information
and Knowledge Management (CIKM). ACM.
Benjamin Roth, Grzegorz Chrupala, Michael Wiegand,
Mittul Singh, and Dietrich Klakow. 2012. General-
izing from freebase and patterns using distant supervi-
sion for slot filling. In Proceedings of the Text Analysis
Conference (TAC).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 455?465. Associa-
tion for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013?1023. Association for Com-
putational Linguistics.
29
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 325?335,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Generalization Methods for In-Domain and Cross-Domain Opinion
Holder Extraction
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
In this paper, we compare three different
generalization methods for in-domain and
cross-domain opinion holder extraction be-
ing simple unsupervised word clustering,
an induction method inspired by distant
supervision and the usage of lexical re-
sources. The generalization methods are
incorporated into diverse classifiers. We
show that generalization causes significant
improvements and that the impact of im-
provement depends on the type of classifier
and on how much training and test data dif-
fer from each other. We also address the
less common case of opinion holders being
realized in patient position and suggest ap-
proaches including a novel (linguistically-
informed) extraction method how to detect
those opinion holders without labeled train-
ing data as standard datasets contain too
few instances of this type.
1 Introduction
Opinion holder extraction is one of the most im-
portant subtasks in sentiment analysis. The ex-
traction of sources of opinions is an essential com-
ponent for complex real-life applications, such
as opinion question answering systems or opin-
ion summarization systems (Stoyanov and Cardie,
2011). Common approaches designed to extract
opinion holders are based on data-driven methods,
in particular supervised learning.
In this paper, we examine the role of general-
ization for opinion holder extraction in both in-
domain and cross-domain classification. General-
ization may not only help to compensate the avail-
ability of labeled training data but also conciliate
domain mismatches.
In order to illustrate this, compare for instance
(1) and (2).
(1) Malaysia did not agree to such treatment of Al-Qaeda sol-
diers as they were prisoners-of-war and should be accorded
treatment as provided for under the Geneva Convention.
(2) Japan wishes to build a $21 billion per year aerospace indus-
try centered on commercial satellite development.
Though both sentences contain an opinion
holder, the lexical items vary considerably. How-
ever, if the two sentences are compared on the ba-
sis of some higher level patterns, some similari-
ties become obvious. In both cases the opinion
holder is an entity denoting a person and this en-
tity is an agent1 of some predictive predicate (i.e.
agree in (1) and wishes in (2)), more specifically,
an expression that indicates that the agent utters a
subjective statement. Generalization methods ide-
ally capture these patterns, for instance, they may
provide a domain-independent lexicon for those
predicates. In some cases, even higher order fea-
tures, such as certain syntactic constructions may
vary throughout the different domains. In (1) and
(2), the opinion holders are agents of a predictive
predicate, whereas the opinion holder her daugh-
ters in (3) is a patient2 of embarrasses.
(3) Mrs. Bennet does what she can to get Jane and Bingley to-
gether and embarrasses her daughters by doing so.
If only sentences, such as (1) and (2), occur in
the training data, a classifier will not correctly ex-
tract the opinion holder in (3), unless it obtains
additional knowledge as to which predicates take
opinion holders as patients.
1By agent we always mean constituents being labeled as
A0 in PropBank (Kingsbury and Palmer, 2002).
2By patient we always mean constituents being labeled
as A1 in PropBank.
325
In this work, we will consider three differ-
ent generalization methods being simple unsuper-
vised word clustering, an induction method and
the usage of lexical resources. We show that gen-
eralization causes significant improvements and
that the impact of improvement depends on how
much training and test data differ from each other.
We also address the issue of opinion holders in
patient position and present methods including a
novel extraction method to detect these opinion
holders without any labeled training data as stan-
dard datasets contain too few instances of them.
In the context of generalization it is also impor-
tant to consider different classification methods
as the incorporation of generalization may have a
varying impact depending on how robust the clas-
sifier is by itself, i.e. how well it generalizes even
with a standard feature set. We compare two state-
of-the-art learning methods, conditional random
fields and convolution kernels, and a rule-based
method.
2 Data
As a labeled dataset we mainly use the MPQA
2.0 corpus (Wiebe et al 2005). We adhere to
the definition of opinion holders from previous
work (Wiegand and Klakow, 2010; Wiegand and
Klakow, 2011a; Wiegand and Klakow, 2011b),
i.e. every source of a private state or a subjective
speech event (Wiebe et al 2005) is considered an
opinion holder.
This corpus contains almost exclusively news
texts. In order to divide it into different domains,
we use the topic labels from (Stoyanov et al
2004). By inspecting those topics, we found that
many of them can grouped to a cluster of news
items discussing human rights issues mostly in
the context of combating global terrorism. This
means that there is little point in considering every
single topic as a distinct (sub)domain and, there-
fore, we consider this cluster as one single domain
ETHICS.3 For our cross-domain evaluation, we
want to have another topic that is fairly different
from this set of documents. By visual inspection,
we found that the topic discussing issues regard-
ing the International Space Station would suit our
purpose. It is henceforth called SPACE.
3The cluster is the union of documents with the following
MPQA-topic labels: axisofevil, guantanamo, humanrights,
mugabe and settlements.
Domain # Sentences # Holders in sentence (average)
ETHICS 5700 0.79
SPACE 628 0.28
FICTION 614 1.49
Table 1: Statistics of the different domain corpora.
In addition to these two (sub)domains, we
chose some text type that is not even news text
in order to have a very distant domain. There-
fore, we had to use some text not included in the
MPQA corpus. Existing text collections contain-
ing product reviews (Kessler et al 2010; Toprak
et al 2010), which are generally a popular re-
source for sentiment analysis, were not found
suitable as they only contain few distinct opinion
holders. We finally used a few summaries of fic-
tional work (two Shakespeare plays and one novel
by Jane Austen4) since their language is notably
different from that of news texts and they con-
tain a large number of different opinion holders
(therefore opinion holder extraction is a meaning-
ful task on this text type). These texts make up
our third domain FICTION. We manually labeled
it with opinion holder information by applying the
annotation scheme of the MPQA corpus.
Table 1 lists the properties of the different do-
main corpora. Note that ETHICS is the largest do-
main. We consider it our primary (source) domain
as it serves both as a training and (in-domain) test
set. Due to their size, the other domains only
serve as test sets (target domains).
For some of our generalization methods, we
also need a large unlabeled corpus. We use the
North American News Text Corpus (LDC95T21).
3 The Different Types of Generalization
3.1 Word Clustering (Clus)
The simplest generalization method that is con-
sidered in this paper is word clustering. By that,
we understand the automatic grouping of words
occurring in similar contexts. Such clusters are
usually computed on a large unlabeled corpus.
Unlike lexical features, features based on clusters
are less sparse and have been proven to signif-
icantly improve data-driven classifiers in related
tasks, such as named-entity recognition (Turian et
4available at: www.absoluteshakespeare.com/
guides/{othello|twelfth night}/summary/
{othello|twelfth night} summary.htm
www.wikisummaries.org/Pride and Prejudice
326
I. Madrid, Dresden, Bordeaux, Istanbul, Caracas, Manila, ...
II. Toby, Betsy, Michele, Tim, Jean-Marie, Rory, Andrew, ...
III. detest, resent, imply, liken, indicate, suggest, owe, expect, ...
IV. disappointment, unease, nervousness, dismay, optimism, ...
V. remark, baby, book, saint, manhole, maxim, coin, batter, ...
Table 2: Some automatically induced clusters.
ETHICS SPACE FICTION
1.47 2.70 11.59
Table 3: Percentage of opinion holders as patients.
al., 2010). Such a generalization is, in particular,
attractive as it is cheaply produced. As a state-
of-the-art clustering method, we consider Brown
clustering (Brown et al 1992) as implemented in
the SRILM-toolkit (Stolcke, 2002). We induced
1000 clusters which is also the configuration used
in (Turian et al 2010).5
Table 2 illustrates a few of the clusters induced
from our unlabeled dataset introduced in Section
(?) 2. Some of these clusters represent location
or person names (e.g. I. & II.). This exempli-
fies why clustering is effective for named-entity
recognition. We also find clusters that intuitively
seem to be meaningful for our task (e.g. III. &
IV.) but, on the other hand, there are clusters that
contain words that with the exception of their part
of speech do not have anything in common (e.g.
V.).
3.2 Manually Compiled Lexicons (Lex)
The major shortcoming of word clustering is that
it lacks any task-specific knowledge. The oppo-
site type of generalization is the usage of manu-
ally compiled lexicons comprising predicates that
indicate the presence of opinion holders, such as
supported, worries or disappointed in (4)-(6).
(4) I always supported this idea. holder:agent.
(5) This worries me. holder:patient
(6) He disappointed me. holder:patient
We follow Wiegand and Klakow (2011b) who
found that those predicates can be best obtained
by using a subset of Levin?s verb classes (Levin,
1993) and the strong subjective expressions of the
Subjectivity Lexicon (Wilson et al 2005). For
those predicates it is also important to consider
in which argument position they usually take an
opinion holder. Bethard et al(2004) found the
5We also experimented with other sizes but they did not
produce a better overall performance.
majority of holders are agents (4). A certain
number of predicates, however, also have opinion
holders in patient position, e.g. (5) and (6).
Wiegand and Klakow (2011b) found that many
of those latter predicates are listed in one of
Levin?s verb classes called amuse verbs. While
on the evaluation on the entire MPQA corpus,
opinion holders in patient position are fairly rare
(Wiegand and Klakow, 2011b), we may wonder
whether the same applies to the individual do-
mains that we consider in this work. Table 3
lists the proportion of those opinion holders (com-
puted manually) based on a random sample of 100
opinion holder mentions from those corpora. The
table shows indeed that on the domains from the
MPQA corpus, i.e. ETHICS and SPACE, those
opinion holders play a minor role but there is a no-
tably higher proportion on the FICTION-domain.
3.3 Task-Specific Lexicon Induction (Induc)
3.3.1 Distant Supervision with Prototypical
Opinion Holders
Lexical resources are potentially much more
expressive than word clustering. This knowledge,
however, is usually manually compiled, which
makes this solution much more expensive. Wie-
gand and Klakow (2011a) present an intermedi-
ate solution for opinion holder extraction inspired
by distant supervision (Mintz et al 2009). The
output of that method is also a lexicon of predi-
cates but it is automatically extracted from a large
unlabeled corpus. This is achieved by collecting
predicates that frequently co-occur with prototyp-
ical opinion holders, i.e. common nouns such as
opponents (7) or critics (8), if they are an agent
of that predicate. The rationale behind this is
that those nouns act very much like actual opin-
ion holders and therefore can be seen as a proxy.
(7) Opponents say these arguments miss the point.
(8) Critics argued that the proposed limits were unconstitutional.
This method reduces the human effort to specify-
ing a small set of such prototypes.
Following the best configuration reported
in (Wiegand and Klakow, 2011a), we extract 250
verbs, 100 nouns and 100 adjectives from our un-
labeled corpus (?2).
3.3.2 Extension for Opinion Holders in
Patient Position
The downside of using prototypical opinion
holders as a proxy for opinion holders is that it
327
anguish?, astonish, astound, concern, convince, daze, delight,
disenchant?, disappoint, displease, disgust, disillusion, dissat-
isfy, distress, embitter?, enamor?, engross, enrage, entangle?,
excite, fatigue?, flatter, fluster, flummox?, frazzle?, hook?, hu-
miliate, incapacitate?, incense, interest, irritate, obsess, outrage,
perturb, petrify?, sadden, sedate?, shock, stun, tether?, trouble
Table 4: Examples of the automatically extracted verbs
taking opinion holders as patients (?: not listed as
amuse verb).
is limited to agentive opinion holders. Opinion
holders in patient position, such as the ones taken
by amuse verbs in (5) and (6), are not covered.
Wiegand and Klakow (2011a) show that consid-
ering less restrictive contexts significantly drops
classification performance. So the natural exten-
sion of looking for predicates having prototypical
opinion holders in patient position is not effective.
Sentences, such as (9), would mar the result.
(9) They criticized their opponents.
In (9) the prototypical opinion holder opponents
(in the patient position) is not a true opinion
holder.
Our novel method to extract those predicates
rests on the observation that the past participle of
those verbs, such as shocked in (10), is very often
identical to some predicate adjective (11) having
a similar if not identical meaning. For the predi-
cate adjective, the opinion holder is, however, its
subject/agent and not its patient.
(10) He had shockedverb me. holder:patient
(11) I was shockedadj . holder:agent
Instead of extracting those verbs directly (10),
we take the detour via their corresponding pred-
icate adjectives (11). This means that we collect
all those verbs (from our large unlabeled corpus
(?2)) for which there is a predicate adjective that
coincides with the past participle of the verb.
To increase the likelihood that our extracted
predicates are meaningful for opinion holder ex-
traction, we also need to check the semantic type
in the relevant argument position, i.e. make sure
that the agent of the predicate adjective (which
would be the patient of the corresponding verb)
is an entity likely to be an opinion holder. Our
initial attempts with prototypical opinion holders
were too restrictive, i.e. the number of prototyp-
ical opinion holders co-occurring with those ad-
jectives was too small. Therefore, we widen the
semantic type of this position from prototypical
opinion holders to persons. This means that we
allow personal pronouns (i.e. I, you, he, she and
we) to appear in this position. We believe that this
relaxation can be done in that particular case, as
adjectives are much more likely to convey opin-
ions a priori than verbs (Wiebe et al 2004).
An intrinsic evaluation of the predicates that we
thus extracted from our unlabeled corpus is dif-
ficult. The 250 most frequent verbs exhibiting
this special property of coinciding with adjectives
(this will be the list that we use in our experi-
ments) contains 42% entries of the amuse verbs
(?3.2). However, we also found many other po-
tentially useful predicates on this list that are not
listed as amuse verbs (Table 4). As amuse verbs
cannot be considered a complete golden standard
for all predicates taking opinion holders as pa-
tients, we will focus on a task-based evaluation
of our automatically extracted list (?6).
4 Data-driven Methods
In the following, we present the two supervised
classifiers we use in our experiments. Both clas-
sifiers incorporate the same levels of representa-
tions, including the same generalization methods.
4.1 Conditional Random Fields (CRF)
The supervised classifier most frequently used
for information extraction tasks, in general, are
conditional random fields (CRF) (Lafferty et al
2001). Using CRF, the task of opinion holder ex-
traction is framed as a tagging problem in which
given a sequence of observations x = x1x2 . . . xn
(words in a sentence) a sequence of output tags
y = y1y2 . . . yn indicating the boundaries of opin-
ion holders is computed by modeling the condi-
tional probability P (x|y).
The features we use (Table 5) are mostly in-
spired by Choi et al(2005) and by the ones
used for plain support vector machines (SVMs)
in (Wiegand and Klakow, 2010). They are orga-
nized into groups. The basic group Plain does not
contain any generalization method. Each other
group is dedicated to one specific generalization
method that we want to examine (Clus, Induc
and Lex). Apart from considering generalization
features indicating the presence of generalization
types, we also consider those types in conjunction
with semantic roles. As already indicated above,
semantic roles are especially important for the de-
tection of opinion holders. Unfortunately, the cor-
328
Group Features
Plain
Token features: unigrams and bigrams
POS/chunk/named-entity features: unigrams, bi-
grams and trigrams
Constituency tree path to nearest predicate
Nearest predicate
Semantic role to predicate+lexical form of predicate
Clus
Cluster features: unigrams, bigrams and trigrams
Semantic role to predicate+cluster-id of predicate
Cluster-id of nearest predicate
Induc
Is there predicate from induced lexicon within win-
dow of 5 tokens?
Semantic role to predicate, if predicate is contained in
induced lexicon
Is nearest predicate contained in induced lexicon?
Lex
Is there predicate from manually compiled lexicons
within window of 5 tokens?
Semantic role to predicate, if predicate is contained in
manually compiled lexicons
Is nearest predicate contained in manually compiled
lexicons?
Table 5: Feature set for CRF.
responding feature from the Plain feature group
that also includes the lexical form of the predicate
is most likely a sparse feature. For the opinion
holder me in (10), for example, it would corre-
spond to A1 shock. Therefore, we introduce for
each generalization method an additional feature
replacing the sparse lexical item by a generaliza-
tion label, i.e. Clus: A1 CLUSTER-35265, Induc:
A1 INDUC-PRED and Lex: A1 LEX-PRED.6
For this learning method, we use CRF++.7 We
choose a configuration that provides good perfor-
mance on our source domain (i.e. ETHICS).8
For semantic role labeling we use SWIRL9, for
chunk parsing CASS (Abney, 1991) and for con-
stituency parsing Stanford Parser (Klein and Man-
ning, 2003). Named-entity information is pro-
vided by Stanford Tagger (Finkel et al 2005).
4.2 Convolution Kernels (CK)
Convolution kernels (CK) are special kernel func-
tions. A kernel function K : X ? X ? R com-
putes the similarity of two data instances xi and
xj (xi ? xj ? X). It is mostly used in SVMs that
estimate a hyperplane to separate data instances
from different classes H(~x) = ~w ? ~x + b = 0
where w ? Rn and b ? R (Joachims, 1999). In
6Predicates in patient position are given the same gener-
alization label as the predicates in agent position. Specially
marking them did not result in a notable improvement.
7http://crfpp.sourceforge.net
8The soft margin parameter ?c is set to 1.0 and all fea-
tures occurring less than 3 times are removed.
9http://www.surdeanu.name/mihai/swirl
convolution kernels, the structures to be compared
within the kernel function are not vectors com-
prising manually designed features but the under-
lying discrete structures, such as syntactic parse
trees or part-of-speech sequences. Since they are
directly provided to the learning algorithm, a clas-
sifier can be built without taking the effort of im-
plementing an explicit feature extraction.
We take the best configuration from (Wiegand
and Klakow, 2010) that comprises a combination
of three different tree kernels being two tree ker-
nels based on constituency parse trees (one with
predicate and another with semantic scope) and
a tree kernel encoding predicate-argument struc-
tures based on semantic role information. These
representations are illustrated in Figure 1. The re-
sulting kernels are combined by plain summation.
In order to integrate our generalization meth-
ods into the convolution kernels, the input struc-
tures, i.e. the linguistic tree structures, have to be
augmented. For that we just add additional nodes
whose labels correspond to the respective gener-
alization types (i.e. Clus: CLUSTER-ID, Induc:
INDUC-PRED and Lex: LEX-PRED). The nodes
are added in such a way that they (directly) domi-
nate the leaf node for which they provide a gener-
alization.10 If several generalization methods are
used and several of them apply for the same lex-
ical unit, then the (vertical) order of the general-
ization nodes is LEX-PRED  INDUC-PRED 
CLUSTER-ID.11 Figure 2 illustrates the predi-
cate argument structure from Figure 1 augmented
with INDUC-PRED and CLUSTER-IDs.
For this learning method, we use the
SVMLight-TK toolkit.12 Again, we tune the
parameters to our source domain (ETHICS).13
5 Rule-based Classifiers (RB)
Finally, we also consider rule-based classifiers
(RB). The main difference towards CRF and CK
is that it is an unsupervised approach not requiring
training data. We re-use the framework by Wie-
gand and Klakow (2011b). The candidate set are
all noun phrases in a test set. A candidate is clas-
sified as an opinion holder if all of the following
10Note that even for the configuration Plain the trees are
already augmented with named-entity information.
11We chose this order as it roughly corresponds to the
specificity of those generalization types.
12disi.unitn.it/moschitti
13The cost parameter?j (Morik et al 1999) was set to 5.
329
Figure 1: The different structures (left: constituency trees, right: predicate argument structure) derived from
Sentence (1) for the opinion holder candidate Malaysia used as input for convolution kernels (CK).
Figure 2: Predicate argument structure augmented
with generalization nodes.
conditions hold:
? The candidate denotes a person or group of persons.
? There is a predictive predicate in the same sentence.
? The candidate has a pre-specified semantic role in the event
that the predictive predicate evokes (default: agent-role).
The set of predicates is obtained from a given lex-
icon. For predicates that take opinion holders as
patients, the default agent-role is overruled.
We consider several classifiers that differ in the
lexicon they use. RB-Lex uses the combination of
the manually compiled lexicons presented in ?3.2.
RB-Induc uses the predicates that have been au-
tomatically extracted from a large unlabeled cor-
pus using the methods presented in ?3.3. RB-
Induc+Lex considers the union of those lexicons.
In order to examine the impact of modeling opin-
ion holders in patient position, we also introduce
two versions of each lexicon. AG just consid-
ers predicates in agentive position while AG+PT
also considers predicates that take opinion hold-
ers as patients. For example, RB-InducAG+PT
is a classifier that uses automatically extracted
predicates in order to detect opinion holders in
both agent and patient argument position, i.e.
RB-InducAG+PT also covers our novel extraction
method for patients (?3.3.2).
The output of clustering will exclusively be
evaluated in the context of learning-based meth-
Features
Induc Lex Induc+Lex
Domains AG AG+PT AG AG+PT AG+PT
ETHICS 50.77 50.99 52.22 52.27 53.07
SPACE 45.81 46.55 47.60 48.47 45.20
FICTION 46.59 49.97 54.84 59.35 63.11
Table 6: F-score of the different rule-based classifiers.
ods, since there is no straightforward way of in-
corporating this output into a rule-based classifier.
6 Experiments
CK and RB have an instance space that is differ-
ent from the one of CRF. While CRF produces
a prediction for every word token in a sentence,
CK and RB only produce a prediction for every
noun phrase. For evaluation, we project the pre-
dictions from RB and CK to word token level in
order to ensure comparability. We evaluate the se-
quential output with precision, recall and F-score
as defined in (Johansson and Moschitti, 2010; Jo-
hansson and Moschitti, 2011).
6.1 Rule-based Classifier
Table 6 shows the cross-domain performance of
the different rule-based classifiers. RB-Lex per-
forms better than RB-Induc. In comparison to the
domains ETHICS and SPACE the difference is
larger on FICTION. Presumably, this is due to the
fact that the predicates in Induc are extracted from
a news corpus (?2). Thus, Induc may slightly suf-
fer from a domain mismatch. A combination of
the two classifiers, i.e. RB-Lex+Induc, results in
a notable improvement in the FICTION-domain.
The approaches that also detect opinion holders as
patients (AG+PT) including our novel approach
(?3.3.2) are effective. A notable improvement can
330
Training Size (%)
Features Alg. 5 10 20 50 100
Plain
CRF 32.14 35.24 41.03 51.05 55.13
CK 42.15 46.34 51.14 56.39 59.52
+Clus
CRF 33.06 37.11 43.47 52.05 56.18
CK 42.02 45.86 51.11 56.59 59.77
+Induc
CRF 37.28 42.31 46.54 54.27 56.71
CK 46.26 49.35 53.26 57.28 60.42
+Lex
CRF 40.69 43.91 48.43 55.37 58.46
CK 46.45 50.59 53.93 58.63 61.50
+Clus+Induc
CRF 37.27 42.19 47.35 54.95 57.14
CK 45.14 48.20 52.39 57.37 59.97
+Clus+Lex
CRF 40.52 44.29 49.32 55.44 58.80
CK 45.89 49.35 53.56 58.74 61.43
+Lex+Induc
CRF 42.23 45.92 49.96 55.61 58.40
CK 47.46 51.44 54.80 58.74 61.58
All
CRF 41.56 45.75 50.39 56.24 59.08
CK 46.18 50.10 54.04 58.92 61.44
Table 7: F-score of in-domain (ETHICS) learning-
based classifiers.
only be measured on the FICTION-domain since
this is the only domain with a significant propor-
tion of those opinion holders (Table 3).
6.2 In-Domain Evaluation of
Learning-based Methods
Table 7 shows the performance of the learning-
based methods CRF and CK on an in-domain
evaluation (ETHICS-domain) using different
amounts of labeled training data. We carry out
a 5-fold cross-validation and use n% of the train-
ing data in the training folds. The table shows that
CK is more robust than CRF. The fewer training
data are used the more important generalization
becomes. CRF benefits much more from gener-
alization than CK. Interestingly, the CRF config-
uration with the best generalization is usually as
good as plain CK. This proves the effectiveness
of CK. In principle, Lex is the strongest general-
ization method while Clus is by far the weakest.
For Clus, systematic improvements towards no
generalization (even though they are minor) can
only be observed with CRF. As far as combina-
tions are concerned, either Lex+Induc or All per-
forms best. This in-domain evaluation proves that
opinion holder extraction is different from named-
entity recognition. Simple unsupervised general-
ization, such as word clustering, is not effective
and popular sequential classifiers are less robust
than margin-based tree-kernels.
Table 8 complements Table 7 in that it com-
pares the learning-based methods with the best
rule-based classifier and also displays precision
and recall. RB achieves a high recall, whereas the
learning-based methods always excel RB in pre-
cision.14 Applying generalization to the learning-
based methods results in an improvement of both
recall and precision if few training data are used.
The impact on precision decreases, however, the
more training data are added. There is always a
significant increase in recall but learning-based
methods may not reach the level of RB even
though they use the same resources. This is a
side-effect of preserving a much higher precision.
It also explains why learning-based methods with
generalization may have a lower F-score than RB.
6.3 Out-of-Domain Evaluation of
Learning-based Methods
Table 9 presents the results of out-of-domain clas-
sifiers. The complete ETHICS-dataset is used for
training. Some properties are similar to the pre-
vious experiments: CK always outperforms CRF.
RB provides a high recall whereas the learning-
based methods maintain a higher precision. Sim-
ilar to the in-domain setting using few labeled
training data, the incorporation of generalization
increases both precision and recall. Moreover, a
combination of generalization methods is better
than just using one method on average, although
Lex is again a fairly robust individual generaliza-
tion method. Generalization is more effective in
this setting than on the in-domain evaluation us-
ing all training data, in particular for CK, since
the training and test data are much more different
from each other and suitable generalization meth-
ods partly close that gap.
There is a notable difference in precision be-
tween the SPACE- and FICTION-domain (and
also the source domain ETHICS (Table 8)). We
strongly assume that this is due to the distribu-
tion of opinion holders in those datasets (Table 1).
The FICTION-domain contains much more opin-
ion holders, therefore the chance that a predicted
opinion holder is correct is much higher.
With regard to recall, a similar level of per-
formance as in the ETHICS-domain can only be
achieved in the SPACE-domain, i.e. CK achieves
a recall of 60%. In the FICTION-domain, how-
ever, the recall is much lower (best recall of CK
is below 47%). This is no surprise as the SPACE-
domain is more similar to the source domain than
14The reason for RB having a high recall is extensively
discussed in (Wiegand and Klakow, 2011b).
331
the FICTION-domain since ETHICS and SPACE
are news texts. FICTION contains more out-of-
domain language. Therefore, RB (which exclu-
sively uses domain-independent knowledge) out-
performs both learning-based methods including
the ones incorporating generalization. Similar re-
sults have been observed for rule-based classifiers
from other tasks in cross-domain sentiment anal-
ysis, such as subjectivity detection and polarity
classification. High-level information as it is en-
coded in a rule-based classifier generalizes better
than learning-based methods (Andreevskaia and
Bergler, 2008; Lambov et al 2009).
We set up another experiment exclusively for
the FICTION-domain in which we combine the
output of our best learning-based method, i.e. CK,
with the prediction of a rule-based classifier. The
combined classifier will predict an opinion holder,
if either classifier predicts one. The motivation for
this is the following: The FICTION-domain is the
only domain to have a significant proportion of
opinion holders appearing as patients. We want
to know how much of them can be recognized
with the best out-of-domain classifier using train-
ing data with only very few instances of this type
and what benefit the addition of using various RBs
which have a clearer notion of these constructions
brings about. Moreover, we already observed that
the learning-based methods have a bias towards
preserving a high precision and this may have as
a consequence that the generalization features in-
corporated into CK will not receive sufficiently
large weights. Unlike the SPACE-domain where
a sufficiently high recall is already achieved with
CK (presumably due to its stronger similarity to-
wards the source domain) the FICTION-domain
may be more severely affected by this bias and
evidence from RB may compensate for this.
Table 10 shows the performance of those com-
bined classifiers. For all generalization types
considered, there is, indeed, an improvement by
adding information from RB resulting in a large
boost in recall. Already the application of our in-
duction approach Induc results in an increase of
more than 8% points compared to plain CK. The
table also shows that there is always some im-
provement if RB considers opinion holders as pa-
tients (AG+PT). This can be considered as some
evidence that (given the available data we use)
opinion holders in patient position can only be ef-
fectively extracted with the help of RBs. It is also
CRF CK
Size Feat. Prec Rec F1 Prec Rec F1
10
Plain 52.17 26.61 35.24 58.26 38.47 46.34
All 62.85 35.96 45.75 63.18 41.50 50.10
50
Plain 59.85 44.50 51.05 59.60 53.50 56.39
All 62.99 50.80 56.24 61.91 56.20 58.92
100
Plain 64.14 48.33 55.13 62.38 56.91 59.52
All 64.75 54.32 59.08 63.81 59.24 61.44
RB 47.38 60.32 53.07 47.38 60.32 53.07
Table 8: Comparison of best RB with learning-based
approaches on in-domain classification.
Algorithms Generalization Prec Rec F
CK (Plain) 66.90 41.48 51.21
CK Induc 67.06 45.15 53.97
CK+RBAG Induc 60.22 54.52 57.23
CK+RBAG+PT Induc 61.09 58.14 59.58
CK Lex 69.45 46.65 55.81
CK+RBAG Lex 67.36 59.02 62.91
CK+RBAG+PT Lex 68.25 63.28 65.67
CK Induc+Lex 69.73 46.17 55.55
CK+RBAG Induc+Lex 61.41 65.56 63.42
CK+RBAG+PT Induc+Lex 62.26 70.56 66.15
Table 10: Combination of out-of-domain CK and rule-
based classifiers on FICTION (i.e. distant domain).
further evidence that our novel approach to extract
those predicates (?3.3.2) is effective.
The combined approach in Table 10 not only
outperforms CK (discussed above) but also RB
(Table 6). We manually inspected the output of
the classifiers to find also cases in which CK de-
tect opinion holders that RB misses. CK has the
advantage that it is not only bound to the relation-
ship between candidate holder and predicate. It
learns further heuristics, e.g. that sentence-initial
mentions of persons are likely opinion holders. In
(12), for example, this heuristics fires while RB
overlooks this instance as to give someone a share
of advice is not part of the lexicon.
(12) She later gives Charlotte her share of advice on running a
household.
7 Related Work
The research on opinion holder extraction has
been focusing on applying different data-driven
approaches. Choi et al(2005) and Choi et al
(2006) explore conditional random fields, Wie-
gand and Klakow (2010) examine different com-
binations of convolution kernels, while Johans-
son and Moschitti (2010) present a re-ranking ap-
proach modeling complex relations between mul-
tiple opinions in a sentence. A comparison of
332
SPACE (similar target domain) FICTION (distant target domain)
CRF CK CRF CK
Features Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Plain 47.32 48.62 47.96 45.89 57.07 50.87 68.58 28.96 40.73 66.90 41.48 51.21
+Clus 49.00 48.62 48.81 49.23 57.64 53.10 71.85 32.21 44.48 67.54 41.21 51.19
+Induc 42.92 49.15 45.82 46.66 60.45 52.67 71.59 34.77 46.80 67.06 45.15 53.97
+Lex 49.65 49.07 49.36 49.60 59.88 54.26 71.91 35.83 47.83 69.45 46.65 55.81
+Clus+Induc 46.61 48.78 47.67 48.65 58.20 53.00 71.32 35.88 47.74 67.46 42.17 51.90
+Lex+Induc 48.75 50.87 49.78 49.92 58.76 53.98 74.02 37.37 49.67 69.73 46.17 55.55
+Clus+Lex 49.72 50.87 50.29 53.70 59.32 56.37 73.41 37.15 49.33 70.59 43.98 54.20
All 49.87 51.03 50.44 51.68 58.76 54.99 72.00 37.44 49.26 70.61 44.83 54.84
best RB 41.72 57.80 48.47 41.72 57.80 48.47 63.26 62.96 63.11 63.26 62.96 63.11
Table 9: Comparison of best RB with learning-based approaches on out-of-domain classification.
those methods has not yet been attempted. In
this work, we compare the popular state-of-the-art
learning algorithms conditional random fields and
convolution kernels for the first time. All these
data-driven methods have been evaluated on the
MPQA corpus. Some generalization methods are
incorporated but unlike this paper they are neither
systematically compared nor combined. The role
of resources that provide the knowledge of argu-
ment positions of opinion holders is not covered
in any of these works. This kind of knowledge
should be directly learnt from the labeled train-
ing data. In this work, we found, however, that
the distribution of argument positions of opinion
holders varies throughout the different domains
and, therefore, cannot be learnt from any arbitrary
out-of-domain training set.
Bethard et al(2004) and Kim and Hovy (2006)
explore the usefulness of semantic roles provided
by FrameNet (Fillmore et al 2003). Bethard
et al(2004) use this resource to acquire labeled
training data while in (Kim and Hovy, 2006)
FrameNet is used within a rule-based classifier
mapping frame-elements of frames to opinion
holders. Bethard et al(2004) only evaluate on an
artificial dataset (i.e. a subset of sentences from
FrameNet and PropBank (Kingsbury and Palmer,
2002)). The only realistic test set on which Kim
and Hovy (2006) evaluate their approach are news
texts. Their method is compared against a sim-
ple rule-based baseline and, unlike this work, not
against a robust data-driven algorithm.
(Wiegand and Klakow, 2011b) is similar to
(Kim and Hovy, 2006) in that a rule-based ap-
proach is used relying on the relationship towards
predictive predicates. Diverse resources are con-
sidered for obtaining such words, however, they
are only evaluated on the entire MPQA corpus.
The only cross-domain evaluation of opinion
holder extraction is reported in (Li et al 2007) us-
ing the MPQA corpus as a training set and the NT-
CIR collection as a test set. A low cross-domain
performance is obtained and the authors conclude
that this is due to the very different annotation
schemes of those corpora.
8 Conclusion
We examined different generalization methods for
opinion holder extraction. We found that for in-
domain classification, the more labeled training
data are used, the smaller is the impact of gener-
alization. Robust learning methods, such as con-
volution kernels, benefit less from generalization
than weaker classifiers, such as conditional ran-
dom fields. For cross-domain classification, gen-
eralization is always helpful. Distant domains
are problematic for learning-based methods, how-
ever, rule-based methods provide a reasonable re-
call and can be effectively combined with the
learning-based methods. The types of generaliza-
tion that help best are manually compiled lexicons
followed by an induction method inspired by dis-
tant supervision. Finally, we examined the case
of opinion holders as patients and also presented
a novel automatic extraction method that proved
effective. Such dedicated extraction methods are
important as common labeled datasets (from the
news domain) do not provide sufficient training
data for these constructions.
Acknowledgements
This work was funded by the German Federal Ministry
of Education and Research (Software-Cluster) under
grant no. ?01IC10S01?. The authors thank Alessandro
Moschitti, Benjamin Roth and Josef Ruppenhofer for
their technical support and interesting discussions.
333
References
Steven Abney. 1991. Parsing By Chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-Based Parsing. Kluwer Academic Pub-
lishers, Dordrecht.
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL/HLT), Columbus, OH,
USA.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Extract-
ing Opinion Propositions and Opinion Holders us-
ing Syntactic and Lexical Cues. In Computing At-
titude and Affect in Text: Theory and Applications.
Springer-Verlag.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Yejin Choi, Claire Cardie, Ellen Riloff, and Sid-
dharth Patwardhan. 2005. Identifying Sources
of Opinions with Conditional Random Fields and
Extraction Patterns. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP), Vancouver, BC, Canada.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
Extraction of Entities and Relations for Opinion
Recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), Sydney, Australia.
Charles. J. Fillmore, Christopher R. Johnson, and
Miriam R. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235 ? 250.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), Ann Arbor, MI, USA.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Richard Johansson and Alessandro Moschitti. 2010.
Reranking Models in Fine-grained Opinion Anal-
ysis. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING), Be-
jing, China.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting Opinion Expressions and Their Polari-
ties ? Exploration of Pipelines and Joint Models. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Portland,
OR, USA.
Jason S. Kessler, Miriam Eckert, Lyndsay Clarke,
and Nicolas Nicolov. 2010. The ICWSM JDPA
2010 Sentiment Corpus for the Automotive Do-
main. In Proceedings of the International AAAI
Conference on Weblogs and Social Media Data
Challange Workshop (ICWSM-DCW), Washington,
DC, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
Opinions, Opinion Holders, and Topics Expressed
in Online News Media Text. In Proceedings of
the ACL Workshop on Sentiment and Subjectivity in
Text, Sydney, Australia.
Paul Kingsbury and Martha Palmer. 2002. From
TreeBank to PropBank. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC), Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the International
Conference on Machine Learning (ICML).
Dinko Lambov, Gae?l Dias, and Veska Noncheva.
2009. Sentiment Classification across Domains. In
Proceedings of the Portuguese Conference on Artifi-
cial Intelligence (EPIA), Aveiro, Portugal. Springer-
Verlag.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Yangyong Li, Kalina Bontcheva, and Hamish Cun-
ningham. 2007. Experiments of Opinion Analy-
sis on the Corpora MPQA and NTCIR-6. In Pro-
ceedings of the NTCIR-6 Workshop Meeting, Tokyo,
Japan.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant Supervision for Relation Extrac-
tion without Labeled Data. In Proceedings of the
Joint Conference of the Annual Meeting of the As-
sociation for Computational Linguistics and the In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing (ACL/IJCNLP), Singapore.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining Statistical Learn-
ing with a Knowledge-based Approach - A Case
Study in Intensive Care Monitoring. In Proceedings
the International Conference on Machine Learning
(ICML).
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
334
ternational Conference on Spoken Language Pro-
cessing (ICSLP), Denver, CO, USA.
Veselin Stoyanov and Claire Cardie. 2011. Auto-
matically Creating General-Purpose Opinion Sum-
maries from Text. In Proceedings of Recent Ad-
vances in Natural Language Processing (RANLP),
Hissar, Bulgaria.
Veselin Stoyanov, Claire Cardie, Diane Litman, and
Janyce Wiebe. 2004. Evaluating an Opinion An-
notation Scheme Using a New Multi-Perspective
Question and Answer Corpus. In Proceedings of
the AAAI Spring Symposium on Exploring Attitude
and Affect in Text, Menlo Park, CA, USA.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and Expression Level Annotation
of Opinions in User-Generated Discourse. In Pro-
ceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Uppsala,
Sweden.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and Gen-
eral Method for Semi-supervised Learning. In Pro-
ceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Uppsala,
Sweden.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing Subjective Language. Computational Linguis-
tics, 30(3).
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation, 39(2/3):164?210.
Michael Wiegand and Dietrich Klakow. 2010. Convo-
lution Kernels for Opinion Holder Extraction. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT/NAACL), Los Angeles, CA, USA.
Michael Wiegand and Dietrich Klakow. 2011a. Proto-
typical Opinion Holders: What We can Learn from
Experts and Analysts. In Proceedings of Recent Ad-
vances in Natural Language Processing (RANLP),
Hissar, Bulgaria.
Michael Wiegand and Dietrich Klakow. 2011b. The
Role of Predicates in Opinion Holder Extraction. In
Proceedings of the RANLP Workshop on Informa-
tion Extraction and Knowledge Acquisition (IEKA),
Hissar, Bulgaria.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing (HLT/EMNLP), Vancouver, BC, Canada.
335
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673?682,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Automatic Food Categorization from Large Unlabeled Corpora and Its
Impact on Relation Extraction
Michael Wiegand and Benjamin Roth and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Benjamin.Roth|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We present a weakly-supervised induc-
tion method to assign semantic informa-
tion to food items. We consider two tasks
of categorizations being food-type classi-
fication and the distinction of whether a
food item is composite or not. The cate-
gorizations are induced by a graph-based
algorithm applied on a large unlabeled
domain-specific corpus. We show that the
usage of a domain-specific corpus is vi-
tal. We do not only outperform a manually
designed open-domain ontology but also
prove the usefulness of these categoriza-
tions in relation extraction, outperforming
state-of-the-art features that include syn-
tactic information and Brown clustering.
1 Introduction
In view of the large interest in food in many parts
of the population and the ever increasing amount
of new dishes/food items, there is a need of au-
tomatic knowledge acquisition. We approach this
task with the help of natural language processing.
We investigate different methods to assign cate-
gories to food items. We focus on two categoriza-
tions, being a classification of food items to cat-
egories of the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) and a categorization of
whether a food item is composite or not.
We present a semi-supervised graph-based ap-
proach to induce these food categorizations from
an unlabeled domain-specific text corpus crawled
from the Web. The method only requires mini-
mal manual guidance for the initialization of the
algorithm with seed terms. It depends, however,
on an automatically constructed high-quality sim-
ilarity graph. For that we choose a pattern-based
representation that outperforms a distributional-
based representation. For initialization, we ex-
amine some manually compiled seed words and
a very few simple surface patterns to automati-
cally induce such expressions. As a hard baseline,
we compare the effectiveness of using a general-
purpose ontology for the same types of categoriza-
tions. Apart from an intrinsic evaluation, we also
examine the categories in relation extraction.
The contributions of this paper are a method re-
quiring minimal supervision for a comprehensive
classification of food items and a proof of con-
cept that the knowledge that can thus be gained is
beneficial for relation extraction. Even though we
focus on a specific domain, the induction method
can be easily translated to other domains. In par-
ticular, other life-style domains, such as fashion,
cosmetics or home & gardening, show parallels
since comparable textual web data are available
and similar relation types (e.g. that two items fit
together or can be substituted by each other) exist.
Our experiments are carried out on German data
but our findings should carry over to other lan-
guages since the issues we address are (mostly)
language universal. For general accessibility, all
examples are given as English translations.
2 Data & Annotation
2.1 Domain-Specific Text Corpus
In order to generate a dataset for our experiments,
we used a crawl of chefkoch.de1 (Wiegand et al.,
2012b) consisting of 418, 558 webpages of food-
related forum entries. chefkoch.de is the largest
German web portal for food-related issues.
2.2 Food Categorization
As a food vocabulary, we employ a list of 1888
food items: 1104 items were directly extracted
from GermaNet (Hamp and Feldweg, 1997), the
German version of WordNet (Miller et al., 1990).
The items were identified by extracting all hy-
ponyms of the synset Nahrung (English: food). By
1www.chefkoch.de
673
Class Description Size Perc.
MEAT meat and fish (products) 394 20.87
BEVERAGE beverages (incl. alcoholic drinks) 298 15.78
VEGE vegetables (incl. salads) 231 12.24
SWEET sweets, pastries and snack mixes 228 12.08
SPICE spices and sauces 216 11.44
STARCH starch-based side dishes 185 9.80
MILK milk products 104 5.51
FRUIT fruits 94 4.98
GRAIN grains, nuts and seeds 77 4.08
FAT fat 41 2.18
EGG eggs 20 1.06
Table 1: The different food types (gold standard).
consulting the relation tuples from Wiegand et al.
(2012c) a further 784 items were added. We man-
ually annotated this vocabulary w.r.t. two tasks:
2.2.1 Task I: Food Types
The food type categories we chose are mainly in-
spired by the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) that divides food items
into categories with similar nutritional properties.
This categorization scheme not only divides the
set of food items in many intuitive homogeneous
classes but it is also the scheme that is most com-
monly agreed upon. Table 1 lists the specific cat-
egories we use. For category assignment of com-
plex dishes comprising different food items we ap-
plied a heuristics: we always assign the category
that dominates the dish. A meat sauce, for exam-
ple, would thus be assigned MEAT (even though
there may be other ingredients than meat).
2.2.2 Task II: Dishes vs. Atomic Food Items
In addition to Task I, we include another catego-
rization that divides food items into dishes and
atomic food items (Table 2). By dish, we mainly
understand food items that are composite food
items made of other (atomic) food items. This
categorization is orthogonal to the previous clas-
sification of food items. We refrained from adding
dishes as a further category of food types in ?2.2.1,
as we would have ended up with a very heteroge-
neous class in the set of homogeneous food type
categories. Thus, dishes that differ greatly in nu-
trient content, such as Waldorf salad and chocolate
cake, would have been subsumed by one class.
3 Method
3.1 Graph-based Induction
We propose a semi-supervised graph-based ap-
proach to label food items with their respective
Class Description Examples Perc.
DISH composite food items cake, falafel, meat loaf 32.10
ATOM non-composite food items apple, steak, potato 67.90
Table 2: Distribution of dishes and atomic food
items among the food vocabulary (gold standard).
food categories. The underlying data structure
is a similarity graph connecting different food
items. Food items that belong to the same category
should be connected by highly weighted edges. In
order to infer the labels for each respective food
item, one first needs to specify a small set of seeds
for each category and then apply a graph-based
clustering method that divides the graph into clus-
ters that represent distinct food categories. Our
method is a low-resource approach that can also
be easily adapted to other domains. The only
domain-specific information required are an unla-
beled corpus and a set of seeds.
3.1.1 Construction of the Similarity Graph
To enable a graph-based induction, we generate a
similarity graph that connects similar food items.
For that purpose, a list of domain-independent
similarity-patterns was compiled. Each pattern is a
lexical sequence that connects the mention of two
food items (Table 3). Each pair of food items ob-
served with any of those patterns is connected via
a weighted edge (the different patterns are treated
equally). The weight is the total frequency of all
patterns co-occurring with a particular food pair.
Due to the high precision of our patterns, with
one or a few prototypical seeds we cannot expect
to find all items of a food category within the set
of items to which the seeds are directly connected.
Instead, one also needs to consider transitive con-
nectedness within the graph. For example, in Fig-
ure 1 banana and redberry are not directly con-
nected but they can be reached via pear or rasp-
berry. However, by considering mediate relation-
ships it becomes more difficult to determine the
most appropriate category for each food item since
most food items are connected to food items of dif-
ferent categories (in Figure 1, there are not only
edges between banana and other types of fruits
but there is also some edge to some sweet, i.e.
chocolate). For a unique class assignment, we ap-
ply a robust graph-based clustering algorithm. (It
will figure out that banana, pear, raspberry and
redberry belong to the same category and choco-
late belongs to another category, since it is mostly
674
Patterns food item
1
(or|or rather|instead of|?(?) food item
2
Example {apple: pineapple, pear, fruit, strawberry, kiwi} {steak:
schnitzel, sausage, roast, meat loaf, cutlet}
Table 3: Domain-independent patterns for build-
ing the similarity graph.
Figure 1: Illustration of the similarity graph.
linked to many other food items not being fruits.)
3.1.2 Semi-Supervised Graph Optimization
Our semi-supervised graph optimization (Belkin
and Niyogi, 2004) is a robust algorithm that was
primarily chosen since it only contains few free
parameters to adjust. It is based on two principles:
First, similar data points should be assigned simi-
lar labels, as expressed by a similarity graph of la-
beled and unlabeled data. Second, for labeled data
points the prediction of the learnt classifier should
be consistent with the (actual) gold labels.
We construct a weighted transition matrix W
of the graph by normalization of the matrix with
co-occurrence counts C which we obtain from the
similarity graph (?3.1.1). We use the common
normalization by a power of the degree function
d
i
=
?
j
C
ij
: it defines W
ij
=
C
ij
d
?
i
d
?
j
if i 6= j,
and W
ii
= 0. The normalization weight ? is the
first of two parameters used in our experiments for
semi-supervised graph optimization. For learning
the semi-supervised classifier, we use the method
of Zhou et al. (2004) to find a classifying function
which is sufficiently smooth with respect to both
the structure of unlabeled and labeled points.
Given a set of data points X = {x
1
, . . . , x
n
}
and label set L = {1, . . . , c}, with x
i:1?i?l
labeled
as y
i
? L and x
i:l+1?i?n
unlabeled. For predic-
tion, a vectorial function F : X ? Rc is estimated
assigning a vector F
i
of label scores to every x
i
.
The predicted labeling follows from these scores
as y?
i
= argmax
j?c
F
ij
. Conversely, the gold la-
beling matrix Y is a n ? c matrix with Y
ij
= 1 if
x
i
is labeled as y
i
= j and Y
ij
= 0 otherwise.
Minimizing the cost function Q aims at a trade-
off between information from neighbours and ini-
tial labeling information, controlled by parameter
Patterns Categorization Examples
patt
hearst
Food Types food item is some food type,
food type such as food item, . . .
patt
dishes
Dishes recipe for food item
patt
atom
Atomic Food Items made of|contains food item
Table 4: List of patterns to extract seeds.
? (the second parameter used in our experiments):
Q =
1
2
(
?
n
i,j=1
W
ij
?
?
?
?
?
1
?
?
i
F
i
?
1
?
?
j
F
j
?
?
?
?
?
+ ?
?
n
i=1
?F
i
? Y
i
?
)
where ?
i
is the degree function of W .
The first term in Q is the smoothness constraint,
its minimization leads to adjacent edges having
similar labels. The second term is the fitting con-
straint, its minimization leads to consistency of the
function F with the labeling of the data. The solu-
tion to the above cost function is found by solving
a system of linear equations (Zhou et al., 2004).
As we do not possess development data for this
work, we set the two free parameters ? = 0.5 and
? = 0.01. This setting is used for both induction
tasks and all configurations. It is a setting that pro-
vided reasonable results without any notable bias
for any particular configuration we examine.
3.1.3 Manually vs. Automatically Extracted
Seeds
We explore two types of seed initializations: (a)
a manually compiled list of seed food items and
(b) a small set of patterns (Table 4) by the help of
which such seeds are automatically extracted.
In order to extract seeds for Task I with the
pattern-based approach, we apply the patterns
from Hearst (1992). These patterns have been de-
signed for the acquisition of hyponyms. Task I can
also be regarded as some type of hyponym extrac-
tion. The food types (fruit, meat, sweets) repre-
sent the hypernyms for which we extract seed hy-
ponyms (banana, beef, chocolate).
In order to extract seeds for Task II, we apply
two domain-specific sets of patterns (patt
dish
and
patt
atom
). We rank the food items according to the
frequency of occurring with the respective pattern
set. Since food items may occur in both rankings,
we merge the two rankings in the following way:
score(food item) = #patt
dish
(food it.)?#patt
atom
(food it.)
The top end of this ranking represents dishes
while the bottom end represents atoms.
3.2 Using a General-Purpose Ontology
As a hard baseline, we also make use of the seman-
tic relationships encoded in GermaNet. Our two
675
types of food categorization schemes can be ap-
proximated with the hypernymy graph in that on-
tology: We manually identify nodes that resemble
our food categories (e.g. fruit, meat or dish) and
label any food item that is an immediate or a me-
diate hyponym of these nodes (e.g. apple for fruit)
with the respective category label. The downside
of this method is that a large amount of food items
is missing from the GermaNet-database (?2.2).
3.3 Other Baselines & Post-Processing
In addition to the previous methods we imple-
ment a heuristic baseline (HEUR) that rests on the
observation that German food items of the same
food category often share the same suffix, e.g.
Schokoladenkuchen (English: chocolate cake) and
Apfelkuchen (English: apple pie). For HEUR, we
manually compiled a set of few typical suffixes for
each food type/dish category (ranging from 3 to 8
suffixes per category). For classification of a food
item, we assign the food item the category label
whose suffix matched with the food item.2
We also examine an unsupervised baseline
(UNSUP) that applies spectral clustering on the
similarity graph following von Luxburg (2007):
? Input: a similarity matrix W and the number of categories to detect k.
? The laplacian L is constructed from W . It is the symmetric laplacian
L = I ?D
1/2
WD
1/2
, where D is a diagonal degree matrix.3
? A matrix U ? Rn?k is constructed that contains as columns the first
k eigenvectors u
1
, . . . , u
k
of L.
? The rows of U are interpreted as the new data points. The final cluster-
ing is obtained by k-means clustering of the rows of U .
UNSUP (which is completely parameter-free)
gives some indication about the intrinsic expres-
siveness of the similarity graph as it lacks any
guidance towards the categories to be predicted.
In graph-based food categorization, one can
only make predictions for food items that are con-
nected (be it directly or indirectly) to seed food
items within the similarity graph. To expand labels
to unconnected food items, we apply some post-
processing (POSTP). Similarly to HEUR, it ex-
ploits the suffix-similarity of food items. It assigns
each unconnected food item the label of the food
item (that could be labeled by the graph optimiza-
tion) that shares the longest suffix. Due to their
similar nature, we refrain from applying POSTP
on HEUR as it would produce no changes.
2Unlike German food items, English food items are of-
ten multi-word expressions. Therefore, we assume that for
English, instead of analyzing suffixes the usage of the head
of a multiword expression (i.e. chocolate cake) would be an
appropriate basis for a similar heuristic.
3That is, D
ii
equals to the sum of the ith row.
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP X 46.2 43.1 35.7 36.0 56.1 41.0 42.5 38.4
HEUR (plain) 25.5 87.9 32.2 42.9 N/A N/A N/A N/A
HEUR X 56.4 73.6 52.1 54.7 68.7 72.3 64.3 60.7
PAT-Top1 X 52.4 60.2 51.2 52.5 64.5 58.2 62.9 57.4
PAT-Top5 X 61.1 70.7 61.9 64.4 74.5 67.9 76.0 69.7
PAT-Top10 X 60.2 69.6 60.5 62.2 73.4 66.7 74.2 67.3
1-PROTO X 58.0 68.0 58.0 59.5 70.2 64.1 71.0 63.8
5-PROTO X 64.5 76.6 63.7 68.6 78.6 73.8 78.5 75.2
10-PROTO X 65.8 79.0 65.5 71.0 80.2 75.9 80.6 77.7
GermaNet (plain) 52.1 94.0 52.0 65.7 75.4 73.2 75.0 72.4
GermaNet X 68.3 84.7 63.4 71.6 82.7 81.8 77.7 79.1
Table 5: Comparison of different food-type classi-
fiers (graph indicates graph-based optimization).
4 Experiments
We report precision, recall and F-score and accu-
racy.4 For precision, recall and F-score, we list the
macro-averaged score.
4.1 Evaluation of Food Categorization
4.1.1 Detection of Food Types
Table 5 compares different classifiers and configu-
rations for the prediction of food types (against the
gold standard from Table 1). Apart from the pre-
viously described baselines, we consider n man-
ually selected prototypes (n-PROTO) and the top
n food items produced by Hearst-patterns (PAT-
Topn) as seeds for graph-based optimization. The
table shows that the semi-supervised graph-based
approach with these seeds outperforms the base-
lines UNSUP and HEUR. Only as few as 5
prototypical seeds (per category) are required to
obtain performance that is even better than us-
ing plain GermaNet. The table also shows that
post-processing (with our suffix-heuristics) con-
sistently improves performance. Manually choos-
ing prototypes is more effective than instantiating
seeds via Hearst-patterns. The quality of the out-
put of Hearst-patterns degrades from top 10 on-
wards. However, considering that PAT-Topn does
not include any manual intervention, it already
produces decent results. Finally, even GermaNet
can be effectively used as seeds.
4.1.2 Detection of Dishes
Table 6 compares different classifiers for the de-
tection of dishes (against the gold standard from
Table 2). Dishes and atomic food items are very
4All manually labeled resources are available at:
www.lsv.uni-saarland.de/personalPages/
michael/relFood.html
676
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP X 54.5 59.6 40.2 37.3 67.9 59.0 50.0 40.6
HEUR (plain) 74.1 84.3 59.9 58.6 N/A N/A N/A N/A
PAT-Top25 X 59.7 72.2 54.6 61.9 74.1 70.1 67.6 68.4
PAT-Top50 X 60.9 74.4 55.6 63.1 75.9 72.7 69.2 70.3
PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
PAT-Top250 X 59.6 71.8 55.1 62.2 74.2 70.3 68.7 69.3
RAND-25 X 61.4 77.1 54.3 61.8 76.1 74.4 67.1 68.4
RAND-50 X 62.6 76.3 60.1 67.2 77.2 74.0 76.8 74.4
RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
GermaNet (plain) 49.5 81.3 46.5 59.3 79.0 75.9 75.5 75.7
GermaNet X 60.8 79.4 51.3 57.6 75.9 78.2 64.4 65.4
Table 6: Comparison of different classifiers dis-
tinguishing between dishes and atomic food items
(graph indicates graph-based optimization).
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
PAT-Top100 (plain) 9.5 89.5 10.5 18.6 63.6 61.5 63.5 61.3
PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
RAND-100 (plain) 10.6 100.0 12.2 21.4 70.2 69.7 69.0 69.0
RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
Table 7: Impact of graph-based optimization
(graph) for the detection of dishes.
heterogeneous classes which is why more seeds
are required for initialization. This means that
we cannot look for prototypes. For simplicity,
we resorted to randomly sample seeds from our
gold standard (RAND-n). For HEUR, we could
not find a small and intuitive set of suffixes that
are shared by many atomic food types, therefore
we considered all food types from our vocabulary
whose suffix did not match a typical dish suffix as
atomic. As this leaves no unspecified food items in
our vocabulary, we cannot use the output of HEUR
as seeds for graph-based optimization.
In contrast to the previous experiment, HEUR is
a more robust baseline. But again, post-processing
mostly improves performance, and patterns are not
as good as manual (random) seeds yet the former
are notably better than HEUR w.r.t. F-Score. Un-
like in the food-type classification, graph-based
optimization applied on GermaNet does not result
in some improvement. We assume that the preci-
sion of plain GermaNet with 81.3% is too low.5
Since GermaNet cannot effectively be used as
seeds for the graph-based optimization and post-
processing has already a strong positive effect, we
may wonder how effective the actual graph-based
5For other seeds for which it worked, we usually mea-
sured a precision of 90% or higher.
optimization is for this classification task. Af-
ter all, significantly more seeds are required for
this classification task than for the previous task,
so we need to show that it is not the mere seeds
(+post-processing) that are required for a reason-
able categorization. Table 7 examines two key
configurations with and without graph-based op-
timization. It shows that also for this classification
task, graph-based optimization produces a catego-
rization superior to the mere seeds. Moreover, the
suffix-based post-processing is complementary to
the improvement by the graph-based optimization.
4.1.3 Comparison of Initialization Methods
Table 8 compares for each food type 5 manually
selected prototypical seeds (i.e. 5-PROTO) and
the 5 food items most frequently been observed
with patt
hearst
(Table 4). While the manually cho-
sen seeds represent the spectrum of food items
within each particular class (e.g. for STARCH,
some type of pasta, rice and potato was chosen),
it is not possible to enforce such diversity with
the automatically extracted seeds. However, most
food items are correct. Table 9 displays the 10
most highly ranked dishes and atomic food items
extracted with patt
dish
and patt
atom
(Table 4). Un-
like the previous task (Table 8), we obtain more
heterogeneous seeds within the same class.
4.1.4 Distributional Similarity
Since many recent methods for related tasks, such
as noun classification, are based on so-called dis-
tributional similarity (Riloff and Shepherd, 1997;
Lin, 1998; Snow et al., 2004; Weeds et al., 2004;
Yamada et al., 2009; Huang and Riloff, 2010;
Lenci and Benotto, 2012), we also examine this as
an alternative representation to the pattern-based
similarity graph (Table 3). We represent each food
item as a vector which itself is an aggregate of
the contexts of all mentions of a particular food
item. We weighted the individual (context) words
co-occurring with the food item at a fixed window
size of 5 words with tf-idf. We can now apply
graph-based optimization on the similarity matrix
encoding the cosine similarities between any pos-
sible pair of vectors representing two food items.
As seeds, we use the best configuration (not em-
ploying GermaNet), i.e. 10-PROTO for food type
classification and RAND-100 for the dish classi-
fication. Since, however, the graph clustering is
not actually necessary, as we have a full similar-
ity matrix (rather than a sparse graph) that also al-
677
Class 5 Manually Chosen Seeds (5-PROTO) 5 Hearst-Pattern Seeds (PAT-Top5)
MEAT schnitzel, rissole, bologna, redfish, trout salmon, beef, chicken, turkey hen, poultry
BEVERAGE coffee, tea, water, beer, coke coffee, beer, mineral water, lemonade, tea
VEGE peas, green salad, tomato, cauliflower, carrot zucchini, lamb?s salad, broccoli, leek, cauliflower
SWEET chocolate, torte, popcorn, apple pie, potato crisps wine gum, marzipan, custard, pancake, biscuits
SPICE pepper, cinnamon, salt, gravy, remoulade cinnamon, laurel, clove, tomato sauce, basil
STARCH spaghetti, basmati rice, white bread, potato, french fries au gratin potatoes, jacket potato, potato, pita, jam
MILK yoghurt, gouda, cream cheese, cream, butter milk butter milk, bovine milk, soured milk, goat cheese, sour cream
FRUIT banana, apple, strawberries, apricot, orange banana, strawberries, pear, melon, kiwi
GRAIN hazelnut, pumpkin seed, rye flour, semolina, wheat sesame, spelt, wheat, millet, barley
FAT margarine, lard, colza oil, spread, butter margarine, lard, resolidified butter, coconut oil, tartar
EGG scrambled eggs, fried eggs, chicken egg, omelette, pickled egg yolk, fried eggs, albumen, offal, easter egg
Table 8: Comparison of different seed initializations for the food type categorization task (underlined
food items represent erroneously extracted food items).
lows us to compare any arbitrary pair of food items
directly, we also employ a second classifier (for
comparison) based on the nearest neighbour prin-
ciple. We assign each food item the label of the
most similar seed food item.
Table 10 compares these two classifiers with the
best previous result. It shows that the pattern-
based representation consistently outperforms the
distributional representation. The former may be
sparse but it produces high-precision similarity
links.6 The vector representation, on the other
hand, may not be sparse but it contains a high
degree of noise. The major problem is that not
only vectors of similar food items, such as chips
(fries), potatoes and rice, are similar to each other,
but also vectors of different food items that are
typically consumed with each other (e.g. fish
and chips). This is because of their frequent co-
occurrence (as in collocations like fish & chips).
Unfortunately, these pairs belong to different food
types. For the dish classification, however, the
vector representation is less of a problem.7
The distributional representation works better
with the simple nearest neighbour classifier. We
assume that graph-based optimization adds further
noise to the classification since, unlike the nearest
neighbour which only calculates the direct similar-
ity between two vectors, it also incorporates indi-
rect relationships (which may be more error-prone
than the direct relationships) between food items.
4.1.5 Do we need a domain-specific corpus?
In this section, we want to provide evidence that
apart from the similarity graph and seeds the tex-
tual source for the graph, i.e. our domain-specific
6By the label propagation within the graph-based opti-
mization, the sparsity problem is also mitigated.
7Fish and chips are both atoms, so in the dish classifica-
tion, it is no mistake to consider them similar food items.
Class 10 Seeds Extracted with Patterns (PAT-Top10)
DISH cookies, cake, praline, bread dumpling, jam, biscuit, cheese
cake, black-and-whites, onion tart, pasta salad
ATOM marzipan, flour, potato, olive oil, water, sugar, cream, choco-
late, milk, tomato
Table 9: Illustration of seed initialization for the
distinction between dishes and atomic food items.
Task Similarity Classifier Acc F1
Food Type distributional nearest neighbour 53.4 51.1
distributional graph 25.6 25.6
pattern-based graph 80.2 77.7
Dish distributional nearest neighbour 76.8 75.2
distributional graph 71.5 71.2
pattern-based graph 83.0 80.1
Table 10: Impact of the similarity representation.
corpus (chefkoch.de), is also important. For that
purpose, we compare our current corpus against
an open-domain corpus. We consider the German
version of Wikipedia since this resource also con-
tains encyclopedic knowledge about food items.
Table 11 compares the graph-based induction. As
in the previous section, we only consider the best
previous configuration. The table clearly shows
that our domain-specific text corpus is a more ef-
fective resource for our purpose than Wikipedia.
4.2 Evaluation for Relation Extraction
We now examine whether automatic food cate-
gorization can be harnessed for relation extrac-
tion. The task is to detect instances of the relation
types SuitsTo, SubstitutedBy and IngredientOf in-
troduced Wiegand et al. (2012b) (repeated in Ta-
ble 12) and motivated in Wiegand et al. (2012a).
These relation types are highly relevant for cus-
tomer advice/product recommendation. In partic-
ular, SuitsTo and SubstitutedBy are fairly domain-
independent relation types. Customers want to
678
know which items can be used together (SuitsTo),
be it two food items that can be used as a meal
or two fashion items that can be worn together.
Substitutes are also relevant for situations in which
item A is out of stock but item B can be offered as
an alternative. Therefore, insights from this work
should carry over to other domains.
We randomly extracted 1500 sentences from
our text corpus (?2.1) in which (at least) two food
items co-occur. Each food pair mention was man-
ually assigned one label. In addition to the three
relation types from above, we introduce the la-
bel Other for cases in which either another rela-
tion between the target food items is expressed or
the co-occurrence is co-incidental. On a subset of
200 sentences, we measured a substantial inter-
annotation agreement of Cohen?s ? = 0.67 (Lan-
dis and Koch, 1977).
We train a supervised classifier and incorporate
the knowledge induced from our domain-specific
corpus as features. We chose Support Vector Ma-
chines with 5-fold cross-validation using SVMlight-
multi-class (Joachims, 1999).
Table 13 displays all features that we examine
for supervised classification. Most features are
widely used throughout different NLP tasks. One
special feature brown takes into consideration the
output of Brown clustering (Brown et al., 1992)
which like our graph-based optimization produces
a corpus-driven categorization of words. Simi-
lar to UNSUP, this method is unsupervised but it
considers the entire vocabulary of our text corpus
rather than only food items. Therefore, this in-
formation can be considered as a generalization
of all contextual words. Such type of informa-
tion has been shown to be useful for named-entity
recognition (Turian et al., 2010) and relation ex-
traction (Plank and Moschitti, 2013).
For syntactic parsing, Stanford Parser (Rafferty
and Manning, 2008) was used. For Brown cluster-
ing, the SRILM-toolkit (Stolcke, 2002) was used.
Following Turian et al. (2010), we induced 1000
clusters (from our domain-specific corpus ?2.1).
4.2.1 Why should food categories be helpful
for relation extraction?
All relation types we consider comprise pairs of
two food items which makes these relation types
likely to be confused. Contextual information may
be used for disambiguation but there may also be
frequent contexts that are not sufficiently informa-
tive. For example, 25% of the instances of Ingre-
PLAIN +POSTP
Task Corpus graph Acc F1 Acc F1
Food Type
Wikipedia X 40.3 49.4 61.4 59.8
chefkoch.de X 65.8 71.0 80.2 77.7
Dish
Wikipedia X 50.4 53.1 75.4 71.1
chefkoch.de X 66.5 71.3 83.0 80.1
Table 11: Comparison of Wikipedia and domain-
specific corpus as a source for the similarity graph.
dientOf follow the lexical pattern food item
1
with
food item
2
(1). However, the same pattern also
covers 15% of the instances of SuitsTo (2).
(1) We had a stew with red lentils. (Relation: IngredientOf)
(2) We had salmon with broccoli. (Relation: SuitsTo)
The food type information we learned from our
text corpus might tell us which of the food items
are dishes. Only in (1), there is a dish, i.e. stew.
So, one may infer that the presence of dishes is
indicative of IngredientOf rather than SuitsTo.
food item
1
and food item
2
is another ambigu-
ous context. It cannot only be observed with the
relation SuitsTo, as in (3) (66% of all instantia-
tions of that pattern), but also SubstitutedBy (20%
of all mentions of that relation match that pattern),
as in (4). For SuitsTo, two food items that belong
to two different classes (e.g. MEAT and STARCH
or MEAT and VEGE) are quite characteristic. For
SubstitutedBy, the two food items are very often of
the same category of the Food Guide Pyramid.
(3) I very often eat fish and chips. (Relation: SuitsTo)
(4) For these types of dishes you can offer both Burgundy wine and
Champagne. (Relation: SubstitutedBy)
Since the second ambiguous context involves
the two general relation types SuitsTo and Substi-
tutedBy, resolving this ambiguity with automati-
cally induced type information has some signifi-
cance for other domains. In particular, for other
life-style domains, domain-specific type informa-
tion could be obtained following our method from
?3.1. The disambiguation rule that two entities of
the same type imply SubstitutedBy otherwise they
imply SuitsTo should also be widely applicable.
4.2.2 Results
Table 14 displays the performance of the different
feature sets for relation extraction. The features
designed from graph-based induction (i.e. graph)
work slightly better than GermaNet. The perfor-
mance of patt is not impressively high. However,
one should consider that patt can be used directly
without a supervised classifier (as each pattern is
679
Relation Description Example Freq. Perc.
SuitsTo food items that are typically consumed together My kids love the simple combination of fish fingers
with mashed potatoes.
633 42.20
SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter. 336 22.40
IngredientOf ingredient of a particular dish Falafel is made of chickpeas. 246 16.40
Other other relation or co-occurrence of food items are co-incidental On my shopping list, I?ve got bread, cauliflower, ... 285 19.00
Table 12: The different relation types and their respective frequency on our dataset.
Features Description
patt lexical surface patterns used in Wiegand et al. (2012b)
word bag-of-words features: all words within the sentence
brown features using Brown clustering: all features from word but
words are replaced by induced clusters
pos part-of-speech sequence between target food items and tags
of the words immediately preceding and following them
synt path from syntactic parse tree from first target food item to
second target food item
conj conjunctive features: patt with brown classes of target food
items; pos sequence with brown classes of target food items;
synt with brown classes of target food items
graph semantic food information induced by graph optimization
(config.: 10-PROTO(+POSTP) and RAND-100(+POSTP))
germanet semantic food information derived from (plain) GermaNet
Table 13: Description of the feature set.
designed for a particular relation type, one can
read off from the matching pattern which class is
predicted). word is slightly better but, unlike patt,
it is dependent on supervised learning.
The only feature that individually manages to
significantly outperform word is graph. The tra-
ditional features (i.e. pos, synt and brown) only
produce some mild improvement when added
jointly to word along some conjunctive fea-
tures. When graph is added to this feature set
(i.e. word+patt+pos+synt+brown+conj), we ob-
tain another significant improvement. In con-
clusion, the information we induced from our
domain-specific corpus cannot be obtained by
other NLP-features, including other state-of-the-
art induction methods such as Brown clustering.
5 Related Work
While many of the previous works on noun catego-
rization also address the task of hypernym classifi-
cation (Hearst, 1992; Caraballo, 1999; Widdows,
2003; Kozareva et al., 2008; Huang and Riloff,
2010; Lenci and Benotto, 2012) and some include
examples involving food items (Widdows and
Dorow, 2002; Cederberg and Widdows, 2003),
only van Hage et al. (2005) and van Hage et al.
(2006) specifically focus on the classification of
food items. van Hage et al. (2005) deal with on-
tology mapping whereas van Hage et al. (2006)
explore part-whole relations.
Features Acc Prec Rec F1
germanet 45.3 41.3 37.2 37.3
graph 46.0 39.4 39.7 38.6
patt 59.8 49.8 41.1 38.7
word 60.1 56.9 54.5 55.1
word+patt 60.3 57.3 54.9 55.5
word+brown 59.5 56.1 54.6 54.9
word+synt 60.3 57.7 55.4 56.0
word+pos 59.8 56.6 54.6 55.1
word+germanet 61.3 58.6 56.0 56.7
word+graph 62.9 59.2 57.6 58.1?
word+patt+brown+synt+pos 60.4 57.3 56.2 56.5
word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2?
word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1?
word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9??
statistical significance testing (paired t-test): better than word ? at p < 0.1/
? at p < 0.05; ? better than word+patt+brown+synt+pos+conj at p < 0.05
Table 14: Comparison of various features (Ta-
ble 13) for (unrestricted) relation extraction.
The task of data-driven lexicon expansion has
also been explored before (Kanayama and Na-
sukawa, 2006; Das and Smith, 2012), however,
our paper presents the first attempt to carry out
a comprehensive categorization for the food do-
main. For the first time, we also show that type
information can effectively improve the extraction
of very common relations. For the twitter domain,
the usage of type information based on cluster-
ing has already been found effective for supervised
learning (Bergsma et al., 2013).
6 Conclusion
We presented an induction method to assign se-
mantic information to food items. We considered
two types of categorizations being food-type infor-
mation and information about whether a food item
is composite or not. The categorization is induced
by graph-based optimization applied on a large
unlabeled domain-specific text corpus. We pro-
duce categorizations that outperform a manually
compiled resource. The usage of such a domain-
specific corpus based on a pattern-based represen-
tation is vital and largely outperforms other text
corpora or a distributional representation. The in-
duced knowledge improves relation extraction.
680
Acknowledgements
This work was performed in the context of the Software-
Cluster project SINNODIUM. Michael Wiegand was funded
by the German Federal Ministry of Education and Research
(BMBF) under grant no. 01IC12SO1X. Benjamin Roth is
a recipient of the Google Europe Fellowship in Natural Lan-
guage Processing, and this research is supported in part by
this Google Fellowship. The authors would like to thank
Stephanie Ko?ser for annotating the dataset presented in this
paper.
References
Mikhail Belkin and Partha Niyogi. 2004. Semi-
supervised learning on Riemannian manifolds. Ma-
chine Learning, 56(1-3):209?239.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly Improving User Classification
via Communication-Based Name and Location
Clustering on Twitter. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
1010?1019, Atlanta, GA, USA.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
120?126, College Park, MD, USA.
Scott Cederberg and Dominic Widdows. 2003. Us-
ing LSA and Noun Coordination Information to Im-
prove the Precision and Recall of Automatic Hy-
ponymy Extraction. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL), pages 111?118, Edmonton, Alberta,
Canada.
Dipanjan Das and Noah A. Smith. 2012. Graph-
Based Lexicon Expansion with Sparsity-Inducing
Penalties. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the ACL (HLT/NAACL), pages 677?
687, Montre?al, Quebec, Canada.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15, Madrid, Spain.
Marti A. Hearst. 1992. Automatic Acquisition of
Hyponyms from Large Text Corpora. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING), pages 539?545,
Nantes, France.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (al-
most) Nothing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 275?285, Uppsala, Sweden.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully Automatic Lexicon Expansion for Domain-
oriented Sentiment Analysis. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 355?363, Syd-
ney, Australia.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1048?
1056, Columbus, OH, USA.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159?174.
Alessandro Lenci and Guilia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of the Joint Conference on Lexical
and Computational Semantics (*SEM), pages 75?
79, Montre?al, Quebec, Canada.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and International Conference on Computa-
tional Linguistics (ACL/COLING), pages 768?774,
Montreal, Quebec, Canada.
George Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Introduction to WordNet: An On-line Lexical
Database. International Journal of Lexicography,
3:235?244.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adapation of Relation Extraction. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1498?
1507, Sofia, Bulgaria.
Anna Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the ACL
Workshop on Parsing German (PaGe), pages 40?46,
Columbus, OH, USA.
681
Ellen Riloff and Jessica Shepherd. 1997. A
Corpus-Based Approach for Building Semantic
Lexicons. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 117?124, Providence, RI, USA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In Advances in Neural Informa-
tion Processing Systems (NIPS), Vancouver, British
Columbia, Canada.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, CO, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 384?394,
Uppsala, Sweden.
Human Nutrition Information Service U.S. Depart-
ment of Agriculture. 1992. The Food Guide Pyra-
mid. Home and Garden Bulletin 252, Washington,
D.C., USA.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguis-
tic Ontology-Mapping Techniques. In Proceedings
of International Semantic Web Conference (ISWC),
pages 732 ? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus
Schreiber. 2006. A Method for Learning Part-
Whole Relations. In Proceedings of International
Semantic Web Conference (ISWC), pages 723 ? 735,
Athens, GA, USA. Springer.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17:395?416.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1015?1021, Geneva, Switzerland.
Dominic Widdows and Beate Dorow. 2002. A
Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 1093?
1099, Taipei, Taiwan.
Dominic Widdows. 2003. Unsupervised methods for
developing taxonomies by combining syntactic and
statistical information. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
197?204, Edmonton, Alberta, Canada.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012a. Knowledge Acquisition with Nat-
ural Language Processing in the Food Domain: Po-
tential and Challenges. In Proceedings of the ECAI-
Workshop on Cooking with Computers (CWC),
pages 46?51, Montpellier, France.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012b. Web-based Relation Extraction
for the Food Domain. In Proceedings of the In-
ternational Conference on Applications of Natu-
ral Language Processing to Information Systems
(NLDB), pages 222?227, Groningen, the Nether-
lands. Springer.
Michael Wiegand, Benjamin Roth, Eva Lasarcyk,
Stephanie Ko?ser, and Dietrich Klakow. 2012c. A
Gold Standard for Relation Extraction in the Food
Domain. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), pages
507?514, Istanbul, Turkey.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym Dis-
covery Based on Distributional Similarity and Hi-
erarchical Structures. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 929?927, Singapore.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004.
Learning with Local and Global Consistency. In
Advances in Neural Information Processing Systems
(NIPS), Vancouver and Whistler, British Columbia,
Canada.
682
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 89?92,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
RelationFactory: A Fast, Modular and Effective System for Knowledge
Base Population
Benjamin Roth
?
Tassilo Barth
?
Grzegorz Chrupa?a
*
Martin Gropp
?
Dietrich Klakow
?
?
Spoken Language Systems, Saarland University, 66123 Saarbr?ucken, Germany
*
Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands
?
{beroth|tbarth|mgropp|dietrich.klakow}@lsv.uni-saarland.de
*
g.chrupala@uvt.nl
Abstract
We present RelationFactory, a highly ef-
fective open source relation extraction sys-
tem based on shallow modeling tech-
niques. RelationFactory emphasizes mod-
ularity, is easily configurable and uses a
transparent pipelined approach.
The interactive demo allows the user to
pose queries for which RelationFactory re-
trieves and analyses contexts that contain
relational information about the query en-
tity. Additionally, a recall error analy-
sis component categorizes and illustrates
cases in which the system missed a correct
answer.
1 Introduction and Overview
Knowledge base population (KBP) is the
task of finding relational information in large
text corpora, and structuring and tabulariz-
ing that information in a knowledge base.
Given an entity (e.g. of type PERSON) with
an associated relational schema (a set of re-
lations, e.g. city of birth(PERSON,
CITY), schools attended(PERSON,
ORGANIZATION), spouse(PERSON,
PERSON)), all relations about the entity that
are expressed in a text corpus would be rele-
vant, and the correct answers would have to be
extracted.
The TAC KBP benchmarks
1
are an effort to for-
malize this task and give researchers in the field
the opportunity to evaluate their algorithms on a
set of currently 41 relations. In TAC KBP, the
task and evaluation setup is established by well-
defined information needs about query entities of
types PERSON and ORGANIZATION (e.g. who is
the spouse of a person, how many employees
1
http://www.nist.gov/tac/about/
does an organization have). A perfect system
would have to return all relevant information (and
only this) contained in the text corpus. TAC KBP
aims at giving a realistic picture of not only pre-
cision but also recall of relation extraction sys-
tems on big corpora, and is therefore an advance-
ment over many other evaluations done for rela-
tion extraction that are often precision oriented
(Suchanek et al., 2007) or restrict the gold key to
answers from a fixed candidate set (Surdeanu et
al., 2012) or to answers contained in a data base
(Riedel et al., 2010). Similar to the classical TREC
evaluation campaigns in document retrieval, TAC
KBP aims at approaching a true recall estimate by
pooling, i.e. merging the answers of a timed-out
manual search with the answers of all participat-
ing systems. The pooled answers are then evalu-
ated by human judges.
It is a big advantage of TAC KBP that the end-
to-end setup (from the query, through retrieval of
candidate contexts and judging whether a relation
is expressed, to normalizing answers and putting
them into a knowledge base) is realistic. At the
same time, the task is very complex and may in-
volve too much work overhead for researchers
only interested in a particular step in relation ex-
traction such as matching and disambiguation of
entities, or judging relational contexts. We there-
fore introduce RelationFactory, a fast, modular
and effective relation extraction system, to the re-
search community as open source software.
2
Rela-
tionFactory is based on distantly supervised classi-
fiers and patterns (Roth et al., 2013), and was top-
ranked (out of 18 systems) in the TAC KBP 2013
English Slot-filling benchmark (Surdeanu, 2013).
In this demo, we give potential users the possi-
bility to interact with the system and to get a feel
for use cases, strengths and limitations of the cur-
rent state of the art in knowledge base population.
2
https://github.com/beroth/
relationfactory
89
The demo illustrates how RelationFactory arrives
at its conclusions and where future potentials in
relation extraction lie. We believe that Relation-
Factory provides an easy start for researchers in-
terested in relation extraction, and we hope that
it may serve as a baseline for new advances in
knowledge base population.
2 System Philosophy and Design
Principles
The design principles of RelationFactory conform
to what is known as the Unix philosophy.
3
For Re-
lationFactory this philosophy amounts to a set of
modules that solve a certain step in the pipeline
and can be run (and tested) independently of the
other modules. For most modules, input and out-
put formats are column-based text representations
that can be conveniently processed with standard
Linux tools for easy diagnostics or prototyping.
Data representation is compact: the system is de-
signed in a way that each module ideally outputs
one new file. Because of modularization and sim-
ple input and output formats, RelationFactory al-
lows for easy extensibility, e.g. for research that
focuses solely on novel algorithms at the predic-
tion stage.
The single modules are connected by a make-
file that controls the data flow and allows for easy
parallelization. RelationFactory is highly config-
urable: new relations can be added without chang-
ing any of the source code, only by changing con-
figuration files and adding or training respective
relational models.
Furthermore, RelationFactory is designed to be
highly scalable: Thanks to feature hashing, large
amounts of training data can be used in a memory-
friendly way. Predicting relations in real-time is
possible using shallow representations. Surface
patterns, ngrams and skip-ngrams allow for highly
accurate relational modeling (Roth et al., 2013),
without incurring the cost of resource-intensive
processing, such as parsing.
3
One popular set of tenets (Gancarz, 2003) summarizes
the Unix philosophy as:
1. Small is beautiful.
2. Make each program do one thing well.
3. Build a prototype as soon as possible.
4. Choose portability over efficiency.
5. Store data in flat text files.
6. Use software leverage to your advantage.
7. Use shell scripts to increase leverage and portability.
8. Avoid captive user interfaces.
9. Make every program a filter.
Figure 1: TAC KBP: Given a set of queries, return
a correct, complete and non-redundant response
with relevant information extracted from the text
corpus.
Figure 2: Data flow of the relation extraction sys-
tem: The candidate generation stage retrieves pos-
sible relational contexts. The candidate validation
stage predicts whether relations actually hold and
produces a valid response.
3 Component Overview
A simplified input and output to RelationFactory
is shown in Figure 1. In general, the pipeline
is divided in a candidate generation stage, where
documents are retrieved and candidate sentences
are identified, and the candidate validation stage,
which predicts and generates a response from the
retrieved candidates (see Figure 2).
In a first step, the system generates aliases for
the query using statistical and rule-based expan-
sion methods, for example:
Query Expansion
Adam Gadahn Azzam the American, Adam Yahiye Gadahn, Gadahn
STX Finland Kvaerner Masa Yards, Aker Finnyards, STX Finland Ltd
The expansions are used for retrieving docu-
ments from a Lucene index. All those sen-
90
tences are retained where the query (or one of
the query aliases) is contained and the named-
entity tagger has identified another entity with
the type of a potential answer for one of the
sought relations. The system is easily con-
figurable to include matching of non-standard
named-entity types from lists. RelationFac-
tory uses lists obtained from Freebase (www.
freebase.com) to match answer candidates
for the types CAUSE-OF-DEATH, JOB-TITLE,
CRIMINAL-CHARGES and RELIGION.
The candidate sentences are output line-by-line
and processed by one of the validation modules,
which determine whether actually one of the rela-
tions is expressed. RelationFactory currently uses
three standard validation modules: One based on
SVM classifiers, one based on automatically in-
duced and scored patterns, and one based on man-
ually crafted patterns. The validation modules
function as a filter to the candidates file. They
do not have to add a particular formatting or con-
form to other requirements of the KBP task such
as establishing non-redundancy or finding the cor-
rect offsets in the text corpus. This is done by
other modules in the pipeline, most notably in
the post-processing step, where statistical meth-
ods and heuristics are applied to produce a well-
formed TAC KBP response.
4 User Perspective
From a user perspective, running the system is as
easy as calling:
./run.sh system.config
The configuration file contains all information
about the general run configuration of the system,
such as the query file to use, the format of the re-
sponse file (e.g. TAC 2012 or TAC 2013 format),
the run directory that will contain the response,
and the Lucene index with the corpus. Optional
configuration can control non-standard validation
modules, and special low or high-recall query ex-
pansion schemes.
The relevant parts of the configuration file for a
standard 2013 TAC KBP run would look like the
following:
query /TAC_EVAL/2013/query.xml
goal response2013
rundir /TAC_RUNS/run2013/
index /TAC_CORPORA/2013/index
rellist /CFG/rellist2013
relations.config /CFG/relations2013.config
The last two lines refer to relation-specific con-
figuration files: The list of relations to use and in-
formation about them. Changing these files (and
adding respective models) allows for inclusion of
further relations. The relation-specific configura-
tion file contains information about the query en-
tity type, the expected answer named-entity tag
and whether a list of answers is expected (com-
pared to relations with just one correct answer):
per:religion enttype PER
per:religion argtag RELIGION
per:religion listtype false
org:top_members_employees enttype ORG
org:top_members_employees argtag PERSON
org:top_members_employees listtype true
RelationFactory comes with batteries included:
The models and configurations for TAC KBP 2013
work out-of-the-box and can easily be used as a
relation extraction module in a bigger setting or as
a baseline for new experiments.
4
5 Illustrating RelationFactory
In TAC KBP 2013, 6 out of 18 systems achieved
an F1 score of over 30%. RelationFactory as
the top-performing system achieved 37.28% com-
pared to 68.49% achieved by human control an-
notators (Surdeanu, 2013). These numbers clearly
show that current systems have just gone halfway
toward achieving human-like performance on an
end-to-end relation extraction task.
The aim of the RelationFactory demo is to il-
lustrate what the current challenges in TAC KBP
are. The demonstration interface therefore not
only shows the answers generated for populating
a potential knowledge base, but also what text was
used to justify the extraction.
The real-time performance of RelationFactory
allows for trying arbitrary queries and changing
the configuration files and immediately seeing the
effects. Different expansion schemes, validation
modules and patterns can be turned on and off, and
intuitions can be obtained about the bottlenecks
and error sources of relation extraction. The demo
also allows for seeing the effect of extracting infor-
mation from different corpora: a Wikipedia corpus
and different TAC KBP corpora, such as newswire
and web text.
4
Training models for new relations requires is a bigger
effort and includes generation of distant supervision train-
ing data by getting argument pairs from relational patterns
or a knowledge base like Freebase. RelationFactory includes
some training scripts but since they are typically run once
only, they are significantly less documented.
91
Figure 3: Screenshot of the RelationFactory demo user interface.
RelationFactory contains a number of diagnos-
tic tools: With a gold key for a set of queries, error
classes can be broken down and examples for cer-
tain error classes can be shown. For example, the
diagnostic tool for missed recall performs the fol-
lowing checks:
1. Is document retrieved?
2. Is query matched? This determines whether a sen-
tence is considered for further processing.
3. Is answer in query sentence? Whether the answer is
in one of the sentences with the query. Our system only
can find answers when this is the case, as there is no co-
reference module included.
4. Do answer tags overlap with gold answer?
5. Do they overlap exactly?
6. Other (validation). If all previous checks are passed,
the candidate has correctly been generated by the can-
didate generation stage, but the validation modules
have failed to predict the relation.
On the TAC KBP 2013 queries, the resulting re-
call error analysis is:
error class missing recall
Doc not retrieved 5.59%
Query not matched 10.37%
Answer not in query sentence 16.63%
Answer tag inexact 5.36%
Answer not tagged 24.85%
Other (validation) 37.17%
The demonstration tool allows for inspection of
instances of each of the error classes.
6 Conclusion
This paper illustrates RelationFactory, a modular
open source knowledge-base population system.
We believe that RelationFactory will become es-
pecially valuable for researchers in the field of re-
lation extraction that focus on one particular prob-
lem of knowledge-base-population (such as entity
expansion or relation prediction) and want to inte-
grate their algorithms in an end-to-end setting.
Acknowledgments
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by this Google
Fellowship. Tassilo Barth was supported in part
by IARPA contract number W911NF-12-C-0015.
References
Mike Gancarz. 2003. Linux and the Unix philosophy.
Digital Press.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148?163.
Springer.
Benjamin Roth, Tassilo Barth, Michael Wiegand, Mit-
tul Singh, and Dietrich Klakow. 2013. Effective slot
filling based on shallow distant supervision methods.
In Proceedings of the Sixth Text Analysis Conference
(TAC 2013).
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697?706. ACM.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing and Natural
Language Learning (EMNLP-CoNLL), pages 455?
465. ACL.
Mihai Surdeanu. 2013. Overview of the tac2013
knowledge base population evaluation: English slot
filling and temporal slot filling. In Proceedings of
the Sixth Text Analysis Conference (TAC 2013).
92
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100?105,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Unsupervised Parsing for Generating Surface-Based
Relation Extraction Patterns
Jens Illig
University of Kassel
Wilhelmsh?oher Allee 73
D-34121 Kassel, Germany
illig@cs.uni-kassel.de
Benjamin Roth and Dietrich Klakow
Saarland University
D-66123 Saarbr?ucken, Germany
{benjamin.roth, dietrich.klakow}
@lsv.uni-saarland.de
Abstract
Finding the right features and patterns for
identifying relations in natural language is
one of the most pressing research ques-
tions for relation extraction. In this pa-
per, we compare patterns based on super-
vised and unsupervised syntactic parsing
and present a simple method for extract-
ing surface patterns from a parsed training
set. Results show that the use of surface-
based patterns not only increases extrac-
tion speed, but also improves the quality
of the extracted relations. We find that, in
this setting, unsupervised parsing, besides
requiring less resources, compares favor-
ably in terms of extraction quality.
1 Introduction
Relation extraction is the task of automatically de-
tecting occurrences of expressed relations between
entities in a text and structuring the detected in-
formation in a tabularized form. In natural lan-
guage, there are infinitely many ways to creatively
express a set of semantic relations in accordance to
the syntax of the language. Languages vary across
domains and change over time. It is therefore im-
possible to statically capture all ways of express-
ing a relation.
Most relation extraction systems (Bunescu and
Mooney, 2005; Snow et al., 2005; Zhang et al.,
2006; Mintz et al., 2009; Alfonseca et al., 2012;
Min et al., 2012) generalize semantic relations
by taking into account statistics about the syntac-
tic construction of sentences. Usually supervised
parsers are applied for parsing sentences.
Statistics are then utilized to machine-learn how
textual mentions of relations can be identified.
Many researchers avoid the need for expensive
corpora with manually labeled relations by apply-
ing a scheme called distant supervision (Mintz et
al., 2009; Roth et al., 2013) which hypothesizes
that all text fragments containing argument co-
occurrences of known semantic relation facts in-
deed express these relations. Still, systems rely-
ing on supervised parsers require training from an-
notated treebanks, which are expensive to create,
and highly domain- and language dependent when
available.
An alternative is unsupervised parsing, which
automatically induces grammars by structurally
analyzing unlabeled corpora. Applying unsuper-
vised parsing thus avoids the limitation to lan-
guages and domains for which annotated data is
available. However, induced grammars do not
match traditional linguistic grammars. In most of
the research on parsing, unsupervised parsers are
still evaluated based on their level of correspon-
dence to treebanks. This is known to be prob-
lematic because there are several different ways of
linguistically analyzing text, and treebank anno-
tations also contain questionable analyses (Klein,
2005). Moreover, it is not guaranteed that the syn-
tactic analysis which is most conforming to a gen-
eral linguistic theory is also best suited in an ex-
trinsic evaluation, such as for relation extraction.
In this work, we apply a supervised and an un-
supervised parser to the relation extraction task by
extracting statistically counted patterns from the
resulting parses. By utilizing the performance of
the overall relation extraction system as an indirect
measure of a parser?s practical qualities, we get a
task-driven evaluation comparing supervised and
unsupervised parsers. To the best of our knowl-
edge, this is the first work to compare general-
purpose unsupervised and supervised parsing on
the application of relation extraction. Moreover,
we introduce a simple method to obtain shallow
patterns from syntactic analyses and show that, be-
sides eliminating the need to parse text during sys-
tem application, such patterns also increase extrac-
tion quality. We discover that, for this method, un-
100
supervised parsing achieves better extraction qual-
ity than the more expensive supervised parsing.
1.1 Related Work
Unsupervised and weakly supervised training
methods have been applied to relation extraction
(Mintz et al., 2009; Banko et al., 2007; Yates
and Etzioni, 2009) and similar applications such
as semantic parsing (Poon and Domingos, 2009)
and paraphrase acquisition (Lin and Pantel, 2001).
However, in such systems, parsing is commonly
applied as a separately trained subtask
1
for which
supervision is used.
H?anig and Schierle (2009) have applied unsu-
pervised parsing to a relation extraction task but
their task-specific data prohibits supervised pars-
ing for comparison.
Unsupervised parsing is traditionally only eval-
uated intrinsically by comparison to gold-standard
parses. In contrast, Reichart and Rappoport (2009)
count POS token sequences inside sub-phrases for
measuring parsing consistency. But this count is
not clearly related to application qualities.
2 Methodology
A complete relation extraction system consists of
multiple components. Our system follows the ar-
chitecture described by Roth et al. (2012). In
short, the system retrieves queries in the form
of entity names for which all relations captured
by the system are to be returned. The en-
tity names are expanded by alias-names extracted
from Wikipedia link anchor texts. An information
retrieval component retrieves documents contain-
ing either the name or one of the aliases. Further
filtering retains only sentences where a named en-
tity tagger labeled an occurrence of the queried
entity as being of a suitable type and furthermore
found a possible entity for the relation?s second ar-
gument. For each candidate sentence, a classifier
component then identifies whether one of the cap-
tured relation types is expressed and, if so, which
one it is. Postprocessing then outputs the classi-
fied relation according to task-specific format re-
quirements. Here, we focus on the relation type
classifier.
1
An exception is the joint syntactic and semantic (super-
vised) parsing model inference by Henderson et al. (2013)
2.1 Pattern Extraction
For our relation extraction system, we use a simple
pattern matching framework. Whenever at least
one candidate sentence containing two entities A
and B matches one of the patterns extracted for a
certain relation type R, the classifier states that R
holds between A and B.
We experimented with two types of patterns.
First, we simply parsed the training set and ex-
tracted shortest dependency path patterns. These
patterns search for matches on the parse tree.
Following Lin and Pantel (2001), the shortest
path connecting two arguments in a dependency
graph has been widely used as a representation
of relation instance mentions. The general idea
is that shortest paths skip over irrelevant op-
tional parts of a sentence such as in $1, who
... founded $2 where the shortest path pattern
$1?founded?$2 matches although an irrel-
evant relative clause appears between the argu-
ments $1 and $2. Similar representations have
been used by Mintz et al. (2009), Alfonseca et al.
(2012) and Snow et al. (2005).
In a second set of experiments, we used the
shortest dependency paths in parsed training sen-
tences to generate surface-based patterns. These
patterns search for matches directly on plain text
and therefore do no longer rely on parsing at appli-
cation time. The patterns are obtained by turning
the shortest paths between relational arguments in
the parsed training data into token sequences with
gaps. The token sequences consist of all words
in the sentence that appear on the shortest depen-
dency path. Argument positions in the surface pat-
terns are specified by special tokens $1 and $2.
At all places, where there are one or more tokens
which are not on the shortest dependency path but
which are surrounded either by tokens on the de-
pendency path or by arguments, an asterisk repre-
sents up to four unspecified tokens. For the short-
est path $1?,?who?$2 connecting Friedman
and economist in the DMV parse depicted in Fig-
ure 1, this method generates the pattern $1,
*
$2 who. As can be seen, such patterns can cap-
ture a conjunction of token presence conditions to
the left, between, and to the right of the arguments.
In cases where argument entities are not parsed as
a single complete phrase, we generate patterns for
each possible combination of outgoing edges from
the two arguments. We dismiss patterns generated
for less than four distinct argument entity pairs of
101
Milton Friedman , a conservative economist who died in 2006 at age 94 , received the Nobel Prize for economics in 1976 .
nn
nsubj
punct
det
amod
appos
nsubj
rcmod
prep
pobj
prep
pobj
num
punct
MALT root
det
nn
dobj
prep
pobj
prep
pobj
punct
DMV root
Figure 1: Comparison of a DMV (above text) and a MALT parse (below text) of the same sentence.
the same relation type. For each pattern, we cal-
culate the precision on the training set and retain
only patterns above a certain precision threshold.
2.2 Supervised and Unsupervised Parsing
Typical applications which require syntactic anal-
yses make use of a parser that has been trained un-
der supervision of a labeled corpus conforming to
a linguistically engineered grammar. In contrast,
unsupervised parsing induces a grammar from fre-
quency structures in plain text.
Various algorithms for unsupervised parsing
have been developed in the past decades. Head-
den (2012) gives a rather recent and extensive
overview of unsupervised parsing models. For our
work, we use the Dependency Model with Valence
(DMV) by Klein and Manning (2004). Most of
the more recent unsupervised dependency pars-
ing research is based on this model. DMV is a
generative head-outward parsing model which is
trained by expectation maximization on part-of-
speech (POS) sequences of the input sentences.
Starting from a single root token, head tokens gen-
erate dependants by a probability conditioned on
the direction (left/right) from the head and the
head?s token type. Each head node generates to-
kens until a stop event is generated with a prob-
ability dependent on the same criteria plus a flag
whether some dependant token has already been
generated in the same direction.
For comparison of unsupervised and supervised
parsing, we apply the (Nivre, 2003) determinis-
tic incremental parsing algorithm Nivre arc-eager,
the default algorithm of the MALT framework
2
(Nivre et al., 2007). In this model, for each word
token, an SVM classifier decides for a parser state
transition, which, in conjunction with other deci-
sions, determines where phrases begin and end.
2
http://www.maltparser.org as of Nov. 2013
3 Experiments
We used the plain text documents of the English
Newswire and Web Text Documents provided for
TAC KBP challenge 2011 (Ji et al., 2011). We
automatically annotated relation type mentions in
these documents by distant supervision using the
online database Freebase
3
, i.e. for all relation
types of TAC KBP 2011, we took relation triples
from Freebase and, applying preprocessing as de-
scribed in Section 2, we retrieved sentences men-
tioning both arguments of some Freebase relation
with matching predicted entity types. We hypothe-
size that all sentences express the respective Free-
base relation. This way we retrieved a distantly
supervised training set of 480 622 English sen-
tences containing 92468 distinct relation instances
instantiating 41 TAC KBP relation types.
3.1 Training and Evaluation
From our retrieved set of sentences, we took those
with a maximum length of 10 tokens and trans-
formed them to POS sequences. We trained DMV
only on this dataset of short POS sequences, which
we expect to form mentions of a modeled relation.
Therefore, we suspect that DMV training assigns
an increased amount of probability mass to depen-
dency paths along structures which are truly re-
lated to these relations. We used the DMV imple-
mentation from Cohen and Smith (2009)
4
.
For the supervised Nivre arc-eager parser we
used MALT (Nivre et al., 2007) with a pre-trained
Penn Treebank (Marcus et al., 1993) model
5
. As
a baseline, we tested left branching parses i.e.
3
http://www.freebase.com as of Nov. 2013
4
publicly available at http://www.ark.cs.cmu.
edu/DAGEEM/ as of Nov. 2013 (parser version 1.0).
5
http://www.maltparser.org/mco/
english_parser/engmalt.linear-1.7.mco
as of Nov. 2013
102
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
m
icr
o-a
ve
rag
e K
BP
 F 1
threshold on pattern-precision
lbranch
dmv surface
dmv dep-graph
malt surface
malt dep-graph
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
pre
cis
ion
recall
lbranch
dmv surface
dmv dep-graph
malt surface
malt dep-graph
Figure 2: micro-averaged F
1
and precision&recall results for varied training precision thresholds
pattern set (+additional DMV pattern) precision recall F
1
MALT generated patterns only .1769 .2010 .1882
+p:title $1 * $2 of +0.73% +8.40% +4.14%
+p:title $1 , * $2 of +0.90% +4.22% +2.39%
+o:state of hqs $1 * in * , $2 +1.35% +1.59% +1.43%
+p:title $1 , * $2 who +0.90% +1.35% +1.22%
+o:parents $1 , * by $2 +0.62% +1.35% +1.06%
+o:city of hqs $1 , * in $2 , +1.01% +1.04% +1.00%
+p:origin $2 ?s $1 won the +0.84% +1.04% +0.95%
+p:employee of $1 * $2 ?s chief +0.28% +1.04% +0.79%
+o:website $1 : $2 +0.28% +1.04% +0.79%
Table 1: DMV patterns improving MALT results
the most, when added to the MALT patternset
dependency trees solely consisting of head-to-
dependent edges from the right to the left
6
.
All the extracted sentences were parsed and pat-
terns were extracted from the parses. The patterns
were then applied to the corpus and their precision
was determined according to Freebase. With dif-
ferent cut-off values on training precision, the full
relation extraction pipeline described in Section 2
was evaluated with respect to the Slot Filling test
queries of TAC KBP 2011.
3.2 Results
Figure 2 (left) depicts F
1
-measured testset results
for pattern sets with varying training precision
thresholds. Figure 2 (right) shows a precision re-
call plot of the same data points.
As can be seen in Figure 2 (left), flattening
graph patterns to surface-based patterns increased
the overall F
1
score. The curve for MALT gen-
erated surface patterns in Figure 2 (right) shows
no increase in precision towards low recall levels
where only the highest-training-precision patterns
are retained. This indicates a lack of precision
6
Since for such parses the shortest path is the complete
observed word sequence between the two relation arguments,
surface and parse-tree patterns become equal.
in MALT-based surface patterns. In contrast, the
corresponding DMV-based graph increases mono-
tonically towards lower recall levels, which is re-
flected by the highest F
1
score (Figure 2, left).
Table 1 shows the increases in evaluation score
of those DMV-generated patterns which help most
to more precisely identify relations when added to
the set of all MALT-generated patterns (sorted by
F
1
score). Figure 1 compares the syntactic analy-
ses of MALT and DMV for an example sentence
where DMV generates one of the listed patterns.
The numbers of Table 1 indicate that such patterns
are missing without alternatives in the pattern set
gained from supervised parsing.
4 Conclusion
We have presented a simple method for generat-
ing surface-based patterns from parse trees which,
besides avoiding the need for parsing test data,
also increases extraction quality. By comparing
supervised and unsupervised parsing, we further-
more found that unsupervised parsing not only
eliminates the dependency on expensive domain-
specific training data, but also produce surface-
based extraction patterns of increased quality. Our
results emphasize the need for task-driven evalu-
ation of unsupervised parsing methods and show
that there exist indicative structures for relation ex-
traction beyond widely agreed-on linguistic syntax
analyses.
5 Acknowledgements
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by this Google
Fellowship.
103
References
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort,
and Guillermo Garrido. 2012. Pattern learning for
relation extraction with a hierarchical topic model.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Short Pa-
pers - Volume 2, ACL ?12, pages 54?59, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence, IJCAI?07, pages 2670?
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages
724?731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL
?09, pages 74?82, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christian H?anig and Martin Schierle. 2009. Rela-
tion extraction based on unsupervised syntactic pars-
ing. In Gerhard Heyer, editor, Text Mining Ser-
vices, Leipziger Beitr?age zur Informatik, pages 65?
70, Leipzig, Germany. Leipzig University.
William Headden. 2012. Unsupervised Bayesian Lexi-
calized Dependency Grammar Induction. Ph.D. the-
sis, Brown University.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multi-lingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Computational Linguistics, 39(4).
Heng Ji, Ralph Grishman, and Hoa Dang. 2011.
Overview of the TAC2011 knowledge base popula-
tion track. In TAC 2011 Proceedings Papers.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In ACL, ACL
?04, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Dan Klein. 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Dekang Lin and Patrick Pantel. 2001. DIRT: Discov-
ery of Inference Rules from Text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD?01), pages 323?328, New York, NY, USA.
ACM Press.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313?330, June.
Bonan Min, Xiang Li, Ralph Grishman, and Sun Ang.
2012. New york university 2012 system for kbp
slot filling. In Proceedings of the Fifth Text Analysis
Conference (TAC 2012). National Institute of Stan-
dards and Technology (NIST), November.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 1003?1011. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
?09, pages 1?10, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Roi Reichart and Ari Rappoport. 2009. Automatic se-
lection of high quality parses created by a fully un-
supervised parser. In Proceedings of the Thirteenth
Conference on Computational Natural Language
Learning, CoNLL ?09, pages 156?164, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Benjamin Roth, Grzegorz Chrupala, Michael Wiegand,
Singh Mittul, and Klakow Dietrich. 2012. General-
izing from freebase and patterns using cluster-based
distant supervision for tac kbp slotfilling 2012. In
Proceedings of the Fifth Text Analysis Conference
(TAC 2012), Gaithersburg, Maryland, USA, Novem-
ber. National Institute of Standards and Technology
(NIST).
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow. 2013. A survey of noise reduction
104
methods for distant supervision. In Proceedings of
the 2013 workshop on Automated knowledge base
construction, pages 73?78. ACM.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. J. Artif. Int. Res., 34(1):255?
296, March.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 825?832, Stroudsburg, PA,
USA. Association for Computational Linguistics.
105
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 325?328,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Word Co-occurrence for Term Clustering
in Language Model-based Sentence Retrieval
Saeedeh Momtazi
Spoken Language Systems
Saarland University
saeedeh.momtazi
@lsv.uni-saarland.de
Sanjeev Khudanpur
Center for Language
and Speech Processing
Johns Hopkins University
khudanpur@jhu.edu
Dietrich Klakow
Spoken Language Systems
Saarland University
dietrich.klakow
@lsv.uni-saarland.de
Abstract
Sentence retrieval is a very important part of
question answering systems. Term clustering,
in turn, is an effective approach for improving
sentence retrieval performance: the more simi-
lar the terms in each cluster, the better the per-
formance of the retrieval system. A key step in
obtaining appropriate word clusters is accurate
estimation of pairwise word similarities, based
on their tendency to co-occur in similar con-
texts. In this paper, we compare four differ-
ent methods for estimating word co-occurrence
frequencies from two different corpora. The re-
sults show that different, commonly-used con-
texts for defining word co-occurrence differ
significantly in retrieval performance. Using an
appropriate co-occurrence criterion and corpus
is shown to improve the mean average preci-
sion of sentence retrieval form 36.8% to 42.1%.
1 Corpus-Driven Clustering of Terms
Since the search in Question Answering (QA) is con-
ducted over smaller segments of text than in docu-
ment retrieval, the problems of data sparsity and ex-
act matching become more critical. The idea of using
class-based language model by applying term clus-
tering, proposed by Momtazi and Klakow (2009), is
found to be effective in overcoming these problems.
Term clustering has a very long history in natu-
ral language processing. The idea was introduced
by Brown et al (1992) and used in different appli-
cations, including speech recognition, named entity
tagging, machine translation, query expansion, text
categorization, and word sense disambiguation. In
most of the studies in term clustering, one of several
well-know notions of co-occurrence?appearing in
the same document, in the same sentence or follow-
ing the same word?has been used to estimate term
similarity. However, to the best of our knowledge,
none of them explored the relationship between dif-
ferent notions of co-occurrence and the effectiveness
of their resulting clusters in an end task.
In this research, we present a comprehensive study
of how different notions of co-occurrence impact re-
trieval performance. To this end, the Brown algo-
rithm (Brown et al, 1992) is applied to pairwise word
co-occurrence statistics based on different definitions
of word co-occurrence. Then, the word clusters are
used in a class-based language model for sentence
retrieval. Additionally, impact of corpus size and do-
main on co-occurrence estimation is studied.
The paper is organized as follows. In Section 2,
we give a brief description of class-based language
model for sentence retrieval and the Brown word
clustering algorithm. Section 3 presents different
methods for estimating the word co-occurrence. In
Section 4, experimental results are presented. Fi-
nally, Section 5 summarizes the paper.
2 Term Clustering Method and Application
In language model-based sentence retrieval, the prob-
ability P (Q|S) of generating query Q conditioned on
a candidate sentence S is first calculated. Thereafter
sentences in the search collection are ranked in de-
scending order of this probability. For word-based
unigram, P (Q|S) is estimated as
P (Q|S) =
?
i=1...M
P (qi|S), (1)
where M is the number of query terms, qi denotes the
ith query term in Q, and S is the sentence model.
325
For class-based unigrams, P (Q|S) is computed
using only the cluster labels of the query terms as
P (Q|S) =
?
i=1...M
P (qi|Cqi , S)P (Cqi |S), (2)
where Cqi is the cluster that contains qi and
P (qi|Cqi , S) is the emission probability of the
ith query term given its cluster and the sen-
tence. P (Cqi |S) is analogous to the sentence model
P (qi|S) in (1), but is based on clusters instead of
terms. To calculate P (Cqi |S), each cluster is con-
sidered an atomic entity, with Q and S interpreted as
sequences of such entities.
In order to cluster lexical items, we use the al-
gorithm proposed by Brown et al(1992), as imple-
mented in the SRILM toolkit (Stolcke, 2002). The al-
gorithm requires an input corpus statistics in the form
?w,w?, fww??, where fww? is the number of times the
word w? is seen in the context w. Both w and w? are
assumed to come from a common vocabulary. Be-
ginning with each vocabulary item in a separate clus-
ter, a bottom-up approach is used to merge the pair of
clusters that minimizes the loss in Average Mutual In-
formation (AMI) between the word cluster Cw? and
its context cluster Cw. Different words seen in the
same contexts are good candidates for merger, as are
different contexts in which the same words are seen.
While originally proposed with bigram statistics,
the algorithm is agnostic to the definition of co-
occurrence. E.g. if ?w,w?? are verb-object pairs,
the algorithm clusters verbs based on their selectional
preferences, if fww? is the number of times w and w?
appear in the same document, it will produce seman-
tically (or topically) related word-clusters, etc.
Several notions of co-occurrence have been used
in the literature to cluster words, as described next.
3 Notions of Word Co-occurrence
Co-occurrence in a Document
If two content words w and w? are seen in the
same document, they are usually topically related. In
this notion of co-occurrence, how near or far away
from each other they are in the document is irrele-
vant, as is their order of appearance in the document.
Document-wise co-occurrence has been successfully
used in many NLP applications such as automatic
thesaurus generation (Manning et al, 2008)
Statistics of document-wise co-occurrence may be
collected in two different ways. In the first case,
fww? = fw?w is simply the number of documents that
contain both w and w?. This is usually the notion
used in ad hoc retrieval. Alternatively, we may want
to treat each instance of w? in a document that con-
tains an instance of w to be a co-occurrence event.
Therefore if w? appears three times in a document
that contains two instances of w, the former method
counts it as one co-occurrence, while the latter as six
co-occurrences. We use the latter statistic, since we
are concerned with retrieving sentence sized ?docu-
ments,? wherein a repeated word is more significant.
Co-occurrence in a Sentence
Since topic changes sometimes happen within a
single document, and our end task is sentence re-
trieval, we also investigate the notion of word co-
occurrence in a smaller segment of text such as a
sentence. In contrast to the document-wise model,
sentence-wise co-occurrence does not consider whole
documents, and only concerns itself with the number
of times that two words occur in the same sentence.
Co-occurrence in a Window of Text
The window-wise co-occurrence statistic is an even
narrower notion of context, considering only terms in
a window surrounding w?. Specifically, a window of
a fixed size is moved along the text, and fww? is set
as the number of times both w and w? appear in the
window. Since the window size is a free parameter,
different sizes may be applied. In our experiments we
use two window sizes, 2 and 5, that have been studied
in related research (Church and Hanks, 1990).
Co-occurrence in a Syntactic Relationship
Another notion of word similarity derives from
having the same syntactic relationship with the con-
text w. This syntax-wise co-occurrence statistic is
similar to the sentence-wise co-occurrence, in that
co-occurrence is defined at the sentence level. How-
ever, in contrast to the sentence-wise model, w and
w? are said to co-occur only if there is a syntactic re-
lation between them in that sentence. E.g., this type
of co-occurrence can help cluster nouns that are used
as objects of same verb, such as ?tea?, ?water?, and
?cola,? which all are used with the verb ?drink?.
To gather such statistics, all sentences in the corpus
must be syntactically parsed. We found that a depen-
dency parser is an appropriate tool for our goal: it
326
directly captures dependencies between words with-
out the mediation of any virtual (nonterminal) nodes.
Having all sentences in the parsed format, fww? is de-
fined as the number of times that the words w and w?
have a parent-child relationship of any syntactic type
in the dependency parse tree. For our experiments we
use MINIPAR (Lin, 1998) to parse the whole corpus
due to its robustness and speed.
4 Sentence Retrieval Experiments
4.1 Derivatives of the TREC QA Data Sets
The set of questions from the TREC 2006 QA track1
was used as the test data to evaluate our models,
while the TREC 2005 set was used for development.
The TREC 2006 QA task contains 75 question-
series, each on one topic, for a total of 403 factoid
questions which is used as queries for sentence re-
trieval. For sentence-level relevance judgments, the
Question Answer Sentence Pair corpus of Kaisser
and Lowe (2008) was used. All the documents
that contain relevant sentences are from the NIST
AQUAINT1 corpus.
QA systems typically employ sentence retrieval af-
ter initial, high quality document retrieval. To simu-
late this, we created a separate search collection for
each question using all sentences from all documents
relevant to the topic (question-series) from which the
question was derived. On average, there are 17 rel-
evant documents per topic, many not relevant to the
question itself: they may be relevant to another ques-
tion. So the sentence search collection is realistic,
even if somewhat optimistic.
4.2 Corpora for Term Clustering
We investigated two different corpora2, AQUAINT1
and Google n-grams, to obtain word co-occurrence
statistics for term clustering. Based on this we can
also evaluate the impact of corpus size and corpus
domain on the result of term clustering.
AQUAINT1 consists of English newswire text ex-
tracted from the Xinhua, the New York Times and the
Associated Press Worldstream News Services.
The Google n-gram counts were generated from
publicly accessible English web pages. Since there is
1See http://trec.nist.gov.
2See catalog numbers LDC2002T31 and LDC2006T13 re-
spectively at http://www.ldc.upenn.edu/Catalog.
Corpus Co-occurrence # Word Pairs
AQUAINT1 document 368,109,133
AQUAINT1 sentence 104,084,473
AQUAINT1 syntax 12,343,947
AQUAINT1 window-5 46,307,650
AQUAINT1 window-2 14,093,661
Google n-grams window-5 12,005,479
Google n-grams window-2 328,431,792
Table 1: Statistics for different notions of co-occurrence.
no possibility of extracting document-wise, sentence-
wise or syntax-wise co-occurrence statistics from the
Google n-gram corpus, we only collect window-wise
statistics to the extent available in the corpus.
Table 1 shows the number of word pairs extracted
from the two corpora with different definitions of co-
occurrence. The statistics only include word pairs
for which both constituent words are present in the
35,000 word vocabulary of our search collection.
4.3 Sentence Retrieval Results and Discussion
Sentence retrieval performance for term clustering
using different definitions of word co-occurrence is
shown in Figure 1. Since the Brown algorithm re-
quires specifying the number of clusters, tests were
conducted for 50, 100, 200, 500, and 1000 clusters
of the term vocabulary. The baseline system is the
word-based sentence retrieval model of Equation (1).
Figure 1(a) shows the Mean Average Precision
(MAP) for class-based sentence retrieval of Equation
(2) using clusters based on different co-occurrence
statistics from AQUAINT1. Note that
(i) the best result achieved by sentence-wise co-
occurence is better the best result of document-
wise, perhaps due to more local and relevant in-
formation that it captures;
(ii) all the results achieved by syntax-wise co-
occurrence are better than sentence-wise, indi-
cating that merely co-occurring in a sentence
is not very indicative of word similarity, while
relations extracted from syntactic structure im-
prove system performance significantly;
(iii) window-2 significantly outperforms all other
notions of co-occurrence; i.e., the bigram statis-
tics achieve the best clustering results. In com-
parison, window-5 has the worst results, with
performance very close to baseline.
Although window-5 co-occurrence has been reported
327
50 5000.35
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
document sentence window2 window5 syntax base?line
log?of?number?of?clusters
MAP
50 5000.35
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
AQUAINT?window2 Google?window2 Google?window5 base?line
log?of?number?of?clusters
MAP
(b)(a)
Figure 1: MAP of sentence retrieval for different word co-occurrence statistics from AQUAINT1 and Google n-grams.
to be effective in other applications, it is not helpful
in sentence retrieval.
Figure 1(b) shows the MAP for class-based sen-
tence retrieval of Equation (2) when window-wise
co-occurrence statistics from the Google n-grams are
used. For better visualization, we repeated the MAP
results using AQUAINT1 window-2 co-occurrence
statistics from Figure 1(a) in 1(b). Note that
(iv) window-2 co-occurrence statistics significantly
outperform window-5 for the Google n-grams,
consistent with results from AQUAINT1;
(v) Google n-gram window-2 co-occurrence statis-
tics consistently result in better MAP than
AQUAINT window-2.
The last result indicates that even though the Google
n-grams are from a different (and much broader) do-
main than the test data, they significantly improve the
system performance due to sheer size. Finally
(vi) Google n-gram window-2 MAP curve is flatter
than AQUAINT window-2; i.e., performance is
not very sensitive to the number of clusters.
The best overall result is from Google window-2
co-occurrence statistics with 100 clusters, achiev-
ing 42.1% MAP while the best result derived
from AQUAINT1 is 41.7% MAP for window-2 co-
occurrence with 100 clusters, and the MAP of the
word-based model (baseline) is 36.8%.
5 Concluding Remarks
We compared different notions of word co-
occurrence for clustering terms, using document-
wise, sentence-wise, window-wise, and syntax-wise
co-occurrence statistics derived from AQUAINT1.
We found that different notions of co-occurrence sig-
nificantly change the behavior of a sentence retrieval
system, in which window-wise model with size 2
achieves the best result. In addition, Google n-grams
were used for window-wise model to study the im-
pact of corpus size and domain on the clustering re-
sult. The result showed that although the domain of
the Google n-grams is dissimilar to the test set, it
outperforms models derived from AQUAINT1 due to
sheer size.
Acknowledgments
Saeedeh Momtazi is funded by the German research
foundation DFG through the International Research
Training Group (IRTG 715).
References
P.F. Brown, V.J.D. Pietra, P.V. Souza, J.C. Lai, and R.L.
Mercer. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18(4):467?479.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
M. Kaisser and J.B. Lowe. 2008. Creating a research
collection of question answer sentence pairs with Ama-
zon?s mechanical turk. In Proc. of LREC.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of the Evaluation of Parsing Systems
Workshop.
C.D. Manning, P. Raghavan, and H. Schu?tze. 2008. Intro-
duction to Information Retrieval. Cambridge Univer-
sity Press.
S. Momtazi and D. Klakow. 2009. A word clustering
approach for language model-based sentence retrieval
in question answering systems. In Proc. of ACM CIKM.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
328
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 795?803,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Convolution Kernels for Opinion Holder Extraction
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
Opinion holder extraction is one of the impor-
tant subtasks in sentiment analysis. The ef-
fective detection of an opinion holder depends
on the consideration of various cues on vari-
ous levels of representation, though they are
hard to formulate explicitly as features. In this
work, we propose to use convolution kernels
for that task which identify meaningful frag-
ments of sequences or trees by themselves.
We not only investigate how different levels
of information can be effectively combined
in different kernels but also examine how the
scope of these kernels should be chosen. In
general relation extraction, the two candidate
entities thought to be involved in a relation are
commonly chosen to be the boundaries of se-
quences and trees. The definition of bound-
aries in opinion holder extraction, however, is
less straightforward since there might be sev-
eral expressions beside the candidate opinion
holder to be eligible for being a boundary.
1 Introduction
In recent years, there has been a growing interest
in the automatic detection of opinionated content
in natural language text. One of the more impor-
tant tasks in sentiment analysis is the extraction of
opinion holders. Opinion holder extraction is one
of the critical components of an opinion question-
answering system (i.e. systems which automatically
answer opinion questions, such as ?What does [X]
like about [Y]??). Such systems need to be able to
distinguish which entities in a candidate answer sen-
tence are the sources of opinions (= opinion holder)
and which are the targets.
On other NLP tasks, in particular, on relation extrac-
tion, there has been much work on convolution ker-
nels, i.e. kernel functions exploiting huge amounts
of features without an explicit feature representa-
tion. Previous research on that task has shown that
convolution kernels, such as sequence and tree ker-
nels, are quite effective when compared to manual
feature engineering (Moschitti, 2008; Bunescu and
Mooney, 2005; Nguyen et al, 2009). In order to
effectively use convolution kernels, it is often nec-
essary to choose appropriate substructures of a sen-
tence rather than represent the sentence as a whole
structure (Bunescu and Mooney, 2005; Zhang et al,
2006; Moschitti, 2008). As for tree kernels, for ex-
ample, one typically chooses the syntactic subtree
immediately enclosing two entities potentially ex-
pressing a specific relation in a given sentence. The
opinion holder detection task is different from this
scenario. There can be several cues within a sen-
tence to indicate the presence of a genuine opinion
holder and these cues need not be member of a par-
ticular word group, e.g. they can be opinion words
(see Sentences 1-3), communication words, such as
maintained in Sentence 2, or other lexical cues, such
as according in Sentence 3.
1. The U.S. commanders consideropinion the prisoners to be un-
lawful combatantsopinion as opposed to prisoners of war.
2. During the summit, Koizumi maintainedcommunication a
clear-cut collaborative stanceopinion towards the U.S. and em-
phasized that the President was objectiveopinion and circum-
spect.
3. Accordingcue to Fernandez, it was the worst mistakeopinion in
the history of the Argentine economy.
795
Thus, the definition of boundaries of the structures
for the convolution kernels is less straightforward in
opinion holder extraction.
The aim of this paper is to explore in how far convo-
lution kernels can be beneficial for effective opinion
holder detection. We are not only interested in how
far different kernel types contribute to this extraction
task but we also contrast the performance of these
kernels with a manually designed feature set used
as a standard vector kernel. Finally, we also exam-
ine the effectiveness of expanding word sequences
or syntactic trees by additional prior knowledge.
2 Related Work
Choi et al (2005) examine opinion holder extraction
using CRFs with various manually defined linguis-
tic features and patterns automatically learnt by the
AutoSlog system (Riloff, 1996). The linguistic fea-
tures focus on named-entity information and syntac-
tic relations to opinion words. In this paper, we use
very similar settings. The features presented in Kim
and Hovy (2005) and Bloom et al (2007) resemble
very much Choi et al (2005). Bloom et al (2007)
also consider communication words to be predictive
cues for opinion holders.
Kim and Hovy (2006) and Bethard et al (2005) ex-
plore the usefulness of semantic roles provided by
FrameNet (Fillmore et al, 2003) for both opinion
holder and opinion target extraction. Due to data
sparseness, Kim and Hovy (2006) expand FrameNet
data by using an unsupervised clustering algorithm.
Choi et al (2006) is an extension of Choi et al
(2005) in that opinion holder extraction is learnt
jointly with opinion detection. This requires that
opinion expressions and their relations to opinion
holders are annotated in the training data. Seman-
tic roles are also taken as a potential source of in-
formation. In our work, we deliberately work with
minimal annotation and, thus, do not consider any
labeled opinion expressions and relations to opinion
holders in the training data. We exclusively rely on
entities marked as opinion holders. In many practi-
cal situations, the annotation beyond opinion holder
labeling is too expensive.
Complex convolution kernels have been success-
fully applied to various NLP tasks, such as rela-
tion extraction (Bunescu and Mooney, 2005; Zhang
et al, 2006; Nguyen et al, 2009), question an-
swering (Zhang and Lee, 2003; Moschitti, 2008),
and semantic role labeling (Moschitti et al, 2008).
In all these tasks, they offer competitive perfor-
mance to manually designed feature sets. Bunescu
and Mooney (2005) combine different sequence ker-
nels encoding different contexts of candidate en-
tities in a sentence. They argue that several ker-
nels encoding different contexts are more effective
than just using one kernel with one specific context.
We build on that idea and compare various scopes
eligible for opinion holder extraction. Moschitti
(2008) and Nguyen et al (2009) suggest that differ-
ent kinds of information, such as word sequences,
part-of-speech tags, syntactic and semantic informa-
tion should be contained in separate convolution ker-
nels. We also adhere to this notion.
3 Data
As labeled data, we use the sentiment annotation of
the MPQA 2.0 corpus1. Opinion holders are not ex-
plicitly labeled as such. However sources of pri-
vate states and subjective speech events (Wiebe et
al., 2003) are a fairly good approximation of the
task. Previous work (Choi et al, 2005; Kim and
Hovy, 2005; Choi et al, 2006) uses similar approxi-
mations.
4 Method
In this work, we consider all noun phrases (NPs)
as possible candidate opinion holders. Therefore,
the set of all data instances is the set of the NPs
within the MPQA 2.0 corpus. Each NP is labeled
as to whether it is a genuine opinion holder or not.
Throughout this section, we will use Sentence 2
from Section 1 as an example.
4.1 The Different Levels of Representation
Several levels of representation are important for
opinion holder extraction. Table 1 lists all the dif-
ferent levels that are used in this work. Generalized
sequences employ named-entity tags, an OPINION
tag for opinion words and a COMM tag for com-
munication words2. Thus, in a generalized word se-
1www.cs.pitt.edu/mpqa/databaserelease
2Note that all candidate tokens are reduced to one generic
CAND token. Thus, we hope to account for data sparseness in
796
quence (WRDGN ) a word is replaced by a general-
ized token whereas in a generalized part-of-speech
sequence (POSGN ) a part-of-speech tag is replaced.
For augmented constituent trees (CONSTAUG), the
same sources of information are used. The differ-
ence to generalizing sequences is that instead of re-
placing words by generalized tokens, we add a node
in the syntax tree with a generalized token so that it
dominates the pertaining leaf node (see also nodes
marked with AUG in Figure 2). All sources used for
this type of generalization are known to be predictive
for opinion holder classification (Choi et al, 2005;
Kim and Hovy, 2005; Choi et al, 2006; Kim and
Hovy, 2006; Bloom et al, 2007).
Note that the grammatical relation paths, i.e.
GRAMWRD and GRAMPOS , can only be applied
in case there is another expression in the focus in
addition to the candidate of the data instance itself,
e.g. the nearest opinion expression to the candidate.
Section 4.4 explains in detail how this is done.
Predicate-argument structures (PAS) are repre-
sented by PropBank trees (Kingsbury and Palmer,
2002).
4.2 Support Vector Machines and Kernel
Methods
Support Vector Machines (SVMs) are one of the
most robust supervised machine learning techniques
in which training data instances ~x are separated by a
hyperplane H(~x) = ~w ? ~x + b = 0 where w ? Rn
and b ? R. One advantage of SVMs is that ker-
nel methods can be applied which map the data to
other feature spaces in which they can be separated
more easily. Given a feature function ? : O ? R,
where O is the set of the objects, the kernel trick
allows the decision hyperplane to be rewritten as:
H(~x) =
(
?
i=1...l
yi?i~xi
)
? ~x + b =
?
i=1...l
yi?i~xi ? ~x+ b =
?
i=1...l
yi?i? (oi) ? ? (o) + b
where yi is equal to 1 for positive and ?1 for
negative examples, ?i ? R with ?i ? 0, oi?i ?
{1, . . . , l} are the training instances and the product
K(oi, o) = ??(oi) ? ?(o)? is the kernel function as-
sociated with the mapping ?.
case there are several tokens making up the candidate.
4.3 Sequence and Tree Kernels
A sequence kernel (SK) measures the similarity
of two sequences by counting the number of com-
mon subsequences. We use the kernel by Taylor
and Christianini (2004) which has the advantage that
it also considers subsequences of the original se-
quence with some elements missing. The extent of
these gaps in a sequence is suitably reflected by a
weighting function incorporated into the kernel.
Tree kernels (TKs) represent trees by their sub-
structures. The feature space of these substructures,
or fragments, is mapped onto a vector space. The
kernel function computes the similarity of pairs of
trees by counting the number of common fragments.
In this work, we evaluate two tree kernels: Subset
Tree Kernel (STK) (Collins and Duffy, 2002) and
Partial Tree Kernel (PTKbasic) (Moschitti, 2006).
In STK , a tree fragment can be any set of nodes
and edges of the original tree provided that every
node has either all or none of its children. This con-
straint makes that kind of kernel well-suited for con-
stituency trees which have been generated by con-
text free grammars since the constraint corresponds
to the restriction that no grammatical rule must be
broken. For example, STK enforces that a subtree,
such as [VP [VBZ, NP]], cannot be matched with
[VP [VBZ]] since the latter VP node only possesses
one of the children of the former.
PTKbasic is more flexible since the constraint
of STK on nodes is relaxed. This makes this
type of tree kernel less suitable for constituency
trees. We, therefore, apply it only to trees
representing predicate-argument structures (PAS)
(see Figure 1). Note that a data instance is
represented by a set of those structures3 rather
than a single structure. Thus, the actual partial
tree kernel function we use for this task, PTK ,
sums over all possible pairs PASl and PASm of
two data instances xi and xj: PTK(xi, xj) =
?
PASl?xi
?
PASm?xj
PTKbasic(PASl, PASm).
To summarize, Table 2 lists the different kernel
types we use coupled with the suitable levels of rep-
resentation. This choice of pairing has already been
motivated and empirically proven suitable on other
3i.e. all predicate-argument structures of a sentence in which
the head of the candidate opinion holder occurs
797
Type Description Example
WRD sequence of words During the summit , KoizumiCAND maintained a clear-cut
collaborative stance . . .
WRDGN sequence of generalized words During the summit , CAND COMM OPINION . . .
POS part-of-speech sequence IN DET NN PUNC CAND VBD DET JJ JJ NN . . .
POSGN generalized part-of-speech sequence IN DET NN PUNC CAND COMM OPINION . . .
CONST constituency tree see Figure 2 without nodes marked AUG
CONSTAUG augmented constituency tree see Figure 2
GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJ? maintained DOBJ? stance
GRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJ? VBD DOBJ? NN
PAS predicate argument structures see Figure 1(a)
PASAUG augmented predicate argument structures see Figure 1(b)
Table 1: The different levels of representation.
(a) plain
(b) augmented
Figure 1: Predicate-argument structures (PAS).
tasks (Moschitti, 2008; Nguyen et al, 2009).
Type Description Levels of Representation
SK Sequential Kernel WRD(GN) , POS(GN),
GRAMWRD , GRAMPOS
STK Subset Tree Kernel CONST(AUG)
PTK Partial Tree Kernel PAS
V K Vector Kernel not restricted
Table 2: The different types of kernels.
4.4 The Different Scopes
We argue that using the entire word sequence or syn-
tax tree of the sentence in which a candidate opinion
holder is situated to represent a data instance pro-
duces too large structures for a convolution kernel.
Since a classifier based on convolution kernels has
to derive meaningful features by itself, the larger
these structures are, the more likely noise is included
in the model. Previous work in relation extraction
has also shown that the usage of more focused sub-
structures, e.g. the smallest subtree containing the
two candidate entities of a relation, is more effec-
tive (Zhang et al, 2006). Unfortunately, in our task
there is only one explicit entity we know of for each
data instance which is the candidate opinion holder.
However, there are several indicative cues within the
context of the candidate which might be considered
important. We identify three different cues being the
nearest predicate, i.e. full verb or nominalization,
opinion word and communication word4. For each
of these expressions, we define a scope where the
boundaries are the candidate opinion holder and the
pertaining cue. Given these scopes, we can define
resulting subsequences/subtrees and combine them.
We further add two background scopes, one being
the semantic scope of the candidate opinion holder
and the entire sentence. As semantic scope we con-
sider the subclause in which a candidate opinion
holder is situated5 .
Figure 2 illustrates the different scopes. Abbre-
viations are explained in Table 3. As already men-
tioned in Section 4.1 for grammatical relation paths,
a second expression in addition to the candidate
opinion holder is required. These expressions can be
derived from the different scopes, i.e. for PRED it
4These three expressions may coincide but do not have to.
5Typically, the subtree representing a subclause has the clos-
est S node dominating the candidate opinion holder as the root
node and it contains only those nodes from the original sentence
parse which are also dominated by that S node and whose path
to that node does not contain another S node.
798
is the nearest predicate to the candidate, for OP it is
the nearest opinion word and for COMM it is the
nearest communication word. For the background
scopes SEM and SENT , however, there is no sec-
ond expression in focus. Therefore, grammatical re-
lation paths cannot be defined for these scopes.
Type Description
PRED scope with the boundaries being the candidate opinion
holder and the nearest predicate
OP scope with the boundaries being the candidate opinion
holder and nearest opinion word
COMM scope with the boundaries being the candidate opinion
holder and the nearest communication word
SEM semantic scope of the candidate opinion holder, i.e.
subclause containing the candidate
SENT entire sentence in which in the opinion holder occurs
Table 3: The different types of scope.
4.5 Manually Designed Feature Set for a
Standard Vector Kernel
In addition to the different types of convolution ker-
nels, we also define an explicit feature set for a vec-
tor kernel (V K). Many of these features mainly de-
scribe properties of the relation between the candi-
date and the nearest predicate6 since in our initial
experiments the nearest predicate has always been
the strongest cue. Adding these types of features
for other cues, e.g. the nearest opinion or commu-
nication word, only resulted in a decrease in perfor-
mance. Table 4 lists all the features we use. Note
that this manual feature set employs all those sources
of information which are also exploited by the con-
volution kernels. Some of the information contained
in the convolution kernels can, however, only be rep-
resented in a more simplified fashion when using
a manual feature set. For example, the first PAS
in Figure 1(a) is converted to just the pair of pred-
icate and argument representing the candidate (i.e.
REL:maintain A0:Koizumi). The entire PAS is not
used since it would create too sparse features. Con-
volution kernels can cope with fairly complex struc-
tures as input since they internally match substruc-
tures. Manual features are less flexible since they do
not account for partial matches.
6We select the nearest predicate by using the syntactic parse
tree. Thus, we hope to select the predicate which syntactically
headword/governing category of CAND
is CAND capitalized/a person?
is CAND subj|dobj|iobj|pobj of OPINION/COMM?
is CAND preceded by according to? (Choi et al, 2005)
does CAND contain possessive and is followed by OPIN-
ION/COMM? (Choi et al, 2005)
is CAND preceded by by which is attached to OPINION/COMM?
(Choi et al, 2005)
predicate-argument pairs in which CAND occurs
lemma/part-of-speech tag/subcategorization frame/voice of nearest
predicate
is nearest predicate OPINION/COMM?
does CAND precede/follow nearest predicate?
words between nearest predicate and CAND (bag of words)
part-of-speech sequence between nearest predicate and CAND
constituency path/grammatical relation path from predicate to
CAND
Table 4: Manually designed feature set.
5 Experiments
We used 400 documents of the MPQA corpus for
five-fold crossvalidation and 133 documents as a de-
velopment set. We report statistical significance on
the basis of a paired t-test using 0.05 as the signif-
icance level. All experiments were done with the
SVM-Light-TK toolkit7. We evaluated on the basis
of exact phrase matching. We set the trade-off pa-
rameter j = 5 for all feature sets. For the manual
feature set we used a polynomial kernel of third de-
gree. These two critical parameters were tuned on
the development set. As far as the sequence and
tree kernels are concerned, we used the parameter
settings from Moschitti (2008), i.e. ? = 0.4 and
? = 0.4. Kernels were combined using plain sum-
mation. The documents were parsed using the Stan-
ford Parser (Klein and Manning, 2003). Named-
entity information was obtained by the Stanford tag-
ger (Finkel et al, 2005). Semantic roles were ob-
tained by using the parser by Zhang et al (2008).
Opinion expressions were identified using the Sub-
jectivity Lexicon from the MPQA project (Wil-
son et al, 2005). Communication words were ob-
tained by using the Appraisal Lexicon (Bloom et al,
2007). Nominalizations were recognized by looking
relates to the candidate opinion holder.
7available at disi.unitn.it/moschitti
799
Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are
marked with CAND.
up nouns in NOMLEX (Macleod et al, 1998).
5.1 Notation
Each kernel is represented as a triple
?levelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel
(Table 2)?, e.g. ?CONST, SENT, STK? is a Subset
Tree Kernel of a constituency parse having the
scope of the entire sentence. Note that not all com-
binations of these three parameters are meaningful.
In the following, we will just focus on important
and effective combinations. The kernel composed
of manually designed features is denoted by just
V K . The kernel composed of predicate-argument
structures is denoted by ?PAS, SENT,PTK?.
5.2 Vector Kernel (VK)
The first line in Table 7 displays the result of the
vector kernel using a manually designed feature set.
It should be interpreted as a baseline. Due to the
high class imbalance we will focus on the compari-
son of F(1)-Score throughout this paper rather than
accuracy which is fairly biased on this data set. The
F-Score of this classifier is at 56.16%.
5.3 Sequence Kernels (SKs)
For both sequence and tree kernels we need to find
out what the best scope is, whether it is worthwhile
to combine different scopes and what different lay-
ers of representation can be usefully combined.
The upper part of Table 5 lists the results of simple
word kernels using the different scopes. The perfor-
mance of the kernels using individual scopes varies
greatly. The best scope is PRED (1), the second
best is SEM (2). The good performance of PRED
does not come as a surprise since the sequence is the
smallest among the different scopes, so this scope is
least affected by data sparseness. Moreover, this re-
sult is consistent with our initial experiments on the
manual feature set (see Section 4.5).
Using different combinations of the word se-
quence kernels shows that PRED and SEM (6)
are a good combination, whereas OP , COMM ,
and SENT (7;8;9) do not positively contribute to
the overall performance which is consistent which
the individual scope evaluation. Apparently, these
scopes capture less linguistically relevant structure.
The next part of Table 5 shows the contribution of
POS kernels when added to WRD kernels. Adding
the corresponding POS kernel to the WRD kernel
with PRED scope (10) results in an improvement
by more than 5% in F-Score. We get another im-
provement by approx. 3% when the corresponding
SEM kernels (11) are added. This suggests that
POS is an effective generalization and that the two
scopes PRED and SEM are complementary.
For the GRAMWRD kernel, the PRED scope
(12) is again most effective. We assume that this ker-
nel most likely expresses meaningful syntactic rela-
tionships for our task. Adding the GRAMPOS ker-
nel (14) gives another boost by almost 4%.
Generalized sequence kernels are important.
800
Adding the corresponding WRDGN kernels to the
WRD kernel with PRED and SEM scope results
in an improvement from 47.77% (1) to 53.00% (15)
which is a bit less than the combination of WRD
and POS(GN) kernels (16). However, these types of
kernels seem to be complementary since their com-
bination provides an F-Score of 56.06% (17). This
kernel combination already performs on a par with
the manually designed vector kernel though less in-
formation is taken into consideration.
Finally, the best combination of sequence ker-
nels (18) comprises WRD, WRDGN , POS, and
POSGN kernels with PRED and SEM scope
combined with a GRAMWRD and a GRAMPOS
kernel with PRED scope. The performance of
58.70% significantly outperforms the vector kernel.
5.4 Tree Kernels (TKs)
Table 6 shows the results of the different tree ker-
nels. The table is divided into two halves. The
left half (A) are plain tree kernels, whereas the right
half (B) are the augmented tree kernels. As far as
CONST kernels are concerned, there is a system-
atic improvement by approximately 2% using tree
augmentation. This proves that further non-syntactic
knowledge added to the tree itself results in an im-
proved F-Score. However, tree augmentation does
not have any impact on the PAS kernels.
The overall performance of the tree kernels shows
that they are much more expressive than sequence
kernels. For instance, in order to obtain the same
performance as of ?CONSTAUG, PRED,STK?
(19B), i.e. a single kernel with an F-Score 56.52, it
requires several sequence kernels, hence much more
effort. The performance of the different CONST
kernels relative to each other resembles the results
of the WRD kernels. The best scope is PRED
(19). By far the worst performance is obtained by
the SENT scope (23). The combination of PRED
and SEM scope achieves an F-Score of 59.67%
(25B) which is already slightly better than the best
configuration of sequence kernels (18).
The performance of the PAS kernel (28A) with
an F-Score of 53.51% is slightly worse than the best
single plain CONST kernel (19A). The PAS ker-
nel and the CONST kernels are complementary,
since their best combination (29B) achieves an F-
Score of 61.67% which is significantly better than
Combination Acc. Prec. Rec. F1
VK 93.63 53.28 59.37 56.16
best SKs 94.21 57.64 59.81 58.70
best TKs 94.16 56.18 68.36 61.67?
VK + best SKs 94.34 58.44 61.27 59.82?
VK + best TKs 94.33 57.41 68.03 62.27?
best SKs + best TKs 94.49 59.22 63.96 61.49?
VK + best SKs + best TKs 94.53 59.10 66.57 62.61??
Table 7: Results of kernel combinations (?: significantly
better than best SKs; ?: significantly better than best TKs;
all convolution kernels are significantly better than VK).
the best combination of CONST kernels (25B) or
sequence kernels (18).
5.5 Combinations
Table 7 lists the results of the different kernel type
combinations. If VK is added to the best TKs, the
best SKs, or both, a slight increase in F-Score is
achieved. The best performance with an F-Score of
62.61% is obtained by combining all kernels.
6 Conclusion
In this paper, we compared convolution kernels for
opinion holder extraction. We showed that, in gen-
eral, a combination of two scopes, namely the scope
immediately encompassing the candidate opinion
holder and its nearest predicate and the subclause
containing the candidate opinion holder provide best
performance. Tree kernels containing constituency
parse information and semantic roles achieve better
performance than sequence kernels or vector kernels
using a manually designed feature set. Best perfor-
mance is achieved if all kernels are combined.
Acknowledgements
Michael Wiegand was funded by the German research
council DFG through the International Research Training
Group ?IRTG? between Saarland University and Univer-
sity of Edinburgh.
The authors would like to thank Yi Zhang for pro-
cessing the MPQA corpus with his semantic-role label-
ing system, the researchers from the MPQA project for
helping to create an opinion holder corpus, and, in partic-
ular, Alessandro Moschitti for insightful comments and
suggestions.
801
ID Kernel Acc. Prec. Rec. F1
1 ?WRD, PRED, SK? 93.25 51.08 42.29 46.26
2 ?WRD, OP, SK? 92.77 46.38 32.52 38.21
3 ?WRD, COMM, SK? 92.42 43.70 35.99 39.46
4 ?WRD, SEM,SK? 93.16 50.32 34.65 41.04
5 ?WRD, SENT, SK? 90.60 29.90 27.29 28.53
6 ?WRD, PRED, SK? + ?WRD, SEM,SK? 93.78 56.55 41.36 47.77
7
P
j?{PRED,OP,COMM}?WRD, j,SK? 93.55 54.26 39.50 45.71
8
P
j?Scopes\SENT ?WRD, j, SK? 93.82 57.21 40.28 47.26
9
P
j?Scopes?WRD, j, SK? 93.63 55.15 39.52 46.03
10 ?WRD, PRED, SK? + ?POS, PRED, SK? 93.03 49.39 53.53 51.37
11
P
i?{PRED,SEM} (?WRD, i, SK? + ?POS, i, SK?) 93.86 55.60 53.22 54.38
12
P
i?{PRED,SEM}?WRD, i, SK? + ?GRAMWRD , PRED, SK? 94.01 58.19 45.88 51.29
13
P
i?{PRED,SEM}?WRD, i, SK? +
P
j?{PRED,OP,COMM}?GRAMWRD , j, SK? 93.83 56.28 45.64 50.40
14
X
i?{PRED,SEM}
?WRD, i, SK?+?GRAMWRD, PRED, SK?+?GRAMPOS, PRED, SK? 93.98 56.59 53.92 55.21
15
P
i?{PRED,SEM} (?WRD, i, SK? + ?WRDGN , i, SK?) 93.97 57.08 49.46 53.00
16
P
i?{PRED,SEM} (?WRD, i, SK? + ?POSGN , i, SK?) 93.97 56.60 52.42 54.42
17
X
i?{PRED,SEM}
(?WRD, i, SK? + ?WRDGN , i, SK? + ?POS, i, SK? + ?POSGN , i, SK?) 93.85 55.16 57.00 56.06
18
X
i?{PRED,SEM}
(?WRD, i, SK? + ?WRDGN , i, SK? + ?POS, i, SK? + ?POSGN , i, SK?) 94.21 57.64 59.81 58.70
+?GRAMWRD , PRED, SK? + ?GRAMPOS , PRED, SK?
Table 5: Results of the different sequence kernels.
A B
i = CONST, j = PAS i = CONSTAUG, j = PASAUG
ID Kernel Acc. Prec. Rec. F1 Acc. Prec. Rec. F1
19 ?i, PRED, STK? 92.89 48.68 62.34 54.67 93.12 49.99 65.04 56.52
20 ?i, OP,STK? 93.04 49.49 54.71 51.96 93.27 50.93 59.06 54.68
21 ?i, COMM,STK? 92.76 47.79 55.89 51.50 92.96 49.03 58.85 53.47
22 ?i, SEM,STK? 93.70 54.40 52.13 53.23 93.90 55.47 56.59 56.03
23 ?i, SENT,STK? 92.42 44.34 39.92 41.99 92.50 45.20 42.40 43.74
24
P
k?{PRED,OP,COMM}?i, k, STK? 93.62 53.26 60.05 56.44 93.77 54.06 63.21 58.26
25
P
k?{PRED,SEM}?i, k, STK? 93.90 55.26 59.50 57.30 94.13 56.57 63.12 59.67
26
P
k?Scopes\SENT ?i, k, STK? 94.09 56.65 59.68 58.11 94.21 57.21 62.61 59.80
27
P
k?Scopes?i, k, STK? 94.14 57.41 57.88 57.63 94.29 58.11 61.10 59.56
28 ?j, SENT, PTK? 92.11 45.02 69.96 53.51 91.92 44.27 67.39 53.43
29
X
k?{PRED,SEM}
?i, k, STK?+?PAS,SENT, PTK? 94.05 55.68 66.01 60.40 94.16 56.18 68.36 61.67
30
X
k?Scopes\SENT
?i, k, STK? + ?PAS,SENT, PTK? 94.30 57.95 62.62 60.19 94.36 58.07 64.94 61.31
Table 6: Results of the different tree kernels.
802
References
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
Opinion Propositions and Opinion Holders using Syn-
tactic and Lexical Cues. In Computing Attitude and
Affect in Text: Theory and Applications. Springer.
Kenneth Bloom, Sterling Stein, and Shlomo Argamon.
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Workshop
Meeting, Tokyo, Japan.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence Kernels for Relation Extraction. In Pro-
ceedings of the Conference on Neural Information
Processing Systems (NIPS), Vancouver, Canada.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying Sources of Opinions
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP), Vancouver,
Canada.
Yejin Choi, Eric Breck, and Claire Cardie. 2006.
Joint Extraction of Entities and Relations for Opin-
ion Recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Philadelphia, USA.
Charles. J. Fillmore, Christopher R. Johnson, and
Miriam R. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235 ? 250.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, USA.
Soo-Min Kim and Eduard Hovy. 2005. Identifying
Opinion Holders for Question Answering in Opin-
ion Texts. In Proceedings of AAAI-05 Workshop
on Question Answering in Restricted Domains, Pitts-
burgh, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting Opin-
ions, Opinion Holders, and Topics Expressed in On-
line News Media Text. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text, Syd-
ney, Australia.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation (LREC),
Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Sapporo, Japan.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings of EU-
RALEX, Lie`ge, Belgium.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree Kernels for Semantic Role Label-
ing. Computational Linguistics, 34(2):193 ? 224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), Berlin, Germany.
Alessandro Moschitti. 2008. Kernel Methods, Syn-
tax and Semantics for Relational Text Categorization.
In Proceedings of the Conference on Information and
Knowledge Management (CIKM), Napa Valley, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution Kernels on
Constituent, Dependency and Sequential Structures
for Relation Extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Singapore.
Ellen Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction.
Artificial Intelligence, 85.
John Taylor and Nello Christianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2003.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 1:2.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP), Vancouver, Canada.
Dell Zhang and Wee Sun Lee. 2003. Question Classifi-
cation using Support Vector Machines. In Proceedings
of the ACM Special Interest Group on Information Re-
trieval (SIGIR), Toronto, Canada.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution Tree Kernel. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), New
York City, USA.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL), Manchester, United Kingdom.
803
Proceedings of NAACL-HLT 2013, pages 534?539,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Predicative Adjectives: An Unsupervised Criterion to Extract Subjective
Adjectives
Michael Wiegand
Spoken Language Systems
Saarland University
michael.wiegand@lsv.uni-saarland.de
Josef Ruppenhofer
Dept. of Information Science
and Language Technology
Hildesheim University
ruppenho@uni-hildesheim.de
Dietrich Klakow
Spoken Language Systems
Saarland University
dietrich.klakow@lsv.uni-saarland.de
Abstract
We examine predicative adjectives as an unsu-
pervised criterion to extract subjective adjec-
tives. We do not only compare this criterion
with a weakly supervised extraction method
but also with gradable adjectives, i.e. another
highly subjective subset of adjectives that can
be extracted in an unsupervised fashion. In or-
der to prove the robustness of this extraction
method, we will evaluate the extraction with
the help of two different state-of-the-art senti-
ment lexicons (as a gold standard).
1 Introduction
Since the early work on sentiment analysis, it has
been established that the part of speech with the
highest proportion of subjective words are adjec-
tives (Wiebe et al, 2004) (see Sentence (1)). How-
ever, not all adjectives are subjective (2).
(1) A grumpy guest made some impolite remarks
to the insecure and inexperienced waitress.
(2) The old man wearing a yellow pullover sat on a
plastic chair.
This justifies the exploration of criteria to automati-
cally separate the subjective adjectives from the non-
subjective adjectives.
In this work, we are interested in an out-of-
context assessment of adjectives and therefore eval-
uate them with the help of sentiment lexicons. We
examine the property of being a predicative adjec-
tive as an extraction criterion. Predicative adjectives
are adjectives that do not modify the head of a noun
phrase, but which predicate a property of the refer-
ent of a noun phrase to which they are linked via a
copula or a control predicate (3).
We show that adjectives that frequently occur as
predicative adjectives are more likely to convey sub-
jectivity (in general) than adjectives that occur non-
predicatively, such as the pre-nominal (attributive)
adjectives (4). A subjective adjective may occur
both as a predicative (3) and a non-predicative (5)
adjective and also convey subjectivity in both con-
texts. However, a large fraction of non-subjective
adjectives do not occur as predicative adjectives (6).
(3) Her idea was brilliant.
(4) This is a financial problem.
(5) She came up with a brilliant idea.
(6) ?The problem is financial.
2 Related Work
The extraction of subjective adjectives has already
attracted some considerable attention in previous re-
search. Hatzivassiloglou and McKeown (1997) ex-
tract polar adjectives by a weakly supervised method
in which subjective adjectives are found by search-
ing for adjectives that are conjuncts of a pre-defined
set of polar seed adjectives. Wiebe (2000) in-
duces subjective adjectives with the help of distribu-
tional similarity. Hatzivassiloglou and Wiebe (2000)
examine the properties of dynamic, gradable and
polar adjectives as a means to detect subjectivity.
Vegnaduzzo (2004) presents another bootstrapping
method of extracting subjective adjectives with the
help of head nouns of the subjective candidates and
distributional similarity. Baroni and Vegnaduzzo
534
(2004) employ Web-based Mutual information for
this task and largely outperform the results produced
by Vegnaduzzo (2004).
3 Method
In the following, we present different features with
the help of which subjective adjectives can be ex-
tracted. For all resulting lists, the adjectives will be
ranked according to their frequency of co-occurring
with a particular feature.
3.1 Extracting Predicative Adjectives (PRD)
For the extraction of predicative adjectives, we ex-
clusively rely on the output of a dependency parser.
Predicative adjectives are usually connected to the
subject of the sentence via the dependency label
nsubj (Example (7) would correspond to Sen-
tence (3)).
(7) nsubj(brilliant, idea)
3.2 Extracting Gradable Adjectives (GRD)
As an alternative extraction method, we consider
morpho-syntactically gradable adjectives. Gradable
adjectives, such as nice or small, are adjectives ?that
can be inflected to specify the degree or grade of
something? (Wiktionary1). It has been stated in pre-
vious work that if some adjective can build a com-
parative (e.g. nicer) or a superlative (e.g. nicest),
then this adjective tends to be subjective (Hatzivas-
siloglou and Wiebe, 2000).
We employ the property of gradability, since,
firstly, it is very predictive towards subjectivity and,
secondly, it is the only other unsupervised criterion
currently known to extract subjective adjectives. For
the extraction of gradable adjectives, we rely, on the
one hand, on the part-of-speech labels JJR (com-
parative) and JJS (superlative). On the other hand,
we also consider adjectives being modified by ei-
ther more or most. For the former case, we need
to normalize the comparative (e.g. nicer) or superla-
tive (e.g. nicest) word form to the canonical positive
word form (e.g. nice) that is commonly used in sen-
timent lexicons.
1http://en.wiktionary.org/wiki/gradable
3.3 Weakly-Supervised Extraction (WKS)
We also consider a weakly supervised extraction
method in this paper, even though it is not strictly
fair to compare such a method with our two pre-
vious extraction methods which are completely un-
supervised. WKS considers an adjective subjective,
if it co-occurs as a conjunct of a previously defined
highly subjective (seed) adjective (8). In order to de-
tect such conjunctions, we employ the dependency
relation conj. By just relying on surface patterns,
we would not be able to exclude spurious conjunc-
tions in which other constituents than the two adjec-
tives are coordinated, such as Sentence (10).
(8) This approach is ill-conceived and ineffective.
(9) conj(ill-conceived,
ineffective)
(10) [Evil witches are stereotypically dressed in
black] and [good fairies in white].
We also experimented with other related weakly-
supervised extraction methods, such as mutual in-
formation of two adjectives at the sentence level (or
even smaller window sizes). However, using con-
junctions largely outperformed these alternative ap-
proaches so we only pursue conjunctions here.
4 Experiments
As a large unlabeled (training) corpus, we chose the
North American News Text Corpus (LDC95T21)
comprising approximately 350 million words of
news text. For syntactic analysis we use the Stan-
ford Parser (Finkel et al, 2005). In order to decide
whether an extracted adjective is subjective or not,
we employ two sentiment lexicons, namely the Sub-
jectivity Lexicon (SUB) (Wilson et al, 2005) and
SO-CAL (SOC) (Taboada et al, 2011). According to
the recent in-depth evaluation presented in Taboada
et al (2011), these two sentiment lexicons are the
most effective resources for English sentiment anal-
ysis. By taking into account two different lexicons,
which have also been built independently of each
other, we want to provide evidence that our pro-
posed criterion to extract subjective adjectives is not
sensitive towards a particular gold standard (which
would challenge the general validity of the proposed
method).
535
ALL other new last many first such next political federal own sev-
eral few good? former same economic public major recent
American second big? foreign high small local military fi-
nancial little? national
PRD able? likely available clear? difficult? important? ready?
willing? hard? good? due possible? sure? interested un-
likely necessary? high responsible? easy? strong? unable?
different enough open aware happy impossible? right?
wrong? confident?
Table 2: The 30 most frequent adjectives (ALL) and pred-
icative adjectives (PRD); ? marks matches with both sen-
timent lexicons SUB and SOC.
In order to produce the subjective seed adjec-
tives for the weakly supervised extraction, we col-
lect from the sentiment lexicon that we evaluate the
n most frequent subjective adjectives according to
our corpus. In order to further improve the quality
of the seed set, we only consider strong subjective
expressions from SUB and expressions with the in-
tensity strength ?5 from SOC.
Table 1 lists the size of the different sentiment lex-
icons and the rankings produced by the different ex-
traction methods. Of course, the list of all adjectives
from the corpus (ALL) is the largest list2 while PRD
is the second largest and GRD the third largest. The
rankings produced by WKS are fairly sparse, in par-
ticular the ones induced with the help of SOC; ap-
parently there are more frequently occurring strong
subjective adjectives in SUB than there are high in-
tensity adjectives in SOC.
4.1 Frequent Adjectives vs. Frequent
Predicative Adjectives
Table 2 compares the 30 most frequent adjectives
(ALL) and predicative adjectives (PRD). Not only
does this table show that the proportion of subjective
adjectives is much larger among the predicative ad-
jectives but we may also gain some insight into what
non-subjective adjectives are excluded. Among the
high frequent adjectives are many quantifiers (many,
few and several) and ordinal expressions (first, next
and last). In principle, most of these expressions
are not subjective. One may argue that these adjec-
tives behave like function words. Since they occur
2It will also contain many words erroneously tagged as ad-
jectives, however, this is unlikely to affect our experiments since
we only focus on the highly ranked (i.e. most frequent) words.
The misclassifications rather concern infrequent words.
very frequently, one might exclude some of them
by just ignoring the most frequent adjectives. How-
ever, there are also other types of adjectives, espe-
cially pertainyms (political, federal, economic, pub-
lic, American, foreign, local, military, financial and
national) that appear on this list which could not be
excluded by that heuristic. We found that these non-
subjective content adjectives are present throughout
the entire ranking and they are fairly frequent (on
the ranking). On the list of predicative adjectives all
these previous types of adjectives are much less fre-
quent. Many of them only occur on lower ranks (and
we assume that several of them only got on the list
due to parsing errors).
4.2 Comparison of the Different Extraction
Methods
Table 3 compares the precision of the different ex-
traction methods at different cut-off values. It is in-
teresting to see that for ALL in particular the higher
ranks are worse than the lower ranks (e.g. rank
1000). We assume that this is due to the high-
frequency adjectives which are similar to function
words (see Section 4.1). At all cut-off values, how-
ever, this baseline is beaten by every other method,
including our proposed method PRD. The two unsu-
pervised methods PRD and GRD perform on a par
with each other. On SUB, PRD even mostly out-
performs GRD. The precision achieved by WKS is
quite good. However, the coverage of this method
is low. It would require more seed expressions to
increase it, however, this would also mean consider-
ably more manual guidance.
Table 3 also shows that the precision of all ex-
traction methods largely drops on the lower ranks.
However, one should not conclude from that the ex-
traction methods proposed only work well for highly
frequent words. The drop can be mostly explained
by the fact that the two sentiment lexicons we use
for evaluation are finite (i.e. SUB: 4396 words/SOC:
2827 words (Table 1)), and that neither of these lexi-
cons (nor their union) represents the complete set of
all English subjective adjectives. Both lexicons will
have a bias towards frequently occurring subjective
expressions.
Inspecting the ranks 3001-3020 produced by PRD
as displayed in Table 4, for example, actually reveals
that there are still many more subjective adjectives
536
Lexicons Extraction Methods
WKS-5 WKS-10 WKS-25 WKS-50
SUB SOC ALL PRD GRD SUB SOC SUB SOC SUB SOC SUB SOC
4396 2827 212287 20793 7942 292 81 440 131 772 319 1035 385
Table 1: Statistics regarding the size (i.e. number of adjectives) of the different sentiment lexicons and rankings.
artistic? appealable airtight adjustable? activist? accommodat-
ing acclimated well-meaning weakest upsetting? unsurpassed
unsatisfying? unopposed unobtrusive? unobjectionable unem-
ployable understanding? uncharacteristic submerged speechless
Table 4: A set of entries PRD produces on lower ranks
(ranks 3001-3020); ? marks matches with either of the
sentiment lexicons SUB or SOC.
than the matches with our sentiment lexicons sug-
gest (e.g. appealable, accomodating, well-meaning,
weakest, unsurpassed, unopposed, unobjectionable,
unemployable, uncharacteristic or speechless). In
other words, these are less frequent words; many
of them are actually subjective even though they are
not listed in the sentiment lexicons. Moreover, irre-
spective of the drop in precision on the lower ranks,
PRD and GRD still outperform ALL on both senti-
ment lexicons (Table 3). Despite the sparseness of
our two gold standards on the lower ranks, we thus
have some indication that PRD and GRD are more
effective than ALL.
The problem of the evaluation of less-frequent
words could not be solved by an extrinsic evaluation,
either, e.g. by using the extracted lists for some text
classification task (at the sentence/document level).
The evaluation on contextual classification on cor-
pora would also be biased towards high-frequency
words (as the word distribution is typically Zipfian).
For instance, on the MPQA-corpus (Wiebe et al,
2005), i.e. the standard dataset for (fine-grained)
sentiment analysis, there is not a single mention of
the subjective words appealable, accommodating,
unsurpassed, unopposed, unobtrusive or speechless,
which were found among the lower ranks 3001-
3020.
4.3 How Different Are Gradable and
Predicative Adjectives?
Since in the previous experiments the proportion of
subjective adjectives was similar among the grad-
able adjectives and the predicative adjectives, we
may wonder whether these two extraction methods
produce the same adjectives. In principle, the set of
gradable adjectives extracted is much smaller than
the list of extracted predicative adjectives (see Ta-
ble 1). We found that the gradable adjectives are
a proper subset of predicative adjectives, which is
in line with the observation by (Bolinger, 1972,
21) that gradable adjectives (which he calls degree
words) readily occur predicatively whereas non-
gradable ones tend not to.
However, while gradability implies compatibility
with predicative use, the reverse is not true. Ac-
cordingly, we found adjectives that are definitely not
gradable among the predicative adjectives that are
subjective, for instance endless, insolvent, nonexis-
tent, stagnant, unavailable or untrue. This means
that with the criterion of predicative adjectives one
is able to extract relevant subjective adjectives that
cannot be caught by the gradability criterion alone,
namely complementary adjectives that refer to a sim-
ple binary opposition (Cruse, 1986, 198-99).
4.4 Intersecting the Different Unsupervised
Criteria
In this section, we want to find out whether we can
increase the precision by considering intersections
of the two different unsupervised extraction crite-
ria. (Due to the sparsity of WKS, it does not make
sense to include that method in this experiment.) In
our previous experiments it turned out that as far as
precision is concerned, our new proposed extraction
criterion was similar to the gradability criterion. If,
however, the intersection of these two criteria pro-
duces better results, then we have provided some
further proof of the effectiveness of our proposed
criterion (even though we may sacrifice some exclu-
sive subjective adjectives in PRD as pointed out in
Section 4.3). It would mean that this criterion is also
beneficial in the presence of the gradability criterion.
Figure 1 shows the corresponding results. We
computed the intersection of PRD and GRD at var-
537
ALL PRD GRD WKS-5 WKS-10 WKS-25 WKS-50
Rank n SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC
10 10.00 30.00 90.00 90.00 80.00 60.00 80.00 90.00 80.00 90.00 90.00 70.00 90.00 70.00
25 20.00 32.00 88.00 60.00 64.00 60.00 92.00 80.00 91.00 80.00 92.00 80.00 92.00 84.00
50 30.00 34.00 88.00 64.00 70.00 68.00 82.00 78.00 92.00 78.00 92.00 84.00 90.00 86.00
100 37.00 38.00 81.00 68.00 79.00 75.00 80.00 N/A 82.00 72.00 89.00 78.00 92.00 77.00
250 45.60 43.20 79.60 75.60 84.80 76.00 70.80 N/A 74.40 N/A 80.40 67.50 82.04 67.20
500 48.00 49.20 77.20 70.00 82.20 74.00 N/A N/A N/A N/A 72.60 N/A 75.20 N/A
1000 48.70 48.10 75.50 65.60 72.60 65.00 N/A N/A N/A N/A N/A N/A 64.30 N/A
1500 49.07 46.53 68.60 59.07 66.27 58.60 N/A N/A N/A N/A N/A N/A N/A N/A
2000 48.00 43.85 64.55 55.40 61.55 54.25 N/A N/A N/A N/A N/A N/A N/A N/A
2500 46.08 40.96 59.52 51.28 56.36 50.00 N/A N/A N/A N/A N/A N/A N/A N/A
3000 44.20 39.17 54.63 47.13 51.47 46.03 N/A N/A N/A N/A N/A N/A N/A N/A
Table 3: Precision at rank n of the different extraction methods; WKS-m denotes that for the extraction the m most
frequent subjective adjectives from the respective sentiment lexicon were considered as seed expressions.
ious cut-off values of n. The resulting intersection
comprises m ranks with m < n. The precision of
the intersection was consequently compared against
the precision of PRD and GRD at rank m. The figure
shows that with the exception of the higher ranks on
SUB (< 200) there is indeed a systematic increase
in precision when the intersection of PRD and GRD
is considered.
5 Conclusion
We examined predicative adjectives as a criterion
to extract subjective adjectives. As this extraction
method is completely unsupervised, it is preferable
to weakly supervised extraction methods since we
are not dependent on a manually designed high qual-
ity seed set and we obtain a much larger set of ad-
jectives. This extraction method is competitive if
not slightly better than gradable adjectives. In ad-
dition, combining these two unsupervised methods
by assessing their intersection results mostly in an
increase in precision.
Acknowledgements
This work was performed in the context of the Software-
Cluster project EMERGENT. Michael Wiegand was
funded by the German Federal Ministry of Education and
Research (BMBF) under grant no. ?01IC10S01?. The
authors would like to thank Maite Taboada for providing
her sentiment lexicon (SO-CAL) to be used for the exper-
iments presented in this paper.
 55
 60
 65
 70
 75
 80
 85
 500  1000  1500  2000  2500
Pr
ec
is
io
n
Top N Ranked Adjectives
Predicative Adjectives (PRD)
Gradable Adjectives (GRD)
Intersection of PRD and GRD
(a) Evaluation on SUB lexicon
 50
 55
 60
 65
 70
 75
 80
 500  1000  1500  2000  2500
Pr
ec
is
io
n
Top N Ranked Adjectives
Predicative Adjectives (PRD)
Gradable Adjectives (GRD)
Intersection of PRD and GRD
(b) Evaluation on SOC lexicon
Figure 1: Comparison of the individual rankings of GRD
and PRD with their intersection.
538
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identify-
ing Subjective Adjectives through Web-based Mutual
Information. In Proceedings of KONVENS, pages 17?
24, Vienna, Austria.
Dwight Bolinger. 1972. Degree words. Mouton, The
Hague.
David Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge, UK.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
363?370, Ann Arbor, MI, USA.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 174?181, Madrid, Spain.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of Adjective Orientation and Gradability on Sen-
tence Subjectivity. In Proceedings of the International
Conference on Computational Linguistics (COLING),
pages 299?305, Saarbru?cken, Germany.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267 ? 307.
Stefano Vegnaduzzo. 2004. Acquisition of Subjective
Adjectives with Limited Resources. In Proceedings of
the AAAI Spring Symposium on Exploring Attitude and
Affect in Text: Theories and Applications, Stanford,
CA, USA.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning Subjective
Language. Computational Linguistics, 30(3).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions
in Language. Language Resources and Evaluation,
39(2/3):164?210.
Janyce M. Wiebe. 2000. Learning Subjective Adjectives
from Corpora. In Proceedings of the National Confer-
ence on Artificial Intelligence (AAAI), pages 735?740,
Austin, TX, USA.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-level
Sentiment Analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 347?354, Vancouver, BC, Canada.
539
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60?68,
Uppsala, July 2010.
A Survey on the Role of Negation in Sentiment Analysis
Michael Wiegand
Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Alexandra Balahur
University of Alicante
Alicante, Spain
abalahur@dlsi.ua.es
Benjamin Roth and Dietrich Klakow
Saarland University
Saarbru?cken, Germany
benjamin.roth@lsv.uni-saarland.de
dietrich.klakow@lsv.uni-saarland.de
Andre?s Montoyo
University of Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
This paper presents a survey on the role of
negation in sentiment analysis. Negation
is a very common linguistic construction
that affects polarity and, therefore, needs
to be taken into consideration in sentiment
analysis.
We will present various computational ap-
proaches modeling negation in sentiment
analysis. We will, in particular, focus
on aspects, such as level of representation
used for sentiment analysis, negation word
detection and scope of negation. We will
also discuss limits and challenges of nega-
tion modeling on that task.
1 Introduction
Sentiment analysis is the task dealing with the
automatic detection and classification of opinions
expressed in text written in natural language.
Subjectivity is defined as the linguistic expression
of somebody?s opinions, sentiments, emotions,
evaluations, beliefs and speculations (Wiebe,
1994). Subjectivity is opposed to objectivity,
which is the expression of facts. It is important to
make the distinction between subjectivity detec-
tion and sentiment analysis, as they are two sep-
arate tasks in natural language processing. Sen-
timent analysis can be dependently or indepen-
dently done from subjectivity detection, although
Pang and Lee (2004) state that subjectivity de-
tection performed prior to the sentiment analysis
leads to better results in the latter.
Although research in this area has started only re-
cently, the substantial growth in subjective infor-
mation on the world wide web in the past years
has made sentiment analysis a task on which con-
stantly growing efforts have been concentrated.
The body of research published on sentiment anal-
ysis has shown that the task is difficult, not only
due to the syntactic and semantic variability of
language, but also because it involves the extrac-
tion of indirect or implicit assessments of objects,
by means of emotions or attitudes. Being a part
of subjective language, the expression of opinions
involves the use of nuances and intricate surface
realizations. That is why the automatic study of
opinions requires fine-grained linguistic analysis
techniques and substantial efforts to extract fea-
tures for machine learning or rule-based systems,
in which subtle phenomena as negation can be ap-
propriately incorporated.
Sentiment analysis is considered as a subsequent
task to subjectivity detection, which should ideally
be performed to extract content that is not factual
in nature. Subsequently, sentiment analysis aims
at classifying the sentiment of the opinions into
polarity types (the common types are positive and
negative). This text classification task is also re-
ferred to as polarity classification.
This paper presents a survey on the role of nega-
tion in sentiment analysis. Negation is a very com-
mon linguistic construction that affects polarity
and, therefore, needs to be taken into considera-
tion in sentiment analysis. Before we describe the
computational approaches that have been devised
to account for this phenomenon in sentiment anal-
ysis, we will motivate the problem.
2 Motivation
Since subjectivity and sentiment are related to ex-
pressions of personal attitudes, the way in which
this is realized at the surface level influences the
manner in which an opinion is extracted and its
polarity is computed. As we have seen, sentiment
analysis goes a step beyond subjectivity detection,
60
including polarity classification. So, in this task,
correctly determining the valence of a text span
(whether it conveys a positive or negative opinion)
is equivalent to the success or failure of the auto-
matic processing.
It is easy to see that Sentence 1 expresses a posi-
tive opinion.
1. I like+ this new Nokia model.
The polarity is conveyed by like which is a polar
expression. Polar expressions, such as like or hor-
rible, are words containing a prior polarity. The
negation of Sentence 1, i.e. Sentence 2, using the
negation word not, expresses a negative opinion.
2. I do [not like+]? this new Nokia model.
In this example, it is straightforward to notice the
impact of negation on the polarity of the opinion
expressed. However, it is not always that easy
to spot positive and negative opinions in text. A
negation word can also be used in other expres-
sions without constituting a negation of the propo-
sition expressed as exemplified in Sentence 3.
3. Not only is this phone expensive but it is also heavy and
difficult to use.
In this context, not does not invert the polarity of
the opinion expressed which remains negative.
Moreover, the presence of an actual negation word
in a sentence does not mean that all its polar opin-
ions are inverted. In Sentence 4, for example, the
negation does not modify the second polar expres-
sion intriguing since the negation and intriguing
are in separate clauses.
4. [I do [not like+]? the design of new Nokia model] but
[it contains some intriguing+ new functions].
Therefore, when treating negation, one must be
able to correctly determine the scope that it has
(i.e. determine what part of the meaning expressed
is modified by the presence of the negation).
Finally, the surface realization of a negation is
highly variable, depending on various factors,
such as the impact the author wants to make on
the general text meaning, the context, the textual
genre etc. Most of the times, its expression is far
from being simple (as in the first two examples),
and does not only contain obvious negation words,
such as not, neither or nor. Research in the field
has shown that there are many other words that in-
vert the polarity of an opinion expressed, such as
diminishers/valence shifters (Sentence 5), connec-
tives (Sentence 6), or even modals (Sentence 7).
5. I find the functionality of the new phone less practical.
6. Perhaps it is a great phone, but I fail to see why.
7. In theory, the phone should have worked even under
water.
As can be seen from these examples, modeling
negation is a difficult yet important aspect of sen-
timent analysis.
3 The Survey
In this survey, we focus on work that has presented
novel aspects for negation modeling in sentiment
analysis and we describe them chronologically.
3.1 Negation and Bag of Words in Supervised
Machine Learning
Several research efforts in polarity classification
employ supervised machine-learning algorithms,
like Support Vector Machines, Na??ve Bayes Clas-
sifiers or Maximum Entropy Classifiers. For these
algorithms, already a low-level representation us-
ing bag of words is fairly effective (Pang et al,
2002). Using a bag-of-words representation, the
supervised classifier has to figure out by itself
which words in the dataset, or more precisely fea-
ture set, are polar and which are not. One either
considers all words occurring in a dataset or, as
in the case of Pang et al (2002), one carries out
a simple feature selection, such as removing infre-
quent words. Thus, the standard bag-of-words rep-
resentation does not contain any explicit knowl-
edge of polar expressions. As a consequence of
this simple level of representation, the reversal
of the polarity type of polar expressions as it is
caused by a negation cannot be explicitly modeled.
The usual way to incorporate negation modeling
into this representation is to add artificial words:
i.e. if a word x is preceded by a negation word,
then rather than considering this as an occurrence
of the feature x, a new feature NOT x is created.
The scope of negation cannot be properly modeled
with this representation either. Pang et al (2002),
for example, consider every word until the next
punctuation mark. Sentence 2 would, therefore,
result in the following representation:
8. I do not NOT like NOT this NOT new NOT Nokia
NOT model.
The advantage of this feature design is that a plain
occurrence and a negated occurrence of a word are
61
reflected by two separate features. The disadvan-
tage, however, is that these two contexts treat the
same word as two completely different entities.
Since the words to be considered are unrestricted,
any word ? no matter whether it is an actual po-
lar expression or not ? is subjected to this nega-
tion modification. This is not only linguistically
inaccurate but also increases the feature space with
more sparse features (since the majority of words
will only be negated once or twice in a corpus).
Considering these shortcomings, it comes to no
surprise that the impact of negation modeling on
this level of representation is limited. Pang et al
(2002) report only a negligible improvement by
adding the artificial features compared to plain bag
of words in which negation is not considered.
Despite the lack of linguistic plausibility, super-
vised polarity classifiers using bag of words (in
particular, if training and testing are done on the
same domain) offer fairly good performance. This
is, in particular, the case on coarse-grained clas-
sification, such as on document level. The suc-
cess of these methods can be explained by the
fact that larger texts contain redundant informa-
tion, e.g. it does not matter whether a classifier
cannot model a negation if the text to be classi-
fied contains twenty polar opinions and only one
or two contain a negation. Another advantage
of these machine learning approaches on coarse-
grained classification is their usage of higher order
n-grams. Imagine a labeled training set of docu-
ments contains frequent bigrams, such as not ap-
pealing or less entertaining. Then a feature set us-
ing higher order n-grams implicitly contains nega-
tion modeling. This also partially explains the ef-
fectiveness of bigrams and trigrams for this task as
stated in (Ng et al, 2006).
The dataset used for the experiments in (Pang et
al., 2002; Ng et al, 2006) has been established as
a popular benchmark dataset for sentiment analy-
sis and is publicly available1.
3.2 Incorporating Negation in Models that
Include Knowledge of Polar Expressions
- Early Works
The previous subsection suggested that appropri-
ate negation modeling for sentiment analysis re-
quires the awareness of polar expressions. One
way of obtaining such expressions is by using a
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
polarity lexicon which contains a list of polar ex-
pressions and for each expression the correspond-
ing polarity type. A simple rule-based polarity
classifier derived from this knowledge typically
counts the number of positive and negative polar
expressions in a text and assigns it the polarity
type with the majority of polar expressions. The
counts of polar expressions can also be used as
features in a supervised classifier. Negation is typ-
ically incorporated in those features, e.g. by con-
sidering negated polar expressions as unnegated
polar expressions with the opposite polarity type.
3.2.1 Contextual Valence Shifters
The first computational model that accounts for
negation in a model that includes knowledge of
polar expressions is (Polanyi and Zaenen, 2004).
The different types of negations are modeled via
contextual valence shifting. The model assigns
scores to polar expressions, i.e. positive scores to
positive polar expressions and negative scores to
negative polar expressions, respectively. If a polar
expression is negated, its polarity score is simply
inverted (see Example 1).
clever (+2) ? not clever (?2) (1)
In a similar fashion, diminishers are taken into
consideration. The difference is, however, that
the score is only reduced rather than shifted to the
other polarity type (see Example 2).
efficient (+2)? rather efficient (+1) (2)
Beyond that the model also accounts for modals,
presuppositional items and even discourse-based
valence shifting. Unfortunately, this model is
not implemented and, therefore, one can only
speculate about its real effectiveness.
Kennedy and Inkpen (2005) evaluate a nega-
tion model which is fairly identical to the one pro-
posed by Polanyi and Zaenen (2004) (as far as
simple negation words and diminishers are con-
cerned) in document-level polarity classification.
A simple scope for negation is chosen. A polar
expression is thought to be negated if the negation
word immediately precedes it. In an extension of
this work (Kennedy and Inkpen, 2006) a parser is
considered for scope computation. Unfortunately,
no precise description of how the parse is used
for scope modeling is given in that work. Neither
is there a comparison of these two scope models
measuring their respective impacts.
62
Final results show that modeling negation is im-
portant and relevant, even in the case of such sim-
ple methods. The consideration of negation words
is more important than that of diminishers.
3.2.2 Features for Negation Modeling
Wilson et al (2005) carry out more advanced
negation modeling on expression-level polarity
classification. The work uses supervised machine
learning where negation modeling is mostly en-
coded as features using polar expressions. The
features for negation modeling are organized in
three groups:
? negation features
? shifter features
? polarity modification features
Negation features directly relate to negation ex-
pressions negating a polar expression. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar
expression. The other feature accounts for a polar
predicate having a negated subject. This frequent
long-range relationship is illustrated in Sentence 9.
9. [No politically prudent Israeli]
subject
could
support
polar pred
either of them.
All negation expressions are additionally disam-
biguated as some negation words do not function
as a negation word in certain contexts, e.g. not to
mention or not just.
Shifter features are binary features checking the
presence of different types of polarity shifters. Po-
larity shifters, such as little, are weaker than ordi-
nary negation expressions. They can be grouped
into three categories, general polarity shifters,
positive polarity shifters, and negative polarity
shifters. General polarity shifters reverse polarity
like negations. The latter two types only reverse
a particular polarity type, e.g. the positive shifter
abate only modifies negative polar expressions as
in abate the damage. Thus, the presence of a pos-
itive shifter may indicate positive polarity. The set
of words that are denoted by these three features
can be approximately equated with diminishers.
Finally, polarity modification features describe
polar expressions of a particular type modify-
ing or being modified by other polar expressions.
Though these features do not explicitly contain
negations, language constructions which are sim-
ilar to negation may be captured. In the phrase
[disappointed? hope+]?, for instance, a negative
polar expression modifies a positive polar expres-
sion which results in an overall negative phrase.
Adding these three feature groups to a feature
set comprising bag of words and features count-
ing polar expressions results in a significant im-
provement. In (Wilson et al, 2009), the experi-
ments of Wilson et al (2005) are extended by a
detailed analysis on the individual effectiveness of
the three feature groups mentioned above. The re-
sults averaged over four different supervised learn-
ing algorithms suggest that the actual negation fea-
tures are most effective whereas the binary polar-
ity shifters have the smallest impact. This is con-
sistent with Kennedy and Inkpen (2005) given the
similarity of polarity shifters and diminishers.
Considering the amount of improvement that is
achieved by negation modeling, the improvement
seems to be larger in (Wilson et al, 2005). There
might be two explanations for this. Firstly, the
negation modeling in (Wilson et al, 2005) is con-
siderably more complex and, secondly, Wilson et
al. (2005) evaluate on a more fine-grained level
(i.e. expression level) than Kennedy and Inkpen
(2005) (they evaluate on document level). As al-
ready pointed out in ?3.1, document-level polar-
ity classification contains more redundant infor-
mation than sentence-level or expression-level po-
larity classification, therefore complex negation
modeling on these levels might be more effective
since the correct contextual interpretation of an in-
dividual polar expression is far more important2.
The fine-grained opinion corpus used in (Wilson
et al, 2005; Wilson et al, 2009) and all the re-
sources necessary to replicate the features used in
these experiments are also publicly available3.
3.3 Other Approaches
The approaches presented in the previous sec-
tion (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2005; Wilson et al, 2005) can be consid-
ered as the works pioneering negation modeling
in sentiment analysis. We now present some more
recent work on that topic. All these approaches,
however, are heavily related to these early works.
2This should also explain why most subsequent works
(see ?3.3) have been evaluated on fine-grained levels.
3The corpus is available under:
http://www.cs.pitt.edu/mpqa/
databaserelease and the resources
for the features are part of OpinionFinder:
http://www.cs.pitt.edu/mpqa/
opinionfinderrelease
63
3.3.1 Semantic Composition
In (Moilanen and Pulman, 2007), a method to
compute the polarity of headlines and complex
noun phrases using compositional semantics is
presented. The paper argues that the principles of
this linguistic modeling paradigm can be success-
fully applied to determine the subsentential polar-
ity of the sentiment expressed, demonstrating it
through its application to contexts involving senti-
ment propagation, polarity reversal (e.g. through
the use of negation following Polanyi and Zae-
nen (2004) and Kennedy and Inkpen (2005)) or
polarity conflict resolution. The goal is achieved
through the use of syntactic representations of sen-
tences, on which rules for composition are de-
fined, accounting for negation (incrementally ap-
plied to constituents depending on the scope) us-
ing negation words, shifters and negative polar ex-
pressions. The latter are subdivided into differ-
ent categories, such that special words are defined,
whose negative intensity is strong enough that they
have the power to change the polarity of the entire
text spans or constituents they are part of.
A similar approach is presented by Shaikh et al
(2007). The main difference to Moilanen and
Pulman (2007) lies in the representation format
on which the compositional model is applied.
While Moilanen and Pulman (2007) use syntac-
tic phrase structure trees, Shaikh et al (2007) con-
sider a more abstract level of representation be-
ing verb frames. The advantage of a more abstract
level of representation is that it more accurately
represents the meaning of the text it describes.
Apart from that, Shaikh et al (2007) design a
model for sentence-level classification rather than
for headlines or complex noun phrases.
The approach by Moilanen and Pulman (2007) is
not compared against another established classifi-
cation method whereas the approach by Shaikh et
al. (2007) is evaluated against a non-compositional
rule-based system which it outperforms.
3.3.2 Shallow Semantic Composition
Choi and Cardie (2008) present a more lightweight
approach using compositional semantics towards
classifying the polarity of expressions. Their
working assumption is that the polarity of a phrase
can be computed in two steps:
? the assessment of polarity of the constituents
? the subsequent application of a set of previously-
defined inference rules
An example rule, such as:
Polarity([NP1]? [IN] [NP2]?) = + (3)
may be applied to expressions, such as
[lack]?NP1 [of]IN [crime]?NP2 in rural areas.
The advantage of these rules is that they restrict
the scope of negation to specific constituents
rather than using the scope of the entire target
expression.
Such inference rules are very reminiscent of
polarity modification features (Wilson et al,
2005), as a negative polar expression is modified
by positive polar expression. The rules presented
by Choi and Cardie (2008) are, however, much
more specific, as they define syntactic contexts of
the polar expressions. Moreover, from each con-
text a direct polarity for the entire expression can
be derived. In (Wilson et al, 2005), this decision
is left to the classifier. The rules are also similar
to the syntactic rules from Moilanen and Pulman
(2007). However, they involve less linguistic
processing and are easier to comprehend4 . The
effectiveness of these rules are both evaluated in
rule-based methods and a machine learning based
method where they are anchored as constraints
in the objective function. The results of their
evaluation show that the compositional methods
outperform methods using simpler scopes for
negation, such as considering the scope of the
entire target expression. The learning method
incorporating the rules also slightly outperforms
the (plain) rule-based method.
3.3.3 Scope Modeling
In sentiment analysis, the most prominent work
examining the impact of different scope models
for negation is (Jia et al, 2009). The scope de-
tection method that is proposed considers:
? static delimiters
? dynamic delimiters
? heuristic rules focused on polar expressions
Static delimiters are unambiguous words, such as
because or unless marking the beginning of an-
other clause. Dynamic delimiters are, however,
4It is probably due to the latter, that these rules have
been successfully re-used in subsequent works, most promi-
nently Klenner et al (2009).
64
ambiguous, e.g. like and for, and require disam-
biguation rules, using contextual information such
as their pertaining part-of-speech tag. These de-
limiters suitably account for various complex sen-
tence types so that only the clause containing the
negation is considered.
The heuristic rules focus on cases in which po-
lar expressions in specific syntactic configurations
are directly preceded by negation words which re-
sults in the polar expression becoming a delimiter
itself. Unlike Choi and Cardie (2008), these rules
require a proper parse and reflect grammatical re-
lationships between different constituents.
The complexity of the scope model proposed
by Jia et al (2009) is similar to the ones of
the compositional models (Moilanen and Pulman,
2007; Shaikh et al, 2007; Choi and Cardie, 2008)
where scope modeling is exclusively incorporated
in the compositional rules.
Apart from scope modeling, Jia et al (2009) also
employ a complex negation term disambiguation
considering not only phrases in which potential
negation expressions do not have an actual negat-
ing function (as already used in (Wilson et al,
2005)), but also negative rhetorical questions and
restricted comparative sentences.
On sentence-level polarity classification, their
scope model is compared with
? a simple negation scope using a fixed window size
(similar to the negation feature in (Wilson et al, 2005))
? the text span until the first occurrence of a polar expres-
sion following the negation word
? the entire sentence
The proposed method consistently outperforms
the simpler methods proving that the incorpora-
tion of linguistic insights into negation modeling
is meaningful. Even on polarity document re-
trieval, i.e. a more coarse-grained classification
task where contextual disambiguation usually
results in a less significant improvement, the
proposed method also outperforms the other
scopes examined.
There have only been few research efforts in
sentiment analysis examining the impact of scope
modeling for negation in contrast to other research
areas, such as the biomedical domain (Huang and
Lowe, 2007; Morante et al, 2008; Morante and
Daelemans, 2009). This is presumably due to the
fact that only for the biomedical domain, publicly
available corpora containing annotation for the
scope of negation exist (Szarvas et al, 2008). The
usability of those corpora for sentiment analysis
has not been tested.
3.4 Negation within Words
So far, negation has only be considered as a phe-
nomenon that affects entire words or phrases.
The word expressing a negation and the words
or phrases being negated are disjoint. There are,
however, cases in which both negation and the
negated content which can also be opinionated
are part of the same word. In case, these words
are lexicalized, such as flaw-less, and are conse-
quently to be found a polarity lexicon, this phe-
nomenon does not need to be accounted for in sen-
timent analysis. However, since this process is (at
least theoretically) productive, fairly uncommon
words, such as not-so-nice, anti-war or offensive-
less which are not necessarily contained in lexical
resources, may emerge as a result of this process.
Therefore, a polarity classifier should also be able
to decompose words and carry out negation mod-
eling within words.
There are only few works addressing this particu-
lar aspect (Moilanen and Pulman, 2008; Ku et al,
2009) so it is not clear how much impact this type
of negation has on an overall polarity classification
and what complexity of morphological analysis is
really necessary. We argue, however, that in syn-
thetic languages where negation may regularly be
realized as an affix rather than an individual word,
such an analysis is much more important.
3.5 Negation in Various Languages
Current research in sentiment analysis mainly fo-
cuses on English texts. Since there are signifi-
cant structural differences among the different lan-
guages, some particular methods may only cap-
ture the idiosyncratic properties of the English lan-
guage. This may also affect negation modeling.
The previous section already stated that the need
for morphological analyses may differ across the
different languages.
Moreover, the complexity of scope modeling may
also be language dependent. In English, for ex-
ample, modeling the scope of a negation as a
fixed window size of words following the oc-
currence of a negation expression already yields
a reasonable performance (Kennedy and Inkpen,
2005). However, in other languages, for example
German, more complex processing is required as
the negated expression may either precede (Sen-
65
tence 10) or follow (Sentence 11) the negation ex-
pression. Syntactic properties of the negated noun
phrase (i.e. the fact whether the negated polar ex-
pression is a verb or an adjective) determine the
particular negation construction.
10. Peter mag den Kuchen nicht.
Peter likes the cake not.
?Peter does not like the cake.?
11. Der Kuchen ist nicht ko?stlich.
The cake is not delicious.
?The cake is not delicious.?
These items show that, clearly, some more ex-
tensive cross-lingual examination is required in or-
der to be able to make statements of the general
applicability of specific negation models.
3.6 Bad and Not Good are Not the Same
The standard approach of negation modeling sug-
gests to consider a negated polar expression, such
as not bad, as an unnegated polar expression with
the opposite polarity, such as good. Liu and Seneff
(2009) claim, however, that this is an oversimpli-
fication of language. Not bad and good may have
the same polarity but they differ in their respec-
tive polar strength, i.e. not bad is less positive
than good. That is why, Liu and Seneff (2009)
suggest a compositional model in which for indi-
vidual adjectives and adverbs (the latter include
negations) a prior rating score encoding their in-
tensity and polarity is estimated from pros and
cons of on-line reviews. Moreover, compositional
rules for polar phrases, such as adverb-adjective or
negation-adverb-adjective are defined exclusively
using the scores of the individual words. Thus,
adverbs function like universal quantifiers scaling
either up or down the polar strength of the specific
polar adjectives they modify. The model indepen-
dently learns what negations are, i.e. a subset of
adverbs having stronger negative scores than other
adverbs. In short, the proposed model provides
a unifying account for intensifiers (e.g. very), di-
minishers, polarity shifters and negation words. Its
advantage is that polarity is treated composition-
ally and is interpreted as a continuum rather than
a binary classification. This approach reflects its
meaning in a more suitable manner.
3.7 Using Negations in Lexicon Induction
Many classification approaches illustrated above
depend on the knowledge of which natural lan-
guage expressions are polar. The process of ac-
quiring such lexical resources is called lexicon in-
duction. The observation that negations co-occur
with polar expressions has been used for inducing
polarity lexicons on Chinese in an unsupervised
manner (Zagibalov and Carroll, 2008). One ad-
vantage of negation is that though the induction
starts with just positive polar seeds, the method
also accomplishes to extract negative polar expres-
sions since negated mentions of the positive po-
lar seeds co-occur with negative polar expressions.
Moreover, and more importantly, the distribution
of the co-occurrence between polar expressions
and negations can be exploited for the selection of
those seed lexical items. The model presented by
Zagibalov and Carroll (2008) relies on the obser-
vation that a polar expression can be negated but it
occurs more frequently without the negation. The
distributional behaviour of an expression, i.e. sig-
nificantly often co-occurring with a negation word
but significantly more often occurring without a
negation word makes up a property of a polar ex-
pression. The data used for these experiments are
publicly available5 .
3.8 Irony ? The Big Challenge
Irony is a rhetorical process of intentionally using
words or expressions for uttering meaning that is
different from the one they have when used liter-
ally (Carvalho et al, 2009). Thus, we consider
that the use of irony can reflect an implicit nega-
tion of what is conveyed through the literal use of
the words. Moreover, due to its nature irony is
mostly used to express a polar opinion.
Carvalho et al (2009) confirm the relevance of
(verbal) irony for sentiment analysis by an error
analysis of their present classifier stating that a
large proportion of misclassifications derive from
their system?s inability to account for irony.
They present predictive features for detecting
irony in positive sentences (which are actually
meant to have a negative meaning). Their find-
ings are that the use of emoticons or expressions
of gestures and the use of quotation marks within
a context in which no reported speech is included
are a good signal of irony in written text. Although
the use of these clues in the defined patterns helps
to detect some situations in which irony is present,
they do not fully represent the phenomenon.
5http://www.informatics.sussex.ac.uk/
users/tz21/coling08.zip
66
A data-driven approach for irony detection on
product-reviews is presented in (Tsur et al, 2010).
In the first stage, a considerably large list of simple
surface patterns of ironic expressions are induced
from a small set of labeled seed sentences. A pat-
tern is a generalized word sequence in which con-
tent words are replaced by a generic CW symbol.
In the second stage, the seed sentences are used to
collect more examples from the web, relying on
the assumption that sentences next to ironic ones
are also ironic. In addition to these patterns, some
punctuation-based features are derived from the
labeled sentences. The acquired patterns are used
as features along the punctuation-based features
within a k nearest neighbour classifier. On an in-
domain test set the classifier achieves a reasonable
performance. Unfortunately, these experiments
only elicit few additional insights into the general
nature of irony. As there is no cross-domain eval-
uation of the system, it is unclear in how far this
approach generalizes to other domains.
4 Limits of Negation Modeling in
Sentiment Analysis
So far, this paper has not only outlined the impor-
tance of negation modeling in sentiment analysis
but it has also shown different ways to account for
this linguistic phenomenon. In this section, we
present the limits of negation modeling in senti-
ment analysis.
Earlier in this paper, we stated that negation mod-
eling depends on the knowledge of polar expres-
sions. However, the recognition of genuine polar
expressions is still fairly brittle. Many polar ex-
pressions, such as disease are ambiguous, i.e. they
have a polar meaning in one context (Sentence 12)
but do not have one in another (Sentence 13).
12. He is a disease to every team he has gone to.
13. Early symptoms of the disease are headaches, fevers,
cold chills and body pain.
In a pilot study (Akkaya et al, 2009), it has al-
ready been shown that applying subjectivity word
sense disambiguation in addition to the feature-
based negation modeling approach of Wilson et al
(2005) results in an improvement of performance
in polarity classification.
Another problem is that some polar opinions are
not lexicalized. Sentence 14 is a negative prag-
matic opinion (Somasundaran and Wiebe, 2009)
which can only be detected with the help of exter-
nal world knowledge.
14. The next time I hear this song on the radio, I?ll throw
my radio out of the window.
Moreover, the effectiveness of specific negation
models can only be proven with the help of cor-
pora containing those constructions or the type of
language behaviour that is reflected in the mod-
els to be evaluated. This presumably explains why
rare constructions, such as negations using con-
nectives (Sentence 6 in ?2), modals (Sentence 7
in ?2) or other phenomena presented in the con-
ceptual model of Polanyi and Zaenen (2004), have
not yet been dealt with.
5 Conclusion
In this paper, we have presented a survey on
the role of negation in sentiment analysis. The
plethora of work presented on the topic proves that
this common linguistic construction is highly rel-
evant for sentiment analysis.
An effective negation model for sentiment analy-
sis usually requires the knowledge of polar expres-
sions. Negation is not only conveyed by common
negation words but also other lexical units, such as
diminishers. Negation expressions are ambiguous,
i.e. in some contexts do not function as a nega-
tion and, therefore, need to be disambiguated. A
negation does not negate every word in a sentence,
therefore, using syntactic knowledge to model the
scope of negation expressions is useful.
Despite the existence of several approaches to
negation modeling for sentiment analysis, in or-
der to make general statements about the effective-
ness of specific methods systematic comparative
analyses examining the impact of different nega-
tion models (varying in complexity) with regard to
classification type, text granularity, target domain,
language etc. still need to be carried out.
Finally, negation modeling is only one aspect that
needs to be taken into consideration in sentiment
analysis. In order to fully master this task, other
aspects, such as a more reliable identification of
genuine polar expressions in specific contexts, are
at least as important as negation modeling.
Acknowledgements
Michael Wiegand was funded by the BMBF project NL-
Search under contract number 01IS08020B. Alexandra Bal-
ahur was funded by Ministerio de Ciencia e Innovacio?n -
Spanish Government (grant no. TIN2009-13391-C04-01),
and Conselleria d?Educacio?n-Generalitat Valenciana (grant
no. PROMETEO/2009/119 and ACOMP/2010/286).
67
References
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Subjec-
tivity Word Sense Disambiguation. In Proceedings
of EMNLP.
P. Carvalho, L. Sarmento, M. J. Silva, and
E. de Oliveira. 2009. Clues for Detecting
Irony in User-Generated Contents: Oh...!! It?s ?so
easy? ;-). In Proceedings of CIKM-Workshop TSA.
Y. Choi and C. Cardie. 2008. Learning with Compo-
sitional Semantics as Structural Inference for Sub-
sentential Sentiment Analysis. In Proceedings of
EMNLP.
Y. Huang and H. J. Lowe. 2007. A Novel Hybrid Ap-
proach to Automated Negation Detection in Clinical
Radiology Reports. JAMIA, 14.
L. Jia, C. Yu, and W. Meng. 2009. The Effect of Nega-
tion on Sentiment Analysis and Retrieval Effective-
ness. In Proceedings of CIKM.
A. Kennedy and D. Inkpen. 2005. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. In Proceedings of FINEXIN.
A. Kennedy and D. Inkpen. 2006. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. Computational Intelligence, 22.
M. Klenner, S. Petrakis, and A. Fahrni. 2009. Robust
Compositional Polarity Classification. In Proceed-
ings of RANLP.
L. Ku, T. Huang, and H. Chen. 2009. Using Morpho-
logical and Syntactic Structures for Chinese Opinion
Analysis. In Proceedings ACL/IJCNLP.
J. Liu and S. Seneff. 2009. Review Sentiment Scoring
via a Parse-and-Paraphrase Paradigm. In Proceed-
ings of EMNLP.
K. Moilanen and S. Pulman. 2007. Sentiment Con-
struction. In Proceedings of RANLP.
K. Moilanen and S. Pulman. 2008. The Good, the Bad,
and the Unknown. In Proceedings of ACL/HLT.
R. Morante and W. Daelemans. 2009. A Metalearning
Approach to Processing the Scope of Negation. In
Proceedings of CoNLL.
R. Morante, A. Liekens, and W. Daelemans. 2008.
Learning the Scope of Negation in Biomedical
Texts. In Proceedings of EMNLP.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of COLING/ACL.
B. Pang and L. Lee. 2004. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of
ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP.
L. Polanyi and A. Zaenen. 2004. Context Valence
Shifters. In Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
M. A. M. Shaikh, H. Prendinger, and M. Ishizuka.
2007. Assessing Sentiment of Text by Semantic De-
pendency and Contextual Valence Analysis. In Pro-
ceedings of ACII.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing Stances in Online Debates. In Proceedings of
ACL/IJCNLP.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope Corpus: Annotation for Negation,
Uncertainty and Their Scope in Biomedical Texts.
In Proceedings of BioNLP.
O. Tsur, D. Davidov, and A. Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceeding of ICWSM.
J. Wiebe. 1994. Tracking Point of View in Narrative.
Computational Linguistics, 20.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of HLT/EMNLP.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing Contextual Polarity: An Exploration for
Phrase-level Analysis. Computational Linguistics,
35:3.
T. Zagibalov and J. Carroll. 2008. Automatic Seed
Word Selection for Unsupervised Sentiment Classi-
fication of Chinese Text. In Proceedings of COL-
ING.
68
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 69?79,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Towards the Detection of Reliable Food-Health Relationships
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We investigate the task of detecting reli-
able statements about food-health relation-
ships from natural language texts. For that
purpose, we created a specially annotated
web corpus from forum entries discussing the
healthiness of certain food items. We ex-
amine a set of task-specific features (mostly)
based on linguistic insights that are instrumen-
tal in finding utterances that are commonly
perceived as reliable. These features are in-
corporated in a supervised classifier and com-
pared against standard features that are widely
used for various tasks in natural language pro-
cessing, such as bag of words, part-of speech
and syntactic parse information.
1 Introduction
In this paper, we explore some linguistic high-level
features to detect food-health relationships in natural
language texts that are perceived reliable. By food-
health relationships we mean relations that claim
that a food item is suitable (1) or unsuitable (2) for
some particular health condition.
(1) Baking soda is an approved remedy against
heartburn.
(2) During pregnancy women should not consume
any alcohol.
The same health claim may be uttered in differ-
ent ways (3)-(5) and, as a consequence, may be per-
ceived and judged differently. For the automatic ex-
traction of health claims, we believe that statements
that are perceived as reliable (4)-(5) are the most im-
portant to retrieve.
(3) Eggs do not have a negative impact on people
suffering from heart diseases.
(4) According to a leading medical scientist, the
consumption of eggs does not have a negative
impact on people suffering from heart diseases.
(5) I?m suffering from a heart disease and all my
life I?ve been eating many eggs; it never had
any impact on my well-being.
In this work, we will mine a web corpus of fo-
rum entries for such relations. Social media are a
promising source of such knowledge as, firstly, the
language employed is not very technical and thus,
unlike medical texts, accessible to the general pub-
lic. Secondly, social media can be considered as an
exclusive repository of popular wisdom. With re-
gard to the health conditions, we can find, for ex-
ample, home remedies. Despite the fact that many
of them are not scientifically proven, there is still a
great interest in that type of knowledge. However,
even though such content is usually not subject to
any scientific review, users would appreciate an au-
tomatic assessment of the quality of each relation ex-
pressed. In this work, we attempt a first step towards
this endeavour by automatically classifying these ut-
terances with regard to reliability.
The features we examine will be (mostly) based
on linguistic insights that are instrumental in finding
utterances that are commonly perceived as reliable.
These features are incorporated in a supervised clas-
sifier and compared against standard features that
are widely used for various tasks in natural language
processing, such as bag of words, part-of speech and
syntactic parse information.
69
Our experiments are carried out on German data.
We believe, however, that our findings carry over
to other languages since the linguistic aspects that
we address are (mostly) language universal. For the
sake of general accessibility, all examples will be
given as English translations.
2 Related Work
As far as the extraction of health relations from
social media are concerned, the prediction of epi-
demics (Fisichella et al, 2011; Torii et al, 2011;
Diaz-Aviles et al, 2012; Munro et al, 2012) has re-
cently attracted the attention of the research commu-
nity.
Relation extraction involving food items has also
been explored in the context of ontology align-
ment (van Hage et al, 2005; van Hage et al, 2006;
van Hage et al, 2010) and also as a means of knowl-
edge acquisition for virtual customer advice in a su-
permarket (Wiegand et al, 2012a).
The works most closely related to this paper are
Yang et al (2011) and Miao et al (2012). Both
of these works address the extraction of food-health
relationships. Unlike this work, they extract rela-
tions from scientific biomedical texts rather than so-
cial media. Yang et al (2011) also cover the task
of strength analysis which bears some resemblance
to the task of finding reliable utterances to some ex-
tent. However, the features applied to that classifica-
tion task are only standard features, such as bag of
words.
3 Data & Annotation
As a corpus for our experiments, we used a crawl
of chefkoch.de1 (Wiegand et al, 2012a) consisting
of 418, 558 webpages of food-related forum entries.
chefkoch.de is the largest web portal for food-related
issues in the German language. From this dataset,
sentences in which some food item co-occurred with
some health condition (e.g. pregnancy, diarrhoea
or flu) were extracted. (In the following, we will
also refer to these entities as target food item and
target health condition.) The food items were iden-
tified with the help of GermaNet (Hamp and Feld-
weg, 1997), the German version of WordNet (Miller
et al, 1990), and the health conditions were used
1www.chefkoch.de
from Wiegand et al (2012b). In total, 2604 sen-
tences were thus obtained.
For the manual annotation, each target sentence
(i.e. a sentence with a co-occurrence of target food
item and health condition) was presented in combi-
nation with the two sentences immediately preced-
ing and following it. Each target sentence was man-
ually assigned two labels, one specifying the type
of suitability (?3.1) and another specifying whether
the relation expressed is considered reliable or not
(?3.2).
3.1 Types of Suitability
The suitability-label indicates whether a polar rela-
tionship holds between the target food item and the
target health condition, and if so, which. Rather than
just focusing on positive polarity, i.e. suitability,
and negative polarity, i.e. unsuitability, we consider
more fine-grained classes. As such, the suitability-
label does not provide any explicit information about
the reliability of the utterance. In principle, ev-
ery polar relationship between target food item and
health condition expressed in a text could also be
formulated in such a way that it is perceived reliable.
In this work, we will consider the suitability-label as
given. We use it as a feature in order to measure the
correlation between suitability and reliability. The
usage of fine-grained labels is to investigate whether
subclasses of suitability or unsuitability have a ten-
dency to co-occur with reliability. (In other words:
we may assume differences among labels with the
same polarity type.) We define the following set of
fine-grained suitability-labels:
3.1.1 Suitable (SUIT)
SUIT encompasses all those statements in which
the consumption of the target food item is claimed to
be suitable for people affected by a particular health
condition (6). By suitable, we mean that there will
not be a negative effect on the health of a person
once he or she consumes the target food item. How-
ever, this relation type does not state that the con-
sumption is likely to improve the condition of the
person either.
(6) I also got dermatitis which is why my mother
used spelt flour [instead of wheat flour]; you
don?t taste a difference.
70
positive labels BENEF, SUIT, PREVENT
negative labels UNSUIT, CAUSE
Table 1: Categorization of suitability-labels.
3.1.2 Beneficial (BENEF)
While SUIT only states that the consumption of
the target food item is suitable for people with a par-
ticular health condition, BENEF actually states that
the consumption alleviates the symptoms of the con-
dition or even cures it (7). While both SUIT and
BENEF have a positive polarity, SUIT is much more
neutral than BENEF.
(7) Usually, a glass of milk helps me when I got a
sore throat.
3.1.3 Prevention (PREVENT)
An even stronger positive effect than the relation
type BENEF presents PREVENT which claims that
the consumption of the target food item can prevent
the outbreak of a particular disease (8).
(8) Citric acid largely reduces the chances of
kidney stones to develop.
3.1.4 Unsuitable (UNSUIT)
UNSUIT describes cases in which the consump-
tion of the target food item is deemed unsuitable (9).
Unsuitability means that one expects a negative ef-
fect (but it need not be mentioned explicitly), that
is, a deterioration of the health situation on the part
of the person who is affected by a particular health
condition.
(9) Raw milk cheese should not be eaten during
pregnancy.
3.1.5 Causation (CAUSE)
CAUSE is the negative counterpart of PREVENT.
It states that the consumption of the target food item
can actually cause a particular health condition (10).
(10) It?s a common fact that the regular consumption
of coke causes caries.
The suitability-labels can also be further sepa-
rated into two polar classes (i.e. positive and neg-
ative labels) as displayed in Table 1.
3.2 Reliability
Each utterance was additionally labeled as to
whether it was considered reliable (4)-(5) or not (3).
It is this label that we try to predict in this work. By
reliable, we understand utterances in which the re-
lations expressed are convincing in the sense that a
reputable source is cited, some explanation or empir-
ical evidence for the relation is given, or the relation
itself is emphasized by the speaker. In this work,
we are exclusively interested in detecting utterances
which are perceived reliable by the reader. We leave
aside whether the statements from our text corpus
are actually correct. Our aim is to identify linguis-
tic cues that evoke the impression of reliability on
behalf of the reader.
3.3 Class Distributions and Annotation
Agreement
Table 2 depicts the distribution of the reliability-
labels on our corpus while Table 3 lists the class dis-
tribution of the suitability-labels including the pro-
portion of the reliable instances among each cate-
gory. The proportion of reliable instances varies
quite a lot among the different suitability-labels,
which indicates that the suitability may be some ef-
fective feature.
Note that the class OTHER in Table 3 comprises
all instances in which the co-occurrence of a health
condition and a food item was co-incidental (11) or
there was some embedding that discarded the valid-
ity of the respective suitability-relation, as it is the
case, for example, in questions (12).
(11) It?s not his diabetes I?m concerned about but
the enormous amounts of fat that he consumes.
(12) Does anyone know whether I can eat tofu dur-
ing my pregnancy?
In order to measure interannotation agreement,
we collected for three health conditions their co-
occurrences with any food item. For the suitability-
labels we computed Cohen?s ? = 0.76 and for the
reliability-labels ? = 0.61. The agreement for reli-
ability is lower than for suitability. We assume that
the reason for that lies in the highly subjective no-
tion of reliability. Still, both agreements can be in-
terpreted as substantial (Landis and Koch, 1977) and
should be sufficiently high for our experiments.
71
Type Frequency Percentage
Reliable 480 18.43
Not Reliable 2124 81.57
Table 2: Distribution of the reliability-labels.
Type Frequency Perc. Perc. Reliable
BENEF 502 19.28 33.39
CAUSE 482 18.51 22.57
SUIT 428 16.44 17.91
UNSUIT 277 10.64 34.05
PREVENT 74 2.84 14.04
OTHER 841 32.30 0.00
Table 3: Distribution of the suitability-labels.
4 Feature Design
4.1 Task-specific High-level Feature Types
We now describe the different task-specific high-
level feature types. We call them high-level feature
types since they model concepts that typically gen-
eralize over sets of individual words (i.e. low-level
features).
4.1.1 Explanatory Statements (EXPL)
The most obvious type of reliability is a
suitability-relation that is also accompanied by some
explanatory statement. That is, some reason for the
relation expressed is given (13). We detect reasons
by scanning a sentence for typical discourse cues
(more precisely: conjunctions) that anchor such re-
marks, e.g. which is why or because.
(13) Honey has an antiseptic effect which is why it
is an ideal additive to milk in order to cure a
sore throat.
4.1.2 Frequent Observation (FREQ)
If a speaker claims to have witnessed a certain
relation very frequently or even at all times, then
there is a high likelihood that this relation actually
holds (14). We use a set of adverbs (18 expressions)
that express high frequency (e.g. often, frequently
etc.) or constancy (e.g. always, at all times etc.).
(14) What always helps me when I have the flu is a
hot chicken broth.
4.1.3 Intensifiers (INTENS)
Some utterances may also be perceived reliable if
their speaker adds some emphasis to them. One way
of doing so is by adding intensifiers to a remark (15).
(15) You can treat nausea with ginger very effec-
tively.
The intensifiers we use are a translation of the lex-
icon introduced in Wilson et al (2005). For the de-
tection, we divide that list into two groups:
The first group INTENSsimple are unambiguous
adverbs that always function as intensifiers no mat-
ter in which context they appear (e.g. very or ex-
tremely).
The second group includes more ambiguous ex-
pressions, such as adjectives that only function as
an intensifier if they modify a polar expression
(e.g. horrible pain or terribly nice) otherwise they
function as typical polar expressions (e.g. you
are horrible? or he sang terribly?). We employ
two methods to detect these ambiguous expressions.
INTENSpolar requires a polar expression of a polar-
ity lexicon to be modified by the intensifier, while
INTENSadj requires an adjective to be modified. In
order to identify polar expressions we use the polar-
ity lexicon underlying the PolArt system (Klenner
et al, 2009). We also consider adjectives since we
must assume that our polarity lexicon does not cover
all possible polar expressions. We chose adjectives
as a complement criterion as this part of speech is
known to contain most polar expressions (Hatzivas-
siloglou and McKeown, 1997; Hatzivassiloglou and
Wiebe, 2000).
4.1.4 Strong Polar Expressions (STROPO)
Instead of adding intensifiers in order to put more
emphasis to a remark (?4.1.3), one may also use
polar expressions that convey a high polar inten-
sity (16). For instance, nice and excellent refer to
the same scale and convey positive polarity but ex-
cellent has a much higher polar intensity than nice.
Taboada et al (2011) introduced an English polar-
ity lexicon SO-CAL in which polar expressions were
also assigned an intensity label. As our German
polarity lexicon (?4.1.3) does not contain compara-
ble intensity labels, we used a German translation
of SO-CAL. We identified polar expressions with a
high intensity score (i.e. ?4 or ?5) as strong po-
lar expressions. It includes 221 highly positive and
344 highly negative polar expressions. We also dis-
tinguish the polarity type (i.e. STROPO+ refers to
positive and STROPO? refers to negative polarity).
72
(16) Baking soda is an excellent remedy against
heartburn.
4.1.5 Superlatives (SUPER)
Another way of expressing high polar intensity is
by applying superlatives (17). Superlatives can only
be formed from gradable adjectives. At the same
time, the greatest amount of such adjectives are also
subjective expressions (Hatzivassiloglou and Wiebe,
2000). As a consequence, the detection of this
grammatical category does not depend on a subjec-
tivity/polarity lexicon but on simple morphological
suffixes (e.g. -est in strongest)2 or combinations
with certain modifiers (e.g. most in most terrific).
(17) Baking soda is the most effective remedy
against heartburn.
4.1.6 Statements Made by Authorities (AUTH)
If a statement is quoted from an authority, then it
is usually perceived more reliable than other state-
ments (4). Authorities in our domain are mostly sci-
entists and medical doctors. Not only does a men-
tion of those types of professions indicate an author-
ity but also the citation of their work. Therefore,
for this feature we also scan for expressions, such as
journal, report, survey etc. Our final look-up list of
cues comprises 53 expressions.
We also considered using the knowledge of user
profiles in order to identify speakers whose profes-
sion fall under our defined set of authorities. Unfor-
tunately, the overwhelming majority of users who
actually specified their profession cannot be consid-
ered as authorities (for the relations that we are inter-
ested in) by mere consideration of their profession.
Most users of chefkoch.de are either office employ-
ees, housewifes, students or chefs. Less than 1% are
authorities according to our definition. Due to the
severe sparsity of authorities, we refrained from us-
ing the professions as they are specified in the user
profiles.
2We could not use part-of-speech tagging for the detec-
tion of superlatives since, unlike the standard English part-of-
speech tag set (i.e. the Penn Treebank Tag Set (Marcus et al,
1993)), information regarding gradation (i.e. comparative and
superlative) is not reflected in the standard German tag set (i.e.
Stuttgart Tu?binger Tag Set (Schiller et al, 1995)).
4.1.7 Doctors? Prescriptions (PRESC)
Some of our food-health relations are also men-
tioned in the context of doctors? prescriptions (5).
That is, a doctor may prescribe a patient to con-
sume a particular food item since it is considered
suitable for their health condition, or he/she may
forbid a food item in case it is considered unsuit-
able. As already pointed out in ?4.1.6, doctors usu-
ally present an authority with regard to food-health
relations. That is why, their remarks should be con-
sidered reliable.
In order to detect doctors? prescriptions, we
mainly look for (modal) verbs in a sentence that ex-
press obligations or prohibitions. We found that, on
our dataset, people rarely mention their doctor ex-
plicitly if they refer to a particular prescription. In-
stead, they just mention that they must or must not
consume a particular food item. From the context,
however, it is obvious that they refer to their doc-
tor?s prescription (18).
(18) Due to my diabetes I must not eat any sweets.
4.1.8 Hedge Cues (HEDGE)
While all previous features were designed to iden-
tify cases of reliable statements, we also include fea-
tures that indicate the opposite. The most obvious
type of utterances that are commonly considered un-
reliable are so-called hedges (Lakoff, 1973) or spec-
ulations (19).
(19) Coke may cause cancer.
For this feature, we use a German translation of En-
glish cue words that have been found useful in pre-
vious work (Morante and Daelemans, 2009) which
results in 47 different expressions.
4.1.9 Types of Suitability-Relations (REL)
Finally, we also incorporate the information
about what type of suitability-relation a statement
was labeled with. The suitability-labels were al-
ready presented and motivated in ?3.1. The con-
crete features are: RELSUIT (?3.1.1), RELBENEF
(?3.1.2), RELPREVENT (?3.1.3), RELUNSUIT
(?3.1.4), RELCAUSE (?3.1.5).
73
Suffix Description
-WNDfood context window around food item
-WNDcond context window around health condition
-TS target sentence only
-EC entire (instance) context
Table 4: Variants for the individual feature types.
4.2 Variants of Feature Types
For our feature types we examine several variants
that differ in the size of context/scope. We distin-
guish between the target sentence and the entire con-
text of an instance, i.e. the target sentence plus the
two preceding and following sentences (?3). If only
the target sentence is considered, we can also con-
fine the occurrence of a cue word to a fixed window
(comprising 5 words) either around the target food
item or the target health condition rather than con-
sidering the entire sentence.
Small contexts usually offer a good precision. For
example, if a feature type occurs nearby a mention
of the target food item or health condition, the fea-
ture type and the target expression are likely to be
related to each other. The downside of such narrow
contexts is that they may be too sparse. Wide con-
texts may be better suited to situations in which a
high recall is desirable. However, ambiguous fea-
ture types may perform poorly with these contexts
as their co-occurrence with a target expression at a
large distance is likely to be co-incidental.
Table 4 lists all the variants that we use. These
variants are applied to all feature types except the
types of suitability (?4.1.9) as this label has only
been assigned to an entire target sentence.
4.3 Other Features
Table 5 lists the entire set of features that we ex-
amine in this work. The simplest classifier that we
can construct for our task is a trivial classifier that
predicts all statements as reliable statements. The
remaining features comprise bag of words, part-of-
speech and syntactic parse information. For the
latter two features, we employ the output of the
Stanford Parser for German (Rafferty and Manning,
2008).
Features Description
all trivial classifier that always predicts a reliable state-
ment
bow bag-of-words features: all words between the target
food item and target health condition and the words
immediately preceding and following each of them
pos part-of-speech features: part-of-speech sequence be-
tween target food item and health condition and tags
of the words immediately preceding and following
each of the target expressions
synt path from syntactic parse tree from target food item
to target health condition
task all task-specific high-level feature types from ?4.1
with their respective variants (?4.2)
Table 5: Description of all feature sets.
5 Experiments
Each instance to be classified is a sentence in which
there is a co-occurrence of a target food item and
a target health condition along its respective con-
text sentences (?3). We only consider sentences in
which the co-occurrence expresses an actual suit-
ability relationship between the target food item and
the target health condition, that is, we ignore in-
stances labeled with the suitability-label OTHER
(?3.3). We make this restriction as the instances
labeled as OTHER are not eligible for being reli-
able statements (Table 3). In this work, we take the
suitability-labels for granted (this allows us to easily
exclude the instances labeled as OTHER). The au-
tomatic detection of suitability-labels would require
a different classifier with a different set of features
whose appropriate discussion would be beyond the
scope of this paper.
5.1 Comparison of the Different Task-specific
High-level Features
In our first experiment, we want to find out how
the different task-specific high-level features that we
have proposed in this work compare to each other.
More specifically, we want to find out how the in-
dividual features correlate with the utterances that
have been manually marked as reliable. For that
purpose, Table 6 shows the top 20 features accord-
ing to Chi-square feature selection computed with
WEKA (Witten and Frank, 2005). More informa-
tion regarding the computation of Chi-square statis-
tics in the context of text classification can be found
in Yang and Pederson (1997). Note that we apply
feature selection only as a means of feature compar-
74
Rank Feature Score
1 FREQ-WNDfood 105.1
2 FREQ-TS 102.8
3 FREQ-WNDcond 75.9
4 FREQ-EC 29.2
5 AUTH-EC 23.7
6 STROPO+-WNDcond 20.5
7 RELBENEF 20.2
8 RELSUIT 16.8
9 INTENSsimple-WNDcond 16.4
10 AUTH-TS 15.4
11 STROPO+-TS 15.0
12 INTENSsimple-EC 14.1
13 STROPO+-WNDfood 13.7
14 INTENSadj-WNDfood 13.2
15 INTENSsimple-WNDfood 12.1
16 INTENSsimple-TS 11.6
17 PRESC-WNDfood 11.0
18 INTENSadj-WNDcond 9.7
19 INTENSpolar-EC 9.0
20 AUTH-WNDfood 7.9
Table 6: Top 20 features according to Chi-square fea-
ture ranking (for each feature type the most highly ranked
variant is highlighted).
ison. For classification (?5.2), we will use the entire
feature set.
5.1.1 What are the most effective features?
There are basically five feature types that dom-
inate the highest ranks. They are FREQ, AUTH,
STROPO, REL and INTENS. This already indicates
that several features presented in this work are ef-
fective. It is interesting to see that two types of
suitability-labels, i.e. RELBENEF and RELSUIT ,
are among the highest ranked features which sug-
gests that suitability and reliability are somehow
connected.
Table 7 shows both precision and recall for each
of the most highly ranked variant of the feature types
that appear on the top 20 ranks according to Chi-
square ranking (Table 6). Thus, we can have an idea
in how far the high performing feature types differ.
We only display one feature per feature type due to
the limited space. The table shows that for most of
these features precision largely outperforms recall.
RELBENEF is the only notable exception (its recall
actually outperforms precision).
5.1.2 Positive Orientation and Reliability
By closer inspection of the highly ranked features,
we found quite a few features with positive ori-
Feature Prec Rec
FREQ-WNDfood 71.13 14.38
AUTH-EC 41.81 15.42
STROPO+-WNDcond 63.38 3.54
RELBENEF 33.39 39.17
INTENSsimple-WNDcond 41.73 11.04
PRESC-WNDfood 45.00 5.63
Table 7: Precision and recall of different features (we list
the most highly ranked variants of the feature types from
Table 6).
entation, i.e. STROPO+-WNDcond, RELBENEF ,
RELSUIT , STROPO+-WNDcond, while their nega-
tive counterparts are absent. This raises the question
whether there is a bias for positive orientation for the
detection of reliability.
We assume that there are different reasons why
the positive suitability-labels (RELBENEF and
RELSUIT ) and strong positive polarity (STROPO+)
are highly ranked features:
As far as polarity features are concerned, it is
known from sentiment analysis that positive polar-
ity is usually easier to detect than negative polar-
ity (Wiegand et al, 2013). This can largely be as-
cribed to social conventions to be less blunt with
communicating negative sentiment. For that reason,
for example, one often applies negated positive polar
expressions (e.g. not okay) or irony to express a neg-
ative sentiment rather than using an explicit negative
polar expression. Of course, such implicit types of
negative polarity are much more difficult to detect
automatically.
The highly ranked suitability-labels may be labels
with the same orientation (i.e. they both describe
relationships that a food item is suitable rather than
unsuitable for a particular health condition), yet they
have quite different properties.3 While RELBENEF
is a feature positively correlating with reliable ut-
terances, the opposite is true of RELSUIT , that is,
there is a correlation but this correlation is nega-
tive. Table 8 compares their respective precision
and also includes the trivial (reference) classifier all
that always predicts a reliable statement. The ta-
ble clearly shows that RELBENEF is above the triv-
3It is not the case that the proportion of reliable utterances
is larger among the entire set of instances tagged with positive
suitability-labels than among the instances tagged with negative
suitability-labels (Table 1). In both cases, they are at approx.
26%.
75
ial feature while RELSUIT is clearly below. (One
may wonder why the gap in precision between those
different features is not larger. These features are
also high-recall features ? we have shown this for
RELBENEF in Table 7 ? so the smaller gaps may
already have a significant impact.) In plain, this
result means that a statement conveying that some
food item alleviates the symptoms of a particular
disease or even cures it (RELBENEF ) is more likely
to be an utterance that is perceived reliable rather
than statements in which the speaker merely states
that the food item is suitable given a particular health
condition (RELSUIT ). Presumably, the latter type
of suitability-relations are mostly uttered parenthet-
ically (not emphatically), or they are remarks in
which the relation is inferred, so that they are un-
likely to provide further background information. In
Sentence (20), for example, the suitability of whole-
meal products is inferred as the speaker?s father eats
these types of food due to his diabetes. The focus
of this remark, however, is the psychic well-being of
the speaker?s father. That entire utterance does not
present any especially reliable or otherwise helpful
information regarding the relationship between dia-
betes and wholemeal products.
(20) My father suffers from diabetes and is fed up
with eating all these wholemeal products. We
are worried that he is going to fall into a de-
pression.
Having explained that the two (frequently occur-
ring) positive suitability-labels are highly ranked
features because they separate reliable from less re-
liable statements, one may wonder why we do not
find a similar behaviour on the negative suitability-
labels. The answer to this lies in the fact that there
is no similar distinction between RELBENEF and
RELSUIT among utterances expressing unsuitabil-
ity. There is no neutral negative suitability-label
similar to RELSUIT . The relation RELUNSUIT
expresses unsuitability which is usually connected
with some deterioration in health.
5.1.3 How important are explanatory
statements for this task?
We were very surprised that the feature type to
indicate explanatory statements EXPL (?4.1.1) per-
formed very poorly (none of its variants is listed in
Feature RELSUIT all RELBENEF
Prec 17.81 26.46 33.39
Table 8: The precision of different REL-features com-
pared to the trivial classifier all that always predicts a re-
liable utterance.
Type EXPLall EXPLcue
Percentage 22.59 8.30
Table 9: Proportion of explanatory statements among re-
liable utterances (EXPLall: all reliable instances that are
explanatory statements; EXPLcue: subset of explanatory
statements that also contain a lexical cue).
Table 6) since we assumed explanatory statements
to be one of the most relevant types of utterances.
In order to find a reason for this, we manually an-
notated all reliable utterances as to whether they can
be regarded as an explanatory statement (EXPLall)
and, if so, whether (in principle) there are lexical
cues (such as our set of conjunctions) to identify
them (EXPLcue). Table 9 shows the proportion of
these two categories among the reliable utterances.
With more than 20% being labeled as this subtype,
explanatory statements are clearly not a fringe phe-
nomenon. However, lexical cues could only be ob-
served in approximately 1/3 of those instances. The
majority of cases, such as Sentence (21), do not con-
tain any lexical cues and are thus extremely difficult
to detect.
(21) Citrus fruits are bad for dermatitis. They in-
crease the itch. Such fruits are rich in acids that
irritate your skin.
In addition, all variants of our feature type EXPL
have a poor precision (between 20 ? 25%). This
means that the underlying lexical cues are too am-
biguous.
5.1.4 How important are the different
contextual scopes?
Table 6 clearly shows that the contextual scope
of a feature type matters. For example, for the fea-
ture type FREQ, the most effective scope achieves
a Chi-square score of 105.1 while the worst vari-
ant only achieves a score of 29.2. However, there
is no unique contextual scope which always outper-
forms the other variants. This is mostly due to the
76
Feature Set Prec Rec F1
all 26.46 100.00 41.85
bow 37.14 62.44 46.45
bow+pos 36.85 57.64 44.88
bow+synt 39.05 58.01 46.58
task 35.16 72.89 47.21
bow+task 42.54 66.01 51.56?
Table 10: Comparison of different feature sets (summary
of features is displayed in Table 5); ? significantly better
than bow at p < 0.05 (based on paired t-test).
fact the different feature types have different proper-
ties. On the one hand, there are unambiguous fea-
ture types, such as AUTH, which work fine with
a wide scope. But we also have ambiguous fea-
ture types that require a fairly narrow context. A
typical example are strong (positive) polar expres-
sions (STROPO+). (Polar expressions are known
to be very ambiguous (Wiebe and Mihalcea, 2006;
Akkaya et al, 2009).)
5.2 Classification
Table 10 compares the different feature sets with
regard to extraction performance. We carry out
a 5-fold cross-validation on our manually labeled
dataset. As a classifier, we chose Support Vector
Machines (Joachims, 1999). As a toolkit, we use
SVMLight4 with a linear kernel.
Table 10 clearly shows the strength of the high-
level features that we proposed. They do not only
represent a strong feature set on their own but they
can also usefully be combined with bag-of-words
features. Apparently, neither part-of-speech nor
parse information are predictive for this task.
5.3 Impact of Training Data
Figure 1 compares bag-of-words features and our
task-specific high-level features on a learning curve.
The curve shows that the inclusion of our task-
specific features improves performance. Interest-
ingly, with task alone we obtain a good performance
on smaller amounts of data. However, this classifier
is already saturated with 40% of the training data.
From then onwards, it is more effective to use the
combination bow+task. Our high-level features gen-
eralize well which is particularly important for situ-
ations in which only few training data are available.
4http://svmlight.joachims.org
 25
 30
 35
 40
 45
 50
 55
 10  20  30  40  50  60  70  80  90  100
F-
sc
or
e
Percentage of training data
bag of words (bow)
task-specific high-level features (task)
combination (bow+task)
Figure 1: Learning curve of the different feature sets.
However, in situations in which large training sets
are available, we additionally need bag of words that
are able to harness more sparse but specific informa-
tion.
6 Conclusion
In this paper, we examined a set of task-specific
high-level features in order to detect food-health re-
lations that are perceived reliable. We found that,
in principle, a subset of these features that include
adverbials expressing frequent observations, state-
ments made by authorities, strong polar expressions
and intensifiers are fairly predictive and complement
bag-of-words information. We also observed a cor-
relation between some suitability-labels and relia-
bility. Moreover, the effectiveness of the different
features depends very much on the context to which
they are applied.
Acknowledgements
This work was performed in the context of the Software-
Cluster project EMERGENT. Michael Wiegand was
funded by the German Federal Ministry of Education and
Research (BMBF) under grant no. ?01IC10S01?. The
authors would like to thank Stephanie Ko?ser for annotat-
ing the dataset presented in the paper. The authors would
also like to thank Prof. Dr. Wolfgang Menzel for provid-
ing the German version of the SO-CAL polarity lexicon
that has been developed at his department.
77
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity Word Sense Disambiguation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 190?199,
Singapore.
Ernesto Diaz-Aviles, Avar Stewart, Edward Velasco, Ker-
stin Denecke, and Wolfgang Nejdl. 2012. Epidemic
Intelligence for the Crowd, by the Crowd. In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media (ICWSM), Dublin, Ireland.
Marco Fisichella, Avar Stewart, Alfredo Cuzzocrea, and
Kerstin Denecke. 2011. Detecting Health Events on
the Social Web to Enable Epidemic Intelligence. In
Proceedings of the International Symposium on String
Processing and Information Retrieval (SPIRE), pages
87?103, Pisa, Italy.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a
Lexical-Semantic Net for German. In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9?15, Madrid, Spain.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 174?181, Madrid, Spain.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of Adjective Orientation and Gradability on Sen-
tence Subjectivity. In Proceedings of the International
Conference on Computational Linguistics (COLING),
pages 299?305, Saarbru?cken, Germany.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Manfred Klenner, Stefanos Petrakis, and Angela Fahrni.
2009. Robust Compositional Polarity Classification.
In Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP), pages 180?184, Borovets,
Bulgaria.
George Lakoff. 1973. Hedging: A Study in Media Crite-
ria and the Logic of Fuzzy Concepts. Journal of Philo-
sophical Logic, 2:458 ? 508.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330, June. Special Issue
on Using Large Corpora.
Qingliang Miao, Shu Zhang, Bo Zhang, Yao Meng, and
Hao Yu. 2012. Extracting and Visualizing Seman-
tic Relationships from Chinese Biomedical Text. In
Proceedings of the Pacific Asia Conference on Lan-
guage, Information and Compuation (PACLIC), pages
99?107, Bali, Indonesia.
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduc-
tion to WordNet: An On-line Lexical Database. Inter-
national Journal of Lexicography, 3:235?244.
Roser Morante and Walter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In Pro-
ceedings of the BioNLP Workshop, pages 28?36, Boul-
der, CO, USA.
Robert Munro, Lucky Gunasekara, Stephanie Nevins,
Lalith Polepeddi, and Evan Rosen. 2012. Track-
ing Epidemics with Natural Language Processing and
Crowdsourcing. In Proceedings of the Spring Sympo-
sium for Association for the Advancement of Artificial
Intelligence (AAAI), pages 52?58, Toronto, Canada.
Anna Rafferty and Christopher D. Manning. 2008. Pars-
ing Three German Treebanks: Lexicalized and Un-
lexicalized Baselines. In Proceedings of the ACL
Workshop on Parsing German (PaGe), pages 40?46,
Columbus, OH, USA.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1995. Vorla?ufige Guidelines
fu?r das Tagging deutscher Textcorpora mit STTS.
Technical report, Universita?t Stuttgart, Insitut fu?r
maschinelle Sprachverarbeitung, and Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267 ? 307.
Manabu Torii, Lanlan Yin, Thang Nguyen, Chand T.
Mazumdar, Hongfang Liu, David M. Hartley, and
Noele P. Nelson. 2011. An exploratory study of a text
classification framework for internet-based surveil-
lance of emerging epidemics. Internal Journal of
Medical Informatics, 80(1):56?66.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguistic
Ontology-Mapping Techniques. In Proceedings of In-
ternational Semantic Web Conference (ISWC), pages
732 ? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber.
2006. A Method for Learning Part-Whole Relations.
In Proceedings of International Semantic Web Con-
ference (ISWC), pages 723 ? 735, Athens, GA, USA.
Springer.
Willem Robert van Hage, Margherita Sini, Lori Finch,
Hap Kolb, and Guus Schreiber. 2010. The OAEI food
78
task: an analysis of a thesaurus alignment task. Ap-
plied Ontology, 5(1):1 ? 28.
Janyce Wiebe and Rada Mihalcea. 2006. Word Sense
and Subjectivity. In Proceedings of the International
Conference on Computational Linguistics and Annual
Meeting of the Association for Computational Linguis-
tics (COLING/ACL), pages 1065?1072, Syndney, Aus-
tralia.
Michael Wiegand, Benjamin Roth, and Dietrich Klakow.
2012a. Web-based Relation Extraction for the Food
Domain. In Proceeding of the International Confer-
ence on Applications of Natural Language Process-
ing to Information Systems (NLDB), pages 222?227,
Groningen, the Netherlands. Springer.
Michael Wiegand, Benjamin Roth, Eva Lasarcyk,
Stephanie Ko?ser, and Dietrich Klakow. 2012b. A
Gold Standard for Relation Extraction in the Food Do-
main. In Proceedings of the Conference on Language
Resources and Evaluation (LREC), pages 507?514, Is-
tanbul, Turkey.
Michael Wiegand, Manfred Klenner, and Dietrich
Klakow. 2013. Bootstrapping polarity classifiers with
rule-based classification. Language Resources and
Evaluation, Online First:1?40.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-level
Sentiment Analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 347?354, Vancouver, BC, Canada.
Ian Witten and Eibe Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques. Morgan
Kaufmann Publishers, San Francisco, US.
Yiming Yang and Jan Pederson. 1997. A Comparative
Study on Feature Selection in Text Categorization. In
Proceedings the International Conference on Machine
Learning (ICML), pages 412?420, Nashville, US.
Hui Yang, Rajesh Swaminathan, Abhishek Sharma, Vi-
las Ketkar, and Jason D?Silva, 2011. Learning Struc-
ture and Schemas from Documents, volume 375 of
Studies in Computational Intelligence, chapter Min-
ing Biomedical Text Towards Building a Quantitative
Food-disease-gene Network, pages 205?225. Springer
Berlin Heidelberg.
79
