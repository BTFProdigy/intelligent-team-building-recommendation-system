Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 305?310,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Towards Tracking Semantic Change by Visual Analytics
Christian Rohrdantz1 Annette Hautli2 Thomas Mayer2
Miriam Butt2 Daniel A. Keim1 Frans Plank2
Department of Computer Science1 Department of Linguistics2
University of Konstanz
Abstract
This paper presents a new approach to detect-
ing and tracking changes in word meaning by
visually modeling and representing diachronic
development in word contexts. Previous stud-
ies have shown that computational models
are capable of clustering and disambiguat-
ing senses, a more recent trend investigates
whether changes in word meaning can be
tracked by automatic methods. The aim of our
study is to offer a new instrument for inves-
tigating the diachronic development of word
senses in a way that allows for a better under-
standing of the nature of semantic change in
general. For this purpose we combine tech-
niques from the field of Visual Analytics with
unsupervised methods from Natural Language
Processing, allowing for an interactive visual
exploration of semantic change.
1 Introduction
The problem of determining and inferring the sense
of a word on the basis of its context has been the
subject of quite a bit of research. Earlier investiga-
tions have mainly focused on the disambiguation of
word senses from information contained in the con-
text, e.g. Schu?tze (1998) or on the induction of word
senses (Yarowsky, 1995). Only recently, the field
has added a diachronic dimension to its investiga-
tions and has moved towards the computational de-
tection of sense development over time (Sagi et al,
2009; Cook and Stevenson, 2010), thereby comple-
menting theoretical investigations in historical lin-
guistics with information gained from large corpora.
These approaches have concentrated on measuring
general changes in the meaning of a word (e.g., nar-
rowing or pejoration), whereas in this paper we deal
with cases where words acquire a new sense by ex-
tending their contexts to other domains.
For the scope of this investigation we restrict our-
selves to cases of semantic change in English even
though the methodology is generally language in-
dependent. Our choice is on the one hand moti-
vated by the extensive knowledge available on se-
mantic change in English. On the other hand, our
choice was driven by the availability of large cor-
pora for English. In particular, we used the New
York Times Annotated Corpus.1 Given the variety
and the amount of text available, we are able to track
changes from 1987 until 2007 in 1.8 million news-
paper articles.
In order to be able to explore our approach in a
fruitful manner, we decided to concentrate on words
which have acquired a new dimension of use due
to the introduction of computing and the internet,
e.g., to browse, to surf, bookmark. In particular,
the Netscape Navigator was introduced in 1994 and
our data show that this does indeed correlate with a
change in use of these words.
Our approach combines methods from the fields
of Information Visualization and Visual Analyt-
ics (Thomas and Cook, 2005; Keim et al, 2010)
with unsupervised techniques from Natural Lan-
guage Processing (NLP). This combination provides
a novel instrument which allows for tracking the di-
achronic development of word meaning by visual-
izing the contexts in which the words occur. Our
overall aim is not to replace linguistic analysis in
1http://http://www.ldc.upenn.edu/
305
this field with an automatic method, but to guide re-
search by generating new hypotheses about the de-
velopment of semantic change.
2 Related work
The computational modeling of word senses is based
on the assumption that the meaning of a word can
be inferred from the words in its immediate con-
text (?context words?). Research in this area mainly
focuses on two related tasks: Word Sense Disam-
biguation (WSD) and Word Sense Induction (WSI).
The goal of WSD is to classify occurrences of pol-
ysemous words according to manually predefined
senses. One popular method for performing such
a classification is Latent Semantic Analysis (LSA)
(Deerwester et al, 1990), with other methods also
suitable for the task (see Navigli (2009) for an ex-
tensive survey).
The aim of WSI is to learn word senses from
text corpora without having a predefined number of
senses. This goal is more difficult to achieve, as it
is not clear beforehand how many senses should be
extracted and how a sense could be described in an
abstract way. Recently, however, Brody and Lapata
(2009) have shown that Latent Dirichlet Allocation
(LDA) (Blei et al, 2003) can be successfully applied
to perform word sense induction from small word
contexts.
The original idea of LSA and LDA is to learn ?top-
ics? from documents, whereas in our scenario word
contexts rather than documents are used, i.e., a small
number of words before and after the word under
investigation (bag of words). Sagi et al (2009)
have demonstrated that broadening and narrowing
of word senses can be tracked over time by applying
LSA to small word contexts in diachronic corpora.
In addition, we will use LDA, which has proven even
more reliable in the course of our investigations.
In general, the aim of our paper is to go beyond
the approach of Sagi et al (2009) and analyze se-
mantic change in more detail. Ideally, a starting
point of change is found and the development over
time can be tracked, paired with a quantitative com-
parison of prevailing senses. We therefore suggest
to visualize word contexts in order to gain a better
understanding of diachronic developments and also
generate hypotheses for further investigations.
3 An interactive visualization approach to
semantic change
In order to test our approach, we opted for a large
corpus with a high temporal resolution. The New
York Times Annotated Corpus with 1.8 million
newspaper articles from 1987 to 2007 has a rather
small time depth of 20 years but provides a time
stamp for the exact publication date. Therefore,
changes can be tracked on a daily basis.
The data processing involved context extraction,
vector space creation, and sense modeling. As
Schu?tze (1998) showed, looking at a context win-
dow of 25 words before and after a key word pro-
vides enough information in order to disambiguate
word senses. Each extracted context is comple-
mented with the time stamp from the corpus. To
reduce the dimensionality, all context words were
lemmatized and stop words were filtered out.
For the set of all contexts of a key word, a global
LDA model was trained using the MALLET toolkit2
(McCallum, 2002). Each context is assigned to its
most probable topic/sense, complemented by a spe-
cific point on the time scale according to its time
stamp from the corpus. Contexts for which the high-
est probability was less than 40% were omitted be-
cause they could not be assigned to a certain sense
unambiguously. The distribution of senses over time
was then visualized.
3.1 Visualization
Different visualizations provide multidimensional
views on the data and yield a better understanding
of the developments. While plotting every word oc-
currence individually offers the opportunity to detect
and inspect outliers, aggregated views on the data
are able to provide insights on overall developments.
Figure 1 provides a view where the percentages of
word contexts belonging to different senses are plot-
ted over time. For the verbs to browse and to surf
seven senses are learned with LDA. Each sense cor-
responds to one row and is described by the top five
terms identified by LDA. The higher the gray area
at a certain x-axis point, the more of the contexts of
the corresponding year belong to the specific sense.
Each shade of gray represents 10% of the overall
data, i.e., three shades of gray mean that between
2http://mallet.cs.umass.edu/
306
to browse to surf
time, library, 
student, music, 
people
shop, street, 
book, store, art
book, read, 
bookstore, find, 
year
deer, plant, 
tree, garden, 
animal
software, microsoft, 
internet, netscape, 
windows
web, internet, 
site, mail , 
computer
store, shop, 
buy, day, 
customer
sport, wind, 
water, ski, offer
wave, surfer, 
board, year, 
sport
channel,  
television, 
show, watch, tv
web, internet, 
site, computer, 
company
film, boy, 
movie, show, 
ride
year, day, time, 
school, friend
beach, wave, 
surfer, long, 
coast
a
b
c
d
e
f
g
h
i
j
k
l
m
n
Figure 1: Temporal development of different senses concerning the verbs to browse (left) and to surf (right)
20% and 30% of the contexts can be attributed to
that sense. For each year one value has been gener-
ated and values between two years are linearly inter-
polated.
Figure 2 shows the development of contexts over
time, with each context plotted individually. The
more recent the context, the darker the color.3 Each
axis represents one sense of to browse, in each sub-
figure different combinations of senses are plotted.
A random jitter has been introduced to avoid over-
laps. Contexts in the middle (not the lower left cor-
ner, but the middle of the graph, e.g., see e vs. f)
belong to both senses with at least 40% probabil-
ity. Senses that share many ambiguous contexts are
usually similar. By mousing over a colored dot, its
context is shown, allowing for an in depth analysis.
3.2 Case studies
In order to be able to judge the effectiveness of our
new approach, we chose key words that are likely
candidates for a change in use in the time from 1987
to 2007. That is, we concentrated on terms relat-
ing to the relatively recent introduction of the inter-
net. The advantage of these terms is that the cause
of change can be located precisely in time.
Figure 1 shows the temporal sense development
of the verbs to browse and to surf, together with
the descriptive terms for each sense. Sense e for to
3The pdf version of this paper contains a bipolar color map.
browse and sense k for to surf pattern quite similarly.
Inspecting their contexts reveals that both senses ap-
pear with the invention of web browsers, peaking
shortly after the introduction of Netscape Navigator
(1994). For to browse, another broader sense (sense
f) concerning browsing in both the internet and dig-
ital media collections shows a continuous increase
over time, dominating in 2007.
The first occurrences assigned to sense f in 1987
are ?browse data bases?, ?word-by-word brows-
ing? in databases and ?browsing files in the cen-
ter?s library?, referring to physical files, namely pho-
tographs. We speculate that the sense of browsing
physical media might haven given rise to the sense
which refers to browsing electronic media, which in
turn becomes the dominating sense with the advent
of the web.
Figure 2 shows pairwise comparisons of word
senses with respect to the contexts they share, i.e.,
contexts that cannot unambiguously be assigned to
one or the other. Each context is represented by
one dot colored according to its time stamp. It can
be seen that senses d (animals that browse) and e
(browsing the web) share no contexts at all. Senses
d (animals that browse) and f (browsing files) share
only few contexts. In turn, senses e and f share a
fair number of contexts, which is to be expected, as
they are closely related. Single contexts, each rep-
resented by a colored dot, can be inspected via a
307
Figure 2: Pairwise comparisons of different senses for the verb ?to browse?. In each subfigure different combinations
of LDA dimensions are mapped on the axes.
LSA dimensions
1 web 0.40, internet 0.38, software 0.36, microsoft 0.28, win-
dows 0.18
2 microsoft 0.24, software 0.23, windows 0.13, internet 0.13,
netscape 0.12
3 microsoft 0.27, store 0.22, shop 0.20, windows 0.19, software
0.16
4 shop 0.32, netscape 0.23, web 0.23, store 0.19, software 0.19
5 book 0.48, netscape 0.26, software 0.17, world 0.13, commu-
nication 0.12
6 internet 0.58, shop 0.25, service 0.16, computer 0.13, people
0.11
7 make 0.39, shop 0.34, site 0.16, windows 0.13, art 0.08
... ...
15 find 0.30, people 0.22, year 0.19, deer 0.16, day 0.15
Table 1: Descriptive terms for the top LSA dimensions for
the contexts of to browse. For each dimension the top 5
positively associated terms were extracted, together with
their value in the corresponding dimension.
mouse roll over. This allows for an in-depth look at
specific data points and a better understanding how
the data points relate to a sense.
3.3 LSA vs. LDA
In comparison, Table 1 shows the LSA dimensions
learned from the contexts of the verb to browse. The
top five associated terms for each dimension have
been extracted as descriptor. The dimensions are
heavily dominated by senses strongly represented
in the corpus (e.g., browsing the web). Infrequent
senses (e.g., animals that browse) only occur in very
low-ranked dimensions and are mixed with other
senses (see the bold term deer in dimension 15).
4 Evaluation
We compared the findings provided by our visual-
ization with word sense information coming from
various resources, namely the 2007 Collins dictio-
nary (COLL), the English WordNet4 (WN) (Fell-
baum, 1998) and the Longman Dictionary (LONG)
from 1987. Senses that evolved later than 1987
should not appear in LONG, but should appear in
later dictionaries.
However, we are well aware that dictionaries are
by no means good gold standards as lexicogra-
phers themselves vary greatly when assigning word
senses. Nevertheless, this comparison can provide a
first indication as to whether the results of our tool
is in line with other methods of identifying senses.
In the case of to browse, COLL and WordNet
suggest the senses ?shopping around; not necessar-
ily buying?, ?feed as in a meadow or pasture? and
?browse a computer directory, surf the internet or the
world wide web.? These senses are also identified in
our visualizations, which even additionally differen-
tiate between the senses of ?browsing the web? and
?browsing a computer directory.? A WordNet sense
that cannot be detected in the data is the meaning ?to
eat lightly and try different dishes.?
Table 2 shows the results of comparing dictionary
word senses (DIC) with the results from our visual-
ization (VIS). What can be seen is that our method
is able to track semantic change diachronically and
4http://wordnetweb.princeton.edu
308
to browse to surf messenger bug bookmark
# of word senses # of word senses # of word senses # of word senses # of word senses
DIC VIS DIC VIS DIC VIS DIC VIS DIC VIS
1987 (LONG) 2 3 1 1 1 2 6 3 1 1
1998 (WN) 5 4 3 3 1 3 5 3 1 2
2007 (COLL) 3 4 3 2 1 3 5 3 2 2
Table 2: A comparison of different word senses as given in dictionaries with the visualization results across time
in the majority of cases, the number of our senses
correspond to the information coming from the dic-
tionaries. In some cases we are even more accurate
in discriminating them. In the case of ?messenger?,
the visualizations suggest another sense related to
?instant messaging? that arises with the advent of
the AOL instant messenger in 1997. This leads us to
the conclusion that our method is appropriate from a
historical linguistic point of view.
5 Discussion and conclusions
When dealing with a complex phenomenon such as
semantic change, one has to be aware of the limita-
tions of an automatic approach in order to be able
to draw the right conclusions from its results. The
first results of the case studies presented in this pa-
per show that LDA is useful for distinguishing dif-
ferent word senses on the basis of word contexts and
performs better than LSA for this task. Further, it
has been demonstrated by exemplary cases that the
emergence of a new word sense can be detected by
our new methodology
One of the main reasons for an interactive visu-
alization approach is the possibility of being able to
detect conspicuous patterns at-a-glance, yet at the
same time being able to delve into the details of the
data by zooming in on the occurrences of particu-
lar words in their contexts. This makes it possible
to compensate for one of the major disadvantages
of generative and vector space models, namely their
functioning as ?black boxes? whose results cannot
be tracked easily.
The biggest problem in dealing with a corpus-
based method of detecting meaning change is the
availability of suitable corpora. First, computing se-
mantic information on the basis of contexts requires
a large amount of data in order to be able to infer re-
liable results. Second, the words in the context from
which the meanings will be distinguished should be
both semantically and orthographically stable over
time so that comparisons between different stages in
the development of the language can be made. Un-
fortunately, both requirements are not always met.
On the one hand words do change their meaning,
after all this is what the present study is all about.
However, we assume that the meanings in a certain
context window are stable enough to infer reliable
results provided it is possible that the forms of the
same words in different periods can be linked. This
of course limits the applicability of the approach to
smaller time ranges due to changes in the phonetic
form of words. Moreover, in particular for older pe-
riods of the language, different variants for the same
word, either due to sound changes or different (or
rather no) spelling conventions, abound. For now,
we circumvent this problem by testing our tool on
corpora where the drawbacks of historical texts are
less severe but at the same time interesting develop-
ments can be detected to prove our approach correct.
For future research, we want to test our methodol-
ogy on a broader range of terms, texts and languages
and develop novel interactive visualizations to aid
investigations in two ways. As a first aim, the user
should be allowed to check the validity and quality
of the visualizations by experimenting with param-
eter settings and inspecting their outcome. Second,
the user is supposed to gain a better understanding of
semantic change by interactively exploring a corpus.
Acknowledgments
This work has partly been funded by the Research
Initiative ?Computational Analysis of Linguistic
Development? at the University of Konstanz and by
the German Research Society (DFG) under the grant
GK-1042, Explorative Analysis and Visualization of
Large Information Spaces, Konstanz. The authors
would like to thank Zdravko Monov for his program-
ming support.
309
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 103?
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally Identifying Changes in the Semantic Orientation
of Words. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), pages 28?34, Valletta, Malta.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Daniel A. Keim, Joern Kohlhammer, Geoffrey Ellis, and
Florian Mansmann, editors. 2010. Mastering The In-
formation Age - Solving Problems with Visual Analyt-
ics. Goslar: Eurographics.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACMComputing Surveys (CSUR), 41(2):1?69.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic Density Analysis: Comparing Word Mean-
ing across Time and Phonetic Space. In Proceedings
of the EACL 2009 Workshop on GEMS: GEometical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
James J. Thomas and Kristin A. Cook. 2005. Illuminat-
ing the Path The Research and Development Agenda
for Visual Analytics. National Visualization and Ana-
lytics Center.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics (ACL ?95), pages 189?196,
Cambridge, Massachusetts.
310
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73?78,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PhonMatrix: Visualizing co-occurrence constraints of sounds
Thomas Mayer
Research Unit
Quantitative Language Comparison
Philipps University of Marburg
thomas.mayer@uni-marburg.de
Christian Rohrdantz
Data Analysis and Visualization Group
University of Konstanz
christian.rohrdantz
@uni-konstanz.de
Abstract
This paper describes the online tool Phon-
Matrix, which analyzes a word list with re-
spect to the co-occurrence of sounds in a
specified context within a word. The co-
occurrence counts from the user-specified
context are statistically analyzed accord-
ing to a number of association measures
that can be selected by the user. The
statistical values then serve as the input
for a matrix visualization where rows and
columns represent the relevant sounds un-
der investigation and the matrix cells indi-
cate whether the respective ordered pair of
sounds occurs more or less frequently than
expected. The usefulness of the tool is
demonstrated with three case studies that
deal with vowel harmony and similar place
avoidance patterns.
1 Introduction
In this paper, we introduce the PhonMatrix1 tool,
which is designed to visualize co-occurrence con-
straints of sounds within words given a reasonably
sized word list of the language. It is a web-based
implementation of the visualization method pro-
posed in (Mayer et al, 2010a), including some
further development such as an interactive com-
ponent and a range of association measures and
sorting methods to choose from. The original mo-
tivation for this tool is to give linguists the oppor-
tunity to upload their own word lists in order to
visually explore co-occurrence constraints in lan-
guages. The basic idea behind the visual compo-
nent of the tool is to provide for a first, at-a-glance
mode of analysis which can be used to generate
hypotheses about the data by simply looking at the
visualization matrices.
1http://paralleltext.info/phonmatrix/
Phonotactic constraints in languages abound.
One of the most well-known and wide-spread con-
straints is commonly referred to as vowel har-
mony (van der Hulst and van de Weijer, 1995). In
vowel harmony languages, vowels are separated
into groups where vowels of the same group tend
to co-occur within words, while vowels from dif-
ferent groups rarely co-occur. Likewise, in some
languages there are patterns of consonant har-
mony (Hansson, 2010) that show a similar behav-
ior with respect to consonants. Less common are
cases of ?synharmonism? (Trubetzkoy, 1967, p.
251) where both vowels and consonants form such
groups and words usually only contain sounds
from the same group (e.g., only front vowels and
palatalized consonants). Whereas vowel harmony
patterns are easily detectable in many harmonic
languages due to the harmonic alternants in af-
fixes, other co-occurrence constraints are less ob-
vious. This is especially true for disharmonic pat-
terns, the most famous of which is the principle
of Similar Place Avoidance (SPA) in Semitic con-
sonantal roots (Greenberg, 1950). Recent studies
have shown that this principle is not only active
in Semitic languages, where it was already known
by grammarians in the Middle Ages, but is a more
widespread tendency among the languages of the
world (Pozdniakov and Segerer, 2007; Mayer et
al., 2010b). One of the reasons why statistical con-
straints like SPA are more difficult to detect is the
fact that they exhibit many frequent counterexam-
ples and are therefore less easily spotted as a gen-
eral (albeit statistical) tendency.
In our view, there are many more phonotactic
constraints that wait to be discovered by linguists.
With the availability of language data in electronic
format such tendencies can be automatically pro-
cessed and presented to the user in a form that
allows for an easy exploration of the results in a
short period of time. Thus a larger number of
phonotactic contexts can be explored in order to
73
find potential patterns in the data. The PhonMa-
trix tool is part of an ongoing effort to integrate
methods and techniques from the field of visual
analytics (Thomas and Cook, 2005) into linguis-
tic research. The present tool will be gradually
augmented with further functionalities in order to
enhance its usefulness.
2 Related work
A related tool that quantifies the co-occurrence of
sounds in a given corpus is the Vowel Harmony
Calculator (Harrison et al, 2004). The major dif-
ference between PhonMatrix and the Vowel Har-
mony Calculator is that the latter is restricted to
the context of vowel harmony and requires the user
to input the harmony classes beforehand whereas
PhonMatrix is designed to detect these classes
by making potential harmonic patterns more eas-
ily perceptible to the user. The Vowel Harmony
Calculator quantifies the notion of vowel har-
mony for the input corpus by giving the percent-
age of harmonic words and the harmony threshold.
The harmony threshold is the percentage of words
that would be expected to be harmonic purely by
chance. The output of the Vowel Harmony Cal-
culator consists of a list of values (number of
polysyllabic words, harmony threshold, percent-
age of harmonic words, harmony index, among
other things) but does not give any information
about the harmonic strength of individual vowel
pairs. In short, the Vowel Harmony Calculator is
a way to quantify the notion of harmony given the
harmony classes of the language whereas Phon-
Matrix is intended to help detect such patterns.
3 System overview
PhonMatrix is a web-based visualization tool that
statistically analyzes sound co-occurrences within
words and displays the result in a symmetric sound
matrix. The statistical components are written
in Python whereas the visualization part is in
Javascript, using the D3 library (Bostock et al,
2011). Before discussing the individual steps of
the system in more detail we will give a brief
overview of the overall processing pipeline (see
Figure 1).
In the first step, the user has to upload the text
file containing the word list that serves as the in-
put to the analysis process. Text files have to be
encoded in UTF-8 and list only one word per line.
For a meaningful analysis the words should be
given in some phonemic transcription (e.g., using
IPA).2
After the file has been uploaded to the server all
symbols in the word list are analyzed according
to their unigram and bigram frequencies. These
frequencies are used to infer an automatic dis-
tinction between vowels, consonants and infre-
quent symbols. Infrequent symbols are consid-
ered to be noise in the data and can be ignored
for further processing. A distinction between vow-
els and consonants is automatically inferred from
the word list by means of Sukhotin?s algorithm
(Sukhotin, 1962). The results of Sukhotin?s algo-
rithm are presented to the user together with the
frequency counts of the individual symbols in the
word list.
In the third step, the user can make changes to
the automatic classification of symbols into vow-
els and consonants and exclude infrequent sym-
bols from further consideration. The subsequent
calculations of co-occurrence values are mostly
based on the distinction of input symbols into con-
sonants (C) and vowels (V). Users can choose
among a number of options that define the con-
text for the co-occurrence calculations.3 Two of
those options will be discussed in more detail in
this paper (vowel co-occurrences in VCV and CC
sequences). Depending on the user?s choice, the
co-occurrences in the selected context are calcu-
lated and analyzed with respect to a number of sta-
tistical association measures from which the user
can choose one for the visualization.
In the last step, the results of the statistical anal-
ysis of the co-occurrence counts are displayed in a
quadratic matrix of sounds. The rows and columns
of the matrix represent the individual sounds that
are relevant for the selected context (e.g., vow-
els in the context of VCV sequences). The rows
thereby stand for the first members of the relevant
sound pairs, whereas the columns contain the sec-
ond members. Each cell of the matrix then shows
the result for the pair of sounds in the respective
row and column.
The final result is a visualization of the co-
occurrence matrix with rows and columns sorted
according to the similarity of the sound vectors
and statistical values represented as colors in the
matrix cells. The visualization features a number
2For more information on the minimum amount of data
necessary see (Mayer et al, 2010a).
3It is also possible for users to define their own contexts
with regular expressions.
74
File%upload% Preprocessing,%VC4dis5nc5on%
User%selects%
context%
Co4occurrence%
sta5s5cs% Visualiza5on%
Figure 1: The processing pipeline of the PhonMatrix visualization tool.
of interactive components that facilitate the detec-
tion of potential patterns in the results by the user.
4 PhonMatrix components
PhonMatrix consists of three main components:
preprocessing (including vowel-consonant dis-
tinction), statistical analysis of co-occurrence
counts and visualization. In what follows, we will
describe each component in more detail, with spe-
cial emphasis on the visualization component.
4.1 Vowel-consonant distinction
Most of the co-occurrence restrictions that might
be of interest make reference to a distinction be-
tween vowels and consonants. Since a manual
classification of all sounds in the input into vowels
and consonants is a tedious task (especially with
a larger number of symbols), the first component
deals with an automatic inference of such a dis-
tinction. Many methods have been discussed in
the literature on how to discriminate vowels from
consonants on the basis of their distribution in
texts. Many of them involve many lines of code
and are computationally demanding. Yet there is a
very simple and fast algorithm that yields reason-
ably good results (Sukhotin, 1962; Guy, 1991).
The basic idea of Sukhotin?s algorithm is that
vowels and consonants have the tendency not to
occur in groups within words but to alternate.
Based on the additional assumption that the most
frequent symbol in the text is a vowel, the algo-
rithm iteratively selects the symbol which occurs
most frequently adjacent to a vowel and deter-
mines it to be a consonant. The algorithm stops
if no more consonants can be selected because no
co-occurrence counts with any remaining vowel
are positive. Although the algorithm is quite old
and very simple, it gives reasonably good results
(Goldsmith and Xanthos, 2009; Guy, 1991; Sas-
soon, 1992). PhonMatrix makes use of Sukhotin?s
algorithm as a preprocessing step to give a first
guess of the class for each symbol, which the user
can then modify if it turns out to be wrong. It
mainly serves to speed up the classification step.
4.2 Co-occurrence statistics
With the distinction of symbols into vowels and
consonants at hand, the user can then select a rel-
evant context for the co-occurrence counts. The
relevant context can be chosen from a list of pre-
defined options. Here we will illustrate the statis-
tical analysis with the context of VCV sequences
to investigate vowel harmony in Turkish. The in-
put consists of 20,968 orthographic words from
the Turkish New Testament.4 The tool automati-
cally extracts all VCV sequences in the words and
counts the co-occurrences of sounds in these se-
quences. The counts are then summarized in a
quadratic contingency table and can be used for
further statistical analyses.
In our experiments, two measures turned out to
be especially useful for the detection of potential
patterns: the probability and ? values. The ? value
is a normalized ?2 measure which allows for an
easier mapping of values to the color scale because
it is always between ?1 and 1.5 The ? values for
the vowels in the Turkish text are shown in Table
1. Apart from probability and ? values, the user
can also choose among a number of other asso-
ciation measures such as pointwise mutual infor-
mation, likelihood ratios or t-scores (Manning and
Schu?tze, 1999).
4.3 Visualization component
The input to the visualization component is a ma-
trix of association measures for each sound pair
in the relevant context. Two additional steps have
to be performed in order to arrive at the final
matrix visualization: 1) the rows and columns
of the matrix have to be sorted in a meaning-
ful way; 2) the association measures have to be
mapped to visual variables. For the matrix ar-
rangement, we decided to have the same order of
symbols for the rows and columns. The order
of symbols is determined by a clustering of the
4Turkish orthography represents the modern pronuncia-
tion with a high degree of accuracy.
5Apart from this, ? makes good use of the off-diagonal
cells in the contingency tables (Church and Gale, 1991).
75
a e i o u o? u? ?
a 0.53699 -0.49730 -0.54579 -0.30421 -0.38117 -0.03895 -0.36874 0.65791
e -0.48371 0.54763 0.64548 -0.28216 -0.37907 -0.05792 -0.32882 -0.53454
i -0.40334 0.37477 0.59682 0.30227 -0.33970 0.09038 -0.30307 -0.49651
o 0.20048 -0.28306 -0.31395 -0.14114 0.65493 -0.05532 -0.20696 -0.33238
u 0.28855 -0.34937 -0.38283 0.17629 0.73451 0.10011 -0.22066 -0.39304
o? -0.28879 0.32352 -0.29843 -0.16465 -0.21329 -0.04885 0.65373 -0.29354
u? -0.31709 0.33094 -0.34774 0.14995 -0.24351 -0.05829 0.75780 -0.35024
? 0.30302 -0.40711 -0.46423 0.32671 -0.33210 -0.07607 -0.28459 0.58548
Table 1: ? values of VCV sequences in Turkish.
symbols based on the similarity of their row val-
ues. The clustering is performed with the Python
scipy.cluster.hierarchy package from
the SciPy library. As a default setting Ward?s al-
gorithm (Ward, 1963) is used but other clustering
algorithms can also be easily integrated.
Whereas the preprocessing steps and the data-
driven sorting of rows and columns have been
written in Python, the actual visualization of the
results in the browser is implemented in Javascript
using the D3 library (Bostock et al, 2011). The
association measures and the order of the sym-
bols are referenced as Javascript variables in the
visualization document. The data is then automat-
ically bound to DOM elements of the HTML doc-
ument through the D3 data operator. The mapping
from association measures to color values is made
with the linear scale method from the d3.scale
package. Scale methods map from an input do-
main to an output range. The input domain for the
? values is the interval [?1; 1], while the output
range can be given as a color scale ranging from
one color to the other. For the ? values we de-
cided to use two unipolar scales, one from ?1 to
0 (red) and the other from 0 to +1 (blue). In order
to reserve a larger color range for the densely pop-
ulated area of low values we did not linearly map
the numerical association measures but used the
square roots of the numerical values as the input
for the scale function. Additionally, the sign of the
? value, which shows whether the co-occurrence
of a certain symbol pair occurs more (+) or less
(?) frequently than expected, is displayed in the
matrix cell.6 The result of the matrix visualization
for the ? values of the vowels in Turkish VCV se-
quences is shown in Section 5.1.
6The algebraic sign is displayed in white and therefore
stands out more clearly with higher absolute ? values.
The matrix visualization also features some in-
teraction to explore the results in more detail. On
mouse-over, the respective matrix cell shows the
actual values that serve as the input for the data
mapping process. Additionally, the row and col-
umn labels are highlighted in order to show more
clearly which pair of symbols is currently selected
(see Figure 2). The size of the matrix can also
be adjusted to the user?s needs with the help of
a slider above the matrix. Next to the slider is a
dropdown menu from which users can choose the
association measure that they want to be displayed
in the visualization.
5 Case studies
After the description of the PhonMatrix system we
will illustrate the usefulness of the visualization of
co-occurrence patterns in sounds with three case
studies. They are presented as a proof of concept
that the visualization component allows for an at-
a-glance exploration of potential patterns. The vi-
sualization part is thereby not considered to be a
replacement of more detailed linguistic investiga-
tions but rather serves as a way to explore a mul-
titude of different contexts and data in a compara-
tively short period of time. After a suspicious pat-
tern has been detected it is indispensable to look
at the actual data to see whether the visualization
result is an artifact of the method or data at hand
or whether the detected pattern is an interesting
phonotactic feature of the language under consid-
eration.
5.1 Turkish vowel harmony
The first case study shows the results of the VCV
sequences in Turkish described above. For this
purpose the vowels a, e, i, o, u, o?, u?, ? are selected
as the relevant sounds that are to be compared in
76
28.05.13 22:02Matrix Visualization
Seite 1 von 1file:///Users/thommy/Dropbox/Code/PhonMatrix/matrix.html
Association measure: Phi matrix File: turkish.txt
Order: Complete linkage
Size:
?+ +?????
?+ +?+???
??+ +????
+?+ + +???
?????+ +?
+???+ + +?
??????+ +
????+?+ +
? ? e i o u a ?
?
?
e
i
o
u
a
?
0.65
Figure 2: The visualization of the ? values of VCV
sequences in the Turkish text.
the visualization. Figure 2 shows the results for
the ? values that have been computed from the
co-occurrence counts of the symbols in VCV se-
quences. The arrangement of the symbols in the
matrix rows and columns already show a distinc-
tion between front (the first four vowels) and back
(the last four vowels) vowels, reflecting the palatal
harmony in Turkish. This distinction can best be
seen when looking at the e- and a-columns where
the top four vowels all have positive ? values for
e and negative ? values for a, whereas the bottom
four vowels show the opposite behavior. On closer
inspection, the labial harmony for high vowels can
also be seen in the matrix visualization. From top
to bottom there are always pairs of vowels that
take the same harmonic vowel, starting with (o?,
u?) taking u? and followed by (e, i) taking i, (o, u)
taking u and finally (a, ?) taking ?. The usefulness
of the visualization component to detect such pat-
terns can best be seen when comparing Figure 2
with Table 1, which contains the same informa-
tion.
5.2 Finnish vowel harmony
The second case study shows that the harmonic
patterns can also be detected in orthographic
words of the Finnish Bible text. Finnish differs
from Turkish in having only one type of harmony
(palatal harmony) and neutral vowels, i.e., vowels
that do not (directly) participate in the harmony
process. As a different underlying association
measure for the visualization consider the proba-
bility values in Figure 3. For probability values
28.05.13 22:03Matrix Visualization
Seite 1 von 1file:///Users/thommy/Dropbox/Code/PhonMatrix/matrix.html
Association measure: Probability matrix File: turkish.txt
Order: Complete linkage
Size:
?+????+ +
+ + +?????
+ + +???+?
???+ + +??
???+ + + +?
???+ + +?+
?????+?+
+????+ +?
o a u ? y ? e i
o
a
u
?
y
?
e
i
Figure 3: The visualization of the probabilities of
VCV sequences in the Finnish text.
we have chosen a bipolar color scale ranging from
white (for 0) over green (for 0.5) to blue (for 1).
The probability matrix clearly shows the relevant
blocks of vowels that mark the harmony groups.7
The clustering algorithm separates the back vow-
els (first three vowels o, a, u) from the front vowels
(vowels four to six, o?, y, a?) and the neutral vowels
(e, i). The blocks along the main diagonal of the
matrix show the harmonic pattern among the har-
mony groups, whereas the neutral vowels do not
display any regular behavior.
5.3 Maltese verbal roots
PhonMatrix is not only useful to find vowel har-
mony patterns. The third case study shows that
other co-occurrence constraints such as SPA can
also be detected. To illustrate this, we show the
visualization of CC patterns in a comprehensive
list of Maltese verbal roots (Spagnol, 2011). The
consonant matrix in Figure 4 shows two clusters,
with one cluster (the first twelve consonants in
the top row) containing labial and dorsal and the
other cluster (the last eleven consonants) compris-
ing only coronal consonants.8 The visualization
also reveals that, unlike in vowel harmony, conso-
nants from the same cluster do not occur next to
each other in the CC sequences, as shown by the
red blocks in the top left and bottom right. This is
exactly what SPA would predict.
7The+/? signs in the matrix are taken from the ? values.
8The consonants are given in their orthographic represen-
tation (Borg and Azzopardi-Alexander, 1997, p. 299).
77
28.05.13 22:00Matrix Visualization
Seite 1 von 1file:///Users/thommy/Dropbox/Code/PhonMatrix/matrix.html
Association measure: Phi matrix File: turkish.txt
Order: Complete linkage
Size:
??????+ + + +??+ + +?+ +?+?+ +
??????+ + +???+ + +????+ + +?
???????+?+ +?+ +?+ +?+ + +?+
????????????+ + + + + + +????
??????+ +??+?+ + + + + +?+ +??
??????+ +????+ + + + + +?+???
+ +??+ +????+??+ + + + + + +?+ +
+ + +?+ +???????+ + + + +?+ + +?
+ +???????????+ + + + +?+ + + +
+?+?????????+ + +????+ + + +
??+?+?+?????+ + +???+ + + + +
????????????+ + +???+ + + + +
+ + + + + +???+ + +???+ + + +?+ + +
+ + + + + + + + + + + +???+??+?+ +?
+ +?+ + + + + + + + +???+ + +?+ + + +
??+ + + + + + +???+ + +????????
+?+ + + + + + +???+?+????????
+??+ + + + + +???+?+????????
??+ +??+???+ + + +?????????
+ + +?+ + + + + + + +??+?????+??
?+ +?+??+ + + + + + + +????+???
+ +????+ + + + + + + + +????????
+?+???+?+ + + + +?+????????
q ? j g g? h m b w f k p n l r ? d ? z x t s ?
q
?
j
g
g?
h
m
b
w
f
k
p
n
l
r
?
d
?
z
x
t
s
?
Figure 4: The visualization of the ? values of con-
sonant sequences in Maltese verbal roots.
6 Conclusions
In this paper, we have presented PhonMatrix, a
web-based, interactive visualization tool for in-
vestigating co-occurrence restrictions of sounds
within words. The case studies of vowel harmony
and SPA have shown that interesting patterns in
the data can easily be seen only by looking at the
matrix visualizations.
Acknowledgments
This work was partially funded by the DFG project
?Algorithmic corpus-based approaches to typolog-
ical comparison.? We are grateful to two anony-
mous reviewers for their valuable comments and
suggestions.
References
Albert Borg and Marie Azzopardi-Alexander. 1997.
Maltese. Descriptive Grammar Series. London:
Routledge.
Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer.
2011. D3: Data-driven documents. IEEE Transac-
tions on Visualization & Computer Graphics (Proc.
InfoVis), 17(12):2301?2309.
Kenneth W. Church and William A. Gale. 1991. Con-
cordances for parallel text. In Proceedings of the
Seventh Annual Conference of the UW Centre for the
New OED and Text Research, pages 40?62.
John Goldsmith and Aris Xanthos. 2009. Learning
phonological categories. Language, 85(1)(1):4?38.
Joseph H. Greenberg. 1950. The patterning of root
morphemes in Semitic. Word, 6:161?182.
Jacques B. M. Guy. 1991. Vowel identification: an old
(but good) algorithm. Cryptologia, 15(3):258?262,
July.
Gunnar O?lafur Hansson. 2010. Consonant Harmony.
Berkeley: University of California Press.
David Harrison, Emily Thomforde, and Michael
O?Keefe. 2004. The vowel harmony cal-
culator. http://www.swarthmore.edu/
SocSci/harmony/public_html/.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Natural Language Processing. Mas-
sachusetts Institute of Technology.
Thomas Mayer, Christian Rohrdantz, Miriam Butt,
Frans Plank, and Daniel A Keim. 2010a. Visual-
izing vowel harmony. Journal of Linguistic Issues
in Language Technology (LiLT), 4(2):1?33.
Thomas Mayer, Christian Rohrdantz, Frans Plank, Pe-
ter Bak, Miriam Butt, and Daniel A. Keim. 2010b.
Consonant co-occurrence in stems across languages:
Automatic analysis and visualization of a phono-
tactic constraint. In Proceedings of the ACL 2010
Workshop on NLP and Linguistics: Finding the
Common Ground, pages 67?75.
Konstantin Pozdniakov and Guillaume Segerer. 2007.
Similar Place Avoidance: A statistical universal.
Linguistic Typology, 11(2)(2):307?348.
George T. Sassoon. 1992. The application of
Sukhotin?s algorithm to certain Non-English lan-
guages. Cryptologia, 16(2)(2):165?173.
Michael Spagnol. 2011. A tale of two morphologies.
Verb structure and argument alternations in Mal-
tese. Ph.D. thesis, Germany: University of Konstanz
dissertation.
Boris V. Sukhotin. 1962. Eksperimental?noe vydelenie
klassov bukv s pomos?c?ju evm. Problemy strukturnoj
lingvistiki, 234:189?206.
James J. Thomas and Kristin A. Cook. 2005. Illu-
minating the Path: The Research and Development
Agenda for Visual Analytics. National Visualization
and Analytics Ctr.
N. S. Trubetzkoy. 1967. Grundzu?ge der Phonologie.
Go?ttingen: Vandenhoeck & Ruprecht. 4. Auflage.
Harry van der Hulst and Jeroen van de Weijer. 1995.
Vowel harmony. In John Goldsmith, editor, The
Handbook of Phonological Theory, chapter 14,
pages 495?534. Basil Blackwell Ltd.
Joe H. Jr. Ward. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
Statistical Association, 58(1)(1):236?244.
78
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 70?78,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Consonant Co-occurrence in Stems Across Languages: Automatic
Analysis and Visualization of a Phonotactic Constraint
Thomas Mayer1, Christian Rohrdantz2, Frans Plank1,
Peter Bak2, Miriam Butt1, Daniel A. Keim2
1Department of Linguistics, 2Department of Computer Science
University of Konstanz, Germany
{thomas.mayer,christian.rohrdantz}@uni-konstanz.de
Abstract
In this paper, we explore the phenomenon
of Similar Place Avoidance (SPA), ac-
cording to which successive consonants
within stems sharing the same place of
articulation are avoided. This principle
has recently been hypothesized as a uni-
versal tendency although evidence from
only a few languages scattered across the
world has been considered. Using meth-
ods taken from the field of Visual Analyt-
ics, which have demonstrably been shown
to help with understanding complex in-
teractions across large data sets, we in-
vestigated a large crosslinguistic lexical
database (comprising data on more than
4,500 languages) and found that a univer-
sal tendency can indeed be maintained.
1 Introduction
Linguistic knowledge has traditionally been ac-
quired by analyzing a manageable set of data, on
the basis of which generalizations are posited that
can then be tested on an extended set of data from
the same language or comparative data from other
languages. Tendencies, rather than absolute prin-
ciples, are difficult to detect under this approach.
This is true especially when they are obscured by
counterexamples that happen to occur with high
frequency, but that may be restricted to just a
small minority of the overall pattern. This may
prompt a researcher to discard a valid generaliza-
tion from the outset. In recent years, a plethora of
statistical and stochastic methods have therefore
been pursued within linguistic research, leading to
approaches such as stochastic Optimality Theory
(Boersma and Hayes, 2001) or the use of statis-
tics to detect crosslinguistic tendencies (Bickel, in
press).
However, although the various statistical meth-
ods deal with data which exhibit very complex and
often ill-understood interactions, analyses have
not to date availed themselves of methodology
currently being developed in the field of Visual
Analytics, which allows us to use our powerful vi-
sual processing ability to understand and evaluate
complex data sets (Keim et al, 2008; Thomas and
Cook, 2005).
In this paper, we present an interdisciplinary
effort whereby linguistically interesting patterns
are automatically extracted, analyzed and visually
presented so that an at-a-glance evaluation of lin-
guistically significant patterns is made possible. In
order to demonstrate that this technique is espe-
cially useful with phenomena that do not mani-
fest themselves in absolute principles, but rather
in statistical tendencies, we investigated a phe-
nomenon that, on the basis of a comparatively
sparse and unrepresentative data set, has recently
been claimed to be a universal tendency (Pozdni-
akov and Segerer, 2007): Similar Place Avoidance
(SPA). In this paper, we conduct a more represen-
tative study of about 4,500 languages. Our results
allow an at-a-glance evaluation which shows that
SPA indeed seems to be a valid language universal
tendency.
Our work on SPA is part of a more widespread
effort currently being conducted with respect to vi-
sually representing crosslinguistic sound patterns.
In Rohrdantz et al (2010), we already showed that
phonological patterns in languages can be auto-
matically extracted and visualized from corpora.
Figure 1 displays the vowel harmony patterns that
were extracted for Turkish in comparison with the
lack of such patterns in a non-harmonic language
like Spanish.
The remainder of this article is organized as fol-
lows. Section 2 introduces SPA. Section 3 pro-
vides an overview of the material that was used. A
description of the calculations and statistical anal-
yses is given in Section 4. Section 5 presents
the results of the geo-spatial visualizations, partly
70
Figure 1: Turkish vowel harmony patterns (left).
The matrix visualizaton was generated on the
basis of the Turkish Bible text and shows the
palatal (front/back) and labial (rounding) harmony
blocks. Rows and columns are automatically
sorted according to the similarity of vowels. For
non-harmonic languages, such as Spanish (right),
no such patterns can be detected.
with respect to a WALS map (Haspelmath et al,
2005). In the final section, we consider some im-
plications of our findings and raise some questions
for future research.
2 Similar Place Avoidance (SPA)
It has long been noted in studies on Semitic lan-
guages, especially Arabic, that there are con-
straints on the structure of triliteral consonant
roots (
?
CCC) with respect to the phonological
features of the individual consonants (Greenberg,
1950). The basic observation is that consonants
with a similar place of articulation are avoided
in non-derived forms. A similar observation has
also been made with respect to the Proto-Indo-
European (PIE) roots. Among other things, Iver-
son and Salmons (1992) note that Stop-V-Stop
roots were very rare in PIE, representing only
3.5% of a lexicon of more than 2,000 items. Plank
(1981:221f) observes that Modern German tends
to avoid verbal stems with identical consonants
in initial and final positions (allowing for differ-
ences in voicing), and that those verbs with iden-
tical initial and final consonants which do exist
are all morphologically regular. This indicates that
they are not basic verbs, but represent a technique
of word formation, perhaps derivative of redupli-
cation as especially common in child or child-
directed language.1
1Note that the early speech of children is characterized by
the opposite effect of SPA: both consonants and vowels tend
to share the same place of articulation (Fikkert and Levelt,
2010), with greater and greater differentiation being achieved
in the course of language acquisition. The reasons for this
remain to be investigated.
Looking at suprasegmental features, Leben
(1973) argued that a similar restriction holds for
the co-occurrence of tones in underlying repre-
sentations. In the framework of Autosegmental
Phonology this has become known as the Oblig-
atory Contour Principle (OCP), which precludes
sequences of identical tones from underlying rep-
resentations. This principle has since been under-
stood more generally as a prohibition on similar
items and has thus also been used in relation with
the SPA bias in Semitic radicals.
More recently, the application of SPA with
respect to stem-internal consonants has been
claimed for other non-Semitic languages as well.
Pozdniakov and Segerer (2007) found impres-
sive support for it in their sample of Atlantic
and Bantu languages of Niger-Congo and fur-
ther tested its crosslinguistic validity for some
more languages or language groups (Mande, Kwa,
Ubangi, Sara-Bongo-Bagirmi, Chadic, Malagasy,
Indo-European, Nostratic, Mongolian, Basque,
Quechua, Kamilaroi, Port Moresby Pidgin En-
glish) with similar results. Table 1 shows their
findings across all 31 languages in their sample.
It can be seen that the highest negative numbers
are in the main diagonal of the matrix, which is
exactly what SPA would predict.
P T C K
P ?15 +11 +5 ?5
T +12 ?10 ?5 +13
C +8 ?5 ?6 +8
K ?3 +8 +5 ?15
Table 1: Results in Pozdniakov and Segerer
(2007). The numbers indicate the overall sum of
cells with negative vs. positive values with regard
to successions of places of articulation (see Sec-
tion 3 for a description of the labels P, T, C and K)
for all languages in their sample. Positive and neg-
ative values have been assigned if the observed ab-
solute value was at least 15% above (respectively
below) the expected value. Compare their results
with the left matrix in Figure 3.
3 Database and methodology
The data that underlies all the subsequent work
presented in this paper have been taken from the
Automated Similarity Judgment Program (ASJP;
Wichmann et al, 2010), which aims at achiev-
71
ing a computerized lexicostatistical analysis of the
world?s languages. To this end, Wichmann and his
collaborators have collected Swadesh list items for
over 4,500 languages. The so-called Swadesh list
was developed by Morris Swadesh in the 1940?
50s with the aim of having a basic set of vocabu-
lary items which are culturally neutral and which
one would expect to be stable over time. The orig-
inal idea of a Swadesh list was to be able to com-
pare and test languages with respect to genealogi-
cal relations.
The Swadesh items in the Wichmann et al
database are transcribed in the ASJP orthogra-
phy, which uses standard ASCII characters to en-
code the sounds of the world?s languages, but does
merge some of the distinctions made by the IPA.
Furthermore, stress, tone and vowel length are not
recorded in the database. However, for the pur-
pose of our investigation the transcription is suit-
able because place of articulation is sufficiently
distinguished.
We decided to experiment with two different ap-
proaches for dividing up the place of articulation
features. One approach (PTCK) is based on the ar-
rangement in Pozdniakov and Segerer (2007) and
distinguishes four places of articulation for labial
(P), dental (and alveolar) (T), (alveo-)palatal (C)
and velar (K) consonants. A second grouping
(LCD) only distinguishes three places of articula-
tion: labial (L), coronal (C) and dorsal (D).2 Ac-
cording to this classification the consonants of all
the items in the database can be assigned to one of
these symbols, as shown in Table 2.
LCD PTCK ASJP IPA
L P
p, b, m, f, v, w p, F, b, B, m,
f, v, w
C
T
8, 4, t, d, s, z,
c, n, S, Z
T, D, n
?
, t, d, s,
z, ts, dz, n, S,
Z
C
C, j, T, l, L, r,
y
?, ?, c, ?, l, L,
?, L, r, R, j
D K
5, k, g, x, N,
q, G, X, 7, h
?, k, g, x, G, N,
q, G, X, K, ?,
Q, P, h, H,
Table 2: Assignment of consonants to symbols.
All varieties of ?click?-sounds have been ignored.
2Radical and laryngeal, which are commonly employed
in the phonological literature as yet another place distinction,
are subsumed under dorsal.
Experiments with using the four-way distinc-
tion vs. the three-way distinction showed that T
and C in the four-way grouping behave very simi-
larly with respect to the transitions to other places
of articulation (see Section 4.2). We therefore de-
cided to use the three-way distinction for the bulk
of our calculations and visualizations and only
sporadically resort to the four-way grouping when
a more fine-grained distinction is needed.
Furthermore, we decided to only include those
cases where the first and second consonants are
preceded (or followed, respectively) by another
vowel or a word boundary and are therefore not
part of a consonant cluster. We mainly did this in
order to minimize the noise caused by consonants
of inflectional markers that tend to assimilate in
such clusters.
In the literature on root morphemes in Semitic,
it has been noted that the consonants within trilit-
eral radicals behave differently with respect to
OCP. Greenberg (1950:162) remarks that while
the first and second consonants are usually not
identical, the same does not hold for the sec-
ond and third consonants, which frequently consti-
tute the well-known geminate subtype of Semitic
verbs. However, for our work we understand OCP
as it was later formulated within the framework
of autosegmental phonology (Leben, 1973; Mc-
Carthy, 1986; Goldsmith, 1976) in that adjacent
identical elements (here in the sense of identical
with respect to place of articulation) are prohib-
ited, under the assumption that consonants are ad-
jacent to each other (on the C tier) even when they
are separated by vowels in the linear sequence of
phonemes within the word.
For the purposes of our experiment, we con-
sidered the relevant context for adjacency to be
one where consonants are separated by exactly one
vowel.3 Note that since the basis for our calcula-
tions were not stems in the language but the cita-
tion forms that are used in the Swadesh lists, we
also get noise from inflectional markers that are
attached to these forms and might have the same
place of articulation irrespective of the stem to
which they attach.4
Finally, there are several shortcomings of the
3Since vowel length is not marked in the ASJP database,
long vowels are also included.
4Assimilation processes are far more frequent than dis-
similation processes in this context so that it is more likely
that the same place of articulation features are to be expected
when an inflectional marker is present.
72
material in the database with respect to our investi-
gation which must be kept in mind. OCP/SPA has
been claimed to apply with respect to underlying
or non-derived representations. Previous work has
been done on the basis of stem (or root) lists. De-
pending on the language, Swadesh list items are
not always stems, but whole words in their cita-
tion forms. For instance, while both English and
German use the infinitive as the citation form for
verbal stems, in English the infinitive is identical
to the stem whereas in German it is marked with
the suffix -en. In other languages, verbs can also
be cited by inflected forms other than the infinitive
(e.g., the 3rd person singular perfective in Arabic,
or the first person singular indicative present in
Latin). The same holds for nouns or other word
classes that are included in the Swadesh list. An-
other problematic aspect is the fact that it also con-
tains items (such as personal pronouns) that are
not lexical in the strict sense of the meaning and
are realized as bound forms in many languages.
Apart from that, the number of items for each
language in the ASJP database varied greatly from
only a few to one hundred. Moreover, the num-
ber of CVC sequences within the items differed
greatly from one language to another, depending
on the phonotactic properties of the languages.
Previous statistical studies have relied on a much
larger number of stems and consonant sequences.
Pozdniakov and Segerer?s (2007) statistics, for ex-
ample, were calculated on the basis of 495 to
17,944 CVC successions for the languages in their
sample.5 In contrast, our statistics are based on
much fewer CVC successions, ranging from 21 to
246 per language. Nevertheless, our results actu-
ally correspond to the main findings of their study
so that we think that the data are good enough for
our purposes.
4 Automated statistical analysis
4.1 Methodology
In a first step, for each language in the sample
an elementary statistical processing is performed.
Thereby, all successions of places of articulation
occurring in the Swadesh list items are identified
and counted. To do so, we define a succession of
5Note that they also included cases where the first and
second consonant are part of a consonant cluster, which we
ignored for our calculations. Furthermore, those languages
where the number of consonant successions in the data was
20 or below were not included in our visualizations, thereby
reducing the number of languages from about 4,500 to 3,200.
places of articulation as a binary sequence of con-
sonants (C-C). These consonants have to appear
within a word and have to be separated by exactly
one vowel (V). Before and after the succession ei-
ther word boundaries (#) or vowels have to ap-
pear. Hence, the following regular expression is
used to extract C-C successions (marked in bold):
[#|V ]CV C[#|V ]. Next, each consonant is as-
signed to one of the three major articulation place
categories labial, coronal and dorsal. The succes-
sion counts are summarized in a quadratic matrix
where the rows represent the preceding place of ar-
ticulation and the columns the following place of
articulation. Each matrix cell contains the number
of times the respective place of articulation suc-
cession could be observed in the corpus. Subse-
quently, for each of the 9 possible successions a
contingency table was created (Table 3).
P2 ?P2
P1 A : n(P1 ? P2) B: n(P1 ? ?P2)
?P1 C : n(?P1 ? P2) D : n(?P1 ? ?P2)
Table 3: Contingency table for the articulation
place (P) succession from P1 to P2.
The succession counts were used to calculate ?
coefficients, where A,B,C and D correspond to
the four cells in Table 3.
? =
?
?2
(A+B + C +D)
(1)
The ? coefficient is a measure for the degree
of association between two variables which can
be derived from the fourfold ?2 statistical signif-
icance test (see Rummel, 1970:298f for details).
Sample ? values for the place of articulation suc-
cessions of Egyptian Arabic can be seen in Table
4. A visual representation of the same matrix is
provided in Figure 2. Note the at-a-glance analy-
sis made possible by Figure 2 vs. Table 4.
labial coronal dorsal
labial ?0.360 +0.187 +0.183
coronal +0.259 ?0.243 ?0.068
dorsal ?0.010 +0.097 ?0.121
Table 4: Matrix of ? values for Egyptian Arabic.
Figure 2 shows an example in which all diag-
onal values (self-successions of places of articu-
lation) have negative associations. This tendency
73
Figure 2: Visualization of the ? matrix from Ta-
ble 4 (Egyptian Arabic), L stands for labial, C for
coronal and D for dorsal. It can be seen that all di-
agonal values (successions of the same place of ar-
ticulation) have negative associations (red color).
to alternate places of articulation can be observed
in most languages and in the overall matrix visu-
alizations including all data from all languages in
the database (Figure 4).
4.2 General relations among places of
articulation
As already mentioned, we tested whether it is use-
ful to distinguish the two different subcategories
dental (and alveolar) (T), and (alveo-)palatal (C).
Figure 3 shows the resulting association values ?
of place successions.
It can clearly be seen that T and C behave very
similarly. A further interesting observation is that
places of articulation tend to alternate (negative di-
agonal values for self-successions). As revealed in
the succession graph of Figure 3, the places of ar-
ticulation do not remain the same, but change to
the closest alternative(s). In the case of P and K
the closest distinct places of articulation (T and C)
are preferred. In the case of T and C, however, this
is somewhat different. Apparently, direct alterna-
tions between both are less probable. One plau-
sible explanation could be that they are not dis-
tinct enough and thus either K or P are preferred
as a following place of articulation, both having
roughly the same distance. These observations
led us to merge the places T and C in our further
analyses and distinguish labial, coronal and dorsal
consonants only, as in Figure 4.
Note that the cross pattern on the left in Figure
4, which now emerges very clearly, reinforces the
hypothesis that the closest distinct place of articu-
lation is preferred as successor.
Figure 4: The ? matrix considering only the three
main categories for all the data across languages.
In the left figure, the categories are sorted accord-
ing to their position in the oral cavity. In the
right figure, the categories are sorted automati-
cally, which shows that D and L are more similar
to each other than D and C.
4.3 Distribution across languages
Next, we examined the distribution of ? values for
self-successions of places of articulation in about
3,200 languages. Self-successions correspond to
the diagonal values of the ? matrices from the up-
per left to the lower right. As can be seen in the
histogram in Figure 6, the peak of the distribution
is clearly located in the area of negative associa-
tion values. In the box-plots of Figure 5, which
show the distributions for all three places of ar-
ticulation separately, it is clearly visible that for
each of the three places of articulation at least 75%
of the languages included show negative associa-
tions. Furthermore, it can be seen that most out-
liers disappear when taking only the languages for
which most data is available and thus statistics are
more reliable. The same can be seen in the scat-
ter plot in Figure 6, where the average ? value is
always negative if the number of successions ex-
ceeds a certain threshold. For all three categories,
the figures demonstrate that the same place of ar-
ticulation is generally less frequently maintained
than expected if there were no interdependencies
between consonant co-occurrences.
5 Visualization of geo-spatial patterns
The most common approach to visually represent
crosslinguistic information on areal (or genealog-
ical) patterns is to put each language as a single
pixel or a small icon to its location on a map.
For instance, the WALS database (Haspelmath et
al., 2005) includes 141 maps on diverse structural
(phonological, grammatical, lexical) properties of
languages. We transformed the results of our SPA
statistics for each language in the ASJP database
74
P
T C
K
Figure 3: Successions of P, T, C and K in all languages. The ?+? and ??? signs indicate the polarity
of a succession (going from row to column category). The color saturation of the background indicates
the strength of association. In the left figure, places of articulation are sorted according to their position
in the oral cavity, in the middle figure an automatic similarity sorting of matrix rows and columns was
applied. The right part of the figure shows an alternative view only on those successions that have a
positive association.
l
l
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
lll
l
l
l
l
l
l
ll
l
l
l
l
l
l
Labial?Labial Coronal?Coronal Dorsal?Dorsal
?
1.0
?
0.5
0.0
0.5
1.0
Distribution of association values across languages (all)
ll
l
l
ll
Labial?Labial Coronal?Coronal Dorsal?Dorsal
?
1.0
?
0.5
0.0
0.5
1.0
Distribution of association values across languages (top)
Figure 5: Boxplots showing the distribution of association strength values (?) for self-successions of
places of articulation. For the left boxplots about 3,200 languages were considered for which the
Swadesh lists contained more than 20 successions. For the right boxplots only the top 99 languages
were considered for which the Swadesh lists contained at least 100 successions, thereby removing most
outliers and reducing the variance.
that is also included in the WALS database into a
WALS map (Figure 7). The matrix visualization
has been simplified in that the color of the icon
represents the number of cells in the diagonal of
the matrix whose value was below zero, i.e., the
higher the number (0-3) the better the language
conforms to SPA.
Some of the drawbacks of these maps include a
high degree of overlap of data points in densely
populated areas and the lack of correlation be-
tween information content and area size. In Figure
7, the fact that those languages with fewer negative
diagonal cells are plotted on top of those with a
higher number slightly distorts the overall picture
that most languages adhere to the principle.6 Be-
sides that, the overall pattern in the densely popu-
lated areas is hardly visible, while sparsely popu-
lated areas waste space and hide the informational
6Likewise, the visualization would suggest to much ad-
herence to the principle if those languages with more nega-
tive diagonal cells were plotted on top of those with fewer
negative cells.
75
ll
lll
l
ll
l
l
l
ll
l
l
l
ll
lll
l
l
l
l
l
l
l
l
l
l
l
l
lll
ll
l
l
l
l
lll
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
lll
ll
l
l
l
l
llll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
ll
ll
lll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
ll
l
ll
l
lll
l
l
lll
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
lll
ll
lll
l
l
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
llll
l
l
lll
l
l
l
l
l
l
l
ll
ll
l
l
ll
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
ll
ll
l
l
l
ll
l
lll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
ll
l
l
ll
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
ll
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
ll
l
l
ll
l
lll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
ll
l
l
lll
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
llll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
ll
l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
50 100 150 200
?
0.4
?
0.2
0.0
0.2
0.4
Average phi values in dependence of data amount
Number of consonant successions in dataset
Ave
rage
 phi 
valu
e
Labial, Coronal and Dorsal
Distribution of association values for all self?successions across languages
Freq
uen
cy
?1.0 ?0.5 0.0 0.5 1.0
0
100
200
300
400
500
600
Figure 6: The scatter plot on the left displays the average ? values for self-successions of all places of
articulation depending on the number of consonant successions (CVC) for each language in the sample.
The histogram on the right shows the distribution of association strength values (?) for self-successions
of places of articulation in more than 3200 languages.
details. Finally, small clusters are difficult to find
? they are not noticeable, and are sometimes even
obscured by large clusters.
In order to avoid overlapping pixels we used
a circular arrangement around the original loca-
tion in the current analysis, taking the given order-
ing of elements into account (Bak et al, 2009a).
The ordering usually corresponds to the coloring
attribute starting with colors that occur least fre-
quently. With this arrangement a natural looking
visualization without artifacts is generated.
A way to obtain more space for regions with a
high point density are Cartograms, which distort
regions such that their size corresponds to a statis-
tical attribute (Bak et al, 2009b; Tobler, 2004), in
this case the number of languages in the database.
The advantage is that more space is reserved to
plot all important information on the map. In Fig-
ure 8, we show the density equalized distortion by
cartograms and the overlap-free representation of
the data points using pixel placement. Neighbor-
hood relations and region shapes are at the same
time maintained as accurately as possible in order
to guarantee recognizability despite of distortion.
The visualization reveals several clusters of non-
conforming languages (marked with boxes). It re-
mains for future work to investigate whether these
clusters are an artifact of the database that we used
or if they manifest an areal feature. Figure 8, in
contrast to Figure 7, shows the 3,200 languages
we investigated more closely and not just the ones
included in WALS.
The representation thereby enables investigat-
ing spatial patterns free of hidden data and distri-
butional biases.
6 Conclusions and future work
Our crosslinguistic investigation of SPA has con-
firmed the hypothesis that the phenomenon of
Similar Place Avoidance is not a particular trait
of Semitic languages, for which it was previously
described, but is a linguistic universal tendency
which can be observed in languages which are
both genealogically and geographically unrelated.
This can clearly be seen in the visualizations that
display the conformity of each language in the
database with respect to SPA. The overall pic-
ture for all languages not only shows that succes-
sive consonants with the same place of articulation
tend to be avoided, but also that there is a tendency
to avoid places of articulation that are too far away
from the preceding place (cf. Figures 3 and 4).
We combine methods from statistics, NLP and
Visual Analytics to provide a novel way of auto-
matically assessing and visualizing linguistic fea-
tures across a wide range of languages, thus al-
76
Figure 7: WALS map of the languages and their behavior with respect to SPA. The color indicates the
number of self-succession ? values which are negative, i.e., which adhere to the SPA principle. Color
mapping is from blue (conforming to SPA) to red. The numbers in square brackets indicate the number
of languages in this group.
Figure 8: Density equalized distribution of the languages with respect to SPA. The area of the geographic
regions corresponds to the number of languages in that location ? represented by dots. Overlap is avoided
using pixel-placement. The color mapping corresponds to the one used in the WALS map (Figure 7). Lo-
cations of nonconforming languages are highlighted with red boxes. Note that the number of languages
in this map is about twice the number in the WALS map (7).
77
lowing for a gain of new insights and raising fur-
ther interesting research questions that otherwise
might easily go unrecognized.
With respect to SPA a more detailed exploration
of the intricacies of phonological interdepencies is
needed as part of our more widespread study of
visually representing sound patterns in languages.
As already hinted at in Pozdniakov and Segerer
(2007), there are various other fascinating phe-
nomena that are worth looking at, especially in re-
gard to the interaction of vowels and consonants or
vowel dependencies (such as vowel harmony) and
consonant dependencies (such as SPA or conso-
nant harmony). In particular, one could investigate
why some languages apparently do not conform to
SPA and if there is any co-variation to be uncov-
ered between the adherence to the principle and
other factors that might be interesting to explore
and possibly reveal new insights into the structure
of languages.
Acknowledgments
This work has been funded by the research ini-
tiative ?Computational Analysis of Linguistic De-
velopment? at the University of Konstanz. The
authors would like to thank Aditi Lahiri and two
anonymous reviewers for valuable comments and
suggestions.
References
Peter Bak, Florian Mansmann, Halldor Janetzko, and
Daniel Keim. 2009a. Spatiotemporal analysis of
sensor logs using growth ring maps. IEEE Trans-
actions on Visualization and Computer Graphics,
15(6):913?920.
Peter Bak, Matthias Schaefer, Andreas Stoffel, Daniel
Keim, and Itzhak Omer. 2009b. Density equalizing
distortion of large geographic point sets. Journal of
Cartographic and Geographic Information Science
(CaGIS), 36(3):237?250.
Balthasar Bickel. in press. Absolute and statistical uni-
versals. In Patrick C. Hogan, editor, The Cambridge
Encyclopedia of the Language Sciences. Cambridge:
Cambridge University Press.
Paul Boersma and Bruce Hayes. 2001. Empirical tests
of the gradual learning algorithm. Linguistic In-
quiry, 32:45?86.
Paula Fikkert and Clara C. Levelt. 2010. How does
place fall into place? The lexicon and emergent con-
straints in the developing phonological grammar. In
Peter Avery, B. Elan Dresher, and Keren Rice, edi-
tors, Contrast in Phonology: Perception and Acqui-
sition. Berlin: Mouton de Gruyter.
John Goldsmith. 1976. Autosegmental phonology.
Ph.D. thesis, Massachusetts Institute of Technology.
Joseph H. Greenberg. 1950. The patterning of root
morphemes in Semitic. Word, 6:161?182.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie. 2005. The World Atlas of Lan-
guage Structures Online. URL: http://wals.
info/.
Gregory K. Iverson and Joseph C. Salmonts. 1992.
The phonology of the Proto-Indo-European root
structure constraint. Lingua, 87:293?320.
Daniel A. Keim, Florian Mansmann, Joern Schnei-
dewind, Jim Thomas, and Hartmut Ziegler. 2008.
Visual analytics: Scope and challenges. In Visual
Data Mining: Theory, Techniques and Tools for Vi-
sual Analytics, Lecture Notes in Computer Science,
pages 76?91. Springer.
Wiliam R. Leben. 1973. Suprasegmental phonology.
Ph.D. thesis, Massachusetts Institute of Technology.
John J. McCarthy. 1986. OCP effects: Gemination and
antigemination. Linguistic Inquiry, 17:207?263.
Frans Plank. 1981. Morphologische (Ir-)Regularita?-
ten: Aspekte der Wortstrukturtheorie. Tu?bingen:
Gunter Narr Verlag.
Konstantin Pozdniakov and Guillaume Segerer. 2007.
Similar Place Avoidance: A statistical universal.
Linguistic Typology, 11(2):307?348.
Christian Rohrdantz, Thomas Mayer, Miriam Butt,
Frans Plank, and Daniel A. Keim. 2010. Compar-
ative visual analysis of cross-linguistic features. In
Proceedings of the International Symposium on Vi-
sual Analytics Science and Technology (EuroVAST
2010), pages 27?32.
Rudolph J. Rummel. 1970. Applied Factor Analysis.
Evanston, IL: Nortwestern University Press.
James J. Thomas and Kristin A. Cook. 2005. Illu-
minating the Path: The Research and Development
Agenda for Visual Analytics. National Visualization
and Analytics Ctr.
Waldo Tobler. 2004. Thirty five years of computer
cartograms. Association of American Geographer,
94(1):58?73.
78
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 63?71,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Toward a Totally Unsupervised, Language-Independent Method for the
Syllabification of Written Texts
Thomas Mayer
Department of Linguistics
University of Konstanz, Germany
thomas.mayer@uni-konstanz.de
Abstract
Unsupervised algorithms for the induction
of linguistic knowledge should at best re-
quire as few basic assumptions as pos-
sible and at the same time in principle
yield good results for any language. How-
ever, most of the time such algorithms are
only tested on a few (closely related) lan-
guages. In this paper, an approach is pre-
sented that takes into account typological
knowledge in order to induce syllabic di-
visions in a fully automatic manner based
on reasonably-sized written texts. Our ap-
proach is able to account for syllable struc-
tures of languages where other approaches
would fail, thereby raising the question
whether computational methods can really
be claimed to be language-universal when
they are not tested on the variety of struc-
tures that are found in the languages of the
world.
1 Introduction
Many approaches developed in the field of com-
putational linguistics are only tested and optimized
for one language (mostly English) or a small set of
closely related languages, but at the same time are
often claimed to be applicable to any natural lan-
guage, cf. Bender (2009). Our aim is to stress the
importance of having a more varied sample of lan-
guages that include the different types that can be
found in the languages of the world in order to do
justice to the range of variation in linguistic struc-
tures across languages. Furthermore, we want to
point to the usefulness of using typological knowl-
edge for a language-universal approach.
In this paper, we present an unsupervised,
language-independent syllabification method
based on raw unannotated texts in a phonemic
transcription. The methods and procedures
presented in this work rest upon insights from
typological work and do not need any additional
language-dependent information. The main pur-
pose of this paper is not to present an improvement
on already established statistical approaches to
the problem of syllabification of an individual
language, but to introduce data from languages
that might constitute a problem for many syl-
labification methods that have been optimized
on languages like English and therefore make it
necessary to integrate an additional component
that is able to handle such cases.
The remainder of the paper is organized as fol-
lows. First, it is argued in Section 2 that ortho-
graphic texts (in any alphabetic script) can be used
for the induction of phonological patterns if the
spelling system is reasonably close to a phonemic
transcription. The syllabification process can be
divided into two steps. In Section 3, we present
and evaluate an algorithm for an unsupervised
classification of all symbols in the input texts into
vowels and consonants. Based on this classifica-
tion, a syllabification procedure is discussed that
makes use of distributional information of clus-
ters in order to break up vowel and consonant se-
quences into syllables (Section 4). Finally, we
conclude with a discussion of the advantages and
disadvantages of the present approach and its im-
plications for future research.
2 Learning phonological patterns on the
basis of written texts?
Most studies that are based on original texts are
concerned with research questions that do not
make use of phonological knowledge that has
been extracted from the texts. The reason for
this is obvious. The orthographies of many well-
studied modern languages contain many idiosyn-
cratic rules and exceptions that would make it dif-
ficult to use them for dealing with phonological
aspects of the languages under consideration. On
63
the other hand, in order to be able to use distribu-
tional information for phonological problems there
are not enough reasonably-sized phonetically tran-
scribed corpora, especially for a wider range of
languages.
However, many spelling systems do not suffer
from these shortcomings and thus can be used for
these purposes. When looking at languages whose
orthographies have been conceived or standard-
ized only recently it can be noted thatmany of them
are pretty close to a phonemic transcription. Pro-
vided the size of the corpus is big enough, smaller
inconsistencies in the spelling system can be con-
sidered to be noise in the data.
Phonemic orthographies as they are usually de-
vised for a new spelling system also show an ad-
vantage that phonetic transcriptions lack, namely
that they already group together those symbols
that represent the same phoneme in the language.1
Moreover, obligatory phonological processes such
as final devoicing are mostly not represented in the
written form (Turkish being a notable exception),
thereby providing a sort of underlying representa-
tion that is useful to induce which sequences can
be grouped together to morphemes.
For these reasons written texts can in our view
also be used for the induction of phonological
knowledge for languages with phonemic spelling
systems, even though their results have to be ana-
lyzed with great care.
3 Sukhotin?s algorithm
Sukhotin?s algorithm (Sukhotin, 1962, 1973) is a
totally unsupervised method to discriminate vow-
els from consonants on the basis of a phonemic
transcription. The approach relies on two fun-
damental assumptions that are grounded on typo-
logical insights. First, vowels and consonants in
words tend to alternate rather than being grouped
together. Second, the most frequent symbol in the
corpus is a vowel. The latter assumption is used
to initialize the classification step by claiming that
the most frequent symbol is the first member of
the vowel class, with the rest of the symbols ini-
tially all classified as consonants. With the help of
the first assumption the other vowels are then clas-
sified by iteratively checking which symbol is less
1In the remainder of this paper we will use the term ?sym-
bol? as a more neutral expression for all letters in the written
texts in order not to be explicit whether the spelling system
really lives up to the goal of representing phonemes by let-
ters.
frequently adjacent to the already detected vowels.
3.1 Typological basis
It has been noticed in the typological literature at
least since Jakobson and Halle (1956) that there is
a tendency in the languages of the world for hav-
ing CV as the basic syllable structure. Of course,
languages differ as to the number and types of syl-
lables; there are languages that allow a huge vari-
ety of consonant (or vowel) clusters whereas others
are stricter in their phonotactic possibilities. How-
ever, all languages seem to obey the universal law
that CV is more basic than other syllable types and
that ?CV is the only universal model of the sylla-
ble.? Evidence for this comes from different ar-
eas of linguistics, including the observation that
no matter how small the number of syllable types
in a language is, it always includes CV. This is
also reflected in the Onset Maximization Princi-
ple (OMP), which states that an intervocalic con-
sonant is attributed to the following syllable and is
assumed to be a language-universal principle for
syllabification.
We are not aware of any cross-linguistic study
that investigated the token frequency of phonemes
in larger samples of texts. Hence, the second as-
sumption that the most frequent symbol in a text
is always a vowel cannot be backed up by typo-
logical knowledge. However, this claim can be
supported indirectly. In his study on consonant-
vowel ratios in 563 languages, Maddieson (2008)
states that the ratio ranges between 1.11 and 29.
The lowest value has been calculated for the isolate
language Andoke, which has 10 consonants and
9 vowels. The mean value is 4.25, though. Pro-
vided that it is always the case that languages have
more consonants than vowel types, it can be argued
that the fewer vowels have higher token frequen-
cies in order to be able to contribute their share to
the make-up of syllables.2 Yet this generalization
is untested and could be wrong for some languages
(or rather corpora of those languages). In our sam-
ple of texts in different languages, nevertheless the
most frequent symbol is always a vowel.
3.2 Description of the algorithm
Sukhotin?s algorithm is computationally simple
and can even be illustrated with a small toy cor-
2In the French corpus that Goldsmith and Xanthos (2009)
used in their studies, the most frequent phoneme turned out to
be a consonant. However, the rest of the classification was not
affected and all remaining phonemes were labelled correctly.
64
pus.3 Given a corpus with the inventory of n sym-
bols S := {s
1
, . . . , s
n
} we construct an n?nma-
trix M where the rows represent the first and the
columns the second symbol in a bigram sequence
and which indicates the number of times the se-
quences occur in the corpus.
M =
?
?
m
11
. . . m
1n
. . . . . . . . .
m
n1
. . . m
nn
?
?
The main diagonal, i.e., the self-succession of
symbols, is ignored by setting all its values to
zero. For instance, given a sample corpus C =
{saat, salat, tal, last, stall, lese, seele} we ob-
tain the following 5 ? 5 matrix (for ease of un-
derstanding the symbols have been put in front of
the cells of the matrix and the row sums in the last
column):
M =
?
?
?
?
?
?
?
?
s a t l e Sum
s 0 3 2 0 3 8
a 3 0 3 4 0 10
t 2 3 0 0 2 7
l 0 4 0 0 3 7
e 3 0 2 3 0 8
?
?
?
?
?
?
?
?
Sukhotin?s algorithm initially considers all sym-
bols to be consonants before it enters an interative
phase. In each cycle of the phase, the symbol with
the highest row sum greater than zero is detected
and classified as a vowel. The row sum for any
symbol s
a
is calculated by adding up all occur-
rences of the symbol s
a
as a first or second mem-
ber in a sequence
?
n
i=1
m
ai
. After a new vowel
has been detected, its row sum is set to zero and
all other row sums are updated by subtracting from
the sum of the row of each remaining symbol twice
the number of times it occurs next to the new-found
vowel. This process is repeated until nomore sym-
bols with positive row sums are left. In our exam-
ple, the vectors of row sums (RSum) for all sym-
bols in the individual steps of the iteration phase
look as follows:
RSum
1
=
(
s a t l e
8 10 7 7 8
)
RSum
2
=
(
s a t l e
2 0 1 ?1 8
)
3More detailed descriptions can be found in Guy (1991)
and Goldsmith and Xanthos (2009).
RSum
3
=
(
s a t l e
?4 0 ?3 ?7 0
)
The rationale behind this algorithm with respect
to its basic assumptions is as follows. The fact that
initially the symbol with the highest sum is consid-
ered to be a vowel reflects the idea that the most
frequent symbol in the corpus has to be a vowel.
What the row sums after each step actually con-
tain is the difference between the number of times
a symbol is found next to a consonant and the num-
ber of times it is found next to a vowel. When-
ever a new vowel has been detected all occurrences
of this vowel have to be subtracted from the other
symbols because this symbol is no longer consid-
ered to be a consonant.
3.3 Evaluation
To the best of our knowledge, the algorithm has
never been tested on a larger cross-linguistic sam-
ple. There are results for a number of languages
in Sukhotin?s original papers, in Sassoon (1992)
and in Goldsmith and Xanthos (2009), yet alost
all languages in those samples belong to the Indo-
European family (except for Georgian, Hungar-
ian and Finnish) or do not fulfill the criterion of
a phonemic transcription (Hebrew). It therefore
still needs to be tested on a more cross-linguistic
sample of languages. In particular, it is an inter-
esting question to see if the algorithm works even
for those languages that are notorious for having
many consonant clusters. On the basis of his sam-
ple of five languages, Sassoon (1992) comes to
the conclusion that it works very well on those
languages that have only few consonant clusters
but has problems when more complex clusters are
involved. However, he also notices that this ef-
fect disappears with larger text samples. Table 1
provides an evaluation of Sukhotin?s algorithm on
the basis of Bible texts (NT) in our sample of 39
languages. The size of the corpora in Sassoon?s
sample range from 1641 to 3781 characters while
the Bible texts contain more than 100,000 char-
acters (e.g., English has 716,301 characters). On
average, Sukhotin?s algorithm classifies 95.66%
of the symbols correctly. However, this percent-
age also includes those languages which do not
fulfill the criterion of having a suitable phonemic
writing system (e.g., Russian, English, German,
French). When looking only at those languages
whose spelling systems are close to a phonemic
transcription (or where the digraphs have been sub-
65
stituted by single symbols), the results are even
better.
Misclassified symbols are either very infrequent
and happen to occur next to symbols of the same
class or are part of one of the digraphs used in the
spelling system of the language. In the Maltese
case, the symbol ? is classified as a consonant be-
cause it only occurs twice in the corpus in the word
elo? where it stands next to a symbol that is clearly
a vowel. For some languages, minor modifications
to the original texts have been made in order to re-
place the most frequent digraphs. In Swahili, for
instance, with the official orthography the symbol
c is classified as a vowel because it only occurs
in the digraph ch. After the digraph has been re-
placed by a single symbol, the classification is cor-
rect in all cases. Sometimes a symbol (e.g., h in
Warlpiri) is misclassified because it does not occur
in the writing system of the language but is part of
a digraph in foreign words (mostly proper names
of people or locations in the Bible texts). Another
problem of the approach is with orthographies that
use the same symbol for both vowels and conso-
nants. Since the classification is global, symbols
like English y, which is a consonant in yoghurt
and a vowel in lady, are always treated as either a
vowel or a consonant for the whole language inde-
pendent of the context where they occur. There-
fore symbols in the input text should always be
able to be classified to one or the other category.
As the discussion of misclassified symbols
shows, the main errors in the results are not due to
the algorithm itself, but a problem of the spelling
systems of the texts at hand. Our results confirm
the findings of Sassoon (1992) that the algorithm
is sensitive to the corpus size and the frequency of
occurrence of individual symbols. Larger corpora,
such as Bible texts, yield much better results for
these languages. Even those languages with many
and very complex consonant clusters (e.g., Geor-
gian, Croatian and Czech) get an almost perfect
classification. It is remarkable that the overall dis-
tribution of the symbols makes up for those cases
where consonants frequently occur in clusters. Ex-
periments with smaller corpus sizes also revealed
that one of the first symbols that get wrongly clas-
sified is the sibilant s. This might be another indi-
cator for the exceptional status of sibilants with re-
spect to syllabification and their occurrence in con-
sonant sequences where they can violate the sonor-
ity principle (e.g., in the sequence str in words like
string the consonant s is to the left of the consonant
t although higher in sonority).
4 Unsupervised syllabification
Based the classification of input symbols into vow-
els and consonants, the syllabification procedure
can then be applied. Knowledge of syllable struc-
ture is not only relevant for a better understand-
ing of the procedures and representations that are
involved in both computer and human language
learning but also interesting from an engineering
standpoint, e.g., for the correct pronunciation of
unknown words in text-to-speech systems or as an
intermediate step for morphology induction.
Several methods have been proposed in the lit-
erature for an unsupervised language-independent
syllabification (see Vogel, 1977 for an overview
and Goldsmith and Larson, 1990 for an imple-
mentation of a more recent approach based on
the sonority hierarchy). Some methods that have
been suggested in the literature (going back to
Herodotus, who observed this for Ancient Greek;
cf. Kury?owicz, 1948) rely on the observation
that word-medial tautosyllabic consonant clusters
mostly constitute a subset of word-peripheral clus-
ters. Intervocalic consonant clusters can therefore
be divided up into a word-final and word-initial
cluster. Theoretically, two types of problems can
be encountered. First, those where more than one
division is possible and second, those in which no
division is possible.
Several approaches have been suggested to re-
solve the first problem, i.e., word-medial conso-
nant sequences where there are several possible di-
visions based on the occurrence of word-initial and
word-final clusters. O?Connor and Trim (1953)
and Arnold (1956) suggest that in cases of ambigu-
ous word-medial clusters the preference for one
syllable division over another can be determined
by the frequency of occurrence of different types of
word-initial and word-final clusters. For this pur-
pose, they determine the frequency of occurrence
of word-initial and word-final CV, VC, etc. sylla-
ble patterns. Based on these frequencies they cal-
culate the probabilities of dividing a word-medial
sequence by summing up the values established for
the different word-peripheral syllable types. The
candidate syllabification with the highest sum is
then chosen as the optimal division.
The approach taken here is a slight modification
of the proposal in O?Connor and Trim (1953) and
66
Language Vowels Consonants
Afrikaans a c* e i o u y ? ? ? ? ? ? ? ? b d f g h j k l m n p q r s t v w x ?* ?* ?* ?*
Albanian a e g* h* i o u y ?* ? ? b c d f j k l m n p q r s t v x z
Armenian (transl.) a e e? y? i o ch* o? b g d z t? jh l x c? k h d? gh tw m y n sh p j r?
s v t r c w p? q f
Basque a e i o u v* ? ? ? ? ? ? b c d f g h k l m n p q r s t x y z ?* ?* ? ?* ?*
Breton a c* e i o u ? b d f g h j k l m n p r s t v w y z ? ?* ?*
Chamorro a e i o u ? ? ? ? ? ? b c d f g h j l m n p q r s t v x y ?* ? ?*
Croatian a e i o u b c d f g h j k l m n p r s t v z ?* ? ?* ? ? ? ? ?
Czech a e i o u y ? ? ? ? ? ? ? ? b c d f g h j k l m n p q r s t v x z ? ? ? ? ? ? ?
Danish a e i o u y ? ? ? b c d f g h j k l m n p r s t v x z
Dutch a c* e i o u y b d f g h j k l m n p q r s t v w x z
English a e g* i o t* u b c d f h j k l m n p q r s v w x y z
Finnish a e i o u y ? ? b c d f g h j k l m n p q r s t v x z
French a e i o u ? ? ? ? ? ? ? ? b c d f g h j k l m n p q r s t v x y z ? ?* ?* ?*
?* ?*
Georgian (transl.) a e i o u h* b g d v z t k? l m n p? zh r s t? p k gh q sh ch ts
dz ts? ch? kh j
German a e h* i o p* u y ? ? ? b c d f g j k l m n q r s t v w x z ?
Gothic a e i o u v* x* ? b d f g h j k l m n p q r s t w z ?* ?
Greek ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Hungarian a c* e i o u y ? ? ? ? ? ? ? ? ? b d f g h j k l m n p r s t v x z
Icelandic a e i o u y ? ? ? ? ? ? ? ? b d f g h j k l m n p r s t v x ? ?
Italian a e h* i o u ? ? ? ? ? b c d f g j k l m n p q r s t v z ?*
Latin a e i o u y b c d f g h l m n p q r s t v x z
Maltese (rev.) a e g* i o u ? ? ? ? ? ? ? b d f h j k l m n p q r s t v w x z ?* ? ? ? ?
Mandarin (toneless) a e i o u ng zh ch b c d f g h j k l m n p q r s t w x y z sh
Maori (rev.) a e i o u g h k m n p r t ng w wh
Norwegian (Bokm?l) a e i o u y ? ? ? ? ? b c d f g h j k l m n p r s t v z ?*
Potawatomi (rev.) a e i o u c d g k l m n p s t w sh y
Romanian a e i o u ? ? b c d f g h j l m n p r s t v x z ? ?
Russian ? ? ? ? ? ? (?*) ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (?*)
(?*)
Scots Gaelic a h* i o u b c d e* f g l m n p r s t
Spanish a e i o u ? ? ? ? ? ? ? ? ? ? b c d f g h j l m n p q r s t v x y z ? ?*
Swahili (rev.) a e i o u b d f g h j k l m n p r s t v w x y z
Swedish a e i o u y ? ? ? ? b c d f g h j k l m n p r s t v x
Tagalog (rev.) a e i o u ng b c d f g h j k l m n p q r s t v w x y z
Turkish a e i o u ? ? ? ? ? ? ? b c d f g h j k l m n p r s t v y z ? ? ?
Ukrainian i ? ? ? ? ? ? ? ? ? ? ? c y ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Uma a e g* i o u b c d f h j k l m n p r s t w y z ?*
Warlpiri (rev.) a e h* i o u c f j k l m n p q r s t v w x y z
Wolof a e i o u ? ? ? ? b c d f g j k l m n p q r s t w x y ? ?
Xhosa a e g* i o t* u ? b c d f h j k l m n p q r s v w x y z
Table 1: Results for Sukhotin?s algorithm on Bible texts in 39 languages. All symbols of the input
Bible texts for the respective languages are listed even if they are very infrequent. For those languages
marked as revised the most frequent digraphs have been replaced by a single symbol. Wrongly classified
symbols are marked with an asterisk. Languages with spelling systems which notoriously contain many
idiosyncratic rules are shaded. We decided to include them as a reference where the problems occur with
these systems.
67
Arnold (1956). Instead of counting the frequency
of occurrence of syllable types, the actual sylla-
bles are counted in order to determine the best split
of word-medial consonant sequences. An example
calculation for the German word fasten ?to abstain
from food? is given in Table 2.
a) fa st
142]
en [fast.en] sum: 142
b) fa s
528] [216
t en [fas.ten] sum: 744
c) fa
[176
st en [fa.sten] sum: 176
Table 2: Example calculations for the word-medial
cluster in the German word fasten.
The example calculations in Table 2 show that
the candidate syllabification in b) yields the high-
est sum and is therefore chosen as the correct syl-
labification of the word. One of the advantages
of this approach (as well as the one proposed by
O?Connor and Trim and Arnold) is that OMP fol-
lows from the fact that word-initial CV sequences
are more frequent than word-final VC sequences
and does not have to be stipulated independently.
The claim that CV is the unmarked syllable
structure for all languages of the world (andOMP a
universal principle) has been challenged by some
Australian languages that seem to behave differ-
ently with respect to syllabification of VCV se-
quences (Breen and Pensalfini, 1999). In those
languages VCV sequences are syllabified as VC.V
instead of V.CV, as OMP would predict. The
authors provide evidence from a number of pro-
cesses in these languages (reduplication, language
games) as well as historical and comparative evi-
dence that support the analysis that VC seems to be
more accurate as the basic syllable type for those
languages.4
For cases where word-medial clusters cannot be
broken up by sequences that are found at word
edges (bad clusters), we decided to go back to the
original method used by O?Connor and Trim and
Arnold and calculate the frequency of occurrence
of syllable types. However, bad clusters are not
very frequent compared to the overall data in our
experiments.
One additional problem when working with
written texts5 rather than transcribed corpora is the
4Note that this does not invalidate one of the basic assump-
tions of Sukhotin?s algorithm, since C and V still alternate
even though in the reverse order.
5Some linguists also believe that stress can lead to a vio-
lation of OMP by attracting an intervocalic consonant to the
coda of the previous stressed syllable. Since stress is usually
Dutch aa (772), oo (510), ie (440), ui (301),
ou (155), eu (110), uu (27)
German ei (1373), au (641), eu (216)
English ea (336), ou (280), io (231), oo (79)
French ai (863), ou (686), eu (397), io
(339), ui (272), au (232), oi (232)
Greek ?? (1687), ?? (1684), ?? (650), ??
(616), ?? (287)
Wolof aa (1027), ee (821), oo (656), ?e
(181), ii (158), ?o (118)
Table 3: ?Diphthongs? for a subset of the lan-
guages in the sample (in brackets the frequency of
adjacent occurrence).
fact that diphthongs are not clearly distinguished
from sequences of monophthongs. Yet this is vi-
tal for a correct syllabification procedure since the
number of syllables of the word is different de-
pending on this choice. In order to retrieve the
diphthongs of the language from the distribution
of vowel sequences in the corpus the following ap-
proach has been used.6 For each bigram vowel se-
quence the number of times the first vowel v
1
is
directly followed by the second vowel v
2
is com-
pared with the number of times both vowels are
separated by one consonant. If the frequency of
direct adjacency is higher than the frequency of
v
1
cv
2
the sequence is considered to be a ?diph-
thong?; if not, the sequence is considered to be a
case of hiatus and both vowels are attributed to dif-
ferent syllables. Similar to Sukhotin?s algorithm
the present syllabification algorithm is also global
in the sense that the diphthong/monophthong dis-
tinction is always used in the same way no matter
in which environment the sequence occurs.7 Ta-
ble 3 gives a list of the diphthongs extracted from
the corpus for a number of languages in our sample
based on this method.
4.1 The problem of evaluating syllabification
methods
There are several reasons why a gold standard
for syllabification, with which the syllabification
methods are compared, is difficult to establish.
not reflected in most orthographies, we do not consider this
option here.
6We thank Bernhard W?lchli (p.c.) for drawing our atten-
tion to this idea.
7In German, for instance, the vowel sequence eu can either
be tautosyllabic and in that case constitute a diphthong as in
heute ?today?; or it can be a case of hiatus and therefore be
broken up by a syllable boundary as in Museum ?museum?.
68
Duanmu (2009) states that even for well-described
languages like English linguists do not agree on the
correct syllabification of comparatively straight-
forward cases. For the English word happy, for in-
stance, four different analyses have been proposed:
[h?.pi] Hayes (1995), Halle (1998), Guss-
mann (2002)
[h?p.i] Selkirk (1982), Hammond (1999)
[h?pi] Kahn (1976), Giegerich (1992),
Kreidler (2004)
[h?p.pi] Burzio (1994)
Table 4: Analyses of happy (cited from Duanmu,
2009). Underlined consonants are ambisyllabic.
The correct syllabification of a word can best
be established when there is some operation in the
language that takes recourse on the syllable struc-
ture of the word. In the case of the Australian
languages with no syllable onsets, Breen and Pen-
salfini (1999:6f) provide evidence from reduplica-
tion processes in Arrernte to support their analysis.
If the Arrernte syllable shape is VC(C), rather than
(C)CV, reduplication is most straightforwardly de-
scribed in terms of syllables. The attenuative pre-
fix is formed by /-elp/ preceded by the first syl-
lable of the base if VC(C) syllabification is as-
sumed. The attenuative form of the base empwa?
?to make? is therefore empwelpempwa?.8 A simi-
lar argumentation can be put forward for languages
that show phonological operations that are based
on the structure of syllables, e.g., syllable-final de-
voicing. If a voiced obstruent is realized unvoiced,
the syllabification might suggest its position to be
in the coda.
Besides disagreement on the correct syllabifi-
cation of words, another crucial aspect of eval-
uating syllabification methods is the question of
whether the test set should consist of a random
sample of words of the language or whether there
should be any constraints on the composition of
the evaluation data. If the evaluation consists of
a huge number of monosyllabic words, the results
are much better than with polysyllabic words be-
cause no consonant clusters have to be broken up.
8As one reviewer remarked, reduplication patterns are
usually described in terms of a CV-template rather than sylla-
ble structures. However, in the case of Arrernte, a description
in terms of syllables rather than VC(C) shapes would be more
elegant and at the same time account for other operations as
well.
For the evaluation of their syllabification methods,
Goldwater and Johnson (2005) distinguish words
with any number of syllables from words with at
least two syllables. Depending on the method that
they test the differences in the percentage of cor-
rectly syllabified words range from a few to almost
30%. It is therefore easier to get better results when
applying the syllabification methods to languages
with a large number of monosyllabic words and
fewer consonant clusters, like Mandarin Chinese,
for instance.
4.2 Discussion and evaluation
One of the problems of a cross-linguistic inves-
tigation is the availability of gold standards for
evaluation. Thus, instead of providing a compara-
tive evaluation, we want to discuss the advantages
and disadvantages of the procedure with respect
to the more common sonority-based syllabifica-
tion method. We tested our method on a manually
created gold standard of 1,000 randomly selected
words in Latin. The precision is 92.50% and the re-
call 94.96% (F-Score 0.94) for each transition from
one symbol to another. Most misplaced syllable
boundaries are due to the vowel cluster io, which
has been treated as a diphthong by our method.
The most interesting aspect of our approach is
that it is able to account for those languages where
intervocalic consonants are better be analyzed as
belonging to the previous syllable, thereby violat-
ing OMP. Approaches relying on the Onset Max-
imization Principle would get al of these syllable
boundaries wrong. Breen and Pensalfini (1999)
note that Arrernte also has only VC in word-initial
position. Consequently, an approach that is based
on word-peripheral clusters can predict the lack of
word-medial onsets correctly. The importance of
word-peripheral clusters is also supported by find-
ings in Goldwater and Johnson (2005) where a bi-
gram model improves after training with Expec-
tation Maximization whereas a positional model
does not, which might be due to the fact that a bi-
gram model (unlike the positional model) can gen-
eralize whatever it learns about clusters no matter
if they occur at word edges or word-medially.
Moreover, the influence of word-peripheral
clusters on the syllabification of word-medial con-
sonant sequences is not restricted to syllable types
only, but sometimes also holds solely for individ-
ual consonants. In Chamorro, for instance, Top-
ping (1973) describes the syllabification of inter-
vocalic consonants as observing OMP. However,
69
this does not apply if the consonant is the glottal
stop /?/, in which case the syllable division occurs
after the consonant, leading to the syllabification
/na?.i/ ?to give?. The interesting observation in this
respect is that the glottal stop phonologically never
occurs at the beginning of a word in Chamorro
whereas all other consonants (with the exception
of /w/) do occur word-initially,9 which leads to the
correct syllabification results with our approach.
Another advantage of the present method is that
clusters with sibilant consonants that do not con-
form to the sonority principle (see the example of
str in Section 3.3) do not have to be treated dif-
ferently. They merely follow from the fact that
these clusters are particularly frequent in word-
peripheral position. The biggest disadvantage is
the fact that the method is sensitive to frequen-
cies of individual clusters and thereby sometimes
breaks up clusters that should be tautosyllabic
(one of the few examples in our Latin corpus was
teneb.rae).
5 Conclusions and future work
A complete model of syllabification involves more
than what has been presented in this paper. The
method proposed here is restricted to single words
and does not take into account resyllabification
across word boundaries as well as some other crite-
ria thatmight influence the actual syllable structure
of words such as stress and morphological bound-
aries. Nevertheless, the discussion of our approach
shows that expanding the range of languages to
other families and areas of the world can challenge
some of the well-established findings that are used
for inferring linguistic knowledge.
The results of Sukhotin?s algorithm show that
the distinction between vowels and consonants,
which is vital for any syllabification method, can
be induced from raw texts on the basis of the sim-
ple assumptions that vowels and consonants tend
to alternate and that a vowel is the most frequent
symbol in a corpus. In contrast to previous stud-
ies of the algorithm (Sassoon, 1992), our results
do not suffer from the fact that the input text is too
short and therefore yield better results.
Based on the classifications of symbols into
vowels and consonants with Sukhotin?s algorithm
our unsupervised syllabification method deter-
9Topping notes that phonetically there is a glottal stop pre-
ceding every word-initial vowel, yet this is totally predictable
in this position and therefore not phonemic.
mines syllable boundaries on distributional infor-
mation. In contrast to other unsupervised ap-
proaches to syllabification that are grounded on at-
tributing a sonority value to each consonant and
OMP, our procedure breaks up word-medial con-
sonant sequences by considering the frequencies
of all possible word-peripheral clusters in order to
get the most probable division. We did not pro-
vide a comparative evaluation of our procedure but
only discussed the problems that can be encoun-
tered when looking at a wider variety of languages
and how they can be solved by our approach. The
question that this paper wants to raise is therefore
if it is more important to optimize a procedure on
a single language (mostly English or related Euro-
pean languages) or whether it should be capable of
dealing with the variety of structures that can be
found in the languages of the world.
For future work we want to apply the present
methods on phonetically transcribed corpora in
order to be able to compare the results for the
well-studied European languages to other meth-
ods. There are still some challenges remaining for
a universal syllabification procedure, one of them
being the detection of syllabic consonants. Ulti-
mately, we also want to integrate a sonority hier-
archy of the input symbols to combine the advan-
tages of both approaches and to create a gradual
value for syllabification that is able to account for
the difference between clear-cut syllable bound-
aries and ambisyllabic consonants or other cases
where a syllable boundary is harder to establish.
Acknowledgments
This work has been funded by the research ini-
tiative ?Computational Analysis of Linguistic De-
velopment? and the DFG Sonderforschungsbere-
ich 471 ?Variation und Entwicklung im Lexikon?
at the University of Konstanz. The author would
like to thank the Australian Institute of Aboriginal
and Torres Strait Islander Studies (AIATSIS) for
the Warlpiri Bible sections as well as Miriam Butt,
Frans Plank, Bernhard W?lchli and three anony-
mous reviewers for valuable comments and sug-
gestions.
References
Gordon F. Arnold. 1955-1956. A phonological ap-
proach to vowel, consonant and syllable in modern
french. Lingua, V:251?287.
70
Emily Bender. 2009. Linguistically naive != language
independent: Why NLP needs linguistic typology.
In Proceedings of the EACL 2009 Workshop on the
Interaction between Linguistics and Computational
Linguistics, pages 26?32.
Gavan Breen and Rob Pensalfini. 1999. Arrernte: A
language with no syllable onsets. Linguistic Inquiry,
30(1):1?25.
Luigi Burzio. 1994. Principles of English Stress. Cam-
bridge: Cambridge University Press.
San Duanmu. 2009. Syllable Structure. Oxford: Ox-
ford University Press.
Heinz Giegerich. 1992. English Phonology. Cam-
bridge: Cambridge University Press.
John Goldsmith and Gary Larson. 1990. Local mod-
elling and syllabification. In Michael Ziolkowski,
Manuela Noske, and Karen Deaton, editors, The
Parasession on the Syllable in Phonetics & Phonol-
ogy, volume 2 of Papers from the 26th Regional
Meeting of the Chicago Linguistic, pages 130?141.
Chicago Linguistic Society.
John Goldsmith and Aris Xanthos. 2009. Learning
phonological categories. Language, 85(1):4?38.
Sharon Goldwater and Mark Johnson. 2005. Rep-
resentational bias in unsupervised learning of syl-
lable structure. In Proceedings of the 9th Confer-
ence on Computational Natural Language Learning
(CONLL), Ann Arbor.
Edmund Gussmann. 2002. Phonoloy: Analysis and
Theory. Cambridge: Cambridge University Press.
Jacques B. M. Guy. 1991. Vowel identification: an old
(but good) algorithm. Cryptologia, XV(3):258?262,
July.
Morris Halle. 1998. The stress of english words. Lin-
guistic Inquiry, 29(4):539?568.
Michael Hammond. 1999. The Phonology of English:
A Prosodic Optimality Theoretic Approach. Oxford:
Oxford University Press.
Bruce Hayes. 1995. Metrical Stress Theory: Prin-
ciples and Case Studies. Chicage: University of
Chicage Press.
Roman Jakobson and Morris Halle. 1956. Fundamen-
tals of Language I. Phonology and Phonetics. ?s-
Gravenhage: Mouton.
Daniel Kahn. 1976. Syllable-based generalizations in
English phonology. Ph.D. thesis, Massachusetts In-
stitute of Technology.
Charles W. Kreidler. 2004. The Pronunciation of En-
glish: A Course Book. Malden, MA: Blackwell.
Jerzy Kury?owicz. 1948. Contribution ? la th?orie de
la syllabe. Bulletin de la Societe Polonaise de Lin-
guistique, 8:5?114.
Ian Maddieson. 2008. Consonant-vowel ratio. In
Martin Haspelmath, Matthew S. Dryer, David Gil,
and Bernard Comrie, editors, The World Atlas of
Language Structures Online, chapter 3. Munich:
Max Planck Digital Library. Available online at
http://wals.info/feature/3. Accessed on 2010-04-23.
J. D. O?Connor and J. L. M. Trim. 1953. Vowel, conso-
nant, and syllable - a phonological definition. Word,
9(2):103?122.
George T. Sassoon. 1992. The application of
Sukhotin?s algorithm to certain Non-English lan-
guages. Cryptologia, 16(2):165?173.
Elisabeth O. Selkirk. 1982. The syllable. In Harry
van der Hulst and Norval Smith, editors, The Struc-
ture of Phonological Representations, part II, pages
337?383. Dordrecht: Foris.
Boris V. Sukhotin. 1962. Eksperimental?noe vydelenie
klassov bukv s pomo??ju evm. Problemy strukturnoj
lingvistiki, 234:189?206.
Boris V. Sukhotin. 1973. M?thode de d?chiffrage,
outil de recherche en linguistique. T.A. Informa-
tions, 2:1?43.
Donald M. Topping. 1980. Chamorro Reference
Grammar. The University Press of Hawaii, Hon-
olulu.
Irene Vogel. 1977. The Syllable in Phonological The-
ory with Special Reference to Italian. Ph.D. thesis,
Stanford University.
71
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 1?6,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualization of Linguistic Patterns
and
Uncovering Language History from Multilingual Resources
Miriam Butt1 Jelena Prokic?2 Thomas Mayer2 Michael Cysouw3
1Department of Linguistics, University of Konstanz
2Research Unit Quantitative Language Comparison, LMU Munich
3Research Center Deutscher Sprachatlas, Philipp University of Marburg
1 Introduction
The LINGVIS and UNCLH (Visualization of Lin-
guistic Patterns & Uncovering Language His-
tory from Multilingual Resources) were originally
conceived of as two separate workshops. Due to
perceived similarities in content, the two work-
shops were combined and organized jointly.
The overal aim of the joint workshop was to
explore how methods developed in computational
linguistics, statistics and computer science can
help linguists in exploring various language phe-
nomena. The workshop focused particularly on
two topics: 1) visualization of linguistic patterns
(LINGVIS); 2) usage of multilingual resources in
computational historical linguistics (UNCLH).
2 LINGVIS
The overall goal of the first half of the work-
shop was to bring together researchers work-
ing within the emerging subfield of computa-
tional linguistics ? using methods established
within Computer Science in the fields of Infor-
mation Visualization (InfoVis) and Visual Ana-
lytics in conjunction with methodology and anal-
yses from theoretical and computational linguis-
tics. Despite the fact that statistical methods for
language analysis have proliferated in the last
two decades, computational linguistics has so far
only marginally availed itself of techniques from
InfoVis and Visual Analytics (e.g., Honkela et
al. (1995); Neumann et al (2007); Collins et
al. (2009); Collins (2010); Mayer et al (2010a);
Mayer et al (2010b); Rohrdantz et al (2011)).
The need to integrate methods from InfoVis and
Visual Analytics arises particularly with respect
to situations in which the amount of data to be
analyzed is huge and the interactions between rel-
evant features are complex. Both of these situ-
ations hold for much of current (computational)
linguistic analysis. The usual methods of sta-
tistical analysis do not allow for quick and easy
grasp and interpretation of the patterns discovered
through statistical processing and an integration
of innovative visualization techniques has become
imperative.
The overall aim of the first half of the workshop
was thus to draw attention to this need and to the
newly emerging type of work that is beginning to
respond to the need. The workshop succeeded in
bringing together researchers interesting in com-
bining techniques and methodology from theo-
retical and computational linguistics with InfoVis
and Visual Analytics.
Three of the papers in the workshop focused
on the investigation and visualization of lexical
semantics. Rohrdantz et al present a diachronic
study of fairly recently coined derivational suf-
fixes (-gate, -geddon, -athon) as used in newspa-
per corpora across several languages. Their anal-
ysis is able to pin-point systematic differences in
contextual use as well as some first clues as to
how and why certain new coinages spread bet-
ter than others. Heylen et al point out that me-
thods such as those used in Rohrdantz et al,
while producing interesting results, are essentially
black boxes for the researchers ? it is not clear
exactly what is being calculated. Their paper
presents some first steps towards making the black
box more transparent. In particular, they take
a close look at individual tokens and their se-
mantic use with respect to Dutch synsets. Cru-
cially, they anticipate an interactive visualization
that will allow linguistically informed lexicogra-
1
phers to work with the available data and patterns.
A slightly different take on synset relations is pre-
sented by Lohk et al, who use visualization me-
thods to help identify errors in WordNets across
different languages.
Understanding differences and relatedness be-
tween languages or types of a language is the sub-
ject of another three papers. Littauer et al use
data from the WALS (World Atlas of Language
Structures; Dryer and Haspelmath (2011)) to
model language relatedness via heat maps. They
overcome two difficulties: one is the sparseness
of the WALS data; another is that WALS does
not directly contain information about possible ef-
fects of language contact. Littauer et al attempt
to model the latter by taking geographical infor-
mation about languages into account (neighboring
languages and their structure). A different kind
of language relatedness is investigated by Yan-
nakoudakis et al, who look at learner corpora and
develop tools that allow an assessment of learner
competence with respect to various linguistic fea-
tures found in the corpora. The number of rel-
evant features is large and many of them are in-
terdependent or interact. Thus, the amount and
complexity of the data present a classic case of
complex data sets that are virtually impossible to
analyze well without the application of visualiza-
tion methods. Finally, Lyding et al take academic
texts and investigate the use of modality across
academic registers and across time in order to
identify whether the academic language used in
different subfields (or adjacent fields) of an aca-
demic field has an effect on the language use of
that field.
3 UNCLH
The second half of the workshop focused on
the usage of multilingual resources in computa-
tional historical linguistics. In the past 20 years,
the application of quantitative methods in his-
torical linguistics has received increasing atten-
tion among linguists (Dunn et al, 2005; Heg-
garty et al, 2010; McMahon and McMahon,
2006), computational linguists (Kondrak, 2001;
Hall and Klein, 2010) and evolutionary anthropol-
ogists (Gray and Atkinson, 2003). Due to the ap-
plication of these quantitative methods, the field
of historical linguistics is undergoing a renais-
sance. One of the main problems that researchers
face is the limited amount of suitable compara-
tive data, often falling back on relatively restricted
?Swadesh type? wordlists. One solution is to use
synchronic data, like dictionaries or texts, which
are available for many languages. For example,
in Kondrak (2001), vocabularies of four Algo-
nquian languages were used in the task of au-
tomatic cognate identification. Another solution
employed by Snyder et al (2010) is to apply a
non-parametric Bayesian framework to two non-
parallel texts in the task of text deciphering. Al-
though very promising, these approaches have so
far only received modest attention. Thus, many
questions and challenges in the automatization
of language resources in computational historical
linguistics remain open and ripe for investigation.
In dialectological studies, there is a long tra-
dition, starting with Se?guy (1971), in which lan-
guage varieties are grouped together on the ba-
sis of their similarity with respect to certain prop-
erties. Later work in this area has incorporated
methods of string alignment for a quantitative
comparison of individual words to obtain an aver-
age measure of the similarity of languages. This
line of research became known as dialectome-
try. Unlike traditional dialectology which is based
on the analysis of individual items, dialectometry
shifts focus on the aggregate level of differences.
Most of the work done so far in dialectometry
is based on the carefully selected wordlists and
problems with the limited amount of suitable data
(i.e. computer readable and comparable across di-
alects) are also present in this field.
This workshop brings together researchers in-
terested in computational approaches that uncover
sound correspondences and sound changes, auto-
matic identification of cognates across languages
and language comparison based both on wordlists
and parallel texts. First, Wettig et al investigate
the sound correspondences in cognate sets in a
sample of Uralic languages. Then, List?s contri-
bution to the volume introduces a novel method
for automatic cognate detection in multilingual
wordlists which combines various previous ap-
proaches for string comparison. The paper by
Mayer & Cysouw presents a first step to use par-
allel texts for a quantitative comparison of lan-
guages. The papers by Scherrer and Prokic? et
al. both are in the spirit of the dialectometric line
of research. Further, Ja?ger reports on quantify-
ing language similarity via phonetic alignment of
core vocabulary items. Finally, some of the pa-
2
pers presented in this workshop deal with further
topics in quantitative language comparison, like
the application of phylogenetic methods in cre-
ole research in the paper by Daval-Markussen &
Bakker, and the study of the evolution of the Aus-
tralian kinship terms reported on in the paper by
McConvell & Dousset.
In the next section, we give a brief introduc-
tion into the papers presented in this workshop,
ordered according to the program of the oral pre-
sentations at the workshop.
4 Papers
Christian Rohrdantz, Andreas Niekler, Annette
Hautli, Miriam Butt and Daniel A. Keim (?Lex-
ical Semantics and Distribution of Suffixes ?
A Visual Analysis) present a quantitative cross-
linguistic investigation of the lexical semantic
content expressed by three suffixes originating in
English: -gate, -geddon and -athon. Using data
from newspapers, they look at the distribution and
lexical semantic usage of these morphemes across
several languages and also across time, with a
time-depth of 20 years for English. Using tech-
niques from InfoVis and Visual Analytics is cru-
cial for the analysis as the occurrence of these suf-
fixes in the available corpora is comparatively rare
and it is only by dint of processing and visualiz-
ing huge amounts of data that a clear pattern can
begin to emerge.
Kris Heylen, Dirk Speelman and Dirk Geer-
aerts (?Looking at Word Meaning. An Interac-
tive Visualization of Semantic Vector Spaces for
Dutch synsets?) focus on the pervasive use of Se-
mantic Vector Spaces (SVS) in statistical NLP
as a standard technique for the automatic mod-
eling of lexical semantics. They take on the
fact that while the method appears to work fairly
well (though they criticize the standardly avail-
able evaluation measures via some created gold
standard), it is in fact quite unclear how it captures
word meaning. That is, the standard technology
can be seen as a black box. In order to find a way
of providing some transparency to the method,
they explore the way an SVS structures the indi-
vidual occurrences of words with respect to the
occurrences of 476 Dutch nouns. These were
grouped into 214 synsets in previous work. This
paper looks at a token-by-token similarity matrix
in conjunction with a visualization that uses the
Google Chart Tools and compares the results with
previous work, especially in light of different uses
in different versions of Dutch.
Ahti Lohk, Kadri Vare and Leo Vo?handu
(?First Steps in Checking and Comparing Prince-
ton WordNet and Estonian WordNet?) use visu-
alization methods to compare two existing Word-
Nets (English and Estonian) in order to identify
errors and semantic inconsistencies that are a re-
sult of the manual coding. Their method opens
up a potentially interesting way of automatically
checking for inconsistencies and errors not only
at a fairly basic and surface level, but by work-
ing with the lexical semantic classification of the
words in question.
Richard Littauer, Rory Turnbull and Alexis
Palmer (?Visualizing Typological Relationships:
Plotting WALS with Heat Maps?) present a novel
way of visualizing relationships between lan-
guages. The paper is based on data extracted from
the World Atlas of Language Structures (WALS),
which is the most complete set of typological and
digitized data available to date, but which presents
two challenges: 1) it actually has very low cover-
age both in terms of languages represented and
in terms of feature description for each language;
2) areal effects are not coded for. While the au-
thors find a way to overcome the first challenge,
the paper?s real contribution lies in proposing a
method for overcoming the second challenge. In
particular, the typological data is filtered by geo-
graphical proximity and then displayed by means
of heat maps, which reflect the strength of similar-
ity between languages for different linguistic fea-
tures. Thus, the data should allow one to be able
to ascertain areal typological effects via a single
integrated visualization.
Helen Yannakoudakis, Ted Briscoe and
Theodora Alexopoulou (?Automatic Second
Language Acquisition Research: Integrating
Information Visualisation and Machine Learn-
ing?) look at yet another domain of application.
They show how data-driven approaches to
learner corpora can support Second Language
Acquisition (SLA) research when integrated
with visualization tools. Learner corpora are
interesting because their analysis requires a good
understanding of a complex set of interacting
linguistic features across corpora with different
distributional patterns (since each corpus po-
tentially diverges from the standard form of the
language by a different set of features). The paper
3
presents a visual user interface which supports
the investigation of a set of linguistic features
discriminating between pass and fail exam
scripts. The system displays directed graphs to
model interactions between features and supports
exploratory search over a set of learner texts.
A very useful result for SLA is the proposal
of a new method for empirically quantifying
the linguistic abilities that characterize different
levels of language learning.
Verena Lyding, Ekaterina Lapshinova-
Koltunski, Stefania Degaetano-Ortlieb, Henrik
Dittmann and Chris Culy (?Visualizing Linguistic
Evolution in Academic Discourse?) describe
methods for visualizing diachronic language
changes in academic writing. In particular, they
look at the use of modality across different aca-
demic subfields and investigate whether adjacent
subfields affect the use of language in a given
academic subfield. Their findings potentially
provide crucial information for further NLP tasks
such as automatic text classification.
Grzegorz Kondrak?s invited contribution
(?Similarity Patterns in Words?) sketches a num-
ber of the author?s research projects on diachronic
linguistics. He first discusses computational tech-
niques for implementing several steps of the
comparative method. These techniques include
algorithms that deal with a wide range of prob-
lems: pairwise and multiple string alignment,
calculation of phonetic similarity between two
strings, automatic extraction of recurrent sound
correspondences, quantification of semantic
similarity between two words, identification of
sets of cognates and building of phylogenetic
trees. In the second part, Kondrak sketches
several NLP projects that directly benefitted
from his research on diachronic linguistics:
statistical machine translation, word align-
ment, identification of confusable drug names,
transliteration, grapheme-to-phoneme conver-
sion, letter-phoneme alignment and mapping of
annotations.
Thomas Mayer and Michael Cysouw (?Lan-
guage Comparison through Sparse Multilingual
Word Alignment?) present a novel approach on
how to calculate similarities among languages
with the help of massively parallel texts. In-
stead of comparing languages pairwise they sug-
gest a simultaneous analysis of languages with re-
spect to their co-occurrence statistics for individ-
ual words on the sentence level. These statistics
are then used to group words into clusters which
are considered to be partial (or ?sparse?) align-
ments. These alignments then serve as the basis
for the similarity count where languages are taken
to be more similar the more words they share in
the various alignments, regardless of the actual
form of the words. In order to cope with the
computationally demanding multilingual analysis
they introduce a sparse matrix representation of
the co-occurrence statistics.
Yves Scherrer (?Recovering Dialect Geogra-
phy from an Unaligned Comparable Corpus?) pro-
poses a simple metric of dialect distance, based
on the ratio between identical word pairs and cog-
nate word pairs occurring in two texts. Scherrer
proceeds from a multidialectal corpus and applies
techniques from machine translation in order to
extract identical words and cognate words. The
dialect distance is defined as as function of the
number of cognate word pairs and identical word
pairs. Different variations of this metric are tested
on a corpus containing comparable texts from dif-
ferent Swiss German dialects and evaluated on the
basis of spatial autocorrelation measures.
Jelena Prokic?, C?ag?r? Co?ltekin and John Ner-
bonne (?Detecting Shibboleths?) propose a gen-
eralization of the well-known precision and re-
call scores to deal with the case of detecting dis-
tinctive, characteristic variants in dialect groups,
in case the analysis is based on numerical differ-
ence scores. This method starts from the data that
has already been divided into groups using clus-
ter analyses, correspondence analysis or any other
technique that can identify groups of language va-
rieties based on linguistic or extra-linguistic fac-
tors (e.g. geography or social properties). The
method seeks items that differ minimally within a
group but differ a great deal with respect to ele-
ments outside it. They demonstrate the effective-
ness of their approach using Dutch and German
dialect data, identifying those words that show
low variation within a given dialect area, and high
variation outside a given area.
Gerhard Ja?ger (?Estimating and Visualizing
Language Similarities Using Weighted Align-
ment and Force-Directed Graph Layout?) reports
several studies to quantify language similarity
via phonetic alignment of core vocabulary items
(taken from the Automated Similarity Judgement
Program data base). Ja?ger compares several string
4
comparison measures based on Levenshtein dis-
tance and based on Needleman-Wunsch similar-
ity score. He also tests two normalization func-
tions, one based on the average score and the
other based on the informatic theoretic similar-
ity measure. The pairwise similarity between all
languages are analyzed and visualized using the
CLANS software, a force directed graph layout
that does not assume an underlying tree structure
of the data.
Aymeric Daval-Markussen and Peter Bakker
(?Explorations in Creole Research with Phyloge-
netic Tools?) employ phylogenetic tools to inves-
tigate and visualize the relationship of creole lan-
guages to other (non-)creole languages on the ba-
sis of structural features. Using the morphosyn-
tactic features described in the monograph on
Comparative Creole Syntax (Holm and Patrick,
2007), they create phylogenetic trees and net-
works for the languages in the sample, which
show the similarity between the various languages
with respect to the grammatical features inves-
tigated. Their results lend support to the uni-
versalist approach which assumes that creoles
show creole-specific characteristics, possibly due
to restructuring universals. They also apply their
methodology to the comparison of creole lan-
guages to other languages, on the basis of typo-
logical features from the World Atlas of Language
Structures. Their findings confirm the hypothe-
sis that creole languages form a synchronically
distinguishable subgroup among the world?s lan-
guages.
Patrick McConvell and Laurent Dousset
(?Tracking the Dynamics of Kinship and So-
cial Category Terms with AustKin II?) give an
overview of their ongoing work on kinship and
social category terms in Australian languages.
They describe the AustKin I database which
allows for the reconstruction of older kinship
systems as well as the visualization of patterns
and changes. In particular, their method recon-
structs so-called ?Kariera? kinship systems for the
proto-languages in Australia. This supports ear-
lier hypotheses about the primordial world social
organization from which Dravidian-Kariera sys-
tems are considered to have evolved. They also
report on more recent work within the AustKin II
project which is devoted to the co-evoluation of
marriage and social category systems.
Hannes Wettig, Kirill Reshetnikov and Roman
Yangarber (?Using Context and Phonetic Fea-
tures in Models of Etymological Sound Change?)
present a novel method for a context-sensitive
alignment of cognate words, which relies on the
information theoretic concept of Minimum De-
scription Length to decide on the most compact
representation of the data given the model. Start-
ing with an initial random alignment for each
word pair, their algorithm iteratively rebuilds de-
cision trees for each feature and realigns the cor-
pus while monotonically decreasing the cost func-
tion until convergence. They also introduce a
novel test for the quality of the models where one
word pair is omitted from the training phase. The
rules that have been learned are then used to guess
one word from the other in the pair. The Lev-
enshtein distance of the correct and the guessed
word is then computed to give an idea of how
good the model actually learned the regularities
in the sound correspondences.
Johann-Mattis List (?LexStat: Automatic De-
tection of Cognates in Multilingual Wordlists?)
presents a new method for automatic cognate
detection in multilingual wordlists. He com-
bines different approaches to sequence compari-
son in historical linguistics and evolutionary bi-
ology into a new framework which closely mod-
els central aspects of the comparative method.
The input sequences, i.e. words, are converted to
sound classes and their sonority profiles are deter-
mined. In step 2, a permutation method is used to
create language specific scoring schemes. In step
3, the pairwise distances between all word pairs,
based on the language-specific scoring schemes,
are computed. In step 4, the sequences are clus-
tered into cognate sets whose average distance is
beyond a certain threshold. The method is tested
on 9 multilingual wordlists.
5 Final remarks
The breadth and depth of the research collected
in this workshop more than testify to the scope
and possibilities for applying new methods that
combine quantitative methods with not only a so-
phisticated linguistic understanding of language
phenomena, but also with visualization methods
coming out of the Computer Science fields of In-
foVis and Visual Analytics. The papers in the
workshop addressed how the emerging new body
of work can provide advances and new insights
for questions pertaining to theoretical linguistics
5
(lexical semantics, derivational morphology, his-
torical linguistics, dialectology and typology) and
applied linguistic fields such as second language
acquisition and statistical NLP.
6 Acknowledgments
We are indebted to the members of the pro-
gram committee of the workshop for their ef-
fort in thoroughly reviewing the papers: Quentin
Atkinson, Christopher Collins, Chris Culy, Dan
Dediu, Michael Dunn, Sheila Embleton, Simon
Greenhill, Harald Hammarstro?m, Annette Hautli,
Wilbert Heeringa, Gerhard Heyer, Eric Hol-
man, Gerhard Ja?ger, Daniel Keim, Tibor Kiss,
Jonas Kuhn, Anke Lu?deling, Steven Moran, John
Nerbonne, Gerald Penn, Don Ringe, Christian
Rohrdantz, Tandy Warnow, S?ren Wichmann.
We also thank the organizers of the EACL 2012
conference for their help in setting up the joint
workshop.
References
Christopher Collins, Sheelagh Carpendale, and Ger-
ald Penn. 2009. Docuburst: Visualizing document
content using language structure. Computer Graph-
ics Forum (Proceedings of Eurographics/IEEE-
VGTC Symposium on Visualization (EuroVis ?09)),
28(3):1039?1046.
Christopher Collins. 2010. Interactive Visualizations
of Natural Language. Ph.D. thesis, University of
Toronto.
Matthew S. Dryer and Martin Haspelmath, editors.
2011. The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich, 2011
edition.
Michael Dunn, Angela Terrill, Ger Resnik, Robert A.
Foley, and Stephen C. Levinson. 2005. Structural
phylogenetics and the reconstruction of ancient lan-
guage history. Science, 309(5743):2072?2075.
Russell Gray and Quentin Atkinson. 2003. Language-
tree divergence times support the Anatolian theory
of Indo-European origins. Nature, 426:435?439.
David LW Hall and Dan Klein. 2010. Finding cognate
groups using phylogenies. In Proceedings of the
Association for Computational Linguistics.
Paul Heggarty, Warren Maguire, and April McMahon.
2010. Splits or waves? trees or webs? how diver-
gence measures and network analysis can unravel
language histories. In Philosophical Transactions
of the Royal Society (B), volume 365, pages 3829?
3843.
John Holm and Peter L. Patrick, editors. 2007. Com-
parative Creole Syntax. London: Battlebridge.
Timo Honkela, Ville Pulkki, and Teuvo Kohonen.
1995. Contextual relations of words in grimm tales,
analyzed by self-organizing map. In Proceedings of
International Conference on Artificial Neural Net-
works (ICANN-95), pages 3?7.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceedings
of the North American Chapter of the Association
of Computational Linguistics.
Thomas Mayer, Christian Rohrdantz, Miriam Butt,
Frans Plank, and Daniel A. Keim. 2010a. Visualiz-
ing vowel harmony. Linguistic Issues in Language
Technology (LiLT), 2(4).
Thomas Mayer, Christian Rohrdantz, Frans Plank,
Peter Bak, Miriam Butt, and Daniel A. Keim.
2010b. Consonant co-occurrence in stems across
languages: Automatic analysis and visualization of
a phonotactic constraint. In Proceedings of the
2010 Workshop on NLP and Linguistics: Finding
the Common Ground, ACL 2010, pages 70?78.
April McMahon and Robert McMahon. 2006. Lan-
guage Classification by Numbers. OUP.
Petra Neumann, Annie Tat, Torre Zuk, and Shee-
lagh Carpendale. 2007. Keystrokes: Personaliz-
ing typed text with visualization. In Proceedings
of Eurographics IEEE VGTC Symposium on Visu-
alization.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual
analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics (Short Papers), pages 305?310. Portland, Ore-
gon.
Jean Se?guy. 1971. La relation entre la distance spa-
tiale et la distance lexicale. Revue de Linguistique
Romane, 35(138):335?357.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language deci-
pherment. In Proceedings of the Association for
Computational Linguistics.
6
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 54?62,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Language comparison through sparse multilingual word alignment
Thomas Mayer
Research Unit
Quantitative Language Comparison
LMU Munich
thommy.mayer@googlemail.com
Michael Cysouw
Research Center
Deutscher Sprachatlas
Philipp University of Marburg
cysouw@uni-marburg.de
Abstract
In this paper, we propose a novel approach
to compare languages on the basis of par-
allel texts. Instead of using word lists or
abstract grammatical characteristics to infer
(phylogenetic) relationships, we use mul-
tilingual alignments of words in sentences
to establish measures of language similar-
ity. To this end, we introduce a new method
to quickly infer a multilingual alignment of
words, using the co-occurrence of words in
a massively parallel text (MPT) to simulta-
neously align a large number of languages.
The idea is that a simultaneous multilin-
gual alignment yields a more adequate clus-
tering of words across different languages
than the successive analysis of bilingual
alignments. Since the method is computa-
tionally demanding for a larger number of
languages, we reformulate the problem us-
ing sparse matrix calculations. The useful-
ness of the approach is tested on an MPT
that has been extracted from pamphlets of
the Jehova?s Witnesses. Our preliminary
experiments show that this approach can
supplement both the historical and the ty-
pological comparison of languages.
1 Introduction
The application of quantitative methods in histor-
ical linguistics has attracted a lot of attention in
recent years (cf. Steiner et al (2011) for a sur-
vey). Many ideas have been adapted from evolu-
tionary biology and bioinformatics, where similar
problems occur with respect to the genealogical
grouping of species and the multiple alignment
of strings/sequences. One of the main differences
between those areas and attempts to uncover lan-
guage history is the limited amount of suitable
data that can serve as the basis for language com-
parison. A widely used resource are Swadesh lists
or similar collections of translational equivalents
in the form of word lists. Likewise, phylogenetic
methods have been applied using structural char-
acteristics (e.g., Dunn et al (2005)). In this paper,
we propose yet another data source, namely par-
allel texts.
Many analogies have been drawn between the
evolution of species and languages (see, for in-
stance, Pagel (2009) for such a comparison). One
of the central problems is to establish what is the
equivalent of the gene in the reproduction of lan-
guages. Like in evolutionary biology, where gene
sequences in organisms are compared to infer
phylogenetic trees, a comparison of the ?genes?
of language would be most appropriate for a quan-
titative analysis of languages. Yet, Swadesh-
like wordlists or structural characteristics do not
neatly fit into this scheme as they are most likely
not the basis on which languages are replicated.
After all, language is passed on as the expression
of propositions, i.e. sentences, which usually con-
sists of more than single words. Hence, follow-
ing Croft (2000), we assume that the basic unit of
replication is a linguistic structure embodied in a
concrete utterance.
According to this view, strings of DNA in bio-
logical evolution correspond to utterances in lan-
guage evolution. Accordingly, genes (i.e., the
functional elements of a string of DNA) corre-
spond to linguistic structures occurring in those
utterances. Linguistic replicators (the ?genes? of
language) are thus structures in the context of an
utterance. Such replicators are not only the words
as parts of the sentence but also constructions to
express a complex semantic structure, or phonetic
54
realizations of a phoneme, to give just a few ex-
amples.
In this paper, we want to propose an approach
that we consider to be a first step in the direc-
tion of using the structure of utterances as the
basic unit for the comparison of languages. For
this purpose, a multilingual alignment of words in
parallel sentences (as the equivalent of utterances
in parallel texts) is computed, similar to multi-
species alignments of DNA sequences.1 These
alignments are clusters of words from different
languages in the parallel translations of the same
sentence.2
The remainder of the paper is organized as fol-
lows. First, we quickly review the position of our
approach in relation to the large body of work on
parallel text analysis (Section 2). Then we de-
scribe the method for the multilingual alignment
of words (Section 3). Since the number of lan-
guages and sentences that have to be analyzed re-
quire a lot of computationally expensive calcula-
tions of co-occurrence counts, the whole analysis
is reformulated into manipulations of sparse ma-
trices. The various steps are presented in detail
to give a better overview of the calculations that
are needed to infer the similarities. Subsequently,
we give a short description of the material that we
used in order to test our method (Section 4). In
Section 5 we report on some of the experiments
that we carried out, followed by a discussion of
the results and their implications. Finally, we con-
clude with directions for future work in this area.
2 Word Alignment
Alignment of words using parallel texts has been
widely applied in the field of statistical ma-
chine translation (cf. Koehn (2010)). Alignment
methods have largely been employed for bitexts,
i.e., parallel texts of two languages (Tiedemann,
2011). In a multilingual context, the same meth-
ods could in principle be used for each pair of lan-
guages in the sample. One of the goals of this pa-
1The choice of translational equivalents in the form of
sentences rather than words accounts for the fact that some
words cannot be translated accurately between some lan-
guages whereas most sentences can.
2In practice, we simply use wordforms as separated by
spaces or punctuation instead of any more linguistically sen-
sible notion of ?word?. For better performance, more detailed
language-specific analysis is necessary, like morpheme sep-
aration, or the recognition of multi-word expressions and
phrase structures.
per, however, is to investigate what can be gained
when including additional languages in the align-
ment process at the same time and not iteratively
looking for correspondences in pairs of languages
(see Simard (1999), Simard (2000) for a similar
approach).
There are basically two approaches to comput-
ing word alignments as discussed in the literature
(cf. Och and Ney (2003)): (i) statistical alignment
models and (ii) heuristic models. The former have
traditionally been used for the training of parame-
ters in statistical machine translation and are char-
acterized by their high complexity, which makes
them difficult to implement and tune. The latter
are considerably simpler and thus easier to im-
plement as they only require a function for the
association of words, which is computed from
their co-occurrence counts. A wide variety of co-
occurrence measures have been employed in the
literature. We decided to use a heuristic method
for the first steps reported on here, but plan to inte-
grate statistical alignment models for future work.
Using a global co-occurrence measure, we pur-
sue an approach in which the words are compared
for each sentence individually, but for all lan-
guages at the same time. That is, a co-occurrence
matrix is created for each sentence, containing all
the words of all languages that occur in the cor-
responding translational equivalents for that sen-
tence. This matrix then serves as the input for
a partitioning algorithm whose results are inter-
preted as a partial alignment of the sentence. In
most cases, the resulting alignments do not in-
clude words from all languages. Only those words
that are close translational equivalents occur in
alignments. This behavior, while not optimal
for machine translation, is highly useful for lan-
guage comparison because differences between
languages are implicitly marked as such by split-
ting different structures into separate alignments.
The languages are then compared on the basis
of having words in the same clusters with other
languages. The more word forms they share in the
same clusters, the more similar the languages are
considered to be.3 The form of the words them-
selves is thereby of no importance. What counts
3A related approach is discussed in Wa?lchli (2011). The
biggest difference to the present approach is that Wa?lchli
only compares languages pairwise. In addition, he makes use
of a global glossing method and not an alignment of words
within the same parallel sentence.
55
is their frequency of co-occurrence in alignments
across languages. This is in stark contrast to
methods which focus on the form of words with
similar meanings (e.g., using Swadesh lists) in or-
der to compute some kind of language similar-
ity. One major disadvantage of the present ap-
proach for a comparison of languages from a his-
torical perspective is the fact that such similarities
also could be a consequence of language contact.
This is a side effect that is shared by the word
list approach, in which loanwords have a simi-
lar effect on the results. It has to be seen how
strongly this influences the final results in order
to assess whether our current approach is useful
for the quantitative analysis of genealogical relat-
edness.
3 Method
We start from a massively parallel text, which we
consider as an n?m matrix consisting of n differ-
ent parallel sentences S = {S1, S2, S3, ..., Sn} in
m different languages L = {L1, L2, L3, ..., Lm}.
This data-matrix is called SL (?sentences ? lan-
guages?). We assume here that the parallel sen-
tences are short enough so that most words occur
only once per sentence. Because of this assump-
tion we can ignore the problem of decoding the
correct alignment of multiple occurring words, a
problem we leave to be tackled in future research.
We also ignore the complications of language-
specific chunking and simply take spaces and
punctuation marks to provide a word-based sep-
aration of the sentences into parts. In future re-
search we are planning to include the (language-
specific) recognition of bound morphemes, multi-
word expressions and phrase structures to allow
for more precise cross-language alignment.
Based on these assumptions, we decompose the
SL matrix into two sparse matrices WS (?words
? sentences?) and WL (?words ? languages?)
based on all words w that occur across all lan-
guages in the parallel texts. We define them as
follows. First, WSij = 1 when word wi oc-
curs in sentence Sj , and is 0 elsewhere. Second,
WLij = 1 when word wi is a word of language
Lj , and is 0 elsewhere. The product WST ?WL
then results in a matrix of the same size as SL,
listing in each cell the number of different words
in each sentence. Instead of the current approach
of using WS only for marking the occurrence of
a word in a sentence (i.e., a ?bag of words? ap-
proach), it is also possible to include the order of
words in the sentences by defining WSij = k
when word wi occurs in position k in sentence
Sj . We will not use this extension in this paper.
The matrix WS will be used to compute co-
occurrence statistics of all pairs of words, both
within and across languages. Basically, we define
O (?observed co-occurrences?) and E (?expected
co-occurrences?) as:
O = WS ?WST
E = WS ?
1SS
n
?WST
Eij thereby gives the expected number of sen-
tences where wi and wj occur in the correspond-
ing translational equivalents, on the assumption
that words from different languages are statisti-
cally independent of each other and occur at ran-
dom in the translational equivalents. Note that
the symbol ?1ab? in our matrix multiplications
refers to a matrix of size a ? b consisting of
only 1?s. Widespread co-occurrence measures are
pointwise mutual information, which under these
definitions simply is logE? logO, or the cosine
similarity, which would be O?
n?E
. However, we
assume that the co-occurrence of words follow
a poisson process (Quasthoff and Wolff, 2002),
which leads us to define the co-occurrence matrix
WW (?words ? words?) using a poisson distri-
bution as:
WW = ? log[
EO exp(?E)
O!
]
= E+ logO!?O logE
This WW matrix represents a similarity ma-
trix of words based on their co-occurrence in
translational equivalents for the respective lan-
guage pair. Using the alignment clustering that
is based on the WW matrices for each sentence,
we then decompose the words-by-sentences ma-
trix WS into two sparse matrices WA (?words?
alignments?) and AS (?alignments ? sentences?)
such that WS = WA ?AS. This decomposition
is the basic innovation of the current paper.
The idea is to compute concrete alignments
from the statistical alignments in WW for each
sentence separately, but for all languages at the
same time. For each sentence Si we take the
subset of the similarity matrix WW only includ-
ing those words that occur in the column WSi,
56
i.e., only those words that occur in sentence Si.
We then perform a partitioning on this subset of
the similarity matrix WW. In this paper we use
the affinity propagation clustering approach from
Frey and Dueck (2007) to identify the clusters, but
this is mainly a practical choice and other meth-
ods could be used here as well. The reason for
this choice is that this clustering does not require
a pre-defined number of clusters, but establishes
the optimal number of clusters together with the
clustering itself.4 In addition, it yields an exem-
plar for each cluster, which is the most typical
member of the cluster. This enables an inspec-
tion of intermediate results of what the clusters
actually contain. The resulting clustering for each
sentence identifies groups of words that are sim-
ilar to each other, which represent words that are
to be aligned across languages. Note that we do
not force such clusters to include words from all
languages, nor do we force any restrictions on the
number of words per language in each cluster.5
In practice, most alignments only include words
from a small number of the languages included.
To give a concrete example for the clustering
results, consider the English sentence given below
(no. 93 in our corpus, see next section) together
with its translational equivalents in German, Bul-
garian, Spanish, Maltese and Ewe (without punc-
tuation and capitalization).
i. who will rule with jesus (English, en)
ii. wer wird mit jesus regieren (German, de)
iii. ko$i we upravlva s isus (Bulgarian, bl)
iv. quie?nes gobernara?n con jesu?s (Spanish, es)
v. min se jah?kem ma g?esu` (Maltese, mt)
vi. amekawoe a?u fia kple yesu (Ewe, ew)
These six languages are only a subset of the
50 languages that served as input for the matrix
WW where all words that occur in the respective
sentence for all 50 languages are listed together
with their co-occurrence significance. When re-
stricting the output of the clustering to those
words that occur in the six languages given above,
4Instead of a prespecified number of clusters, affinity
propagation in fact takes a real number as input for each data
point where data points with larger values are more likely to
be chosen as exemplars. If no input preference is given for
each data point, as we did in our experiments, exemplar pref-
erences are initialized as the median of non infinity values in
the input matrix.
5Again, this takes into account that some words cannot
be translated accurately between some languages.
however, the following clustering result is ob-
tained:
1. isusbl jesusen fiaew yesuew g?esu`mt jesu?ses
jesusde
2. ko$ibl whoen minmt werde
3. regierende
4. upravlvabl a?uew jah?kemmt gobernara?nes
5. amekawoeew quie?neses
6. webl willen semtwirdde
7. sbl withen cones mitde
8. kpleew
9. mamt
10. ruleen
First note that the algorithm does not require
all languages to be given in the same script. Bul-
garian isus is grouped together with its transla-
tional equivalents in cluster 1 even though it does
not share any grapheme with them. Rather, words
from different languages end up in the same clus-
ter if they behave similarly across languages in
terms of their co-occurrence frequency. Further,
note that the ?question word? clusters 2 and 5 dif-
fer in their behavior as will be discussed in more
detail in Section 5.2. Also note that the English
?rule? and German ?regieren? are not included in
the cluster 4 with similar translations in the other
languages. This turns out to be a side effect of the
very low frequency of these words in the current
corpus.
In the following, we will refer to these clusters
of words as alignments (many-to-many mappings
between words) within the same sentence across
languages. For instance, sentences i., iii. and v.
above would have the following alignment, where
indices mark those words that are aligned by the
alignment clusters (1.-10.) above:
who2 will6 rule10 with7 jesus1
min2 se6 jah?kem4 ma7 g?esu`1
ko$i2 we6 upravlva4 s7 isus1
All alignment-clusters from all sentences are
summarized as columns in the sparse matrixWA,
defined as WAij = 1 when word wi is part of
alignment Aj , and is 0 elsewhere.6 We also estab-
lish the ?book-keeping? matrix AS to keep track
6For instance, the alignment in 2. above contains the four
words {ko$i, who, min, wer}, which are thus marked with 1
whereas all other words have 0 in this column of the WA
matrix.
57
of which alignment belongs to which sentence,
defined as ASij = 1 when the alignment Ai oc-
curs in sentence Sj , and as 0 elsewhere. The
alignment matrix WA is the basic information
to be used for language comparison. For exam-
ple, the product WA ?WAT represents a sparse
version of the words ? words similarity matrix
WW.
A more interesting usage of WA is to derive
a similarity between the alignments AA. We de-
fine both a sparse version of AA, based on the
number of words that co-occur in a pair of align-
ments, and a statistical version of AA, based on
the average similarity between the words in the
two alignments:
AAsparse = WAT ?WA
AAstatistical =
WAT ?WW ?WA
WAT ? 1WW ?WA
The AA matrices will be used to select suit-
able alignments from the parallel texts to be used
for language comparison. Basically, the statistical
AA will be used to identify similar alignments
within a single sentence and the sparse AA will
be used to identify similar alignments across dif-
ferent sentences. Using a suitable selection of
alignments (we here use the notation A? for a se-
lection of alignments7), a similarity between lan-
guages LL can be defined as:
LL = LA? ? LA?T
by defining LA? (?languages ? alignments?) as
the number of words per language that occur in
each selected alignment:
LA? = WLT ?WA?
The similarity between two languages LL is then
basically defined as the number of times words
are attested in the selected alignments for both
languages. It thus gives an overview of how
structurally similar two languages are, where lan-
guages are considered to have a more similar
structure the more words they share in the align-
ment clusters.
7Note that the prime in this case does not stand for the
transpose of a matrix, as it is sometimes used.
4 Data
Parallel corpora have received a lot of attention
since the advent of statistical machine translation
(Brown et al, 1988) where they serve as training
material for the underlying alignment models. For
this reason, the last two decades have seen an in-
creasing interest in the collection of parallel cor-
pora for a number of language pairs (Hansard8),
also including text corpora which contain texts
in three or more languages (OPUS9, Europarl10,
Multext-East11). Yet there are only few resources
which comprise texts for which translations are
available into many different languages. Such
texts are here referred to as ?massively parallel
texts? (MPT; cf. Cysouw and Wa?lchli (2007)).
The most well-known MPT is the Bible, which
has a long tradition in being used as the basis
for language comparison. Apart from that, other
religious texts are also available online and can
be used as MPTs. One of them is a collection
of pamphlets of the Jehova?s Witnesses, some of
which are available for over 250 languages.
In order to test our methods on a variety of
languages, we collected a number of pamphlets
from the Watchtower website http://www.
watchtower.org) together with their trans-
lational equivalents for 146 languages in total.
The texts needed some preprocessing to remove
HTML markup, and they were aligned with re-
spect to the paragraphs according to the HTML
markup. We extracted all paragraphs which con-
sisted of only one sentence in the English ver-
sion and contained exactly one English question
word (how, who, where, what, why, whom, whose,
when, which) and a question mark at the end.
From these we manually excluded all sentences
where the ?question word? is used with a differ-
ent function (e.g., where who is a relative pronoun
rather than a question word). In the end we were
left with 252 questions in the English version and
the corresponding sentences in the 145 other lan-
guages. Note that an English interrogative sen-
tence is not necessarily translated as a question
in each other language (e.g., the English question
what is the truth about God? is simply translated
into German as die Wahrheit u?ber Gott ?the truth
8http://www.isi.edu/natural-language/
download/hansard/
9http://opus.lingfil.uu.se
10http://www.statmt.org/europarl/
11http://nl.ijs.si/ME/
58
about God?). However, such translations appear
to be exceptions.
5 Experiments
5.1 Global comparison of Indo-European
As a first step to show that our method yields
promising results we ran the method for the 27
Indo-European languages in our sample in order
to see what kind of global language similarity
arises when using the present approach. In our
procedure, each sentence is separated into various
multilingual alignments. Because the structures
of languages are different, not each alignment will
span across all languages. Most alignments will
be ?sparse?, i.e., they will only include words from
a subset of all languages included. In total, we
obtained 6, 660 alignments (i.e., 26.4 alignments
per sentence on average), with each alignment in-
cluding on average 9.36 words. The number of
alignments per sentence turns out to be linearly
related to the average number of words per sen-
tence, as shown in Fig. 1. A linear interpolation
results in a slope of 2.85, i.e., there are about three
times as many alignments per sentence as the av-
erage number of words. We expect that this slope
depends on the number of languages that are in-
cluded in the analysis: the more languages, the
steeper the slope.
5 10 15
10
20
30
40
50
average sentence length in words
numbe
r of ali
gnmen
ts per 
senten
ce
Figure 1: Linear relation between the average number
of words per sentence and number of alignments per
sentence
We use the LL matrix as the similarity matrix
for languages including all 6, 660 alignments. For
each language pair this matrix contains the num-
ber of times words from both languages are at-
tested in the same alignment. This similarity ma-
trix is converted into a distance matrix by sub-
tracting the similarity value from the highest value
that occurs in the matrix:
LLdist = max(LL)? LL
This distance matrixLLdist is transformed into
a NeighborNet visualization for an inspection of
the structures that are latent in the distance ma-
trix. The NeighborNet in Fig. 2 reveals an ap-
proximate grouping of languages according to the
major language families, the Germanic family on
the right, the Romance family on the top and the
Slavic family at the bottom. Note that the sole
Celtic language in our sample, Welsh, is included
inside the Germanic languages, closest to English.
This might be caused by horizontal influence from
English on Welsh. Further, the only Baltic lan-
guage in our sample, Lithuanian, is grouped with
the Slavic languages (which is phylogenetically
expected behavior in line with Gray and Atkin-
son (2003)), though note that it is grouped par-
ticularly close to Russian and Polish, which sug-
gests more recent horizontal transfer. Interest-
ingly, the separate languages Albanian and Greek
roughly group together with two languages from
the other families: Romanian (Romance) and Bul-
garian (Slavic). This result is not in line with their
phylogenetic relatedness but rather reflects a con-
tact situation in which all four languages are part
of the Balkan Sprachbund.
Although the NeighborNet visualization ex-
hibits certain outcomes that do not correspond to
the attested genealogical relationship of the lan-
guages, the method still fares pretty well based
on a visual inspection of the resulting Neighbor-
Net. In the divergent cases, the groupings can be
explained by the fact that the languages are in-
fluenced by the surrounding languages (as is most
clear for the Balkan languages) through direct lan-
guage contact. As mentioned before, a similar
problem also exists when using word lists to in-
fer phylogenetic trees when loanwords introduce
noise into the calculations and thus lead to a closer
relationship of languages than is genealogically
tenable. However, in the case of our alignments
59
Afrikaans
English
Welsh
German
Icelandic
Lithuanian
Polish
Russian
UkrainianCzechSlovak
SlovenianCroatian
Serbian
Albanian
Greek
Bulgarian
Romanian
PortugueseSpanishCatalan
FrenchItalian Danish
NorwegianSwedish
Dutch
1000000.0
Figure 2: NeighborNet (created with SplitsTree, Huson and Bryant (2006)) of all Indo-European languages in
the sample
the influence of language contact is not related to
loanwords but to the borrowing of similar con-
structions or structural features. In the Balkan
case, linguists have noted over one hundred such
shared structural features, among them the loss
of the infinitive, syncretism of dative and geni-
tive case and postposed articles (cf. Joseph (1992)
and references therein). These features are partic-
ularly prone to lead to a higher similarity in our
approach where the alignment of words within
sentences is sensitive to the fact that certain word
forms are identical or different even though the
exact form of the word is not relevant.
5.2 Typology of PERSON interrogatives
A second experiment we conducted involved a
closer study of just a few questions in the data at
hand to obtain a better impression of the results
of the alignment procedure. For this experiment,
we took the same 252 questions for a worldwide
sample of 50 languages. After running the whole
procedure, we selected just the six sentences in
the sample that were formulated in English with a
who interrogative, i.e., questions as to the person
who did something. The English sentences are the
following:
I Who will be resurrected?
II Who will rule with Jesus?
III Who created all living things?
IV Who are god?s true worshipers on earth to-
day?
V Who is Jesus Christ?
VI Who is Michael the Archangel?
We expected to be able to find all translations
of English who in the alignments. Interestingly,
this is not what happened. The six alignments that
comprised the English who only included words
in 23 to 30 other languages in the sample, so we
are clearly not finding all translations of who. By
using a clustering on AAstatistical we were able
to find seven more alignments that appear to be
highly similar to the six alignments including En-
glish who. Together, these 13 alignments included
words for almost all languages in the six sentences
(on average 47.7 words for each sentence). We
computed a language similarity LL only on the
basis of these 13 alignments, which represents a
typology of the structure of PERSON interrog-
atives. This typology clearly separates into two
60
clusters of languages, two ?types? so to speak, as
can be seen in Fig. 3.
Investigating the reason for these two types, it
turns out that the languages in the right cluster of
Fig. 3 consistently separate the six sentences into
two groups. The first, second, and fourth sen-
tence are differently marked than the third, fifth
and sixth sentence. For example, Finnish uses
ketka? vs. kuka and Spanish quie?nes vs. quie?n.
These are both oppositions in number, suggesting
that all languages in the right cluster of Fig. 3 dis-
tinguish between a singular and a plural form of
who. Interpreting the meaning of the English sen-
tences quoted above, this distinction makes com-
plete sense. The Ewe form amekawoe in example
vi. (see Section 3) contains the plural marker -wo,
which distinguishes it from the singular form and
indeed correctly clusters together with quie?nes in
the alignment cluster 5.
This example shows that it is possible to use
parallel texts to derive a typology of languages for
a highly specific characteristic.
6 Conclusion and Future Work
One major problem with using our approach for
phylogentic reconstruction is the influence of lan-
guage contact. Traits of the languages which are
not inherited from a common proto-language but
are transmitted through contact situations lead to
noise in the similarity matrix which does not re-
flect a genealogical signal. However, other meth-
ods also suffer from the shortcoming that lan-
guage contact cannot be automatically subtracted
from the comparison of languages without man-
ual input (such as manually created cognate lists).
With translational equivalents, a further problem
for the present approach is the influence of trans-
lationese on the results. If one version in a lan-
guage is a direct translation of another language,
the structural similarity might get a higher score
due to the fact that constructions will be literally
translated which otherwise would be expressed
differently in that language.
The experiments that have been presented in
this paper are only a first step. However, we firmly
believe that a multilingual alignment of words is
more appropriate for a large-scale comparison of
languages than an iterative bilingual alignment.
Yet so far we do not have the appropriate evalu-
ation method to prove this. We therefore plan to
include a validation scheme in order to test how
much can be gained from the simultaneous analy-
sis of more than two languages. Apart from this,
we intend to improve the alignment method itself
by integrating techniques from statistical align-
ment models, like adding morpheme separation or
phrase structures into the analysis.
Another central problem for the further devel-
opment of this method is the selection of align-
ments for the language comparison. As our sec-
ond experiment showed, just starting from a se-
lection of English words will not automatically
generate the corresponding words in the other lan-
guages. It is possible to use the AA matrices to
search for further similar alignments, but this pro-
cedure is not yet formalized enough to automati-
cally produce language classification for selected
linguistic domains (like for the PERSON interrog-
atives in our experiment). When this step is better
understood, we will be able to automatically gen-
erate typological parameters for a large number
of the world?s languages, and thus easily produce
more data on which to base future language com-
parison.
Acknowledgements
This work has been funded by the DFG project
?Algorithmic corpus-based approaches to typo-
logical comparison?. We are grateful to four
anonymous reviewers for their valuable com-
ments and suggestions.
References
Peter F. Brown, John Cocke, Stephen A. Della-Pietra,
Vincent J. Della-Pietra, Frederick Jelinek, Robert L.
Mercer, and Paul S. Roossin. 1988. A statistical
approach to language translation. In Proceedings
of the 12th International Conference on Computa-
tional Linguistics (COLING-88), pages 71?76.
William Croft. 2000. Explaining Language Change:
An Evolutionary Approach. Harlow: Longman.
Michael Cysouw and Bernhard Wa?lchli. 2007. Paral-
lel texts: using translational equivalents in linguis-
tic typology. Sprachtypologie und Universalien-
forschung STUF, 60(2):95?99.
Michael Dunn, Angela Terrill, Ger Reesink, R. A. Fo-
ley, and Steve C. Levinson. 2005. Structural phylo-
genetics and the reconstruction of ancient language
history. Science, 309(5743):2072?5, 9.
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972?976.
61
Alb
an
ian
Ra
ro
to
ng
an
Ma
lte
se
Ma
lag
as
y
Lit
hu
an
ian
Ilo
ko
Cr
oa
tia
n
Ch
ich
ew
a
Bu
lga
ria
n
Ge
rm
an
Po
na
pe
an
Pa
pi
am
en
to
 (A
ru
ba
)
Pa
pi
am
en
to
 (C
ur
a?
ao
)
Du
tch
Ni
ue
an
M
isk
ito
In
do
ne
sia
n
Ita
lia
n
Kir
iba
ti
Fr
en
ch
En
gli
sh
Da
nis
h
Ha
itia
n 
Cr
eo
le
Ca
tal
an
Af
rik
aa
ns
At
es
o
Fij
ian
Tu
va
lua
n
Sw
ed
ish
Gu
na
Hu
ng
ar
ian
Qu
ec
hu
a (
An
ca
sh
)
Kw
an
ya
ma
Tu
mb
uk
a
Ch
in 
(H
ak
ha
)
Ts
wa
na
Sp
an
ish
Nd
on
ga
Ny
an
ek
a
Gr
ee
k
Fin
nis
h
Ew
e
Da
ng
me
Ch
ito
ng
a
Sh
on
a
Bi
co
l
Xit
sh
wa
Ac
ho
li
Lu
ga
nd
a
Se
pe
di
10
15
20
25
30
Cluster Dendrogram
hclust (*, "complete")
as.dist(max(LL) - LL)
He
igh
t
Figure 3: Hierarchical cluster using Ward?s minimum variance method (created with R, R Development Core
Team (2010)) depicting a typology of languages according to the structure of their PERSON interrogatives
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?439.
Daniel H. Huson and David Bryant. 2006. Applica-
tion of phylogenetic networks in evolutionary stud-
ies. Molecular Biology and Evolution, 23(2):254?
267.
Brian D. Joseph. 1992. The Balkan languages. In
William Bright, editor, International Encyclopedia
of Linguistics, pages 153?155. Oxford: Oxford Uni-
versity Press.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Mark Pagel. 2009. Human language as a culturally
transmitted replicator. Nature Reviews Genetics,
10:405?415.
Uwe Quasthoff and Christian Wolff. 2002. The
poisson collocation measure and its applications.
In Proceedings of the 2nd International Workshop
on Computational Approaches to Collocations, Vi-
enna, Austria.
R Development Core Team, 2010. R: A language
and environment for statistical computing. Wien:
R Foundation for Statistical Computing.
Michel Simard. 1999. Text-translation alignment:
Three languages are better than two. In Proceed-
ings of EMNLP/VLC-99, pages 2?11.
Michel Simard. 2000. Text-translation alignment:
Aligning three or more versions of a text. In Jean
Ve?ronis, editor, Parallel Text Processing: Align-
ment and Use of Translation Corpora, pages 49?67.
Dordrecht: Kluwer Academic Publishers.
Lydia Steiner, Peter F. Stadler, and Michael Cysouw.
2011. A pipeline for computational historical
linguistics. Language Dynamics and Change,
1(1):89?127.
Jo?rg Tiedemann. 2011. Bitext Alignment. Morgan &
Claypool Publishers.
Bernhard Wa?lchli. 2011. Quantifying inner form: A
study in morphosemantics. Arbeitspapiere. Bern:
Institut fu?r Sprachwissenschaft.
62
