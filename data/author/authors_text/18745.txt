Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 89?98, Dublin, Ireland, August 23-29 2014.
Group Non-negative Matrix Factorization with Natural Categories for
Question Retrieval in Community Question Answer Archives
Guangyou Zhou, Yubo Chen, Daojian Zeng, and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,yubo.chen,djzeng,jzhao}@nlpr.ia.ac.cn
Abstract
Community question answering (CQA) has become an important service due to the popularity of
CQA archives on the web. A distinctive feature is that CQA services usually organize questions
into a hierarchy of natural categories. In this paper, we focus on the problem of question re-
trieval and propose a novel approach, called group non-negative matrix factorization with natural
categories (GNMFNC). This is achieved by learning the category-specific topics for each cate-
gory as well as shared topics across all categories via a group non-negative matrix factorization
framework. We derive an efficient algorithm for learning the factorization, analyze its complex-
ity, and provide proof of convergence. Experiments are carried out on a real world CQA data set
from Yahoo! Answers. The results show that our proposed approach significantly outperforms
various baseline methods and achieves the state-of-the-art performance for question retrieval.
1 Introduction
Community question answering (CQA) such as Yahoo! Answers
1
and Quora
2
, has become an important
service due to the popularity of CQA archives on the web. To make use of the large-scale questions and
their answers, it is critical to have functionality of helping users to retrieve previous answers (Duan et
al., 2008). Typically, such functionality is achieved by first retrieving the historical questions that best
match a user?s queried question, and then using answers of these returned questions to answer the queried
question. This is what we called question retrieval in this paper.
The major challenge for question retrieval, as for most information retrieval tasks, is the lexical gap
between the queried questions and the historical questions in the archives. For example, if a queried ques-
tion contains the word ?company? but a relevant historical question instead contains the word ?firm?, then
there is a mismatch and the historical question may not be easily distinguished from an irrelevant one.
To solve the lexical gap problem, most researchers focused on translation-based approaches since the
relationships between words (or phrases) can be explicitly modeled through word-to-word (or phrases)
translation probabilities (Jeon et al., 2005; Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008; Bern-
hard and Gurevych, 2009; Zhou et al., 2011; Singh, 2012). However, these existing methods model the
relevance ranking without considering the category-specific and shared topics with natural categories, it
is not clear whether this information is useful for question retrieval.
A distinctive feature of question-answer pairs in CQA is that CQA services usually organize questions
into a hierarchy of natural categories. For example, Yahoo! Answers contains a hierarchy of 26 categories
at the first level and more than 1262 subcategories at the leaf level. When a user asks a question, the user
is typically required to choose a category label for the question from a predefined hierarchy. Questions in
the predefined hierarchy usually share certain generic topics while questions in different categories have
their specific topics. For example, questions in categories ?Arts & Humanities? and ?Beauty & Style?
may share the generic topic of ?dance? but they also have the category-specific topics of ?poem? and
?wearing?, respectively.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1
http://answers.yahoo.com/
2
http://www.quora.com/
89
Inspired by the above observation, we propose a novel approach, called group non-negative matrix
factorization with natural categories (GNMFNC). GNMFNC assumes that there exists a set of category-
specific topics for each of the category, and there also exists a set of shared topics for all of the categories.
Each question in CQA is specified by its category label, category-specific topics, as well as shared topics.
In this way, the large-scale question retrieval problem can be decomposed into small-scale subproblems.
In GNMFNC, questions in each category are represented as a term-question matrix. The term-question
matrix is then approximated as the product of two matrices: one matrix represents the category-specific
topics as well as the shared topics, and the other matrix denotes the question representation based on
topics. An objective function is defined to measure the goodness of prediction of the data with the
model. Optimization of the objective function leads to the automatic discovery of topics as well as
the topic representation of questions. Finally, we calculate the relevance ranking between the queried
questions and the historical questions in the latent topic space.
Past studies by (Cao et al., 2009; Cao et al., 2010; Ming et al., 2010; Cai et al., 2011; Ji et al., 2012;
Zhou et al., 2013) confirmed a significant retrieval improvement by adding the natural categories into
various existing retrieval models. However, all these previous work regarded natural categories indi-
vidually without considering the relationships among them. On the contrary, this paper can effectively
capture the relationships between the shared aspects and the category-specific individual aspects with
natural categories via a group non-negative matrix factorization framework. Also, our work models the
relevance ranking in the latent topic space rather than using the existing retrieval models. To date, no at-
tempts have been made regarding group non-negative matrix factorization in studies of question retrieval,
which remains an under-explored area.
The remainder of this paper is organized as follows. Section 2 describes our proposed group non-
negative matrix factorization with natural categories for question retrieval. Section 3 presents the exper-
imental results. In Section 4, we conclude with ideas for future research.
2 Group Non-negative Matrix Factorization with Natural Categories
2.1 Problem Formulation
In CQA, all questions are usually organized into a hierarchy of categories. When a user asks a question,
the user is typically required to choose a category label for the question from a predefined hierarchy of
categories. Hence, each question in CQA has a category label. Suppose that we are given a question col-
lection D in CQA archive with size N , containing terms from a vocabulary V with size M . A question
d is represented as a vector d ? R
M
where each entry denotes the weight of the corresponding term,
for example tf-idf is used in this paper. Let C = {c
1
, c
2
, ? ? ? , c
P
} denote the set of categories (subcat-
egories) of question collection D, where P is the number of categories (subcategories). The question
collection D is organized into P groups according to their category labels and can be represented as
D = {D
1
,D
2
, ? ? ? ,D
P
}. D
p
= {d
(p)
1
, ? ? ? ,d
(p)
N
p
} ? R
M?N
p
is the term-question matrix corresponding
to category c
p
, in which each row stands for a term and each column stands for a question. N
p
is the
number of questions in category c
p
such that
?
P
p=1
N
p
= N .
LetU
?
p
= [U
s
,U
p
] ? R
M?(K
s
+K
p
)
be the term-topic matrix corresponding to category c
p
, where K
s
is the number of shared topics, K
p
is the number of category-specific topics corresponding to category
c
p
, and p ? [1, P ]. Term-topic matrix U
s
can be represented as U
s
= [u
(s)
1
, ? ? ? ,u
(s)
K
s
] ? R
M?K
s
, in
which each column corresponds to a shared topic. While the term-topic matrix U
p
can be represented
as U
p
= [u
(p)
1
, ? ? ? ,u
(p)
K
p
] ? R
M?K
p
. The total number of topics in the question collection D is K =
K
s
+ PK
p
. Let V
p
= [v
(p)
1
, ? ? ? ,v
(p)
N
p
] ? R
(K
s
+K
p
)?N
p
be the topic-question matrix corresponding to
category c
p
, in which each column denotes the question representation in the topic space. We also denote
V
T
p
= [H
T
p
,W
T
p
], where H
p
? R
K
s
?N
p
and W
p
? R
K
p
?N
p
correspond to the coefficients of shared
topicsU
s
and category-specific topicsU
p
, respectively.
Thus, given a question collection D = {D
1
,D
2
, ? ? ? ,D
P
} together with the category labels C =
{c
1
, c
2
, ? ? ? , c
P
}, our proposed GNMFNC amounts to modeling the question collection D with P group
90
simultaneously, arriving at the following objective function:
O =
P
?
p=1
{
?
p
?
?
D
p
? [U
s
,U
p
]V
p
?
?
2
F
+ R(U
s
,U
p
)
}
(1)
where ?
p
, ?D
p
?
?2
F
. R(U
s
,U
p
) is a regularization term used to penalize the ?similarity? between the
shared topics and category-specific topics throughU
s
andU
p
.
In this paper, we aim to ensure that matrix U
s
captures only shared topics and matrix U
p
captures
only the category-specific topics. For example, if matricesU
s
andU
p
are mutually orthogonal, we have
U
T
s
U
p
= 0. To impose this constraint, we attempt to minimize the sum-of-squares of entries of the
matrix U
T
s
U
p
(e.g., ?U
T
s
U
p
?
2
F
which uniformly optimizes each entry of U
T
s
U
p
). With this choice, the
regularization term of R(U
s
,U
p
) is given by
R(U
s
,U
p
) =
P
?
p=1
?
p
?
?
U
T
s
U
p
?
?
2
F
+
P
?
l=1,l?=p
?
l
?
?
U
T
p
U
l
?
?
2
F
(2)
where ?
p
and ?
l
are the regularization parameters, ?p ? [1, P ], ?l ? [1, P ].
Learning the objective function in equation (1) involves the following optimization problem:
min
U
s
,U
p
,V
p
?0
L = O + ?
1
?
?
U
T
s
1
M
? 1
K
s
?
?
2
F
+ ?
2
?
?
U
T
p
1
M
? 1
K
p
?
?
2
F
+ ?
3
?
?
V
p
1
N
p
? 1
K
s
+K
p
?
?
2
F
(3)
where ?
1
, ?
2
and ?
3
are the shrinkage regularization parameters. Based on the shrinkage methodology,
we can approximately satisfy the normalization constraints for each column of [U
s
,U
p
] and V
T
p
by
guaranteeing the optimization converges to a stationary point.
2.2 Learning Algorithm
We present the solution to the GNMFNC optimization problem in equation (3) as the following theorem.
The theoretical aspects of the optimization are presented in the next subsection.
Theorem 2.1. UpdatingU
s
,U
p
andV
p
using equations (4)?(6) corresponds to category c
p
will mono-
tonically decrease the objective function in equation (3) until convergence.
U
s
? U
s
?
[
?
P
p=1
?
p
D
p
H
T
p
]
[
?
P
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+ ?
p
U
p
U
T
p
U
s
]
(4)
U
p
? U
p
?
[
?
p
D
p
W
T
p
]
[
?
p
[U
s
,U
p
]V
p
W
T
p
+ ?
p
U
s
U
T
s
U
p
+
?
P
l=1,l?=p
?
l
U
l
U
T
l
U
p
]
(5)
V
p
? V
p
?
[
?
p
D
T
p
[U
s
,U
p
]
]
[
?
p
V
T
p
[U
s
,U
p
]
T
[U
s
,U
p
]
]
(6)
where operator ? is element-wise product and
[?]
[?]
is element-wise division.
Based on Theorem 2.1, we note that multiplicative update rules given by equations (4)?(6) are ob-
tained by extending the updates of standard NMF (Lee and Seung, 2001). A number of techniques can
be used here to optimize the objective function in equation (3), such as alternating least squares (Kim
and Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin,
2007). Nonetheless, the multiplicative updates derived in this paper have reasonably fast convergence
behavior as shown empirically in the experiments.
2.3 Theoretical Analysis
In this subsection, we give the theoretical analysis of the optimization, convergence and computational
complexity.
91
Without loss of generality, we only show the optimization ofU
s
and formulate the Lagrange function
with constraints as follows:
L(U
s
) = O + ?
1
?
?
U
T
s
1
M
? 1
K
s
?
?
2
F
+ Tr(?
s
U
T
s
)
(7)
where Tr(?) denotes the trace of a matrix, ?
s
? R
K
s
?K
s
is the Lagrange multiplier for the nonnegative
constraintU
s
? 0.
The partial derivative of L(U
s
) w.r.t. U
s
is
?
U
s
L(U
s
) = ?2
P
?
p=1
?
p
D
p
H
T
p
+ 2
P
?
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+ 2
P
?
p=1
?
p
U
p
U
T
p
U
s
+ 2?
1
U
s
? 2?
1
+ ?
s
(8)
Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ?
s
?U
s
= 0, we
obtain
?
U
s
L(U
s
) ?U
s
=
{
?
?
P
p=1
?
p
D
p
H
T
p
+
?
P
p=1
?
p
[U
s
,U
p
]V
p
H
T
p
+
?
P
p=1
?
p
U
p
U
T
p
U
s
+ ?
1
U
s
? ?
1
}
?U
s
= 0 (9)
After normalization ofU
s
, the terms ?
1
U
s
and ?
1
are in fact equal. They can be safely ignored from
the above formula without influencing convergence. This leads to the updating rule for U
s
in equation
(4). Following the similar derivations as shown above, we can obtain the updating rules for the rest
variablesU
p
andV
p
in GNMFNC optimization, as shown in equations (5) and (6).
2.3.1 Convergence Analysis
In this subsection, we prove the convergence of multiplicative updates given by equations (4)?(6). We
first introduce the definition of auxiliary function as follows.
Definition 2.1. F(X,X
?
) is an auxiliary function for L(X) if L(X) ? F(X,X
?
) and equality holds if
and only if L(X) = F(X,X).
Lemma 2.1. (Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under the
update
X
(t+1)
= argmin
X
F(X,X
(t)
)
Proof. By Definition 2.1, L(X
(t+1)
) ? F(X
(t+1)
,X
(t)
) ? F(X
(t)
,X
(t)
) = L(X
(t)
)
Theorem 2.2. Let L(U
(t+1)
s
) denote the sum of all terms in L that containU
(t+1)
s
, the following function
is an auxiliary function for L(U
(t+1)
s
)
F(U
(t+1)
s
,U
(t)
s
) = L(U
(t)
s
) + (U
(t+1)
s
?U
(t)
s
)?
U
(t)
s
L(U
(t)
s
) +
1
2
(U
(t+1)
s
?U
(t)
s
)
2
P(U
(t)
s
)
(10)
P(U
(t)
s
) =
?
ij
[
?
P
p=1
?
p
[U
(t)
s
,U
p
]V
p
W
T
p
+ ?
p
U
p
U
T
p
U
(t)
s
+ ?
1
U
(t)
s
]
ij
?
ij
[U
(t)
s
]
ij
where ?
U
(t)
s
L(U
(t)
s
) is the first-order derivative of L(U
(t)
s
) with respect toU
(t)
s
. Theorem 2.2 can be
proved similarly to (Lee and Seung, 2001) by validating L(U
(t+1)
s
) ? F(U
(t+1)
s
,U
(t)
s
), L(U
(t+1)
s
) =
F(U
(t+1)
s
,U
(t+1)
s
), and the Hessian matrix ??
U
(t+1)
s
F(U
(t+1)
s
,U
(t)
s
) ? 0. Due to limited space, we
omit the details of the validation.
92
addition multiplication division overall
GNMFNC:U
s
P (3MN
p
K
s
+MN
p
K
p
+MK
2
s
) P (3MN
p
K
s
+MN
p
K
p
+MK
2
s
) MK
s
O(PMN
p
K
max
)
GNMFNC:U
p
3MN
p
K
p
+MN
p
K
s
+ PM
2
K
?
3MN
p
K
p
+MN
p
K
s
+ PM
2
K
?
MK
p
O(PMRK
?
)
GNMFNC:V
p
3MN
p
K
?
3MN
p
K
?
N
p
K
?
O(MN
p
K
?
)
Table 1: Computational operation counts for each iteration in GNMFNC.
Based on Theorem 2.2, we can fixU
(t)
s
and minimize F(U
(t+1)
s
,U
(t)
s
) with respect toU
(t+1)
s
. When
setting ?
U
(t+1)
s
F(U
(t+1)
s
,U
(t)
s
) = 0, we get the following updating rule
U
(t+1)
s
? U
(t)
s
?
[
?
P
p=1
?
p
D
p
H
T
p
+ ?
1
]
[
?
P
p=1
?
p
[U
(t)
s
,U
p
]V
p
W
T
p
+ ?
p
U
p
U
T
p
U
(t)
s
+ ?
1
U
(t)
s
]
(11)
which is consistent with the updating rule derived from the KKT conditions aforementioned.
By Lemma 2.1 and Theorem 2.2, we have L(U
(0)
s
) = F(U
(0)
s
,U
(0)
s
) ? F(U
(1)
s
,U
(0)
s
) ?
F(U
(1)
s
,U
(1)
s
) = L(U
(1)
s
) ? ? ? ? ? L(U
(Iter)
s
), where Iter is the number of iterations. Therefore,
U
s
is monotonically decreasing. Since the objective function L is lower bounded by 0, the correctness
and convergence of Theorem 2.1 is validated.
2.3.2 Computational Complexity
In this subsection, we discuss the time computational complexity of the proposed algorithm GNMFNC.
Besides expressing the complexity of the algorithm using big O notation, we also count the number of
arithmetic operations to provide more details about running time. We show the results in Table 1, where
K
max
= max{K
s
,K
p
}, K
?
= K
s
+ K
p
and R = max{M,N
p
}.
Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates
then becomes O(Iter ? PMRK
?
). We set Iter = 100 empirically in rest of the paper. Therefore, the
overall running time of GNMFNC is linear with respect to the size of word vocabulary, the number of
questions and categories.
2.4 Relevance Ranking
The motivation of incorporating matrix factorization into relevance ranking is to learn the word rela-
tionships and reduce the ?lexical gap? (Zhou et al., 2013a). To do so, given a queried question q with
category label c
p
from Yahoo! Answers, we first represent it in the latent topic space as v
q
,
v
q
= argmin
v?0
?q? [U
s
,U
p
]v?
2
2
(12)
where vector q is the tf-idf representation of queried question q in the term space.
For each historical question d (indexed by r) in question collection D, with representation v
d
= r-th
column ofV, we compute its similarity with queried question v
q
as following
s
topic
(q, d) =
< v
q
,v
d
>
?v
q
?
2
? ?v
d
?
2
(13)
The latent topic space score s
topic
(q, d) is combined with the conventional term matching score
s
term
(q, d) for final relevance ranking. There are several ways to conduct the combination. Linear
combination is a simple and effective way. The final relevance ranking score s(q, d) is:
s(q, d) = ?s
topic
(q, d) + (1? ?)s
term
(q, d) (14)
where ? ? [0, 1] is the parameter which controls the relative importance of the latent topic space score
and term matching score. s
term
(q, d) can be calculated with any of the conventional relevance models
such as BM25 (Robertson et al., 1994) and LM (Zhai and Lafferty, 2001).
93
3 Experiments
3.1 Data Set and Evaluation Metrics
We collect the data set from Yahoo! Answers and use the getByCategory function provided in Yahoo!
Answers API
3
to obtain CQA threads from the Yahoo! site. More specifically, we utilize the resolved
questions and the resulting question repository that we use for question retrieval contains 2,288,607 ques-
tions. Each resolved question consists of four parts: ?question title?, ?question description?, ?question
answers? and ?question category?. We only use the ?question title? and ?question category? parts, which
have been widely used in the literature for question retrieval (Cao et al., 2009; Cao et al., 2010). There
are 26 first-level categories in the predefined natural hierarchy, i.e., each historical question is categorized
into one of the 26 categories. The categories include ?Arts & Humanities?, ?Beauty & Style?, ?Business
& Finance?, etc.
In order to evaluate our approach, we randomly select 2,000 questions as queried questions from the
above data collection to construct the validation/test sets, and the remaining data collection as training
set. Note that we select the queried questions in proportion to the number of questions and categories
against the whole distribution to have a better control over a possible imbalance. To obtain the ground-
truth, we employ the Vector Space Model (VSM) (Salton et al., 1975) to retrieve the top 10 results and
obtain manual judgements. The top 10 results don?t include the queried question itself. Given a returned
result by VSM, an annotator is asked to label it with ?relevant? or ?irrelevant?. If a returned result
is considered semantically equivalent to the queried question, the annotator will label it as ?relevant?;
otherwise, the annotator will label it as ?irrelevant?. Two annotators are involved in the annotation
process. If a conflict happens, a third person will make judgement for the final result. In the process
of manually judging questions, the annotators are presented only the questions. As a result, there are in
total 20,000 judged question pairs. We randomly split the 2,000 queried questions into validation/test
sets, each has 1,000/1,000 queried questions. We use the validation set for parameter tuning and the test
set for evaluation.
Evaluation Metrics: We evaluate the performance of question retrieval using the following metrics:
Mean Average Precision (MAP) and Precision@N (P@N). MAP rewards methods that return relevant
questions early and also rewards correct ranking of the results. P@N reports the fraction of the top-N
questions retrieved that are relevant. We perform a significant test, i.e., a t-test with a default significant
level of 0.05.
There are several parameters used in the paper, we tune these parameters on the validation set.
Specifically, we set the number of category-specific topics per category and the number of shared
topics in GNMFNC as (K
s
,K
p
) = {(5, 2), (10, 4), (20, 8), (40, 16), (80, 32)}, resulting in K =
{57, 114, 228, 456, 912} total number of topics. (Note that the total number of topics in GNMFNC
is K
s
+ 26 ?K
p
, where 26 is the number of categories in the first-level predefined natural hierarchy
4
).
Finally, we set (K
s
,K
p
) = (20, 8) and K = 228 empirically as this setting yields the best performance.
For regularization parameters ?
p
and ?
l
, it is difficult to directly tune on the validation set, we present
an alternative way by adding a common factor a to look at the objective function of optimization problem
in equation (3) on the training data. In other words, we set ?
p
=
a
K
s
?K
p
and ?
l
=
a
K
p
?K
l
. Therefore, we
tune the parameters ?
p
and ?
l
by alternatively adjusting the common factor a via grid search. As a result,
we set a = 100, resulting in ?
p
= ?
l
= 0.625 in the following experiments. The trade-off parameter ?
in the linear combination is set from 0 to 1 in steps of 0.1 for all methods. We set ? = 0.6 empirically.
For shrinkage regularization parameters, we empirically set ?
1
= ?
2
= ?
3
= 1.
3.2 Question Retrieval Results
In this experiment, we present the experimental results for question retrieval on the test data set. Specif-
ically, for our proposed GNMFNC, we combine the latent topic matching scores with the term matching
scores given by BM25 and LM, denoted as ?BM25+GNMFNC? and ?LM+GNMFNC?. Table 2 shows
3
http://developer.yahoo.com/answers
4
Here we do not use the leaf categories because we find that it is not possible to run GNMFNC with such large number of
topics on the current machines, and we will leave it for future work.
94
Table 2: Comparison with different methods
for question retrieval.
# Methods MAP P@10
1 BM25 0.243 0.225
2 LM 0.286 0.232
3 (Jeon et al., 2005) 0.327 0.235
4 (Xue et al., 2008) 0.341 0.238
5 (Zhou et al., 2011) 0.365 0.243
6 (Singh, 2012) 0.354 0.240
7 (Cao et al., 2010) 0.358 0.242
8 (Cai et al., 2011) 0.331 0.236
9 BM25+GNMFNC 0.369 0.248
10 LM+GNMFNC 0.374 0.251
Table 3: Comparison of matrix factoriza-
tions for question retrieval.
# Methods MAP P@10
1 BM25 0.243 0.225
2 BM25+NMF 0.325 0.235
3 BM25+CNMF 0.344 0.239
4 BM25+GNMF 0.361 0.242
5 BM25+GNMFNC 0.369 0.248
6 LM 0.286 0.232
7 LM+NMF 0.337 0.237
8 LM+CNMF 0.352 0.240
9 LM+GNMF 0.365 0.243
10 LM+GNMFNC 0.374 0.251
the main retrieval performances under the evaluation metrics MAP, P@1 and P@10. Row 1 and row
2 are the baseline systems, which model the relevance ranking using BM25 (Robertson et al., 1994)
and language model (LM) (Zhai and Lafferty, 2001) in the term space. Row 3 is word-based transla-
tion model (Jeon et al., 2005), and row 4 is word-based translation language model (TRLM) (Xue et
al., 2008). Row 5 is phrase-based translation model (Zhou et al., 2011), and row 6 is the entity-based
translation model (Singh, 2012). Row 7 to row 11 explore the natural categories for question retrieval.
In row 7, Cao et al. (2010) employed the natural categories to compute the local and global relevance
with different model combination, here we use the combination VSM + TRLM for comparison because
this combination obtains the superior performance than others. In row 8, Cai et al. (2011) proposed a
category-enhanced TRLM for question retrieval. There are some clear trends in the results of Table 2:
(1) BM25+GNMFNC and LM+GNMFNC perform significantly better than BM25 and LM respec-
tively (t-test, p-value < 0.05, row 1 vs. row 9; row 2 vs. row 10), indicating the effective of GNMFNC.
(2) BM25+GNMFNC and LM+GNMFNC perform better than translation methods, some improve-
ments are statistical significant (t-test, p-value < 0.05, row 3 and row 4 vs. row 9 and row 10). The
reason may be that GNMFNC models the relevance ranking in the latent topic space, which can also
effectively solve the the lexical gap problem.
(3) Capturing the shared aspects and the category-specific individual aspects with natural categories
in the group modeling framework can significantly improve the performance of question retrieval (t-test,
p-value < 0.05, row 7 and row 8 vs. row 9 and row 10).
(4) Natural categories are useful and effectiveness for question retrieval, no matter in the group mod-
eling framework or existing retrieval models (row 3? row 6 vs. row 7?row 10).
3.3 Comparison of Matrix Factorizations
We note that our proposed GNMFNC is related to non-negative matrix factorization (NMF) (Lee and
Seung, 2001) and its variants, we introduce three baselines. The first baseline is NMF, which is trained
on the whole training data. The second baseline is CNMF, which is trained on each category without
considering the shared topics. The third baseline is GNMF (Lee and Choi, 2009; Wang et al., 2012),
which is similar to our GNMFNC but there are no constraints on the category-specific topics to prevent
them from capturing the information from the shared topics.
NMF and GNMF are trained on the training data with the same parameter settings in section 4.1 for
fair comparison. For CNMF, we also train the model on the training data with the same parameter settings
in section 4.1, except parameter K
s
, as there exists no shared topics in CNMF.
Table 3 shows the question retrieval performance of NMF families on the test set, obtained with the
best parameter settings determined by the validation set. From the results, we draw the following obser-
vations:
(1) All of these methods can significantly improve the performance in comparison to the baseline
BM25 and LM (t-test, p-value < 0.05).
(2) GNMF and GNMFNC perform significantly better than NMF and CNMF respectively (t-test, p-
value < 0.05), indicating the effectiveness of group matrix factorization framework, especially the use
of shared topics.
95
0 20 40 60 80 1000.41
0.42
0.43
0.44
0.45
0.46
0.47
0.48
0.49
0.5
Iteration number
Obje
ctive 
funct
ion v
alue
Figure 1: Convergence curve of GNMFNC.
-4 -3 -2 -1 0 1 2 3 40.414
0.416
0.418
0.42
0.422
0.424
0.426
0.428
0.43
Log10a
Conv
erged
 obje
ctive 
funct
ion v
alue
Figure 2: Objective function value vs. factor a.
(3) GNMFNC performs significantly better than GNMF (t-test, p-value < 0.05, row 4 vs. row 5; row
9 vs. row 10), indicating the effectiveness of the regularization term on the category-specific topics to
prevent them from capturing the information from the shared topics.
From the experimental results reported above, we can conclude that our proposed GNMFNC is useful
for question retrieval with high accuracies. To the best of our knowledge, it is the first time to investigate
the group matrix factorization for question retrieval.
3.4 Convergence Behavior
In subsection 2.3.1, we have shown that the multiplicative updates given by equations (4)?(6) are con-
vergent. Here, we empirically show the convergence behavior of GNMFNC.
Figure 1 shows the convergence curve of GNMFNC on the training data set. From the figure, y-axis is
the value of objective function and x-axis denotes the iteration number. We can see that the multiplicative
updates for GNMFNC converge very fast, usually within 80 iterations.
3.5 Regularization Parameters Selection
One success of this paper is to use regularized constrains on the category-specific topics to prevent them
from capturing the information from the shared topics. It is necessary to give an in-depth analysis of
the regularization parameters used in the paper. Consider the regularization term used in equation (2),
each element in U
T
s
U
p
and U
T
p
U
l
has a value between 0 and 1 as each column of U
s
, U
p
and U
l
is
normalized. Therefore, it is appropriate to normalize the term having ?U
T
s
U
p
?
2
F
by K
s
K
p
since there
are K
s
?K
p
elements inU
T
s
U
p
. Similarly, ?U
T
p
U
l
?
2
F
is normalized by K
l
K
p
. Note that K
l
= K
p
and
l ?= p. As discussed in subsection 4.1, we present an alternative way by adding a common factor a and
set ?
p
=
a
K
s
?K
p
and ?
l
=
a
K
p
?K
l
. The common factor a is used to adjust a trade-off between the matrix
factorization errors and the mutual orthogonality, which cannot directly tune on the validation set. Thus,
we look at the objective function of optimization problem in equation (3) on the training data and find
the optimum value for a.
Figure 2 shows the objective function value vs. common factor a, where y-axis denotes the converged
objective function value, and x-axis denotes Log
10
a . We can see that the optimum value of a is 100.
Therefore, the common factor a can be fixed at 100 for our data set used in the paper, resulting in
?
p
= ?
l
= 0.625. Note that the optimum value of (K
s
,K
p
) are set as (20, 8) in subsection 4.1. Due to
limited space, we do not give an in-depth analysis for other parameters.
4 Conclusion and Future Work
In this paper, we propose a novel approach, called group non-negative matrix factorization with natural
categories (GNMFNC). The proposed method is achieved by learning the category-specific topics for
each category as well as shared topics across all categories via a group non-negative matrix factorization
framework. We derive an efficient algorithm for learning the factorization, analyze its complexity, and
96
provide proof of convergence. Experiments show that our proposed approach significantly outperforms
various baseline methods and achieves state-of-the-art performance for question retrieval.
There are some ways in which this research could be continued. First, the optimization of GNMFNC
can be decomposed into many sub-optimization problems, a natural avenue for future research is to
reduce the running time by executing the optimization in a distributed computing environment (e.g.,
MapReduce (Dean et al., 2004)). Second, another combination approach will be used to incorporate the
latent topic match score as a feature in a learning to rank model, e.g., LambdaRank (Burges et al., 2007).
Third, we will try to investigate the use of the proposed approach for other kinds of data sets with larger
categories, such as categorized documents from ODP project.
5
Acknowledgments
This work was supported by the National Natural Science Foundation of China (No. 61333018 and
No. 61303180), the Beijing Natural Science Foundation (No. 4144087), CCF Opening Project of Chi-
nese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund. We thank the
anonymous reviewers for their insightful comments.
References
D. Bernhard and I. Gurevych. 2009. Combining lexical semantic resources with question & answer archives for
translation-based answer finding. In Proceedings of ACL, pages 728-736.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge university press.
C. Boutsidis and E. Gallopoulos. 2008. SVD based initialization: a head start for nonnegative matrix factorization.
Pattern Recognition, 41(4):1350-1362.
C. Burges, R. Ragno, and Q. Le. 2007. Learning to rank with nonsmooth cost function. In Proceedings of NIPS.
L. Cai, G. Zhou, K. Liu, and J. Zhao. 2011. Learning the latent topics for question retrieval in community QA. In
Proceedings of IJCNLP.
X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang. 2009. The use of categorization information in language
models for question retrieval. In Proceedings of CIKM, pages 265-274.
X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A generalized framework of exploring category information for
question retrieval in community question answer archives. In Proceedings of WWW.
J. Dean, S. Ghemanwat, and G. Inc. 2004. Mapreduce: simplified data processing on large clusters. In Proceed-
ings of OSDI.
H. Duan, Y. Cao, C. Lin, and Y. Yu. 2008. Searching questions by identifying questions topics and question focus.
In Proceedings of ACL, pages 156-164.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar questions in large question and answer archives. In Proceed-
ings of CIKM, pages 84-90.
Z. Ji, F. Xu, and B. Wang. 2012. A category-integrated language model for question retrieval in community
question answering. In Proceedings of AIRS, pages 14-25.
H. Kim and H. Park. 2008. Non-negative matrix factorization based on alternating non-negativity constrained
least squares and active set method. SIAM J Matrix Anal Appl, 30(2):713-730.
A. Langville, C. Meyer, R. Albright, J. Cox, and D. Duling. 2006. Initializations for the nonnegative matrix
factorization. In Proceedings of KDD.
J. Lee, S. Kim, Y. Song, and H. Rim. 2008. Bridging lexical gaps between queries and questions on large online
Q&A collections with compact translation models. In Proceedings of EMNLP, pages 410-418.
D. Lee and H. Seung. 2001. Algorithms for non-negative matrix factorization. In Proceedings of NIPS.
5
http://www.dmoz.org/
97
H. Lee and S. Choi. 2009. Group nonnegative matrix factorization for eeg classification. In Proceedings of
AISTATS, pages 320-327.
C. Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural Comput, 19(10):2756-
2779.
Z. Ming, T. Chua, and G. Cong. 2010. Exploring domain-specific term weight in archived question search. In
Proceedings of CIKM, pages 1605-1608.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proceedings of ACL, pages 464-471.
S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at trec-3. In Proceedings
of TREC, pages 109-126.
G. Salton, A. Wong, and C. Yang. 1975. A vector space model for automatic indexing. Communications of the
ACM, 18(11):613-620.
A. Singh. 2012. Entity based q&a retrieval. In Proceedings of EMNLP-CoNLL, pages 1266-1277.
Q. Wang, Z. Cao, J. Xun, and H. Li. 2012. Group matrix factorizaiton for scalable topic modeling. In Proceedings
of SIGIR.
X. Xue, J. Jeon, and W. Croft. 2008. Retrieval models for question and answer archives. In Proceedings of SIGIR,
pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth methods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-based translation model for question retrieval in community
question answer archives. In Proceedings of ACL, pages 653-662.
G. Zhou, F. Liu, Y. Liu, S. He, and J. Zhao. 2013. Statistical machine translation improves question retrieval in
community question answering via matrix factorization. In Proceedings of ACL, pages 852-861.
G. Zhou, Y. Chen, D. Zeng, and J. Zhao. 2013. Toward faster and better retrieval models for question search. In
Proceedings of CIKM, pages 2139-2148.
98
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 677?687, Dublin, Ireland, August 23-29 2014.
Joint Opinion Relation Detection Using One-Class Deep Neural Network
Liheng Xu, Kang Liu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, jzhao}@nlpr.ia.ac.cn
Abstract
Detecting opinion relation is a crucial step for fine-gained opinion summarization. A valid opin-
ion relation has three requirements: a correct opinion word, a correct opinion target and the
linking relation between them. Previous works prone to only verifying two of these requirements
for opinion extraction, while leave the other requirement unverified. This could inevitably intro-
duce noise terms. To tackle this problem, this paper proposes a joint approach, where all three
requirements are simultaneously verified by a deep neural network in a classification scenario.
Some seeds are provided as positive labeled data for the classifier. However, negative labeled
data are hard to acquire for this task. We consequently introduce one-class classification problem
and develop a One-Class Deep Neural Network. Experimental results show that the proposed
joint approach significantly outperforms state-of-the-art weakly supervised methods.
1 Introduction
Opinion summarization aims to extract and summarize customers? opinions from reviews on products or
services (Hu and Liu, 2004; Cardie et al., 2004). With the rapid expansion of e-commerce, the number of
online reviews is growing at a high speed, which makes it impractical for customers to read throughout
large amounts of reviews to choose better products. Therefore, it is imperative to automatically gener-
ate opinion summarization to help customers make more informed purchase decisions, where detecting
opinion relation is a crucial step for opinion summarization.
Before going further, we first introduce some notions. An opinion relation, is a triple o = (s, t, r),
where three factors are involved: s is an opinion word which refers to those words indicating sentiment
polarities; t is an opinion target, which can be any entity or aspect of an entity about which an opinion has
been expressed; r refers to the linking relation between s and t. As in Example 1, s={clear}, t={sceen},
and there is a linking relation between the two words because clear is used to modify screen.
Example 1. This mp3 has a clear screen.
For a valid opinion relation, there are three requirements corresponding to the three factors: (i) the
opinion word indicates sentiment polarity; (ii) the opinion target is related to current domain; (iii) the
opinion word modifies the opinion target. Previous weakly supervised methods often expand a seed set
and identify opinion relation either by co-occurrence statistics (Hu and Liu, 2004; Hai et al., 2012) or
syntactic dependencies (Popescu and Etzioni, 2005; Qiu et al., 2009) following the assumption below.
Assumption 1. Terms that are likely to have linking relation with the seed terms are believed to be
opinion words or opinion targets.
For example, if one has an opinion word seed clear (which satisfies requirement i), and one finds that
it modifies the word screen in Example 1 (which satisfies requirement iii). Then one infers that screen
is an opinion target according to Assumption 1 (whether screen is correct is not checked). However, in
Example 2(a), we can see that good is an opinion word and it modifies thing, but thing is not related to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
677
mp3 domain. If one follows Assumption 1, thing will be mistaken as an opinion target. Similarly, in
Example 2(b), if one uses mp3 to extract another as an opinion word, he may get an objective word.
Example 2. (a) This mp3 has many good things. (b) Just another mp3 I bought.
The reason for the errors above is that Assumption 1 only verifies two requirements for an opinion
relation. Unfortunately, this issue occurs frequently in online reviews. As a result, previous methods
often suffer from these noise terms. To produce more precise opinion summary, it is argued that we shall
follow a more restricted assumption as follows.
Assumption 2. The three requirements: the opinion word, the opinion target and the linking relation
between them, shall be all verified during opinion relation detection.
To make accordance with Assumption 2, this paper proposes a novel joint opinion relation detection
method, where opinion words, opinion targets and linking relations are simultaneously considered in a
classification scenario. Following previous works, we provide a small set of seeds (i.e. opinion words
or targets) for supervision, which are regarded as positive labeled examples for classification. However,
negative labeled examples (i.e. noise terms) are hard to acquire, because we do not know which term
is not an opinion word or target. This leads to One-Class Classification (OCC) problem (Moya et al.,
1993). The key to OCC is semantic similarity measuring between terms, and Deep Neural Network
(DNN) with word embeddings is a powerful tool for handling this problem. We consequently inte-
grate DNN into a OCC classifier and develop a One-Class Deep Neural Network (OCDNN). Concretely,
opinion words/targets/relations are first represented by embedding vectors and then jointly classified.
Experimental results show that the proposed joint method which follows Assumption 2 significantly
outperforms state-of-the-art weakly supervised methods which are based on Assumption 1.
2 Related Work
In opinion relation detection task, previous works often used co-occurrence statistics or syntax informa-
tion to identify opinion relations. For co-occurrence statistical methods, Hu and Liu (2004) proposed a
pioneer research for opinion summarization based on association rules. Popescu and Etzioni (2005) de-
fined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features.
Hai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests
(LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff
and Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang
et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to su-
pervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for
useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double
Propagation which introduced eight heuristic syntactic rules to detect opinion relations.
However, none of the above methods could verify opinion words/targets/relations simultaneously dur-
ing opinion relation detection. To perform joint extraction, various models had been proposed, most of
which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM
(Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, op-
timal models such as Integer Linear Programming (ILP) were also employed to perform joint inference
for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013).
Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless,
most existing joint models rely on full supervision, which have the difficulty of obtaining annotated
training data in practical applications. Also, supervised models that are trained on one domain often fail
to give satisfactory results when shifted to another domain. Our method does not require annotated data.
3 The Proposed Method
To detect opinion relations, previous methods often leverage some seed terms, such as opinion word
seeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion target seeds (Jijkoun et al., 2010; Hai
et al., 2012). These seeds can be used as positive labeled examples to train a classifier. However, it is
hard to get negative labeled examples for this task. Because opinion words or targets are often domain
678
dependent and words that do not bear any sentiment polarity in one domain may be used to express
opinion in another domain. It is also very hard to specify in what case there is no linking relation
between two words.
To deal with this problem, we employ one-class classification, and develop a One-Class Deep Neural
Network (OCDNN) for opinion relation detection. The architecture of OCDNN is shown in Figure 1,
which consists of two levels. The lower level learns feature representations unsupervisedly for opinion
words/targets/relations, where the left component uses word embedding learning to represent opinion
words/targets, and the right component maps linking relations to embedding vectors by a recursive au-
toencoder. Then the upper level uses the learnt features to perform one-class classification.
 
Figure 1: The architecture of OCDNN.
 
Figure 2: An example of recursive autoencoder.
3.1 Opinion Seed Generation
To obtain training data for OCDNN, we shall first get some seed terms as follows.
Opinion Word Seeds. We manually pick 186 domain independent opinion words from SentiWordNet
(Baccianella et al., 2010) as the opinion word seed set SS.
Opinion Target Seeds. Likelihood Ratio Tests (LRT) (Dunning, 1993) used in (Hai et al., 2012) is
employed to generate opinion target seeds. LRT aims to measure how greatly two terms T
i
and T
j
are
associated with each other by sentence-level corpus statistics which is defined as follows,
LRT = 2[logL(p
1
, k
1
, n
1
) + logL(p
2
, k
2
, n
2
)? logL(p, k
1
, n
1
)? logL(p, k
2
, n
2
)] (1)
where k
1
= tf(T
i
, T
j
), k
2
= tf(T
i
,
?
T
j
), k
3
= tf(
?
T
i
, T
j
), k
4
= tf(
?
T
i
,
?
T
j
), tf(?) denotes term frequency;
L(p, k, n) = p
k
(1 ? p)
n?k
, n
1
= k
1
+ k
3
, n
2
= k
2
+ k
4
, p
1
= k
1
/n
1
, p
2
= k
2
/n
2
and p =
(k
1
+ k
2
)/(n
1
+ n
2
). We measure LRT between a domain name (e.g. mp3, hotel, etc.) and all opinion
target candidates. Then N terms with highest LRT scores are added into the opinion target seed set TS.
Linking Relation Seeds. Linking relation can be naturally captured by syntactic dependency, because
it directly models the modification relation between opinion word and opinion target. We employ an
automatic syntactic opinion pattern learning method called Sentiment Graph Walking (Xu et al., 2013)
and get 12 opinion patterns with highest confidence as the linking relation seed set RS.
After seed generation, every opinion relation s
o
= (s
s
, s
t
, s
r
) in review corpus that satisfies s
s
? SS,
s
t
? TS and s
r
? RS is taken as a positive labeled training instance.
3.2 Opinion Relation Candidate Generation
The opinion term candidate set is denoted by C = {SC, TC}, where SC/TC represents opinion
word/target candidate. Following previous works (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu
et al., 2009), we take adjectives or verbs as opinion word candidates, and take nouns or noun phrases as
opinion target candidates. A statistic-based method in Zhu et al. (2009) is used to detect noun phrases.
An opinion relation candidate is denoted by c
o
= (c
s
, c
t
, c
r
), where c
s
? SC, c
t
? TC, and c
r
is a
potential linking relation. To get c
r
, we first get dependency tree of a sentence using Stanford Parser (de
679
Marneffe et al., 2006). Then, the shortest dependency path between a c
s
and a c
t
is taken as a c
r
. To
avoid introducing too many noise candidates, we constrain that there are at most four terms in a c
r
.
3.3 Word Representation by Word Embedding Learning
Word embedding, a.k.a word representation, is a mathematical object associated with each word, which
is often used in a vector form, where each dimension?s value corresponds to a feature and might even
have a semantic or grammatical interpretation (Turian et al., 2010). By word embedding learning, words
are embedded into a hyperspace, where two words that are more semantically similar to each other are
located closer. This characteristic is precisely what we want, because the key to one-class classification
is semantic similarity measuring (illustrated in Section 3.5).
For word representation, we use a matrix LT ? R
n?|V
w
|
, where i-th column represents the embedding
vector for term t
i
, n is the size of embedding vector and V
w
is the vocabulary of LT . Therefore, we
can denote t
i
by a binary vector b
i
? R
|V
w
|
and get its embedding vector by x
i
= LTb
i
. The training
criterion for word embeddings is,
?
? = argmin
?
?
c?C
?
v?V
w
max{0, 1? s
?
(c) + s
?
(v)} (2)
where ? is the parameters of neural network used for training. See Collobert et al. (2011) for the detailed
implementation.
3.4 Linking Relation Representation by Using Recursive Autoencoder
The goal of this section is to represent the linking relation between an opinion word and an opinion target
by a n-element vector as we do during word representation. Specifically, we combine embedding vectors
of words in a linking relation by a recursive autoencoder (Socher et al., 2011) according to syntactic
dependency structure. In this way, linking relations are no longer limited to the initial seeds during
classification, because linking relations that are similar to the seed relations will have similar vector
representations.
Figure 2 shows a linking relation representation process by an example: too loud to listen to the player.
First, we get its dependency path between the opinion word c
s
:loud and the opinion target c
t
:player.
Then c
s
and c
t
are replaced by wildcards [SC] and [TC] because they are not concerned in the linking
relation. The dash line box in Figure 2 shows a standard autoencoder, which is a three-layer neural
network, where the number of nodes in input layer is equal to that of output layer. It takes two n-element
vectors as input and compresses semantics of the two vectors into one n-element vector in hidden layer
by,
y = f(W
(dep)
[x
1
;x
2
] + b), W
(dep)
=
1
2
[I
1
; I
2
; I
b
] +  (3)
where [x
1
;x
2
] is the concatenation of the two input vectors and f is the sigmoid function; W
(dep)
is a
parameter matrix that is chosen according to the dependency relation between x
1
and x
2
(In the case of
y
1
, W
(dep)
= W
(xcomp)
), which is initialized by I
i
, where I
i
is a n ? n unit matrix, I
b
is a n-element
null vector, and  is sampled from a uniform distribution U [?0.001, 0.001] (Socher et al., 2013). Then
W
(dep)
are updated during training. The training criterion of autoencoder is to minimize Euclidean
distance between the original input and its output,
E
rae
= ||[x
1
;x
2
]? [x
?
1
;x
?
2
]||
2
(4)
where [x
?
1
;x
?
2
] = W
(out)
y and W
(out)
is initialized by W
(dep)
T
.
We always start the combination process from [SC] and it is repeated along the dependency path. For
example, the result vector y
1
of the first combination is used as the input vector when computing y
2
.
Finally, the linking relation is represented by a n-element vector (the green vector in Figure 2).
680
3.5 One-Class Classification for Opinion Relation Detection
We represent an opinion relation candidate c
o
= (c
s
, c
t
, c
r
) by a vector v
o
= [v
s
; v
t
; v
r
], which is
a concatenation of the opinion word embedding v
s
, the opinion target embedding v
t
and the linking
relation embedding v
r
. Then v
o
is feed to the upper level autoencoder in Figure 1.
To perform one-class classification, the number of nodes in the hidden layer of the upper level autoen-
coder is constrained to be smaller than that of the input layer. By using such a ?bottleneck? network
structure, characteristics of the input are first compressed into the hidden layer and then reconstructed
by the output layer (Japkowicz et al., 1995). Concretely, characteristics of positive labeled opinion rela-
tions are first compressed into the hidden layer, and then the autoencoder should be able to adequately
reconstruct positive instances in the output layer, but should fail to reconstruct negative instances which
present different characteristics from positive instances. Therefore, the detection of opinion relation is
equivalent to assessing how well a candidate is reconstructed by the autoencoder. As the input vector
v
o
consists of representations for opinion words/targets/relations, characteristics of the three factors are
jointly compressed by one hidden layer. Either false opinion word/target/relation will lead to failure of
reconstruction. Consequently, our approach follows Assumption 2.
For opinion relation detection, candidates with reconstruction error scores that are smaller than a
threshold ? are classified as positive. Determining the exact value of ? is very difficult. Inspired by other
one-class approaches (Liu et al., 2002; Manevitz and Yousef, 2007), we introduce some negative opinion
terms to help to estimate ?.
1
Although negative instances are hard to acquire, Xu et al. (2013) show that
a set of general nouns (such as thing, one, etc., we denote them by GN ) seldom appear to be opinion
targets. One the other hand, we create a 50-opinion-word validation set SV from SentiWordNet.
To estimate ?, we first introduce a positive proportion (pp) score,
pp(t) = tf
+
(t)/tf(t), t ? PE, PE = {c
o
|E
r
(c
o
) < ?} (5)
where PE denotes the opinion relations that are classified as positive, E
r
(?) is the reconstruction error
of OCDNN and tf
+
(?) is the frequency of term in PE. Then an error function E
?
is minimized, which
balances between the proportion of non-target terms (GN ) in PE (which shall be as small as possible)
and the proportion of opinion words in validation set (SV ) in PE (which shall be as large as possible).
E
?
=
?
t?GN?PE
[pp(t)? 0]
2
+
?
s?SV ?PE
[pp(s)? 1]
2
(6)
3.6 Opinion Target Expansion
We apply bootstrapping to iteratively expand opinion target seeds. It is because the vocabulary of seed
set is limited, which cannot fully represent the distribution of opinion targets. So we expand opinion
target seeds in a self-training manner to alleviate this issue. After training OCDNN, all opinion relation
candidates are classified, and opinion targets are ranked in descent order by,
s(t) = log tf(t)? pp(t). (7)
Then, top M candidates are added into the target seed set TS for the next training iteration.
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets. Three real world datasets are selected for evaluation. The first one is called Customer Review
Dataset (CRD)
2
which contains reviews on five products (denoted by D1 to D5). The second is a bench-
mark dataset (Wang et al., 2011) on MP3 and Hotel
3
. The last one is crawled from www.amazon.com,
which involves Mattress and Phone. Two annotating criteria are applied.
1
This is not in contradiction with OCC problem, because these negative examples are NOT used during training.
2
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
3
http://timan.cs.uiuc.edu/downloads.html
681
Annotation 1 is used to evaluate opinion words/targets extraction. Firstly, 10,000 sentences are ran-
domly selected from reviews and all possible terms are extracted along with their contexts. Then, anno-
tators are required to judge whether each term is an opinion word or an opinion target.
Annotation 2 is used to evaluate intra-sentence opinion relation detection. Annotators are required to
carefully read through each sentence and find out every opinion relation, which consists of an opinion
word, an opinion target, as well as the linking relation between them. The annotation is very labor-
intensive, so only 5,000 sentences are annotated for MP3 and Hotel.
Two annotators were required to annotate following the criteria above. When conflicts happened, a
third annotator would make the final judgment. Note that Annotation 1 and Annotation 2 were annotated
by two different groups. Detailed information of the annotated datasets are shown in Table 1. Further-
more, the kappa values between Annotation 1 and Annotation 2 are 0.88 for opinion words and 0.84 for
opinion targets, showing highly substantial agreement.
Domain #OW #OT Kappa OW Kappa OT
Hotel 434 1,015 0.72 0.67
MP3 559 1,158 0.69 0.65
Mattress 366 523 0.67 0.62
Phone 391 862 0.68 0.64
(a) Annotation 1
Domain #LR #OW #OT Kappa LR
Hotel 2,196 317 735 0.62
MP3 2,328 342 791 0.61
(b) Annotation 2
Table 1: The detailed information of Annotations. OW/OT/LR stands for opinion words/opinion tar-
gets/linking relations. The Kappa-values are calculated by using exact matching metric for Annotation 1
and overlap matching metric for Annotation 2.
Evaluation Metrics. We perform evaluation in terms of Precision(P), Recall(R) and F-measure(F)
according to exact and overlap matching metrics (Wiebe et al., 2005). The exact metric is used to
evaluate opinion word/target extraction, which requires exact string match. And the overlap metric is
used to evaluate opinion relation detection, where an extracted opinion relation is regarded as correct
when both the opinion word and the opinion target in it overlap with the gold standard.
4
Evaluation Settings. Four state-of-the-art weakly supervised approaches are selected as competi-
tors. Two are co-occurrence statistical methods and two are syntax-based methods, all of which follow
Assumption 1.
AdjRule extracts opinion words/targets by using adjacency rules (Hu and Liu, 2004).
LRTBOOT is a bootstrapping algorithm which employs Likelihood Ratio Tests (Dunning, 1993) as
the co-occurrence statistical measure (Hai et al., 2012).
DP denotes the Double Propagation algorithm (Qiu et al., 2009).
DP-HITS is an enhanced version of DP proposed by Zhang et al. (2010), which ranks terms by
s(t) = log tf(t)? importance(t) (8)
where importance(t) is estimated by the HITS algorithm (Kleinberg, 1999).
OCDNN is the proposed method. The target seed size N = 40, the opinion targets expanded in each
iteration M = 20, and the max bootstrapping iteration number is X = 10. The representation learning
in lower level of OCDNN is trained on the whole corpus, while the test data are the same for all settings.
All results of OCDNN are taken by average performance over five runs with randomized parameters.
4.2 OCDNN vs. the State-of-the-art
We compare OCDNN with state-of-the-art methods for opinion words/targets extraction. In OCDNN,
Eq. 7 is used to rank opinion words/targets. The results on CRD and the four domains are shown in
Table 2 and Table 3. DP-HITS does not extract opinion words so their results for opinion words are not
taken into account.
4
Determining the exact boundaries of opinion terms is hard even for human (Wiebe et al., 2005), so we use this relaxation.
682
Opinion Targets
Method
D1 D2 D3 D4 D5
Avg.
P R F P R F P R F P R F P R F F
AdjRule 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
DP-HITS 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
LRTBOOT 0.77 0.87 0.82 0.74 0.90 0.81 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.80 0.81
OCDNN 0.83 0.82 0.82 0.86 0.85 0.85 0.86 0.87 0.86 0.78 0.84 0.81 0.89 0.85 0.87 0.84
Opinion Words
AdjRule 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
LRTBOOT 0.60 0.79 0.68 0.52 0.82 0.64 0.60 0.76 0.67 0.56 0.70 0.62 0.66 0.71 0.68 0.66
OCDNN 0.64 0.77 0.70 0.63 0.79 0.70 0.66 0.73 0.69 0.68 0.70 0.69 0.70 0.69 0.69 0.70
Table 2: Results of opinion terms extraction on Customer Review Dataset.
Opinion Targets
Method
MP3 Hotel Mattress Phone
Avg.
P R F P R F P R F P R F F
AdjRule 0.53 0.55 0.54 0.55 0.57 0.56 0.50 0.60 0.55 0.52 0.51 0.51 0.54
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.55 0.60 0.57 0.60 0.53 0.56 0.59
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.55 0.67 0.60 0.62 0.64 0.63 0.63
LRTBOOT 0.60 0.77 0.67 0.59 0.78 0.67 0.55 0.78 0.65 0.57 0.76 0.65 0.66
OCDNN 0.70 0.68 0.69 0.71 0.70 0.70 0.63 0.69 0.66 0.69 0.68 0.68 0.68
Opinion Words
AdjRule 0.48 0.65 0.55 0.51 0.68 0.58 0.51 0.68 0.58 0.48 0.61 0.54 0.56
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.54 0.68 0.60 0.55 0.59 0.57 0.60
LRTBOOT 0.52 0.69 0.59 0.54 0.74 0.62 0.51 0.73 0.60 0.50 0.68 0.58 0.60
OCDNN 0.68 0.65 0.66 0.70 0.68 0.69 0.59 0.70 0.64 0.63 0.59 0.61 0.65
Table 3: Results of opinion terms extraction on the four domains.
From Table 2, we can see that our method outperforms co-occurrence-based methods AdjRule and
LRTBOOT, but achieves comparable or a little worse results than syntax-based methods DP and DP-
HITS. This is because CRD is quite small, which only contains several hundred sentences for each prod-
uct review set. In this case, methods based on careful-designed syntax rules have superiority over those
based on statistics (Liu et al., 2013). For results on larger datasets shown in Table 3, our method out-
performs all of the competitors. Comparing OCDNN with DP-HITS, the two approaches use similar
term ranking metrics (Eq. 7 and Eq. 8), but OCDNN significantly outperforms DP-HITS. Therefore, the
positive proportion score estimated by OCDNN is more effective than the importance score in DP-HITS.
Comparing OCDNN with LRTBOOT, we find that LRTBOOT achieves better recall but lower precision.
This is because LRTBOOT follows Assumption 1 during bootstrapping, which suffers a lot from error
propagation, while our joint classification approach effectively alleviates this issue. We will discuss the
impact of error propagation in detail later.
4.3 Assumption 1 vs. Assumption 2
This section evaluates intra-sentence opinion relation detection, which is more useful for practical appli-
cations. It also reflects the impacts of Assumption 1 and Assumption 2. The results are shown in Table
4 and Table 5, where OCDNN significantly outperforms all competitors. The average improvement of
F-measure over the best competitor is 6% on CRD and 9% on Hotel and MP3.
As Assumption 1 only verifies two of the requirements in an opinion relation, it would inevitably
introduce noise terms during extraction. For syntax-based method DP, it extracts many false opinion
relations such as good thing and nice one (where thing and one are false opinion targets) or objective
expressions like another mp3 and every mp3 (which contain false opinion words another and every). For
co-occurrence statistical methods AdjRule and LRTBOOT, it is very hard to deal with ambiguous linking
relations. For example, in phrase this mp3 is very good except the size, co-occurrence statistical methods
could hardly tell which opinion target does good modify (mp3 or size). Our method follows Assumption
683
Method
D1 D2 D3 D4 D5
Avg.
P R F P R F P R F P R F P R F F
AdjRule 0.51 0.66 0.58 0.53 0.63 0.58 0.50 0.61 0.55 0.48 0.60 0.53 0.50 0.61 0.55 0.56
DP 0.66 0.63 0.64 0.68 0.60 0.64 0.69 0.62 0.65 0.66 0.57 0.61 0.67 0.60 0.63 0.64
LRTBOOT 0.53 0.70 0.60 0.57 0.72 0.64 0.55 0.69 0.61 0.52 0.70 0.60 0.55 0.68 0.61 0.61
OCDNN 0.76 0.66 0.71 0.74 0.67 0.70 0.77 0.67 0.72 0.70 0.65 0.67 0.77 0.66 0.71 0.70
Table 4: Results of opinion relation detection on Customer Review Dataset.
Method
MP3 Hotel
Avg.
P R F P R F F
AdjRule 0.49 0.55 0.52 0.45 0.53 0.49 0.50
DP 0.63 0.51 0.56 0.59 0.50 0.54 0.55
LRTBOOT 0.54 0.63 0.58 0.50 0.60 0.55 0.56
OCDNN 0.73 0.60 0.66 0.70 0.59 0.64 0.65
Table 5: Results of opinion relation detection on the two domains.
2, which verifies all three requirements for opinion word/target/relation in an opinion relation, so the
above errors are greatly reduced. Therefore, Assumption 2 is more reasonable than Assumption 1.
4.4 The Effect of Joint Classification
We evaluate the three bootstrapping methods (DP, LRTBOOT and OCDNN) for opinion target expansion.
The precision of each iteration is shown in Figure 3. We can see that DP and LRTBOOT gradually suffer
from error propagation and the precision drops quickly along with the number of iteration increases. For
OCDNN, although error propagation is inevitable, the precision curve retains at a high level. Therefore,
the joint approach produces more precise results.
For more detailed analysis, we give a variation of the proposed method named 3NN, which uses
3 individual autoencoders to classify opinion words/targets/relations separately. An opinion relation
candidate is classified as positive only when the three factors are all classified as positive. Then opinion
relations are ranked by the sum of reconstruction scores of the three factors. In the results of opinion
relation detection, when the recall is fixed at 0.6, the precisions of 3NN are 0.67 for MP3 and 0.65
for Hotel, while the precisions of OCDNN are 0.73 for MP3 and 0.70 for Hotel. Therefore, OCDNN
achieves much better performance than 3NN.
An example may explain the reason of why 3NN gets worse performance. In our experiment on Hotel,
a false opinion relation happy day is misclassified as positive by 3NN. It is because the word day has
a small reconstruction score in 3NN. At the same time, happy is a correct opinion word, so the whole
expression happy day also has a small reconstruction score and then be misclassified. In contrast, the
reconstruction score of happy day from OCDNN is quite large so the phrase is dropped. The reason
is that the joint approach captures the semantic of a whole phrase rather than its single components.
Therefore, it is more reasonable.
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(a) MP3
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(b) Hotel
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(c) Mattress
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(d) Phone
Figure 3: Precision (y-axis) of opinion target seed expansion at each bootstrapping iteration (x-axis).
684
5 Conclusion and Future Work
This paper proposes One-Class Deep Neural Network for joint opinion relation detection in one-class
classification scenario, where opinion words/targets/relations are simultaneously verified during classifi-
cation. Experimental results show the proposed method significantly outperforms state-of-the-art weakly
supervised methods that only verify two factors in an opinion relation.
In future work, we plan to adapt our method and make it be capable of capturing implicit opinion
relations.
Acknowledgement
This work was sponsored by the National Natural Science Foundation of China (No. 61202329 and No.
61333018) and CCF-Tencent Open Research Fund.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical re-
source for sentiment analysis and opinion mining. Seventh conference on International Language Resources
and Evaluation, pages 2200?2204.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings
of the 20th international joint conference on Artifical intelligence, IJCAI?07, pages 2683?2688, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Claire Cardie, Janyce Wiebe, Theresa Wilson, and Diane Litman. 2004. Low-level annotations and summary
representations of opinions for multi-perspective question answering.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 431?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language
Technology. The Stanford Natural Language Processing Group.
Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Comput. Linguist., 19(1):61?
74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One seed to find them all: mining opinion features via association.
In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM
?12, pages 255?264, New York, NY, USA. ACM.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 168?177.
Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting with
conditional random fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 1035?1045, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nathalie Japkowicz, Catherine Myers, and Mark Gluck. 1995. A novelty detection approach to classification. In
Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI?95, pages
518?523, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Valentin Jijkoun, Maarten de Rijke, and Wouter Weerkamp. 2010. Generating focused topic-specific sentiment
lexicons. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL
?10, pages 585?594, Stroudsburg, PA, USA. Association for Computational Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized hmm-based learning framework for web opinion mining. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ?09, pages 465?472.
685
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604?632, September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1065?1074,
Prague, Czech Republic, June. Association for Computational Linguistics.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware
review mining and summarization. In Proceedings of the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 653?661, Stroudsburg, PA, USA. Association for Computational Linguistics.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li. 2002. Partially supervised classification of text documents.
In Proceedings of the Nineteenth International Conference on Machine Learning, ICML ?02, pages 387?394,
San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion tar-
gets from online reviews. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1754?1763, August.
Larry Manevitz and Malik Yousef. 2007. One-class document classification via neural networks. Neurocomput-
ing, 70(7C9):1466?1481.
Mary M. Moya, Mark W. Koch, and Larry D. Hostetler. 1993. One-class classifier networks for target recognition
applications. In Proceedings world congress on neural networks, pages 797?801.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceed-
ings of the conference on Human Language Technology and Empirical Methods in Natural Language Process-
ing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI?09, pages
1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings
of the 2003 conference on Empirical methods in natural language processing, EMNLP ?03, pages 105?112,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 151?161, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 455?465, Sofia, Bulgaria, August. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 384?394, Stroudsburg, PA, USA. Association for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword
supervision. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 618?626.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 -
Volume 3, EMNLP ?09, pages 1533?1541, Stroudsburg, PA, USA. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
686
Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1640?1649, Sofia, Bulgaria, August. Association for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain. 2010. Extracting and ranking product features
in opinion documents. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, COLING ?10, pages 1462?1470, Stroudsburg, PA, USA. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the
15th ACM international conference on Information and knowledge management, CIKM ?06, pages 43?50, New
York, NY, USA. ACM.
687
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1331?1340, Dublin, Ireland, August 23-29 2014.
Sentiment Classification with Graph Co-Regularization
Guangyou Zhou, Jun Zhao, and Daojian Zeng
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao,djzeng}@nlpr.ia.ac.cn
Abstract
Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or neg-
ative) of user-generated sentiment data (e.g., reviews, blogs). To obtain sentiment classifica-
tion with high accuracy, supervised techniques require a large amount of manually labeled data.
The labeling work can be time-consuming and expensive, which makes unsupervised (or semi-
supervised) sentiment analysis essential for this application. In this paper, we propose a novel
algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF), from the
geometric perspective. GNMTF assumes that if two words (or documents) are sufficiently close
to each other, they tend to share the same sentiment polarity. To achieve this, we encode the
geometric information by constructing the nearest neighbor graphs, in conjunction with a non-
negative matrix tri-factorization framework. We derive an efficient algorithm for learning the
factorization, analyze its complexity, and provide proof of convergence. Our empirical study on
two open data sets validates that GNMTF can consistently improve the sentiment classification
accuracy in comparison to the state-of-the-art methods.
1 Introduction
Recently, sentiment classification has gained a wide interest in natural language processing (NLP) com-
munity. Methods for automatically classifying sentiments expressed in products and movie reviews can
roughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis. Super-
vised techniques have been proved promising and widely used in sentiment classification (Pang et al.,
2002; Pang and Lee, 2008; Liu, 2012). However, the performance of these methods relies on manually
labeled training data. In some cases, the labeling work may be time-consuming and expensive. This
motivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised)
paradigm.
A traditional way to perform unsupervised sentiment analysis is the lexicon-based method (Turney,
2002; Taboada et al., 2011). Lexicon-based methods employ a sentiment lexicon to determine overall
sentiment orientation of a document. However, it is difficult to define a universally optimal sentiment
lexicon to cover all words from different domains (Lu et al., 2011a). Besides, most semi-automated
lexicon-based methods yield unsatisfactory lexicons, with either high coverage and low precision or
vice versa (Ng et al., 2006). Thus it is challenging for lexicon-based methods to accurately identify
the overall sentiment polarity of users generated sentiment data. Recently, Li et al. (2009) proposed a
constrained non-negative matrix tri-factorization (CNMTF) approach to sentiment classification, with
a domain-independent sentiment lexicon as prior knowledge. Experimental results show that CNMTF
achieves state-of-the-art performance.
From the geometric perspective, the data points (words or documents) may be sampled from a distribu-
tion supported by a low-dimensional manifold embedded in a high-dimensional space (Cai et al., 2011).
This geometric structure, meaning that two words (or documents) sufficiently close to each other tend to
share the same sentiment polarity, should be preserved during the matrix factorization. Research studies
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1331
have shown that learning performance can be significantly enhanced in many real applications (e.g., text
mining, computer vision, etc.) if the geometric structure is exploited (Roweis and Saul, 2000; Tenen-
baum et al., 2000). However, CNMTF fails to exploit the geometric structure, it is not clear whether this
geometric information is useful for sentiment classification, which remains an under-explored area. This
paper is thus designed to fill the gap.
In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-
factorization (GNMTF). We construct two affinity graphs to encode the geometric information under-
lying the word space and the document space, respectively. Intuitively, if two words or documents are
sufficiently close to each other, they tend to share the same sentiment polarity. Taking these two graphs
as co-regularization for the non-negative matrix tri-factorization, leading to the better sentiment polarity
prediction which respects to the geometric structures of the word space and document space. We also de-
rive an efficient algorithm for learning the tri-factorization, analyze its complexity, and provide proof of
convergence. Empirical study on two open data sets shows encouraging results of the proposed method
in comparison to state-of-the-art methods.
The remainder of this paper is organized as follows. Section 2 introduces the basic concept of matrix
tri-factorization. Section 3 describes our graph co-regularized non-negative matrix tri-factorization (GN-
MTF) for sentiment classification. Section 4 presents the experimental results. Section 5 introduces the
related work. In section 6, we conclude the paper and discuss future research directions.
2 Preliminaries
2.1 Non-negative Matrix Tri-factorization
Li et al. (2009) proposed a matrix factorization based framework for unsupervised (or semi-supervised)
sentiment analysis. The proposed framework is built on the orthogonal non-negative matrix tri-
factorization (NMTF) (Ding et al., 2006). In these models, a term-document matrixX = [x
1
, ? ? ? ,x
n
] ?
R
m?n
is approximated by three factor matrices that specify cluster labels for words and documents by
solving the following optimization problem:
min
U,H,V?0
O =
?
?
X?UHV
T
?
?
2
F
+ ?
1
?
?
U
T
U? I
?
?
2
F
+ ?
2
?
?
V
T
V ? I
?
?
2
F
(1)
where ?
1
and ?
2
are the shrinkage regularization parameters, U = [u
1
, ? ? ? ,u
k
] ? R
m?k
+
is the word-
sentiment matrix, V = [v
1
, ? ? ? ,v
n
] ? R
n?k
+
is the document-sentiment matrix, and k is the number of
sentiment classes for documents. Our task is polarity sentiment classification (positive or negative), i.e.,
k = 2. For example,V
i1
= 1 (orU
i1
= 1) represents that the sentiment polarity of document i (or word
i) is positive, andV
i2
= 1 (orU
i2
= 1) represents that the sentiment polarity of document i (or word i)
is negative. V
i?
= 0 (orU
i?
= 0) represents unknown, i.e., the document i (or word i) is neither positive
or negative. H ? R
k?k
+
provides a condensed view of X; ? ? ?
F
is the Frobenius norm and I is a k ? k
identity matrix with all entries equal to 1. Based on the shrinkage methodology, we can approximately
satisfy the orthogonality constraints forU andV by preventing the second and third terms from getting
too large.
2.2 Constrained NMTF
Lexical knowledge in the form of the polarity of words in the lexicon can be introduced in matrix tri-
factorization. By partially specifying word polarity viaU, the lexicon influences the sentiment prediction
V over documents. Following the literature (Li et al., 2009), let U
0
represent lexical prior knowledge
about sentiment words in the lexicon, e.g., if word i is positive (U
0
)
i1
= 1 while if it is negative
(U
0
)
i2
= 1, and if it does not exist in the lexicon (U
0
)
i?
= 0. Li et al. (2009) also investigated that we
had a few documents manually labeled for the purpose of capturing some domain-specific connotations.
LetV
0
denote the manually labeled documents, if the document expresses positive sentiment (V
0
)
ii
= 1,
and (V
0
)
i2
= 1 for negative sentiment. Therefore, the semi-supervised learning with lexical knowledge
can be written as:
min
U,H,V?0
O + ?Tr
[
(U?U
0
)
T
C
u
(U?U
0
)
]
+ ?Tr
[
(V ?V
0
)
T
C
v
(V ?V
0
)
]
(2)
1332
where Tr(?) denotes the trace of a matrix, ? > 0 and ? > 0 are the parameters which control the
contribution of lexical prior knowledge and manually labeled documents. C
u
? {0, 1}
m?m
is a diagonal
matrix whose entry C
u
ii
= 1 if the category of the i-th word is known and C
u
ii
= 0 otherwise. C
v
?
{0, 1}
n?n
is a diagonal matrix whose entry C
v
ii
= 1 if the category of the i-th document is labeled and
C
v
ii
= 0 otherwise.
3 Graph Co-regularized Non-negative Matrix Tri-factorization
In this section, we introduce our proposed graph co-regularized non-negative matrix tri-factorization
(GNMTF) algorithm which avoids this limitation by incorporating the geometrically based co-
regularization.
3.1 Model Formulation
Based on the manifold assumption (Belkin and Niyogi, 2001), if two documents x
i
and x
j
are sufficiently
close to each other in the intrinsic geometric of the documents distribution, then their sentiment polarity
v
i
and v
j
should be close. In order to model the geometric structure, we construct a document-document
graphG
v
. In the graph, nodes represent documents in the corpus and edges represent the affinity between
the documents. The affinity matrixW
v
? R
n?n
of the graph G
v
is defined as
W
v
ij
=
{
cos(x
i
,x
j
) if x
i
? N
p
(x
j
) or x
j
? N
p
(x
i
)
0 otherwise
(3)
where N
p
(x
i
) represents the p-nearest neighbors of document x
i
. Many matrices, e.g., 0-1 weighting,
textual similarity and heat kernel weighting (Belkin and Niyogi, 2001), can be used to obtain nearest
neighbors of a document, and further define the affinity matrix. Since W
v
ij
in our paper is only for
measuring the closeness, we only use the simple textual similarity and do not treat the different weighting
schemes separately due to the limited space. For further information, please refer to (Cai et al., 2011).
Preserving the geometric structure in the document space is reduced to minimizing the following loss
function:
R
v
=
1
2
n
?
i,j=1
?
?
v
i
? v
j
?
?
2
2
W
v
ij
=
n
?
i=1
v
T
i
v
i
D
v
ii
?
n
?
i,j=1
v
T
i
v
j
W
v
ij
= Tr(V
T
D
v
V)? Tr(V
T
W
v
V) = Tr(V
T
L
v
V)
(4)
whereD
v
? R
n?n
is a diagonal matrix whose entries are column (or row, sinceD
v
is symmetric) sums
ofW
v
,D
v
ii
=
?
n
j=1
W
v
ij
, and L
v
= D
v
?W
v
is the Laplacian matrix (Chung, 1997) of the constructed
graph G
v
.
Similarly to document-document geometric structure, if two words w
i
= [x
i1
, ? ? ? ,x
in
] and w
j
=
[x
j1
, ? ? ? ,x
jn
] are sufficiently close to each other in the intrinsic geometric of the words distribution,
then their sentiment polarity u
i
and u
j
should be close. In order to model the geometric structure in the
word space, we construct a word-word graph G
u
. In the graph, nodes represent distinct words and edges
represent the affinity between words. The affinity matrixW
u
? R
m?m
of the graph G
u
is defined as
W
u
ij
=
{
cos(w
i
,w
j
) ifw
i
? N
p
(w
j
) orw
j
? N
p
(w
i
)
0 otherwise
(5)
where N
p
(w
j
) represents the p-nearest neighbor of word w
j
. Here, we represent a term w
j
as a docu-
ment vector [x
j1
, ? ? ? ,x
jn
]. To measure the closeness of two words, a common way is to calculate the
similarity of their vector representations. Although there are several ways (e.g., co-occurrence infor-
mation, semantic similarity computed by WordNet, Wikipedia, or search engine have been empirically
studied in NLP literature (Hu et al., 2009)) to define the affinity matrixW
u
, we do not treat the different
ways separately and leave this investigation for future work.
Preserving the geometric structure in the word space is reduced to minimizing the following loss
function:
R
u
=
1
2
m
?
i,j=1
?
?
u
i
? u
j
?
?
2
2
W
u
ij
= Tr(U
T
L
u
U) (6)
1333
where L
u
= D
u
?W
u
is the Laplacian matrix of the constructed graph G
u
, and D
u
? R
m?m
is a
diagonal matrix whose entries areD
u
ii
=
?
m
j=1
W
u
ij
.
Finally, we treat unsupervised (or semi-supervised) sentiment classification as a clustering problem,
employing lexical prior knowledge and partial manually labeled data to guide the learning process. More-
over, we introduce the geometric structures from both document and word sides as co-regularization.
Therefore, our proposed unsupervised (or semi-supervised) sentiment classification framework can be
mathematically formulated as solving the following optimization problem:
min
U,H,V?0
L =
?
?
X?UHV
T
?
?
2
F
+ ?
1
?
?
U
T
U? I
?
?
2
F
+ ?
2
?
?
V
T
V ? I
?
?
2
F
+ ?Tr
[
(U?U
0
)
T
C
u
(U?U
0
)
]
+ ?Tr(U
T
L
u
U)
+ ?Tr
[
(V ?V
0
)
T
C
v
(V ?V
0
)
]
+ ?Tr(V
T
L
v
V)
(7)
where ? > 0 and ? > 0 are parameters which control the contributions of document space and word
space geometric information, respectively. With the optimization results, the sentiment polarity of a new
document x
i
can be easily inferred by f(x
i
) = argmax
j?{p, n}
V
ij
.
3.2 Learning Algorithm
We present the solution to the GNMTF optimization problem in equation (7) as the following theorem.
The theoretical aspects of the optimization are presented in the next subsection.
Theorem 3.1. Updating U, H and V using equations (8)?(10) will monotonically decrease the objec-
tive function in equation (7) until convergence.
U? U ?
[
XVH
T
+ ?
1
U+ ?C
u
U
0
+ ?W
u
U
]
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
(8)
H? H ?
[
U
T
XV
]
[
U
T
UHV
T
V
]
(9)
V? V ?
[
X
T
UH+ ?
2
V + ?C
v
V
0
+ ?W
v
V
]
[
VH
T
U
T
UH+ ?
2
VV
T
V + ?C
v
V + ?D
v
V
]
(10)
where operator ? is element-wise product and
[?]
[?]
is element-wise division.
Based on Theorem 3.1, we note that the multiplicative update rules given by equations (8)?(10) are
obtained by extending the updates of standard NMTF (Ding et al., 2006). A number of techniques can
be used here to optimize the objective function in equation (7), such as alternating least squares (Kim
and Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin,
2007). Nonetheless, the multiplicative updates derived in this paper has reasonably fast convergence
behavior as shown empirically in the experiments.
3.3 Theoretical Analysis
In this subsection, we give the theoretical analysis of the optimization, convergence and computational
complexity. Without loss of generality, we only show the optimization ofU and formulate the Lagrange
function with constraints as follows:
L(U) =
?
?
X?UHV
T
?
?
2
F
+ ?
1
?
?
U
T
U? I
?
?
2
F
+ ?Tr
[
(U?U
0
)
T
C
u
(U?U
0
)
]
+ Tr(?U
T
)
(11)
where ? is the Lagrange multiplier for the nonnegative constraintU ? 0.
The partial derivative of L(U) w.r.t. U is
?
U
L(U) = ?2XVH
T
+ 2UHV
T
VH
T
+ 2?
1
UU
T
U? 2?
1
U
+ 2?C
u
U? 2?C
u
U
0
+ 2?D
u
U? 2?W
u
U+?
1334
Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ??U = 0, we can
obtain
?
U
L(U) ?U =
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
?U
?
[
XVH
T
+ ?
1
U+ ?C
u
U
0
+ ?W
u
U
]
?U = 0
This leads to the update rule in equation (8). Following the similar derivations as shown above, we
can obtain the updating rules for all the other variables H and V in GNMTF optimization, as shown in
equations (9) and (10).
3.3.1 Convergence Analysis
In this subsection, we prove the convergence of multiplicative updates given by equations (8)?(10). We
first introduce the definition of auxiliary function as follows.
Definition 3.1. F(Y,Y
?
) is an auxiliary function for L(Y) if L(Y) ? F(Y,Y
?
) and equality holds if
and only if L(Y) = F(Y,Y).
Lemma 3.1. (Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under the
updateY
(t+1)
= argmin
Y
F(Y,Y
(t)
)
Proof. By Definition 3.1, L(Y
(t+1)
) ? F(Y
(t+1)
,Y
(t)
) ? F(Y
(t)
,Y
(t)
) = L(Y
(t)
)
Theorem 3.2. Let function
F(U
ij
,U
(t)
ij
) = L(U
(t)
ij
) + L
?
(U
(t)
ij
)(U
ij
?U
(t)
ij
)
+
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
ij
U
ij
(
U
ij
?U
(t)
ij
)
(12)
be a proper auxiliary function for L(U
ij
), where L
?
(U
ij
) = [?
U
L(U)]
ij
is the first-order derivatives
of L(U
ij
) with respect toU
ij
.
Theorem 3.2 can be proved similarly to (Ding et al., 2006). Due to limited space, we omit the details
of the validation. Based on Lemmas 3.1 and Theorem 3.2, the update rule for U can be obtained by
minimizing F(U
(t+1)
ij
,U
(t)
ij
). When setting ?
U
(t+1)
ij
F(U
(t+1)
ij
,U
(t)
ij
), we can obtain
U
(t+1)
ij
= U
(t)
ij
[
XVH
T
+ ?
1
U+ ?C
u
U
0
+ ?W
u
U
]
ij
[
UHV
T
VH
T
+ ?
1
UU
T
U+ ?C
u
U+ ?D
u
U
]
ij
By Lemma 3.1 and Theorem 3.2, we have L(U
(0)
) = F(U
(0)
,U
(0)
) ? F(U
(1)
,U
(0)
) ?
F(U
(1)
,U
(1)
) = L(U
(1)
) ? ? ? ? ? L(U
(Iter)
), where Iter denotes the number of iteration number.
Therefore, U is monotonically decreasing. Since the objective function L is lower bounded by 0, the
correctness and convergence of Theorem 3.1 is validated.
3.3.2 Time Complexity Analysis
In this subsection, we discuss the time computational complexity of the proposed algorithm GNMTF.
Besides expressing the complexity of the algorithm using big O notation, we also count the number of
arithmetic operations to provide more details about running time. We show the results in Table 1, where
m ? k and n ? k.
Based on the updating rules summarized in Theorem 3.1, it it not hard to count the arithmetic operators
of each iteration in GNMTF. It is important to note thatC
u
is a diagonal matrix, the nonzero elements on
each row of C
u
is 1. Thus, we only need zero addition and mk multiplications to compute C
u
U. Simi-
larly, forC
u
U
0
,C
v
V,C
v
V
0
,D
u
U andD
v
V, we also only need zero addition and mk multiplications
for each of them. Besides, we also note thatW
u
is a sparse matrix, if we use a p-nearest neighbor graph,
the average nonzero elements on each row of W
u
is p. Thus, we only need mpk additions and mpk
multiplications to compute W
u
U. Similarly, for W
v
V, we need the same operation counts as W
u
U.
Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates then
becomes O(Iter ? mnk). Therefore, the overall running time of GNMTF is similar to the standard
NMTF and CNMTF.
1335
addition multiplication division overall
GNMTF:U 2k
3
+ (2m+ n)k
2
+m(n+ p)k 2k
3
+ (2m+ n)k
2
+m(n+ p+ 7)k mk O(mnk)
GNMTF:H 2k
3
+ (m+ n+ 2)k
2
+mnk 2k
3
+ (m+ n+ 1)k
2
+mnk k
2
O(mnk)
GNMTF:V 2k
3
+ (2n+m)k
2
+ n(m+ p)k 2k
3
+ (2n+m)k
2
+ n(m+ p+ 7)k nk O(mnk)
Table 1: Computational operation counts for each iteration in GNMTF.
4 Experiments
4.1 Data Sets
Sentiment classification has been extensively studied in the literature. Among these, a large majority
proposed experiments performed on the benchmarks made of Movies Reviews (Pang et al., 2002) and
Amazon products (Blitzer et al., 2007).
Movies data This data set has been widely used for sentiment analysis in the literature (Pang et
al., 2002), which consists of 1000 positive and 1000 negative reviews drawn from the IMDB archive of
rec.arts.movies.reviews.newsgroups.
Amazon data This data set is heterogeneous, heavily unbalanced and large-scale, a smaller ver-
sion has been released. The reduced data set contains 4 product types: Kitchen, Books, DVDs, and
Electronics (Blitzer et al., 2007). There are 4000 positive and 4000 negative reviews.
1
For these two data sets, we select 8000 words with highest document-frequency to generate the vo-
cabulary. Stopwords
2
are removed and a normalized term-frequency representation is used. In order to
construct the lexical prior knowledge matrixU
0
, we use the sentiment lexicon generated by (Hu and Liu,
2004). It contains 2,006 positive words (e.g., ?beautiful?) and 4,783 negative words (e.g., ?upset?).
4.2 Unsupervised Sentiment Classification
Our first experiment is to explore the benefits of incorporating the geometric information in the unsu-
pervised paradigm (that is C
v
= 0). Therefore, the third part in equation (7) will be ignored. For this
unsupervised paradigm of GNMTF, we empirically set ? = ? = ? = 1, ?
1
= ?
2
= 1, Iter = 100 and
run GNMTF 10 repeated times to remove any randomness caused by the random initialization. Due to
limited space, we do not present the impacts of the parameters on the learning model. Now we compare
our proposed GNMTF with the following four categories of methods:
(1) Lexicon-Based Methods (LBM in short): Taboada et al. (2011) proposed to incorporate intensifi-
cation and negation to refine the sentiment score for each document. This is the state-of-the-art lexicon-
based method for unsupervised sentiment classification.
(2) Document Clustering Methods: We choose the most representative cluster methods, K-means,
NMTF, Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clustering
method (ECC) (Cho et al., 2004). We set the number of clusters as two in these methods. Note that all
these methods do not make use of the sentiment lexicon.
(3) Constrained NMTF (CNMTF in short): Li et al. (2009) incorporated the sentiment lexicon into
NMTF as a domain-independent prior constraint.
(4) Graph co-regularized Non-negative Matrix Tri-factorization (GNMTF in short): It is a new algo-
rithm proposed in this paper. We use cosine similarity for constructing the p-nearest neighbor graph for
its simplicity. The number of nearest neighbor p is set to 10 empirically both on document and word
spaces.
4.2.1 Sentiment Classification Results
The experimental results are reported in Table 2. We perform a significant test, i.e., a t-test with a default
significant level of 0.05. From Table 2, we can see that (1) Both CNMTF and GNMTF consider the
lexical prior knowledge from off-the-shelf sentiment lexicon and achieve better performance than NMTF.
This suggests the importance of the lexical prior knowledge in learning the sentiment classification (row
1
The data set can be freely downloaded from http://www.cs.jhu.edu/ mdredze/datasets/sentiment/.
2
http://truereader.com/manuals/onix/stopwords1.html
1336
# Methods Movies Amazon
1 LBM 0.632 0.580
2 K-means 0.543 (-8.9%) 0.535 (-4.5%)
3 NMTF 0.561 (-7.1%) 0.547 (-3.3%)
4 ECC 0.678 (+4.6%) 0.642 (+6.2%)
5 ITCC 0.714 (+8.2%) 0.655 (+7.5%)
6 CNMTF 0.695 (+6.3%) 0.658 (+7.8%)
7 GNMTF 0.736 (+10.4%) 0.705 (+12.5%)
Table 2: Sentiment classification accuracy of unsupervised paradigm on the data sets. Improvements of
K-means, NMTF, ITCC, ECC, CNMTF and GNMTF over baseline LBM are shown in parentheses.
0 20 40 60 80 1000.2
0.25
0.3
0.35
0.4
0.45
0.5
(a) Movies data
Objec
tive fu
nction
 value
0 20 40 60 80 1000.46
0.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
(b) Amazon data
Objec
tive fu
nction
 value
Figure 1: Convergence curves of GNMTF on both data sets.
3 vs. row 6 and row 7); (2) Regardless of the data sets, our GNMTF significantly outperforms state-of-
the-art CNMTF and achieves the best performance. This shows the superiority of geometric information
and graph co-regularization framework (row 4 vs. row 5, the improvements are statistically significant at
p < 0.05).
4.2.2 Convergence Behavior
In subsection 3.3.1, we have shown that the multiplicative updates given by equations (8)?(10) are
convergent. Here, we empirically show the convergence behavior of GNMTF.
Figure 1 shows the convergence curves of GNMTF on Movies and Amazon data sets. From the figure,
y-axis is the value of objective function and x-axis denotes the iteration number. We can see that the
multiplicative updates for GNMTF converge very fast, usually within 50 iterations.
4.3 Semi-supervised Sentiment Classification
In this subsection, we describe our proposed GNMTF with a few labeled documents. For this semi-
supervised paradigm of GNMTF, we empirically set Iter = 100, ?
1
= ?
2
= 2, ? = ? = ? = ? = 1 and
p = 10 on document and word spaces and also run 10 repeated times to remove any randomness caused
by the random initialization. Due to limited space, we do not give an in-depth parameter analysis. For
CNMTF, we set ? = ? = 1 for fair comparison. We also compare our proposed GNMTF with some
representative semi-supervised approaches described in (Li et al., 2009): (1) Semi-supervised learning
with local and global consistency (Consistency Method in short) (Zhou et al., 2004); (2) Semi-supervised
learning using gaussian fields and harmonic functions (GFHF in short) (Zhu et al., 2003). Besides,
we also compare the results of our proposed GNMTF with the representative supervised classification
method: support vector machine (SVM), which has been widely used in sentiment classification (Pang
et al., 2002).
The results are presented in Figure 2. From the figure, we can see that GNMTF outperforms other
methods over the entire range of number of labeled documents on both data sets. By this observation,
we can conclude that taking the geometric information can still improve the sentiment classification
accuracy in semi-supervised paradigm.
1337
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
(a) Movies data
Senti
ment
 class
ificati
on ac
curac
y
SVMConsistency MethodGFHFCNMTFGNMTF 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.55
0.6
0.65
0.7
0.75
0.8
0.85
(b) Amazon data
Senti
ment
 class
ificati
on ac
curac
y
SVMConsistency MethodGFHFCNMTFGNMTF
Figure 2: Sentiment classification accuracy vs. different percentage of labeled documents, where x-axis
denotes the number of documents labeled as a fraction of the original labeled documents.
5 Related Work
Sentiment classification has gained widely interest in NLP community, we point the readers to recent
books (Pang and Lee, 2008; Liu, 2012) for an in-depth survey of literature on sentiment analysis.
Methods for automatically classifying sentiments expressed in products and movie reviews can
roughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis. Super-
vised techniques have been proved promising and widely used in sentiment classification (Pang et al.,
2002; Pang and Lee, 2008; Liu, 2012). However, the performance of these methods relies on manually
labeled training data. In some cases, the labeling work may be time-consuming and expensive. This
motivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised)
paradigm.
The most representative way to perform semi-supervised paradigm is to employ partial labeled data to
guide the sentiment classification (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009;
Li et al., 2011). However, we do not have any labeled data at hand in many situations, which makes
the unsupervised paradigm possible. The most representative way to perform unsupervised paradigm
is to use a sentiment lexicon to guide the sentiment classification (Turney, 2002; Taboada et al., 2011)
or learn sentiment orientation via a matrix factorization clustering framework (Li et al., 2009; ?; Hu
et al., 2013). In contrast, we perform sentiment classification with the different model formulation and
learning algorithm, which considers both word-level and document-level sentiment-related contextual
information (e.g., the neighboring words or documents tend to share the same sentiment polarity) into
a unified framework. The proposed framework makes use of the valuable geometric information to
compensate the problem of lack of labeled data for sentiment classification. In addition, some researchers
also explored the matrix factorization techniques for other NLP tasks, such as relation extraction (Peng
and Park, 2013) and question answering (Zhou et al., 2013)
Besides, many studies address some other aspects of sentiment analysis, such as cross-domain senti-
ment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al., 2011; Glorot
et al., 2011), cross-lingual sentiment classification (Wan, 2009; Lu et al., 2011b; Meng et al., 2012) and
imbalanced sentiment classification (Li et al., 2011), which are out of scope of this paper.
6 Conclusion and Future Work
In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-
factorization (GNMTF), from a geometric perspective. GNMTF assumes that if two words (or docu-
ments) are sufficiently close to each other, they tend to share the same sentiment polarity. To achieve
this, we encode the geometric information by constructing the nearest neighbor graphs, in conjunction
with a non-negative matrix tri-factorization framework. We derive an efficient algorithm for learning
the factorization, analyze its complexity, and provide proof of convergence. Our empirical study on two
open data sets validates that GNMTF can consistently improve the sentiment classification accuracy in
comparison to state-of-the-art methods.
1338
There are some ways in which this research could be continued. First, some other ways should be
considered to construct the graphs (e.g., hyperlinks between documents, synonyms or co-occurrences
between words). Second, we will try to extend the proposed framework for other aspects of sentiment
analysis, such as cross-domain or cross-lingual settings.
Acknowledgments
This work was supported by the National Natural Science Foundation of China (No. 61303180 and
No. 61272332), the Beijing Natural Science Foundation (No. 4144087), CCF Opening Project of Chi-
nese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund. We thank the
anonymous reviewers for their insightful comments.
References
M. Belkin and P. Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In
Proceedings of NIPS, pages 585-591.
J. Blitzer, M. Dredze and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: domain adaptation
for sentiment classification. In Proceedings of ACL, pages 440-447.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using multiples sources to construct a sentiment sensitive thesaurus.
In Proceedings of ACL, pages 132-141.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge university press.
D. Cai, X. He, J. Han, and T. Huang. 2011. Graph regularized non-negative matrix factorization for data represen-
tation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8): 1548-1560.
H. Cho, I. Dhillon, Y. Guan, and S. Sra. 2004. Minimum sum squared residue co-clutering of gene expression
data. In Proceedings of SDM, pages 22-24.
F. Chung. 1997. Spectral graph theory. Regional Conference Series in Mathematics, Volume 92.
I. Dhillon, S. Mallela, and D. Modha. 2003. Information-theoretic Co-clustering. In Proceedings of KDD, pages
89-98.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogonal non-negative matrix tri-factorization for clustering. In
Proceedings of KDD, pages 126-135.
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain adaptation for larage-scale sentiment classification: a deep
learning approach. In Proceedings of ICML.
A. Goldberg and X. Zhu. 2006. Seeing stars when there aren?t many stars: graph-based semi-supervised learning
for sentiment categorization. In Proceedings of NAACL Workshop.
Y. He, C. Lin and H. Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment
classification. In Proceedings of ACL, pages 123-131.
M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD.
X. Hu, J. Tang, H. Gao, and H. Liu. 2013. Unsupervised sentiment analysis with emotional signals. In Proceedings
of WSDM.
X. Hu, N. Sun, C. Zhang, and T. Chua. 2009. Exploiting internal and external semantics for the clustering of short
texts using world knowldge. In Proceedings of CIKM, pages 919-928.
H. Kim and H. Park. 2008. Non-negative matrix factorization based on alternating non-negativity constrained
least squares and active set method. SIAM J Matrix Anal Appl, 30(2):713-730.
D. Lee and H. Seung. 2001. Algorithms for non-negative matrix factorization. In Proceedings of NIPS.
S. Li, Z. Wang, G. Zhou, and S. Lee. 2011. Semi-supervised learning for imbalanced sentiment classification. In
Proceedings of IJCAI, pages 1826-1831.
1339
T. Li, Y. Zhang, and V. Singhwani. 2009. A non-negative matrix tri-factorization approach to sentiment classifica-
tion with lexical prior knowledge. In Proceedings of ACL, pages 244-252.
C. Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural Comput, 19(10):2756-
2779.
B. Liu. 2012. Sentiment analysis and opinion mining. Morgan & Claypool Publishers.
B. Lu, C. Tan, C. Cardie, and B. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel
corpora. In Proceedings of ACL, pages 320-330.
Y. Lu, M. Castellanos, U. Dayal, and C. Zhai. 2011. Automatic construction of a context-aware sentiment lexicon:
an optimization approach. In Proceedings of WWW, pages 347-356.
X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang. 2012. Cross-lingual mixture model for sentiment
classification. In Proceedings of ACL, pages 572-581.
V. Ng, S. Dasgupta, and S. Arifin. 2006. Examing the role of linguistic knowlege sources in the automatic
identification and classificaton of reviews. In Proceedings of ACL.
S. Pan, X. Ni, J. Sun, Q. Yang, and Z. Chen. 2010. Cross-domain sentiment classification via spectral feature
alignment. In Proceedings of WWW.
B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Informaiton
Retrieval, pages 1-135.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques.
In Proceedings of EMNLP, pages 79-86.
S. Riedel, L. Yao, A. McCallum, and B. Marlin. 2013. Relation extraction with matrix factorization and universal
schemas. In Proceedings of NAACL.
W. Peng and D. Park. 2011. Generative adjective sentiment dictionary for social media sentiment analysis using
constrained nonnegative matrix factorization. In Proceedings of ICWSM.
S. Roweis and L. Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323-2326.
V. Sindhwani and P. Melville. 2008. Document-word co-regulariztion for semi-supervised sentiment analysis. In
Proceedings of ICDM, pages 1025-1030.
J. Tenenbaum, V. Silva, and J. Langford. 2000. A global geometric framework for nonlinear dimensionality
reduction. Science, 290(5500):2319-2323.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede. 2011. Lexicon-based methods for sentiment analysis.
Computational Linguistics.
P. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of
reviews. In Proceedings of ACL, pages 417-424.
X. Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of ACL, pages 235-243.
D. Zhou, Q. Bousquet, T. Lal, J. Weston, and B. Scholkopf. 2004. Learning with local and global consistency. In
Proceedings of NIPS.
G. Zhou, F. Liu, Y. Liu, S. He, and J. Zhao. 2013. Statistical machine translation improves question retrieval in
community question answering via matrix factorization. In Proceedings of ACL, pages 852-861.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic
functions. In Proceedings of ICML.
1340
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2107?2116, Dublin, Ireland, August 23-29 2014.
Exploring Fine-grained Entity Type Constraints for Distantly Supervised
Relation Extraction
Yang Liu Kang Liu Liheng Xu Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Zhongguancun East Road #95, Beijing 100190, China
{yang.liu, kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Distantly supervised relation extraction, which can automatically generate training data by align-
ing facts in the existing knowledge bases to text, has gained much attention. Previous work used
conjunction features with coarse entity types consisting of only four types to train their model-
s. Entity types are important indicators for a specific relation, for example, if the types of two
entities are ?PERSON? and ?FILM? respectively, then there is more likely a ?DirectorOf? rela-
tion between the two entities. However, the coarse entity types are not sufficient to capture the
constraints of a relation between entities. In this paper, we propose a novel method to explore
fine-grained entity type constraints, and we study a series of methods to integrate the constraints
with the relation extracting model. Experimental results show that our methods achieve bet-
ter precision/recall curves in sentential extraction with smoother curves in aggregated extraction
which mean more stable models.
1 Introduction
Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences
containing them. It can potentially benefit many applications, such as knowledge base construction,
question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Tra-
ditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to
manually label training data, which is expensive and limits the ability to scale up. Due to the shortcom-
ing of supervised approaches mentioned above, recently, a more promising approach named distantly
supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has be-
come popular. Instead of manual labeling, it automatically generates training data by aligning facts in
existing knowledge bases to text.
However, the paradigm of distant supervision also causes new problems of noisy training data both in
positive training instances and negative training instances. To overcome the false positive problem caused
by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Sur-
deanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they
assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al.
(Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail
when there was only one sentence containing both entities. They proposed a method to learn and filter
noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu
et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative
training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudo-
relevance feedback method trying to find out the false negative instances and add them into positive
training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training in-
stances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni,
2013) used hidden variables to model the missing data in databases based on a graphical model. The
training data generation process for all the above work is under the framework of (Mintz et al., 2009),
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2107
one important step of which is to recognize entity mentions from text and assign them entity types which
are used to compose features for training the model. The entity types they used are very coarse only con-
sisting of four categories (PERSON, ORGANIZATION, LOCATION, NONE). We argue that the coarse
entity types are not sufficient to indicate relations.
A specific relation constrains the entity types of its two entities. For instance, the SingerOf relation
limits the entity type of its first entity as PERSON or more fine-grained ARTIST, and the entity type of
its second entity as ART or more fine-grained MUSIC. Therefore, when extracting a relation instance,
the entity types of its two entities are important indicators for a specific relation. Previous work used
conjunction features (Details in Section 3.3) by combining the coarse entity types of entity mentions
with its contextual lexical and syntactic features. However, the conjunction features may fail to dis-
tinguish the relations. For example, the following two sentences contain two relation instances, one is
DirectorOf(Ang Lee, Life of Pi), and the other is AuthorOf(George R.R. Martin, A Song of Ice and Fire).
1. Ang Lee?s Life of Pi surprised many by scoring a leading four Oscars on Sunday night...
2. Westeros is the premiere fansite for George R.R. Martin?s A Song of Ice and Fire.
Only using the above conjunction features, we cannot tell the difference between the two entity pairs,
and are probable to incorrectly classify them as the same relation. By contrast, if we can assign each
entity with fine-grained entity types, for example, Ang Lee as the entity type ARTIST and George R.R.
Martin as AUTHOR, we may succeed in classifying the two entity pairs correctly.
To achieve the goal mentioned above, there are mainly three challenges: (1) how to define the fine-
grained type set; (2) how to assign the types to entity mentions; (3) how to integrate the fine-grained
entity type constraints with the relation extracting model. To address these challenges, in this paper,
we propose a novel approach to explore the fine-grained entity type constraints for distantly supervised
relation extraction. First, we use the types defined in (Ling and Weld, 2012) stemmed from Freebase
1
as the fine-grained entity type set (introduced in Section 3.1). Second, we leverage Web knowledge
to train a fine-grained entity type classifier and predict entity types for each entity mention. Third, we
study several methods to integrate the type constraints with an existing system MULTIR, a multi-instance
multi-label model in (Hoffmann et al., 2011), to train the extractor.
In summary, the contribution of this paper can be concluded as follows.
(a) We explore the effect of fine-grained entity type constraints on distantly supervised relation extrac-
tion. A novel method is proposed to leverage Web knowledge to automatically train a fine-grained
entity type classifier, which is used to predict the fine-grained types of each entity mention.
(b) We study a series of methods for integrating the fine-grained entity type constraints with the extract-
ing model and compare their performance with different parameter settings.
(c) We conduct experiments to demonstrate the effects of the newly exploited fine-grained entity type
constraints. It shows that our method achieves a much better precision/recall curves over the base-
line system in sentential extraction, and improves the performance with a smoother precision/recall
curve in aggregated extraction, which means a more stable model.
2 Distant Supervision for Relation Extraction
We define a relation instance (or a fact), which means a binary relation, as r(e
1
, e
2
). r is the relation, and
e
1
and e
2
mean the two entities in the relation instance, for example, BornIn(Y ao Ming, Shanghai).
Distant supervision supplies a method to automatically generate training data. In this part, we will
introduce the general steps in distant supervision for relation extraction. First, we define the notations
we use. ? denotes sentences comprising the corpus, E denotes entity mentions in the corpus which are
consecutive words with the same named entity tags assigned by an NER system, ? denotes the facts (or
relation instances) in the existing knowledge base. R denotes the relations in ?.
1
http://www.freebase.com/
2108
 Figure 1: Fine-grained entity type set.
Figure 2: Framework of fine-grained entity type classifier.
To generate training data, we align pairs of entity mentions in the same sentence with ?. The aligned
entity mentions E
train
and their sentences ?
train
along with R
train
are used as training data. Features
are extracted from them to train the relation extracting model.
To predict the unknown data for extracting new relation instances, we input pairs of entity mentions
E
predict
and the sentences containing them ?
preidct
into the trained extracting model for extracting new
relation instances.
3 Fine-grained Entity Type Constraints
Entity mentions in sentences are considered consecutive words with the same entity types (Section 2).
The entity types are part of the lexical and syntactic features(Mintz et al., 2009), and the feature setting
is followed by other related work. Their entity types are assigned by an NER system and consist of
four categories (PERSON, ORGANIZATION, LOCATION, NONE). The types of entity mentions in
a relation are important indicators for the very type of relation. However, the coarse (only four types)
entity types may not capture sufficient constraints to distinguish a relation. In this section, we explore
fine-grained entity type constraints and study different methods to integrate them with the extracting
model.
This section first introduces the fine-grained entity type set(Section 3.1), and then describes our method
which leverages Web knowledge to train the fine grained entity type classifier and assign entity mentions
with the fine-grained entity types (Section 3.2). At last, we illustrate methods to integrate fine-grained
entity type constraints with the relation extracting model.
2109
Entity pair [Hank Ratner], [Cablevision]
Sentence
Cablevision?s $600 million offer came in the form of a letter to Peter S.Kalikow,
chairman of the M.T.A., from the Garden?s vice chairman, Hank Ratner.
Conjunction Reverse Left NE1 Middle NE2 Right
Feature examples
False PER ORG
False Hank[NMOD] PER [NMOD]chairman ... offer[SBJ] ORG
True B -1 ORG POS $ ... NN NN, PER .B 1
Table 1: Examples of conjunction features.
3.1 Fine-grained Entity Types
Figure 1 is the type set we use. It was introduced in (Ling and Weld, 2012) and was derived from
Freebase types. The bold types in each small box of Figure 1 are upper-class types for others in that
small box. For example, /actor is a lower-class type of /person which is denoted as /person/actor.
And /person and /person/actor coexist in the type set.
3.2 Fine-grained Entity Type Classifier
In this section, we describe our method that leverages Web knowledge to train a fine-grained entity type
classifier and predict entity types of each entity mention. Its architecture is shown in Figure 2.
3.2.1 Training
The training data are obtained from Wikipedia. Because the defined fine-grained types are tailored based
on Freebase types, we can find the mappings between the two type sets, for example, /person/doctor
maps to two Freebase types /medicine/physician and /medicine/surgeon. And Freebase WEX
2
supplies a mapping between Freebase types to Wikipedia articles. As a result, we can map Wikipedia
articles to defined fine-grained types.
Based on the mappings, we obtain Wikipedia articles for each type as training data and negative
training examples are sampled from articles not contained in the mappings. We preprocess the articles
by: stop words filtering, stemming, and term frequency filtering and use a maxent model to train the
classifier.
3.2.2 Predicting
To predict types of each entity mention, we first use search engines to expand entity mentions. Specif-
ically, each entity mention is used as a query sent to the search engine
3
. Titles and descriptions of top
k returned snippets are selected (We keep the top 20 in the experiments). The obtained text are pre-
processed with the same method as training examples. Then we use the trained fine-grained entity type
classifier to predict the types of each entity mention.
After predicting, we obtain a ranked list of types for each entity mention, which are ranked by the
predicting scores.
3.3 Integrating Fine-grained Entity Type Constraints into the Extracting Model
This section introduces our methods to integrate the fine-grained entity type constraints with the ex-
tracting model. First of all, we briefly review the features used in previous models which derived from
(Mintz et al., 2009) and (Riedel et al., 2010). Their features mainly comprise two types: lexical features
(POS tags, words and entity types) and syntactic features (dependency parsing tags, words and entity
types). Each feature is a conjunction with several parts: entity types of two entity mentions, the left
context window of the first entity mention, the right context window of the second entity mention and
the part between them (the window contains none or one or two words ). Table 1 shows an example of
the conjunction features.
2
http://wiki.freebase.com/wiki/WEX
3
We use Bing search API. http://datamarket.azure.com/dataset/bing/search
2110
To integrate the exploited fine-grained entity type constraints with the extracting model, we proposed
three methods (substitution, augment and selection) to make the type constraints take effects.
3.3.1 Substitution Method
In this method, we substitute coarse entity types of the features with the entity mentions? fine-grained
types, and use the new features to train the model. Instead of substituting directly, an entity mention
is first represented by its fine-grained types and the upper-class of the fine-grained type, for example,
/person/politician derives two types /person and /person/politician itself. The reason is that the
extracting model can benefit from the related types like the upper-class types. And then we use the
obtained entity types to substitute the old coarse entity types as new features greedily, which mean-
s that all the possible combinations of types between the entity pair are considered. For example,
?Barack Obama? has the fine-grained type /person/politician and his birth place ?Hawaii? has
the type /location/island, then there are 4 combinations between the two entities, they are (/person,
/location), (/person, /location/island), (/person/politician, /location) and (/person/politician,
/location).
3.3.2 Augment Method
In this method, we generate new features by substituting the coarse entity types with predicted fine-
grained types, and expand the old features with new features. Different from the substitution method, we
do not add the upper-class types, for that we think the coarse types in old features have the same effect.
In this method, we use the fine-grained constraints as a complementary.
3.3.3 Selection Method
The selection method is similar to the augment method. The difference is that we do not expand all
old features with new features. We select some of them to expand. The reason is that some of the
conjunction features are of high-precision themselves, it can clearly indicate the relations with its left,
middle and right parts, even without the entity types (informative ones). If we expand these features,
it may cause more noisy features. So we expect to only expand the ones that lack of the indicating
abilities (non-informative ones). In this paper, we employ a simple method to distinguish between the
informative ones and non-informative ones by the length of the features, which means that the longer is
more informative than the shorter. In our experiments, the length threshold is set as 20.
In the predicting phase (Section 3.2), we obtain a ranked type list for each entity mention. The top list
types are considered in our methods. Experiments in Section 4.3 are conducted on top k {k ? 1, 2, 3}
type/types in the obtained ranked list. And they are combined with a greedy method similar to that in the
substitution method explained above.
4 Experiments
4.1 Settings
We use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentences
in the years 2005-2006 are used as training corpus ?
train
for distant supervision and sentences in 2007
are used as testing corpus ?
predict
. The data was first tagged with an NER system (Finkel et al., 2005)
and consecutive words with the same tag are extracted as entity mentions. And then, entity mentions
E
train
in training corpus are aligned to facts ? in Freebase as training examples to train the models.
We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multi-
label extracting model in (Hoffmann et al., 2011). Following their setttings, we conduct experiments on
aggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints.
? Aggregated extraction: Aggregated extraction is corpus-level extraction. When given an entity
pair, it predicts its relation types based on the whole corpus. After extraction, the precision and
recall are computed by comparing the results with facts in Freebase. The evaluation underestimates
the accuracy because there may be correct facts in the extracted results but not existing in Freebase,
these facts are labeled as incorrect by mistake here. Because aggregated extraction is an automatic
evaluation, it is used to tune parameters like held-out evaluation in (Mintz et al., 2009).
2111
(a) PR curves of the substitution method (b) PR curves of the augment method
(c) PR curves of the selection method (d) Comparison with other methods
Figure 3: Precision-recall (PR) curves of the aggregated extraction.
? Sentential extraction: Sentential extraction predicts an entity pair only based on a specified sen-
tence containing the pair of entities. We use manually labeled data in (Hoffmann et al., 2011) as
benchmark. The data consist of 1,000 sentences and are sampled from the results their system out-
puts and sentences aligned with facts in Freebase. As they stated in their paper, these results provide
a good approximation to the true precision but can overestimate the actual recall.
4.2 Experimental Results
In aggregated extraction, we first evaluate the three type-constraint integration methods (substitution,
augment and selection) with the top k {k ? 1, 2, 3} type/types (Section 3.3). And then, we compare the
best parameter setting methods with previous work. In sentential extraction, we compare methods tuned
in aggregated extraction with MULTIR.
4.2.1 Aggregated Extraction
Figure 3 shows the precision-recall (PR) curves of the aggregated extraction. In it, Sub topk {k ?
1, 2, 3} means using the substitution method (Section 3.3) with top k fine-grained entities types re-
turned by the type classifier in Section 3.2. Correspondingly, Aug topk is for the augment method
and Select topk is for the selection method.
Figure 3(a) shows that Sub top3 outperforms the other two settings of k in the substitution method,
it seems that more fine-grained types produce better curves. In Figure 3(b), Aug top1 and Aug top2
achieve similar performances. However, when adding one more type with k = 3, we obtain a lower
curve, which contradicts the trend showed in the curves of the substitution method (Figure 3(a)). Fig-
ure 3(c) shows the PR curves of three selection methods, Select top1 has a better performance at the
beginning. Then Select top2 exceeds it a bit consistently.
In Figure 3(d), we demonstrate the comparison of best tuned methods above with previous work.
They are Sub top3, Aug top1 and Select top2. From Figure 3(d), it shows that, among the three of
our methods, Aug top1 achieves better precisions along the PR curves, and Select top2 reaches the best
2112
Figure 4: Comparison with MULTIR
recall at the highest recall point. Comparing to other methods, the PR curve of Aug top1 reaches a higher
recall with 29.3% at the highest recall point than MULTIR (24.5%). Select top2 achieves 29.3% at the
highest recall point, best among all methods. And by integrating the fine-grained entity type constraints,
they improve the PR curve of MULTIR with a more smoother curve without most of the depressions seen
in MULTIR. As stated in (Hoffmann et al., 2011), the smoother curve indicated a more stable model.
4.2.2 Sentential Extraction
Figure 4 shows the precision-recall (PR) curves of the sentential extraction. In the evaluation, we com-
pare the three best integration methods tuned in aggregated extraction with original MULTIR. Among our
three method, Aug top1 outperforms in precision and achieves a better curve in general among the three
methods, however, Select top2 gains a better recall at the end. Sub top3 has the worst recall. In gen-
eral, our methods have much better precisions than MULTIR. Aug top1 and Select top2 achieve better
curves than MULTIR. Since the evaluation of sentential extraction is a good approximation of precision,
it implies that the proposed methods are effective.
4.2.3 Analysis
On one hand, among the three proposed integration methods, generally, the augment method and selec-
tion method get better performance. The reason is that substitution method uses predicted fine-grained
entity types to replace the old coarse features in the conjunction features completely, and the conjunction
features are sensitive to entity types for different entity types indicate different conjunction features, as
a result, if we can not promise a good accuracy in the type classification which is hard to achieve in
classifying hundreds of fine-grained types, the performance will be badly influenced. Different from the
substitution method, augment method and selection method keep the old features with coarse features,
they use the features with fine-grained entity type constraints as extra information to help the extraction
and achieve better results.
On the other hand, comparing to other methods, by integration the exploited fine-grained entity type
constraints, our methods achieve improvements in both aggregated and sentential extraction. It proves
that the fine-grained entity type constraints we exploit are effective, and our proposed integration meth-
ods succeed in integrating the constraints into the extracting model. Our augment method outperforms
MULTIR in precision along the PR curves in sentential extraction and improve it performance with a
more smoother PR curve in aggregated extraction, which indicates a more stable model. Moreover, the
method gets a better recall. And our selection method consistently outperforms MULTIR in sentential
2113
k=1 k=2 k=3
Recall@k 0.596 0.740 0.806
Table 2: Evaluation of the fine-grained type classifier.
extraction. In aggregated extraction, it also achieves a smoother curve and an impressive promotion at
the highest recall point. Since the evaluation of aggregated extraction only considers the facts existing
in Freebase which may incorrectly label the right extracting results and underestimate the true precision,
and based on its better performance of precision in sentential extraction, we consider it is a more promis-
ing method. This paper only employs very naive method to select the non-informative features by its
length (Section 3.3.3), a more effective selecting method may lead further improvements.
4.3 Performance of Entity Type Classifier
We evaluate the performance of the fine-grained entity type classifier (Section 3.2). In section 3.2, we
sample the training examples from a collection of Wikipeida articles mapped with the fine-grained types.
To generate test entity mentions, we first remove the sampled training articles from the collection, and
then sample the articles from it, where the titles of sampled articles are used as the test entity mentions
(we sample 12,000 test entity mentions) and their mapped fine-grained types are used as benchmark.
After that, the predicting method in Section 3.2.2 is used to expand mentions and predict the types of
each test entity mention. After predicting, we obtain a ranked list of types for each test entity mention.
To evaluate, we define a notation of Hit@k, which equals 1 if the true type of an entity mention is
hit in the top k predicted types, otherwise equals 0. And then we evaluate it by the Recall@k defined
bellow.
Recall@k =
?
12000
i=1
Hit@k
i
12000
(1)
In equation (1), i means the ith test entity mention. Table 2 shows the results for the top 3 predicted
types.
5 Related Work
Distant supervision (also known as weak supervision or self supervision) is used to a broad class of meth-
ods in information extraction which aims to automatically generate labeled data by aligning with data
in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast
Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum
(Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN
system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and
trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUN-
NER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic
labeled data from Penn Treebank and Wikipedia infoboxes respectively.
Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation
extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and
trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a
method to generate training data automatically, however it also bring the problem of noisy labeling. After
their work, a variety of methods focused to solve this problem. Riedel (Riedel et al., 2010) proposed a
multi-instance model to model the false positive noise in training data with the assumption that at least
one of the labeled sentences truly expressed their relation. After their work, Hoffmann (Hoffmann et
al., 2011) and Surdeanu (Surdeanu et al., 2012) tried to not only model the noisy training data, but also
overcame the problem of multi-label where two entities may exist more than one relation, they proposed
graphic models as kinds of multi-instance multi-label learning methods and made improvements over
previous work. The at-least-one assumption would fail when encountering entity pairs with only one
aligned sentence. Takamatsu (Takamatsu et al., 2012) employed an alternative approach without the
mentioned assumptions. Their work predicted negative patterns using a generative model and remove
labeled data containing negative patterns to reducing noise in labeled data.
2114
Besides the problem of false positive training examples caused by distant supervision. There were a
bunch of researches trying to solve the problem of false negative training examples caused by incomplete
knowledge bases. Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative training
examples. And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback. Min
(Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layer
graphic model to model the incomplete knowledge base. Ritter (Ritter and Etzioni, 2013) employed
similar intuition with (Xu et al., 2013) that they thought rear entities missing in the database would
be often mentioned in the text. They proposed a latent-variable approach to model it and showed its
improvement over aggregate and sentential extraction.
6 Conclusion
In this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantly
supervised relation extraction. We leverage Web knowledge to automatically train a fine-grained entity
type classifier and predict entity types of each entity mention. And we study a series of methods to inte-
grate the type constraints with a relation extraction model. At last, thorough experiments are conducted.
The experimental results imply our methods are effective with better precision/recall curves in senten-
tial extraction and smoother precision/recall curves in aggregated extraction, which indicate more stable
models.
In the future we hope to explore more details of integration methods that integrates fine-grained entity
type constraints with relation extraction models, especially the selection integration method. We consider
that a more effective method to distinguish between the informative and non-informative features will
lead more improvements.
Acknowledgements
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61202329). This work was supported in part by
Noahs Ark Lab of Huawei Tech. Co. Ltd.
References
Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration on the Web.
Mark Craven, Johan Kumlien, et al. 1999. Constructing biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular
Biology, pages 77?86. Heidelberg, Germany.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 363?370. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 541?
550.
Xiao Ling and DS Weld. 2012. Fine-Grained Entity Recognition. In AAAI.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In Proceedings of NAACL-HLT, pages 777?782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages
1003?1011. Association for Computational Linguistics.
2115
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148?163. Springer.
Alan Ritter and Oren Etzioni. 2013. Modeling Missing Data in Distant Supervision for Information Extraction.
Transactions of the Association for Computational Linguistics, 1:367?378.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 455?465. Association for
Computational Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura Coppola, et al. 2005. Scaling Web-based aquisition of
entailment relations. Ph.D. thesis, Tel Aviv University.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 721?729. Association for Computational Linguistics.
Fei Wu and Daniel S Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the sixteenth ACM
conference on Conference on information and knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
W Xu, RH Le Zhao, and R Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation
Extraction. Proceedings of Association for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.
2007. Textrunner: open information extraction on the web. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:
Demonstrations, pages 25?26. Association for Computational Linguistics.
Xingxing Zhang, jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zuifang Sui. 2013. Towards Accurate
Distant Supervision for Relational Facts Extraction. In Proceedings of Association for Computational Linguis-
tics.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
Association for Computational Linguistics.
GuoDong Zhou, Min Zhang, Dong Hong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning.
2116
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2335?2344, Dublin, Ireland, August 23-29 2014.
Relation Classification via Convolutional Deep Neural Network
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{djzeng,kliu,swlai,gyzhou,jzhao}@nlpr.ia.ac.cn
Abstract
The state-of-the-art methods used for relation classification are primarily based on statistical ma-
chine learning, and their performance strongly depends on the quality of the extracted features.
The extracted features are often derived from the output of pre-existing natural language process-
ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders
the performance of these systems. In this paper, we exploit a convolutional deep neural network
(DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as
input without complicated pre-processing. First, the word tokens are transformed to vectors by
looking up word embeddings
1
. Then, lexical level features are extracted according to the given
nouns. Meanwhile, sentence level features are learned using a convolutional approach. These
two level features are concatenated to form the final extracted feature vector. Finally, the fea-
tures are fed into a softmax classifier to predict the relationship between two marked nouns. The
experimental results demonstrate that our approach significantly outperforms the state-of-the-art
methods.
1 Introduction
The task of relation classification is to predict semantic relations between pairs of nominals and can
be defined as follows: given a sentence S with the annotated pairs of nominals e
1
and e
2
, we aim
to identify the relations between e
1
and e
2
(Hendrickx et al., 2010). There is considerable interest in
automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP
applications.
The most representative methods for relation classification use supervised paradigm; such methods
have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu
and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided
into feature-based methods and kernel-based methods. Feature-based methods use a set of features that
are selected after performing textual analysis. They convert these features into symbolic IDs, which are
then transformed into a vector using a paradigm that is similar to the bag-of-words model
2
. Conversely,
kernel-based methods require pre-processed input data in the form of parse trees (such as dependency
parse trees). These approaches are effective because they leverage a large body of linguistic knowledge.
However, the extracted features or elaborately designed kernels are often derived from the output of pre-
existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the
performance of such systems (Bach and Badaskar, 2007). It is attractive to consider extracting features
that are as independent from existing NLP tools as possible.
To identify the relations between pairs of nominals, it is necessary to a skillfully combine lexical and
sentence level clues from diverse syntactic and semantic structures in a sentence. For example, in the
sentence ?The [fire]
e
1
inside WTC was caused by exploding [fuel]
e
2
?, to identify that fire and fuel are in a
This work is licenced under a Creative Commons Attribution 4.0 International License.Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
A word embedding is a distributed representation for a word. For example, Collobert et al. (2011) use a 50-dimensional
vector to represent a word.
2
http://en.wikipedia.org/wiki/Bag-of-words model
2335
Cause-Effect relationship, we usually leverage the marked nouns and the meanings of the entire sentence.
In this paper, we exploit a convolutional DNN to extract lexical and sentence level features for relation
classification. Our method takes all of the word tokens as input without complicated pre-processing,
such as Part-of-Speech (POS) tagging and syntactic parsing. First, all the word tokens are transformed
into vectors by looking up word embeddings. Then, lexical level features are extracted according to the
given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two
level features are concatenated to form the final extracted feature vector. Finally, the features are feed
into a softmax classifier to predict the relationship between two marked nouns.
The idea of extracting features for NLP using convolutional DNN was previously explored by Col-
lobert et al. (2011), in the context of POS tagging, chunking (CHUNK), Named Entity Recogni-
tion (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of Collobert
et al. (2011). In (Collobert et al., 2011), all of the tasks are considered as the sequential labeling prob-
lems in which each word in the input sentence is given a tag. However, our task, ?relation classification?,
can be considered a multi-class classification problem, which results in a different objective function.
Moreover, relation classification is defined as assigning relation labels to pairs of words. It is thus nec-
essary to specify which pairs of words to which we expect to assign relation labels. For that purpose, the
position features (PF) are exploited to encode the relative distances to the target noun pairs. To the best
of our knowledge, this work is the first example of using a convolutional DNN for relation classification.
The contributions of this paper can be summarized as follows.
? We explore the feasibility of performing relation classification without complicated NLP pre-
processing. A convolutional DNN is employed to extract lexical and sentence level features.
? To specify pairs of words to which relation labels should be assigned, position features are proposed
to encode the relative distances to the target noun pairs in the convolutional DNN.
? We conduct experiments using the SemEval-2010 Task 8 dataset. The experimental results demon-
strate that the proposed position features are critical for relation classification. The extracted lexical
and sentence level features are effective for relation classification. Our approach outperforms the
state-of-the-art methods.
2 Related Work
Relation classification is one of the most important topics in NLP. Many approaches have been explored
for relation classification, including unsupervised relation discovery and supervised classification. Re-
searchers have proposed various features to identify the relations between nominals using different meth-
ods.
In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory (Harris,
1954) indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is
assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa
et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply
selected the most frequent words in the contexts to represent the relation between the nominals. Chen
et al. (2005) proposed a novel unsupervised method based on model order selection and discriminative
label identification to address this problem.
In the supervised paradigm, relation classification is considered a multi-classification problem, and re-
searchers concentrate on extracting more complex features. Generally, these methods can be categorized
into two types: feature-based and kernel-based. In feature-based methods, a diverse set of strategies
have been exploited to convert the classification clues (such as sequences and parse trees) into feature
vectors (Kambhatla, 2004; Suchanek et al., 2006). Feature-based methods suffer from the problem
of selecting a suitable feature set when converting the structured representation into feature vectors.
Kernel-based methods provide a natural alternative to exploit rich representations of the input classifica-
tion clues, such as syntactic parse trees. Kernel-based methods allow the use of a large set of features
without explicitly extracting the features. Various kernels, such as the convolution tree kernel (Qian et
2336
WordRepresentation
FeatureExtraction
Output W3x
Figure 1: Architecture of the neural network used
for relation classification.
WindowProcessing
max over timesConvolution
tanh W2x
W1
WF
PF
Sentence levelFeatures
Figure 2: The framework used for extracting sen-
tence level features.
al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and
Mooney, 2005), have been proposed to solve the relation classification problem. However, the methods
mentioned above suffer from a lack of sufficient labeled data for training. Mintz et al. (2009) proposed
distant supervision (DS) to address this problem. The DS method selects sentences that match the facts
in a knowledge base as positive examples. The DS algorithm sometimes faces the problem of wrong
labels, which results in noisy labeled data. To address the shortcoming of DS, Riedel et al. (2010) and
Hoffmann et al. (2011) cast the relaxed DS assumption as multi-instance learning. Furthermore, Taka-
matsu et al. (2012) noted that the relaxed DS assumption would fail and proposed a novel generative
model to model the heuristic labeling process in order to reduce the wrong labels.
The supervised method has been demonstrated to be effective for relation detection and yields rela-
tively high performance. However, the performance of this method strongly depends on the quality of the
designed features. With the recent revival of interest in DNN, many researchers have concentrated on us-
ing Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed
representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al.
(2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in
the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto
et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting
of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality
features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical
and sentence level features for relation classification; our method effectively alleviates the shortcomings
of traditional features.
3 Methodology
3.1 The Neural Network Architecture
Figure 1 describes the architecture of the neural network that we use for relation classification. The
network takes an input sentence and discovers multiple levels of feature extraction, where higher levels
represent more abstract aspects of the inputs. It primarily includes the following three components: Word
Representation, Feature Extraction and Output. The system does not need any complicated syntactic or
semantic preprocessing, and the input of the system is a sentence with two marked nouns. Then, the
word tokens are transformed into vectors by looking up word embeddings. In succession, the lexical and
sentence level features are respectively extracted and then directly concatenated to form the final feature
vector. Finally, to compute the confidence of each relation, the feature vector is fed into a softmax
classifier. The output of the classifier is a vector, the dimension of which is equal to the number of
predefined relation types. The value of each dimension is the confidence score of the corresponding
relation.
2337
Features Remark
L1 Noun 1
L2 Noun 2
L3 Left and right tokens of noun 1
L4 Left and right tokens of noun 2
L5 WordNet hypernyms of nouns
Table 1: Lexical level features.
3.2 Word Representation
In the word representation component, each input word token is transformed into a vector by looking
up word embeddings. Collobert et al. (2011) reported that word embeddings learned from significant
amounts of unlabeled data are far more satisfactory than the randomly initialized embeddings. In relation
classification, we should first concentrate on learning discriminative word embeddings, which carry more
syntactic and semantic information, using significant amounts of unlabeled data. Unfortunately, it usually
takes a long time to train the word embeddings
3
. However, there are many trained word embeddings that
are freely available (Turian et al., 2010). A comparison of the available word embeddings is beyond
the scope of this paper. Our experiments directly utilize the trained embeddings provided by Turian et
al.(2010).
3.3 Lexical Level Features
Lexical level features serve as important cues for deciding relations. The traditional lexical level features
primarily include the nouns themselves, the types of the pairs of nominals and word sequences between
the entities, the quality of which strongly depends on the results of existing NLP tools. Alternatively,
this paper uses generic word embeddings as the source of base features. We select the word embeddings
of marked nouns and the context tokens. Moreover, the WordNet hypernyms
4
are adopted as MVRNN
(Socher et al., 2012). All of these features are concatenated into our lexical level features vector l. Table
1 presents the selected word embeddings that are related to the marked nouns in the sentence.
3.4 Sentence Level Features
As mentioned in section 3.2, all of the tokens are represented as word vectors, which have been demon-
strated to correlate well with human judgments of word similarity. Despite their success, single word
vector models are severely limited because they do not capture long distance features and semantic com-
positionality, the important quality of natural language that allows humans to understand the meanings
of a longer expression. In this section, we propose a max-pooled convolutional neural network to offer
sentence level representation and automatically extract sentence level features. Figure 2 shows the frame-
work for sentence level feature extraction. In the Window Processing component, each token is further
represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the
vector goes through a convolutional component. Finally, we obtain the sentence level features through a
non-linear transformation.
3.4.1 Word Features
Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend
to have similar meanings. To capture this characteristic, the WF combines a word?s vector representation
and the vector representations of the words in its context. Assume that we have the following sequence
of words.
S : [People]
0
have
1
been
2
moving
3
back
4
into
5
[downtown]
6
The marked nouns are associated with a label y that defines the relation type that the marked pair contains.
Each word is also associated with an index into the word embeddings. All of the word tokens of the
sentence S are then represented as a list of vectors (x
0
,x
1
, ? ? ? ,x
6
), where x
i
corresponds to the word
3
Collobert et al. (2011) proposed a pairwise ranking approach to train the word embeddings, and the total training time for
an English corpus (Wikipedia) was approximately four weeks.
4
http://sourceforge.net/projects/supersensetag/
2338
embedding of the i-th word in the sentence. To use a context size of w, we combine the size w windows
of vectors into a richer feature. For example, when we take w = 3, the WF of the third word ?moving?
in the sentence S is expressed as [x
2
,x
3
,x
4
]. Similarly, considering the whole sentence, the WF can be
represented as follows:
{[x
s
,x
0
,x
1
], [x
0
,x
1
,x
2
], ? ? ? , [x
5
,x
6
,x
e
]}
5
3.4.2 Position Features
Relation classification is a very complex task. Traditionally, structure features (e.g., the shortest depen-
dency path between nominals) are used to solve this problem (Bunescu and Mooney, 2005). Apparently,
it is not possible to capture such structure information only through WF. It is necessary to specify which
input tokens are the target nouns in the sentence. For this purpose, PF are proposed for relation classi-
fication. In this paper, the PF is the combination of the relative distances of the current word to w
1
and
w
2
. For example, the relative distances of ?moving? in sentence S to ?people? and ?downtown? are 3
and -3, respectively. In our method, the relative distances also are mapped to a vector of dimension d
e
(a
hyperparameter); this vector is randomly initialized. Then, we obtain the distance vectors d
1
and d
2
with
respect to the relative distances of the current word to w
1
and w
2
, and PF = [d
1
,d
2
]. Combining the WF
and PF, the word is represented as [WF,PF]
T
, which is subsequently fed into the convolution component
of the algorithm.
3.4.3 Convolution
We will see that the word representation approach can capture contextual information through combina-
tions of vectors in a window. However, it only produces local features around each word of the sentence.
In relation classification, an input sentence that is marked with target nouns only corresponds to a re-
lation type rather than predicting label for each word. Thus, it might be necessary to utilize all of the
local features and predict a relation globally. When using neural network, the convolution approach is a
natural method to merge all of the features. Similar to Collobert et al. (2011), we first process the output
of Window Processing using a linear transformation.
Z = W
1
X (1)
X ? R
n
0
?t
is the output of the Window Processing task, where n
0
= w? n, n (a hyperparameter) is the
dimension of feature vector, and t is the token number of the input sentence. W
1
? R
n
1
?n
0
, where n
1
(a
hyperparameter) is the size of hidden layer 1, is the linear transformation matrix. We can see that the
features share the same weights across all times, which greatly reduces the number of free parameters to
learn. After the linear transformation is applied, the output Z ? R
n
1
?t
is dependent on t. To determine
the most useful feature in the each dimension of the feature vectors, we perform a max operation over
time on Z.
m
i
= maxZ(i, ?) 0 ? i ? n
1
(2)
where Z(i, ?) denote the i-th row of matrix Z. Finally, we obtain the feature vector m =
{m
1
,m
2
, ? ? ? ,m
n
1
}, the dimension of which is no longer related to the sentence length.
3.4.4 Sentence Level Feature Vector
To learn more complex features, we designed a non-linear layer and selected hyperbolic tanh as the
activation function. One useful property of tanh is that its derivative can be expressed in terms of the
function value itself:
d
dx
tanhx = 1? tanh
2
x (3)
It has the advantage of making it easy to compute the gradient in the backpropagation training procedure.
Formally, the non-linear transformation can be written as
g = tanh(W
2
m) (4)
5
x
s
and x
e
are special word embeddings that correspond to the beginning and end of the sentence, respectively.
2339
W2
? R
n
2
?n
1
is the linear transformation matrix, where n
2
(a hyperparameter) is the size of hidden
layer 2. Compared with m ? R
n
1
?1
, g ? R
n
2
?1
can be considered higher level features (sentence level
features).
3.5 Output
The automatically learned lexical and sentence level features mentioned above are concatenated into a
single vector f = [l, g]. To compute the confidence of each relation, the feature vector f ? R
n
3
?1
(n
3
equals n
2
plus the dimension of the lexical level features) is fed into a softmax classifier.
o = W
3
f (5)
W
3
? R
n
4
?n
3
is the transformation matrix and o ? R
n
4
?1
is the final output of the network, where n
4
is equal to the number of possible relation types for the relation classification system. Each output can
be then interpreted as the confidence score of the corresponding relation. This score can be interpreted
as a conditional probability by applying a softmax operation (see Section 3.6).
3.6 Backpropagation Training
The DNN based relation classification method proposed here could be stated as a quintuple ? =
(X,N,W
1
,W
2
,W
3
)
6
. In this paper, each input sentence is considered independently. Given an in-
put example s, the network with parameter ? outputs the vector o, where the i-th component o
i
contains
the score for relation i. To obtain the conditional probability p(i|x, ?), we apply a softmax operation over
all relation types:
p(i|x, ?) =
e
o
i
n
4
?
k=1
e
o
k
(6)
Given all our (suppose T ) training examples (x
(i)
; y
(i)
), we can then write down the log likelihood of the
parameters as follows:
J (?) =
T
?
i=1
log p(y
(i)
|x
(i)
, ?) (7)
To compute the network parameter ?, we maximize the log likelihood J(?) using a simple optimization
technique called stochastic gradient descent (SGD). N,W
1
,W
2
and W
3
are randomly initialized and
X is initialized using the word embeddings. Because the parameters are in different layers of the neural
network, we implement the backpropagation algorithm: the differentiation chain rule is applied through
the network until the word embedding layer is reached by iteratively selecting an example (x, y) and
applying the following update rule.
? ? ? + ?
? log p(y|x, ?)
??
(8)
4 Dataset and Evaluation Metrics
To evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset (Hen-
drickx et al., 2010). The dataset is freely available
7
and contains 10,717 annotated examples, including
8,000 training instances and 2,717 test instances. There are 9 relationships (with two directions) and
an undirected Other class. The following are examples of the included relationships: Cause-Effect,
Component-Whole and Entity-Origin. In the official evaluation framework, directionality is taken into
account. A pair is counted as correct if the order of the words in the relationship is correct. For example,
both of the following instances S
1
and S
2
have the relationship Component-Whole.
S
1
: The [haft]
e
1
of the [axe]
e
2
is make ? ? ? ? Component-Whole(e
1
,e
2
)
S
2
: This [machine]
e
1
has two [units]
e
2
? ? ? ? Component-Whole(e
2
,e
1
)
6
N represents the word embeddings of WordNet hypernyms.
7
http://docs.google.com/View?id=dfvxd49s 36c28v9pmw
2340
?# Window size
1 2 3 4 5 6 7
F1
72
74
76
78
80
82
# Hidden layer 1
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
# Hidden layer 2
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
Figure 3: Effect of hyperparameters.
However, these two instances cannot be classified into the same category because Component-
Whole(e
1
,e
2
) and Component-Whole(e
2
,e
1
) are different relationships. Furthermore, the official rank-
ing of the participating systems is based on the macro-averaged F1-scores for the nine proper relations
(excluding Other). To compare our results with those obtained in previous studies, we adopt the macro-
averaged F1-score and also account for directionality into account in our following experiments
8
.
5 Experiments
In this section, we conduct three sets of experiments. The first is to test several variants via cross-
validation to gain some understanding of how the choice of hyperparameters impacts upon the perfor-
mance. In the second set of experiments, we make comparison of the performance among the convolu-
tional DNN learned features and various traditional features. The goal of the third set of experiments is
to evaluate the effectiveness of each extracted feature.
5.1 Parameter Settings
In this section, we experimentally study the effects of the three parameters in our proposed method:
the window size in the convolutional component w, the number of hidden layer 1, and the number of
hidden layer 2. Because there is no official development dataset, we tuned the hyperparameters by trying
different architectures via 5-fold cross-validation.
In Figure 3, we respectively vary the number of hyper parameters w, n
1
and n
2
and compute the F1.
We can see that it does not improve the performance when the window size is greater than 3. Moreover,
because the size of our training dataset is limited, the network is prone to overfitting, especially when
using large hidden layers. From Figure 3, we can see that the parameters have a limited impact on the
results when increasing the numbers of both hidden layers 1 and 2. Because the distance dimension has
little effect on the result (this is not illustrated in Figure 3), we heuristically choose d
e
= 5. Finally,
the word dimension and learning rate are the same as in Collobert et al. (2011). Table 2 reports all the
hyperparameters used in the following experiments.
Hyperparameter Window size Word dim. Distance dim. Hidden layer 1 Hidden layer 2 Learning rate
Value w = 3 n = 50 d
e
= 5 n
1
= 200 n
2
= 100 ? = 0.01
Table 2: Hyperparameters used in our experiments.
5.2 Results of Comparison Experiments
To obtain the final performance of our automatically learned features, we select seven approaches as com-
petitors to be compared with our method in Table 3. The first five competitors are described in Hendrickx
et al. (2010), all of which use traditional features and employ SVM or MaxEnt as the classifier. These
systems design a series of features and take advantage of a variety of resources (WordNet, ProBank,
and FrameNet, for example). RNN represents recursive neural networks for relation classification, as
8
The corpus contains a Perl-based automatic evaluation tool.
2341
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, stemming, syntactic patterns, WordNet 74.8
MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6
SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank,
FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner
82.2
RNN - 74.8
POS, NER, WordNet 77.6
MVRNN - 79.1
POS, NER, WordNet 82.4
Proposed word pair, words around word pair, WordNet 82.7
Table 3: Classifier, their feature sets and the F1-score for relation classification.
proposed by Socher et al. (2012). This method learns vectors in the syntactic tree path that connect two
nominals to determine their semantic relationship. The MVRNN model builds a single compositional
semantics for the minimal constituent, including both nominals as RNN (Socher et al., 2012). It is almost
certainly too much to expect a single fixed transformation to be able to capture the meaning combination
effects of all natural language operators. Thus, MVRNN assigns a matrix to every word and modifies the
meanings of other words instead of only considering word embeddings in the recursive procedure.
Table 3 illustrates the macro-averaged F1 measure results for these competing methods along with the
resources, features and classifier used by each method. Based on these results, we make the following
observations:
(1) Richer feature sets lead to better performance when using traditional features. This improvement
can be explained by the need for semantic generalization from training to test data. The quality of
traditional features relies on human ingenuity and prior NLP knowledge. It is almost impossible to
manually choose the best feature sets.
(2) RNN and MVRNN contain feature learning procedures; thus, they depend on the syntactic tree used
in the recursive procedures. Errors in syntactic parsing inhibit the ability of these methods to learn
high quality features. RNN cannot achieve a higher performance than the best method that uses
traditional features, even when POS, NER and WordNet are added to the training dataset. Compared
with RNN, the MVRNN model can capture the meaning combination effectively and achieve a higher
performance.
(3) Our method achieves the best performance among all of the compared methods. We also perform
a t-test (p 6 0.05), which indicates that our method significantly outperforms all of the compared
methods.
5.3 The Effect of Learned Features
Feature Sets F1
Lexical L1 34.7
+L2 53.1
+L3 59.4
+L4 65.9
+L5 73.3
Sentence WF 69.7
+PF 78.9
Combination all 82.7
Table 4: Score obtained for various sets of features on for the test set. The bottom portion of the table
shows the best combination of lexical and sentence level features.
In our method, the network extract lexical and sentence level features. The lexical level features pri-
marily contain five sets of features (L1 to L5). We performed ablation tests on the five sets of features
from the lexical part of Table 4 to determine which type of features contributed the most. The results are
2342
presented in Table 4, from which we can observe that our learned lexical level features are effective for
relation classification. The F1-score is improved remarkably when new features are added. Similarly, we
perform experiment on the sentence level features. The system achieves approximately 9.2% improve-
ments when adding PF. When all of the lexical and sentence level features are combined, we achieve the
best result.
6 Conclusion
In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence
level features for relation classification. In the network, position features (PF) are successfully proposed
to specify the pairs of nominals to which we expect to assign relation labels. The system obtains a
significant improvement when PF are added. The automatically learned features yield excellent results
and can replace the elaborately designed features that are based on the outputs of existing NLP tools.
Acknowledgments
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61272332, 61333018, 61202329, 61303180).
This work was supported in part by Noah?s Ark Lab of Huawei Tech. Co. Ltd. We thank the anonymous
reviewers for their insightful comments.
References
Nguyen Bach and Sameer Badaskar. 2007. A review of relation extraction. Literature review for Language and
Statistics II.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In
Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language
Processing, pages 724?731.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2005. Unsupervised feature selection for relation
extraction. In Proceedings of the International Joint Conference on Natural Language Processing, pages 262?
267.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zellig Harris. 1954. Distributional structure. Word, 10(23):146?162.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from
large corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages
415?422.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization
of recursive neural networks for semantic relation classification. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, pages 1372?1376.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O. S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic
Evaluation, SemEval ?10, pages 33?38.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 541?
550.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for
extracting relations. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics
on Interactive poster and demonstration sessions.
2343
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003?1011.
Raymond J Mooney and Razvan C Bunescu. 2005. Subsequence kernels for relation extraction. In Advances in
neural information processing systems, pages 171?178.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference
on Computational Linguistics, pages 697?704.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery
in databases: Part III, pages 148?163.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis
to extract relations from web documents. In Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 712?717.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers - Volume 1, pages 721?729.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The
Journal of Machine Learning Research, 3:1083?1106.
GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
2344
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1346?1356, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Opinion Target Extraction Using Word-Based Translation Model 
 
Kang Liu, Liheng Xu, Jun Zhao 
 
National Laboratory of Pattern Recognition,  
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn  
 
 
Abstract 
This paper proposes a novel approach to 
extract opinion targets based on word-
based translation model (WTM). At first, 
we apply WTM in a monolingual scenario 
to mine the associations between opinion 
targets and opinion words. Then, a graph-
based algorithm is exploited to extract 
opinion targets, where candidate opinion 
relevance estimated from the mined 
associations, is incorporated with candidate 
importance to generate a global measure. 
By using WTM, our method can capture 
opinion relations more precisely, especially 
for long-span relations. In particular, 
compared with previous syntax-based 
methods, our method can effectively avoid 
noises from parsing errors when dealing 
with informal texts in large Web corpora. 
By using graph-based algorithm, opinion 
targets are extracted in a global process, 
which can effectively alleviate the problem 
of error propagation in traditional 
bootstrap-based methods, such as Double 
Propagation. The experimental results on 
three real world datasets in different sizes 
and languages show that our approach is 
more effective and robust than state-of-art 
methods. 
1 Introduction 
With the rapid development of e-commerce, most 
customers express their opinions on various kinds 
of entities, such as products and services. These 
reviews not only provide customers with useful 
information for reference, but also are valuable for 
merchants to get the feedback from customers and 
enhance the qualities of their products or services. 
Therefore, mining opinions from these vast 
amounts of reviews becomes urgent, and has 
attracted a lot of attentions from many researchers.  
In opinion mining, one fundamental problem is 
opinion target extraction. This task is to extract 
items which opinions are expressed on. In reviews, 
opinion targets are usually nouns/noun phrases. 
For example, in the sentence of ?The phone has a 
colorful and even amazing screen?, ?screen? is an 
opinion target. In online product reviews, opinion 
targets often are products or product features, so 
this task is also named as product feature 
extraction in previous work (Hu et al 2004; Ding 
et al 2008; Liu et al 2005; Popescu et al 2005; 
Wu et al 2005; Su et al 2008).  
To extract opinion targets, many studies 
regarded opinion words as strong indicators (Hu et 
al., 2004; Popescu et al 2005; Liu et al 2005; 
Qiu et al 2011; Zhang et al 2010), which is 
based on the observation that opinion words are 
usually located around opinion targets, and there 
are associations between them. Therefore, most 
pervious methods iteratively extracted opinion 
targets depending upon the associations between 
opinion words and opinion targets (Qiu et al 2011; 
Zhang et al 2010). For example, ?colorful? and 
?amazing? is usually used to modify ?screen? in 
reviews about cell phone, so there are strong 
associations between them. If ?colorful? and 
?amazing? had been known to be opinion words, 
?screen? is likely to be an opinion target in this 
domain. In addition, the extracted opinion targets 
can be used to expand more opinion words 
according to their associations. It?s a mutual 
reinforcement procedure. 
Therefore, mining associations between opinion 
targets and opinion words is a key for opinion 
1346
target extraction (Wu et al 2009). To this end, 
most previous methods (Hu et al 2004; Ding et al 
2004; Wang et al 2008), named as adjacent 
methods, employed the adjacent rule, where an 
opinion target was regarded to have opinion 
relations with the surrounding opinion words in a 
given window. However, because of the limitation 
of window size, opinion relations cannot be 
captured precisely, especially for long-span 
relations, which would hurt estimating associations 
between opinion targets and opinion words. To 
resolve this problem, several studies exploited 
syntactic information such as dependency trees 
(Popescu et al 2005; Qiu et al 2009; Qiu et al 
2011; Wu et al 2009; Zhang et al 2010). If the 
syntactic relation between an opinion word and an 
opinion target satisfied a designed pattern, then 
there was an opinion relation between them. 
Experiments consistently reported that syntax-
based methods could yield better performance than 
adjacent methods for small or medium corpora 
(Zhang et al 2010). The performance of syntax-
based methods heavily depends on the parsing 
performance. However, online reviews are often 
informal texts (including grammar mistakes, typos, 
improper punctuations etc.). As a result, parsing 
may generate many mistakes. Thus, for large 
corpora from Web including a great deal of 
informal texts, these syntax-based methods may 
suffer from parsing errors and introduce many 
noises. Furthermore, this problem maybe more 
serious on non-English language reviews, such as 
Chinese reviews, because that the performances of 
parsing on these languages are often worse than 
that on English. 
To overcome the weakness of the two kinds of 
methods mentioned above, we propose a novel 
unsupervised approach to extract opinion targets 
by using word-based translation model (WTM). 
We formulate identifying opinion relations 
between opinion targets and opinion words as a 
word alignment task. We argue that an opinion 
target can find its corresponding modifier through 
monolingual word alignment. For example in 
Figure 1, the opinion words ?colorful? and 
?amazing? are aligned with the target ?screen? 
through word alignment. To this end, we use WTM 
to perform monolingual word alignment for mining 
associations between opinion targets and opinion 
words. In this process, several factors, such as 
word co-occurrence frequencies, word positions 
etc., can be considered globally. Compared with 
adjacent methods, WTM doesn?t identify opinion 
relations between words in a given window, so 
long-span relations can be effectively captured 
(Liu et al 2009). Compared with syntax-based 
methods, without using parsing, WTM can 
effectively avoid errors from parsing informal texts. 
So it will be more robust. In addition, by using 
WTM, our method can capture the ?one-to-many? 
or ?many-to-one? relations (?one-to-many? means 
that, in a sentence one opinion word modifies 
several opinion targets, and ?many-to-one? means 
several opinion words modify one opinion target). 
Thus, it?s reasonable to expect that WTM is likely 
to yield better performance than traditional 
methods for mining associations between opinion 
targets and opinion words.  
Based on the mined associations, we extract 
opinion targets in a ranking framework. All 
nouns/noun phrases are regarded as opinion target 
candidates. Then a graph-based algorithm is 
exploited to assign confidences to each candidate, 
in which candidate opinion relevance and 
importance are incorporated to generate a global 
measure. At last, the candidates with higher ranks 
are extracted as opinion targets. Compared with 
most traditional methods (Hu et al2004; Liu et al 
2005; Qiu et al 2011), we don?t extract opinion 
targets iteratively based on the bootstrapping 
strategy, such as Double Propagation (Qiu et al 
2011), instead all candidates are dynamically 
ranked in a global process. Therefore, error 
propagation can be effectively avoided and the 
performance can be improved.  
 
 Figure 1: Word-based translation model for 
opinion relation identification 
The main contributions of this paper are as 
follows. 
1) We formulate the opinion relation 
identification between opinion targets and 
opinion words as a word alignment task. To 
our best knowledge, none of previous methods 
deal with this task using monolingual word 
alignment model (in Section 3.1). 
Translation 
The phone has a colorful and even amazing screen 
The phone has a colorful and even amazing screen 
1347
2) We propose a graph-based algorithm for 
opinion target extraction in which candidate 
opinion relevance and importance are 
incorporated into a unified graph to estimate 
candidate confidence. Then the candidates 
with higher confidence scores are extracted as 
opinion targets (in Section 3.2). 
3) We have performed experiments on three 
datasets in different sizes and languages. The 
experimental results show that our approach 
can achieve performance improvement over 
the traditional methods. (in Section 4). 
The rest of the paper is organized as follows. In 
the next section, we will review related work in 
brief. Section 3 describes our approach in detail. 
Then experimental results will be given in Section 
4. At the same time, we will give some analysis 
about the results. Finally, we give the conclusion 
and the future work. 
2 Related Work 
Many studies have focused on the task of opinion 
target extraction, such as (Hu et al 2004; Ding et 
al., 2008; Liu et al 2006; Popescu et al 2005; 
Wu et al 2005; Wang et al 2008; Li et al 2010; 
Su et al 2008; Li et al 2006). In general, the 
existing approaches can be divided into two main 
categories: supervised and unsupervised methods. 
In supervised approaches, the opinion target 
extraction task was usually regarded as a sequence 
labeling task (Jin et al2009; Li et al2010; Wu et 
al., 2009; Ma et al2010; Zhang et al 2009). Jin et 
al. (2009) proposed a lexicalized HMM model to 
perform opinion mining. Li et al(2010) proposed 
a Skip-Tree CRF model for opinion target 
extraction. Their methods exploited three 
structures including linear-chain structure, 
syntactic structure, and conjunction structure. In 
addition, Wu et al(2009) utilized a SVM classifier 
to identify relations between opinion targets and 
opinion expressions by leveraging phrase 
dependency parsing. The main limitation of these 
supervised methods is that labeling training data 
for each domain is impracticable because of the 
diversity of the review domains.  
In unsupervised methods, most approaches 
regarded opinion words as the important indicators 
for opinion targets (Hu et al 2004; Popsecu et al 
2005; Wang et al 2008; Qiu et al 2011; Zhang et 
al., 2010). The basic idea was that reviewers often 
use the same opinion words when they comment 
on the similar opinion targets. The extraction 
procedure was often a bootstrapping process which 
extracted opinion words and opinion targets 
iteratively, depending upon their associations. 
Popsecu et al(2005) used syntactic patterns to 
extract opinion target candidates. After that they 
computed the point-wise mutual information (PMI) 
score between a candidate and a product category 
to refine the extracted results. Hu et al(2004) 
exploited an association rule mining algorithm and 
frequency information to extract frequent explicit 
product features. The adjective nearest to the 
frequent explicit feature was extracted as an 
opinion word. Then the extracted opinion words 
were used to extract infrequent opinion targets. 
Wang et al(2008) adopted the similar idea, but 
their method needed a few seeds to weakly 
supervise the extraction process. Qiu et al(2009, 
2011) proposed a Double Propagation method to 
expand a domain sentiment lexicon and an opinion 
target set iteratively. They exploited direct 
dependency relations between words to extract 
opinion targets and opinion words iteratively. The 
main limitation of Qiu?s method is that the patterns 
based on dependency parsing tree may introduce 
many noises for the large corpora (Zhang et al 
2010). Meanwhile, Double Propagation is a 
bootstrapping strategy which is a greedy process 
and has the problem of error propagation. Zhang et 
al. (2010) extended Qiu?s method. Besides the 
patterns used in Qiu?s method, they adopted some 
other patterns, such as phrase patterns, sentence 
patterns and ?no? pattern, to increase recall. In 
addition they used the HITS (Klernberg et al 1999) 
algorithm to compute the feature relevance scores, 
which were simply multiplied by the log of feature 
frequencies to rank the extracted opinion targets. In 
this way, the precision of result can be improved.  
3 Opinion Target Extraction Using 
Word-Based Translation Model 
3.1 Method Framework 
As mentioned in the first section, our approach for 
opinion target extraction is composed of the 
following two main components:  
1) Mining associations between opinion targets 
and opinion words: Given a collection of 
reviews, we adopt a word-based translation 
1348
model to identify potential opinion relations in 
all sentences, and then the associations 
between opinion targets and opinion words are 
estimated.  
2) Candidate confidence estimation: Based on 
these associations, we exploit a graph-based 
algorithm to compute the confidence of each 
opinion target candidate. Then the candidates 
with higher confidence scores are extracted as 
opinion targets.  
3.2 Mining associations between opinion 
targets and opinion words using Word-
based Translation Model 
This component is to identify potential opinion 
relations in sentences and estimate associations 
between opinion targets and opinion words. We 
assume opinion targets and opinion words 
respectively to be nouns/noun phrases and 
adjectives, which have been widely adopted in 
previous work (Hu et al 2004; Ding et al 2008; 
Wang et al 2008; Qiu et al 2011). Thus, our aim 
is to find potential opinion relations between 
nouns/noun phrases and adjectives in sentences, 
and calculate the associations between them. As 
mentioned in the first section, we formulate 
opinion relation identification as a word alignment 
task. We employ the word-based translation model 
(Brown et al1993) to perform monolingual word 
alignment, which has been widely used in many 
tasks, such as collocation extraction (Liu et al 
2009), question retrieval (Zhou et al 2011) and so 
on. In our method, every sentence is replicated to 
generate a parallel corpus, and we apply the 
bilingual word alignment algorithm to the 
monolingual scenario to align a noun/noun phase 
with its modifier. 
Given a sentence with n words 
1 2{ , ,..., }nS w w w? , the word alignment 
{( , ) | [1, ]}iA i a i n? ? can be obtained by 
maximizing the word alignment probability of the 
sentence as follows. 
?=arg max ( | )
A
A P A S
                   (1) 
where ( , )ii a  means that a noun/noun phrase at 
positioni  is aligned with an adjective at position ia . 
If we directly use this alignment model to our task, 
a noun/noun phrase may align with the irrelevant 
words other than adjectives, like prepositions or 
conjunctions and so on. Thus, in the alignment 
procedure, we introduce some constrains: 1) 
nouns/noun phrases (adjectives) must be aligned 
with adjectives (nouns/noun phrases) or null words; 
2) other words can only align with themselves. 
Totally, we employ the following 3 WTMs (IBM 
1~3) to identify opinion relations. 
1
1
( | ) ( | )j
n
IBM j a
j
P A S t w w?
?
??
 
2
1
( | ) ( | ) ( | , )j
n
IBM j a j
j
P A S t w w d j a n?
?
??
 
3
1 1
( | ) ( | ) ( | ) ( | , )j
n n
IBM i i j a j
i j
P A S n w t w w d j a n??
? ?
?? ?
(2) 
There are three main factors: ( | )jj at w w
, 
( | , )jd j a n
and ( | )i in w? , which respectively 
models different information.  
1) ( | )jj at w w
models the co-occurrence 
information of two words in corpora. If an 
adjective co-occurs with a noun/noun phrase 
frequently in the reviews, this adjective has high 
association with this noun/noun phrase. For 
example, in reviews of cell phone, ?big? often co-
occurs with ?phone?s size?, so ?big? has high 
association with ?phone?s size?. 
2) ( | , )jd j a l
 models word position information, 
which describes the probability of a word in 
position 
ja aligned with a word in position j .  
3) ( | )i in w? models the fertility of words, which 
describe the ability of a word for ?one-to-many? 
alignment. 
i? denotes the number of words that are 
aligned with 
iw . For example, ?Iphone4 has 
amazing screen and software?. In this sentence, 
?amazing? is used to modify two words: ?screen? 
and ?software?. So? equals to 2 for ?amazing?.  
Therefore, in Eq. (2), 
1( | )IBMP A S?  only models 
word co-occurrence information. 
2 ( | )IBMP A S?  
additionally employs word position information. 
Besides these two information, 
3( | )IBMP A S?  
considers the ability of a word for ?one-to-many? 
alignment. In the following experiments section, 
we will discuss the performance difference among 
these models in detail. Moreover, these models 
1349
may capture ?one-to-many? or ?many-to-one? 
opinion relations (mentioned in the first section). 
In our knowledge, it isn?t specifically considered 
by previous methods including adjacent methods 
and syntax-based methods. Meanwhile ? the 
alignment results may contain empty-word 
alignments, which means a noun/noun phrase has 
no modifier or an adjective modify nothing in the 
sentence. 
After gathering all word pairs from the review 
sentences, we can estimate the translation 
probabilities between nouns/noun phrases and 
adjectives as follows. 
( , )( | ) ( )
N A
N A
A
Count w wp w w Count w?
           (3) 
where ( | )N Ap w w means the translation 
probabilities from adjectives to nouns/noun 
phrases. Similarly, we can obtain translation 
probability ( | )A Np w w . Therefore, similar to (Liu 
et al2009), the association between a noun/noun 
phrase and an adjective is estimated as follows. 
1| |
( , )
( ( ) (1 ) ( ))
N A
N NA A
Association w w
t p w w t p w w ?? ? ?
    (4) 
where t is the harmonic factor to combine these 
two translation probabilities. In this paper, we set 
0.5t ? . For demonstration, we give some 
examples in Table 1. We can see that our method 
using WTM can successfully capture associations 
between opinion targets and opinion words. 
 battery life sound software 
wonderful 0.000 0.042 0.000 
poor 0.032 0.000 0.026 
long 0.025 0.000 0.000 
Table 1: Examples of associations between opinion 
targets and opinion words. 
3.3 Candidate Confidence Estimation 
In this component, we compute the confidence of 
each opinion target candidate and rank them. The 
candidates with higher confidence are regarded as 
the opinion targets. We argue that the confidence 
of a candidate is determined by two factors: 1) 
Opinion Relevance; 2) Candidate Importance. 
Opinion Relevance reflects the degree that a 
candidate is associated to opinion words. If an 
adjective has higher confidence to be an opinion 
word, the noun/noun phrase it modifies will have 
higher confidence to be an opinion target. 
Similarly, if a noun/noun phrase has higher 
confidence to be an opinion target, the adjective 
which modifies it will be highly possible to be an 
opinion word. It?s an iterative reinforcement 
process, which indicates that existing graph-based 
algorithms are applicable.  
Candidate Importance reflects the salience of a 
candidate in the corpus. We assign an importance 
score to an opinion target candidate f according to 
its -tf idf score, which is further normalized by the 
sum of -tf idf scores of all candidates. 
- ( )( )
- ( )
c
tf idf cImportance c
tf idf c
??
              (5) 
where c represents a candidate, tf is the term 
frequency in the dataset, and df is computed by 
using the Google n-gram corpus1. 
To model these two factors, a bipartite graph is 
constructed, the vertices of which include all 
nouns/noun phrases and adjectives. As shown in 
Figure 2, the white vertices represent nouns/noun 
phrases and the gray vertices represent adjectives. 
An edge between a noun/noun phrase and an 
adjective represents that there is an opinion 
relation between them. The weight on the edges 
represents the association between them, which are 
estimated by using WTM, as shown in Eq. (4).  
To estimate the confidence of each candidate on 
this bipartite graph, we exploit a graph-based 
algorithm, where we use C to represent candidate 
confidence vector, a 1n? vector. We set the 
candidate initial confidence with candidate 
importance score, i.e. 0C S? , where S is the 
candidate initial confidence vector and each item 
in S is computed using Eq. (5). 
 
 
Figure 2: Bipartite graph for modeling relations 
between opinion targets and opinion words 
                                                          
1 http://books.google.com/ngrams/datasets 
..... 
..... 
Opinion Word Candidates (adjectives) 
Opinion Target Candidates (nouns/noun phrases) 
1350
Then we compute the candidate confidence by 
using the following iterative formula. 
1t T tC M M C? ? ? ?                  (6) 
where tC is the candidate confidence vector at 
time t , and 1tC ?  is the candidate confidence 
vector at time 1t ? . M is an opinion relevance 
matrix, a m n? matrix, where ,i jM is the 
associated weight between a noun/noun phrase 
i and an adjective j . 
To consider the candidate importance scores, we 
introduce a reallocate condition: combining the 
candidate opinion relevance with the candidate 
importance at each step. Thus we can get the final 
recursive form of the candidate confidence as 
follows. 
1 (1 )t T tC M M C S? ?? ? ? ? ? ? ? ?       (7) 
where [0,1]?? is the proportion of candidate 
importance in the candidate confidence. When 
1? ? , the candidate confidence is completely 
determined by the candidate importance; and when 
0? ? , the candidate confidence is determined by 
the candidate opinion relevance. We will discuss 
its effect in the section of experiments.  
To solve Eq. (7), we rewrite it as the following 
form. 
1( (1 ) )TC I M M S? ? ?? ? ? ? ? ? ?        (8) 
where I is an identity matrix. To handle the 
inverse of the matrix, we expand the Eq. (8) as a 
power series as following. 
[ ]kC I B B S?? ? ? ? ? ?              (9) 
where (1 ) TB M M?? ? ? ?and [0, )k? ? is an 
approximate factor. In experiments, we set 
100k ? . Using this equation, we estimate 
confidences for opinion target candidates. The 
candidates with higher confidence scores than the 
threshold will be extracted as the opinion targets.  
4 Experiments 
4.1 Datasets and Evaluation Metrics 
In our experiments, we select three real world 
datasets to evaluate our approach. The first dataset 
is COAE2008 dataset22, which contains Chinese 
reviews of four different products. The detailed 
                                                          
2 http://ir-china.org.cn/coae2008.html 
information can be seen in Table 2. Moreover, to 
evaluate our method comprehensively, we collect a 
larger collection named by Large, which includes 
three corpora from three different domains and 
different languages. The detailed statistical 
information of this dataset is also shown in Table 2. 
Restaurant is crawled from the Chinese Web site: 
www.dianping.com. The Hotel and MP3 3  were 
used in (Wang et al 2011), which are respectively 
clawed from www.tripadvisor.com and 
www.amazon.com. For each collection, we 
perform random sampling to generate testing 
dataset, which include 6,000 sentences for each 
domain. Then the opinion targets in Large were 
manually annotated as the gold standard for 
evaluations. Three annotators are involved in the 
annotation process as follows. First, every 
noun/noun phrase and its contexts in review 
sentences are extracted. Then two annotators were 
required to judge whether every noun/noun phrase 
is opinion target or not. If a conflict happens, a 
third annotator will make judgment for finial 
results. The inter-agreement was 0.72. In total, we 
respectively obtain 1,112, 1,241 and 1,850 opinion 
targets in Hotel, MP3 and Restaurant. The third 
dataset is Customer Review Datasets 4  (English 
reviews of five products), which was also used in 
(Hu et al 2004; Qiu et al 2011). They have 
labeled opinion targets. The detailed information 
can be found in (Hu et al 2004).  
 
Domain Language #Sentence #Reviews 
Camera Chinese 2075 137 
Car Chinese 4783 157 
Laptop Chinese 1034 56 
Phone Chinese 2644 123 
(a) COAE2008 dataset2 
Domain Language #Sentence #Reviews 
Hotel English 1,855,351 185,829 
MP3 English 289,931 30,837 
Restaurant Chinese 1,683,129 395,124 
(b) Large 
Table 2: Experimental Data Sets, # denotes the size 
of the reviews/sentences 
In experiments, each review is segmented into 
sentences according to punctuations. Then 
sentences are tokenized and the part-of-speech of 
                                                          
3 http://sifaka.cs.uiuc.edu/~wang296/Data/index.html 
4 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
1351
Methods 
Camera Car Laptop Phone 
P R F P R F P R F P R F 
Hu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64 
DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72 
Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75 
Ours 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78 
Table 3: Experiments on COAE2008 dataset2 
Methods 
Hotel MP3 Restaurant 
P R F P R F P R F 
Hu 0.60  0.65  0.62  0.61  0.68  0.64  0.64  0.69  0.66  
DP 0.67  0.69  0.68  0.69  0.70  0.69  0.74  0.72  0.73  
Zhang 0.67  0.76  0.71  0.67  0.77  0.72  0.75  0.79  0.77  
Ours 0.71  0.80  0.75  0.70  0.82  0.76  0.80  0.84  0.82  
Table 4: Experiments on Large 
Methods 
D1 D2 D3 D4 D5 
P R F P R F P R F P R F P R F 
Hu 0.75  0.82  0.78  0.71  0.79  0.75  0.72  0.76  0.74  0.69  0.82  0.75  0.74  0.80  0.77  
DP 0.87  0.81  0.84  0.90  0.81  0.85  0.90  0.86  0.88  0.81  0.84  0.82  0.92  0.86  0.89  
Zhang 0.83  0.84  0.83  0.86  0.85  0.85  0.86  0.88  0.87  0.80  0.85  0.83  0.86  0.86  0.86  
Ours 0.84  0.85  0.84  0.87  0.85  0.86  0.88  0.89  0.88  0.81  0.85  0.83  0.89  0.87  0.88  
Table 5: Experiments on Customer Review Dataset 
each word is assigned. Stanford NLP tool5 is used 
to perform POS-tagging and dependency parsing. 
The method in (Zhu et al 2009) is used to identify 
noun phrases. We select precision, recall and F-
measure as the evaluation metrics. We also 
perform a significant test, i.e., a t-test with a 
default significant level of 0.05. 
4.2 Our Methods vs. State-of-art Methods 
To prove the effectiveness of our method, we 
select the following state-of-art unsupervised 
methods as baselines for comparison. 
1) Hu is the method described in (Hu et al 2004), 
which extracted opinion targets by using adjacent 
rule.  
2) DP is the method described in (Qiu et al 2011), 
which used Double Propagation algorithm to 
extract opinion targets depending on syntactic 
relations between words.  
3) Zhang is the method described in (Zhang et al 
2010), which is an extension of DP. They extracted 
opinion targets candidates using syntactic patterns 
and other specific patterns. Then HITS (Kleinberg 
1999) algorithm combined with candidate 
frequency is employed to rank the results for 
opinion target extraction.  
Hu is selected to represent adjacent methods for 
opinion target extraction. And DP and Zhang are 
                                                          
5 http://nlp.stanford.edu/software/tagger.shtml 
selected to represent syntax-based methods. The 
parameter settings in these three baselines are the 
same as the original papers. In special, for DP and 
Zhang, we used the same patterns for different 
language reviews. The overall performance results 
are shown in Table 3, 4 and 5, respectively, where 
?P? denotes precision, ?R? denotes recall and ?F? 
denotes F-measure. Ours denotes full model of our 
method, in which we use IBM-3 model for 
identifying opinion relations between words. 
Moreover, we set
max 2? ? in Eq. (2) and 0.3? ? in 
Eq. (7). From results, we can make the following 
observations. 
1) Ours achieves performance improvement over 
other methods. This indicates that our method 
based on word-based translation model is 
effective for opinion target extraction.  
2) The graph-based methods (Ours and Zhang) 
outperform the methods using Double 
Propagation (DP). Similar observations have 
been made by Zhang et al(2010). The reason 
is that graph-based methods extract opinion 
targets in a global framework and they can 
effectively avoid the error propagation made 
by traditional methods based on Double 
Propagation. Moreover, Ours outperforms 
Zhang. We believe the reason is that Ours 
consider the opinion relevance and the 
candidate importance in a unified graph-based 
framework. By contrast, Zhang only simply 
1352
plus opinion relevance with frequency to 
determine the candidate confidence. 
3) In Table 4, the improvement made by Ours on 
Restaurant (Chinese reviews) is larger than 
that on Hotel and MP3 (English reviews). The 
same phenomenon can be found when we 
compare the improvement made by Ours in 
Table 3 (Chinese reviews) with that in Table 5 
(English reviews). We believe that reason is 
that syntactic patterns used in DP and Zhang 
were exploited based on English grammar, 
which may not be suitable to Chinese language. 
Moreover, another reason is that the 
performance of parsing on Chinese texts is not 
better than that on English texts, which will 
hurt the performance of syntax-based methods 
(DP and Zhang).  
4) Compared the results in Table 3 with the 
results in Table 4, we can observe that Ours 
obtains larger improvements with the increase 
of the data size. This indicates that our method 
is more effective for opinion target extraction 
than state-of-art methods, especially for large 
corpora. When the data size increase, the 
methods based on syntactic patterns will 
introduce more noises due to the parsing errors 
on informal texts. On the other side, Ours uses 
WTM other than parsing to identify opinion 
relations between words, and the noises made 
by inaccurate parsing can be avoided. Thus, 
Ours can outperform baselines. 
5) In Table 5, Ours makes comparable results 
with baselines in Customer Review Datasets, 
although there is a little loss in precision in 
some domains. We believe the reason is that 
the size of Customer Review Datasets is too 
small. As a result, WTM may suffer from data 
sparseness for association estimation. 
Nevertheless, the average recall is improved. 
An Example In Table 6, we show top 10 opinion 
targets extracted by Hu, DP, Zhang and Ours in 
MP3 of Large. In Hu and DP, since they didn?t 
rank the results, their results are ranked according 
to frequency in this experiment. The errors are 
marked in bold face. From these examples, we can 
see Ours extracts more correct opinion targets than 
others. In special, Ours outperforms Zhang. It 
indicates the effectiveness of our graph-based 
method for candidate confidence estimation. 
Moreover, Ours considers candidate importance 
besides opinion relevance, so some specific 
opinion targets are ranked to the fore, such as 
?voice recorder?, ?fm radio? and ?lcd screen?.  
4.3 Effect of Word-based Translation Model 
In this subsection, we aim to prove the 
effectiveness of our WTM for estimating 
associations between opinion targets and opinion 
words. For comparison, we select two baselines for 
comparison, named as Adjacent and Syntax. These 
baselines respectively use adjacent rule (Hu et al
2004; Wang et al 2008) and syntactic patterns 
(Qiu et al 2009) to identify opinion relations in 
sentences. Then the same method (Eq.3 and Eq.4) 
is used to estimate associations between opinion 
targets and opinion words. At last the same graph-
based method (in Section 3.3) is used to extract 
opinion targets. Due to the limitation of the space, 
the experimental results only on COAE2008 
dataset2 and Large are shown in Figure 3. 
 
 
Figure 3: Experimental comparison among 
different relation identification methods 
 
Hu quality, thing, drive, feature, battery, sound, 
time, music, price 
DP quality, battery, software, device, screen, file, 
thing, feature, battery life 
Zhang quality, size, battery life, hour, version, function, 
upgrade, number, music 
Ours quality, battery life, voice recorder, video, fm 
radio, battery, file system, screen, lcd screen 
Table 6: Top 10 opinion targets extracted by 
different methods. 
In Figure 3, we observe that Ours using WTM 
makes significant improvements compared with 
1353
two baselines, both on precision and recall. It 
indicates that WTM is effective for identifying 
opinion relations, which makes the estimation of 
the associations be more precise. 
4.4 Effect of Our Graph-based Method 
In this subsection, we aim to prove the 
effectiveness of our graph-based method for 
opinion target extraction. We design two baselines, 
named as WTM_DP and WTM_HITS. Both 
WTM_DP and WTM_HITS use WTM to mine 
associations between opinion targets and opinion 
words. Then, WTM_DP uses Double Propagation 
adapted in (Wang et al2008; Qiu et al2009) to 
extract opinion targets, which only consider the 
candidate opinion relevance. WTM_HITS uses a 
graph-based method of Zhang et al(2010) to 
extract opinion targets, which consider both 
candidate opinion relevance and frequency. Figure 
4 gives the experimental results on COAE2008 
dataset2 and Large. In Figure 4, we can observe 
that our graph-based algorithm outperforms not 
only the method based on Double Propagation, but 
also the previous graph-based approach.  
 
 
Figure 4: Experimental Comparison between 
different ranking algorithms 
4.5 Parameter Influences 
4.5.1 Effect of Different WTMs 
In section 3, we use three different WTMs in Eq. 
(2) to identify opinion relations. In this subsection, 
we make comparison among them. Experimental 
results on COAE2008 dataset2 and Large are 
shown in Figure 5. Ours_1, Ours_2 and Ours_3 
respectively denote our method using different 
WTMs (IBM 1~3). From the results in Figure 5, 
we observe that Ours_2 outperforms Ours_1, 
which indicates that word position is useful for 
identifying opinion relations. Furthermore, Ours_3 
outperforms other models, which indicates that 
considering the fertility of a word can produce 
better performance. 
4.5.2 Effect of ?  
In our method, when we employ Eq. (7) to assign 
confidence score to each candidate, 
[0,1]?? decides the proportion of candidate 
importance in our method. Due to the limitation of 
space, we only show the F-measure of Ours on 
COAE2008 dataset2 and Large when varying ? in 
Figure 6.  
In Figure 6, curves increase firstly, and decrease 
with the increase of ? . The best performance is 
obtained when ? is around 0.3. It indicates that 
candidate importance and candidate opinion 
relevance are both important for candidate 
confidence estimation. The performance of opinion 
target extraction benefits from their combination. 
 
 
 
Figure 5. Experimental results by using different 
word-based translation model. 
 
 
Figure 6. Experimental results when varying ?  
1354
5 Conclusions and Future Work 
This paper proposes a novel graph-based approach 
to extract opinion targets using WTM. Compared 
with previous adjacent methods and syntax-based 
methods, by using WTM, our method can capture 
opinion relations more precisely and therefore be 
more effective for opinion target extraction, 
especially for large informal Web corpora.  
In future work, we plan to use other word 
alignment methods, such as discriminative model 
(Liu et al 2010) for this task. Meanwhile, we will 
add some syntactic information into WTM to 
constrain the word alignment process, in order to 
identify opinion relations between words more 
precisely. Moreover, we believe that there are 
some verbs or nouns can be opinion words and 
they may be helpful for opinion target extraction. 
And we think that it?s useful to add some prior 
knowledge of opinion words (sentiment lexicon) in 
our model for estimating candidate opinion 
relevance. 
Acknowledgements 
The work is supported by the National Natural 
Science Foundation of China (Grant No. 
61070106), the National Basic Research Program 
of China (Grant No. 2012CB316300), Tsinghua 
National Laboratory for Information Science and 
Technology (TNList) Cross-discipline Foundation 
and the Opening Project of Beijing Key Laboratory 
of Internet Culture and Digital Dissemination 
Research (Grant No. 5026035403). We thank the 
anonymous reviewers for their insightful 
comments. 
 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311.  
Xiaowen Ding, Bing Liu and Philip S. Yu. 2008. A 
Holistic Lexicon-Based Approach to Opinion Mining. 
In Proceedings of WSDM 2008. 
Xiaowen Ding and Bing Liu. 2010. Resolving Object 
and Attribute Reference in Opinion Mining. In 
Proceedings of COLING 2010. 
Mingqin Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. In Proceedings of 
KDD 2004 
Minqing Hu and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
AAAI-2004, San Jose, USA, July 2004. 
Wei Jin and Huang Hay Ho. A Novel Lexicalized 
HMM-based Learning Framework for Web Opinion 
Mining. In Proceedings of ICML 2009. 
Jon Klernberg. 1999. Authoritative Sources in 
Hyperlinked Environment. Journal of the ACM 46(5): 
604-632 
Zhuang Li, Feng Jing, Xiao-yan Zhu. 2006. Movie 
Review Mining and Summarization. In Proceedings 
of CIKM 2006 
Fangtao Li, Chao Han, Minlie Huang and Xiaoyan Zhu. 
2010. Structure-Aware Review Mining and 
Summarization. In Proceedings of COLING 2010. 
Zhichao Li, Min Zhang, Shaoping Ma, Bo Zhou, Yu 
Sun. Automatic Extraction for Product Feature 
Words from Comments on the Web. In Proceedings 
of AIRS 2009.  
Bing Liu, Hu Mingqing and Cheng Junsheng. 2005. 
Opinion Observer: Analyzing and Comparing 
Opinions on the Web. In Proceedings of WWW 2005 
Bing Liu. 2006. Web Data Mining: Exploring 
Hyperlinks, contents and usage data. Springer, 2006 
Bing Liu. 2010. Sentiment analysis and subjectivity. 
Handbook of Natural Language Processing, second 
edition, 2010. 
Yang Liu, Qun Liu, and Shouxun Lin. 2010. 
Discriminative word alignment by linear modeling. 
Computational Linguistics, 36(3):303?339. 
Zhanyi Liu, Haifeng Wang, Hua Wu and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Model. In Proceedings of EMNLP 
2009.  
Tengfei Ma and Xiaojun Wan. 2010. Opinion Target 
Extraction in Chinese News Comments. In 
Proceedings of COLING 2010. 
Popescu, Ana-Maria and Oren, Etzioni. 2005. 
Extracting produt fedatures and opinions from 
reviews. In Proceedings of EMNLP 2005 
Guang Qiu, Bing Liu., Jiajun Bu and Chun Che. 2009. 
Expanding Domain Sentiment Lexicon through 
Double Popagation. In Proceedings of IJCAI 2009 
Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen. 2011. 
Opinion Word Expansion and Target Extraction 
1355
through Double Propagation. Computational 
Linguistics, March 2011, Vol. 37, No. 1: 9.27 
Qi Su, Xinying Xu., Honglei Guo, Zhili Guo, Xian Wu, 
Xiaoxun Zhang, Bin Swen and Zhong Su. 2008. 
Hidden Sentiment Association in Chinese Web 
Opinion Mining. In Proceedings of WWW 2008 
Bo Wang, Houfeng Wang. Bootstrapping both Product 
Features and Opinion Words from Chinese Customer 
Reviews with Cross-Inducing. In Proceedings of 
IJCNLP 2008. 
Hongning Wang, Yue Lu and Chengxiang Zhai. 2011. 
Latent Aspect Rating Analysis without Aspect 
Keyword Supervision. In Proceedings of KDD 2011. 
Yuanbin Wu, Qi Zhang, Xuangjing Huang and Lide 
Wu, 2009, Phrase Dependency Parsing For Opinion 
Mining, In Proceedings of EMNLP 2009 
Lei Zhang, Bing Liu, Suk Hwan Lim and Eamonn 
O?Brien-Strain. 2010. Extracting and Ranking 
Product Features in Opinion Documents. In 
Proceedings of COLING 2010. 
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, 
Joseph Johnson, Xuanjing Huang. 2009. Mining 
Product Reviews Based on Shallow Dependency 
Parsing, In Proceedings of SIGIR 2009.  
Guangyou Zhou, Li Cai, Jun Zhao and Kang Liu. 2011. 
Phrase-based Translation Model for Question 
Retrieval in Community Question Answer Archives. 
In Proceedings of ACL 2011. 
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou and 
Muhua Zhu. 2009. Multi-aspect Opinion Polling 
from Textual Reviews. In Proceedings of CIKM 
2009. 
1356
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1092?1103,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Question Answering over Linked Data Using First-order Logic
?
Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{shizhu.he, kliu, yzzhang, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Question Answering over Linked Data
(QALD) aims to evaluate a question an-
swering system over structured data, the
key objective of which is to translate
questions posed using natural language
into structured queries. This technique
can help common users to directly ac-
cess open-structured knowledge on the
Web and, accordingly, has attracted much
attention. To this end, we propose a
novel method using first-order logic. We
formulate the knowledge for resolving
the ambiguities in the main three steps
of QALD (phrase detection, phrase-to-
semantic-item mapping and semantic item
grouping) as first-order logic clauses in a
Markov Logic Network. All clauses can
then produce interacted effects in a unified
framework and can jointly resolve all am-
biguities. Moreover, our method adopts a
pattern-learning strategy for semantic item
grouping. In this way, our method can
cover more text expressions and answer
more questions than previous methods us-
ing manually designed patterns. The ex-
perimental results using open benchmarks
demonstrate the effectiveness of the pro-
posed method.
1 Introduction
With the rapid development of the Web of Data,
many RDF datasets have been published as Linked
Data (Bizer et al., 2009), such as DBpedia (Auer
et al., 2007), Freebase (Bollacker et al., 2008)
and YAGO (Suchanek et al., 2007). The grow-
ing amount of Linked Data contains a wealth of
knowledge, including entities, classes and rela-
tions. Moreover, these linked data usually have
?
Shizhu He and Kang Liu have equal contribution to this
work.
complex structures and are highly heterogeneous.
As a result, there are gaps for users regarding ac-
cess. Although a few experts can write queries us-
ing structured languages (such as SPARQL) based
on their needs, this skill cannot be easily utilized
by common users (Christina and Freitas, 2014).
Thus, providing user-friendly, simple interfaces
to access these linked data becomes increasingly
more urgent.
Because of this, question answering over linked
data (QALD) (Walter et al., 2012) has recently
received much interest, and most studies on this
topic have focused on translating natural lan-
guage questions into structured queries (Freitas
and Curry, 2014; Yahya et al., 2012; Unger et al.,
2012; Shekarpour et al., 2013; Yahya et al., 2013;
Bao et al., 2014; Zou et al., 2014). For example,
with respect to the question
?Which software has been developed by organi-
zations founded in California, USA??,
the aim is to automatically convert this utterance
into an SPARQL query that contains the follow-
ing subject-property-object (SPO) triple format:
??url rdf:type dbo:Software, ?url dbo:developer ?x1,
?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace
dbr:California?
1
.
To fulfill this objective, existing systems (Lopez
et al., 2006; Unger et al., 2012; Yahya et al., 2012;
Zou et al., 2014) usually adopt a pipeline frame-
work that contains four major steps: 1) decompos-
ing the question and detecting phrases, 2) map-
ping the detected phrases into semantic items of
Linked Data, 3) grouping the mapped semantic
items into semantic triples, and 4) generating the
correct SPARQL query.
However, completing these four steps and con-
structing such a structured query is not easy. The
first three steps mentioned above are subject to the
1
The prefixes in semantic items indicate the source of
their vocabularies.
1092
problem of ambiguity, which is the major chal-
lenge in QALD. Using the question mentioned
above as an example, we can choose Califor-
nia or California, USA when detecting phrases,
the phrase California can be mapped to the en-
tity California State or California Film, and the class
Software (mapped from the phrase software) can
be matched with the first argument of the rela-
tion producer or developer (these two relations can
be mapped from the phrase developed). Previ-
ous methods (Lopez et al., 2006; Lehmann et
al., 2012; Freitas and Curry, 2014) have usu-
ally performed disambiguation at each step only,
and the subsequent step was performed based on
the disambiguation results in the previous step(s).
However, we argue that the three steps men-
tioned above have mutual effects. In the previ-
ous example, the phrase founded in (verb) can
be mapped to the entities (Founding of Rome and
Founder (company)), classes (Company and Depart-
ment) or relations (foundedBy and foundationPlace).
If we know that the phrase California can refer
to the entity California State, and which can be the
second argument of the relation foundationPlace,
together with a verb phrase being more likely
to be mapped to Relation, we should map the
phrase founded in to foundationPlace in this ques-
tion. Thus, we aim to determine if joint disam-
biguation is better than individual disambigua-
tion. (Question One)
In addition, previous systems usually employed
manually designed patterns to extract predicate-
argument structures that are used to guide the dis-
ambiguation process in the three steps mentioned
above (Yahya et al., 2012; Unger et al., 2012; Zou
et al., 2014). For example, (Yahya et al., 2012)
used only three dependency patterns to group the
mapped semantic items into semantic triples. Nev-
ertheless, these three manually designed patterns
miss many cases because of the diversity of the
question expressions. We gathered statistics on
144 questions and found that the macro-average
F1 and micro-average F1 of the three patterns
2
used in (Yahya et al., 2012) are only 62.8 and
66.2%, respectively. Furthermore, these specially
designed patterns may not be valid with variations
in domains or languages. Therefore, another im-
portant question arises: can we automatically
learn rules or patterns to achieve the same ob-
2
They are 1) verbs and their arguments, 2) adjectives and
their arguments and 3) propositionally modified tokens and
objects of prepositions.
jective? (Question Two)
Focusing on the two problems mentioned
above, this paper proposes a novel algorithm based
on a learning framework, Markov Logic Networks
(MLNs) (Richardson and Domingos, 2006), to
learn a joint model for constructing structured
queries from natural language utterances. MLN
is a statistical relational learning framework that
combines first-order logic and Markov networks.
The appealing property of MLN is that it is read-
ily interpretable by humans and that it is a natural
framework for performing joint learning. We for-
mulate the knowledge for resolving the ambigui-
ties in the main three steps of QALD (phrase de-
tection, phrase-to-semantic-item mapping and se-
mantic item grouping) as first-order logic clauses
in an MLN. In the framework of MLN, all clauses
will produce interacted effects that jointly resolve
all problems into a unified process. In this way,
the result in each step can be globally optimized.
Moreover, in contrast to previous methods, we
adopt a learning strategy to automatically learn
the patterns for semantic item grouping. We de-
sign several meta patterns as opposed to the spe-
cific patterns. In addition, these meta patterns are
formulated as the first-order logic formulas in the
MLN. The specific patterns can be generated by
these meta patterns based on the training data. The
model will learn the weights of each clause to de-
termine the most effective patterns for semantic
triple construction. In this way, with little effort,
our approach can cover more semantic expressions
and answer more questions than previous meth-
ods, which depend on manually designed patterns.
We evaluate the proposed method using several
benchmarks (QALD-1, QALD-3, QALD-4). The
experimental results demonstrate the advantage of
the joint disambiguation process mentioned above.
They also prove that our approach, employing
MLN to automatically learn the patterns of seman-
tic triple grouping, is effective. Our system can
answer more questions and obtain better perfor-
mance than the traditional methods based on man-
ually designed heuristic rules.
2 Background
2.1 Linked Data Sources
Linked Data consist of many relational data,
which are usually inter-linked as subject-property-
object (SPO) triple statements (such as using the
owl:sameAs relation). In this paper, we mainly use
1093
Subject(Arg1) Relation(Property) Object(Arg2) 
ProgrammingLanguage subClassOf Software 
Java_(programming_language) type Software 
Java_(programming_language) developer Oracle_Corporation 
Oracle_Corporation foundationPlace California_(State) 
foundationPlace domain Organisation 
California_(State) label ?California? 
California_(1977_film) label ?California? 
Oracle_Corporation numEmployees 118119(xsd:integer) 
 
Figure 1: Sample knowledge base facts.
DBpedia
3
and some classes from Yago
4
. These
knowledge bases (KBs) are composed of many on-
tological and instance statements, and all state-
ments are expressed by SPO triple facts. Figure
1 shows some triple fact samples from DBpedia.
Each fact is composed of three semantic items. A
semantic item can be an entity (California (State),
Oracle Corporation, etc.), a class (Software, Organ-
isation, etc.) or a relation (called a property
or predicate in some occasions). Some entities
are literals including strings, numbers and dates
(118119(xsd:integer), etc.). Relations contain stan-
dard Semantic Web relations (subClassOf, type, do-
main and label) and ontological relations (developer,
foundationPlace and numEmployees).
2.2 Task Statement
Given a knowledge base (KB), our objective is to
translate a natural language question q
NL
into a
formal language query q
FL
that targets the seman-
tic vocabularies given by the KB, and the query
q
FL
should capture the user information needs ex-
pressed by q
NL
.
Following (Yahya et al., 2012), we focus on the
factoid questions, and the answers to such ques-
tions are an entity or a set of entities. We ignore
the questions that need the aggregation
5
(max/min,
etc.) and negation operations. That is, we generate
queries that consist of a plentiful number of triple
patterns, which are multiple conjunctions of SPO
search conditions.
3 Framework
Figure 2 shows the entire framework of our system
for translating a question into a formal SPARQL
query. The first three steps address the input ques-
tion through 1) Phrase Detection (detecting pos-
sible phrases), 2) Phrase Mapping (mapping all
3
http://dbpedia.org/
4
http://www.mpi-inf.mpg.de/yago-naga/yago/
5
We can address the count query questions, which will
be explained in Section 3.
phrase candidates to the corresponding seman-
tic items), and 3) Feature Extraction (extracting
the linguistic features and semantic item features
from the question and the Linked Data, respec-
tively). As a result, a space of candidates is con-
structed, including possible phrases, mapped se-
mantic items and the possible argument match re-
lations among them. Next, the fourth step (In-
ference) formulates the joint disambiguation as a
generalized inference task. We employ rich fea-
tures and constraints (including hard and soft con-
straints) to infer a joint decision through an MLN.
Finally, with the inference results, we can con-
struct a semantic item query graph and generate
an executable SPARQL query. In the following
subsection, we demonstrate each step in detail.
1) Phrase detection. In this step, we detect
phrases (sequences of tokens) that probably indi-
cate semantic items in the KB. We do not use a
named entity recognizer (NER) because of its low
coverage. We perform testing on two commonly
used question corpora, QALD-3 and free917
6
, us-
ing the Stanford NER tool
7
. The results demon-
strate that only 51.5 and 23.8% of the NEs are
correctly recognized, respectively. To avoid miss-
ing useful phrases, we retain all n-grams as phrase
candidates, and then use some rules to filter them.
The rules include the following: the span length
must be less than 4 (accepting that all contiguous
tokens are capitalizations), the POS tag of the start
token must be jj, nn, rb and vb, all contiguous
capitalization tokens must not be split, etc. For
instance, software, developed by, organizations,
founded in and California are detected in the ex-
ample of the first section.
2) Phrase mapping. After the phrases are de-
tected, each phrase can be mapped to the corre-
sponding semantic item in KB (entity, class and
relation). For example, software is mapped to
dbo:Software, dbo:developer, etc., and California is
mapped to dbr:California, dbr:California (wine), etc.
For different types of semantic items, we use dif-
ferent techniques. For mapping phrases to en-
tities, considering that the entities in DBpedia
and Wikipedia are consistent, we employ anchor,
redirection and disambiguation information from
Wikipedia. For mapping phrases to classes, con-
sidering that classes have lexical variation, espe-
cially synonyms, e.g., dbo:Film can be mapped
6
http://www.cis.temple.edu/?yates/open-sem-
parsing/index.html
7
http://nlp.stanford.edu/software/CRF-NER.shtml
1094
Which software has been developed by
organizations founded in California, USA? software, developed, developed by, organizations,
founded, founded in, California, USA
software
developed by
...
...
...
California
phraseIndex
phrasePosTag
resourceType
priorMatchScore
hasMeanWord
phraseDepTag
hasRelatedness
...
isTypeCompatible
hasPhrase hasResource
hasRelation
Figure 2: Framework of our system.
from film, movie and show, we compute the simi-
larity between the phrase and the class in the KB
with the word2vec tool
8
. The word2vec tool com-
putes fixed-length vector representations of words
with a recurrent-neural-network based language
model (Mikolov et al., 2010). The similarity scor-
ing methods are introduced in Section 4.2. Then,
the top-N most similar classes for each phrase are
returned. For mapping phrases to relations, we
employ the resources from PATTY (Nakashole et
al., 2012) and ReVerb (Fader et al., 2011). Specif-
ically, we first compute the associations between
the ontological relations in DBpedia and the re-
lation patterns in PATTY and ReVerb through in-
stance alignments as in (Berant et al., 2013). Next,
if a detected phrase is matched to some relation
pattern, the corresponding ontological relations in
DBpedia will be returned as a candidate. This step
only generates candidates for every possible map-
ping, and the decision of the best selection will be
performed in the next step.
3) Feature extraction and joint inference.
There exist ambiguities in phrase detection and in
mapping phrases to semantic items. This step fo-
cuses on addressing these ambiguities and deter-
8
https://code.google.com/p/word2vec/
mining the argument match relations among the
mapped semantic items. This is the core compo-
nent of our system, and it performs disambigua-
tion in a unified manner. First, feature extraction
is performed to prepare a rich number of features
from the input question and from the KB. Next,
the disambiguation is performed in a joint fashion
with a Markov Logic Network. Detailed informa-
tion will be presented in Section 4.
4) Semantic item query graph construction.
Based on the inference results, we construct a
query graph. The vertices contain the following:
the detected phrase, the token span indexes of
the phrases, the mapped semantic items and their
types. The edge indicates the argument match re-
lation between two semantic items. For example,
we use 1 2 to indicate that the first argument of
an item matches the second argument of another
item
9
. The right bottom in Figure 2 shows an ex-
ample of this.
5) Query generation. The SPARQL queries
require the grouped triples of semantic items.
Thus, in this step, we convert a query graph
into multiple joined semantic triples. Three in-
terconnected semantic items, whereby it must
9
The other marks will be introduced in Section 4.2.
1095
be ensured that the middle item is a rela-
tion, are converted into a semantic triple (mul-
tiple joined facts containing variables). For
example, the query graph Vdbo:Book[Class] 1 2
??
dbo:author[Relation] 1 1
??
dbr:Danielle Steel[Entity]W is
converted into ??x rdf:type dbo:Book, dbr:Danielle
dbo:author ?x?, and Vdbo:populationTotal[Relation]
1 2
??
dbo:capital[Relation] 1 1
??
dbr:Australia[Entity]W
10
is
converted into ??x1 dbo:populationTotal ?answer, ?x1
dbo:capital dbr:Australia?. If the query graph only
contains one vertex that indicates a class ClassURI,
we generate ??x rdf:type ClassURI?. If the query
graph only contains two connected vertexes, we
append a variable to bind the missing match argu-
ment of the semantic item.
The final SPARQL query is constructed by join-
ing the semantic item triples based on the cor-
responding SPARQL template. We divide the
questions into three types: Yes/No, Normal and
Number. Yes/No questions use the ASK WHERE
template. Normal questions use the SELECT ?url
WHERE template. Number questions first use the
normal question template, and if they cannot ob-
tain a correct answer (a valid numeric value), we
use the SELECT COUNT(?url) WHERE template to
generate a query again. For instance, we construct
the SPARQL query SELECT(?url) WHERE{ ?url
rdf:type dbo:Software. ?url dbo:developer ?x1. ?x1 rdf:type
dbo:Company. ?x1 dbo:foundationPlace dbr:California.}
for this example.
4 Joint Disambiguation with MLN
In this section, we present our method for ques-
tion answering over linked data using a Markov
Logic Network (MLN). In the following subsec-
tions, we first briefly describe the MLN. Then, we
present the predicates and the first-order logic for-
mulas used in the model.
4.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic in a probabilistic framework
(Richardson and Domingos, 2006). An MLNM
consists of several weighted formulas {(?
i
, w
i
)}
i
,
where ?
i
is a first order formula and w
i
is the
penalty (the formula?s weight). In contrast to
the first-order logic, whereby a formula repre-
sents a hard constraint, these logic formulas are
relaxed and can be violated with penalties in the
10
This corresponds to the question ?How many people live
in the capital of Australia??
MLN. Each formula ?
i
consists of a set of first-
order predicates, logical connectors and variables.
These weighted formulas define a probability dis-
tribution over a possible world. Let y denote a pos-
sible world. Then p(y) is defined as follows:
p(y) =
1
Z
exp
?
?
?
(?
i
,w
i
)?M
w
i
?
c?C
n
?
i
f
?
i
c
(y)
?
?
,
where each c is a binding of the free variables in
?
i
to constants; f
?
i
c
is a binary feature function
that returns 1 if the ground formula that we ob-
tain through replacing the free variables in ?
i
with
the constants in c under the given possible world
y is true and is 0 otherwise; and C
n
?
i
is the set of
all possible bindings for the free variables in ?
i
.
Z is a normalized constant. The Markov network
corresponds to this distribution, where nodes rep-
resent ground atoms and factors represent ground
formulas.
4.2 Predicates
In the MLN, we design several predicates to re-
solve the ambiguities in phrase detection, map-
ping phrases to semantic items and semantic item
grouping. Specifically, we design a hidden pred-
icate hasPhrase(i) to indicate that the i-th candi-
date phrase has been chosen. The predicate hasRe-
source(i,j) indicates that the i-th phrase is mapped
to the j-th semantic item. The predicate hasRe-
lation(j,k,rr) indicates that the j-th semantic item
and the k-th semantic item should be grouped to-
gether with the argument-match-type rr. Note that
we define four argument match types between two
semantic items: 1 1, 1 2, 2 1 and 2 2. Here, the
argument match type t s denotes that the t-th argu-
ment of the first semantic item corresponds to the
s-th argument of the second semantic item
11
. The
detailed illustration is shown in Table 1.
Type Example Question
1 1 dbo:height 1 1 dbr:Michael Jordan How tall is Michael Jor-
dan?
1 2 dbo:River 1 2 dbo:crosses Which river does the
Brooklyn Bridge cross?
2 1 dbo:creator 2 1 dbr:Walt Disney Which television shows
were created by Walt
Disney?
2 2 dbo:birthPlace 2 2 dbo:capital Which actors were born in
the capital of American?
Table 1: Examples of the argument match types.
11
The 2-nd argument is corresponding to the object argu-
ment of the relation, and the 1-st argument is corresponding
with the subject argument of the relation and the entity (in-
cluding the class) itself.
1096
Describing the attributes of phrases and relation between two phrases
phraseIndex(p, i, j) The start and end position of phrase p in question.
phrasePosTag(p, pt) The POS tag of the head word in phrase p.
phraseDepTag(p, q, dt) The dependency path tags between phrase p and q.
phraseDepOne (p, q) If there is only one tag in the dependency path, the predicate is true.
hasMeanWord (p, q) If there is any one meaning word in the dependency path of two phrases, the predicate is true.
Describing the attributes of semantic item and the mappings between phrases and semantic items
resourceType(r, rt) The type of semantic item r. Types of semantic items include Entity, Class and Relation
priorMatchScore(p, r, s) The prior score of phrase p mapping to semantic item r.
Describing the attributes of relation between two semantic items in a knowledge base
hasRelatedness(p, q, s) The semantic coherence of semantic items.
isTypeCompatible(p, q, rr) If the semantic items p are type-compatible with the semantic items q, the predicate is true.
hasQueryResult(s, p, o, rr1, rr2) If the triple pattern consisting of semantic items s, p, o and argument-match-types rr1 and rr2 have query
results, the predicate is true.
Table 2: Descriptions of observed predicates.
Moreover, we define a set of observed predi-
cates to describe the properties of phrases, seman-
tic items, relations between phrases and relations
between semantic items. The observed predicates
and descriptions are shown in Table 2.
Previous methods usually designed some
heuristic patterns to group semantic items, which
usually employed a human-designed syntactic
path between two phrases to determine their re-
lations. In contrast, we collect all the tokens in
the dependency path between two phrases as pos-
sible patterns. The predicates phraseDepTag and
hasMeanWord are designed to indicate the possi-
ble patterns. Note that if these tokens only contain
POS tags dt|in|wdt|to|cc|ex|pos|wp or stop words,
the value of the predicate hasMeanWord is false;
otherwise, it is true. In this way, our system is ex-
pected to cover more question expressions. More-
over, the SPARQL endpoint is used to verify the
type compatibility of two semantic items and if
one triple pattern can obtain query results.
The predicate hasRelatedness needs to compute
the coherence score between two semantic items.
Following (Yahya et al., 2012), we use the Jaccard
coefficient (Jaccard, 1908) based on the inlinks be-
tween two semantic items.
The predicate priorMatchScore assigns a prior
score when mapping a phrase to a semantic item.
We use different methods to compute this score
according to different semantic item types. For
entities, we use a normalized score based on the
frequencies of a phrase referring to an entity.
For classes and relations, we use different meth-
ods. We first define the following three similar-
ity metrics: a) s
1
: The Levenshtein distance score
(Navarro, 2001) between the labels of the seman-
tic item and the phrase; b) s
2
: The word embed-
ding (Mikolov et al., 2010) score, which measures
the similarity between two phrases and is the max-
imum cosine value of the words? word embed-
dings between two phrases; and c) s
3
: the instance
overlap score, which is computed using the Jac-
card coefficient of the instance overlap. All scores
are normalized to produce a comparable scores
in the interval of (0, 1). The final prior scores
for mapping phrases to classes and relations are
?s
1
+ (1? ?)s
2
and ?s
1
+ ?s
2
+ (1? ?? ?)s
3
,
respectively. The parameters are set to empirical
values
12
.
4.3 Formulas
According to these predicates, we design several
first-order logic formulas for joint disambiguation.
As mentioned in the first section, these formulas
represent the meta patterns. The concrete pat-
terns can be generated through these meta pat-
terns with training data. Specifically, we use two
types of formulas for the joint decisions: Boolean
and Weighted formulas. Boolean formulas are
hard constraints, which must be satisfied by all
of the ground atoms in the final inference results.
Weighted formulas are soft constraints, which can
be violated with some penalties.
4.3.1 Boolean Formulas (Hard Constraints)
Table 3 lists the Boolean formulas used in this
work. The ? ? notation in the formulas indicates
an arbitrary constant. The ?|f |? notation expresses
the number of true grounded atoms in the formula
f . These formulas express the following con-
straints:
hf1: If a phrase is chosen, then it must have a
mapped semantic item;
hf2: If a semantic item is chosen, then its mapped
phrase must be chosen;
hf3: A phrase can be mapped to at most one se-
mantic item;
hf4: If the phrase is not chosen, then its mapped
12
Set ? to 0.6 for Class and set ? and ? to 0.3 and 0.3 for
Relation, respectively.
1097
hf1 hasPhrase(p)? hasResource(p, )
hf2 hasResource(p, )? hasPhrase(p)
hf3 |hasResource(p, )| ? 1
hf4 !hasPhrase(p)?!hasResource(p, r)
hf5 hasResource( , r)? hasRelation(r, , ) ? hasRelation( , r, )
hf6 |hasRelation(r1, r2, )| ? 1
hf7 hasRelation(r1, r2, )? hasResource( , r1) ? hasResource( , r2)
hf8 phraseIndex(p1, s1, e1) ? phraseIndex(p2, s2, e2) ? overlap(s1, e1, s2, e2) ? hasPhrase(p1)?!hasPhrase(p2)
hf9 resourceType(r, ?Entity?)?!hasRelation(r, , ?2 1?) ? !hasRelation(r, , ?2 2?)
hf10 resourceType(r, ?Entity?)?!hasRelation( , r, ?2 1?) ? !hasRelation(r, , ?2 2?)
hf11 resourceType(r, ?Class?)?!hasRelation(r, , ?2 1?) ? !hasRelation(r, , ?2 2?)
hf12 resourceType(r, ?Class?)?!hasRelation( , r, ?2 1?) ? !hasRelation(r, , ?2 2?)
hf13 !isTypeCompatible(r1, r2, rr)?!hasRelation(r1, r2, rr)
Table 3: Descriptions of Boolean formulas.
sf1 priorMatchScore(p, r, s)? hasPhrase(p)
sf2 priorMatchScore(p, r, s)? hasResource(p)
sf3 phrasePosTag(p, pt+) ? resourceType(r, rt+)? hasResource(p, r)
sf4 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2)? hasRelation(r1, r2, rr+)
sf5 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2)?!hasMeanWord(p1, p2) ?
hasRelation(r1, r2, rr+)
sf6 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2) ? phraseDepOne(p1, p2) ?
hasRelation(r1, r2, rr+)
sf7 hasRelatedness(r1, r2, s) ? hasResource( , r1) ? hasResource( , r2)? hasRelation(r1, r2, )
sf8 hasQueryResult(r1, r2, r3, rr1, rr2)? hasRelation(r1, r2, rr1) ? hasRelation(r2, r3, rr2)
Table 4: Descriptions of weighted formulas.
semantic item should not be chosen;
hf5: If a semantic item is chosen, then it should
have at least one argument match relation with
other semantic items;
hf6: Two semantic items have at most one argu-
ment match relation;
hf7: If an argument match relation for two seman-
tic items is chosen, then they must be chosen;
hf8: Each of two chosen phrases must not overlap;
hf9, hf10, hf11, hf12: The semantic item with
type Entity and Class should not have a second ar-
gument that matches with others;
hf13: The chosen argument match relation for two
sematic items must be type compatible.
4.3.2 Weighted Formulas (Soft Constraints)
Table 4 lists the weighted formulas used in this
work. The ?+? notation in the formulas indicates
that each constant of the logic variable should be
weighted separately. Those formulas express the
following properties in joint decisions:
sf1, sf2: The larger the score of the phrase map-
ping to a semantic item, the more likely the cor-
responding phrase and semantic item should been
chosen;
sf3: There are some associations between the POS
tags of phase and the types of mapped semantic
items;
sf4, sf5, sf6: There are some associations be-
tween the dependency tags in the dependency pat-
tern path of two phases and the types of argument
match relations of two mapped semantic items;
sh7: The larger the relatedness of two seman-
tic items, the more likely they have an argument
match relation;
sf8: If the triple pattern has query results, these se-
mantic items should have corresponding argument
match relations.
5 Experiments
5.1 Dataset & Evaluation Metrics
We use the following three collections of questions
from the QALD
13
task for question answering
over linked data: QALD-1, QALD-3 and QALD-
4. The generated SPARQL queries are evaluated
on Linked Data from DBpedia and YAGO using
a Virtuoso engine
14
. A typical example question
from the QALD benchmark is ?Which books writ-
ten by Kerouac were published by Viking Press??.
As mentioned in Section 2.2, our system is not de-
signed to answer questions that contain numbers,
date comparisons and aggregation operations such
as group by or order by. Therefore, we remove
these types of questions and retain 110 questions
from the QALD-4 training set for generating the
specific formulas and for training their weights in
MLN. We test our system using 37, 75 and 26
questions from the training set of QALD-1
15
, and
the testing set of QALD-3 and QALD-4 respec-
tively. We use #T, #Q and #A to indicate the total
13
www.sc.cit-ec.uni-bielefeld.de/qald/
14
https://github.com/openlink/virtuoso-opensource
15
We use the training set because we try to make a fair
comparison with (Yahya et al., 2012).
1098
number of questions in the testing set, the num-
ber of questions we could address and the number
of questions answered correct, respectively. We
select Precision (P =
#A
#Q
), Recall (R =
#A
#T
),
and F1-score (F1 =
2?P ?R
P+R
) as the evaluation met-
rics. To assess the effectiveness of the disambigua-
tion process in the MLN, we computed the overall
quality measures by precision and recall with the
manually obtained results.
5.2 Experimental Configurations
The Stanford dependency parser (De Marneffe et
al., 2006) is used for extracting features from the
dependency parse trees. We use the toolkit the-
beast
16
to learn the weights of the formulas and
to perform the MAP inference. The inference al-
gorithm uses a cutting plane approach. In addi-
tion, for the parameter learning, we set all ini-
tial weights to zero and use an online learning
algorithm with MIRA update rules to update the
weights of the formulas. The number of iterations
for the training and testing are set to 10 and 200,
respectively.
5.3 Results and Discussion
5.3.1 The Effect of Joint Learning
To demonstrate the advantages of our joint learn-
ing, we design a pipeline system for compari-
son, which independently performs phrase detec-
tion, phrase mapping, and semantic item grouping
by removing the unrelated formulas in MLN. For
example, the formulas
17
related to the predicates
hasResource and hasRelation are removed when
detecting phrases in questions.
Table 5 shows the results, where Joint de-
notes the proposed method with joint inference
and Pipeline denotes the compared method per-
forming each step independently. We perform a
comparison with the question answering results of
QALD (QA), and comparisons at each of the fol-
lowing steps: PD (phrase detection), PM (phrase
mapping) and MG (mapped semantic items group-
ing). From the results, we observe that our method
answers over half of the questions. Moreover, our
joint model based on MLN can obtain better per-
formance in question answering compared to the
pipeline system. We also observe that Joint ex-
hibits better performance than Pipeline in most
steps, except for MG in QALD-3. We believe this
16
http://code.google.com/p/thebeast
17
including entire formulas, excluding hf8 and sf1
is because the three tasks (phrase detection, phrase
mapping, and semantic item grouping) are con-
nected with each other. Each step can provide use-
ful information for the other two tasks. Therefore,
performing joint inference can effectively improve
the performance. Finally, we observe that the for-
mer task usually produces better results than the
subsequent tasks (phrase detection exhibits a bet-
ter performance than phrase mapping, and phrase
mapping exhibits a better performance than se-
mantic item grouping). The main reason is that
the latter subtask is more complex than the former
task. The decisions of the latter subtask strongly
rely on the former results even though they have
interacted effects.
5.3.2 The Effect of Pattern Learning
Table 6 shows a comparison of our system with
DEANNA (Yahya et al., 2012), which is based
on a joint disambiguation model but which em-
ploys hand-written patterns in its system. Because
DEANNA only reports its results of the QALD-1
dataset, we do not show the results for QALD-3
and QALD-4 for equity. From the results, we can
see that our system solved more questions and ex-
hibited a better performance than did DEANNA.
One of the greatest strengths of our system is that
the learning system can address more questions
than hand-written pattern rules.
System #T #Q #A P R F1
DEANNA (Yahya et al., 2012) 50 27 13 0.48 0.26 0.33
Ours 50 37 20 0.54 0.4 0.46
Table 6: Comparisons with DEANNA using the
QALD-1 test questions.
Compared to the ILP (Integer Linear Program-
ming) used in (Yahya et al., 2012) for joint disam-
biguation, we argue that there are two major dif-
ferences to our method. 1) Our method is a data-
driven approach that can learn effective patterns
or rules for the task. Therefore, it exhibits more
robustness and adaptability for various KBs. 2)
We design several meta rules in MLN as opposed
to specific ones. The specific rules can be gen-
erated by these meta rules based on the training
data. By contrast, the traditional approach using
ILP needs to set specific rules in advance, which
requires more intensive labor than our approach.
To further illustrate the effectiveness of our
pattern-learning strategy, we show the weights of
the learned patterns corresponding to formula sf3
in the MLN, as shown in Table 7. From the table,
1099
Benchmark
PD PM MG QA
P R F1 P R F1 P R F1 #T #Q #A P R F1
QALD-1(Joint) 0.93 0.981 0.955 0.895 0.944 0.919 0.703 0.813 0.754 50 37 20 0.54 0.4 0.46
QALD-1(Pipeine) 0.921 0.972 0.946 0.868 0.917 0.892 0.585 0.859 0.696 50 34 17 0.5 0.34 0.41
QALD-3(Joint) 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 99 75 45 0.6 0.46 0.52
QALD-3(Pipeline) 0.912 0.912 0.912 0.829 0.867 0.848 0.677 0.789 0.729 99 75 42 0.56 0.42 0.48
QALD-4(Joint) 0.947 0.978 0.963 0.937 0.967 0.952 0.776 0.865 0.817 50 26 15 0.58 0.3 0.4
QALD-4(Pipeline) 0.937 0.967 0.952 0.905 0.935 0.920 0.683 0.827 0.748 50 24 13 0.54 0.26 0.35
Table 5: The performance of joint learning on three benchmark datasets.
we can see that nn
18
is more likely mapped to En-
tity
19
than to Class and Relation, and vb is most
likely mapped to Relation. This proves that our
model can learn effective and reasonable patterns
for QALD.
POS tag of Phrase type of mapped Item Weight
nn Entity 2.11
nn Class 0.243
nn Relation 0.335
vb Relation 0.517
wp Class 0.143
wr Class 0.025
Table 7: Sample weights of formulas, correspond-
ing with formula sf3.
5.3.3 Comparison to the state of the art
To illustrate the effectiveness of the proposed
method, we perform comparisons to the state-of-
the-art methods. Table 8 shows the results using
QALD-3 and QALD-4. These systems are the
participants in the QALD evaluation campaigns.
From the results, we can see that our system out-
performs most systems at a competitive perfor-
mance. They further prove the effectiveness of the
proposed method.
Test set System #T #Q #A P R F1
QALD-3
CASIA (He et al.,
2013)
99 52 29 0.56 0.3 0.38
Scalewelis (Joris
and Ferr?e, 2013)
99 70 32 0.46 0.32 0.38
RTV (Cristina et
al., 2013)
99 55 30 0.55 0.3 0.39
Intui2 (Corina,
2013)
99 99 28 0.28 28 0.28
SWIP (Pradel et al.,
2013)
99 21 15 0.71 0.15 0.25
Ours 99 75 45 0.6 0.46 0.52
QALD-4
20
gAnswer 50 25 16 0.64 0.32 0.43
Intui3 50 33 10 0.30 0.2 0.24
ISOFT 50 50 10 0.2 0.2 0.2
RO FII 50 50 6 0.12 0.12 0.12
Ours 50 26 15 0.58 0.3 0.4
Table 8: Comparisons with state-of-the-art sys-
tems using the QALD benchmark.
18
The POS tag of the head word in the phrase
19
The type of semantic item
20
Because the QALD-4 conference does not start un-
til after submission, we have no citation for the state-of-
5.3.4 The Effect of Different Formulas
To determine which formulas are more useful for
QALD, we evaluate the performance of the pro-
posed method with different predicate sets. We
subtract one weighted formula from the original
sets at a time, except retaining the first two for-
mulas sf1 and sf2 for basic inference. Because of
space limitations, only the results using QALD-3
testing set are shown in Table 9.
From the results, we can observe that remov-
ing some formulas can boost the performance on
some single tasks, but employing all formulas can
produce the best performance. This illustrates that
solely resolving the steps in QALD (phrase detec-
tion, phrase mapping, semantic items grouping)
can obtain local results, and that making joint in-
ference is necessary and useful.
6 Related Work
Our proposed method is related to two lines of
work: Question Answering over Knowledge bases
and Markov Logic Networks.
Question answering over knowledge bases
has attracted a substantial amount of interest over
a long period of time. The initial attempts in-
cluded BaseBall (Green Jr et al., 1961) and Lu-
nar (Woods, 1977). However, these systems were
mostly limited to closed domains due to a lack of
knowledge resources. With the rapid development
of structured data, such as DBpedia, Freebase and
Yago, the need for providing user-friendly inter-
face to these data has become increasingly urgent.
Keyword (Elbassuoni and Blanco, 2011) and se-
mantic (Pound et al., 2010) searches are limited
to their ability to specify the relations among the
different keywords.
The open topic progress has also been pushed
by the QALD evaluation campaigns (Walter et al.,
2012). Lopez et al. (2011) gave a comprehensive
survey in this research area. The authors devel-
oped the PowerAqua system (Lopez et al., 2006) to
the-art systems in QALD-4. The results can be found at
http://greententacle.techfak.uni-bielefeld.de/ cunger/qald.
1100
Formulas
PD PM MG Avg
P R F1 P R F1 P R F1 P R F1
All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869
-sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864
-sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846
-sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862
-sf6 0.922 0.922 0.922 0.844 0.883 0.863 0.702 0.746 0.723 0.842 0.868 0.855
-sf7 0.931 0.917 0.924 0.881 0.908 0.894 0.621 0.763 0.685 0.833 0.88 0.856
-sf8 0.927 0.927 0.927 0.868 0.908 0.888 0.639 0.807 0.713 0.83 0.893 0.861
Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 question
set.
answer questions on large, heterogeneous datasets.
For questions containing quantifiers, comparatives
or superlatives, Unger et al. (2012) translated
NL to FL using several SPARQL templates and
using a set of heuristic rules mapping phrases
to semantic items. The system most similar to
ours is DEANNA (Yahya et al., 2012). However,
DEANNA extracts predicate-argument structures
from the questions using three hand-written pat-
terns. Our system jointly learns these mappings
and extractions completely from scratch.
Recently, the Semantic Parsing (SP) community
targeted this problem from limited domains (Tang
and Mooney, 2001; Liang et al., 2013) to open do-
mains (Cai and Yates, 2013; Berant et al., 2013).
The methods in semantic parsing answer questions
by first converting natural language utterances into
meaningful representations (e.g., the lambda cal-
culus) and subsequently executing the formal log-
ical forms over KBs. Compared to deriving the
complete logical representation, our method aims
to parse a question into a limited logic form with
the semantic item query, which we believe is more
appropriate for answering factoid questions.
Markov Logic Networks have been widely
used in NLP tasks. Huang (2012) applied MLN
to compress sentences by formulating the task as a
word/phrase deletion problem. Fahrni and Strube
(2012) jointly disambiguated and clustered con-
cepts using MLN. MLN has also been used in
coreference resolution (Song et al., 2012). For
the task of identifying subjective text segments
and of extracting their corresponding explanations
from product reviews, Zhang et al. (2013) mod-
eled these segments with MLN. To discover log-
ical knowledge for deep question answering, Liu
(2012) used MLN to resolve the inconsistencies
of multiple knowledge bases.
Meza-Ruiz and Riedel (2009) employed MLN
for Semantic Role Labeling (SRL). They jointly
performed the following tasks for a sentence:
predicate identification, frame disambiguation, ar-
gument identification and argument classification.
The semantic analysis of SRL solely rested on
the lexical level, but our analysis focuses on the
knowledge-base level and aims to obtain an exe-
cutable query and to support natural language in-
ference.
7 Conclusions and Future Work
For the task of QALD, we present a joint learn-
ing framework for phrase detection, phrase map-
ping and semantic item grouping. The novelty of
our method lies in the fact that we perform joint
inference and pattern learning for all subtasks in
QALD using first-order logic. Our experimental
results demonstrate the effectiveness of the pro-
posed method.
In the future, we plan to address the follow-
ing limitations that still exist in the current sys-
tem: a) numerous hand-labeled data are required
for training the MLN, and we could use a la-
tent form of semantic item query graphs (Liang et
al., 2013); b) more robust solutions can be devel-
oped to find the implicit relations in questions; c)
our system can be scaled up to large-scale open-
domain knowledge bases (Fader et al., 2013; Yao
and Van Durme, 2014); and d) the learning system
has the advantage of being easily adapted to new
settings, and we plan to extend it to other domains
and languages (Liang and Potts, 2014).
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. This
work was sponsored by the National Basic Re-
search Program of China (No. 2014CB340503)
and the National Natural Science Foundation of
China (No. 61202329, 61272332), CCF-Tencent
Open Fund. This work was also supported in part
by Noahs Ark Lab of Huawei Tech. Ltm.
1101
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009. Linked data-the story so far. International
journal on semantic web and information systems,
5(3):1?22.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL.
Unger Christina and Andr Freitas. 2014. Question an-
swering over linked data: Challenges, approaches,
trends. In ESWC.
Dima Corina. 2013. Intui2: A prototype system
for question answering over linked data. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Giannone Cristina, Bellomaria Valentina, and Basili
Roberto. 2013. A hmm-based approach to question
answering against linked data. In Work. Multilin-
gual Question Answering over Linked Data (QALD-
3).
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In LREC.
Shady Elbassuoni and Roi Blanco. 2011. Keyword
search over rdf graphs. In CIKM.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with markov logic. In COLING.
Andre Freitas and Edward Curry. 2014. Natural
language queries over heterogeneous linked data
graphs: A distributional-compositional semantics
approach. In IUI.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219?224. ACM.
Shizhu He, Shulin Liu, Yubo Chen, Guangyou Zhou,
Kang Liu, and Jun Zhao. 2013. Casia@qald-3:
A question answering system over linked data. In
Work. Multilingual Question Answering over Linked
Data (QALD-3).
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Paul. Jaccard. 1908. Nouvelles recherches sur la dis-
tribution florale. Bulletin de la Soci`ete Vaudense des
Sciences Naturelles, 44:223?270.
Guyonvarc?H Joris and S?ebastien Ferr?e. 2013.
Scalewelis: a scalable query-based faceted search
system on top of sparql endpoints. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Jens Lehmann, Tim Furche, Giovanni Grasso, Axel-
Cyrille Ngonga Ngomo, Christian Schallhart, An-
drew Sellers, Christina Unger, Lorenz B?uhmann,
Daniel Gerber, Konrad H?offner, et al. 2012. Deqa:
deep web extraction for question answering. In
ISWC.
Percy Liang and Christopher Potts. 2014. Bringing
machine learning and compositional semantics to-
gether. Annual Reviews of Linguistics (to appear).
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Zhao Liu, Xipeng Qiu, Ling Cao, and Xuanjing Huang.
2012. Discovering logical knowledge for deep ques-
tion answering. In CIKM.
Vanessa Lopez, Enrico Motta, and Victoria Uren.
2006. Poweraqua: Fishing the semantic web. In
The Semantic Web: research and applications, pages
393?410. Springer.
Vanessa Lopez, Victoria Uren, Marta Sabou, and En-
rico Motta. 2011. Is question answering fit for the
semantic web?: a survey. Semantic Web, 2(2):125?
155.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In NAACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In EMNLP.
1102
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Jeffrey Pound, Ihab F Ilyas, and Grant Weddell. 2010.
Expressive and flexible access to web-extracted
data: a keyword-based structured query language.
In SIGMOD.
C Pradel, G Peyet, O Haemmerl?e, and N Hernandez.
2013. Swip at qald-3: results, criticisms and les-
son learned (working notes). In Work. Multilingual
Question Answering over Linked Data (QALD-3).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning, 62(1-
2):107?136.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S?oren Auer. 2013. Question answering on in-
terlinked data. In WWW.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li,
and Houfeng Wang. 2012. Joint learning for coref-
erence resolution with markov logic. In EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing, pages 466?477.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over rdf data. In WWW.
Sebastian Walter, Christina Unger, Philipp Cimiano,
and Daniel B?ar. 2012. Evaluation of a layered
approach to question answering over linked data.
In The Semantic Web?ISWC 2012, pages 362?374.
Springer.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. In Linguistic structures processing, pages
521?569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In EMNLP.
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
and Gerhard Weikum. 2013. Robust question an-
swering over the web of linked data. In CIKM.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In ACL.
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-
anjing Huang. 2013. Discourse level explanatory
relation extraction from product reviews using first-
order logic. In ACL.
Lei Zou, Ruizhe Huang, Haixun WangZou, Jeffrey Xu
Yu, Wenqiang He, and Dongyan Zhao. 2014. Natu-
ral language question answering over rdf ? a graph
data driven approach. In SIGMOD.
1103
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 852?861,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Statistical Machine Translation Improves Question Retrieval in
Community Question Answering via Matrix Factorization
Guangyou Zhou, Fang Liu, Yang Liu, Shizhu He, and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,fliu,liuyang09,shizhu.he,jzhao}@nlpr.ia.ac.cn
Abstract
Community question answering (CQA)
has become an increasingly popular re-
search topic. In this paper, we focus on the
problem of question retrieval. Question
retrieval in CQA can automatically find
the most relevant and recent questions that
have been solved by other users. However,
the word ambiguity and word mismatch
problems bring about new challenges for
question retrieval in CQA. State-of-the-art
approaches address these issues by implic-
itly expanding the queried questions with
additional words or phrases using mono-
lingual translation models. While use-
ful, the effectiveness of these models is
highly dependent on the availability of
quality parallel monolingual corpora (e.g.,
question-answer pairs) in the absence of
which they are troubled by noise issue.
In this work, we propose an alternative
way to address the word ambiguity and
word mismatch problems by taking advan-
tage of potentially rich semantic informa-
tion drawn from other languages. Our pro-
posed method employs statistical machine
translation to improve question retrieval
and enriches the question representation
with the translated words from other lan-
guages via matrix factorization. Experi-
ments conducted on a real CQA data show
that our proposed approach is promising.
1 Introduction
With the development of Web 2.0, community
question answering (CQA) services like Yahoo!
Answers,1 Baidu Zhidao2 and WkiAnswers3 have
attracted great attention from both academia and
industry (Jeon et al, 2005; Xue et al, 2008;
Adamic et al, 2008; Wang et al, 2009; Cao et al,
2010). In CQA, anyone can ask and answer ques-
tions on any topic, and people seeking information
are connected to those who know the answers. As
answers are usually explicitly provided by human,
they can be helpful in answering real world ques-
tions.
In this paper, we focus on the task of question
retrieval. Question retrieval in CQA can automati-
cally find the most relevant and recent questions
(historical questions) that have been solved by
other users, and then the best answers of these his-
torical questions will be used to answer the users?
queried questions. However, question retrieval is
challenging partly due to the word ambiguity and
word mismatch between the queried questions
and the historical questions in the archives. Word
ambiguity often causes the retrieval models to re-
trieve many historical questions that do not match
the users? intent. This problem is also amplified
by the high diversity of questions and users. For
example, depending on different users, the word
?interest? may refer to ?curiosity?, or ?a charge
for borrowing money?.
Another challenge is word mismatch between
the queried questions and the historical questions.
The queried questions may contain words that are
different from, but related to, the words in the rele-
vant historical questions. For example, if a queried
question contains the word ?company? but a rele-
vant historical question instead contains the word
?firm?, then there is a mismatch and the historical
1http://answers.yahoo.com/
2http://zhidao.baidu.com/
3http://wiki.answers.com/
852
English Chinese
word ambiguity
How do I get a loan ?(w?)??(r?h?)?(c?ng)
from a bank? ??(y?nh?ng)??(d?iku?n)?
How to reach the ??(r?h?)??(qi?nw?ng)
bank of the river? ??(h??n)?
word mismatch
company ??(g?ngs?)
firm ??(g?ngs?)
rheum ??(g?nm?o)
catarrh ??(g?nm?o)
Table 1: Google translate: some illustrative examples.
question may not be easily distinguished from an
irrelevant one.
Researchers have proposed the use of word-
based translation models (Berger et al, 2000;
Jeon et al, 2005; Xue et al, 2008; Lee et al,
2008; Bernhard and Gurevych, 2009) to solve
the word mismatch problem. As a principle ap-
proach to capture semantic word relations, word-
based translation models are built by using the
IBM model 1 (Brown et al, 1993) and have
been shown to outperform traditional models (e.g.,
VSM, BM25, LM) for question retrieval. Be-
sides, Riezler et al (2007) and Zhou et al (2011)
proposed the phrase-based translation models for
question and answer retrieval. The basic idea is
to capture the contextual information in model-
ing the translation of phrases as a whole, thus
the word ambiguity problem is somewhat allevi-
ated. However, all these existing studies in the
literature are basically monolingual approaches
which are restricted to the use of original language
of questions. While useful, the effectiveness of
these models is highly dependent on the availabil-
ity of quality parallel monolingual corpora (e.g.,
question-answer pairs) in the absence of which
they are troubled by noise issue. In this work,
we propose an alternative way to address the word
ambiguity and word mismatch problems by taking
advantage of potentially rich semantic information
drawn from other languages. Through other lan-
guages, various ways of adding semantic informa-
tion to a question could be available, thereby lead-
ing to potentially more improvements than using
the original language only.
Taking a step toward using other languages, we
propose the use of translated representation by al-
ternatively enriching the original questions with
the words from other languages. The idea of im-
proving question retrieval with statistical machine
translation is based on the following two observa-
tions: (1) Contextual information is exploited dur-
ing the translation from one language to another.
For example in Table 1, English words ?interest?
and ?bank? that have multiple meanings under
different contexts are correctly addressed by us-
ing the state-of-the-art translation tool ??Google
Translate.4 Thus, word ambiguity based on con-
textual information is naturally involved when
questions are translated. (2) Multiple words that
have similar meanings in one language may be
translated into an unique word or a few words in a
foreign language. For example in Table 1, English
words such as ?company? and ?firm? are trans-
lated into ??? (g?ngs?)?, ?rheum? and ?catarrh?
are translated into ???(g?nm?o)? in Chinese.
Thus, word mismatch problem can be somewhat
alleviated by using other languages.
Although Zhou et al (2012) exploited bilin-
gual translation for question retrieval and obtained
the better performance than traditional monolin-
gual translation models. However, there are two
problems with this enrichment: (1) enriching
the original questions with the translated words
from other languages increases the dimensionality
and makes the question representation even more
sparse; (2) statistical machine translation may in-
troduce noise, which can harm the performance of
question retrieval. To solve these two problems,
we propose to leverage statistical machine transla-
tion to improve question retrieval via matrix fac-
torization.
The remainder of this paper is organized as fol-
lows. Section 2 describes the proposed method
by leveraging statistical machine translation to im-
prove question retrieval via matrix factorization.
Section 3 presents the experimental results. In sec-
tion 4, we conclude with ideas for future research.
4http://translate.google.com/translate t
853
2 Our Approach
2.1 Problem Statement
This paper aims to leverage statistical machine
translation to enrich the question representation.
In order to address the word ambiguity and word
mismatch problems, we expand a question by
adding its translation counterparts. Statistical ma-
chine translation (e.g., Google Translate) can uti-
lize contextual information during the question
translation, so it can solve the word ambiguity and
word mismatch problems to some extent.
Let L = {l1, l2, . . . , lP } denote the language
set, where P is the number of languages con-
sidered in the paper, l1 denotes the original lan-
guage (e.g., English) while l2 to lP are the for-
eign languages. Let D1 = {d(1)1 , d(1)2 , . . . , d(1)N }
be the set of historical question collection in origi-
nal language, where N is the number of historical
questions in D1 with vocabulary size M1. Now
we first translate each original historical question
from language l1 into other languages lp (p ?
[2, P ]) by Google Translate. Thus, we can ob-
tain D2, . . . , DP in different languages, and Mp is
the vocabulary size of Dp. A question d(p)i in Dp
is simply represented as a Mp dimensional vector
d(p)i , in which each entry is calculated by tf-idf.
The N historical questions in Dp are then repre-
sented in a Mp ? N term-question matrix Dp =
{d(p)1 ,d
(p)
2 , . . . ,d
(p)
N }, in which each row corre-
sponds to a term and each column corresponds to
a question.
Intuitively, we can enrich the original ques-
tion representation by adding the translated words
from language l2 to lP , the original vocabu-
lary size is increased from M1 to ?Pp=1 Mp.
Thus, the term-question matrix becomes D =
{D1,D2, . . . ,DP } and D ? R(
?P
p=1 Mp)?N .
However, there are two problems with this enrich-
ment: (1) enriching the original questions with the
translated words from other languages makes the
question representation even more sparse; (2) sta-
tistical machine translation may introduce noise.5
To solve these two problems, we propose to
leverage statistical machine translation to improve
question retrieval via matrix factorization. Figure
1 presents the framework of our proposed method,
where qi represents a queried question, and qi is a
vector representation of qi.
5Statistical machine translation quality is far from satis-
factory in real applications.
??
??
??
??
 
HistoricalQuestionCollectionRepresentation
 
QueryRepresentation
Figure 1: Framework of our proposed approach
for question retrieval.
2.2 Model Formulation
To tackle the data sparseness of question represen-
tation with the translated words, we hope to find
two or more lower dimensional matrices whose
product provides a good approximate to the orig-
inal one via matrix factorization. Previous stud-
ies have shown that there is psychological and
physiological evidence for parts-based representa-
tion in the human brain (Wachsmuth et al, 1994).
The non-negative matrix factorization (NMF) is
proposed to learn the parts of objects like text
documents (Lee and Seung, 2001). NMF aims
to find two non-negative matrices whose product
provides a good approximation to the original ma-
trix and has been shown to be superior to SVD in
document clustering (Xu et al, 2003; Tang et al,
2012).
In this paper, NMF is used to induce the reduced
representation Vp of Dp, Dp is independent on
{D1,D2, . . . ,Dp?1,Dp+1, . . . ,DP }. When ig-
noring the coupling between Vp, it can be solved
by minimizing the objective function as follows:
O1(Up,Vp) = minUp?0,Vp?0 ?Dp ?UpVp?
2
F (1)
where ? ? ?F denotes Frobenius norm of a matrix.
Matrices Up ? RMp?K and Vp ? RK?N are the
reduced representation for terms and questions in
the K dimensional space, respectively.
To reduce the noise introduced by statistical ma-
chine translation, we assume that Vp from lan-
guage Dp (p ? [2, P ]) should be close to V1
854
from the original language D1. Based on this as-
sumption, we minimize the distance between Vp
(p ? [2, P ]) and V1 as follows:
O2(Vp) = minVp?0
P?
p=2
?Vp ?V1?2F (2)
Combining equations (1) and (2), we get the fol-
lowing objective function:
O(U1, . . . ,UP ;V1, . . . ,VP ) (3)
=
P?
p=1
?Dp ?UpVp?2F +
P?
p=2
?p?Vp ?V1?2F
where parameter ?p (p ? [2, P ]) is used to adjust
the relative importance of these two components.
If we set a small value for ?p, the objective func-
tion behaves like the traditional NMF and the im-
portance of data sparseness is emphasized; while a
big value of ?p indicatesVp should be very closed
to V1, and equation (3) aims to remove the noise
introduced by statistical machine translation.
By solving the optimization problem in equa-
tion (4), we can get the reduced representation of
terms and questions.
minO(U1, . . . ,UP ;V1, . . . ,VP ) (4)
subject to : Up ? 0,Vp ? 0, p ? [1, P ]
2.3 Optimization
The objective function O defined in equation (4)
performs data sparseness and noise removing si-
multaneously. There are 2P coupling components
in O, and O is not convex in both U and V to-
gether. Therefore it is unrealistic to expect an al-
gorithm to find the global minima. In the follow-
ing, we introduce an iterative algorithm which can
achieve local minima. In our optimization frame-
work, we optimize the objective function in equa-
tion (4) by alternatively minimizing each compo-
nent when the remaining 2P ? 1 components are
fixed. This procedure is summarized in Algorithm
1.
2.3.1 Update of MatrixUp
Holding V1, . . . ,VP and U1, . . . ,Up?1,Up+1,
. . . ,UP fixed, the update of Up amounts to the
following optimization problem:
min
Up?0
?Dp ?UpVp?2F (5)
Algorithm 1 Optimization framework
Input: Dp ? Rmp?N , p ? [1, P ]
1: for p = 1 : P do
2: V(0)p ? RK?N ? random matrix
3: for t = 1 : T do  T is iteration times
4: U(t)p ? UpdateU(Dp,V(t?1)p )
5: V(t)p ? UpdateV(Dp,U(t)p )
6: end for
7: returnU(T )p , V(T )p
8: end for
Algorithm 2 Update Up
Input: Dp ? RMp?N , Vp ? RK?N
1: for i = 1 : Mp do
2: u?(p)?i = (VpVTp )?1Vpd?(p)i
3: end for
4: returnUp
Let d?(p)i = (d(p)i1 , . . . , d(p)iK )T and u?(p)i =
(u(p)i1 , . . . , u
(p)
iK )T be the column vectors whose en-
tries are those of the ith row of Dp and Up re-
spectively. Thus, the optimization of equation (5)
can be decomposed into Mp optimization prob-
lems that can be solved independently, with each
corresponding to one row of Up:
min
u?(p)i ?0
?d?(p)i ?VTp u?
(p)
i ?22 (6)
for i = 1, . . . ,Mp.
Equation (6) is a standard least squares prob-
lems in statistics and the solution is:
u?(p)?i = (VpVTp )?1Vpd?
(p)
i (7)
Algorithm 2 shows the procedure.
2.3.2 Update of MatrixVp
Holding U1, . . . ,UP and V1, . . . ,Vp?1,Vp+1,
. . . ,VP fixed, the update of Vp amounts to the
optimization problem divided into two categories.
if p ? [2, P ], the objective function can be writ-
ten as:
min
Vp?0
?Dp ?UpVp?2F + ?p?Vp ?V1?2F (8)
if p = 1, the objective function can be written
as:
min
Vp?0
?Dp ?UpVp?2F + ?p?Vp?2F (9)
855
Let d(p)j be the jth column vector of Dp, and
v(p)j be the jth column vector of Vp, respectively.
Thus, equation (8) can be rewritten as:
min
{v(p)j ?0}
N?
j=1
?d(p)j ?Upv
(p)
j ?22+
N?
j=1
?p?v(p)j ?v
(1)
j ?22
(10)
which can be decomposed into N optimization
problems that can be solved independently, with
each corresponding to one column of Vp:
min
v(p)j ?0
?d(p)j ?Upv
(p)
j ?22+?p?v
(p)
j ?v
(1)
j ?22 (11)
for j = 1, . . . , N .
Equation (12) is a least square problem with L2
norm regularization. Now we rewrite the objective
function in equation (12) as
L(v(p)j ) = ?d
(p)
j ?Upv
(p)
j ?22 + ?p?v
p
j ? v
(1)
j ?22
(12)
where L(v(1)j ) is convex, and hence has a unique
solution. Taking derivatives, we obtain:
?L(v(p)j )
?v(p)j
= ?2UTp (d(p)j ?Upv
(p)
j )+2?p(v
(p)
j ?v
(1)
j )
(13)
Forcing the partial derivative to be zero leads to
v(p)?j = (UTpUp + ?pI)?1(UTp d
(p)
j + ?pv
(1)
j )
(14)
where p ? [2, P ] denotes the foreign language rep-
resentation.
Similarly, the solution of equation (9) is:
v(p)?j = (UTpUp + ?pI)?1UTp d
(p)
j (15)
where p = 1 denotes the original language repre-
sentation.
Algorithm 3 shows the procedure.
2.4 Time Complexity Analysis
In this subsection, we discuss the time complex-
ity of our proposed method. The optimization
u?(p)i using Algorithm 2 should calculate VpVTp
and Vpd?(p)i , which takes O(NK2 + NK) op-
erations. Therefore, the optimization Up takes
O(NK2 + MpNK) operations. Similarly, the
time complexity of optimization Vi using Algo-
rithm 3 is O(MpK2 + MpNK).
Another time complexity is the iteration times
T used in Algorithm 1 and the total number of
Algorithm 3 Update Vp
Input: Dp ? RMp?N , Up ? RMp?K
1: ?? (UTpUp + ?pI)?1
2: ?? UTpDp
3: if p = 1 then
4: for j = 1 : N do
5: v(p)j ? ??j , ?j is the jth column of ?
6: end for
7: end if
8: returnV1
9: if p ? [2, P ] then
10: for j = 1 : N do
11: v(p)j ? ?(?j + ?pv(1)j )
12: end for
13: end if
14: returnVp
languages P , the overall time complexity of our
proposed method is:
P?
p=1
T ?O(NK2 + MpK2 + 2MpNK) (16)
For each language Dp, the size of vocabulary
Mp is almost constant as the number of questions
increases. Besides, K ? min(Mp, N), theoreti-
cally, the computational time is almost linear with
the number of questions N and the number of lan-
guages P considered in the paper. Thus, the pro-
posed method can be easily adapted to the large-
scale information retrieval task.
2.5 Relevance Ranking
The advantage of incorporating statistical machine
translation in relevance ranking is to reduce ?word
ambiguity? and ?word mismatch? problems. To
do so, given a queried question q and a historical
question d from Yahoo! Answers, we first trans-
late q and d into other foreign languages (e.g., Chi-
nese, French etc.) and get the corresponding trans-
lated representation qi and di (i ? [2, P ]), where
P is the number of languages considered in the pa-
per. For queried question q = q1, we represent it
in the reduced space:
vq1 = argminv?0 ?q1 ?U1v?
2
2 + ?1?v?22 (17)
where vector q1 is the tf-idf representation of
queried question q1 in the term space. Similarly,
for historical question d = d1 (and its tf-idf repre-
sentation d1 in the term space) we represent it in
the reduced space as vd1 .
856
The relevance score between the queried ques-
tion q1 and the historical question d1 in the re-
duced space is, then, calculated as the cosine sim-
ilarity between vq1 and vd1 :
s(q1, d1) =
< vq1 ,vd1 >
?vq1?2 ? ?vd1?2
(18)
For translated representation qi (i ? [2, P ]), we
also represent it in the reduced space:
vqi = argminv?0 ?qi?Uiv?
2
2+?i?v?vq1?22 (19)
where vector qi is the tf-idf representation of qi
in the term space. Similarly, for translated rep-
resentation di (and its tf-idf representation di in
the term space) we also represent it in the reduced
space as vdi . The relevance score s(qi, di) be-
tween qi and di in the reduced space can be cal-
culated as the cosine similarity between vqi and
vdi .
Finally, we consider learning a relevance func-
tion of the following general, linear form:
Score(q, d) = ?T ??(q, d) (20)
where feature vector ?(q, d) =
(sV SM (q, d), s(q1, d1), s(q2, d2), . . . , s(qP , dP )),
and ? is the corresponding weight vector, we
optimize this parameter for our evaluation metrics
directly using the Powell Search algorithm (Paul
et al, 1992) via cross-validation. sV SM (q, d) is
the relevance score in the term space and can be
calculated using Vector Space Model (VSM).
3 Experiments
3.1 Data Set and Evaluation Metrics
We collect the data set from Yahoo! Answers and
use the getByCategory function provided in Ya-
hoo! Answers API6 to obtain CQA threads from
the Yahoo! site. More specifically, we utilize
the resolved questions and the resulting question
repository that we use for question retrieval con-
tains 2,288,607 questions. Each resolved ques-
tion consists of four parts: ?question title?, ?ques-
tion description?, ?question answers? and ?ques-
tion category?. For question retrieval, we only use
the ?question title? part. It is assumed that ques-
tion title already provides enough semantic infor-
mation for understanding the users? information
needs (Duan et al, 2008). There are 26 categories
6http://developer.yahoo.com/answers
Category #Size Category # Size
Arts & Humanities 86,744 Home & Garden 35,029
Business & Finance 105,453 Beauty & Style 37,350
Cars & Transportation 145,515 Pet 54,158
Education & Reference 80,782 Travel 305,283
Entertainment & Music 152,769 Health 132,716
Family & Relationships 34,743 Sports 214,317
Politics & Government 59,787 Social Science 46,415
Pregnancy & Parenting 43,103 Ding out 46,933
Science & Mathematics 89,856 Food & Drink 45,055
Computers & Internet 90,546 News & Events 20,300
Games & Recreation 53,458 Environment 21,276
Consumer Electronics 90,553 Local Businesses 51,551
Society & Culture 94,470 Yahoo! Products 150,445
Table 2: Number of questions in each first-level
category.
at the first level and 1,262 categories at the leaf
level. Each question belongs to a unique leaf cat-
egory. Table 2 shows the distribution across first-
level categories of the questions in the archives.
We use the same test set in previous work (Cao
et al, 2009; Cao et al, 2010). This set contains
252 queried questions and can be freely down-
loaded for research communities.7
The original language of the above data set is
English (l1) and then they are translated into four
other languages (Chinese (l2), French (l3), Ger-
man (l4), Italian (l5)), thus the number of language
considered is P = 5) by using the state-of-the-art
translation tool ??Google Translate.
Evaluation Metrics: We evaluate the perfor-
mance of question retrieval using the following
metrics: Mean Average Precision (MAP) and
Precision@N (P@N). MAP rewards methods that
return relevant questions early and also rewards
correct ranking of the results. P@N reports the
fraction of the top-N questions retrieved that are
relevant. We perform a significant test, i.e., a t-
test with a default significant level of 0.05.
We tune the parameters on a small development
set of 50 questions. This development set is also
extracted from Yahoo! Answers, and it is not in-
cluded in the test set. For parameter K, we do an
experiment on the development set to determine
the optimal values among 50, 100, 150, ? ? ? , 300 in
terms of MAP. Finally, we set K = 100 in the ex-
periments empirically as this setting yields the best
performance. For parameter ?1, we set ?1 = 1
empirically, while for parameter ?i (i ? [2, P ]),
we set ?i = 0.25 empirically and ensure that?
i ?i = 1.
7http://homepages.inf.ed.ac.uk/gcong/qa/
857
# Methods MAP P@10
1 VSM 0.242 0.226
2 LM 0.385 0.242
3 Jeon et al (2005) 0.405 0.247
4 Xue et al (2008) 0.436 0.261
5 Zhou et al (2011) 0.452 0.268
6 Singh (2012) 0.450 0.267
7 Zhou et al (2012) 0.483 0.275
8 SMT + MF (P = 2, l1, l2) 0.527 0.284
9 SMT + MF (P = 5) 0.564 0.291
Table 3: Comparison with different methods for
question retrieval.
3.2 Question Retrieval Results
Table 3 presents the main retrieval performance.
Row 1 and row 2 are two baseline systems, which
model the relevance score using VSM (Cao et al,
2010) and language model (LM) (Zhai and Laf-
ferty, 2001; Cao et al, 2010) in the term space.
Row 3 and row 6 are monolingual translation mod-
els to address the word mismatch problem and
obtain the state-of-the-art performance in previ-
ous work. Row 3 is the word-based translation
model (Jeon et al, 2005), and row 4 is the word-
based translation language model, which linearly
combines the word-based translation model and
language model into a unified framework (Xue et
al., 2008). Row 5 is the phrase-based translation
model, which translates a sequence of words as
whole (Zhou et al, 2011). Row 6 is the entity-
based translation model, which extends the word-
based translation model and explores strategies to
learn the translation probabilities between words
and the concepts using the CQA archives and a
popular entity catalog (Singh, 2012). Row 7 is
the bilingual translation model, which translates
the English questions from Yahoo! Answers into
Chinese questions using Google Translate and ex-
pands the English words with the translated Chi-
nese words (Zhou et al, 2012). For these previ-
ous work, we use the same parameter settings in
the original papers. Row 8 and row 9 are our pro-
posed method, which leverages statistical machine
translation to improve question retrieval via ma-
trix factorization. In row 8, we only consider two
languages (English and Chinese) and translate En-
glish questions into Chinese using Google Trans-
late in order to compare with Zhou et al (2012).
In row 9, we translate English questions into other
four languages. There are some clear trends in the
result of Table 3:
(1) Monolingual translation models signifi-
cantly outperform the VSM and LM (row 1 and
row 2 vs. row 3, row 4, row 5 and row 6).
(2) Taking advantage of potentially rich seman-
tic information drawn from other languages via
statistical machine translation, question retrieval
performance can be significantly improved (row 3,
row 4, row 5 and row 6 vs. row 7, row 8 and row 9,
all these comparisons are statistically significant at
p < 0.05).
(3) Our proposed method (leveraging statisti-
cal machine translation via matrix factorization,
SMT + MF) significantly outperforms the bilin-
gual translation model of Zhou et al (2012) (row
7 vs. row 8, the comparison is statistically signifi-
cant at p < 0.05). The reason is that matrix factor-
ization used in the paper can effectively solve the
data sparseness and noise introduced by the ma-
chine translator simultaneously.
(4) When considering more languages, ques-
tion retrieval performance can be further improved
(row 8 vs. row 9).
Note that Wang et al (2009) also addressed the
word mismatch problem for question retrieval by
using syntactic tree matching. We do not compare
with Wang et al (2009) in Table 3 because pre-
vious work (Ming et al, 2010) demonstrated that
word-based translation language model (Xue et
al., 2008) obtained the superior performance than
the syntactic tree matching (Wang et al, 2009).
Besides, some other studies attempt to improve
question retrieval with category information (Cao
et al, 2009; Cao et al, 2010), label ranking (Li et
al., 2011) or world knowledge (Zhou et al, 2012).
However, their methods are orthogonal to ours,
and we suspect that combining the category infor-
mation or label ranking into our proposed method
might get even better performance. We leave it for
future research.
3.3 Impact of the Matrix Factorization
Our proposed method (SMT +MF) can effectively
solve the data sparseness and noise via matrix fac-
torization. To further investigate the impact of
the matrix factorization, one intuitive way is to
expand the original questions with the translated
words from other four languages, without consid-
ering the data sparseness and noise introduced by
machine translator. We compare our SMT + MF
with this intuitive enriching method (SMT + IEM).
Besides, we also employ our proposed matrix fac-
torization to the original question representation
(VSM + MF). Table 4 shows the comparison.
858
# Methods MAP P@10
1 VSM 0.242 0.226
2 VSM + MF 0.411 0.253
3 SMT + IEM (P = 5) 0.495 0.280
4 SMT + MF (P = 5) 0.564 0.291
Table 4: The impact of matrix factorization.
(1) Our proposed matrix factorization can sig-
nificantly improve the performance of question re-
trieval (row 1 vs. row2; row3 vs. row4, the
improvements are statistically significant at p <
0.05). The results indicate that our proposed ma-
trix factorization can effectively address the issues
of data spareness and noise introduced by statisti-
cal machine translation.
(2) Compared to the relative improvements of
row 3 and row 4, the relative improvements of row
1 and row 2 is much larger. The reason may be
that although matrix factorization can be used to
reduce dimension, it may impair the meaningful
terms.
(3) Compared to VSM, the performance of
SMT + IEM is significantly improved (row 1
vs. row 3), which supports the motivation that
the word ambiguity and word mismatch problems
could be partially addressed by Google Translate.
3.4 Impact of the Translation Language
One of the success of this paper is to take ad-
vantage of potentially rich semantic information
drawn from other languages to solve the word am-
biguity and word mismatch problems. So we con-
struct a dummy translator (DT) that translates an
English word to itself. Thus, through this trans-
lation, we do not add any semantic information
into the original questions. The comparison is pre-
sented in Table 5. Row 1 (DT + MF) represents
integrating two copies of English questions with
our proposed matrix factorization. From Table 5,
we have several different findings:
(1) Taking advantage of potentially rich seman-
tic information drawn from other languages can
significantly improve the performance of question
retrieval (row 1 vs. row 2, row 3, row 4 and row 5,
the improvements relative to DT + MF are statisti-
cally significant at p < 0.05).
(2) Different languages contribute unevenly for
question retrieval (e.g., row 2 vs. row 3). The
reason may be that the improvements of leverag-
ing different other languages depend on the qual-
ity of machine translation. For example, row 3
# Methods MAP
1 DT + MF (l1, l1) 0.352
2 SMT + MF (P = 2, l1, l2) 0.527
3 SMT + MF (P = 2, l1, l3) 0.553
4 SMT + MF (P = 2, l1, l4) 0.536
5 SMT + MF (P = 2, l1, l5) 0.545
6 SMT + MF (P = 3, l1, l2, l3) 0.559
7 SMT + MF (P = 4, l1, l2, l3, l4) 0.563
8 SMT + MF (P = 5, l1, l2, l3, l4, l5) 0.564
Table 5: The impact of translation language.
Method Translation MAP
SMT + MF (P = 2, l1, l2) Dict 0.468GTrans 0.527
Table 6: Impact of the contextual information.
is better than row 2 because the translation qual-
ity of English-French is much better than English-
Chinese.
(3) Using much more languages does not seem
to produce significantly better performance (row 6
and row 7 vs. row 8). The reason may be that in-
consistency between different languages may exist
due to statistical machine translation.
3.5 Impact of the Contextual Information
In this paper, we translate the English questions
into other four languages using Google Translate
(GTrans), which takes into account contextual in-
formation during translation. If we translate a
question word by word, it discards the contextual
information. We would expect that such a transla-
tion would not be able to solve the word ambiguity
problem.
To investigate the impact of contextual infor-
mation for question retrieval, we only consider
two languages and translate English questions
into Chinese using an English to Chinese lexicon
(Dict) in StarDict8. Table 6 shows the experi-
mental results, we can see that the performance is
degraded when the contextual information is not
considered for the translation of questions. The
reason is that GTrans is context-dependent and
thus produces different translated Chinese words
depending on the context of an English word.
Therefore, the word ambiguity problem can be
solved during the English-Chinese translation.
4 Conclusions and Future Work
In this paper, we propose to employ statistical ma-
chine translation to improve question retrieval and
8StarDict is an open source dictionary software, available
at http://stardict.sourceforge.net/.
859
enrich the question representation with the trans-
lated words from other languages via matrix fac-
torization. Experiments conducted on a real CQA
data show some promising findings: (1) the pro-
posed method significantly outperforms the pre-
vious work for question retrieval; (2) the pro-
posed matrix factorization can significantly im-
prove the performance of question retrieval, no
matter whether considering the translation lan-
guages or not; (3) considering more languages can
further improve the performance but it does not
seem to produce significantly better performance;
(4) different languages contribute unevenly for
question retrieval; (5) our proposed method can
be easily adapted to the large-scale information re-
trieval task.
As future work, we plan to incorporate the ques-
tion structure (e.g., question topic and question fo-
cus (Duan et al, 2008)) into the question represen-
tation for question retrieval. We also want to fur-
ther investigate the use of the proposed method for
other kinds of data set, such as categorized ques-
tions from forum sites and FAQ sites.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 61070106, No.
61272332 and No. 61202329), the National High
Technology Development 863 Program of China
(No. 2012AA011102), the National Basic Re-
search Program of China (No. 2012CB316300),
We thank the anonymous reviewers for their in-
sightful comments. We also thank Dr. Gao Cong
for providing the data set and Dr. Li Cai for some
discussion.
References
L. Adamic, J. Zhang, E. Bakshy, and M. Ackerman.
2008. Knowledge sharing and yahoo answers: ev-
eryone knows and something. In Proceedings of
WWW.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V.Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
D. Bernhard and I. Gurevych. 2009. Combining
lexical semantic resources with question & answer
archives for translation-based answer finding. In
Proceedings of ACL, pages 728-736.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang.
2009. The use of categorization information in lan-
guage models for question retrieval. In Proceedings
of CIKM, pages 265-274.
X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A
generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW, pages
201-210.
H. Duan, Y. Cao, C. Y. Lin, and Y. Yu. 2008. Searching
questions by identifying questions topics and ques-
tion focus. In Proceedings of ACL, pages 156-164.
C. L. Lawson and R. J. Hanson. 1974. Solving least
squares problems. Prentice-Hall.
J. -T. Lee, S. -B. Kim, Y. -I. Song, and H. -C. Rim.
2008. Bridging lexical gaps between queries and
questions on large online Q&A collections with
compact translation models. In Proceedings of
EMNLP, pages 410-418.
W. Wang, B. Li, and I. King. 2011. Improving ques-
tion retrieval in community question answering with
label ranking. In Proceedings of IJCNN, pages 349-
356.
D. D. Lee and H. S. Seung. 2001. Algorithms for
non-negative matrix factorization. In Proceedings
of NIPS.
Z. Ming, K. Wang, and T. -S. Chua. 2010. Prototype
hierarchy based clustering for the categorization and
navigation of web collections. In Proceedings of SI-
GIR, pages 2-9.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM, pages 84-90.
C. Paige and M. Saunders. 1982. LSQR: an algo-
rithm for sparse linear equations and sparse least
squares. ACM Transaction on Mathematical Soft-
ware, 8(1):43-71.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B.
P. Flannery. 1992. Numerical Recipes In C. Cam-
bridge Univ. Press.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal,
and Y. Liu. 2007. Statistical machine translation for
query expansion in answer retrieval. In Proceedings
of ACL, pages 464-471.
A. Singh. 2012. Entity based q&a retrieval. In Pro-
ceedings of EMNLP-CoNLL, pages 1266-1277.
J. Tang, X. Wang, H. Gao, X. Hu, and H. Liu. 2012.
Enriching short text representation in microblog for
clustering. Front. Comput., 6(1):88-101.
860
E. Wachsmuth, M. W. Oram, and D. I. Perrett. 1994.
Recognition of objects and their component parts:
responses of single units in the temporal cortex of
teh macaque. Cerebral Cortex, 4:509-522.
K. Wang, Z. Ming, and T-S. Chua. 2009. A syntac-
tic tree matching approach to find similar questions
in community-based qa services. In Proceedings of
SIGIR, pages 187-194.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In Proceedings of
ACL, pages 1230-1238.
W. Xu, X. Liu, and Y. Gong. 2003. Document cluster-
ing based on non-negative matrix factorization. In
Proceedings of SIGIR, pages 267-273.
X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval mod-
els for question and answer archives. In Proceedings
of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc informa-
tion retrieval. In Proceedings of SIGIR, pages 334-
342.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-
based translation model for question retrieval in
community question answer archives. In Proceed-
ings of ACL, pages 653-662.
G. Zhou, K. Liu, and J. Zhao. 2012. Exploiting bilin-
gual translation for question retrieval in community-
based question answering. In Proceedings of COL-
ING, pages 3153-3170.
G. Zhou, Y. Liu, F. Liu, D. Zeng, and J. Zhao. 2013.
Improving Question Retrieval in Community Ques-
tion Answering Using World Knowledge. In Pro-
ceedings of IJCAI.
861
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1754?1763,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Syntactic Patterns versus Word Alignment: Extracting Opinion Targets
from Online Reviews
Kang Liu, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Mining opinion targets is a fundamen-
tal and important task for opinion min-
ing from online reviews. To this end,
there are usually two kinds of methods:
syntax based and alignment based meth-
ods. Syntax based methods usually ex-
ploited syntactic patterns to extract opin-
ion targets, which were however prone to
suffer from parsing errors when dealing
with online informal texts. In contrast,
alignment based methods used word align-
ment model to fulfill this task, which could
avoid parsing errors without using pars-
ing. However, there is no research fo-
cusing on which kind of method is more
better when given a certain amount of re-
views. To fill this gap, this paper empiri-
cally studies how the performance of these
two kinds of methods vary when chang-
ing the size, domain and language of the
corpus. We further combine syntactic pat-
terns with alignment model by using a par-
tially supervised framework and investi-
gate whether this combination is useful or
not. In our experiments, we verify that
our combination is effective on the corpus
with small and medium size.
1 Introduction
With the rapid development of Web 2.0, huge
amount of user reviews are springing up on the
Web. Mining opinions from these reviews be-
come more and more urgent since that customers
expect to obtain fine-grained information of prod-
ucts and manufacturers need to obtain immediate
feedbacks from customers. In opinion mining, ex-
tracting opinion targets is a basic subtask. It is
to extract a list of the objects which users express
their opinions on and can provide the prior infor-
mation of targets for opinion mining. So this task
has attracted many attentions. To extract opin-
ion targets, pervious approaches usually relied on
opinion words which are the words used to ex-
press the opinions (Hu and Liu, 2004a; Popescu
and Etzioni, 2005; Liu et al, 2005; Wang and
Wang, 2008; Qiu et al, 2011; Liu et al, 2012). In-
tuitively, opinion words often appear around and
modify opinion targets, and there are opinion re-
lations and associations between them. If we have
known some words to be opinion words, the words
which those opinion words modify will have high
probability to be opinion targets.
Therefore, identifying the aforementioned opin-
ion relations between words is important for ex-
tracting opinion targets from reviews. To fulfill
this aim, previous methods exploited the words
co-occurrence information to indicate them (Hu
and Liu, 2004a; Hu and Liu, 2004b). Obviously,
these methods cannot obtain precise extraction be-
cause of the diverse expressions by reviewers, like
long-span modified relations between words, etc.
To handle this problem, several methods exploited
syntactic information, where several heuristic pat-
terns based on syntactic parsing were designed
(Popescu and Etzioni, 2005; Qiu et al, 2009; Qiu
et al, 2011). However, the sentences in online
reviews usually have informal writing styles in-
cluding grammar mistakes, typos, improper punc-
tuation etc., which make parsing prone to gener-
ate mistakes. As a result, the syntax-based meth-
ods which heavily depended on the parsing per-
formance would suffer from parsing errors (Zhang
et al, 2010). To improve the extraction perfor-
mance, we can only employ some exquisite high-
precision patterns. But this strategy is likely to
miss many opinion targets and has lower recall
with the increase of corpus size. To resolve these
problems, Liu et al (2012) formulated identifying
opinion relations between words as an monolin-
gual alignment process. A word can find its cor-
responding modifiers by using a word alignment
1754
Figure 1: Mining Opinion Relations between Words using Partially Supervised Alignment Model
model (WAM). Without using syntactic parsing,
the noises from parsing errors can be effectively
avoided. Nevertheless, we notice that the align-
ment model is a statistical model which needs suf-
ficient data to estimate parameters. When the data
is insufficient, it would suffer from data sparseness
and may make the performance decline.
Thus, from the above analysis, we can observe
that the size of the corpus has impacts on these
two kinds of methods, which arises some impor-
tant questions: how can we make selection be-
tween syntax based methods and alignment based
method for opinion target extraction when given
a certain amount of reviews? And which kind of
methods can obtain better extraction performance
with the variation of the size of the dataset? Al-
though (Liu et al, 2012) had proved the effective-
ness of WAM, they mainly performed experiments
on the dataset with medium size. We are still curi-
ous about that when the size of dataset is larger
or smaller, can we obtain the same conclusion?
To our best knowledge, these problems have not
been studied before. Moreover, opinions may be
expressed in different ways with the variation of
the domain and language of the corpus. When the
domain or language of the corpus is changed, what
conclusions can we obtain? To answer these ques-
tions, in this paper, we adopt a unified framework
to extract opinion targets from reviews, in the key
component of which we vary the methods between
syntactic patterns and alignment model. Then we
run the whole framework on the corpus with dif-
ferent size (from #500 to #1, 000, 000), domain
(three domains) and language (Chinese and En-
glish) to empirically assess the performance varia-
tions and discuss which method is more effective.
Furthermore, this paper naturally addresses an-
other question: is it useful for opinion targets ex-
traction when we combine syntactic patterns and
word alignment model into a unified model? To
this end, we employ a partially supervised align-
ment model (PSWAM) like (Gao et al, 2010; Liu
et al, 2013). Based on the exquisitely designed
high-precision syntactic patterns, we can obtain
some precisely modified relations between words
in sentences, which provide a portion of links of
the full alignments. Then, these partial alignment
links can be regarded as the constrains for a stan-
dard unsupervised word alignment model. And
each target candidate would find its modifier un-
der the partial supervision. In this way, the er-
rors generated in standard unsupervised WAM can
be corrected. For example in Figure 1, ?kindly?
and ?courteous? are incorrectly regarded as the
modifiers for ?foods? if the WAM is performed
in an whole unsupervised framework. However,
by using some high-precision syntactic patterns,
we can assert ?courteous? should be aligned to
?services?, and ?delicious? should be aligned to
?foods?. Through combination under partial su-
pervision, we can see ?kindly? and ?courteous?
are correctly linked to ?services?. Thus, it?s rea-
sonable to expect to yield better performance than
traditional methods. As mentioned in (Liu et al,
2013), using PSWAM can not only inherit the
advantages of WAM: effectively avoiding noises
from syntactic parsing errors when dealing with
informal texts, but also can improve the mining
performance by using partial supervision. How-
ever, is this kind of combination always useful for
opinion target extraction? To access this problem,
we also make comparison between PSWAM based
method and the aforementioned methods in the
same corpora with different size, language and do-
main. The experimental results show the combina-
tion by using PSWAM can be effective on dataset
with small and medium size.
1755
2 Related Work
Opinion target extraction isn?t a new task for opin-
ion mining. There are much work focusing on
this task, such as (Hu and Liu, 2004b; Ding et al,
2008; Li et al, 2010; Popescu and Etzioni, 2005;
Wu et al, 2009). Totally, previous studies can be
divided into two main categories: supervised and
unsupervised methods.
In supervised approaches, the opinion target ex-
traction task was usually regarded as a sequence
labeling problem (Jin and Huang, 2009; Li et al,
2010; Ma and Wan, 2010; Wu et al, 2009; Zhang
et al, 2009). It?s not only to extract a lexicon or list
of opinion targets, but also to find out each opin-
ion target mentions in reviews. Thus, the contex-
tual words are usually selected as the features to
indicate opinion targets in sentences. And classi-
cal sequence labeling models are used to train the
extractor, such as CRFs (Li et al, 2010), HMM
(Jin and Huang, 2009) etc.. Jin et al (2009) pro-
posed a lexicalized HMM model to perform opin-
ion mining. Both Li et al (2010) and Ma et al
(2010) used CRFs model to extract opinion tar-
gets in reviews. Specially, Li et al proposed a
Skip-Tree CRF model for opinion target extrac-
tion, which exploited three structures including
linear-chain structure, syntactic structure, and con-
junction structure. However, the main limitation
of these supervised methods is the need of labeled
training data. If the labeled training data is insuf-
ficient, the trained model would have unsatisfied
extraction performance. Labeling sufficient train-
ing data is time and labor consuming. And for dif-
ferent domains, we need label data independently,
which is obviously impracticable.
Thus, many researches focused on unsupervised
methods, which are mainly to extract a list of opin-
ion targets from reviews. Similar to ours, most ap-
proaches regarded opinion words as the indicator
for opinion targets. (Hu and Liu, 2004a) regarded
the nearest adjective to an noun/noun phrase as
its modifier. Then it exploited an association
rule mining algorithm to mine the associations be-
tween them. Finally, the frequent explicit prod-
uct features can be extracted in a bootstrapping
process by further combining item?s frequency in
dataset. Only using nearest neighbor rule to mine
the modifier for each candidate cannot obtain pre-
cise results. Thus, (Popescu and Etzioni, 2005)
used syntax information to extract opinion targets,
which designed some syntactic patterns to capture
the modified relations between words. The experi-
mental results showed that their method had better
performance than (Hu and Liu, 2004a). Moreover,
(Qiu et al, 2011) proposed a Double Propagation
method to expand sentiment words and opinion
targets iteratively, where they also exploited syn-
tactic relations between words. Specially, (Qiu
et al, 2011) didn?t only design syntactic patterns
for capturing modified relations, but also designed
patterns for capturing relations among opinion tar-
gets and relations among opinion words. How-
ever, the main limitation of Qiu?s method is that
the patterns based on dependency parsing tree may
miss many targets for the large corpora. There-
fore, Zhang et al (2010) extended Qiu?s method.
Besides the patterns used in Qiu?s method, they
adopted some other special designed patterns to
increase recall. In addition they used the HITS
(Kleinberg, 1999) algorithm to compute opinion
target confidences to improve the precision. (Liu
et al, 2012) formulated identifying opinion re-
lations between words as an alignment process.
They used a completely unsupervised WAM to
capture opinion relations in sentences. Then the
opinion targets were extracted in a standard ran-
dom walk framework where two factors were con-
sidered: opinion relevance and target importance.
Their experimental results have shown that WAM
was more effective than traditional syntax-based
methods for this task. (Liu et al, 2013) extend
Liu?s method, which is similar to our method and
also used a partially supervised alignment model
to extract opinion targets from reviews. We notice
these two methods ((Liu et al, 2012) and (Liu et
al., 2013)) only performed experiments on the cor-
pora with a medium size. Although both of them
proved that WAM model is better than the meth-
ods based on syntactic patterns, they didn?t dis-
cuss the performance variation when dealing with
the corpora with different sizes, especially when
the size of the corpus is less than 1,000 and more
than 10,000. Based on their conclusions, we still
don?t know which kind of methods should be se-
lected for opinion target extraction when given a
certain amount of reviews.
3 Opinion Target Extraction
Methodology
To extract opinion targets from reviews, we adopt
the framework proposed by (Liu et al, 2012),
which is a graph-based extraction framework and
1756
has two main components as follows.
1) The first component is to capture opinion
relations in sentences and estimate associations
between opinion target candidates and potential
opinion words. In this paper, we assume opinion
targets to be nouns or noun phrases, and opinion
words may be adjectives or verbs, which are usu-
ally adopted by (Hu and Liu, 2004a; Qiu et al,
2011; Wang and Wang, 2008; Liu et al, 2012).
And a potential opinion relation is comprised of
an opinion target candidate and its corresponding
modified word.
2) The second component is to estimate the
confidence of each candidate. The candidates with
higher confidence scores than a threshold will be
extracted as opinion targets. In this procedure, we
formulate the associations between opinion target
candidates and potential opinion words in a bipar-
tite graph. A random walk based algorithm is em-
ployed on this graph to estimate the confidence of
each target candidate.
In this paper, we fix the method in the sec-
ond component and vary the algorithms in the
first component. In the first component, we re-
spectively use syntactic patterns and unsupervised
word alignment model (WAM) to capture opinion
relations. In addition, we employ a partially super-
vised word alignment model (PSWAM) to incor-
porate syntactic information into WAM. In exper-
iments, we run the whole framework on the differ-
ent corpora to discuss which method is more effec-
tive. In the following subsections, we will present
them in detail.
3.1 The First Component: Capturing
Opinion Relations and Estimating
Associations between Words
3.1.1 Syntactic Patterns
To capture opinion relations in sentences by using
syntactic patterns, we employ the manual designed
syntactic patterns proposed by (Qiu et al, 2011).
Similar to Qiu, only the syntactic patterns based
on the direct dependency are employed to guar-
antee the extraction qualities. The direct depen-
dency has two types. The first type indicates that
one word depends on the other word without any
additional words in their dependency path. The
second type denotes that two words both depend
on a third word directly. Specifically, we employ
Minipar1 to parse sentences. To further make syn-
1http://webdocs.cs.ualberta.ca/lindek/minipar.htm
tactic patterns precisely, we only use a few depen-
dency relation labels outputted by Minipar, such
as mod, pnmod, subj, desc etc. To make a clear
explanation, we give out some syntactic pattern
examples in Table 1. In these patterns, OC is a
potential opinion word which is an adjective or a
verb. TC is an opinion target candidate which is
a noun or noun phrase. The item on the arrows
means the dependency relation type. The item in
parenthesis denotes the part-of-speech of the other
word. In these examples, the first three patterns
are based on the first direct dependency type and
the last two patterns are based on the second direct
dependency type.
Pattern#1: <OC> mod????<TC>
Example: This phone has an amazing design
Pattern#2: <TC> obj???<OC>
Example: I like this phone very much
Pattern#3: <OC> pnmod?????<TC>
Example: the buttons easier to use
Pattern#4: <OC> mod????(NN) subj????<TC>
Example: IPhone is a revolutionary smart phone
Pattern#5: <OC> pred????(VBE) subj????<TC>
Example: The quality of LCD is good
Table 1: Some Examples of Used Syntactic Pat-
terns
3.1.2 Unsupervised Word Alignment Model
In this subsection, we present our method for cap-
turing opinion relations using unsupervised word
alignment model. Similar to (Liu et al, 2012),
every sentence in reviews is replicated to gener-
ate a parallel sentence pair, and the word align-
ment algorithm is applied to the monolingual sce-
nario to align a noun/noun phase with its modi-
fiers. We select IBM-3 model (Brown et al, 1993)
as the alignment model. Formally, given a sen-
tence S = {w1, w2, ..., wn}, we have
Pibm3(A|S)
?
N?
i=1
n(?i|wi)
N?
j=1
t(wj |waj )d(j|aj , N)
(1)
where t(wj |waj ) models the co-occurrence infor-
mation of two words in dataset. d(j|aj , n) mod-
els word position information, which describes the
probability of a word in position aj aligned with a
word in position j. And n(?i|wi) describes the
ability of a word for modifying (being modified
by) several words. ?i denotes the number of words
1757
that are aligned with wi. In our experiments, we
set ?i = 2.
Since we only have interests on capturing opin-
ion relations between words, we only pay at-
tentions on the alignments between opinion tar-
get candidates (nouns/noun phrases) and potential
opinion words (adjectives/verbs). If we directly
use the alignment model, a noun (noun phrase)
may align with other unrelated words, like prepo-
sitions or conjunctions and so on. Thus, we set
constrains on the model: 1) Alignment links must
be assigned among nouns/noun phrases, adjec-
tives/verbs and null words. Aligning to null words
means that this word has no modifier or modifies
nothing; 2) Other unrelated words can only align
with themselves.
3.1.3 Combining Syntax-based Method with
Alignment-based Method
In this subsection, we try to combine syntactic in-
formation with word alignment model. As men-
tioned in the first section, we adopt a partially
supervised alignment model to make this com-
bination. Here, the opinion relations obtained
through the high-precision syntactic patterns (Sec-
tion 3.1.1) are regarded as the ground truth and
can only provide a part of full alignments in sen-
tences. They are treated as the constrains for the
word alignment model. Given some partial align-
ment links A? = {(k, ak)|k ? [1, n], ak ? [1, n]},
the optimal word alignment A? = {(i, ai)|i ?
[1, n], ai ? [1, n]} can be obtained as A? =
argmax
A
P (A|S, A?), where (i, ai) means that a
noun (noun phrase) at position i is aligned with
its modifier at position ai.
Since the labeled data provided by syntactic pat-
terns is not a full alignment, we adopt a EM-based
algorithm, named as constrained hill-climbing al-
gorithm(Gao et al, 2010), to estimate the parame-
ters in the model. In the training process, the con-
strained hill-climbing algorithm can ensure that
the final model is marginalized on the partial align-
ment links. Particularly, in the E step, their method
aims to find out the alignments which are consis-
tent to the alignment links provided by syntactic
patterns, where there are main two steps involved.
1) Optimize towards the constraints. This step
aims to generate an initial alignments for align-
ment model (IBM-3 model in our method), which
can be close to the constraints. First, a simple
alignment model (IBM-1, IBM-2, HMM etc.) is
trained. Then, the evidence being inconsistent
to the partial alignment links will be got rid of
by using the move operator operator mi,j which
changes aj = i and the swap operator sj1,j2 which
exchanges aj1 and aj2 . The alignment is updated
iteratively until no additional inconsistent links
can be removed.
2) Towards the optimal alignment under the
constraints. This step aims to optimize towards
the optimal alignment under the constraints which
starts from the aforementioned initial alignments.
Gao et.al. (2010) set the corresponding cost value
of the invalid move or swap operation in M and
S to be negative, where M and S are respec-
tively called Moving Matrix and Swapping Ma-
trix, which record all possible move and swap
costs between two different alignments. In this
way, the invalid operators will never be picked
which can guarantee that the final alignment links
to have high probability to be consistent with the
partial alignment links provided by high-precision
syntactic patterns.
Then in M-step, evidences from the neighbor of
final alignments are collected so that we can pro-
duce the estimation of parameters for the next iter-
ation. In the process, those statistics which come
from inconsistent alignment links aren?t be picked
up. Thus, we have
P (wi|wai , A?)
=
{ ?, otherwise
P (wi|wai) + ?, inconsistent with A?(2)
where ? means that we make soft constraints on
the alignment model. As a result, we expect some
errors generated through high-precision patterns
(Section 3.1.1) may be revised in the alignment
process.
3.2 Estimating Associations between Words
After capturing opinion relations in sentences, we
can obtain a lot of word pairs, each of which is
comprised of an opinion target candidate and its
corresponding modified word. Then the condi-
tional probabilities between potential opinion tar-
get wt and potential opinion word wo can be es-
timated by using maximum likelihood estimation.
Thus, we have P (wt|wo) = Count(wt,wo)Count(wo) , where
Count(?) means the item?s frequency informa-
tion. P (wt|wo) means the conditional probabili-
ties between two words. At the same time, we can
obtain conditional probability P (wo|wt). Then,
1758
similar to (Liu et al, 2012), the association be-
tween an opinion target candidate and its modifier
is estimated as follows. Association(wt, wo) =
(?? P (wt|wo) + (1? ?)? P (wo|wt))?1, where
? is the harmonic factor. We set ? = 0.5 in our
experiments.
3.3 The Second Component: Estimating
Candidate Confidence
In the second component, we adopt a graph-based
algorithm used in (Liu et al, 2012) to compute
the confidence of each opinion target candidate,
and the candidates with higher confidence than the
threshold will be extracted as the opinion targets.
Here, opinion words are regarded as the impor-
tant indicators. We assume that two target candi-
dates are likely to belong to the similar category, if
they are modified by similar opinion words. Thus,
we can propagate the opinion target confidences
through opinion words.
To model the mined associations between
words, a bipartite graph is constructed, which
is defined as a weighted undirected graph G =
(V,E,W ). It contains two kinds of vertex: opin-
ion target candidates and potential opinion words,
respectively denoted as vt ? V and vo ? V .
As shown in Figure 2, the white vertices repre-
sent opinion target candidates and the gray ver-
tices represent potential opinion words. An edge
evt,vo ? E between vertices represents that there is
an opinion relation, and the weight w on the edge
represents the association between two words.
Figure 2: Modeling Opinion Relations between
Words in a Bipartite Graph
To estimate the confidence of each opinion tar-
get candidate, we employ a random walk algo-
rithm on our graph, which iteratively computes
the weighted average of opinion target confidences
from neighboring vertices. Thus we have
Ci+1 = (1? ?)?M ?MT ? Ci + ? ? I (3)
where Ci+1 and Ci respectively represent the
opinion target confidence vector in the (i + 1)th
and ith iteration. M is the matrix of word asso-
ciations, where Mi,j denotes the association be-
tween the opinion target candidate i and the po-
tential opinion word j. And I is defined as the
prior confidence of each candidate for opinion tar-
get. Similar to (Liu et al, 2012), we set each item
in Iv = tf(v)idf(v)?
v tf(v)idf(v)
, where tf(v) is the term fre-
quency of v in the corpus, and df(v) is computed
by using the Google n-gram corpus2. ? ? [0, 1]
represents the impact of candidate prior knowl-
edge on the final estimation results. In experi-
ments, we set ? = 0.4. The algorithm run un-
til convergence which is achieved when the confi-
dence on each node ceases to change in a tolerance
value.
4 Experiments
4.1 Datasets and Evaluation Metrics
In this section, to answer the questions men-
tioned in the first section, we collect a large
collection named as LARGE, which includes re-
views from three different domains and differ-
ent languages. This collection was also used
in (Liu et al, 2012). In the experiments, re-
views are first segmented into sentences accord-
ing to punctuation. The detailed statistical in-
formation of the used collection is shown in Ta-
ble 2, where Restaurant is crawled from the Chi-
nese Web site: www.dianping.com. The Hotel and
MP3 are used in (Wang et al, 2011), which are re-
spectively crawled from www.tripadvisor.com and
www.amazon.com. For each dataset, we perform
random sampling to generate testing set with dif-
ferent sizes, where we use sampled subsets with
#sentences = 5? 102, 103, 5? 103, 104, 5?
104, 105 and 106 sentences respectively. Each
Domain Language Sentence Reviews
Restaurant Chinese 1,683,129 395,124
Hotel English 1,855,351 185,829
MP3 English 289,931 30,837
Table 2: Experimental Dataset
sentence is tokenized, part-of-speech tagged by
using Stanford NLP tool3, and parsed by using
Minipar toolkit. And the method of (Zhu et al,
2009) is used to identify noun phrases.
2http://books.google.com/ngrams/datasets
3http://nlp.stanford.edu/software/tagger.shtml
1759
We select precision and recall as the metrics.
Specifically, to obtain the ground truth, we man-
ually label all opinion targets for each subset. In
this process, three annotators are involved. First,
every noun/noun phrase and its contexts in review
sentences are extracted. Then two annotators were
required to judge whether every noun/noun phrase
is opinion target or not. If a conflict happens, a
third annotator will make judgment for final re-
sults. The average inter-agreements is 0.74. We
also perform a significant test, i.e., a t-test with a
default significant level of 0.05.
4.2 Compared Methods
We select three methods for comparison as fol-
lows.
? Syntax: It uses syntactic patterns mentioned
in Section 3.1.1 in the first component to
capture opinion relations in reviews. Then
the associations between words are estimated
and the graph based algorithm proposed in
the second component (Section 3.3) is per-
formed to extract opinion targets.
? WAM: It is similar to Syntax, where the only
difference is that WAM uses unsupervised
WAM (Section 3.1.2) to capture opinion re-
lations.
? PSWAM is similar to Syntax and WAM,
where the difference is that PSWAM uses the
method mentioned in Section 3.1.3 to capture
opinion relations, which incorporates syntac-
tic information into word alignment model by
using partially supervised framework.
The experimental results on different domains are
respectively shown in Figure 3, 4 and 5.
4.3 Syntax based Methods vs. Alignment
based Methods
Comparing Syntax with WAM and PSWAM, we
can obtain the following observations:
Figure 3: Experimental results on Restaurant
Figure 4: Experimental results on Hotel
Figure 5: Experimental results on MP3
1) When the size of the corpus is small, Syntax
has better precision than alignment based meth-
ods (WAM and PSWAM). We believe the reason
is that the high-precision syntactic patterns em-
ployed in Syntax can effectively capture opinion
relations in a small amount of texts. In contrast,
the methods based on word alignment model may
suffer from data sparseness for parameter estima-
tion, so the precision is lower.
2) However, when the size of the corpus in-
creases, the precision of Syntax decreases, even
worse than alignment based methods. We believe
it?s because more noises were introduced from
parsing errors with the increase of the size of the
corpus , which will have more negative impacts on
extraction results. In contrast, for estimating the
parameters of alignment based methods, the data
is more sufficient, so the precision is better com-
pared with syntax based method.
3) We also observe that recall of Syntax is
worse than other two methods. It?s because the
human expressions of opinions are diverse and the
manual designed syntactic patterns are limited to
capture all opinion relations in sentences, which
may miss an amount of correct opinion targets.
4) It?s interesting that the performance gap be-
tween these three methods is smaller with the in-
crease of the size of the corpus (more than 50,000).
We guess the reason is that when the data is suffi-
cient enough, we can obtain sufficient statistics for
each opinion target. In such situation, the graph-
based ranking algorithm in the second component
will be apt to be affected by the frequency infor-
mation, so the final performance could not be sen-
sitive to the performance of opinion relations iden-
1760
tification in the first component. Thus, in this situ-
ation, we can get conclusion that there is no obvi-
ously difference on performance between syntax-
based approach and alignment-based approach.
5) From the results on dataset with different lan-
guages and different domains, we can obtain the
similar observations. It indicates that choosing ei-
ther syntactic patterns or word alignment model
for extracting opinion targets can take a few con-
sideration on the language and domain of the cor-
pus.
Thus, based on the above observations, we can
draw the following conclusions: making chooses
between different methods is only related to the
size of the corpus. The method based on syn-
tactic patterns is more suitable for small cor-
pus (#sentences < 5 ? 103 shown in our
experiments). And word alignment model is
more suitable for medium corpus (5 ? 103 <
#sentences < 5 ? 104). Moreover, when the
size of the corpus is big enough, the performance
of two kinds of methods tend to become the same
(#sentences ? 105 shown in our experiments).
4.4 Is It Useful Combining Syntactic Patterns
with Word Alignment Model
In this subsection, we try to see whether combin-
ing syntactic information with alignment model by
using PSWAM is effective or not for opinion tar-
get extraction. From the results in Figure 3, 4 and
5, we can see that PSWAM has the similar recall
compared with WAM in all datasets. PSWAM
outperforms WAM on precision in all dataset. But
the precision gap between PSWAM and WAM
decreases when the size of the corpus increases.
When the size is larger than 5 ? 104, the perfor-
mance of these two methods is almost the same.
We guess the reason is that more noises from pars-
ing errors will be introduced by syntactic patterns
with the increase of the size of corpus , which have
negative impacts on alignment performance. At
the same time, as mentioned above, a great deal of
reviews will bring sufficient statistics for estimat-
ing parameters in alignment model, so the roles
of partial supervision from syntactic information
will be covered by frequency information used in
our graph based ranking algorithm.
Compared with State-of-the-art Methods.
However, it?s not say that this combination is
not useful. From the results, we still see that
PSWAM outperforms WAM in all datasets on
precision when size of corpus is smaller than
5 ? 104. To further prove the effectiveness of
our combination, we compare PSWAM with some
state-of-the-art methods, including Hu (Hu and
Liu, 2004a), which extracted frequent opinion tar-
get words based on association mining rules, DP
(Qiu et al, 2011), which extracted opinion tar-
gets through syntactic patterns, and LIU (Liu et
al., 2012), which fulfilled this task by using un-
supervised WAM. The parameter settings in these
baselines are the same as the settings in the orig-
inal papers. Because of the space limitation, we
only show the results on Restaurant and Hotel, as
shown in Figure 6 and 7.
Figure 6: Compared with the State-of-the-art
Methods on Restaurant
Figure 7: Compared with the State-of-the-art
Methods on Hotel
From the experimental results, we can obtain
the following observations. PSWAM outperforms
other methods in most datasets. This indicates
that our method based on PSWAM is effective
for opinion target extraction. Especially compared
PSWAM with LIU, both of which are based on
word alignment model, we can see PSWAM iden-
tifies opinion relations by performing WAM under
partial supervision, which can effectively improve
the precision when dealing with small and medium
corpus. However, these improvements are limited
when the size of the corpus increases, which has
the similar observations obtained above.
The Impact of Syntactic Information on
Word Alignment Model. Although we have
prove the effectiveness of PSWAM in the corpus
with small and medium size, we are still curious
about how the performance varies when we incor-
1761
porate different amount of syntactic information
into WAM. In this experiment, we rank the used
syntactic patterns mentioned in Section 3.1.1 ac-
cording to the quantities of the extracted alignment
links by these patterns. Then, to capture opin-
ion relations, we respectively use top N syntactic
patterns according to frequency mentioned above
to generate partial alignment links for PSWAM in
section 3.1.3. We respectively define N=[1,7]. The
larger is N , the more syntactic information is in-
corporated. Because of the space limitation, only
the average performance of all dataset is shown in
Figure 8.
Figure 8: The Impacts of Different Syntactic In-
formation on Word Alignment Model
In Figure 8, we can observe that the syntactic in-
formation mainly have effect on precision. When
the size of the corpus is small, the opinion rela-
tions mined by high-precision syntactic patterns
are usually correct, so incorporating more syntac-
tic information can improve the precision of word
alignment model more. However, when the size of
the corpus increases, incorporating more syntactic
information has little impact on precision.
5 Conclusions and Future Work
This paper discusses the performance variation of
syntax based methods and alignment based meth-
ods on opinion target extraction task for the dataset
with different sizes, different languages and dif-
ferent domains. Through experimental results, we
can see that choosing which method is not related
with corpus domain and language, but strongly
associated with the size of the corpus . We can
conclude that syntax-based method is likely to be
more effective when the size of the corpus is small,
and alignment-based methods are more useful for
the medium size corpus. We further verify that in-
corporating syntactic information into word align-
ment model by using PSWAM is effective when
dealing with the corpora with small or medium
size. When the size of the corpus is larger and
larger, the performance gap between syntax based,
WAM and PSWAM will decrease.
In future work, we will extract opinion targets
based on not only opinion relations. Other seman-
tic relations, such as the topical associations be-
tween opinion targets (or opinion words) should
also be employed. We believe that considering
multiple semantic associations will help to im-
prove the performance. In this way, how to model
heterogenous relations in a unified model for opin-
ion targets extraction is worthy to be studied.
Acknowledgement
This work was supported by the National Natu-
ral Science Foundation of China (No. 61070106,
No. 61272332 and No. 61202329), the Na-
tional High Technology Development 863 Pro-
gram of China (No. 2012AA011102), the Na-
tional Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining (WSDM).
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 1?10, Uppsala, Sweden,
July. Association for Computational Linguistics.
1762
Mingqin Hu and Bing Liu. 2004a. Mining opinion fea-
tures in customer reviews. In Proceedings of Con-
ference on Artificial Intelligence (AAAI).
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Wei Jin and Hay Ho Huang. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of International Confer-
ence on Machine Learning (ICML).
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Chu-Ren Huang and Dan Jurafsky, editors, COL-
ING, pages 653?661. Tsinghua University Press.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Allan Ellis and Tatsuya Hagino,
editors, WWW, pages 342?351. ACM.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013.
Opinion target extraction using partially supervised
word alignment model.
Tengfei Ma and Xiaojun Wan. 2010. Opinion tar-
get extraction in chinese news comments. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 782?790. Chinese Information Pro-
cessing Society of China.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation.
Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Chid Apt, Joydeep Ghosh,
and Padhraic Smyth, editors, KDD, pages 618?626.
ACM.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?09, pages
726?727, New York, NY, USA. ACM.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking
product features in opinion documents. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 1462?1470. Chinese Information
Processing Society of China.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In David Wai-Lok Cheung,
Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and
Jimmy J. Lin, editors, CIKM, pages 1799?1802.
ACM.
1763
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1764?1773,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Opinion Words and Opinion Targets in a Two-Stage Framework
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, ybchen, jzhao}@nlpr.ia.ac.cn
Abstract
This paper proposes a novel two-stage
method for mining opinion words and
opinion targets. In the first stage, we
propose a Sentiment Graph Walking algo-
rithm, which naturally incorporates syn-
tactic patterns in a Sentiment Graph to ex-
tract opinion word/target candidates. Then
random walking is employed to estimate
confidence of candidates, which improves
extraction accuracy by considering confi-
dence of patterns. In the second stage, we
adopt a self-learning strategy to refine the
results from the first stage, especially for
filtering out high-frequency noise terms
and capturing the long-tail terms, which
are not investigated by previous meth-
ods. The experimental results on three real
world datasets demonstrate the effective-
ness of our approach compared with state-
of-the-art unsupervised methods.
1 Introduction
Opinion mining not only assists users to make in-
formed purchase decisions, but also helps busi-
ness organizations understand and act upon cus-
tomer feedbacks on their products or services in
real-time. Extracting opinion words and opinion
targets are two key tasks in opinion mining. Opin-
ion words refer to those terms indicating positive
or negative sentiment. Opinion targets represent
aspects or attributes of objects toward which opin-
ions are expressed. Mining these terms from re-
views of a specific domain allows a more thorough
understanding of customers? opinions.
Opinion words and opinion targets often co-
occur in reviews and there exist modified relations
(called opinion relation in this paper) between
them. For example, in the sentence ?It has a clear
screen?, ?clear? is an opinion word and ?screen? is
an opinion target, and there is an opinion relation
between the two words. It is natural to identify
such opinion relations through common syntactic
patterns (also called opinion patterns in this pa-
per) between opinion words and targets. For ex-
ample, we can extract ?clear? and ?screen? by us-
ing a syntactic pattern ?Adj-{mod}-Noun?, which
captures the opinion relation between them. Al-
though previous works have shown the effective-
ness of syntactic patterns for this task (Qiu et al,
2009; Zhang et al, 2010), they still have some lim-
itations as follows.
False Opinion Relations: As an example, the
phrase ?everyday at school? can be matched by
a pattern ?Adj-{mod}-(Prep)-{pcomp-n}-Noun?,
but it doesn?t bear any sentiment orientation. We
call such relations that match opinion patterns but
express no opinion false opinion relations. Pre-
vious pattern learning algorithms (Zhuang et al,
2006; Kessler and Nicolov, 2009; Jijkoun et al,
2010) often extract opinion patterns by frequency.
However, some high-frequency syntactic patterns
can have very poor precision (Kessler and Nicolov,
2009).
False Opinion Targets: In another case, the
phrase ?wonderful time? can be matched by
an opinion pattern ?Adj-{mod}-Noun?, which is
widely used in previous works (Popescu and Et-
zioni, 2005; Qiu et al, 2009). As can be seen, this
phrase does express a positive opinion but unfortu-
nately ?time? is not a valid opinion target for most
domains such as MP3. Thus, false opinion targets
are extracted. Due to the lack of ground-truth
knowledge for opinion targets, non-target terms
introduced in this way can be hardly filtered out.
Long-tail Opinion Targets: We further no-
tice that previous works prone to extract opinion
targets with high frequency (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Qiu et al, 2009; Zhu
et al, 2009), and they often have difficulty in iden-
tifying the infrequent or long-tail opinion targets.
1764
To address the problems stated above, this pa-
per proposes a two-stage framework for mining
opinion words and opinion targets. The under-
lying motivation is analogous to the novel idea
?Mine the Easy, Classify the Hard? (Dasgupta and
Ng, 2009). In our first stage, we propose a Senti-
ment Graph Walking algorithm to cope with the
false opinion relation problem, which mines easy
cases of opinion words/targets. We speculate that
it may be helpful to introduce a confidence score
for each pattern. Concretely, we create a Sen-
timent Graph to model opinion relations among
opinion word/target/pattern candidates and apply
random walking to estimate confidence of them.
Thus, confidence of pattern is considered in a uni-
fied process. Patterns that often extract false opin-
ion relations will have low confidence, and terms
introduced by low-confidence patterns will also
have low confidence accordingly. This could po-
tentially improve the extraction accuracy.
In the second stage, we identify the hard cases,
which aims to filter out false opinion targets and
extract long-tail opinion targets. Previous super-
vised methods have been shown to achieve state-
of-the-art results for this task (Wu et al, 2009; Jin
and Ho, 2009; Li et al, 2010). However, the big
challenge for fully supervised method is the lack
of annotated training data. Therefore, we adopt a
self-learning strategy. Specifically, we employ a
semi-supervised classifier to refine the target re-
sults from the first stage, which uses some highly
confident target candidates as the initial labeled
examples. Then opinion words are also refined.
Our main contributions are as follows:
? We propose a Sentiment Graph Walking al-
gorithm to mine opinion words and opinion
targets from reviews, which naturally incor-
porates confidence of syntactic pattern in a
graph to improve extraction performance. To
our best knowledge, the incorporation of pat-
tern confidence in such a Sentiment Graph
has never been studied before for opinion
words/targets mining task (Section 3).
? We adopt a self-learning method for refining
opinion words/targets generated by Sentiment
Graph Walking. Specifically, it can remove
high-frequency noise terms and capture long-
tail opinion targets in corpora (Section 4).
? We perform experiments on three real world
datasets, which demonstrate the effectiveness
of our method compared with state-of-the-art
unsupervised methods (Section 5).
2 Related Work
In opinion words/targets mining task, most unsu-
pervised methods rely on identifying opinion rela-
tions between opinion words and opinion targets.
Hu and Liu (2004) proposed an association mining
technique to extract opinion words/targets. The
simple heuristic rules they used may potentially
introduce many false opinion words/targets. To
identify opinion relations more precisely, subse-
quent research work exploited syntax information.
Popescu and Etzioni (2005) used manually com-
plied syntactic patterns and Pointwise Mutual In-
formation (PMI) to extract opinion words/targets.
Qiu et al (2009) proposed a bootstrapping frame-
work called Double Propagation which intro-
duced eight heuristic syntactic rules. While man-
ually defining syntactic patterns could be time-
consuming and error-prone, we learn syntactic
patterns automatically from data.
There have been extensive works on mining
opinion words and opinion targets by syntac-
tic pattern learning. Riloff and Wiebe (2003)
performed pattern learning through bootstrapping
while extracting subjective expressions. Zhuang
et al (2006) obtained various dependency re-
lationship templates from an annotated movie
corpus and applied them to supervised opinion
words/targets extraction. Kobayashi et al (2007)
adopted a supervised learning technique to search
for useful syntactic patterns as contextual clues.
Our approach is similar to (Wiebe and Riloff,
2005) and (Xu et al, 2013), all of which apply
syntactic pattern learning and adopt self-learning
strategy. However, the task of (Wiebe and Riloff,
2005) was to classify sentiment orientations in
sentence level, while ours needs to extract more
detailed information in term level. In addition,
our method extends (Xu et al, 2013), and we
give a more complete and in-depth analysis on
the aforementioned problems in the first section.
There were also many works employed graph-
based method (Li et al, 2012; Zhang et al, 2010;
Hassan and Radev, 2010; Liu et al, 2012), but
none of previous works considered confidence of
patterns in the graph.
In supervised approaches, various kinds of
models were applied, such as HMM (Jin and Ho,
2009), SVM (Wu et al, 2009) and CRFs (Li et al,
2010). The downside of supervised methods was
the difficulty of obtaining annotated training data
in practical applications. Also, classifiers trained
1765
on one domain often fail to give satisfactory re-
sults when shifted to another domain. Our method
does not rely on annotated training data.
3 The First Stage: Sentiment Graph
Walking Algorithm
In the first stage, we propose a graph-based al-
gorithm called Sentiment Graph Walking to mine
opinion words and opinion targets from reviews.
3.1 Opinion Pattern Learning for Candidates
Generation
For a given sentence, we first obtain its depen-
dency tree. Following (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Qiu et al, 2009), we regard all
adjectives as opinion word candidates (OC) and
all nouns or noun phrases as opinion target can-
didates (TC). A statistic-based method in (Zhu et
al., 2009) is used to detect noun phrases. Then
candidates are replaced by wildcards ?<OC>? or
?<TC>?. Figure 1 gives a dependency tree exam-
ple generated by Minipar (Lin, 1998).
p red s det
m od
gor geous<OC>
is(VBE)
style<TC>
the(Det)
of(P r ep) scr een<TC>pcom p-n
the(Det)
det
Figure 1: The dependency tree of the sentence
?The style of the screen is gorgeous?.
We extract two kinds of opinion patterns: ?OC-
TC? pattern and ?TC-TC? pattern. The ?OC-
TC? pattern is the shortest path between an OC
wildcard and a TC wildcard in dependency tree,
which captures opinion relation between an opin-
ion word candidate and an opinion target can-
didate. Similarly, the ?TC-TC? pattern cap-
tures opinion relation between two opinion tar-
get candidates.1 Words in opinion patterns are
replaced by their POS tags, and we constrain
that there are at most two words other than
wildcards in each pattern. In Figure 1, there
are two opinion patterns marked out by dash
lines: ?<OC>{pred}(VBE){s}<TC>? for the
?OC-TC? type and ?<TC>{mod}(Prep){pcomp-
n}<TC>? for the ?TC-TC? type. After all pat-
1We do not identify the opinion relation ?OC-OC? be-
cause this relation is often unreliable.
terns are generated, we drop those patterns with
frequency lower than a threshold F .
3.2 Sentiment Graph Construction
To model the opinion relations among opinion
words/targets and opinion patterns, a graph named
as Sentiment Graph is constructed, which is a
weighted, directed graph G = (V,E,W ), where
? V = {Voc ? Vtc ? Vp} is the set of vertices in
G, where Voc, Vtc and Vp represent the set of
opinion word candidates, opinion target can-
didates and opinion patterns, respectively.
? E = {Epo?Ept} ? {Vp?Voc}?{Vp?Vtc}
is the weighted, bi-directional edge set in G,
where Epo and Ept are mutually exclusive
sets of edges connecting opinion word/target
vertices to opinion pattern vertices. Note that
there are no edges between Voc and Vtc.
? W : E ? R+ is the weight function which
assigns non-negative weight to each edge.
For each (e : va ? vb) ? E, where
va, vb ? V , the weight function w(va, vb) =
freq(va, vb)/freq(va), where freq(?) is the
frequency of a candidate extracted by opinion
patterns or co-occurrence frequency between
two candidates.
Figure 2 shows an example of Sentiment Graph.
n icelarge
screen display
<OC>{mod}<TC> <OC>{mod}<TC>{con j}<TC>
1
0.8
0.7
0.2
0.3
0.4
0.2
0.33
0.33
0.33
0.6
0.4
0.2 0.2
Figure 2: An example of Sentiment Graph.
3.3 Confidence Estimation by Random
Walking with Restart
We believe that considering confidence of patterns
can potentially improve the extraction accuracy.
Our intuitive idea is: (i) If an opinion word/target
is with higher confidence, the syntactic patterns
containing this term are more likely to be used to
express customers? opinion. (ii) If an opinion pat-
tern has higher confidence, terms extracted by this
pattern are more likely to be correct. It?s a rein-
forcement process.
1766
We use Random Walking with Restart (RWR)
algorithm to implement our idea described above.
Let Moc p denotes the transition matrix from Voc
to Vp, for vo ? Voc, vp ? Vp, Moc p(vo, vp) =
w(vo, vp). Similarly, we have Mtc p, Mp oc,
Mp tc. Let c denotes confidence vector of candi-
dates so ctoc, cttc and ctp are confidence vectors for
opinion word/target/pattern candidates after walk-
ing t steps. Initially c0oc is uniformly distributed
on a few domain-independent opinion word seeds,
then the following formula are updated iteratively
until cttc and ctoc converge:
ct+1p = MToc p ? ctoc +MTtc p ? cttc (1)
ct+1oc = (1? ?)MTp oc ? ctp + ?c0oc (2)
ct+1tc = MTp tc ? ctp (3)
where MT is the transpose of matrix M and ? is
a small probability of teleporting back to the seed
vertices which prevents us from walking too far
away from the seeds. In the experiments below, ?
is set 0.1 empirically.
4 The Second Stage: Refining Extracted
Results Using Self-Learning
At the end of the first stage, we obtain a ranked
list of opinion words and opinion targets, in which
higher ranked terms are more likely to be correct.
Nevertheless, there are still some issues needed to
be addressed:
1) In the target candidate list, some high-
frequency frivolous general nouns such as
?thing? and ?people? are also highly ranked.
This is because there exist many opinion ex-
pressions containing non-target terms such as
?good thing?, ?nice people?, etc. in reviews.
Due to the lack of ground-truth knowledge
for opinion targets, the false opinion target
problem still remains unsolved.
2) In another aspect, long-tail opinion targets
may have low degree in Sentiment Graph.
Hence their confidence will be low although
they may be extracted by some high qual-
ity patterns. Therefore, the first stage is in-
capable of dealing with the long-tail opinion
target problem.
3) Furthermore, the first stage also extracts
some high-frequency false opinion words
such as ?every?, ?many?, etc. Many terms
of this kind are introduced by high-frequency
false opinion targets, for there are large
amounts of phrases like ?every time? and
?many people?. So this issue is a side effect
of the false opinion target problem.
To address these issues, we exploit a self-
learning strategy. For opinion targets, we use a
semi-supervised binary classifier called target re-
fining classifier to refine target candidates. For
opinion words, we use the classified list of opin-
ion targets to further refine the extracted opinion
word candidates.
4.1 Opinion Targets Refinement
There are two keys for opinion target refinement:
(i) How to generate the initial labeled data for tar-
get refining classifier. (ii) How to properly repre-
sent a long-tail opinion target candidate other than
comparing frequency between different targets.
For the first key, it is clearly improper to select
high-confidence targets as positive examples and
choose low-confidence targets as negative exam-
ples2, for there are noise with high confidence and
long-tail targets with low confidence. Fortunately,
a large proportion of general noun noises are the
most frequent words in common texts. Therefore,
we can generate a small domain-independent gen-
eral noun (GN) corpus from large web corpora to
cover some most frequently used general noun ex-
amples. Then labeled examples can be drawn from
the target candidate list and the GN corpus.
For the second key, we utilize opinion words
and opinion patterns with their confidence scores
to represent an opinion target. By this means, a
long-tail opinion target can be determined by its
own contexts, whose weights are learnt from con-
texts of frequent opinion targets. Thus, if a long-
tail opinion target candidate has high contextual
support, it will have higher probability to be found
out in despite of its low frequency.
Creation of General Noun Corpora. 1000
most frequent nouns in Google-1-gram3 were se-
lected as general noun candidates. On the other
hand, we added all nouns in the top three levels of
hyponyms in four WordNet (Miller, 1995) synsets
?object?, ?person?, ?group? and ?measure? into
the GN corpus. Our idea was based on the fact that
a term is more general when it sits in higher level
in the WordNet hierarchy. Then inapplicable can-
didates were discarded and a 3071-word English
2Note that the ?positive? and ?negative? here denote opin-
ion targets and non-target terms respectively and they do not
indicate sentiment polarities.
3http://books.google.com/ngrams.
1767
GN corpus was created. Another Chinese GN cor-
pus with 3493 words was generated in the similar
way from HowNet (Gan and Wong, 2000).
Generation of Labeled Examples. Let T =
{Y+1,Y?1} denotes the initial labeled set, where
N most highly confident target candidates but not
in our GN corpora are regarded as the positive ex-
ample set Y+1, other N terms from GN corpora
which are also top ranked in the target list are se-
lected as the negative example set Y?1. The re-
minder unlabeled candidates are denoted by T ?.
Feature Representation for Classifier. Given
T and T ? in the form of {(xi, yi)}. For a target
candidate ti, xi = (o1, . . . , on, p1, . . . , pm)T rep-
resents its feature vector, where oj is the opinion
word feature and pk is the opinion pattern feature.
The value of feature is defined as follows,
x(oj) = conf(oj)?
?
pk freq(ti, oj , pk)
freq(oj)
(4)
x(pk) = conf(pk)?
?
oj freq(ti, oj , pk)
freq(pk)
(5)
where conf(?) denotes confidence score estimated
by RWR, freq(?) has the same meaning as in Sec-
tion 3.2. Particularly, freq(ti, oj , pk) represents
the frequency of pattern pk extracting opinion tar-
get ti and opinion word oj .
Target Refinement Classifier: We use support
vector machine as the binary classifier. Hence, the
classification problem can be formulated as to find
a hyperplane < w, b > that separates both labeled
set T and unlabeled set T ? with maximum mar-
gin. The optimization goal is to minimize over
(T ,T ?,w, b, ?1, ..., ?n, ??1 , ..., ??k):
1
2 ||w||
2 + C
n?
i=0
?i + C?
k?
j=0
??j
subject to : ?ni=1 : yi[w ? xi + b] ? 1? ?i
?kj=1 : y?j [w ? x?j + b] ? 1? ??j
?ni=1 : ?i > 0
?kj=1 : ??j > 0
where yi, y?j ? {+1,?1}, xi and x?j represent
feature vectors, C and C? are parameters set by
user. This optimization problem can be imple-
mented by a typical Transductive Support Vector
Machine (TSVM) (Joachims, 1999).
4.2 Opinion Words Refinement
We use the classified opinion target results to re-
fine opinion words by the following equation,
s(oj) =
?
ti?T
?
pk
s(ti)conf(pk)freq(ti, oj , pk)
freq(ti)
where T is the opinion target set in which each el-
ement is classified as positive during opinion tar-
get refinement, s(ti) denotes confidence score ex-
ported by the target refining classifier. Particularly,
freq(ti) =
?
oj
?
pk freq(ti, oj , pk). A higher
score of s(oj) means that candidate oj is more
likely to be an opinion word.
5 Experiments
5.1 Datasets and Evaluation Metrics
Datasets: We select three real world datasets to
evaluate our approach. The first one is called
Customer Review Dataset (CRD) (Hu and Liu,
2004) which contains reviews on five different
products (represented by D1 to D5) in English.
The second dataset is pre-annotated and published
in COAE084, where two domains of Chinese re-
views are selected. At last, we employ a bench-
mark dataset in (Wang et al, 2011) and named it
as Large. We manually annotated opinion words
and opinion targets as the gold standard. Three
annotators were involved. Firstly, two annotators
were required to annotate out opinion words and
opinion targets in sentences. When conflicts hap-
pened, the third annotator would make the final
judgment. The average Kappa-values of the two
domains were 0.71 for opinion words and 0.66
for opinion targets. Detailed information of our
datasets is shown in Table 1.
Dataset Domain #Sentences #OW #OT
Large
(English)
Hotel 10,000 434 1,015
MP3 10,000 559 1,158
COAE08(Chinese)
Camera 2,075 351 892
Car 4,783 622 1,179
Table 1: The detailed information of datasets. OW
stands for opinion words and OT stands for targets.
Pre-processing: Firstly, HTML tags are re-
moved from texts. Then Minipar (Lin, 1998)
is used to parse English corpora, and Standford
Parser (Chang et al, 2009) is used for Chinese
4http://ir-china.org.cn/coae2008.html
1768
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
Ours-Stage1 0.79 0.85 0.82 0.82 0.87 0.84 0.83 0.87 0.85 0.78 0.88 0.83 0.82 0.88 0.85 0.84
Ours-Full 0.86 0.82 0.84 0.88 0.83 0.85 0.89 0.86 0.87 0.83 0.86 0.84 0.89 0.85 0.87 0.86
Table 2: Results of opinion target extraction on the Customer Review Dataset.
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
Ours-Stage1 0.61 0.75 0.67 0.55 0.80 0.65 0.63 0.75 0.68 0.60 0.69 0.64 0.68 0.70 0.69 0.67
Ours-Full 0.64 0.74 0.69 0.59 0.79 0.68 0.66 0.71 0.68 0.65 0.67 0.66 0.72 0.67 0.69 0.68
Table 3: Results of opinion word extraction on the Customer Review Dataset.
corpora. Stemming and fuzzy matching are also
performed following previous work (Hu and Liu,
2004).
Evaluation Metrics: We evaluate our method
by precision(P), recall(R) and F-measure(F).
5.2 Our Method vs. the State-of-the-art
Three state-of-the-art unsupervised methods are
used as competitors to compare with our method.
Hu extracts opinion words/targets by using ad-
jacency rules (Hu and Liu, 2004).
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009).
Zhang is an enhanced version of DP and em-
ploys HITS algorithm (Kleinberg, 1999) to rank
opinion targets (Zhang et al, 2010).
Ours-Full is the full implementation of our
method. We employ SVMlight (Joachims, 1999)
as the target refining classifier. Default parameters
are used except the bias item is set 0.
Ours-Stage1 only uses Sentiment Graph Walk-
ing algorithm which does?t have opinion word and
opinion target refinement.
All of the above approaches use same five
common opinion word seeds. The choice of opin-
ion seeds seems reasonable, as most people can
easily come up with 5 opinion words such as
?good?, ?bad?, etc. The performance on five prod-
ucts of CRD dataset is shown in Table 2 and Ta-
ble 3. Zhang does not extract opinion words so
their results for opinion words are not taken into
account. We can see that Ours-Stage1 achieves
superior recall but has some loss in precision com-
pared with DP and Zhang. This may be because
the CRD dataset is too small and our statistic-
based method may suffer from data sparseness.
In spite of this, Ours-Full achieves comparable F-
measure with DP, which is a well-designed rule-
based method.
The results on two larger datasets are shown
in Table 4 and Table 5, from which we can have
the following observation: (i) All syntax-based-
methods outperform Hu, showing the importance
of syntactic information in opinion relation identi-
fication. (ii) Ours-Full outperforms the three com-
petitors on all domains provided. (iii) Ours-Stage1
outperforms Zhang, especially in terms of recall.
We believe it benefits from our automatical pattern
learning algorithm. Moreover, Ours-Stage1 do
not loss much in precision compared with Zhang,
which indicates the applicability to estimate pat-
tern confidence in Sentiment Graph. (iv) Ours-
Full achieves 4-9% improvement in precision over
the most accurate method, which shows the effec-
tiveness of our second stage.
5.3 Detailed Discussions
This section gives several variants of our method
to have a more detailed analysis.
Ours-Bigraph constructs a bi-graph between
opinion words and targets, so opinion patterns
are not included in the graph. Then RWR algo-
rithm is used to only assign confidence to opinion
word/target candidates.
Ours-Stage2 only contains the second stage,
which doesn?t apply Sentiment Graph Walking al-
gorithm. Hence the confidence score conf(?) in
Equations (4) and (5) have no values and they are
set to 1. The initial labeled examples are exactly
the same as Ours-Full. Due to the limitation of
space, we only give analysis on opinion target ex-
traction results in Figure 3.
1769
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.53 0.55 0.54 0.55 0.57 0.56 0.63 0.65 0.64 0.62 0.58 0.60 0.58
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
Zhang 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
Ours-Stage1 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
Ours-Full 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
Table 4: Results of opinion targets extraction on Large and COAE08.
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.48 0.65 0.55 0.51 0.68 0.58 0.72 0.74 0.73 0.70 0.71 0.70 0.64
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.80 0.73 0.76 0.79 0.71 0.75 0.68
Ours-Stage1 0.59 0.69 0.64 0.61 0.71 0.66 0.79 0.78 0.78 0.77 0.77 0.77 0.71
Ours-Full 0.64 0.67 0.65 0.67 0.69 0.68 0.82 0.78 0.80 0.80 0.76 0.78 0.73
Table 5: Results of opinion words extraction on Large and COAE08.
Figure 3: Opinion target extraction results.
5.3.1 The Effect of Sentiment Graph Walking
We can see that our graph-based methods (Ours-
Bigraph and Ours-Stage1) achieve higher recall
than Zhang. By learning patterns automatically,
our method captures opinion relations more ef-
ficiently. Also, Ours-Stage1 outperforms Ours-
Bigraph, especially in precision. We believe it is
because Ours-Stage1 estimated confidence of pat-
terns so false opinion relations are reduced. There-
fore, the consideration of pattern confidence is
beneficial as expected, which alleviates the false
opinion relation problem. On another hand, we
find that Ours-Stage2 has much worse perfor-
mance than Ours-Full. This shows the effective-
ness of Sentiment Graph Walking algorithm since
the confidence scores estimated in the first stage
are indispensable and indeed key to the learning
of the second stage.
5.3.2 The Effect of Self-Learning
Figure 4 shows the average Precision@N curve of
four domains on opinion target extraction. Ours-
GN-Only is implemented by only removing 50
initial negative examples found by our GN cor-
pora. We can see that the GN corpora work quite
well, which find out most top-ranked false opin-
ion targets. At the same time, Ours-Full has much
better performance than Ours-GN-Only which in-
dicates that Ours-Full can filter out more noises
other than the initial negative examples. There-
fore, our self-learning strategy alleviates the short-
coming of false opinion target problem. More-
over, Table 5 shows that the performance of opin-
ion word extraction is also improved based on the
classified results of opinion targets.
Figure 4: The average precision@N curve of the
four domains on opinion target extraction.
1770
ID Pattern Example #Ext. Conf. PrO PrT
#1 <OC>{mod}<TC> it has a clear screen 7344 0.3938 0.59 0.66
#2 <TC>{subj}<OC> the sound quality is excellent 2791 0.0689 0.62 0.70
#3 <TC>{conj}<TC> the size and weight make it convenient 3620 0.0208 N/A 0.67
#4 <TC>{subj}<TC> the button layout is a simplistic plus 1615 0.0096 N/A 0.67
#5 <OC>{pnmod}<TC> the buttons easier to use 128 0.0014 0.61 0.34
#6 <TC>{subj}(V){s}(VBE){subj}<OC> software provided is simple 189 0.0015 0.54 0.33
#7 <OC>{mod}(Prep){pcomp-c}(V){obj}<TC> great for playing audible books 211 0.0013 0.43 0.48
Table 6: Examples of English patterns. #Ext. represent number of terms extracted, Conf. denotes confi-
dence score estimated by RWR and PrO/PrT stand for precisions of extraction on opinion words/targets
of a pattern respectively. Opinion words in examples are in bold and opinion targets are in italic.
Figure 5 gives the recall of long-tail opinion
targets5 extracted, where Ours-Full is shown to
have much better performance than Ours-Stage1
and the three competitors. This observation proves
that our method can improve the limitation of
long-tail opinion target problem.
Figure 5: The recall of long-tail opinion targets.
5.3.3 Analysis on Opinion Patterns
Table 6 shows some examples of opinion pattern
and their extraction accuracy on MP3 reviews in
the first stage. Pattern #1 and #2 are the two
most high-confidence opinion patterns of ?OC-
TC? type, and Pattern #3 and #4 demonstrate two
typical ?TC-TC? patterns. As these patterns ex-
tract too many terms, the overall precision is very
low. We give Precision@400 of them, which is
more meaningful because only top listed terms
in the extracted results are regarded as opinion
targets. Pattern #5 and #6 have high precision
on opinion words but low precision on opinion
targets. This observation demonstrates the false
opinion target problem. Pattern #7 is a pattern ex-
ample that extracts many false opinion relations
and it has low precision for both opinion words
and opinion targets. We can see that Pattern #7 has
5Since there is no explicit definition for the notion ?long-
tail?, we conservatively regard 60% opinion targets with the
lowest frequency as the ?long-tail? terms.
a lower confidence compared with Pattern #5 and
#6 although it extracts more words. It?s because
it has a low probability of walking from opinion
seeds to this pattern. This further proves that our
method can reduce the confidence of low-quality
patterns.
5.3.4 Sensitivity of Parameters
Finally, we study the sensitivity of parameters
when recall is fixed at 0.70. Figure 6 shows the
precision curves at different N initial training ex-
amples and F filtering frequency. We can see that
the performance saturates when N is set to 50 and
it does not vary much under different F , showing
the robustness of our method. We thus set N to
50, and F to 3 for CRD, 5 for COAE08 and 10 for
Large accordingly.
Figure 6: Influence of parameters.
1771
6 Conclusion and Future Work
This paper proposes a novel two-stage framework
for mining opinion words and opinion targets. In
the first stage, we propose a Sentiment Graph
Walking algorithm, which incorporates syntactic
patterns in a Sentiment Graph to improve the ex-
traction performance. In the second stage, we pro-
pose a self-learning method to refine the result of
first stage. The experimental results show that our
method achieves superior performance over state-
of-the-art unsupervised methods.
We further notice that opinion words are not
limited to adjectives but can also be other type of
word such as verbs or nouns. Identifying all kinds
of opinion words is a more challenging task. We
plan to study this problem in our future work.
Acknowledgement
Thanks to Prof. Yulan He for her insightful
advices. This work was supported by the Na-
tional Natural Science Foundation of China (No.
61070106, No. 61272332 and No. 61202329),
the National High Technology Development 863
Program of China (No. 2012AA011102), the
National Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, SSST
?09, pages 51?59.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to au-
tomatic sentiment classification. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 701?709.
Kok Wee Gan and Ping Wai Wong. 2000. Anno-
tating information structures in chinese texts using
hownet. In Proceedings of the second workshop on
Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for
Computational Linguistics - Volume 12, CLPW ?00,
pages 85?92, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 395?
403, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 585?594,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, ICML
?09, pages 465?472.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pages 200?209.
Jason Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking
of linguistic configurations. In Proceedings of the
Third International AAAI Conference on Weblogs
and Social Media.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-
of relations in opinion mining. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1065?1074, June.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
653?661, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
410?419, July.
1772
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on Evaluation of Parsing Sys-
tems at ICLRE.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ?12, pages 1346?1356,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, EMNLP ?03,
pages 105?112, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th international
conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?05, pages 486?497.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Walk and learn: A two-stage approach
for opinion words and opinion targets co-extraction.
In Proceedings of the 22nd International World Wide
Web Conference, WWW ?13.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM conference on Information and knowledge
management, CIKM ?09, pages 1799?1802.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management, CIKM
?06, pages 43?50.
1773
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104?109,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Inference for Heterogeneous Dependency Parsing
Guangyou Zhou and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao}@nlpr.ia.ac.cn
Abstract
This paper is concerned with the problem
of heterogeneous dependency parsing. In
this paper, we present a novel joint infer-
ence scheme, which is able to leverage
the consensus information between het-
erogeneous treebanks in the parsing phase.
Different from stacked learning meth-
ods (Nivre and McDonald, 2008; Martins
et al, 2008), which process the depen-
dency parsing in a pipelined way (e.g., a
second level uses the first level outputs), in
our method, multiple dependency parsing
models are coordinated to exchange con-
sensus information. We conduct experi-
ments on Chinese Dependency Treebank
(CDT) and Penn Chinese Treebank (CTB),
experimental results show that joint infer-
ence can bring significant improvements
to all state-of-the-art dependency parsers.
1 Introduction
Dependency parsing is the task of building depen-
dency links between words in a sentence, which
has recently gained a wide interest in the natu-
ral language processing community and has been
used for many problems ranging from machine
translation (Ding and Palmer, 2004) to question
answering (Zhou et al, 2011a). Over the past few
years, supervised learning methods have obtained
state-of-the-art performance for dependency pars-
ing (Yamada and Matsumoto, 2003; McDonald
et al, 2005; McDonald and Pereira, 2006; Hall
et al, 2006; Zhou et al, 2011b; Zhou et al,
2011c). These methods usually rely heavily on
the manually annotated treebanks for training the
dependency models. However, annotating syntac-
?(with) ??(eyes) ??(cast) ??(Hongkong )   BA                      NN                 VV                         NR
?(with) ??(eyes) ??(cast) ??(Hongkong )      p                       n                      v                            ns
Figure 1: Different grammar formalisms of syn-
tactic structures between CTB (upper) and CDT
(below). CTB is converted into dependency gram-
mar based on the head rules of (Zhang and Clark,
2008).
tic structure, either phrase-based or dependency-
based, is both time consuming and labor intensive.
Making full use of the existing manually annotated
treebanks would yield substantial savings in data-
annotation costs.
In this paper, we present a joint inference
scheme for heterogenous dependency parsing.
This scheme is able to leverage consensus in-
formation between heterogenous treebanks dur-
ing the inference phase instead of using individual
output in a pipelined way, such as stacked learning
methods (Nivre and McDonald, 2008; Martins et
al., 2008). The basic idea is very simple: although
heterogenous treebanks have different grammar
formalisms, they share some consensus informa-
tion in dependency structures for the same sen-
tence. For example in Figure 1, the dependency
structures actually share some partial agreements
for the same sentence, the two words ?eyes? and
?Hongkong? depend on ?cast? in both Chinese
Dependency Treebank (CDT) (Liu et al, 2006)
and Penn Chinese Treebank (CTB) (Xue et al,
2005). Therefore, we would like to train the de-
pendency parsers on individual heterogenous tree-
bank and jointly parse the same sentences with
consensus information exchanged between them.
The remainder of this paper is divided as fol-
104
Treebank1 Treebank2Parser1 Parser2
 consensus information exchangeJoint inference test data
Figure 2: General joint inference scheme of het-
erogeneous dependency parsing.
lows. Section 2 gives a formal description of
the joint inference for heterogeneous dependency
parsing. In section 3, we present the experimental
results. Finally, we conclude with ideas for future
research.
2 Our Approach
The general joint inference scheme of heteroge-
neous dependency parsing is shown in Figure 2.
Here, heterogeneous treebanks refer to two Chi-
nese treebanks: CTB and CDT, therefore we have
only two parsers, but the framework is generic
enough to integrate more parsers. For easy expla-
nation of the joint inference scheme, we regard a
parser without consensus information as a base-
line parser, a parser incorporates consensus infor-
mation called a joint parser. Joint inference pro-
vides a framework that accommodates and coordi-
nates multiple dependency parsing models. Sim-
ilar to Li et al (2009) and Zhu et al (2010),
the joint inference for heterogeneous dependency
parsing consists of four components: (1) Joint In-
ference Model; (2) Parser Coordination; (3) Joint
Inference Features; (4) Parameter Estimation.
2.1 Joint Inference Model
For a given sentence x, a joint dependency parsing
model finds the best dependency parsing tree y?
among the set of possible candidate parses Y(x)
based on a scoring function Fs:
y? = argmax
y?Y(x)
Fs(x, y) (1)
Following (Li et al, 2009), we will use dk to de-
note the kth joint parser, and also use the notation
Hk(x) for a list of parse candidates of sentence
x determined by dk. The sth joint parser can be
written as:
Fs(x, y) = Ps(x, y) +
?
k,k ?=s
?k(y,Hk(x)) (2)
where Ps(x, y) is the score function of the sth
baseline model, and each?k(y,Hk(x)) is a partial
consensus score function with respect to dk and is
defined over y andHk(x):
?k(y,Hk(x)) =
?
l
?k,lfk,l(y,Hk(x)) (3)
where each fk,l(y,Hk(x)) is a feature function
based on a consensus measure between y and
Hk(x), and ?k,l is the corresponding weight pa-
rameter. Feature index l ranges over all consensus-
based features in equation (3).
2.2 Parser Coordination
Note that in equation (2), though the baseline score
function Ps(x, y) can be computed individually,
the case of ?k(y,Hk(x)) is more complicated. It
is not feasible to enumerate all parse candidates
for dependency parsing. In this paper, we use a
bootstrapping method to solve this problem. The
basic idea is that we can use baseline models? n-
best output as seeds, and iteratively refine joint
models? n-best output with joint inference. The
joint inference process is shown in Algorithm 1.
Algorithm 1 Joint inference for multiple parsers
Step1: For each joint parser dk, perform inference with
a baseline model, and memorize all dependency parsing
candidates generated during inference in Hk(x);
Step2: For each candidate in Hk(x), we extract subtrees
and store them inH?k(x). First, we extract bigram-subtreesthat contain two words. If two words have a dependency
relation, we add these two words as a subtree into H?k(x).Similarly, we can extract trigram-subtrees. Note that the
dependency direction is kept. Besides, we also store the
?ROOT? word of each candidate in H?k(x);
Step3: Use joint parsers to re-parse the sentence x with
the baseline features and joint inference features (see sub-
section 2.3). For joint parser dk, consensus-based features
of any dependency parsing candidate are computed based
on current setting of H?s(x) for all s but k. New depen-
dency parsing candidates generated by dk in re-parsing are
cached in H??k(x);
Step4: Update all Hk(x) with H??k(x);
Step5: Iterate from Step2 to Step4 until a preset iteration
limit is reached.
In Algorithm 1, dependency parsing candidates
of different parsers can be mutually improved. For
example, given two parsers d1 and d2 with candi-
dates H1 and H2, improvements on H1 enable d2
to improve H2, and H1 benefits from improved
H2, and so on.
We can see that a joint parser does not en-
large the search space of its baseline model, the
only change is parse scoring. By running a com-
plete inference process, joint model can be applied
to re-parsing all candidates explored by a parser.
105
Thus Step3 can be viewed as full-scale candidates
reranking because the reranking scope is beyond
the limited n-best output currently cached inHk.
2.3 Joint Inference Features
In this section we introduce the consensus-based
feature functions fk,l(y,Hk(x)) introduced in
equation (3). The formulation can be written as:
fk,l(y,Hk(x)) =
?
y??Hk(x)
P (y?|dk)Il(y, y?) (4)
where y is a dependency parse of x by using parser
ds (s ?= k), y? is a dependency parse in Hk(x)
and P (y?|dk) is the posterior probability of depen-
dency parse y? parsed by parser dk given sentence
x. Il(y, y?) is a consensus measure defined on y
and y? using different feature functions.
Dependency parsing model P (y?|dk) can be
predicted by using the global linear models
(GLMs) (e.g., McDonald et al (2005); McDonald
and Pereira (2006)). The consensus-based score
functions Il(y, y?) include the following parts:
(1) head-modifier dependencies. Each head-
modifier dependency (denoted as ?edge?) is a tu-
ple t =< h,m, h ? m >, so Iedge(y, y?) =?
t?y ?(t, y?).
(2) sibling dependencies: Each sibling de-
pendency (denoted as ?sib?) is a tuple t =<
i, h,m, h ? i ? m >, so Isib(y, y?) =?
t?y ?(t, y?).
(3) grandparent dependencies: Each grand-
parent dependency (denoted as ?gp?) is a tuple
t =< h, i,m, h ? i ? m >, so Igp(y, y?) =?
<h,i,m,h?i?m>?y ?(t, y?).
(4) root feature: This feature (denoted as
?root?) indicates whether the multiple depen-
dency parsing trees share the same ?ROOT?, so
Iroot(y, y?) =
?
<ROOT>?y ?(< ROOT >, y?).
?(?, ?) is a indicator function??(t, y?) is 1 if
t ? y? and 0 otherwise, feature index l ?
{edge, sib, gp, root} in equation (4). Note that
< h,m, h ? m > and < m,h,m ? h > are
two different edges.
In our joint model, we extend the baseline fea-
tures of (McDonald et al, 2005; McDonald and
Pereira, 2006; Carreras, 2007) by conjoining with
the consensus-based features, so that we can learn
in which kind of contexts the different parsers
agree/disagree. For the third-order features (e.g.,
grand-siblings and tri-siblings) described in (Koo
et al, 2010), we will discuss it in future work.
2.4 Parameter Estimation
The parameters are tuned to maximize the depen-
dency parsing performance on the development
set, using an algorithm similar to the average per-
ceptron algorithm due to its strong performance
and fast training (Koo et al, 2008). Due to lim-
ited space, we do not present the details. For more
information, please refer to (Koo et al, 2008).
3 Experiments
In this section, we describe the experiments
to evaluate our proposed approach by using
CTB4 (Xue et al, 2005) and CDT (Liu et al,
2006). For the former, we adopt a set of head-
selection rules (Zhang and Clark, 2008) to convert
the phrase structure syntax of treebank into a de-
pendency tree representation. The standard data
split of CTB4 from Wang et al (2007) is used. For
the latter, we randomly select 2,000 sentences for
test set, another 2,000 sentences for development
set, and others for training set.
We use two baseline parsers, one trained on
CTB4, and another trained on CDT in the ex-
periments. We choose the n-best size of 16 and
the best iteration time of four on the development
set since these settings empirically give the best
performance. CTB4 and CDT use two different
POS tag sets and transforming from one tag set
to another is difficult (Niu et al, 2009). To over-
come this problem, we use Stanford POS Tagger1
to train a universal POS tagger on the People?s
Daily corpus,2 a large-scale Chinese corpus (ap-
proximately 300 thousand sentences and 7 mil-
lion words) annotated with word segmentation and
POS tags. Then the POS tagger produces a uni-
versal layer of POS tags for both the CTB4 and
CDT. Note that the word segmentation standards
of these corpora (CTB4, CDT and People?s Daily)
slightly differs; however, we do not consider this
problem and leave it for future research.
The performance of the parsers is evaluated us-
ing the following metrics: UAS, DA, and CM,
which are defined by (Hall et al, 2006). All the
metrics except CM are calculated as mean scores
per word, and punctuation tokens are consistently
excluded.
We conduct experiments incrementally to eval-
uate the joint features used in our first-order and
second-order parsers. The first-order parser
1http://nlp.stanford.edu/software/tagger.shtml
2http://www.icl.pku.edu.cn
106
? Features CTB4 CDTUAS CM UAS CM
dep1
baseline 86.6 42.5 75.4 16.6
+ edge 88.01 (?1.41) 44.28 (?1.78) 77.10 (?1.70) 17.82 (?1.22)
+ root 87.22 (?0.62) 43.03 (?0.53) 75.83 (?0.43) 16.81 (?0.21)
+ both 88.19 (?1.59) 44.54 (?2.04) 77.16 (?1.76) 17.90 (?1.30)
CTB4 + CDT 87.32 43.08 75.91 16.89
dep2
baseline 88.38 48.81 77.52 19.70
+ edge 89.17 (?0.79) 49.73 (?0.92) 78.44 (?0.92) 20.85 (?1.15)
+ sib 88.94 (?0.56) 49.26 (?0.45) 78.02 (?0.50) 20.13 (?0.43)
+ gp 88.90 (?0.52) 49.11 (?0.30) 77.97 (?0.45) 20.06 (?0.36)
+ root 88.61 (?0.23) 48.88 (?0.07) 77.65 (?0.13) 19.88 (?0.18)
+ all 89.62 (?1.24) 50.15 (?1.34) 79.01 (?1.49) 21.11 (?1.41)
CTB4 + CDT 88.91 49.13 78.03 20.12
Table 1: Dependency parsing results on the test set with different joint inference features. Abbreviations:
dep1/dep2 = first-order parser and second-order parser; baseline = dep1 without considering any joint
inference features; +* = the baseline features conjoined with the joint inference features derived from the
heterogeneous treebanks; CTB4 + CDT = we simply concatenate the two corpora and train a dependency
parser, and then test on CTB4 and CDT using this single model. Improvements of joint models over
baseline models are shown in parentheses.
Type Systems ? 40 Full
D
dep2 90.86 88.38
MaltParser 87.1 85.8
Wang et al (2007) 86.6 -
C
MSTMalt? 90.55 88.82
Martins et al (2008)? 90.63 88.84
Surdeanu et al (2010)? 89.40 86.63
H Zhao et al (2009) 88.9 86.1Ours 91.48 89.62
S Yu et al (2008) - 87.26Chen et al (2009) 92.34 89.91
Chen et al (2012) - 91.59
Table 2: Comparison of different approach on
CTB4 test set using UAS metric. MaltParser =
Hall et al (2006); MSTMalt=Nivre and McDon-
ald (2008). Type D = discriminative dependency
parsers without using any external resources; C =
combined parsers (stacked and ensemble parsers);
H = discriminative dependency parsers using ex-
ternal resources derived from heterogeneous tree-
banks, S = discriminative dependency parsers us-
ing external unlabeled data. ? The results on CTB4
were not directly reported in these papers, we im-
plemented the experiments in this paper.
(dep1) only incorporates head-modifier depen-
dency part (McDonald et al, 2005). The second-
order parser (dep2) uses the head-modifier and
sibling dependency parts (McDonald and Pereira,
2006), as well as the grandparent dependency
part (Carreras, 2007; Koo et al, 2008). Table 1
shows the experimental results.
As shown in Table 1, we note that adding more
joint inference features incrementally, the depen-
dency parsing performance is improved consis-
tently, for both treebanks (CTB4 or CDT). As a
final note, all comparisons between joint models
and baseline models in Table 1 are statistically sig-
nificant.3 Furthermore, we also present a base-
line method called ?CTB4 + CDT? for compari-
son. This method first tags both CTB4 and CDT
with the universal POS tagger trained on the Peo-
ple?s Daily corpus, then simply concatenates the
two corpora and trains a dependency parser, and
finally tests on CTB4 and CDT using this single
model. The comparisons in Table 1 tell us that
very limited information is obtained without con-
sensus features by simply taking a union of the
dependencies and their contexts from the two tree-
banks.
To put our results in perspective, we also com-
pare our second-order joint parser with other best-
performing systems. ?? 40? refers to the sentence
with the length up to 40 and ?Full? refers to all
the sentences in test set. The results are shown
in Table 2, our approach significantly outperforms
many systems evaluated on this data set. Chen
et al (2009) and Chen et al (2012) reported a
very high accuracy using subtree-based features
and dependency language model based features
derived from large-scale data. Our systems did not
use such knowledge. Moreover, their technique is
orthogonal to ours, and we suspect that combin-
ing their subtree-based features into our systems
might get an even better performance. We do not
present the comparison of our proposed approach
3We use the sign test at the sentence level. All the com-
parisons are significant at p < 0.05.
107
Type Systems UAS DA
D
Duan et al (2007) 83.88 84.36
Huang and Sagae (2010) 85.20 85.52
Zhang and Nivre (2011) 86.0 -
C Zhang and Clark (2008) - 86.21Bohnet and Kuhn (2012) 87.5 -
H Li et al (2012) 86.44 -Ours 85.88 86.52
S Chen et al (2009) - 86.70
Table 3: Comparison of different approaches on
CTB5 test set. Abbreviations D, C, H and S are as
in Table 2.
Treebanks #Sen # Better # NoChange # Worse
CTB4 355 74 255 26
CDT 2,000 341 1,562 97
Table 4: Statistics on joint inference output on
CTB4 and CDT development set.
with the state-of-the-art methods on CDT because
there is little work conducted on this treebank.
Some researchers conducted experiments on
CTB5 with a different data split: files 1-815 and
files 1,001-1,136 for training, files 886-931 and
1,148-1,151 for development, files 816-885 and
files 1,137-1,147 for testing. The development
and testing sets were also performed using gold-
standard assigned POS tags. We report the experi-
mental results on CTB5 test set in Table 4. Our re-
sults are better than most systems on this data split,
except Zhang and Nivre (2011), Li et al (2012)
and Chen et al (2009).
3.1 Additional Results
To obtain further information about how depen-
dency parsers benefit from the joint inference, we
conduct an initial experiment on CTB4 and CDT.
From Table 4, we find that out of 355 sentences on
the development set of CTB4, 74 sentences ben-
efit from the joint inference, while 26 sentences
suffer from it. For CDT, we also find that out of
2,000 sentences on the development set, 341 sen-
tences benefit from the joint inference, while 97
sentences suffer from it. Although the overall de-
pendency parsing results is improved, joint infer-
ence worsens dependency parsing result for some
sentences. In order to obtain further information
about the error sources, it is necessary to investi-
gate why joint inference gives negative results, we
will leave it for future work.
4 Conclusion and Future Work
We proposed a novel framework of joint infer-
ence, in which multiple dependency parsing mod-
els were coordinated to search for better depen-
dency parses by leveraging the consensus infor-
mation between heterogeneous treebanks. Exper-
imental results showed that joint inference signif-
icantly outperformed the state-of-the-art baseline
models.
There are some ways in which this research
could be continued. First, recall that the joint in-
ference scheme involves an iterative algorithm by
using bootstrapping. Intuitively, there is a lack of
formal guarantee. A natural avenue for further re-
search would be the use of more powerful algo-
rithms that provide certificates of optimality; e.g.,
dual decomposition that aims to develop decod-
ing algorithms with formal guarantees (Rush et
al., 2010). Second, we would like to combine our
heterogeneous treebank annotations into a unified
representation in order to make dependency pars-
ing results comparable across different annotation
guidelines (e.g., Tsarfaty et al (2011)).
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 61070106, No.
61272332 and No. 61202329), the National High
Technology Development 863 Program of China
(No. 2012AA011102), the National Basic Re-
search Program of China (No. 2012CB316300),
We thank the anonymous reviewers and the prior
reviewers of ACL-2012 and AAAI-2013 for their
insightful comments. We also thank Dr. Li Cai for
providing and preprocessing the data set used in
this paper.
References
B. Bohnet and J. Kuhn. 2012. The best of both worlds-
a graph-based completion model for transition-
based parsers. In Proceedings of EACL.
X. Carreras. 2007. Experiments with a Higher-order
Projective Dependency Parser. In Proceedings of
EMNLP-CoNLL, pages 957-961.
W. Chen, D. Kawahara, K. Uchimoto, and Torisawa.
2009. Improving Dependency Parsing with Subtrees
from Auto-Parsed Data. In Proceedings of EMNLP,
pages 570-579.
W. Chen, M. Zhang, and H. Li. 2012. Utilizing depen-
dency language models for graph-based dependency
parsing models. In Proceedings of ACL.
Y. Ding and M. Palmer. 2004. Synchronous depen-
dency insertion grammars: a grammar formalism
for syntax based statistical MT. In Proceedings of
108
the Workshop on Recent Advances in Dependency
Grammar, pages 90-97.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic Mod-
els for Action-based Chinese Dependency Parsing.
In Proceedings of ECML/PKDD.
J. M. Eisner. 2000. Bilexical Grammars and Their
Cubic-Time Parsing Algorithm. Advanced in Prob-
abilistic and Other Parsing Technologies, pages 29-
62.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
Classifier for Deterministic Dependency Parsing. In
Proceedings of ACL, pages 316-323.
L. Huang and K. Sagae. 2010. Dynamic Programming
for Linear-Time Incremental Parsing. In Proceed-
ings of ACL, pages 1077-1086.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
Semi-Supervised Dependency Parsing. In Proceed-
ings of ACL.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D.
Sontag. 2010. Dual Decomposition for Parsing with
Non-Projective Head Automata. In Proceedings of
EMNLP.
M. Li, N. Duan, D. Zhang, C.-H. Li, and M. Zhou.
2009. Collaborative Decoding: Partial Hypothesis
Re-ranking Using Translation Consensus Between
Decoders. In Proceedings of ACL, pages 585-592.
Z. Li, T. Liu, and W. Che. 2012. Exploiting multiple
treebanks for parsing with Quasi-synchronous gram-
mars. In Proceedings of ACL.
T. Liu, J. Ma, and S. Li. 2006. Building a Dependency
Treebank for Improving Chinese Parser. Journal of
Chinese Languages and Computing, 16(4):207-224.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking Dependency Parsers. In Proceed-
ings of EMNLP, pages 157-166.
R. McDonald and F. Pereira. 2006. Online Learning of
Approximate Dependency Parsing Algorithms. In
Proceedings of EACL, pages 81-88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-margin Training of Dependency Parsers.
In Proceedings of ACL, pages 91-98.
Z. Niu, H. Wang, and H. Wu. 2009. Exploiting Het-
erogeneous Treebanks for Parsing. In Proceedings
of ACL, pages 46-54.
J. Nivre and R. McDonld. 2008. Integrating Graph-
based and Transition-based Dependency Parsing. In
Proceedings of ACL, pages 950-958.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On Dual Decomposition and Linear Program-
ming Relation for Natural Language Processing. In
Proceedings of EMNLP.
M. Surdeanu and C. D. Manning. 2010. Ensemble
Models for Dependency Parsing: Cheap and Good?
In Proceedings of NAACL.
R. Tsarfaty, J. Nivre, and E. Andersson. 2011. Eval-
uating Dependency Parsing: Robust and Heuristics-
Free Cross-Annotation Evaluation. In Proceedings
of EMNLP.
J.-N Wang, J-.S. Chang, and K.-Y. Su. 1994. An Au-
tomatic Treebank Conversion Algorithm for Corpus
Sharing. In Proceedings of ACL, pages 248-254.
Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Sim-
ple Training of Dependency Parsers via Structured
Boosting. In Proceedings of IJCAI, pages 1756-
1762.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2005.
The Penn Chinese Treebank: Phrase Structure An-
notation of a Large Corpus. Natural Language En-
gineering, 10(4):1-30.
Yamada and Matsumoto. 2003. Statistical Sependency
Analysis with Support Vector Machines. In Pro-
ceedings of IWPT, pages 195-206.
D. H. Younger. 1967. Recognition and Parsing of
Context-Free Languages in Time n3. Information
and Control, 12(4):361-379, 1967.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese Dependency Parsing with Large Scale Auto-
matically Constructed Case Structures. In Proceed-
ings of COLING, pages 1049-1056.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing Using
Beam-Search. In Proceedings of EMNLP, pages
562-571.
Y. Zhang and J. Nivre. 2011. Transition-based De-
pendency Parsing with Rich Non-local Features. In
Proceedings of ACL, pages 188-193.
H. Zhao, Y. Song, C. Kit, and G. Zhou. 2009. Cross
Language Dependency Parsing Using a Bilingual
Lexicon. In Proceedings of ACL, pages 55-63.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-
Based Translation Model for Question Retrieval in
Community Question Answer Archives. In Pro-
ceedings of ACL, pages 653-662.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploit-
ing Web-Derived Selectional Preference to Improve
Statistical Dependency Parsing. In Proceedings of
ACL, pages 1556-1565.
G. Zhou, L. Cai, K. Liu, and J. Zhao. 2011. Improving
Dependency Parsing with Fined-Grained Features.
In Proceedings of IJCNLP, pages 228-236.
M. Zhu, J. Zhu, and T. Xiao. 2010. Heterogeneous
Parsing via Collaborative Decoding. In Proceedings
of COLING, pages 1344-1352.
109
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 314?324,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Extracting Opinion Targets and Opinion Words from Online Reviews
with Graph Co-ranking
Kang Liu, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Extracting opinion targets and opinion
words from online reviews are two fun-
damental tasks in opinion mining. This
paper proposes a novel approach to col-
lectively extract them with graph co-
ranking. First, compared to previous
methods which solely employed opinion
relations among words, our method con-
structs a heterogeneous graph to model
two types of relations, including seman-
tic relations and opinion relations. Next,
a co-ranking algorithm is proposed to es-
timate the confidence of each candidate,
and the candidates with higher confidence
will be extracted as opinion targets/words.
In this way, different relations make coop-
erative effects on candidates? confidence
estimation. Moreover, word preference
is captured and incorporated into our co-
ranking algorithm. In this way, our co-
ranking is personalized and each candi-
date?s confidence is only determined by its
preferred collocations. It helps to improve
the extraction precision. The experimen-
tal results on three data sets with differ-
ent sizes and languages show that our ap-
proach achieves better performance than
state-of-the-art methods.
1 Introduction
In opinion mining, extracting opinion targets and
opinion words are two fundamental subtasks.
Opinion targets are objects about which users?
opinions are expressed, and opinion words are
words which indicate opinions? polarities. Ex-
tracting them can provide essential information
for obtaining fine-grained analysis on customers?
opinions. Thus, it has attracted a lot of attentions
(Hu and Liu, 2004b; Liu et al, 2012; Moghaddam
and Ester, 2011; Mukherjee and Liu, 2012).
To this end, previous work usually employed a
collective extraction strategy (Qiu et al, 2009; Hu
and Liu, 2004b; Liu et al, 2013b). Their intuition
is: opinion words usually co-occur with opinion
targets in sentences, and there are strong modifi-
cation relationship between them (called opinion
relation in (Liu et al, 2012)). If a word is an
opinion word, other words with which that word
having opinion relations will have highly proba-
bility to be opinion targets, and vice versa. In this
way, extraction is alternatively performed and mu-
tual reinforced between opinion targets and opin-
ion words. Although this strategy has been widely
employed by previous approaches, it still has sev-
eral limitations.
1) Only considering opinion relations is in-
sufficient. Previous methods mainly focused on
employing opinion relations among words for
opinion target/word co-extraction. They have in-
vestigated a series of techniques to enhance opin-
ion relations identification performance, such as
nearest neighbor rules (Liu et al, 2005), syntactic
patterns (Zhang et al, 2010; Popescu and Etzioni,
2005), word alignment models (Liu et al, 2012;
Liu et al, 2013b; Liu et al, 2013a), etc. How-
ever, we are curious that whether merely employ-
ing opinion relations among words is enough for
opinion target/word extraction? We note that there
are additional types of relations among words. For
example, ?LCD? and ?LED? both denote the same
aspect ?screen? in TV set domain, and they are
topical related. We call such relations between
homogeneous words as semantic relations. If we
have known ?LCD? to be an opinion target, ?LED?
is naturally to be an opinion target. Intuitively,
besides opinion relations, semantic relations may
provide additional rich clues for indicating opin-
ion targets/words. Which kind of relations is more
effective for opinion targets/words extraction? Is it
beneficial to consider these two types of relations
together for the extraction? To our best knowl-
314
edge, these problems have seldom been studied
before (see Section 2).
2) Ignoring word preference. When employ-
ing opinion relations to perform mutual reinforc-
ing extraction between opinion targets and opin-
ion words, previous methods depended on opin-
ion associations among words, but seldom consid-
ered word preference. Word preference denotes
a word?s preferred collocations. Intuitively, the
confidence of a candidate being an opinion tar-
get (opinion word) should mostly be determined
by its word preferences rather than all words hav-
ing opinion relations with it. For example
?This camera?s price is expensive for me.?
?It?s price is good.?
?Canon 40D has a good price.?
In these three sentences, ?price? is modified by
?good? more times than ?expensive?. In tradi-
tional extraction strategy, opinion associations are
usually computed based on the co-occurrence fre-
quency. Thus, ?good? has more strong opinion
association with ?price? than ?expensive?, and it
would have more contributions on determining
?price? to be an opinion target or not. It?s un-
reasonable. ?Expensive? actually has more re-
latedness with ?price? than ?good?, and ?expen-
sive? is likely to be a word preference for ?price?.
The confidence of ?price? being an opinion target
should be influenced by ?expensive? in greater ex-
tent than ?good?. In this way, we argue that the
extraction will be more precise.
????4 ????6 ????5 ????1 ????3 ????2 
????2 ????4 ????3 ????5 ????6 ????1 
?????? 
?????? 
?????? 
Figure 1: Heterogeneous Graph: OC means opin-
ion word candidates. TC means opinion target
candidates. Solid curves and dotted lines respec-
tively mean semantic relations and opinion rela-
tions between two candidates.
Thus, to resolve these two problems, we present
a novel approach with graph co-ranking. The col-
lective extraction of opinion targets/words is per-
formed in a co-ranking process. First, we oper-
ate over a heterogeneous graph to model seman-
tic relations and opinion relations into a unified
model. Specifically, our heterogeneous graph is
composed of three subgraphs which model differ-
ent relation types and candidates, as shown in Fig-
ure 1. The first subgraph G
tt
represents semantic
relations among opinion target candidates, and the
second subgraph G
oo
models semantic relations
among opinion word candidates. The third part
is a bipartite subgraph G
to
, which models opinion
relations among different candidate types and con-
nects the above two subgraphs together. Then we
perform a random walk algorithm onG
tt
, G
oo
and
G
to
separately, to estimate all candidates? confi-
dence, and the entries with higher confidence than
a threshold are correspondingly extracted as opin-
ion targets/words. The results could reflect which
type of relation is more useful for the extraction.
Second, a co-ranking algorithm, which incor-
porates three separate random walks on G
tt
, G
oo
and G
to
into a unified process, is proposed to
perform candidate confidence estimation. Differ-
ent relations may cooperatively affect candidate
confidence estimation and generate more global
ranking results. Moreover, we discover each can-
didate?s preferences through topics. Such word
preference will be different for different candi-
dates. We add word preference information into
our algorithm and make our co-ranking algorithm
be personalized. A candidate?s confidence would
mainly absorb the contributions from its word
preferences rather than its all neighbors with opin-
ion relations, which may be beneficial for improv-
ing extraction precision.
We perform experiments on real-world datasets
from different languages and different domains.
Results show that our approach effectively im-
proves extraction performance compared to the
state-of-the-art approaches.
2 Related Work
There are many significant research efforts on
opinion targets/words extraction (sentence level
and corpus level). In sentence level extraction,
previous methods (Wu et al, 2009; Ma and Wan,
2010; Li et al, 2010; Yang and Cardie, 2013)
mainly aimed to identify all opinion target/word
mentions in sentences. They regarded it as a se-
quence labeling task, where several classical mod-
els were used, such as CRFs (Li et al, 2010) and
SVM (Wu et al, 2009).
This paper belongs to corpus level extraction,
and aims to generate a sentiment lexicon and a
target list rather than to identify mentions in sen-
315
tences. Most of previous corpus-level methods
adopted a co-extraction framework, where opin-
ion targets and opinion words reinforce each other
according to their opinion relations. Thus, how
to improve opinion relations identification perfor-
mance was their main focus. (Hu and Liu, 2004a)
exploited nearest neighbor rules to mine opinion
relations among words. (Popescu and Etzioni,
2005) and (Qiu et al, 2011) designed syntactic
patterns to perform this task. (Zhang et al, 2010)
promoted Qiu?s method. They adopted some spe-
cial designed patterns to increase recall. (Liu et
al., 2012; Liu et al, 2013a; Liu et al, 2013b) em-
ployed word alignment model to capture opinion
relations rather than syntactic parsing. The exper-
imental results showed that these alignment-based
methods are more effective than syntax-based ap-
proaches for online informal texts. However, all
aforementioned methods only employed opinion
relations for the extraction, but ignore consider-
ing semantic relations among homogeneous can-
didates. Moreover, they all ignored word prefer-
ence in the extraction process.
In terms of considering semantic relations
among words, our method is related with sev-
eral approaches based on topic model (Zhao et
al., 2010; Moghaddam and Ester, 2011; Moghad-
dam and Ester, 2012a; Moghaddam and Ester,
2012b; Mukherjee and Liu, 2012). The main
goals of these methods weren?t to extract opin-
ion targets/words, but to categorize all given as-
pect terms and sentiment words. Although these
models could be used for our task according to the
associations between candidates and topics, solely
employing semantic relations is still one-sided and
insufficient to obtain expected performance.
Furthermore, there is little work which consid-
ered these two types of relations globally (Su et
al., 2008; Hai et al, 2012; Bross and Ehrig, 2013).
They usually captured different relations using co-
occurrence information. That was too coarse to
obtain expected results (Liu et al, 2012). In ad-
dition, (Hai et al, 2012) extracted opinion tar-
gets/words in a bootstrapping process, which had
an error propagation problem. In contrast, we per-
form extraction with a global graph co-ranking
process, where error propagation can be effec-
tively alleviated. (Su et al, 2008) used heteroge-
neous relations to find implicit sentiment associ-
ations among words. Their aim was only to per-
form aspect terms categorization but not to extract
opinion targets/words. They extracted opinion tar-
gets/words in advanced through simple phrase de-
tection. Thus, the extraction performance is far
from expectation.
3 The Proposed Method
In this section, we propose our method in detail.
We formulate opinion targets/words extraction as
a co-ranking task. All nouns/noun phrases are re-
garded as opinion target candidates, and all ad-
jectives/verbs are regarded as opinion word candi-
dates, which are widely adopted by pervious meth-
ods (Hu and Liu, 2004a; Qiu et al, 2011; Wang
and Wang, 2008; Liu et al, 2012). Then each can-
didate will be assigned a confidence and ranked,
and the candidates with higher confidence than a
threshold will be extracted as the results.
Different from traditional methods, besides
opinion relations among words, we additionally
capture semantic relations among homogeneous
candidates. To this end, a heterogeneous undi-
rected graph G = (V,E) is constructed. V =
V
t
? V
o
denotes the vertex set, which includes
opinion target candidates v
t
? V
t
and opinion
word candidates v
o
? V
o
. E denotes the edge
set, where e
ij
? E means that there is a relation
between two vertices. E
tt
? E represents the se-
mantic relations between two opinion target candi-
dates. E
oo
? E represents the semantic relations
between two opinion word candidates. E
to
? E
represents the opinion relations between opinion
target candidates and opinion word candidates.
Based on different relation types, we used three
matrices M
tt
? R
|V
t
|?|V
t
|
, M
oo
? R
|V
o
|?|V
o
|
and M
to
? R
|V
t
|?|V
o
|
to record the association
weights between any two vertices, respectively.
Section 3.4 will illustrate how to construct them.
3.1 Only Considering Opinion Relations
To estimate the confidence of each candidate, we
use a random walk algorithm on our graph to per-
form co-ranking. Most previous methods (Hu and
Liu, 2004a; Qiu et al, 2011; Wang and Wang,
2008; Liu et al, 2012) only considered opinion
relations among words. Their basic assumption is
as follows.
Assumption 1: If a word is likely to
be an opinion word, the words which
it has opinion relation with will have
higher confidence to be opinion targets,
and vice versa.
316
In this way, candidates? confidences (v
t
or v
o
) are
collectively determined by each other iteratively.
It equals to making random walk on subgraph
G
to
= (V,E
to
) of G. Thus we have
C
t
= (1? ?)?M
to
? C
o
+ ?? I
t
C
o
= (1? ?)?M
T
to
? C
t
+ ?? I
o
(1)
where C
t
and C
o
respectively represent confi-
dences of opinion targets and opinion words.
m
to
i,j
?M
to
means the association weight between
the ith opinion target and the jth opinion word ac-
cording to their opinion relations.
It?s worthy noting that I
t
and I
o
respectively de-
note prior confidences of opinion target candidates
and opinion word candidates. We argue that opin-
ion targets are usually domain-specific, and there
are remarkably distribution difference of them on
different domains (in-domain D
in
vs. out-domain
D
out
). If a candidate is salient inD
in
but common
in D
out
, it?s likely to be an opinion target in D
in
.
Thus, we use a domain relevance measure (DR)
(Hai et al, 2013) to compute I
t
.
DR(t) =
R(t,D
in
)
R(t,D
out
)
(2)
where R(t,D) =
w?
t
s
t
?
?
N
j=1
(w
tj
?
1
W
j
?
?
W
j
k=1
w
kj
) represents candidate relevance with
domain D. w
tj
= (1 + logTF
tj
) ? log
N
DF
t
is a TF-IDF-like weight of candidate t in doc-
ument j. TF
tj
is the frequency of the candi-
date t in the jth document, and DF
t
is docu-
ment frequency. N means the document num-
ber in domain D. R(t,D) includes two mea-
sures to reflect the salient of a candidate in D. 1)
w
tj
?
1
W
j
?
?
W
j
k=1
w
kj
reflects how frequently a
term is mentioned in a particular document. W
j
denotes the word number in document j. 2)
w?
t
s
t
quantifies how significantly a term is mentioned
across all documents in D. w?
t
=
1
N
?
?
N
k=1
w
tk
denotes average weight across all documents for
t. s
t
=
?
1
N
?
?
N
j=1
(w
tj
? w?
j
)
2
denotes the
standard variance of term t. We use the given
reviews as in-domain collection D
in
and Google
n-gram corpus
1
as out-domain collection D
out
.
Finally, each entry in I
t
is a normalized DR(t)
score. In contrast, opinion words are usually
domain-independent. Users may use same words
to express theirs opinions, like ?good?, ?bad?, etc.
But there are still some domain-dependent opinion
1
http://books.google.com/ngrams/datasets
words, like ?delicious? in the restaurant domain,
?powerful? in the car domain. It?s difficult to dis-
criminate them from other words by using statisti-
cal information. So we simply set al entries in I
o
to be 1. ? ? [0, 1] in Eq.1 determines the impact
of the prior confidence on results.
3.2 Only Considering Semantic Relations
To estimate candidates? confidences by only con-
sidering semantic relations among words, we
make two separately random walks on the sub-
graphs of G, G
tt
= (V,E
tt
) and G
oo
= (V,E
oo
).
The basic assumption is as follows:
Assumption 2: If a word is likely to
be an opinion target (opinion word), the
words which it has strong semantic rela-
tion with will have higher confidence to
be opinion targets (opinion words).
In this way, the confidence of the candidate is
determined only by its homogeneous neighbours.
There is no mutual reinforcement between opinion
targets and opinion words. Thus we have
C
t
= (1? ?)?M
tt
? C
t
+ ? ? I
t
C
o
= (1? ?)?M
oo
? C
o
+ ? ? I
o
(3)
where ? has the same role as ? in Eq.1.
3.3 Considering Semantic Relations and
Opinion Relations Together
To jointly model semantic relations and opinion
relations for opinion targets/words extraction, we
couple two random walking algorithms mentioned
above together. Here, Assumption 1 and As-
sumption 2 are both satisfied. Thus, an opinion
target/word candidate?s confidence is collectively
determined by its neighbours according to differ-
ent relation types. Meanwhile, each item may
make influence on it?s neighbours. It?s an iterative
reinforcement process. Thus, we have
C
t
= (1? ?? ?)?M
to
? C
o
+ ??M
tt
? C
t
+ ?? I
t
C
o
= (1? ?? ?)?M
T
to
? C
t
+ ??M
oo
? C
o
+ ?? I
o
(4)
where ? ? [0, 1] determines which type of rela-
tions dominates candidate confidence estimation.
? = 0 means that each candidate?s confidence
is estimated by only considering opinion relations
among words, which equals to Eq.1. Otherwise,
when ? = 1, candidate confidence estimation only
317
considers semantic relations among words, which
equals to Eq.3. ?, I
o
and I
t
have the same meaning
in Eq.1. Our algorithm will run iteratively until it
converges or in a fixed iteration number Iter. In
experiments, we set Iter = 200.
Obtaining Word Preference. The co-ranking
algorithm in Eq.4 is based on a standard random
walking algorithm, which randomly selects a link
according to the association matrix M
to
, M
tt
and
M
oo
, or jumps to a random node with prior confi-
dence value. However, it generates a global rank-
ing over all candidates without taking the node
preference (word preference) into account. As
mentioned in the first section, each opinion tar-
get/word has its preferred collocations, it?s reason-
able that the confidence of an opinion target (opin-
ion word) candidate should be preferentially de-
termined by its preferences, rather than all of its
neighbors with opinion relations.
To obtain the word preference, we resort to top-
ics. We believe that if an opinion word v
i
o
is
topical related with a target word v
j
t
, v
i
o
can be
regarded as a word preference for v
j
t
, and vice
versa. For example, ?price? and ?expensive? are
topically related in phone?s domain, so they are a
word preference for each other.
Specifically, we use a vector P
T
i
=
[P
T
i
1
, ..., P
T
i
k
, ..., P
T
i
|V
o
|
]
1?|V
o
|
to represent word
preference of the ith opinion target candidate.
P
T
i
k
means the preferred probability of the ith
potential opinion target for the kth potential
opinion words. To compute P
T
i
k
, we first use
Kullback-Leibler divergence to measure the
semantic distance between any two candidates on
the bridge of topics. Thus, we have
D(v
i
, v
j
) =
1
2
?
z
(KL
z
(v
i
||v
j
) +KL
z
(v
j
||v
i
))
whereKL
z
(v
i
||v
j
) = p(z|v
i
)log
p(z|v
i
)
p(z|v
j
)
means the
KL-divergence from candidate v
i
to v
j
based on
topic z. p(z|v) = p(v|z)
p(z)
p(v)
, where p(v|z) is the
probability of the candidate v to topic z (see Sec-
tion 3.4). p(z) is the probability that topic z in
reviews. p(v) is the probability that a candidate
occurs in reviews. Then, a logistic function is used
to map D(v
i
, v
j
) into [0, 1].
SA(v
i
, v
j
) =
1
1 + e
D(v
i
,v
j
)
(5)
Then, we calculate P
T
i
k
by normalize SA(v
i
, v
j
)
score, i.e. P
T
i
k
=
SA(v
t
i
,v
o
k
)
?
|V
o
|
p=1
SA(v
t
i
,v
o
p
)
. For demon-
stration, we give some examples in Table 1, where
each entry denotes a SA(v
i
, v
j
) score between two
candidates. We can see that using topics can suc-
cessfully capture the preference information for
each opinion target/word.
expensive good long colorful
price 0.265 0.043 0.003 0.000
LED 0.002 0.035 0.007 0.098
battery 0.000 0.015 0.159 0.001
Table 1: Examples of Calculated Word Preference
And we use a vector P
O
j
=
[P
O
j
1
, ..., P
O
j
q
, ..., P
O
j
|V
t
|
]
1?|V
t
|
to represent
the preference information of the jth opin-
ion word candidate. Similarly, we have
P
O
j
q
=
SA(v
t
q
,v
o
j
)
?
|V
t
|
k=1
SA(v
t
k
,v
o
j
)
.
Incorporating Word Preference into Co-
ranking. To consider such word preference in
our co-ranking algorithm, we incorporate it into
the random walking on G
to
. Intuitively, prefer-
ence vectors will be different for different can-
didates. Thus, the co-ranking algorithm would
be personalized. It allows that the candidate
confidence propagates to other candidates only
in its preference cluster. Specifically, we make
modification on original transition matrix M
to
=
(M
to
1
,M
to
2
, ...,M
to
|V
t
|
) and add each candidate?s
preference in it. Let
?
M
to
= (
?
M
to
1
,
?
M
to
2
, ...,
?
M
to
|V
t
|
)
be the modified transition matrix, which records
the associations between opinion target candi-
dates and opinion word candidates. Here M
to
k
?
R
1?|V
o
|
and
?
M
to
k
? R
1?|V
o
|
denotes the kth col-
umn vector in M
to
and
?
M
to
, respectively. And
let Diag(P
T
k
) denote a diagonal matrix whose
eigenvalue is vector P
T
k
, we have
?
M
to
k
= M
to
k
Diag(P
T
k
)
Similarly, let U
to
k
? R
1?|V
t
|
and
?
U
to
k
? R
1?|V
t
|
denotes the kth row vector in M
T
to
and
?
M
T
to
, re-
spectively. Diag(P
O
k
) denote a diagonal matrix
whose eigenvalue is vector P
O
k
. Then we have
?
U
to
k
= U
to
k
Diag(P
O
k
)
In this way, each candidate?s preference is in-
corporated into original associations based on
opinion relation M
to
through Diag(P
O
k
) and
Diag(P
T
k
). And candidates? confidences will
mainly come from the contributions of its prefer-
ences. Thus, C
t
and C
o
in Eq.4 become:
318
Ct
= (1? ?? ?)?
?
M
to
? C
o
+ ??M
tt
? C
t
+ ?? I
t
C
o
= (1? ?? ?)?
?
M
T
to
? C
t
+ ??M
oo
? C
o
+ ?? I
o
(6)
3.4 Capturing Semantic and Opinion
Relations
In this section, we explain how to capture seman-
tic relations and opinion relations for constructing
transition matrices M
tt
, M
oo
and M
to
.
Capturing Semantic Relations: For captur-
ing semantic relations among homogenous candi-
dates, we employ topics. We believe that if two
candidates share similar topics in the corpus, there
is a strong semantic relation between them. Thus,
we employ a LDA variation (Mukherjee and Liu,
2012), an extension of (Zhao et al, 2010), to dis-
cover topic distribution on words, which sampled
all words into two separated observations: opinion
targets and opinion words. It?s because that we are
only interested in topic distribution of opinion tar-
gets/words, regardless of other useless words, in-
cluding conjunctions, prepositions etc. This model
has been proven to be better than the standard
LDA model and other LDA variations for opinion
mining (Mukherjee and Liu, 2012).
After topic modeling, we obtain the proba-
bility of the candidates (v
t
and v
o
) to topic z,
i.e. p(z|v
t
) and p(z|v
o
), and topic distribution
p(z). Then, a symmetric Kullback-Leibler diver-
gence as same as Eq.5 is used to calculate the se-
mantical associations between any two homoge-
nous candidates. Thus, we obtain SA(v
t
, v
t
) and
SA(v
o
, v
o
), which correspond to the entries in
M
tt
and M
oo
, respectively.
Capturing Opinion Relations: To capture
opinion relations among words and construct the
transition matrix M
to
, we used an alignment-
based method proposed in (Liu et al, 2013b).
This approach models capturing opinion relations
as a monolingual word alignment process. Each
opinion target can find its corresponding mod-
ifiers in sentences through alignment, in which
multiple factors are considered globally, such as
co-occurrence information, word position in sen-
tence, etc. Moreover, this model adopted a par-
tially supervised framework to combine syntac-
tic information with alignment results, which has
been proven to be more precise than the state-of-
the-art approaches for opinion relations identifica-
tion (Liu et al, 2013b).
After performing word alignment, we obtain
a set of word pairs composed of a noun (noun
phrase) and its corresponding modified word.
Then, we simply employ Pointwise Mutual Infor-
mation (PMI) to calculate the opinion associations
among words as the entries in M
to
. OA(v
t
, v
o
) =
log
p(v
t
,v
o
)
p(v
t
)p(v
o
)
, where v
t
and v
o
denote an opinion
target candidate and an opinion word candidate,
respectively. p(v
t
, v
o
) is the co-occurrence prob-
ability of v
t
and v
o
based on the opinion relation
identification results. p(v
t
) and p(v
o
) give the in-
dependent occurrence probability of of v
t
and v
o
,
respectively
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: To evaluate the proposed method, we
used three datasets. The first one is Customer
Review Datasets (CRD), used in (Hu and Liu,
2004a), which contains reviews about five prod-
ucts. The second one is COAE2008 dataset2
2
,
which contains Chinese reviews about four prod-
ucts. The third one is Large, also used in (Wang
et al, 2011; Liu et al, 2012; Liu et al, 2013a),
where two domains are selected (Mp3 and Hotel).
As mentioned in (Liu et al, 2012), Large con-
tains 6,000 sentences for each domain. Opinion
targets/words are manually annotated, where three
annotators were involved. Two annotators were
required to annotate out opinion words/targets in
reviews. When conflicts occur, the third annota-
tor make final judgement. In total, we respectively
obtain 1,112, 1,241 opinion targets and 334, 407
opinion words in Hotel, MP3.
Pre-processing: All sentences are tagged to
obtain words? part-of-speech tags using Stanford
NLP tool
3
. And noun phrases are identified using
the method in (Zhu et al, 2009) before extraction.
Evaluation Metrics: We select precision(P),
recall(R) and f-measure(F) as metrics. And a sig-
nificant test is performed, i.e., a t-test with a de-
fault significant level of 0.05.
4.2 Our Method vs. The State-of-the-art
Methods
To prove the effectiveness of the proposed method,
we select some state-of-the-art methods for com-
parison as follows:
2
http://ir-china.org.cn/coae2008.html
3
http://nlp.stanford.edu/software/tagger.shtml
319
Methods
D1 D2 D3 D4 D5 Avg.
P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.758
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.856
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.846
SAS 0.80 0.79 0.79 0.82 0.76 0.79 0.79 0.74 0.76 0.77 0.78 0.77 0.80 0.76 0.78 0.778
Liu 0.84 0.85 0.84 0.87 0.85 0.86 0.88 0.89 0.88 0.81 0.85 0.83 0.89 0.87 0.88 0.858
Hai 0.77 0.87 0.83 0.79 0.86 0.82 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.81 0.818
CR 0.84 0.86 0.85 0.87 0.85 0.86 0.87 0.90 0.88 0.81 0.87 0.83 0.89 0.88 0.89 0.862
CR WP 0.86 0.86 0.86 0.88 0.86 0.87 0.89 0.90 0.89 0.81 0.87 0.83 0.91 0.89 0.90 0.870
Table 2: Results of Opinion Targets Extraction on Customer Review Dataset
Methods
Camera Car Laptop Phone Mp3 Hotel Avg.
P R F P R F P R F P R F P R F P R F F
Hu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64 0.61 0.68 0.64 0.60 0.65 0.62 0.587
DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72 0.69 0.70 0.69 0.67 0.69 0.68 0.683
Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75 0.67 0.77 0.72 0.67 0.76 0.71 0.712
SAS 0.72 0.72 0.72 0.71 0.64 0.67 0.59 0.72 0.65 0.78 0.69 0.73 0.69 0.75 0.72 0.69 0.74 0.71 0.700
Liu 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78 0.70 0.82 0.76 0.71 0.80 0.75 0.749
Hai 0.68 0.84 0.76 0.69 0.75 0.72 0.58 0.86 0.72 0.75 0.76 0.76 0.65 0.83 0.74 0.62 0.82 0.75 0.742
CR 0.75 0.83 0.79 0.72 0.74 0.73 0.60 0.85 0.70 0.83 0.77 0.80 0.70 0.84 0.76 0.71 0.83 0.77 0.758
CR WP 0.78 0.84 0.81 0.74 0.75 0.74 0.64 0.85 0.73 0.84 0.76 0.80 0.74 0.84 0.79 0.74 0.82 0.78 0.773
Table 3: Results of Opinion Targets Extraction on COAE 2008 and Large
Hu extracted opinion targets/words using asso-
ciation mining rules (Hu and Liu, 2004a).
DP used syntax-based patterns to capture opin-
ion relations in sentences, and then used a boot-
strapping process to extract opinion targets/words
(Qiu et al, 2011),.
Zhang is proposed by (Zhang et al, 2010).
They also used syntactic patterns to capture opin-
ion relations between words. Then a HITS (Klein-
berg, 1999) algorithm is employed to extract opin-
ion targets.
Liu is proposed by (Liu et al, 2013a), an ex-
tension of (Liu et al, 2012). They employed a
word alignment model to capture opinion relations
among words, and then used a random walking al-
gorithm to extract opinion targets.
Hai is proposed by (Hai et al, 2012), which is
similar to our method. They employed both of se-
mantic relations and opinion relations to extract
opinion words/targets in a bootstrapping frame-
work. But they captured relations only using co-
occurrence statistics. Moreover, word preference
was not considered.
SAS is proposed by (Mukherjee and Liu, 2012),
an extended lda-based model of (Zhao et al,
2010). The top K items for each aspect are ex-
tracted as opinion targets/words. It means that
only semantic relations among words are consid-
ered in SAS. And we set aspects number to be 9 as
same as (Mukherjee and Liu, 2012).
CR: is the proposed method in this paper by us-
ing co-ranking, referring to Eq.4. CR doesn?t con-
sider word preference.
CR WP: is the full implementation of our
method, referring to Eq.6.
Hu, DP, Zhang and Liu are the methods which
only consider opinion relations among words.
SAS is the methods which only consider seman-
tic relations among words. Hai, CR and CR WP
consider these two types of relations together. The
parameter settings of state-of-the-art methods are
same as their original paper. In CR and CR WP,
we set ? = 0.4 and ? = 0.1. The experimental
results are shown in Table 2, 3, 4 and 5, where the
last column presents the average F-measure scores
for multiple domains. Since Liu and Zhang aren?t
designed for opinion words extraction, we don?t
present their results in Table 4 and 5. From exper-
imental results, we can see.
1) Our methods (CR and CR WP) outperform
other methods not only on opinion targets extrac-
tion but on opinion words extraction in most do-
mains. It proves the effectiveness of the proposed
method.
2) CR and CR WP have much better perfor-
mance than Liu and Zhang, especially on Recall.
Liu and Zhang also use a ranking framework like
ours, but they only employ opinion relations for
extraction. In contrast, besides opinion relations,
CR and CR WP further take semantic relations
into account. Thus, more opinion targets/words
can be extracted. Furthermore, we observe that
CR and CR WP outperform SAS. SAS only ex-
ploits semantic relations, but ignores opinion re-
lations among words. Its extraction is performed
separately and neglects the reinforcement between
opinion targets and opinion words. Thus, SAS has
worse performance than our methods. It demon-
strates the usefulness of considering multiple rela-
tion types.
320
Methods
D1 D2 D3 D4 D5 Avg.
P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.624
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.666
SAS 0.64 0.68 0.66 0.55 0.70 0.62 0.62 0.65 0.63 0.60 0.61 0.60 0.68 0.63 0.65 0.632
Hai 0.62 0.77 0.69 0.52 0.80 0.64 0.60 0.74 0.67 0.56 0.69 0.62 0.66 0.70 0.68 0.660
CR 0.62 0.75 0.68 0.57 0.79 0.67 0.64 0.75 0.69 0.63 0.69 0.66 0.68 0.69 0.69 0.678
CR WP 0.65 0.75 0.70 0.59 0.80 0.68 0.65 0.74 0.70 0.66 0.68 0.67 0.71 0.70 0.70 0.690
Table 4: Results of Opinion Words Extraction on Customer Review Dataset
Methods
Camera Car Laptop Phone Mp3 Hotel Avg.
P R F P R F P R F P R F P R F P R F F
Hu 0.72 0.74 0.73 0.70 0.71 0.70 0.66 0.70 0.68 0.70 0.70 0.70 0.48 0.67 0.56 0.52 0.69 0.59 0.660
DP 0.80 0.73 0.76 0.79 0.71 0.75 0.75 0.69 0.72 0.78 0.68 0.73 0.60 0.65 0.62 0.61 0.66 0.63 0.702
SAS 0.73 0.70 0.71 0.75 0.68 0.71 0.72 0.68 0.69 0.71 0.66 0.68 0.64 0.62 0.63 0.66 0.61 0.63 0.675
Hai 0.76 0.74 0.75 0.72 0.74 0.73 0.69 0.72 0.70 0.72 0.70 0.71 0.61 0.69 0.64 0.59 0.68 0.64 0.690
CR 0.80 0.75 0.77 0.77 0.74 0.75 0.73 0.71 0.72 0.75 0.71 0.73 0.63 0.69 0.64 0.63 0.68 0.66 0.710
CR WP 0.80 0.75 0.77 0.80 0.74 0.77 0.77 0.71 0.74 0.78 0.72 0.75 0.66 0.68 0.67 0.67 0.69 0.68 0.730
Table 5: Results of Opinion Words Extraction on COAE 2008 and Large
3) CR and CR WP both outperform Hai. We
believe the reasons are as follows. First, CR and
CR WP considers multiple relations in a unified
process by using graph co-ranking. In contrast,
Hai adopts a bootstrapping framework which per-
forms extraction step by step and may have the
problem of error propagation. It demonstrates
that our graph co-ranking is more suitable for this
task than bootstrapping-based strategy. Second,
our method captures semantic relations using topic
modeling and captures opinion relations through
word alignments, which are more precise than Hai
which merely uses co-occurrence information to
indicate such relations among words. In addition,
word preference is not handled in Hai, but pro-
cessed in CR WP. The results show the usefulness
of word preference for opinion targets/words ex-
traction.
4) CR WP outperforms CR, especially on pre-
cision. The only difference between them is that
CR WP considers word preference when perform-
ing graph ranking for candidate confidence esti-
mation, but CR does not. Each candidate confi-
dence estimation in CR WP gives more weights
for this candidate?s preferred words than CR.
Thus, the precision can be improved.
4.3 Semantic Relation vs. Opinion Relation
In this section, we discuss which relation type
is more effective for this task. For comparison,
we design two baselines, called OnlySA and On-
lyOA. OnlyOA only employs opinion relations
among words, which equals to Eq.1. OnlySA only
employs semantic relations among words, which
equals to Eq.3. Moreover, Combine is our method
which considers both of opinion relations and se-
mantic relations together, referring to Eq.4 with
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.65
.70
.75
.80
.85
.90
.95
OnlySA
OnlyOA
Combine
(a) Opinion Target Extraction Results
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.60
.65
.70
.75
.80
OnlySA
OnlyOA
Combine
(b) Opinion Word Extraction Results
Figure 2: Semantic Relations vs. Opinion Rela-
tions
? = 0.5. Figure 2 presents experimental results.
The left graph presents opinion targets extraction
results and the right graph presents opinion words
extraction results. Because of space limitation, we
only shown the results of four domains (MP3, Ho-
tel, Laptop and Phone).
From results, we observe that OnlyOA outper-
forms OnlySA in all domains. It demonstrates
that employing opinion relations are more useful
than semantic relations for co-extracting opinion
targets/words. And it is necessary to utilize the
mutual reinforcement relationship between opin-
ion words and opinion targets. Moreover, Com-
bine outperforms OnlySA and OnlyOA in all do-
mains. It indicates that combining different rela-
tions among words together is effective.
4.4 The Effectiveness of Considering Word
Preference
In this section, we try to prove the necessity of
considering word preference in Eq.6. Besides the
comparison between CR and CR WP performed
321
in the main experiment in Section 4.2, we fur-
ther incorporate word preference in aforemen-
tioned OnlyOA, named as OnlyOA WP, which
only employs opinion relations among words and
equals to Eq.6 with ? = 0. Experimental results
are shown in Figure 3. Because of space limita-
tion, we only show the results of the same domains
in section 4.3,
Form results, we observe that CR WP out-
performs CR, and OnlyOA WP outperforms On-
lyOA in all domains, especially on precision.
These observations demonstrate that considering
word preference is very important for opinion tar-
gets/words extraction. We believe the reason is
that exploiting word preference can provide more
fine information for opinion target/word candi-
dates? confidence estimation. Thus the perfor-
mance can be improved.
 
MP3 Hotel Laptop Phone
P
re
c
i
si
o
n
.60
.65
.70
.75
.80
.85
.90
OnlyOA
OnlyOA_WP
CR
CR_WP
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.70
.75
.80
.85
.90
.95
OnlyOA
OnlyOA_WP
CR
CR_WP
(a) Opinion Target Extraction Results
 
MP3 Hotel Laptop Phone
P
re
c
i
si
o
n
.60
.65
.70
.75
.80
OnlyOA
OnlyOA_WP
CR
CR_WP
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.60
.65
7
.75
.80
OnlyOA
OnlyOA_WP
CR
CR_WP
(b) Opinion Word Extraction Results
Figure 3: Experimental results when considering
word preference
4.5 Parameter Sensitivity
In this subsection, we discuss the variation of ex-
traction performance when changing ? and ? in
Eq.6. Due to space limitation, we only show the
F-measure of CR WP on four domains. Experi-
mental results are shown in Figure 4 and Figure
5. The left graphs in Figure 4 and 5 present the
performance variation of CR WP with varying ?
from 0 to 0.9 and fixing ? = 0.1. The right graphs
in Figure 4 and 5 present the performance varia-
tion of CR WP with varying ? from 0 to 0.6 and
fixing ? = 0.4.
In the left graphs in Figure 4 and 5, we observe
the best performance is obtained when ? = 0.4.
It indicates that opinion relations and semantic re-
lations are both useful for extracting opinion tar-
gets/words. The extraction performance is benefi-
cial from their combination. In the right graphs in
Figure 4 and 5, the best performance is obtained
when ? = 0.1. It indicates prior knowledge is
useful for extraction. When ? increases, perfor-
mance, however, decreases. It demonstrates that
incorporating more prior knowledge into our al-
gorithm would restrain other useful clues on esti-
mating candidate confidence, and hurt the perfor-
mance.
 
0.0 .1 .2 .3 .4 .5 .6 .7 .8 .9
F-M
e
a
sure
.60
.65
.70
.75
.80
.85
MP3
Hotel 
Laptop
Phone
 
0.0 .1 .2 .3 .4 .5 .6
F-M
e
a
sure
.65
.70
.75
.80
.85
MP3
Hotel 
Laptop
Phone
Figure 4: Opinion targets extraction results
 
0.0 .1 .2 .3 .4 .5 .6 .7 .8 .9
F-M
e
a
sure
.55
.60
.65
.70
.75
.80
MP3
Hotel 
Laptop
Phone
 
0.0 .1 .2 .3 .4 .5 .6
F-M
e
a
sure
.50
.55
.60
.65
.70
.75
.80
MP3
Hotel 
Laptop
Phone
Figure 5: Opinion words extraction results
5 Conclusions
This paper presents a novel method with graph co-
ranking to co-extract opinion targets/words. We
model extracting opinion targets/words as a co-
ranking process, where multiple heterogenous re-
lations are modeled in a unified model to make co-
operative effects on the extraction. In addition, we
especially consider word preference in co-ranking
process to perform more precise extraction. Com-
pared to the state-of-the-art methods, experimental
results prove the effectiveness of our method.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2014CB340500), the National Natural Sci-
ence Foundation of China (No. 61272332
and No. 61202329), the National High Tech-
nology Development 863 Program of China
(No. 2012AA011102), and CCF-Tencent Open
Research Fund.
References
Juergen Bross and Heiko Ehrig. 2013. Automatic con-
struction of domain and aspect specific sentiment
322
lexicons for customer review mining. In Proceed-
ings of the 22nd ACM international conference on
Conference on information &#38; knowledge man-
agement, CIKM ?13, pages 1077?1086, New York,
NY, USA. ACM.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One
seed to find them all: mining opinion features via
association. In CIKM, pages 255?264.
Zhen Hai, Kuiyu Chang, Jung-Jae Kim, and Christo-
pher C. Yang. 2013. Identifying features in opinion
mining via intrinsic and extrinsic domain relevance.
IEEE Transactions on Knowledge and Data Engi-
neering, 99(PrePrints):1.
Mingqin Hu and Bing Liu. 2004a. Mining opinion fea-
tures in customer reviews. In Proceedings of Con-
ference on Artificial Intelligence (AAAI).
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Chu-Ren Huang and Dan Jurafsky, editors, COL-
ING, pages 653?661. Tsinghua University Press.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Allan Ellis and Tatsuya Hagino,
editors, WWW, pages 342?351. ACM.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013a.
Opinion target extraction using partially supervised
word alignment model.
Kang Liu, Liheng Xu, and Jun Zhao. 2013b. Syntactic
patterns versus word alignment: Extracting opinion
targets from online reviews.
Tengfei Ma and Xiaojun Wan. 2010. Opinion tar-
get extraction in chinese news comments. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 782?790. Chinese Information Pro-
cessing Society of China.
Samaneh Moghaddam and Martin Ester. 2011. Ilda:
Interdependent lda model for learning latent aspects
and their ratings from online product reviews. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?11, pages 665?674, New
York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012a.
Aspect-based opinion mining from product reviews.
In Proceedings of the 35th International ACM SIGIR
Conference on Research and Development in In-
formation Retrieval, SIGIR ?12, pages 1184?1184,
New York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012b. On
the design of lda models for aspect-based opinion
mining. In CIKM, pages 803?812.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation.
Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian
Wu, Xiaoxun Zhang, Bin Swen, and Zhong Su.
2008. Hidden sentiment association in chinese web
opinion mining. In Jinpeng Huai, Robin Chen,
Hsiao-Wuen Hon, Yunhao Liu, Wei-Ying Ma, An-
drew Tomkins, and Xiaodong Zhang 0001, editors,
WWW, pages 959?968. ACM.
Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Chid Apt, Joydeep Ghosh,
and Padhraic Smyth, editors, KDD, pages 618?626.
ACM.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
323
Papers), pages 1640?1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking
product features in opinion documents. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 1462?1470. Chinese Information
Processing Society of China.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In David Wai-Lok Cheung,
Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and
Jimmy J. Lin, editors, CIKM, pages 1799?1802.
ACM.
324
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 336?346,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Product Feature Mining: Semantic Clues versus Syntactic Constituents
Liheng Xu, Kang Liu, Siwei Lai and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, jzhao}@nlpr.ia.ac.cn
Abstract
Product feature mining is a key subtask
in fine-grained opinion mining. Previ-
ous works often use syntax constituents in
this task. However, syntax-based methods
can only use discrete contextual informa-
tion, which may suffer from data sparsity.
This paper proposes a novel product fea-
ture mining method which leverages lexi-
cal and contextual semantic clues. Lexical
semantic clue verifies whether a candidate
term is related to the target product, and
contextual semantic clue serves as a soft
pattern miner to find candidates, which ex-
ploits semantics of each word in context
so as to alleviate the data sparsity prob-
lem. We build a semantic similarity graph
to encode lexical semantic clue, and em-
ploy a convolutional neural model to cap-
ture contextual semantic clue. Then Label
Propagation is applied to combine both se-
mantic clues. Experimental results show
that our semantics-based method signif-
icantly outperforms conventional syntax-
based approaches, which not only mines
product features more accurately, but also
extracts more infrequent product features.
1 Introduction
In recent years, opinion mining has helped cus-
tomers a lot to make informed purchase decisions.
However, with the rapid growth of e-commerce,
customers are no longer satisfied with the over-
all opinion ratings provided by traditional senti-
ment analysis systems. The detailed functions or
attributes of products, which are called product
features, receive more attention. Nevertheless, a
product may have thousands of features, which
makes it impractical for a customer to investigate
them all. Therefore, mining product features au-
tomatically from online reviews is shown to be a
key step for opinion summarization (Hu and Liu,
2004; Qiu et al, 2009) and fine-grained sentiment
analysis (Jiang et al, 2011; Li et al, 2012).
Previous works often mine product features via
syntactic constituent matching (Popescu and Et-
zioni, 2005; Qiu et al, 2009; Zhang et al, 2010).
The basic idea is that reviewers tend to comment
on product features in similar syntactic structures.
Therefore, it is natural to mine product features by
using syntactic patterns. For example, in Figure 1,
the upper box shows a dependency tree produced
by Stanford Parser (de Marneffe et al, 2006), and
the lower box shows a common syntactic pattern
from (Zhang et al, 2010), where <feature/NN>
is a wildcard to be fit in reviews and NN denotes
the required POS tag of the wildcard. Usually, the
product name mp3 is specified, and when screen
matches the wildcard, it is likely to be a product
feature of mp3.
 
Figure 1: An example of syntax-based prod-
uct feature mining procedure. The word screen
matches the wildcard <feature/NN>. Therefore,
screen is likely to be a product feature of mp3.
Generally, such syntactic patterns extract prod-
uct features well but they still have some limita-
tions. For example, the product-have-feature pat-
tern may fail to find the fm tuner in a very similar
case in Example 1(a), where the product is men-
tioned by using player instead of mp3. Similarly,
it may also fail on Example 1(b), just with have re-
placed by support. In essence, syntactic pattern is
336
a kind of one-hot representation for encoding the
contexts, which can only use partial and discrete
features, such as some key words (e.g., have) or
shallow information (e.g., POS tags). Therefore,
such a representation often suffers from the data
sparsity problem (Turian et al, 2010).
One possible solution for this problem is us-
ing a more general pattern such as NP-VB-feature,
where NP represents a noun or noun phrase and
VB stands for any verb. However, this pattern be-
comes too general that it may find many irrelevant
cases such as the one in Example 1(c), which is not
talking about the product. Consequently, it is very
difficult for a pattern designer to balance between
precision and generalization.
Example 1:
(a) This player has an
::
fm
:::::
tuner.
(b) This mp3 supports
::::
wma
:::
file.
(c) This review has helped
:::::
people a lot.
(d) This mp3 has some
:::::
flaws.
To solve the problems stated above, it is ar-
gued that deeper semantics of contexts shall be ex-
ploited. For example, we can try to automatically
discover that the verb have indicates a part-whole
relation (Zhang et al, 2010) and support indicates
a product-function relation, so that both sth. have
and sth. support suggest that terms following them
are product features, where sth. can be replaced
by any terms that refer to the target product (e.g.,
mp3, player, etc.). This is called contextual se-
mantic clue. Nevertheless, only using contexts is
not sufficient enough. As in Example 1(d), we can
see that the word flaws follows mp3 have, but it
is not a product feature. Thus, a noise term may
be extracted even with high contextual support.
Therefore, we shall also verify whether a candi-
date is really related to the target product. We call
it lexical semantic clue.
This paper proposes a novel bootstrapping ap-
proach for product feature mining, which lever-
ages both semantic clues discussed above. Firstly,
some reliable product feature seeds are automat-
ically extracted. Then, based on the assumption
that terms that are more semantically similar to
the seeds are more likely to be product features,
a graph which measures semantic similarities be-
tween terms is built to capture lexical semantic
clue. At the same time, a semi-supervised con-
volutional neural model (Collobert et al, 2011) is
employed to encode contextual semantic clue. Fi-
nally, the two kinds of semantic clues are com-
bined by a Label Propagation algorithm.
In the proposed method, words are represented
by continuous vectors, which capture latent se-
mantic factors of the words (Turian et al, 2010).
The vectors can be unsupervisedly trained on large
scale corpora, and words with similar semantics
will have similar vectors. This enables our method
to be less sensitive to lexicon change, so that the
data sparsity problem can be alleviated . The con-
tributions of this paper include:
? It uses semantics of words to encode contextual
clues, which exploits deeper level information
than syntactic constituents. As a result, it mines
product features more accurately than syntax-
based methods.
? It exploits semantic similarity between words
to capture lexical clues, which is shown to be
more effective than co-occurrence relation be-
tween words and syntactic patterns. In addition,
experiments show that the semantic similarity
has the advantage of mining infrequent product
features, which is crucial for this task. For ex-
ample, one may say ?This hotel has low water
pressure?, where low water pressure is seldom
mentioned, but fatal to someone?s taste.
? We compare the proposed semantics-based ap-
proach with three state-of-the-art syntax-based
methods. Experiments show that our method
achieves significantly better results.
The rest of this paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed method in details. Section 4 gives the
experimental results. Lastly, we conclude this pa-
per in Section 5.
2 Related Work
In product feature mining task, Hu and Liu (2004)
proposed a pioneer research. However, the asso-
ciation rules they used may potentially introduce
many noise terms. Based on the observation that
product features are often commented on by simi-
lar syntactic structures, it is natural to use patterns
to capture common syntactic constituents around
product features.
Popescu and Etzioni (2005) designed some syn-
tactic patterns to search for product feature candi-
dates and then used Pointwise Mutual Information
(PMI) to remove noise terms. Qiu et al (2009)
proposed eight heuristic syntactic rules to jointly
extract product features and sentiment lexicons,
where a bootstrapping algorithm named Double
337
Propagation was applied to expand a given seed
set. Zhang et al (2010) improved Qiu?s work
by adding more feasible syntactic patterns, and the
HITS algorithm (Kleinberg, 1999) was employed
to rank candidates. Moghaddam and Ester (2010)
extracted product features by automatical opinion
pattern mining. Zhuang et al (2006) used various
syntactic templates from an annotated movie cor-
pus and applied them to supervised movie feature
extraction. Wu et al (2009) proposed a phrase
level dependency parsing for mining aspects and
features of products.
As discussed in the first section, syntactic pat-
terns often suffer from data sparsity. Further-
more, most pattern-based methods rely on term
frequency, which have the limitation of finding
infrequent but important product features. A re-
cent research (Xu et al, 2013) extracted infrequent
product features by a semi-supervised classifier,
which used word-syntactic pattern co-occurrence
statistics as features for the classifier. However,
this kind of feature is still sparse for infrequent
candidates. Our method adopts a semantic word
representation model, which can train dense fea-
tures unsupervisedly on a very large corpus. Thus,
the data sparsity problem can be alleviated.
3 The Proposed Method
We propose a semantics-based bootstrapping
method for product feature mining. Firstly, some
product feature seeds are automatically extracted.
Then, a semantic similarity graph is created to
capture lexical semantic clue, and a Convolutional
Neural Network (CNN) (Collobert et al, 2011) is
trained in each bootstrapping iteration to encode
contextual semantic clue. Finally we use Label
Propagation to find some reliable new seeds for
the training of the next bootstrapping iteration.
3.1 Automatic Seed Generation
The seed set consists of positive labeled examples
(i.e. product features) and negative labeled exam-
ples (i.e. noise terms). Intuitively, popular product
features are frequently mentioned in reviews, so
they can be extracted by simply mining frequently
occurring nouns (Hu and Liu, 2004). However,
this strategy will also find many noise terms (e.g.,
commonly used nouns like thing, one, etc.). To
produce high quality seeds, we employ a Domain
Relevance Measure (DRM) (Jiang and Tan, 2010),
which combines term frequency with a domain-
specific measuring metric called Likelihood Ratio
Test (LRT) (Dunning, 1993). Let ?(t) denotes the
LRT score of a product feature candidate t,
?(t) =
p
k
1
(1? p)
n
1
?k
1
p
k
2
(1? p)
n
2
?k
2
p
k
1
1
(1? p
1
)
n
1
?k
1
p
k
2
2
(1? p
2
)
n
2
?k
2
(1)
where k
1
and k
2
are the frequencies of t in the
review corpus R and a background corpus
1
B, n
1
and n
2
are the total number of terms in R and B,
p = (k
1
+ k
2
)/(n
1
+ n
2
), p
1
= k
1
/n
1
and p
2
=
k
2
/n
2
. Then a modified DRM
2
is proposed,
DRM(t) =
tf(t)
max[tf(?)]
?
1
log df(t)
?
| log ?(t)| ?min| log ?(?)|
max| log ?(?)| ?min| log ?(?)|
(2)
where tf(t) is the frequency of t inR and df(t) is
the frequency of t in B.
All nouns in R are ranked by DRM(t) in de-
scent order, where top N nouns are taken as the
positive example set V
+
s
. On the other hand, Xu
et al (2013) show that a set of general nouns sel-
dom appear to be product features. Therefore, we
employ their General Noun Corpus to create the
negative example set V
?
s
, where N most frequent
terms are selected. Besides, it is guaranteed that
V
+
s
? V
?
s
= ?, i.e., conflicting terms are taken as
negative examples.
3.2 Capturing Lexical Semantic Clue in a
Semantic Similarity Graph
To capture lexical semantic clue, each word is first
converted into word embedding, which is a con-
tinuous vector with each dimension?s value corre-
sponds to a semantic or grammatical interpretation
(Turian et al, 2010). Learning large-scale word
embeddings is very time-consuming (Collobert et
al., 2011), we thus employ a faster method named
Skip-gram model (Mikolov et al, 2013).
3.2.1 Learning Word Embedding for
Semantic Representation
Given a sequence of training words W =
{w
1
, w
2
, ..., w
m
}, the goal of the Skip-gram
model is to learn a continuous vector space EB =
{e
1
, e
2
, ..., e
m
}, where e
i
is the word embedding
of w
i
. The training objective is to maximize the
1
Google-n-Gram (http://books.google.com/ngrams) is
used as the background corpus.
2
The df(t) part of the original DRM is slightly modified
because we want a tf ? idf -like scheme (Liu et al, 2012).
338
average log probability of using word w
t
to pre-
dict a surrounding word w
t+j
,
?
EB = argmax
e
t
?EB
1
m
m
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
; e
t
)
(3)
where c is the size of the training window. Basi-
cally, p(w
t+j
|w
t
; e
t
) is defined as,
p(w
t+j
|w
t
; e
t
) =
exp(e
?
T
t+j
e
t
)
?
m
w=1
exp(e
?
T
w
e
t
)
(4)
where e
?
i
is an additional training vector associ-
ated with e
i
. This basic formulation is impracti-
cal because it is proportional to m. A hierarchical
softmax approximation can be applied to reduce
the computational cost to log
2
(m), see (Morin and
Bengio, 2005) for details.
To alleviate the data sparsity problem, EB is
first trained on a very large corpus
3
(denoted by
C), and then fine-tuned on the target review cor-
pusR. Particularly, for phrasal product features, a
statistic-based method in (Zhu et al, 2009) is used
to detect noun phrases in R. Then, an Unfold-
ing Recursive Autoencoder (Socher et al, 2011) is
trained on C to obtain embedding vectors for noun
phrases. In this way, semantics of infrequent terms
in R can be well captured. Finally, the phrase-
based Skip-gram model in (Mikolov et al, 2013)
is applied onR.
3.2.2 Building the Semantic Similarity Graph
Lexical semantic clue is captured by measuring se-
mantic similarity between terms. The underlying
motivation is that if we have known some product
feature seeds, then terms that are more semanti-
cally similar to these seeds are more likely to be
product features. For example, if screen is known
to be a product feature of mp3, and lcd is of high
semantic similarity with screen, we can infer that
lcd is also a product feature. Analogously, terms
that are semantically similar to negative labeled
seeds are not product features.
Word embedding naturally meets the demand
above: words that are more semantically similar
to each other are located closer in the embedding
space (Collobert et al, 2011). Therefore, we can
use cosine distance between two embedding vec-
tors as the semantic distance measuring metric.
Thus, our method does not rely on term frequency
3
Wikipedia(http://www.wikipedia.org) is used in practice.
to rank candidates. This could potentially improve
the ability of mining infrequent product features.
Formally, we create a semantic similarity graph
G = (V,E,W ), where V = {V
s
? V
c
} is the
vertex set, which contains the labeled seed set V
s
and the unlabeled candidate set V
c
; E is the edge
set which connects every vertex pair (u, v), where
u, v ? V ; W = {w
uv
: cos(EB
u
, EB
v
)} is a
function which associates a weight to each edge.
3.3 Encoding Contextual Semantic Clue
Using Convolutional Neural Network
The CNN is trained on each occurrence of seeds
that is found in review texts. Then for a candidate
term t, the CNN classifies all of its occurrences.
Since seed terms tend to have high frequency in
review texts, only a few seeds will be enough to
provide plenty of occurrences for the training.
3.3.1 The architecture of the Convolutional
Neural Network
The architecture of the Convolutional Neural Net-
work is shown in Figure 2. For a product feature
candidate t in sentence s, every consecutive sub-
sequence q
i
of s that containing t with a window
of length l is fed to the CNN. For example, as
in Figure 2, if t = {screen}, and l = 3, there
are three inputs: q
1
= [the, ipod, screen], q
2
=
[ipod, screen, is], q
3
= [screen, is, impressive].
Partially, t is replaced by a token ?*PF*? to re-
move its lexicon influence
4
.
 
Figure 2: The architecture of the Convolutional
Neural Network.
To get the output score, q
i
is first converted into
a concatenated vector x
i
= [e
1
; e
2
; ...; e
l
], where
e
j
is the word embedding of the j-th word. In
this way, the CNN serves as a soft pattern miner:
4
Otherwise, the CNN will quickly get overfitting on t, be-
cause very few seed lexicons are used for the training.
339
since words that have similar semantics have sim-
ilar low-dimension embedding vectors, the CNN
is less sensitive to lexicon change. The network is
computed by,
y
(1)
i
= tanh(W
(1)
x
i
+ b
(1)
) (5)
y
(2)
= max(y
(1)
i
) (6)
y
(3)
= W
(3)
y
(2)
+ b
(3)
(7)
where y
(i)
is the output score of the i-th layer, and
b
(i)
is the bias of the i-th layer; W
(1)
? R
h?(nl)
and W
(3)
? R
2?h
are parameter matrixes, where
n is the dimension of word embedding, and h is
the size of nodes in the hidden layer.
In conventional neural models, the candidate
term t is placed in the center of the window. How-
ever, from Example 2, when l = 5, we can see that
the best windows should be the bracketed texts
(Because, intuitively, the windows should contain
mp3, which is a strong evidence for finding the
product feature), where t = {screen} is at the
boundary. Therefore, we use Equ. 6 to formulate
a max-convolutional layer, which is aimed to en-
able the CNN to find more evidences in contexts
than conventional neural models.
Example 2:
(a) The [screen of this mp3 is] great.
(b) This [mp3 has a great screen].
3.3.2 Training
Let ? = {EB,W
(?)
, b
(?)
} denotes all the trainable
parameters. The softmax function is used to con-
vert the output score of the CNN to a probability,
p(t|X; ?) =
exp(y
(3)
)
?
|C|
j=1
exp(y
(3)
j
)
(8)
whereX is the input set for term t, andC = {0, 1}
is the label set representing product feature and
non-product feature, respectively.
To train the CNN, we first use V
s
to collect each
occurrence of the seeds in R to form a training
set T
s
. Then, the training criterion is to minimize
cross-entropy over T
s
,
?
? = argmin
?
|T
s
|
?
i=1
? log ?
i
p(t
i
|X
i
; ?) (9)
where ?
i
is the binomial target label distribution
for one entry. Backpropagation algorithm with
mini-batch stochastic gradient descent is used to
solve this optimization problem. In addition, some
useful tricks can be applied during the training.
The weight matrixes W
(?)
are initialized by nor-
malized initialization (Glorot and Bengio, 2010).
W
(1)
is pre-trained by an autoencoder (Hinton,
1989) to capture semantic compositionality. To
speed up the learning, a momentum method is ap-
plied (Sutskever et al, 2013).
3.4 Combining Lexical and Contextual
Semantic Clues by Label Propagation
We propose a Label Propagation algorithm to
combine both semantic clues in a unified process.
Each term t ? V is assumed to have a label dis-
tribution L
t
= (p
+
t
, p
?
t
), where p
+
t
denotes the
probability of the candidate being a product fea-
ture, and on the contrary, p
?
t
= 1? p
+
t
. The clas-
sified results of the CNN which encode contextual
semantic clue serve as the prior knowledge,
I
t
=
?
?
?
(1, 0), if t ? V
+
s
(0, 1), if t ? V
?
s
(r
+
t
, r
?
t
), if t ? V
c
(10)
where (r
+
t
, r
?
t
) is estimated by,
r
+
t
=
count
+
(t)
count
+
(t) + count
?
(t)
(11)
where count
+
(t) is the number of occurrences of
term t that are classified as positive by the CNN,
and count
?
(t) represents the negative count.
Label Propagation is applied to propagate the
prior knowledge distribution I to the product fea-
ture distribution L via semantic similarity graph
G, so that a product feature candidate is deter-
mined by exploring its semantic relations to all of
the seeds and other candidates globally. We pro-
pose an adapted version on the random walking
view of the Adsorption algorithm (Baluja et al,
2008) by updating the following formula until L
converges,
L
i+1
= (1? ?)M
T
L
i
+ ?DI (12)
where M is the semantic transition matrix built
from G; D = Diag[log tf(t)] is a diagonal ma-
trix of log frequencies, which is designed to as-
sign higher ?confidence? scores to more frequent
seeds; and ? is a balancing parameter. Particu-
larly, when ? = 0, we can set the prior knowledge
I without V
c
to L
0
so that only lexical semantic
clue is used; otherwise if ? = 1, only contextual
semantic clue is used.
340
3.5 The Bootstrapping Framework
We summarize the bootstrapping framework of the
proposed method in Algorithm 1. During boot-
strapping, the CNN is enhanced by Label Propaga-
tion which finds more labeled examples for train-
ing, and then the performance of Label Propaga-
tion is also improved because the CNN outputs a
more accurate prior distribution. After running for
several iterations, the algorithm gets enough seeds,
and a final Label Propagation is conducted to pro-
duce the results.
Algorithm 1: Bootstrapping using semantic clues
Input: The review corpusR, a large corpus C
Output: The mined product feature list P
Initialization: Train word embedding set EB first on
C, and then onR
Step 1: Generate product feature seeds V
s
(Section 3.1)
Step 2: Build semantic similarity graph G (Section 3.2)
while iter < MAX ITER do
Step 3: Use V
s
to collect occurrence set T
s
fromR
for training
Step 4: Train a CNNN on T
s
(Section 3.3)
Apply mini-batch SGD on Equ. 9;
Step 5: Run Label Propagation (Section 3.4)
Classify candidates usingN to setup I;
L
0
? I;
repeat
L
i+1
? (1? ?)M
T
L
i
+ ?DI;
until ||L
i+1
? L
i
||
2
< ?;
Step 6: Expand product feature seeds
Move top T terms from V
c
to V
s
;
iter++
end
Step 7: Run Label Propagation for a final result L
f
Rank terms by L
+
f
to get P , where L
+
f
> L
?
f
;
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: We select two real world datasets to
evaluate the proposed method. The first one
is a benchmark dataset in Wang et al (2011),
which contains English review sets on two do-
mains (MP3 and Hotel)
5
. The second dataset is
proposed by Chinese Opinion Analysis Evalua-
tion 2008 (COAE 2008)
6
, where two review sets
(Camera and Car) are selected. Xu et al (2013)
had manually annotated product features on these
four domains, so we directly employ their annota-
tion as the gold standard. The detailed information
can be found in their original paper.
5
http://timan.cs.uiuc.edu/downloads.html
6
http://ir-china.org.cn/coae2008.html
Evaluation Metrics: We evaluate the proposed
method in terms of precision(P), recall(R) and F-
measure(F). The English results are evaluated by
exact string match. And for Chinese results, we
use an overlap matching metric, because deter-
mining the exact boundaries is hard even for hu-
man (Wiebe et al, 2005).
4.2 Experimental Settings
For English corpora, the pre-processing are the
same as that in (Qiu et al, 2009), and for Chinese
corpora, the Stanford Word Segmenter (Chang
et al, 2008) is used to perform word segmenta-
tion. We select three state-of-the-art syntax-based
methods to be compared with our method:
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009), which is
a conventional syntax-based method.
DP-HITS is an enhanced version of DP pro-
posed by Zhang et al (2010), which ranks product
feature candidates by
s(t) = log tf(t) ? importance(t) (13)
where importance(t) is estimated by the HITS al-
gorithm (Kleinberg, 1999).
SGW is the Sentiment Graph Walking algo-
rithm proposed in (Xu et al, 2013), which first
extracts syntactic patterns and then uses random
walking to rank candidates. Afterwards, word-
syntactic pattern co-occurrence statistic is used
as feature for a semi-supervised classifier TSVM
(Joachims, 1999) to further refine the results. This
two-stage method is denoted as SGW-TSVM.
LEX only uses lexical semantic clue. Label
Propagation is applied alone in a self-training
manner. The dimension of word embedding n =
100, the convergence threshold ? = 10
?7
, and the
number of expanded seeds T = 40. The size of
the seed set N is 40. To output product features,
it ranks candidates in descent order by using the
positive score L
+
f
(t).
CONT only uses contextual semantic clue,
which only contains the CNN. The window size
l is 5. The CNN is trained with a mini-batch size
of 50. The hidden layer size h = 250. Finally,
importance(t) in Equ. 13 is replaced with r
+
t
in
Equ. 11 to rank candidates.
LEX&CONT leverages both semantic clues.
341
Method
MP3 Hotel Camera Car Avg.
P R F P R F P R F P R F F
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
SGW 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
LEX 0.64 0.74 0.69 0.65 0.75 0.70 0.69 0.84 0.76 0.68 0.78 0.73 0.72
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72 0.71
SGW-TSVM 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
LEX&CONT 0.74 0.75 0.74 0.75 0.77 0.76 0.80 0.84 0.82 0.79 0.79 0.79 0.78
Table 1: Experimental results of product feature mining. The precision or recall of CONT is the average
performance over five runs with different random initialization of parameters of the CNN. Avg. stands
for the average score.
4.3 The Semantics-based Methods vs.
State-of-the-art Syntax-based Methods
The experimental results are shown in Table 1,
from which we have the following observations:
(i) Our method achieves the best performance
among all of the compared methods. We
also equally split the dataset into five sub-
sets, and perform one-tailed t-test (p ? 0.05),
which shows that the proposed semantics-
based method (LEX&CONT) significantly out-
performs the three syntax-based strong com-
petitors (DP, DP-HITS and SGW-TSVM).
(ii) LEX&CONT which leverages both lexical and
contextual semantic clues outperforms ap-
proaches that only use one kind of semantic
clue (LEX and CONT), showing that the com-
bination of the semantic clues is helpful.
(iii) Our methods which use only one kind of
semantic clue (LEX and CONT) outperform
syntax-based methods (DP, DP-HITS and
SGW). Comparing DP-HITS with LEX and
CONT, the difference between them is that
DP-HITS uses a syntax-pattern-based algo-
rithm to estimate importance(t) in Equ. 13,
while our methods use lexical or contextual se-
mantic clue instead. We believe the reason that
LEX or CONT is better is that syntactic pat-
terns only use discrete and local information.
In contrast, CONT exploits latent semantics of
each word in context, and LEX takes advantage
of word embedding, which is induced from
global word co-occurrence statistic. Further-
more, comparing SGW and LEX, both methods
are base on random surfer model, but LEX gets
better results than SGW. Therefore, the word-
word semantic similarity relation used in LEX
is more reliable than the word-syntactic pattern
relation used in SGW.
(iv) LEX&CONT achieves the highest recall
among all of the evaluated methods. Since
DP and DP-HITS rely on frequency for rank-
ing product features, infrequent candidates are
ranked low in their extracted list. As for SGW-
TSVM, the features they used for the TSVM
suffer from the data sparsity problem for in-
frequent terms. In contrast, LEX&CONT is
frequency-independent to the review corpus.
Further discussions on this observation are
given in the next section.
4.4 The Results on Extracting Infrequent
Product Features
We conservatively regard 30% product features
with the highest frequencies in R as frequent fea-
tures, so the remaining terms in the gold standard
are infrequent features. In product feature mining
task, frequent features are relatively easy to find.
Table 2 shows the recall of all the four approaches
for mining frequent product features. We can see
that the performance are very close among differ-
ent methods. Therefore, the recall mainly depends
on mining the infrequent features.
Method MP3 Hotel Camera Car
DP 0.89 0.92 0.86 0.84
DP-HITS 0.89 0.91 0.86 0.85
SGW-TSVM 0.87 0.92 0.88 0.87
LEX&CONT 0.89 0.91 0.89 0.87
Table 2: The recall of frequent product features.
Figure 3 gives the recall of infrequent prod-
uct features, where LEX&CONT achieves the best
performance. So our method is less influenced
by term frequency. Furthermore, LEX gets better
recall than CONT and all syntax-based methods,
which indicates that lexical semantic clue does aid
to mine more infrequent features as expected.
342
 1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(a) MP3
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(b) Hotel
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(c) Camera
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(d) Car
Figure 4: Accuracy (y-axis) of product feature seed expansion at each bootstrapping iteration (x-axis).
The error bar shows the standard deviation over five runs.
Method
MP3 Hotel Camera Car
P R F P R F P R F P R F
FW-5 0.62 0.63 0.62 0.64 0.64 0.64 0.68 0.73 0.70 0.67 0.66 0.66
FW-9 0.64 0.65 0.64 0.66 0.68 0.67 0.70 0.76 0.73 0.71 0.70 0.70
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72
Table 3: The results of convolutional method vs. the results of non-convolutional methods.
MP3 Hotel Camera Car
Reca
ll
.4
.5
.6
.7
.8
.9 DPDP-HITSSGW-TSVMCONTLEXLEX&CONT
Figure 3: The recall of infrequent features. The
error bar shows the standard deviation over five
different runs.
4.5 Lexical Semantic Clue vs. Contextual
Semantic Clue
This section studies the effects of lexical seman-
tic clue and contextual semantic clue during seed
expansion (Step 6 in Algorithm 1), which is con-
trolled by ?. When ? = 1, we get the CONT; and
if ? is set 0, we get the LEX. To take into account
the correctly expanded terms for both positive and
negative seeds, we use Accuracy as the evaluation
metric,
Accuracy =
#TP + #TN
# Extracted Seeds
where TP denotes the true positive seeds, and TN
denotes the true negative seeds.
Figure 4 shows the performance of seed ex-
pansion during bootstrapping, in which the accu-
racy is computed on 40 seeds (20 being positive
and 20 being negative) expanded in each itera-
tion. We can see that the accuracies of CONT and
LEX&CONT retain at a high level, which shows
that they can find reliable new product feature
seeds. However, the performance of LEX oscil-
lates sharply and it is very low for some points,
which indicates that using lexical semantic clue
alone is infeasible. On another hand, comparing
CONT with LEX in Table 1, we can see that LEX
performs generally better than CONT. Although
LEX is not so accurate as CONT during seed ex-
pansion, its final performance surpasses CONT.
Consequently, we can draw conclusion that CONT
is more suitable for the seed expansion, and LEX
is more robust for the final result production.
To combine advantages of the two kinds of se-
mantic clues, we set ? = 0.7 in Step 5 of Algo-
rithm 1, so that contextual semantic clue plays a
key role to find new seeds accurately. For Step 7,
we set ? = 0.3. Thus, lexical semantic clue is
emphasized for producing the final results.
4.6 The Effect of Convolutional Layer
Two non-convolutional variations of the proposed
method are used to be compared with the convo-
lutional method in CONT. FW-5 uses a traditional
neural network with a fixed window size of 5 to
replace the CNN in CONT, and the candidate term
to be classified is placed in the center of the win-
dow. Similarly, FW-9 uses a fixed window size
of 9. Note that CONT uses a 5-term dynamic
window containing the candidate term, so the ex-
ploited number of words in the context is equiva-
lent to FW-9.
343
Table 3 shows the experimental results. We can
see that the performance of FW-5 is much worse
than CONT. The reason is that FW-5 only exploits
half of the context as that of CONT, which is not
sufficient enough. Meanwhile, although FW-9 ex-
ploits equivalent range of context as that of CONT,
it gets lower precisions. It is because FW-9 has
approximately two times parameters in the param-
eter matrix W
(1)
than that in Equ. 5 of CONT,
which makes it more difficult to be trained with
the same amount of data. Also, lengths of many
sentences in the review corpora are shorter than 9.
Therefore, the convolutional approach in CONT is
the most effective way among these settings.
4.7 Parameter Study
We investigate two key parameters of the proposed
method: the initial number of seeds N , and the
size of the window l used by the CNN.
Figure 5 shows the performance under differ-
ent N , where the F-Measure saturates when N
equates to 40 and beyond. Hence, very few seeds
are needed for starting our algorithm.
 
N
10 20 30 40 50 60
F-Measure
.65
.70
.75
.80
.85
MP3
Hote l
Came ra
Car
Figure 5: F-Measure vs. N for the final results.
Figure 6 shows F-Measure under different win-
dow size l. We can see that the performance is
improved little when l is larger than 5. Therefore,
l = 5 is a proper window size for these datasets.
 
l
2 3 4 5 6 7
F-Measure
.5
.6
.7
.8
.9
MP3
Hote l
Came ra
Car
Figure 6: F-Measure vs. l for the final results.
5 Conclusion and Future Work
This paper proposes a product feature mining
method by leveraging contextual and lexical se-
mantic clues. A semantic similarity graph is built
to capture lexical semantic clue, and a convo-
lutional neural network is used to encode con-
textual semantic clue. Then, a Label Propaga-
tion algorithm is applied to combine both seman-
tic clues. Experimental results prove the effec-
tiveness of the proposed method, which not only
mines product features more accurately than con-
ventional syntax-based method, but also extracts
more infrequent product features.
In future work, we plan to extend the proposed
method to jointly mine product features along with
customers? opinions on them. The learnt seman-
tic representations of words may also be utilized
to predict fine-grained sentiment distributions over
product features.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2012CB316300), the National Natural Sci-
ence Foundation of China (No. 61272332 and
No. 61202329), the National High Technol-
ogy Development 863 Program of China (No.
2012AA011102), and CCF-Tencent Open Re-
search Fund. This work was also supported in
part by Noahs Ark Lab of Huawei Tech. Ltm.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak
Ravichandran, and Mohamed Aly. 2008. Video
suggestion and discovery for youtube: Taking ran-
dom walks through the view graph. In Proceedings
of the 17th International Conference on World Wide
Web, WWW ?08, pages 895?904, New York, NY,
USA. ACM.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, StatMT ?08, pages 224?232.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
344
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL?06 Workshop on
Spoken Language Technology.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the International Con-
ference on Artificial Intelligence and Statistics.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artificial Intelligence, 40(1C3):185 ? 234.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Xing Jiang and Ah-Hwee Tan. 2010. Crctol: A
semantic-based domain ontology learning system.
Journal of the American Society for Information Sci-
ence and Technology, 61(1):150?168.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the 16th International Conference on
Machine Learning, pages 200?209.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 410?419, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: An unsupervised opinion miner from
unstructured product reviews. In Proceedings of
the 19th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ?10, pages
1825?1828, New York, NY, USA. ACM.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on arti-
ficial intelligence and statistics, AISTATS05, pages
246?252.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS?2011, vol-
ume 24, pages 801?809.
Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. Distributed representations of
words and phrases and their compositionality. In
Proceedings of the 30 th International Conference
on Machine Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, EMNLP ?09, pages 1533?
1541, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
345
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining opinion words and opinion tar-
gets in a two-stage framework. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1764?1773, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1462?1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM Conference on Information and Knowledge
Management, CIKM ?09, pages 1799?1802, New
York, NY, USA. ACM.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM International Conference
on Information and Knowledge Management, CIKM
?06, pages 43?50, New York, NY, USA. ACM.
346
