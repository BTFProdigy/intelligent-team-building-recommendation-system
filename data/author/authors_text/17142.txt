Transactions of the Association for Computational Linguistics, 1 (2013) 139?150. Action Editor: Joakim Nivre.
Submitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Efficient Stacked Dependency Parsing by Forest Reranking
Katsuhiko Hayashi and Shuhei Kondo and Yuji Matsumoto
Graduate School of Information Science Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{katsuhiko-h,shuhei-k,matsu}@is.naist.jp
Abstract
This paper proposes a discriminative for-
est reranking algorithm for dependency pars-
ing that can be seen as a form of efficient
stacked parsing. A dynamic programming
shift-reduce parser produces a packed deriva-
tion forest which is then scored by a discrim-
inative reranker, using the 1-best tree output
by the shift-reduce parser as guide features in
addition to third-order graph-based features.
To improve efficiency and accuracy, this pa-
per also proposes a novel shift-reduce parser
that eliminates the spurious ambiguity of arc-
standard transition systems. Testing on the
English Penn Treebank data, forest reranking
gave a state-of-the-art unlabeled dependency
accuracy of 93.12.
1 Introduction
There are two main approaches of data-driven de-
pendency parsing ? one is graph-based and the other
is transition-based.
In the graph-based approach, global optimiza-
tion algorithms find the highest-scoring tree with
locally factored models (McDonald et al, 2005).
While third-order graph-based models achieve state-
of-the-art accuracy, it has O(n4) time complexity
for a sentence of length n. Recently, some prun-
ing techniques have been proposed to improve the
efficiency of third-order models (Rush and Petrov,
2012; Zhang and McDonald, 2012).
The transition-based approach usually employs
the shift-reduce parsing algorithm with linear-time
complexity (Nivre, 2008). It greedily chooses the
transition with the highest score and the result-
ing transition sequence is not always globally op-
timal. The beam search algorithm improves pars-
ing flexibility in deterministic parsing (Zhang and
Clark, 2008; Zhang and Nivre, 2011), and dy-
namic programming makes beam search more effi-
cient (Huang and Sagae, 2010).
There is also an alternative approach that in-
tegrates graph-based and transition-based models
(Sagae and Lavie, 2006; Zhang and Clark, 2008;
Nivre and McDonald, 2008; Martins et al, 2008).
Martins et al (2008) formulated their approach as
stacking of parsers where the output of the first-stage
parser is provided to the second as guide features. In
particular, they used a transition-based parser for the
first stage and a graph-based parser for the second
stage. The main drawback of this approach is that
the efficiency of the transition-based parser is sacri-
ficed because the second-stage employs full parsing.
This paper proposes an efficient stacked pars-
ing method through discriminative reranking with
higher-order graph-based features, which works on
the forests output by the first-stage dynamic pro-
gramming shift-reduce parser and integrates non-
local features efficiently with cube-pruning (Huang
and Chiang, 2007). The advantages of our method
are as follows:
? Unlike the conventional stacking approach, the
first-stage shift-reduce parser prunes the search
space of the second-stage graph-based parser.
? In addition to guide features, the second-stage
graph-based parser can employ the scores of
the first-stage parser which cannot be incorpo-
139
axiom(c0) : 0 : (0, 1,w0) : ?
goal(c2n) : 2n : (0, n, s0) : ?
shift :
state p? ?? ?
? : ( , j, sd|sd?1| . . . |s1|s0) :
? + 1 : (j, j + 1, sd?1|sd?2| . . . |s0|wj) : (p) i < n
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?1|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s0) : pi?
s?0.h.w ?= w0 ? p ? pi
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?1|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s0) : pi?
p ? pi
Figure 1: The arc-standard transition-based dependency parsing system with dynamic programming: means ?take
anything?. a?b denotes that a tree b is attached to a tree a.
rated in standard graph-based models.
? In contrast to joint transition-based/graph-
based approaches (Zhang and Clark, 2008;
Bohnet and Kuhn, 2012) which require a large
beam size and make dynamic programming im-
practical, our two-stage approach can integrate
both models with little loss of efficiency.
In addition, the elimination of spurious ambiguity
from the arc-standard shift-reduce parser improves
the efficiency and accuracy of our approach.
2 Arc-Standard Shift-Reduce Parsing
We use a beam search shift-reduce parser with dy-
namic programming as our baseline system. Fig-
ure 1 shows it as a deductive system (Shieber et al,
1995). A state is defined as the following:
? : (i, j, sd|sd?1| . . . |s1|s0) : pi
where ? is the step size, [i, j] is the span of the top-
most stack element s0, and sd|sd?1| . . . |s1 shows
a stack with d elements at the top, where d is the
window size used for defining features. The ax-
iom is initialized with an input sentence of length n,
x = w0 . . .wn where w0 is a special root symbol $0.
The system takes 2n steps for a complete analysis.
pi is a set of pointers to the predictor states, each of
which is the state just before shifting the root word
s0.h.t ? s0.lc.t ? s0.lc2.t s0.h.t ? s0.rc.t ? s0.rc2.t
s1.h.t ? s1.lc.t ? s1.lc2.t s1.h.t ? s1.rc.t ? s1.rc2.t
s0.h.t ? s0.lc.t ? s0.lc2.t ? q0.t
s0.h.t ? s0.rc.t ? s0.rc2.t ? q0.t
s0.h.t ? s1.h.t ? q0.t ? q1.t
s0.h.w ? s1.h.t ? q0.t ? q1.t
Table 1: Additional feature templates for shift-reduce
parsers: q denotes input queue. h, lc and rc are head, left-
most child and rightmost child of a stack element s. lc2
and rc2 denote the second leftmost and rightmost chil-
dren. t and w are a part-of-speech (POS) tag and a word.
of s0 into stack1. Dynamic programming merges
equivalent states in the same step if they have the
same feature values. We add the feature templates
shown in Table 1 to Huang and Sagae (2010)?s fea-
ture templates.
Dynamic programming not only makes the shift-
reduce parser with beam search more efficient but
also produces a packed forest that encodes an expo-
nential number of dependency trees. A packed de-
pendency forest can be represented by a weighted
(directed) hypergraph. A weighted hypergraph is a
pair H = ?V,E?, where V is the set of vertices and
E is the set of hyperedges. Each hyperedge e ? E
is a tuple e = ?T (e), h(e), fe?, where h(e) ? V is
1Huang and Sagae (2010)?s dynamic programming is based
on a notion of a push computation (Kuhlmann et al, 2011). The
details are out of scope here and readers may refer to the paper.
140
X($)saw0, 7
IXher(saw)1, 7
IXher(saw)1, 4
IX(saw)1, 3
X(I)1, 2
I
X(saw)2, 3
saw
X(her)3, 4
her
IXher,with(saw)1, 7
Xwith(her)3, 7
Xman(with)4, 7
X(with)4, 5
with
aX(man)5, 7
X(a)5, 6
a
X(man)6, 7
man
Figure 2: An example of packed dependency (derivation) forest: each vertex has information about the topmost stack
element of the corresponding state to it.
its head vertex, T (e) ? V + is an ordered list of tail
vertices, and fe is a weight for e.
Figure 2 shows an example of a packed forest.
Each binary hyperedge corresponds to a reduce ac-
tion, and each leaf vertex corresponds to a shift ac-
tion. Each vertex also corresponds to a state, and
parse histories on the states can be encoded into the
vertices. In the example, information about the top-
most stack element is attached to the corresponding
vertex marked with a non-terminal symbol X.
Weights are omitted in the example. In practice,
we attach each reduction weight to the correspond-
ing hyperedge, and add the shift weight to the reduc-
tion weight when a shifted word is reduced.
3 Arc-Standard Shift-Reduce Parsing
without Spurious Ambiguity
One solution to remove spurious ambiguity in the
arc-standard transition system is to give priority to
the construction of left arcs over that of right arcs
(or vice versa) like Eisner (1997). For example, an
Earley dependency parser (Hayashi et al, 2012) at-
taches all left dependents to a word before right de-
pendents. The parser uses a scan action to stop the
construction of left arcs.
We apply this idea to the arc-standard transition
system and show the resulting transition system in
Figure 3. We introduce the ? symbol to indicate that
the root node of the topmost element on the stack
has not been scanned yet. The shift and reduce? ac-
tions can be used only when the root of the topmost
element on the stack has already been scanned, and
all left arcs are always attached to the head before
the head is scanned.
The arc-standard shift-reduce parser without spu-
rious ambiguity takes 3n steps to finish parsing, and
the additional n scan actions add surplus vertices
and (unary) hyperedges to a packed forest. How-
ever, it is easy to remove them from the packed for-
est because the consequent state of a scan action has
a unique antecedent state and all the hyperedges go-
ing out from a vertex corresponding to the conse-
quent state can be attached to the vertex correspond-
ing to the antecedent state. The scan weight of the
removed unary hyperedge is added to each weight of
the hyperedges attached to the antecedent.
4 Experiments (Spurious Ambiguity vs.
Non-Spurious Ambiguity)
We conducted experiments on the English Penn
Treebank (PTB) data to compare spurious and non-
spurious shift-reduce parsers. We split the WSJ
part of PTB into sections 02-21 for training, sec-
tion 22 for development, and section 23 for test. We
used the head rules (Yamada and Matsumoto, 2003)
to convert phrase structure to dependency structure.
141
axiom(c0) : 0 : (0, 1,w0) : ?
goal(c3n) : 3n : (0, n, s0) : ?
shift :
state p? ?? ?
? : ( , j, sd|sd?1| . . . |s1|s0) :
? + 1 : (j, j + 1, sd?1|sd?2| . . . |s0|w?j ) : (p)
j < n
scan : ? : (i, j, sd|sd?1| . . . |s1|s
?
0) : pi
? + 1 : (i, j, sd|sd?1| . . . |s1|s0) : pi
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?0|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s?0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s?0) : pi?
s?0.h.w ?= w0 ? p ? pi
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?1|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s0) : pi?
p ? pi
Figure 3: The dynamic programming arc-standard transition-based deductive system without spurious ambiguity: the
symbol represents that the root node of the topmost element on the stack has not been scanned yet.
8 16 32 64 128
spurious UAS (w/o punc.) 92.5 (93.5) 92.7 (93.6) 92.6 (93.6) 92.6 (93.6) 92.6 (93.6)
dev. sec. (per sent.) 0.01 0.017 0.03 0.06 0.13
non-sp. UAS (w/o punc.) 92.5 (93.6) 92.6 (93.6) 92.6 (93.6) 92.6 (93.6) 92.6 (93.6)sec. (per sent.) 0.01 0.018 0.03 0.07 0.13
spurious UAS (w/o punc.) 92.7 (93.3) 92.7 (93.3) 92.7 (93.3) 92.8 (93.3) 92.8 (93.3)
test sec. (per sent.) 0.01 0.017 0.03 0.06 0.13
non-sp. UAS (w/o punc.) 92.8 (93.4) 92.9 (93.5) 92.9 (93.5) 92.9 (93.5) 92.9 (93.5)sec. (per sent.) 0.01 0.018 0.03 0.06 0.13
Table 2: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing
development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.)
using several beam sizes.
We used an early update version of the averaged per-
ceptron algorithm (Collins and Roark, 2004; Huang
et al, 2012) to train two shift-reduce dependency
parsers with beam size of 12.
Table 2 shows experimental results of parsing the
development and test datasets with each of the spu-
rious and non-spurious shift-reduce parsers using
several beam sizes. Parsing accuracies were eval-
uated by unlabeled accuracy scores (UAS) with and
without punctuations. The parsing times were mea-
sured on an Intel Core i7 2.8GHz. The average cpu
time (per sentence) includes that of dumping packed
forests. This result indicates that the non-spurious
parser achieves better accuracies than the spurious
beam size 8 32 128
% of distinct trees (10) 93.5 94.8 95.0
% of distinct trees (100) 81.8 84.9 87.2
% of distinct trees (1000) 70.6 73.1 77.6
% of distinct trees (10000) 62.1 64.3 65.6
Table 3: The percentages of distinct dependency trees in
10, 100, 1000 and 10000 best trees extracted from spuri-
ous forests with several beam sizes.
parser without loss of efficiency.
Figure 4 shows oracle unlabeled accuracies of
spurious k-best lists, non-spurious k-best lists, spu-
rious forests, and non-spurious forests. We extract
an oracle tree from each packed forest using the for-
142
 0.96
 0.965
 0.97
 0.975
 0.98
 0.985
 0.99
 0.995
 0  500  1000  1500  2000  2500  3000
o
ra
cl
e 
un
la
be
le
d 
ac
cu
ra
cy
ave. # of hyperedges
beam 16
beam 64
beam 64
"kbest"
"forest"
"non-sp-kbest"
"non-sp-forest"
Figure 4: Each plot shows oracle unlabeled accuracies of
spurious k-best lists, spurious forests, and non-spurious
forests. The oracle accuracies are evaluated using UAS
with punctuations.
est oracle algorithm (Huang, 2008). Both forests
produce much better results than the k-best lists, and
non-spurious forests have almost the same oracle ac-
curacies as spurious forests.
However, as shown in Table 3, spurious forests
encode a number of non-unique dependency trees
while all dependency trees in non-spurious forests
are distinct from each other.
5 Forest Reranking
5.1 Discriminative Reranking Model
We define a reranking model based on the graph-
based features as the following:
y? = argmax
y?H
? ? fg(x, y) (1)
where ? is a weight vector, fg is a feature vector (g
indicates ?graph-based?), x is the input sentence, y
is a dependency tree and H is a dependency for-
est. This model assumes a hyperedge factorization
which induces a decomposition of the feature vector
as the following:
? ? fg(x, y) =
?
e?y
? ? fg,e(e). (2)
The search problem can be solved by simply using
the (generalized) Viterbi algorithm (Klein and Man-
ning, 2001). When using non-local features, the hy-
peredge factorization is redefined to the following:
? ? fg(x, y) =
?
e?y
? ? fg,e(e) + ? ? fg,e,N (e) (3)
where fg,e,N is a non-local feature vector. Though
the cube-pruning algorithm (Huang and Chiang,
2007) is an approximate decoding technique based
on a k-best Viterbi algorithm, it can calculate the
non-local scores efficiently.
The baseline score can be taken into the reranker
as a linear interpolation:
y? = argmax
y?H
? ? sctr(x, y) + ? ? fg(x, y) (4)
where sctr is the score from the baseline parser (tr in-
dicates ?transition-based?), and ? is a scaling factor.
5.2 Features for Discriminative Model
5.2.1 Local Features
While the inference algorithm is a simple Viterbi
algorithm, the discriminative model can use all tri-
sibling features and some grand-sibling features2
(Koo and Collins, 2010) as a local scoring factor in
addition to the first- and sibling second-order graph-
based features. This is because the first stage shift-
reduce parser uses features described in Section 2
and this information can be encoded into vertices of
a hypergraph.
The reranking model also uses guide features ex-
tracted from the 1-best tree predicted by the first
stage shift-reduce parser. We define the guide fea-
tures as first-order relations like those used in Nivre
and McDonald (2008) though our parser handles
only unlabeled and projective dependency struc-
tures. We summarize the features for discriminative
reranking model as the following:
? First- and second-order features: these features
are the same as those used in MST parser3.
? Grand-child features: we define tri-gram POS
features with POS tags of grand parent, parent,
and rightmost or leftmost child.
? Tri-sibling features: we define tri-gram features
with three POS-tags of child, sibling, and tri-
sibling. We also define tri-gram features with
one word and two POS tags of the above.
2The grand-child and grand-sibling features can be used
only when interacting with the leftmost or rightmost child and
sibling. In case of local reranking, we did not use grand-sibling
features because in our experiments, they were not effective.
3http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
143
? Guide feaures: we define a feature indicating
whether an arc from a child to its parent is
present in the 1-best tree predicted by the first-
stage shift-reduce parser, conjoined with the
POS tags of the parent and child.
? PP-Attachment features: when a parent word is
a preposition, we define tri-gram features with
the parent word and POS tags of grand parent
and the rightmost child.
5.2.2 Non-local Features
To define richer features as a non-local factor, we
extend a local reranking algorithm by augmenting
each k-best item with all child vertices of its head
vertex4. Information about all children enables the
reranker to calculate the following features when re-
ducing the head vertex:
? Grand-child features: we define tri-gram fea-
tures with one word and two POS tags of grand
parent, parent, and child.
? Grand-sibling features: we define 4-gram POS
features with POS tags of grand parent, parent,
child and sibling. We also define coordination
features with POS tags of grand parent, parent
and child when the sibling word is a coordinate
conjunction.
? Valency features: we define a feature indicat-
ing the number of children of a head, conjoined
with each of its word and POS tag.
When using non-local features, we removed the lo-
cal grand-child features from the model.
5.3 Oracle for Discriminative Training
A discriminative reranking model is trained on
packed forests by using their oracle trees as the cor-
rect parse. More accurate oracles are essential to
train a discriminative reranking model well.
While large size forests have much more accurate
oracles than small size forests, large forests have too
many hyperedges to train a discriminative model on
them, as shown in Figure 4. The usual forest rerank-
ing algorithms (Huang, 2008; Hayashi et al, 2011)
4If each item is augmented with richer information, even
features based on the entire subtree can be defined.
remove low quality hyperedges from large forests by
using inside-outside forest pruning.
However, producing large forests and pruning
them is computationally very expensive. Instead, we
propose a simpler method to produce small forests
which have more accurate oracles by forcing the
beam search shift-reduce parser to keep the correct
state in the beam buffer. As a result, the correct tree
will always be encoded in a packed forest.
6 Experiments (Discriminative Reranking)
6.1 Experimental Setting
Following (Huang, 2008), the training set (WSJ02-
21) is split into 20 folds, and each fold is parsed by
each of the spurious and non-spurious shift-reduce
parsers using beam size 12 with the model trained
on sentences from the remaining 19 folds, dumping
the outputs as packed forests.
The reranker is modeled by either equation (1) or
(4). By our preliminary experiments using develop-
ment data (WSJ22), we modeled the reranker with
equation (1) when training, and with equation (4)
when testing5 (i.e., the scores of the first-stage parser
are not considered during training of the reranking
model). This prevents the discriminative reranking
features from under-training (Sutton et al, 2006;
Hollingshead and Roark, 2008).
A discriminative reranking model is trained on
the packed forests by using the averaged percep-
tron algorithm with 5 iterations. When training non-
local reranking models, we set k-best size of cube-
pruning to 5.
For dumping packed forests for test data, spurious
and non-spurious shift-reduce parsers are trained by
the averaged perceptron algorithm. In all experi-
ments on English data, we fixed beam size to 12 for
training both parsers.
6.2 Test with Gold POS tags
We show the comparison of dumped spurious and
non-spurious packed forests for training data in Ta-
ble 4. Both oracle accuracies are 100.0 due to the
5The scaling factor ? was tuned by minimum error rate
training (MERT) algorithm (Och, 2003) using development
data. The MERT algorithm is suited to tune low-dimensional
parameters. The ? was set to about 1.2 in case of local rerank-
ing, and to about 1.5 in case of non-local reranking.
144
system w/ rerank. sec. (per sent.) UAS (w/o punc.)
sr (12) ? 0.011 92.8 (93.3)
(8) w/ local 0.009 + 0.0056 93.03 (93.69)
(12) w/ local 0.011 + 0.0079 93.03 (93.68)
(32) w/ local 0.03 + 0.019 93.07 (93.67)
(64) w/ local 0.06 + 0.039 93.0 (93.61)
(12, k=3) w/ non-local 0.011 + 0.0085 93.17 (93.78)
(64, k=3) w/ non-local 0.06 + 0.046 93.19 (93.78)
non-sp sr (12) ? 0.012 92.9 (93.5)
(8) w/ local 0.01 + 0.005 93.05 (93.73)
(12) w/ local 0.012 + 0.0074 93.21 (93.87)
(32) w/ local 0.031 + 0.0184 93.22 (93.84)
(64) w/ local 0.061 + 0.0375 93.23 (93.83)
(12, k=3) w/ non-local 0.012 + 0.0083 93.28 (93.9)
(64, k=3) w/ non-local 0.061 + 0.045 93.39 (93.96)
Table 7: Unlabeled accuracy scores and cpu times per sentence (parsing+reranking) when parsing and reranking test
data (WSJ23) with gold POS tags: shift-reduce parser is denoted as sr (beam size, k: k-best size of cube pruning).
sp. non-sp.
ave. # of hyperedges 141.9 133.3
ave. # of vertices 199.1 187.6
ave. % of distinct trees 82.5 100.0
1-best UAS w/ punc. 92.5 92.6
oracle UAS w/ punc. 100.0 100.0
Table 4: Comparison of spurious (sp.) and non-spurious
(non-sp.) forests: each forest is produced by baseline
and proposed shift-reduce parsers using beam size 12 for
39832 training sentences with gold POS tags.
method described in Section 5.3. The 1-best accu-
racy of the non-spurious forests is higher than that
of the spurious forests. As we expected, the results
show that there are many non-unique dependency
trees in the spurious forests. The spurious forests
also get larger than the non-spurious forests.
Table 5 shows how long the training on spurious
and non-spurious forests took on an Opteron 8356
2.3GHz. It is clear from the results that training on
non-spurious forests is more efficient than that on
spurious forests.
Table 6 shows the statistics of spurious and non-
spurious packed forests dumped by shift-reduce
parsers using beam size 12 for test data. The trends
are similar to those for training data shown in Ta-
ble 4. We show the results of the forest rerank-
ing algorithms for test data in Table 7. Each spu-
rious and non-spurious shift-reduce parser produces
reranker pre-comp. training
spurious 16.4 min. 34.9 min.
non-spurious 15.5 min. 32.9 min.
spurious non-local 17.3 min. 64.3 min.
non-spurious non-local 16.2 min. 60.3 min.
Table 5: Training times on both spurious and non-
spurious packed forests (beam 12): pre-comp. denotes
cpu time for feature extraction and attaching features to
all hyperedges. The non-local models were trained set-
ting k-best size of cube-pruning to 5, and non-local fea-
tures were calculated on-the-fly while training.
packed forests using four beam sizes 8, 12, 32, and
64. The reranking on non-spurious forests achieves
better accuracies and is slightly faster than that on
spurious forests consistently.
6.3 Test with Automatic POS tags
To compare the proposed reranking system with
other systems, we evaluate its parsing accuracy on
test data with automatic POS tags. We used the Stan-
ford POS tagger6 with a model trained on sections
02-21 to tag development and test data, and used
10-way jackknifing to tag training data. The tagging
accuracies on training, development, and test data
were 97.1, 97.2, and 97.5.
Table 8 lists the accuracy and parsing speed of
6http://nlp.stanford.edu/software/
tagger.shtml
145
sp. non-sp.
ave. # of hyperedges 127.0 119.1
ave. # of vertices 178.6 168.5
ave. % of distinct trees 82.4 100.0
1-best UAS w/ punc. 92.8 92.9
oracle UAS w/ punc. 97.0 97.0
Table 6: Comparison of spurious (sp.) and non-spurious
(non-sp.) forests: each forest is produced by baseline and
proposed shift-reduce parsers using beam size 12 for test
data (WSJ23) with gold POS tags.
system tok./sec. UAS w/o punc.
sr (12) 2130 92.5
w/ local (12) 1290 92.8
non-sp sr (12) 1950 92.6
w/ local (12) 1300 92.98
w/ non-local (12, k=1) 1280 93.1
w/ non-local (12, k=3) 1180 93.12
w/ non-local (12, k=12) 1060 93.12
Huang10 sr (8) 782 92.1
Rush12 sr (16) 4780 92.5
Rush12 sr (64) 1280 92.7
Koo10 ? 93.04
Rush12 third 20 93.3
Rush12 vine 4400 93.1
H-Zhang12 third 50 92.81
H-Zhang12 (label) 220 93.06
Y-Zhang11 (64, label) 680 92.9
Bohnet12 (80, label) 120 93.39
Table 8: Comparison with other systems: the results were
evaluated on testing data (WSJ23) with automatic POS
tags: label means labeled dependency parsing and the cpu
times of our systems were taken on Intel Core i7 2.8GHz.
our proposed systems together with results from re-
lated work. The parsing times are reported in to-
kens/second for comparison. Note that, however,
the difference of the parsing time does not represent
the efficiency of the algorithm directly because each
system was implemented in different programming
language and the times were measured on different
environments.
The accuracy of local reranking on non-spurious
forests is the best among unlabeled shift-reduce
parsers, but slightly behind the third-order graph-
based systems (Koo and Collins, 2010; Zhang and
McDonald, 2012; Rush and Petrov, 2012). It is
likely that the difference comes from the fact that
our local reranking model can define only some of
the grand-child related features.
w/ guide. w/o guide.PPPPPPPfeature
UAS 92.98 92.86
Linear (first) 89,330 89,215
CorePos (first) 1,047,948 1,053,796
TwoObs (first) 1,303,911 1,325,990
Sibling (second) 290,291 292,849
Trip (second) 19,333 19,267
Grand-child 16,975 16,951
Guide 4,934 ?
Tri-sibling 277,770 279,720
PP-Attachment 32,695 32,993
total 3,083,187 3,110,781
Table 9: Accuracy and the number of non-zero weighted
features of the local reranking models with and without
guide features: the first- and second-order features are
named for MSTParser.
To define all grand-child features and other non-
local features, we also experimented with the non-
local reranking algorithm on non-spurious packed
forests. It achieved almost the same accuracy as the
previous third-order graph-based algorithms. More-
over, the computational overhead is very small when
setting k-best size of cube-pruning small.
6.4 Analysis
One advantage of our reranking approach is that
guide features can be defined as in stacked parsing.
To analyze the effect of the guide features on parsing
accuracy, we remove the guide features from base-
line reranking models with and without non-local
features used in Section 6.3. The results are shown
in Table 9 and 10. The parsing accuracies of the
baseline reranking models are better than those of
the models without guide features though the num-
ber of guide features is not large. Additionally, each
model with guide features is smaller than that with-
out guide features. This indicates that stacking has a
good effect on training the models.
To further investigate the effects of guide features,
we tried to define unlabeled versions of the second-
order guide features used in (Martins et al, 2008;
McClosky et al, 2012). However, these features did
not produce good results, and investigation to find
the cause is an important future work.
We also examined parsing errors in more de-
tail. Table 11 shows root and sentence complete
rates of three systems, the non-spurious shift-reduce
146
w/ guide. w/o guide.PPPPPPPfeature
UAS 93.12 93.04
Linear (first) 88,634 88,934
CorePos (first) 1,035,897 1,045,242
TwoObs (first) 1,274,834 1,301,103
Sibling (second) 284,341 288,796
Trip (second) 19,201 19,219
Guide 4,916 ?
Tri-sibling 272,418 276,025
PP-Attachment 32,085 32,577
Grand-child 718,064 730,663
Grand-sibling 72,865 73,103
Valency 49,262 49,677
total 3,852,517 3,905,339
Table 10: Accuracy and the number of non-zero weighted
features of the non-local reranking models with and with-
out guide features: the first- and second-order features are
named for MSTParser.
system UAS root comp.
non-sp sr 92.6 95.8 45.6
local 92.98 96.1 48.1
non-local 93.12 96.3 48.2
Table 11: Unlabeled accuracy, root correct rate, and sen-
tence complete rate: these scores are measured on test
data (WSJ23) without punctuations.
parser, local reranking, and non-local reranking.
The two reranking systems outperform the shift-
reduce parser significantly, and the non-local rerank-
ing system is the best among them.
Part of the difference between the shift-reduce
parser and reranking systems comes from the correc-
tion of coordination errors. Table 12 shows the head
correct rate, recall, precision, F-measure and com-
plete rate of coordination structures, by which we
mean the head and siblings of a token whose POS
tag is CC. The head correct rate denotes how cor-
rect a head of the CC token is. The recall, precision,
F-measure are measured by counting arcs between
the head and siblings. When the head of the CC to-
ken is incorrect, all arcs of the coordination structure
are counted as incorrect. Therefore, the recall, preci-
sion, F-measure are greatly affected by the head cor-
rect rate, and though the complete rate of non-local
reranking is higher than that of local reranking, the
results of the first three measures are lower.
non-sp sr local non-local
head correct 87.73 88.97 88.83
recall 82.38 84.35 84.11
precision 83.07 84.57 83.98
F-measure 82.72 84.46 84.05
comp. 62.92 64.52 65.18
Table 12: Head correct rate, recall, precision, F-measure,
and complete rate of coordination strutures: these are
measured on test data (WSJ23).
system recall precision F-measure
non-sp sr 91.58 92.5 92.04
local 91.96 92.95 92.45
non-local 92.44 93.07 92.75
Table 13: Recall, precision, and F-measure of grand-child
structures whose grand parent is an artificial root symbol:
these are measured on test data (WSJ23).
We assume that the improvements of non-local
reranking over the others can be mainly attributed
to the better prediction of the structures around the
sentence root because most of the non-local features
are useful for predicting these structures. Table 13
shows the recall, precision and F-measure of grand-
child structures whose grand parent is a sentence
root symbol $. The results support the above as-
sumption. The root correct rate directly influences
on prediction of the overall structures of a sentence,
and it is likely that the reduction of root prediction
errors brings better results.
6.5 Experiments on Chinese
We also experiment on the Penn Chinese Treebank
(CTB5). Following Huang and Sagae (2010), we
split it into training (secs 001-815 and 1001-1136),
development (secs 886-931 and 1148-1151), and
test (secs 816-885 and 1137-1147) sets, and use the
head rules of Zhang and Clark (2008). The training
set is split into 10 folds to dump packed forests for
training of reranking models.
We set the beam size of both spurious and non-
spurious parsers to 12, and the number of perceptron
training iterations to 25 for the parsers and to 8 for
both rerankers. Table 14 shows the results for the
test sets. As we expected, reranking on non-spurious
forests outperforms that on spurious forests.
147
system UAS root comp.
sr (12) 85.3 78.6 33.4
w/ non-local (12, k=3) 85.8 79.4 34.2
non-sp sr (12) 85.3 78.4 33.7
w/ non-local (12, k=3) 85.9 79.6 34.3
Table 14: Results on Chinese Treebank data (CTB5):
evaluations are performed without punctuations.
7 Related Works
7.1 How to Handle Spurious Ambiguity
The graph-based approach employs Eisner and Satta
(1999)?s algorithm where spurious ambiguities are
eliminated by the notion of split head automaton
grammars (Alshawi, 1996).
However, the arc-standard transition-based parser
has the spurious ambiguity problem. Cohen et al
(2012) proposed a method to eliminate the spurious
ambiguity of shift-reduce transition systems. Their
method covers existing systems such as the arc-
standard and non-projective transition-based parsers
(Attardi, 2006). Our system copes only with the pro-
jective case, but is simpler than theirs and we show
its efficacy empirically through some experiments.
The arc-eager shift-reduce parser also has a spuri-
ous ambiguity problem. Goldberg and Nivre (2012)
addressed this problem by not only training with a
canonical transition sequence but also with alternate
optimal transitions that are calculated dynamically
for a current state.
7.2 Methods to Improve Dependency Parsing
Higher-order features like third-order dependency
relations are essential to improve dependency pars-
ing accuracy (Koo and Collins, 2010; Rush and
Petrov, 2012; Zhang and McDonald, 2012). A
reranking approach is one effective solution to intro-
duce rich features to a parser model in the context of
constituency parsing (Charniak and Johnson, 2005;
Huang, 2008).
Hall (2007) applied a k-best maximum spanning
tree algorithm to non-projective dependency analy-
sis, and showed that k-best discriminative rerank-
ing improves parsing accuracy in several languages.
Sangati et al (2009) proposed a k-best dependency
reranking algorithm using a third-order generative
model, and Hayashi et al (2011) extended it to a
forest algorithm. Though forest reranking requires
some approximations such as cube-pruning to inte-
grate non-local features, it can explore larger search
space than k-best reranking.
The stacking approach (Nivre and McDonald,
2008; Martins et al, 2008) uses the output of one
dependency parser to provide guide features for an-
other. Stacking improves the parsing accuracy of
second stage parsers on various language datasets.
The joint graph-based and transition-based approach
(Zhang and Clark, 2008; Bohnet and Kuhn, 2012)
uses an arc-eager shift-reduce parser with a joint
graph-based and transition-based model. Though
it improves parsing accuracy significantly, the large
beam size of the shift-reduce parser harms its effi-
ciency. Sagae and Lavie (2006) showed that com-
bining the outputs of graph-based and transition-
based parsers can improve parsing accuracies.
8 Conclusion
We have presented a discriminative forest reranking
algorithm for dependency parsing. This can be seen
as a kind of joint transition-based and graph-based
approach because the first-stage parser is a shift-
reduce parser and the second-stage reranker uses a
graph-based model.
Additionally, we have proposed a dynamic pro-
gramming arc-standard transition-based dependency
parser without spurious ambiguity, along with a
heuristic that encodes the correct tree in the output
packed forest for reranker training, and shown that
forest reranking works well on packed forests pro-
duced by the proposed parser.
To improve the accuracy of reranking, we will en-
gage in feature engineering. We need to further in-
vestigate effective higher-order guide and non-local
features. It also seems promising to extend the un-
labeled reranker to a labeled one because labeled in-
formation often improves unlabeled accuracy.
In this paper, we adopt a reranking approach, but
a rescoring approach is more promising to improve
efficiency because it does not have the overhead of
dumping packed forests.
Acknowledgments
We would like to thank the anonymous reviewers
for their valuable comments. This work was partly
148
supported by Grant-in-Aid for Japan Society for the
Promotion of Science (JSPS) Research Fellowship
for Young Scientists.
References
H. Alshawi. 1996. Head automata for speech translation.
In Proc. the ICSLP.
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proc. of the 10th
Conference on Natural Language Learning, pages
166?170.
B. Bohnet and J. Kuhn. 2012. The best of bothworlds ?
a graph-based completion model for transition-based
parsers. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 77?87.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 173?180.
S. B. Cohen, C. Go?mez-Rodr??guez, and G. Satta. 2012.
Elimination of spurious ambiguity in transition-based
dependency parsing. Technical report.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL?04).
J. M. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics,
pages 457?464.
J. Eisner. 1997. Bilexical grammars and a cubic-time
probabilistic parser. In Proceedings of the 5th Inter-
national Workshop on Parsing Technologies (IWPT),
pages 54?65.
Y. Goldberg and J. Nivre. 2012. A dynamic oracle for
arc-eager dependency parsing. In Proceedings of the
24rd International Conference on Computational Lin-
guistics (Coling 2012).
K. Hall. 2007. K-best spanning tree parsing. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 392?399.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2011. The third-order variational reranking
on packed-shared dependency forests. In Proceedings
of the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1479?1488.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2012. Head-driven transition-based parsing
with top-down prediction. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 657?665.
K. Hollingshead and B. Roark. 2008. Reranking with
baseline system scores and ranks as features. In
CSLU-08-001, Center for Spoken Language Under-
standing, Oregon Health and Science University.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 144?151.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL?10), pages 1077?1086.
L. Huang, S. Fayong, and Y. Guo. 2012. Structured per-
ceptron with inexact search. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142?151.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 586?594.
D. Klein and C. D. Manning. 2001. Parsing and hyper-
graphs. In Proceedings of the 7th International Work-
shop on Parsing Technologies.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?10), pages 1?11.
M. Kuhlmann, C. Go?mez-Rodr??guez, and G. Satta. 2011.
Dynamic programming algorithms for transition-
based dependency parsers. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 673?682.
Andre? F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 157?166.
D.McClosky, W. Che, M. Recasens, M.Wang, R. Socher,
and C. D. Manning. 2012. Stanfords system for pars-
ing the english web. In Proceedings of First Work-
shop on Syntactic Analysis of Non-Canonical Lan-
guage (SANCL) at NAACL 2012.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
91?98.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, pages 950?958.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34:513?553.
149
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160?167.
A. Rush and S. Petrov. 2012. Vine pruning for effi-
cient multi-pass dependency parsing. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 498?507.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. HLT, pages 129?132.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proceed-
ings of the 11th International Conference on Parsing
Technologies (IWPT?09), pages 238?241.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing. J.
Log. Program., 24(1&2):3?36.
C. Sutton, M. Sindelar, and A. McCallum. 2006. Reduc-
ing weight undertraining in structured discriminative
learning. In Conference on Human Language Tech-
nology and North American Association for Computa-
tional Linguistics (HLT-NAACL).
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies (IWPT?03), pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?571.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 320?331.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 188?193.
150
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 281?288,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
NAIST at the HOO 2012 Shared Task
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, Lis Kanashiro
Tomoya Mizumoto, Mamoru Komachi, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{ keisuke-sa, yuta-h, shuhei-k, lis-k, tomoya-m, komachi, matsu }@is.naist.jp
Abstract
This paper describes the Nara Institute of Sci-
ence and Technology (NAIST) error correc-
tion system in the Helping Our Own (HOO)
2012 Shared Task. Our system targets prepo-
sition and determiner errors with spelling cor-
rection as a pre-processing step. The re-
sult shows that spelling correction improves
the Detection, Correction, and Recognition F-
scores for preposition errors. With regard to
preposition error correction, F-scores were not
improved when using the training set with cor-
rection of all but preposition errors. As for
determiner error correction, there was an im-
provement when the constituent parser was
trained with a concatenation of treebank and
modified treebank where all the articles ap-
pearing as the first word of an NP were re-
moved. Our system ranked third in preposi-
tion and fourth in determiner error corrections.
1 Introduction
Researchers in natural language processing have fo-
cused recently on automatic grammatical error de-
tection and correction for English as a Second Lan-
guage (ESL) learners? writing. There have been a lot
of papers on these challenging tasks, and remark-
ably, an independent session for grammatical error
correction took place in the ACL-2011.
The Helping Our Own (HOO) shared task (Dale
and Kilgarriff, 2010) is proposed for improving the
quality of ESL learners? writing, and a pilot run with
six teams was held in 2011.
The HOO 2012 shared task focuses on the cor-
rection of preposition and determiner errors. There
has been a lot of work on correcting preposition and
determiner errors, where discriminative models such
as Maximum Entropy and Averaged Perceptron (De
Felice and Pulman, 2008; Rozovskaya and Roth,
2011) and/or probablistic language models (Gamon,
2010) are generally used.
In addition, it is pointed out that spelling and
punctuation errors often disturb grammatical error
correction. In fact, some teams reported in the
HOO 2011 that they corrected spelling and punc-
tuation errors before correcting grammatical errors
(Dahlmeier et al, 2011).
Our strategy for HOO 2012 follows the above
procedure. In other words, we correct spelling er-
rors at the beginning, and then train classifiers for
correcting preposition and determiner errors. The
result shows our system achieved 24.42% (third-
ranked) in F-score for preposition error correc-
tion, 29.81% (fourth-ranked) for determiners, and
27.12% (fourth-ranked) for their combined.
In this report, we describe our system architec-
ture and the experimental results. Sections 2 to 4
describe the system for correcting spelling, prepo-
sition, and determiner errors. Section 5 shows the
experimental design and results.
2 System Architecture for Spelling
Correction
Spelling errors in second language learners? writing
often disturb part-of-speech (POS) tagging and de-
pendency parsing, becoming an obstacle for gram-
matical error detection and correction tasks. For ex-
ample, POS tagging for learners? writing fails be-
281
e.g. I think it is *verey/very *convent/convenient for the group.
without spelling error correction: ... (?it?, ?PRP?), (?is?, ?VBZ?), (?verey?, ?PRP?), (?convent?, ?NN?), ...
with spelling error correction : ... (?It?, ?PRP?), (?is?, ?VBZ?), (?very?, ?RB?), (?convenient?, ?JJ?), ...
Figure 1: POS tagging for learners? writing without and with spelling error correction.
cause of misspelled words (Figure 1).1
To reduce errors derived from misspelled words,
we conduct spelling error correction as a pre-
processing task. The procedure of spelling error cor-
rection we use is as follows. First of all, we look for
misspelled words and suggest candidates by GNU
Aspell2, an open-source spelling checker. The can-
didates are ranked by the probability of 5-gram lan-
guage model built from Google N-gram (Web 1T
5-gram Version 1)3 (Brants and Franz, 2006) with
IRST LM Toolkit (Federico and Cettolo, 2007).4 Fi-
nally, according to the rank, we changed the mis-
spelled word into the 1-best candidate word.
In a preliminary experiment, where we use the
original CLC FCE dataset,5 our spelling error cor-
rection obtains 52.4% of precision, 72.2% of recall,
and 60.7% of F-score.
We apply the spelling error correction to the train-
ing and test sets provided, and use both spelling-
error and spelling-error-free sets for comparison.
3 System Architecture for Preposition
Error Correction
There are so many prepositions in English. Because
it is difficult to perform multi-class classification,
we focus on twelve prepositions: of, in, for, to, by,
with, at, on, from, as, about, since, which account
for roughly 91% of preposition usage (Chodorow et
al., 2010).
The errors are classified into three categories ac-
cording to their ways of correction. First, replace-
ment error indicates that learners use a wrong
preposition. For instance, with in Example (1) is a
1The example is extracted from the CLC FCE dataset and
part-of-speech tagged by Natural Language Toolkit (NLTK).
http://www.nltk.org/
2GNU Aspell 0.60.6.1 http://aspell.net/
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13
4irstlm5.70 http://sourceforge.net/projects/irstlm/
5In the CLC FCE dataset, misspelled words are corrected
and tagged with a label ?S?.
replacement error.
I went there withby bus. (1)
Second, insertion error points out they incor-
rectly inserted a preposition, such as ?about? in Ex-
ample (2).6
We discussed aboutNONE the topic. (2)
Third, deletion error means they fail to write
obligatory prepositions. For example, ?NONE? in
Example (3) is an deletion error.
This is the place to relax NONEin. (3)
Replacement and insertion error correction can be
regarded as a multi-class classification task at each
preposition occurrence. However, deletion errors
differ from the other two types of errors in that they
may occur at any place in a sentence. Therefore, we
build two models, a combined model for replace-
ment and insertion errors and a model for deletion
errors, taking the difference into account.
For the model of replacement and insertion errors,
we simultaneously perform error detection and cor-
rection with a single model.
For the model of deletion errors, we only check
whether direct objects of verbs need prepositions,
because it is time consuming to check all the gaps
between words. Still, it covers most deletion errors.7
We merge the outputs of the two models to get the
final output.
We used two types of training sets extracted from
the original CLC-FCE dataset. One is the ?gold?
set, where training sentences are corrected except
for preposition errors. In the gold set, spelling er-
rors are also corrected to the gold data in the corpus.
The other is the ?original? set, which includes the
6?NONE? means there are no words.
72,407 out of 5,324 preposition errors in CLC-FCE are be-
tween verbs and nouns.
282
Type Name Description (NP and PRED refer a noun phrase and a predicate.)
Lexical Token n-gram Token n-grams in a 2 word window around the preposition
POS n-gram POS n-grams in a 2 word window around the preposition
HEAD PREC VP The head verb in the preceding verb phrase
HEAD PREC NP The head noun in the preceding noun phrase
HEAD FOLLOW NP The head noun in the following noun phrase
Parsing HEAD Head of the preposition
HEAD POS POS of the head
COMP Complement of the preposition
COMPLEMENT POS POS of the complement
HEAD RELATION Prep-Head relation name
COMPLEMENT RELATION Prep-Comp relation name
Phrase Structure PARENT TAG TAG of the preposition?s parent
GRANDPARENT TAG TAG of the preposition?s grandparent
PARENT LEFT Left context of the preposition parent
PARENT RIGHT Right context of the preposition?s parent
Web N-gram COUNT For the frequency fprep,i of i (3 to 5) window size phrase including
the preposition prep, the value of log100(fi + 1)
PROPORTION The proportion pprep,i (i is 3 to 5).
pprep,i = fprep,i?
k?T fk,i
, given the set of target prepositions T .
Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic
categories for all words used as surface features. As De Felice and
Pulman (2008) did not perform word sense disambiguation, neither
did we.
Table 1: Baseline features for English preposition error correction.
original CLC-FCE plain sentences.
We performed sentence splitting using the im-
plementation of Kiss and Strunk (2006) in NLTK
2.0.1rc2. We conducted dependency parsing by
Stanford parser 1.6.9.8
We used the features described in (Tetreault et al,
2010) as shown in Table 1 with Maximum Entropy
(ME) modeling (Berger et al, 1996) as a multi-class
classifier. We used the implementation of Maximum
Entropy Modeling Toolkit9 with its default parame-
ters. For web n-gram calculation, we used Google
N-gram with a search system for giga-scale n-gram
corpus, called SSGNC 0.4.6.10
4 System Architecture for Determiner
Error Correction
We focused on article error correction in the deter-
miner error correction subtask, because the errors
related to articles significantly outnumber the errors
unrelated to them. Though more than twenty types
of determiners are involved in determiner error cor-
rections of the HOO training set, over 90% of errors
8http://nlp.stanford.edu/software/lex-parser.shtml
9https://github.com/lzhang10/maxent
10http://code.google.com/p/ssgnc/
are related to three articles a, an and the. We defined
article error correction as a multi-class classification
problem with three classes, a, the and null article,
and assumed that target articles are placed at the left
boundary of a noun phrase (NP). The indefinite ar-
ticle an was normalized to a in training and testing,
and restored to an later in an example-based post-
processing step. If the system output was a and the
word immediately after a appeared more frequently
with an than with a in the training corpus, a was re-
stored to an. If the word appeared equally frequently
with a and an or didn?t appear in the training corpus,
a was restored to an if the word?s first character was
one of a, e, i, o, u.
Each input sentence was parsed using the Berke-
ley Parser11 with two models, ?normal? and
?mixed?. The ?normal? model was trained on a tree-
bank of normal English sentences. In preliminary
experiments, the ?normal? model sometimes mis-
judged the span of NPs in ESL writers? sentences
due to missing articles. So we trained the ?mixed?
model on a concatenation of the normal treebank
and a modified treebank in which all the articles ap-
pearing as the first word of an NP were removed. By
11version 1.1, http://code.google.com/p/berkeleyparser/
283
Name Description
HeadNounWord The word form of the head noun
HeadNounTag The POS tag of the head noun
ObjOfPrep Indicates that the head noun is an object of a preposition
PrepWord The word form of the preposition
PrepHeadWord The word form of the preposition?s syntactic parent
PrepHeadTag The POS tag of the preposition?s syntactic parent
ContextWindowTag
The POS tag of the words in a 3 word window
around the candidate position for the article
ContextWindowWord
The word form of the word immediately following
the candidate position for the article
ModByDetWord The word form of the determiner that modifies the head noun
ModByAdjWord The word form of the adjective that modifies the head noun
ModByAdjTag The POS tag of the adjective that modifies the head noun
ModByPrep Indicates that the head noun is modified by a preposition
ModByPrepWord The word form of the preposition that modifies the head noun
ModByPossesive Indicates that the head noun is modified by a possesive
ModByCardinal Indicates that the head noun is modified by a cardinal number
ModByRelative Indicates that the head noun is modified by a relative clause
Table 2: Feature templates for English determiner correction.
augmenting the training data for the parser model
with sentences lacking articles, the span of NPs that
lack an article might have better chance of being cor-
rectly recognized. In addition, dependency informa-
tion was extracted from the parse using the Stanford
parser 1.6.9.
For each NP in the parse, we extracted a feature
vector representation. We used the feature templates
shown in Table 2, which are inspired by (De Felice,
2008) and adapted to the CFG representation.
For the parser models, we trained the ?normal?
model on the WSJ part of Penn Treebank sections
02-21 with the NP annotation by Vadas and Curran
(2007). The ?mixed? model was trained on the con-
catenation of the WSJ part and its modified version.
For the classification model, we used the written part
of the British National Corpus (BNC) in addition to
the CLC FCE Dataset, because the amount of in-
domain data was limited. In examples taken from
the CLC FCE Dataset, the true labels after the cor-
rection were used. In examples taken from the BNC,
the article of each NP was used as the label. We
trained a linear classifier using opal12 with the PA-I
algorithm. We also used the feature augmentation
12http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
Subsystem Parameters
Run Spelling Preposition Determiner
0 no change gold mixed
1 no change gold normal
2 no change original mixed
3 no change original normal
4 corrected gold mixed
5 corrected gold normal
6 corrected original mixed
7 corrected original normal
Table 3: Distinct configurations of the system.
approach of (Daume? III, 2007) for domain adapta-
tion.
5 Experiment and Result
Previously undisclosed data extracted from the
CLC-FCE dataset was provided as a test set by the
HOO organizers. The test set includes 100 essays
and each contains 180.1 word tokens on average.
We defined eight distinct configurations based
on our subsystem parameters (Table 3). The offi-
cial task evaluation uses three metrics (Detection,
Recognition, and Correction), and three measures
Precision, Recall, and F-score were computed13 for
13For details about the evaluation metrics, see http://
284
Detection Correction Recognition
Run R P F R P F R P F
0 29.58 34.09 31.67 19.86 22.90 21.27 26.71 30.78 28.60
1 28.69 36.41 32.09 19.42 24.64 21.72 25.82 32.77 28.88
2? 28.91 37.21 32.54 20.97 26.98 23.60 26.26 33.80 29.56
3 28.03 40.18 33.02 20.52 29.43 24.18 25.38 36.39 29.90
4 30.24 33.66 31.86 20.75 23.09 21.86 27.37 30.46 28.83
5 29.13 35.57 32.03 19.64 23.98 21.60 26.26 32.07 28.88
6 29.35 36.23 32.43 21.41 26.43 23.65 26.26 32.42 29.02
7 28.25 38.67 32.65 20.30 27.29 23.46 25.16 34.44 29.08
Table 4: Result for preposition and determiner errors combined before revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.
Detection Correction Recognition
Spelling Preposition R P F R P F R P F
no change gold 25.00 34.70 29.06 14.40 20.00 16.74 20.76 28.82 24.13
no change original 23.30 42.63 30.13 16.52 30.23 21.36 19.91 36.43 25.75
corrected gold 26.69 34.80 30.21 15.25 19.88 17.26 22.45 29.28 25.41
corrected original 24.57 41.13 30.76 16.52 27.65 20.68 20.33 34.04 25.46
Table 5: Result for preposition errors before revisions.
each metric.
Table 4 to Table 9 show the overall results of our
systems. In terms of the effect of pre-processing,
spelling correction improved the F-score of Detec-
tion, Correction, and Recognition for preposition er-
rors after revision, whereas there were fluctuations
in other conditions. This may be because there were
a few spelling errors corrected in the test set.14 An-
other reason why no stable improvement was found
in determiner error correction is because spelling
correction often produces nouns that affect the de-
terminer error detection and correction more sensi-
tively than prepositions. For example, a misspelled
word *freewho / free who was corrected as freezer.
This type of error may have increased false posi-
tives. The example *National Filharmony / the Na-
tional Philharmony was corrected as National Flem-
ing, where the proper noun Fleming does not need a
determiner and this type of error increased false neg-
atives.
As for preposition error correction, the classifier
performed better when it was trained with the ?origi-
nal? set rather than the error-corrected (all but prepo-
sition errors) ?gold? set. The reason for this is that
the gold set is trained with the test set that contains
correcttext.org/hoo2012/eval.html
14There was one spelling correction per document in average.
several types of errors which the original CLC-FCE
dataset alo contains. Therefore, the ?original? clas-
sifier is more optimised and suitable for the test set
than the ?gold? one.
For determiner error correction, the ?mixed?
model improved precision and F-score in the addi-
tional experiments.
5.1 Error Analysis of Preposition Correction
We briefly analyze some errors in our proposed
model according to the three categories of errors.
First, most replacement errors require deep under-
standing of context. For instance, for in Example (4)
must be changed to to. However, modifications of is
also often used, so it is hard to decide either to or of
is suitable based on the values of N-gram frequen-
cies.
Its great news to hear you have been given
extra money and that you will spend it in
modifications forto the cinema.
(4)
Second, most insertion errors need a grammatical
judgement rather than a semantic one. For instance,
?in? in Example (5) must be changed to ?NONE.?
Their love had always been kept inNONE se-
cret
(5)
In order to correct this error, we need to recog-
285
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 34.10 33.18 33.63 25.80 25.11 25.45 33.17 32.28 32.72
no change normal 32.25 37.43 34.65 24.88 28.87 26.73 31.33 36.36 33.66
corrected mixed 33.64 32.30 32.95 26.72 25.66 26.18 32.71 31.41 32.05
corrected normal 31.33 35.78 33.41 24.42 27.89 26.04 30.41 34.73 32.43
Table 6: Result for determiner errors before revisions.
Detection Correction Recognition
Run R P F R P F R P F
0 31.28 37.65 34.18 22.62 27.22 24.71 28.54 34.35 31.17
1 30.44 40.33 34.69 22.19 29.41 25.30 27.69 36.69 31.56
2? 31.07 41.76 35.63 23.04 30.96 26.42 28.11 30.96 32.24
3 30.23 45.25 36.24 22.62 33.86 27.12 27.27 40.82 32.69
4 31.92 37.10 34.31 23.46 27.27 25.22 29.17 33.90 31.36
5 30.86 39.35 34.59 22.41 28.57 25.11 28.11 35.84 31.51
6 31.71 40.87 35.71 23.89 30.79 26.90 28.75 37.05 32.38
7 30.65 43.80 36.06 22.83 32.62 26.86 27.69 39.57 32.58
Table 7: Result for preposition and determiner errors combined after revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.
nize ?keep? takes an object and a complement; in
Example (5) ?love? is the object and ?secret? is
the complement of ?keep? while the former is left-
extraposed. A rule-based approach may be better
suited for these cases than a machine learning ap-
proach.
Third, most deletion errors involve discrimination
between transitive and intransitive. For instance,
?NONE? in Example (6) must be changed to ?for?,
because ?wait? is intransitive.
I?ll wait NONEfor your next letter. (6)
To deal with these errors, we may use rich knowl-
edge about verbs such as VerbNet (Kipper et al,
2000) and FrameNet (Baker et al, 1998) in order
to judge whether a verb is transitive or intransitive.
5.2 Error Analysis of Determiner Correction
We conducted additional experiments for determiner
errors and report the results here because the sub-
mitted system contained a bug. In the submit-
ted system, while the test data were parsed by the
?mixed? model, the training data and the test data
were parsed by the default grammar provided with
Berkeley Parser. Moreover, though there were about
5.5 million sentences in the BNC corpus, only about
2.7 million of them had been extracted. Though
these errors seem to have improved the performance,
it is difficult to specify which errors had positive ef-
fects.
Table 10 shows the result of additional experi-
ments. Unlike the submitted system, the ?mixed?
model contributed toward a higher precision and F-
score. Though the two parser models parsed the
sentences differently, the difference in the syntactic
analysis of test sentences did not always led to dif-
ferent output by the downstream classifiers. On the
contrary, the classifiers often returned different out-
puts even for an identically parsed sentence. In fact,
the major source of the performance gap between the
two models was the number of the wrong outputs
rather than the number of correct ones. While the
?mixed? model without spelling correction returned
146 outputs, of which 83 were spurious, the ?nor-
mal? model without spelling correction produced
209 outputs, of which 143 were spurious. This may
suggest the difference of the two models can be at-
tributed to the difference in the syntactic analysis of
the training data.
One of the most frequent types of errors com-
mon to the two models were those caused by mis-
spelled words. For example, when your letter was
misspelled to be *yours letter, it was regarded as an
286
Detection Correction Recognition
Spelling Preposition R P F R P F R P F
no change gold 26.63 38.23 31.40 17.62 25.29 20.77 23.36 33.52 27.53
no change original 26.22 49.61 34.31 18.44 34.88 24.12 22.54 42.63 29.49
corrected gold 28.27 38.12 32.47 18.44 24.86 21.17 25.00 33.70 28.70
corrected original 27.86 48.22 35.32 19.26 33.33 24.41 24.18 41.84 30.64
Table 8: Result for preposition errors after revisions.
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 35.37 36.32 35.84 27.94 28.69 28.31 34.06 34.97 34.51
no change normal 33.62 41.17 37.01 27.07 33.15 29.80 32.31 39.57 35.57
corrected mixed 34.93 35.39 35.16 28.82 29.20 29.01 33.62 34.07 33.84
corrected normal 32.75 39.47 35.79 26.63 32.10 29.11 31.44 37.89 34.36
Table 9: Result for determiner errors after revisions.
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 27.39 43.15 33.51 23.04 36.30 28.19 27.39 43.15 33.51
no change normal 28.69 31.57 30.06 22.61 24.88 23.69 28.69 31.57 30.06
corrected mixed 27.39 41.44 31.98 22.61 34.21 27.22 26.96 40.79 32.46
corrected normal 30.43 33.33 31.82 24.34 26.67 25.45 30.00 32.86 31.36
Table 10: Result of additional experiments for determiner errors after revisions.
NP without a determiner resulting in a false posi-
tive such as *a yours letter. Among the other types
of errors, several seemed to be caused by the infor-
mation from the context window. For instance, the
system output for It was last month and ... was it
was *the last month and .... It is likely that the word
last triggered the misinsertion here. Such kind of
errors might be avoided by conjunctive features of
context information and the head word. Last but not
least, compound errors were also frequent and prob-
ably the most difficult to solve. For example, it is
quite difficult to correct *for a month to per month
if we are dealing with determiner errors and prepo-
sition errors separately. A more sophisticated ap-
proach such as joint modeling seems necessary to
correct this kind of errors.
6 Conclusion
This report described the architecture of our prepo-
sition and determiner error correction system. The
experimental result showed that spelling correction
advances the performance of Detection, Correction
and Recognition for preposition errors. In terms of
preposition error correction, F-scores were not im-
proved when the error-corrected dataset was used.
As to determiner error correction, there was an im-
provement when the constituent parser was trained
on a concatenation of treebank and modified tree-
bank where all the articles appearing as the first
word of an NP were removed.
Acknowledgements
This work was partly supported by the National In-
stitute of Information and Communications Tech-
nology Japan.
287
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics, pages 86?90, Montreal,
Quebec, Canada.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1):39?71.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Error Cor-
rection Systems for English Language Learners: Feed-
back and Assessment. Language Testing, 27(3):419?
436.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation, pages 257?259, Nancy,
France.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
pages 261?266, Trim, Co. Meath, Ireland.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 169?176, Manchester, UK.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, University of Ox-
ford.
Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of N-gram Language Models for Statistical
Machine Translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
88?95, Prague, Czech Republic.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based Construction of a Verb Lexicon. In
Proceedings of the 7th National Conference on Artifi-
cial Intelligence, pages 691?696, Austin, Texas, USA.
Tibor Kiss and Jan Strunk. 2006. Unsupervised Multi-
lingual Sentence Boundary Detection. Computational
Linguistics, 32(4):485?525.
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 924?933, Portland, Ore-
gon, USA.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selec-
tion and Error Detection. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics Short Papers, pages 353?358, Uppsala,
Sweden.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 240?247, Prague, Czech
Republic.
288
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 139?144,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Construction of English MWE Dictionary and
its Application to POS Tagging
Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei Kondo, Tomoya Kose,
Keisuke Sakaguchi, Akifumi Yoshimoto, Frances Yung, Yuji Matsumoto
Nara Institute Science of Technology (NAIST)
Ikoma, Nara 630-0192 Japan
yutaro-s@is.naist.jp
Abstract
This paper reports our ongoing project for
constructing an English multiword expression
(MWE) dictionary and NLP tools based on
the developed dictionary. We extracted func-
tional MWEs from the English part of Wik-
tionary, annotated the Penn Treebank (PTB)
with MWE information, and conducted POS
tagging experiments. We report how the
MWE annotation is done on PTB and the re-
sults of POS and MWE tagging experiments.
1 Introduction
While there have been a great progress in POS
tagging and parsing of natural language sentences
thanks to the advancement of statistical and corpus-
based methods, there still remains difficulty in sen-
tence processing stemming from syntactic discrep-
ancies. One of such discrepancies is caused by mul-
tiword expressions (MWEs), which are known and
defined as expressions having ?idiosyncratic inter-
pretations that cross word boundaries (or spaces)?
(Sag et al, 2002).
Sag et al (2002) classifies MWEs largely into the
following categories:
? Lexicalized phrases
? fixed expressions: Those having fixed
word order and form (e.g. by and large).
? semi-fixed expressions: Those having
fixed word order with lexical variation
such as inflection, determiner selection,
etc. (e.g. come up with).
? syntactically flexible expressions: Those
having a wide range of syntactic variabil-
ity (e.g. phrasal verbs that take an NP ar-
gument between or following the verb and
the particle).
? Institutionalized phrases
? Phrases that are semantically and syntac-
tically compositional, such as collocations
(e.g. traffic light).
This paper reports our ongoing project for devel-
oping an English MWE dictionary of a broad cov-
erage and MWE-aware natural language processing
tools. The main contributions of this paper are as
follows:
1. Construction of an English MWE dictionary
(mainly consisting of functional expressions)
through extraction from Wiktionary1.
2. Annotation of MWEs in the Penn Treebank
(PTB).
3. Implementation of an MWE-aware POS tagger
and evaluation of its performance.
2 Related work
While there is a variety of MWE researches only a
few of them focus on MWE lexicon construction.
Though some examples, such as French adverb dic-
tionaries (Laporte and Voyatzi, 2008; Laporte et al,
2008), a Dutch MWE dictionary (Gre?goire, 2007)
and a Japanese MWE dictionary (Shudo et al, 2011)
have been constructed, there is no freely available
English MWE dictionary with a broad coverage.
Moreover, MWE-annotated corpora are only
available for a few languages, including French and
1https://en.wiktionary.org
139
Swedish. While the British National Corpus is anno-
tated with MWEs, its coverage is far from complete.
Considering this situation, we started construction
of an English MWE dictionary (with functional ex-
pressions first) and classified their occurrences in
PTB into MWE or literal usage, obtaining MWE-
annotated version of PTB.
The effect of MWE dictionaries have been re-
ported for various NLP tasks. Nivre and Nilsson
(2004) investigated the effect of recognizing MWEs
in syntactic dependency parsing of Swedish. Ko-
rkontzelos and Manandhar (2010) showed perfor-
mance improvement of base phrase chunking by an-
notating compound and proper nouns. Finlayson
and Kulkarni (2011) reported the effect of recogniz-
ing MWEs on word sense disambiguation.
Most of the previous approaches to MWE recog-
nition are based on frequency or collocation mea-
sures of words in large scale corpora. On the other
hand, some previous approaches tried to recognize
new MWEs using an MWE lexicon and MWE-
annotated corpora. Constant and Sigogne (2011)
presented MWE recognition using a Conditional
Random Fields (CRFs)-based tagger with the BIO
schema. Green et al (2011) proposed an MWE
recognition method using Tree Substitution Gram-
mars. Constant et al (2012) compared two phrase
structure analysis methods, one that uses MWE
recognition as preprocessing and the other that uses
a reranking method.
Although MWEs show a variety of flexibilities
in their appearance, most of the linguistic analyses
consider the fixed type of MWEs. For example, the
experiments by Nivre and Nilsson (2004) focus on
fixed expressions that fall into the following cate-
gories:
1. Multiword names
2. Numerical expressions
3. Compound function words
(a) Adverbs
(b) Prepositions
(c) Subordinating conjunctions
(d) Determiners
(e) Pronouns
Multiword names and numerical expressions be-
have as noun phrases and have limited syntactic
functionalities. On the other hand, compound func-
tion words have a variety of functionalities that may
affect language analyses such as POS tagging and
parsing. In this work, we extract compound func-
tional expressions from the English part of Wik-
tionary, and classify their occurrences in PTB into
either literal or MWE usages. We then build a POS
tagger that takes MWEs into account. In implement-
ing this, we use CRFs that can handle a sequence of
tokens as a single item (Kudo et al, 2004). We eval-
uate the performance of the tagger and compare it
with the method that uses the BIO schema for iden-
tifying MWE usages (Constant and Sigogne, 2011).
3 MWEs Extraction from Wiktionary
To construct an English MWE dictionary, we extract
entries from the English part of Wiktionary (as of
July 14, 2012) that include white spaces. We ex-
tract only fixed expressions that are categorized ei-
ther as adverbs, conjunctions, determiners, prepo-
sitions, prepositional phrases or pronouns. We ex-
clude compound nouns and phrasal verbs since the
former are easily recognized by an existing method
such as chunking and the latter need more sophis-
ticated analyzing methods because of their syntac-
tic flexibility. We also exclude multiword adjec-
tives since many of them are semi-fixed and behave
differently from lexical adjective, having predica-
tive usage only. Table 1 summarizes the numbers
of MWE entries in Wiktionary and the numbers of
them that appear at least once in PTB.
4 Annotation of MWEs in PTB
While it is usually not easy to identify the usage of
an MWE as either an MWE or a literal usage, we
initially thought that the phrase structure tree an-
notations in PTB would have enough information
to identify their usages. This assumption is cor-
rect in many cases (Figures 1(a) and 1(b)). The
MWE usage of ?a bit? in Figure 1(a) is analyzed as
?NP-ADV?, suggesting it is used as an adverb, and
the literal usage of ?a bit? in Figure 1(b) is labeled
as ?NP?, suggesting it is used literally. However,
there are a number of examples that are annotated
differently while their usages are the same. For ex-
ample, Figures 1(c), 1(d) and 1(e) all show RB us-
140
Table 1: Number of MWE types in Wiktionary and Penn Treebank
Adverb Conjunction Determiner Preposition Prepositional Phrase Pronoun
Wiktionary 1501 49 15 110 165 83
PTB 468 35 9 77 66 18
Examples after all as wll as a number of according to against the law no one
VP
VB
heat
PRT
up
NP-ADV
DT
a
NN
bit
(a) MWE usage as RB
ADVP
NP
DT
a
NN
bit
PP
IN
of
NP
NN
chromosome
CD
13
(b) Literal usage as NP
ADVP
NP-ADV
DT
a
RB
bit
JJR
smaller
(c) MWE usage as RB
ADVP
NP
DT
a
NN
bit
RBR
better
(d) MWE usage as RB
ADJP-PRD
NP
DT
a
RB
bit
JJR
isolated
(e) MWE usage as RB
Figure 1: Examples of phrase structures annotated to ?a bit?
age of ?a bit? while they are annotated differently 2.
Sometimes, the same structure tree is annotated to
instances of different usages (Figures 1(b) and 1(d)).
Therefore, for eachMWE candidate, we first clus-
ter its occurrences in PTB according to their phrase
tree structures. Some of the clusters clearly indi-
cate MWE usages (such as ?NP-ADV? trees in Fig-
ures 1(a) and 1(c)). In such cases, we regarded all in-
stances as MWE usages and annotated them as such.
For inconsistent or ambiguous cases (such as ?NP?
trees in Figures 1(b), 1(d) and 1(e)), we manually
classify each of them into either MWE or literal us-
age (some MWEs have multiple MWE usages). We
find a number of inconsistent POS annotations on
some internal words of MWEs (e.g. ?bit? in Fig-
ures 1(c) and 1(e) are annotated as RB while they
should be NN). We correct such inconsistent cases
(correction is only done on internal words of MWEs,
selecting the majority POS tags as correct). The total
number of POS tag corrections made on PTB (chap-
ter 00-24) was 1084.
2The POS tags in the trees are: RB(adverb), IN(preposition),
DT(determiner), NN(common noun) ...
5 Experiments of POS tagging and MWE
recognition
5.1 Experiment Setting
We conduct POS tagging experiments on the MWE-
annotated PTB, using sections 0-18 for training and
sections 22-24 for test as usual.
For the experiments, we use four versions of PTB
with the following POS annotations.
(a) Original: PTB with the original POS annota-
tion
(b) Revised: PTB with correction of inconsistent
POS tags
(c) BIO MWE: MWEs are annotated with the BIO
schema
(d) MWE: MWEs are annotated as single words
Concerning the MWE annotation in (c) and (d),
the total number of MWE tokens in PTB is 12131
(9417 in the training chapters, 1396 in the test
chapters, and 1319 for the remaining (development)
chapters).
Each word is annotated with the following in-
141
Figure 2: Example of lattice containing MWE (?about to/RB?) (correct path is marked with bold boxes.)
Table 2: Examples of MWE annotations in four versions
Version Word/POS
(a) Original about/RB to/TO
(b) Revised about/IN to/TO
(c) BIO MWE about/RB-B to/RB-I
(d) MWE about to/RB
formation: coarse-grained POS tag (CPOS), fine-
grained POS tag (FPOS) and surface form. Each
MWE is further annotated with its POS tag, surface
form, its internal words with their POS tags.
Table 2 shows sample annotations of MWE
?about to? in each of the four versions of PTB. In
(a), ?about/RB? is annotated incorrectly, which is
corrected in (b). In (c), ?-B? indicates the beginning
token of an MWE and ?-I? indicates an inside posi-
tion of an MWE. In (d), ?about to? is annotated as
an RB (we omit the POS tags for its internal words,
which are IN and TO).
We use a CRF-based tagger for training and test
on all the four PTB versions. Our CRF can han-
dle ?words with spaces? (e.g. ?about to? as a single
token as well as separated tokens) as shown in Fig-
ure 2. This extension is only relevant to the case of
the (d) MWE version.
Table 3 summarizes the set of feature templates
used in the experiments. In Table 3, ?Head POS?
means the POS tag of the beginning token of an
MWE. In the same way, ?Tail POS? means the POS
tag of the last token of an MWE. For example, for
?a lot of /DT?, its Head POS is DT and its Tail POS
is IN.
We evaluate POS tagging accuracy and MWE
recognition accuracy. In POS evaluation, each to-
ken receives a tag in the cases of (a), (b) and (c), so
the tagging accuracy is straightforwardly calculated.
Table 3: Feature templates used in CRF training
Unigram features
Surface form
FPOS, Surface form
CPOS, Surface form
Bigram features (left context / right context)
Surface form / FPOS, Surface form
FPOS, Surface form / Surface form
Tail POS, Surface form / Head POS, Surface form
Surface form / Head POS
Tail POS / Head POS
Tail POS / Surface form
In the case of (d), since MWEs are analyzed as sin-
gle words, they are expanded into the internal words
with their POS tags and the evaluated on the token
basis.
MWE recognition accuracy is evaluated for the
cases of (c) and (d). For the purpose of comparison,
we employ a simple baseline as well. This baseline
assigns each occurrence of an MWE its most fre-
quent usage in the training part of PTB. Evaluation
of MWE recognition accuracy is shown in precision,
recall and F-measure.
We use the standard set of features based on uni-
gram/bi-gram of words/POS. For our MWE version,
we add the word forms and POS tags of the first and
the last internal words of MWEs as shown in Ta-
ble 3.
5.2 Experimental Results
Table 4 shows the results of POS tagging. A slight
improvement is observed in (b) compared with (a)
because some of inconsistent tags are corrected.
Further improvement is achieved in (d). The exper-
iment on (c) does not show improvement even over
142
Figure 3: Example of errors: ?after all /RB? and ?a /DT bit /JJ.?
Table 4: Per token accuracy (precision)
Version Accuracy
(a) Original 97.54
(b) Revised 97.56
(c) BIO MWE 97.32
(d) split MWE 97.62
Table 5: Recognition performance of MWEs
Precision Recall F-measure
Baseline 78.79 80.26 79.51
(c) BIO 92.81 90.90 90.18
(d) MWE 95.75 97.16 96.45
(a). The reason may attribute to the data sparseness
caused by the increased size of POS tags.
Table 5 shows the results of MWE recognition.
Our MWE-aware CRF model (d) shows the best re-
sults. While the BIO model (c) significantly outper-
forms the baseline, it gives significantly lower re-
sults than our model.
We investigated errors in (d) and categorized them
into three types.
? False Positive: System finds an MWE, while it
is actually literal.
? False Negative: System misses to identify an
MWE.
? Misrecognition: System finds an MWE
wrongly (correct answer is another MWE).
Table 6 shows number of recognition errors of
MWEs.
An example of the False Positive is ?a bit /RB? in
Figure 3, which actually is a literal usage and should
be tagged as ?a /DT, bit /NN?.
An example of the False Negative is ?in black and
white /RB?, which is not recognized as an MWE.
One reason of this type of errors is low or zero fre-
quency of such MWEs in training data. ?after all
/RB? (in Figure 3) is another False Negative exam-
ple.
Table 6: Recognition error of MWEs
Error types # of errors
False Positives 33
False Negatives 19
Misrecognition 17
One example of Misrecognition errors stems from
ambiguous MWEs. For example, while ?how much?
only has MWE usages as RB, there are two RB
usages of ?how much? that have different POS
tag sequences for the internal words. Other ex-
amples of Misrecognition are due to zero or low
frequency MWEs, whose substrings also matches
shorter MWEs: ?quite/RB, a few/PRP? while cor-
rect analysis is ?quite a few/RB?, and ?the hell /RB,
out of /IN? while the correct analysis is ?the hell out
of /RB?.
6 Conclusion and Future work
This paper presented our ongoing project for con-
struction of an English MWE dictionary, and its ap-
plication to MWE-aware POS tagging. The exper-
imental results show that the MWE-aware tagger
achieved better performance on POS tagging and
MWE recognition. Although our current MWE dic-
tionary only covers fixed types of functional MWEs,
this dictionary and MWE annotation information on
PTB will be made publicly available.
We plan to handle a wider range of MWEs such as
phrasal verbs and other semi-fixed and syntactically
flexible MWEs, and to develop a POS tagger and a
syntactic parser on top of them.
References
Matthieu Constant and Anthony Sigogne. 2011. MWU-
Aware Part-of-Speech Tagging with a CRF Model and
Lexical Resources. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Generation
to the Real World, MWE ?11, pages 49?56.
143
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative Strategies to Integrate Mul-
tiword Expression Recognition and Parsing. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics, ACL ?12, pages 204?
212.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting Multi-Word Expressions improves Word Sense
Disambiguation. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Generation
to the Real World, MWE ?11, pages 20?24.
Spence Green, Marie-Catherine deMarneffe, John Bauer,
and Christopher D Manning. 2011. Multiword Ex-
pression Identification with Tree Substitution Gram-
mars: A Parsing tour de force with French. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?11, pages
725?735.
Nicole Gre?goire. 2007. Design and Implementation of
a Lexicon of Dutch Multiword Expressions. In Pro-
ceedings of the Workshop on a Broader Perspective on
Multiword Expressions, MWE ?07, pages 17?24.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
Recognising Multiword Expressions Improve Shallow
Parsing? In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 636?644.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?04, pages 230?237.
Eric Laporte and Stavroula Voyatzi. 2008. An Electronic
Dictionary of French Multiword Adverbs. In Lan-
guage Resources and Evaluation Conference. Work-
shop Towards a Shared Task for Multiword Expres-
sions, MWE ?08, pages 31?34.
Eric Laporte, Takuya Nakamura, and Stavroula Voy-
atzi. 2008. A French Corpus Annotated for Mul-
tiword Nouns. In Proceedings of the Language Re-
sources and Evaluation Conference. Workshop To-
wards a Shared Task on Multiword Expressions, MWE
?08, pages 27?30.
Joakim Nivre and Jens Nilsson. 2004. Multiword Units
in Syntactic Parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications, MEMURA ?04, pages 39?46.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann A
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of the Third International Conference on Com-
putational Linguistics and Intelligent Text Processing,
CICLing ?02, pages 1?15.
Kosho Shudo, Akira Kurahone, and Toshifumi Tanabe.
2011. A Comprehensive Dictionary of Multiword Ex-
pressions. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, HLT ?11, pages 161?
170.
144
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 503?511,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Hidden Markov Tree Model for Word Alignment
Shuhei Kondo Kevin Duh Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara, 630-0192, Japan
{shuhei-k,kevinduh,matsu}@is.naist.jp
Abstract
We propose a novel unsupervised word
alignment model based on the Hidden
Markov Tree (HMT) model. Our model
assumes that the alignment variables have
a tree structure which is isomorphic to the
target dependency tree and models the dis-
tortion probability based on the source de-
pendency tree, thereby incorporating the
syntactic structure from both sides of the
parallel sentences. In English-Japanese
word alignment experiments, our model
outperformed an IBM Model 4 baseline
by over 3 points alignment error rate.
While our model was sensitive to poste-
rior thresholds, it also showed a perfor-
mance comparable to that of HMM align-
ment models.
1 Introduction
Automatic word alignment is the first step in the
pipeline of statistical machine translation. Trans-
lation models are usually extracted from word-
aligned bilingual corpora, and lexical translation
probabilities based on word alignment models are
also used for translation.
The most widely used models are the IBM
Model 4 (Brown et al, 1993) and Hidden Markov
Models (HMM) (Vogel et al, 1996). These mod-
els assume that alignments are largely monotonic,
possibly with a few jumps. While such assump-
tion might be adequate for alignment between sim-
ilar languages, it does not necessarily hold be-
tween a pair of distant languages like English and
Japanese.
Recently, several models have focused on in-
corporating syntactic structures into word align-
ment. As an extension to the HMM alignment,
Lopez and Resnik (2005) present a distortion
model conditioned on the source-side dependency
tree, and DeNero and Klein (2007) propose a
distortion model based on the path through the
source-side phrase-structure tree. Some super-
vised models receive syntax trees as their input
and use them to generate features and to guide the
search (Riesa and Marcu, 2010; Riesa et al, 2011),
and other models learn a joint model for pars-
ing and word alignment from word-aligned par-
allel trees (Burkett et al, 2010). In the context of
phrase-to-phrase alignment, Nakazawa and Kuro-
hashi (2011) propose a Bayesian subtree align-
ment model trained with parallel sampling. None
of these models, however, can incorporate syntac-
tic structures from both sides of the language pair
and can be trained computationally efficiently in
an unsupervised manner at the same time.
The Hidden Markov Tree (HMT) model
(Crouse et al, 1998) is one such model that sat-
isfies the above-mentioned properties. The HMT
model assumes a tree structure of the hidden vari-
ables, which fits well with the notion of word-to-
word dependency, and it can be trained from un-
labeled data via the EM algorithm with the same
order of time complexity as HMMs.
In this paper, we propose a novel word align-
ment model based on the HMT model and show
that it naturally enables unsupervised training
based on both source and target dependency trees
in a tractable manner. We also compare our HMT
word alignment model with the IBM Model 4 and
the HMM alignment models in terms of the stan-
dard alignment error rates on a publicly available
English-Japanese dataset.
2 IBM Model 1 and HMM Alignment
We briefly review the IBM Model 1 (Brown et
al., 1993) and the Hidden Markov Model (HMM)
word alignment (Vogel et al, 1996) in this section.
Both are probabilistic generative models that fac-
503
tor as
p(f |e) =
?
a
p(a, f |e)
p(a, f |e) =
J?
j=1
pd(aj |aj )pt(fj |eaj )
where e = {e1, ..., eI} is an English (source) sen-
tence and f = {f1, ..., fJ} is a foreign (target)
sentence. a = {a1, ..., aJ} is an alignment vec-
tor such that aj = i indicates the j-th target word
aligns to the i-th source word and aj = 0 means
the j-th target word is null-aligned. j is the index
of the last non null-aligned target word before the
index j.
In both models, pt(fj |eaj ) is the lexical transla-
tion probability and can be defined as conditional
probability distributions. As for the distortion
probability pd(aj |aj ), pd(aj = 0|aj = i?) = p0
where p0 is NULL probability in both models.
pd(aj = i|aj = i?) is uniform in the Model 1
and proportional to the relative count c(i ? i?) in
the HMM for i 6= 0. DeNero and Klein (2007)
proposed a syntax-sensitive distortion model for
the HMM alignment, in which the distortion prob-
ability depends on the path from the i-th word to
the i?-th word on the source-side phrase-structure
tree, instead of the linear distance between the two
words.
These models can be trained efficiently using
the EM algorithm. In practice, models in two di-
rections (source to target and target to source) are
trained and then symmetrized by taking their in-
tersection, union or using other heuristics. Liang
et al (2006) proposed a joint objective of align-
ment models in both directions and the probability
of agreement between them, and an EM-like algo-
rithm for training.
They also proposed posterior thresholding for
decoding and symmetrization, which take
a = {(i, j) : p(aj = i|f , e) > ?}
with a threshold ? . DeNero and Klein (2007) sum-
marized some criteria for posterior thresholding,
which are
? Soft-Union
?
pf (aj = i|f , e) ? pr(ai = j|f , e)
? Soft-Intersection
pf (aj = i|f , e) + pr(ai = j|f , e)
2
? Hard-Union
max(pf (aj = i|f , e), pr(ai = j|f , e))
? Hard-Intersection
min(pf (aj = i|f , e), pr(ai = j|f , e))
where pf (aj = i|f , e) is the alignment probabil-
ity under the source-to-target model and pr(ai =
j|f , e) is the one under the target-to-source model.
They also propose a posterior decoding heuris-
tic called competitive thresholding. Given a j ? i
matrix of combined weights c and a threshold ? , it
choose a link (j, i) only if its weight cji ? ? and it
is connected to the link with the maximum weight
both in row j and column i.
3 Hidden Markov Tree Model
The Hidden Markov Tree (HMT) model was first
introduced by Crouse et al (1998). Though it has
been applied successfully to various applications
such as image segmentation (Choi and Baraniuk,
2001), denoising (Portilla et al, 2003) and biol-
ogy (Durand et al, 2005), it is largely unnoticed
in the field of natural language processing. To
the best of our knowledge, the only exception is
Z?abokrtsky` and Popel (2009) who used a variant
of the Viterbi algorithm for HMTs in the transfer
phase of a deep-syntax based machine translation
system.
An HMT model consists of an observed random
tree X = {x1, ..., xN} and a hidden random tree
S = {s1, ..., sN}, which is isomorphic to the ob-
served tree.
The parameters of the model are
? P (s1 = j), the initial hidden state prior
? P (st = j|s?(t) = i), transition probabilities
? P (xt = h|st = j), emission probabilities,
where ?() is a function that maps the index of a
hidden node to the index of its parent node. These
parameters can be trained via the EM algorithm.
The ?upward-downward? algorithm proposed
in Crouse et al (1998), an HMT analogue of the
forward-backward algorithm for HMMs, can be
used in the E-step. However, it is based on the de-
composition of joint probabilities and suffers from
numerical underflow problems.
Durand et al (2004) proposed a smoothed vari-
ant of the upward-downward algorithm, which is
504
based on the decomposition of smoothed probabil-
ities and immune to underflow. In the next section,
we will explain this variant in the context of word
alignment.
4 Hidden Markov Tree Word Alignment
We present a novel word alignment model based
on the HMT model. Given a target sentence f =
{f1, ..., fJ}with a dependency treeF and a source
sentence e = {e1, ..., eI} with a dependency tree
E, an HMT word alignment model factors as
p(f |e) =
?
a
p(a, f |e)
p(a, f |e) =
J?
j=1
pd(aj |aj )pt(fj |eaj ).
While these equations appear identical to the ones
for the HMM alignment, they are different in that
1) e, f and a are not chain-structured but tree-
structured, and 2) j is the index of the non null-
aligned lowest ancestor of the j-th target word1,
rather than that of the last non null-aligned word
preceding the j-th word as in the HMM alignment.
Note that A, the tree composed of alignment vari-
ables a = {a1, ..., aJ}, is isomorphic to the target
dependency tree F.
Figure 1 shows an example of a target depen-
dency tree with an alignment tree, and a source
dependency tree. Note that English is the target
(or foreign) language and Japanese is the source
(or English) language here. We introduce the fol-
lowing notations following Durand et al (2004),
slightly modified to better match the context of
word alignment.
? ?(j) denotes the index of the head of the j-th
target word.
? c(j) denotes the set of indices of the depen-
dents of the j-th target word.
? Fj = f j denotes the target dependency sub-
tree rooted at the j-th word.
As for the parameters of the model, the initial
hidden state prior described in Section 3 can be
defined by assuming an artificial ROOT node for
both dependency trees, forcing the target ROOT
node to be aligned only to the source ROOT
1This dependence on aj can be implemented as a first-
order HMT, analogously to the case of the HMM alignment
(Och and Ney, 2003).
node and prohibiting other target nodes from be-
ing aligned to the source ROOT node. The lexi-
cal translation probability pt(fj |eaj ), which corre-
sponds to the emission probability, can be defined
as conditional probability distributions just like in
the IBM Model 1 and the HMM alignment.
The distortion probability pd(aj = i|aj = i?),
which corresponds to the transition probability,
depends on the distance between the i-th source
word and the i?-th source word on the source de-
pendency tree E, which we denote d(i, i?) here-
after. We model the dependence of pd(aj =
i|aj = i?) on d(i, i?) with the counts c(d(i, i?)).
In our model, d(i, i?) is represented by a pair
of non-negative distances (up, down), where up
is the distance between the i-th word and the
lowest common ancestor (lca) of the two words,
down is the one between the i?-th word and the
lca. For example in Figure 1b, d(0, 2) = (0, 4),
d(2, 5) = (2, 2) and d(4, 7) = (3, 0). In practice,
we clip the distance by a fixed window size w and
store c(d(i, i?)) in a two-dimensional (w + 1 ) ?
(w + 1 ) matrix. When w = 3, for example, the
distance d(0, 2) = (0, 3) after clipping.
We can use the smoothed variant of upward-
downward algorithm (Durand et al, 2004) for the
E-step of the EM algorithm. We briefly explain
the smoothed upward-downward algorithm in the
context of tree-to-tree word alignment below. For
the detailed derivation, see Durand et al (2004).
In the smoothed upward-downward algorithm,
we first compute the state marginal probabilities
p(aj = i)
=
?
i?
p(a?(j) = i?)pd(aj = i|a?(j) = i?)
for each target node and each state, where
pd(aj = i|a?(j) = i?) = p0
if the j-th word is null-aligned, and
pd(aj = i|a?(j) = i?)
= (1? p0) ?
c(d(i?, i))?
i?? 6=0 c(d(i?, i??))
if the j-th word is aligned. Note that we must ar-
tificially normalize pd(aj = i|a?(j) = i?), because
unlike in the case of the linear distance, multiple
words can have the same distance from the j-th
word on a dependency tree.
505
a0 a1 a2 a3 a4 a5
? ? ? ? ? ?
?ROOT?f0 Thatf1 wasf2 Mountf3 Kuramaf4 .f5
(a) Target sentence with its dependency/alignment tree. Target words {f0, ..., f5} are emitted from
alignment variables {a0, ..., a5}. Ideally, a0 = 0, a1 = 1, a2 = 7, a3 = 5, a4 = 4 and a5 = 9.
?ROOT?e0 ??e1 ?e2 ?e3 ??e4 ?e5 ?e6 ??e7 ?e8 ?e9
that mountain Kurama mountain be .
(b) Source sentence with its dependency tree. None of the target words are aligned to e2, e3, e6 and e8.
Figure 1: An example of sentence pair under the Hidden Markov Tree word alignment model. If we
ignore the source words to which no target words are aligned, the dependency structures look similar to
each other.
In the next phase, the upward recursion, we
compute p(aj = i|Fj = f j) in a bottom-up man-
ner. First, we initialize the upward recursion for
each leaf by
?j(i) = p(aj = i|Fj = fj)
= pt(fj |ei)p(aj = i)Nj
,
where
Nj = p(Fj = fj) =
?
i
pt(fj |ei)p(aj = i).
Then, we proceed from the leaf to the root with the
following recursion,
?j(i) = p(aj = i|Fj = f j)
=
{?j??c(j) ?j,j?(i)}pt(fj |ei)p(aj = i)
Nj
,
where
Nj =
p(Fj = f j)?
j??c(j) p(Fj? = f j?)
=
?
i
{
?
j??c(j)
?j,j?(i)}pt(fj |ei)p(aj = i)
and
??(j),j(i) =
p(Fj = f j |a?(j) = i)
p(Fj = f j)
=
?
i?
?j(i?)pd(aj = i?|a?(j) = i)
p(aj = i?)
.
After the upward recursion is completed, we
compute p(aj = i|F0 = f0) in the downward
recursion. It is initialized at the root node by
?0(i) = p(a0 = i|F0 = f0).
Then we proceed in a top-down manner, comput-
ing
?j(i) = p(aj = i|F0 = f0)
= ?j(i)p(aj = i)
?
?
i?
pd(aj = i|a?(j) = i?)??(j)(i?)
??(j),j(i?)
.
for each node and each state.
The conditional probabilities
p(aj = i, a?(j) = i?|F0 = f0)
=
?j(i)pd(aj = i|a?(j) = i?)??(j)(i?)
p(aj = i)??(j),j(i?)
,
506
which is used for the estimation of distortion prob-
abilities, can be extracted during the downward re-
cursion.
In the M-step, the lexical translation model can
be updated with
pt(f |e) =
c(f, e)
c(e) ,
just like the IBM Models and HMM alignments,
where c(f, e) and c(e) are the count of the word
pair (f, e) and the source word e. However, the
update for the distortion model is a bit compli-
cated, because the matrix that stores c(d(i, i?))
does not represent a probability distribution. To
approximate the maximum likelihood estimation,
we divide the counts c(d(i, i?)) calculated during
the E-step by the number of distortions that have
the distance d(i, i?) in the training data. Then we
normalize the matrix by
c(d(i, i?)) = c(d(i, i
?))?w
i=0
?w
i?=0 c(d(i, i?))
.
Given initial parameters for the lexical trans-
lation model and the distortion counts, an HMT
aligner collects the expected counts c(f, e), c(e)
and c(d(i, i?)) with the upward-downward algo-
rithm in the E-step and re-estimate the parameters
in the M-Step. Dependency trees for the sentence
pairs in the training data remain unchanged during
the training procedure.
5 Experiment
We evaluate the performance of our HMT align-
ment model in terms of the standard alignment er-
ror rate2 (AER) on a publicly available English-
Japanese dataset, and compare it with the IBM
Model 4 (Brown et al, 1993) and HMM alignment
with distance-based (HMM) and syntax-based (S-
HMM) distortion models (Vogel et al, 1996;
Liang et al, 2006; DeNero and Klein, 2007).
We use the data from the Kyoto Free Transla-
tion Task (KFTT) version 1.3 (Neubig, 2011). Ta-
ble 1 shows the corpus statistics. Note that these
numbers are slightly different from the ones ob-
served under the dataset?s default training proce-
dure because of the difference in the preprocessing
scheme, which is explained below.
2Given sure alignments S and possible alignments P , the
alignment error rate of alignments A is 1 ? |A?S|+|A?P ||A|+|S|
(Och and Ney, 2003).
The tuning set of the KFTT has manual align-
ments. As the KFTT doesn?t distinguish between
sure and possible alignments, F-measure equals
1?AER on this dataset.
5.1 Preprocessing
We tokenize the English side of the data using the
Stanford Tokenizer3 and parse it with the Berkeley
Parser4 (Petrov et al, 2006). We use the phrase-
structure trees for the Berkeley Aligner?s syntactic
distortion model, and convert them to dependency
trees for our dependency-based distortion model5.
As the Berkeley Parser couldn?t parse 7 (out of
about 330K) sentences in the training data, we re-
moved those lines from both sides of the data. All
the sentences in the other sets were parsed suc-
cessfully.
For the Japanese side of the data, we first con-
catenate the function words in the tokenized sen-
tences using a script6 published by the author
of the dataset. Then we re-segment and POS-
tag them using MeCab7 version 0.996 and parse
them using CaboCha8 version 0.66 (Kudo and
Matsumoto, 2002), both with UniDic. Finally,
we modify the CoNLL-format output of CaboCha
where some kind of symbols such as punctuation
marks and parentheses have dependent words. We
chose this procedure for a reasonable compromise
between the dataset?s default tokenization and the
dependency parser we use.
As we cannot use the default gold alignment due
to the difference in preprocessing, we use a script9
published by the author of the dataset to modify
the gold alignment so that it better matches the
new tokenization.
5.2 Training
We initialize our models in two directions with
jointly trained IBM Model 1 parameters (5 itera-
tions) and train them independently for 5 iterations
3http://nlp.stanford.edu/software/
4We use the model trained on the WSJ portion of
Ontonotes (Hovy et al, 2006) with the default setting.
5We use Stanford?s tool (de Marneffe et al, 2006)
with options -conllx -basic -makeCopulaHead
-keepPunct for conversion.
6https://github.com/neubig/
util-scripts/blob/master/
combine-predicate.pl
7http://code.google.com/p/mecab/
8http://code.google.com/p/cabocha/
9https://github.com/neubig/
util-scripts/blob/master/
adjust-alignments.pl
507
Sentences English Tokens Japanese Tokens
Train 329,974 5,912,543 5,893,334
Dev 1,166 24,354 26,068
Tune 1,235 30,839 33,180
Test 1,160 26,730 27,693
Table 1: Corpus statistics of the KFTT.
Precision Recall AER
HMT (Proposed) 71.77 55.23 37.58
IBM Model 4 60.58 57.71 40.89
HMM 69.59 56.15 37.85
S-HMM 71.60 56.14 37.07
Table 2: Alignment error rates (AER) based on
each model?s peak performance.
with window size w = 4 for the distortion model.
The entire training procedure takes around 4 hours
on a 3.3 GHz Xeon CPU.
We train the IBM Model 4 using GIZA++ (Och
and Ney, 2003) with the training script of the
Moses toolkit (Koehn et al, 2007).
The HMM and S-HMM alignment models are
initialized with jointly trained IBM Model 1 pa-
rameters (5 iterations) and trained independently
for 5 iterations using the Berkeley Aligner. We
find that though initialization with jointly trained
IBM Model 1 parameters is effective, joint train-
ing of HMM alignment models harms the perfor-
mance on this dataset (results not shown).
5.3 Result
We use posterior thresholding for the HMT and
HMM alignment models, and the grow-diag-final-
and heuristic for the IBM Model 4.
Table 2 and Figure 2 show the result. As
the Soft-Union criterion performed best, we don?t
show the results based on other criteria. On the
other hand, as the peak performance of the HMT
model is better with competitive thresholding and
those of HMM models are better without it, we
compare Precision/Recall curves and AER curves
both between the same strategy and the best per-
forming strategy for each model.
As shown in Table 2, the peak performance of
the HMT alignment model is better than that of
the IBM Model 4 by over 3 point AER, and it was
somewhere between the HMM and the S-HMM.
Taking into account that our distortion model is
simpler than that of S-HMM, these results seem
natural, and it would be reasonable to expect that
replacing our distortion model with more sophisti-
cated one might improve the performance.
When we look at Precision/Recall curves and
AER curves in Figures 2a and 2d, the HMT model
is performing slightly better in the range of 50 to
60 % precision and 0.15 to 0.35 posterior thresh-
old with the Soft-Union strategy. Results in Fig-
ures 2b and 2e show that the HMT model performs
better around the range around 60 to 70 precision
and it corresponds to 0.2 to 0.4 posterior thresh-
old with the competitive thresholding heuristic. In
addition, results on both strategies show that per-
formance curve of the HMT model is more peaked
than those of HMM alignment models.
We suspect that a part of the reason behind such
behavior can be attributed to the fact that the HMT
model?s distortion model is more uniform than that
of HMM models. For example, in our model, all
sibling nodes have the same distortion probability
from their parent node. This is in contrast with the
situation in HMM models, where nodes within a
fixed distance have different distortion probabili-
ties. With more uniform distortion probabilities,
many links for a target word may have a consider-
able amount of posterior probability. If that is true,
too many links will be above the threshold when it
is set low, and too few links can exceed the thresh-
old when it is set high. More sophisticated distor-
tion model may help mitigate such sensitivity to
the posterior threshold.
6 Related Works
Lopez and Resnik (2005) consider an HMM
model with distortions based on the distance in
dependency trees, which is quite similar to our
model?s distance. DeNero and Klein (2007) pro-
pose another HMM model with syntax-based dis-
tortions based on the path through constituency
trees, which improves translation rule extraction
for tree-to-string transducers. Both models as-
508
(a) Precision/Recall Curve with Soft-Union. (b) Precision/Recall Curve with Soft-Union + Competi-
tive Thresholding.
(c) Precision/Recall Curve with the Best Strategy. (d) Alignment Error Rate with Soft-Union.
(e) Precision/Recall Curve with Soft-Union + Competi-
tive Thresholding.
(f) Alignment Error Rate with with the Best Strategy.
Figure 2: Precision/Recall Curve and Alignment Error Rate with Different Models and Strategies.
509
sume a chain structure for hidden variables (align-
ment) as opposed to a tree structure as in our
model, and condition distortions on the syntactic
structure only in one direction.
Nakazawa and Kurohashi (2011) propose
a dependency-based phrase-to-phrase alignment
model with a sophisticated generative story, which
leads to an increase in computational complexity
and requires parallel sampling for training.
Several supervised, discriminative models use
syntax structures to generate features and to guide
the search (Burkett et al, 2010; Riesa and Marcu,
2010; Riesa et al, 2011). Such efforts are orthog-
onal to ours in the sense that discriminative align-
ment models generally use statistics obtained by
unsupervised, generative models as features and
can benefit from their improvement. It would be
interesting to incorporate statistics of the HMT
word alignment model into such discriminative
models.
Z?abokrtsky` and Popel (2009) use HMT mod-
els for the transfer phase in a tree-based MT sys-
tem. While our model assumes that the tree struc-
ture of alignment variables is isomorphic to tar-
get side?s dependency tree, they assume that the
deep-syntactic tree of the target side is isomorphic
to that of the source side. The parameters of the
HMT model is given and not learned by the model
itself.
7 Conclusion
We have proposed a novel word alignment model
based on the Hidden Markov Tree (HMT) model,
which can incorporate the syntactic structures of
both sides of the language into unsupervised word
alignment in a tractable manner. Experiments on
an English-Japanese dataset show that our model
performs better than the IBM Model 4 and com-
parably to the HMM alignment models in terms
of alignment error rates. It is also shown that the
HMT model with a simple tree-based distortion
is sensitive to posterior thresholds, perhaps due to
the flat distortion probabilities.
As the next step, we plan to improve the dis-
tortion component of our HMT alignment model.
Something similar to the syntax-sensitive distor-
tion model of DeNero and Klein (2007) might be
a good candidate.
It is also important to see the effect of our
model on the downstream translation. Apply-
ing our model to recently proposed models that
directly incorporate dependency structures, such
as string-to-dependency (Shen et al, 2008) and
dependency-to-string (Xie et al, 2011) models,
would be especially interesting.
Last but not least, though the dependency struc-
tures don?t pose a hard restriction on the align-
ment in our model, it is highly likely that parse
errors have negative effects on the alignment ac-
curacy. One way to estimate the effect of parse
errors on the accuracy is to parse the input sen-
tences with inferior models, for example trained
on a limited amount of training data. Moreover,
preserving some ambiguities using k-best trees or
shared forests might help mitigate the effect of 1-
best parse errors.
Acknowledgments
We thank anonymous reviewers for insightful sug-
gestions and comments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational linguistics,
19(2):263?311.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint Parsing and Alignment with Weakly Synchro-
nized Grammars. In Proceedings of NAACL HLT
2010, pages 127?135.
Hyeokho Choi and Richard G. Baraniuk. 2001. Mul-
tiscale Image Segmentation Using Wavelet-Domain
Hidden Markov Models. IEEE Transactions on Im-
age Processing, 10(9):1309?1321.
Matthew S. Crouse, Robert D. Nowak, and Richard G.
Baraniuk. 1998. Wavelet-Based Statistical Signal
Processing Using Hidden Markov Models. IEEE
Transactions on Signal Processing, 46(4):886?902.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC?06, pages 449?454.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of ACL 2007, pages 17?24.
Jean-Baptiste Durand, Paulo Gonc?alve`s, and Yann
Gue?don. 2004. Computational Methods for Hid-
den Markov Tree Models-An Application to Wavelet
Trees. IEEE Transactions on Signal Processing,
52(9):2551?2560.
510
J.-B. Durand, Y. Gue?don, Y. Caraglio, and E. Costes.
2005. Analysis of the plant architecture via tree-
structured statistical models: the hidden Markov tree
models. New Phytologist, 166(3):813?825.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
HLT-NAACL 2006, Short Papers, pages 57?60.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL 2007, Demonstration Ses-
sion, pages 177?180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of CoNLL-2002, pages 63?69.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of HLT-NAACL
2006, pages 104?111.
Adam Lopez and Philip Resnik. 2005. Im-
proved HMM Alignment Models for Languages
with Scarce Resources. In Proceedings of the ACL
Workshop on Building and Using Parallel Texts,
pages 83?86.
Toshiaki Nakazawa and Sadao Kurohashi. 2011.
Bayesian Subtree Alignment Model based on De-
pendency Trees. In Proceedings of IJCNLP 2011,
pages 794?802.
Graham Neubig. 2011. The Kyoto Free Translation
Task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational linguistics, 29(1):19?51.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING/ACL 2006, pages 433?440.
Javier Portilla, Vasily Strela, Martin J. Wainwright,
and Eero P. Simoncelli. 2003. Image Denoising
Using Scale Mixtures of Gaussians in the Wavelet
Domain. IEEE Transactions on Image Processing,
12(11):1338?1351.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
Search for Word Alignment. In Proceedings of ACL
2010, pages 157?166.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-Rich Language-Independent Syntax-Based
Alignment for Statistical Machine Translation. In
Proceedings of EMNLP 2011, pages 497?507.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-Based Word Alignment in Statisti-
cal Translation. In Proceedings of COLING 1996,
pages 836?841.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A
Novel Dependency-to-String Model for Statistical
Machine Translation. In Proceedings of EMNLP
2011, pages 216?226.
Zdene?k Z?abokrtsky` and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of ACL-IJCNLP 2009,
Short Papers, pages 145?148.
511
