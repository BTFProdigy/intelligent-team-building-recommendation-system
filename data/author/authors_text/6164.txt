Improved Iterative Scaling can yield multiple globally optimal
models with radically diering performance levels
Iain Bancarz and Miles Osborne
fiainrb,osborneg@cogsci.ed.ac.uk
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland
Abstract
Log-linear models can be e?ciently estimated us-
ing algorithms such as Improved Iterative Scaling
(IIS)(Laerty et al, 1997). Under certain conditions
and for a particular class of problems, IIS is guaran-
teed to approach both the maximum-likelihood and
maximum entropy solution. This solution, in like-
lihood space, is unique. Unfortunately, in realistic
situations, multiple solutions may exist, all of which
are equivalent to each other in terms of likelihood,
but radically dierent from each other in terms of
performance. We show that this behaviour can occur
when a model contains overlapping features and the
training material is sparse. Experimental results,
from the domain of parse selection for stochastic at-
tribute value grammars, shows the wide variation
in performance that can be found when estimating
models using IIS. Further results show that the in-
uence of the initial model can be diminished by
selecting either uniform weights, or else by model
averaging.
1 Background
When statistically modelling linguistic phenomena
of one sort or another, researchers typically t log-
linear models to the data (for example (Johnson et
al., 1999)). There are (at least) three reasons for
the popularity of such models: they do not make un-
warranted independence assumptions, the maximum
likelihood solution of such models coincides with the
maximum entropy solution, and nally, they can be
e?ciently estimated (using algorithms such as Im-
proved Iterative Scaling (IIS) (Laerty et al, 1997)).
Now, the solution found by IIS is guaranteed to
approach a global maximum for both likelihood and
entropy under certain conditions. Although this is
appealing, in realistic situations it turns out that
multiple models exist, all of which are equivalent in
terms of likelihood but dierent from each other in
terms of their performance at some task. In particu-
lar, the initial weight settings can inuence the qual-
ity of the nal model, even though this nal model
is the maximum entropy solution (as found by IIS).
At rst glance, this seems very strange. The IIS
algorithm is guaranteed to converge to a globally op-
timal solution regardless of the initial parameters.
If the initial weights assigned to some of the fea-
tures are wildly inappropriate then the algorithm
may take longer to converge, but one would expect
the nal destination to remain the same. However,
as we show later, what is unique in terms of likeli-
hood need not be unique in terms of performance,
and so IIS can be sensitive to the initial weight set-
tings.
Some of the reason for this behaviour may lie
in a relatively subtle eect which we call overlap-
ping features.
1
If some features behave identically in
the training set (but not necessarily in future sam-
ples), IIS cannot distinguish between dierent sets
of weights for those features unless the sum of all
weights in each set is dierent. In fact, the nal
weights assigned to such features will be dependent
on their initial values. Under these conditions, there
will be a family of models, all of which are identical
as far as IIS is concerned, but distinguishable from
each other in terms of performance. This means that
in terms of performance space, the landscape will
contain local maxima. In terms of likelihood space,
the landscape will continue to contain just the single
(global) maximum.
This indeterminacy is clearly undesirable. How-
ever, there are (at least) two practical ways of deal-
ing with it: one either initialises IIS with weights
that are identical (0 is a good choice), or else one
takes a Bayesian approach, and averages over the
space of models. In this paper, we show that the
performance of IIS is sensitive to the initial weight
settings. We show that setting the weights to zero
yields performance that is better than a large set
of randomly initialised models. Also, we show that
model averaging can also reduce indeterminacies in-
troduced through the choice of initial weights.
The rest of this paper is as follows. Section
2 is a restatement of the theory behind IIS. Sec-
tion 3 shows how sparse training material, coupled
1
We do not claim that overlapping features are the sole
reason for our observed eects. As an anonymous reviewer
noted, sparse statistics may also yield similar ndings.
with large models can result in situations whereby
IIS produces suboptimal results. We then move
on to experimental support for our theoretical re-
sults. Our domain is parse selection for broad-
coverage stochastic attribute-value grammars. Sec-
tion 4 shows how we model the broad coverage
attribute-value grammar (mentioned in section 5
used in our experiments), and then present our re-
sults. The rst set of experiments (section 7) deals
with how well IIS, with uniform initial settings. out-
performs models with randomised initial settings.
The second set of experiments shows how model
averging can deal with the problem of initialising the
model. We conclude the paper with some comments
on our work.
2 The Duality Lemma
How can we be certain that IIS seeks out a global
maximum for both likelihood and entropy? The an-
swer is that for the class of problems under consider-
ation, there is only a single maximum - which is then
necessarily the global one. The IIS algorithm sim-
ply `hill-climbs' and seeks to increase the likelihood
of the model being trained. Its success is guaranteed
because of a result which we refer to as the Duality
Lemma
2
.
The proof of the Duality Lemma is contained in
(Laerty et al, 1997) but will be omitted here. In
order to state the lemma, we rst dene the setting
and establish some notation.
Suppose that we have a probability measure space
(
;F ;P), where as usual 
 is a set made up of ele-
ments !, F is a sigma-eld on 
, and P is a prob-
ability measure on F . Suppose further that X is a
simple random variable on 
 - that is to say, it is a
real-valued function on 
, having nite range, and
such that [! : X(!) = x] 2 F . As usual, we will omit
the argument !, so that X indicates a general value
of the function as well as the function itself, and x
denotes [! : X(!) = x]. We will also abuse notation
by letting X denote the set of possible values of x;
the meaning should be clear from context.
We now consider a stochastic process on (
;F ; P ).
We are given a sample of past outputs (data points)
from this process which make up the training data.
We again abuse notation by using ~p to refer to both
the set of data points and the distribution dened by
that set. Let  denote the set of all possible proba-
bility distributions over X ; we seek a model q

2 
which is in some sense the best possible probability
distribution over future outputs. We specically ex-
amine generalized Gibbs distributions, which are of
the form:
q
h
(x) =
1
(Z
q
(h))
exph(x)q(x) (1)
2
This result was called Proposition 4 (Laerty et al, 1997).
In this case, h is a real-valued function on X , q is an
initial probability distribution overX (which may be
the uniform distribution), and Z
q
(h) is a normalising
constant, taking a value such that
P
x2X
q
h
(x) = 1.
The function h here takes the form:
h(x) =
n
X
i=1

i
f
i
(x) = (  f)(x) (2)
where the f
i
(x) are integer feature functions. The
real numbers 
i
are adjustable parameters.
Suppose that we are given the data ~p, an initial
model q
0
, and a set of features f . Laerty et al
(1997) describe two natural sets of models. The rst
is the set P(f; ~p) of all distributions that agree with
~p as to the expected value of the feature function f :
P(f; ~p) = [p 2  : p [f ] = ~p [f ]] (3)
where, as usual, p [f ] denotes the expectation of the
function f under the distribution p.
The second is Q(f; q
0
), the set of generalized
Gibbs distributions based on q
0
and with feature set
f :
Q(f; q
0
) = [(  f) ? q
0
] (4)
Let Q denote the closure of Q in , with respect
to the topology  inherits as a subset of Euclidean
space.
These in turn determine two natural candidates
for the `best' model q

. Let D(pkq) denote the
Kullback-Leibler divergence between two distribu-
tions p and q. The suitable models are:
 Maximum Likelihood Gibbs Distribution. A dis-
tribution in Q with maximum likelihood with
respect to ~p: q
ML

= argmin
q2Q
D(~pkq).
 Maximum Entropy Constrained Distribution. A
distribution in P with maximum entropy rela-
tive to q
0
: q
ME

= argmin
q2P(f;~p)
D(pkq
0
)
The key result of Laerty et al (1997) is that
there is a unique q

satisfying q

= q
ML

= q
ME

.
In Appendix 1 of that paper, the following result is
proved:
The Duality Lemma. Suppose that D(~pkq
0
) <
1. Then there exists a unique q

2  satisfying:
1. q

2 P \ Q
2. D(pkq) = D(pkq

) +D(q

kq) 8p 2 P; q 2 Q
3. q

= argmin
q2Q
D(~pkq)
4. q

= argmin
q2P(f;~p)
D(pkq
0
)
The Duality Lemma is a very useful result, funda-
mental to the ability of IIS to converge upon the sin-
gle maximum likelihood, maximum entropy solution.
The IIS algorithm itself uses a fairly straightforward
technique to look for any maximum of the likelihood
function; because of the lemma, IIS is guaranteed to
approach the solution q

.
3 Limitations With Sparse Training
Data
3.1 Overlapping Features
In this paper, all models have the same training data
and a uniform initial distribution q
0
. This ensures
that D(pkq
0
)  1 and that, for all models, the IIS
algorithm approaches the same optimal distribution
q

. However, our experiments show that not all dis-
tributions obtained by running IIS (to convergence)
are equally good at modelling a set of test data. Per-
formance appears to depend (at least) on the start-
ing values for the weights 
i
. This may be a result
of the following situation:
Consider two features, f
i
and f
j
with i < j,
with weights 
i
and 
j
respectively. Suppose that
f
i
(x) = f
j
(x) for all values of x in ~p, but there exist
values of x outside ~p such that f
i
(x) 6= f
j
(x); that
is, the two functions take exactly the same values
on the set of training data but dier outside of it.
This phenomenon can be called overlapping.
3
Over-
lapping features are commonly found in maximum
entropy models, and are one of the main reasons for
their popularity. In fact, one could argue that all
maximum entropy models found in natural language
applications contain overlapping features. Overlap-
ping features may be present by explicit design (for
example when emulating backing-o smoothing), or
else naturally, for example when using features to
treat words co-occurring with each other.
We assign initial weights 
(0)
i
and 
(0)
j
to the fea-
tures, and after n iterations of the algorithm, they
have been adjusted to 
(n)
i
and 
(n)
j
respectively.
Now consider the target solution q

, as determined
by q
0
and p. Let us assume for convenience that q

is of the form:
q

=
1
Z

exp (
n
X
k=1


k
f
k
) (5)
where Z

is the usual normalising constant. Notice
that q

may not belong to the family of exponential
models Q, but instead may be part of the larger
set Q. Indeed, della Pietra et al state that P \
Q may be empty. This is not a serious limitation
as one can come arbitrarily close to any element of
Q while remaining inside Q. Thus, if q

=2 Q, we
may consider the above expression to be a very close
approximation to q

, such as could be obtained by
running IIS to convergence.
Because the ith and jth features are equal on the
training data, the exact values of 

i
and 

j
have
no eect on the likelihood of the model as long as
3
We can relax this denition of overlapping and allow fea-
tures to largely co-occur together. Depending upon the de-
gree of co-occurrence, we would expect to continue to nd our
results.
their sum remains the same. In this instance, q

is not a single model at all, but rather a family of
models satisfying the condition 

i
+ 

j
= 

ij
for
a particular 

ij
.
4
All models in this family assign
equal likelihood to the training data, and so the IIS
algorithm is unable to distinguish between them.
Under these circumstances we should ensure that


i
= 

j
. Since we have no way to distinguish be-
tween the two features, our model should assign
them equal importance. However, this is not guar-
anteed by the IIS algorithm. In particular, it is less
likely to occur if the initial weights 
(0)
i
and 
(0)
j
are
not equal.
3.2 IIS and Overlapping Features - A
Simple Example
Suppose that our model has a vector of parameters
 = [
i
: i = 1 : : : n] and we wish to change it to
 + ?, where ? = [?
i
: i = 1 : : : n] The IIS algorithm
considers each 
i
in turn. It chooses ?
i
to maximize
an auxiliary function B(?j), which provides a lower
bound on the change in log-likelihood of the model.
Each adjustment 
i
 
i
+ ?
i
is guaranteed to in-
crease the likelihood of the model and thus approach
our ideal solution q

. This process is repeated un-
til convergence. There are no inherent restrictions
on the value of ?
i
; it may be positive or negative,
and large or small compared to the values taken for
dierent features.
Now, suppose that the features f
i
and f
j
over-
lap and we halt the algorithm after t iterations.
Clearly, the algorithm cannot guarantee that the -
nal weights 
t
i
and 
t
j
will be equal. The following
highly simplied example should demonstrate why
this is the case.
Example 1: Imagine that our model has two
overlapping features f
1
and f
2
, and q

is the fam-
ily of models in which 
1
+ 
2
= 5. Suppose that
the initial weights are 
(0)
1
= 5; 
0
2
= 0. Recall that
at the ith step of an iteration, the IIS algorithm
considers the change in log-likelihood of the model
which can be made by only adjusting the ith pa-
rameter. In this case no change is possible as the
sum 
1
+ 
2
is already at its optimum value, so the
algorithm terminates with 
t
1
= 5 and 
t
2
= 0. We
have assigned far greater importance to f
1
than to
f
2
with no justication for doing so. Clearly, this
assumption might not be warranted.
3.3 IIS and Overlapping Features in
Practice
The situation will obviously be much more compli-
cated in practice. Most importantly, there is no such
4
The number 
ij
is not a `constant' as such, since any
vector of weights is in a sense unique only up to multiplication
by a positive constant. This will be examined in greater detail
when we consider overlapping features in practice.
thing as an absolute \optimum" vector of weights


. As a result, no one parameter can be regarded
as xed until the algorithm has converged for all pa-
rameters. In the above example, our \true" set of
target solutions is those for which 
1
+ 
2
is a con-
stant. Since there are no other features, IIS would in
fact terminate at once for any pair of initial weights.
Suppose that we added a third feature f
3
which
did not overlap the rst two. Our set of target solu-
tions would then be of the form 
1
+
2
= 

12
k; 
3
=


3
k for some xed 
12
and 
3
and for any k > 0.
The adjustments made to 
1
and 
2
will depend on
those made to 
3
, and the algorithm will not neces-
sarily terminate after the rst pass.
The following example will describe what happens
in this more realistic situation. It will also explain
the possible benets of setting all initial weights to
the same value.
Example 2: Let the model have the three fea-
tures described above, with initial weights 
0
1
=
5; 
0
2
= 0; 
0
3
= 1 and a family of target solutions q

dened by 
1
+ 
2
= 5k; 
3
= k for any k > 0. IIS
again terminates at once, because the initial model
is already part of the family q

. Again there is an
unjustied dierence between the nal weights for
f
1
and f
2
.
Now suppose that all three initial weights are
set to zero. Imagine for simplicity that we have
restricted the algorithm to adjust weights only by
zero or 1. On the rst pass, all three weights are
changed to 1. On the second, 
1
and 
2
are increased
to 2; 
3
remains at 1. In the third iteration, 
1
is set
to 3, 
2
remains at 2 and 
3
at 1, and the algorithm
terminates.
5
Notice that, although there is still a dierence be-
tween the nal weights for f
1
and f
2
, it is much less
than before. This more closely approaches the ideal
situation in which the weights are equal.
This concludes our theoretical treatment of IIS.
We now show experimentally the inuence that the
initial weight settings have, and how it can be
minimised. Our strategy is to use plausible fea-
tures, as found in a realistic domain (parse selec-
tion for stochastic attribute-value grammars), and
rstly show what happens when the initial weight
settings are set uniformly (to zero). We then show
what happens when these initial settings are ran-
domly set, and nally, what happens when we av-
erage over randomly initialised maximum likelihood
solutions.
5
The exact behaviour of the algorithm will depend on the
training data, so this is not the only imaginable outcome, but
it is certainly a plausible one.
4 Log-linear Modelling of
Attribute-Value Grammars
Here we show how attribute-value grammars may be
modelled using log-linear models. Abney gives fuller
details (Abney, 1997).
Let G be an attribute-value grammar, and D a set
of sentences within the string-set dened by L(G).
A log-linear model, M , consist of two components:
a set of features, F and a set of weights, .
The (unnormalised) total weight of a parse x,
 (x), is a function of the k features that are `active'
on a parse:
 (x) = exp(
k
X
i=1

i
f
i
(x)) (6)
The probability of a parse, P (x j M), is simply
the result of normalising the total weight associated
with that parse:
P (x jM) =
1
Z
 (x) (7)
Z =
X
y2

 (y) (8)

 is the union of the set of parses assigned to each
sentence in D by the grammar G, such that each
parse in 
 is unique in terms of the features that are
active on it. Normally a parse can be viewed as the
set of features that are active on it.
The interpretation of this probability (equation 7)
depends upon the application of the model. Here,
we use parse probabilities to reect preferences for
parses.
5 The Grammar
The grammar we model with log-linear models
(called the Tag Sequence Grammar (Briscoe and
Carroll, 1996), or TSG for short) was manually de-
veloped with regard to coverage, and when compiled
consists of 455 Denite Clause Grammar (DCG)
rules. It does not parse sequences of words directly,
but instead assigns derivations to sequences of part-
of-speech tags (using the CLAWS2 tagset). The
grammar is relatively shallow (for example, it does
not fully analyse unbounded dependencies).
6 Modelling the Grammar
Modelling the TSG with respect to the parsed Wall
Street Journal consists of two steps: creation of a
feature set and denition of a reference distribution
(the target model, ~p).
6.1 Feature Set
Modelling the TSG with respect to the parsed Wall
Street Journal consists of two steps: creation of a
AP/a1:unimpeded
A1/app1:unimpeded
a
a
a
!
!
!
unimpeded PP/p1:by
P1/pn1:by
b
b
"
"
by N1/n:tra?c
tra?c
Figure 1: TSG Parse Fragment
feature set and denition of the reference distribu-
tion.
Our feature set is created by parsing sentences
in the training set, and using each parse to instan-
tiate templates. Each template denes a family of
features. Our templates are motivated by the ob-
servations that linguistically-stipulated units (DCG
rules) are informative, and that many DCG appli-
cations in preferred parses can be predicted using
lexical information.
The rst template creates features that count
the number of times a DCG instantiationis present
within a parse.
6
For example, suppose we parsed
the Wall Street Journal AP:
1 unimpeded by tra?c
A parse tree generated by TSG might be as shown
in gure 1. Here, to save on space, we have labelled
each interior node in the parse tree with TSG rule
names, and not attribute-value bundles. Further-
more, we have annotated each node with the head
word of the phrase in question. Within our gram-
mar, heads are (usually) explicitly marked. This
means we do not have to make any guesses when
identifying the head of a local tree. With head in-
formation, we are able to lexicalise models. We have
suppressed tagging information.
For example, a feature dened using this template
might count the number of times the we saw:
AP/a1
A1/app1
in a parse. Such features record some of the context
of the rule application, in that rule applications that
6
Note, all our features suppress any terminals that appear
in a local tree. Lexical information is included when we decide
to lexicalise features.
dier in terms of how attributes are bound will be
modelled by dierent features.
Our second template creates features that are par-
tially lexicalised. For each local tree (of depth one)
that has a PP daughter, we create a feature that
counts the number of times that local tree, decorated
with the head-word of the PP, was seen in a parse.
An example of such a lexicalised feature would be:
A1/app1
PP/p1:by
These features are designed to model PP attach-
ments that can be resolved using the head of the
PP.
The third and nal template creates features that
are again partially lexicalised. This time, we create
local trees of depth one that are decorated with the
head word. For example, here is one such feature:
AP/a1:unimpeded
A1/app1
Note the second and third templates result in fea-
tures that overlap with features resulting from ap-
plications of the rst template.
6.2 Reference Distribution
We create the reference distribution R (an associa-
tion of probabilities with TSG parses of sentences,
such that the probabilities reect parse preferences)
using the following process:
1. Take the training set of parses and for each
parse, compare the structural dierences be-
tween it and a reference treebank parse.
2. Map these tree similarity scores into probabili-
ties (where the sum of all reference probabilities
for all parses sums to one).
Again, see Anon for more details.
This concludes our discussion of how we model
grammars. We now go on to present our experimen-
tal investigation of the inuence of the initial weight
settings.
7 Experiments
Here we present three sets of experiments. The
rst set shows the performance of maximum entropy
when the initial weight setting are zero. The sec-
ond set show the eects of randomised initial setting,
and so establishes (an estimate of) the variation in
performance space. The third set of experiments
showed how the inuence of the initial weight set-
tings could be minimised by averaging over manyh
models.
Throughout, we used the same training set. This
consisted of a sample of 53795 parses (produced from
sentences at most 15 tokens long, with at most 15
parses per sentence). The sentences were drawn
from the parsed Wall Street Journal, and all could
be parsed using our grammar. The motivation for
this choice of training set came from the fact that
when the sample of sentences is too small, the result-
ing model will tend to undert. Likewise, when the
training set is too large, the model will tend to over-
t. A sample of appropriate size (which can be found
using a simple search, as Osborne (2000) demon-
strated) will therefore neither signicantly undert
nor overt. Quite apart from estimation issues re-
lated to sample size, because we repeatedly estimate
models, using a sample that is just su?ciently large
(and no larger) allows us to make signicant compu-
tational savings.
We used a disjoint development set and testing
set. The development set consisted of 2620 parses,
derived from parsing sentences at most 30 tokens
long, with at most 100 parses per sentence. The test-
ing set was randomly sampled from the Wall Street
Journal, and consisted of 469 sentences, with each
sentence at most 30 tokens long, with at most 100
parses per sentence. Each sentence has on average
60:0 parses per sentence.
The model we used throughout contained 75171
features.
For all experiments, we ran IIS for the same num-
ber of iterations (75). This number was selected as
the number of iterations that produced the best per-
formance for maximum entropy (our yardstick).
Evaluation was in terms of exact match: for each
sentence in the test set, we awarded ourselves a
point if the estimated model ranked highest the same
parse that was ranked highest using the reference
probabilities.
7
Ties were randomly broken.
7.1 Maximum Entropy Results using
Uniform Initialisation
A model trained using weights all initialised to zero
yielded an exact match score of 52:0%.
7.2 Randomised Models
A pool of 10; 000 models was created by randomly
setting the initial weights to values in the range [-
0.3,0.3] and then estimating the nal weights using
the training set.
The histogram in gure 2 shows the number of
models that produced the same results on the test-
ing material. As can be seen, performance is roughly
normally distributed, with some local minima hav-
ing a much wider basin of attraction than other min-
ima. Also, note that all of these models underper-
7
We use exact match as an arbitrary evaluation metric.
The particular choice of metric is not crucial to the results of
this paper.
forms a model crated using uniform initialisation.
8
The performance of the worst model was 24:1%.
This is our lower bound, and is less than half that
of basic maxent. The best randomly selected model
had a performance of 46:5%.
0
50
100
150
200
250
300
350
400
450
20 25 30 35 40 45 50
N
um
be
r o
f M
od
el
s
Exact Match
"rand-hist04"
Figure 2: Distribution of models with randomly ini-
tialised starting conditions
7.3 Averaging over models
To see whether an ensemble of such randomised
models could cancel-out the inuence of the ini-
tial weight settings, we created a pool of 600 ran-
domised models, and then combined then together
using equation 9:
P (x jM
1
: : :M
n
) =
Q
n
i=1
P (x jM
i
)
P
y
Q
n
j=1
P (y jM
j
)
(9)
Because it is possible that some subset of the mod-
els outperforms an ensemble using all models, we
uniformly sampled, with replacement, from this pool
models for inclusion into the nal ensemble. Ran-
dom selection introduces variation, so we repeated
our results ten time and averaged the results.
Figure 3 shows our results (N is the number of
models in each ensemble, x is the mean performance
and 
2
is the standard deviation). The nal entry
(marked all) shows the performance obtained using
an ensemble consisting of all models in the model,
equally weighted.
As can be seen, increasing the number of models
in the ensemble reduces the inuence of the initial
weight settings. However, even with a large pool
of models, we still marginally underperform uniform
initialisation.
8
Running IIS until convergence did not narrow the gap,
so our ndings cannot be attributed to dierential rates of
convergence.
N x 
2
N x 
2
1 38.78 0.12 50 49.94 1.24
2 41.41 1.04 100 50.55 0.70
3 44.24 1.63 150 50.62 0.88
5 46.57 1.69 200 50.66 0.75
10 47.21 1.71 300 50.81 0.73
20 49.23 1.19 600 51.04 0.36
all 51.39 -
Figure 3: Model averaging results
Note that we have not carried out the control ex-
periment of repeating our runs using features which
do not overlap with each other. Unfortunately, doing
this would probably mean having to create models
that are unnatural. We therefore cannot be abso-
lutely certain that the sole reason for the existance
of local optima is the presence of overlapping fea-
tures. However, we can be sure that they do exist,
and that varying the initial weight settings will re-
veal them.
8 Comments
We have established that, in the presence of overlap-
ping features, the values of the initial weights can
aect the utility of the nal model. Our experi-
mental results support this nding. In general, one
might encounter a similar problem (overlapping fea-
tures) with what might be called `semi-overlapping
features': features which are very similar (but not
identical) on the training data and very dierent
outside of it.
IIS could be made more robust to the choice of
initial parameters in a number of ways:
 The simplest course of action is to set al ini-
tial weights to the same value. Although zero
is often a convenient initial value, in principle
any real number would do, since the IIS algo-
rithm can reach a given optimal solution from
any starting point in the space of initial param-
eters.
 We could also examine all the features to de-
termine which ones overlap, and force it to bal-
ance the nal weights of these features. For very
large models, this may be prohibitively di?cult
and time-consuming.
 Model averaging can also cancel out variations
caused by a particular choice of initial settings.
However, this implies a greater computational
burden as IIS will need to be run many times in
order to gain a representative sample of models.
 The number of features in the model could be
reduced using feature selection methods (for ex-
ample (Mullen and Osborne, 2000)).
Although IIS is a useful tool for estimating log-
linear models, we have since moved-on to estimating
models using limited-memory variable-metric meth-
ods (Malouf, 2002). Our ndings show that conver-
gence, for a range of problems, is faster. An inter-
esting question is seeing the extent to which other
numerical methods for estimating log-linear models
are sensitive to initial parameter values. Finally, it
should be noted that our theoretical results apply to
a more general setting than that of log-linear mod-
els trained using the IIS algorithm. The problem of
overlapping features could in principle occur in any
situation in which a model has a linear combination
of features, and a `hill-climbing' algorithm is used to
seek a maximum-likelihood solution.
Acknowledgements
We wish to thank Steve Clark for useful discussions
about IIS, Rob Malouf for supplying the IIS imple-
mentation and the two anonymous reviewers. Iain
Bancarz was supported by the EPSRC grant POEM.
References
Steven P. Abney. 1997. Stochastic Attribute-
Value Grammars. Computational Linguistics,
23(4):597{618, December.
Ted Briscoe and John Carroll. 1996. Automatic
Extraction of Subcategorization from Corpora.
In Proceedings of the 5
th
Conference on Applied
NLP, pages 356{363, Washington, DC.
Mark Johnson, Stuart Geman, Stephen Cannon,
Zhiyi Chi, and Stephan Riezler. 1999. Estimators
for Stochastic \Unication-Based" Grammars. In
37
th
Annual Meeting of the ACL.
J. Laerty, S. Della Pietra, and V. Della Pietra.
1997. Inducing features of random elds. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 19(4):380{393, April.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
Proceedings of the joint CoNLL-WVLC Meeting,
Taipei, Taiwan. ACL. To appear.
Tony Mullen and Miles Osborne. 2000. Overtting
Avoidance for Stochastic Modeling of Attribute-
Value Grammars. In Claire Cardie, Walter Daele-
mans, Claire Nedellec, and Erik Tjong Kim Sang,
editors, Proceedings of the Computational Natural
Language learning 2000, pages 49{54. ACL, Lis-
bon, Portugal.
Kamal Nigam, John Laerty, , and Andrew Mc-
Callum. 1999. Using maximum entropy for text
classication. In IJCAI-99 Workshop on Machine
Learning for Information Filtering,.
Miles Osborne. 2000. Estimation of Stochastic
Attribute-Value Grammars using an Informative
Sample. In The 18
th
International Conference on
Computational Linguistics, Saarbrucken, August.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 468?476, Prague, June 2007. c?2007 Association for Computational Linguistics
Smoothed Bloom filter language models: Tera-Scale LMs on the Cheap
David Talbot and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract
A Bloom filter (BF) is a randomised data
structure for set membership queries. Its
space requirements fall significantly below
lossless information-theoretic lower bounds
but it produces false positives with some
quantifiable probability. Here we present
a general framework for deriving smoothed
language model probabilities from BFs.
We investigate how a BF containing n-gram
statistics can be used as a direct replacement
for a conventional n-gram model. Recent
work has demonstrated that corpus statistics
can be stored efficiently within a BF, here
we consider how smoothed language model
probabilities can be derived efficiently from
this randomised representation. Our pro-
posal takes advantage of the one-sided error
guarantees of the BF and simple inequali-
ties that hold between related n-gram statis-
tics in order to further reduce the BF stor-
age requirements and the error rate of the
derived probabilities. We use these models
as replacements for a conventional language
model in machine translation experiments.
1 Introduction
Language modelling (LM) is a crucial component in
statistical machine translation (SMT). Standard n-
gram language models assign probabilities to trans-
lation hypotheses in the target language, typically
as smoothed trigram models (Chiang, 2005). Al-
though it is well-known that higher-order language
models and models trained on additional monolin-
gual corpora can significantly improve translation
performance, deploying such language models is not
trivial. Increasing the order of an n-gram model can
result in an exponential increase in the number of
parameters; for the English Gigaword corpus, for
instance, there are 300 million distinct trigrams and
over 1.2 billion distinct five-grams. Since a language
model is potentially queried millions of times per
sentence, it should ideally reside locally in memory
to avoid time-consuming remote or disk-based look-
ups.
Against this background, we consider a radically
different approach to language modelling. Instead
of explicitly storing all distinct n-grams from our
corpus, we create an implicit randomised represen-
tation of these statistics. This allows us to drastically
reduce the space requirements of our models. In
this paper, we build on recent work (Talbot and Os-
borne, 2007) that demonstrated how the Bloom filter
(Bloom (1970); BF), a space-efficient randomised
data structure for representing sets, could be used to
store corpus statistics efficiently. Here, we propose
a framework for deriving smoothed n-gram models
from such structures and show via machine trans-
lation experiments that these smoothed Bloom filter
language modelsmay be used as direct replacements
for standard n-gram models in SMT.
The space requirements of a Bloom filter are quite
spectacular, falling significantly below information-
theoretic error-free lower bounds. This efficiency,
however, comes at the price of false positives: the fil-
ter may erroneously report that an item not in the set
is a member. False negatives, on the other hand, will
468
never occur: the error is said to be one-sided. Our
framework makes use of the log-frequency Bloom
filter presented in (Talbot and Osborne, 2007), and
described briefly below, to compute smoothed con-
ditional n-gram probabilities on the fly. It takes
advantage of the one-sided error guarantees of the
Bloom filter and certain inequalities that hold be-
tween related n-gram statistics drawn from the same
corpus to reduce both the error rate and the compu-
tation required in deriving these probabilities.
2 The Bloom filter
In this section, we give a brief overview of the
Bloom filter (BF); refer to Broder andMitzenmacher
(2005) for a more in detailed presentation. A BF rep-
resents a set S = {x1, x2, ..., xn} with n elements
drawn from a universe U of size N . The structure is
attractive when N  n. The only significant stor-
age used by a BF consists of a bit array of size m.
This is initially set to hold zeroes. To train the filter
we hash each item in the set k times using distinct
hash functions h1, h2, ..., hk. Each function is as-
sumed to be independent from each other and to map
items in the universe to the range 1 to m uniformly
at random. The k bits indexed by the hash values
for each item are set to 1; the item is then discarded.
Once a bit has been set to 1 it remains set for the life-
time of the filter. Distinct items may not be hashed
to k distinct locations in the filter; we ignore col-
lisons. Bits in the filter can, therefore, be shared by
distinct items allowing significant space savings but
introducing a non-zero probability of false positives
at test time. There is no way of directly retrieving or
ennumerating the items stored in a BF.
At test time we wish to discover whether a given
item was a member of the original set. The filter is
queried by hashing the test item using the same k
hash functions. If all bits referenced by the k hash
values are 1 then we assume that the item was a
member; if any of them are 0 then we know it was
not. True members are always correctly identified,
but a false positive will occur if all k corresponding
bits were set by other items during training and the
item was not a member of the training set.
The probability of a false postive, f , is clearly the
probability that none of k randomly selected bits in
the filter are still 0 after training. Letting p be the
proportion of bits that are still zero after these n ele-
ments have been inserted, this gives,
f = (1? p)k.
As n items have been entered in the filter by hashing
each k times, the probability that a bit is still zero is,
p
?
=
(
1?
1
m
)kn
? e?
kn
m
which is the expected value of p. Hence the false
positive rate can be approximated as,
f = (1? p)k ? (1? p
?
)k ?
(
1? e?
kn
m
)k
.
By taking the derivative we find that the number of
functions k? that minimizes f is,
k? = ln 2 ?
m
n
,
which leads to the intuitive result that exactly half
the bits in the filter will be set to 1 when the optimal
number of hash functions is chosen.
The fundmental difference between a Bloom fil-
ter?s space requirements and that of any lossless rep-
resentation of a set is that the former does not depend
on the size of the (exponential) universe N from
which the set is drawn. A lossless representation
scheme (for example, a hash map, trie etc.) must de-
pend on N since it assigns a distinct representation
to each possible set drawn from the universe.
3 Language modelling with Bloom filters
Recent work (Talbot and Osborne, 2007) presented a
scheme for associating static frequency information
with a set of n-grams in a BF efficiently.1
3.1 Log-frequency Bloom filter
The efficiency of the scheme for storing n-gram
statistics within a BF presented in Talbot and Os-
borne (2007) relies on the Zipf-like distribution of
n-gram frequencies: most events occur an extremely
small number of times, while a small number are
very frequent. We assume that raw counts are quan-
tised and employ a logarithmic codebook that maps
counts, c(x), to quantised counts, qc(x), as follows,
qc(x) = 1 + blogb c(x)c. (1)
1Note that as described the Bloom filter is not an associative
data structure and provides only a Boolean function character-
ising the set that has been stored in it.
469
Algorithm 1 Training frequency BF
Input: Strain, {h1, ...hk} and BF = ?
Output: BF
for all x ? Strain do
c(x)? frequency of n-gram x in Strain
qc(x)? quantisation of c(x) (Eq. 1)
for j = 1 to qc(x) do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
BF [hi(x)]? 1
end for
end for
end for
return BF
The precision of this codebook decays exponentially
with the raw counts and the scale is determined by
the base of the logarithm b; we examine the effect
of this parameter on our language models in experi-
ments below.
Given the quantised count qc(x) for an n-gram
x, the filter is trained by entering composite events
consisting of the n-gram appended by an integer
counter j that is incremented from 1 to qc(x) into
the filter. To retrieve an n-gram?s frequency, the n-
gram is first appended with a counter set to 1 and
hashed under the k functions; if this tests positive,
the counter is incremented and the process repeated.
The procedure terminates as soon as any of the k
hash functions hits a 0 and the previous value of the
counter is reported. The one-sided error of the BF
and the training scheme ensure that the actual quan-
tised count cannot be larger than this value. As the
counts are quantised logarithmically, the counter is
usually incremented only a small number of times.
We can then approximate the original frequency
of the n-gram by taking its expected value given the
quantised count retrieved,
E[c(x)|qc(x) = j] =
bj?1 + bj ? 1
2
. (2)
These training and testing routines are repeated here
as Algorithms 1 and 2 respectively.
As noted in Talbot and Osborne (2007), errors for
this log-frequency BF scheme are one-sided: fre-
quencies will never be underestimated. The prob-
ability of overestimating an item?s frequency decays
Algorithm 2 Test frequency BF
Input: x, MAXQCOUNT , {h1, ...hk} and BF
Output: Upper bound on c(x) ? Strain
for j = 1 to MAXQCOUNT do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
if BF [hi(x)] = 0 then
return E[c(x)|qc(x) = j ? 1] (Eq. 2)
end if
end for
end for
exponentially with the size of the overestimation er-
ror d (i.e. as fd for d > 0) since each erroneous
increment corresponds to a single false positive and
d such independent events must occur together.
The efficiency of the log-frequency BF scheme
can be understood from an entropy encoding per-
spective under the distribution over frequencies of
n-gram types: the most common frequency (the sin-
gleton count) is assigned the shortest code (length k)
while rarer frequencies (those for more common n-
grams) are assigned increasingly longer codes (k ?
qc(x)).
3.2 Smoothed BF language models
A standard n-gram language model assigns condi-
tional probabilities to target words given a certain
context. In practice, most standard n-gram language
models employ some form of interpolation whereby
probabilities conditioned on the most specific con-
text consisting usually of the n ? 1 preceding to-
kens are combined with more robust estimates based
on less specific conditioning events. To compute
smoothed language model probabilities, we gener-
ally require access to the frequencies of n-grams of
length 1 to n in our training corpus. Depending on
the smoothing scheme, we may also need auxiliary
statistics regarding the number of distinct suffixes
for each n-gram (e.g., Witten-Bell and Kneser-Ney
smoothing) and the number of distinct prefixes or
contexts in which they appear (e.g., Kneser-Ney).
We can use a single BF to store these statistics but
need to distinguish each type of event (e.g., raw
counts, suffix counts, etc.). Here we use a distinct
set of k hash functions for each such category.
Our motivation for storing the corpus statistics
470
directly rather than precomputed probabilities is
twofold: (i) the efficiency of the scheme described
above for storing frequency information together
with items in a BF relies on the frequencies hav-
ing a Zipf-like distribution; while this is definitely
true for corpus statistics, it may well not hold for
probabilities estimated from them; (ii) as will be-
come apparent below, by using the corpus statistics
directly, we will be able to make additional savings
in terms of both space and error rate by using simple
inequalities that hold for related information drawn
consistently from the same corpus; it is not clear
whether such bounds can be established for proba-
bilities computed from these statistics.
3.2.1 Proxy items
There is a potential risk of redundancy if we rep-
resent related statistics using the log-frequency BF
scheme presented in Talbot and Osborne (2007). In
particular, we do not need to store information ex-
plicitly that is necessarily implied by the presence
of another item in the training set, if that item can
be identified efficiently at query time when needed.
We use the term proxy item to refer to items whose
presence in the filter implies the existence of another
item and that can be efficiently queried given the im-
plied item. In using a BF to store corpus statistics
for language modelling, for example, we can use the
event corresponding to an n-gram and the counter
set to 1 as a proxy item for a distinct prefix, suffix or
context count of 1 for the same n-gram since (ignor-
ing sentence boundaries) it must have been preceded
and followed by at least one distinct type, i.e.,
qc(w1, ..., wn) ? 1 ? BF ? s(w1, ..., wn) ? 1,
where s(?) is the number of the distinct types follow-
ing this n-gram in the training corpus. We show be-
low that such lower bounds allow us to significantly
reduce the memory requirements for a BF language
model.
3.2.2 Monotonicity of n-gram event space
The error analysis in Section 2 focused on the
false positive rate of a BF; if we deploy a BF within
an SMT decoder, however, the actual error rate will
also depend on the a priori membership probability
of items presented to it. The error rate Err is,
Err = Pr(x /? Strain|Decoder)f.
This implies that, unlike a conventional lossless data
structure, the model?s accuracy depends on other
components in system and how it is queried.
Assuming that statistics are entered consistently
from the same corpus, we can take advantage of the
monotonicity of the n-gram event space to place up-
per bounds on the frequencies of events to be re-
trieved from the filter prior to querying it, thereby
reducing the a priori probability of a negative and
consequently the error rate.
Specifically, since the log-frequency BF scheme
will never underestimate an item?s frequency, we
can apply the following inequality recursively and
bound the frequency of an n-gram by that of its least
frequent subsequence,
c(w1, ..., wn) ? min {c(w1, ..., wn?1), c(w2, ..., wn)}.
We use this to reduce the error rate of an interpolated
BF language model described below.
3.3 Witten-Bell smoothed BF LM
As an example application of our framework, we
now describe a scheme for creating and querying
a log-frequency BF to estimate n-gram language
model probabilities using Witten-Bell smoothing
(Bell et al, 1990). Other smoothing schemes, no-
tably Kneser-Ney, could be described within this
framework using additional proxy relations for infix
and prefix counts.
In Witten-Bell smoothing, an n-gram?s probabil-
ity is discounted by a factor proportional to the num-
ber of times that the n ? 1-gram preceding the cur-
rent word was observed preceding a novel type in
the training corpus. It is defined recursively as,
Pwb(wi|w
i?1
i?n+1) = ?wi?1i?n+1
Pml(wi|w
i?1
i?n+1)
+(1??wi?1i?n+1
)Pwb(wi|w
i?1
i?n+2)
where ?x is defined via,
1? ?x =
c(x)
s(x) + c(x)
,
and Pml(?) is the maximum likelihood estimator cal-
culated from relative frequencies.
The statistics required to compute the Witten-Bell
estimator for the conditional probability of an n-
gram consist of the counts of all n-grams of length
471
1 to n as well as the counts of the number of distinct
types following all n-grams of length 1 to n ? 1.
In practice we use the c(w1, ..., wi) = 1 event as a
proxy for s(w1, ..., wi) = 1 and thereby need not
store singleton suffix counts in the filter.
Distinct suffix counts of 2 and above are stored
by subtracting this proxy count and converting to the
log quantisation scheme described above, i.e.,
qs(x) = 1 + blogb(s(x)? 1)c
In testing for a suffix count, we first query the item
c(w1, ..., wn) = 1 as a proxy for s(w1, ..., wn) =
1 and, if found, query the filter for incrementally
larger suffix counts, taking the reconstructed suffix
count of an n-gram with a non-zero n-gram count to
be the expected value, i.e.,
E[s(x)|qs(x) = j ? j > 0] = 1 +
(bj?1 + bj ? 1)
2
Having created a BF containing these events, the
algorithm we use to compute the interpolated WB
estimate makes use of the inequalities described
above to reduce the a priori probability of querying
for a negative. In particular, we bound the count of
each numerator in the maximum likelihood term by
the count of the corresponding denominator and the
count of distinct suffixes of an n-gram by its respec-
tive token frequency.
Unlike more traditional LM formulations that
back-off from the highest-order to lower-order mod-
els, our algorithm works up from the lowest-order
model. Since the conditioning context increases in
specificity at each level, each statistic is bound from
above by its corresponding value at the previous less
specific level. The bounds are applied by passing
them as the parameter MAXQCOUNT to the fre-
quency test routine shown as Algorithm 2. We ana-
lyze the effect of applying such bounds on the per-
formance of the model within an SMT decoder in
the experiments below. Working upwards from the
lower-order models also allows us to truncate the
computation before the highest level if the denomi-
nator in the maximum likelihood term is found with
a zero count at any stage (no higher-order terms can
be non-zero given this).
4 Experiments
We conducted a range of experiments to explore
the error-space trade-off of using a BF-based model
as a replacement for a conventional n-gram model
within an SMT system and to assess the benefits of
specific features of our framework for deriving lan-
guage model probabilities from a BF.
4.1 Experimental set-up
All of our experiments use publically available re-
sources. Our main experiments use the French-
English section of the Europarl (EP) corpus for par-
allel data and language modelling (Koehn, 2003).
Decoding is carried-out using the Moses decoder
(Koehn and Hoang, 2007). We hold out 1,000 test
sentences and 500 development sentences from the
parallel text for evaluation purposes. The parame-
ters for the feature functions used in this log-linear
decoder are optimised using minimum error rate
(MER) training on our development set unless other-
wise stated. All evaluation is in terms of the BLEU
score on our test set (Papineni et al, 2002).
Our baseline language models were created us-
ing the SRILM toolkit (Stolcke, 2002). We built 3,
4 and 5-gram models from the Europarl corpus us-
ing interpolated Witten-Bell smoothing (WB); no n-
grams are dropped from these models or any of the
BF-LMs. The number of distinct n-gram types in
these baseline models as well as their sizes on disk
and as compressed by gzip are given in Table 1; the
gzip figures are given as an approximate (and opti-
mistic) lower bound on lossless representations of
these models.2
The BF-LM models used in these experiments
were all created from the same corpora following the
scheme outlined above for storing n-gram statistics.
Proxy relations were used to reduce the number of
items that must be stored in the BF; in addition, un-
less specified otherwise, we take advantage of the
bounds described above that hold between related
statistics to avoid presenting known negatives to the
filter. The base of the logarithm used in quantization
is specified on all figures.
The SRILM and BF-based models are both
queried via the same interface in the Moses decoder.
2Note, in particular, that gzip compressed files do not sup-
port direct random access as required by in language modelling.
472
n Types Mem. Gzip?d BLEU
3 5.9M 174Mb 51Mb 28.54
4 14.1M 477Mb 129Mb 28.99
5 24.2M 924Mb 238Mb 29.07
Table 1: WB-smoothed SRILM baseline models.
We assign a small cache to the BF-LM models (be-
tween 1 and 2MBs depending on the order of the
model) to store recently retrieved statistics and de-
rived probabilities. Translation takes between 2 to 5
times longer using the BF-LMs as compared to the
corresponding SRILM models.
4.2 Machine translation experiments
Our first set of experiments examines the relation-
ship between memory allocated to the BF-LM and
translation performance for a 3-gram and a 5-gram
WB smoothed BF-LM. In these experiments we use
the log-linear weights of the baseline model to avoid
variation in translation performance due to differ-
ences in the solutions found by MER training: this
allows us to focus solely on the quality of each BF-
LM?s approximation of the baseline. These exper-
iments consider various settings of the base for the
logarithm used during quantisation (b in Eq. (1)).
We also analyse these results in terms of the re-
lationships between BLEU score and the underlying
error rate of the BF-LM and the number of bits as-
signed per n-gram in the baseline model.
MER optimised BLEU scores on the test set are
then given for a range of BF-LMs.
4.3 Mean squared error experiments
Our second set of experiments focuses on the accu-
racy with which the BF-LM can reproduce the base-
line model?s distribution. Unfortunately, perplex-
ity or related information-theoretic quantities are not
applicable in this case since the BF-LM is not guar-
anteed to produce a properly normalised distribu-
tion. Instead we evaluate the mean squared error
(MSE) between the log-probabilites assigned by the
baseline model and by BF-LMs to n-grams in the
English portion of our development set; we also con-
sider the relation between MSE and the BLEU score
from the experiments above.
 
22
 
24
 
26
 
28
 
30
 
32
 
0.02
 
0.017
5
 
0.015
 
0.012
5
 
0.01
 
0.007
5
 
0.005
 
0.002
5
BLEU Score
Memo
ry in G
B
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
SRILM
 Witte
n-Bell
 3-gra
m (174
MB)
Figure 1: WB-smoothed 3-gram model (Europarl).
4.4 Analysis of BF-LM framework
Our third set of experiments evaluates the impact of
the use of upper bounds between related statistics on
translation performance. Here the standard model
that makes use of these bounds to reduce the a pri-
ori negative probability is compared to a model that
queries the filter in a memoryless fashion.3
We then present details of the memory savings ob-
tained by the use of proxy relations for the models
used here.
5 Results
5.1 Machine translation experiments
Figures 1 and 2 show the relationship between trans-
lation performance as measured by BLEU and the
memory assigned to the BF respectively for WB-
smoothed 3-gram and 5-gram BF-LMs. There is a
clear degradation in translation performance as the
memory assigned to the filter is reduced. Models
using a higher quantisation base approach their opti-
mal performance faster; this is because these more
coarse-grained quantisation schemes store fewer
items in the filter and therefore have lower underly-
ing false positive rates for a given amount of mem-
ory.
Figure 3 presents these results in terms of the re-
lationship between translation performance and the
false positive rate of the underlying BF. We can see
that for a given false positive rate, the more coarse-
grained quantisation schemes (e.g., base 3) perform
3In both cases we apply ?sanity check? bounds to ensure that
none of the ratios in the WB formula (Eq. 3) are greater than 1.
473
 
22
 
24
 
26
 
28
 
30
 
32
 
0.07
 
0.06
 
0.05
 
0.04
 
0.03
 
0.02
 
0.01
BLEU Score
Memo
ry in G
B
WB-s
mooth
ed BF
-LM 5
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
SRILM
 Witte
n-Bell
 5-gra
m (924
MB)
Figure 2: WB-smoothed 5-gram model (Europarl).
 
22
 
23
 
24
 
25
 
26
 
27
 
28
 
29
 
30  0.0
1
 
0.1
 
1
BLEU Score
False
 posit
ive ra
te (prob
ability)
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
Figure 3: False positive rate vs. BLEU .
worse than the more fine-grained schemes.4
Figure 4 presents the relationship in terms of the
number of bits per n-gram in the baseline model.
This suggests that between 10 and 15 bits is suf-
ficient for the BF-LM to approximate the baseline
model. This is a reduction of a factor of between 16
and 24 on the plain model and of between 4 and 7
on gzip compressed model.
The results of a selection of BF-LM models with
decoder weights optimised using MER training are
given in Table 2; these show that the models perform
consistently close to the baseline models that they
approximate.
5.2 Mean squared error experiments
Figure 5 shows the relationship between memory as-
signed to the BF-LMs and the mean squared error
4Note that in this case the base 3 scheme will use approxi-
mately two-thirds the amount of memory required by the base
1.5 scheme.
 
20
 
22
 
24
 
26
 
28
 
30
 
32
 
19
 
17
 
15
 
13
 
11
 
9
 
7
 
5
 
3
 
1
BLEU Score
Bits p
er n-g
ram
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
Figure 4: Bits per n-gram vs. BLEU.
n Memory Bits / n-gram base BLEU
3 10MB 14 bits 1.5 28.33
3 10MB 14 bits 2.0 28.47
4 20MB 12 bits 1.5 28.63
4 20MB 12 bits 2.0 28.63
5 40MB 14 bits 1.5 28.53
5 40MB 14 bits 2.0 28.72
5 50MB 17 bits 1.5 29.31
5 50MB 17 bits 2.0 28.67
Table 2: MERT optimised WB-smoothed BF-LMS.
(MSE) of log-probabilities that these models assign
to the development set compared to those assigned
by the baseline model. This shows clearly that the
more fine-grained quantisation scheme (e.g. base
1.1) can reach a lower MSE but also that the more
coarse-grained schemes (e.g., base 3) approach their
minimum error faster.
Figure 6 shows the relationship between MSE
between the BF-LM and the baseline model and
BLEU. The MSE appears to be a good predictor of
BLEU score across all quantisation schemes. This
suggests that it may be a useful tool for optimising
BF-LM parameters without the need to run the de-
coder assuming a target (lossless) LM can be built
and queried for a small test set on disk. An MSE of
below 0.05 appears necessary to achieve translation
performance matching the baseline model here.
5.3 Analysis of BF-LM framework
We refer to (Talbot and Osborne, 2007) for empiri-
cal results establishing the performance of the log-
frequency BF-LM: overestimation errors occur with
474
 
0.01
 
0.025 0.05 0.1 0.25 0.5
 
0.03
 
0.02
 
0.01
 
0.005
 
0.002
5
 
0.001
Mean squared error of log probabilites
Memo
ry in G
B
MSE 
betwe
en WB
 3-gra
m SR
ILM a
nd BF
-LMs
Base
 3
Base
 1.5
Base
 1.1
Figure 5: MSE between SRILM and BF-LMs
 
22
 
23
 
24
 
25
 
26
 
27
 
28
 
29
 
30  0.0
1
 
0.1
 
1
BLEU Score
Mean
 squa
red er
ror
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
Figure 6: MSE vs. BLEU for WB 3-gram BF-LMs
a probability that decays exponentially in the size of
the overestimation error.
Figure 7 shows the effect of applying upper
bounds to reduce the a priori probability of pre-
senting a negative event to the filter in our in-
terpolation algorithm for computing WB-smoothed
probabilities. The application of upper bounds im-
proves translation performance particularly when
the amount of memory assigned to the filter is lim-
ited. Since both filters have the same underlying
false positive rate (they are identical), we can con-
clude that this improvement in performance is due
to a reduction in the number of negatives that are
presented to the filter and hence errors.
Table 3 shows the amount of memory saved by
the use of proxy items to avoid storing singleton
suffix counts for the Witten-Bell smoothing scheme.
The savings are given as ratios over the amount of
memory needed to store the statistics without proxy
items. These models have the same underlying false
 
22
 
24
 
26
 
28
 
30
 
32
 
0.01
 
0.007
5
 
0.005
 
0.002
5
BLEU Score
Memo
ry in G
B
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 2 wit
h bou
nds
BF-LM
 base
 2 wit
hout b
ounds
Figure 7: Effect of upper bounds on BLEU
n-gram order Proxy space saving
3 0.885
4 0.783
5 0.708
Table 3: Space savings via proxy items .
positive rate (0.05) and quantisation base (2). Sim-
ilar savings may be anticipated when applying this
framework to infix and prefix counts for Kneser-Ney
smoothing.
6 Related Work
Previous work aimed at reducing the size of n-gram
language models has focused primarily on quanti-
sation schemes (Whitaker and Raj, 2001) and prun-
ing (Stolcke, 1998). The impact of the former seems
limited given that storage for the n-gram types them-
selves will generally be far greater than that needed
for the actual probabilities of the model. Pruning
on the other hand could be used in conjunction with
the framework proposed here. This holds also for
compression schemes based on clustering such as
(Goodman and Gao, 2000). Our approach, however,
avoids the significant computational costs involved
in the creation of such models.
Other schemes for dealing with large language
models include per-sentence filtering of the model
or its distribution over a cluster. The former requires
time-consuming adaptation of the model for each
sentence in the test set while the latter incurs sig-
nificant overheads for remote calls during decoding.
Our framework could, however, be used to comple-
ment either of these approaches.
475
7 Conclusions and Future Work
We have proposed a framework for computing
smoothed language model probabilities efficiently
from a randomised representation of corpus statis-
tics provided by a Bloom filter. We have demon-
strated that models derived within this framework
can be used as direct replacements for equivalent
conventional language models with significant re-
ductions in memory requirements. Our empirical
analysis has also demonstrated that by taking advan-
tage of the one-sided error guarantees of the BF and
simple inequalities that hold between related n-gram
statistics we are able to further reduce the BF stor-
age requirements and the effective error rate of the
derived probabilities.
We are currently implementing Kneser-Ney
smoothing within the proposed framework. We hope
the present work will, together with Talbot and Os-
borne (2007), establish the Bloom filter as a practi-
cal alternative to conventional associative data struc-
tures used in computational linguistics. The frame-
work presented here shows that with some consider-
ation for its workings, the randomised nature of the
Bloom filter need not be a significant impediment to
is use in applications.
Acknowledgements
References
T.C. Bell, J.G. Cleary, and I.H. Witten. 1990. Text Compres-
sion. Prentice Hall, Englewood Cliffs, NJ.
B. Bloom. 1970. Space/time tradeoffs in hash coding with
allowable errors. CACM, 13:422?426.
A. Broder and M. Mitzenmacher. 2005. Network applications
of Bloom filters: A survey. Internet Mathematics, 1(4):485?
509.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 263?270, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
J. Goodman and J. Gao. 2000. Language model size reduction
by pruning and clustering. In ICSLP?00, Beijing, China.
Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proc. of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP/Co-NLL).
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation, draft. Available
at:http://people.csail.mit.edu/ koehn/publications/europarl.ps.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU:
a method for automatic evaluation of machine translation.
In ACL-2002: 40th Annual meeting of the Association for
Computational Linguistics.
Andreas Stolcke. 1998. Entropy-based pruning of back-off lan-
guage models. In Proc. DARPA Broadcast News Transcrip-
tion and Understanding Workshop, pages 270?274.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Proc. Intl. Conf. on Spoken Language Processing.
D. Talbot and M. Osborne. 2007. Randomised language mod-
elling for statistical machine translation. In 45th Annual
Meeting of the Association of Computational Linguists (To
appear).
E. Whitaker and B. Raj. 2001. Quantization-based language
model compression (tr-2001-41). Technical report, Mit-
subishi Electronic Research Laboratories.
476
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215?223,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Probabilistic Inference for Machine Translation
Phil Blunsom and Miles Osborne
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
{pblunsom,miles}@inf.ed.ac.uk
Abstract
We advance the state-of-the-art for discrimi-
natively trained machine translation systems
by presenting novel probabilistic inference
and search methods for synchronous gram-
mars. By approximating the intractable space
of all candidate translations produced by inter-
secting an ngram language model with a
synchronous grammar, we are able to train
and decode models incorporating millions of
sparse, heterogeneous features. Further, we
demonstrate the power of the discriminative
training paradigm by extracting structured
syntactic features, and achieving increases in
translation performance.
1 Introduction
The goal of creating statistical machine translation
(SMT) systems incorporating rich, sparse, features
over syntax and morphology has consumed much
recent research attention. Discriminative approaches
are widely seen as a promising technique, poten-
tially allowing us to further the state-of-the-art.
Most work on discriminative training for SMT has
focussed on linear models, often with margin based
algorithms (Liang et al, 2006; Watanabe et al,
2006), or rescaling a product of sub-models (Och,
2003; Ittycheriah and Roukos, 2007).
Recent work by Blunsom et al (2008) has shown
how translation can be framed as a probabilistic
log-linear model, where the distribution over trans-
lations is modelled in terms of a latent variable
on derivations. Their approach was globally opti-
mised and discriminative trained. However, a lan-
guage model, an information source known to be
crucial for obtaining good performance in SMT, was
notably omitted. This was because adding a lan-
guage model would mean that the normalising parti-
tion function could no longer be exactly calculated,
thereby preventing efficient parameter estimation.
Here, we show how language models can be
incorporated into large-scale discriminative transla-
tion models, without losing the probabilistic inter-
pretation of the model. The key insight is that we
can use Monte-Carlo methods to approximate the
partition function, thereby allowing us to tackle the
extra computational burden associated with adding
the language model. This approach is theoreti-
cally justified and means that the model contin-
ues to be both probabilistic and globally optimised.
As expected, using a language model dramatically
increases translation performance.
Our second major contribution is an exploita-
tion of syntactic features. By encoding source syn-
tax as features allows the model to use, or ignore,
this information as it sees fit, thereby avoiding the
problems of coverage and sparsity associated with
directly incorporating the syntax into the grammar
(Huang et al, 2006; Mi et al, 2008). We report on
translation gains using this approach.
We begin by introducing the synchronous gram-
mar approach to SMT in Section 2. In Section
3 we define the parametric form of our model
and describe techniques for approximating the
intractable space of all translations for a given
source sentence. In Section 4 we evaluate the abil-
ity of our model to effectively estimate the highly
dependent weights for the sparse features and real-
valued language model. In addition we describe how
215
X?
 ? ??? , Brown?
X
?
 ? ?? X
? 
?? X
? 
, arrived in X
?
 from X
?
?
X
?
 ? ??? , Shanghai?
X
?
 ? ??? , Beijing?
X
?
 ? ?X
? 
? ?? ?? X
? 
, X
? 
X
? 
late last night?
S ? ?X
? 
?
 
? , X
?
 .?
Figure 1. An example SCFG derivation from a Chi-
nese source sentence which yields the English sentence:
?Brown arrived in Shanghai from Beijing late last night.?
our model can easily integrate rich features over
source syntax trees and compare our training meth-
ods to a state-of-the-art benchmark.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) describes the gener-
ation of pairs of strings. A string pair is generated
by applying a series of paired context-free rewrite
rules of the form,X ? ??, ?,??, whereX is a non-
terminal, ? and ? are strings of terminals and non-
terminals and ? specifies a one-to-one alignment
between non-terminals in ? and ?. In the context of
SMT, by assigning the source and target languages
to the respective sides of a SCFG it is possible to
describe translation as the process of parsing the
source sentence, while generating the target trans-
lation (Chiang, 2007).
In this paper we only consider grammars
extracted using the heuristics described for the Hiero
SMT system (Chiang, 2007). Note however that our
approach is general and could be used with other
synchronous grammar transducers (e.g., (Galley et
al., 2006)). SCFG productions can specify that the
order of the child non-terminals is the same in both
languages (a monotone production), or is reversed (a
reordering production). Without loss of generality,
here we add the restriction that non-terminals on the
source and target sides of the grammar must have the
same category. Figure 1 shows an example deriva-
tion for Chinese to English translation.
3 Model
We start by defining a log-linear model for the con-
ditional probability distribution over target transla-
tions of a given source sentence. A sequence of
SCFG rule applications which produce a translation
from a source sentence is referred to as a derivation,
and each translation may be produced by many dif-
ferent derivations. As the training data only provides
source and target sentences, the derivations are mod-
elled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f ,
is given by:
p?(d, e|f) =
exp
?
k ?kHk(d, e, f)
Z?(f)
(1)
where Hk(d, e, f) =
?
r?d
hk(f , r, q(r,d)) (2)
Using Equation (1), the conditional probability of
a target translation given the source is the sum over
all of its derivations:
p?(e|f) =
?
d??(e,f)
p?(d, e|f)
where ?(e, f) is the set of all derivations of the
target sentence e from the source f.
Here k ranges over the model?s features, and
? = {?k} are the model parameters (weights for
their corresponding features). The function q(r,d)
returns the target ngram context, for a language
model with order m, of rule r in derivation d.
For a rule which spans the target words (i, j) and
target yield(d) = {t0, ? ? ? , tl}:
q(r,d) =
{
ti???ti+m?2?tj?m+2???tj if j ? i > m
ti???tj otherwise
The feature functions hk are real-valued functions
over the source and target sentences, and can include
overlapping and non-independent features of the
data. The features must decompose with the deriva-
tion and the ngram context defined by the function q,
as shown in Equation (2). The features can reference
the entire source sentence coupled with each rule, r,
and its target context, in a derivation.
By directly incorporating the language model
context q into the model formulation, we will not
216
be able to exactly compute the partition function
Z?(f), which sums over all possible derivations.
Even though a dynamic program over this space
would still run in polynomial time, as shown by Chi-
ang (2007), a packed chart representation of the par-
tition function for the binary Hiero grammars used
in this work would require O(n3|T |4(m?1)) space,1
which is far too large to be practical.
Instead we approximate the partition function
using a sum over a large subset of the possible
derivations (?(e, f)):
Z?(f) ?
?
e
?
d?{??(e,f)}
exp
?
k
?kHk(d, e, f)
= Z??(f)
This model formulation raises the questions of
what an appropriate large subset of derivations for
training is, and how to efficiently calculate the sum
over all derivations in decoding. In the following
sections we elucidate and evaluate our solutions to
these problems.
3.1 Sampling Derivations
The training and decoding algorithms presented in
the following sections rely upon Monte-Carlo tech-
niques, which in turn require the ability to draw
derivation samples from the probability distribution
defined by our log-linear model. Here we adapt
previously presented algorithms for sampling from
a PCFG (Goodman, 1998) for use with our syn-
chronous grammar model. Algorithm 1 describes the
algorithm for sampling derivations. The sampling
algorithm assumes the pre-existance of a packed
chart representation of all derivations for a given
source sentence. The inside algorithm is then used
to calculate the scores needed to define a multino-
mial distribution over all partial derivations associ-
ated with expanding a given child rule. These ini-
tial steps are performed once and then an unlim-
ited number of samples can be drawn by calling the
recursive SAMPLE procedure. MULTI draws a sample
from the distribution over rules for a given chart cell,
CHILDREN enumerates the chart cells connected to
a rule as variables, and DERIVATION is a recursive
tree data structure for derivations. The algorithm is
1where |T | is the size of the terminal alphabet, i.e. the num-
ber of unique English words.
Algorithm 1 Top-down recursive derivation sam-
pling algorithm.
1: procedure SAMPLE(X, i, k)
2: rule? MULTI(inside chart(X, i, k))
3: c = ?
4: for (child category, x, y) ? CHILDREN(rule)
do
5: c? c ? SAMPLE(child category, x, y)
6: end for
7: return DERIVATION(X, children)
8: end procedure
first called on a category and chart cell spanning the
entire chart, and then proceeds top down by using
the function MULTI to draw the next rule to expand
from the distribution defined by the inside scores.
3.2 Approximate Inference
Approximating the partition function with Z??(f)
could introduce biases into inference and in the fol-
lowing discussion we describe measures taken to
minimise the effects of the approximation bias.
An obvious approach to approximating the parti-
tion function, and the feature expectations required
for calculating the derivative in training, is to use
the packed chart of derivations produced by running
the cube pruning beam search algorithm of Chiang
(2007) on the source sentence. In this case Z??(f)
includes all the derivations that fall within the cube
pruning beam, hopefully representing the majority
of the probability mass. We denote the partition
function estimated with this cube beam approxima-
tion as Z?cb? (f). This approach has the advantage of
using the same beam search dynamic program dur-
ing training as is used for decoding. As the approxi-
mated partition function does not contain all deriva-
tions, it is possible that some, or all, of the deriva-
tions of the reference translation from the parallel
corpus may be excluded. We must therefore intersect
the packed chart built from the cube beam with that
of the reference derivations to ensure consistency.
Although, as would be done using cube-pruning,
it would seem intuitively sensible to approximate
the partition function using only high probability
derivations, it is possible that doing so will bias
our model in odd ways. The space of derivations
contained within the beam will be tightly clustered
about a maximum, and thus a model trained with
such an approximation will only see a very small
217
Alles / 
Everything
und / 
and
jedes / 
anything
ist / 
is
vorstellbar / 
possible
X
X
X
S
Alles / 
Everything
und / 
and
jedes / 
everyone
ist / 
is
vorstellbar / 
conceivable
X
X
X
S
X
[1,2]
 
Everything
X
[3,4]
 
anything
X
[1,4]
 
Everything 
and
 s
X
[4,5]
 
is
X
[5,6]
 
possible
X
[1,5]
 
Everything * 
is
S
[1,6]
 
Everything * 
possible
X
[5,6]
 
conceivable
X
[2,3]
and
X
[1,3]
 
Everything * 
anything
X
[1,3]
 
Everything * 
everyone
X
[3,4]
 
everyone
S
[1,6]
 
Everything * 
conceivable
Alles / 
Everything
und / 
and
jedes / 
anything
ist / 
is
vorstellbar / 
conceivable
X
X
X
S
(a)
(c)
(b)
2
1
3
4
5
6
2
1
3
4
5
6
2
1
3
4
5
6
Figure 2. A German-English translation example of building Z?sam? (f) from samples. (a) Two sample derivations are
drawn from the model, (b) these samples are then combined into a packed representation, here represented by a
hypergraph with target translations elided for a bigram language model. The derivation in (c) is contained within the
hypergraph even though it was never explicitly inserted.
part of the overall distribution, possibly leading it
astray. Consider the example of a language model
feature: as this is a very strong indicator of transla-
tion quality, we would expect all derivations within
the beam to have a similar (high) language model
score, thereby robbing this feature of its discriminat-
ing power. However if our model could also see the
low probability derivations it would be clear that this
feature is indeed very strongly correlated with good
translations. Thus a good approximation of the space
of derivations is one that includes both good and bad
examples, not just a cluster around the maximum.
A principled solution to the problem of approx-
imating the partition function would be to use a
Markov Chain Monte Carlo (MCMC) sampler to
estimate the sum with a large number of samples.
Most of the sampled derivations would be in the
high probability region of the distribution, however
there would also be a number of samples drawn
from the rest of the space, giving the model a more
global view of the distribution, avoiding the pit-
falls of the narrow view obtained by a beam search.
Although effective, the computational cost of such
an approach is prohibitive as we would need to draw
hundreds of thousands of samples to obtain conver-
gence, for every training iteration.
Here we mediate between the computational
advantages of a beam and the broad view of the dis-
tribution provided by sampling. Using the algorithm
outlined in Section 3.1 we draw samples from the
distribution of derivations and then insert these sam-
ples into a packed chart representation. This process
is illustrated in Figure 2. The packed chart created
by intersecting the sample derivations represents a
space of derivations much greater than the original
samples. In Figure 2 the chart is built from the first
two sampled derivations, while the third derivation
can be extracted from the chart even though it was
never explicitly entered. This approximation of the
partition function (denoted Z?sam? (f)) allows us to
build an efficient packed chart representation of a
large number of derivations, centred on those with
high probability while still including a significant
representation of the low probability space. Deriva-
tions corresponding to the reference can be detected
during sampling and thus we can build the chart
for the reference derivations at the same time as
the one approximating the partition function. This
could lead to some, or none of, the possible ref-
erence derivations being included, as they may not
have been sampled. Although we could intersect all
of the reference derivations with the sampled chart,
this could distort the distribution over derivations,
218
and we believe it to be advantageous to keep the
distributions between the partition function and ref-
erence charts consistent.
Both of the approximations proposed above,
Z?cb? (f) and Z?
sam
? (f), rely on the pre-existence of a
trained translation model in order to either guide the
cube-pruning beam, or define the probability distri-
bution from which we draw samples. We solve this
chicken and egg problem by first training an exact
translation model without a language model, and
then use this model to create the partition function
approximations for training with a language model.
We denote the distribution without a language model
as p?LM? (e|f) and that with as p
+LM
? (e|f).
A final training problem that we need to address
is the appropriate initialisation of the model param-
eters. In theory we could simply randomly initialise
? for p+LM? (e|f), however in practice we found that
this resulted in poor performance on the develop-
ment data. This is due to the complex non-convex
optimisation function, and the fact that many fea-
tures will fall outside the approximated charts result-
ing in random, or zero, weights in testing. We intro-
duce a novel solution in which we use the Gaus-
sian prior over model weights to tie the exact model
trained without a language model, which assigns
sensible values to all rule features, with the approx-
imated model. The prior over model parameters for
p+LM? (e|f) is defined as:
p+LM(?k) ? e
?
??k??
?LM
k ?
2
2?2
Here we have set the mean parameters of the Gaus-
sian distribution for the approximated model to
those learnt for the exact one. This has the effect
that any features that fall outside the approximated
model will simply retain the weight assigned by the
exact model. While for other feature weights the
prior will penalise substantial deviations away from
??LM , essentially encoding the intuition that the
rule rule parameters should not change substantially
with the inclusion of language model features.
This results in the following log-likelihood objec-
tive and corresponding gradient:
L =
?
(ei,fi)?D
log p+LM? (ei|fi) +
?
k
log p+LM0 (?k)
?L
??k
= Ep+LM? (d|ei,fi)
[hk]? Ep+LM? (e|fi)
[hk]
?
?+LMk ? ?
?LM
k
?2
3.3 Decoding
As stated in Equation 3 the probability of a given
translation string is calculated as the sum of the
probabilities of all the derivations that yield that
string. In decoding, where the reference translation
is not known, the exact calculation of this summa-
tion is NP-Hard. This problem also arises in mono-
lingual parsing with probabilistic tree substitution
grammars and has been tackled in the literature
using Monte-Carlo sampling methods (Chappelier
and Rajman, 2000). Their approach is directly appli-
cable to our SCFG decoding problem and we can use
Algorithm 1 to draw sample translation derivations
for the source sentence. The probability of a trans-
lation can be calculated simply from the number of
times a derivation that yields it was sampled, divided
by the total number of samples. For the p?LM? (e|f)
model we can build the full chart of all possible
derivations and thus sample from the true distribu-
tion over derivations. For the p+LM? (e|f) model we
suffer the same problem as in training and cannot
build the full chart. Instead a chart is built using
the cube-pruning algorithm with a wide beam and
we then draw samples from this chart. Although
sampling from a reduced chart will result in biased
samples, in Section 4 we show this approach to be
effective in practice.2 In Section 4 we compare our
sampling approach to the heuristic beam search pro-
posed by Blunsom et al (2008).
It is of interest to compare our proposed decoding
algorithms to minimum Bayes risk (MBR) decoding
(Kumar and Byrne, 2004), a commonly used decod-
ing method. From a theoretical standpoint, the sum-
ming of derivations for a given translation is exactly
2We have experimented with using a Metropolis Hastings
sampler, with p?LM? (e|f) as the proposal distribution, to sam-
ple from the true distribution with the language model. Unfor-
tunately the sample rejection rate was very high such that this
method proved infeasibly slow.
219
equivalent to performing MBR with a 0/1 loss func-
tion over derivations. From a practical perspective,
MBR is normally performed with BLEU as the loss
and approximated using n-best lists. These n-best
lists are produced using algorithms tuned to remove
multiple derivations of the same translation (which
have previously been seen as undesirable). However,
it would be simple to extend our sampling based
decoding algorithm to calculate the MBR estimate
using BLEU , in theory providing a lower variance
estimate than attained with n-best lists.
4 Evaluation
We evaluate our model on the IWSLT 2005 Chinese
to English translation task (Eck and Hori, 2005),
using the 2004 test set as development data for
tuning the hyperparameters and MERT training the
benchmark systems. The statistics for this data are
presented in Table 1.3 The training data made avail-
able for this task consisted of 40k pairs of tran-
scribed utterances, drawn from the travel domain.
The development and test data for this task are some-
what unusual in that each sentence has a single
human translated reference, and fifteen paraphrases
of this reference, provided by monolingual anno-
tators. Model performance is evaluated using the
standard BLEU metric (Papineni et al, 2002) which
measures average n-gram precision, n ? 4, and we
use the NIST definition of the brevity penalty for
multiple reference test sets. We provide evaluation
against both the entire multi-reference sets, and the
single human translation.
Our translation grammar is induced using the
standard alignment and rule extraction heuristics
used in hierarchical translation models (Chiang,
2007).4 As these heuristics aren?t based on a genera-
tive model, and don?t guarantee that the target trans-
lation will be reachable from the source, we discard
those sentence pairs for which we cannot produce a
derivation, leaving 38,405 sentences for training.
Our base model contains a single feature for each
rule which counts the number of times it appeared in
a particular derivation. For models which include a
3Development and test set statistics are for the single human
translation reference.
4With the exception that we allow unaligned words at the
boundary of rules. This improves training set coverage.
language model, we train a standard Kneser-Ney tri-
gram model on the target side of the training corpus.
We also include a word penalty feature to compen-
sate for the shortening effect of the language model.
In total our model contains 2.9M features.
The aims of our evaluation are: (1) to deter-
mine that our proposed training regimes are able to
realise performance increase when training sparse
rule features and a real valued language model fea-
ture together, (2) that the model is able to effectively
use rich features over the source sentence, and (3)
to confirm that our model obtains performance com-
petitive with the current state-of-the-art.
4.1 Inference and Decoding
We have described a number of modelling choices
which aim to compensate for the training biases
introduced by incorporating a language model fea-
ture through approximate inference. Our a priori
knowledge from other SMT systems suggests that
incorporating a language model should lead to large
increases in BLEU score. In this evaluation we aim
to determine whether our training regimes are able
to realises these expected gains.
Table 2 shows a matrix of development BLEU
scores achieved by varying the approximation of the
partition function in training, and varying the decod-
ing algorithm. If we consider the vertical axis we
can see that the sampling method for approximat-
ing the partition function has a small but consistent
advantage over using the cube-pruning beam. The
charts produced by the sampling approach occupy
roughly half the disc space as those produced by
the beam search, so in subsequent experiments we
present results using the Z?sam? (f) approximation.
Comparing the decoding algorithms on the hori-
zontal axis we can reconfirm the findings of Blun-
som et al (2008) that the max-translation decod-
ing outperforms the Viterbi max-derivation approx-
imation. It is also of note that this BLEU increase
is robust to the introduction of the language model
feature, assuaging fears that the max-translation
approach may have been doing the job of the lan-
guage model. We also compare using Monte-Carlo
sampling for decoding with the previously pro-
posed heuristic beam search algorithm. The differ-
ence between the two algorithms is small, however
220
Training Development Test
Chinese English Chinese English Chinese English
Utterances 38405 500 506
Segments/Words 317621 353116 3464 3752 3784 3823
Av. Utterances Length 8 9 6 7 7 7
Longest Utterance 55 68 58 62 61 56
Table 1. IWSLT Chinese to English translation corpus statistics.
Model Max-derivation Max-translation(Beam) Max-translation(Sampling)
p?LM? (e|f) 31.0 32.5 32.6
p+LM? (e|f) (Z?
cb
? (f)) 39.1 39.8 39.8
p+LM? (e|f) (Z?
sam
? (f)) 39.9 40.5 40.6
Table 2. Development set results for varying the approximation of the partition function in training, Z?cb? (f) vs. Z?sam? (f),
and decoding using the Viterbi max-derivation algorithm, or the max-translation algorithm with either a beam approxi-
mation or Monte-Carlo sampling.
we feeling the sampling approach is more theoreti-
cally justified and adopt it for our later experiments.
The most important result from this evaluation
is that both our training regimes realise substantial
gains from the introduction of the language model
feature. Thus we can be confident that our model
is capable of modelling the distribution over trans-
lations even when the space over all derivations is
intractable to dynamically program exactly.
4.2 A Discriminative Syntactic Translation
Model
In the previous sections we?ve described and evalu-
ated a statistical model of translation that is able to
estimate a probability distribution over translations
using millions of sparse features. A prime motiva-
tion for such a model is the ability to define com-
plex features over more than just the surface forms
of the source and target strings. There are limit-
less options for such features, and previous work
has focused on defining token based features such
as part-of-speech and morphology (Ittycheriah and
Roukos, 2007). Although such features are applica-
ble to our model, here we attempt to test the model?s
ability to incorporate complex features over source-
side syntax trees, essentially subsuming and extend-
ing previous work on tree-to-string translation mod-
els (Huang et al, 2006; Mi et al, 2008).
We first parse the source side of our training,
development and test corpora using the Stanford
parser.5 Next, while building the synchronous charts
5http://nlp.stanford.edu/software/lex-parser.shtml
required for training, whenever a rule is used in a
derivation a feature is activated which captures: (1)
the constituent spanning the rule?s source side in the
syntax tree (if any) (2) constituents spanning any
variables in the rule, and (3) the rule?s target side
surface form. Figure 3 illustrates this process.
These syntactic features are equivalent to the
grammar rules extracted for tree-to-string translation
systems. The key difference in our model is that the
source syntax tree is treated as conditioning context
and it?s information encoded as features, thus this
information can be used or ignored as the model sees
fit. This avoids the problems associated with explic-
itly encoding the source syntax in the grammar, such
as sparsity and overly constraining the model. In
addition we could easily incorporate features over
multiple source trees, for example mixing labelled
syntax trees with dependency graphs.
We limit the extraction of syntactic features to
those that appear in at least two training derivations,
giving a total of 4.2M syntactic features, for an over-
all total of 7.1M features.
4.3 Discussion
Table 3 shows the results from applying our
described models to the test set. We benchmark our
results against a model (Hiero) which was directly
trained to optimise BLEUNIST using the standard
MERT algorithm (Och, 2003) and the full set of
translation and lexical weight features described
for the Hiero model (Chiang, 2007). As well as
221
Model BLEUNIST BLEUIBM BLEUHumanRef
p?LM? (e|f) 33.5 35.2 25.2
p+LM? (e|f) 44.6 44.6 31.2
p+LM? (e|f) + syntax 45.3 45.2 31.8
MERT (BLEUNIST ) 46.2 44.5 30.2
Table 3. Test set results.
?? 
???
? ?? ?
V WH ?NN
NP
VP
SQ
Where is the currency exchange office ?
(Step 2) X
2
 -> < [X
1
] ? ?? ?, Where is the [X
1
] ?>
(Step 1) X
1
 -> <?? ???, currency exchange office>
NP
SQ
? ?? ?
Where is the [X
1
] ? Example Syntax feature =
for Step 2
Example Derivation:
X
1
(a) (b)
(c)
Figure 3. Syntax feature example: For the parsed source and candidate translation (a), with the derivation (b), we
extract the syntax feature in (c) by combining the grammar rule with the source syntax of the constituents contained
within that rule.
Source ???????????????????
p?LM? (e|f) don ?t have enough bag on me change please go purchase a new by plane .
p+LM? (e|f) i have enough money to buy a new one by air .
p+LM? (e|f) + syntax i don ?t have enough money to buy a new airline ticket .
MERT (BLEUNIST ) i don ?t have enough money to buy a new ticket .
Reference i do n?t have enough money with me to buy a new airplane ticket .
Table 4. Example test set output produced when: not using a language model, using a language model, also using
syntax, output optimised using MERT and finally the reference
BLEUNIST (brevity penalty uses the shortest ref-
erence), we also include results from the IBM
(BLEUIBM ) metric (brevity penalty uses the closest
reference), and using only the actual human transla-
tion in the test set, not the monolingual paraphrase
multiple references (BLEUHumanRef ).
The first result of interest is that we see an
increase in performance through the incorporation
of the source syntax features. This is an encourag-
ing result as the transcribed speech source sentences
are well out of the domain of the data on which the
parser was trained, suggesting that our model is able
to sift the good information from the noisy in the
unreliable source syntax trees. Table 4 shows illus-
trative system output on the test set.
On the BLEUNIST metric we see that our mod-
els under-perform the MERT trained system. We
hypothesise that this is predominately due to the
interaction of the brevity penalty with the unusual
nature of the multiple paraphrase reference train-
ing and development data. Their performance is
however quite consistent across the different inter-
pretations of the brevity penalty (NIST vs. IBM).
This contrasts with the MERT trained model, which
clearly over-fits to the NIST metric that it was
trained on and underperforms our models when eval-
uated on the single human test translations. If we
directly compare the brevity penalties of the MERT
model (0.868) and our discriminative model incor-
porating source syntax (0.942), on the these single
222
references, we can see that the MERT training has
optimised to the shortest paraphrase reference.
From these results it is difficult to draw any hard
conclusions on the relative performance of the dif-
ferent training regimes. However we feel confident
in claiming that we have achieved our goal of train-
ing a probabilistic model on millions of sparse fea-
tures which obtains performance competitive with
the current state-of-the-art training algorithm.
5 Conclusion
In this paper we have shown that statistical machine
translation can be effectively modelled as a well
posed machine learning task. In doing so we have
described a model capable of estimating a probabil-
ity distribution over translations using sparse com-
plex features, and achieving performance compara-
ble to the state-of-the-art on standard metrics. With
further work on scaling these models to large data
sets, and engineering high performance features, we
believe this research has the potential to provide sig-
nificant increases in translation quality.
Acknowledgements
The authors acknowledge the support of the EPSRC
grant EP/D074959/1.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of the 46th Annual Con-
ference of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08:HLT),
pages 200?208, Columbus, Ohio, June.
Jean-Ce?dric Chappelier and Martin Rajman. 2000.
Monte-carlo sampling for np-hard maximization prob-
lems in the framework of weighted parsing. In NLP
?00: Proceedings of the Second International Confer-
ence on Natural Language Processing, pages 106?
117, London, UK.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Pittsburgh, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 961?968, Sydney,
Australia, July.
Joshua T. Goodman. 1998. Parsing inside-out. Ph.D.
thesis, Cambridge, MA, USA. Adviser-Stuart Shieber.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In In Proceedings of the 7th Bien-
nial Conference of the Association for Machine Trans-
lation in the Americas (AMTA), Boston, MA.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57?64, Rochester, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. of the 4th International Conference on Human
Language Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 169?
176.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465?488.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of the
44th Annual Meeting of the ACL and 21st Inter-
national Conference on Computational Linguistics
(COLING/ACL-2006), pages 761?768, Sydney, Aus-
tralia, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of the 46th Annual Confer-
ence of the Association for Computational Linguistics:
Human Language Technologies (ACL-08:HLT), pages
192?199, Columbus, Ohio, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the ACL and 3rd Annual Meeting of
the NAACL (ACL-2002), pages 311?318, Philadelphia,
Pennsylvania.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of the 44th Annual
Meeting of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
pages 777?784, Sydney, Australia.
223
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 745?754,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Predicting Success in Machine Translation
Alexandra Birch Miles Osborne Philipp Koehn
a.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
The performance of machine translation sys-
tems varies greatly depending on the source
and target languages involved. Determining
the contribution of different characteristics of
language pairs on system performance is key
to knowing what aspects of machine transla-
tion to improve and which are irrelevant. This
paper investigates the effect of different ex-
planatory variables on the performance of a
phrase-based system for 110 European lan-
guage pairs. We show that three factors are
strong predictors of performance in isolation:
the amount of reordering, the morphological
complexity of the target language and the his-
torical relatedness of the two languages. To-
gether, these factors contribute 75% to the
variability of the performance of the system.
1 Introduction
Statistical machine translation (SMT) has improved
over the last decade of intensive research, but for
some language pairs, translation quality is still low.
Certain systematic differences between languages
can be used to predict this. Many researchers have
speculated on the reasons whymachine translation is
hard. However, there has never been, to our knowl-
edge, an analysis of what the actual contribution of
different aspects of language pairs is to translation
performance. This understanding of where the diffi-
culties lie will allow researchers to know where to
most gainfully direct their efforts to improving the
current models of machine translation.
Many of the challenges of SMT were first out-
lined by Brown et al (1993). The original IBM
Models were broken down into separate translation
and distortion models, recognizing the importance
of word order differences in modeling translation.
Brown et al also highlighted the importance of mod-
eling morphology, both for reducing sparse counts
and improving parameter estimation and for the cor-
rect production of translated forms.We see these two
factors, reordering and morphology, as fundamental
to the quality of machine translation output, and we
would like to quantify their impact on system per-
formance.
It is not sufficient, however, to analyze the mor-
phological complexity of the source and target lan-
guages. It is also very important to know how sim-
ilar the morphology is between the two languages,
as two languages which are morphologically com-
plex in very similar ways, could be relatively easy
to translate. Therefore, we also include a measure of
the family relatedness of languages in our analysis.
The impact of these factors on translation is mea-
sured by using linear regression models. We perform
the analysis with data from 110 different language
pairs drawn from the Europarl project (Koehn,
2005). This contains parallel data for the 11 official
language pairs of the European Union, providing a
rich variety of different language characteristics for
our experiments. Many research papers report re-
sults on only one or two languages pairs. By analyz-
ing so many language pairs, we are able to provide
a much wider perspective on the challenges facing
machine translation. This analysis is important as it
provides very strong motivation for further research.
The findings of this paper are as follows: (1) each
of the main effects, reordering, target language com-
plexity and language relatedness, is a highly signif-
icant predictor of translation performance, (2) indi-
vidually these effects account for just over a third of
745
the variation of the BLEU score, (3) taken together,
they account for 75% of the variation of the BLEU
score, (4) when removing Finnish results as out-
liers, reordering explains the most variation, and fi-
nally (4) the morphological complexity of the source
language is uncorrelated with performance, which
suggests that any difficulties that arise with sparse
counts are insignificant under the experimental con-
ditions outlined in this paper.
2 Europarl
In order to analyze the influence of different lan-
guage pair characteristics on translation perfor-
mance, we need access to a large variety of compa-
rable parallel corpora. A good data source for this is
the Europarl Corpus (Koehn, 2005). It is a collection
of the proceedings of the European Parliament, dat-
ing back to 1996. Version 3 of the corpus consists of
up to 44 million words for each of the 11 official lan-
guages of the European Union: Danish (da), German
(de), Greek (el), English (en), Spanish (es), Finnish
(fi), French (fr), Italian (it), Dutch (nl), Portuguese
(pt), and Swedish (sv).
In trying to determine the effect of properties of
the languages involved in translation performance,
it is very important that other variables be kept con-
stant. Using Europarl, the size of the training data
for the different language pairs is very similar, and
there are no domain differences as all sentences are
roughly trained on translations of the same data.
3 Morphological Complexity
The morphological complexity of the language pairs
involved in translation is widely recognized as one
of the factors influencing translation performance.
However, most statistical translation systems treat
different inflected forms of the same lemma as com-
pletely independent of one another. This can result in
sparse statistics and poorly estimated models. Fur-
thermore, different variations of the lemma may re-
sult in crucial differences in meaning that affect the
quality of the translation.
Work on improving MT systems? treatment of
morphology has focussed on either reducing word
forms to lemmas to reduce sparsity (Goldwater
and McClosky, 2005; Talbot and Osborne, 2006)
or including morphological information in decod-
Language
Av. 
Voc
abu
lary
 Siz
e
en fr it es pt el nl sv da de fi
?
?
?
?
?
100k
200k
300k
400k
500k
Figure 1. Average vocabulary size for each language.
ing (Dyer, 2007).
Although there is a significant amount of research
into improving the treatment of morphology, in this
paper we aim to discover the effect that different lev-
els of morphology have on translation. We measure
the amount of morphological complexity that exists
in both languages and then relate this to translation
performance.
Some languages seem to be intuitively more com-
plex than others, for instance Finnish appears more
complex than English. There is, however, no obvi-
ous way of measuring this complexity. One method
of measuring complexity is by choosing a number
of hand-picked, intuitive properties called complex-
ity indicators (Bickel and Nichols, 2005) and then
to count their occurrences. Examples of morpholog-
ical complexity indicators could be the number of in-
flectional categories or morpheme types in a typical
sentence. This method suffers from the major draw-
back of finding a principled way of choosing which
of the many possible linguistic properties should be
included in the list of indicators.
A simple alternative employed by Koehn (2005)
is to use vocabulary size as a measure of morpho-
logical complexity. Vocabulary size is strongly in-
fluenced by the number of word forms affected by
number, case, tense etc. and it is also affected by the
number of agglutinations in the language. The com-
plexity of the morphology of languages can there-
fore be approached by looking at vocabulary size.
746
Figure 1 shows the vocabulary size for all rele-
vant languages. Each language pair has a slightly
different parallel corpus, and so the size of the vo-
cabularies for each language needs to be averaged.
You can see that the size of the Finnish vocabulary is
about six times larger (510,632 words) than the En-
glish vocabulary size (88,880 words). The reason for
the large vocabulary size is that Finnish is character-
ized by a rich inflectional morphology, and it is typo-
logically classified as an agglutinative-fusional lan-
guage. As a result, words are often polymorphemic,
and become remarkably long.
4 Language Relatedness
The morphological complexity of each language in
isolation could be misleading. Large differences in
morphology between two languages could be more
relevant to translation performance than a complex
morphology that is very similar in both languages.
Languages which are closely related could share
morphological forms which might be captured rea-
sonably well in translation models. We include a
measure of language relatedness in our analyses to
take this into account.
Comparative linguistics is a field of linguistics
which aims to determine the historical relatedness
of languages. Lexicostatistics, developed by Morris
Swadesh in the 1950s (Swadesh, 1955), is an ap-
proach to comparative linguistics that is appropriate
for our purposes because it results in a quantitative
measure of relatedness by comparing lists of lexical
cognates.
The lexicostatistic percentages are extracted as
follows. First, a list of universal culture-free mean-
ings are generated. Words are then collected for
these meanings for each language under consider-
ation. Lists for particular purposes have been gen-
erated. For example, we use the data from Dyen et
al. (1992) who developed a list of 200 meanings for
84 Indo-European languages. Cognacy decisions are
then made by a trained linguist. For each pair of lists
the cognacy of a form can be positive, negative or in-
determinate. Finally, the lexicostatistic percentage is
calculated. This percentage is related to the propor-
tion of meanings for a particular language pair that
are cognates, i.e. relative to the total without inde-
terminacy. Factors such as borrowing, tradition and
Language ?animal? ?black?
French animal noir
Italian animale nero
Spanish animal negro
English animal black
German tier schwarz
Swedish djur svart
Danish dyr sort
Dutch dier zwart
Table 1. An example from the (Dyen et al, 1992) cognate
list.
taboo can skew the results.
A portion of the Dyen et al (1992) data set is
shown in Table 1 as an example. From this data a
trained linguist would calculate the relatedness of
French, Italian and Spanish as 100% because their
words for ?animal? and ?black? are cognates. The
Romance languages share one cognate with English,
?animal? but not ?black?, which means that the lex-
icostatistic percentage here would be 50%, and no
cognates with the rest of the languages, 0%.
We use the Dyen lexicostatistic percentages as our
measure of language relatedness or similarity for all
bidirectional language pairs except for Finnish, for
which there is not data. Finnish is a Finno-Ugric
language and is not part of the Indo-European lan-
guage family and is therefore not included in the
Dyen results. We were not able to recreate the con-
ditions of this study to generate the data for Finnish
- expert linguists with knowledge of all the lan-
guages would be required. Excluding Finnish would
have been a shame as it is an interesting language
to look at, however we took care to confirm which
effects found in this paper still held when exclud-
ing Finnish. Not being part of the Indo-European
languages means that its historical similarity with
our other languages is very low. For example, En-
glish would be more closely related to Hindu than to
Finnish. We therefore assume that Finnish has zero
similarity with the other languages in the set.
Figure 2 shows the symmetric matrix of language
relatedness, where the width of the square is pro-
portional to the value of relatedness. Finnish is the
language which is least related to the other lan-
guages and has a relatedness score of 0%. Spanish-
Portuguese is the most related language pair with a
747
it sv en el pt da es fr nl fi de
it
sv
en
el
pt
da
es
fr
nl
fi
de
= 0.17 = 0.35 = 0.52 = 0.7 = 0.87
Figure 2. Language relatedness - the width of the squares
indicates the lexicostatical relatedness.
score of 0.87%.
A measure of family relatedness should improve
our understanding of the relationship between mor-
phological complexity and translation performance.
5 Reordering
Reordering refers to differences in word order that
occur in a parallel corpus and the amount of reorder-
ing affects the performance of a machine translation
system. In order to determine how much it affects
performance, we first need to measure it.
5.1 Extracting Reorderings
Reordering is largely driven by syntactic differences
between languages and can involve complex rear-
rangements between nodes in synchronous trees.
Modeling reordering exactly would require a syn-
chronous tree-substitution grammar. This represen-
tation would be sparse and heterogeneous, limiting
its usefulness as a basis for analysis. We make an
important simplifying assumption in order for the
detection and extraction of reordering data to be
tractable and useful. We assume that reordering is
a binary process occurring between two blocks that
are adjacent in the source. This is similar to the
ITG constraint (Wu, 1997), however our reorder-
ings are not dependent on a synchronous grammar
or a derivation which covers the sentences. There are
also similarities with the Human-Targeted Transla-
tion Edit Rate metric (HTER) (Snover et al, 2006)
which attempts to find the minimum number of hu-
man edits to correct a hypothesis, and admits mov-
ing blocks of words, however our algorithm is auto-
matic and does not consider inserts or deletes.
Before describing the extraction of reorderings we
need to define some concepts. We define a block A
as consisting of a source span, As, which contains
the positions from Asmin to Asmax and is aligned to
a set of target words. The minimum and maximum
positions (Atmin and Atmax) of the aligned target
words mark the block?s target span, At.
A reordering r consists of the two blocks rA and
rB , which are adjacent in the source and where the
relative order of the blocks in the source is reversed
in the target. More formally:
rAs < rBs , rAt > rBt , rAsmax = rBsmin ? 1
A consistent block means that betweenAtmin and
Atmax there are no target word positions aligned
to source words outside of the block?s source span
As. A reordering is consistent if the block projected
from rAsmin to rBsmax is consistent.
The following algorithm detects reorderings and
determines the dimensions of the blocks involved.
We step through all the source words, and if a word
is reordered in the target with respect to the previ-
ous source word, then a reordering is said to have
occurred. These two words are initially defined as
the blocks A and B. Then the algorithm attempts
to grow block A from this point towards the source
starting position, while the target span ofA is greater
than that of block B, and the new block A is consis-
tent. Finally it attempts to grow block B towards the
source end position, while the target span of B is
less than that of A and the new reordering is incon-
sistent.
See Figure 3 for an example of a sentence pair
with two reorderings. Initially a reordering is de-
tected between the Chinese words aligned to ?from?
and ?late?. The block A is grown from ?late? to in-
clude the whole phrase pair ?late last night?. Then
the block B is grown from ?from? to include ?Bei-
jing? and stops because the reordering is then con-
sistent. The next reordering is detected between ?ar-
rived in? and ?Beijing?. We can see that block A at-
tempts to grow as large a block as possible and block
748
Figure 3. A sentence pair from the test corpus, with its
alignment. Two reorderings are shown with two different
dash styles.
B attempts to grow the smallest block possible. The
reorderings thus extracted would be comparable to
those of a right-branching ITG with inversions. This
allows for syntactically plausible embedded reorder-
ings. This algorithm has the worst case complexity
of O(n
2
2 ) when the words in the target occur in the
reverse order to the words in the source.
5.2 Measuring Reordering
Our reordering extraction technique allows us to an-
alyze reorderings in corpora according to the dis-
tribution of reordering widths. In order to facilitate
the comparison of different corpora, we combine
statistics about individual reorderings into a sen-
tence level metric which is then averaged over a cor-
pus.
RQuantity =
?
r?R |rAs | + |rBs |
I
where R is the set of reorderings for a sentence, I
is the source sentence length, A and B are the two
blocks involved in the reordering, and |rAs | is the
size or span of block A on the source side. RQuan-
tity is thus the sum of the spans of all the reordering
blocks on the source side, normalized by the length
of the source sentence.
RQuantity
Europarl, auto align 0.620
WMT06 test, auto align 0.647
WMT06 test, manual align 0.668
Table 2. The reordering quantity for the different reorder-
ing corpora for DE-EN.
5.3 Automatic Alignments
Reorderings extracted from manually aligned data
can be reliably assumed to be correct. The only
exception to this is that embedded reorderings are
always right branching and these might contradict
syntactic structure. In this paper, however, we use
alignments that are automatically extracted from the
training corpus using GIZA++. Automatic align-
ments could give very different reordering results.
In order to justify using reordering data extracted
from automatic alignments, we must show that they
are similar enough to gold standard alignments to be
useful as a measure of reordering.
5.3.1 Experimental Design
We select the German-English language pair be-
cause it has a reasonably high level of reordering. A
manually aligned German-English corpus was pro-
vided by Chris Callison-Burch and consists of the
first 220 sentences of test data from the 2006 ACL
Workshop on Machine Translation (WMT06) test
set. This test set is from a held out portion of the
Europarl corpus.
The automatic alignments were extracted by ap-
pending the manually aligned sentences on to the
respective Europarl v3 corpora and aligning them
using GIZA++ (Och and Ney, 2003) and the grow-
final-diag algorithm (Koehn et al, 2003).
5.3.2 Results
In order to use automatic alignments to extract re-
ordering statistics, we need to show that reorderings
from automatic alignments are comparable to those
from manual alignments.
We first look at global reordering statistics and
then we look in more detail at the reordering dis-
tribution of the corpora. Table 2 shows the amount
of reordering in the WMT06 test corpora, with both
manual and automatic alignments, and in the auto-
matically aligned Europarl DE-EN parallel corpus.
749
5 10 15 20
0.0
0.2
0.4
0.6
0.8
1.0
Reordering Width
Av. 
Reo
rder
ings
 per 
Sen
tenc
e
ACL Test ManualACL Test AutomaticEuromatrix
Figure 4. Average number of reorderings per sentence
mapped against the total width of the reorderings for DE-
EN.
We can see that all three corpora show a similar
amount of reordering.
Figure 4 shows that the distribution of reorder-
ings between the three corpora is also very similar.
These results provide evidence to support our use of
automatic reorderings in lieu of manually annotated
alignments. Firstly, they show that our WMT06 test
corpus is very similar to the Europarl data, which
means that any conclusions that we reach using the
WMT06 test corpus will be valid for the Europarl
data. Secondly, they show that the reordering behav-
ior of this corpus is very similar when looking at
automatic vs. manual alignments.
Although differences between the reorderings de-
tected in the manually and automatically aligned
German-English corpora are minor, there we accept
that there could be a language pair whose real re-
ordering amount is very different to the expected
amount given by the automatic alignments. A par-
ticular language pair could have alignments that are
very unsuited to the stochastic assumptions of the
IBM or HMM alignment models. However, manu-
ally aligning 110 language pairs is impractical.
5.4 Amount of reordering for the matrix
Extracting the amount of reordering for each of the
110 language pairs in the matrix required a sam-
pling approach. We randomly extracted a subset of
2000 sentences from each of the parallel training
corpora. From this subset we then extracted the av-
Sou
rce 
Lan
gua
ges
it sv en el pt da es fr nl fi de
it
sv
en
el
pt
da
es
fr
nl
fi
de
= 0.13 = 0.25 = 0.38 = 0.51 = 0.64
Target Languages
Figure 5. Reordering amount - the width of the squares
indicates the amount of reordering or RQuantity.
erage RQuantity.
In Figure 5 the amount of reordering for each
of the language pairs is proportional to the width
of the relevant square. Note that the matrix is not
quite symmetrical - reordering results differ de-
pending on which language is chosen to measure
the reordering span. The lowest reordering scores
are generally for languages in the same language
group (like Portuguese-Spanish, 0.20, and Danish-
Swedish, 0.24) and the highest for languages from
different groups (like German-French, 0.64, and
Finnish-Spanish, 0.61).
5.5 Language similarity and reordering
In this paper we use linear regression models to de-
termine the correlation and significance of various
explanatory variables with the dependent variable,
the BLEU score. Ideally the explanatory variables
involved should be independent of each other, how-
ever the amount of reordering in a parallel corpus
could easily be influenced by family relatedness. We
investigate the correlation between these variables.
Figure 6 shows the plot of the reordering amount
against language similarity. The regression is highly
significant and has an R2 of 0.2347. This means that
reordering is correlated with language similarity and
that 23% of reordering can be explained by language
similarity.
750
ll
l
ll l lll l
l
l
ll
l l
l
l
l
ll
l ll
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
ll
l
l
l
lll
l
l
ll l
l
l
l l
ll
l ll lll ll lll l
ll l ll ll l
0.2 0.3 0.4 0.5 0.60
.0
0.2
0.4
0.6
0.8
Reordering Amount
Lang
uage
 Sim
ilarit
y
Figure 6. Reordering compared to language similarity
with regression.
6 Experimental Design
We used the phrase-based model Moses (Koehn et
al., 2007) for the experiments with all the standard
settings, including a lexicalized reordering model,
and a 5-gram language model. Tests were run on
the ACL WSMT 2008 test set (Callison-Burch et al,
2008).
6.1 Evaluation of Translation Performance
We use the BLEU score (Papineni et al, 2002) to
evaluate our systems. While the role of BLEU in
machine translation evaluation is a much discussed
topic, it is generally assumed to be a adequate metric
for comparing systems of the same type.
Figure 7 shows the BLEU score results for the ma-
trix. Comparing this figure to Figure 5 there seems
to be a clear negative correlation between reordering
amount and translation performance.
6.2 Regression Analysis
We perform multiple linear regression analyses us-
ing measures of morphological complexity, lan-
guage relatedness and reordering amount as our in-
dependent variables. The dependent variable is the
translation performance metric, the BLEU score.
We then use a t-test to determine whether the co-
efficients for the independent variables are reliably
different from zero. We also test how well the model
explains the data using an R2 test. The two-tailed
significance levels of coefficients and R2 are also
Sou
rce 
Lan
gua
ges
it sv en el pt da es fr nl fi de
it
sv
en
el
pt
da
es
fr
nl
fi
de
= 0.08 = 0.16 = 0.24 = 0.32 = 0.4
Target Languages
Figure 7. System performance - the width of the squares
indicates the system performance in terms of the BLEU
score.
Explanatory Variable Coefficient
Target Vocab. Size -3.885 ***
Language Similarity 3.274 ***
Reordering Amount -1.883 ***
Target Vocab. Size2 1.017 ***
Language Similarity2 -1.858 **
Interaction: Reord/Sim -1.4536 ***
Table 3. The impact of the various explanatory features
on the BLEU score via their coefficients in the minimal ad-
equate model.
given where * means p < 0.05, ** means p < 0.01,
and *** means p < 0.001.
7 Results
7.1 Combined Model
The first question we are interested in answering is
which factors contribute most and how they interact.
We fit a multiple regression model to the data. The
source vocabulary size has no significant effect on
the outcome. All explanatory variable vectors were
normalized to be more comparable.
In Table 3 we can see the relative contribution of
the different features to the model. Source vocabu-
lary size did not contribute significantly to the ex-
planatory power of this multiple regression model
and was therefore not included. The fraction of the
variance explained by the model, or its goodness of
fit, the R2, is 0.750 which means that 75% of the
751
variation in BLEU can be explained by these three
factors. The interaction of reordering amount and
language relatedness is the product of the values of
these two features, and in itself it is an important ex-
planatory feature.
To make sure that our regression is valid, we need
to consider the special case of Finnish. Data points
where Finnish is the target language are outliers.
Finnish has the lowest language similarity with all
other languages, and the largest vocabulary size. It
also has very high amounts of reordering, and the
lowest BLEU scores when it is the target language.
The multiple regression of Table 3 where Finnish as
the source and target language is excluded, shows
that all the effects are still very significant, with the
model?s R2 dropping only slightly to 0.68.
The coefficients of the variables in the multiple
regression model have only limited usefulness as a
measure of the impact of the explanatory variables
in the model. One important factor to consider is that
if the explanatory variables are highly correlated,
then the values of the coefficients are unstable. The
model could attribute more importance to one or the
other variable without changing the overall fit of the
model. This is the problem of multicollinearity. Our
explanatory variables are all correlated, but a large
amount of this correlation can be explained by look-
ing at language pairs with Finnish as the target lan-
guage. Excluding these data points, only language
relatedness and reordering amount are still corre-
lated, see Section 5.5 for more details.
7.2 Contribution in isolation
In order to establish the relative contribution of vari-
ables, we isolate their impact on the BLEU score by
modeling them in separate linear regression models.
Figure 8 shows a simple regression model over
the plot of BLEU scores against target vocabulary
size. This figure shows groups of data points with the
same target language in almost vertical lines. Each
language pair has a separate parallel training corpus,
but the target vocabulary size for one language will
be very similar in all of them. The variance in BLEU
amongst the group with the same target language is
then largely explained by the other factors, similarity
and reordering.
Figure 9 shows a simple regression model over the
plot of BLEU scores against source vocabulary size.
l
l
l
l
ll
l
ll
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
ll
l
lll
l
l
l
l
l
l
ll
l l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
lll l
l
l
l
l lll
llllll
l
l
l
l
l
l
l
l
l
lll
0.15
0.20
0.25
0.30
0.35
0.40
Target Vocabulary Size
BLE
U
| | | | |100k 200k 300k 400k 500k
Figure 8. BLEU score of experiments compared to target
vocabulary size showing regression
This regression model shows that in isolation source
vocabulary size is significant (p< 0.05), but that this
is due to the distorting effect of Finnish. Excluding
results that include Finnish, there is no longer any
significant correlation with BLEU. The source mor-
phology might be significant for models trained on
smaller data sets, where model parameters are more
sensitive to sparse counts.
Figure 10 shows the simple regression model over
the plot of BLEU scores against the amount of re-
ordering. This graph shows that with more reorder-
ing, the performance of the translation model re-
duces. Data points with low levels of reordering and
high BLEU scores tend to be language pairs where
both languages are Romance languages. High BLEU
scores with high levels of reordering tend to have
German as the source language and a Romance lan-
guage as the target.
Figure 11 shows the simple regression model over
the plot of BLEU scores against the amount of lan-
guage relatedness. The left hand line of points are
the results involving Finnish. The vertical group of
points just to the right, are results where Greek
is involved. The next set of points are the results
where the translation is between Germanic and Ro-
mance languages. The final cloud to the right are re-
sults where languages are in the same family, either
within the Romance or the Germanic languages.
Table 4 shows the amount of the variance of
BLEU explained by the different models. As these
752
l
l
l
l
ll
l
ll
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l ll
l
l
l
l
l
l
ll
l
l
ll
lll
l
lll
lll
l ll
l
l
l
ll ll
l
l
l
l
l
ll
l ll ll
ll
l
l
l
l
l
l
llll
0.15
0.20
0.25
0.30
0.35
0.40
Source Vocabulary Size
BLE
U
| | | | |100k 200k 300k 400k 500k
l
l
l
FinnishOther
Figure 9. BLEU score of experiments compared to source
vocabulary size highlighting the Finnish source vocabu-
lary data points. The regression includes Finnish in the
model.
Explanatory Variable R2
Target Vocab. Size 0.388 ***
Reordering Amount 0.384 ***
Language Similarity 0.366 ***
Source Vocab. Size 0.045 *
Excluding Finnish
Target Vocab. Size 0.219 ***
Reordering Amount 0.332 ***
Language Similarity 0.188 ***
Source Vocab. Size 0.007
Table 4. Goodness of fit of different simple linear regres-
sion models which use just one explanatory variable. The
significance level represents the level of probability that
the regression is appropriate. The second set of results
excludes Finnish in the source and target language.
are simple regression models, with just one explana-
tory variable, multicolinearity is avoided. This table
shows that each of the main effects explains about a
third of the variance of BLEU, which means that they
can be considered to be of equal importance. When
Finnish examples are removed, only reordering re-
tains its power, and target vocabulary and language
similarity reduce in importance and source vocabu-
lary size no longer correlates with performance.
8 Conclusion
We have broken down the relative impact of the
characteristics of different language pairs on trans-
l
l
l
l
l l
l
ll
ll
l
l
l
l
l
l l
l
l
l
l
l
l
l
l l l
l
l l
l
l
l
l
ll
l
l l l
l
l
l
l
l
l
l
ll
ll
l
ll
ll
l
l ll
ll l
ll
l
l ll
l
l
l
ll l l
l
l
l
ll
l
l l
l lll ll
ll
l
l
l
l
l
l
l
ll l
0.2 0.3 0.4 0.5 0.6
0.15
0.20
0.25
0.30
0.35
0.40
Reordering Amount
BLE
U
Figure 10. BLEU score of experiments compared to
amount of reordering.
l
l
l
l
ll
l
ll
ll
l
l
l
l
l
ll
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
ll
ll
l
l
l
l
l
l
ll
l
l
l
l
lll
ll
l l
l
ll ll
l
lll
l
l
l
l
l
ll
lll
l
l
l
l
l
l
llll
0.0 0.2 0.4 0.6 0.8
0.15
0.20
0.25
0.30
0.35
0.40
Language Similarity
BLE
U
Figure 11. BLEU score of experiments compared to lan-
guage relatedness.
lation performance. The analysis done is able to ac-
count for a large percentage (75%) of the variabil-
ity of the performance of the system, which shows
that we have captured the core challenges for the
phrase-based model. We have shown that their im-
pact is about the same, with reordering and target
vocabulary size each contributing about 0.38%.
These conclusions are only strictly relevant to the
model for which this analysis has been performed,
the phrase-based model. However, we suspect that
the conclusions would be similar for most statisti-
cal machine translation models because of their de-
pendence on automatic alignments. This will be the
topic of future work.
753
References
Balthasar Bickel and Johanna Nichols, 2005. The World
Atlas of Language Structures, chapter Inflectional syn-
thesis of the verb. Oxford University Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Isidore Dyen, Joseph Kruskal, and Paul Black. 1992. An
indoeuropean classification, a lexicostatistical experi-
ment. Transactions of the American Philosophical So-
ciety, 82(5).
Chris Dyer. 2007. The ?noisier channel?: Transla-
tion from morphologically complex languages. In
Proceedings on the Workshop on Statistical Machine
Translation, Prague, Czech Republic.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proceedings of Empirical Methods in Natural
Language Processing.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the
Human Language Technology and North American As-
sociation for Computational Linguistics Conference,
pages 127?133, Edmonton, Canada. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Association for Computational Linguistics Com-
panion Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT-
Summit.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):9?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, USA.
M Snover, B Dorr, R Schwartz, L Micciulla, and
J Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
Morris Swadesh. 1955. Lexicostatistic dating of prehis-
toric ethnic contacts. In Proceedings American Philo-
sophical Society, volume 96, pages 452?463.
David Talbot and Miles Osborne. 2006. Modelling lex-
ical redundancy for machine translation. In Proceed-
ings of the Association of Computational Linguistics,
Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
754
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 756?764,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Stream-based Randomised Language Models for SMT
Abby Levenberg
School of Informatics
University of Edinburgh
a.levenberg@sms.ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
Randomised techniques allow very big
language models to be represented suc-
cinctly. However, being batch-based
they are unsuitable for modelling an un-
bounded stream of language whilst main-
taining a constant error rate. We present a
novel randomised language model which
uses an online perfect hash function
to efficiently deal with unbounded text
streams. Translation experiments over
a text stream show that our online ran-
domised model matches the performance
of batch-based LMs without incurring the
computational overhead associated with
full retraining. This opens up the possibil-
ity of randomised language models which
continuously adapt to the massive volumes
of texts published on the Web each day.
1 Introduction
Language models (LM) are an integral feature
of statistical machine translation (SMT) systems.
They assign probabilities to generated hypothe-
ses in the target language informing lexical selec-
tion. The most common form of LMs in SMT
systems are smoothed n-gram models which pre-
dict a word based on a contextual history of n? 1
words. For some languages (such as English) tril-
lions of words are available for training purposes.
This fact, along with the observation that ma-
chine translation quality improves as the amount
of monolingual training material increases, has
lead to the introduction of randomised techniques
for representing large LMs in small space (Talbot
and Osborne, 2007; Talbot and Brants, 2008).
Randomised LMs (RLMs) solve the problem of
representing large, static LMs but they are batch
oriented and cannot incorporate new data with-
out fully retraining from scratch. This property
makes current RLMs ill-suited for modelling the
massive volume of textual material published daily
on the Web. We present a novel RLM which is
capable of incremental (re)training. We use ran-
dom hash functions coupled with an online perfect
hashing algorithm to represent n-grams in small
space. This makes it well-suited for dealing with
an unbounded stream of training material. To our
knowledge this is the first stream-based RLM re-
ported in the machine translation literature. As
well as introducing the basic stream-based RLM,
we also consider adaptation strategies. Perplex-
ity and translation results show that populating
the language model with material chronologically
close to test points yields good results. As with
previous randomised language models, our experi-
ments focus on machine translation but we also ex-
pect that our findings are general and should help
inform the design of other stream-based models.
Section 2 introduces the incrementally retrain-
able randomised LM and section 3 considers re-
lated work; Section 4 then considers the question
of how unbounded text streams should be mod-
elled. Sections 5 and 6 show stream-based trans-
lation results and properties of our novel data-
structure. Section 7 concludes the paper.
2 Online Bloomier Filter LM
Our online randomised LM (O-RLM) is based
on the dynamic Bloomier filter (Mortensen et al,
2005). It is a variant of the batch-based Bloomier
filter LM of Talbot and Brants (2008) which we
refer to as the TB-LM henceforth. As with the
TB-LM, the O-RLM uses random hash functions
to represent n-grams as fingerprints which is the
main source of space savings for the model.
2.1 Online Perfect Hashing
The key difference in our model as compared to
the TB-LM is we use an online perfect hashing
756
Figure 1: Inserting an n-gram into the dynamic Bloomier filter. Above: an n-gram is hashed to its target
bucket. Below: the n-gram is transformed into a fingerprint and the same target bucket is scanned. If a
collision occurs that n-gram is diverted to the overflow dictionary; otherwise the fingerprint is stored in
the bucket.
function instead of having to precompute the per-
fect hash offline prior to data insertion.
The online perfect hash function uses two data
structures: A and D. A is the main, randomised
data structure and is an array of b dictionaries
A
0
, . . . , A
b?1
. D is a lossless data structure which
handles collisions in A. Each of the dictionaries in
A is referred to as a ?bucket?. In our implementa-
tion the buckets are equally sized arrays of w-bit
cells. These cells hold the fingerprints and values
of n-grams (one n-gram-value pair per cell).
To insert an n-gram x and associated value
v(x) into the model, we select a bucket A
i
by
hashing x into the range i ? [0, . . . , b ? 1].
Each bucket has an associated random hash func-
tion, h
A
i
, drawn from a universal hash func-
tion (UHF) family h (Carter and Wegman, 1977),
which is then used to generate the n-gram finger-
print: f(x) = h
A
i
(x).
If the bucket A
i
is not full we conduct a scan of
its cells. If the fingerprint f(x) is not already en-
coded in the bucket A
i
we add the fingerprint and
value to the first empty cell available. We allocate
a preset number of the least significant bits of each
w-bit cell to hold v(x) and the remaining most sig-
nificant bits for f(x) but this is arbitrary. Any en-
coding scheme, such as the packed representation
of Talbot and Brants (2008), is viable here.
However, if f(x) ? A
i
already (there is a colli-
sion) we store the n-gram x and associated value
v(x) in the lossless overflow dictionary D instead.
D also holds the n-grams that were hashed to any
buckets that are already full.
To query for the value of an n-gram, we first
check if the gram is in the overflow dictionary D.
If it is, we return the associated value. Otherwise
we query A using the same hash functions and
procedure as insertion. If we find a matching fin-
gerprint in the appropriate bucket A
i
we have a
hit with high probability. Deletions and updates
are symmetric to querying except we reset the cell
to the null value or update its value respectively.
As with other randomised models we construct
queries with the appropriate sanity checks to lower
the error rate efficiently (Talbot and Brants, 2008).
2.2 Data Insertion
Initially we seed the language model with a large
corpus S in the usual manner associated with
batch LMs. Then, when processing the stream,
we aggregate n-gram counts for some consecu-
tive portion, or epoch, of the input stream. We
can vary the size of stream window. For example
we might batch-up a day or week?s worth of mate-
rial. Intuitively, smaller windows produce results
that are sensitive to small variation in the stream,
while longer windows (corresponding to data over
a longer time period) average out local spikes. The
exact window size is a matter of experimentation.
In our MT experiments (section 5) we can com-
pute counts within the streaming window exactly
but randomised approaches (such as the approxi-
mate counting schemes from section 3) can easily
be employed instead.
757
These n-grams and counts are then considered
for insertion into the online model. If we decide
to insert an n-gram, we either update the count of
that n-gram if we previously inserted it or else we
insert it as a new entry. Note that there is some
probability we may encounter a false positive and
update some other n-gram in the model.
2.3 Properties
The online perfect hash succeeds by associating
each n-gram with only one cell in A rather than
having it depend on cells (or bits) which may be
shared by other n-grams as with the TB-LM. Since
each n-gram?s encoding in the model uses distinct
bits and is independent of all other events it can
not corrupt other n-grams when deleted.
Adding the overflow dictionary D means that
we use more space than the TB-LM for the same
support. It is shown in Mortensen et al (2005) that
the expected size of D is a small fraction of the to-
tal number of events and its space usage comprises
less than O(|S|) bits with high probability.
There is a nonzero probability for false posi-
tives. Since the overflow dictionary D has no er-
rors, the expected error rate for our dynamic struc-
ture is the probability of a random collision in the
hash range of each h
A
i
for each bucket cell com-
pared. In our setup we have
Pr(falsepos) =
|A
i
|
2
|f(x)|
where |f(x)| is the number of bits of each w-bit
cell used for the fingerprint f(x). w also primar-
ily governs space used in the model. The O-RLM
assumes only valid updates and deletions are per-
formed (i.e. we do not remove or update entries
that were never inserted prior).
The O-RLM takes time linear to the input size
for training and uses worst-case constant time for
querying and deletions where the constant is de-
pendent on the number of cells per bucket in A.
The number of bucket cells also effects the overall
error rate significantly since smaller ranges reduce
the probability of a collision. However, too few
cells per bucket will result in many full buckets
when the bucket hash function is not highly IID.
2.4 Basic RLM Comparisons
Table 1 compares expected versus observed false
positive rates for the Bloom filter, TB-LM, and O-
RLM obtained by querying a model of approxi-
mately 280M events with 100K unseen n-grams.
LM Expected Observed RAM
Lossless 0 0 7450MB
Bloom 0.0039 0.0038 390MB
TB-LM 0.0039 0.0033 640MB
O-RLM 0.0039 0.0031 705MB
Table 1: Example false postive rates and corre-
sponding memory usage for all randomised LMs.
We see the bit-based Bloom filter uses signifi-
cantly less memory than the cell-based alternatives
and the O-RLM consumes more memory than the
TB-LM for the same expected error rate.
3 Related Work
3.1 Randomised Language Models
Talbot and Osborne (2007) used a Bloom filter
(Bloom, 1970) to encode a smoothed LM. A
Bloom filter (BF) represents a set S from arbitrary
domain U and supports membership queries such
as?Is x ? S??. The BF uses an array of m bits and
k independent UHFs each with range 0, . . . ,m?1.
For insertion, each item is hashed through the k
hash functions and the resulting target bits are set
to one. During testing, an event x ? U is passed
through the same k hash functions and if any bit
tested is zero then x was not in the support S.
The Bloomier filter directly represents key-
value pairs by using a table of cells and a family of
k associated hash functions (Chazelle et al, 2004).
Each key-value pair is associated with k cells in
the table via a perfect hash function. Talbot and
Brants (2008) used a Bloomier filter to encode a
LM. Before data can be added to the Bloomier fil-
ter, a greedy perfect hashing of all entries needs to
be computed in advance; this attempts to associate
each event in the support with one unique table cell
so no other entry collides with it. The procedure
can fail and might need to be repeated many times.
Neither of these two randomised language mod-
els are suitable for modelling a stream. Given the
fact that the stream is of unbounded size, we are
forced to delete items if we wish to maintain a
constant error rate and account for novel n-grams.
However, the Bloom filter LM nor the Bloomier
Filter LM support deletions. The bit sharing of the
Bloom filter (BF) LM (Talbot and Osborne, 2007)
means deletions may corrupt shared stored events.
The Bloomier filter LM (Talbot and Brants, 2008)
has a precomputed matching of keys shared be-
tween a constant number of cells in the filter array.
758
Deleting items from a Bloomier Filter without re-
computing the perfect hash will corrupt it.
3.2 Probabilistic Counting
Concurrent work has used approximate counting
schemes based on Morris (1978) to estimate in
small space frequencies over a high volume in-
put text stream (Van Durme and Lall, 2009; Goyal
et al, 2009). The space savings are due to com-
pact storage of counts and retention of only a
small subset of the available n-grams in the data
stream. Since the final LMs are still lossless (mod-
ulo counts), the resulting LM needs significant
space. It is trivial to use probabilistic counting
within our framework.
3.3 Compact Exact Language Models
Randomised algorithms are not the only com-
pact representation schemes. Church et al (2007)
looked at Golomb Coding and Brants et al (2007)
used tries in a distributed setting. These methods
are less succinct than randomised approaches.
3.4 Adaptive Language Models
There is a large literature on adaptive LMs from
the speech processing domain (Bellegarda, 2004).
The primary difference between the O-RLM and
other adaptive LMs is that we add and remove n-
grams from the model instead of adapting only the
parameters of the current support set.
3.5 Domain adaptation in Machine
Translation
Within MT there has been a variety of approaches
dealing with domain adaption (for example (Wu
et al, 2008; Koehn and Schroeder, 2007). Typi-
cally LMs are interpolated with one another, yield-
ing good results. These models are usually stat-
ically trained, exact and unable to deal with an
unbounded stream of monolingual data. Domain
adaptation has similarities with streaming, in that
our stream may be non-stationary. A crucial dif-
ference however is that the stream is of unbounded
length, whereas domain adaptation usually as-
sumes some finite and fixed training set.
4 Stream-based translation
Streaming algorithms have numerous applications
in mainstream computer science (Muthukrishnan,
2003) but to date there has been very little aware-
ness of this field within computational linguistics.
Figure 2: Stream-based translation. The online
RLM uses data from the target stream and the last
test point in the source stream for adaptation.
A text stream can be thought of as a unbounded
sequence of documents that are time-stamped and
we have access to them in strict chronological or-
der. The volume of the stream is so large we can
afford only a limited number of passes over the
data (typically one).
Text streams naturally arise on the Web when
millions of new documents are published each day
in many languages. For instance, 18 thousand
websites continuously publish news stories in 40
languages and there are millions of multilingual
blog postings per day. There are over 30 billion
e-mails sent daily and social networking sites, in-
cluding services such as Twitter, generate an adun-
dance of textual data in real time. Web crawlers
that spidered all these new documents would pro-
duce an unbounded input stream.
The stream-based translation scenario is as fol-
lows: we assume that each day we see a source
stream of many new newswire stories that need
translation. We also assume a stream of newswire
stories in the target language. Intuitively, since the
concurrent streams are from the same domain, we
can use the contexts provided in the target stream
to help with the translation of the source stream
(Figure 2). From a theoretical perspective, since
we cannot represent the entirety of the stream and
wish to maintain a constant error rate, we are
forced to throw some information away.
Given that the incoming text stream contains far
too much data to store in its entirety an immediate
question we would like to answer is: within our
LM, which subset of the target text stream should
759
 180
 200
 220
 240
 260
 280
 300
 20  25  30  35  40  45  50
pe
rp
le
xi
ty
weeks
Reuters 96-97 LM subsets
51-week baseline
20-week subset test 1
20-week subset test 2
Figure 3: Perplexity results using streamed data.
Perplexity decreases as we retrain LMs using data
chronologically closer to the (two) test dates.
we represent in our model?
Using perplexity, we investigated this question
using a text stream based on Reuter?s RCV1 text
collection (Rose et al, 2002). This contains 800k
time-stamped newswire stories from a full calen-
der year (8.20.1996 - 8.19.1997). We used the
SRILM (Stolcke, 2002) to construct an exact tri-
gram model built using all the RCV1 data with the
exception of the final week which we held out as
test data. This served as an oracle since we store
all of the stream.
We then trained multiple exact LMs of much
smaller sizes, coined subset LMs, to simulate
memory constraints. For a given date in the RCV1
stream, these subset LMs were trained using a
fixed window of previously seen documents up to
that data. Then we obtained perplexity results for
each subset LM against our test set.
Figure 3 shows an example. For this experiment
subset LMs were trained using a sliding window
of 20 weeks with the window advancing over a
period of three weeks each time. The two arcs
correspond to two different test sets drawn from
different days. The arcs show that recency has a
clear effect: populating LMs using material closer
to the test data date produces improved perplexity
performance. The LM chronologically closest to
a given test set has perplexity closest to the results
of the significantly larger baseline LM which uses
all the stream. As expected, using all of the data
yields the lowest perplexity.
We note that this is a robust finding, since we
also observe it in other domains. For example, we
Epoch Stream Window
1 08.20.1996 to 01.01.1997
2 01.02.1997 to 04.23.1997
3 04.24.1997 to 08.18.1997
Table 2: The stream timeline is divided into win-
dowed epochs for our recency experiments.
conducted the same tests over a stream of 18 bil-
lion tokens drawn from 80 million time-stamped
blog posts downloaded from the web with match-
ing results. The effect of recency on perplexity has
also been observed elsewhere (see, for example,
Rosenfeld (1995) and Whittaker (2001)).
Our experiments show that a possible way to
tackle stream-based translation is to always focus
the attention of the LM on the most recent part
of the stream. This means we remove data from
the model that came from the receding parts of the
stream and replace it with the present.
5 SMT Experiments
5.1 Experimental Setup
We used publicly available resources for all our
tests: for decoding we used Moses (Koehn and
Hoang, 2007) and our parallel data was taken from
the Spanish-English section of Europarl. For test
material, we translated 63 documents (800 sen-
tences) from three randomly selected dates spaced
throughout the RCV1 year (January 2nd, April
24, and August 19).1 This effectively divided the
stream into three epochs between the test dates (
table 2). We held out 300 sentences for minimum
error rate training (MERT) (Och, 2003) and opti-
mised the parameters of the feature functions of
the decoder for each experimental run.
The RCV1 is not a large corpus when compared
to the entire web but it is multilingual, chronologi-
cal, and large enough to enable us to test the effect
of recency in a translation setting.
5.2 Adaption
We looked at a number of ways of adapting the
O-RLM:
1. (Random) Randomly sample the stream and
for each new n-gram encountered, insert
1As RCV1 is not a parallel corpus we translated the ref-
erence documents ourselves. This parallel corpus is available
from the authors.
760
Order Full Epoch 1 Epoch 3
1 1.25M 0.6M 0.7M
2 14.6 M 6.8M 7.0M
3 50.6 M 21.3M 21.7M
4 90.3 M 34.8M 35.4M
5 114.7M 41.8M 42.6M
Total 271.5M 105M 107.5M
Table 3: Distinct n-grams (in millions) encoun-
tered in the full stream and example epochs.
it and remove some previously inserted n-
gram, irrespective of whether it was ever re-
quested by the decoder or is a prefix.
2. (Conservative) For each new n-gram en-
countered in the stream, insert it in the filter
and remove one previously inserted n-gram
which was never requested by the decoder.
To preserve consistency we do not remove
lower-order grams that are needed to estimate
backoff probability for higher-order smooth-
ing. Counts are updated for n-grams already
in the model if the new count observed is
larger than the current one.
3. (Severe) Differs from the conservative ap-
proach only in that we delete all unused n-
grams (i.e. all those not requested by the de-
coder in the previous translation task) from
the O-RLM before adapting with data from
the stream. This means the data structure is
sparsely populated for all runs.
All the TB-LMs and O-RLMs were unpruned 5-
gram models and used Stupid-backoff smoothing
(Brants et al, 2007) 2 with the backoff parameter
set to 0.4 as suggested. The number of distinct n-
grams encountered in the stream for two epochs is
shown in Table 3.
Table 6 shows translation results using these
adaption strategies. In practice, the random ap-
proach does not work while the conservative and
severe adaption techniques produce equivalent re-
sults due to the small proportion of data in the
model that is queried during decoding. All the MT
experiments that follow use the severe method and
the overflow dictionary always holds less than 1%
of the total elements in the model.
2Smoothing text input data streams poses an interesting
problem we hope to investigate in the future.
Date Lossless TB-LM O-RLM
Jan 37.83 37.12 37.17
Apr 34.88 34.21 34.79
Aug 29.05 28.52 28.44
Avg 33.92 33.28 33.46
Table 4: Baseline translation results in BLEU us-
ing data from the first stream epoch with a lossless
LM (4.5GB RAM), the TB-LM and the O-RLM
(300MB RAM). All LMs are static.
5.3 Training Regimes
We now consider stream-based translation. Our
first naive approach is to continually add new data
from the stream to the training set without delet-
ing anything. Given a constant memory bound this
strategy only increases the error rate over time as
discussed. Our second, computationally demand-
ing approach is, before each test point, to rebuild
the TB-LM from scratch using the stream data
from the most recent epoch as the training set.
This is batch retraining. The final approach in-
crementally retrains online. This utilizes the same
training data as above (the stream data from the
last epoch) but instead of full retraining it replaces
n-grams currently in the model with unseen n-
grams and counts encountered in the data stream.
5.4 Streaming Translation Results
Each table shows translation results for the three
different test times in the stream. All results re-
ported use the case-sensitive BLEU score.
For our baselines we use static LMs trained on
the first epoch?s data to test all three translation
points in the source stream. This is the tradi-
tional approach. We trained an exact, modified
Kneser-Ney smoothed LM (here we do not en-
force a memory constraint) and also used the TB-
LM and O-RLM to verify our structures adequecy.
Results are shown in table 4. The exact model
gives better performance overall due to the more
sophisticated smoothing used.
Table 5 shows results for a set of stream-based
LMs using the TB-LM and the O-RLM with mem-
ory bounds of 200MB and 300MB. As expected,
the naive models performance degrades over time
as we funnel more data into the TB-LM and the
error rises. The batch retrained TB-LMs and O-
RLMs have constant error rates of 1
2
8
and 1
2
12
and
so outperform the naive approach. Since the train-
ing data is identical we see (approximately) equal
761
Naive TB-LM Batch Retrained TB-LM O-RLM
Date 200MB 300MB 200MB 300MB 200MB 300MB
Jan 35.94 37.12 35.94 37.12 36.44 37.17
Apr 33.55 35.79 36.01 35.99 35.87 36.10
Aug 22.44 26.07 28.97 29.38 29.00 29.18
Avg 30.64 32.99 33.64 34.16 33.77 34.15
Table 5: Translation results for stream-based LMs in BLEU. Performance degrades with time using the
Naive approach. The batch retrained TB-LM and stream-based O-RLM use constant error rates of 1
2
8
and 1
2
12
.
performance from the batch retrained and online
models. We also see some improvement compared
to the static baselines when the LMs use the most
recent data from the target language stream with
respect to the current translation point.
The key difference is that each time we batch
retrain the TB-LM, we must compute a perfect
hashing of the new training set. This is computa-
tionally demanding since the perfect hashing algo-
rithm uses Monte Carlo randomisation which fails
routinely and must be repeated. To make the al-
gorithm tractable the training data set must be di-
vided into lexically sorted subsets as well. This
requires extra passes over the data which may not
be trivial in a streaming environment.
In contrast, the O-RLM is incrementally re-
trained online. This makes it more resource ef-
ficient since we find bits in the model for the n-
grams dynamically without using more memory
than we intially set. Note that even though the O-
RLM is theoretically less space efficient than the
TB-LM, when using the same amount of memory
translation performance is comparable.
6 O-RLM Properties
The previous experiments confirm that the O-
RLM can be employed as a LM in an SMT setting
but it is useful to get insight into the intrinsic prop-
erties of the data structure. Many of the properties
of the model, such as the number of bits per fin-
gerprint, follow directly from the TB-LM but the
relationship between the overflow dictionary and
the randomised buckets is novel.
Figures 4 and 5 shows properties of the O-RLM
while varying only the number of cells in each
bucket and keeping all other model parameters
constant. We test membership of n-grams in an
unseen corpus against those stored in the table.
Our tests were conducted over a larger stream of
1.25B n-grams from the Gigaword corpus(Graff,
Date Severe Random Conservative
Jan 36.44 36.44 36.44
Apr 35.87 31.08 35.51
Aug 29.00 19.31 29.14
Avg 33.77 29.11 33.70
Table 6: Adaptation results measured in BLEU.
Random deletions degrade performance when
adapting a 200MB O-RLM.
2003). We set our space usage to match the 3.08
bytes per n-gram reported in Talbot and Brants
(2008) and held out just over 1M unseen n-grams
to test the error rates of our models.
In Figure 4 we see a direct correlation between
model error and cells per buckets. As the num-
ber of cells decreases the false positive rate drops
as well since fewer cells to compare against per
bucket means a lower chance of producing colli-
sions. If the range is decreased too much though
more data is diverted to the overflow dictionary
due to many buckets reaching capacity when in-
serting and adapting. Clearly this is less space ef-
ficient. Figure 5 shows the relationship between
the percent of data in the overflow dictionary and
the total cells per bucket.
7 Conclusions
Our experiments have shown that for stream-based
translation, using recent data can benefit perfor-
mance but simply adding entries to a randomised
representation will only reduce translation perfor-
mance over time. We have presented a novel ran-
domised language model based on dynamic per-
fect hashing that supports online insertions and
deletions. As a consequence, it is considerably
faster and more efficient than batch retraining.
While not advocating the idea that only small
amounts of data are needed for language mod-
762
 0
 0.001
 0.002
 0.003
 0.004
 0.005
 0.006
 0.007
 50  100  150  200  250
fa
ls
e 
po
si
tiv
e 
ra
te
s
cells per bucket
O-RLM Error rate
Figure 4: The O-RLM error rises in correlation
with the number of cells per bucket.
 0
 0.002
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 0.016
 0.018
 50  100  150  200  250
%
 o
f d
at
a 
in
 o
ve
rf
lo
w
 d
ic
tio
na
ry
cells per bucket
Overflow Dictionary Size
Figure 5: Too few cells per bucket causes a higher
percentage of the data to be stored in the overflow
dictionary due to full buckets.
elling, within a bounded amount of space our re-
sults show that it is better to have a low error rate
and store a wisely chosen fraction of the data than
having a high error rate and storing more of it.
Clearly tradeoffs will vary between applications.
This is the first stream-based randomised lan-
guage model and associated machine translation
system reported in the literature. Clearly there are
many interesting open questions for future work.
For example, can we use small randomised repre-
sentations called sketches to compactly represent
side-information on the stream telling us which as-
pects of it we should insert into our data? How
can we efficiently deal with smoothing in this set-
ting? Our adaptation scheme is simple and our
data stream is tractable. Currently we are con-
ducting tests over much larger, higher variance
text streams from crawled blog data. In the fu-
ture we will also consider randomised representa-
tions of other adaptive LMs in the literature using
a static background LM in conjunction with our
online one. We ultimately hope to deploy large-
scale LMs which continuously adapt to the vast
amount of material published on the Web without
incurring significant computational overhead.
Acknowledgements
The authors would like to thank David Talbot,
Adam Lopez and Phil Blunsom for their valu-
able comments and insight. This work was sup-
ported in part under the GALE program of the De-
fense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022.
References
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93?108.
Burton H. Bloom. 1970. Space/time trade-offs in
hash coding with allowable errors. Commun. ACM,
13(7):422?426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867.
J. Lawrence Carter and Mark N. Wegman. 1977. Uni-
versal classes of hash functions (extended abstract).
In STOC ?77: Proceedings of the ninth annual ACM
symposium on Theory of computing, pages 106?112,
New York, NY, USA. ACM Press.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: an ef-
ficient data structure for static support lookup ta-
bles. In SODA ?04: Proceedings of the fifteenth an-
nual ACM-SIAM symposium on Discrete algorithms,
pages 30?39, Philadelphia, PA, USA. Society for In-
dustrial and Applied Mathematics.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 199?207,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
763
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Boulder, CO.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium (LDC-2003T05).
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Robert Morris. 1978. Counting large numbers
of events in small registers. Commun. ACM,
21(10):840?842.
Christian Worm Mortensen, Rasmus Pagh, and Mihai
Pa?trac?cu. 2005. On dynamic range reporting in one
dimension. In STOC ?05: Proceedings of the thirty-
seventh annual ACM symposium on Theory of com-
puting, pages 104?111, New York, NY, USA. ACM.
S. Muthukrishnan. 2003. Data streams: algorithms
and applications. In SODA ?03: Proceedings of the
fourteenth annual ACM-SIAM symposium on Dis-
crete algorithms, pages 413?413, Philadelphia, PA,
USA. Society for Industrial and Applied Mathemat-
ics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volume 1 - from yester-
days news to tomorrows language resources. In In
Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29?
31.
Ronald Rosenfeld. 1995. Optimizing lexical and n-
gram coverage via judicious use of linguistic data.
In In Proc. European Conf. on Speech Technology,
pages 1763?1766.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, 2002.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468?476.
Benjamin Van Durme and Ashwin Lall. 2009. Prob-
abilistic counting with randomized storage. In
Twenty-First International Joint Conference on Ar-
tificial Intelligence (IJCAI-09), Pasadena, CA, July.
E. W. D. Whittaker. 2001. Temporal adaptation of lan-
guage models. In In Adaptation Methods for Speech
Recognition, ISCA Tutorial and Research Workshop
(ITRW), pages 203?206.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 993?1000. Coling 2008 Organizing
Committee, August.
764
331
332
333
334
335
336
337
338
Re-evaluating the Role of BLEU in Machine Translation Research
Chris Callison-Burch Miles Osborne Philipp Koehn
School on Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
We argue that the machine translation
community is overly reliant on the Bleu
machine translation evaluation metric. We
show that an improved Bleu score is nei-
ther necessary nor sufficient for achieving
an actual improvement in translation qual-
ity, and give two significant counterex-
amples to Bleu?s correlation with human
judgments of quality. This offers new po-
tential for research which was previously
deemed unpromising by an inability to im-
prove upon Bleu scores.
1 Introduction
Over the past five years progress in machine trans-
lation, and to a lesser extent progress in natural
language generation tasks such as summarization,
has been driven by optimizing against n-gram-
based evaluation metrics such as Bleu (Papineni
et al, 2002). The statistical machine translation
community relies on the Bleu metric for the pur-
poses of evaluating incremental system changes
and optimizing systems through minimum er-
ror rate training (Och, 2003). Conference pa-
pers routinely claim improvements in translation
quality by reporting improved Bleu scores, while
neglecting to show any actual example transla-
tions. Workshops commonly compare systems us-
ing Bleu scores, often without confirming these
rankings through manual evaluation. All these
uses of Bleu are predicated on the assumption that
it correlates with human judgments of translation
quality, which has been shown to hold in many
cases (Doddington, 2002; Coughlin, 2003).
However, there is a question as to whether min-
imizing the error rate with respect to Bleu does in-
deed guarantee genuine translation improvements.
If Bleu?s correlation with human judgments has
been overestimated, then the field needs to ask it-
self whether it should continue to be driven by
Bleu to the extent that it currently is. In this
paper we give a number of counterexamples for
Bleu?s correlation with human judgments. We
show that under some circumstances an improve-
ment in Bleu is not sufficient to reflect a genuine
improvement in translation quality, and in other
circumstances that it is not necessary to improve
Bleu in order to achieve a noticeable improvement
in translation quality.
We argue that Bleu is insufficient by showing
that Bleu admits a huge amount of variation for
identically scored hypotheses. Typically there are
millions of variations on a hypothesis translation
that receive the same Bleu score. Because not all
these variations are equally grammatically or se-
mantically plausible there are translations which
have the same Bleu score but a worse human eval-
uation. We further illustrate that in practice a
higher Bleu score is not necessarily indicative of
better translation quality by giving two substantial
examples of Bleu vastly underestimating the trans-
lation quality of systems. Finally, we discuss ap-
propriate uses for Bleu and suggest that for some
research projects it may be preferable to use a fo-
cused, manual evaluation instead.
2 BLEU Detailed
The rationale behind the development of Bleu (Pa-
pineni et al, 2002) is that human evaluation of ma-
chine translation can be time consuming and ex-
pensive. An automatic evaluation metric, on the
other hand, can be used for frequent tasks like
monitoring incremental system changes during de-
velopment, which are seemingly infeasible in a
manual evaluation setting.
The way that Bleu and other automatic evalu-
ation metrics work is to compare the output of a
machine translation system against reference hu-
man translations. Machine translation evaluation
metrics differ from other metrics that use a refer-
ence, like the word error rate metric that is used
249
Orejuela appeared calm as he was led to the
American plane which will take him to Mi-
ami, Florida.
Orejuela appeared calm while being escorted
to the plane that would take him to Miami,
Florida.
Orejuela appeared calm as he was being led
to the American plane that was to carry him
to Miami in Florida.
Orejuela seemed quite calm as he was being
led to the American plane that would take
him to Miami in Florida.
Appeared calm when he was taken to
the American plane, which will to Miami,
Florida.
Table 1: A set of four reference translations, and
a hypothesis translation from the 2005 NIST MT
Evaluation
in speech recognition, because translations have a
degree of variation in terms of word choice and in
terms of variant ordering of some phrases.
Bleu attempts to capture allowable variation in
word choice through the use of multiple reference
translations (as proposed in Thompson (1991)).
In order to overcome the problem of variation in
phrase order, Bleu uses modified n-gram precision
instead of WER?s more strict string edit distance.
Bleu?s n-gram precision is modified to elimi-
nate repetitions that occur across sentences. For
example, even though the bigram ?to Miami? is
repeated across all four reference translations in
Table 1, it is counted only once in a hypothesis
translation. Table 2 shows the n-gram sets created
from the reference translations.
Papineni et al (2002) calculate their modified
precision score, pn, for each n-gram length by
summing over the matches for every hypothesis
sentence S in the complete corpus C as:
pn =
?
S?C
?
ngram?S Countmatched(ngram)
?
S?C
?
ngram?S Count(ngram)
Counting punctuation marks as separate tokens,
the hypothesis translation given in Table 1 has 15
unigram matches, 10 bigram matches, 5 trigram
matches (these are shown in bold in Table 2), and
three 4-gram matches (not shown). The hypoth-
esis translation contains a total of 18 unigrams,
17 bigrams, 16 trigrams, and 15 4-grams. If the
complete corpus consisted of this single sentence
1-grams: American, Florida, Miami, Orejuela, ap-
peared, as, being, calm, carry, escorted, he, him, in, led,
plane, quite, seemed, take, that, the, to, to, to, was , was,
which, while, will, would, ,, .
2-grams: American plane, Florida ., Miami ,, Miami
in, Orejuela appeared, Orejuela seemed, appeared calm,
as he, being escorted, being led, calm as, calm while, carry
him, escorted to, he was, him to, in Florida, led to, plane
that, plane which, quite calm, seemed quite, take him, that
was, that would, the American, the plane, to Miami, to
carry, to the, was being, was led, was to, which will, while
being, will take, would take, , Florida
3-grams: American plane that, American plane which,
Miami , Florida, Miami in Florida, Orejuela appeared
calm, Orejuela seemed quite, appeared calm as, appeared
calmwhile, as he was, being escorted to, being led to, calm
as he, calm while being, carry him to, escorted to the, he
was being, he was led, him to Miami, in Florida ., led to
the, plane that was, plane that would, plane which will,
quite calm as, seemed quite calm, take him to, that was to,
that would take, the American plane, the plane that, to
Miami ,, to Miami in, to carry him, to the American, to
the plane, was being led, was led to, was to carry, which
will take, while being escorted, will take him, would take
him, , Florida .
Table 2: The n-grams extracted from the refer-
ence translations, with matches from the hypoth-
esis translation in bold
then the modified precisions would be p1 = .83,
p2 = .59, p3 = .31, and p4 = .2. Each pn is com-
bined and can be weighted by specifying a weight
wn. In practice each pn is generally assigned an
equal weight.
Because Bleu is precision based, and because
recall is difficult to formulate over multiple refer-
ence translations, a brevity penalty is introduced to
compensate for the possibility of proposing high-
precision hypothesis translations which are too
short. The brevity penalty is calculated as:
BP =
{
1 if c > r
e1?r/c if c ? r
where c is the length of the corpus of hypothesis
translations, and r is the effective reference corpus
length.1
Thus, the Bleu score is calculated as
Bleu = BP ? exp(
N
?
n=1
wn logpn)
A Bleu score can range from 0 to 1, where
higher scores indicate closer matches to the ref-
erence translations, and where a score of 1 is as-
signed to a hypothesis translation which exactly
1The effective reference corpus length is calculated as the
sum of the single reference translation from each set which is
closest to the hypothesis translation.
250
matches one of the reference translations. A score
of 1 is also assigned to a hypothesis translation
which has matches for all its n-grams (up to the
maximum n measured by Bleu) in the clipped ref-
erence n-grams, and which has no brevity penalty.
The primary reason that Bleu is viewed as a use-
ful stand-in for manual evaluation is that it has
been shown to correlate with human judgments of
translation quality. Papineni et al (2002) showed
that Bleu correlated with human judgments in
its rankings of five Chinese-to-English machine
translation systems, and in its ability to distinguish
between human and machine translations. Bleu?s
correlation with human judgments has been fur-
ther tested in the annual NIST Machine Transla-
tion Evaluation exercise wherein Bleu?s rankings
of Arabic-to-English and Chinese-to-English sys-
tems is verified by manual evaluation.
In the next section we discuss theoretical rea-
sons why Bleu may not always correlate with hu-
man judgments.
3 Variations Allowed By BLEU
While Bleu attempts to capture allowable variation
in translation, it goes much further than it should.
In order to allow some amount of variant order in
phrases, Bleu places no explicit constraints on the
order that matching n-grams occur in. To allow
variation in word choice in translation Bleu uses
multiple reference translations, but puts very few
constraints on how n-gram matches can be drawn
from the multiple reference translations. Because
Bleu is underconstrained in these ways, it allows a
tremendous amount of variation ? far beyond what
could reasonably be considered acceptable varia-
tion in translation.
In this section we examine various permutations
and substitutions allowed by Bleu. We show that
for an average hypothesis translation there are mil-
lions of possible variants that would each receive
a similar Bleu score. We argue that because the
number of translations that score the same is so
large, it is unlikely that all of them will be judged
to be identical in quality by human annotators.
This means that it is possible to have items which
receive identical Bleu scores but are judged by hu-
mans to be worse. It is also therefore possible to
have a higher Bleu score without any genuine im-
provement in translation quality. In Sections 3.1
and 3.2 we examine ways of synthetically produc-
ing such variant translations.
3.1 Permuting phrases
One way in which variation can be introduced is
by permuting phrases within a hypothesis trans-
lation. A simple way of estimating a lower bound
on the number of ways that phrases in a hypothesis
translation can be reordered is to examine bigram
mismatches. Phrases that are bracketed by these
bigram mismatch sites can be freely permuted be-
cause reordering a hypothesis translation at these
points will not reduce the number of matching n-
grams and thus will not reduce the overall Bleu
score.
Here we denote bigram mismatches for the hy-
pothesis translation given in Table 1 with vertical
bars:
Appeared calm | when | he was | taken |
to the American plane | , | which will |
to Miami , Florida .
We can randomly produce other hypothesis trans-
lations that have the same Bleu score but are rad-
ically different from each other. Because Bleu
only takes order into account through rewarding
matches of higher order n-grams, a hypothesis
sentence may be freely permuted around these
bigram mismatch sites and without reducing the
Bleu score. Thus:
which will | he was | , | when | taken |
Appeared calm | to the American plane
| to Miami , Florida .
receives an identical score to the hypothesis trans-
lation in Table 1.
If b is the number of bigram matches in a hy-
pothesis translation, and k is its length, then there
are
(k ? b)! (1)
possible ways to generate similarly scored items
using only the words in the hypothesis transla-
tion.2 Thus for the example hypothesis transla-
tion there are at least 40,320 different ways of per-
muting the sentence and receiving a similar Bleu
score. The number of permutations varies with
respect to sentence length and number of bigram
mismatches. Therefore as a hypothesis translation
approaches being an identical match to one of the
reference translations, the amount of variance de-
creases significantly. So, as translations improve
2Note that in some cases randomly permuting the sen-
tence in this way may actually result in a greater number of
n-gram matches; however, one would not expect random per-
mutation to increase the human evaluation.
251
 0
 20
 40
 60
 80
 100
 120
 1  1e+10  1e+20  1e+30  1e+40  1e+50  1e+60  1e+70  1e+80
Se
nt
en
ce
 L
en
gt
h
Number of Permutations
Figure 1: Scatterplot of the length of each trans-
lation against its number of possible permutations
due to bigram mismatches for an entry in the 2005
NIST MT Eval
spurious variation goes down. However, at today?s
levels the amount of variation that Bleu admits is
unacceptably high. Figure 1 gives a scatterplot
of each of the hypothesis translations produced by
the second best Bleu system from the 2005 NIST
MT Evaluation. The number of possible permuta-
tions for some translations is greater than 1073.
3.2 Drawing different items from the
reference set
In addition to the factorial number of ways that
similarly scored Bleu items can be generated
by permuting phrases around bigram mismatch
points, additional variation may be synthesized
by drawing different items from the reference n-
grams. For example, since the hypothesis trans-
lation from Table 1 has a length of 18 with 15
unigram matches, 10 bigram matches, 5 trigram
matches, and three 4-gram matches, we can arti-
ficially construct an identically scored hypothesis
by drawing an identical number of matching n-
grams from the reference translations. Therefore
the far less plausible:
was being led to the | calm as he was |
would take | carry him | seemed quite |
when | taken
would receive the same Bleu score as the hypoth-
esis translation from Table 1, even though human
judges would assign it a much lower score.
This problem is made worse by the fact that
Bleu equally weights all items in the reference
sentences (Babych and Hartley, 2004). There-
fore omitting content-bearing lexical items does
not carry a greater penalty than omitting function
words.
The problem is further exacerbated by Bleu not
having any facilities for matching synonyms or
lexical variants. Therefore words in the hypothesis
that did not appear in the references (such as when
and taken in the hypothesis from Table 1) can be
substituted with arbitrary words because they do
not contribute towards the Bleu score. Under Bleu,
we could just as validly use the words black and
helicopters as we could when and taken.
The lack of recall combined with naive token
identity means that there can be overlap between
similar items in the multiple reference transla-
tions. For example we can produce a translation
which contains both the words carry and take even
though they arise from the same source word. The
chance of problems of this sort being introduced
increases as we add more reference translations.
3.3 Implication: BLEU cannot guarantee
correlation with human judgments
Bleu?s inability to distinguish between randomly
generated variations in translation hints that it may
not correlate with human judgments of translation
quality in some cases. As the number of identi-
cally scored variants goes up, the likelihood that
they would all be judged equally plausible goes
down. This is a theoretical point, and while the
variants are artificially constructed, it does high-
light the fact that Bleu is quite a crude measure-
ment of translation quality.
A number of prominent factors contribute to
Bleu?s crudeness:
? Synonyms and paraphrases are only handled
if they are in the set of multiple reference
translations.
? The scores for words are equally weighted
so missing out on content-bearing material
brings no additional penalty.
? The brevity penalty is a stop-gap measure to
compensate for the fairly serious problem of
not being able to calculate recall.
Each of these failures contributes to an increased
amount of inappropriately indistinguishable trans-
lations in the analysis presented above.
Given that Bleu can theoretically assign equal
scoring to translations of obvious different qual-
ity, it is logical that a higher Bleu score may not
252
Fluency
How do you judge the fluency of this translation?
5 = Flawless English
4 = Good English
3 = Non-native English
2 = Disfluent English
1 = Incomprehensible
Adequacy
How much of the meaning expressed in the refer-
ence translation is also expressed in the hypothesis
translation?
5 = All
4 = Most
3 = Much
2 = Little
1 = None
Table 3: The scales for manually assigned ade-
quacy and fluency scores
necessarily be indicative of a genuine improve-
ment in translation quality. This begs the question
as to whether this is only a theoretical concern or
whether Bleu?s inadequacies can come into play
in practice. In the next section we give two signif-
icant examples that show that Bleu can indeed fail
to correlate with human judgments in practice.
4 Failures in Practice: the 2005 NIST
MT Eval, and Systran v. SMT
The NIST Machine Translation Evaluation exer-
cise has run annually for the past five years as
part of DARPA?s TIDES program. The quality of
Chinese-to-English and Arabic-to-English transla-
tion systems is evaluated both by using Bleu score
and by conducting a manual evaluation. As such,
the NIST MT Eval provides an excellent source
of data that allows Bleu?s correlation with hu-
man judgments to be verified. Last year?s eval-
uation exercise (Lee and Przybocki, 2005) was
startling in that Bleu?s rankings of the Arabic-
English translation systems failed to fully corre-
spond to the manual evaluation. In particular, the
entry that was ranked 1st in the human evaluation
was ranked 6th by Bleu. In this section we exam-
ine Bleu?s failure to correctly rank this entry.
The manual evaluation conducted for the NIST
MT Eval is done by English speakers without ref-
erence to the original Arabic or Chinese docu-
ments. Two judges assigned each sentence in
Iran has already stated that Kharazi?s state-
ments to the conference because of the Jor-
danian King Abdullah II in which he stood
accused Iran of interfering in Iraqi affairs.
n-gram matches: 27 unigrams, 20 bigrams,
15 trigrams, and ten 4-grams
human scores: Adequacy:3,2 Fluency:3,2
Iran already announced that Kharrazi will not
attend the conference because of the state-
ments made by the Jordanian Monarch Ab-
dullah II who has accused Iran of interfering
in Iraqi affairs.
n-gram matches: 24 unigrams, 19 bigrams,
15 trigrams, and 12 4-grams
human scores: Adequacy:5,4 Fluency:5,4
Reference: Iran had already announced
Kharazi would boycott the conference after
Jordan?s King Abdullah II accused Iran of
meddling in Iraq?s affairs.
Table 4: Two hypothesis translations with similar
Bleu scores but different human scores, and one of
four reference translations
the hypothesis translations a subjective 1?5 score
along two axes: adequacy and fluency (LDC,
2005). Table 3 gives the interpretations of the
scores. When first evaluating fluency, the judges
are shown only the hypothesis translation. They
are then shown a reference translation and are
asked to judge the adequacy of the hypothesis sen-
tences.
Table 4 gives a comparison between the output
of the system that was ranked 2nd by Bleu3 (top)
and of the entry that was ranked 6th in Bleu but
1st in the human evaluation (bottom). The exam-
ple is interesting because the number of match-
ing n-grams for the two hypothesis translations
is roughly similar but the human scores are quite
different. The first hypothesis is less adequate
because it fails to indicated that Kharazi is boy-
cotting the conference, and because it inserts the
word stood before accused which makes the Ab-
dullah?s actions less clear. The second hypothe-
sis contains all of the information of the reference,
but uses some synonyms and paraphrases which
would not picked up on by Bleu: will not attend
for would boycott and interfering for meddling.
3The output of the system that was ranked 1st by Bleu is
not publicly available.
253
 2
 2.5
 3
 3.5
 4
 0.38  0.4  0.42  0.44  0.46  0.48  0.5  0.52
H
um
an
 S
co
re
Bleu Score
Adequacy
Correlation
Figure 2: Bleu scores plotted against human judg-
ments of adequacy, with R2 = 0.14 when the out-
lier entry is included
Figures 2 and 3 plot the average human score
for each of the seven NIST entries against its
Bleu score. It is notable that one entry received
a much higher human score than would be antici-
pated from its low Bleu score. The offending en-
try was unusual in that it was not fully automatic
machine translation; instead the entry was aided
by monolingual English speakers selecting among
alternative automatic translations of phrases in the
Arabic source sentences and post-editing the result
(Callison-Burch, 2005). The remaining six entries
were all fully automatic machine translation sys-
tems; in fact, they were all phrase-based statistical
machine translation system that had been trained
on the same parallel corpus and most used Bleu-
based minimum error rate training (Och, 2003) to
optimize the weights of their log linear models?
feature functions (Och and Ney, 2002).
This opens the possibility that in order for Bleu
to be valid only sufficiently similar systems should
be compared with one another. For instance, when
measuring correlation using Pearson?s we get a
very low correlation of R2 = 0.14 when the out-
lier in Figure 2 is included, but a strong R2 = 0.87
when it is excluded. Similarly Figure 3 goes from
R2 = 0.002 to a much stronger R2 = 0.742.
Systems which explore different areas of transla-
tion space may produce output which has differ-
ing characteristics, and might end up in different
regions of the human scores / Bleu score graph.
We investigated this by performing a manual
evaluation comparing the output of two statisti-
cal machine translation systems with a rule-based
machine translation, and seeing whether Bleu cor-
 2
 2.5
 3
 3.5
 4
 0.38  0.4  0.42  0.44  0.46  0.48  0.5  0.52
H
um
an
 S
co
re
Bleu Score
Fluency
Correlation
Figure 3: Bleu scores plotted against human judg-
ments of fluency, with R2 = 0.002 when the out-
lier entry is included
rectly ranked the systems. We used Systran for the
rule-based system, and used the French-English
portion of the Europarl corpus (Koehn, 2005) to
train the SMT systems and to evaluate all three
systems. We built the first phrase-based SMT sys-
tem with the complete set of Europarl data (14-
15 million words per language), and optimized its
feature functions using minimum error rate train-
ing in the standard way (Koehn, 2004). We eval-
uated it and the Systran system with Bleu using
a set of 2,000 held out sentence pairs, using the
same normalization and tokenization schemes on
both systems? output. We then built a number of
SMT systems with various portions of the training
corpus, and selected one that was trained with 164
of the data, which had a Bleu score that was close
to, but still higher than that for the rule-based sys-
tem.
We then performed a manual evaluation where
we had three judges assign fluency and adequacy
ratings for the English translations of 300 French
sentences for each of the three systems. These
scores are plotted against the systems? Bleu scores
in Figure 4. The graph shows that the Bleu score
for the rule-based system (Systran) vastly under-
estimates its actual quality. This serves as another
significant counter-example to Bleu?s correlation
with human judgments of translation quality, and
further increases the concern that Bleu may not be
appropriate for comparing systems which employ
different translation strategies.
254
 2
 2.5
 3
 3.5
 4
 4.5
 0.18  0.2  0.22  0.24  0.26  0.28  0.3
H
um
an
 S
co
re
Bleu Score
Adequacy
Fluency
SMT System 1
SMT System 2
Rule-based System
(Systran)
Figure 4: Bleu scores plotted against human
judgments of fluency and adequacy, showing that
Bleu vastly underestimates the quality of a non-
statistical system
5 Related Work
A number of projects in the past have looked into
ways of extending and improving the Bleu met-
ric. Doddington (2002) suggested changing Bleu?s
weighted geometric average of n-gram matches to
an arithmetic average, and calculating the brevity
penalty in a slightly different manner. Hovy and
Ravichandra (2003) suggested increasing Bleu?s
sensitivity to inappropriate phrase movement by
matching part-of-speech tag sequences against ref-
erence translations in addition to Bleu?s n-gram
matches. Babych and Hartley (2004) extend Bleu
by adding frequency weighting to lexical items
through TF/IDF as a way of placing greater em-
phasis on content-bearing words and phrases.
Two alternative automatic translation evaluation
metrics do a much better job at incorporating re-
call than Bleu does. Melamed et al (2003) for-
mulate a metric which measures translation accu-
racy in terms of precision and recall directly rather
than precision and a brevity penalty. Banerjee and
Lavie (2005) introduce the Meteor metric, which
also incorporates recall on the unigram level and
further provides facilities incorporating stemming,
and WordNet synonyms as a more flexible match.
Lin and Hovy (2003) as well as Soricut and Brill
(2004) present ways of extending the notion of n-
gram co-occurrence statistics over multiple refer-
ences, such as those used in Bleu, to other natural
language generation tasks such as summarization.
Both these approaches potentially suffer from the
same weaknesses that Bleu has in machine trans-
lation evaluation.
Coughlin (2003) performs a large-scale inves-
tigation of Bleu?s correlation with human judg-
ments, and finds one example that fails to corre-
late. Her future work section suggests that she
has preliminary evidence that statistical machine
translation systems receive a higher Bleu score
than their non-n-gram-based counterparts.
6 Conclusions
In this paper we have shown theoretical and prac-
tical evidence that Bleu may not correlate with hu-
man judgment to the degree that it is currently be-
lieved to do. We have shown that Bleu?s rather
coarse model of allowable variation in translation
can mean that an improved Bleu score is not suffi-
cient to reflect a genuine improvement in transla-
tion quality. We have further shown that it is not
necessary to receive a higher Bleu score in order
to be judged to have better translation quality by
human subjects, as illustrated in the 2005 NIST
Machine Translation Evaluation and our experi-
ment manually evaluating Systran and SMT trans-
lations.
What conclusions can we draw from this?
Should we give up on using Bleu entirely? We
think that the advantages of Bleu are still very
strong; automatic evaluation metrics are inexpen-
sive, and do allow many tasks to be performed
that would otherwise be impossible. The impor-
tant thing therefore is to recognize which uses of
Bleu are appropriate and which uses are not.
Appropriate uses for Bleu include tracking
broad, incremental changes to a single system,
comparing systems which employ similar trans-
lation strategies (such as comparing phrase-based
statistical machine translation systems with other
phrase-based statistical machine translation sys-
tems), and using Bleu as an objective function to
optimize the values of parameters such as feature
weights in log linear translation models, until a
better metric has been proposed.
Inappropriate uses for Bleu include comparing
systems which employ radically different strate-
gies (especially comparing phrase-based statistical
machine translation systems against systems that
do not employ similar n-gram-based approaches),
trying to detect improvements for aspects of trans-
lation that are not modeled well by Bleu, and
monitoring improvements that occur infrequently
within a test corpus.
These comments do not apply solely to Bleu.
255
Meteor (Banerjee and Lavie, 2005), Precision and
Recall (Melamed et al, 2003), and other such au-
tomatic metrics may also be affected to a greater
or lesser degree because they are all quite rough
measures of translation similarity, and have inex-
act models of allowable variation in translation.
Finally, that the fact that Bleu?s correlation with
human judgments has been drawn into question
may warrant a re-examination of past work which
failed to show improvements in Bleu. For ex-
ample, work which failed to detect improvements
in translation quality with the integration of word
sense disambiguation (Carpuat and Wu, 2005), or
work which attempted to integrate syntactic infor-
mation but which failed to improve Bleu (Char-
niak et al, 2003; Och et al, 2004) may deserve a
second look with a more targeted manual evalua-
tion.
Acknowledgments
The authors are grateful to Amittai Axelrod,
Frank Keller, Beata Kouchnir, Jean Senellart, and
Matthew Stone for their feedback on drafts of this
paper, and to Systran for providing translations of
the Europarl test set.
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing the Bleu MT evaluation method with frequency
weightings. In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, Michigan.
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of ACL.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proceedings of MT Summit IX.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Human Language Technol-
ogy: Notebook Proceedings, pages 128?132, San
Diego.
Eduard Hovy and Deepak Ravichandra. 2003. Holy
and unholy grails. Panel Discussion at MT Summit
IX.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee and Mark Przybocki. 2005. NIST 2005
machine translation evaluation official results. Of-
ficial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Ed Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT-NAACL.
Dan Melamed, Ryan Green, and Jospeh P. Turian.
2003. Precision and recall of machine translation.
In Proceedings of HLT/NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of NAACL-04, Boston.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Radu Soricut and Eric Brill. 2004. A unified frame-
work for automatic evaluation using n-gram co-
occurrence statistics. In Proceedings of ACL.
Henry Thompson. 1991. Automatic evaluation of
translation quality: Outline of methodology and re-
port on pilot experiment. In (ISSCO) Proceedings
of the Evaluators Forum, pages 215?223, Geneva,
Switzerland.
256
Regularisation Techniques for Conditional
Random Fields: Parameterised Versus
Parameter-Free
Andrew Smith and Miles Osborne
School of Informatics, University of Edinburgh, United Kingdom
a.p.smith-2@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract. Recent work on Conditional Random Fields (CRFs) has
demonstrated the need for regularisation when applying these models
to real-world NLP data sets. Conventional approaches to regularising
CRFs has focused on using a Gaussian prior over the model parameters.
In this paper we explore other possibilities for CRF regularisation. We
examine alternative choices of prior distribution and we relax the usual
simplifying assumptions made with the use of a prior, such as constant
hyperparameter values across features. In addition, we contrast the effec-
tiveness of priors with an alternative, parameter-free approach. Specifi-
cally, we employ logarithmic opinion pools (LOPs). Our results show
that a LOP of CRFs can outperform a standard unregularised CRF and
attain a performance level close to that of a regularised CRF, without
the need for intensive hyperparameter search.
1 Introduction
Recent work on Conditional Random Fields (CRFs) has demonstrated the need
for regularisation when applying these models to real-world NLP data sets ([8],
[9]). Standard approaches to regularising CRFs, and log-linear models in general,
has focused on the use of a Gaussian prior. Typically, for simplicity, this prior is
assumed to have zero mean and constant variance across model parameters. To
date, there has been little work exploring other possibilities. One exception is
Peng & McCallum [8]. They investigated feature-dependent variance for a Gaus-
sian prior, and explored different families of feature sets. They also compared
different priors for CRFs on an information extraction task.
In the first part of this paper, we compare priors for CRFs on standard
sequence labelling tasks in NLP: NER and POS tagging. Peng & McCallum
used variable hyperparameter values only for a Gaussian prior, based on feature
counts in the training data. We use an alternative Bayesian approach to mea-
sure confidence in empirical expected feature counts, and apply this to all the
priors we test. We also look at varying the Gaussian prior mean. Our results
show that: (1) considerable search is required to identify good hyperparameter
values for all priors (2) for optimal hyperparameter values, the priors we tested
perform roughly equally well (3) in some cases performance can be improved
using feature-dependent hyperparameter values.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 896?907, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Regularisation Techniques for Conditional Random Fields 897
As can be seen, a significant short-coming of using priors for CRF regular-
isation is the requirement for intensive search of hyperparameter space. In the
second part of the paper we contrast this parameterised prior approach with
an alternative, parameter-free method. We factor the CRF distribution into a
weighted product of individual expert CRF distributions, each focusing on a
particular subset of the distribution. We call this model a logarithmic opinion
pool (LOP) of CRFs (LOP-CRFs).
Our results show that LOP-CRFs, which are unregularised, can outperform
the unregularised standard CRF and attain a performance level that rivals that
of the standard CRF regularised with a prior. This performance may be achieved
with a considerably lower time for training by avoiding the need for intensive
hyperparameter search.
2 Conditional Random Fields
A linear chain CRF defines the conditional probability of a label sequence s
given an observed sequence o via:
p(s | o) = 1
Z(o)
exp
(
T+1
?
t=1
?
k
?kfk(st?1, st,o, t)
)
(1)
where T is the length of both sequences, ?k are parameters of the model and Z(o)
is the partition function that ensures (1) represents a probability distribution.
The functions fk are feature functions representing the occurrence of different
events in the sequences s and o.
The parameters ?k can be estimated by maximising the conditional log-
likelihood of a set of labelled training sequences. The log-likelihood is given by:
LL(?) =
?
o
p?(o)
?
s
p?(s|o)
[
T+1
?
t=1
? ? f(s,o, t)
]
?
?
o
p?(o) log Z(o; ?)
where p?(s|O) and p?(o) are empirical distributions defined by the training set. At
the maximum likelihood solution the model satisfies a set of feature constraints,
whereby the expected count of each feature under the model is equal to its
empirical count on the training data:
Ep?(o,s)[fk] ? Ep(s|o)[fk] = 0, ?k
In general this cannot be solved for the ?k in closed form so numerical routines
must be used. Malouf [6] and Sha & Pereira [9] show that gradient-based algo-
rithms, particularly limited memory variable metric (LMVM), require much less
time to reach convergence, for some NLP tasks, than the iterative scaling meth-
ods previously used for log-linear optimisation problems. In all our experiments
we use the LMVM method to train the CRFs.
For CRFs with general graphical structure, calculation of Ep(s|o)[fk] is in-
tractable, but for the linear chain case Lafferty et al [5] describe an efficient
898 A. Smith and M. Osborne
dynamic programming procedure for inference, similar in nature to the forward-
backward algorithm in hidden Markov models.
Given a trained CRF model defined as in (1), the most probable labelling
under the model for a new observed sequence o is given by argmaxsp(s|o). This
can be recovered efficiently using the Viterbi algorithm.
3 Parameterised Regularisation: Priors for CRFs
Most approaches to CRF regularisation have focused on the use of a prior distri-
bution over the model parameters. A prior distribution encodes prior knowledge
about the nature of different models. However, prior knowledge can be difficult
to encode reliably and the optimal choice of prior family may vary from task to
task. In this paper we investigate the use of three prior families for the CRF.
3.1 Gaussian Prior
The most common prior used for CRF regularisation has been the Gaussian. Use
of the Gaussian prior assumes that each model parameter is drawn independently
from a Gaussian distribution. Ignoring terms that do not affect the parameters,
the regularised log-likelihood with a Gaussian prior becomes:
LL(?) ? 1
2
?
k
(
?k ? ?k
?k
)2
where ?k is the mean and ?k the variance for parameter ?k. At the optimal
point, for each ?k, the model satisfies:
Ep?(o,s)[fk] ? Ep(s|o)[fk] =
?k ? ?k
?2k
(2)
Usually, for simplicity, each ?k is assumed zero and ?k is held constant across
the parameters. In this paper we investigate other possibilities. In particular,
we allow the means to take on non-zero values, and the variances to be feature-
dependent. This is described in more detail later. In each case values for means
and variances may be optimised on a development set.
We can see from (2) that use of a Gaussian prior enforces the constraint that
the expected count of a feature under the model is discounted with respect to the
count of that feature on the training data. As discussed in [1], this corresponds
to a form of logarithmic discounting in feature count space and is similar in
nature to discounting schemes employed in language modelling.
3.2 Laplacian Prior
Use of the Laplacian prior assumes that each model parameter is drawn inde-
pendently from the Laplacian distribution. Ignoring terms that do not affect the
parameters, the regularised log-likelihood with a Laplacian prior becomes:
LL(?) ?
?
k
|?k|
?k
Regularisation Techniques for Conditional Random Fields 899
where ?k is a hyperparameter, and at the optimal point the model satisfies:
Ep?(o,s)[fk] ? Ep(s|o)[fk] =
sign(?k)
?k
, ?k = 0 (3)
Peng & McCallum [8] note that the exponential prior (a one-sided version of the
Laplacian prior here) represents applying an absolute discount to the empirical
feature count. They fix the ?k across features and set it using an expression for
the discount used in absolute discounting for language modelling. By contrast we
allow the ?k to vary with feature and optimise values using a development set.
The derivative of the penalty term above with respect to a parameter ?k is
discontinuous at ?k = 0. To tackle this problem we use an approach described
by Williams, who shows how the discontinuity may be handled algorithmically
[13]. His method leads to sparse solutions, where, at convergence, a substantial
proportion of the model parameters are zero. The result of this pruning effect is
different, however, to feature induction, where features are included in the model
based on their effect on log-likelihood.
3.3 Hyperbolic Prior
Use of the hyperbolic prior assumes that each model parameter is drawn inde-
pendently from the hyperbolic distribution. Ignoring constant terms that do not
involve the parameters, the regularised log-likelihood becomes:
LL(?) ?
?
k
log
(
e?k?k + e??k?k
2
)
where ?k is a hyperparameter, and at the optimal point the model satisfies:
Ep?(o,s)[fk] ? Ep(s|o)[fk] = ?k
(
e?k?k ? e??k?k
e?k?k + e??k?k
)
(4)
3.4 Feature-Dependent Regularisation
For simplicity it is usual when using a prior to assume constant hyperparameter
values across all features. However, as a hyperparameter value determines the
amount of regularisation applied to a feature, we may not want to assume equal
values. We may have seen some features more frequently than others and so
be more confident that their empirical expected counts are closer to the true
expected counts in the underlying distribution.
Peng & McCallum [8] explore feature-dependent variance for the Gaussian
prior. They use different schemes to determine the variance for a feature based
on its observed count in the training data. In this paper we take an alternative,
Bayesian approach motivated more directly by our confidence in the reliability
of a feature?s empirical expected count.
In equations (2), (3) and (4) the level of regularisation applied to a feature
takes the form of a discount to the expected count of the feature on the training
900 A. Smith and M. Osborne
data. It is natural, therefore, that the size of this discount, controlled through
a hyperparameter, is related to our confidence in the reliability of the empiri-
cal expected count. We formulate a measure of this confidence. We follow the
approach of Kazama & Tsujii [4], extending it to CRFs.
The empirical expected count, Ep?(o,s)[fk], of a feature fk is given by:
?
o,s
p?(o, s)
?
t
fk(st?1, st,o, t)=
?
o
p?(o)
?
s
p?(s|o)
?
t
fk(st?1, st,o, t)
=
?
o
p?(o)
?
t,s?,s??
p?(st?1 = s?, st = s??|o)fk(s?, s??,o, t)
Now, our CRF features have the following form:
fk(st?1, st,o, t) =
{
1 if st?1 = s1, st = s2 and hk(o, t) = 1
0 otherwise
where s1 and s2 are the labels associated with feature fk and hk(o, t) is a binary-
valued predicate defined on observation sequence o at position t. With this
feature definition, and contracting notation for the empirical probability to save
space, Ep?(o,s)[fk] becomes:
?
o
p?(o)
?
t,s?,s??
p?(s?, s??|o)?(s?, s1)?(s??, s2)hk(o, t) =
?
o
p?(o)
?
t
p?(s1, s2|o)hk(o, t)
=
?
o
p?(o)
?
t:hk(o,t)=1
p?(s1, s2|o)
Contributions to the inner sum are only made at positions t in sequence o where
the hk(o, t) = 1. Suppose that we make the assumption that at these positions
p?(s?, s??|o) ? p?(s?, s??|hk(o, t) = 1). Then:
Ep?(o,s)[fk] =
?
o
p?(o)
?
t:hk(o,t)=1
p?(s1, s2|hk(o, t) = 1)
Now, if we assume that we can get a reasonable estimate of p?(o) from the training
data then the only source of uncertainty in the expression for Ep?(o,s)[fk] is the term
p?(st?1 = s1, st = s2|hk(o, t) = 1). Assuming this term is independent of sequence
o and position t, we can model it as the parameter ? of a Bernoulli random variable
that takes the value 1 when feature fk is active and 0 when the feature is not active
but hk(o, t) = 1. Suppose there are a and b instances of these two events, respec-
tively. We endow the Bernoulli parameter with a uniform prior Beta distribution
Be(1,1) and, having observed the training data, we calculate the variance of the
posterior distribution, Be(1 + a, 1 + b). The variance is given by:
var[?] = V =
(1 + a)(1 + b)
(a + b + 2)2(a + b + 3)
The variance of Ep?(o,s)[fk] therefore given by:
var
[
Ep?(o,s)[fk]
]
= V
?
?
?
o
?
t:hk(o,t)=1
p?(o)2
?
?
Regularisation Techniques for Conditional Random Fields 901
We use this variance as a measure of the confidence we have in Ep?(o,s)[fk] as an
estimate of the true expected count of feature fk. We therefore adjust hyper-
parameters in the different priors according to this confidence for each feature.
Note that this value for each feature can be calculated off-line.
4 Parameter-Free Regularisation: Logarithmic Opinion
Pools
So far we have considered CRF regularisation through the use of a prior. As
we have seen, most prior distributions are parameterised by a hyperparameter,
which may be used to tune the level of regularisation. In this paper we also
consider a parameter-free method. Specifically, we explore the use of logarithmic
opinion pools [3].
Given a set of CRF model experts with conditional distributions p?(s|o)
and a set of non-negative weights w? with
?
? w? = 1, a logarithmic opinion
pool is defined as the distribution:
p?(s|o) = 1
Z?(o)
?
?
[p?(s|o)]w? , with Z?(o) =
?
s
?
?
[p?(s|o)]w?
Suppose that there is a ?true? conditional distribution q(s|o) which each
p?(s|o) is attempting to model. In [3] Heskes shows that the KL divergence
between q(s|o) and the LOP can be decomposed as follows:
K (q, p?) =
?
?
w?K (q, p?) ?
?
?
w?K (p?, p?) = E ? A (5)
This explicitly tells us that the closeness of the LOP model to q(s|o) is governed
by a trade-off between two terms: an E term, which represents the closeness of
the individual experts to q(s|o), and an A term, which represents the closeness of
the individual experts to the LOP, and therefore indirectly to each other. Hence
for the LOP to model q well, we desire models p? which are individually good
models of q (having low E) and are also diverse (having large A).
Training LOPs for CRFs. The weights w? may be defined a priori or may be
found by optimising an objective criterion. In this paper we combine pre-trained
expert CRF models under a LOP and train the weights w? to maximise the
likelihood of the training data under the LOP. See [10] for details.
Decoding LOPs for CRFs. Because of the log-linear form of a CRF, a
weighted product of expert CRF distributions corresponds to a single CRF distri-
bution with log potentials given by a linear combination (with the same weights)
of the corresponding log potentials of the experts. Consequently, it is easy to form
the LOP given a set of weights and expert models, and decoding with the LOP
is no more complex than decoding with a standard CRF. Hence LOP decoding
can be achieved efficiently using the Viterbi algorithm.
902 A. Smith and M. Osborne
5 The Tasks
In this paper we compare parametric and LOP-based regularisation techniques
for CRFs on two sequence labelling tasks in NLP: named entity recognition
(NER) and part-of-speech tagging (POS tagging).
5.1 Named Entity Recognition
All our results for NER are reported on the CoNLL-2003 shared task dataset
[12]. For this dataset the entity types are: persons (PER), locations (LOC),
organisations (ORG) and miscellaneous (MISC). The training set consists of
14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466
sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and
46, 666 tokens.
5.2 Part-of-Speech Tagging
For our experiments we use the CoNLL-2000 shared task dataset [11]. This has
48 different POS tags. In order to make training time manageable, we collapse
the number of POS tags from 48 to 5 following the procedure used in [7]. In
summary: (1) All types of noun collapse to category N. (2) All types of verb
collapse to category V. (3) All types of adjective collapse to category J. (4)
All types of adverb collapse to category R. (5) All other POS tags collapse to
category O. The training set consists of 7, 300 sentences and 173, 542 tokens, the
development set consists of 1, 636 sentences and 38, 185 tokens and the test set
consists of 2, 012 sentences and 47, 377 tokens.
5.3 Experts and Expert Sets
As we have seen, our parameter-free LOP models require us to define and train
a number of expert models. For each task we define a single, complex CRF,
which we call a monolithic CRF, and a range of expert sets. The monolithic
CRF for NER comprises a number of word and POS features in a window of five
words around the current word, along with a set of orthographic features defined
on the current word. The monolithic CRF for NER has 450, 345 features. The
monolithic CRF for POS tagging comprises word and POS features similar to
those in the NER monolithic model, but over a smaller number of orthographic
features. The monolithic model for POS tagging has 188, 488 features.
Each of our expert sets consists of a number of CRF experts. Usually these
experts are designed to focus on modelling a particular aspect or subset of the
distribution. The experts from a particular expert set are combined under a
LOP-CRF with the unregularised monolithic CRF.
We define our expert sets as follows: (1) Simple consists of the monolithic
CRF and a single expert comprising a reduced subset of the features in the
monolithic CRF. This reduced CRF models the entire distribution rather than
focusing on a particular aspect or subset, but is much less expressive than the
Regularisation Techniques for Conditional Random Fields 903
monolithic model. The reduced model comprises 24, 818 features for NER and
47, 420 features for POS tagging. (2) Positional consists of the monolithic CRF
and a partition of the features in the monolithic CRF into three experts, each
consisting only of features that involve events either behind, at or ahead of
the current sequence position. (3) Label consists of the monolithic CRF and a
partition of the features in the monolithic CRF into five experts, one for each
label. For NER an expert corresponding to label X consists only of features that
involve labels B-X or I-X at the current or previous positions, while for POS
tagging an expert corresponding to label X consists only of features that involve
label X at the current or previous positions. These experts therefore focus on
trying to model the distribution of a particular label. (4) Random consists of
the monolithic CRF and a random partition of the features in the monolithic
CRF into four experts. This acts as a baseline to ascertain the performance
that can be expected from an expert set that is not defined via any linguistic
intuition.
6 Experimental Results
For each task our baseline model is the monolithic model, as defined earlier.
All the smoothing approaches that we investigate are applied to this model. For
NER we report F-scores on the development and test sets, while for POS tagging
we report accuracies on the development and test sets.
6.1 Priors
Feature-Independent Hyperparameters. Tables 1 and 2 give results on the
two tasks for different priors with feature-independent hyperparameters. In the
case of the Gaussian prior, the mean was fixed at zero with the variance being the
adjustable hyperparameter. In each case hyperparameter values were optimised
on the development set. In order to obtain the results shown, extensive search
of the hyperparameter space was required. The results show that: (1) For each
prior there is a performance improvement over the unregularised model. (2) Each
of the priors gives roughly the same optimal performance.
These results are contrary to the conclusions of Peng & McCallum in [8]. On
an information extraction task they found that the Gaussian prior performed
Table 1. F-scores for priors on NER
Prior Development Test
Unreg. monolithic 88.33 81.87
Gaussian 89.84 83.98
Laplacian 89.56 83.43
Hyperbolic 89.84 83.90
Table 2. Accuracies for priors on POS
tagging
Prior Development Test
Unreg. monolithic 97.92 97.65
Gaussian 98.02 97.84
Laplacian 98.05 97.78
Hyperbolic 98.00 97.85
904 A. Smith and M. Osborne
significantly better than alternative priors. Indeed they appeared to report per-
formance figures for the hyperbolic and Laplacian priors that were lower than
those of the unregularised model. There are several possible reasons for these
differences. Firstly, for the hyperbolic prior, Peng & McCallum appeared not
to use an adjustable hyperparameter. In that case the discount applied to each
empirical expected feature count was dependent only on the current value of the
respective model parameter and corresponds in our case to using a fixed value
of 1 for the ? hyperparameter. Our results for this value of the hyperparameter
are similarly poor. The second reason is that for the Laplacian prior, they again
used a fixed value for the hyperparameter, calculated via an absolute discount-
ing method used language modelling [1]. Having achieved poor results with this
value they experimented with other values but obtained even worse performance.
By contrast, we find that, with some search of the hyperparameter space, we can
achieve performance close to that of the other two priors.
Feature-Dependent Hyperparameters. Tables 3 and 4 give results for dif-
ferent priors with feature-dependent hyperparameters. Again, for the Gaussian
prior the mean was held at 0. We see here that trends differ between the two
tasks. For POS tagging we see performance improvements with all the priors over
the corresponding feature-independent hyperparameter case. Using McNemar?s
matched-pairs test [2] on point-wise labelling errors, and testing at a significance
level of 5% level, all values in Table 4 represent a significant improvement over
the corresponding model with feature-independent hyperparameter values, ex-
cept the one marked with ?. However, for NER the opposite is true. There is a
performance degradation over the corresponding feature-independent hyperpa-
rameter case. Values marked with ? are significantly worse at the 5% level. The
hyperbolic prior performs particularly badly, giving no improvement over the
unregularised monolithic. The reasons for these results are not clear. One pos-
sibility is that defining the degree of regularisation on a feature specific basis is
too dependent on the sporadic properties of the training data. A better idea may
be to use an approach part-way between feature-independent hyperparameters
and feature-specific hyperparameters. For example, features could be clustered
based on confidence in their empirical expected counts, with a single confidence
being associated with each cluster.
Varying the Gaussian Mean. When using a Gaussian prior it is usual to fix
the mean at zero because there is usually no prior information to suggest penal-
ising large positive values of model parameters any more or less than large mag-
Table 3. F-scores for priors on NER
Prior Development Test
Gaussian 89.43 83.27?
Laplacian 89.28 83.37
Hyperbolic 88.34? 81.63?
Table 4. Accuracies for priors on POS
tagging
Prior Development Test
Gaussian 98.12 97.88?
Laplacian 98.12 97.92
Hyperbolic 98.15 97.92
Regularisation Techniques for Conditional Random Fields 905
nitude negative values. It also simplifies the hyperparameter search, requiring
the need to optimise only the variance hyperparameter. However, it is unlikely
that optimal performance is always achieved for a mean value of zero.
To investigate this we fix the Gaussian variance at the optimal value found
earlier on the development set, with a mean of zero, and allow the mean to
vary away from zero. For both tasks we found that we could achieve significant
performance improvements for non-zero mean values. On NER a model with
mean 0.7 (and variance 40) achieved an F-score of 90.56% on the development set
and 84.71% on the test set, a significant improvement over the best model with
mean 0. We observe a similar pattern for POS tagging. These results suggest
that considerable benefit may be gained from a well structured search of the
joint mean and variance hyperparameter space when using a Gaussian prior
for regularisation. There is of course a trade-off here, however, between finding
better hyperparameters values and suffering increased search complexity.
6.2 LOP-CRFs
Tables 5 and 6 show the performance of LOP-CRFs for the NER and POS
tagging experts respectively. The results demonstrate that: (1) In every case
the LOPs significantly outperform the unregularised monolithic. (2) In most
cases the performance of LOPs is comparable to that obtained using the different
priors on each task. In fact, values marked with ? show a significant improvement
over the performance obtained with the Gaussian prior with feature-independent
hyperparameter values. Only the value marked with ? in Table 6 significantly
under performs that model.
Table 5. LOP F-scores on NER
Expert set Development set Test set
Unreg. monolithic 88.33 81.87
Simple 90.26 84.22?
Positional 90.35 84.71?
Label 89.30 83.27
Random 88.84 83.06
Table 6. LOP accuracies on POS tagging
Expert set Development set Test set
Unreg. monolithic 97.92 97.65
Simple 98.31? 98.12?
Positional 98.03 97.81
Label 97.99 97.77
Random 97.99 97.76?
We can see that the performance of the LOP-CRFs varies with the choice of
expert set. For example, on NER the LOP-CRFs for the simple and positional
expert sets perform better than those for the label and random sets. Looking
back to equation 5, we conjecture that the simple and positional expert sets
achieve good performance in the LOP-CRF because they consist of experts that
are diverse while simultaneously being reasonable models of the data. The label
expert set exhibits greater diversity between the experts, because each expert
focuses on modelling a particular label only, but each expert is a relatively poor
model of the entire distribution. Similarly, the random experts are in general
better models of the entire distribution but tend to be less diverse because they
906 A. Smith and M. Osborne
do not focus on any one aspect or subset of it. Intuitively, then, we want to
devise experts that are simultaneously diverse and accurate.
The advantage of the LOP-CRF approach over the use of a prior is that it
is ?parameter-free? in the sense that each expert in the LOP-CRF is unregu-
larised. Consequently, we are not required to search a hyperparameter space.
For example, to carefully tune the hyperbolic hyperparameter in order to obtain
the optimal value we report here, we ran models for 20 different hyperparameter
values. In addition, in most cases the expert CRFs comprising the expert sets
are small, compact models that train more quickly than the monolithic with a
prior, and can be trained in parallel.
7 Conclusion
In this paper we compare parameterised and parameter-free approaches to
smoothing CRFs on two standard sequence labelling tasks in NLP. For the
parameterised methods, we compare different priors. We use both feature-
independent and feature-dependent hyperparameters in the prior distributions.
In the latter case we derive hyperparameter values using a Bayesian approach
to measuring our confidence in empirical expected feature counts. We find that:
(1) considerable search is required to identify good hyperparameter values for
all priors (2) for optimal hyperparameter values, the priors we tested perform
roughly equally well (3) in some cases performance can be improved using
feature-dependent hyperparameter values.
We contrast the use of priors to an alternative, parameter-free method using
logarithmic opinion pools. Our results show that a LOP of CRFs, which contains
unregularised models, can outperform the unregularised standard CRF and at-
tain a performance level that rivals that of the standard CRF regularised with a
prior. The important point, however, is that this performance may be achieved
with a considerably lower time for training by avoiding the need for intensive
hyperparameter search.
References
1. Chen, S. and Rosenfeld, R.: A Survey of Smoothing Techniques for ME Models.
IEEE Transactions on Speech and Audio Processing (2000) 8(1) 37?50
2. Gillick, L., Cox, S.: Some statistical issues in the comparison of speech recognition
algorithms. ICASSP (1989) 1 532?535
3. Heskes, T.: Selecting weighting factors in logarithmic opinion pools. NIPS (1998)
4. Kazama, J. and Tsujii, J.: Evaluation and Extension of Maximum Entropy Models
with Inequality Constraints. EMNLP (2003)
5. Lafferty, J. and McCallum, A. and Pereira, F.: Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Sequence Data. ICML (2001)
6. Malouf, R.: A comparison of algorithms for maximum entropy parameter estima-
tion. CoNLL (2002)
7. McCallum, A., Rohanimanesh, K. Sutton, C.: Dynamic Conditional Random Fields
for Jointly Labeling Multiple Sequences. NIPS Workshop on Syntax, Semantics,
Statistics (2003)
Regularisation Techniques for Conditional Random Fields 907
8. Peng, F. and McCallum, A.: Accurate Information Extraction from Research Pa-
pers using Conditional Random Fields. HLT-NAACL (2004)
9. Sha, F. and Pereira, F.: Shallow Parsing with Conditional Random Fields. HLT-
NAACL (2003)
10. Smith, A., Cohn, T., Osborne, M.: Logarithmic Opinion Pools for Conditional
Random Fields. ACL (2005)
11. Tjong Kim Sang, E. F. and Buchholz, S.: Introduction to the CoNLL-2000 shared
task: Chunking. CoNLL (2000)
12. Tjong Kim Sang, E. F. and De Meulder, F.: Introduction to the CoNLL-2003
Shared Task: Language-Independent Named Entity Recognition. CoNLL (2003)
13. Williams, P.: Bayesian Regularisation and Pruning using a Laplace Prior. Neural
Computation (1995) 7(1) 117?143
Example Selection for Bootstrapping Statistical Parsers
Mark Steedman?, Rebecca Hwa?, Stephen Clark?, Miles Osborne?, Anoop Sarkar?
Julia Hockenmaier?, Paul Ruhlen? Steven Baker?, Jeremiah Crim?
?School of Informatics, University of Edinburgh
{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk
?Institute for Advanced Computer Studies, University of Maryland
hwa@umiacs.umd.edu
?School of Computing Science, Simon Fraser University
anoop@cs.sfu.ca
?Center for Language and Speech Processing, Johns Hopkins University
jcrim@jhu.edu,ruhlen@cs.jhu.edu
?Department of Computer Science, Cornell University
sdb22@cornell.edu
Abstract
This paper investigates bootstrapping for statis-
tical parsers to reduce their reliance on manu-
ally annotated training data. We consider both
a mostly-unsupervised approach, co-training,
in which two parsers are iteratively re-trained
on each other?s output; and a semi-supervised
approach, corrected co-training, in which a
human corrects each parser?s output before
adding it to the training data. The selection of
labeled training examples is an integral part of
both frameworks. We propose several selection
methods based on the criteria of minimizing er-
rors in the data and maximizing training util-
ity. We show that incorporating the utility cri-
terion into the selection method results in better
parsers for both frameworks.
1 Introduction
Current state-of-the-art statistical parsers (Collins, 1999;
Charniak, 2000) are trained on large annotated corpora
such as the Penn Treebank (Marcus et al, 1993). How-
ever, the production of such corpora is expensive and
labor-intensive. Given this bottleneck, there is consider-
able interest in (partially) automating the annotation pro-
cess.
To overcome this bottleneck, two approaches from ma-
chine learning have been applied to training parsers. One
is sample selection (Thompson et al, 1999; Hwa, 2000;
Tang et al, 2002), a variant of active learning (Cohn et al,
1994), which tries to identify a small set of unlabeled sen-
tences with high training utility for the human to label1.
Sentences with high training utility are those most likely
to improve the parser. The other approach, and the fo-
cus of this paper, is co-training (Sarkar, 2001), a mostly-
unsupervised algorithm that replaces the human by hav-
ing two (or more) parsers label training examples for each
other. The goal is for both parsers to improve by boot-
strapping off each other?s strengths. Because the parsers
may label examples incorrectly, only a subset of their out-
put, chosen by some selection mechanism, is used in or-
der to minimize errors. The choice of selection method
significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training
examples for co-training parsers by incorporating the idea
of maximizing training utility from sample selection. The
selection mechanism is integral to both sample selection
and co-training; however, because co-training and sam-
ple selection have different goals, their selection methods
focus on different criteria: co-training typically favors se-
lecting accurately labeled examples, while sample selec-
tion typically favors selecting examples with high train-
ing utility, which often are not sentences that the parsers
already label accurately. In this work, we investigate se-
lection methods for co-training that explore the trade-off
between maximizing training utility and minimizing er-
rors.
Empirical studies were conducted to compare selection
methods under both co-training and a semi-supervised
framework called corrected co-training (Pierce and
Cardie, 2001), in which the selected examples are man-
ually checked and corrected before being added to the
1In the context of training parsers, a labeled example is a
sentence with its parse tree. Throughout this paper, we use the
term ?label? and ?parse? interchangeably.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 157-164
                                                         Proceedings of HLT-NAACL 2003
training data. For co-training, we show that the benefit of
selecting examples with high training utility can offset the
additional errors they contain. For corrected co-training,
we show that selecting examples with high training util-
ity reduces the number of sentences the human annotator
has to check. For both frameworks, we show that selec-
tion methods that maximize training utility find labeled
examples that result in better trained parsers than those
that only minimize error.
2 Co-training
Blum and Mitchell (1998) introduced co-training to
bootstrap two classifiers with different views of the data.
The two classifiers are initially trained on a small amount
of annotated seed data; then they label unannotated data
for each other in an iterative training process. Blum and
Mitchell prove that, when the two views are conditionally
independent given the label, and each view is sufficient
for learning the task, co-training can boost an initial
weak learner using unlabeled data.
The theory underlying co-training has been extended
by Dasgupta et al (2002) to prove that, by maximizing
their agreement over the unlabeled data, the two learn-
ers make few generalization errors (under the same in-
dependence assumption adopted by Blum and Mitchell).
Abney (2002) argues that this assumption is extremely
strong and typically violated in the data, and he proposes
a weaker independence assumption.
Goldman and Zhou (2000) show that, through care-
ful selection of newly labeled examples, co-training can
work even when the classifiers? views do not satisfy
the independence assumption. In this paper we investi-
gate methods for selecting labeled examples produced by
two statistical parsers. We do not explicitly maximize
agreement (along the lines of Abney?s algorithm (2002))
because it is too computationally intensive for training
parsers.
The pseudocode for our co-training framework is given
in Figure 1. It consists of two different parsers and a cen-
tral control that interfaces between the two parsers and
the data. At each co-training iteration, a small set of sen-
tences is drawn from a large pool of unlabeled sentences
and stored in a cache. Both parsers then attempt to label
every sentence in the cache. Next, a subset of the newly
labeled sentences is selected to be added to the train-
ing data. The examples added to the training set of one
parser (referred to as the student) are only those produced
by the other parser (referred to as the teacher), although
the methods we use generalize to the case in which the
parsers share a single training set. During selection, one
parser first acts as the teacher and the other as the student,
and then the roles are reversed.
A and B are two different parsers.
M iA and M iB are the models of A and B at step i.
U is a large pool of unlabeled sentences.
U i is a small cache holding a subset of U at step i.
L is the manually labeled seed data.
LiA and LiB are the labeled training examples for A and B
at step i.
Initialize:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
Loop:
U i ? Add unlabeled sentences from U .
M iA and M iB parse the sentences in U i and
assign scores to them according to their scoring
functions fA and fB .
Select new parses {PA} and {PB} according to some
selection method S, which uses the scores
from fA and fB .
Li+1A is L
i
A augmented with {PB}
Li+1B is L
i
B augmented with {PA}
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
Figure 1: The pseudo-code for the co-training algorithm
3 Selecting Training Examples
In each iteration, selection is performed in two steps.
First, each parser uses some scoring function, f , to assess
the parses it generated for the sentences in the cache.2
Second, the central control uses some selection method,
S, to choose a subset of these labeled sentences (based on
the scores assigned by f ) to add to the parsers? training
data. The focus of this paper is on the selection phase, but
to more fully investigate the effect of different selection
methods we also consider two possible scoring functions.
3.1 Scoring functions
The scoring function attempts to quantify the correctness
of the parses produced by each parser. An ideal scor-
ing function would give the true accuracy rates (e.g., F-
score, the combined labeled precision and recall rates).
In practice, accuracy is approximated by some notion
of confidence. For example, one easy-to-compute scor-
ing function measures the conditional probability of the
(most likely) parse. If a high probability is assigned, the
parser is said to be confident in the label it produced.
In our experimental studies, we considered the selec-
tion methods? interaction with two scoring functions: an
oracle scoring function fF-score that returns the F-score
of the parse as measured against a gold standard, and a
2In our experiments, both parsers use the same scoring func-
tion.
practical scoring function fprob that returns the condi-
tional probability of the parse.3
3.2 Selection methods
Based on the scores assigned by the scoring function,
the selection method chooses a subset of the parser la-
beled sentences that best satisfy some selection criteria.
One such criterion is the accuracy of the labeled exam-
ples, which may be estimated by the teacher parser?s con-
fidence in its labels. However, the examples that the
teacher correctly labeled may not be those that the stu-
dent needs. We hypothesize that the training utility of
the examples for the student parser is another important
criterion.
Training utility measures the improvement a parser
would make if that sentence were correctly labeled and
added to the training set. Like accuracy, the utility of
an unlabeled sentence is difficult to quantify; therefore,
we approximate it with values that can be computed from
features of the sentence. For example, sentences contain-
ing many unknown words may have high training util-
ity; so might sentences that a parser has trouble parsing.
Under the co-training framework, we estimate the train-
ing utility of a sentence for the student by comparing the
score the student assigned to its parse (according to its
scoring function) against the score the teacher assigned
to its own parse.
To investigate how the selection criteria of utility and
accuracy affect the co-training process, we considered a
number of selection methods that satisfy the requirements
of accuracy and training utility to varying degrees. The
different selection methods are shown below. For each
method, a sentence (as labeled by the teacher parser) is
selected if:
? above-n (Sabove-n): the score of the teacher?s parse
(using its scoring function) ? n.
? difference (Sdiff-n): the score of the teacher?s parse
is greater than the score of the student?s parse by
some threshold n.
? intersection (Sint-n): the score of the teacher?s parse
is in the set of the teacher?s n percent highest-
scoring labeled sentences, and the score of the stu-
dent?s parse for the same sentence is in the set of
the student?s n percent lowest-scoring labeled sen-
tences.
Each selection method has a control parameter, n, that
determines the number of labeled sentences to add at each
co-training iteration. It also serves as an indirect control
3A nice property of using conditional probability,
Pr(parse|sentence), as the scoring function is that it
normalizes for sentence length.
of the number of errors added to the training set. For ex-
ample, the Sabove-n method would allow more sentences
to be selected if n was set to a low value (with respect to
the scoring function); however, this is likely to reduce the
accuracy rate of the training set.
The above-n method attempts to maximize the accu-
racy of the data (assuming that parses with higher scores
are more accurate). The difference method attempts to
maximize training utility: as long as the teacher?s label-
ing is more accurate than that of the student, it is cho-
sen, even if its absolute accuracy rate is low. The inter-
section method attempts to maximize both: the selected
sentences are accurately labeled by the teacher and incor-
rectly labeled by the student.
4 Experiments
Experiments were performed to compare the effect of
the selection methods on co-training and corrected co-
training. We consider a selection method, S1, superior
to another, S2, if, when a large unlabeled pool of sen-
tences has been exhausted, the examples selected by S1
(as labeled by the machine, and possibly corrected by the
human) improve the parser more than those selected by
S2. All experiments shared the same general setup, as
described below.
4.1 Experimental Setup
For two parsers to co-train, they should generate com-
parable output but use independent statistical models.
In our experiments, we used a lexicalized context free
grammar parser developed by Collins (1999), and a lex-
icalized Tree Adjoining Grammar parser developed by
Sarkar (2002). Both parsers were initialized with some
seed data. Since the goal is to minimize human annotated
data, the size of the seed data should be small. In this pa-
per we used a seed set size of 1, 000 sentences, taken from
section 2 of the Wall Street Journal (WSJ) Penn Tree-
bank. The total pool of unlabeled sentences was the re-
mainder of sections 2-21 (stripped of their annotations),
consisting of about 38,000 sentences. The cache size is
set at 500 sentences. We have explored using different
settings for the seed set size (Steedman et al, 2003).
The parsers were evaluated on unseen test sentences
(section 23 of the WSJ corpus). Section 0 was used as
a development set for determining parameters. The eval-
uation metric is the Parseval F-score over labeled con-
stituents: F-score = 2?LR?LPLR+LP , where LP and LR
are labeled precision and recall rate, respectively. Both
parsers were evaluated, but for brevity, all results reported
here are for the Collins parser, which received higher Par-
seval scores.
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-10%int-60%No selection(Human annotated)
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection(Human annotated)
(a) (b)
Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the label
quality of the training data. (a) The average accuracy rates are about 85%. (b) The average accuracy rates (except for
those selected by Sdiff-10%) are about 95%.
4.2 Experiment 1: Selection Methods and
Co-Training
We first examine the effect of the three selection meth-
ods on co-training without correction (i.e., the chosen
machine-labeled training examples may contain errors).
Because the selection decisions are based on the scores
that the parsers assign to their outputs, the reliability of
the scoring function has a significant impact on the per-
formance of the selection methods. We evaluate the ef-
fectiveness of the selection methods using two scoring
functions. In Section 4.2.1, each parser assesses its out-
put with an oracle scoring function that returns the Par-
seval F-score of the output (as compared to the human
annotated gold-standard). This is an idealized condition
that gives us direct control over the error rate of the la-
beled training data. By keeping the error rates constant,
our goal is to determine which selection method is more
successful in finding sentences with high training utility.
In Section 4.2.2 we replace the oracle scoring function
with fprob, which returns the conditional probability of
the best parse as the score. We compare how the selection
methods? performances degrade under the realistic con-
dition of basing selection decisions on unreliable parser
output assessment scores.
4.2.1 Using the oracle scoring function, fF-score
The goal of this experiment is to evaluate the selection
methods using a reliable scoring function. We therefore
use an oracle scoring function, fF-score, which guaran-
tees a perfect assessment of the parser?s output. This,
however, may be too powerful. In practice, we expect
even a reliable scoring function to sometimes assign high
scores to inaccurate parses. We account for this effect by
adjusting the selection method?s control parameter to af-
fect two factors: the accuracy rate of the newly labeled
training data, and the number of labeled sentences added
at each training iteration. A relaxed parameter setting
adds more parses to the training data, but also reduces
the accuracy of the training data.
Figure 2 compares the effect of the three selection
methods on co-training for the relaxed (left graph) and
the strict (right graph) parameter settings. Each curve in
the two graphs charts the improvement in the parser?s ac-
curacy in parsing the test sentences (y-axis) as it is trained
on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection
methods chose a different number of sentences from the
same 38K unlabeled pool. For reference, we also plotted
the improvement of a fully-supervised parser (i.e., trained
on human-annotated data, with no selection).
For the more relaxed setting, the parameters are chosen
so that the newly labeled training data have an average
accuracy rate of about 85%:
? Sabove-70% requires the labels to have an F-score ?
70%. It adds about 330 labeled sentences (out of the
500 sentence cache) with an average accuracy rate
of 85% to the training data per iteration.
? Sdiff-10% requires the score difference between the
teacher?s labeling and the student?s labeling to be at
least 10%. It adds about 50 labeled sentences with
an average accuracy rate of 80%.
? Sint-60% requires the teacher?s parse to be in the
top 60% of its output and the student?s parse for the
same sentence to be in its bottom 60%. It adds about
150 labeled sentences with an average accuracy rate
of 85%.
Although none rivals the parser trained on human an-
notated data, the selection method that improves the
parser the most is Sdiff-10%. One interpretation is that
the training utility of the examples chosen by Sdiff-10%
outweighs the cost of errors introduced into the training
data. Another interpretation is that the other two selection
methods let in too many sentences containing errors. In
the right graph, we compare the same Sdiff-10% with the
other two selection methods using stricter control, such
that the average accuracy rate for these methods is now
about 95%:
? Sabove-90% now requires the parses to be at least
90% correct. It adds about 150 labeled sentences
per iteration.
? Sint-30% now requires the teacher?s parse to be in
the top 30% of its output and the student?s parse for
the same sentence in its bottom 30%. It adds about
15 labeled sentences.
The stricter control on Sabove-90% improved the
parser?s performance, but not enough to overtake
Sdiff-10% after all the sentences in the unlabeled pool
had been considered, even though the training data of
Sdiff-10% contained many more errors. Sint-30% has a
faster initial improvement4, closely tracking the progress
of the fully-supervised parser. However, the stringent re-
quirement exhausted the unlabeled data pool before train-
ing the parser to convergence. Sint-30% might continue
to help the parser to improve if it had access to more un-
labeled data, which is easier to acquire than annotated
data5.
Comparing the three selection methods under both
strict and relaxed control settings, the results suggest that
training utility is an important criterion in selecting train-
ing examples, even at the cost of reduced accuracy.
4.2.2 Using the fprob scoring function
To determine the effect of unreliable scores on the se-
lection methods, we replace the oracle scoring function,
fF-score, with fprob, which approximates the accuracy
of a parse with its conditional probability. Although this
is a poor estimate of accuracy (especially when computed
from a partially trained parser), it is very easy to compute.
The unreliable scores also reduce the correlation between
the selection control parameters and the level of errors in
the training data. In this experiment, we set the parame-
ters for all three selection methods so that approximately
4A fast improvement rate is not a central concern here, but
it will be more relevant for corrected co-training.
5This oracle experiment is bounded by the size of the anno-
tated portion of the WSJ corpus.
79.8
80
80.2
80.4
80.6
80.8
81
81.2
1000 1500 2000 2500 3000 3500 4000 4500 5000
Par
sin
g A
ccu
rac
y o
n T
est
 Da
ta (F
scor
e)
Number of Training Sentences
above-70%diff-30%int-30%
Figure 3: A comparison of selection methods using the
conditional probability scoring function, fprob.
30-50 sentences were added to the training data per iter-
ation. The average accuracy rate of the training data for
Sabove-70% was about 85%, and the rate for Sdiff-30%
and Sint-30% was about 75%.
As expected, the parser performances of all three selec-
tion methods using fprob (shown in Figure 3) are lower
than using fF-score (see Figure 2). However, Sdiff-30%
and Sint-30% helped the co-training parsers to improve
with a 5% error reduction (1% absolute difference) over
the parser trained only on the initial seed data. In con-
trast, despite an initial improvement, using Sabove-70%
did not help to improve the parser. In their experiments on
NP identifiers, Pierce and Cardie (2001) observed a sim-
ilar effect. They hypothesize that co-training does not
scale well for natural language learning tasks that require
a huge amount of training data because too many errors
are accrued over time. Our experimental results suggest
that the use of training utility in the selection process can
make co-training parsers more tolerant to these accumu-
lated errors.
4.3 Experiment 2: Selection Methods and
Corrected Co-training
To address the problem of the training data accumulating
too many errors over time, Pierce and Cardie proposed
a semi-supervised variant of co-training called corrected
co-training, which allows a human annotator to review
and correct the output of the parsers before adding it to
the training data. The main selection criterion in their
co-training system is accuracy (approximated by confi-
dence). They argue that selecting examples with nearly
correct labels would require few manual interventions
from the annotator.
We hypothesize that it may be beneficial to consider
the training utility criterion in this framework as well.
We perform experiments to determine whether select-
ing fewer (and possibly less accurately labeled) exam-
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-90%diff-10%int-30%No selection
(a) (b)
Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
ples with higher training utility would require less effort
from the annotator. In our experiments, we simulated
the interactive sample selection process by revealing the
gold standard. As before, we compare the three selection
methods using both fF-score and fprob as scoring func-
tions.6
4.3.1 Using the oracle scoring function, fF-score
Figure 4 shows the effect of the three selection meth-
ods (using the strict parameter setting) on corrected co-
training. As a point of reference, we plot the improve-
ment rate for a fully supervised parser (same as the one
in Figure 2). In addition to charting the parser?s perfor-
mance in terms of the number of labeled training sen-
tences (left graph), we also chart the parser?s performance
in terms of the the number of constituents the machine
mislabeled (right graph). The pair of graphs indicates the
amount of human effort required: the left graph shows
the number of sentences the human has to check, and the
right graph shows the number of constituents the human
has to correct.
Comparing Sabove-90% and Sdiff-10%, we see that
Sdiff-10% trains a better parser than Sabove-90% when all
the unlabeled sentences have been considered. It also im-
proves the parser using a smaller set of training exam-
ples. Thus, for the same parsing performance, it requires
the human to check fewer sentences than Sabove-90% and
the reference case of no selection (Figure 4(a)). On the
other hand, because the labeled sentences selected by
Sdiff-10% contain more mistakes than those selected by
Sabove-90%, Sdiff-10% requires slightly more corrections
6The selection control parameters are the same as the previ-
ous set of experiments, using the strict setting (i.e., Figure 2(b))
for fF-score.
than Sabove-90% for the same level of parsing perfor-
mance; though both require fewer corrections than the
reference case of no selection (Figure 4(b)). Because
the amount of effort spent by the annotator depends on
the number of sentences checked as well as the amount
of corrections made, whether Sdiff-10% or Sabove-90% is
more effort reducing may be a matter of the annotator?s
preference.
The selection method that improves the parser at the
fastest rate is Sint-30%. For the same parser performance
level, it selects the fewest number of sentences for a hu-
man to check and requires the human to make the least
number of corrections. However, as we have seen in the
earlier experiment, very few sentences in the unlabeled
pool satisfy its stringent criteria, so it ran out of data be-
fore the parser was trained to convergence. At this point
we cannot determine whether Sint-30% might continue to
improve the parser if we used a larger set of unlabeled
data.
4.3.2 Using the fprob scoring function
We also consider the effect of unreliable scores in the
corrected co-training framework. A comparison between
the selection methods using fprob is reported in Figure
5. The left graph charts parser performance in terms of
the number of sentences the human must check; the right
charts parser performance in terms of the number of con-
stituents the human must correct. As expected, the unreli-
able scoring function degrades the effectiveness of the se-
lection methods; however, compared to its unsupervised
counterpart (Figure 3), the degradation is not as severe.
In fact, Sdiff-30% and Sint-30% still require fewer train-
ing data than the reference parser. Moreover, consistent
with the other experiments, the selection methods that at-
tempt to maximize training utility achieve better parsing
performance than Sabove-70%. Finally, in terms of reduc-
ing human effort, the three selection methods require the
human to correct comparable amount of parser errors for
the same level of parsing performance, but for Sdiff-30%
and Sint-30%, fewer sentences need to be checked.
4.3.3 Discussion
Corrected co-training can be seen as a form of active
learning, whose goal is to identify the smallest set of un-
labeled data with high training utility for the human to
label. Active learning can be applied to a single learner
(Lewis and Catlett, 1994) and to multiple learners (Fre-
und et al, 1997; Engelson and Dagan, 1996; Ngai and
Yarowsky, 2000). In the context of parsing, all previ-
ous work (Thompson et al, 1999; Hwa, 2000; Tang et
al., 2002) has focussed on single learners. Corrected co-
training is the first application of active learning for mul-
tiple parsers. We are currently investigating comparisons
to the single learner approaches.
Our approach is similar to co-testing (Muslea et al,
2002), an active learning technique that uses two classi-
fiers to find contentious examples (i.e., data for which the
classifiers? labels disagree) for a human to label. There is
a subtle but significant difference, however, in that their
goal is to reduce the total number of labeled training ex-
amples whereas we also wish to reduce the number of
corrections made by the human. Therefore, our selection
methods must take into account the quality of the parse
produced by the teacher in addition to how different its
parse is from the one produced by the student. The inter-
section method precisely aims at selecting sentences that
satisfy both requirements. Exploring different selection
methods is part of our on-going research effort.
5 Conclusion
We have considered three selection methods that have dif-
ferent priorities in balancing the two (often competing)
criteria of accuracy and training utility. We have em-
pirically compared their effect on co-training, in which
two parsers label data for each other, as well as corrected
co-training, in which a human corrects the parser labeled
data before adding it to the training set. Our results sug-
gest that training utility is an important selection criterion
to consider, even at the cost of potentially reducing the ac-
curacy of the training data. In our empirical studies, the
selection method that aims to maximize training utility,
Sdiff-n, consistently finds better examples than the one
that aims to maximize accuracy, Sabove-n. Our results
also suggest that the selection method that aims to maxi-
mize both accuracy and utility, Sint-n, shows promise in
improving co-training parsers and in reducing human ef-
fort for corrected co-training; however, a much larger un-
labeled data set is needed to verify the benefit of Sint-n.
The results of this study indicate the need for scor-
ing functions that are better estimates of the accuracy of
the parser?s output than conditional probabilities. Our
oracle experiments show that, by using effective selec-
tion methods, the co-training process can improve parser
peformance even when the newly labeled parses are
not completely accurate. This suggests that co-training
may still be beneficial when using a practical scoring
function that might only coarsely distinguish accurate
parses from inaccurate parses. Further avenues to ex-
plore include the development of selection methods to
efficiently approximate maximizing the objective func-
tion of parser agreement on unlabeled data, following the
work of Dasgupta et al (2002) and Abney (2002). Also,
co-training might be made more effective if partial parses
were used as training data. Finally, we are conducting ex-
periments to compare corrected co-training with other ac-
tive learning methods. We hope these studies will reveal
ways to combine the strengths of co-training and active
learning to make better use of unlabeled data.
Acknowledgments
This work has been supported, in part, by NSF/DARPA
funded 2002 Human Language Engineering Workshop
at JHU, EPSRC grant GR/M96889, the Department of
Defense contract RD-02-5700, and ONR MURI Con-
tract FCPO.810548265. We would like to thank Chris
Callison-Burch, Michael Collins, John Henderson, Lil-
lian Lee, Andrew McCallum, and Fernando Pereira for
helpful discussions; to Ric Crabbe, Adam Lopez, the par-
ticipants of CS775 at Cornell University, and the review-
ers for their comments on this paper.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of the
11th Annual Conference on Computational Learning Theory,
pages 92?100, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of the NAACL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Improv-
ing generalization with active learning. Machine Learning,
15(2):201?221.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-30%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-70%diff-30%int-30%No selection
(a) (b)
Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
in Neural Information Processing Systems 14, Cambridge,
MA. MIT Press.
Sean P. Engelson and Ido Dagan. 1996. Minimizing manual
annotation cost in supervised training from copora. In Pro-
ceedings of the 34th Annual Meeting of the ACL, pages 319?
326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28(2-3):133?168.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th In-
ternational Conference on Machine Learning, Stanford, CA.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proceedings of the 2000 Joint SIGDAT Confer-
ence on EMNLP and VLC, pages 45?52, Hong Kong, China,
October.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Proceedings
of the Eleventh International Conference on Machine Learn-
ing, pages 148?156.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selec-
tive sampling with redundant views. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence,
pages 621?626.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual Meeting of the
ACL, pages 117?125, Hong Kong, China, October.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of the Empirical Methods in NLP Conference,
Pittsburgh, PA.
Anoop Sarkar. 2001. Applying co-training methods to statisti-
cal parsing. In Proceedings of the 2nd Annual Meeting of the
NAACL, pages 95?102, Pittsburgh, PA.
Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexi-
calized Tree Adjoining Grammars. Ph.D. thesis, University
of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven
Baker, and Jeremiah Crim. 2003. Bootstrapping statistical
parsers from small datasets. In The Proceedings of the An-
nual Meeting of the European Chapter of the ACL. To ap-
pear.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active
learning for statistical natural language parsing. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages 120?127,
July.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proceedings of ICML-99,
pages 406?414, Bled, Slovenia.
Ensemble-based Active Learning for Parse Selection
Miles Osborne and Jason Baldridge
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
 
miles,jbaldrid  @inf.ed.ac.uk
Abstract
Supervised estimation methods are widely seen
as being superior to semi and fully unsuper-
vised methods. However, supervised methods
crucially rely upon training sets that need to
be manually annotated. This can be very ex-
pensive, especially when skilled annotators are
required. Active learning (AL) promises to
help reduce this annotation cost. Within the
complex domain of HPSG parse selection, we
show that ideas from ensemble learning can
help further reduce the cost of annotation. Our
main results show that at times, an ensemble
model trained with randomly sampled exam-
ples can outperform a single model trained us-
ing AL. However, converting the single-model
AL method into an ensemble-based AL method
shows that even this much stronger baseline
model can be improved upon. Our best results
show a  reduction in annotation cost com-
pared with single-model random sampling.
1 Introduction
Active learning (AL) methods, such as uncertainty sam-
pling (Cohn et al, 1995) or query by committee (Seung
et al, 1992), can dramatically reduce the cost of creat-
ing an annotated dataset. In particular, they enable rapid
creation of labeled datasets which can then be used for
trainable speech and language technologies. Progress in
AL will therefore translate into even greater savings in
annotation costs and hence faster creation of speech and
language systems.
In this paper, we:
 Present a novel way of improving uncertainty sam-
pling by generalizing it from using a single model to
using an ensemble model. This generalization easily
outperforms single-model uncertainty sampling.
 Introduce a new, extremely simple AL method
(called lowest best probability selection) which is
competitive with uncertainty sampling and can also
be improved using ensemble techniques.
 Show that an ensemble of models trained using ran-
domly sampled examples can outperform a single
model trained using (single model) AL methods.
 Demonstrate further reductions in annotation cost
when we train the ensemble parse selection model
using examples selected by an ensemble-based ac-
tive learner. This result shows that ensemble learn-
ing can improve both the underlying model and also
the way we select examples for it.
Our domain is parse selection for Head-Driven Phrase
Structure Grammar (HPSG). Although annotated corpora
exist for HPSG, such corpora do not exist in significant
volumes and are limited to a few small domains (Oepen
et al, 2002). Even if it were possible to bootstrap from
the Penn Treebank, it is still unlikely that there would be
sufficient quantities of high quality material necessary to
improve parse selection for detailed linguistic formalisms
such as HPSG. There is thus a pressing need to efficiently
create significant volumes of annotated material.
AL applied to parse selection is much more challeng-
ing than applying it to simpler tasks such as text classifi-
cation or part-of-speech tagging. Our labels are complex
objects rather than discrete values drawn from a small,
fixed set. Furthermore, the fact that sentences are of vari-
able length and have variable numbers of parses poten-
tially adds to the complexity of the task.
Our results specific to parse selection show that:
 An ensemble of three parse selection models is able
to achieve a 10.8% reduction in error rate over the
best single model.
 Annotation cost should not assume a unit expendi-
ture per example. Using a more refined cost met-
ric based upon efficiently selecting the correct parse
from a set of possible parses, we are able to show
that some AL methods are more effective than oth-
ers, even though they perform similarly when mak-
ing the unit cost per example assumption.
 Ad-hoc selection methods based upon superficial
characteristics of the data, such as sentence length
or ambiguity rate, are typically worse than random
sampling. This motivates using AL methods.
 Labeling sentences in the order they appear in the
corpus ? as is typically done in annotation ? per-
forms much worse than using random selection.
Throughout this paper, we shall treat the terms sen-
tences and examples as interchangeable; we shall also
consider parses and labels as equivalent. Also, we shall
use the term method whenever we are talking about AL,
and model whenever we are talking about parse selection.
2 Parse selection
2.1 The Redwoods treebank
Many broad coverage grammars providing detailed syn-
tactic and semantic analyses of sentences exist for a va-
riety of computational grammar frameworks, but their
purely symbolic nature means that when ordering li-
censed analyses, parse selection models are necessary. To
overcome this limitation for the HPSG English Resource
Grammar (ERG, Flickinger (2000)), the Redwoods tree-
bank has been created to provide annotated training ma-
terial (Oepen et al, 2002).
For each utterance in Redwoods, analyses licensed by
the ERG are enumerated and the correct one, if present,
is indicated. Each analysis is represented as a tree that
records the grammar rules which were used to derive it.
For example, Figure 1a shows the preferred derivation
tree, out of three analyses, for what can I do for you?.
Using these trees and the ERG, several different views
of analyses can be recovered: phrase structures, semantic
interpretations, and elementary dependency graphs. The
phrase structures contain detailed HPSG non-terminals
but are otherwise of the variety familiar from context-free
grammar, as can be seen in Figure 1b.
Unlike most treebanks, Redwoods also provides se-
mantic information for utterances. The semantic interpre-
tations are expressed using Minimal Recursion Seman-
tics (MRS) (Copestake et al, 2001), which provides the
means to represent interpretations with a flat, underspec-
ified semantics using terms of the predicate calculus and
generalized quantifiers. An example MRS structure is
given in Figure 2.
An elementary dependency graph is a simplified ab-
straction on a full MRS structure which uses no under-
specification and retains only the major semantic predi-
cates and their relations to one another.
In this paper, we report results using the third growth
of Redwoods, which contains 5302 sentences for which
there are at least two parses and for which a unique pre-
ferred parse is identified. These sentences have 9.3 words
and 58.0 parses on average. Due to the small size of Red-
woods and the underlying complexity of the system, ex-
ploring the effect of AL techniques for this domain is of
practical, as well as theoretical, interest.
2.2 Modeling parse selection
As is now standard for feature-based grammars, we use
log-linear models for parse selection (Johnson et al,
1999). Log-linear models are popular for their ability to
incorporate a wide variety of features without making as-
sumptions about their independence.1
For log-linear models, the conditional probability of
an analysis   given a sentence with a set of analyses 

 
	 is given as:

  
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17?24,
New York, June 2006. c?2006 Association for Computational Linguistics
Improved Statistical Machine Translation Using Paraphrases
Chris Callison-Burch Philipp Koehn Miles Osborne
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
Parallel corpora are crucial for training
SMT systems. However, for many lan-
guage pairs they are available only in
very limited quantities. For these lan-
guage pairs a huge portion of phrases en-
countered at run-time will be unknown.
We show how techniques from paraphras-
ing can be used to deal with these oth-
erwise unknown source language phrases.
Our results show that augmenting a state-
of-the-art SMT system with paraphrases
leads to significantly improved coverage
and translation quality. For a training
corpus with 10,000 sentence pairs we in-
crease the coverage of unique test set un-
igrams from 48% to 90%, with more than
half of the newly covered items accurately
translated, as opposed to none in current
approaches.
1 Introduction
As with many other statistical natural language pro-
cessing tasks, statistical machine translation (Brown
et al, 1993) produces high quality results when am-
ple training data is available. This is problematic for
so called ?low density? language pairs which do not
have very large parallel corpora. For example, when
words occur infrequently in a parallel corpus param-
eter estimates for word-level alignments can be in-
accurate, which can in turn lead to inaccurate phrase
translations. Limited amounts of training data can
further lead to a problem of low coverage in that
many phrases encountered at run-time are not ob-
served in the training data and therefore their trans-
lations will not be learned.
Here we address the problem of unknown phrases.
Specifically we show that upon encountering an un-
known source phrase, we can substitute a paraphrase
for it and then proceed using the translation of that
paraphrase. We derive these paraphrases from re-
sources that are external to the parallel corpus that
the translation model is trained from, and we are
able to exploit (potentially more abundant) parallel
corpora from other language pairs to do so.
In this paper we:
? Define a method for incorporating paraphrases
of unseen source phrases into the statistical ma-
chine translation process.
? Show that by translating paraphrases we
achieve a marked improvement in coverage and
translation quality, especially in the case of un-
known words which to date have been left un-
translated.
? Argue that while we observe an improvement
in Bleu score, this metric is particularly poorly
suited to measuring the sort of improvements
that we achieve.
? Present an alternative methodology for targeted
manual evaluation that may be useful in other
research projects.
2 The Problem of Coverage in SMT
Statistical machine translation made considerable
advances in translation quality with the introduc-
tion of phrase-based translation (Marcu and Wong,
2002; Koehn et al, 2003; Och and Ney, 2004). By
17
 0 10
 20 30
 40 50
 60 70
 80 90
 100
 10000  100000  1e+06  1e+07
T
e
s
t
 
S
e
t
 
I
t
e
m
s
 
w
i
t
h
 
T
r
a
n
s
l
a
t
i
o
n
s
 
(
%
)
Training Corpus Size (num words)
unigramsbigramstrigrams4-grams
Figure 1: Percent of unique unigrams, bigrams, tri-
grams, and 4-grams from the Europarl Spanish test
sentences for which translations were learned in in-
creasingly large training corpora
increasing the size of the basic unit of translation,
phrase-based machine translation does away with
many of the problems associated with the original
word-based formulation of statistical machine trans-
lation (Brown et al, 1993). For instance, with multi-
word units less re-ordering needs to occur since lo-
cal dependencies are frequently captured. For exam-
ple, common adjective-noun alternations are mem-
orized. However, since this linguistic information
is not explicitly and generatively encoded in the
model, unseen adjective noun pairs may still be han-
dled incorrectly.
Thus, having observed phrases in the past dramat-
ically increases the chances that they will be trans-
lated correctly in the future. However, for any given
test set, a huge amount of training data has to be ob-
served before translations are learned for a reason-
able percentage of the test phrases. Figure 1 shows
the extent of this problem. For a training corpus
containing 10,000 words translations will have been
learned for only 10% of the unigrams (types, not
tokens). For a training corpus containing 100,000
words this increases to 30%. It is not until nearly
10,000,000 words worth of training data have been
analyzed that translation for more than 90% of the
vocabulary items have been learned. This problem
is obviously compounded for higher-order n-grams
(longer phrases), and for morphologically richer lan-
guages.
encargarnos to ensure, take care, ensure that
garantizar guarantee, ensure, guaranteed, as-
sure, provided
velar ensure, ensuring, safeguard, making
sure
procurar ensure that, try to, ensure, endeavour
to
asegurarnos ensure, secure, make certain
usado used
utilizado used, use, spent, utilized
empleado used, spent, employee
uso use, used, usage
utiliza used, uses, used, being used
utilizar to use, use, used
Table 1: Example of automatically generated para-
phrases for the Spanish words encargarnos and us-
ado along with their English translations which were
automatically learned from the Europarl corpus
2.1 Handling unknown words
Currently most statistical machine translation sys-
tems are simply unable to handle unknown words.
There are two strategies that are generally employed
when an unknown source word is encountered. Ei-
ther the source word is simply omitted when pro-
ducing the translation, or alternatively it is passed
through untranslated, which is a reasonable strategy
if the unknown word happens to be a name (assum-
ing that no transliteration need be done). Neither of
these strategies is satisfying.
2.2 Using paraphrases in SMT
When a system is trained using 10,000 sentence
pairs (roughly 200,000 words) there will be a num-
ber of words and phrases in a test sentence which it
has not learned the translation of. For example, the
Spanish sentence
Es positivo llegar a un acuerdo sobre los
procedimientos, pero debemos encargar-
nos de que este sistema no sea susceptible
de ser usado como arma pol??tica.
may translate as
It is good reach an agreement on proce-
dures, but we must encargarnos that this
system is not susceptible to be usado as
political weapon.
18
what is more, the relevant cost dynamic is completely under control
im ?brigen ist die diesbez?gliche kostenentwicklung v?llig  unter kontrolle
we owe it to the taxpayers to keep in checkthe costs
wir sind es den steuerzahlern die kosten zu habenschuldig  unter kontrolle
Figure 2: Using a bilingual parallel corpus to extract paraphrases
The strategy that we employ for dealing with un-
known source language words is to substitute para-
phrases of those words, and then translate the para-
phrases. Table 1 gives examples of paraphrases and
their translations. If we had learned a translation of
garantizar we could translate it instead of encargar-
nos, and similarly for utilizado instead of usado.
3 Acquiring Paraphrases
Paraphrases are alternative ways of expressing the
same information within one language. The auto-
matic generation of paraphrases has been the focus
of a significant amount of research lately. Many
methods for extracting paraphrases (Barzilay and
McKeown, 2001; Pang et al, 2003) make use of
monolingual parallel corpora, such as multiple trans-
lations of classic French novels into English, or the
multiple reference translations used by many auto-
matic evaluation metrics for machine translation.
Bannard and Callison-Burch (2005) use bilin-
gual parallel corpora to generate paraphrases. Para-
phrases are identified by pivoting through phrases in
another language. The foreign language translations
of an English phrase are identified, all occurrences
of those foreign phrases are found, and all English
phrases that they translate back to are treated as po-
tential paraphrases of the original English phrase.
Figure 2 illustrates how a German phrase can be
used as a point of identification for English para-
phrases in this way.
The method defined in Bannard and Callison-
Burch (2005) has several features that make it an
ideal candidate for incorporation into statistical ma-
chine translation system. Firstly, it can easily be ap-
plied to any language for which we have one or more
parallel corpora. Secondly, it defines a paraphrase
probability, p(e2|e1), which can be incorporated into
the probabilistic framework of SMT.
3.1 Paraphrase probabilities
The paraphrase probability p(e2|e1) is defined
in terms of two translation model probabilities:
p(f |e1), the probability that the original English
phrase e1 translates as a particular phrase f in the
other language, and p(e2|f), the probability that the
candidate paraphrase e2 translates as the foreign lan-
guage phrase. Since e1 can translate as multiple for-
eign language phrases, we marginalize f out:
p(e2|e1) =
?
f
p(f |e1)p(e2|f) (1)
The translation model probabilities can be com-
puted using any standard formulation from phrase-
based machine translation. For example, p(e2|f)
can be calculated straightforwardly using maximum
likelihood estimation by counting how often the
phrases e and f were aligned in the parallel corpus:
p(e2|f) ?
count(e2, f)
?
e2 count(e2, f)
(2)
There is nothing that limits us to estimating para-
phrases probabilities from a single parallel corpus.
We can extend the definition of the paraphrase prob-
ability to include multiple corpora, as follows:
p(e2|e1) ?
?
c?C
?
f in c p(f |e1)p(e2|f)
|C|
(3)
where c is a parallel corpus from a set of paral-
lel corpora C. Thus multiple corpora may be used
19
by summing over all paraphrase probabilities calcu-
lated from a single corpus (as in Equation 1) and
normalized by the number of parallel corpora.
4 Experimental Design
We examined the application of paraphrases to deal
with unknown phrases when translating from Span-
ish and French into English. We used the pub-
licly available Europarl multilingual parallel corpus
(Koehn, 2005) to create six training corpora for the
two language pairs, and used the standard Europarl
development and test sets.
4.1 Baseline
For a baseline system we produced a phrase-based
statistical machine translation system based on the
log-linear formulation described in (Och and Ney,
2002)
e? = argmax
e
p(e|f) (4)
= argmax
e
M?
m=1
?mhm(e, f) (5)
The baseline model had a total of eight feature
functions, hm(e, f): a language model probabil-
ity, a phrase translation probability, a reverse phrase
translation probability, lexical translation probabil-
ity, a reverse lexical translation probability, a word
penalty, a phrase penalty, and a distortion cost. To
set the weights, ?m, we performed minimum error
rate training (Och, 2003) on the development set us-
ing Bleu (Papineni et al, 2002) as the objective func-
tion.
The phrase translation probabilities were deter-
mined using maximum likelihood estimation over
phrases induced from word-level alignments pro-
duced by performing Giza++ training on each of the
three training corpora. We used the Pharaoh beam-
search decoder (Koehn, 2004) to produce the trans-
lations after all of the model parameters had been
set.
When the baseline system encountered unknown
words in the test set, its behavior was simply to re-
produce the foreign word in the translated output.
This is the default behavior for many systems, as
noted in Section 2.1.
4.2 Translation with paraphrases
We extracted all source language (Spanish and
French) phrases up to length 10 from the test and
development sets which did not have translations in
phrase tables that were generated for the three train-
ing corpora. For each of these phrases we gener-
ated a list of paraphrases using all of the parallel cor-
pora from Europarl aside from the Spanish-English
and French-English corpora. We used bitexts be-
tween Spanish and Danish, Dutch, Finnish, French,
German, Italian, Portuguese, and Swedish to gener-
ate our Spanish paraphrases, and did similarly for
the French paraphrases. We manage the parallel
corpora with a suffix array -based data structure
(Callison-Burch et al, 2005). We calculated para-
phrase probabilities using the Bannard and Callison-
Burch (2005) method, summarized in Equation 3.
Source language phrases that included names and
numbers were not paraphrased.
For each paraphrase that had translations in the
phrase table, we added additional entries in the
phrase table containing the original phrase and the
paraphrase?s translations. We augmented the base-
line model by incorporating the paraphrase probabil-
ity into an additional feature function which assigns
values as follows:
h(e, f1) =
?
??
??
p(f2|f1) If phrase table entry (e, f1)
is generated from (e, f2)
1 Otherwise
Just as we did in the baseline system, we performed
minimum error rate training to set the weights of the
nine feature functions in our translation model that
exploits paraphrases.
We tested the usefulness of the paraphrase fea-
ture function by performing an additional experi-
ment where the phrase table was expanded but the
paraphrase probability was omitted.
4.3 Evaluation
We evaluated the efficacy of using paraphrases in
three ways: by calculating the Bleu score for the
translated output, by measuring the increase in cov-
erage when including paraphrases, and through a tar-
geted manual evaluation of the phrasal translations
of unseen phrases to determine how many of the
newly covered phrases were accurately translated.
20
ca
u
s
a
s
Alignment Tool
for
citizens
of
treatment
the
in
inequality
and
discrimination
combats
article
The
reasons
the
therein.
listed
l
a
s
p
o
r
c
i
u
d
a
d
a
n
o
s
l
o
s
d
e
d
e
s
i
g
u
a
l
t
r
a
t
o
e
l
yc
o
m
b
a
t
e
a
r
t
?
c
u
l
o
E
l
e
n
e
n
u
m
e
r
a
d
a
s
m
i
s
m
o
.
e
l
d
i
s
c
r
i
m
i
n
a
c
i
?
n
l
a
Figure 3: Test sentences and reference translations
were manually word-aligned. This allowed us to
equate unseen phrases with their corresponding En-
glish phrase. In this case enumeradas with listed.
Although Bleu is currently the standard metric for
MT evaluation, we believe that it may not meaning-
fully measure translation improvements in our setup.
By substituting a paraphrase for an unknown source
phrase there is a strong chance that its translation
may also be a paraphrase of the equivalent target
language phrase. Bleu relies on exact matches of
n-grams in a reference translation. Thus if our trans-
lation is a paraphrase of the reference, Bleu will fail
to score it correctly.
Because Bleu is potentially insensitive to the type
of changes that we were making to the translations,
we additionally performed a focused manual evalu-
ation (Callison-Burch et al, 2006). To do this, had
bilingual speakers create word-level alignments for
the first 150 and 250 sentence in the Spanish-English
and French-English test corpora, as shown in Figure
3. We were able to use these alignments to extract
the translations of the Spanish and French words that
we were applying our paraphrase method to.
Knowing this correspondence between foreign
phrases and their English counterparts allowed us to
directly analyze whether translations that were be-
ing produced from paraphrases remained faithful to
the meaning of the reference translation. When pro-
The article combats discrimination and inequality
in the treatment of citizens for the reasons listed
therein.
The article combats discrimination and the dif-
ferent treatment of citizens for the reasons men-
tioned in the same.
The article fights against uneven and the treatment
of citizens for the reasons enshrined in the same.
The article is countering discrimination and the
unequal treatment of citizens for the reasons that
in the same.
Figure 4: Judges were asked whether the highlighted
phrase retained the same meaning as the highlighted
phrase in the reference translation (top)
ducing our translations using the Pharaoh decoder
we employed its ?trace? facility, which tells which
source sentence span each target phrase was derived
from. This allowed us to identify which elements
in the machine translated output corresponded to the
paraphrased foreign phrase. We asked a monolin-
gual judge whether the phrases in the machine trans-
lated output had the same meaning as of the refer-
ence phrase. This is illustrated in Figure 4.
In addition to judging the accuracy of 100 phrases
for each of the translated sets, we measured how
much our paraphrase method increased the cover-
age of the translation system. Because we focus
on words that the system was previously unable to
translate, the increase in coverage and the transla-
tion quality of the newly covered phrases are the
two most relevant indicators as to the efficacy of the
method.
5 Results
We produced translations under five conditions for
each of our training corpora: a set of baseline
translations without any additional entries in the
phrase table, a condition where we added the trans-
lations of paraphrases for unseen source words along
with paraphrase probabilities, a condition where we
added the translations of paraphrases of multi-word
phrases along with paraphrase probabilities, and two
additional conditions where we added the transla-
tions of paraphrases of single and multi-word para-
phrase without paraphrase probabilities.
21
Spanish-English French-English
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Baseline 22.6 25.0 26.5 26.5 28.7 30.0 21.9 24.3 26.3 27.8 28.8 29.5
Single word 23.1 25.2 26.6 28.0 29.0 30.0 22.7 24.2 26.9 27.7 28.9 29.8
Multi-word 23.3 26.0 27.2 28.0 28.8 29.7 23.7 25.1 27.1 28.5 29.1 29.8
Table 2: Bleu scores for the various training corpora, including baseline results without paraphrasing, results
for only paraphrasing unknown words, and results for paraphrasing any unseen phrase. Corpus size is
measured in sentences.
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Single w/o-ff 23.0 25.1 26.7 28.0 29.0 29.9 22.5 24.1 26.0 27.6 28.8 29.6
Multi w/o-ff 20.6 22.6 21.9 24.0 25.4 27.5 19.7 22.1 24.3 25.6 26.0 28.1
Table 3: Bleu scores for the various training corpora, when the paraphrase feature function is not included
5.1 Bleu scores
Table 2 gives the Bleu scores for each of these con-
ditions. We were able to measure a translation im-
provement for all sizes of training corpora, under
both the single word and multi-word conditions, ex-
cept for the largest Spanish-English corpus. For the
single word condition, it would have been surprising
if we had seen a decrease in Bleu score. Because we
are translating words that were previously untrans-
latable it would be unlikely that we could do any
worse. In the worst case we would be replacing one
word that did not occur in the reference translation
with another, and thus have no effect on Bleu.
More interesting is the fact that by paraphrasing
unseen multi-word units we get an increase in qual-
ity above and beyond the single word paraphrases.
These multi-word units may not have been observed
in the training data as a unit, but each of the compo-
nent words may have been. In this case translating
a paraphrase would not be guaranteed to received
an improved or identical Bleu score, as in the single
word case. Thus the improved Bleu score is notable.
Table 3 shows that incorporating the paraphrase
probability into the model?s feature functions plays a
critical role. Without it, the multi-word paraphrases
harm translation performance when compared to the
baseline.
5.2 Manual evaluation
We performed a manual evaluation by judging the
accuracy of phrases for 100 paraphrased translations
from each of the sets using the manual word align-
ments.1 Table 4 gives the percentage of time that
each of the translations of paraphrases were judged
to have the same meaning as the equivalent target
phrase. In the case of the translations of single word
paraphrases for the Spanish accuracy ranged from
just below 50% to just below 70%. This number
is impressive in light of the fact that none of those
items are correctly translated in the baseline model,
which simply inserts the foreign language word. As
with the Bleu scores, the translations of multi-word
paraphrases were judged to be more accurate than
the translations of single word paraphrases.
In performing the manual evaluation we were ad-
ditionally able to determine how often Bleu was ca-
pable of measuring an actual improvement in trans-
lation. For those items judged to have the same
meaning as the gold standard phrases we could
track how many would have contributed to a higher
Bleu score (that is, which of them were exactly
the same as the reference translation phrase, or had
some words in common with the reference trans-
lation phrase). By counting how often a correct
phrase would have contributed to an increased Bleu
score, and how often it would fail to increase the
Bleu score we were able to determine with what fre-
quency Bleu was sensitive to our improvements. We
found that Bleu was insensitive to our translation im-
provements between 60-75% of the time, thus re-
1Note that for the larger training corpora fewer than 100
paraphrases occurred in the first 150 and 250 sentence pairs.
22
Spanish-English French-English
Corpus size 10k 20k 40k 80k 160k 320k 10k 20k 40k 80k 160k 320k
Single word 48% 53% 57% 67%? 33%? 50%? 54% 49% 45% 50% 39%? 21%?
Multi-word 64% 65% 66% 71% 76%? 71%? 60% 67% 63% 58% 65% 42%?
Table 4: Percent of time that the translation of a paraphrase was judged to retain the same meaning as the
corresponding phrase in the gold standard. Starred items had fewer than 100 judgments and should not be
taken as reliable estimates.
Size 1-gram 2-gram 3-gram 4-gram
10k 48% 25% 10% 3%
20k 60% 35% 15% 6%
40k 71% 45% 22% 9%
80k 80% 55% 29% 12%
160k 86% 64% 37% 17%
320k 91% 71% 45% 22%
Table 5: The percent of the unique test set phrases
which have translations in each of the Spanish-
English training corpora prior to paraphrasing
inforcing our belief that it is not an appropriate mea-
sure for translation improvements of this sort.
5.3 Increase in coverage
As illustrated in Figure 1, translation models suffer
from sparse data. When only a very small paral-
lel corpus is available for training, translations are
learned for very few of the unique phrases in a test
set. If we exclude 451 words worth of names, num-
bers, and foreign language text in 2,000 sentences
that comprise the Spanish portion of the Europarl
test set, then the number of unique n-grams in text
are: 7,331 unigrams, 28,890 bigrams, 44,194 tri-
grams, and 48,259 4-grams. Table 5 gives the per-
centage of these which have translations in each of
the three training corpora, if we do not use para-
phrasing.
In contrast after expanding the phrase table using
the translations of paraphrases, the coverage of the
unique test set phrases goes up dramatically (shown
in Table 6). For the first training corpus with 10,000
sentence pairs and roughly 200,000 words of text in
each language, the coverage goes up from less than
50% of the vocabulary items being covered to 90%.
The coverage of unique 4-grams jumps from 3% to
16% ? a level reached only after observing more
Size 1-gram 2-gram 3-gram 4-gram
10k 90% 67% 37% 16%
20k 90% 69% 39% 17%
40k 91% 71% 41% 18%
80k 92% 73% 44% 20%
160k 92% 75% 46% 22%
320k 93% 77% 50% 25%
Table 6: The percent of the unique test set phrases
which have translations in each of the Spanish-
English training corpora after paraphrasing
than 100,000 sentence pairs, or roughly three mil-
lion words of text, without using paraphrases.
6 Related Work
Previous research on trying to overcome data spar-
sity issues in statistical machine translation has
largely focused on introducing morphological anal-
ysis as a way of reducing the number of types ob-
served in a training text. For example, Nissen and
Ney (2004) apply morphological analyzers to En-
glish and German and are able to reduce the amount
of training data needed to reach a certain level
of translation quality. Goldwater and McClosky
(2005) find that stemming Czech and using lemmas
improves the word-to-word correspondences when
training Czech-English alignment models. Koehn
and Knight (2003) show how monolingual texts and
parallel corpora can be used to figure out appropriate
places to split German compounds.
Still other approaches focus on ways of acquiring
data. Resnik and Smith (2003) develop a method
for gathering parallel corpora from the web. Oard
et al (2003) describe various methods employed
for quickly gathering resources to create a machine
translation system for a language with no initial re-
sources.
23
7 Discussion
In this paper we have shown that significant gains in
coverage and translation quality can be had by inte-
grating paraphrases into statistical machine transla-
tion. In effect, paraphrases introduce some amount
of generalization into statistical machine translation.
Whereas before we relied on having observed a par-
ticular word or phrase in the training set in order to
produce a translation of it, we are no longer tied to
having seen every word in advance. We can exploit
knowledge that is external to the translation model
about what words have similar meanings and use
that in the process of translation. This method is
particularly pertinent to small data conditions, which
are plagued by sparse data problems.
In future work, we plan to determine how much
data is required to learn useful paraphrases. The sce-
nario described in this paper was very favorable to
creating high quality paraphrases. The large number
of parallel corpora between Spanish and the other
languages present in the Europarl corpus allowed
us to generate high quality, in domain data. While
this is a realistic scenario, in that many new official
languages have been added to the European Union,
some of which do not yet have extensive parallel cor-
pora, we realize that this may be a slightly idealized
scenario.
Finally, we plan to formalize our targeted manual
evaluation method, in the hopes of creating a eval-
uation methodology for machine translation that is
more thorough and elucidating than Bleu.
Acknowledgments
Thank you to Alexandra Birch and Stephanie Van-
damme for creating the word alignments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL-2005.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In ACL-2001.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation. In Proceedings of EACL.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proceedings of EMNLP.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Sonja Nissen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic analysis. Computational Linguistics,
30(2):181?204.
Doug Oard, David Doermann, Bonnie Dorr, Daqing He,
Phillip Resnik, William Byrne, Sanjeeve Khudanpur,
David Yarowsky, Anton Leuski, Philipp Koehn, and
Kevin Knight. 2003. Desperately seeking Cebuano.
In Proceedings of HLT-NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
24
Statistical Machine Translation with
Word- and Sentence-Aligned Parallel Corpora
Chris Callison-Burch David Talbot Miles Osborne
School on Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
The parameters of statistical translation models are
typically estimated from sentence-aligned parallel
corpora. We show that significant improvements in
the alignment and translation quality of such mod-
els can be achieved by additionally including word-
aligned data during training. Incorporating word-
level alignments into the parameter estimation of
the IBM models reduces alignment error rate and
increases the Bleu score when compared to training
the same models only on sentence-aligned data. On
the Verbmobil data set, we attain a 38% reduction
in the alignment error rate and a higher Bleu score
with half as many training examples. We discuss
how varying the ratio of word-aligned to sentence-
aligned data affects the expected performance gain.
1 Introduction
Machine translation systems based on probabilistic
translation models (Brown et al, 1993) are gener-
ally trained using sentence-aligned parallel corpora.
For many language pairs these exist in abundant
quantities. However for new domains or uncommon
language pairs extensive parallel corpora are often
hard to come by.
Two factors could increase the performance of
statistical machine translation for new language
pairs and domains: a reduction in the cost of cre-
ating new training data, and the development of
more efficient methods for exploiting existing train-
ing data. Approaches such as harvesting parallel
corpora from the web (Resnik and Smith, 2003)
address the creation of data. We take the second,
complementary approach. We address the prob-
lem of efficiently exploiting existing parallel cor-
pora by adding explicit word-level alignments be-
tween a number of the sentence pairs in the train-
ing corpus. We modify the standard parameter esti-
mation procedure for IBM Models and HMM vari-
ants so that they can exploit these additional word-
level alignments. Our approach uses both word- and
sentence-level alignments for training material.
In this paper we:
1. Describe how the parameter estimation frame-
work of Brown et al (1993) can be adapted to
incorporate word-level alignments;
2. Report significant improvements in alignment
error rate and translation quality when training
on data with word-level alignments;
3. Demonstrate that the inclusion of word-level
alignments is more effective than using a bilin-
gual dictionary;
4. Show the importance of amplifying the contri-
bution of word-aligned data during parameter
estimation.
This paper shows that word-level alignments im-
prove the parameter estimates for translation mod-
els, which in turn results in improved statistical
translation for languages that do not have large
sentence-aligned parallel corpora.
2 Parameter Estimation Using
Sentence-Aligned Corpora
The task of statistical machine translation is to
choose the source sentence, e, that is the most prob-
able translation of a given sentence, f , in a for-
eign language. Rather than choosing e? that di-
rectly maximizes p(e|f), Brown et al (1993) apply
Bayes? rule and select the source sentence:
e? = argmax
e
p(e)p(f |e). (1)
In this equation p(e) is a language model probabil-
ity and is p(f |e) a translation model probability. A
series of increasingly sophisticated translation mod-
els, referred to as the IBM Models, was defined in
Brown et al (1993).
The translation model, p(f |e) defined as a
marginal probability obtained by summing over
word-level alignments, a, between the source and
target sentences:
p(f |e) =
?
a
p(f ,a|e). (2)
While word-level alignments are a crucial com-
ponent of the IBM models, the model parame-
ters are generally estimated from sentence-aligned
parallel corpora without explicit word-level align-
ment information. The reason for this is that
word-aligned parallel corpora do not generally ex-
ist. Consequently, word level alignments are treated
as hidden variables. To estimate the values of
these hidden variables, the expectation maximiza-
tion (EM) framework for maximum likelihood esti-
mation from incomplete data is used (Dempster et
al., 1977).
The previous section describes how the trans-
lation probability of a given sentence pair is ob-
tained by summing over all alignments p(f |e) =
?
a p(f ,a|e). EM seeks to maximize the marginal
log likelihood, log p(f |e), indirectly by iteratively
maximizing a bound on this term known as the ex-
pected complete log likelihood, ?log p(f ,a|e)?q(a),1
log p(f |e) = log
?
a
p(f ,a|e) (3)
= log
?
a
q(a)
p(f ,a|e)
q(a)
(4)
?
?
a
q(a) log
p(f ,a|e)
q(a)
(5)
= ?log p(f ,a|e)?q(a) + H(q(a))
where the bound in (5) is given by Jensen?s inequal-
ity. By choosing q(a) = p(a|f , e) this bound be-
comes an equality.
This maximization consists of two steps:
? E-step: calculate the posterior probability
under the current model of every permissi-
ble alignment for each sentence pair in the
sentence-aligned training corpus;
? M-step: maximize the expected log like-
lihood under this posterior distribution,
?log p(f ,a|e)?q(a), with respect to the model?s
parameters.
While in standard maximum likelihood estima-
tion events are counted directly to estimate param-
eter settings, in EM we effectively collect frac-
tional counts of events (here permissible alignments
weighted by their posterior probability), and use
these to iteratively update the parameters.
1Here ? ??q(?) denotes an expectation with respect to q(?).
Since only some of the permissible alignments
make sense linguistically, we would like EM to use
the posterior alignment probabilities calculated in
the E-step to weight plausible alignments higher
than the large number of bogus alignments which
are included in the expected complete log likeli-
hood. This in turn should encourage the parame-
ter adjustments made in the M-step to converge to
linguistically plausible values.
Since the number of permissible alignments for
a sentence grows exponentially in the length of the
sentences for the later IBM Models, a large num-
ber of informative example sentence pairs are re-
quired to distinguish between plausible and implau-
sible alignments. Given sufficient data the distinc-
tion occurs because words which are mutual trans-
lations appear together more frequently in aligned
sentences in the corpus.
Given the high number of model parameters and
permissible alignments, however, huge amounts of
data will be required to estimate reasonable transla-
tion models from sentence-aligned data alone.
3 Parameter Estimation Using Word- and
Sentence-Aligned Corpora
As an alternative to collecting a huge amount of
sentence-aligned training data, by annotating some
of our sentence pairs with word-level alignments
we can explicitly provide information to highlight
plausible alignments and thereby help parameters
converge upon reasonable settings with less training
data.
Since word-alignments are inherent in the IBM
translation models it is straightforward to incorpo-
rate this information into the parameter estimation
procedure. For sentence pairs with explicit word-
level alignments marked, fractional counts over all
permissible alignments need not be collected. In-
stead, whole counts are collected for the single hand
annotated alignment for each sentence pair which
has been word-aligned. By doing this the expected
complete log likelihood collapses to a single term,
the complete log likelihood (p(f ,a|e)), and the E-
step is circumvented.
The parameter estimation procedure now in-
volves maximizing the likelihood of data aligned
only at the sentence level and also of data aligned
at the word level. The mixed likelihood function,
M, combines the expected information contained
in the sentence-aligned data with the complete in-
formation contained in the word-aligned data.
M =
Ns?
s=1
(1? ?)?log p(fs,as|es)?q(as)
+
Nw?
w=1
? log p(fw,aw|ew) (6)
Here s and w index the Ns sentence-aligned sen-
tences and Nw word-aligned sentences in our cor-
pora respectively. Thus M combines the expected
complete log likelihood and the complete log likeli-
hood. In order to control the relative contributions
of the sentence-aligned and word-aligned data in
the parameter estimation procedure, we introduce a
mixing weight ? that can take values between 0 and
1.
3.1 The impact of word-level alignments
The impact of word-level alignments on parameter
estimation is closely tied to the structure of the IBM
Models. Since translation and word alignment pa-
rameters are shared between all sentences, the pos-
terior alignment probability of a source-target word
pair in the sentence-aligned section of the corpus
that were aligned in the word-aligned section will
tend to be relatively high.
In this way, the alignments from the word-aligned
data effectively percolate through to the sentence-
aligned data indirectly constraining the E-step of
EM.
3.2 Weighting the contribution of
word-aligned data
By incorporating ?, Equation 6 becomes an interpo-
lation of the expected complete log likelihood pro-
vided by the sentence-aligned data and the complete
log likelihood provided by word-aligned data.
The use of a weight to balance the contributions
of unlabeled and labeled data in maximum like-
lihood estimation was proposed by Nigam et al
(2000). ? quantifies our relative confidence in the
expected statistics and observed statistics estimated
from the sentence- and word-aligned data respec-
tively.
Standard maximum likelihood estimation (MLE)
which weighs all training samples equally, corre-
sponds to an implicit value of lambda equal to the
proportion of word-aligned data in the whole of
the training set: ? = NwNw+Ns . However, having
the total amount of sentence-aligned data be much
larger than the amount of word-aligned data implies
a value of ? close to zero. This means that M can be
maximized while essentially ignoring the likelihood
of the word-aligned data. Since we believe that the
explicit word-alignment information will be highly
effective in distinguishing plausible alignments in
the corpus as a whole, we expect to see benefits by
setting ? to amplify the contribution of the word-
aligned data set particularly when this is a relatively
small portion of the corpus.
4 Experimental Design
To perform our experiments with word-level aligne-
ments we modified GIZA++, an existing and freely
available implementation of the IBM models and
HMM variants (Och and Ney, 2003). Our modifi-
cations involved circumventing the E-step for sen-
tences which had word-level alignments and incor-
porating these observed alignment statistics in the
M-step. The observed and expected statistics were
weighted accordingly by ? and (1? ?) respectively
as were their contributions to the mixed log likeli-
hood.
In order to measure the accuracy of the predic-
tions that the statistical translation models make un-
der our various experimental settings, we choose
the alignment error rate (AER) metric, which is de-
fined in Och and Ney (2003). We also investigated
whether improved AER leads to improved transla-
tion quality. We used the alignments created during
our AER experiments as the input to a phrase-based
decoder. We translated a test set of 350 sentences,
and used the Bleu metric (Papineni et al, 2001) to
automatically evaluate machine translation quality.
We used the Verbmobil German-English parallel
corpus as a source of training data because it has
been used extensively in evaluating statistical trans-
lation and alignment accuracy. This data set comes
with a manually word-aligned set of 350 sentences
which we used as our test set.
Our experiments additionally required a very
large set of word-aligned sentence pairs to be in-
corporated in the training set. Since previous work
has shown that when training on the complete set
of 34,000 sentence pairs an alignment error rate as
low as 6% can be achieved for the Verbmobil data,
we automatically generated a set of alignments for
the entire training data set using the unmodified ver-
sion of GIZA++. We wanted to use automatic align-
ments in lieu of actual hand alignments so that we
would be able to perform experiments using large
data sets. We ran a pilot experiment to test whether
our automatic would produce similar results to man-
ual alignments.
We divided our manual word alignments into
training and test sets and compared the performance
of models trained on human aligned data against
models trained on automatically aligned data. A
Size of training corpus
Model .5k 2k 8k 16k
Model 1 29.64 24.66 22.64 21.68
HMM 18.74 15.63 12.39 12.04
Model 3 26.07 18.64 14.39 13.87
Model 4 20.59 16.05 12.63 12.17
Table 1: Alignment error rates for the various IBM
Models trained with sentence-aligned data
100-fold cross validation showed that manual and
automatic alignments produced AER results that
were similar to each other to within 0.1%.2
Having satisfied ourselves that automatic align-
ment were a sufficient stand-in for manual align-
ments, we performed our main experiments which
fell into the following categories:
1. Verifying that the use of word-aligned data has
an impact on the quality of alignments pre-
dicted by the IBM Models, and comparing the
quality increase to that gained by using a bilin-
gual dictionary in the estimation stage.
2. Evaluating whether improved parameter esti-
mates of alignment quality lead to improved
translation quality.
3. Experimenting with how increasing the ratio of
word-aligned to sentence-aligned data affected
the performance.
4. Experimenting with our ? parameter which al-
lows us to weight the relative contributions
of the word-aligned and sentence-aligned data,
and relating it to the ratio experiments.
5. Showing that improvements to AER and trans-
lation quality held for another corpus.
5 Results
5.1 Improved alignment quality
As a staring point for comparison we trained
GIZA++ using four different sized portions of the
Verbmobil corpus. For each of those portions we
output the most probable alignments of the testing
data for Model 1, the HMM, Model 3, and Model
2Note that we stripped out probable alignments from our
manually produced alignments. Probable alignments are large
blocks of words which the annotator was uncertain of how to
align. The many possible word-to-word translations implied by
the manual alignments led to lower results than with the auto-
matic alignments, which contained fewer word-to-word trans-
lation possibilities.
Size of training corpus
Model .5k 2k 8k 16k
Model 1 21.43 18.04 16.49 16.20
HMM 14.42 10.47 9.09 8.80
Model 3 20.56 13.25 10.82 10.51
Model 4 14.19 10.13 7.87 7.52
Table 2: Alignment error rates for the various IBM
Models trained with word-aligned data
4,3 and evaluated their AERs. Table 1 gives align-
ment error rates when training on 500, 2000, 8000,
and 16000 sentence pairs from Verbmobil corpus
without using any word-aligned training data.
We obtained much better results when incorpo-
rating word-alignments with our mixed likelihood
function. Table 2 shows the results for the differ-
ent corpus sizes, when all of the sentence pairs have
been word-aligned. The best performing model in
the unmodified GIZA++ code was the HMM trained
on 16,000 sentence pairs, which had an alignment
error rate of 12.04%. In our modified code the
best performing model was Model 4 trained on
16,000 sentence pairs (where all the sentence pairs
are word-aligned) with an alignment error rate of
7.52%. The difference in the best performing mod-
els represents a 38% relative reduction in AER. In-
terestingly, we achieve a lower AER than the best
performing unmodified models using a corpus that
is one-eight the size of the sentence-aligned data.
Figure 1 show an example of the improved
alignments that are achieved when using the word
aligned data. The example alignments were held
out sentence pairs that were aligned after training on
500 sentence pairs. The alignments produced when
the training on word-aligned data are dramatically
better than when training on sentence-aligned data.
We contrasted these improvements with the im-
provements that are to be had from incorporating a
bilingual dictionary into the estimation process. For
this experiment we allowed a bilingual dictionary
to constrain which words can act as translations of
each other during the initial estimates of translation
probabilities (as described in Och and Ney (2003)).
As can be seen in Table 3, using a dictionary reduces
the AER when compared to using GIZA++ without
a dictionary, but not as dramatically as integrating
the word-alignments. We further tried combining a
dictionary with our word-alignments but found that
the dictionary results in only very minimal improve-
ments over using word-alignments alone.
3We used the default training schemes for GIZA++, and left
model smoothing parameters at their default settings.
Th
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(a) Sentence-aligned
T
h
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(b) Word-aligned
T
h
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(c) Reference
Figure 1: Example alignments using sentence-aligned training data (a), using word-aligned data (b), and a
reference manual alignment (c)
Size of training corpus
Model .5k 2k 8k 16k
Model 1 23.56 20.75 18.69 18.37
HMM 15.71 12.15 9.91 10.13
Model 3 22.11 16.93 13.78 12.33
Model 4 17.07 13.60 11.49 10.77
Table 3: The improved alignment error rates when
using a dictionary instead of word-aligned data to
constrain word translations
Sentence-aligned Word-aligned
Size AER Bleu AER Bleu
500 20.59 0.211 14.19 0.233
2000 16.05 0.247 10.13 0.260
8000 12.63 0.265 7.87 0.278
16000 12.17 0.270 7.52 0.282
Table 4: Improved AER leads to improved transla-
tion quality
5.2 Improved translation quality
The fact that using word-aligned data in estimat-
ing the parameters for machine translation leads to
better alignments is predictable. A more signifi-
cant result is whether it leads to improved transla-
tion quality. In order to test that our improved pa-
rameter estimates lead to better translation quality,
we used a state-of-the-art phrase-based decoder to
translate a held out set of German sentences into
English. The phrase-based decoder extracts phrases
from the word alignments produced by GIZA++,
and computes translation probabilities based on the
frequency of one phrase being aligned with another
(Koehn et al, 2003). We trained a language model
AER when when
Ratio ? = Standard MLE ? = .9
0.1 11.73 9.40
0.2 10.89 8.66
0.3 10.23 8.13
0.5 8.65 8.19
0.7 8.29 8.03
0.9 7.78 7.78
Table 5: The effect of weighting word-aligned data
more heavily that its proportion in the training data
(corpus size 16000 sentence pairs)
using the 34,000 English sentences from the train-
ing set.
Table 4 shows that using word-aligned data leads
to better translation quality than using sentence-
aligned data. Particularly, significantly less data is
needed to achieve a high Bleu score when using
word alignments. Training on a corpus of 8,000 sen-
tence pairs with word alignments results in a higher
Bleu score than when training on a corpus of 16,000
sentence pairs without word alignments.
5.3 Weighting the word-aligned data
We have seen that using training data consisting
of entirely word-aligned sentence pairs leads to
better alignment accuracy and translation quality.
However, because manually word-aligning sentence
pairs costs more than just using sentence-aligned
data, it is unlikely that we will ever want to label
an entire corpus. Instead we will likely have a rel-
atively small portion of the corpus word aligned.
We want to be sure that this small amount of data
labeled with word alignments does not get over-
whelmed by a larger amount of unlabeled data.
 
0.07
 
0.075 0.08
 
0.085 0.09
 
0.095 0.1
 
0.105 0.11
 
0.115 0.12
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9
 
1
Alignment Error Rate
Lamb
da20% w
ord-a
ligned
50% 
word-
aligne
d
70% 
word-
aligne
d
100%
 word
-align
ed
Figure 2: The effect on AER of varying ? for a train-
ing corpus of 16K sentence pairs with various pro-
portions of word-alignments
Thus we introduced the ? weight into our mixed
likelihood function.
Table 5 compares the natural setting of ? (where
it is proportional to the amount of labeled data in the
corpus) to a value that amplifies the contribution of
the word-aligned data. Figure 2 shows a variety of
values for ?. It shows as ? increases AER decreases.
Placing nearly all the weight onto the word-aligned
data seems to be most effective.4 Note this did not
vary the training data size ? only the relative contri-
butions between sentence- and word-aligned train-
ing material.
5.4 Ratio of word- to sentence-aligned data
We also varied the ratio of word-aligned to
sentence-aligned data, and evaluated the AER and
Bleu scores, and assigned high value to ? (= 0.9).
Figure 3 shows how AER improves as more
word-aligned data is added. Each curve on the graph
represents a corpus size and shows its reduction in
error rate as more word-aligned data is added. For
example, the bottom curve shows the performance
of a corpus of 16,000 sentence pairs which starts
with an AER of just over 12% with no word-aligned
training data and decreases to an AER of 7.5% when
all 16,000 sentence pairs are word-aligned. This
curve essentially levels off after 30% of the data is
word-aligned. This shows that a small amount of
word-aligned data is very useful, and if we wanted
to achieve a low AER, we would only have to label
4,800 examples with their word alignments rather
than the entire corpus.
Figure 4 shows how the Bleu score improves as
more word-aligned data is added. This graph also
4At ? = 1 (not shown in Figure 2) the data that is only
sentence-aligned is ignored, and the AER is therefore higher.
 
0.06
 
0.08 0.1
 
0.12
 
0.14
 
0.16
 
0.18 0.2
 
0.22  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Alignment error rate
Ratio
 of wo
rd-alig
ned to
 sente
nce-a
ligned
 data
500 s
enten
ce pa
irs
2000 
sente
nce p
airs
8000 
sente
nce p
airs
16000
 sente
nce p
airs
Figure 3: The effect on AER of varying the ratio of
word-aligned to sentence-aligned data
 
0.2
 
0.21
 
0.22
 
0.23
 
0.24
 
0.25
 
0.26
 
0.27
 
0.28
 
0.29  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Bleu Score
Ratio
 of wo
rd-alig
ned to
 sente
nce-a
ligned
 data
500 s
enten
ce pa
irs
2000 
sente
nce p
airs
8000 
sente
nce p
airs
16000
 sente
nce p
airs
Figure 4: The effect on Bleu of varying the ratio of
word-aligned to sentence-aligned data
reinforces the fact that a small amount of word-
aligned data is useful. A corpus of 8,000 sentence
pairs with only 800 of them labeled with word align-
ments achieves a higher Bleu score than a corpus of
16,000 sentence pairs with no word alignments.
5.5 Evaluation using a larger training corpus
We additionally tested whether incorporating word-
level alignments into the estimation improved re-
sults for a larger corpus. We repeated our experi-
ments using the Canadian Hansards French-English
parallel corpus. Figure 6 gives a summary of the im-
provements in AER and Bleu score for that corpus,
when testing on a held out set of 484 hand aligned
sentences.
On the whole, alignment error rates are higher
and Bleu scores are considerably lower for the
Hansards corpus. This is probably due to the dif-
ferences in the corpora. Whereas the Verbmobil
corpus has a small vocabulary (<10,000 per lan-
Sentence-aligned Word-aligned
Size AER Bleu AER Bleu
500 33.65 0.054 25.73 0.064
2000 25.97 0.087 18.57 0.100
8000 19.00 0.115 14.57 0.120
16000 16.59 0.126 13.55 0.128
Table 6: Summary results for AER and translation
quality experiments on Hansards data
guage), the Hansards has ten times that many vocab-
ulary items and has a much longer average sentence
length. This made it more difficult for us to create a
simulated set of hand alignments; we measured the
AER of our simulated alignments at 11.3% (which
compares to 6.5% for our simulated alignments for
the Verbmobil corpus).
Nevertheless, the trend of decreased AER and in-
creased Bleu score still holds. For each size of train-
ing corpus we tested we found better results using
the word-aligned data.
6 Related Work
Och and Ney (2003) is the most extensive analy-
sis to date of how many different factors contribute
towards improved alignments error rates, but the in-
clusion of word-alignments is not considered. Och
and Ney do not give any direct analysis of how
improved word alignments accuracy contributes to-
ward better translation quality as we do here.
Mihalcea and Pedersen (2003) described a shared
task where the goal was to achieve the best AER. A
number of different methods were tried, but none
of them used word-level alignments. Since the best
performing system used an unmodified version of
Giza++, we would expected that our modifed ver-
sion would show enhanced performance. Naturally
this would need to be tested in future work.
Melamed (1998) describes the process of manu-
ally creating a large set of word-level alignments of
sentences in a parallel text.
Nigam et al (2000) described the use of weight
to balance the respective contributions of labeled
and unlabeled data to a mixed likelihood function.
Corduneanu (2002) provides a detailed discussion
of the instability of maximum likelhood solutions
estimated from a mixture of labeled and unlabeled
data.
7 Discussion and Future Work
In this paper we show with the appropriate modifi-
cation of EM significant improvement gains can be
had through labeling word alignments in a bilingual
corpus. Because of this significantly less data is re-
quired to achieve a low alignment error rate or high
Bleu score. This holds even when using noisy word
alignments such as our automatically created set.
One should take our research into account when
trying to efficiently create a statistical machine
translation system for a language pair for which a
parallel corpus is not available. Germann (2001)
describes the cost of building a Tamil-English paral-
lel corpus from scratch, and finds that using profes-
sional translations is prohibitively high. In our ex-
perience it is quicker to manually word-align trans-
lated sentence pairs than to translate a sentence, and
word-level alignment can be done by someone who
might not be fluent enough to produce translations.
It might therefore be possible to achieve a higher
performance at a fraction of the cost by hiring a non-
professional produce word-alignments after a lim-
ited set of sentences have been translated.
We plan to investigate whether it is feasible to
use active learning to select which examples will
be most useful when aligned at the word-level. Sec-
tion 5.4 shows that word-aligning a fraction of sen-
tence pairs in a training corpus, rather than the entire
training corpus can still yield most of the benefits
described in this paper. One would hope that by se-
lectively sampling which sentences are to be manu-
ally word-aligned we would achieve nearly the same
performance as word-aligning the entire corpus.
Acknowledgements
The authors would like to thank Franz Och, Her-
mann Ney, and Richard Zens for providing the
Verbmobil data, and Linear B for providing its
phrase-based decoder.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Adrian Corduneanu. 2002. Stable mixing of complete
and incomplete information. Master?s thesis, Mas-
sachusetts Institute of Technology, February.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?38, Nov.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France,
July 7.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the HLT/NAACL.
I. Dan Melamed. 1998. Manual annotation of trans-
lational equivalence: The blinker project. Cognitive
Science Technical Report 98/07, University of Penn-
sylvania.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts.
Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,
and Tom M. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2/3):103?134.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
Proceedings of the 43rd Annual Meeting of the ACL, pages 10?17,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling Conditional Random Fields Using Error-Correcting Codes
Trevor Cohn
Department of Computer Science
and Software Engineering
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Conditional Random Fields (CRFs) have
been applied with considerable success to
a number of natural language processing
tasks. However, these tasks have mostly
involved very small label sets. When
deployed on tasks with larger label
sets, the requirements for computational
resources mean that training becomes
intractable.
This paper describes a method for train-
ing CRFs on such tasks, using error cor-
recting output codes (ECOC). A number
of CRFs are independently trained on the
separate binary labelling tasks of distin-
guishing between a subset of the labels
and its complement. During decoding,
these models are combined to produce a
predicted label sequence which is resilient
to errors by individual models.
Error-correcting CRF training is much
less resource intensive and has a much
faster training time than a standardly
formulated CRF, while decoding
performance remains quite comparable.
This allows us to scale CRFs to previously
impossible tasks, as demonstrated by our
experiments with large label sets.
1 Introduction
Conditional random fields (CRFs) (Lafferty et
al., 2001) are probabilistic models for labelling
sequential data. CRFs are undirected graphical
models that define a conditional distribution over
label sequences given an observation sequence.
They allow the use of arbitrary, overlapping,
non-independent features as a result of their global
conditioning. This allows us to avoid making
unwarranted independence assumptions over the
observation sequence, such as those required by
typical generative models.
Efficient inference and training methods exist
when the graphical structure of the model forms
a chain, where each position in a sequence is
connected to its adjacent positions. CRFs have been
applied with impressive empirical results to the
tasks of named entity recognition (McCallum and
Li, 2003), simplified part-of-speech (POS) tagging
(Lafferty et al, 2001), noun phrase chunking (Sha
and Pereira, 2003) and extraction of tabular data
(Pinto et al, 2003), among other tasks.
CRFs are usually estimated using gradient-based
methods such as limited memory variable metric
(LMVM). However, even with these efficient
methods, training can be slow. Consequently, most
of the tasks to which CRFs have been applied are
relatively small scale, having only a small number
of training examples and small label sets. For
much larger tasks, with hundreds of labels and
millions of examples, current training methods
prove intractable. Although training can potentially
be parallelised and thus run more quickly on large
clusters of computers, this in itself is not a solution
to the problem: tasks can reasonably be expected
to increase in size and complexity much faster
than any increase in computing power. In order to
provide scalability, the factors which most affect the
resource usage and runtime of the training method
10
must be addressed directly ? ideally the dependence
on the number of labels should be reduced.
This paper presents an approach which enables
CRFs to be used on larger tasks, with a significant
reduction in the time and resources needed for
training. This reduction does not come at the cost
of performance ? the results obtained on benchmark
natural language problems compare favourably,
and sometimes exceed, the results produced from
regular CRF training. Error correcting output
codes (ECOC) (Dietterich and Bakiri, 1995) are
used to train a community of CRFs on binary
tasks, with each discriminating between a subset
of the labels and its complement. Inference is
performed by applying these ?weak? models to an
unknown example, with each component model
removing some ambiguity when predicting the label
sequence. Given a sufficient number of binary
models predicting suitably diverse label subsets, the
label sequence can be inferred while being robust
to a number of individual errors from the weak
models. As each of these weak models are binary,
individually they can be efficiently trained, even
on large problems. The number of weak learners
required to achieve good performance is shown to
be relatively small on practical tasks, such that the
overall complexity of error-correcting CRF training
is found to be much less than that of regular CRF
training methods.
We have evaluated the error-correcting CRF on
the CoNLL 2003 named entity recognition (NER)
task (Sang and Meulder, 2003), where we show
that the method yields similar generalisation perfor-
mance to standardly formulated CRFs, while requir-
ing only a fraction of the resources, and no increase
in training time. We have also shown how the error-
correcting CRF scales when applied to the larger
task of POS tagging the Penn Treebank and also
the even larger task of simultaneously noun phrase
chunking (NPC) and POS tagging using the CoNLL
2000 data-set (Sang and Buchholz, 2000).
2 Conditional random fields
CRFs are undirected graphical models used to spec-
ify the conditional probability of an assignment of
output labels given a set of input observations. We
consider only the case where the output labels of the
model are connected by edges to form a linear chain.
The joint distribution of the label sequence, y, given
the input observation sequence, x, is given by
p(y|x) = 1Z(x) exp
T+1
?
t=1
?
k
?kfk(t,yt?1,yt,x)
where T is the length of both sequences and ?k are
the parameters of the model. The functions fk are
feature functions which map properties of the obser-
vation and the labelling into a scalar value. Z(x)
is the partition function which ensures that p is a
probability distribution.
A number of algorithms can be used to find the
optimal parameter values by maximising the log-
likelihood of the training data. Assuming that the
training sequences are drawn IID from the popula-
tion, the conditional log likelihood L is given by
L =
?
i
log p(y(i)|x(i))
=
?
i
?
?
?
T (i)+1
?
t=1
?
k
?kfk(t,y(i)t?1,y
(i)
t ,x(i))
? log Z(x(i))
}
where x(i) and y(i) are the ith observation and label
sequence. Note that a prior is often included in the
L formulation; it has been excluded here for clar-
ity of exposition. CRF estimation methods include
generalised iterative scaling (GIS), improved itera-
tive scaling (IIS) and a variety of gradient based
methods. In recent empirical studies on maximum
entropy models and CRFs, limited memory variable
metric (LMVM) has proven to be the most efficient
method (Malouf, 2002; Wallach, 2002); accord-
ingly, we have used LMVM for CRF estimation.
Every iteration of LMVM training requires the
computation of the log-likelihood and its deriva-
tive with respect to each parameter. The partition
function Z(x) can be calculated efficiently using
dynamic programming with the forward algorithm.
Z(x) is given by?y ?T (y) where ? are the forward
values, defined recursively as
?t+1(y) =
?
y?
?t(y?) exp
?
k
?kfk(t + 1, y?, y,x)
11
The derivative of the log-likelihood is given by
?L
??k
=
?
i
?
?
?
T (i)+1
?
t=1
fk(t,y(i)t?1,y
(i)
t ,x(i))
?
?
y
p(y|x(i))
T (i)+1
?
t=1
fk(t,yt?1,yt,x(i))
?
?
?
The first term is the empirical count of feature k,
and the second is the expected count of the feature
under the model. When the derivative equals zero ?
at convergence ? these two terms are equal. Evalu-
ating the first term of the derivative is quite simple.
However, the sum over all possible labellings in the
second term poses more difficulties. This term can
be factorised, yielding
?
t
?
y?,y
p(Yt?1 = y?, Yt = y|x(i))fk(t, y?, y,x(i))
This term uses the marginal distribution over pairs of
labels, which can be efficiently computed from the
forward and backward values as
?t?1(y?) exp
?
k ?kfk(t, y?, y,x(i))?t(y)
Z(x(i))
The backward probabilities ? are defined by the
recursive relation
?t(y) =
?
y?
?t+1(y?) exp
?
k
?kfk(t + 1, y, y?,x)
Typically CRF training using LMVM requires
many hundreds or thousands of iterations, each of
which involves calculating of the log-likelihood
and its derivative. The time complexity of a single
iteration is O(L2NTF ) where L is the number
of labels, N is the number of sequences, T is
the average length of the sequences, and F is
the average number of activated features of each
labelled clique. It is not currently possible to state
precise bounds on the number of iterations required
for certain problems; however, problems with a
large number of sequences often require many more
iterations to converge than problems with fewer
sequences. Note that efficient CRF implementations
cache the feature values for every possible clique
labelling of the training data, which leads to a
memory requirement with the same complexity of
O(L2NTF ) ? quite demanding even for current
computer hardware.
3 Error Correcting Output Codes
Since the time and space complexity of CRF
estimation is dominated by the square of the number
of labels, it follows that reducing the number
of labels will significantly reduce the complexity.
Error-correcting coding is an approach which recasts
multiple label problems into a set of binary label
problems, each of which is of lesser complexity than
the full multiclass problem. Interestingly, training a
set of binary CRF classifiers is overall much more
efficient than training a full multi-label model. This
is because error-correcting CRF training reduces
the L2 complexity term to a constant. Decoding
proceeds by predicting these binary labels and then
recovering the encoded actual label.
Error-correcting output codes have been used for
text classification, as in Berger (1999), on which the
following is based. Begin by assigning to each of the
m labels a unique n-bit string Ci, which we will call
the code for this label. Now train n binary classi-
fiers, one for each column of the coding matrix (con-
structed by taking the labels? codes as rows). The j th
classifier, ?j , takes as positive instances those with
label i where Cij = 1. In this way, each classifier
learns a different concept, discriminating between
different subsets of the labels.
We denote the set of binary classifiers as
? = {?1, ?2, . . . , ?n}, which can be used for
prediction as follows. Classify a novel instance x
with each of the binary classifiers, yielding a n-bit
vector ?(x) = {?1(x), ?2(x), . . . , ?n(x)}. Now
compare this vector to the codes for each label. The
vector may not exactly match any of the labels due
to errors in the individual classifiers, and thus we
chose the actual label which minimises the distance
argmini?(?(x), Ci). Typically the Hamming
distance is used, which simply measures the number
of differing bit positions. In this manner, prediction
is resilient to a number of prediction errors by the
binary classifiers, provided the codes for the labels
are sufficiently diverse.
3.1 Error-correcting CRF training
Error-correcting codes can also be applied to
sequence labellers, such as CRFs, which are capable
of multiclass labelling. ECOCs can be used with
CRFs in a similar manner to that given above for
12
classifiers. A series of CRFs are trained, each
on a relabelled variant of the training data. The
relabelling for each binary CRF maps the labels
into binary space using the relevant column of the
coding matrix, such that label i is taken as a positive
for the jth model example if Cij = 1.
Training with a binary label set reduces the time
and space complexity for each training iteration to
O(NTF ); the L2 term is now a constant. Pro-
vided the code is relatively short (i.e. there are
few binary models, or weak learners), this translates
into considerable time and space savings. Coding
theory doesn?t offer any insights into the optimal
code length (i.e. the number of weak learners).
When using a very short code, the error-correcting
CRF will not adequately model the decision bound-
aries between all classes. However, using a long
code will lead to a higher degree of dependency
between pairs of classifiers, where both model simi-
lar concepts. The generalisation performance should
improve quickly as the number of weak learners
(code length) increases, but these gains will diminish
as the inter-classifier dependence increases.
3.2 Error-correcting CRF decoding
While training of error-correcting CRFs is simply
a logical extension of the ECOC classifier method
to sequence labellers, decoding is a different mat-
ter. We have applied three decoding different strate-
gies. The Standalone method requires each binary
CRF to find the Viterbi path for a given sequence,
yielding a string of 0s and 1s for each model. For
each position t in the sequence, the tth bit from
each model is taken, and the resultant bit string
compared to each of the label codes. The label
with the minimum Hamming distance is then cho-
sen as the predicted label for that site. This method
allows for error correction to occur at each site, how-
ever it discards information about the uncertainty of
each weak learner, instead only considering the most
probable paths.
The Marginals method of decoding uses the
marginal probability distribution at each position
in the sequence instead of the Viterbi paths. This
distribution is easily computed using the forward
backward algorithm. The decoding proceeds as
before, however instead of a bit string we have a
vector of probabilities. This vector is compared
to each of the label codes using the L1 distance,
and the closest label is chosen. While this method
incorporates the uncertainty of the binary models, it
does so at the expense of the path information in the
sequence.
Neither of these decoding methods allow the
models to interact, although each individual weak
learner may benefit from the predictions of the
other weak learners. The Product decoding method
addresses this problem. It treats each weak model
as an independent predictor of the label sequence,
such that the probability of the label sequence given
the observations can be re-expressed as the product
of the probabilities assigned by each weak model.
A given labelling y is projected into a bit string for
each weak learner, such that the ith entry in the
string is Ckj for the jth weak learner, where k is
the index of label yi. The weak learners can then
estimate the probability of the bit string; these are
then combined into a global product to give the
probability of the label sequence
p(y|x) = 1Z ?(x)
?
j
pj(bj(y)|x)
where pj(q|x) is the predicted probability of q given
x by the jth weak learner, bj(y) is the bit string
representing y for the jth weak learner and Z ?(x)
is the partition function. The log probability is
?
j
{Fj(bj(y), x) ? ?j ? log Zj(x)} ? log Z ?(x)
where Fj(y, x) = ?T+1t=1 fj(t,yt?1,yt,x). This log
probability can then be maximised using the Viterbi
algorithm as before, noting that the two log terms are
constant with respect to y and thus need not be eval-
uated. Note that this decoding is an equivalent for-
mulation to a uniformly weighted logarithmic opin-
ion pool, as described in Smith et al (2005).
Of the three decoding methods, Standalone
has the lowest complexity, requiring only a binary
Viterbi decoding for each weak learner. Marginals
is slightly more complex, requiring the forward
and backward values. Product, however, requires
Viterbi decoding with the full label set, and many
features ? the union of the features of each weak
learner ? which can be quite computationally
demanding.
13
3.3 Choice of code
The accuracy of ECOC methods are highly depen-
dent on the quality of the code. The ideal code
has diverse rows, yielding a high error-correcting
capability, and diverse columns such that the weak
learners model highly independent concepts. When
the number of labels, k, is small, an exhaustive
code with every unique column is reasonable, given
there are 2k?1 ? 1 unique columns. With larger
label sets, columns must be selected with care to
maximise the inter-row and inter-column separation.
This can be done by randomly sampling the column
space, in which case the probability of poor separa-
tion diminishes quickly as the number of columns
increases (Berger, 1999). Algebraic codes, such as
BCH codes, are an alternative coding scheme which
can provide near-optimal error-correcting capabil-
ity (MacWilliams and Sloane, 1977), however these
codes provide no guarantee of good column separa-
tion.
4 Experiments
Our experiments show that error-correcting CRFs
are highly accurate on benchmark problems with
small label sets, as well as on larger problems with
many more labels, which would be otherwise prove
intractable for traditional CRFs. Moreover, with a
good code, the time and resources required for train-
ing and decoding can be much less than that of the
standardly formulated CRF.
4.1 Named entity recognition
CRFs have been used with strong results on the
CoNLL 2003 NER task (McCallum, 2003) and thus
this task is included here as a benchmark. This data
set consists of a 14,987 training sentences (204,567
tokens) drawn from news articles, tagged for per-
son, location, organisation and miscellaneous enti-
ties. There are 8 IOB-2 style labels.
A multiclass (standardly formulated) CRF was
trained on these data using features covering word
identity, word prefix and suffix, orthographic tests
for digits, case and internal punctuation, word
length, POS tag and POS tag bigrams before and
after the current word. Only features seen at least
once in the training data were included in the model,
resulting in 450,345 binary features. The model was
Model Decoding MLE Regularised
Multiclass 88.04 89.78
Coded standalone 88.23? 88.67?
marginals 88.23? 89.19
product 88.69? 89.69
Table 1: F1 scores on NER task.
trained without regularisation and with a Gaussian
prior. An exhaustive code was created with all
127 unique columns. All of the weak learners
were trained with the same feature set, each having
around 315,000 features. The performance of the
standard and error-correcting models are shown in
Table 1. We tested for statistical significance using
the matched pairs test (Gillick and Cox, 1989) at
p < 0.001. Those results which are significantly
better than the corresponding multiclass MLE or
regularised model are flagged with a ?, and those
which are significantly worse with a ?.
These results show that error-correcting CRF
training achieves quite similar performance to the
multiclass CRF on the task (which incidentally
exceeds McCallum (2003)?s result of 89.0 using
feature induction). Product decoding was the
better of the three methods, giving the best
performance both with and without regularisation,
although this difference was only statistically
significant between the regularised standalone and
the regularised product decoding. The unregularised
error-correcting CRF significantly outperformed
the multiclass CRF with all decoding strategies,
suggesting that the method already provides some
regularisation, or corrects some inherent bias in the
model.
Using such a large number of weak learners is
costly, in this case taking roughly ten times longer
to train than the multiclass CRF. However, much
shorter codes can also achieve similar results. The
simplest code, where each weak learner predicts
only a single label (a.k.a. one-vs-all), achieved an
F score of 89.56, while only requiring 8 weak learn-
ers and less than half the training time as the multi-
class CRF. This code has no error correcting capa-
bility, suggesting that the code?s column separation
(and thus interdependence between weak learners)
is more important than its row separation.
14
An exhaustive code was used in this experiment
simply for illustrative purposes: many columns
in this code were unnecessary, yielding only a
slight gain in performance over much simpler
codes while incurring a very large increase in
training time. Therefore, by selecting a good subset
of the exhaustive code, it should be possible to
reduce the training time while preserving the strong
generalisation performance. One approach is to
incorporate skew in the label distribution in our
choice of code ? the code should minimise the
confusability of commonly occurring labels more
so than that of rare labels. Assuming that errors
made by the weak learners are independent, the
probability of a single error, q, as a function of the
code length n can be bounded by
q(n) ? 1 ?
?
l
p(l)
bhl?12 c
?
i=0
(
n
i
)
p?i(1 ? p?)n?i
where p(l) is the marginal probability of the label l,
hl is the minimum Hamming distance between l and
any other label, and p? is the maximum probability
of an error by a weak learner. The performance
achieved by selecting the code with the minimum
loss bound from a large random sample of codes
is shown in Figure 1, using standalone decoding,
where p? was estimated on the development set. For
comparison, randomly sampled codes and a greedy
oracle are shown. The two random sampled codes
show those samples where no column is repeated,
and where duplicate columns are permitted (random
with replacement). The oracle repeatedly adds to the
code the column which most improves its F1 score.
The minimum loss bound method allows the per-
formance plateau to be reached more quickly than
random sampling; i.e. shorter codes can be used,
thus allowing more efficient training and decoding.
Note also that multiclass CRF training required
830Mb of memory, while error-correcting training
required only 380Mb. Decoding of the test set
(51,362 tokens) with the error-correcting model
(exhaustive, MLE) took between 150 seconds for
standalone decoding and 173 seconds for integrated
decoding. The multiclass CRF was much faster,
taking only 31 seconds, however this time difference
could be reduced with suitable optimisations.
 83
 84
 85
 86
 87
 88
 89
 90
 10  15  20  25  30  35  40  45  50
F1
 s
co
re
code length
random
random with replacement
minimum loss bound
oracle
MLE multiclass CRF
Regularised multiclass CRF
Figure 1: NER F1 scores for standalone decoding
with random codes, a minimum loss code and a
greedy oracle.
Coding Decoding MLE Regularised
Multiclass 95.69 95.78
Coded - 200 standalone 95.63 96.03
marginals 95.68 96.03
One-vs-all product 94.90 96.57
Table 2: POS tagging accuracy.
4.2 Part-of-speech Tagging
CRFs have been applied to POS tagging, however
only with a very simple feature set and small training
sample (Lafferty et al, 2001). We used the Penn
Treebank Wall Street Journal articles, training on
sections 2?21 and testing on section 24. In this
task there are 45,110 training sentences, a total of
1,023,863 tokens and 45 labels.
The features used included word identity, prefix
and suffix, whether the word contains a number,
uppercase letter or a hyphen, and the words one
and two positions before and after the current word.
A random code of 200 columns was used for this
task. These results are shown in Table 2, along with
those of a multiclass CRF and an alternative one-vs-
all coding. As for the NER experiment, the decod-
ing performance levelled off after 100 bits, beyond
which the improvements from longer codes were
only very slight. This is a very encouraging char-
acteristic, as only a small number of weak learners
are required for good performance.
15
The random code of 200 bits required 1,300Mb
of RAM, taking a total of 293 hours to train and
3 hours to decode (54,397 tokens) on similar
machines to those used before. We do not have
figures regarding the resources used by Lafferty et
al.?s CRF for the POS tagging task and our attempts
to train a multiclass CRF for full-scale POS tagging
were thwarted due to lack of sufficient available
computing resources. Instead we trained on a
10,000 sentence subset of the training data, which
required approximately 17Gb of RAM and 208
hours to train.
Our best result on the task was achieved using
a one-vs-all code, which reduced the training
time to 25 hours, as it only required training 45
binary models. This result exceeds Lafferty et al?s
accuracy of 95.73% using a CRF but falls short of
Toutanova et al (2003)?s state-of-the-art 97.24%.
This is most probably due to our only using a
first-order Markov model and a fairly simple feature
set, where Tuotanova et al include a richer set of
features in a third order model.
4.3 Part-of-speech Tagging and Noun Phrase
Segmentation
The joint task of simultaneously POS tagging and
noun phrase chunking (NPC) was included in order
to demonstrate the scalability of error-correcting
CRFs. The data was taken from the CoNLL 2000
NPC shared task, with the model predicting both the
chunk tags and the POS tags. The training corpus
consisted of 8,936 sentences, with 47,377 tokens
and 118 labels.
A 200-bit random code was used, with the follow-
ing features: word identity within a window, pre-
fix and suffix of the current word and the presence
of a digit, hyphen or upper case letter in the cur-
rent word. This resulted in about 420,000 features
for each weak learner. A joint tagging accuracy of
90.78% was achieved using MLE training and stan-
dalone decoding. Despite the large increase in the
number of labels in comparison to the earlier tasks,
the performance also began to plateau at around 100
bits. This task required 220Mb of RAM and took a
total of 30 minutes to train each of the 200 binary
CRFs, this time on Pentium 4 machines with 1Gb
RAM. Decoding of the 47,377 test tokens took 9,748
seconds and 9,870 seconds for the standalone and
marginals methods respectively.
Sutton et al (2004) applied a variant of the CRF,
the dynamic CRF (DCRF), to the same task, mod-
elling the data with two interconnected chains where
one chain predicted NPC tags and the other POS
tags. They achieved better performance and train-
ing times than our model; however, this is not a
fair comparison, as the two approaches are orthogo-
nal. Indeed, applying the error-correcting CRF algo-
rithms to DCRF models could feasibly decrease the
complexity of the DCRF, allowing the method to be
applied to larger tasks with richer graphical struc-
tures and larger label sets.
In all three experiments, error-correcting CRFs
have achieved consistently good generalisation per-
formance. The number of weak learners required
to achieve these results was shown to be relatively
small, even for tasks with large label sets. The time
and space requirements were lower than those of a
traditional CRF for the larger tasks and, most impor-
tantly, did not increase substantially when the num-
ber of labels was increased.
5 Related work
Most recent work on improving CRF performance
has focused on feature selection. McCallum (2003)
describes a technique for greedily adding those
feature conjuncts to a CRF which significantly
improve the model?s log-likelihood. His experi-
mental results show that feature induction yields a
large increase in performance, however our results
show that standardly formulated CRFs can perform
well above their reported 73.3%, casting doubt
on the magnitude of the possible improvement.
Roark et al (2004) have also employed feature
selection to the huge task of language modelling
with a CRF, by partially training a voted perceptron
then removing all features that the are ignored
by the perceptron. The act of automatic feature
selection can be quite time consuming in itself,
while the performance and runtime gains are often
modest. Even with a reduced number of features,
tasks with a very large label space are likely to
remain intractable.
16
6 Conclusion
Standard training methods for CRFs suffer greatly
from their dependency on the number of labels,
making tasks with large label sets either difficult
or impossible. As CRFs are deployed more widely
to tasks with larger label sets this problem will
become more evident. The current ?solutions? to
these scaling problems ? namely feature selection,
and the use of large clusters ? don?t address the
heart of the problem: the dependence on the square
of number of labels.
Error-correcting CRF training allows CRFs to be
applied to larger problems and those with larger
label sets than were previously possible, without
requiring computationally demanding methods such
as feature selection. On standard tasks we have
shown that error-correcting CRFs provide compa-
rable or better performance than the standardly for-
mulated CRF, while requiring less time and space to
train. Only a small number of weak learners were
required to obtain good performance on the tasks
with large label sets, demonstrating that the method
provides efficient scalability to the CRF framework.
Error-correction codes could be applied to
other sequence labelling methods, such as the
voted perceptron (Roark et al, 2004). This may
yield an increase in performance and efficiency
of the method, as its runtime is also heavily
dependent on the number of labels. We plan to
apply error-correcting coding to dynamic CRFs,
which should result in better modelling of naturally
layered tasks, while increasing the efficiency and
scalability of the method. We also plan to develop
higher order CRFs, using error-correcting codes to
curb the increase in complexity.
7 Acknowledgements
This work was supported in part by a PORES travel-
ling scholarship from the University of Melbourne,
allowing Trevor Cohn to travel to Edinburgh.
References
Adam Berger. 1999. Error-correcting output coding for
text classification. In Proceedings of IJCAI: Workshop on
machine learning for information filtering.
Thomas G. Dietterich and Ghulum Bakiri. 1995. Solving mul-
ticlass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Reseach, 2:263?286.
L. Gillick and Stephen Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Pro-
ceedings of the IEEE Conference on Acoustics, Speech and
Signal Processing, pages 532?535, Glasgow, Scotland.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings of
ICML 2001, pages 282?289.
Florence MacWilliams and Neil Sloane. 1977. The theory of
error-correcting codes. North Holland, Amsterdam.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
CoNLL 2002, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature
induction and web-enhanced lexicons. In Proceedings of
CoNLL 2003, pages 188?191.
Andrew McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI 2003,
pages 403?410.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, pages 235?242.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Pro-
ceedings of ACL 2004, pages 48?55.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proceed-
ings of CoNLL 2000 and LLL 2000, pages 127?132.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL 2003,
pages 142?147, Edmonton, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of HLT-NAACL
2003, pages 213?220.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Pro-
ceedings of ACL 2005.
Charles Sutton, Khashayar Rohanimanesh, and Andrew McCal-
lum. 2004. Dynamic conditional random fields: Factorized
probabilistic models for labelling and segmenting sequence
data. In Proceedings of the ICML 2004.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of HLT-
NAACL 2003, pages 252?259.
Hanna Wallach. 2002. Efficient training of conditional random
fields. Master?s thesis, University of Edinburgh.
17
Proceedings of the 43rd Annual Meeting of the ACL, pages 18?25,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Logarithmic Opinion Pools for Conditional Random Fields
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Trevor Cohn
Department of Computer Science
and Software Engineering
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Recent work on Conditional Random
Fields (CRFs) has demonstrated the need
for regularisation to counter the tendency
of these models to overfit. The standard
approach to regularising CRFs involves a
prior distribution over the model parame-
ters, typically requiring search over a hy-
perparameter space. In this paper we ad-
dress the overfitting problem from a dif-
ferent perspective, by factoring the CRF
distribution into a weighted product of in-
dividual ?expert? CRF distributions. We
call this model a logarithmic opinion
pool (LOP) of CRFs (LOP-CRFs). We ap-
ply the LOP-CRF to two sequencing tasks.
Our results show that unregularised expert
CRFs with an unregularised CRF under
a LOP can outperform the unregularised
CRF, and attain a performance level close
to the regularised CRF. LOP-CRFs there-
fore provide a viable alternative to CRF
regularisation without the need for hyper-
parameter search.
1 Introduction
In recent years, conditional random fields (CRFs)
(Lafferty et al, 2001) have shown success on a num-
ber of natural language processing (NLP) tasks, in-
cluding shallow parsing (Sha and Pereira, 2003),
named entity recognition (McCallum and Li, 2003)
and information extraction from research papers
(Peng and McCallum, 2004). In general, this work
has demonstrated the susceptibility of CRFs to over-
fit the training data during parameter estimation. As
a consequence, it is now standard to use some form
of overfitting reduction in CRF training.
Recently, there have been a number of sophisti-
cated approaches to reducing overfitting in CRFs,
including automatic feature induction (McCallum,
2003) and a full Bayesian approach to training and
inference (Qi et al, 2005). These advanced meth-
ods tend to be difficult to implement and are of-
ten computationally expensive. Consequently, due
to its ease of implementation, the current standard
approach to reducing overfitting in CRFs is the use
of a prior distribution over the model parameters,
typically a Gaussian. The disadvantage with this
method, however, is that it requires adjusting the
value of one or more of the distribution?s hyper-
parameters. This usually involves manual or auto-
matic tuning on a development set, and can be an ex-
pensive process as the CRF must be retrained many
times for different hyperparameter values.
In this paper we address the overfitting problem
in CRFs from a different perspective. We factor the
CRF distribution into a weighted product of indi-
vidual expert CRF distributions, each focusing on
a particular subset of the distribution. We call this
model a logarithmic opinion pool (LOP) of CRFs
(LOP-CRFs), and provide a procedure for learning
the weight of each expert in the product. The LOP-
CRF framework is ?parameter-free? in the sense that
it does not involve the requirement to adjust hyper-
parameter values.
LOP-CRFs are theoretically advantageous in that
their Kullback-Leibler divergence with a given dis-
tribution can be explicitly represented as a function
of the KL-divergence with each of their expert dis-
tributions. This provides a well-founded framework
for designing new overfitting reduction schemes:
18
look to factorise a CRF distribution as a set of di-
verse experts.
We apply LOP-CRFs to two sequencing tasks in
NLP: named entity recognition and part-of-speech
tagging. Our results show that combination of un-
regularised expert CRFs with an unregularised stan-
dard CRF under a LOP can outperform the unreg-
ularised standard CRF, and attain a performance
level that rivals that of the regularised standard CRF.
LOP-CRFs therefore provide a viable alternative to
CRF regularisation without the need for hyperpa-
rameter search.
2 Conditional Random Fields
A linear chain CRF defines the conditional probabil-
ity of a state or label sequence s given an observed
sequence o via1:
p(s |o) = 1
Z(o) exp
(T+1
?
t=1
?
k
?k fk(st?1,st ,o, t)
)
(1)
where T is the length of both sequences, ?k are pa-
rameters of the model and Z(o) is the partition func-
tion that ensures (1) represents a probability distri-
bution. The functions fk are feature functions rep-
resenting the occurrence of different events in the
sequences s and o.
The parameters ?k can be estimated by maximis-
ing the conditional log-likelihood of a set of labelled
training sequences. The log-likelihood is given by:
L (? ) = ?
o,s
p?(o,s) log p(s |o;? )
= ?
o,s
p?(o,s)
[T+1
?
t=1
? ? f(s,o, t)
]
? ?
o
p?(o) logZ(o;? )
where p?(o,s) and p?(o) are empirical distributions
defined by the training set. At the maximum like-
lihood solution the model satisfies a set of feature
constraints, whereby the expected count of each fea-
ture under the model is equal to its empirical count
on the training data:
1In this paper we assume there is a one-to-one mapping be-
tween states and labels, though this need not be the case.
Ep?(o,s)[ fk]?Ep(s|o)[ fk] = 0, ?k
In general this cannot be solved for the ?k in
closed form so numerical routines must be used.
Malouf (2002) and Sha and Pereira (2003) show
that gradient-based algorithms, particularly limited
memory variable metric (LMVM), require much
less time to reach convergence, for some NLP tasks,
than the iterative scaling methods (Della Pietra et
al., 1997) previously used for log-linear optimisa-
tion problems. In all our experiments we use the
LMVM method to train the CRFs.
For CRFs with general graphical structure, calcu-
lation of Ep(s|o)[ fk] is intractable, but for the linear
chain case Lafferty et al (2001) describe an efficient
dynamic programming procedure for inference, sim-
ilar in nature to the forward-backward algorithm in
hidden Markov models.
3 Logarithmic Opinion Pools
In this paper an expert model refers a probabilistic
model that focuses on modelling a specific subset of
some probability distribution. The concept of com-
bining the distributions of a set of expert models via
a weighted product has previously been used in a
range of different application areas, including eco-
nomics and management science (Bordley, 1982),
and NLP (Osborne and Baldridge, 2004).
In this paper we restrict ourselves to sequence
models. Given a set of sequence model experts, in-
dexed by ? , with conditional distributions p?(s |o)
and a set of non-negative normalised weights w? , a
logarithmic opinion pool 2 is defined as the distri-
bution:
pLOP(s |o) = 1ZLOP(o) ?? [p?(s |o)]
w? (2)
with w? ? 0 and ?? w? = 1, and where ZLOP(o) is
the normalisation constant:
ZLOP(o) = ?
s
?? [p?(s |o)]
w? (3)
2Hinton (1999) introduced a variant of the LOP idea called
Product of Experts, in which expert distributions are multiplied
under a uniform weight distribution.
19
The weight w? encodes our confidence in the opin-
ion of expert ? .
Suppose that there is a ?true? conditional distri-
bution q(s | o) which each p?(s | o) is attempting to
model. Heskes (1998) shows that the KL divergence
between q(s | o) and the LOP, can be decomposed
into two terms:
K(q, pLOP) = E ?A (4)
= ?
?
w?K (q, p?)??
?
w?K (pLOP, p?)
This tells us that the closeness of the LOP model
to q(s | o) is governed by a trade-off between two
terms: an E term, which represents the closeness
of the individual experts to q(s | o), and an A term,
which represents the closeness of the individual
experts to the LOP, and therefore indirectly to each
other. Hence for the LOP to model q well, we desire
models p? which are individually good models of q
(having low E) and are also diverse (having large A).
3.1 LOPs for CRFs
Because CRFs are log-linear models, we can see
from equation (2) that CRF experts are particularly
well suited to combination under a LOP. Indeed, the
resulting LOP is itself a CRF, the LOP-CRF, with
potential functions given by a log-linear combina-
tion of the potential functions of the experts, with
weights w? . As a consequence of this, the nor-
malisation constant for the LOP-CRF can be calcu-
lated efficiently via the usual forward-backward al-
gorithm for CRFs. Note that there is a distinction be-
tween normalisation constant for the LOP-CRF, ZLOP
as given in equation (3), and the partition function of
the LOP-CRF, Z. The two are related as follows:
pLOP(s |o) = 1ZLOP(o) ?? [p?(s |o)]
w?
= 1
ZLOP(o) ??
[
U?(s |o)
Z?(o)
]w?
= ?? [U?(s |o)]
w?
ZLOP(o)?? [Z?(o)]w?
where U? = exp?T+1t=1 ?k ??k f?k(st?1,st ,o, t) and so
logZ(o) = logZLOP(o)+?
?
w? logZ?(o)
This relationship will be useful below, when we de-
scribe how to train the weights w? of a LOP-CRF.
In this paper we will use the term LOP-CRF
weights to refer to the weights w? in the weighted
product of the LOP-CRF distribution and the term
parameters to refer to the parameters ??k of each
expert CRF ? .
3.2 Training LOP-CRFs
In our LOP-CRF training procedure we first train
the expert CRFs unregularised on the training data.
Then, treating the experts as static pre-trained mod-
els, we train the LOP-CRF weights w? to maximise
the log-likelihood of the training data. This training
process is ?parameter-free? in that neither stage in-
volves the use of a prior distribution over expert CRF
parameters or LOP-CRF weights, and so avoids the
requirement to adjust hyperparameter values.
The likelihood of a data set under a LOP-CRF, as
a function of the LOP-CRF weights, is given by:
L(w) = ?
o,s
pLOP(s |o;w) p?(o,s)
= ?
o,s
[ 1
ZLOP(o;w) ?? p?(s |o)
w?
]p?(o,s)
After taking logs and rearranging, the log-
likelihood can be expressed as:
L (w) = ?
o,s
p?(o,s)?
?
w? log p?(s |o)
? ?
o
p?(o) logZLOP(o;w)
= ?
?
w? ?
o,s
p?(o,s) log p?(s |o)
+ ?
?
w? ?
o
p?(o) logZ?(o)
? ?
o
p?(o) logZ(o;w)
For the first two terms, the quantities that are mul-
tiplied by w? inside the (outer) sums are indepen-
dent of the weights, and can be evaluated once at the
20
beginning of training. The third term involves the
partition function for the LOP-CRF and so is a func-
tion of the weights. It can be evaluated efficiently as
usual for a standard CRF.
Taking derivatives with respect to w? and rear-
ranging, we obtain:
?L (w)
?w? = ?o,s p?(o,s) log p? (s |o)
+ ?
o
p?(o) logZ? (o)
? ?
o
p?(o)EpLOP(s|o)
[
?
t
logU? t(o,s)
]
where U? t(o,s) is the value of the potential function
for expert ? on clique t under the labelling s for ob-
servation o. In a way similar to the representation
of the expected feature count in a standard CRF, the
third term may be re-written as:
??
o
?
t
?
s?,s??
pLOP(st?1 = s?,st = s??,o) logU? t(s?,s??,o)
Hence the derivative is tractable because we can use
dynamic programming to efficiently calculate the
pairwise marginal distribution for the LOP-CRF.
Using these expressions we can efficiently train
the LOP-CRF weights to maximise the log-
likelihood of the data set.3 We make use of the
LMVM method mentioned earlier to do this. We
will refer to a LOP-CRF with weights trained using
this procedure as an unregularised LOP-CRF.
3.2.1 Regularisation
The ?parameter-free? aspect of the training pro-
cedure we introduced in the previous section relies
on the fact that we do not use regularisation when
training the LOP-CRF weights w? . However, there
is a possibility that this may lead to overfitting of
the training data. In order to investigate this, we
develop a regularised version of the training proce-
dure and compare the results obtained with each. We
3We must ensure that the weights are non-negative and nor-
malised. We achieve this by parameterising the weights as func-
tions of a set of unconstrained variables via a softmax transfor-
mation. The values of the log-likelihood and its derivatives with
respect to the unconstrained variables can be derived from the
corresponding values for the weights w? .
use a prior distribution over the LOP-CRF weights.
As the weights are non-negative and normalised we
use a Dirichlet distribution, whose density function
is given by:
p(w) = ?(?? ??)?? ?(??) ?? w
???1?
where the ?? are hyperparameters.
Under this distribution, ignoring terms that are
independent of the weights, the regularised log-
likelihood involves an additional term:
?
?
(?? ?1) logw?
We assume a single value ? across all weights. The
derivative of the regularised log-likelihood with
respect to weight w? then involves an additional
term 1w? (? ? 1). In our experiments we use thedevelopment set to optimise the value of ? . We will
refer to a LOP-CRF with weights trained using this
procedure as a regularised LOP-CRF.
4 The Tasks
In this paper we apply LOP-CRFs to two sequence
labelling tasks in NLP: named entity recognition
(NER) and part-of-speech tagging (POS tagging).
4.1 Named Entity Recognition
NER involves the identification of the location and
type of pre-defined entities within a sentence and is
often used as a sub-process in information extrac-
tion systems. With NER the CRF is presented with
a set of sentences and must label each word so as to
indicate whether the word appears outside an entity
(O), at the beginning of an entity of type X (B-X) or
within the continuation of an entity of type X (I-X).
All our results for NER are reported on the
CoNLL-2003 shared task dataset (Tjong Kim Sang
and De Meulder, 2003). For this dataset the en-
tity types are: persons (PER), locations (LOC),
organisations (ORG) and miscellaneous (MISC).
The training set consists of 14,987 sentences and
204,567 tokens, the development set consists of
3,466 sentences and 51,578 tokens and the test set
consists of 3,684 sentences and 46,666 tokens.
21
4.2 Part-of-Speech Tagging
POS tagging involves labelling each word in a sen-
tence with its part-of-speech, for example noun,
verb, adjective, etc. For our experiments we use the
CoNLL-2000 shared task dataset (Tjong Kim Sang
and Buchholz, 2000). This has 48 different POS
tags. In order to make training time manageable4,
we collapse the number of POS tags from 48 to 5
following the procedure used in (McCallum et al,
2003). In summary:
? All types of noun collapse to category N.
? All types of verb collapse to category V.
? All types of adjective collapse to category J.
? All types of adverb collapse to category R.
? All other POS tags collapse to category O.
The training set consists of 7,300 sentences and
173,542 tokens, the development set consists of
1,636 sentences and 38,185 tokens and the test set
consists of 2,012 sentences and 47,377 tokens.
4.3 Expert sets
For each task we compare the performance of the
LOP-CRF to that of the standard CRF by defining
a single, complex CRF, which we call a monolithic
CRF, and a range of expert sets.
The monolithic CRF for NER comprises a num-
ber of word and POS tag features in a window of
five words around the current word, along with a
set of orthographic features defined on the current
word. These are based on those found in (Curran and
Clark, 2003). Examples include whether the cur-
rent word is capitalised, is an initial, contains a digit,
contains punctuation, etc. The monolithic CRF for
NER has 450,345 features.
The monolithic CRF for POS tagging comprises
word and POS features similar to those in the NER
monolithic model, but over a smaller number of or-
thographic features. The monolithic model for POS
tagging has 188,448 features.
Each of our expert sets consists of a number of
CRF experts. Usually these experts are designed to
4See (Cohn et al, 2005) for a scaling method allowing the
full POS tagging task with CRFs.
focus on modelling a particular aspect or subset of
the distribution. As we saw earlier, the aim here is
to define experts that model parts of the distribution
well while retaining mutual diversity. The experts
from a particular expert set are combined under a
LOP-CRF and the weights are trained as described
previously.
We define our range of expert sets as follows:
? Simple consists of the monolithic CRF and a
single expert comprising a reduced subset of
the features in the monolithic CRF. This re-
duced CRF models the entire distribution rather
than focusing on a particular aspect or subset,
but is much less expressive than the monolithic
model. The reduced model comprises 24,818
features for NER and 47,420 features for POS
tagging.
? Positional consists of the monolithic CRF and
a partition of the features in the monolithic
CRF into three experts, each consisting only of
features that involve events either behind, at or
ahead of the current sequence position.
? Label consists of the monolithic CRF and a
partition of the features in the monolithic CRF
into five experts, one for each label. For NER
an expert corresponding to label X consists
only of features that involve labels B-X or I-
X at the current or previous positions, while for
POS tagging an expert corresponding to label
X consists only of features that involve label
X at the current or previous positions. These
experts therefore focus on trying to model the
distribution of a particular label.
? Random consists of the monolithic CRF and a
random partition of the features in the mono-
lithic CRF into four experts. This acts as a
baseline to ascertain the performance that can
be expected from an expert set that is not de-
fined via any linguistic intuition.
5 Experiments
To compare the performance of LOP-CRFs trained
using the procedure we described previously to that
of a standard CRF regularised with a Gaussian prior,
we do the following for both NER and POS tagging:
22
? Train a monolithic CRF with regularisation us-
ing a Gaussian prior. We use the development
set to optimise the value of the variance hyper-
parameter.
? Train every expert CRF in each expert set with-
out regularisation (each expert set includes the
monolithic CRF, which clearly need only be
trained once).
? For each expert set, create a LOP-CRF from
the expert CRFs and train the weights of the
LOP-CRF without regularisation. We compare
its performance to that of the unregularised and
regularised monolithic CRFs.
? To investigate whether training the LOP-CRF
weights contributes significantly to the LOP-
CRF?s performance, for each expert set we cre-
ate a LOP-CRF with uniform weights and com-
pare its performance to that of the LOP-CRF
with trained weights.
? To investigate whether unregularised training
of the LOP-CRF weights leads to overfitting,
for each expert set we train the weights of the
LOP-CRF with regularisation using a Dirich-
let prior. We optimise the hyperparameter in
the Dirichlet distribution on the development
set. We then compare the performance of the
LOP-CRF with regularised weights to that of
the LOP-CRF with unregularised weights.
6 Results
6.1 Experts
Before presenting results for the LOP-CRFs, we
briefly give performance figures for the monolithic
CRFs and expert CRFs in isolation. For illustration,
we do this for NER models only. Table 1 shows F
scores on the development set for the NER CRFs.
We see that, as expected, the expert CRFs in iso-
lation model the data relatively poorly compared to
the monolithic CRFs. Some of the label experts, for
example, attain relatively low F scores as they focus
only on modelling one particular label. Similar be-
haviour was observed for the POS tagging models.
Expert F score
Monolithic unreg. 88.33
Monolithic reg. 89.84
Reduced 79.62
Positional 1 86.96
Positional 2 73.11
Positional 3 73.08
Label LOC 41.96
Label MISC 22.03
Label ORG 29.13
Label PER 40.49
Label O 60.44
Random 1 70.34
Random 2 67.76
Random 3 67.97
Random 4 70.17
Table 1: Development set F scores for NER experts
6.2 LOP-CRFs with unregularised weights
In this section we present results for LOP-CRFs with
unregularised weights. Table 2 gives F scores for
NER LOP-CRFs while Table 3 gives accuracies for
the POS tagging LOP-CRFs. The monolithic CRF
scores are included for comparison. Both tables il-
lustrate the following points:
? In every case the LOP-CRFs outperform the
unregularised monolithic CRF
? In most cases the performance of LOP-CRFs
rivals that of the regularised monolithic CRF,
and in some cases exceeds it.
We use McNemar?s matched-pairs test (Gillick
and Cox, 1989) on point-wise labelling errors to ex-
amine the statistical significance of these results. We
test significance at the 5% level. At this threshold,
all the LOP-CRFs significantly outperform the cor-
responding unregularised monolithic CRF. In addi-
tion, those marked with ? show a significant im-
provement over the regularised monolithic CRF.
Only the value marked with ? in Table 3 significantly
under performs the regularised monolithic. All other
values a do not differ significantly from those of the
regularised monolithic CRF at the 5% level.
These results show that LOP-CRFs with unreg-
ularised weights can lead to performance improve-
ments that equal or exceed those achieved from a
conventional regularisation approach using a Gaus-
sian prior. The important difference, however, is that
the LOP-CRF approach is ?parameter-free? in the
23
Expert set Development set Test set
Monolithic unreg. 88.33 81.87
Monolithic reg. 89.84 83.98
Simple 90.26 84.22?
Positional 90.35 84.71?
Label 89.30 83.27
Random 88.84 83.06
Table 2: F scores for NER unregularised LOP-CRFs
Expert set Development set Test set
Monolithic unreg. 97.92 97.65
Monolithic reg. 98.02 97.84
Simple 98.31? 98.12?
Positional 98.03 97.81
Label 97.99 97.77
Random 97.99 97.76?
Table 3: Accuracies for POS tagging unregularised
LOP-CRFs
sense that each expert CRF in the LOP-CRF is un-
regularised and the LOP weight training is also un-
regularised. We are therefore not required to search
a hyperparameter space. As an illustration, to ob-
tain our best results for the POS tagging regularised
monolithic model, we re-trained using 15 different
values of the Gaussian prior variance. With the
LOP-CRF we trained each expert CRF and the LOP
weights only once.
As an illustration of a typical weight distribution
resulting from the training procedure, the positional
LOP-CRF for POS tagging attaches weight 0.45 to
the monolithic model and roughly equal weights to
the other three experts.
6.3 LOP-CRFs with uniform weights
By training LOP-CRF weights using the procedure
we introduce in this paper, we allow the weights to
take on non-uniform values. This corresponds to
letting the opinion of some experts take precedence
over others in the LOP-CRF?s decision making. An
alternative, simpler, approach would be to com-
bine the experts under a LOP with uniform weights,
thereby avoiding the weight training stage. We
would like to ascertain whether this approach will
significantly reduce the LOP-CRF?s performance.
As an illustration, Table 4 gives accuracies for LOP-
CRFs with uniform weights for POS tagging. A sim-
ilar pattern is observed for NER. Comparing these
values to those in Tables 2 and 3, we can see that in
Expert set Development set Test set
Simple 98.30 98.12
Positional 97.97 97.79
Label 97.85 97.73
Random 97.82 97.74
Table 4: Accuracies for POS tagging uniform LOP-
CRFs
general LOP-CRFs with uniform weights, although
still performing significantly better than the unreg-
ularised monolithic CRF, generally under perform
LOP-CRFs with trained weights. This suggests that
the choice of weights can be important, and justifies
the weight training stage.
6.4 LOP-CRFs with regularised weights
To investigate whether unregularised training of the
LOP-CRF weights leads to overfitting, we train
the LOP-CRF with regularisation using a Dirich-
let prior. The results we obtain show that in most
cases a LOP-CRF with regularised weights achieves
an almost identical performance to that with unreg-
ularised weights, and suggests there is little to be
gained by weight regularisation. This is probably
due to the fact that in our LOP-CRFs the number
of experts, and therefore weights, is generally small
and so there is little capacity for overfitting. We con-
jecture that although other choices of expert set may
comprise many more experts than in our examples,
the numbers are likely to be relatively small in com-
parison to, for example, the number of parameters in
the individual experts. We therefore suggest that any
overfitting effect is likely to be limited.
6.5 Choice of Expert Sets
We can see from Tables 2 and 3 that the performance
of a LOP-CRF varies with the choice of expert set.
For example, in our tasks the simple and positional
expert sets perform better than those for the label
and random sets. For an explanation here, we re-
fer back to our discussion of equation (5). We con-
jecture that the simple and positional expert sets
achieve good performance in the LOP-CRF because
they consist of experts that are diverse while simulta-
neously being reasonable models of the data. The la-
bel expert set exhibits greater diversity between the
experts, because each expert focuses on modelling a
particular label only, but each expert is a relatively
24
poor model of the entire distribution and the corre-
sponding LOP-CRF performs worse. Similarly, the
random experts are in general better models of the
entire distribution but tend to be less diverse because
they do not focus on any one aspect or subset of it.
Intuitively, then, we want to devise experts that pro-
vide diverse but accurate views on the data.
The expert sets we present in this paper were
motivated by linguistic intuition, but clearly many
choices exist. It remains an important open question
as to how to automatically construct expert sets for
good performance on a given task, and we intend to
pursue this avenue in future research.
7 Conclusion and future work
In this paper we have introduced the logarithmic
opinion pool of CRFs as a way to address overfit-
ting in CRF models. Our results show that a LOP-
CRF can provide a competitive alternative to con-
ventional regularisation with a prior while avoiding
the requirement to search a hyperparameter space.
We have seen that, for a variety of types of expert,
combination of expert CRFs with an unregularised
standard CRF under a LOP with optimised weights
can outperform the unregularised standard CRF and
rival the performance of a regularised standard CRF.
We have shown how these advantages a LOP-
CRF provides have a firm theoretical foundation in
terms of the decomposition of the KL-divergence
between a LOP-CRF and a target distribution, and
how this provides a framework for designing new
overfitting reduction schemes in terms of construct-
ing diverse experts.
In this work we have considered training the
weights of a LOP-CRF using pre-trained, static ex-
perts. In future we intend to investigate cooperative
training of LOP-CRF weights and the parameters of
each expert in an expert set.
Acknowledgements
We wish to thank Stephen Clark, our colleagues in
Edinburgh and the anonymous reviewers for many
useful comments.
References
R. F. Bordley. 1982. A multiplicative formula for aggregating
probability assessments. Management Science, (28):1137?
1148.
T. Cohn, A. Smith, and M. Osborne. 2005. Scaling conditional
random fields using error-correcting codes. In Proc. ACL
2005.
J. Curran and S. Clark. 2003. Language independent NER
using a maximum entropy tagger. In Proc. CoNLL-2003.
S. Della Pietra, Della Pietra V., and J. Lafferty. 1997. Induc-
ing features of random fields. In IEEE PAMI, volume 19(4),
pages 380?393.
L. Gillick and S. Cox. 1989. Some statistical issues in the
comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, volume 1, pages 532?535.
T. Heskes. 1998. Selecting weighting factors in logarithmic
opinion pools. In Advances in Neural Information Process-
ing Systems 10.
G. E. Hinton. 1999. Product of experts. In ICANN, volume 1,
pages 1?6.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML 2001.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. CoNLL-2002.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proc. CoNLL-2003.
A. McCallum, K. Rohanimanesh, and C. Sutton. 2003. Dy-
namic conditional random fields for jointly labeling multiple
sequences. In NIPS-2003 Workshop on Syntax, Semantics
and Statistics.
A. McCallum. 2003. Efficiently inducing features of condi-
tional random fields. In Proc. UAI 2003.
M. Osborne and J. Baldridge. 2004. Ensemble-based active
learning for parse selection. In Proc. NAACL 2004.
F. Peng and A. McCallum. 2004. Accurate information extrac-
tion from research papers using conditional random fields.
In Proc. HLT-NAACL 2004.
Y. Qi, M. Szummer, and T. P. Minka. 2005. Bayesian condi-
tional random fields. In Proc. AISTATS 2005.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL 2003.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proc. CoNLL-
2000.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proc. CoNLL-2003.
25
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 969?976,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modelling lexical redundancy for machine translation
David Talbot and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract
Certain distinctions made in the lexicon
of one language may be redundant when
translating into another language. We
quantify redundancy among source types
by the similarity of their distributions over
target types. We propose a language-
independent framework for minimising
lexical redundancy that can be optimised
directly from parallel text. Optimisation
of the source lexicon for a given target lan-
guage is viewed as model selection over a
set of cluster-based translation models.
Redundant distinctions between types may
exhibit monolingual regularities, for ex-
ample, inflexion patterns. We define a
prior over model structure using a Markov
random field and learn features over sets
of monolingual types that are predictive
of bilingual redundancy. The prior makes
model selection more robust without the
need for language-specific assumptions re-
garding redundancy. Using these mod-
els in a phrase-based SMT system, we
show significant improvements in transla-
tion quality for certain language pairs.
1 Introduction
Data-driven machine translation (MT) relies on
models that can be efficiently estimated from par-
allel text. Token-level independence assumptions
based on word-alignments can be used to decom-
pose parallel corpora into manageable units for pa-
rameter estimation. However, if training data is
scarce or language pairs encode significantly dif-
ferent information in the lexicon, such as Czech
and English, additional independence assumptions
may assist the model estimation process.
Standard statistical translation models use sep-
arate parameters for each pair of source and target
types. In these models, distinctions in either lex-
icon that are redundant to the translation process
will result in unwarranted model complexity and
make parameter estimation from limited parallel
data more difficult. A natural way to eliminate
such lexical redundancy is to group types into ho-
mogeneous clusters that do not differ significantly
in their distributions over types in the other lan-
guage. Cluster-based translation models capture
the corresponding independence assumptions.
Previous work on bilingual clustering has fo-
cused on coarse partitions of the lexicon that
resemble automatically induced part-of-speech
classes. These were used to model generic
word-alignment patterns such as noun-adjective
re-ordering between English and French (Och,
1998). In contrast, we induce fine-grained parti-
tions of the lexicon, conceptually closer to auto-
matic lemmatisation, optimised specifically to as-
sign translation probabilities. Unlike lemmatisa-
tion or stemming, our method specifically quanti-
fies lexical redundancy in a bilingual setting and
does not make language-specific assumptions.
We tackle the problem of redundancy in the
translation lexicon via Bayesian model selection
over a set of cluster-based translation models. We
search for the model, defined by a clustering of
the source lexicon, that maximises the marginal
likelihood of target tokens in parallel data. In this
optimisation, source types are combined into clus-
ters if their distributions over target types are too
similar to warrant distinct parameters.
Redundant distinctions between types may ex-
hibit regularities within a language, for instance,
inflexion patterns. These can be used to guide
model selection. Here we show that the inclusion
of a model ?prior? over the lexicon structure leads
to more robust translation models. Although a pri-
ori we do not know which monolingual features
characterise redundancy for a given language pair,
by defining a model over the prior monolingual
969
space of source types and cluster assignments, we
can introduce an inductive bias that allows cluster-
ing decisions in different parts of the lexicon to in-
fluence one another via monolingual features. We
use an EM-type algorithm to learn weights for a
Markov random field parameterisation of this prior
over lexicon structure.
We obtain significant improvements in transla-
tion quality as measured by BLEU, incorporating
these optimised model within a phrase-based SMT
system for three different language pairs. The
MRF prior improves the results and picks up fea-
tures that appear to agree with linguistic intuitions
of redundancy for the language pairs considered.
2 Lexical redundancy between languages
In statistical MT, the source and target lexicons
are usually defined as the sets of distinct types ob-
served in the parallel training corpus for each lan-
guage. Such models may not be optimal for cer-
tain language pairs and training regimes.
A word-level statistical translation model ap-
proximates the probability Pr(E|F ) that a source
type indexed by F will be translated as a target
type indexed by E. Standard models, e.g. Brown
et al (1993), consist of discrete probability distri-
butions with separate parameters for each unique
pairing of a source and target types; no attempt is
made to leverage structure within the event spaces
E and F during parameter estimation. This results
in a large number of parameters that must be esti-
mated from limited amounts of parallel corpora.
We refer to distinctions made between lexical
types in one language that do not result in different
distributions over types in the other language as
lexically redundant for the language pair. Since
the role of the translation model is to determine a
distribution over target types given a source type,
when the corresponding target distributions do not
vary significantly over a set of source types, the
model gains nothing by maintaining a distinct set
of parameters for each member of this set.
Lexical redundancy may arise when languages
differ in the specificity with which they refer to the
same concepts. For instance, colours of the spec-
trum may be partitioned differently (e.g. blue in
English v.s. sinii and goluboi in Russian). It will
also arise when languages explicitly encode differ-
ent information in the lexicon. For example, trans-
lating from French to English, a standard model
would treat the following pairs of source and tar-
get types as distinct events with entirely unre-
lated parameters: (vert, green), (verte, green),
(verts, green) and (vertes, green). Here the
French types differ only in their final suffixes due
to adjectival agreement. Since there is no equiva-
lent mechanism in English, these distinctions are
redundant with respect to this target language.
Distinctions that are redundant in the source
lexicon when translating into one language may,
however, be significant when translating into an-
other. For instance, the French adjectival number
agreement (the addition of an s) may be significant
when translating to Russian which also marks ad-
jectives for number (the inflexion to -ye).
We can remove redundancy from the translation
model by conflating redundant types, e.g. vert .=
{vert, verte, verts, vertes}, and averaging bilin-
gual statistics associated with these events.
3 Eliminating redundancy in the model
Redundancy in the translation model can be
viewed as unwarranted model complexity. A
cluster-based translation model defined via a hard-
clustering of the lexicon can reduce this com-
plexity by introducing additional independence as-
sumptions: given the source cluster label, cj , the
target type, ei, is assumed to be independent of the
exact source type, fj , observed, i.e., p(ei|fj) ?
p(ei|cj). Optimising the model for lexical redun-
dancy can be viewed as model selection over a set
of such cluster-based translation models.
We formulate model search as a maximum a
posteriori optimisation: the data-dependent term,
p(D|C), quantifies evidence provided for a model,
C, by bilingual training data, D, while the prior,
p(C), can assert a preference for a particular
model structure (clustering of the source lexicon)
on the basis of monolingual features. Both terms
have parameters that are estimated from data. For-
mally, we search for C?,
C? = argmaxC p(C|D)
= argmaxC p(C)p(D|C). (1)
Evaluating the data-dependent term, p(D|C), for
different partitions of the source lexicon, we can
compare how well different models predict the tar-
get tokens aligned in a parallel corpus. This term
will prefer models that group together source types
with similar distributions over target types. By
using the marginal likelihood (integrating out the
parameters of the translation model) to calculate
970
p(D|C), we can account explicitly for the com-
plexity of the translation model and compare mod-
els with different numbers of clusters as well as
different assignments of types to clusters.
In addition to an implicit uniform prior over
cluster labels as in k-means clustering (e.g. Chou
(1991)), we also consider a Markov random field
(MRF) parameterisation of the p(C) term to cap-
ture monolingual regularities in the lexicon. The
MRF induces dependencies between clustering
decisions in different parts of the lexicon via a
monolingual feature space biasing the search to-
wards models that exhibit monolingual regulari-
ties. Rather than assuming a priori knowledge of
redundant distinctions in the source language, we
use an EM algorithm to update parameters for fea-
tures defined over sets of source types on the basis
of existing cluster assignments. While initially the
model search will be guided only by information
from the bilingual statistics in p(D|C), monolin-
gual regularities in the lexicon, such as inflexion
patterns, may gradually be propagated through the
model as p(C) becomes informative. Our exper-
iments suggest that the MRF prior enables more
robust model selection.
As stated, the model selection procedure ac-
counts for redundancy in the source lexicon us-
ing the target distributions. The target lexicon
can be optimised analogously. Clustering target
types allows the implementation of independence
assumptions asserting that the exact specification
of a target type is independent of the source type
given knowledge of the target cluster label. For ex-
ample, when translating an English adjective into
French it may be more efficient to use the trans-
lation model to specify only that the translation
lies within a certain set of French adjectives, corre-
sponding to a single lemma, and have the language
model select the exact form. Our experiments sug-
gest that it can be useful to account for redundancy
in both languages in this way; this can be incorpo-
rated simply within our optimisation procedure.
In Section 3.1 we describe the bilingual
marginal likelihood, p(D|C), clustering proce-
dure; in Section 3.2 we introduce the MRF param-
eterisation of the prior, p(C), over model struc-
ture; and in Section 3.3, we describe algorithmic
approximations.
3.1 Bilingual model selection
Assume we are optimising the source lexicon (the
target lexicon is optimised analogously). A clus-
tering of the lexicon is a unique mapping CF :
F ? CF defined for all f ? F where, in addition
to all source types observed in the parallel training
corpus, F may include items seen in other mono-
lingual corpora (and, in the case of the source lex-
icon only, the development and test data). The
standard SMT lexicon can be viewed as a cluster-
ing with each type observed in the parallel training
corpus assigned to a distinct cluster and all other
types assigned to a single ?unknown word? cluster.
We optimise a conditional model of target to-
kens from word-aligned parallel corpora, D =
{Dc0 , ..., DcN }, where Dci represents the set of
target words that were aligned to the set of source
types in cluster ci. We assume that each target to-
ken in the corpus is generated conditionally i.i.d.
given the cluster label of the source type to which
it is aligned. Sufficient statistics for this model
consist of co-occurrence counts of source and tar-
get types summed across each source cluster,
#cf (e)
.=
?
f ??cf
#(e, f ?). (2)
Maximising the likelihood of the data under this
model would require us to specify the number of
clusters (the size of the lexicon) in advance. In-
stead we place a Dirichlet prior parameterised by
?1 over the translation model parameters of each
cluster, ?cf ,e, defining the conditional distribu-
tions over target types. Given a clustering, the
Dirichlet prior, and independent parameters, the
distribution over data and parameters factorises,
p(D,?|CF , ?) =
?
cf?CF
p(Dcf , ?cf |cf , ?)
?
?
cf?CF
?
e?E
?
??1+#cf (e)
cf ,e
We optimise cluster assignments with respect to
the marginal likelihood which averages the like-
lihood of the set of counts assigned to a cluster,
Dcf , under the current model over the prior,
p(Dcf |?, cf ) =
?
p(?cf |?)p(Dcf |?cf , cf )d?cf .
This can be evaluated analytically for a Dirichlet
prior with multinomial parameters.
Assuming a (fixed) uniform prior over model
structure, p(C), model selection involves itera-
tively re-assigning source types to clusters such
as to maximise the marginal likelihood. Re-
assignments may alter the total number of clusters
1Distinct from the prior over model structure, p(C).
971
at any point. Updates can be calculated locally, for
instance, given the sets of target tokens Dci and
Dcj aligned to source types currently in clusters
ci and cj , the change in log marginal likelihood if
clusters ci and cj are merged into cluster c? is,
?ci,cj?c? = log
p(Dc?|?, c?)
p(Dci |?, ci)p(Dcj |?, cj)
, (3)
which is a Bayes factor in favour of the hypothe-
sis that Dci and Dcj were sampled from the same
distribution (Wolpert, 1995). Unlike its equivalent
in maximum likelihood clustering, Eq.(3) may as-
sume positive values favouring a smaller number
of clusters when the data does not support a more
complex hypothesis. The more complex model,
with ci and cj modelled separately, is penalised
for being able to model a wider range of data sets.
The hyperparameter, ?, is tied across clusters
and taken to be proportional to the marginal (the
?background?) distribution over target types in the
corpus. Under this prior, source types aligned to
the same target types, will be clustered together
more readily if these target types are less frequent
in the corpus as a whole.
3.2 Markov random field model prior
As described above we consider a Markov random
field (MRF) parameterisation of the prior over
model structure, p(C). This defines a distribution
over cluster assignments of the source lexicon as a
whole based solely on monolingual characteristics
of the lexical types and the relations between their
respective cluster assignments.
Viewed as graph, each variable in the MRF is
modelled as conditionally independent of all other
variables given the values of its neighbours (the
Markov property; (Geman and Geman, 1984)).
Each variable in the MRF prior corresponds to a
lexical source type and its cluster assignment. Fig.
1 shows a section of the complete model including
the MRF prior for a Welsh source lexicon; shad-
ing denotes cluster assignments and English tar-
get tokens are shown as directed nodes.2 From the
Markov property it follows that this prior decom-
poses over neighbourhoods,
pMRF(C)? e
?
?
f?F
?
f ??Nf
?
i
?i?i(f,f ?,cf ,c?f )
Here Nf is the set of neighbours of source type f ;
i indexes a set of functions ?i(?) that pick out fea-
tures of a clique; each function has a parameter ?i
2The plates represent repeated sampling; each Welsh
source type may be aligned to multiple English tokens.
Figure 1: Model with Markov random field prior
#(f)
#(f)
#(f) #(f)
car
car
#(f)
wales
wales
car
gar cymru
gymru
bar
mar
that we learn from the data; these are tied across
the graph. ? is a free parameter used to control the
overall contribution of the prior in Eq. (1). Here
features are defined over pairs of types but higher-
order interactions can also be modelled. We only
consider ?positive? prior knowledge that is indica-
tive of redundancy among source types. Hence all
features are non-zero only when their arguments
are assigned to the same cluster.
Features can be defined over any aspects of the
lexicon; in our experiments we use binary features
over constrained string edits between types. The
following feature would be 1, for instance, if the
Welsh types cymru and gymru (see Fig. 1), were
assigned to the same cluster.3
?1(fi = (c ?) ? fj = (g ?) ? ci = cj)
Setting the parameters of the MRF prior over
this feature space by hand would require a priori
knowledge of redundancies for the language pair.
In the absence of such knowledge, we use an it-
erative EM algorithm to update the parameters on
the basis of the previous solution to the bilingual
clustering procedure. EM parameter estimation
forces the cluster assignments of the MRF prior to
agree with those obtained on the basis of bilingual
data using monolingual features alone. Since fea-
tures are tied across the MRF, patterns that char-
acterise redundant relations between types will be
re-enforced across the model. For instance (see
Fig. 1), if cymru and gymru are clustered to-
gether, the parameter for feature ?1, shown above,
may increase. This induces a prior preference for
car and gar to form a cluster on subsequent it-
erations. A similar feature defined for mar and
gar in the a priori string edit feature space, on
the other hand, may remain uninformative if not
observed frequently on pairs of types assigned to
the same clusters. In this way, the model learns to
3Here? matches a common substring of both arguments.
972
generalise language-specific redundancy patterns
from a large a priori feature space. Changes in the
prior due to re-assignments can be calculated lo-
cally and combined with the marginal likelihood.
3.3 Algorithmic approximations
The model selection procedure is an EM algo-
rithm. Each source type is initially assigned to
its own cluster and the MRF parameters, ?i, are
initialised to zero. A greedy E-step iteratively re-
assigns each source type to the cluster that max-
imises Eq. (1); cluster statistics are updated af-
ter any re-assignment. To reduce computation, we
only consider re-assignments that would cause at
least one (non-zero) feature in the MRF to fire, or
to clusters containing types sharing target word-
alignments with the current type; types may also
be re-assigned to a cluster of their own at any iter-
ation. When clustering both languages simultane-
ously, we average ?target? statistics over the num-
ber of events in each ?target? cluster in Eq. (2).
We re-estimate the MRF parameters after each
pass through the vocabulary. These are updated
according to MLE using a pseudolikelihood ap-
proximation (Besag, 1986). Since MRF parame-
ters can only be non-zero for features observed on
types clustered together during an E-step, we use
lazy instantiation to work with a large implicit fea-
ture set defined by a constrained string edit.
The algorithm has two free parameters: ? deter-
mining the strength of the Dirichlet prior used in
the marginal likelihood, p(D|C), and ? which de-
termines the contribution of pMRF(C) to Eq. (1).
4 Experiments
Phrase-based SMT systems have been shown to
outperform word-based approaches (Koehn et al,
2003). We evaluate the effects of lexicon model
selection on translation quality by considering two
applications within a phrase-based SMT system.
4.1 Applications to phrase-based SMT
A phrase-based translation model can be estimated
in two stages: first a parallel corpus is aligned at
the word-level and then phrase pairs are extracted
(Koehn et al, 2003). Aligning tokens in paral-
lel sentences using the IBM Models (Brown et
al., 1993), (Och and Ney, 2003) may require less
information than full-blown translation since the
task is constrained by the source and target tokens
present in each sentence pair. In the phrase-level
translation table, however, the model must assign
Source Tokens Types Singletons Test OOV
Czech 468K 54K 29K 6K 469
French 5682K 53K 19K 16K 112
Welsh 4578K 46K 18K 15K 64
Table 1: Parallel corpora used in the experiments.
probabilities to a potentially unconstrained set of
target phrases. We anticipate the optimal model
sizes to be different for these two tasks.
We can incorporate an optimised lexicon at the
word-alignment stage by mapping tokens in the
training corpus to their cluster labels. The map-
ping will not change the number of tokens in a
sentence, hence the word-alignments can be asso-
ciated with the original corpus (see Exp. 1).
To extrapolate a mapping over phrases from our
type-level models we can map each type within
a phrase to its corresponding cluster label. This,
however, results in a large number of distinct
phrases being collapsed down to a single ?clus-
tered phrase?. Using these directly may spread
probability mass too widely. Instead we use
them to smooth the phrase translation model (see
Exp. 2). Here we consider a simple interpolation
scheme; they could also be used within a backoff
model (Yang and Kirchhoff, 2006).
4.2 Experimental set-up
The system we use is described in (Koehn,
2004). The phrase-based translation model in-
cludes phrase-level and lexical weightings in both
directions. We use the decoder?s default behaviour
for unknown words copying them verbatim to the
output. Smoothed trigram language models are es-
timated on training sections of the parallel corpus.
We used the parallel sections of the Prague
Treebank (Cmejrek et al, 2004), French and En-
glish sections of the Europarl corpus (Koehn,
2005) and parallel text from the Welsh Assem-
bly4 (see Table1). The source languages, Czech,
French and Welsh, were chosen on the basis that
they may exhibit different degrees of redundancy
with respect to English and that they differ mor-
phologically. Only the Czech corpus has explicit
morphological annotation.
4.3 Models
All models used in the experiments are defined as
mappings of the source and target vocabularies.
The target vocabulary includes all distinct types
4This Welsh-English parallel text is in the public domain.
Contact the first author for details.
973
seen in the training corpus; the source vocabu-
lary also includes types seen only in development
and test data. Free parameters were set to max-
imize our evaluation metric, BLEU, on develop-
ment data. The results are reported on the test sets
(see Table 1). The baseline mappings used were:
? standard: the identity mapping;
? max-pref : a prefix of no more than n letters;
? min-freq: a prefix with a frequency of at least
n in the parallel training corpus.
? lemmatize: morphological lemmas (Czech)
standard corresponds to the standard SMT lexi-
con. max-pref and min-freq are both simple stem-
ming algorithms that can be applied to raw text.
These mappings result in models defined over
fewer distinct events that will have higher frequen-
cies; min-freq optimises the latter directly. We
optimise over (possibly different) values of n for
source and target languages. The lemmatize map-
ping which maps types to their lemmas was only
applicable to the Czech corpus.
The optimised lexicon models define mappings
directly via their clusterings of the vocabulary. We
consider the following four models:
? src: clustered source lexicon;
? src+mrf : as src with MRF prior;
? src+trg: clustered source and target lexicons;
? src+trg+mrf : as src+trg with MRF priors.
In each case we optimise over ? (a single value for
both languages) and, when using the MRF prior,
over ? (a single value for both languages).
4.4 Experiments
The two sets of experiments evaluate the base-
line models and optimised lexicon models dur-
ing word-alignment and phrase-level translation
model estimation respectively.
? Exp. 1: map the parallel corpus, perform
word-alignment; estimate the phrase transla-
tion model using the original corpus.
? Exp. 2: smooth the phrase translation model,
p(e|f) =
#(e, f) + ?#(ce, cf )
#(f) + ?#(cf )
Here e, f and ce, cf are phrases mapped un-
der the standard model and the model be-
ing tested respectively; ? is set once for all
experiments on development data. Word-
alignments were generated using the optimal
max-pref mapping for each training set.
5 Results
Table 2 shows the changes in BLEU when we in-
corporate the lexicon mappings during the word-
alignment process. The standard SMT lexicon
model is not optimal, as measured by BLEU, for
any of the languages or training set sizes consid-
ered. Increases over this baseline, however, di-
minish with more training data. For both Czech
and Welsh, the explicit model selection procedure
that we have proposed results in better translations
than all of the baseline models when the MRF
prior is used; again these increases diminish with
larger training sets. We note that the stemming
baseline models appear to be more effective for
Czech than for Welsh. The impact of the MRF
prior is also greater for smaller training sets.
Table 3 shows the results of using these models
to smooth the phrase translation table.5 With the
exception of Czech, the improvements are smaller
than for Exp 1. For all source languages and mod-
els we found that it was optimal to leave the tar-
get lexicon unmapped when smoothing the phrase
translation model.
Using lemmatize for word-alignment on the
Czech corpus gave BLEU scores of 32.71 and
37.21 for the 10K and 21K training sets respec-
tively; used to smooth the phrase translation model
it gave scores of 33.96 and 37.18.
5.1 Discussion
Model selection had the largest impact for smaller
data sets suggesting that the complexity of the
standard model is most excessive in sparse data
conditions. The larger improvements seen for
Czech and Welsh suggest that these languages en-
code more redundant information in the lexicon
with respect to English. Potential sources could be
grammatical case markings (Czech) and mutation
patterns (Welsh). The impact of the MRF prior for
smaller data sets suggests it overcomes sparsity in
the bilingual statistics during model selection.
The location of redundancies, in the form of
case markings, at the ends of words in Czech as
assumed by the stemming algorithms may explain
why these performed better on this language than
5The standard model in Exp. 2 is equivalent to the opti-
mised max-pref in Exp. 1.
974
Table 2: BLEU scores with optimised lexicon applied during word-alignment (Exp. 1)
Czech-English French-English Welsh-English
Model 10K sent. 21K 10K 25K 100K 250K 10K 25K 100K 250K
standard 32.31 36.17 20.76 23.17 26.61 27.63 35.45 39.92 45.02 46.47
max-pref 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11
min-freq 33.95 36.98 21.22 23.77 26.74 27.98 36.23 40.65 45.38 46.35
src 33.95 37.27 21.43 24.42 26.99 27.82 36.98 40.98 45.81 46.45
src+mrf 33.97 37.89 21.63 24.38 26.74 28.39 37.36 41.13 46.50 46.56
src+trg 34.24 38.28 22.05 24.02 26.53 27.80 36.83 41.31 45.22 46.51
src+trg+mrf 34.70 38.44 22.33 23.95 26.69 27.75 37.56 42.19 45.18 46.48
Table 3: BLEU scores with optimised lexicon used to smooth phrase-based translation model (Exp. 2)
Czech-English French-English Welsh-English
Model 10K sent. 21K 10K 25K 100K 250K 10K 25K 100K 250K
(standard)5 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11
max-pref 35.63 38.81 22.49 24.10 26.99 28.26 37.31 40.09 45.57 46.41
min-freq 34.65 37.75 21.14 23.41 26.29 27.47 36.40 40.84 45.75 46.45
src 34.38 37.98 21.28 24.17 26.88 28.35 36.94 39.99 45.75 46.65
src+mrf 36.24 39.70 22.02 24.10 26.82 28.09 37.81 41.04 46.16 46.51
Table 4: System output (Welsh 25K; Exp. 2)
Src ehangu o ffilm i deledu.
Ref an expansion from film into television.
standard expansion of footage to deledu.
max-pref expansion of ffilm to television.
src+mrf expansion of film to television.
Src yw gwarchod cymru fel gwlad brydferth
Ref safeguarding wales as a picturesque country
standard protection of wales as a country brydferth
max-pref protection of wales as a country brydferth
src+mrf protecting wales as a beautiful country
Src cynhyrchu canlyniadau llai na pherffaith
Ref produces results that are less than perfect
standard produce results less than pherffaith
max-pref produce results less than pherffaith
src+mrf generates less than perfect results
Src y dynodiad o graidd y broblem
Ref the identification of the nub of the problem
standard the dynodiad of the heart of the problem
max-pref the dynodiad of the heart of the problem
src+mrf the identified crux of the problem
on Welsh. The highest scoring features in the
MRF (see Table 5) show that Welsh redundancies,
on the other hand, are primarily between initial
characters. Inspection of system output confirms
that OOV types could be mapped to known Welsh
words with the MRF prior but not via stemming
(see Table 4). For each language pair the MRF
learned features that capture intuitively redundant
patterns: adjectival endings for French, case mark-
ings for Czech, and mutation patterns for Welsh.
The greater improvements in Exp. 1 were mir-
rored by higher compression rates for these lex-
icons (see Table. 6) supporting the conjecture
that word-alignment requires less information than
full-blown translation. The results of the lemma-
Table 5: Features learned by MRF prior
Czech French Welsh
(?,? m) (?,? s) (c ?, g ?)
(?,? u) (?,? e) (d ?, dd ?)
(?,? a) (?,? es) (d ?, t ?)
(?,? ch) (? e,? es) (b ?, p ?)
(?,? ho) (? e,? er) (c ?, ch ?)
(? a,? u) (? e,? ent) (b ?, f ?)
Note: Features defined over pairs of source types assigned to
the same cluster; here ? matches a common substring.
Table 6: Optimal lexicon size (ratio of raw vocab.)
Czech French Welsh
Word-alignment 0.26 0.22 0.24
TM smoothing 0.28 0.38 0.51
tizemodel on Czech show the model selection pro-
cedure improving on a simple supervised baseline.
6 Related Work
Previous work on automatic bilingual word clus-
tering has been motivated somewhat differently
and not made use of cluster-based models to as-
sign translation probabilities directly (Wang et
al., 1996), (Och, 1998). There is, however, a
large body of work using morphological analy-
sis to define cluster-based translation models sim-
ilar to ours but in a supervised manner (Zens and
Ney, 2004), (Niessen and Ney, 2004). These
approaches have used morphological annotation
(e.g. lemmas and part of speech tags) to pro-
vide explicit supervision. They have also involved
manually specifying which morphological distinc-
975
tions are redundant (Goldwater and McClosky,
2005). In contrast, we attempt to learn both equiv-
alence classes and redundant relations automat-
ically. Our experiments with orthographic fea-
tures suggest that some morphological redundan-
cies can be acquired in an unsupervised fashion.
The marginal likelihood hard-clustering algo-
rithm that we propose here for translation model
selection can be viewed as a Bayesian k-means al-
gorithm and is an application of Bayesian model
selection techniques, e.g., (Wolpert, 1995). The
Markov random field prior over model structure
extends the fixed uniform prior over clusters im-
plicit in k-means clustering and is common in
computer vision (Geman and Geman, 1984). Re-
cently Basu et al (2004) used an MRF to embody
hard constraints within semi-supervised cluster-
ing. In contrast, we use an iterative EM algo-
rithm to learn soft constraints within the ?prior?
monolingual space based on the results of cluster-
ing with bilingual statistics.
7 Conclusions and Future Work
We proposed a framework for modelling lexical
redundancy in machine translation and tackled op-
timisation of the lexicon via Bayesian model se-
lection over a set of cluster-based translation mod-
els. We showed improvements in translation qual-
ity incorporating these models within a phrase-
based SMT sytem. Additional gains resulted from
the inclusion of an MRF prior over model struc-
ture. We demonstrated that this prior could be
used to learn weights for monolingual features that
characterise bilingual redundancy. Preliminary
experiments defining MRF features over morpho-
logical annotation suggest this model can also
identify redundant distinctions categorised lin-
guistically (for instance, that morphological case
is redundant on Czech nouns and adjectives with
respect to English, while number is redundant only
on adjectives). In future work we will investigate
the use of linguistic resources to define feature sets
for the MRF prior. Lexical redundancy would ide-
ally be addressed in the context of phrases, how-
ever, computation and statistical estimation may
then be significantly more challenging.
Acknowledgements
The authors would like to thank Philipp Koehn for providing
training scripts used in this work; and Steve Renals, Mirella
Lapata and members of the Edinburgh SMT Group for valu-
able comments. This work was supported by an MRC Prior-
ity Area Studentship to the School of Informatics, University
of Edinburgh.
References
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In Proc. of the 10th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2004).
Julian Besag. 1986. The statistical analysis of dirty pictures.
Journal of the Royal Society Series B, 48(2):259?302.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and
Robert Mercer. 1993. The mathematics of machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Philip A. Chou. 1991. Optimal partitioning for classification
and regression trees. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 13(4).
M. Cmejrek, J. Curin, J. Havelka, J. Hajic, and V. Kubon.
2004. Prague Czech-English dependency treebank: Syn-
tactically annotated resources for machine translation. In
4th International Conference on Language Resources and
Evaluation, Lisbon, Portugal
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of im-
ages. IEEE Trans. on Pattern Analysis and Machine Intel-
ligence, 6:721?741.
Sharon Goldwater and David McClosky. 2005. Improving
statistical MT through morphological analysis. In Proc.
of the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002).
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of the
HLT/NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit 2005.
S. Niessen and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic infor-
mation. Computational Linguistics, 30(2):181?204.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F.-J. Och. 1998. An efficient method for determining bilin-
gual word classes. In Proc. of the European Chapter of
the Association for Computational Linguistics 1998.
Ye-Yi Wang, John Lafferty, and Alex Waibel. 1996. Word
clustering with parallel spoken language corpora. In Proc.
of 4th International Conference on Spoken Language Pro-
cessing, ICSLP 96, Philadelphia, PA.
D.H. Wolpert. 1995. Determining whether two data sets are
from the same distribution. In 15th international work-
shop on Maximum Entropy and Bayesian Methods.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected lan-
guages. In Proc. of the the European Chapter of the Asso-
ciation for Computational Linguistics 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In Proc. of the Human
Language Technology Conference (HLT-NAACL 2004).
976
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512?519,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Randomised Language Modelling for Statistical Machine Translation
David Talbot and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract
A Bloom filter (BF) is a randomised data
structure for set membership queries. Its
space requirements are significantly below
lossless information-theoretic lower bounds
but it produces false positives with some
quantifiable probability. Here we explore the
use of BFs for language modelling in statis-
tical machine translation.
We show how a BF containing n-grams can
enable us to use much larger corpora and
higher-order models complementing a con-
ventional n-gram LM within an SMT sys-
tem. We also consider (i) how to include ap-
proximate frequency information efficiently
within a BF and (ii) how to reduce the er-
ror rate of these models by first checking for
lower-order sub-sequences in candidate n-
grams. Our solutions in both cases retain the
one-sided error guarantees of the BF while
taking advantage of the Zipf-like distribution
of word frequencies to reduce the space re-
quirements.
1 Introduction
Language modelling (LM) is a crucial component in
statistical machine translation (SMT). Standard n-
gram language models assign probabilities to trans-
lation hypotheses in the target language, typically as
smoothed trigram models, e.g. (Chiang, 2005). Al-
though it is well-known that higher-order LMs and
models trained on additional monolingual corpora
can yield better translation performance, the chal-
lenges in deploying large LMs are not trivial. In-
creasing the order of an n-gram model can result in
an exponential increase in the number of parameters;
for corpora such as the English Gigaword corpus, for
instance, there are 300 million distinct trigrams and
over 1.2 billion 5-grams. Since a LMmay be queried
millions of times per sentence, it should ideally re-
side locally in memory to avoid time-consuming re-
mote or disk-based look-ups.
Against this background, we consider a radically
different approach to language modelling: instead
of explicitly storing all distinct n-grams, we store a
randomised representation. In particular, we show
that the Bloom filter (Bloom (1970); BF), a sim-
ple space-efficient randomised data structure for rep-
resenting sets, may be used to represent statistics
from larger corpora and for higher-order n-grams to
complement a conventional smoothed trigrammodel
within an SMT decoder. 1
The space requirements of a Bloom filter are quite
spectacular, falling significantly below information-
theoretic error-free lower bounds while query times
are constant. This efficiency, however, comes at the
price of false positives: the filter may erroneously
report that an item not in the set is a member. False
negatives, on the other hand, will never occur: the
error is said to be one-sided.
In this paper, we show that a Bloom filter can be
used effectively for language modelling within an
SMT decoder and present the log-frequency Bloom
filter, an extension of the standard Boolean BF that
1For extensions of the framework presented here to stand-
alone smoothed Bloom filter language models, we refer the
reader to a companion paper (Talbot and Osborne, 2007).
512
takes advantage of the Zipf-like distribution of cor-
pus statistics to allow frequency information to be
associated with n-grams in the filter in a space-
efficient manner. We then propose a mechanism,
sub-sequence filtering, for reducing the error rates
of these models by using the fact that an n-gram?s
frequency is bound from above by the frequency of
its least frequent sub-sequence.
We present machine translation experiments us-
ing these models to represent information regarding
higher-order n-grams and additional larger mono-
lingual corpora in combination with conventional
smoothed trigram models. We also run experiments
with these models in isolation to highlight the im-
pact of different order n-grams on the translation
process. Finally we provide some empirical analysis
of the effectiveness of both the log frequency Bloom
filter and sub-sequence filtering.
2 The Bloom filter
In this section, we give a brief overview of the
Bloom filter (BF); refer to Broder andMitzenmacher
(2005) for a more in detailed presentation. A BF rep-
resents a set S = {x1, x2, ..., xn} with n elements
drawn from a universe U of size N . The structure is
attractive when N  n. The only significant stor-
age used by a BF consists of a bit array of size m.
This is initially set to hold zeroes. To train the filter
we hash each item in the set k times using distinct
hash functions h1, h2, ..., hk. Each function is as-
sumed to be independent from each other and to map
items in the universe to the range 1 to m uniformly
at random. The k bits indexed by the hash values
for each item are set to 1; the item is then discarded.
Once a bit has been set to 1 it remains set for the life-
time of the filter. Distinct items may not be hashed
to k distinct locations in the filter; we ignore col-
lisons. Bits in the filter can, therefore, be shared by
distinct items allowing significant space savings but
introducing a non-zero probability of false positives
at test time. There is no way of directly retrieving or
ennumerating the items stored in a BF.
At test time we wish to discover whether a given
item was a member of the original set. The filter is
queried by hashing the test item using the same k
hash functions. If all bits referenced by the k hash
values are 1 then we assume that the item was a
member; if any of them are 0 then we know it was
not. True members are always correctly identified,
but a false positive will occur if all k corresponding
bits were set by other items during training and the
item was not a member of the training set. This is
known as a one-sided error.
The probability of a false postive, f , is clearly the
probability that none of k randomly selected bits in
the filter are still 0 after training. Letting p be the
proportion of bits that are still zero after these n ele-
ments have been inserted, this gives,
f = (1? p)k.
As n items have been entered in the filter by hashing
each k times, the probability that a bit is still zero is,
p
?
=
(
1?
1
m
)kn
? e?
kn
m
which is the expected value of p. Hence the false
positive rate can be approximated as,
f = (1? p)k ? (1? p
?
)k ?
(
1? e?
kn
m
)k
.
By taking the derivative we find that the number of
functions k? that minimizes f is,
k? = ln 2 ?
m
n
.
which leads to the intuitive result that exactly half
the bits in the filter will be set to 1 when the optimal
number of hash functions is chosen.
The fundmental difference between a Bloom fil-
ter?s space requirements and that of any lossless rep-
resentation of a set is that the former does not depend
on the size of the (exponential) universe N from
which the set is drawn. A lossless representation
scheme (for example, a hash map, trie etc.) must de-
pend on N since it assigns a distinct representation
to each possible set drawn from the universe.
3 Language modelling with Bloom filters
In our experiments we make use of both standard
(i.e. Boolean) BFs containing n-gram types drawn
from a training corpus and a novel BF scheme, the
log-frequency Bloom filter, that allows frequency
information to be associated efficiently with items
stored in the filter.
513
Algorithm 1 Training frequency BF
Input: Strain, {h1, ...hk} and BF = ?
Output: BF
for all x ? Strain do
c(x)? frequency of n-gram x in Strain
qc(x)? quantisation of c(x) (Eq. 1)
for j = 1 to qc(x) do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
BF [hi(x)]? 1
end for
end for
end for
return BF
3.1 Log-frequency Bloom filter
The efficiency of our scheme for storing n-gram
statistics within a BF relies on the Zipf-like distribu-
tion of n-gram frequencies in natural language cor-
pora: most events occur an extremely small number
of times, while a small number are very frequent.
We quantise raw frequencies, c(x), using a loga-
rithmic codebook as follows,
qc(x) = 1 + blogb c(x)c. (1)
The precision of this codebook decays exponentially
with the raw counts and the scale is determined by
the base of the logarithm b; we examine the effect of
this parameter in experiments below.
Given the quantised count qc(x) for an n-gram x,
the filter is trained by entering composite events con-
sisting of the n-gram appended by an integer counter
j that is incremented from 1 to qc(x) into the filter.
To retrieve the quantised count for an n-gram, it is
first appended with a count of 1 and hashed under
the k functions; if this tests positive, the count is in-
cremented and the process repeated. The procedure
terminates as soon as any of the k hash functions hits
a 0 and the previous count is reported. The one-sided
error of the BF and the training scheme ensure that
the actual quantised count cannot be larger than this
value. As the counts are quantised logarithmically,
the counter will be incremented only a small number
of times. The training and testing routines are given
here as Algorithms 1 and 2 respectively.
Errors for the log-frequency BF scheme are one-
sided: frequencies will never be underestimated.
Algorithm 2 Test frequency BF
Input: x, MAXQCOUNT , {h1, ...hk} and BF
Output: Upper bound on qc(x) ? Strain
for j = 1 to MAXQCOUNT do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
if BF [hi(x)] = 0 then
return j ? 1
end if
end for
end for
The probability of overestimating an item?s fre-
quency decays exponentially with the size of the
overestimation error d (i.e. as fd for d > 0) since
each erroneous increment corresponds to a single
false positive and d such independent events must
occur together.
3.2 Sub-sequence filtering
The error analysis in Section 2 focused on the false
positive rate of a BF; if we deploy a BF within an
SMT decoder, however, the actual error rate will also
depend on the a priori membership probability of
items presented to it. The error rate Err is,
Err = Pr(x /? Strain|Decoder)f.
This implies that, unlike a conventional lossless data
structure, the model?s accuracy depends on other
components in system and how it is queried.
We take advantage of the monotonicity of the n-
gram event space to place upper bounds on the fre-
quency of an n-gram prior to testing for it in the filter
and potentially truncate the outer loop in Algorithm
2 when we know that the test could only return pos-
tive in error.
Specifically, if we have stored lower-order n-
grams in the filter, we can infer that an n-gram can-
not present, if any of its sub-sequences test nega-
tive. Since our scheme for storing frequencies can
never underestimate an item?s frequency, this rela-
tion will generalise to frequencies: an n-gram?s fre-
quency cannot be greater than the frequency of its
least frequent sub-sequence as reported by the filter,
c(w1, ..., wn) ? min {c(w1, ..., wn?1), c(w2, ..., wn)}.
We use this to reduce the effective error rate of BF-
LMs that we use in the experiments below.
514
3.3 Bloom filter language model tests
A standard BF can implement a Boolean ?language
model? test: have we seen some fragment of lan-
guage before? This does not use any frequency in-
formation. The Boolean BF-LM is a standard BF
containing all n-grams of a certain length in the
training corpus, Strain. It implements the following
binary feature function in a log-linear decoder,
?bool(x) ? ?(x ? Strain)
Separate Boolean BF-LMs can be included for
different order n and assigned distinct log-linear
weights that are learned as part of a minimum error
rate training procedure (see Section 4).
The log-frequency BF-LM implements a multino-
mial feature function in the decoder that returns the
value associated with an n-gram by Algorithm 2.
?logfreq(x) ? qc(x) ? Strain
Sub-sequence filtering can be performed by using
the minimum value returned by lower-order models
as an upper-bound on the higher-order models.
By boosting the score of hypotheses containing n-
grams observed in the training corpus while remain-
ing agnostic for unseen n-grams (with the exception
of errors), these feature functions have more in com-
mon with maximum entropy models than conven-
tionally smoothed n-gram models.
4 Experiments
We conducted a range of experiments to explore the
effectiveness and the error-space trade-off of Bloom
filters for language modelling in SMT. The space-
efficiency of these models also allows us to inves-
tigate the impact of using much larger corpora and
higher-order n-grams on translation quality. While
our main experiments use the Bloom filter models in
conjunction with a conventional smoothed trigram
model, we also present experiments with these mod-
els in isolation to highlight the impact of different
order n-grams on the translation process. Finally,
we present some empirical analysis of both the log-
frequency Bloom filter and the sub-sequence filter-
ing technique which may be of independent interest.
Model EP-KN-3 EP-KN-4 AFP-KN-3
Memory 64M 99M 1.3G
gzip size 21M 31M 481M
1-gms 62K 62K 871K
2-gms 1.3M 1.3M 16M
3-gms 1.1M 1.0M 31M
4-gms N/A 1.1M N/A
Table 1: Baseline and Comparison Models
4.1 Experimental set-up
All of our experiments use publically available re-
sources. We use the French-English section of the
Europarl (EP) corpus for parallel data and language
modelling (Koehn, 2003) and the English Giga-
word Corpus (LDC2003T05; GW) for additional
language modelling.
Decoding is carried-out using the Moses decoder
(Koehn and Hoang, 2007). We hold out 500 test sen-
tences and 250 development sentences from the par-
allel text for evaluation purposes. The feature func-
tions in our models are optimised using minimum
error rate training and evaluation is performed using
the BLEU score.
4.2 Baseline and comparison models
Our baseline LM and other comparison models are
conventional n-gram models smoothed using modi-
fied Kneser-Ney and built using the SRILM Toolkit
(Stolcke, 2002); as is standard practice these models
drop entries for n-grams of size 3 and above when
the corresponding discounted count is less than 1.
The baseline language model, EP-KN-3, is a trigram
model trained on the English portion of the parallel
corpus. For additional comparisons we also trained a
smoothed 4-gram model on this Europarl data (EP-
KN-4) and a trigram model on the Agence France
Press section of the Gigaword Corpus (AFP-KN-3).
Table 1 shows the amount of memory these mod-
els take up on disk and compressed using the gzip
utility in parentheses as well as the number of dis-
tinct n-grams of each order. We give the gzip com-
pressed size as an optimistic lower bound on the size
of any lossless representation of each model.2
2Note, in particular, that gzip compressed files do not sup-
port direct random access as required by our application.
515
Corpus Europarl Gigaword
1-gms 61K 281K
2-gms 1.3M 5.4M
3-gms 4.7M 275M
4-gms 9.0M 599M
5-gms 10.3M 842M
6-gms 10.7M 957M
Table 2: Number of distinct n-grams
4.3 Bloom filter-based models
To create Bloom filter LMs we gathered n-gram
counts from both the Europarl (EP) and the whole
of the Gigaword Corpus (GW). Table 2 shows the
numbers of distinct n-grams in these corpora. Note
that we use no pruning for these models and that
the numbers of distinct n-grams is of the same or-
der as that of the recently released Google Ngrams
dataset (LDC2006T13). In our experiments we cre-
ate a range of models referred to by the corpus used
(EP or GW), the order of the n-gram(s) entered into
the filter (1 to 10), whether the model is Boolean
(Bool-BF) or provides frequency information (Freq-
BF), whether or not sub-sequence filtering was used
(FTR) and whether it was used in conjunction with
the baseline trigram (+EP-KN-3).
4.4 Machine translation experiments
Our first set of experiments examines the relation-
ship between memory allocated to the BF and BLEU
score. We present results using the Boolean BF-
LM in isolation and then both the Boolean and log-
frequency BF-LMS to add 4-grams to our baseline
3-gram model.Our second set of experiments adds
3-grams and 5-grams from the Gigaword Corpus to
our baseline. Here we constrast the Boolean BF-
LM with the log-frequency BF-LM with different
quantisation bases (2 = fine-grained and 5 = coarse-
grained). We then evaluate the sub-sequence fil-
tering approach to reducing the actual error rate of
these models by adding both 3 and 4-grams from the
Gigaword Corpus to the baseline. Since the BF-LMs
easily allow us to deploy very high-order n-gram
models, we use them to evaluate the impact of dif-
ferent order n-grams on the translation process pre-
senting results using the Boolean and log-frequency
BF-LM in isolation for n-grams of order 1 to 10.
Model EP-KN-3 EP-KN-4 AFP-KN-3
BLEU 28.51 29.24 29.17
Memory 64M 99M 1.3G
gzip size 21M 31M 481M
Table 3: Baseline and Comparison Models
4.5 Analysis of BF extensions
We analyse our log-frequency BF scheme in terms
of the additional memory it requires and the error
rate compared to a non-redundant scheme. The non-
redundant scheme involves entering just the exact
quantised count for each n-gram and then searching
over the range of possible counts at test time starting
with the count with maximum a priori probability
(i.e. 1) and incrementing until a count is found or
the whole codebook has been searched (here the size
is 16).
We also analyse the sub-sequence filtering
scheme directly by creating a BF with only 3-grams
and a BF containing both 2-grams and 3-grams and
comparing their actual error rates when presented
with 3-grams that are all known to be negatives.
5 Results
5.1 Machine translation experiments
Table 3 shows the results of the baseline (EP-KN-
3) and other conventional n-gram models trained on
larger corpora (AFP-KN-3) and using higher-order
dependencies (EP-KN-4). The larger models im-
prove somewhat on the baseline performance.
Figure 1 shows the relationship between space al-
located to the BF models and BLEU score (left) and
false positive rate (right) respectively. These experi-
ments do not include the baseline model. We can see
a clear correlation between memory / false positive
rate and translation performance.
Adding 4-grams in the form of a Boolean BF or a
log-frequency BF (see Figure 2) improves on the 3-
gram baseline with little additional memory (around
4MBs) while performing on a par with or above
the Europarl 4-gram model with around 10MBs;
this suggests that a lossy representation of the un-
pruned set of 4-grams contains more useful informa-
tion than a lossless representation of the pruned set.3
3An unpruned modified Kneser-Ney 4-gram model on the
Eurpoparl data scores slightly higher - 29.69 - while taking up
489MB (132MB gzipped).
516
 
29
 
28
 
27
 
26
 
25
 
10
 
8
 
6
 
4
 
2
 
1
 
0.8
 
0.6
 
0.4
 
0.2
 
0
BLEU Score
False positive rate
Mem
ory in
 MB
Europ
arl Bo
olean
 BF 4
-gram
 (alone
)
BLEU
 Scor
e Boo
l-BF-E
P-4
False
 posit
ive ra
te
Figure 1: Space/Error vs. BLEU Score.
 
30.5  30
 
29.5  29
 
28.5  28
 
9
 
7
 
5
 
3
 
1
BLEU Score
Mem
ory in
 MB
EP-B
ool-B
F-4 a
nd Fr
eq-BF
-4 (with
 EP-KN
-3)
EP-B
ool-B
F-4 +
 EP-K
N-3
EP-F
req-B
F-4 +
 EP-K
N-3
EP-K
N-4 c
ompa
rison 
(99M / 
31M gz
ip)
EP-K
N-3 b
aselin
e (64M
 / 21M 
gzip)
Figure 2: Adding 4-grams with Bloom filters.
As the false positive rate exceeds 0.20 the perfor-
mance is severly degraded. Adding 3-grams drawn
from the whole of the Gigaword corpus rather than
simply the Agence France Press section results in
slightly improved performance with signficantly less
memory than the AFP-KN-3 model (see Figure 3).
Figure 4 shows the results of adding 5-grams
drawn from the Gigaword corpus to the baseline. It
also contrasts the Boolean BF and the log-frequency
BF suggesting in this case that the log-frequency BF
can provide useful information when the quantisa-
tion base is relatively fine-grained (base 2). The
Boolean BF and the base 5 (coarse-grained quan-
tisation) log-frequency BF perform approximately
the same. The base 2 quantisation performs worse
 
30.5  30
 
29.5  29
 
28.5  28
 
27.5  27
 
1
 
0.8
 
0.6
 
0.4
 
0.2
 
0.1
BLEU Score
Mem
ory in
 GB
GW-B
ool-B
F-3 a
nd GW
-Freq
-BF-3
 (with E
P-KN-3
)
GW-B
ool-B
F-3 +
 EP-K
N-3
GW-F
req-B
F-3 +
 EP-K
N-3
AFP-
KN-3
 + EP
-KN-3
Figure 3: Adding GW 3-grams with Bloom filters.
 
30.5  30
 
29.5  29
 
28.5  28
 
1
 
0.8
 
0.6
 
0.4
 
0.2
 
0.1
BLEU Score
Mem
ory in
 GB
GW-B
ool-B
F-5 a
nd GW
-Freq
-BF-5
 (base 
2 and 5
) (with 
EP-KN
-3)
GW-B
ool-B
F-5 +
 EP-K
N-3
GW-F
req-B
F-5 (ba
se 2) +
 EP-KN
-3
GW-F
req-B
F-5 (ba
se 5) +
 EP-KN
-3
AFP-
KN-3
 + EP
-KN-3
Figure 4: Comparison of different quantisation rates.
for smaller amounts of memory, possibly due to the
larger set of events it is required to store.
Figure 5 shows sub-sequence filtering resulting in
a small increase in performance when false positive
rates are high (i.e. less memory is allocated). We
believe this to be the result of an increased a pri-
ori membership probability for n-grams presented
to the filter under the sub-sequence filtering scheme.
Figure 6 shows that for this task the most useful
n-gram sizes are between 3 and 6.
5.2 Analysis of BF extensions
Figure 8 compares the memory requirements of
the log-frequencey BF (base 2) and the Boolean
517
 
31
 
30
 
29
 
28
 
1
 
0.8
 
0.6
 
0.4
 
0.2
BLEU Score
Mem
ory in
 MB
GW-B
ool-B
F-3-4
-FTR 
and G
W-Bo
ol-BF
-3-4 (w
ith EP-
KN-3)
GW-B
ool-B
F-3-4
-FTR 
+ EP-
KN-3
GW-B
ool-B
F-3-4
 + EP
-KN-3
Figure 5: Effect of sub-sequence filtering.
 
27
 
26
 
25
 
24
 
10
 
9
 
8
 
7
 
6
 
5
 
4
 
3
 
2
 
1
BLEU Score
N-gra
m ord
er
EP-B
ool-B
F and
 EP-F
req-B
F with
 differ
ent or
der N
-gram
s (alon
e)
EP-B
ool-B
F
EP-F
req-B
F
Figure 6: Impact of n-grams of different sizes.
BF for various order n-gram sets from the Giga-
word Corpus with the same underlying false posi-
tive rate (0.125). The additional space required by
our scheme for storing frequency information is less
than a factor of 2 compared to the standard BF.
Figure 7 shows the number and size of frequency
estimation errors made by our log-frequency BF
scheme and a non-redundant scheme that stores only
the exact quantised count. We presented 500K nega-
tives to the filter and recorded the frequency of over-
estimation errors of each size. As shown in Section
3.1, the probability of overestimating an item?s fre-
quency under the log-frequency BF scheme decays
exponentially in the size of this overestimation er-
ror. Although the non-redundant scheme requires
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
16
 
15
 
14
 
13
 
12
 
11
 
10
 
9
 
8
 
7
 
6
 
5
 
4
 
3
 
2
 
1
Frequency (K)
Size o
f over
estim
ation 
error
Frequ
ency 
Estim
ation 
Errors
 on 50
0K Ne
gative
s
Log-fr
equen
cy BF
 (Bloom
 error =
 0.159)
Non-r
edund
ant sc
heme
 (Bloom
 error =
 0.076)
Figure 7: Frequency estimation errors.
 
0
 
100
 
200
 
300
 
400
 
500
 
600
 
700  1
 
2
 
3
 
4
 
5
 
6
 
7
Memory (MB)
N-gra
m ord
er (Gig
aword)
Mem
ory re
quirem
ents f
or 0.1
25 fal
se po
sitive 
rate
Bool-
BF
Freq-
BF (log
 base-2
 quanti
sation)
Figure 8: Comparison of memory requirements.
fewer items be stored in the filter and, therefore, has
a lower underlying false positive rate (0.076 versus
0.159), in practice it incurs a much higher error rate
(0.717) with many large errors.
Figure 9 shows the impact of sub-sequence filter-
ing on the actual error rate. Although, the false pos-
itive rate for the BF containing 2-grams, in addition,
to 3-grams (filtered) is higher than the false positive
rate of the unfiltered BF containing only 3-grams,
the actual error rate of the former is lower for mod-
els with less memory. By testing for 2-grams prior
to querying for the 3-grams, we can avoid perform-
ing some queries that may otherwise have incurred
errors using the fact that a 3-gram cannot be present
if one of its constituent 2-grams is absent.
518
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7  0.
5
 
1
 
1.5
 
2
 
2.5
 
3
 
3.5
Error rate
Mem
ory (MB
)
Error
 rate 
with s
ub-se
quenc
e filte
ring
Filter
ed fal
se po
sitive 
rate
Unfilt
ered 
false 
pos ra
te / ac
tual e
rror ra
te
Filter
ed ac
tual e
rror ra
te
Figure 9: Error rate with sub-sequence filtering.
6 Related Work
We are not the first people to consider building very
large scale LMs: Kumar et al used a four-gram
LM for re-ranking (Kumar et al, 2005) and in un-
published work, Google used substantially larger n-
grams in their SMT system. Deploying such LMs
requires either a cluster of machines (and the over-
heads of remote procedure calls), per-sentence fil-
tering (which again, is slow) and/or the use of some
other lossy compression (Goodman and Gao, 2000).
Our approach can complement all these techniques.
Bloom filters have been widely used in database
applications for reducing communications over-
heads and were recently applied to encode word
frequencies in information retrieval (Linari and
Weikum, 2006) using a method that resembles the
non-redundant scheme described above. Exten-
sions of the BF to associate frequencies with items
in the set have been proposed e.g., (Cormode and
Muthukrishn, 2005); while these schemes are more
general than ours, they incur greater space overheads
for the distributions that we consider here.
7 Conclusions
We have shown that Bloom Filters can form the ba-
sis for space-efficient language modelling in SMT.
Extending the standard BF structure to encode cor-
pus frequency information and developing a strat-
egy for reducing the error rates of these models by
sub-sequence filtering, our models enable higher-
order n-grams and larger monolingual corpora to be
used more easily for language modelling in SMT.
In a companion paper (Talbot and Osborne, 2007)
we have proposed a framework for deriving con-
ventional smoothed n-gram models from the log-
frequency BF scheme allowing us to do away en-
tirely with the standard n-gram model in an SMT
system. We hope the present work will help estab-
lish the Bloom filter as a practical alternative to con-
ventional associative data structures used in compu-
tational linguistics. The framework presented here
shows that with some consideration for its workings,
the randomised nature of the Bloom filter need not
be a significant impediment to is use in applications.
References
B. Bloom. 1970. Space/time tradeoffs in hash coding with
allowable errors. CACM, 13:422?426.
A. Broder and M. Mitzenmacher. 2005. Network applications
of bloom filters: A survey. Internet Mathematics, 1(4):485?
509.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 263?270, Ann Arbor, Michigan.
G. Cormode and S. Muthukrishn. 2005. An improved data
stream summary: the count-min sketch and its applications.
Journal of Algorithms, 55(1):58?75.
J. Goodman and J. Gao. 2000. Language model size reduction
by pruning and clustering. In ICSLP?00, Beijing, China.
Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proc. of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP/Co-NLL).
P. Koehn. 2003. Europarl: A multilingual corpus for eval-
uation of machine translation philipp koehn, draft. Available
at:http://people.csail.mit.edu/ koehn/publications/europarl.ps.
S. Kumar, Y. Deng, and W. Byrne. 2005. Johns Hopkins Uni-
versity - Cambridge University Chinese-English and Arabic-
English 2005 NIST MT Evaluation Systems. In Proceedings
of 2005 NIST MT Workshop, June.
Alessandro Linari and Gerhard Weikum. 2006. Efficient peer-
to-peer semantic overlay networks based on statistical lan-
guage models. In Proceedings of the International Workshop
on IR in Peer-to-Peer Networks, pages 9?16, Arlington.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. of the Intl. Conf. on Spoken Lang.
Processing, 2002.
David Talbot and Miles Osborne. 2007. Smoothed Bloom fil-
ter language models: Tera-scale LMs on the cheap. In Pro-
ceedings of the 2007 Conference on Empirical Methods in
Natural Language Processing (EMNLP/Co-NLL), June.
519
Proceedings of ACL-08: HLT, pages 200?208,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Discriminative Latent Variable Model
for Statistical Machine Translation
Phil Blunsom, Trevor Cohn and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
{pblunsom,tcohn,miles}@inf.ed.ac.uk
Abstract
Large-scale discriminative machine transla-
tion promises to further the state-of-the-art,
but has failed to deliver convincing gains over
current heuristic frequency count systems. We
argue that a principle reason for this failure is
not dealing with multiple, equivalent transla-
tions. We present a translation model which
models derivations as a latent variable, in both
training and decoding, and is fully discrimina-
tive and globally optimised. Results show that
accounting for multiple derivations does in-
deed improve performance. Additionally, we
show that regularisation is essential for max-
imum conditional likelihood models in order
to avoid degenerate solutions.
1 Introduction
Statistical machine translation (SMT) has seen
a resurgence in popularity in recent years, with
progress being driven by a move to phrase-based and
syntax-inspired approaches. Progress within these
approaches however has been less dramatic. We be-
lieve this is because these frequency count based1
models cannot easily incorporate non-independent
and overlapping features, which are extremely use-
ful in describing the translation process. Discrimi-
native models of translation can include such fea-
tures without making assumptions of independence
or explicitly modelling their interdependence. How-
ever, while discriminative models promise much,
they have not been shown to deliver significant gains
1We class approaches using minimum error rate training
(Och, 2003) frequency count based as these systems re-scale a
handful of generative features estimated from frequency counts
and do not support large sets of non-independent features.
over their simpler cousins. We argue that this is due
to a number of inherent problems that discrimina-
tive models for SMT must address, in particular the
problems of spurious ambiguity and degenerate so-
lutions. These occur when there are many ways to
translate a source sentence to the same target sen-
tence by applying a sequence of steps (a derivation)
of either phrase translations or synchronous gram-
mar rules, depending on the type of system. Exist-
ing discriminative models require a reference deriva-
tion to optimise against, however no parallel cor-
pora annotated for derivations exist. Ideally, a model
would account for this ambiguity by marginalising
out the derivations, thus predicting the best transla-
tion rather than the best derivation. However, doing
so exactly is NP-complete. For this reason, to our
knowledge, all discriminative models proposed to
date either side-step the problem by choosing simple
model and feature structures, such that spurious am-
biguity is lessened or removed entirely (Ittycheriah
and Roukos, 2007; Watanabe et al, 2007), or else ig-
nore the problem and treat derivations as translations
(Liang et al, 2006; Tillmann and Zhang, 2007).
In this paper we directly address the problem of
spurious ambiguity in discriminative models. We
use a synchronous context free grammar (SCFG)
translation system (Chiang, 2007), a model which
has yielded state-of-the-art results on many transla-
tion tasks. We present two main contributions. First,
we develop a log-linear model of translation which
is globally trained on a significant number of paral-
lel sentences. This model maximises the conditional
likelihood of the data, p(e|f), where e and f are the
English and foreign sentences, respectively. Our es-
timation method is theoretically sound, avoiding the
biases of the heuristic relative frequency estimates
200
l
l
l
l
l l
l
l
l
l
l
sentence length
deriv
ation
s
5 7 9 11 13 15
1e+0
3
1e+0
5
1e+0
8
Figure 1. Exponential relationship between sentence
length and the average number of derivations (on a log
scale) for each reference sentence in our training corpus.
(Koehn et al, 2003). Second, within this frame-
work, we model the derivation, d, as a latent vari-
able, p(e,d|f), which is marginalised out in train-
ing and decoding. We show empirically that this
treatment results in significant improvements over a
maximum-derivation model.
The paper is structured as follows. In Section 2
we list the challenges that discriminative SMT must
face above and beyond the current systems. We sit-
uate our work, and previous work, on discrimina-
tive systems in this context. We present our model
in Section 3, including our means of training and de-
coding. Section 4 reports our experimental setup and
results, and finally we conclude in Section 5.
2 Challenges for Discriminative SMT
Discriminative models allow for the use of expres-
sive features, in the order of thousands or millions,
which can reference arbitrary aspects of the source
sentence. Given most successful SMT models have
a highly lexicalised grammar (or grammar equiva-
lent), these features can be used to smuggle in lin-
guistic information, such as syntax and document
context. With this undoubted advantage come four
major challenges when compared to standard fre-
quency count SMT models:
1. There is no one reference derivation. Often
there are thousands of ways of translating a
source sentence into the reference translation.
Figure 1 illustrates the exponential relationship
between sentence length and the number of
derivations. Training is difficult without a clear
target, and predicting only one derivation at test
time is fraught with danger.
2. Parallel translation data is often very noisy,
with such problems as non-literal translations,
poor sentence- and word-alignments. A model
which exactly translates the training data will
inevitably perform poorly on held-out data.
This problem of over-fitting is exacerbated
in discriminative models with large, expres-
sive, feature sets. Regularisation is essential for
models with more than a handful of features.
3. Learning with a large feature set requires many
training examples and typically many iterations
of a solver during training. While current mod-
els focus solely on efficient decoding, discrim-
inative models must also allow for efficient
training.
Past work on discriminative SMT only address
some of these problems. To our knowledge no sys-
tems directly address Problem 1, instead choosing to
ignore the problem by using one or a small handful
of reference derivations in an n-best list (Liang et al,
2006; Watanabe et al, 2007), or else making local
independence assumptions which side-step the issue
(Ittycheriah and Roukos, 2007; Tillmann and Zhang,
2007; Wellington et al, 2006). These systems all in-
clude regularisation, thereby addressing Problem 2.
An interesting counterpoint is the work of DeNero et
al. (2006), who show that their unregularised model
finds degenerate solutions. Some of these discrim-
inative systems have been trained on large training
sets (Problem 3); these systems are the local models,
for which training is much simpler. Both the global
models (Liang et al, 2006; Watanabe et al, 2007)
use fairly small training sets, and there is no evi-
dence that their techniques will scale to larger data
sets.
Our model addresses all three of the above prob-
lems within a global model, without resorting to n-
best lists or local independence assumptions. Fur-
thermore, our model explicitly accounts for spurious
ambiguity without altering the model structure or ar-
bitrarily selecting one derivation. Instead we model
the translation distribution with a latent variable for
the derivation, which we marginalise out in training
and decoding.
201
the hat
le chapeau
red
the hat
le chapeau
red
Figure 2. The dropping of an adjective in this example
means that there is no one segmentation that we could
choose that would allow a system to learn le ? the and
chapeau? hat.
?S? ? ?S 1 X 2 , S 1 X 2 ?
?S? ? ?X 1 , X 1 ?
?X? ? ?ne X 1 pas, does not X 1 ?
?X? ? ?va, go?
?X? ? ?il, he?
Figure 3. A simple SCFG, with non-terminal symbols S
and X, which performs the transduction: il ne vas pas ?
he does not go
This itself provides robustness to noisy data, in
addition to the explicit regularisation from a prior
over the model parameters. For example, in many
cases there is no one perfect derivation, but rather
many imperfect ones which each include some good
translation fragments. The model can learn from
many of these derivations and thereby learn from
all these translation fragments. This situation is il-
lustrated in Figure 2 where the non-translated ad-
jective red means neither segmentation is ?correct?,
although both together present positive evidence for
the two lexical translations.
We present efficient methods for training and pre-
diction, demonstrating their scaling properties by
training on more than a hundred thousand train-
ing sentences. Finally, we stress that our main find-
ings are general ones. These results could ? and
should ? be applied to other models, discriminative
and generative, phrase- and syntax-based, to further
progress the state-of-the-art in machine translation.
3 Discriminative Synchronous
Transduction
A synchronous context free grammar (SCFG) con-
sists of paired CFG rules with co-indexed non-
terminals (Lewis II and Stearns, 1968). By assign-
ing the source and target languages to the respective
sides of a SCFG it is possible to describe translation
as the process of parsing the source sentence using
a CFG, while generating the target translation from
the other (Chiang, 2007). All the models we present
use the grammar extraction technique described in
Chiang (2007), and are bench-marked against our
own implementation of this hierarchical model (Hi-
ero). Figure 3 shows a simple instance of a hierar-
chical grammar with two non-terminals. Note that
our approach is general and could be used with other
synchronous grammar transducers (e.g., Galley et al
(2006)).
3.1 A global log-linear model
Our log-linear translation model defines a condi-
tional probability distribution over the target trans-
lations of a given source sentence. A particular se-
quence of SCFG rule applications which produces a
translation from a source sentence is referred to as a
derivation, and each translation may be produced by
many different derivations. As the training data only
provides source and target sentences, the derivations
are modelled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f ,
is given by:
p?(d, e|f) =
exp
?
k ?kHk(d, e, f)
Z?(f)
(1)
where Hk(d, e, f) =
?
r?d
hk(f , r) (2)
Here k ranges over the model?s features, and
? = {?k} are the model parameters (weights for
their corresponding features). The feature functions
Hk are predefined real-valued functions over the
source and target sentences, and can include over-
lapping and non-independent features of the data.
The features must decompose with the derivation,
as shown in (2). The features can reference the en-
tire source sentence coupled with each rule, r, in a
derivation. The distribution is globally normalised
by the partition function, Z?(f), which sums out the
numerator in (1) for every derivation (and therefore
every translation) of f :
Z?(f) =
?
e
?
d??(e,f)
exp
?
k
?kHk(d, e, f)
Given (1), the conditional probability of a target
translation given the source is the sum over all of
its derivations:
p?(e|f) =
?
d??(e,f)
p?(d, e|f) (3)
202
where ?(e, f) is the set of all derivations of the tar-
get sentence e from the source f.
Most prior work in SMT, both generative and dis-
criminative, has approximated the sum over deriva-
tions by choosing a single ?best? derivation using a
Viterbi or beam search algorithm. In this work we
show that it is both tractable and desirable to directly
account for derivational ambiguity. Our findings
echo those observed for latent variable log-linear
models successfully used in monolingual parsing
(Clark and Curran, 2007; Petrov et al, 2007). These
models marginalise over derivations leading to a de-
pendency structure and splits of non-terminal cate-
gories in a PCFG, respectively.
3.2 Training
The parameters of our model are estimated
from our training sample using a maximum a
posteriori (MAP) estimator. This maximises
the likelihood of the parallel training sen-
tences, D = {(e, f)}, penalised using a prior,
i.e., ?MAP = arg max? p?(D)p(?). We use a
zero-mean Gaussian prior with the probability
density function p0(?k) ? exp
(
??2k/2?
2
)
.2 This
results in the following log-likelihood objective and
corresponding gradient:
L =
?
(e,f)?D
log p?(e|f) +
?
k
log p0(?k) (4)
?L
??k
= Ep?(d|e,f)[hk]? Ep?(e|f)[hk]?
?k
?2
(5)
In order to train the model, we maximise equation
(4) using L-BFGS (Malouf, 2002; Sha and Pereira,
2003). This method has been demonstrated to be ef-
fective for (non-convex) log-linear models with la-
tent variables (Clark and Curran, 2004; Petrov et al,
2007). Each L-BFGS iteration requires the objective
value and its gradient with respect to the model pa-
rameters. These are calculated using inside-outside
inference over the feature forest defined by the
SCFG parse chart of f yielding the partition func-
tion, Z?(f), required for the log-likelihood, and the
marginals, required for its derivatives.
Efficiently calculating the objective and its gradi-
ent requires two separate packed charts, each rep-
resenting a derivation forest. The first one is the full
chart over the space of possible derivations given the
2In general, any conjugate prior could be used instead of a
simple Gaussian.
source sentence. The inside-outside algorithm over
this chart gives the marginal probabilities for each
chart cell, from which we can find the feature ex-
pectations. The second chart contains the space of
derivations which produce the reference translation
from the source. The derivations in this chart are a
subset of those in the full derivation chart. Again,
we use the inside-outside algorithm to find the ?ref-
erence? feature expectations from this chart. These
expectations are analogous to the empirical observa-
tion of maximum entropy classifiers.
Given these two charts we can calculate the log-
likelihood of the reference translation as the inside-
score from the sentence spanning cell of the ref-
erence chart, normalised by the inside-score of the
spanning cell from the full chart. The gradient is cal-
culated as the difference of the feature expectations
of the two charts. Clark and Curran (2004) provides
a more complete discussion of parsing with a log-
linear model and latent variables.
The full derivation chart is produced using a CYK
parser in the same manner as Chiang (2005), and has
complexity O(|e|3). We produce the reference chart
by synchronously parsing the source and reference
sentences using a variant of CYK algorithm over two
dimensions, with a time complexity of O(|e|3|f |3).
This is an instance of the ITG alignment algorithm
(Wu, 1997). This step requires the reference transla-
tion for each training instance to be contained in the
model?s hypothesis space. Achieving full coverage
implies inducing a grammar which generates all ob-
served source-target pairs, which is difficult in prac-
tise. Instead we discard the unreachable portion of
the training sample (24% in our experiments). The
proportion of discarded sentences is a function of
the grammar used. Extraction heuristics other than
the method used herein (Chiang, 2007) could allow
complete coverage (e.g., Galley et al (2004)).
3.3 Decoding
Accounting for all derivations of a given transla-
tion should benefit not only training, but also decod-
ing. Unfortunately marginalising over derivations in
decoding is NP-complete. The standard solution is
to approximate the maximum probability translation
using a single derivation (Koehn et al, 2003).
Here we approximate the sum over derivations di-
rectly using a beam search in which we produce a
beam of high probability translation sub-strings for
each cell in the parse chart. This algorithm is sim-
203
X[1,2]
 
on
X
[2,3]
 
the
X
[3,4]
 
table
X
[1,3]
 
on the
X
[2,4]
 
the table
X
[1,3]
 
on the table
X
[3,4]
 
chart
X
[2,4]
 
the chart
X
[1,3]
 
on the chart
 s
1
  
sur  
2
  
la  
3
  
table 
 
4
Figure 4. Hypergraph representation of max translation
decoding. Each chart cell must store the entire target
string generated.
ilar to the methods for decoding with a SCFG in-
tersected with an n-gram language model, which re-
quire language model contexts to be stored in each
chart cell. However, while Chiang (2005) stores an
abbreviated context composed of the n ? 1 target
words on the left and right edge of the target sub-
string, here we store the entire target string. Addi-
tionally, instead of maximising scores in each beam
cell, we sum the inside scores for each derivation
that produces a given string for that cell. When the
beam search is complete we have a list of trans-
lations in the top beam cell spanning the entire
source sentence along with their approximated in-
side derivation scores. Thus we can assign each
translation string a probability by normalising its in-
side score by the sum of the inside scores of all the
translations spanning the entire sentence.
Figure 4 illustrates the search process for the sim-
ple grammar from Table 2. Each graph node repre-
sents a hypothesis translation substring covering a
sub-span of the source string. The space of trans-
lation sub-strings is exponential in each cell?s span,
and our algorithm can only sum over a small fraction
of the possible strings. Therefore the resulting prob-
abilities are only estimates. However, as demon-
strated in Section 4, this algorithm is considerably
more effective than maximum derivation (Viterbi)
decoding.
4 Evaluation
Our model evaluation was motivated by the follow-
ing questions: (1) the effect of maximising transla-
tions rather than derivations in training and decod-
ing; (2) whether a regularised model performs better
than a maximum likelihood model; (3) how the per-
formance of our model compares with a frequency
count based hierarchical system; and (4) how trans-
lation performance scales with the number of train-
ing examples.
We performed all of our experiments on the
Europarl V2 French-English parallel corpus.3 The
training data was created by filtering the full cor-
pus for all the French sentences between five and
fifteen words in length, resulting in 170K sentence
pairs. These limits were chosen as a compromise
between experiment turnaround time and leaving
a large enough corpus to obtain indicative results.
The development and test data was taken from the
2006 NAACL and 2007 ACL workshops on ma-
chine translation, also filtered for sentence length.4
Tuning of the regularisation parameter and MERT
training of the benchmark models was performed on
dev2006, while the test set was the concatenation
of devtest2006, test2006 and test2007, amounting to
315 development and 1164 test sentences.
Here we focus on evaluating our model?s basic
ability to learn a conditional distribution from sim-
ple binary features, directly comparable to those
currently employed in frequency count models. As
such, our base model includes a single binary iden-
tity feature per-rule, equivalent to the p(e|f) param-
eters defined on each rule in standard models.
As previously noted, our model must be able to
derive the reference sentence from the source for it
to be included in training. For both our discrimina-
tive and benchmark (Hiero) we extracted our gram-
mar on the 170K sentence corpus using the approach
described in Chiang (2007), resulting in 7.8 million
rules. The discriminative model was then trained on
the training partition, however only 130K of the sen-
tences were used as the model could not produce
a derivation of the reference for the remaining sen-
tences. There were many grammar rules that the dis-
criminative model did not observe in a reference
derivation, and thus could not assign their feature a
positive weight. While the benchmark model has a
3http://www.statmt.org/europarl/
4http://www.statmt.org/wmt0{6,7}
204
Decoding
Training derivation translation
All Derivations 28.71 31.23
Single Derivation 26.70 27.32
ML (?2 =?) 25.57 25.97
Table 1. A comparison on the impact of accounting for all
derivations in training and decoding (development set).
positive count for every rule (7.8M), the discrimina-
tive model only observes 1.7M rules in actual refer-
ence derivations. Figure 1 illustrates the massive am-
biguity present in the training data, with fifteen word
sentences averaging over 70M reference derivations.
Performance is evaluated using cased BLEU4
score on the test set. Although there is no direct rela-
tionship between BLEU and likelihood, it provides
a rough measure for comparing performance.
Derivational ambiguity Table 1 shows the im-
pact of accounting for derivational ambiguity in
training and decoding.5 There are two options for
training, we could use our latent variable model and
optimise the probability of all derivations of the
reference translation, or choose a single derivation
that yields the reference and optimise its probability
alone. The second option raises the difficult question
of which one, of the thousands available, we should
choose? We use the derivation which contains the
most rules. The intuition is that small rules are likely
to appear more frequently, and thus generalise bet-
ter to a test set. In decoding we can search for the
maximum probability derivation, which is the stan-
dard practice in SMT, or for the maximum probabil-
ity translation which is what we actually want from
our model, i.e. the best translation.
The results clearly indicate the value in opti-
mising translations, rather than derivations. Max-
translation decoding for the model trained on single
derivations has only a small positive effect, while for
the latent variable model the impact is much larger.6
For example, our max-derivation model trained
on the Europarl data translates carte sur la table as
on the table card. This error in the reordering of card
(which is an acceptable translation of carte) is due
to the rule ?X? ? ?carte X 1 , X 1 card? being the
highest scoring rule for carte. This is reasonable, as
5When not explicitly stated, both here and in subsequent re-
sults, the regularisation parameter was set to one, ?2 = 1.
6We also experimented with using max-translation decoding
for standard MER trained translation models, finding that it had
a small negative impact on BLEU score.
l
l
l
l l l
l
beam width
deve
lopm
ent B
LEU 
(%)
29.0
29.5
30.0
30.5
31.0
31.5
100 1k 10k
Figure 5. The effect of the beam width (log-scale) on max-
translation decoding (development set).
carte is a noun, which in the training data, is often
observed with a trailing adjective which needs to be
reordered when translating into English. In the ex-
ample there is no adjective, but the simple hierarchi-
cal grammar cannot detect this. The max-translation
model finds a good translation card on the table.
This is due to the many rules that enforce monotone
ordering around sur la, ?X? ? ?X 1 sur, X 1 in?
?X? ? ?X 1 sur la X 2 , X 1 in the X 2 ? etc.
The scores of these many monotone rules sum to be
greater than the reordering rule, thus allowing the
model to use the weight of evidence to settle on the
correct ordering.
Having established that the search for the best
translation is effective, the question remains as to
how the beam width over partial translations affects
performance. Figure 5 shows the relationship be-
tween beam width and development BLEU. Even
with a very tight beam of 100, max-translation de-
coding outperforms maximum-derivation decoding,
and performance is increasing even at a width of
10k. In subsequent experiments we use a beam of
5k which provides a good trade-off between perfor-
mance and speed.
Regularisation Table 1 shows that the per-
formance of an unregularised maximum likeli-
hood model lags well behind the regularised max-
translation model. From this we can conclude that
the maximum likelihood model is overfitting the
training set. We suggest that is a result of the degen-
erate solutions of the conditional maximum likeli-
hood estimate, as described in DeNero et al (2006).
Here we assert that our regularised maximum a pos-
205
Grammar Rules ML MAP
(?2 =?) (?2 = 1)
?X???carte, map? 1.0 0.5
?X???carte, notice? 0.0 0.5
?X???sur, on? 1.0 1.0
?X???la, the? 1.0 1.0
?X???table, table? 1.0 0.5
?X???table, chart? 0.0 0.5
?X???carte sur, notice on? 1.0 0.5
?X???carte sur, map on? 0.0 0.5
?X???sur la, on the? 1.0 1.0
?X???la table, the table? 0.0 0.5
?X???la table, the chart? 1.0 0.5
Training data:
carte sur la table? map on the table
carte sur la table? notice on the chart
Table 2. Comparison of the susceptibility to degenerate
solutions for a ML and MAP optimised model, using a sim-
ple grammar with one parameter per rule and a monotone
glue rule: ?X? ? ?X 1 X 2 , X 1X 2 ?
teriori model avoids such solutions.
This is illustrated in Table 2, which shows the
conditional probabilities for rules, obtained by lo-
cally normalising the rule feature weights for a sim-
ple grammar extracted from the ambiguous pair of
sentences presented in DeNero et al (2006). The
first column of conditional probabilities corresponds
to a maximum likelihood estimate, i.e., without reg-
ularisation. As expected, the model finds a degener-
ate solution in which overlapping rules are exploited
in order to minimise the entropy of the rule trans-
lation distributions. The second column shows the
solution found by our model when regularised by a
Gaussian prior with unit variance. Here we see that
the model finds the desired solution in which the true
ambiguity of the translation rules is preserved. The
intuition is that in order to find a degenerate solu-
tion, dispreferred rules must be given large negative
weights. However the prior penalises large weights,
and therefore the best strategy for the regularised
model is to evenly distribute probability mass.
Translation comparison Having demonstrated
that accounting for derivational ambiguity leads to
improvements for our discriminative model, we now
place the performance of our system in the context
of the standard approach to hierarchical translation.
To do this we use our own implementation of Hiero
(Chiang, 2007), with the same grammar but with the
traditional generative feature set trained in a linear
model with minimum BLEU training. The feature
set includes: a trigram language model (lm) trained
System Test (BLEU)
Discriminative max-derivation 25.78
Hiero (pd, gr, rc, wc) 26.48
Discriminative max-translation 27.72
Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc) 28.14
Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc, lm) 32.00
Table 3. Test set performance compared with a standard
Hiero system
on the English side of the unfiltered Europarl corpus;
direct and reverse translation scores estimated as rel-
ative frequencies (pd, pr); lexical translation scores
(plexd , p
lex
r ), a binary flag for the glue rule which al-
lows the model to (dis)favour monotone translation
(gr); and rule and target word counts (rc, wc).
Table 3 shows the results of our system on the
test set. Firstly we show the relative scores of our
model against Hiero without using reverse transla-
tion or lexical features.7 This allows us to directly
study the differences between the two translation
models without the added complication of the other
features. As well as both modelling the same dis-
tribution, when our model is trained with a single
parameter per-rule these systems have the same pa-
rameter space, differing only in the manner of esti-
mation.
Additionally we show the scores achieved by
MERT training the full set of features for Hiero, with
and without a language model.8 We provide these
results for reference. To compare our model directly
with these systems we would need to incorporate ad-
ditional features and a language model, work which
we have left for a later date.
The relative scores confirm that our model, with
its minimalist feature set, achieves comparable per-
formance to the standard feature set without the lan-
guage model. This is encouraging as our model was
trained to optimise likelihood rather than BLEU, yet
it is still competitive on that metric. As expected,
the language model makes a significant difference to
BLEU, however we believe that this effect is orthog-
onal to the choice of base translation model, thus we
would expect a similar gain when integrating a lan-
guage model into the discriminative system.
An informal comparison of the outputs on the de-
velopment set, presented in Table 4, suggests that the
7Although the most direct comparison for the discriminative
model would be with pd model alone, omitting the gr, rc and
wc features and MERT training produces poor translations.
8Hiero (pd, pr, plexd , p
lex
r , gr, rc, wc, lm) represents state-
of-the-art performance on this training/testing set.
206
S: C?est pourquoi nous souhaitons que l?affaire nous soit ren-
voye?e.
R: We therefore want the matter re-referred to ourselves.
D: That is why we want the that matters we to be referred
back.
T: That is why we would like the matter to be referred back.
H: That is why we wish that the matter we be referred back.
S: Par contre, la transposition dans les E?tats membres reste
trop lente.
R: But implementation by the Member States has still been
too slow.
D: However, it is implemented in the Member States is still
too slow.
T: However, the implementation measures in Member States
remains too slow.
H: In against, transposition in the Member States remains too
slow.
S: Aussi, je conside`re qu?il reste e?norme?ment a` faire dans ce
domaine.
R: I therefore consider that there is an incredible amount still
to do in this area.
D: So I think remains a lot to be done in this field.
T: So I think there is still much to be done in this area.
H: Therefore, I think it remains a vast amount to do in this
area.
Table 4. Example output produced by the max-
derivation (D), max-translation (T) decoding algorithms
and Hiero(pd, pr, plexd , p
lex
r , gr, rc, wc) (H) models, relative
to the source (S) and reference (R).
translation optimising discriminative model more
often produces quite fluent translations, yet not in
ways that would lead to an increase in BLEU score.9
This could be considered a side-effect of optimising
likelihood rather than BLEU.
Scaling In Figure 6 we plot the scaling charac-
teristics of our models. The systems shown in the
graph use the full grammar extracted on the 170k
sentence corpus. The number of sentences upon
which the iterative training algorithm is used to esti-
mate the parameters is varied from 10k to the max-
imum 130K for which our model can reproduce the
reference translation. As expected, the more data
used to train the system, the better the performance.
However, as the performance is still increasing sig-
nificantly when all the parseable sentences are used,
it is clear that the system?s performance is suffering
from the large number (40k) of sentences that are
discarded before training.
5 Discussion and Further Work
We have shown that explicitly accounting for com-
peting derivations yields translation improvements.
9Hiero was MERT trained on this set and has a 2% higher
BLEU score compared to the discriminative model.
l
l
l
l
l
l
training sentences
deve
lopm
ent 
BLE
U (%
)
26
27
28
29
30
31
10k 25k 50k 75k 100k 130k
Figure 6. Learning curve showing that the model contin-
ues to improve as we increase the number of training sen-
tences (development set)
Our model avoids the estimation biases associated
with heuristic frequency count approaches and uses
standard regularisation techniques to avoid degener-
ate maximum likelihood solutions.
Having demonstrated the efficacy of our model
with very simple features, the logical next step is
to investigate more expressive features. Promising
features might include those over source side re-
ordering rules (Wang et al, 2007) or source con-
text features (Carpuat and Wu, 2007). Rule fre-
quency features extracted from large training cor-
pora would help the model to overcome the issue of
unreachable reference sentences. Such approaches
have been shown to be effective in log-linear word-
alignment models where only a small supervised
corpus is available (Blunsom and Cohn, 2006).
Finally, while in this paper we have focussed on
the science of discriminative machine translation,
we believe that with suitable engineering this model
will advance the state-of-the-art. To do so would
require integrating a language model feature into
the max-translation decoding algorithm. The use of
richer, more linguistic grammars (e.g., Galley et al
(2004)) may also improve the system.
Acknowledgements
The authors acknowledge the support of the EPSRC
(Blunsom & Osborne, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
207
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 65?72, Sydney, Aus-
tralia, July.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of the 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2007), pages 61?72, Prague, Czech Republic.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
Annual Meeting of the ACL (ACL-2005), pages 263?
270, Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proc. of the
42nd Annual Meeting of the ACL (ACL-2004), pages
103?110, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33(4).
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on Statistical Machine Translation, pages
31?38, New York City, June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. of
the 4th International Conference on Human Language
Technology Research and 5th Annual Meeting of the
NAACL (HLT-NAACL 2004), Boston, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st In-
ternational Conference on Computational Linguistics
(COLING/ACL-2006), pages 961?968, Sydney, Aus-
tralia, July.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57?64, Rochester, USA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc. of
the 3rd International Conference on Human Language
Technology Research and 4th Annual Meeting of the
NAACL (HLT-NAACL 2003), pages 81?88, Edmonton,
Canada, May.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465?488.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of the 44th An-
nual Meeting of the ACL and 21st International Con-
ference on Computational Linguistics (COLING/ACL-
2006), pages 761?768, Sydney, Australia, July.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
the 6th Conference on Natural Language Learning
(CoNLL-2002), pages 49?55, Taipei, Taiwan, August.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st An-
nual Meeting of the ACL (ACL-2003), pages 160?167,
Sapporo, Japan.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Discrim-
inative log-linear grammars with latent variables. In
Advances in Neural Information Processing Systems
20 (NIPS), Vancouver, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of the
3rd International Conference on Human Language
Technology Research and 4th Annual Meeting of the
NAACL (HLT-NAACL 2003), pages 134?141, Edmon-
ton, Canada.
Christoph Tillmann and Tong Zhang. 2007. A block bi-
gram prediction model for statistical machine transla-
tion. ACM Transactions Speech Language Processing,
4(3):6.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proc. of the 2007 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2007), pages 737?745, Prague, Czech Re-
public.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 764?773, Prague,
Czech Republic.
Benjamin Wellington, Joseph Turian, Chris Pike, and
I. Dan Melamed. 2006. Scalable purely-
discriminative training for word and tree transducers.
In Proc. of the 7th Biennial Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Boston, USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
208
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782?790,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Gibbs Sampler for Phrasal Synchronous Grammar Induction
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Trevor Cohn?
tcohn@inf.ed.ac.uk
Miles Osborne?
miles@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
We present a phrasal synchronous gram-
mar model of translational equivalence.
Unlike previous approaches, we do not
resort to heuristics or constraints from
a word-alignment model, but instead
directly induce a synchronous grammar
from parallel sentence-aligned corpora.
We use a hierarchical Bayesian prior
to bias towards compact grammars with
small translation units. Inference is per-
formed using a novel Gibbs sampler
over synchronous derivations. This sam-
pler side-steps the intractability issues of
previous models which required inference
over derivation forests. Instead each sam-
pling iteration is highly efficient, allowing
the model to be applied to larger transla-
tion corpora than previous approaches.
1 Introduction
The field of machine translation has seen many
advances in recent years, most notably the shift
from word-based (Brown et al, 1993) to phrase-
based models which use token n-grams as trans-
lation units (Koehn et al, 2003). Although very
few researchers use word-based models for trans-
lation per se, such models are still widely used in
the training of phrase-based models. These word-
based models are used to find the latent word-
alignments between bilingual sentence pairs, from
which a weighted string transducer can be induced
(either finite state (Koehn et al, 2003) or syn-
chronous context free grammar (Chiang, 2007)).
Although wide-spread, the disconnect between the
translation model and the alignment model is arti-
ficial and clearly undesirable. Word-based mod-
els are incapable of learning translational equiv-
alences between non-compositional phrasal units,
while the algorithms used for inducing weighted
transducers from word-alignments are based on
heuristics with little theoretical justification. A
model which can fulfil both roles would address
both the practical and theoretical short-comings of
the machine translation pipeline.
The machine translation literature is littered
with various attempts to learn a phrase-based
string transducer directly from aligned sentence
pairs, doing away with the separate word align-
ment step (Marcu and Wong, 2002; Cherry and
Lin, 2007; Zhang et al, 2008b; Blunsom et al,
2008). Unfortunately none of these approaches
resulted in an unqualified success, due largely
to intractable estimation. Large training sets with
hundreds of thousands of sentence pairs are com-
mon in machine translation, leading to a parameter
space of billions or even trillions of possible bilin-
gual phrase-pairs. Moreover, the inference proce-
dure for each sentence pair is non-trivial, prov-
ing NP-complete for learning phrase based models
(DeNero and Klein, 2008) or a high order poly-
nomial (O(|f |3|e|3))1 for a sub-class of weighted
synchronous context free grammars (Wu, 1997).
Consequently, for such models both the param-
eterisation and approximate inference techniques
are fundamental to their success.
In this paper we present a novel SCFG transla-
tion model using a non-parametric Bayesian for-
mulation. The model includes priors to impose a
bias towards small grammars with few rules, each
of which is as simple as possible (e.g., terminal
productions consisting of short phrase pairs). This
explicitly avoids the degenerate solutions of max-
imum likelihood estimation (DeNero et al, 2006),
without resort to the heuristic estimator of Koehn
et al (2003). We develop a novel Gibbs sampler
to perform inference over the latent synchronous
derivation trees for our training instances. The
sampler reasons over the infinite space of possi-
ble translation units without recourse to arbitrary
restrictions (e.g., constraints drawn from a word-
alignment (Cherry and Lin, 2007; Zhang et al,
2008b) or a grammar fixed a priori (Blunsom et al,
1f and e are the input and output sentences respectively.
782
2008)). The sampler performs local edit operations
to nodes in the synchronous trees, each of which
is very fast, leading to a highly efficient inference
technique. This allows us to train the model on
large corpora without resort to punitive length lim-
its, unlike previous approaches which were only
applied to small data sets with short sentences.
This paper is structured as follows: In Sec-
tion 3 we argue for the use of efficient sam-
pling techniques over SCFGs as an effective solu-
tion to the modelling and scaling problems of
previous approaches. We describe our Bayesian
SCFG model in Section 4 and a Gibbs sampler
to explore its posterior. We apply this sampler
to build phrase-based and hierarchical translation
models and evaluate their performance on small
and large corpora.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) generalizes context-
free grammars to generate strings concurrently in
two (or more) languages. A string pair is gener-
ated by applying a series of paired rewrite rules
of the form, X ? ?e, f ,a?, where X is a non-
terminal, e and f are strings of terminals and non-
terminals and a specifies a one-to-one alignment
between non-terminals in e and f . In the context of
SMT, by assigning the source and target languages
to the respective sides of a probabilistic SCFG it
is possible to describe translation as the process
of parsing the source sentence, which induces a
parallel tree structure and translation in the tar-
get language (Chiang, 2007). Figure 1 shows an
example derivation for Japanese to English trans-
lation using an SCFG. For efficiency reasons we
only consider binary or ternary branching rules
and don?t allow rules to mix terminals and non-
terminals. This allows our sampler to more effi-
ciently explore the space of grammars (Section
4.2), however more expressive grammars would be
a straightforward extension of our model.
3 Related work
Most machine translation systems adopt the
approach of Koehn et al (2003) for ?training?
a phrase-based translation model.2 This method
starts with a word-alignment, usually the latent
state of an unsupervised word-based aligner such
2We include grammar based transducers, such as Chiang
(2007) and Marcu et al (2006), in our definition of phrase-
based models.
Grammar fragment:
X ? ?X
1
X
2
X
3
, X
1
X
3
X
2
?
X ? ?John-ga, John?
X ? ?ringo-o, an apple?
X ? ?tabeta, ate?
Sample derivation:
?S
1
,S
1
? ? ?X
2
, X
2
?
? ?X
3
X
4
X
5
, X
3
X
5
X
4
?
? ?John-ga X
4
X
5
, John X
5
X
4
?
? ?John-ga ringo-o X
5
, John X
5
an apple?
? ?John-ga ringo-o tabeta, John ate an apple?
Figure 1: A fragment of an SCFG with a ternary
non-terminal expansion and three terminal rules.
as GIZA++. Various heuristics are used to com-
bine source-to-target and target-to-source align-
ments, after which a further heuristic is used to
read off phrase pairs which are ?consistent? with
the alignment. Although efficient, the sheer num-
ber of somewhat arbitrary heuristics makes this
approach overly complicated.
A number of authors have proposed alterna-
tive techniques for directly inducing phrase-based
translation models from sentence aligned data.
Marcu and Wong (2002) proposed a phrase-based
alignment model which suffered from a massive
parameter space and intractable inference using
expectation maximisation. Taking a different tack,
DeNero et al (2008) presented an interesting new
model with inference courtesy of a Gibbs sampler,
which was better able to explore the full space of
phrase translations. However, the efficacy of this
model is unclear due to the small-scale experi-
ments and the short sampling runs. In this work we
also propose a Gibbs sampler but apply it to the
polynomial space of derivation trees, rather than
the exponential space of the DeNero et al (2008)
model. The restrictions imposed by our tree struc-
ture make sampling considerably more efficient
for long sentences.
Following the broad shift in the field from finite
state transducers to grammar transducers (Chiang,
2007), recent approaches to phrase-based align-
ment have used synchronous grammar formalisms
permitting polynomial time inference (Wu, 1997;
783
Cherry and Lin, 2007; Zhang et al, 2008b; Blun-
som et al, 2008). However this asymptotic time
complexity is of high enough order (O(|f |3|e|3))
that inference is impractical for real translation
data. Proposed solutions to this problem include
imposing sentence length limits, using small train-
ing corpora and constraining the search space
using a word-alignment model or parse tree. None
of these limitations are particularly desirable as
they bias inference. As a result phrase-based align-
ment models are not yet practical for the wider
machine translation community.
4 Model
Our aim is to induce a grammar from a train-
ing set of sentence pairs. We use Bayes? rule
to reason under the posterior over grammars,
P (g|x) ? P (x|g)P (g), where g is a weighted
SCFG grammar and x is our training corpus. The
likelihood term, P (x|g), is the probability of the
training sentence pairs under the grammar, while
the prior term, P (g), describes our initial expec-
tations about what consitutes a plausible gram-
mar. Specifically we incorporate priors encoding
our preference for a briefer and more succinct
grammar, namely that: (a) the grammar should be
small, with few rules rewriting each non-terminal;
and (b) terminal rules which specify phrasal trans-
lation correspondence should be small, with few
symbols on their right hand side.
Further, Bayesian non-parametrics allow the
capacity of the model to grow with the data.
Thereby we avoid imposing hard limits on the
grammar (and the thorny problem of model selec-
tion), but instead allow the model to find a gram-
mar appropriately sized for its training data.
4.1 Non-parametric form
Our Bayesian model of SCFG derivations resem-
bles that of Blunsom et al (2008). Given a gram-
mar, each sentence is generated as follows. Start-
ing with a root non-terminal (z1), rewrite each
frontier non-terminal (zi) using a rule chosen from
our grammar expanding zi. Repeat until there are
no remaining frontier non-terminals. This gives
rise to the following derivation probability:
p(d) = p(z1)
?
ri?d
p(ri|zi)
where the derivation is a sequence of rules d =
(r1, . . . , rn), and zi denotes the root node of ri.
We allow two types of rules: non-terminal and
terminal expansions. The former rewrites a non-
terminal symbol as a string of two or three non-
terminals along with an alignment, specifying
the corresponding ordering of the child trees in
the source and target language. Terminal expan-
sions rewrite a non-terminal as a pair of terminal
n-grams, representing a phrasal translation pair,
where either but not both may be empty.
Each rule in the grammar, ri, is generated from
its root symbol, zi, by first choosing a rule type
ti ? {TERM, NON-TERM} from a Bernoulli distribu-
tion, ri ? Bernoulli(?). We treat ? as a random
variable with its own prior, ? ? Beta(?R, ?R) and
integrate out the parameters, ?. This results in the
following conditional probability for ti:
p(ti|r?i, zi, ?R) =
n?iti,zi + ?
R
n?i?,zi + 2?R
where n?iri,zi is the number of times ri has been
used to rewrite zi in the set of all other rules, r?i,
and n?i?,zi =
?
r n
?i
r,zi is the total count of rewriting
zi. The Dirichlet (and thus Beta) distribution are
exchangeable, meaning that any permutation of its
events are equiprobable. This allows us to reason
about each event given previous and subsequent
events (i.e., treat each item as the ?last?.)
When ti = NON-TERM, we generate a binary
or ternary non-terminal production. The non-
terminal sequence and alignment are drawn from
(z, a) ? ?Nzi and, as before, we define a prior over
the parameters, ?Nzi ? Dirichlet(?
T ), and inte-
grate out ?Nzi . This results in the conditional prob-
ability:
p(ri|ti = NON-TERM, r?i, zi, ?N ) =
nN,?iri,zi + ?
N
nN,?i?,zi + |N |?N
where nN,?iri,zi is the count of rewriting zi with non-
terminal rule ri, n
N,?i
?,zi the total count over all non-
terminal rules and |N | is the number of unique
non-terminal rules.
For terminal productions (ti = TERM) we first
decide whether to generate a phrase in both lan-
guages or in one language only, according to a
fixed probability pnull.3 Contingent on this deci-
sion, the terminal strings are then drawn from
3To discourage null alignments, we used pnull = 10?10
for this value in the experiments we report below.
784
either ?Pzi for phrase pairs or ?
null for single lan-
guage phrases. We choose Dirichlet process (DP)
priors for these parameters:
?Pzi ? DP(?
P , PP1 )
?nullzi ? DP(?
null, Pnull1 )
where the base distributions, PP1 and P
null
1 , range
over phrase pairs or monolingual phrases in either
language, respectively.
The most important choice for our model is
the priors on the parameters of these terminal
distributions. Phrasal SCFG models are subject
to a degenerate maximum likelihood solution in
which all probability mass is placed on long, or
whole sentence, phrase translations (DeNero et al,
2006). Therefore, careful consideration must be
given when specifying the P1 distribution on ter-
minals in order to counter this behavior.
To construct a prior over string pairs, first we
define the probability of a monolingual string (s):
PX0 (s) = PPoisson(|s|; 1)?
1
V |s|X
where the PPoisson(k; 1) is the probability under a
Poisson distribution of length k given an expected
length of 1, while VX is the vocabulary size of
language X . This distribution has a strong bias
towards short strings. In particular note that gener-
ally a string of length k will be less probable than
two of length k2 , a property very useful for finding
?minimal? translation units. This contrasts with a
geometric distribution in which a string of length
k will be more probable than its segmentations.
We define Pnull1 as the string probability of the
non-null part of the rule:
Pnull1 (z ? ?e, f?) =
{ 1
2P
E
0 (e) if |f | = 0
1
2P
F
0 (f) if |e| = 0
The terminal translation phrase pair distribution
is a hierarchical Dirichlet Process in which each
phrase are independently distributed according to
DPs:4
PP1 (z ? ?e, f?) = ?
E
z (e)? ?
F
z (f)
?Ez ? DP(?
PE , PE0 )
4This prior is similar to one used by DeNero et al (2008),
who used the expected table count approximation presented
in Goldwater et al (2006). However, Goldwater et al (2006)
contains two major errors: omitting P0, and using the trun-
cated Taylor series expansion (Antoniak, 1974) which fails
for small ?P0 values common in these models. In this work
we track table counts directly.
and ?Fz is defined analogously. This prior encour-
ages frequent phrases to participate in many differ-
ent translation pairs. Moreover, as longer strings
are likely to be less frequent in the corpus this has
a tendency to discourage long translation units.
4.2 A Gibbs sampler for derivations
Markov chain Monte Carlo sampling allows us to
perform inference for the model described in 4.1
without restricting the infinite space of possible
translation rules. To do this we need a method for
sampling a derivation for a given sentence pair
from p(d|d?). One possible approach would be
to first build a packed chart representation of the
derivation forest, calculate the inside probabilities
of all cells in this chart, and then sample deriva-
tions top-down according to their inside probabil-
ities (analogous to monolingual parse tree sam-
pling described in Johnson et al (2007)). A prob-
lem with this approach is that building the deriva-
tion forest would take O(|f |3|e|3) time, which
would be impractical for long sentences.
Instead we develop a collapsed Gibbs sam-
pler (Teh et al, 2006) which draws new sam-
ples by making local changes to the derivations
used in a previous sample. After a period of burn
in, the derivations produced by the sampler will
be drawn from the posterior distribution, p(d|x).
The advantage of this algorithm is that we only
store the current derivation for each training sen-
tence pair (together these constitute the state of
the sampler), but never need to reason over deriva-
tion forests. By integrating over (collapsing) the
parameters we only store counts of rules used
in the current sampled set of derivations, thereby
avoiding explicitly representing the possibly infi-
nite space of translation pairs.
We define two operators for our Gibbs sam-
pler, each of which re-samples local derivation
structures. Figures 2 and 4 illustrate the permu-
tations these operators make to derivation trees.
The omitted tree structure in these figures denotes
the Markov blanket of the operator: the structure
which is held constant when enumerating the pos-
sible outcomes for an operator.
The Split/Join operator iterates through the
positions between each source word sampling
whether a terminal boundary should exist at
that position (Figure 2). If the source position
785
... ... ...
... ...
... ...
... ...
Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The
dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a
solid line is a null alignment.
...
......
...
...
......
...
Figure 4: Rule insert/delete sampler. A pair of
adjacent nodes in a ternary rule can be re-parented
as a binary rule, or vice-versa.
falls between two existing terminals whose tar-
get phrases are adjacent, then any new target seg-
mentation within those target phrases can be sam-
pled, including null alignments. If the two exist-
ing terminals also share the same parent, then any
possible re-ordering is also a valid outcome, as
is removing the terminal boundary to form a sin-
gle phrase pair. Otherwise, if the visited boundary
point falls within an existing terminal, then all tar-
get split and re-orderings are possible outcomes.
The probability for each of these configurations
is evaluated (see Figure 3) from which the new
configuration is sampled.
While the first operator is theoretically capa-
ble of exploring the entire derivation forest (by
flattening the tree into a single phrase and then
splitting), the series of moves required would be
highly improbable. To allow for faster mixing we
employ the Insert/Delete operator which adds and
deletes the parent non-terminal of a pair of adja-
cent nodes. This is illustrated in Figure 4. The
update equations are analogous to those used for
the Split/Join operator in Figure 3. In order for this
operator to be effective we need to allow greater
than binary branching nodes, otherwise deleting a
nodes would require sampling from a much larger
set of outcomes. Hence our adoption of a ternary
branching grammar. Although such a grammar
would be very inefficient for a dynamic program-
ming algorithm, it allows our sampler to permute
the internal structure of the trees more easily.
4.3 Hyperparameter Inference
Our model is parameterised by a vector of hyper-
parameters, ? = (?R, ?N , ?P , ?PE , ?PF , ?null),
which control the sparsity assumption over var-
ious model parameters. We could optimise each
concentration parameter on the training corpus by
hand, however this would be quite an onerous task.
Instead we perform inference over the hyperpa-
rameters following Goldwater and Griffiths (2007)
by defining a vague gamma prior on each con-
centration parameter, ?x ? Gamma(10?4, 104).
This hyper-prior is relatively benign, allowing the
model to consider a wide range of values for
the hyperparameter. We sample a new value for
each ?x using a log-normal distribution with mean
?x and variance 0.3, which is then accepted into
the distribution p(?x|d, ??) using the Metropolis-
Hastings algorithm. Unlike the Gibbs updates, this
calculation cannot be distributed over a cluster
(see Section 4.4) and thus is very costly. Therefore
for small corpora we re-sample the hyperparame-
ter after every pass through the corpus, for larger
experiments we only re-sample every 20 passes.
4.4 A Distributed approximation
While employing a collapsed Gibbs sampler
allows us to efficiently perform inference over the
786
p(JOIN) ? p(ti = TERM|zi, r?)? p(ri = (zi ? ?e, f?)|zi, r?) (1)
p(SPLIT) ? p(ti = NON-TERM|zi, r?)? p(ri = (zi ? ?zl, zr, ai?)|zi, r
?) (2)
? p(tl = TERM|ti, zi, r
?)? p(rl = (zl ? ?el, fl?)|zl, r
?)
? p(tr = TERM|ti, tl, zi, r
?)? p(rr = (zr ? ?er, fr?)|zl, r
? ? (zl ? ?el, fl?))
Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in
Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the
choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered).
massive space of possible grammars, it induces
dependencies between all the sentences in the
training corpus. These dependencies make it diffi-
cult to scale our approach to larger corpora by dis-
tributing it across a number of processors. Recent
work (Newman et al, 2007; Asuncion et al, 2008)
suggests that good practical parallel performance
can be achieved by having multiple processors
independently sample disjoint subsets of the cor-
pus. Each process maintains a set of rule counts for
the entire corpus and communicates the changes
it has made to its section of the corpus only
after sampling every sentence in that section. In
this way each process is sampling according to
a slightly ?out-of-date? distribution. However, as
we confirm in Section 5 the performance of this
approximation closely follows the exact collapsed
Gibbs sampler.
4.5 Extracting a translation model
Although we could use our model directly as a
decoder to perform translation, its simple hier-
archical reordering parameterisation is too weak
to be effective in this mode. Instead we use our
sampler to sample a distribution over translation
models for state-of-the-art phrase based (Moses)
and hierarchical (Hiero) decoders (Koehn et al,
2007; Chiang, 2007). Each sample from our model
defines a hierarchical alignment on which we can
apply the standard extraction heuristics of these
models. By extracting from a sequence of samples
we can directly infer a distribution over phrase
tables or Hiero grammars.
5 Evaluation
Our evaluation aims to determine whether the
phrase/SCFG rule distributions created by sam-
pling from the model described in Section 4
impact upon the performance of state-of-the-
art translation systems. We conduct experiments
translating both Chinese (high reordering) and
Arabic (low reordering) into English. We use the
GIZA++ implementation of IBM Model 4 (Brown
et al, 1993; Och and Ney, 2003) coupled with the
phrase extraction heuristics of Koehn et al (2003)
and the SCFG rule extraction heuristics of Chiang
(2007) as our benchmark. All the SCFG models
employ a single X non-terminal, we leave experi-
ments with multiple non-terminals to future work.
Our hypothesis is that our grammar based
induction of translation units should benefit lan-
guage pairs with significant reordering more than
those with less. While for mostly monotone trans-
lation pairs, such as Arabic-English, the bench-
mark GIZA++-based system is well suited due to
its strong monotone bias (the sequential Markov
model and diagonal growing heuristic).
We conduct experiments on both small and
large corpora to allow a range of alignment quali-
ties and also to verify the effectiveness of our dis-
tributed approximation of the Bayesian inference.
The samplers are initialised with trees created
from GIZA++ Model 4 alignments, altered such
that they are consistent with our ternary grammar.
This is achieved by using the factorisation algo-
rithm of Zhang et al (2008a) to first create ini-
tial trees. Where these factored trees contain nodes
with mixed terminals and non-terminals, or more
than three non-terminals, we discard alignment
points until the node factorises correctly. As the
alignments contain many such non-factorisable
nodes, these trees are of poor quality. However,
all samplers used in these experiments are first
?burnt-in? for 1000 full passes through the data.
This allows the sampler to diverge from its ini-
tialisation condition, and thus gives us confidence
that subsequent samples will be drawn from the
posterior. An expectation over phrase tables and
Hiero grammars is built from every 50th sample
after the burn-in, up until the 1500th sample.
We evaluate the translation models using IBM
BLEU (Papineni et al, 2001). Table 1 lists the
statistics of the corpora used in these experiments.
787
IWSLT NIST
English?Chinese English?Chinese English?Arabic
Sentences 40k 300k 290k
Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M
Av. Sent. Len. 9 8 36 28 32 29
Longest Sent. 75 64 80 80 80 80
Table 1: Corpora statistics.
System Test 05
Moses (Heuristic) 47.3
Moses (Bayes SCFG) 49.6
Hiero (Heuristic) 48.3
Hiero (Bayes SCFG) 51.8
Table 2: IWSLT Chinese to English translation.
5.1 Small corpus
Firstly we evaluate models trained on a small
Chinese-English corpus using a Gibbs sampler on
a single CPU. This corpus consists of transcribed
utterances made available for the IWSLT work-
shop (Eck and Hori, 2005). The sparse counts and
high reordering for this corpus means the GIZA++
model produces very poor alignments.
Table 2 shows the results for the benchmark
Moses and Hiero systems on this corpus using
both the heuristic phrase estimation, and our pro-
posed Bayesian SCFG model. We can see that
our model has a slight advantage. When we look
at the grammars extracted by the two models we
note that the SCFG model creates considerably
more translation rules. Normally this would sug-
gest the alignments of the SCFG model are a lot
sparser (more unaligned tokens) than those of the
heuristic, however this is not the case. The pro-
jected SCFG derivations actually produce more
alignment points. However these alignments are
much more locally consistent, containing fewer
spurious off-diagonal alignments, than the heuris-
tic (see Figure 5), and thus produce far more valid
phrases/rules.
5.2 Larger corpora
We now test our model?s performance on a larger
corpus, representing a realistic SMT experiment
with millions of words and long sentences. The
Chinese-English training data consists of the FBIS
corpus (LDC2003E14) and the first 100k sen-
tences from the Sinorama corpus (LDC2005E47).
The Arabic-English training data consists of
the eTIRR corpus (LDC2004E72), the Arabic
l
l
l
l
l
l
l
l
l l
l l
Number of Sampling Passes
Negative 
Log?Post
erior
l l
l
l
l
l
l
l l
l l
476
478
480
482
484
486
488
490
20 40 60 80 100 120 140 160 180 200 220 240
single (exact)distributed
Figure 6: The posterior for the single CPU sampler
and distributed approximation are roughly equiva-
lent over a sampling run.
news corpus (LDC2004T17), the Ummah cor-
pus (LDC2004T18), and the sentences with confi-
dence c > 0.995 in the ISI automatically extracted
web parallel corpus (LDC2006T02). The Chinese
text was segmented with a CRF-based Chinese
segmenter optimized for MT (Chang et al, 2008).
The Arabic text was preprocessed according to the
D2 scheme of Habash and Sadat (2006), which
was identified as optimal for corpora this size. The
parameters of the NIST systems were tuned using
Och?s algorithm to maximize BLEU on the MT02
test set (Och, 2003).
To evaluate whether the approximate distributed
inference algorithm described in Section 4.4 is
effective, we compare the posterior probability of
the training corpus when using a single machine,
and when the inference is distributed on an eight
core machine. Figure 6 plots the mean posterior
and standard error for five independent runs for
each scenario. Both sets of runs performed hyper-
parameter inference every twenty passes through
the data. It is clear from the training curves that the
distributed approximation tracks the corpus prob-
ability of the correct sampler sufficiently closely.
This concurs with the findings of Newman et al
788
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(a) Giza++
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(b) Gibbs
Figure 5: Alignment example. The synchronous tree structure is shown for (b) using brackets to indicate
constituent spans; these are omitted for single token constituents. The right alignment is roughly correct,
except that ?of? and ?an? should be left unaligned (? ?to be? is missing from the English translation).
System MT03 MT04 MT05
Moses (Heuristic) 26.2 30.0 25.3
Moses (Bayes SCFG) 26.4 30.2 25.8
Hiero (Heuristic) 26.4 30.8 25.4
Hiero (Bayes SCFG) 26.7 30.9 26.0
Table 3: NIST Chinese to English translation.
System MT03 MT04 MT05
Moses (Heuristic) 48.5 43.9 49.2
Moses (Bayes SCFG) 48.5 43.5 48.7
Hiero (Heuristic) 48.1 43.5 48.4
Hiero (Bayes SCFG) 48.4 43.4 47.7
Table 4: NIST Arabic to English translation.
(2007) who also observed very little empirical dif-
ference between the sampler and its distributed
approximation.
Tables 3 and 4 show the result on the two NIST
corpora when running the distributed sampler on
a single 8-core machine.5 These scores tally with
our initial hypothesis: that the hierarchical struc-
ture of our model suits languages that exhibit less
monotone reordering.
Figure 5 shows the projected alignment of a
headline from the thousandth sample on the NIST
Chinese data set. The effect of the grammar based
alignment can clearly be seen. Where the combi-
nation of GIZA++ and the heuristics creates out-
lier alignments that impede rule extraction, the
SCFG imposes a more rigid hierarchical struc-
ture on the alignments. We hypothesise that this
property may be particularly useful for syntac-
tic translation models which often have difficulty
5Producing the 1.5K samples for each experiment took
approximately one day.
with inconsistent word alignments not correspond-
ing to syntactic structure.
The combined evidence of the ability of our
Gibbs sampler to improve posterior likelihood
(Figure 6) and our translation experiments demon-
strate that we have developed a scalable and effec-
tive method for performing inference over phrasal
SCFG, without compromising the strong theoreti-
cal underpinnings of our model.
6 Discussion and Conclusion
We have presented a Bayesian model of SCFG
induction capable of capturing phrasal units of
translational equivalence. Our novel Gibbs sam-
pler over synchronous derivation trees can effi-
ciently draw samples from the posterior, overcom-
ing the limitations of previous models when deal-
ing with long sentences. This avoids explicitly
representing the full derivation forest required by
dynamic programming approaches, and thus we
are able to perform inference without resorting to
heuristic restrictions on the model.
Initial experiments suggest that this model per-
forms well on languages for which the monotone
bias of existing alignment and heuristic phrase
extraction approaches fail. These results open the
way for the development of more sophisticated
models employing grammars capable of capturing
a wide range of translation phenomena. In future
we envision it will be possible to use the tech-
niques developed here to directly induce gram-
mars which match state-of-the-art decoders, such
as Hiero grammars or tree substitution grammars
of the form used by Galley et al (2004).
789
Acknowledgements
The authors acknowledge the support of
the EPSRC (Blunsom & Osborne, grant
EP/D074959/1; Cohn, grant GR/T04557/01)
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001 (Dyer).
References
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
A. Asuncion, P. Smyth, M. Welling. 2008. Asynchronous
distributed learning of topic models. In NIPS. MIT Press.
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian syn-
chronous grammar induction. In Proceedings of NIPS 21,
Vancouver, Canada.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, R. L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics,
19(2):263?311.
P.-C. Chang, D. Jurafsky, C. D. Manning. 2008. Optimizing
Chinese word segmentation for machine translation per-
formance. In Proc. of the Third Workshop on Machine
Translation, Prague, Czech Republic.
C. Cherry, D. Lin. 2007. Inversion transduction grammar for
joint phrasal translation modeling. In Proc. of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST 2007), Rochester, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, D. Klein. 2008. The complexity of phrase align-
ment problems. In Proceedings of ACL-08: HLT, Short
Papers, 25?28, Columbus, Ohio. Association for Compu-
tational Linguistics.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006. Why gener-
ative phrase models underperform surface heuristics. In
Proc. of the HLT-NAACL 2006 Workshop on Statistical
Machine Translation, 31?38, New York City.
J. DeNero, A. Bouchard-Co?te?, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
M. Eck, C. Hori. 2005. Overview of the IWSLT 2005 eval-
uation campaign. In Proc. of the International Workshop
on Spoken Language Translation, Pittsburgh.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004. What?s
in a translation rule? In Proc. of the 4th International Con-
ference on Human Language Technology Research and
5th Annual Meeting of the NAACL (HLT-NAACL 2004),
Boston, USA.
S. Goldwater, T. Griffiths. 2007. A fully bayesian approach
to unsupervised part-of-speech tagging. In Proc. of the
45th Annual Meeting of the ACL (ACL-2007), 744?751,
Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
N. Habash, F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In Proc. of the 6th
International Conference on Human Language Technol-
ogy Research and 7th Annual Meeting of the NAACL
(HLT-NAACL 2006), New York City. Association for
Computational Linguistics.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo. In
Proc. of the 7th International Conference on Human Lan-
guage Technology Research and 8th Annual Meeting of the
NAACL (HLT-NAACL 2007), 139?146, Rochester, New
York.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the 3rd International Con-
ference on Human Language Technology Research and
4th Annual Meeting of the NAACL (HLT-NAACL 2003),
81?88, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,
C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses:
Open source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
P. M. Lewis II, R. E. Stearns. 1968. Syntax-directed trans-
duction. J. ACM, 15(3):465?488.
D. Marcu, W. Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Proc. of the
2002 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 133?139, Philadelphia.
Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006. SPMT:
Statistical machine translation with syntactified target lan-
guage phrases. In Proc. of the 2006 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
2006), 44?52, Sydney, Australia.
D. Newman, A. Asuncion, P. Smyth, M. Welling. 2007.
Distributed inference for latent dirichlet alocation. In
NIPS. MIT Press.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?52.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41st Annual Meeting
of the ACL (ACL-2003), 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a
method for automatic evaluation of machine translation,
2001.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
H. Zhang, D. Gildea, D. Chiang. 2008a. Extracting syn-
chronous grammar rules from word-level alignments in
linear time. In Proc. of the 22th International Con-
ference on Computational Linguistics (COLING-2008),
1081?1088, Manchester, UK.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008b.
Bayesian learning of non-compositional phrases with syn-
chronous parsing. In Proc. of the 46th Annual Conference
of the Association for Computational Linguistics: Human
Language Technologies (ACL-08:HLT), 97?105, Colum-
bus, Ohio.
790
In: Proceedings o/CoNLL-2000 and LLL-2000, pages 49-54, Lisbon, Portugal, 2000. 
Overfitting Avoidance for Stochastic Modeling of 
Attribute-Value Grammars 
Tony  Mu l len  
Alfa-Informatica 
University of Groningen 
mullen@let, rug. nl 
Mi les  Osborne  
Division of Informatics 
University of Edinburgh 
osborne@cogsci, ed. ac. uk 
Abst rac t  
We present a novel approach to the problem 
of overfitting in the training of stochastic mod- 
els for selecting parses generated by attribute- 
valued grammars. In this approach, statistical 
features are merged according to the frequency 
of linguistic elements within the features. The 
resulting models are more general than the orig- 
inal models, and contain fewer parameters. Em- 
pirical results from the task of parse selection 
suggest hat the improvement in performance 
over repeated iterations of iterative scaling is 
more reliable with such generalized models than 
with ungeneralized models. 
1 In t roduct ion  
The maximum entropy technique of statistical 
modeling using random fields has proved to be 
an effective way of dealing with a variety of lin- 
guistic phenomena, in particular where mod- 
eling of attribute-valued grammars (AVG's) is 
concerned (Abney, 1997). This is largely be- 
cause its capacity for considering overlapping 
information sources allows the most to be made 
of situations where data is sparse. Neverthe- 
less, it is important that the statistical features 
employed be appropriate to the job. If the infor- 
mation contributed by the features is too spe- 
cific to the training data, overfitting becomes a
problem (Chen and Rosenfeld, 1999; Osborne, 
2000). In this event, a peak in model perfor- 
mance will be reached early on, and continued 
training yields progressive deterioration i per- 
formance. From a theoretical standpoint, over- 
fitting indicates that the model distribution is 
unrepresentative of the actual probabilities. In 
practice, it makes the performance of the model 
dependent upon early stopping of training. The 
point at which this must be done is not always 
reliably predictable. 
This paper describes an approach to feature 
selection for maximum entropy models which 
reduces the effects of overfitting. Candidate fea- 
tures are built up from basic grammatical el- 
ements found in the corpus. This "composi- 
tional" quality of the features is exploited for 
the purpose of overfitting reduction by means 
of \]eature merging. In this process, features 
which are similar to each other, save for cer- 
tain elements, are merged; i.e, their disjunc- 
tion is considered as a feature in itself, thus 
reducing the number of features in the model. 
The motivation behind this methodology is sim- 
ilar to that behind that of Kohavi and John 
(1997), but rather than seeking a proper subset 
of the candidate feature set, the merging pro- 
cedure attempts to compress the feature set, 
diminishing both noise and redundancy. The 
method iffers from a simple feature cutoff, such 
as that described in Ratnaparkhi (1998), in 
that the feature cutoff eliminates statistical fea- 
tures directly, whereas the merging procedure 
attempts to generalize them. The method em- 
ployed here also derives inspiration from the no- 
tion of Bayesian model merging introduced by 
Stolcke and Omohundro (1994). 
Section 2 describes parse selection and dis- 
cusses the "compositional" statistical features 
employed in a maximum entropy approach to 
the task. Section 3 introduces the notion of fea- 
ture merging and discusses its relationship with 
overfitting reduction. Sections 4 and 5 describe 
the experimental models built and the results of 
merging on their performance. Finally, section 
6 sums up briefly and indicates ome further di- 
rections for inquiry on the subject. 
49 
2 Max imum ent ropy-based  parse  
se lec t ion  
The task of parse selection involves electing the 
best possible parse for a sentence from a set 
of possible parses produced by an AVG. In the 
present approach, parses are ranked according 
to their goodness by a statistical model built us- 
ing the maximum entropy technique, which in- 
volves building a distribution over events which 
is the most uniform possible, given constraints 
derived from training data. Events are com- 
posed of features, the fundamental statistical 
units whose distribution is modeled. The model 
is characterized by constraints upon the dis- 
tributions of features, derived from the fea- 
tures' empirical frequencies. An untrained (thus 
unconstrained) max ent model is by defini- 
tion characterized by the uniform distribution. 
The constraints which characterize the model 
are expressed as weights on individual features. 
Training the model involves deriving the best 
weights from the training data by means of 
an algorithm such as Improved Iterative Scaling 
(IIS) (Della Pietra et al, 1995). 
IIS assigns weights to features which reflect 
their distribution and significance. With each 
iteration, these weights reflect he empirical dis- 
tribution of the features in the training data 
with increasing accuracy. In ideal circum- 
stances, where the distribution of features in 
the training data accurately represents he true 
probability of the features, the performance of
the model should increase asymptotically with 
each iteration of training until it eventually con- 
verges. If the training data is corrupt, or noisy, 
or if it contains features which are too sparsely 
distributed to accurately represent their proba- 
bility, then overfitting arises. 
2.1 The s t ructure  of the features 
The statistical features used for parse selection 
should contain information pertinent o sen- 
tence structure, as it is the information encoded 
in these features which will be brought o bear 
in prefering one parse over another. Information 
regarding constituent heads, POS tags, and lex- 
ical information is pertinent, as is information 
on constituent ordering and other grammatical 
information present in the data. Most or all 
of these factors are considered in some form 
or another by current state-of-the-art statisti- 
cal parsers such as those of Charniak (1997), 
Magerman (1995) and Collins (1996). 
In the present approach, each feature in the 
feature set corresponds to a depth-one tree 
structure in the data, i.e. a mother node and 
all of its daughters. Within this general struc- 
ture various schemata may be used to derive ac- 
tual features, where the information about each 
node employed in the feature is determined by 
which schema is used. For example, one schema 
might call for POS information from all nodes 
and lexical information only from head nodes. 
Another might call for lexical information only 
from nodes which also contain the POS tag for 
prepositions. The term compositional is used 
in this context o describe features built up ac- 
cording to some such schema from basic linguis- 
tic elements uch as these. Thus each compo- 
sitional feature is an ordered sequence of ele- 
ments, where the order reflects the position in 
the tree of the elements. Instantiations of these 
schemata in the data are used as the statistical 
features. The first step is to run a given schema 
over the data, collecting a set of features. The 
next step is to characterize all events in the data 
in terms of those features. 
This general structure for features allows con- 
siderable versatility; models of widely varying 
quality may be constructed. This structure for 
statistical features might be compared with the 
Data-Oriented Parsing (DOP) of Bod (1998) in 
that it considers ubtrees of parses as the struc- 
tural units from which statistical information 
is taken. The present approach differs sharply 
from DOP in that its trees are limited to a depth 
of one node below the mother and, more impor- 
tantly, in the fact that the maximum entropy 
framework allows modeling without the inde- 
pendence assumptions made in DOP. 
Since maximum entropy allows for overlap- 
ping information sources, features derived using 
different schemata (that is, collecting different 
pieces of node-specific information) may be col- 
lected from the same subtrees, and used simul- 
taneously in a single model. 
3 Feature  merging and over f i t t ing  
reduction 
The idea behind feature merging is to reduce 
overfitting through changes made directly to 
the model. This is done by combining highly 
50 
VVD\] 
\ [~VD\ ]  \[bIPl\] \[~IN1\] 
offered\] 
U 
\[VVD\] 
V+VD \[?qP 1\] \[IqN 1\] 
allow J 
\[oVffVerDed V allow\] \[IqP1\] \[~qN1\] 
Figure 1: An example of feature merging. The 
top two features are merged in the form of the 
bottom feature, where the lexical elements have 
been replaced by their disjunction. The merged 
feature represents he union of the sets of tokens 
described by the unmerged feature types. All 
instances of the original two features would now 
be replaced in the data by the merged feature. 
specific features which occur rarely to produce 
more general features which occur more often, 
resulting in fewer total features used. Even if 
the events are not noisy or inaccurate in actual 
fact, they may still contribute to overfitting if 
their features occur too infrequently in the data 
to give accurate frequencies. The merging pro- 
cedure seeks to address overfitting at the level of 
the features themselves and remain true to the 
spirit of the maximum entropy approach, which 
seeks to represent what is unknown about the 
data with uniformity of the distribution, rather 
than by making adjustments on the model dis- 
tribution itself, such as the Gaussian prior of 
Osborne (2000). 
Each feature, as described above, is made up 
of discrete elements, which may include such 
objects as lexical items, POS tags, and gram- 
matical attribute information, depending on the 
schema being used. The rarity of the feature 
in the data is largely--although not entirely-- 
determined by the rarity of elements within 
it. In the present merging scheme, a set of 
elements is collected whose empirical frequen- 
cies are below some predetermined cutoff point. 
Note that the use of the term "cutoff" here 
refers to the empirical frequency of elements of 
features rather than of features themselves, as 
in Ratnaparkhi (1998). All features containing 
elements in this set will be altered such that 
the cutoff element is replaced by a uniform dis- 
junctive element, effectively merging all simi- 
larly structured features into one, with the dis- 
parate elements replaced by the disjunctive l- 
ement. An example may be seen in figure 1, 
where the union of the two features at top of the 
figure is represented as the feature below them. 
The merged elements in this case are the lexical 
items offered and allow. Such a merge would 
take place on the condition that the empirical 
frequencies of both elements are below a certain 
cutoff point. If so, the elements are replaced by 
a new element representing the disjunction of 
the original elements, creating a single feature. 
This feature then replaces all instances of both 
of the original features. If both of the original 
features appear once each together in an event, 
then two instances of the merged feature will 
appear in that event in the new model. 
4 Experiments 
The experiments described here were conducted 
using the Wall Street Journal Penn Treebank 
corpus (Marcus et al, 1993). The gram- 
mar used was a manually written broad cov- 
erage DCG style grammar (Briscoe and Car- 
roll, 1997). Parses of WSJ sentences produced 
by the grammar were ranked empirically using 
the treebank parse as a gold standard accord- 
ing to a weighted linear combination of cross- 
ing brackets, precision, and recall. If more than 
fifty parses were produced for a sentence, the 
51 
best fifty were used and the rest discarded. For 
the training data, the empirical rankings of all 
parses for each sentence were normalized so the 
total parse scores for each sentence added to a 
constant. The events of the training data con- 
sisted of parses and their corresponding nor- 
malized score. These scores were furthermore 
treated as frequencies. Thus, high ranked parses 
would be treated as events occurring more fre- 
quently in the training data, and low ranked 
parses would be treated as occurring rarely. 
The features of the unmerged model consisted 
of depth-one trees carrying node information ac- 
cording to the following schema: the POS tag of 
the mother, POS tags of all daughters ordered 
left to right, HEAD+ information for the head 
daughter, and lexical information for all daugh- 
ters carrying a verbal or prepositional POS tag. 
The features themselves were culled using this 
schema on 2290 sentences from the training 
data. The feature set consisted of 38,056 fea- 
tures in total, of which 6561 were active in the 
model (assigned non-zero weights) after the fi- 
nal iteration of IIS. Two models using this fea- 
ture set were trained, one on only 498 training 
sentences, a subset of the 2290 sentences used 
to collect the features, and the other on nearly 
ten times that number, 4600 training sentences, 
a superset of the same set of sentences. 
Several merged models were made based on 
each of these unmerged models, using various 
cutoff numbers. Cutoffs were set at empirical 
frequencies of 100, 500, 1000, 1250, and 1500 
elements. For each model merge, all elements 
which occurred in the training data fewer times 
than the cutoff number were replaced in each 
feature they appeared in by the uniform dis- 
junctive element, and the merged features then 
took the place of the unmerged features. 
Iterative scaling was performed for 150 iter- 
ations on each model. This number was cho- 
sen arbitrarily as a generous but not gratuitous 
number of iterations, allowing general trends to 
be observed. 
The models were tested on approximately 
5,000 unseen sentences from other parts of 
the corpus. The performance of each model 
was measured at each iteration by binary best 
match. The model chose a single top parse and 
if this parse's empirical rank was the highest 
(or equal to the highest) of all the parses for the 
sentence, the model was awarded a point for the 
match, otherwise the model was awarded zero. 
The performance rating reflects the percentage 
of times that the model chose the best parse of 
all possible parses, averaged over all test sen- 
tences. 
5 Resu l ts  
5.1 Per fo rmance  of unmerged models  
Of the unmerged models, as expected, the one 
trained on the smaller set shows the worst per- 
formance and most drastic overfitting. Its peak 
at approximately 42.5% performance comes 
early, at around 20 iterations of IIS, and sub- 
sequently drops to 40.5% at around 50 itera- 
tions. At around 80 iterations, it plunges to 
about 39%, where it remains. This model's per- 
formance may be seen in figure 2 represented by
the solid black line. 
In figure 3, the solid black line represents he 
original model trained on 4600 sentences. The 
feature set is the same, although in this case all 
of the 38,057 features are active. The advantage 
of having so much more training data is evident. 
The performance peaks at a much higher level 
and overfitting, although present, is much less 
drastic at the end of 150 iterations. Neverthe- 
less, the curve still reaches a maximum point 
fairly early on, at about 40 iterations, and the 
performance diminishes from there. 
5.2 Per fo rmance  of merged models  
Different cutoffs yielded varying degrees of im- 
provement. A cutoff of 100 elements seemed to 
make no meaningful difference ither way with 
either model. Increasing the cutoff for the 498 
sentence-trained model both lowered the peak 
before 40 iterations and raised the dip after 80 
in a fairly regular fashion. The best balance 
seemed to be struck with a cutoff of 1250. In 
this case the number of active features in the 
model was reduced to 4801. 
As can be seen from figure 2, the merged 
model, represented by the dotted line, shows a 
much more predictable improvement, i s curve 
much closer to the optimal asymptotic mprove- 
ment. In terms of actual performance, the early 
peak of the unmerged model is not present at all 
in the merged model, which catches up between 
around 40 and 80 iterations. After 80 iterations, 
the merged model begins to outperform the un- 
merged model, which has begun to suffer from 
52 
43 , , 
. . . .  42 
41.5  
~ " ~'~ 
.... //.///..',,i .... ~ .. ,x r "~'~' 
i 
; 2o 2o ;o ' ' ,,o 100 120 
iteraUons 
"unmerged" - -  
"merged" - . . . .  
46  
45  
44  
~ 43 
~. 42 
41 
4o 
39 
"unmerge? - -  
"merged" 
i 
20 40  60  80  100 120 t40  160 
i te rat ions  
Figure 2: For the model trained on 498 sen- 
tences, features containing elements appearing 
fewer than 1250 times are merged. The early 
peak of the unmerged model gives way to dras- 
tic overfitting. The merged model, on the other 
hand, does not reach this peak, but overfitting 
is not present. 
severe overfitting. The merged model, on the 
other hand, shows no evidence of overfitting. 
Likewise, the merged model represented by 
the dotted line in figure 3 shows no overfitting 
either, an improvement in that regard over its 
unmerged counterpart. For this model, the best 
cutoff of those tried appeared to be 500, and 
the number of active features was reduced to 
77,286. Higher cutoffs led to slower rates of im- 
provement and lower levels of performance. 
Both merging operations may be viewed as 
yielding improvements over the unmerged mod- 
els, as the accuracy of the model should ide- 
ally increase with each iteration of the IIS al- 
gorithm until it converges. It is likely that fur- 
ther iterations would yield even more clear im- 
provement, although it is also possible that the 
merged models themselves would begin to ex- 
hibit overfitting after some point. The rate of 
increase in performance and the point of onset 
of overfitting varies from model to model. In 
general, predictable improvement, even if grad- 
ual, is preferable to sporadic peaking and dras- 
tic overfitting. This may not always be the case 
in practice. 
6 Conc lus ion  
The feature merging strategy described in this 
paper may be employed to reduce overfitting in 
Figure 3: For the model trained on 4600 sen- 
tences, features containing elements appearing 
fewer than 500 times are merged. The overfit- 
ting in the unmerged model, represented by the 
solid line, is less drastic due to more extensive 
training material, but an improvement can still 
be seen in the curve of the merged model. 
situations where statistical features are built up 
compositionally from basic elements. As men- 
tioned, the merging strategy bears certain simi- 
larities with other methods of overfitting reduc- 
tion, such as standard feature cutoffs where en- 
tire features appearing less than some number 
of times are ignored (Ratnaparkhi, 1998). Intu- 
itively, it seems that in a sparse data situation, 
it would be beneficial to retain the general in- 
formation in features, rather than ignoring rare 
features entirely. It would be worthwhile to ver- 
ify this suspicion by comparing the present ap- 
proach directly with a simple feature cutoff, and 
furthermore comparing a simple cutoff to one 
where the low-frequency features were merged 
according to the present scheme, rather than 
simply discarded. It is to be expected that a 
combination of both approaches would be likely 
to outperform either individual approach. How 
much improvement may be gained remains to 
be seen. 
References  
Steven P. Abney. 1997. Stochastic attribute-value 
grammars. Computational Linguistics, 23(4):597- 
618, December. 
Rens Bod and Ronald M. Kaplan. 1998. A proba- 
bilistic corpus-driven model for lexical-functional 
analysis. In Proceedings o/ A CL/COLING '98, 
53 
Montreal. 
Ted Briscoe and John Carroll. 1997. Automatic 
extraction of subcategorization from corpora. In 
Proceedings of the 5 th Conference on Applied 
NLP, pages 356-363, Washington, DC. 
Eugene Charniak. 1997. Statistical parsing with a 
context-free grammar and word statistics. Tech- 
nical Report CS-95-28, Department of Computer 
Science, Brown University. 
Stanley F. Chen and Ronald Rosenfeld. 1999. 
A gaussian prior for smoothing maximum en- 
tropy models. Technical Report CMU-CS-99-108, 
Carnegie Mellon University, School of Computer 
Science. 
Michael John Collins. 1996. A new statistical parser 
based on bigram lexical dependencies. In Arivind 
Joshi and Martha Palmer, editors, Proceedings 
of the 3~th Annual Meeting of the ACL, pages 
184-191, San Francisco. Association for Compu- 
tational Linguistics, Morgan Kaufmann Publish- 
ers. 
Stephen Della Pietra, Vincent Della Pietra, and 
John Lafferty. 1995. Inducing features of ran- 
dom fields. Technical Report CS-95-144, Carnegie 
Mellon University, School of Computer Science. 
Ron Kohavi and George H. John. 1997. Wrappers 
for feature subset selection. Artificial Intelligence: 
special issue on relevance, 97:273-324. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proceedings of the 33rd An- 
nual Meeting of the A CL, pages 276-283. 
M.P. Marcus, B. Santorini, and M.A. Marcinkievicz. 
1993. Building a large annotated corpus of En- 
glish: the Penn Treebank. Computational Lin- 
guistics, 19:313-330. 
Miles Osborne. 2000. Estimation of stochas- 
tic attribute-value grammars using an informa- 
tive sample. In Proceedings of Coling 2000, 
Saarbrficken. 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models for Natural Language Ambiguity Resolu- 
tion. Ph.D. thesis, University of Pennsylvania. 
Andreas Stolcke and Stephen Omohundro. 1994. In- 
ducing probabilistic grammars by bayesian model 
merging. In R.C. Carrasco and J. Oncina, editors, 
Grammatical Inference and Applications, pages 
106-118. Springer. 
54 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 145-147, Lisbon, Portugal, 2000. 
Shallow Parsing as Part-of-Speech Tagging* 
Mi les  Osborne  
University of Edinburgh 
Division of Informatics 
2 Buccleuch Place 
Edinburgh EH8 9LW , Scotland 
osborne@cogsci, ed. ac. uk 
Abst rac t  
Treating shallow parsing as part-of-speech tag- 
ging yields results comparable with other, more 
elaborate approaches. Using the CoNLL 2000 
training and testing material, our best model 
had an accuracy of 94.88%, with an overall FB1 
score of 91.94%. The individual FB1 scores for 
NPs were 92.19%, VPs 92.70% and PPs 96.69%. 
1 Introduction 
Shallow parsing has received a reasonable 
amount of attention in the last few years (for 
example (Ramshaw and Marcus, 1995)). In 
this paper, instead of modifying some existing 
technique, or else proposing an entirely new ap- 
proach, we decided to build a shallow parser us- 
ing an off-the-shelf part-of-speech (POS) tagger. 
We deliberately did not modify the POS tag- 
ger's internal operation in any way. Our results 
suggested that achieving reasonable shallow- 
parsing performance does not in general require 
anything more elaborate than a simple POS tag- 
ger. However, an error analysis suggested the 
existence of a small set of constructs that are 
not so easily characterised by finite-state ap- 
proaches uch as ours. 
2 The  Tagger 
We used Ratnaparkhi's maximum entropy- 
based POS tagger (Ratnaparkhi, 1996). When 
tagging, the model tries to recover the most 
likely (unobserved) tag sequence, given a se- 
quence of observed words. 
For our experiments, we used the binary-only 
distribution of the tagger (Ratnaparkhi, 1996). 
" The full version of this paper can be found at 
ht tp: / /www.cogsc i .ed.ac .uk/ 'osborne/shal low.ps  
3 Conv inc ing  the  Tagger  to  Sha l low 
Parse  
The insight here is that one can view (some 
of) the differences between tagging and (shal- 
low) parsing as one of context: shallow pars- 
ing requires access to a greater part of the 
surrounding lexical/POS syntactic environment 
than does simple POS tagging. This extra in- 
formation can be encoded in a state. 
However, one must balance this approach 
with the fact that as the amount of information 
in a state increases, with limited training ma- 
terial, the chance of seeing such a state again 
in the future diminishes. We therefore would 
expect performance to increase as we increased 
the amount of information in a state, and then 
decrease when overfitting and/or sparse statis- 
tics become dominate factors. 
We trained the tagger using 'words' that were 
various 'configurations' (concatenations) of ac- 
tual words, POS tags, chunk-types, and/or suf- 
fixes or prefixes of words and/or chunk-types. 
By training upon these concatenations, wehelp 
bridge the gap between simple POS tagging and 
shallow parsing. 
In the rest of the paper, we refer to what the 
tagger considers to be a word as a configura- 
tion. A configuration will be a concatenation f 
various elements of the training set relevant o 
decision making regarding chunk assignment. A 
'word' will mean a word as found in the train- 
ing set. 'Tags' refer to the POS tags found in 
the training set. Again, such tags may be part 
of a configuration. We refer to what the tagger 
considers as a tag as a prediction. Predictions 
will be chunk labels. 
4 Exper iments  
We now give details of the experiments we ran. 
To make matters clearer, consider the following 
145 
fragment of the training set: 
Word 
POS Tag 
Chunk 
Wl w2 w3 
t l  t2 t3 
Cl c2 c3 
Words  are wl,w2 and w3, tags are t~L,t2 and t3 
and chunk labels are cl, c2 and ca. Throughout, 
we built various configurations when predicting 
1 the chunk label for word wl. 
With respect o the situation just mentioned 
(predicting the label for word wl), we gradu- 
ally increased the amount of information i  each 
configuration as follows: 
1. A configuration consisting of just words 
(word wl). Results: 
Chunk type 
Overall 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
P 
88.06 
67.57 
74.34 
54.55 
100.00 
0.00 
87.84 
94.80 
71.00 
82.30 
86.68 
R 
88.71 
51.37 
74.25 
66.67 
50.00 
0.00 
89.41 
95.91 
66.98 
72.15 
88.15 
FB1 
88.38 
58.37 
74.29 
6O.00 
66.67 
0.00 
88.62 
95.35 
68.93 
76.89 
87.41 
Overall accuracy: 92.76% 
2. A configuration consisting of just tags (tag 
t l ) .  Resul ts :  
Chunk type 
Overall 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
P R FB1 
88.15 88.07 88.11 
67.99 54.79 60.68 
71.61 70.79 71.20 
35.71 55.56 43.48 
0.00 0.00 0.00 
0.00 0.00 0.00 
89.47 89.57 89.52 
87.70 95.28 91.33 
52.27 21.70 30.67 
83.92 31.21 45.50 
90.38 91.18 \[ 90.78 
Overall accuracy 92.66%. 
3. Both words, tags and the current chunk 
label (wl, tl, Cl) in a configuration. We 
allowed the tagger access to the current 
chunk label by training another model with 
1For space reasons, we had to remove many of these 
experiments. The longer version of the paper gives rele- 
vant details. 
configurations consisting of tags and words 
(wl and tl). The training set was then re- 
duced to consist of just tag-word configura- 
tions and tagged using this model. After- 
wards, we collected the predictions for use 
in the second model. Results: 
Chunk type 
Overall 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
P 
89.79 
69.61 
74.72 
54.55 
50.00 
0.00 
89.80 
95.15 
71.84 
85.63 
89.54 
R 
90.70 
57.53 
77.14 
66.67 
50.00 
0.00 
91.12 
96.26 
69.81 
80.19 
91.31 
FB1 
90.24 
63.00 
75.91 
60.00 
50.00 
0.00 
90.4 
95.70 
70.81 
82.82 
90.41 
. 
Overall accuracy: 93.79% 
The final configuration made an attempt 
to take deal with sparse statistics. It con- 
sisted of the current ag tl, the next tag t2, 
the current chunk label cl, the last two let- 
ters of the next chunk label c2, the first two 
letters of the current word wl and the last 
four letters of the current word wl. This 
configuration was the result of numerous 
experiments and gave the best overall per- 
formance. The results can be found in Ta- 
ble 1. 
We remark upon our experiments in the com- 
ments section. 
5 Er ror  Ana lys i s  
We examined the performance of our final 
model with respect o the testing material and 
found that errors made by our shallow parser 
could be grouped into three categories: diffi- 
cult syntactic onstructs, mistakes made in the 
training or testing material by the annotators, 
and errors peculiar to our approach. 2 
Taking each category of the three in turn, 
problematic constructs included: co-ordination, 
punctuation, treating ditransitive VPs as being 
transitive VPs, confusions regarding adjective 
or adverbial phrases, and copulars een as be- 
ing possessives. 
2The raw results can be found at: http://www.cog 
sci.ed.ac.uk/-osborne/conll00-results.txt The mis-anal- 
ysed sentences can be found at: http://www.cogsci.ed. 
ac.uk/-osborne/conll00-results.txt. 
146 
Mistakes (noise) in the training and testing 
material were mainly POS tagging errors. An 
additional source of errors were odd annotation 
decisions. 
The final source of errors were peculiar to our 
system. Exponential distributions (as used by 
our tagger) assign a non-zero probability to all 
possible vents. This means that the tagger will 
at times assign chunk labels that are illegal, for 
example assigning a word the label I-NP when 
the word is not in a NP. Although these errors 
were infrequent, eliminating them would require 
'opening-up' the tagger and rejecting illegal hy- 
pothesised chunk labels from consideration. 
6 Comments  
As was argued in the introduction, increasing 
the size of the context produces better esults, 
and such performance is bounded by issues uch 
as sparse statistics. Our experiments suggest 
that this was indeed true. 
We make no claims about the generality of 
our modelling. Clearly it is specific to the tagger 
used. 
In more detail, we found that: 
? PPs seem easy to identify. 
? ADJP and ADVP chunks were hard to 
identify correctly. We suspect that im- 
provements here require greater syntactic 
information than just base-phrases. 
? Our performance at NPs should be 
improved-upon. In terms of modelling, we 
did not treat any chunk differently from 
any other chunk. We also did not treat any 
words differently from any other words. 
? The performance using just words and just 
POS tags were roughly equivalent. How- 
ever, the performance using both sources 
was better than when using either source 
of information in isolation. The reason for 
this is that words and POS tags have differ- 
ent properties, and that together, the speci- 
ficity of words can overcome the coarseness 
of tags, whilst the abundance of tags can 
deal with the sparseness of words. 
Our results were not wildly worse than those 
reported by Buchholz et al(Sabine Buchholz 
and Daelemans, 1999). This comparable vel of 
performance suggests that shallow parsing (base 
test data 
ADJP  
ADVP 
CONJP  
INT J  
LST  
NP  
PP  
PRT  
SBAR 
VP  
precision 
72.42% 
75.94% 
50.00% 
100.00% 
0.00% 
91.92% 
95.95% 
73.33% 
86.40% 
92.13% 
recall 
64.16% 
79.10% 
55.56% 
50.00% 
0.00% 
92.45% 
97.44% 
72.64% 
80.75% 
93.28% 
all 91.65% 92.23% 
F fl=l 
68.04 
77.49 
52.63 
66.67 
0.00 
92.19 
96.69 
72.99 
83.48 
92.70 
91.94 
Table 1: The results for configuration 4. Overall 
accuracy: 94.88% 
phrasal recognition) is a fairly easy task. Im- 
provements might come from better modelling, 
dealing with illegal chunk sequences, allowing 
multiple chunks with confidence intervals, sys- 
tem combination etc, but we feel that such im- 
provements will be small. Given this, we believe 
that base-phrasal chunking is close to being a 
solved problem. 
Acknowledgements  
We would like to thank Erik Tjong Kim Sang 
for supplying the evaluation code, and Donnla 
Nic Gearailt for dictating over the telephone, 
and from the top-of-her-head, a Perl program 
to help extract wrongly labelled sentences from 
the results. 
Re ferences  
Claire Cardie and David Pierce. 1998. Error-Driven 
Pruning of Treebank Grammars for Base Noun 
Phrase Identification. In Proceedings o/the 17 th 
International Conference on Computational Lin- 
guistics, pages 218-224. 
Lance A. Ramshaw and Mitchell P. Marcus. 
1995. Text Chunking Using Transformation- 
Based Learning. In Proceedings of the 3 rd ACL 
Workshop on Very Large Corpora, pages 82-94, 
June. 
Adwait Ratnaparkhi. 1996. A Maximum En- 
tropy Part-Of-Speech Tagger. In Proceed- 
ings of Empirical Methods in Natural Lan- 
guage, University of Pennsylvania, May. Tagger: 
ftp ://ftp. cis. upenn, edu/pub/adwait / j mx. 
Jorn Veenstra Sabine Buchholz and Walter Daele- 
mans. 1999. Cascaded Grammatical Relation As- 
signment. In Proceedings of EMNLP/VLC-99. 
Association for Computational Linguistics. 
147 
Using Maximum Entropy for Sentence Extraction
Miles Osborne
osborne@cogsci.ed.ac.uk
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
United Kingdom.
Abstract
A maximum entropy classier can be used
to extract sentences from documents. Ex-
periments using technical documents show
that such a classier tends to treat features
in a categorical manner. This results in per-
formance that is worse than when extract-
ing sentences using a naive Bayes classier.
Addition of an optimised prior to the max-
imum entropy classier improves perfor-
mance over and above that of naive Bayes
(even when naive Bayes is also extended
with a similar prior). Further experiments
show that, should we have at our disposal
extremely informative features, then max-
imum entropy is able to yield excellent re-
sults. Naive Bayes, in contrast, cannot ex-
ploit these features and so fundamentally
limits sentence extraction performance.
1 Introduction
Sentence extraction |the recovery of a given set
of sentences from some document| is useful for
tasks such as document summarisation (where the
extracted sentences can form the basis of a summary)
or question-answering (where the extracted sentences
can form the basis of an answer). In this paper, we
concentrate upon extraction of sentences for inclu-
sion into a summary. From a machine learning per-
spective, sentence extraction is interesting because
typically, the number of sentences to be extracted
is a very small fraction of the total number of sen-
tences in the document. Furthermore, those clues
which determine whether a sentence should be ex-
tracted or not tend to be either extremely specic,
or very weak, and furthermore interact together in
non-obvious ways. From a linguistic perspective, the
task is challenging since success hinges upon the abil-
ity to integrate together diverse levels of linguistic
description.
Frequently (see section 6 for examples), sentence
extraction systems are based around simple algo-
rithms which assume independence between those
features used to encode the task. A consequence of
this assumption is that such approaches are funda-
mentally unable to exploit dependencies which pre-
sumably exist in the features that would be present
in an ideal sentence extraction system. This situ-
ation may be acceptable when the features used to
model sentence extraction are simple. However, it
will rapidly become unacceptable when more sophis-
ticated heuristics, with complicated interactions, are
brought to bear upon the problem. For example,
Boguraev and Ne (2000a) argue that the quality of
summarisation can be increased if lexical cohesion
factors (rhetorical devices which help achieve cohe-
sion between related document utterances) are mod-
elled by a sentence extraction system. Clearly such
devices (for example, lexical repetition, ellipsis, co-
reference and so on) all contribute towards the gen-
eral discourse structure of some text and furthermore
are related to each other in non-obvious ways.
Maximum entropy (log-linear) models, on the
other hand, do not make unnecessary independence
assumptions. Within the maximum entropy frame-
work, we are able to optimally integrate together
whatever sources of knowledge we believe potentially
to be useful for the task. Should we use features that
are benecial, then the model will be able to exploit
this fact. Should we use features that are irrelevant,
then again, the model will be able to notice this, and
eectively ignore them. Models based on maximum
entropy are therefore well suited to the sentence ex-
traction task, and furthermore, yield competitive re-
sults on a variety of language tasks (Ratnaparkhi,
1996; Berger et al, 1996; Charniak, 1999; Nigam et
al., 1999).
In this paper, we outline a conditional maximum
         Philadelphia, July 2002, pp. 1-8.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
entropy classication model for sentence extraction.
Our model works incrementally, and does not always
need to process the entire document before assign-
ing classication.
1
It discriminates between those
sentences which should and should not be extracted.
This contrasts with ranking approaches which need
to process the entire document before extracting sen-
tences. Because we model whether a sentence should
be extracted or not in terms of features that are ex-
tracted from the sentence (and its context in the doc-
ument), we do not need to specify the size of the sum-
mary. Again, this contrasts with ranking approaches
which need to specify a priori the summary size.
Our maximum entropy approach for sentence ex-
traction does not come without problems. Using
reasonably standard features, and when extracting
sentences from technical papers, we nd that pre-
cision levels are high, but recall is very low. This
arises from the fact that those features which pre-
dict whether a sentence should be extracted tend to
be very specic and occur infrequently. Features for
sentences that should not be extracted tend to be
much more abundant, and so more likely to be seen
in the future. A simple prior probability is shown
to help counter-act this tendency. Using our prior,
we nd that the maximum entropy approach is able
to yield results that are better than a naive Bayes
classier.
Our nal set of experiments looks more closely at
the dierences between maximum entropy and naive
Bayes. We show that when we have access to an ora-
cle that is able to tell us when to extract a sentence,
then in the situation when that information is en-
coded in dependent features, maximum entropy eas-
ily outperforms naive Bayes. Furthermore, we also
show that even when that information is encoded in
terms of independent features, naive Bayes can be
incapable of fully utilising this information, and so
produces worse results than maximum entropy.
2
1
Incremental classication means that a document is
processed from start-to-nish and decisions are made as
soon as sentences are encountered. Some of our features
(in particular, those which encode sentence position in
a document) do require processing the entire document.
Using such features prevents true incremental process-
ing. However, it is trivial to remove such features and so
ensure true incrementality.
2
As a reviewer commented, under certain circum-
stances, naive Bayes can do well even when there are
strong dependencies within features (Domingos and Paz-
zani, 1997). For example, when the sample size is small,
naive Bayes can be competitive with more sophisticated
approaches such as maximum entropy. Given this, a
fuller comparison of naive Bayes and maximum entropy
for sentence extraction requires considering sample size
in addition to the choice of features.
The rest of this paper is as follows. Section 2
outlines the general framework for sentence extrac-
tion using maximum entropy modelling. Section 3
presents our naive Bayes classier (which is used as a
comparison with maximum entropy). We then show
in section 4 how both our maximum entropy and
naive Bayes classiers can be extended with an (op-
timised) prior. The issue of summary size is touched
upon in section 5. Section 6 discusses related work.
We then present our main results (section 7). Fi-
nally, section 8 discusses our results and considers
future work.
2 Maximum Entropy for Sentence
Extraction
2.1 Conditional Maximum Entropy
The parametric form for a conditional maximum en-
tropy model is as follows (Nigam et al, 1999):
P (c j s) =
1
Z(s)
exp(
X
i

i
f
i
(c; s)) (1)
Z(s) =
X
c
exp(
X
i

i
f
i
(c; s)) (2)
Here, c is a label (from the set of labels C) and s is
the item we are interested in labelling (from the set
of items S). In our domain, C simply consists of two
labels: one indicating that a sentence should be in
the summary (`keep'), and another label indicating
that the sentence should not be in the summary (`re-
ject'). S consists of a training set of sentences, linked
to their originating documents. This means that we
can recover the position of any given sentence in any
given document.
Within maximum entropy models, the training set
is viewed in terms of a set of features. Each fea-
ture expresses some characteristic of the domain.
For example, a feature might capture the idea that
abstract-worthy sentences contain the words in this
paper. In equation 1, f
i
(c; s) is a feature. In this pa-
per we restrict ourselves to integer-valued functions.
An example feature might be as follows:
f
i
(c; s) =
8
>
>
<
>
>
:
1 if s contains the phrase
in this paper
and c is the label keep
0 otherwise
(3)
Features are related to each other through weights
(as can be seen in equation 1, where some feature
f
i
has a weight 
i
). Weights are real-valued num-
bers. When a closed form solution cannot be found,
they are determined by numerical optimisation tech-
niques. In this paper, we use conjugate gradient de-
scent to nd the optimal set of weights. Conjugate
Gradient descent converges faster than Improved It-
erative Scaling (Laerty et al, 1997), and empirically
we nd that it is numerically more stable.
2.2 Maximum Entropy Classication
When classifying sentences with maximum entropy,
we use the equation:
label(s) = argmax
c2C
P (c j s) (4)
In practice, we are not interested in the probabil-
ity of a label given a sentence. Instead we use the
unnormalised score:
label(s) = argmax
c2C
exp(
X
i

i
f
i
(c; s)) (5)
Note that this maximum entropy classier assumes
a uniform prior. Section 4 shows how a non-uniform
prior is used in place of this uniform prior.
We now present our basic naive Bayes classier.
Afterwards, we extend this classier with a non-
uniform prior.
3 Naive Bayes Classication
As an alternative to maximum entropy, we also inves-
tigated a naive Bayes classier. Unlike maximum en-
tropy, naive Bayes assumes features are conditionally
independent of each other. So, comparing the two to-
gether will give an indication of the level of statistical
dependencies which exist between features in the sen-
tence extraction domain. For our experiments, we
used a variant of the multi-variate Bernoulli event
model (McCallum and Nigam, 1998). In particular,
we did not consider features that are absent in some
example. This allows us to avoid summing over all
features in the model for each example. Note that
our maximum entropy model also did not consider
absent features.
Within our naive Bayes approach, the probability
of a label given the sentence is as follows:
P (c j s) =
P (c)
Q
n
i=1
P (g
i
j c)
P (s)
(6)
As before, s is some sentence, c the label, and g
i
is some active feature describing sentence s. Naive
Bayes models can be estimated in a closed form
by simple counting. For features which have zero
counts, we use add-k smoothing (where k is a small
number less than one).
Since the probability of the data (P (s)) is con-
stant:
P (c j s) / P (c)
n
Y
i=1
P (g
i
j c) (7)
If we assume a uniform prior (in which case P (c) is a
constant for all c), this can be further simplied to:
P (c j s) /
n
Y
i=1
P (g
i
j c) (8)
Our basic naive Bayes classier is as follows:
label(s) = argmax
c2C
n
Y
i=1
P (g
i
j c) (9)
As with the maximum entropy classier, we later
replace the uniform prior with a non-uniform prior.
4 Maximum a Posteriori
Classication
In this section, we show how our classiers can be
extended with a non-uniform prior. We also describe
how such a prior can be optimised.
4.1 Adding a non-uniform prior
Now, the two classiers mentioned previously (equa-
tions 9 and 5) are both based on maximum likeli-
hood estimation. However, as we describe later, for
sentence extraction, the maximum entropy classier
tends to over-select labels. In particular, it tends
to reject too many sentences for inclusion into the
summary. So, it it useful to extend the two previous
classiers with a non-uniform prior. For the naive
Bayes classier, we have:
label(s) = argmax
c2C
P (c)
n
Y
i=1
P (g
i
j c) (10)
Here, P (c) is our prior. The probability of the data
(P (s)) is constant and so can be dropped.
For the maximum entropy case, we are not inter-
ested in the actual probability:
label(s) = argmax
c2C
F (c) exp(
X
i

i
f
i
(c; s)) (11)
F (c) is a function equivalent to the prior when
using the unnormalised classier. When this prior
distribution (or equivalent function) is uniform, clas-
sication is as before (namely as outlined in sections
2 and 3), and depends upon the maximum entropy
or naive Bayes component. When the prior is non-
uniform, the classier behaviour will change. This
prior therefore allows us to aect the performance
of our system. In particular, we can change the
precision-recall balance.
4.2 Optimising the prior
We treat the problem of selecting a prior as an opti-
misation task: select some P (c) (or F (c)) such that
performance, as measured by some objective func-
tion of the overall classier, is maximised. Since the
choice of objective function is up to us, we can eas-
ily optimise the classier in any way we decide. For
example, we could optimise for recall by using as our
objective function an f-measure that weighted recall
more highly than precision. In this paper, we opti-
mise the prior using as an objective function the f2
score of the classier (section 7 details this score).
Our prior therefore does not reect relative frequen-
cies of labels (as found in some corpus).
We now need to optimise our prior. Brent's one
dimensional function minimisation method is well
suited to this task (Press et al, 1993), since for a
random variable taking two values, the probability
of one value can be dened in terms of the other
value. Section 7 describes the held-out optimisation
strategy used in our experiments.
Should we decide to use a more elaborate prior (for
example, one which was also sensitive to properties
of documents) then we would need to use a multi-
dimensional function minimisation method.
Note that we have not simultaneously optimised
the likelihood and prior probabilities. This means
that we do not necessarily nd the optimal maxi-
mum a posteriori (MAP) solution. It is possible to
integrate into maximum entropy estimation (simple)
conjugate priors that do allow MAP solutions to be
found (Chen and Rosenfeld, 1999). Although it is
an open question whether more complex priors can
be directly integrated, future work ought to consider
the e?cacy of such approaches in the context of sum-
marisation.
5 Summary size
Determining the size of the summary is an impor-
tant consideration for summarisation. Frequently,
this is carried out dynamically, and specied by the
user. For example, when there is limited opportu-
nity to display long summaries a user might want a
terse summary. Alternatively, when recall is impor-
tant, a user might prefer a longer summary. Usually,
systems rank all sentences in terms of how abstract-
worthy that are, and then take the top n most highly
ranked sentences. This always requires the size of
summary to be specied.
In our classication framework, sentences are pro-
cessed (largely) independently of each other, and so
there is no direct way of controlling the size of the
summary. Altering the prior will indirectly inuence
the summary size. For more direct control over sum-
mary size, we can rank sentences using our classiers
(we not only label but can also assign label prob-
abilities) and select the top n most highly ranked
sentences.
Within our classication approach, the optimised
prior plays a similar role to the user-dened number
of sentences that a ranking approach might return.
Experiments (not reported here) showed that
ranking sentences using our maximum entropy classi-
er, and then selecting the top n most highly ranked
sentences produced slightly worse results than when
selecting sentences in terms of classication.
6 Related Work
The summarisation literature is large. Here we con-
sider only a representative sample.
Kupiec et al (1995) used Naive Bayes for sentence
extraction. They did not consider the role of the
prior, nor did they use Naive Bayes for classica-
tion. Instead, they used it to rank sentences and
selected the top n sentences. The TEXTRACT
system included a sentence extraction component
that is frequency-based (Boguraev and Ne, 2000b).
Whilst the system uses a wide variety of linguis-
tic cues when scoring sentences, it does not com-
bine these scores in an optimal manner. Also, it
does not consider interactions between the linguistic
cues. Goldstein et al (1999) used a centroid similar-
ity measure to score sentences. They do not appear
to have optimised their metric, nor do they deal with
statistical dependencies between their features.
7 Experiments
Summarisation evaluation is a hard task, principally
because the notion of an objective summary is ill-
dened. That aside, in order to compare our various
systems, we used an intrinsic evaluation approach.
Our summaries were evaluated using the standard
f2 score:
r =
j
m
p =
j
k
f2 =
2pr
p + r
where:
r = Recall
p = Precision
j = Number of correct sentences in summary
k = Number of sentences in summary
m = Number of correct sentences in the document
A sentence being `correct' means that it was
marked as being somehow important (abstract-
worthy) by a human and labelled `keep' by one of
our classiers. Summaries produced by our systems
will therefore attempt to mimic the process of select-
ing what it means for a sentence to be important in
a document.
Naturally this premise |that an annotator can
decide a priori whether a sentence is abstract-worthy
or not| is open to question. That aside, in other
sentence extraction scenarios, it may well be the case
that sentences can be reliably annotated.
The f2 score treats recall and precision equally.
This is a sensible metric to use as we have no a priori
reason to believe in some other non-equal ratio of the
two components.
Our evaluation results are based on the following
approach:
1. Split the set of documents into two disjoint sets
(T1 and T2), with 70 documents in T1 and 10
documents in T2.
2. Further split T1 into two disjoint sets T3 and
T4. T3 is used to train a model, and T4 is
a held-out set. The prior is estimated using
Brent's line minimisation method, when train-
ing using T3 and evaluating on T4. T3 consisted
of 60 documents and T4 consisted of 10 docu-
ments.
3. Results are then presented using a model trained
on T1, with the prior just found, and evaluated
using T2. T1 is therefore the training set and
T2 is the testing set. Results are also presented
using a at prior.
4. The whole process is then repeated after ran-
domising the documents. The nal results are
then averaged over these n runs. We set n to
40.
7.1 Document set
For data, we used the same documents that
Teufel (2001) used in her experiments.
3
In brief,
these were 80 conference papers, taken from the
Comp-lang preprint archive, and semi-automatically
converted from L
A
T
E
Xto XML. The XML annotated
documents were then additionally manually marked-
up with tags indicating the status of various sen-
tences. This document set is modest in size. On the
other hand, the actual documents are longer than
newswire messages typically used for summarisation
tasks. Also, the documents show variation in style.
For example, some documents are written by non-
native speakers, some by students, some by multiple
authors and so on. Summarisation is therefore hard.
3
A superset of the documents is described in (Teufel
and Moens, 1997).
Here are some properties of the documents. On
average, each document contained 8 sentences that
were marked as being abstract-worthy (standard de-
viation of 3.1). The documents on average each
contained in total 174 sentences (standard deviation
50.7). Here, a `sentence' is either any sequence of
words that happened to be in a title, or else any se-
quence of words in the rest of the document. As can
be seen, the summaries are not uniformly long. Also,
the documents vary considerably in length. Sum-
mary size is therefore not constant.
7.2 Features
We used the following, fairly standard features when
describing all sentences in the documents:
 Word pairs. Word pairs are consecutive words
as found in a sentence. A word pair feature sim-
ply indicates whether a particular word pair is
present. All words were reduced: truncated to
be at most 10 characters long. Stemming (as
for example carried out by the Porter stemmer)
produced worse results. We extracted all word
pairs found in all sentences, and for any given
sentence, found the set of (reduced) word pairs.
 Sentence length. We encoded in three binary
features whether a sentence was less than 6
words in length, whether it was greater than 20
words in length, or whether it was in between
these two ranges. We also used a feature which
encoded whether a previous sentence was less
than 5 words or longer. This captured the idea
that summary sentences tend to follow headings
(which are short).
 Sentence position. Summary sentences tend to
occur either at the start, or the end of a docu-
ment. We used three features: whether a given
sentence was within the rst 8 paragraphs of a
document, whether a sentence was in the last
3 paragraphs, or whether the sentence was in
a paragraph between these two ranges to en-
code sentence position. Note that this feature
requires the whole document to be processed be-
fore classication can take place.
 (Limited) discourse features. Our features de-
scribed whether a sentence immediately followed
typical headings such as conclusion or introduc-
tion, whether a sentence was at the start of a
paragraph, or whether a sentence followed some
generic heading.
Our features are not exhaustive, and are not designed
to maximise performance. Instead, they are designed
to be typical of those found in sentence extraction
systems. Note that some of our features exploit the
fact that the documents are annotated with struc-
tural information (such as headers etc).
Experiments with removing stop words from docu-
ments resulted in decreased performance. We conjec-
ture that this is because our word pairs are extremely
crude syntax approximations. Removing stop words
from sentences and then creating word pairs makes
these pairs even worse syntax approximations. How-
ever, using stop words increased the number of fea-
tures in our model, and so again reduced perfor-
mance. We therefore compromised between these
two positions, and mapped all stop words to the same
symbol prior to creation of word pair features. We
also found it useful to remove word pairs which con-
sisted solely of stop words. Finally, for maximum
entropy, we deleted any feature that occurred less
than 4 times. Naive Bayes did not benet from a
frequency-based cuto.
7.3 Classier comparison
Here we report on our classiers.
As a baseline model, we simply extracted the rst
n sentences from a given document. Figure 1 sum-
marises our results as n varies. In this table, as in all
subsequent tables, P and R are averaged precision
and recall values, whilst F2 is the f2 score of these
averaged values.
n F2 P R n F2 P R
1 0 0 0 26 16 10 36
6 3 3 2 31 18 12 45
11 19 15 26 36 18 11 53
16 20 16 29 41 17 10 58
21 23 16 38 46 16 9 58
Figure 1: Results for the baseline model
Figure 2 shows our results for maximum entropy,
both with and without the prior. Prior optimisation
was with respect to the f2 score. As in subsequent
tables, we show system performance when adding
more and more features.
Performance without the prior is heavily skewed
towards precision. This is because our features are
largely acting categorically: the sheer presence of
some feature is su?cient to inuence labelling choice.
Further evidence for this analysis is supported by in-
specting one of the models produced when using the
full set of all feature types. We see that of the 85883
Features Flat prior Optimised prior
F2 P R F2 P R
Word pairs 8 5 30 20 40 14
and sent length 25 63 16 36 36 36
and sent position 28 62 18 39 35 45
and discourse 35 63 24 42 43 41
Figure 2: Results for the maximum entropy model
feature instances in the model, the vast majority are
deeded irrelevant by maximum entropy, and assigned
a zero weight. Only 7086 features (roughly 10% in
total) had non zero weights.
Performance using the optimised prior shows more
balanced results, with an increase in F2 score.
Clearly optimising the prior has helped counter the
categorical behaviour of features in our maximum
entropy classier.
Figure 3 shows the results we obtained when us-
ing a naive Bayes classier. As before, the results
show performance with and without the addition of
the optimised prior. Naive Bayes outperforms maxi-
mum entropy when both classiers do not use a prior.
Performance with and without the prior however, is
worse than the performance of our maximum entropy
classier with the prior. Evidently, even our rela-
tively simple features interact with each other, and
so approaches such as maximum entropy are required
to fully exploit them.
Features Flat prior Optimised prior
F2 P R F2 P R
Word pairs 26 29 23 29 26 32
and sent length 31 33 28 32 29 35
and sent position 33 34 33 36 31 43
and discourse 38 39 37 39 38 40
Figure 3: Results for the naive Bayes model
7.4 Using informative features
Our previous results showed that maximum entropy
could outperform naive Bayes. However, the dier-
ences, though present, were not large. Clearly, our
feature set was imperfect.
4
It is therefore instruc-
tive to see what happens if we had access to an or-
acle who always told us the true status of some un-
seen sentence. To make things more interesting, we
4
Another possible reason for the closeness of the re-
sults is the small sample size. There may just not be
enough evidence to reliably estimate dependencies within
the data.
Features Naive Bayes Maxent
F2 P R F2 P R
Word pairs 30 34 26 32 93 19
and sent length 35 38 32 99 100 99
and sent position 40 41 39 100 100 100
and discourse 43 44 41 99 100 97
Figure 4: Results for basic naive Bayes and max-
imum entropy models using dependent informative
features
Features Naive Bayes Maxent
F2 P R F2 P R
Word pairs 84 74 97 25 15 91
and sent length 85 75 97 100 100 100
and sent position 84 73 97 100 100 100
and discourse 84 74 97 100 100 100
Figure 5: Results for basic naive Bayes and maxi-
mum entropy models using independent informative
features
encoded this information in terms of dependent fea-
tures. We simulated this oracle by using two features
which were active whenever a sentence should not
be in the summary; for sentences that should be in-
cluded in the summary, we let either one of those two
features be active, but on a random basis. Our fea-
tures therefore are only informative when the learner
is capable of noting that there are dependencies. We
then repeated our previous maximum entropy and
naive Bayes experiments. Figure 4 summarise our
results.
Unsurprisingly, we see that when features are
highly dependent upon each other, maximum en-
tropy easily outperforms naive Bayes.
Even when we have access to features that are in-
dependent of each other, naive Bayes can still do
worse than maximum entropy. To demonstrate this,
we used a feature that was active whenever a sen-
tence should be in the summary. This feature was
not active on sentences that should not be in the
summary. Figure 5 summarises our results.
As can be seen (gure 5), even when naive Bayes
has access to a perfectly reliable informative feature,
the fact that the other features are not suitably dis-
counted means that performance is worse than that
of maximum entropy. Maximum entropy can dis-
count the other features, and so can take advantage
of reliable features.
8 Comments and Future Work
We showed how maximum entropy could be used for
sentence extraction, and in particular, that adding
a prior could deal with the categorical nature of
the features. Maximum entropy, with an opti-
mised prior, did yield marginally better results than
naive Bayes (with and without a similarly optimised
prior). However, the dierences were not that great.
Our further experiments with informative features
showed that this lack of dierence was probably due
(at least in part) to the actual features used, and not
due to the technique itself.
Our oracle results are an idealisation. A fuller
comparison should use more sophisticated features,
along with more data. As a result of this, we conjec-
ture that should we use a much more sophisticated
feature set, we would expect that the dierences be-
tween maximum entropy and naive Bayes would be-
come greater.
Our approach treated sentences largely indepen-
dently of each other. However, abstract-worthy sen-
tences tend to bunch together, particularly at the
beginning and end of a document. We intend cap-
turing this idea by making our approach sequence-
based: future decisions should also be conditioned
on previous choices.
A problem with supervised approaches (such as
ours) is that we need annotated material (Marcu,
1999). This is costly to produce. Future work will
consider weakly supervised approaches (for example
cotraining) as a way of bootstrapping labelled mate-
rial from unlabelled documents (Blum and Mitchell,
1998). Note that there is a close connection between
multi-document summarisation (where many alter-
native documents all consider similar issues) and the
concept of a view in cotraining. We expect that this
redundancy could be exploited as a means of provid-
ing more annotated training material, and so yield
better results.
In summary, maximum entropy can be benecially
used in sentence extraction. However, one needs to
guard against categorial features. An optimised prior
can provide such help.
Acknowledgement
We would like to thank Rob Malouf for supplying the
excellent log-linear estimation code, Simone Teufel
for providing the annotated data, Karen Spark Jones
for a discussion about summarisation, Steve Clark
for spotting textual bugs and the anonymous review-
ers for useful comments.
References
Adam Berger, Stephen Della Pietra, and Vin-
cent Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 21{22.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training.
In Proceedings of the Workshop on Computational
Learning Theory. Morgan Kaufmann Publishers.
Branimir K. Boguraev and Mary S. Ne. 2000a. The
eects of analysing cohesion on document sum-
marisation. In Proceedings of the 18
th
Interna-
tional Conference on Computational Linguistics,
volume 1, pages 76{82, Saarbrucken.
Branmir K. Boguraev and Mary S. Ne. 2000b. Dis-
course Segmentation in Aid of Document Summa-
rization. In Proceedings of the 33
rd
Hawaii Inter-
national Conference on Systems Science.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS99-12, De-
partment of Computer Science, Brown University.
Stanley F. Chen and Ronald Rosenfeld. 1999.
A Gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple bayesian classier un-
der zero-one loss. Machine Learning, 29(2-3):103{
130.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mit-
tal, and Jaime G. Carbonell. 1999. Summarizing
text documents: Sentence selection and evaluation
metrics. In Research and Development in Informa-
tion Retrieval, pages 121{128.
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A Trainable Document Summarizer. In
Proceedings of the 18
th
ACM-SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68{73.
J. Laerty, S. Della Pietra, and V. Della Pietra.
1997. Inducing features of random elds. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 19(4):380{393, April.
Daniel Marcu. 1999. The automatic construction
of large-scale corpora for summarization research.
In Research and Development in Information Re-
trieval, pages 137{144.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classicatio. In
AAAI-98 Workshop on Learning for Text Catego-
rization.
Kamal Nigam, John Laerty, , and Andrew Mc-
Callum. 1999. Using maximum entropy for text
classication. In IJCAI-99 Workshop on Machine
Learning for Information Filtering,.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1993. Numeri-
cal Recipes in C: the Art of Scientic Computing.
Cambridge University Press, second edition.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Part-Of-Speech Tagger. In Proceed-
ings of Empirical Methods in Natural Lan-
guage, University of Pennsylvania, May. Tagger:
ftp://ftp.cis.upenn.edu/pub/adwait/jmx.
S. Teufel and M. Moens. 1997. Sentence extraction
as a classication task. In ACL/EACL-97 Work-
shop on Intelligent and Scalable Text Summariza-
tion, Madrid, Spain.
Simone Teufel. 2001. Task-Based Evaluation of
Summary Quality: Describing Relationships Be-
tween Scientic Papers. In NAACL Workshop on
Automatic Summarization, Pittsburgh, Pennsyl-
vania, USA, June. Carnegie Mellon University.
A very very large corpus doesn?t always yield reliable estimates
James R. Curran and Miles Osborne
Institute for Communicating and Collaborative Systems
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
United Kingdom
  jamesc,osborne  @cogsci.ed.ac.uk
Abstract
Banko and Brill (2001) suggested that the develop-
ment of very large training corpora may be more ef-
fective for progress in empirical Natural Language
Processing than improving methods that use exist-
ing smaller training corpora.
This work tests their claim by exploring whether
a very large corpus can eliminate the sparseness
problems associated with estimating unigram prob-
abilities. We do this by empirically investigating
the convergence behaviour of unigram probability
estimates on a one billion word corpus. When us-
ing one billion words, as expected, we do find that
many of our estimates do converge to their eventual
value. However, we also find that for some words,
no such convergence occurs. This leads us to con-
clude that simply relying upon large corpora is not
in itself sufficient: we must pay attention to the sta-
tistical modelling as well.
1 Introduction
The quantity and reliability of linguistic informa-
tion is primarily determined by the size of the train-
ing corpus: with limited data available, extracting
statistics for any given language phenomenon and
its surrounding context is unreliable. Overcoming
the sparse distribution of linguistic events is a key
design problem in any statistical NLP system.
For some tasks, corpus size is no longer a limit-
ing factor, since it has become feasible to acquire
homogeneous document collections two or three or-
ders of magnitude larger than existing resources.
Banko and Brill (2001) report on confusion set
disambiguation experiments where they apply rela-
tively simple learning methods to a one billion word
training corpus. Their experiments show a logarith-
mic trend in performance as corpus size increases
without performance reaching an upper bound. This
leads them to believe that the development of large
scale training material will yield superior results
than further experimentation with machine learning
methods on existing smaller scale training corpora.
Recent work has replicated the Banko and Brill
(2001) results on the much more complex task of
automatic thesaurus extraction, showing that con-
textual statistics, collected over a very large corpus,
significantly improve system performance (Curran
and Moens, 2002). Other research has shown that
query statistics from a web search engine can be
used as a substitute for counts collected from large
corpora (Volk, 2001; Keller et al, 2002).
To further investigate the benefits of using very
large corpora we empirically analyse the conver-
gence behaviour of unigram probability estimates
for a range of words with different relative frequen-
cies. By dramatically increasing the size of the
training corpus, we expect our confidence in the
probability estimates for each word to increase. As
theory predicts, unigram probability estimates for
many words do converge as corpus size grows.
However, contrary to intuition, we found that for
many commonplace words, for example tight-
ness, there was no sign of convergence as corpus
size approaches one billion words. This suggests
that for at least some words, simply using a much
larger corpus to reduce sparseness will not yield re-
liable estimates. This leads us to conclude that ef-
fective use of large corpora demands, rather than
discourages, further research into sophisticated sta-
tistical language modelling methods. In our case,
this means adding extra conditioning to the model.
Only then could we reasonably predict how much
training material would be required to ameliorate
sparse statistics problems in NLP.
The next section briefly introduces the relevant
limit theorems from statistics. Section 3 describes
our experimental procedure and the collection of
the billion word corpus. Section 4 gives examples
of words with convergent and non-convergent be-
haviour covering a range of relative frequencies. We
conclude with a discussion of the implications for
language modelling and the use of very large cor-
pora that our results present.
2 Theoretical Convergence Behaviour
Standard results in the theory of statistical infer-
ence govern the convergence behaviour and de-
viance from that behaviour of expectation statistics
in the limit of sample size. The intuitive ?Law of
Averages? convergence of probabilities estimated
from increasingly large samples is formalised by the
Law(s) of Large Numbers. The definition1 given in
Theorem 1 is taken from Casella and Berger (1990):
Theorem 1 (Strong Law of Large Numbers)
Let X1, X2, X3, . . . be i.i.d. random variables with
EXi = ? and Var Xi = ?2 < ?, and dene the
average Xn = 1n
?n
i=1 Xi. Then, for every ? > 0:
P
(
limn?? |Xn ? ?| < ?
)
= 1 (1)
The Law of the Iterated Logarithm relates the
degree of deviance from convergent behaviour to
the variance of the converging expectation estimates
and the size of the sample. The definition in Theo-
rem 2 is taken from Petrov (1995):
Theorem 2 (Law of the Iterated Logarithm)
Let X1, X2, X3, . . . be i.i.d. random variables with
EXi = ?, ?2 < ?, and Var Xi = ?2 < ?, and dene
the average Xn = 1n
?n
i=1 Xi. Then:
P
?
?
?
?
?
?
?
lim sup
n??
Xn ? ?
?2 log log n = ?
?
?
?
?
?
?
?
= 1 (2)
Limit theorems codify behaviour as sample size
n approaches infinity. Thus, they can only provide
an approximate guide to the finite convergence be-
haviour of the expectation statistics, particularly for
smaller samples. Also, the assumptions these limit
theorems impose on the random variables may not
be reasonable or even approximately so. It is there-
fore an open question whether a billion word corpus
is sufficiently large to yield reliable estimates.
1There are two different standard formulations: the weak
and strong Law of Large Numbers. In the weak law, the prob-
ability is converging in the limit to one (called convergence
in probability). In the strong law, the absolute difference is
converging in the limit to less than epsilon with probability 1
(called almost sure convergence).
Corpus # Words
NANC 434.4 million
NANC Supplement 517.4 million
RCV1 193.0 million
Table 1: Components of the billion word corpus
3 Experiments
We would like to answer the question: how much
training material is required to estimate the unigram
probability of a given word with arbitrary confi-
dence. This is clearly dependent on the relative fre-
quency of the word in question. Words which ap-
pear to have similar probability estimates on small
corpora can exhibit quite different convergence be-
haviour as the sample size increases.
To demonstrate this we compiled a homogeneous
corpus of 1.145 billion words of newspaper and
newswire text from three existing corpora: the
North American News Text Corpus, NANC (Graff,
1995), the NANC Supplement (MacIntyre, 1998)
and the Reuters Corpus Volume 1, RCV1 (Rose et
al., 2002). The number of words in each corpus is
shown in Table 1.
These corpora were concatenated together in the
order given in Table 1 without randomising the in-
dividual sentence order. This emulates the process
of collecting a large quantity of text and then calcu-
lating statistics based counts from the entire collec-
tion. Random shuffling removes the discourse fea-
tures and natural clustering of words which has such
a significant influence on the probability estimates.
We investigate the large-sample convergence be-
haviour of words that appear at least once in a
standard small training corpus, the Penn Treebank
(PTB). The next section describes the convergence
behaviour for words with frequency ranging from
the most common down to hapax legomena.
From the entire 1.145 billion word corpus we cal-
culated the gold-standard unigram probability esti-
mate, that is, the relative frequency for each word.
We also calculated the probability estimates for
each word using increasing subsets of the full cor-
pus. These subset corpora were sampled every 5
million words up to 1.145 billion.
To determine the rate of convergence to the gold-
standard probability estimate as the training set in-
creases, we plotted the ratio between the subset and
gold-standard estimates. Note that the horizontal
lines on all of the graphs are the same distance apart.
The exception is Figure 5, where there are no lines
0 100 200 300 400 500 600 700 800 900 1000 1100 1200
Number of words (in millions)
0.8
0.9
1
1.1
Pr
ob
ab
ili
ty
 R
at
io
the (4.95%)
of (2.15%)
to (2.16%)
a (1.91%)
in (1.77%)
and (1.81%)
N
A
N
C
N
A
N
C
 S
up
p.
N
A
N
C
 S
up
p.
R
V
C
1
Figure 1: Estimate ratios for function words
0 100 200 300 400 500 600 700 800 900 1000 1100 1200
Number of words (in millions)
0.8
0.9
1
1.1
Pr
ob
ab
ili
ty
 R
at
io
bringing (4.20x10
-3
%)
form (1.06x10
-2
%)
no (1.12x10
-1
%)
car (1.72x10
-2
%)
N
A
N
C
N
A
N
C
 S
up
p.
N
A
N
C
 S
up
p.
R
V
C
1
Figure 2: Ratios for accurate non-function words
because there would be too many to plot within the
range of the graph. The legends list the selected
words with the relative frequency (as a percentage)
of each word in the full corpus. Vertical lines show
the boundaries between the concatenated corpora.
4 Empirical Convergence Behaviour
Figure 1 shows the convergence behaviour of some
very frequent closed-class words selected from the
PTB. This graph shows that for most of these ex-
tremely common words, the probability estimates
are accurate to within approximately ?10% (a ra-
tio of 1 ? 0.1) of their final value for a very small
corpus of only 5 million words (the size of the first
subset sample).
Some function words, for example, the and in,
display much more stable probability estimates even
amongst the function words, suggesting their us-
age is very uniform throughout the corpus. By
chance, there are also some open-class words, such
0 100 200 300 400 500 600 700 800 900 1000 1100 1200
Number of words (in millions)
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
Pr
ob
ab
ili
ty
 R
at
io
speculation (5.21x10
-3
%)
grew (5.92x10
-3
%)
social (1.34x10
-2
%)
newly (3.12x10
-3
%)
eye (4.07x10
-3
%)
N
A
N
C
N
A
N
C
 S
up
p.
N
A
N
C
 S
up
p.
R
V
C
1
Figure 3: Ratios for commonplace words
as bringing,form and crucial, that also have
very stable probability estimates. Examples of these
are shown in Figure 2. The main difference between
the convergence behaviour of these words and the
function words is the fine-grained smoothness of the
convergence, because the open-class words are not
as uniformly distributed across each sample.
Figure 3 shows the convergence behaviour of
commonplace words that appear in the PTB be-
tween 30 and 100 times each. Their convergence
behaviour is markedly different to the closed-class
words. We can see that many of these words have
very poor initial probability estimates, consistently
low by up to a factor of almost 50%, five times
worse than the closed-class words.
speculation is an example of convergence
from a low initial estimate. After approximately 800
million words, many (but not all) of the estimates
are correct to within about ?10%, which is the same
error as high frequency words sampled from a 5 mil-
lion words corpus. This is a result of the sparse
distribution of these words and their stronger con-
text dependence. Their relative frequency is two to
three orders of magnitude smaller than the relative
frequencies of the closed-class words in Figure 1.
What is most interesting is the convergence be-
haviour of rare but not necessarily unusual words,
which is where using a large corpus should be most
beneficial in terms of reducing sparseness. Figure
4 shows the very large corpus behaviour of selected
hapax legomena from the PTB. Many of the words
in this graph show similar behaviour to Figure 3,
in that some words appear to converge relatively
smoothly to an estimate within ?20% of the final
value. This shows the improvement in stability of
the estimates from using large corpora, although
?20% is a considerable deviation from the gold-
standard estimate.
However, other words, for instance tightness,
fail spectacularly to converge to their final estimate
before the influence of the forced convergence of
the ratio starts to take effect. tightness is an
extreme example of the case where a word is seen
very rarely, until it suddenly becomes very popu-
lar. A similar convergence behaviour can be seen
for words with a very high initial estimate in Figure
5. The maximum decay ratio curve is the curve we
would see if a word appeared at the very beginning
of the corpus, but did not appear in the remainder of
the corpus. A smooth decay with a similar gradient
to the maximum decay ratio indicates that the word
is extremely rare in the remainder of the corpus, af-
ter a high initial estimate. rebelled, kilome-
ters and coward are examples of exceedingly
high initial estimates, followed by very rare or no
other occurrences. extremists, shelling and
cricket are examples of words that were used
more consistently for a period of time in the cor-
pus, and then failed to appear later, with cricket
having two periods of frequent usage.
Unfortunately, if we continue to assume that a un-
igram model is correct, these results imply that we
cannot be at all confident about the probability esti-
mates of some rare words even with over one billion
words of material. We cannot dismiss this as an un-
reliable low frequency count because tightness
occurs 2652 times in the full corpus. Thus we must
look for an alternative explanation: and the most
reasonable explanation is burstiness, the fact that
word occurrence is not independent and identically
distributed. So given that one billion words does not
always yield reliable estimates for rare but not un-
usual words, it leaves us to ask if any finite number
of words could accurately estimate the probability
of pathologically bursty word occurrences.
5 Discussion
It is worth reflecting on why some words appear
to have more bursty behaviour than others. As we
would expect, function words are distributed most
evenly throughout the corpus. There are also some
content words that appear to be distributed evenly.
On the other hand, some words appear often in the
first 5 million word sample but are not seen again in
the remainder of the corpus.
Proper names and topic-specific nouns and verbs
0 100 200 300 400 500 600 700 800 900 1000 1100 1200
Number of words (in millions)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
Pr
ob
ab
ili
ty
 R
at
io
jocks (4.39x10
-5
%)
punch (8.13x10
-4
%)
revise (4.89x10
-4
%)
tightness (2.33x10
-4
%)
twenty (5.49x10
-4
%)
N
A
N
C
N
A
N
C
 S
up
p.
N
A
N
C
 S
up
p.
R
V
C
1
Figure 4: Example ratios for hapax legomena
0 100 200 300 400 500 600 700 800 900 1000 1100 1200
Number of words (in millions)
0
1
2
3
4
5
6
7
8
9
10
11
12
Pr
ob
ab
ili
ty
 R
at
io
rebelled (2.33x10
-4
%)
kilometers (7.51x10
-3
%)
coward (4.37x10
-4
%)
extremists (1.22x10
-3
%)
shelling (1.24x10
-3
%)
cricket (1.83x10
-3
%)
Maximum Decay Ratio
N
A
N
C
N
A
N
C
 S
up
p.
N
A
N
C
 S
up
p.
R
V
C
1
Figure 5: Example ratios for decaying initial words
exhibit the most bursty behaviour, since the newspa-
per articles are naturally clustered together accord-
ing to the chronologically grouped events. The most
obvious and expected conditioning of the random
variables is the topic of the text in question.
However, it is hard to envisage seemingly topic-
neutral words, such as tightness and newly,
being conditioned strongly on topic. Other factors
that apply to many different types of words include
the stylistic and idiomatic expressions favoured by
particular genres, authors, editors and even the in-
house style guides.
These large corpus experiments demonstrate the
failure of simple Poisson models to account for the
burstiness of words. The fact that words are not dis-
tributed by a simple Poisson process becomes even
more apparent as corpus size increases, particularly
as the effect of noise and sparseness on the language
model is reduced, giving a clearer picture of how
badly the current language models fail. With a very
large corpus it is obvious that the usual indepen-
dence assumptions are not always appropriate.
Using very large corpora for simple probability
estimation demonstrates the need for more sophis-
ticated statistical models of language. Without bet-
ter models, all that training upon large corpora can
achieve is better estimates of words which are ap-
proximately i.i.d.
To fully leverage the information in very large
corpora, we need to introduce more dependencies
into the models to capture the non-stationary nature
of language data. This means that to gain a signifi-
cant advantage from large corpora, we must develop
more sophisticated statistical language models.
We should also briefly mention the other main
benefit of increasing corpus size: the acquisition of
occurrences of otherwise unseen words. Previously
unseen linguistic events are frequently presented
to NLP systems. To handle these unseen events
the statistical models used by the system must be
smoothed. Smoothing typically adds considerable
computational complexity to the system since mul-
tiple models need to be estimated and applied to-
gether, and it is often considered a black art (Chen
and Goodman, 1996). Having access to very large
corpora ought to reduce the need for smoothing, and
so ought to allow us to design simpler systems.
6 Conclusion
The difficulty of obtaining reliable probability esti-
mates is central to many NLP tasks. Can we improve
the performance of these systems by simply using
a lot more data? As might be expected, for many
words, estimating probabilities on a very large cor-
pus can be valuable, improving system performance
significantly. This is due to the improved estimates
of sparse statistics, made possible by the relatively
uniform distribution of these words.
However, there is a large class of commonplace
words which fail to display convergent behaviour
even on very large corpora. What is striking about
these words is that proficient language users would
not recognise them as particularly unusual or spe-
cialised in their usage, which means that broad-
coverage NLP systems should also be expected to
handle them competently.
The non-convergence of these words is an indi-
cation of their non-stationary distributions, which a
simple Poisson model is unable to capture. Since
it is no longer a problem of sparseness, even excep-
tionally large corpora cannot be expected to produce
reliable probability estimates. Instead we must relax
the independence assumptions underlying the exist-
ing language models and incorporate conditional in-
formation into the language models.
To fully harness the extra information in a very
large corpus we must spend more, and not less, time
and effort developing sophisticated language mod-
els and machine learning systems.
7 Further Work
We are particularly interested in trying to charac-
terise the burstiness tendencies of individual words
and word classes, and the resulting convergence be-
haviour of their probability estimates. An exam-
ple of this is calculating the area between unity and
the ratio curves. Some example words with differ-
ent convergence behaviour selected using this area
measure are given in Table 2 in the Appendix. We
are also interested in applying the exponential mod-
els of lexical attraction and repulsion described by
Beeferman et al (1997) to the very large corpus.
We would like to investigate the overall error in
the probability mass distribution by comparing the
whole distributions at each sample with the final dis-
tribution. To estimate the error properly will require
smoothing methods to be taken into consideration.
Acknowledgements
We would like to thank Marc Moens, Steve Finch,
Tara Murphy, Yuval Krymolowski and the many
anonymous reviewers for their insightful comments
that have contributed significantly to this paper.
This research is partly supported by a Common-
wealth scholarship and a Sydney University Trav-
elling scholarship.
References
Michele Banko and Eric Brill. 2001. Scaling to
very very large corpora for natural language dis-
ambiguation. In Proceedings of the 39th annual
meeting of the Association for Computational
Linguistics, pages 26?33, Toulouse, France, 9?11
July.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A model of lexical attraction and repul-
sion. In Proceedings of the 35th annual meeting
of the Association for Computational Linguistics
and the 8th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 373?380, Madrid, Spain, 7?11 July.
George Casella and Roger L. Berger. 1990. Sta-
tistical Inference. Duxbury Press, Belmont, CA
USA.
Stanley F. Chen and Joshua T. Goodman. 1996.
An empirical study of smoothing techniques for
language modeling. In Proceedings of the 34th
annual meeting of the Association for Computa-
tional Linguistics, pages 310?318, Santa Cruz,
CA USA, 23?28 June.
James R. Curran and Marc Moens. 2002. Scal-
ing context space. In Proceedings of the 40th
annual meeting of the Association for Computa-
tional Linguistics, Philadelphia, PA USA, 7?12
July.
David Graff. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Frank Keller, Maria Lapata, and Olga Ourioupina.
2002. Using the web to overcome data sparse-
ness. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Philadelphia, PA USA, 6?7 July.
Robert MacIntyre. 1998. North American News
Text Supplement. Linguistic Data Consortium.
LDC98T30.
Valentin V. Petrov. 1995. Limit theorems of proba-
bility theory: Sequences of independent random
variables, volume 4 of Oxford Studies in Proba-
bility. Clarendon Press, Oxford, UK.
T.G. Rose, M. Stevenson, and M. Whitehead. 2002.
The Reuters Corpus Volume 1 - from yesterday?s
news to tomorrow?s language resources. In Pro-
ceedings of the Third International Conference
on Language Resources and Evaluation, Las Pal-
mas, Canary Islands, Spain, 29?31 May.
Martin Volk. 2001. Exploiting the WWW as a cor-
pus to resolve PP attachment ambiguities. In Pro-
ceedings of the Corpus Linguistics 2001 Confer-
ence, pages 601?606, Lancaster, UK, 29 March?
2 April.
Appendix
It is possible to get some sense of the convergence
behaviour of individual words by calculating the
area between the ratio curve and unity. Table 2 lists
words with largest and smallest areas, and words
that fell in between large and small areas. A large
area (MAX ?ni=1 Xi? ) indicates either non-convergentbehaviour or convergence from poor initial esti-
mates, and so many of the words are highly con-
ditioned (primarily on topics such as war). These
words behave like the words shown in Figure 4 and
Figure 5. A small area (MIN ?ni=1 Xi? ) indicatesstrongly convergent behaviour with accurate initial
estimates, and so includes a number of function
words. These words behave like the words shown
in Figure 1 and Figure 2.
MAX ?ni=1 Xi? MID
?n
i=1
Xi
? MIN
?n
i=1
Xi
?
convoys unending bringing
rebelled buildings has
coward instrument string
hick poisoning the
routing awesome been
shelling livelihood give
secede sharpness form
truce likewise remains
convoy phantom received
kilometers acquitted before
artillery comfortable quit
kilometer complement wants
shelled entities crucial
atolls generous allowing
quake island seek
showers advancements considered
gunners demonstrates no
centimeters linden in
kilograms politicking chosen
shells spur involved
armored veer nearest
hideouts scoop hands
seahorse drill with
expedited skill car
meters arrows respect
airlift bats day
skirmished rewrite dominate
clays toughness avoid
civilians expands stay
stronghold negligence joins
centimeter swaying covered
neighboring mellowed removing
downed rendering established
besieged wording asked
hostilities disaffected being
cessation tempt preparation
detaining discourages houses
meson jumpy reeling
rebel landlords into
disarm geared food
thunderstorm planet face
Table 2: Convergence detection using curve area
Bootstrapping Parallel Corpora
Chris Callison-Burch
School of Informatics
University of Edinburgh
callison-burch@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
We present two methods for the automatic cre-
ation of parallel corpora. Whereas previous
work into the automatic construction of parallel
corpora has focused on harvesting them from
the web, we examine the use of existing paral-
lel corpora to bootstrap data for new language
pairs. First, we extend existing parallel cor-
pora using co-training, wherein machine trans-
lations are selectively added to training corpora
with multiple source texts. Retraining transla-
tion models yields modest improvements. Sec-
ond, we simulate the creation of training data
for a language pair for which a parallel corpus
is not available. Starting with no human trans-
lations from German to English we produce a
German to English translation model with 45%
accuracy using parallel corpora in other lan-
guages. This suggests the method may be use-
ful in the creation of parallel corpora for lan-
guages with scarce resources.
1 Introduction
Statistical translation models (such as those formulated in
Brown et al (1993)) are trained from bilingual sentence-
aligned texts. The bilingual data used for constructing
translation models is often gathered from government
documents produced in multiple languages. For exam-
ple, the Candide system (Berger et al, 1994) was trained
on ten years? worth of Canadian Parliament proceed-
ings, which consists of 2.87 million parallel sentences
in French and English. While the Candide system was
widely regarded as successful, its success is not indica-
tive of the potential for statistical translation between ar-
bitrary language pairs. The reason for this is that collec-
tions of parallel texts as large as the Canadian Hansards
are rare.
Al-Onaizan et al (2000) explains in simple terms the
reasons that using large amounts of training data en-
sures translation quality: if a program sees a partic-
ular word or phrase one thousand times during train-
ing, it is more likely to learn a correct translation than
if sees it ten times, or once, or never. Increasing the
amount of training material therefore leads to improved
quality. This is illustrated in Figure 1, which plots
translation accuracy (measured as 100 minus word er-
ror rate) for French?English, German?English, and
Spanish?English translation models trained on incre-
mentally larger parallel corpora. The quality of the
translations produced by each system increases over the
100,000 training items, and the graph suggests the the
trend would continue if more data were added. Notice
that the rate of improvement is slow: after 90,000 manu-
ally provided training sentences pairs, we only see a 4-6%
change in performance. Sufficient performance for sta-
tistical models may therefore only come when we have
access to many millions of aligned sentences.
One approach that has been proposed to address the
problem of limited training data is to harvest the web for
bilingual texts (Resnik, 1998). The STRAND method au-
tomatically gathers web pages that are potential transla-
tions of each other by looking for documents in one lan-
guage which have links whose text contains the name of
another language. For example, if an English web page
had a link with the text ?Espan?ol? or ?en Espan?ol? then
the page linked to is treated as a candidate translation of
the English page. Further checks verify the plausibility
of its being a translation (Smith, 2002).
Instead of attempting to gather new translations from
the web, we describe an alternate method for automat-
ically creating parallel corpora. Specifically, we exam-
ine the use of existing translations as a resource to boot-
strap more training data, and to create data for new lan-
guage pairs. We generate translation models from exist-
ing data and use them to produce translations of new sen-
4446485052545658606264 1000
0
20000
30000
40000
50000
60000
70000
80000
90000
10000
0
Accuracy (100 - Word Error Rate)
Trainin
g Corp
us Siz
e (numb
er of se
ntence p
airs)
Germa
n
Frenc
h
Spanis
h
Figure 1: Translation accuracy plotted against training
corpus size
tences. Incorporating this machine-created parallel data
to the original set, and retraining the translation models
improves the translation accuracy. To perform the retrain-
ing we use co-training (Blum and Mitchell, 1998; Abney,
2002) which is a weakly supervised learning technique
that relies on having distinct views of the items being
classified. The views that we employ for co-training are
multiple source documents.
Section 2 motivates the use of weakly supervised learn-
ing, and introduces co-training for machine translation.
Section 3 reports our experimental results. One experi-
ment shows that co-training can modestly benefit trans-
lation systems trained from similarly sized corpora. A
second experiment shows that co-training can have a dra-
matic benefit when the size of initial training corpora are
mismatched. This suggests that co-training for statisti-
cal machine translation is especially useful for languages
with impoverished training corpora. Section 4 discusses
the implications of our experiments, and discusses ways
which our methods might be used more practically.
2 Co-training for Statistical Machine
Translation
Most statistical natural language processing tasks use su-
pervised machine learning, meaning that they require
training data that contains examples that have been an-
notated with some sort of labels. Two conflicting factors
make this reliance on annotated training data a problem:
? The accuracy of machine learning improves as more
data is available (as we have shown for statistical
machine translation in Figure 1).
? Annotated training data usually has some cost asso-
ciated with its creation. This cost can often be sub-
stantial, as with the Penn Treebank (Marcus et al,
1993).
There has recently been considerable interest in weakly
supervised learning within the statistical NLP commu-
nity. The goal of weakly supervised learning is to reduce
the cost of creating new annotated corpora by (semi-) au-
tomating the process.
Co-training is a weakly supervised learning techniques
which uses an initially small amount of human labeled
data to automatically bootstrap larger sets of machine la-
beled training data. In co-training implementations mul-
tiple learners are used to label new examples and re-
trained on some of each other?s labeled examples. The
use of multiple learners increases the chance that use-
ful information will be added; an example which is eas-
ily labeled by one learner may be difficult for the other
and therefore adding the confidently labeled example will
provide information in the next round of training.
Self-training is a weakly supervised method in which
a single learner retrains on the labels that it applies to
unlabeled data itself. We describe its application to
machine translation in order to clarify how co-training
would work. In self-training a translation model would be
trained for a language pair, say German?English, from
a German-English parallel corpus. It would then produce
English translations for a set of German sentences. The
machine translated German-English sentences would be
added to the initial bilingual corpus, and the translation
model would be retrained.
Co-training for machine translation is slightly more
complicated. Rather than using a single translation
model to translate a monolingual corpus, it uses mul-
tiple translation models to translate a bi- or multi-
lingual corpus. For example, translation models could
be trained for German?English, French?English and
Spanish?English from appropriate bilingual corpora,
and then used to translate a German-French-Spanish par-
allel corpus into English. Since there are three candidate
English translations for each sentence alignment, the best
translation out of the three can be selected and used to
retrain the models. The process is illustrated in Figure 2.
Co-training thus automatically increases the size of
parallel corpora. There are a number of reasons why
machine translated items added during co-training can be
useful in the next round of training:
? vocabulary acquisition ? One problem that arises
from having a small training corpus is incomplete
word coverage. Without a word occurring in its
training corpus it is unlikely that a translation model
will produce a reasonable translation of it. Because
the initial training corpora can come from different
sources, a collection of translation models will be
more likely to have encountered a word before. This
Maison bleu Casa azulblaues Haus
???
Blue
maison
blaues
House
Blue house
2
Maison bleu Casa azulblaues Haus
Blue house
Blue
maison
blaues
Haus
Blue house
3
French German
Spanish
English target
4
French
some english sentence
some french sentenc
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
German English
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
Spanish English
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
1
English
French
some english sentence
some french sentenc
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
English
Maison
bleu
Blue
house
+
Spanish
some english sentence
some french sentenc
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
English
Casa azul
Blue
house
+
blaues
Haus
Blue
house
+
German
some english sentence
some french sentenc
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
some english sentence
some french
sentence
English
Figure 2: Co-training using German, French, and Spanish sources to produce English machine translations
leads to vocabulary acquisition during co-training.
? coping with morphology ? The problem mentioned
above is further exacerbated by the fact that most
current statistical translation formulations have an
incomplete treatment of morphology. This would be
a problem if the training data for a Spanish transla-
tion model contained the masculine form of a adjec-
tive, but not the feminine. Because languages vary
in how they use morphology (some languages have
grammatical gender whereas others don?t) one lan-
guage?s translation model might have the translation
of a particular word form whereas another?s would
not. Thus co-training can increase the inventory of
word forms and reduce the problem that morphol-
ogy poses to simple statistical translation models.
? improved word order ? A significant source of er-
rors in statistical machine translation is the word re-
ordering problem (Och et al, 1999). The word or-
der between related languages is often similar while
word order between distant language may differ sig-
nificantly. By including more examples through co-
training with related languages, the translation mod-
els for distant languages will better learn word order
mappings to the target language.
In all these cases the diversity afforded by multiple trans-
lation models increases the chances that the machine
translated sentences added to the initial bilingual corpora
will be accurate. Our co-training algorithm allows many
source languages to be used.
3 Experimental Results
In order to conduct co-training experiments we first
needed to assemble appropriate corpora. The corpus used
in our experiments was assembled from the data used in
the (Och and Ney, 2001) multiple source translation pa-
per. The data was gathered from the Bulletin of the Eu-
ropean Union which is published on the Internet in the
eleven official languages of the European Union. We
used a subset of the data to create a multi-lingual cor-
pus, aligning sentences between French, Spanish, Ger-
man, Italian and Portuguese (Simard, 1999). Addition-
ally we created bilingual corpora between English and
each of the five languages using sentences that were not
included in the multi-lingual corpus.
Och and Ney (2001) used the data to find a transla-
tion that was most probable given multiple source strings.
Och and Ney found that multi-source translations using
two source languages reduced word error rate when com-
pared to using source strings from a single language.
For multi-source translations using source strings in six
languages a greater reduction in word error rate was
achieved. Our work is similar in spirit, although instead
of using multi-source translation at the time of transla-
tion, we integrate it into the training stage. Whereas
Och and Ney use multiple source strings to improve the
quality of one translation only, our co-training method at-
tempts to improve the accuracy of all translation models
by bootstrapping more training data from multiple source
documents.
3.1 Software
The software that we used to train the statistical mod-
els and to produce the translations was GIZA++ (Och
and Ney, 2000), the CMU-Cambridge Language Model-
ing Toolkit (Clarkson and Rosenfeld, 1997), and the ISI
ReWrite Decoder. The sizes of the language models used
in each experiment were fixed throughout, in order to en-
sure that any gains that were made were not due to the
trivial reason of the language model improving (which
could be done by building a larger monolingual corpus of
the target language).
The experiments that we conducted used GIZA++ to
produce IBM Model 4 translation models. It should be
observed, however, that our co-training algorithm is en-
tirely general and may be applied to any formulation of
statistical machine translation which relies on parallel
Round Number
Translation Pair 0 1 2 3
French?English 55.2 56.3 57.0 55.5
Spanish?English 57.2 57.8 57.6 56.9
German?English 45.1 46.3 47.4 47.6
Italian?English 53.8 54.0 53.6 53.5
Portuguese?Eng 55.2 55.2 55.7 54.3
Table 1: Co-training results over three rounds
corpora for its training data.
3.2 Evaluation
The performance of translation models was evaluated us-
ing a held-out set of 1,000 sentences in each language,
with reference translations into English. Each translation
model was used to produce translation of these sentences
and the machine translations were compared to the ref-
erence human translations using word error rate (WER).
The results are reported in terms of increasing accuracy,
rather than decreasing error. We define accuracy as 100
minus WER.
Other evaluation metrics such as position independent
WER or the Bleu method (Papineni et al, 2001) could
have been used. While WER may not be the best measure
of translation quality, it is sufficient to track performance
improvements in the following experiments.
3.3 Co-training
Table 1 gives the result of co-training using the most
accurate translation from the candidate translations pro-
duced by five translation models. Each translation model
was initially trained on bilingual corpora consisting of
around 20,000 human translated sentences. These trans-
lation models were used to translate 63,000 sentences, of
which the top 10,000 were selected for the first round.
At the next round 53,000 sentences were translated and
the top 10,000 sentences were selected for the second
round. The final candidate pool contained 43,000 trans-
lations and again the top 10,000 were selected. The table
indicates that gains may be had from co-training. Each
of the translation models improves over its initial training
size at some point in the co-training. The German to En-
glish translation model improves the most ? exhibiting a
2.5% improvement in accuracy.
The table further indicates that co-training for ma-
chine translation suffers the same problem reported in
Pierce and Cardie (2001): gains above the accuracy of
the initial corpus are achieved, but decline as after a cer-
tain number of machine translations are added to the
training set. This could be due in part to the manner
in items are selected for each round. Because the best
translations are transferred from the candidate pool to the
2727.52828.52929.530 100
00
15000
20000
25000
30000
35000
40000
Accuracy (100 - Word Error Rate)
Trainin
g Corp
us Siz
e (numb
er of se
ntence p
airs)Coachin
g of G
erman
Figure 3: ?Coaching? of German to English by a French
to English translation model
43.84444.244.444.644.84545.2 100
000
15000
0
20000
0
25000
0
30000
0
35000
0
40000
0
Accuracy (100 - Word Error Rate)
Trainin
g Corp
us Siz
e (numb
er of se
ntence p
airs)Coachin
g of G
erman
Figure 4: ?Coaching? of German to English by multiple
translation models
training pool at each round the number of ?easy? trans-
lations diminishes over time. Because of this, the av-
erage accuracy of the training corpora decreased with
each round, and the amount of noise being introduced
increased. The accuracy gains from co-training might
extend for additional rounds if the size of the candidate
pool were increased, or if some method were employed
to reduce the amount of noise being introduced.
3.4 Coaching
In order to simulate using co-training for language pairs
without extensive parallel corpora, we experimented with
a variation on co-training for machine translation that
we call ?coaching?. It employs two translation models
of vastly different size. In this case we used a French
to English translation model built from 60,000 human
translated sentences and a German to English translation
model that contained no human translated sentences. The
German-English translation model was meant to repre-
sent a language pair with extremely impoverished paral-
lel corpus. Coaching is therefore a special case of co-
training in that one view (the superior one) never retrains
upon material provided by the other (inferior) view.
A German-English parallel corpus was created by tak-
ing a French-German parallel corpus, translating the
French sentences into English and then aligning the trans-
lations with the German sentences. In this experiment the
machine translations produced by the French?English
translation model were always selected. Figure 3 shows
the performance of the resulting German to English trans-
lation model for various sized machine produced parallel
corpora.
We explored this method further by translating 100,000
sentences with each of the non-German translation mod-
els from the co-training experiment in Section 3.3. The
result was a German-English corpus containing 400,000
sentence pairs. The performance of the resulting model
matches the initial accuracy of the model. Thus machine-
translated corpora achieved equivalent quality to human-
translated corpora after two orders of magnitude more
data was added.
The graphs illustrate that increasing the performance
of translation models may be achievable using machine
translations alone. Rather than the 2.5% improvement
gained in co-training experiments wherein models of sim-
ilar sizes were used, coaching achieves an 18%(+) im-
provement by pairing translation models of radically dif-
ferent sizes.
4 Discussion and Future Work
In this paper we presented two methods for the automatic
creation of additional parallel corpora. Co-training uses a
number of different human translated parallel corpora to
create additional data for each of them, leading to modest
increases in translation quality. Coaching uses existing
resources to create a fully machine translated corpora ?
essentially reverse engineering the knowledge present in
the human translated corpora and transferring that to an-
other language. This has significant implications for the
feasibility of using statistical translation methods for lan-
guage pairs for which extensive parallel corpora do not
exist.
A setting in which this would become extremely use-
ful is if the European Union extends membership to a
new country like Turkey, and wants develop translation
resources for its language. One can imagine that sizable
parallel corpora might be available between Turkish and a
few EU languages like Greek and Italian. However, there
may be no parallel corpora between Turkish and Finnish.
Our methods could exploit existing parallel corpora be-
tween the current EU language and use machine transla-
tions from Greek and Italian in order to create a machine
translation system between Turkish and Finnish.
We plan to extend our work by moving from co-
training and its variants to another weakly supervised
learning method, active learning. Active learning incor-
porates human translations along with machine transla-
tions, which should ensure better resulting quality than
using machine translations alone. It will reduce the cost
of creating a parallel corpus entirely by hand, by selec-
tively and judiciously querying a human translator. In
order to make the most effective use of the human trans-
lator?s time we will be required to design an effective se-
lection algorithm, which is something that was neglected
in our current research. An effective selection algorithm
for active learning will be one which chooses those exam-
ples which will add the most information to the machine
translation system, and therefore minimizes the amount
of time a human needs to spend translating sentences.
References
Steve Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics.
Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Daniel Marcu, and Ya-
mada Kenji. 2000. Translating with scarce resources.
In Proceedings of the National Conference on Artificial
Intelligence (AAAI).
Adam Berger, Peter Brown, Stephen Della Pietra, Vin-
cent Della Pietra, John Gillett, John Lafferty, Robert
Mercer, Harry Printz, and Lubos Ures. 1994. The
Candide system for machine translation.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Workshop on Computational Learning The-
ory.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Compuata-
tional Linguistics, 19(2):263?311, June.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge toolkit.
In ESCA Eurospeech Proceedings.
Mitchell P. Marcus, Beatrice Santori, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Franz Joseph Och and Herman Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, October.
Franz Joseph Och and Herman Ney. 2001. Statistical
multi-source translation. In MT Summit 2001, pages
253?258, Santiago de Compostela, Spain, September.
Franz Joseph Och, Christop Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint Confer-
ence of Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, College Park, Mary-
land, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report,
September.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing.
Philip Resnik. 1998. Parallel strands: A preliminary
investigation into mining the web for bilingual text.
In Third Conference of the Association for Machine
Translation in the Americas.
Michel Simard. 1999. Text-translation alignment:
Aligning three or more versions of a text. In Jean
Veronis, editor, Parallel Text Processing. Kluwer Aca-
demic.
Noah Smith. 2002. From words to corpora: Recognizing
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
Philadelphia, Pennsylvania.
Active learning for HPSG parse selection
Jason Baldridge and Miles Osborne
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
  jmb,osborne  @cogsci.ed.ac.uk
Abstract
We describe new features and algorithms for
HPSG parse selection models and address the
task of creating annotated material to train
them. We evaluate the ability of several sam-
ple selection methods to reduce the number
of annotated sentences necessary to achieve a
given level of performance. Our best method
achieves a 60% reduction in the amount of
training material without any loss in accuracy.
1 Introduction
Even with significant resources such as the Penn Tree-
bank, a major bottleneck for improving statistical parsers
has been the lack of sufficient annotated material from
which to estimate their parameters. Most statistical pars-
ing research, such as Collins (1997), has centered on
training probabilistic context-free grammars using the
Penn Treebank. For richer linguistic frameworks, such as
Head-Driven Phrase Structure Grammar (HPSG), there
is even less annotated material available for training
stochastic parsing models. There is thus a pressing need
to create significant volumes of annotated material in a
logistically efficient manner. Even if it were possible to
bootstrap from the Penn Treebank, it is still unlikely that
there would be sufficient quantities of high quality mate-
rial.
There has been a strong focus in recent years on us-
ing the active learning technique of selective sampling to
reduce the amount of human-annotated training material
needed to train models for various natural language pro-
cessing tasks. The aim of selective sampling is to iden-
tify the most informative examples, according to some se-
lection method, from a large pool of unlabelled material.
Such selected examples are then manually labelled. Se-
lective sampling has typically been applied to classifica-
tion tasks, but has also been shown to reduce the number
of examples needed for inducing Lexicalized Tree Inser-
tion Grammars from the Penn Treebank (Hwa, 2000).
The suitability of active learning for HPSG-type gram-
mars has as yet not been explored. This paper addresses
the problem of minimizing the human effort expended in
creating annotated training material for HPSG parse se-
lection by using selective sampling. We do so in the con-
text of Redwoods (Oepen et al, 2002), a treebank that
contains HPSG analyses for sentences from the Verbmo-
bil appointment scheduling and travel planning domains.
We show that sample selection metrics based on tree en-
tropy (Hwa, 2000) and disagreement between two differ-
ent parse selection models significantly reduce the num-
ber of annotated sentences necessary to match a given
level of performance according to random selection. Fur-
thermore, by combining these two methods as an ensem-
ble selection method, we require even fewer examples ?
achieving a 60% reduction in the amount of annotated
training material needed to outperform a model trained
on randomly selected material. These results suggest
that significant reductions in human effort can be real-
ized through selective sampling when creating annotated
material for linguistically rich grammar formalisms.
As the basis of our active learning approach, we create
both log-linear and perceptron models, the latter of which
has not previously been used for feature-based grammars.
We show that the different biases of the two types of mod-
els is sufficient to create diverse members for a commit-
tee, even when they use exactly the same features. With
respect to the features used to train models, we demon-
strate that a very simple feature selection strategy that ig-
nores the proper structure of trees is competitive with one
that fully respects tree configurations.
The structure of the paper is as follows. In sections 2
and 3, we briefly introduce active learning and the Red-
woods treebank. Section 4 discusses the parse selection
models that we use in the experiments. In sections 5 and
6, we explain the different selection methods that we use
for active learning and explicate the setup in which the
experiments were conducted. Finally, the results of the
experiments are presented and discussed in section 7.
2 Active Learning
Active learning attempts to reduce the number of exam-
ples needed for training statistical models by allowing
the machine learner to directly participate in creating the
corpus it uses. There are a several approaches to active
learning; here, we focus on selective sampling (Cohn et
al., 1994), which involves identifying the most informa-
tive examples from a pool of unlabelled data and pre-
senting only these examples to a human expert for an-
notation. The two main flavors of selective sampling are
certainty-based methods and committee-based methods
(Thompson et al, 1999). For certainty-based selection,
the examples chosen for annotation are those for which
a single learner is least confident, as determined by some
criterion. Committee-based selection involves groups of
learners that each maintain different hypotheses about
the problem; examples on which the learners disagree in
some respect are typically regarded as the most informa-
tive.
Active learning has been successfully applied to a
number of natural language oriented tasks, including text
categorization (Lewis and Gale, 1994) and part-of-speech
tagging (Engelson and Dagan, 1996). Hwa (2000) shows
that certainty-based selective sampling can reduce the
amount of training material needed for inducing Prob-
abilistic Lexicalized Tree Insertion Grammars by 36%
without degrading the quality of the grammars. Like
Hwa, we investigate active learning for parsing and thus
seek informative sentences; however, rather than induc-
ing grammars, our task is to select the best parse from the
output of an existing hand-crafted grammar by using the
Redwoods treebank.
3 The Redwoods Treebank
The English Resource Grammar (ERG, Flickinger
(2000)) is a broad coverage HPSG grammar that provides
deep semantic analyses of sentences but has no means to
prefer some analyses over others because of its purely
symbolic nature. To address this limitation, the Red-
woods treebank has been created to provide annotated
training material to permit statistical models for ambigu-
ity resolution to be combined with the precise interpreta-
tions produced by the ERG (Oepen et al, 2002).
Whereas the Penn Treebank has an implicit grammar
underlying its parse trees, Redwoods uses the ERG ex-
plicitly. For each utterance, Redwoods enumerates the
set of analyses, represented as derivation trees, licensed
by the ERG and identifies which analysis is the preferred
one. For example, Figure 1 shows the preferred deriva-
fillhead wh r
noptcomp
what1
what
hcomp
hcomp
sailr
can aux pos
can
i
i
hadj i uns
extracomp
bse verb infl rule
do2
do
hcomp
for
for
you
you
Figure 1: Redwoods derivation tree for the sentence what
can I do for you? The node labels are the names of the
ERG rules used to build the analysis.
tion tree, out of three ERG analyses, for what can I do
for you?. From such derivation trees, the parse trees and
semantic interpretations can be recovered using an HPSG
parser.
Redwoods is (semi-automatically) updated after
changes have been made to the ERG, and it has thus far
gone through three growths. Some salient characteris-
tics of the first and third growths are given in Table 1 for
utterances for which a unique preferred parse has been
identified and for which there are at least two analyses.1
The ambiguity increased considerably between the first
and third growths, reflecting the increased coverage of
the ERG for more difficult sentences.
corpus sentences length parses
Redwoods-1 3799 7.9 9.7
Redwoods-3 5302 9.3 58.0
Table 1: Characteristics of subsets of Redwoods versions
used for the parse selection task. The columns indi-
cate the number of sentences in the subset, their average
length, and their average number of parses.
The small size of the treebank makes it essential to
explore the possibility of using methods such as active
learning to speed the creation of more annotated material
for training parse selection models.
4 Parse Selection
Committee-based active learning requires multiple learn-
ers which have different biases that cause them to make
different predictions sometimes. As in co-training, one
1There are over 1400 utterances in both versions for which
the ERG produces only one analysis and which therefore are
irrelevant for parse selection. They contain no discriminating
information and are thus not useful for the machine learning
algorithms discussed in the next section.
uni   hcomp 	
bi  


what1 	  



hcomp 	
tri  


noptcomp 	  


what1 	  



hcomp 	
Figure 2: Three example ngram features based on the
derivation tree in Figure 1.
way such diverse learners can be created is by using in-
dependent or partially independent feature sets to reduce
the error correlation between the learners. Another way
is to use different machine learning algorithms trained on
the same feature set. In this section, we discuss two fea-
ture sets and two machine learning algorithms that are
used to produce four distinct models and we give their
overall performance on the parse selection task.
4.1 Features
Our two feature sets are created by using only the deriva-
tion trees made available in Redwoods. The configura-
tional set is loosely based on the derivation tree features
given by Toutanova and Manning (2002), and thus en-
codes standard relations such as grandparent-of and left-
sibling for the nodes in the tree. The ngram set is created
by flattening derivation trees and treating them as strings
of rule names over which ngrams are extracted, taking up
to four rule names at a time and including the number of
intervening parentheses between them. We ignore ortho-
graphic values for both feature sets.
As examples of typical ngram features, the derivation
tree given in Figure 1 generates features such as those de-
picted in Figure 2. Such features provide a reasonable ap-
proximation of trees that implicitly encodes many of the
interesting relationships that are typically gathered from
them, such as grandparent and sibling relations. They
also capture further relationships that cross the brackets
of the actual tree, providing some more long-distance re-
lationships than the configurational features.
4.2 Algorithms
We use both log-linear and perceptron algorithms to cre-
ate parse selection models. Both frameworks use iter-
ative procedures to determine the weights 


of a corresponding set of features Bootstrapping POS taggers using Unlabelled Data
Stephen Clark, James R. Curran and Miles Osborne
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh. EH8 9LW
fstephenc,jamesc,osborneg@cogsci.ed.ac.uk
Abstract
This paper investigates booststrapping part-of-
speech taggers using co-training, in which two
taggers are iteratively re-trained on each other?s
output. Since the output of the taggers is noisy,
there is a question of which newly labelled ex-
amples to add to the training set. We investi-
gate selecting examples by directly maximising
tagger agreement on unlabelled data, a method
which has been theoretically and empirically
motivated in the co-training literature. Our
results show that agreement-based co-training
can significantly improve tagging performance
for small seed datasets. Further results show
that this form of co-training considerably out-
performs self-training. However, we find that
simply re-training on all the newly labelled data
can, in some cases, yield comparable results to
agreement-based co-training, with only a frac-
tion of the computational cost.
1 Introduction
Co-training (Blum and Mitchell, 1998), and several vari-
ants of co-training, have been applied to a number of
NLP problems, including word sense disambiguation
(Yarowsky, 1995), named entity recognition (Collins
and Singer, 1999), noun phrase bracketing (Pierce and
Cardie, 2001) and statistical parsing (Sarkar, 2001;
Steedman et al, 2003). In each case, co-training was
used successfully to bootstrap a model from only a small
amount of labelled data and a much larger pool of un-
labelled data. Previous co-training approaches have typ-
ically used the score assigned by the model as an indi-
cator of the reliability of a newly labelled example. In
this paper we take a different approach, based on theoret-
ical work by Dasgupta et al (2002) and Abney (2002), in
which newly labelled training examples are selected us-
ing a greedy algorithm which explicitly maximises the
POS taggers? agreement on unlabelled data.
We investigate whether co-training based upon di-
rectly maximising agreement can be successfully ap-
plied to a pair of part-of-speech (POS) taggers: the
Markov model TNT tagger (Brants, 2000) and the max-
imum entropy C&C tagger (Curran and Clark, 2003).
There has been some previous work on boostrap-
ping POS taggers (e.g., Zavrel and Daelemans (2000) and
Cucerzan and Yarowsky (2002)), but to our knowledge
no previous work on co-training POS taggers.
The idea behind co-training the POS taggers is very
simple: use output from the TNT tagger as additional
labelled data for the maximum entropy tagger, and vice
versa, in the hope that one tagger can learn useful infor-
mation from the output of the other. Since the output of
both taggers is noisy, there is a question of which newly
labelled examples to add to the training set. The addi-
tional data should be accurate, but also useful, providing
the tagger with new information. Our work differs from
the Blum and Mitchell (1998) formulation of co-training
by using two different learning algorithms rather than two
independent feature sets (Goldman and Zhou, 2000).
Our results show that, when using very small amounts
of manually labelled seed data and a much larger amount
of unlabelled material, agreement-based co-training can
significantly improve POS tagger accuracy. We also show
that simply re-training on all of the newly labelled data
is surprisingly effective, with performance depending on
the amount of newly labelled data added at each itera-
tion. For certain sizes of newly labelled data, this sim-
ple approach is just as effective as the agreement-based
method. We also show that co-training can still benefit
both taggers when the performance of one tagger is ini-
tially much better than the other.
We have also investigated whether co-training can im-
prove the taggers already trained on large amounts of
                                                               Edmonton, May-June 2003
                                                    held at HLT-NAACL 2003 , pp. 49-55
                                            Proceeings of the Seventh CoNLL conference
manually annotated data. Using standard sections of the
WSJ Penn Treebank as seed data, we have been unable
to improve the performance of the taggers using self-
training or co-training.
Manually tagged data for English exists in large quan-
tities, which means that there is no need to create taggers
from small amounts of labelled material. However, our
experiments are relevant for languages for which there
is little or no annotated data. We only perform the ex-
periments in English for convenience. Our experiments
can also be seen as a vehicle for exploring aspects of co-
training.
2 Co-training
Given two (or more) ?views? (as described in
Blum and Mitchell (1998)) of a classification task,
co-training can be informally described as follows:
 Learn separate classifiers for each view using a
small amount of labelled seed data.
 Use each classifier to label some previously unla-
belled data.
 For each classifier, add some subset of the newly la-
belled data to the training data.
 Retrain the classifiers and repeat.
The intuition behind the algorithm is that each classi-
fier is providing extra, informative labelled data for the
other classifier(s). Blum and Mitchell (1998) derive PAC-
like guarantees on learning by assuming that the two
views are individually sufficient for classification and the
two views are conditionally independent given the class.
Collins and Singer (1999) present a variant of the
Blum and Mitchell algorithm, which directly maximises
an objective function that is based on the level of
agreement between the classifiers on unlabelled data.
Dasgupta et al (2002) provide a theoretical basis for this
approach by providing a PAC-like analysis, using the
same independence assumption adopted by Blum and
Mitchell. They prove that the two classifiers have low
generalisation error if they agree on unlabelled data.
Abney (2002) argues that the Blum and Mitchell in-
dependence assumption is very restrictive and typically
violated in the data, and so proposes a weaker indepen-
dence assumption, for which the Dasgupta et al (2002)
results still hold. Abney also presents a greedy algorithm
that maximises agreement on unlabelled data, which pro-
duces comparable results to Collins and Singer (1999) on
their named entity classification task.
Goldman and Zhou (2000) show that, if the newly la-
belled examples used for re-training are selected care-
fully, co-training can still be successful even when the
views used by the classifiers do not satisfy the indepen-
dence assumption.
In remainder of the paper we present a practical
method for co-training POS taggers, and investigate the
extent to which example selection based on the work of
Dasgupta et al and Abney can be effective.
3 The POS taggers
The two POS taggers used in the experiments are TNT, a
publicly available Markov model tagger (Brants, 2000),
and a reimplementation of the maximum entropy (ME)
tagger MXPOST (Ratnaparkhi, 1996). The ME tagger,
which we refer to as C&C, uses the same features as MX-
POST, but is much faster for training and tagging (Cur-
ran and Clark, 2003). Fast training and tagging times
are important for the experiments performed here, since
the bootstrapping process can require many tagging and
training iterations.
The model used by TNT is a standard tagging Markov
model, consisting of emission probabilities, and transi-
tion probabilities based on trigrams of tags. It also deals
with unknown words using a suffix analysis of the target
word (the word to be tagged). TNT is very fast for both
training and tagging.
The C&C tagger differs in a number of ways from
TNT. First, it uses a conditional model of a tag sequence
given a string, rather than a joint model. Second, ME
models are used to define the conditional probabilities of
a tag given some context. The advantage of ME mod-
els over the Markov model used by TNT is that arbitrary
features can easily be included in the context; so as well
as considering the target word and the previous two tags
(which is the information TNT uses), the ME models also
consider the words either side of the target word and, for
unknown and infrequent words, various properties of the
string of the target word.
A disadvantage is that the training times for ME mod-
els are usually relatively slow, especially with iterative
scaling methods (see Malouf (2002) for alternative meth-
ods). Here we use Generalised Iterative Scaling (Dar-
roch and Ratcliff, 1972), but our implementation is much
faster than Ratnaparkhi?s publicly available tagger. The
C&C tagger trains in less than 7 minutes on the 1 million
words of the Penn Treebank, and tags slightly faster than
TNT.
Since the taggers share many common features, one
might think they are not different enough for effective
co-training to be possible. In fact, both taggers are suffi-
ciently different for co-training to be effective. Section 4
shows that both taggers can benefit significantly from the
information contained in the other?s output.
The performance of the taggers on section 00 of the
WSJ Penn Treebank is given in Table 1, for different seed
set sizes (number of sentences). The seed data is taken
Tagger 50 seed 500 seed ? 40,000 seed
TNT 81.3 91.0 96.5
C&C 73.2 88.3 96.8
Table 1: Tagger performance for different seed sets
from sections 2?21 of the Treebank. The table shows that
the performance of TNT is significantly better than the
performance of C&C when the size of the seed data is
very small.
4 Experiments
The co-training framework uses labelled examples from
one tagger as additional training data for the other. For
the purposes of this paper, a labelled example is a tagged
sentence. We chose complete sentences, rather than
smaller units, because this simplifies the experiments and
the publicly available version of TNT requires complete
tagged sentences for training. It is possible that co-
training with sub-sentential units might be more effective,
but we leave this as future work.
The co-training process is given in Figure 1. At
each stage in the process there is a cache of unla-
belled sentences (selected from the total pool of un-
labelled sentences) which is labelled by each tagger.
The cache size could be increased at each iteration,
which is a common practice in the co-training litera-
ture. A subset of those sentences labelled by TNT is
then added to the training data for C&C, and vice versa.
Blum and Mitchell (1998) use the combined set of newly
labelled examples for training each view, but we fol-
low Goldman and Zhou (2000) in using separate labelled
sets. In the remainder of this section we consider two pos-
sible methods for selecting a subset. The cache is cleared
after each iteration.
There are various ways to select the labelled examples
for each tagger. A typical approach is to select those ex-
amples assigned a high score by the relevant classifier,
under the assumption that these examples will be the most
reliable. A score-based selection method is difficult to
apply in our experiments, however, since TNT does not
provide scores for tagged sentences.
We therefore tried two alternative selection methods.
The first is to simply add all of the cache labelled by one
tagger to the training data of the other. We refer to this
method as naive co-training. The second, more sophisti-
cated, method is to select that subset of the labelled cache
which maximises the agreement of the two taggers on un-
labelled data. We call this method agreement-based co-
training. For a large cache the number of possible subsets
makes exhaustive search intractable, and so we randomly
sample the subsets.
S is a seed set of labelled sentences
LT is labelled training data for TNT
LC is labelled training data for C&C
U is a large set of unlabelled sentences
C is a cache holding a small subset of U
initialise:
LT ? LC ? S
Train TNT and C&C on S
loop:
Partition U into the disjoint sets C and U?.
Label C with TNT and C&C
Select sentences labelled by TNT and add to LC
Train C&C on LC
Select sentences labelled by C&C and add to LT
Train TNT on LT
U = U?.
Until U is empty
Figure 1: The general co-training process
C is a cache of sentences labelled by the other tagger
U is a set of sentences, used for measuring agreement
initialise:
cmax ? ?; Amax ? 0
Repeat n times:
Randomly sample c ? C
Retrain current tagger using c as additional data
if new agreement rate, A, on U > Amax
Amax ? A; cmax ? c
return cmax
Figure 2: Agreement-based example selection
The pseudo-code for the agreement-based selection
method is given in Figure 2. The current tagger is the
one being retrained, while the other tagger is kept static.
The co-training process uses the selection method for se-
lecting sentences from the cache (which has been labelled
by one of the taggers). Note that during the selection pro-
cess, we repeatedly sample from all possible subsets of
the cache; this is done by first randomly choosing the
size of the subset and then randomly choosing sentences
based on the size. The number of subsets we consider is
determined by the number of times the loop is traversed
in Figure 2.
If TNT is being trained on the output of C&C, then the
most recent version of C&C is used to measure agreement
(and vice versa); so we first attempt to improve one tag-
ger, then the other, rather than both at the same time. The
agreement rate of the taggers on unlabelled sentences is
the per-token agreement rate; that is, the number of times
each word in the unlabelled set of sentences is assigned
the same tag by both taggers.
For the small seed set experiments, the seed data was
an arbitrarily chosen subset of sections 10?19 of the
WSJ Penn Treebank; the unlabelled training data was
taken from 50, 000 sentences of the 1994 WSJ section
of the North American News Corpus (NANC); and the
unlabelled data used to measure agreement was around
10, 000 sentences from sections 1?5 of the Treebank.
Section 00 of the Treebank was used to measure the ac-
curacy of the taggers. The cache size was 500 sentences.
4.1 Self-Training and Agreement-based Co-training
Results
Figure 3 shows the results for self-training, in which each
tagger is simply retrained on its own labelled cache at
each round. (By round we mean the re-training of a sin-
gle tagger, so there are two rounds per co-training itera-
tion.) TNT does improve using self-training, from 81.4%
to 82.2%, but C&C is unaffected. Re-running these ex-
periments using a range of unlabelled training sets, from
a variety of sources, showed similar behaviour.
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 3: Self-training TNT and C&C (50 seed sen-
tences). The upper curve is for TNT; the lower curve is
for C&C.
Figure 4 gives the results for the greedy agreement co-
training, using a cache size of 500 and searching through
100 subsets of the labelled cache to find the one that max-
imises agreement. Co-training improves the performance
of both taggers: TNT improves from 81.4% to 84.9%,
and C&C improves from 73.2% to 84.3% (an error re-
duction of over 40%).
Figures 5 and 6 show the self-training results and
agreement-based results when a larger seed set, of 500
sentences, is used for each tagger. In this case, self-
training harms TNT and C&C is again unaffected. Co-
training continues to be beneficial.
Figure 7 shows how the size of the labelled data set (the
number of sentences) grows for each tagger per round.
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 4: Agreement-based co-training between
TNT and C&C (50 seed sentences). The curve that
starts at a higher value is for TNT.
0.88
0.885
0.89
0.895
0.9
0.905
0.91
0.915
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 5: Self-training TNT and C&C (500 seed sen-
tences). The upper curve is for TNT; the lower curve is
for C&C.
Towards the end of the co-training run, more material is
being selected for C&C than TNT. The experiments us-
ing a seed set size of 50 showed a similar trend, but the
difference between the two taggers was less marked. By
examining the subsets chosen from the labelled cache at
each round, we also observed that a large proportion of
the cache was being selected for both taggers.
4.2 Naive Co-training Results
Agreement-based co-training for POS taggers is effective
but computationally demanding. The previous two agree-
ment maximisation experiments involved retraining each
tagger 2, 500 times. Given this, and the observation that
maximisation generally has a preference for selecting a
large proportion of the labelled cache, we looked at naive
co-training: simply retraining upon all available mate-
0.88
0.885
0.89
0.895
0.9
0.905
0.91
0.915
0.92
0 5 10 15 20 25 30 35 40 45 50
Ac
cu
ra
cy
Number of rounds
TnT
C&C
Figure 6: Agreement-based co-training between
TNT and C&C (500 seed sentences). The curve that
starts at a higher value is for TNT.
0
2000
4000
6000
8000
10000
12000
0 5 10 15 20 25 30 35 40 45 50
C&C
tntTnT
Figure 7: Growth in training-set sizes for co-training
TNT and C&C (500 seed sentences). The upper curve
is for C&C.
rial (i.e. the whole cache) at each round. Table 2 shows
the naive co-training results after 50 rounds of co-training
when varying the size of the cache. 50 manually labelled
sentences were used as the seed material. Table 3 shows
results for the same experiment, but this time with a seed
set of 500 manually labelled sentences.
We see that naive co-training improves as the cache
size increases. For a large cache, the performance lev-
els for naive co-training are very similar to those pro-
duced by our agreement-based co-training method. Af-
ter 50 rounds of co-training using 50 seed sentences,
the agreement rates for naive and agreement-based co-
training were very similar: from an initial value of 73%
to 97% agreement.
Naive co-training is more efficient than agreement-
based co-training. For the parameter settings used in
Amount added TNT C&C
0 81.3 73.2
50 82.9 82.7
100 83.5 83.3
150 84.4 84.3
300 85.0 84.9
500 85.3 85.1
Table 2: Naive co-training accuracy results when varying
the amount added after each round (50 seed sentences)
Amount added TNT C&C
0 91.0 88.3
100 92.0 91.9
300 92.0 91.9
500 92.1 92.0
1000 92.0 91.9
Table 3: Naive co-training accuracy results when varying
the amount added after each round (500 seed sentences)
the previous experiments, agreement-based co-training
required the taggers to be re-trained 10 to 100 times
more often then naive co-training. There are advan-
tages to agreement-based co-training, however. First,
the agreement-based method dynamically selects the best
sample at each stage, which may not be the whole cache.
In particular, when the agreement rate cannot be im-
proved upon, the selected sample can be rejected. For
naive co-training, new samples will always be added,
and so there is a possibility that the noise accumulated
at later stages will start to degrade performance (see
Pierce and Cardie (2001)). Second, for naive co-training,
the optimal amount of data to be added at each round (i.e.
the cache size) is a parameter that needs to be determined
on held out data, whereas the agreement-based method
determines this automatically.
4.3 Larger-Scale Experiments
We also performed a number of experiments using much
more unlabelled training material than before. Instead
of using 50, 000 sentences from the 1994 WSJ section of
the North American News Corpus, we used 417, 000 sen-
tences (from the same section) and ran the experiments
until the unlabelled data had been exhausted.
One experiment used naive co-training, with 50 seed
sentences and a cache of size 500. This led to an agree-
ment rate of 99%, with performance levels of 85.4% and
85.4% for TNT and C&C respectively. 230, 000 sen-
tences (? 5 million words) had been processed and were
used as training material by the taggers. The other ex-
periment used our agreement-based co-training approach
(50 seed sentences, cache size of 1, 000 sentences, explor-
ing at most 10 subsets in the maximisation process per
round). The agreement rate was 98%, with performance
levels of 86.0% and 85.9% for both taggers. 124, 000
sentences had been processed, of which 30, 000 labelled
sentences were selected for training TNT and 44, 000 la-
belled sentences were selected for training C&C.
Co-training using this much larger amount of unla-
belled material did improve our previously mentioned re-
sults, but not by a large margin.
4.4 Co-training using Imbalanced Views
It is interesting to consider what happens when one view
is initially much more accurate than the other view. We
trained one of the taggers on much more labelled seed
data than the other, to see how this affects the co-training
process. Both taggers were initialised with either 500 or
50 seed sentences, and agreement-based co-training was
applied, using a cache size of 500 sentences. The results
are shown in Table 4.
Seed material Initial Perf Final Perf
TNT C&C TNT C&C TNT C&C
50 500 81.3 88.3 90.0 89.4
500 50 91.0 73.2 91.3 91.3
Table 4: Co-training Results for Imbalanced Views
Co-training continues to be effective, even when the
two taggers are imbalanced. Also, the final performance
of the taggers is around the same value, irrespective of
the direction of the imbalance.
4.5 Large Seed Experiments
Although bootstrapping from unlabelled data is particu-
larly valuable when only small amounts of training ma-
terial are available, it is also interesting to see if self-
training or co-training can improve state of the art POS
taggers.
For these experiments, both C&C and TNT were ini-
tially trained on sections 00?18 of the WSJ Penn Tree-
bank, and sections 19?21 and 22?24 were used as the
development and test sets. The 1994?1996 WSJ text
from the NANC was used as unlabelled material to fill the
cache.
The cache size started out at 8000 sentences and in-
creased by 10% in each round to match the increasing
labelled training data. In each round of self-training or
naive co-training 10% of the cache was randomly se-
lected and added to the labelled training data. The ex-
periments ran for 40 rounds.
The performance of the different training regimes is
listed in Table 5. These results show no significant im-
provement using either self-training or co-training with
very large seed datasets. Self-training shows only a slight
Method WSJ19?21 WSJ22?24
C&C TNT C&C TNT
Initial 96.71 96.50 96.78 96.46
Self-train 96.77 96.45 96.87 96.42
Naive co-train 96.74 96.48 96.76 96.46
Table 5: Performance with large seed sets
improvement for C&C1 while naive co-training perfor-
mance is always worse.
5 Conclusion
We have shown that co-training is an effective technique
for bootstrapping POS taggers trained on small amounts
of labelled data. Using unlabelled data, we are able to
improve TNT from 81.3% to 86.0%, whilst C&C shows
a much more dramatic improvement of 73.2% to 85.9%.
Our agreement-based co-training results support
the theoretical arguments of Abney (2002) and
Dasgupta et al (2002), that directly maximising the
agreement rates between the two taggers reduces gen-
eralisation error. Examination of the selected subsets
showed a preference for a large proportion of the cache.
This led us to propose a naive co-training approach,
which significantly reduced the computational cost
without a significant performance penalty.
We also showed that naive co-training was unable to
improve the performance of the taggers when they had
already been trained on large amounts of manually anno-
tated data. It is possible that agreement-based co-training,
using more careful selection, would result in an improve-
ment. We leave these experiments to future work, but
note that there is a large computational cost associated
with such experiments.
The performance of the bootstrapped taggers is still
a long way behind a tagger trained on a large amount
of manually annotated data. This finding is in accord
with earlier work on bootstrapping taggers using EM (El-
worthy, 1994; Merialdo, 1994). An interesting question
would be to determine the minimum number of manually
labelled examples that need to be used to seed the sys-
tem before we can achieve comparable results as using
all available manually labelled sentences.
For our experiments, co-training never led to a de-
crease in performance, regardless of the number of itera-
tions. The opposite behaviour has been observed in other
applications of co-training (Pierce and Cardie, 2001).
Whether this robustness is a property of the tagging prob-
lem or our approach is left for future work.
1This is probably by chance selection of better subsets.
Acknowledgements
This work has grown out of many fruitful discus-
sions with the 2002 JHU Summer Workshop team that
worked on weakly supervised bootstrapping of statistical
parsers. The first author was supported by EPSRC grant
GR/M96889, and the second author by a Commonwealth
scholarship and a Sydney University Travelling scholar-
ship. We would like to thank the anonymous reviewers
for their helpful comments, and also Iain Rae for com-
puter support.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100, Madisson, WI.
Thorsten Brants. 2000. TnT - a statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, pages 224?231.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Empirical Methods in NLP Conference, pages
100?110, University of Maryland, MD.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th Workshop on
Computational Language Learning, Taipei, Taiwan.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and Smoothing for Maximum Entropy Taggers.
In Proceedings of the 11th Annual Meeting of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Budapest, Hungary. (to appear).
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. The Annals of Math-
ematical Statistics, 43(5):1470?1480.
Sanjoy Dasgupta, Michael Littman, and David
McAllester. 2002. PAC generalization bounds
for co-training. In T. G. Dietterich, S. Becker,
and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14, pages 375?382,
Cambridge, MA. MIT Press.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of the 4th Conference
on Applied Natural Language Processing, pages 53?
58, Stuttgart, Germany.
Sally Goldman and Yan Zhou. 2000. Enhancing super-
vised learning with unlabeled data. In Proceedings of
the 17th International Conference on Machine Learn-
ing, Stanford, CA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Language
Learning, pages 49?55, Taipei, Taiwan.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the Empirical Methods in
NLP Conference, Pittsburgh, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP Con-
ference, pages 133?142, Philadelphia, PA.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of the 2nd Annual
Meeting of the NAACL, pages 95?102, Pittsburgh, PA.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the 11th Annual Meeting of the European
Chapter of the Association for Computational Linguis-
tics, Budapest, Hungary. (to appear).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
Jakub Zavrel and Walter Daelemans. 2000. Bootstrap-
ping a tagged corpus through combination of exist-
ing heterogeneous taggers. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation, pages 17?20, Athens, Greece.
Active Learning and the Total Cost of Annotation
Jason Baldridge and Miles Osborne
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
{jbaldrid,miles}@inf.ed.ac.uk
Abstract
Active learning (AL) promises to reduce
the cost of annotating labeled datasets for
trainable human language technologies.
Contrary to expectations, when creating
labeled training material for HPSG parse
selection and later reusing it with other
models, gains from AL may be negligible
or even negative. This has serious impli-
cations for using AL, showing that addi-
tional cost-saving strategies may need to
be adopted. We explore one such strategy:
using a model during annotation to auto-
mate some of the decisions. Our best re-
sults show an 80% reduction in annotation
cost compared with labeling randomly se-
lected data with a single model.
1 Introduction
AL methods such as uncertainty sampling (Cohn
et al, 1995) or query by committee (Seung et al,
1992) have all been shown to dramatically reduce
the cost of creating highly informative labeled sets
for speech and language technologies. However, ex-
periments using AL assume a model that is fixed
ahead of time: the model used in AL is the same
one we are currently developing training material
for. For many complex tasks, we are unlikely to have
a clear idea how best to model the task at the time of
annotation; thus, in practice, we will need to reuse
the labeled training material with other models.
In this paper, we show that AL can be brittle: un-
der a variety of natural reuse scenarios (for example,
allowing the later model to improve in quality, or
else reusing the labeled training material using a dif-
ferent machine learning algorithm) performance of
later models can be significantly undermined when
training upon material created using AL. The key
to knowing how well one model will be able to use
material selected by another is their relatedness ? yet
there may be no means to determine this prior to an-
notation, leading to a chicken-and-egg problem.
Our reusability results thus demonstrate that, ad-
ditionally, other strategies must be adopted to en-
sure we reduce the total cost of annotation. In Os-
borne and Baldridge (2004), we showed that ensem-
ble models can increase model performance and also
produce annotation savings when incorporated into
the AL process. An obvious next step is automating
some decisions. Here, we consider a simple automa-
tion strategy that reduces annotation costs indepen-
dently of AL and examine its effect on reusability.
We find that using both semi-automation and AL
with high-quality models can eliminate the perfor-
mance gap found in many reuse scenarios. However,
for weak models, we show that semi-automation
with random sampling is more effective for improv-
ing reusability than using it with AL ? demonstrat-
ing further cause for caution with AL.
Finally, we show that under the standard assump-
tion of reuse by the selecting model, using a strat-
egy which combines AL, ensembling, and semi-
automated annotation, we are able to achieve our
highest annotation savings to date on the complex
task of parse selection for HPSG: an 80% reduction
in annotation cost compared with labeling randomly
selected data with our best single model.
2 Parse selection for Redwoods
We now briefly describe the Redwoods treebanking
environment (Oepen et al, 2002), our parse selec-
tion models and their performance.
2.1 The Redwoods Treebank
The Redwoods treebank project provides tools and
annotated training material for creating parse se-
lection models for the English Resource Grammar
(ERG, Flickinger (2000)). The ERG is a hand-built
broad-coverage HPSG grammar that provides an ex-
plicit grammar for the treebank. Using this approach
has the advantage that analyses for within-coverage
sentences convey more information than just phrase
structure: they also contain derivations, semantic in-
terpretations, and basic dependencies.
For each sentence, Redwoods records all analyses
licensed by the ERG and indicates which of them,
if any, the annotators selected as being contextually
correct. When selecting such distinguished parses,
rather than simply enumerating all parses and pre-
senting them to the annotator, annotators make use
of discriminants which disambiguate the parse for-
est more rapidly, as described in section 3.
In this paper, we report results using the third
growth of Redwoods, which contains English sen-
tences from appointment scheduling and travel plan-
ning domains of Verbmobil. In all, there are 5302
sentences for which there are at least two parses and
a unique preferred parse is identified. These sen-
tences have 9.3 words and 58.0 parses on average.
2.2 Modeling parse selection
As is now standard for feature-based grammars, we
mainly use log-linear models for parse selection
(Johnson et al, 1999). For log-linear models, the
conditional probability of an analysis ti given a sen-
tence with a set of analyses ? = {t . . .} is given as:
P (ti|s,Mk) =
exp(
?m
j=1 fj(ti)wj)
Z(s)
(1)
where fj(ti) returns the number of times feature
j occurs in analysis t, wj is a weight from model
Mk, and Z(s) is a normalization factor for the sen-
tence. The parse with the highest probability is taken
as the preferred parse for the model. We use the
limited memory variable metric algorithm to deter-
mine the weights. We do not regularize our log-
linear models since labeled data -necessary to set
hyperparameters- is in short supply in AL.
We also make use of simpler perceptron models
for parse selection, which assign scores rather than
probabilities. Scores are computed by taking the in-
ner product of the analysis? feature vector with the
parameter vector:
score(ti, s,Mk) =
m?
j=1
fj(ti)wj (2)
The preferred parse is that with the highest score out
of all analyses. We do not use voted perceptrons
here (which indeed have better performance) as for
the reuse experiments described later in section 6 we
really do wish to use a model that is (potentially)
worse than a log-linear model.
Later for AL , it will be useful to map perceptron
scores into probabilities, which we do by exponenti-
ating and renormalizing the score:
Pp(ti | s,Mk) =
exp(score(ti, s,Mk))
Z(s)
(3)
Z(s) is again a normalizing constant.
The previous parse selection models (equations
1 and 3) use a single model (feature set). It is
possible to improve performance using an ensem-
ble parse selection model. We create our ensemble
model (called a product model) using the product-
of-experts formulation (Hinton, 1999):
P (ti|s,M1 . . .Mn) =
?n
j=1 P (ti|s,Mj)
Z(s)
(4)
Note that each individual model Mi is a well-defined
distribution usually taken from a fixed set of mod-
els. Z(s) is a constant to ensure the product distri-
bution sums to one over the set of possible parses. A
product model effectively averages the contributions
made by each of the individual models. Though sim-
ple, this model is sufficient to show enhanced perfor-
mance when using multiple models.
2.3 Parse selection performance
Osborne and Baldridge (2004) describe three dis-
tinct feature sets ? configurational, ngram, and
conglomerate ? which utilize the various struc-
tures made available in Redwoods: derivation trees,
phrase structures, semantic interpretations, and ele-
mentary dependency graphs. They incorporate dif-
ferent aspects of the parse selection task; this is
crucial for creating diverse models for use in prod-
uct parse selection models as well as for ensemble-
based AL methods. Here, we also use models cre-
ated from a subset of the conglomerate feature set:
the mrs feature set. This only has features from the
semantic interpretations.
The three main feature sets are used to train three
log-linear models ? LL-CONFIG, LL-NGRAM, and
LL-CONGLOM? and a product ensemble of those
three feature sets, LL-PROD, using equation 4. Addi-
tionally, we use a perceptron with the conglomerate
feature set, P-CONGLOM. Finally, we include a log-
linear model that uses the mrs feature set, LL-MRS,
and a perceptron, P-MRS.
Parse selection accuracy is measured using exact
match. A model is awarded a point if it picks some
parse for a sentence and that parse is the correct anal-
ysis indicated by the corpus. To deal with ties, the
accuracy is given as 1/m when a model ranks m
parses highest and the best parse is one of them.
The results for a chance baseline (selecting a
parse at random), the base models and the product
model are given in Table 1. These are 10-fold cross-
validation results, using all the training data for esti-
mation and the test split for evaluation. See section
5 for more details.
Model Perf. Model Perf.
LL-CONFIG 75.05 LL-PROD 77.78
LL-NGRAM 74.01 LL-MRS 64.98
LL-CONGLOM 74.85 P-CONGLOM 73.00
Chance 22.70 P-MRS 62.11
Table 1: Parse selection accuracy.
3 Measuring annotation cost
To aid identification of the best parse out of all those
licensed by the ERG, the Redwoods annotation envi-
ronment provides local discriminants which the an-
notator can mark as true or false properties for the
analysis of a sentence in order to disambiguate large
portions of the parse forest. As such, the annotator
does not need to inspect all parses and so parses are
narrowed down quickly (usually exponentially so)
even for sentences with a large number of parses.
More interestingly, it means that the labeling burden
is relative to the number of possible parses (rather
than the number of constituents in a parse).
Data about how many discriminants were needed
to annotate each sentence is recorded in Redwoods.
Typically, more ambiguous sentences require more
discriminant values to be set, reflecting the extra ef-
fort put into identifying the best parse. We showed
in Osborne and Baldridge (2004) that discriminant
cost does provide a more accurate approximation of
annotation cost than assigning a fixed unit cost for
each sentence. We thus use discriminants as the ba-
sis of calculating annotation cost to evaluate the ef-
fectiveness of different experiment AL conditions.
Specifically, we set the cost of annotating a given
sentence as the number of discriminants whose
value were set by the human annotator plus one to
indicate a final ?eyeball? step where the annotator se-
lects the best parse of the few remaining ones.1 The
discriminant cost of the examples we use averages
3.34 and ranges from 1 to 14.
4 Active learning
Suppose we have a set of examples and labels Dn =
{?x1, y1?, ?x2, y2?, . . .} which is to be extended with
a new labeled example {?xi, yi?}. The information
gain for some model is maximized after selecting,
labeling, and adding a new example xi to Dn such
that the noise level of xi is low and both the bias and
variance of some model using Dn ? {?xi, yi?} are
minimized (Cohn et al, 1995).
In practice, selecting data points for labeling such
that a model?s variance and/or bias is maximally
minimized is computationally intractable, so ap-
proximations are typically used instead. One such
approximation is uncertainty sampling. Uncertainty
sampling (also called tree entropy by Hwa (2000)),
measures the uncertainty of a model over the set of
parses of a given sentence, based on the conditional
1This eyeball step is not always taken, but Redwoods does
not contain information about when this occurred, so we apply
the cost for the step uniformly for all examples.
distribution it assigns to them. Following Hwa, we
use the following measure to quantify uncertainty:
fus(s, ?,Mk) = ?
?
t??
P (t|s,Mk) logP (t|s,Mk) (5)
? denotes the set of analyses produced by the ERG
for the sentence and Mk is some model. Higher val-
ues of fus(s, ?,Mk) indicate examples on which the
learner is most uncertain . Calculating fus is triv-
ial with the conditional log-linear and perceptrons
models described in section 2.2.
Uncertainty sampling as defined above is a single-
model approach. It can be improved by simply re-
placing the probability of a single log-linear (or per-
ceptron) model with a product probability:
fenus (s, ?,M) = ?
?
t??
P (t|s,M) logP (t|s,M) (6)
M is the set of models M1 . . .Mn. As we men-
tioned earlier, AL for parse selection is potentially
problematic as sentences vary both in length and the
number of parses they have. Nonetheless, the above
measures do not use any extra normalization as we
have found no major differences after experimenting
with a variety of normalization strategies.
We use random sampling as a baseline and un-
certainty sampling for AL. Osborne and Baldridge
(2004) show that uncertainty sampling produces
good results compared with other AL methods.
5 Experimental framework
For all experiments, we used a 20-fold cross-
validation strategy by randomly selecting 10%
(roughly 500 sentences) for the test set and select-
ing samples from the remaining 90% (roughly 4500
sentences) as training material. Each run of AL be-
gins with a single randomly chosen annotated seed
sentence. At each round, new examples are selected
for annotation from a randomly chosen, fixed sized
500 sentence subset according to random selection
or uncertainty sampling until models reach certain
desired accuracies. We select 20 examples for anno-
tation at each round, and exclude all examples that
have more than 500 parses.2
2Other parameter settings (such as how many examples to
label at each stage) did not produce substantially different re-
sults to those reported here.
AL results are usually presented in terms of the
amount of labeling necessary to achieve given per-
formance levels. We say that one method is bet-
ter than another method if, for a given performance
level, less annotation is required. The performance
metric used here is parse selection accuracy as de-
scribed in section 2.3.
6 Reusing training material
AL can be considered as selecting some labeled
training set which is ?tuned? to the needs of a particu-
lar model. Typically, we might wish to reuse labeled
training material, so a natural question to ask is how
general are training sets created using AL. So, if we
later improved upon our feature set, or else improved
upon our learner, would the previously created train-
ing set still be useful? If AL selects highly idiosyn-
cratic datasets then we would not be able to reuse our
datasets and thus it might, for example, actually be
better to label datasets using random sampling. This
is a realistic situation since models typically change
and evolve over time ? it would be very problem-
atic if the training set itself inherently limits the ben-
efit of later attempts to improve the model.
We use two baselines to evaluate how well a
model is able to reuse data selected for labeling by
another model: (1) Selecting the data randomly.
This provides the essential baseline; if AL in reuse
situations is going to be useful, it ought to outper-
form this model-free approach. (2) Reuse by the
AL model itself. This is the standard AL scenario;
against this, we can determine if reused data can be
as good as when a model selects data for itself.
We evaluate a variety of reuse scenarios. We re-
fer to the model used with AL as the selector and
the model that is reusing that labeled data as the
reuser. Models can differ in the machine learning al-
gorithm and/or the feature set they use. To measure
relatedness, we use Spearman?s rank correlation on
the rankings that two models assign to the parses of
a sentence. The overall relatedness of two models
is calculated as the average rank correlation on all
examples tested in a 10-fold parse selection experi-
ment using all available training material.
Figure 1 shows complete learning curves for LL-
CONFIG when it reuses material selected by itself,
LL-CONGLOM, P-MRS, and random sampling. The
graph shows that self-reuse is the most effective of
all strategies ? this is the idealized situation com-
monly assumed in active learning studies. However,
the graph reveals that random sampling is actually
more effective than selection both by LL-CONGLOM
until nearly 70% accuracy is reached and by P-MRS
until about 73%. Finally, we see that the material
selected by LL-CONGLOM is always more effective
for LL-CONFIG than that selected by P-MRS. The
reason for this can be explained by the relatedness
of each of these selector models to LL-CONFIG: LL-
CONGLOM and LL-CONFIG have an average rank
correlation of 0.84 whereas P-MRS and LL-CONFIG
have a correlation of 0.65.
 50
 55
 60
 65
 70
 75
 80
 0  1000  2000  3000  4000  5000  6000  7000  8000
A
c
c
u
r
a
c
y
Annotation cost
Selector: LL-CONFIGSelector: LL-CONGLOMSelector: RANDSelector: P-MRS
Figure 1: Learning curves for LL-CONFIG when
reusing material by different selectors.
Table 2 fleshes out the relationship between relat-
edness and reusability more fully. It shows the anno-
tation cost incurred by various reusers to reach 65%,
70%, and 73% accuracy when material is selected
by various models. The list is ordered from top to
bottom according to the rank correlation of the two
models. The first three lines provide the baselines of
when LL-PROD, LL-CONGLOM, and LL-CONFIG se-
lect material for themselves. The last three show the
amount of material needed by these models when
random sampling is used. The rest gives the results
for when the selector differs from the reuser.
For each performance level, the percent increase
in annotation cost over self-reuse is given. For
example, a cost of 2300 discriminants is required
for LL-PROD to reach the 73% performance level
when it reuses material selected by LL-CONGLOM;
this is a 10% increase over the 2100 discriminants
needed when LL-PROD selects for itself. Similarly,
the 5500 discriminants needed by LL-CONGLOM to
reach 73% when reusing material selected by LL-
CONFIG is a 31% increase over the 4200 discrimi-
nants LL-CONGLOM needs with its own selection.
As can be seen from Table 2, reuse always leads
to an increase in cost over self-reuse to reach a given
level of performance. How much that increase will
be is in general inversely related to the rank corre-
lation of the two models. Furthermore, considering
each reusing model individually, this relationship is
almost entirely inversely related at all performance
levels, with the exception of P-CONGLOM and LL-
MRS selecting for LL-CONFIG at the 73% level.
The reason for some models being more related
to others is generally easy to see. For example, LL-
CONFIG and LL-CONGLOM are highly related to LL-
PROD, of which they are both components. In both
of these cases, using AL for use by LL-PROD beats
random sampling by a large amount.
That LL-MRS is more related to LL-CONGLOM
than to LL-CONFIG is explained by the fact the mrs
feature set is actually a subset of the conglom set.
The former contains 15% of the latter?s features.
Accordingly, material selected by LL-MRS is also
generally more reusable by LL-CONGLOM than to
LL-CONFIG. This is encouraging since the case of
LL-CONGLOM reusing material selected by LL-MRS
represents the common situation in which an initial
model ? that was used to develop the corpus ? is
continually improved upon.
A particularly striking aspect revealed by Figure 1
and Table 2 is that random sampling is overwhelm-
ingly a better strategy when there is still little la-
beled material. AL tends to select examples which
are more ambiguous and hence have a higher dis-
criminant cost. So, while these examples may be
highly informative for the selector model, they are
not cheap ? and are far less effective when reused
by another model.
Considering unit cost (i.e., each sentence costs the
same) instead of discriminant cost (which assigns a
variable cost per sentence), AL is generally more
effective than random sampling for reuse through-
out all accuracy levels ? but not always. For exam-
ple, even using unit cost, random sampling is bet-
ter than selection by LL-MRS or P-MRS for reuse by
Rank 65% 70% 73%
Selector Reuser Corr. DC Incr DC Incr DC Incr
LL-PROD LL-PROD 1.00 690 0.0% 1200 0.0% 2050 0.0%
LL-CONGLOM LL-CONGLOM 1.00 1190 0.0% 2330 0.0% 4160 0.0%
LL-CONFIG LL-CONFIG 1.00 1160 0.0% 2530 0.0% 4780 0.0%
LL-CONFIG LL-PROD .92 850 23.2% 1470 22.5% 2430 18.5%
LL-CONGLOM LL-PROD .92 840 21.7% 1560 30.0% 2630 28.3%
LL-CONFIG LL-CONGLOM .84 1340 12.6% 2610 12.0% 4720 13.5%
LL-CONGLOM LL-CONFIG .84 1660 43.1% 3760 48.6% 6840 43.1%
P-CONGLOM LL-CONFIG .79 1960 69.0% 3910 54.5% 7940 66.1%
LL-MRS LL-CONGLOM .77 1600 34.5% 3400 45.9% 6420 54.3%
LL-MRS LL-PROD .76 1080 56.5% 2040 70.0% 3700 80.5%
LL-MRS LL-CONFIG .71 2100 81.0% 4270 68.8% 6870 43.7%
P-MRS LL-CONFIG .65 2650 128.4% 4870 92.5% 8260 72.8%
RAND LL-PROD - 820 18.8% 1950 62.5% 3680 79.5%
RAND LL-CONGLOM - 1400 17.6% 3470 48.9% 7150 71.9%
RAND LL-CONFIG - 1160 0.0% 3890 53.8% 8560 79.1%
Table 2: Comparison of various selection and reuse conditions. Values are given for discriminant cost (DC)
and the percent increase (Incr) in cost over use of material selected by the reuser.
LL-CONFIG until 67% accuracy. Thus, LL-MRS and
P-MRS are so divergent from LL-CONFIG that their
selections are truly sub-optimal for LL-CONFIG, par-
ticularly in the initial stages.
Together, these results shows that AL cannot be
used blindly and always be expected to reduce the
total cost of annotation. The data is tuned to the
models used during AL and how useful that data
will be for other models depends on the degree of
relatedness of the models under consideration.
Given that AL may or may not provide cost reduc-
tions, we consider the effect that semi-automating
annotation has on reducing the total cost of annota-
tion when used with and without AL.
7 Semi-automated labeling
Corpus building, with or without AL, is generally
viewed as selecting examples and then from scratch
labeling such examples. This can be inefficient, es-
pecially when dealing with labels that have complex
internal structures, as a model may be able to rule-
out some of the labeling possibilities.
For our domain, we exploit the fact that we may
already have partial information about an example?s
label by presenting only the top n-best parses to
the annotator, who then navigates to the best parse
within that set using those discriminants relevant to
that set of parses. Rather than using a value for n
that is fixed or proportional to the ambiguity of the
sentence, we simply select all parses for which the
model assigns a probability higher than chance. This
has the advantage of reducing the number of parses
presented to the annotator as the model uses more
training material and reduces its uncertainty.
When the true best parse is within the top n pre-
sented to the annotator, the cost we record is the
number of discriminants needed to identify it from
that subset, plus one ? the same calculation as when
all parses are presented, with the advantage that
fewer discriminants and parses need to be inspected.
When the best parse is not present in the n-best
subset, there is a question as to how to record the
annotation cost. The discriminant decisions made
in reducing the subset are still valid and useful in
identifying the best parse from the entire set, but we
must incur some penalty for the fact that the anno-
tator must confirm that this is the case. To deter-
mine the cost for such situations, we add one to the
usual full cost of annotating the sentence. This en-
codes what we feel is a reasonable reflection of the
penalty since decisions taken in the n-best phase are
still valid in the context of all parses.3
Performance level
65% 70% 73%
1. RAND 820 1950 3680
2. LL-PROD 690 1200 2050
3. RAND (NB) 670 1350 2430
4. LL-PROD (NB) 680 1120 1760
Table 3: Cost for LL-PROD to reach given perfor-
mance levels when using n-best automation (NB).
Table 3 shows the effects of using semi-automated
labeling with LL-PROD. As can be seen, random
selection costs reduce dramatically with n-best au-
tomation (compare rows 1 and 3). It is also an early
winner over basic uncertainty sampling (row 2),
though the latter eventually reaches the higher ac-
curacies more quickly. Nonetheless, the mixture of
AL and semi-automation provides the biggest over-
all gains: to reach 73% accuracy, n-best uncertainty
sampling (row 4) reduces the cost by 17% over n-
best random sampling (row 3) and by 15% over ba-
sic uncertainty sampling (row 2). Similar patterns
hold for n-best automation with LL-CONFIG.
Figure 2 provides an overall view on the accumu-
lative effects of ensembling, n-best automation, and
uncertainty sampling in the ideal situation of reuse
by the AL model itself. Ensemble models and n-best
automation show that massive improvements can be
made without AL. Nonetheless, we see the largest
reductions by using AL, n-best automation, and en-
semble models together: LL-PROD using uncertainty
sampling and n-best automation (row 4 of Table 3)
reaches 73% accuracy with a cost of 1760 compared
to 8560 needed by LL-CONFIG using random sam-
pling without automation. This is our best annota-
tion saving: a cost reduction of 80%.
8 Closing the reuse gap
The previous section?s semi-automated labeling ex-
periments did not involve reuse. If models are ex-
pected to evolve, could n-best automation fill in the
cost gap created by reuse? To test this, we con-
sidered reusing examples with our best model (LL-
3When we do not allow ourselves to benefit from such la-
beling decisions, our annotation savings naturally decrease, but
not below when we do not use n-best labeling.
 50
 55
 60
 65
 70
 75
 80
 0  500  1000  1500  2000  2500  3000  3500  4000  4500  5000
A
c
c
u
r
a
c
y
Annotation cost
LL-PRODUCT, N-best, Uncertainty samplingLL-PRODUCT, N-best, Random samplingLL-PRODUCT, Random samplingLL-CONFIG, Random sampling
Figure 2: Learning curves for accumulative im-
provements to the annotation scenario starting from
random sampling with LL-CONFIG: ensembling, n-
best automation, and uncertainty sampling.
PROD), as selected by different models using both
AL and n-best automation as a combined strategy.
For LL-CONFIG and LL-CONGLOM as selectors, the
gap is entirely closed: costs for reuse were virtually
equal to when LL-PROD selects examples for itself
without n-best (Table 3, row 2).
The gap also closes when n-best automation and
AL are used with the weaker LL-MRS model. Per-
formance (Table 4, row 1) still falls far short of LL-
PROD selecting for itself without n-best (Table 3,
row 2). However, the gap closes even more when n-
best automation and random sampling are used with
LL-MRS (Table 4, line 2).
Performance level
65% 70% 73%
1. NB & US 1040 1920 3320
2. NB & RAND 680 1450 2890
Table 4: Cost for LL-PROD to reach given perfor-
mance levels in reuse situations where n-best au-
tomation (NB) was used with LL-MRS with uncer-
tainty sampling (US) or random sampling (RAND).
Interestingly, when using a weak selector (LL-
MRS), n-best automation combined with random
sampling was more effective than when combined
with uncertainty sampling. The reason for this is
clear. Since AL typically selects more ambiguous
examples, a weak model has more difficulty getting
the best parse within the n-best when AL is used.
Thus, the gains from the more informative examples
selected by AL are surpassed by the gains that come
with the easier labeling with random sampling.
For most situations, n-best automation is benefi-
cial: the gap introduced by reuse can be reduced. n-
best automation never results in an increase in cost.
This is still true even if we do not allow ourselves to
reuse those discriminants which were used to select
the best parse from the n-best subset and the best
parse was not actually present in that subset.
9 Related work
There is a large body of AL work in the machine
learning literature, but less so within natural lan-
guage processing (NLP). Most work in NLP has
primarily focused upon uncertainty sampling (Hwa,
2000; Tang et al, 2002). Hwa (2001) considered
reuse of examples selected for one parser by an-
other with uncertainty sampling. This performed
better than sequential sampling but was only half as
effective as self-selection. Here, we have consid-
ered reuse with respect to many models and their
co-relatedness. Also, we compare reuse perfor-
mance against against random sampling, which we
showed previously to be a much stronger baseline
than sequential sampling for the Redwoods corpus
(Osborne and Baldridge, 2004). Hwa et al (2003)
showed that for parsers, AL outperforms the closely
related co-training, and that some of the labeling
could be automated. However, their approach re-
quires strict independence assumptions.
10 Discussion
AL should only be considered for creating labeled
data when the the task is either well-understood or
else the model is unlikely to substantially change.
Otherwise, it would be prudent to consider improv-
ing either the model itself (using, for example, en-
semble techniques) or else semi-automating the la-
beling task. Naturally, there is a cost associated with
creating the model itself, and this in turn will need
to be factored into the total cost. When there is gen-
uine uncertainty about the model, or else how the
labeled data is going to be eventually used, then the
best strategy may well be to use random selection
rather than AL ? especially when using some form
of automated annotation.
Acknowledgments
We would like to thank Markus Becker, Jeremiah
Crim, Dan Flickinger, Alex Lascarides, Stephan
Oepen, and Andrew Smith. We?d also like to
thank pc-jbaldrid and pc-rosie for their
hard work and 24/7 dedication. This work was sup-
ported by Edinburgh-Stanford Link R36763, ROSIE
project.
References
David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan.
1995. Active learning with statistical models. In G. Tesauro,
D. Touretzky, and T. Leen, editors, Advances in Neural Infor-
mation Processing Systems, volume 7, pages 705?712. The
MIT Press.
Dan Flickinger. 2000. On building a more efficient grammar by
exploiting types. Natural Language Engineering, 6(1):15?
28. Special Issue on Efficient Processing with HPSG.
G. E. Hinton. 1999. Products of experts. In Proc. of the 9th Int.
Conf. on Artificial Neural Networks, pages 1?6.
Rebecca Hwa, Miles Osborne, Anoop Sarkar, and Mark Steed-
man. 2003. Corrected Co-training for Statistical Parsers. In
Proceedings of the ICML Workshop ?The Continuum from
Labeled to Unlabeled Data?, pages 95?102. ICML-03.
Rebecca Hwa. 2000. Sample selection for statistical gram-
mar induction. In Proc. of the 2000 Joint SIGDAT Conf. on
EMNLP and VLC, pages 45?52, Hong Kong, China.
Rebecca Hwa. 2001. On minimizing training corpus for parser
acquisition. In Proc. of the 5th Conference on Natural Lan-
guage Learning, Toulouse.
Mark Johnson, Stuart Geman, Stephen Cannon, Zhiyi Chi,
and Stephan Riezler. 1999. Estimators for Stochastic
?Unification-Based? Grammars. In 37th Annual Meeting of
the ACL.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher
Manning, Dan Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods Treebank: Motivation and preliminary ap-
plications. In Proc. of the 19th International Conference on
Computational Linguistics, Taipei, Taiwan.
Miles Osborne and Jason Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proc. of HLT-NAACL,
Boston.
H. S. Seung, Manfred Opper, and Haim Sompolinsky. 1992.
Query by committee. In Computational Learning Theory,
pages 287?294.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Ac-
tive Learning for Statistical Natural Language Parsing. In
Proc. of the 40th Annual Meeting of the ACL, pages 120?
127, Philadelphia, Pennsylvania, USA, July.
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 133?140, New York City, June 2006. c?2006 Association for Computational Linguistics
Using Gazetteers in Discriminative Information Extraction
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Much work on information extraction has
successfully used gazetteers to recognise
uncommon entities that cannot be reliably
identified from local context alone. Ap-
proaches to such tasks often involve the
use of maximum entropy-style models,
where gazetteers usually appear as highly
informative features in the model. Al-
though such features can improve model
accuracy, they can also introduce hidden
negative effects. In this paper we de-
scribe and analyse these effects and sug-
gest ways in which they may be overcome.
In particular, we show that by quarantin-
ing gazetteer features and training them
in a separate model, then decoding using
a logarithmic opinion pool (Smith et al,
2005), we may achieve much higher accu-
racy. Finally, we suggest ways in which
other features with gazetteer feature-like
behaviour may be identified.
1 Introduction
In recent years discriminative probabilistic models
have been successfully applied to a number of infor-
mation extraction tasks in natural language process-
ing (NLP), such as named entity recognition (NER)
(McCallum and Li, 2003), noun phrase chunking
(Sha and Pereira, 2003) and information extraction
from research papers (Peng and McCallum, 2004).
Discriminative models offer a significant advantage
over their generative counterparts by allowing the
specification of powerful, possibly non-independent
features which would be difficult to tractably encode
in a generative model.
In a task such as NER, one sometimes encoun-
ters an entity which is difficult to identify using lo-
cal contextual cues alone because the entity has not
be seen before. In these cases, a gazetteer or dic-
tionary of possible entity identifiers is often useful.
Such identifiers could be names of people, places,
companies or other organisations. Using gazetteers
one may define additional features in the model that
represent the dependencies between a word?s NER
label and its presence in a particular gazetteer. Such
gazetteer features are often highly informative, and
their inclusion in the model should in principle re-
sult in higher model accuracy. However, these fea-
tures can also introduce hidden negative effects tak-
ing the form of labelling errors that the model makes
at places where a model without the gazetteer fea-
tures would have labelled correctly. Consequently,
ensuring optimal usage of gazetteers can be difficult.
In this paper we describe and analyse the labelling
errors made by a model, and show that they gen-
erally result from the model?s over-dependence on
the gazetteer features for making labelling decisions.
By including gazetteer features in the model we
may, in some cases, transfer too much explanatory
dependency to the gazetteer features from the non-
gazetteer features. In order to avoid this problem, a
more careful treatment of these features is required
during training. We demonstrate that a traditional
regularisation approach, where different features are
regularised to different degrees, does not offer a sat-
133
isfactory solution. Instead, we show that by training
gazetteer features in a separate model to the other
features, and decoding using a logarithmic opinion
pool (LOP) (Smith et al, 2005), much greater ac-
curacy can be obtained. Finally, we identify other
features with gazetteer feature-like properties and
show that similar results may be obtained using our
method with these features.
We take as our model a linear chain conditional
random field (CRF), and apply it to NER in English.
2 Conditional Random Fields
A linear chain conditional random field (CRF) (Laf-
ferty et al, 2001) defines the conditional probability
of a label sequence s given an observed sequence o
via:
p
 
s  o 
1
Z
 
o 
exp

T  1
?
t  1
?
k
?k fk
 
st  1 	 st 	 o 	 t 
 (1)
where T is the length of both sequences, ?k are pa-rameters of the model and Z   o  is a partition func-
tion that ensures that (1) represents a probability dis-
tribution. The functions fk are feature functions rep-resenting the occurrence of different events in the
sequences s and o.
The parameters ?k can be estimated by maximis-ing the conditional log-likelihood of a set of labelled
training sequences. At the maximum likelihood so-
lution the model satisfies a set of feature constraints,
whereby the expected count of each feature under
the model is equal to its empirical count on the train-
ing data:
E p?  o  s  fk  Ep  s  o  fk   0 	 k
In general this cannot be solved for the ?k in closedform, so numerical optimisation must be used. For
our experiments we use the limited memory variable
metric (LMVM) (Sha and Pereira, 2003) routine,
which has become the standard algorithm for CRF
training with a likelihood-based objective function.
To avoid overfitting, a prior distribution over the
model parameters is typically used. A common ex-
ample of this is the Gaussian prior. Use of a prior
involves adding extra terms to the objective and its
derivative. In the case of a Gaussian prior, these ad-
ditional terms involve the mean and variance of the
distribution.
3 Previous Use of Gazetteers
Gazetteers have been widely used in a variety of in-
formation extraction systems, including both rule-
based systems and statistical models. In addition to
lists of people names, locations, etc., recent work
in the biomedical domain has utilised gazetteers of
biological and genetic entities such as gene names
(Finkel et al, 2005; McDonald and Pereira, 2005).
In general gazetteers are thought to provide a useful
source of external knowledge that is helpful when
an entity cannot be identified from knowledge con-
tained solely within the data set used for training.
However, some research has questioned the useful-
ness of gazetteers (Krupka and Hausman, 1998).
Other work has supported the use of gazetteers in
general but has found that lists of only moderate
size are sufficient to provide most of the benefit
(Mikheev et al, 1999). Therefore, to date the ef-
fective use of gazetteers for information extraction
has in general been regarded as a ?black art?. In this
paper we explain some of the likely reasons for these
findings, and propose ways to more effectively han-
dle gazetteers when they are used by maxent-style
models.
In work developed independently and in parallel
to the work presented here, Sutton et al (2006) iden-
tify general problems with gazetteer features and
propose a solution similar to ours. They present re-
sults on NP-chunking in addition to NER, and pro-
vide a slightly more general approach. By contrast,
we motivate the problem more thoroughly through
analysis of the actual errors observed and through
consideration of the success of other candidate solu-
tions, such as traditional regularisation over feature
subsets.
4 Our Experiments
In this section we describe our experimental setup,
and provide results for the baseline models.
4.1 Task and Dataset
Named entity recognition (NER) involves the iden-
tification of the location and type of pre-defined en-
tities within a sentence. The CRF is presented with
a set of sentences and must label each word so as
to indicate whether the word appears outside an en-
tity, at the beginning of an entity of a certain type or
134
within the continuation of an entity of a certain type.
Our results are reported on the CoNLL-2003
shared task English dataset (Sang and Meulder,
2003). For this dataset the entity types are: per-
sons (PER), locations (LOC), organisations (ORG)
and miscellaneous (MISC). The training set consists
of 14
	
987 sentences and 204
	
567 tokens, the devel-
opment set consists of 3
	
466 sentences and 51
	
578
tokens and the test set consists of 3
	
684 sentences
and 46
	
666 tokens.
4.2 Gazetteers
We employ a total of seven gazetteers for our ex-
periments. These cover names of people, places
and organisations. Specifically, we have gazetteers
containing surnames (88
	
799 entries), female first
names (4
	
275 entries), male first names (1
	
219 en-
tries), names of places (27
	
635 entries), names of
companies (20
	
638 and 279
	
195 entries) and names
of other organisations (425 entries).
4.3 Feature set
Our experiments are centred around two CRF mod-
els, one with and one without gazetteer features.
The model without gazetteer features, which we call
standard, comprises features defined in a window
of five words around the current word. These in-
clude features encoding n-grams of words and POS
tags, and features encoding orthographic properties
of the current word. The orthographic features are
based on those found in (Curran and Clark, 2003).
Examples include whether the current word is capi-
talised, is an initial, contains a digit, contains punc-
tuation, etc. In total there are 450
	
345 features in the
standard model.
We call the second model, with gazetteer features,
standard+g. This includes all the features contained
in the standard model as well as 8
	
329 gazetteer
features. Our gazetteer features are a typical way
to represent gazetteer information in maxent-style
models. They are divided into two categories: un-
lexicalised and lexicalised. The unlexicalised fea-
tures model the dependency between a word?s pres-
ence in a gazetteer and its NER label, irrespective
of the word?s identity. The lexicalised features, on
the other hand, include the word?s identity and so
provide more refined word-specific modelling of the
Model Development TestUnreg. Reg. Unreg. Reg.
standard 88.21 89.86 81.60 83.97
standard+g 89.19 90.40 83.10 84.70
Table 1: Model F scores
standard+g 
7
sta
nda
rd   44,945 160
7 228 1,333
Table 2: Test set errors
gazetteer-NER label dependency.1 There are 35 un-
lexicalised gazetteer features and 8
	
294 lexicalised
gazetteer features, giving a total of 458
	
675 features
in the standard+g model.
4.4 Baseline Results
Table 1 gives F scores for the standard and stan-
dard+g models. Development set scores are in-
cluded for completeness, and are referred to later in
the paper. We show results for both unregularised
and regularised models. The regularised models are
trained with a zero-mean Gaussian prior, with the
variance set using the development data.
We see that, as expected, the presence of the
gazetteer features allows standard+g to outperform
standard, for both the unregularised and regularised
models. To test significance, we use McNemar?s
matched-pairs test (Gillick and Cox, 1989) on point-
wise labelling errors. In each case, the standard+g
model outperforms the standard model at a signif-
icance level of p  0  02. However, these results
camouflage the fact that the gazetteer features intro-
duce some negative effects, which we explore in the
next section. As such, the real benefit of including
the gazetteer features in standard+g is not fully re-
alised.
5 Problems with Gazetteer Features
We identify problems with the use of gazetteer fea-
tures by considering test set labelling errors for
both standard and standard+g. We use regularised
models here as an illustration. Table 2 shows the
1Many gazetteer entries involve strings of words where the
individual words in the string do not appear in the gazetteer in
isolation. For this reason the lexicalised gazetteer features are
not simply determined by the word identity features.
135
number of sites (a site being a particular word at a
particular position in a sentence) where labellings
have improved, worsened or remained unchanged
with respect to the gold-standard labelling with the
addition of the gazetteer features. For example, the
value in the top-left cell is the number of sites where
both the standard and standard+g label words cor-
rectly.
The most interesting cell in the table is the top-
right one, which represents sites where standard is
correctly labelling words but, with the addition of
the gazetteer features, standard+g mislabels them.
At these sites, the addition of the gazetteer features
actually worsens things. How well, then, could
the standard+g model do if it could somehow re-
duce the number of errors in the top-right cell? In
fact, if it had correctly labelled those sites, a signifi-
cantly higher test set F score of 90  36% would have
been obtained. This potential upside suggests much
could be gained from investigating ways of correct-
ing the errors in the top-right cell. It is not clear
whether there exists any approach that could correct
all the errors in the top-right cell while simultane-
ously maintaining the state in the other cells, but ap-
proaches that are able to correct at least some of the
errors should prove worthwhile.
On inspection of the sites where errors in the top-
right cell occur, we observe that some of the er-
rors occur in sequences where no words are in any
gazetteer, so no gazetteer features are active for any
possible labelling of these sequences. In other cases,
the errors occur at sites where some of the gazetteer
features appear to have dictated the label, but have
made an incorrect decision. As a result of these ob-
servations, we classify the errors from the top-right
cell of Table 2 into two types: type A and type B.
5.1 Type A Errors
We call type A errors those errors that occur at sites
where gazetteer features seem to have been directly
responsible for the mislabelling. In these cases the
gazetteer features effectively ?over-rule? the other
features in the model causing a mislabelling where
the standard model, without the gazetteer features,
correctly labels the word.
An example of a type A error is given in the sen-
tence extract below:
about/O Healy/I-LOC
This is the labelling given by standard+g. The cor-
rect label for Healy here is I-PER. The standard
model is able to decode this correctly as Healy
appears in the training data with the I-PER label.
The reason for the mislabelling by the standard+g
model is that Healy appears in both the gazetteer of
place names and the gazetteer of person surnames.
The feature encoding the gazetteer of place names
with the I-LOC label has a ? value of 4  20, while
the feature encoding the gazetteer of surnames with
the I-PER label has a ? value of 1  96, and the fea-
ture encoding the word Healy with the I-PER la-
bel has a ? value of 0  25. Although other features
both at the word Healy and at other sites in the sen-
tence contribute to the labelling of Healy, the influ-
ence of the first feature above dominates. So in this
case the addition of the gazetteer features has con-
fused things.
5.2 Type B Errors
We call type B errors those errors that occur at
sites where the gazetteer features seem to have been
only indirectly responsible for the mislabelling. In
these cases the mislabelling appears to be more at-
tributable to the non-gazetteer features, which are in
some sense less expressive after being trained with
the gazetteer features. Consequently, they are less
able to decode words that they could previously la-
bel correctly.
An example of a type B error is given in the sen-
tence extract below:
Chanderpaul/O was/O
This is the labelling given by standard+g. The
correct labelling, given by standard, is I-PER for
Chanderpaul. In this case no words in the sen-
tence (including the part not shown) are present in
any of the gazetteers so no gazetteer features are ac-
tive for any labelling of the sentence. Consequently,
the gazetteer features do not contribute at all to the
labelling decision. Non-gazetteer features in stan-
dard+g are, however, unable to find the correct la-
belling for Chanderpaul when they previously
could in the standard model.
For both type A and type B errors it is clear that
the gazetteer features in standard+g are in some
136
sense too ?powerful? while the non-gazetteers fea-
tures have become too ?weak?. The question, then,
is: can we train all the features in the model in a
more sophisticated way so as to correct for these ef-
fects?
6 Feature Dependent Regularisation
One interpretation of the findings of our error analy-
sis above is that the addition of the gazetteer features
to the model is having an implicit over-regularising
effect on the other features. Therefore, is it possible
to adjust for this effect through more careful explicit
regularisation using a prior? Can we directly reg-
ularise the gazetteer features more heavily and the
non-gazetteer features less? We investigate this pos-
sibility in this section.
The standard+g model is regularised by fitting
a single Gaussian variance hyperparameter across
all features. The optimal value for this single hy-
perparameter is 45. We now relax this single con-
straint by allocating a separate variance hyperparam-
eter to different feature subsets, one for the gazetteer
features (?gaz) and one for the non-gazetteer fea-tures (?non-gaz). The hope is that the differing sub-sets of features are best regularised using different
prior hyperparameters. This is a natural approach
within most standardly formulated priors for log-
linear models. Clearly, by doing this we increase
the search space significantly. In order to make the
search manageable, we constrain ourselves to three
scenarios: (1) Hold ?non-gaz at 45, and regularise thegazetteer features a little more by reducing ?gaz. (2)Hold ?gaz at 45, and regularise the non-gazetteer fea-tures a little less by increasing ?non-gaz. (3) Simulta-neously regularise the gazetteer features a little more
than at the single variance optimum, and regularise
the non-gazetteer features a little less.
Table 3 gives representative development set F
scores for each of these three scenarios, with each
scenario separated by a horizontal dividing line. We
see that in general the results do not differ signifi-
cantly from that of the single variance optimum. We
conjecture that the reason for this is that the regu-
larising effect of the gazetteer features on the non-
gazetteer features is due to relatively subtle inter-
actions during training that relate to the dependen-
cies the features encode and how these dependen-
?gaz ?non   gaz F score42 45 90.40
40 45 90.30
45 46 90.39
45 50 90.38
44.8 45.2 90.41
43 47 90.35
Table 3: FDR development set F scores
cies overlap. Regularising different feature subsets
by different amounts with a Gaussian prior does not
directly address these interactions but instead just
rather crudely penalises the magnitude of the pa-
rameter values of different feature sets to different
degrees. Indeed this is true for any standardly for-
mulated prior. It seems therefore that any solution to
the regularising problem should come through more
explicit restricting or removing of the interactions
between gazetteer and non-gazetteer features during
training.
7 Combining Separately Trained Models
We may remove interactions between gazetteer and
non-gazetteer features entirely by quarantining the
gazetteer features and training them in a separate
model. This allows the non-gazetteer features to
be protected from the over-regularising effect of the
gazetteer features. In order to decode taking advan-
tage of the information contained in both models, we
must combine the models in some way. To do this
we use a logarithmic opinion pool (LOP) (Smith
et al, 2005). This is similar to a mixture model,
but uses a weighted multiplicative combination of
models rather than a weighted additive combination.
Given models p? and per-model weights w? , theLOP distribution is defined by:
pLOP
 
s  o 
1
ZLOP
 
o  ??  p?
 
s  o 

w? (2)
with w?  0 and ?? w?  1, and where ZLOP   o  isa normalising function. The weight w? encodes thedependence of the LOP on model ? . In the case of a
CRF, the LOP itself is a CRF and so decoding is no
more complex than for standard CRF decoding.
In order to use a LOP for decoding we must set
the weights w? in the weighted product. In (Smith et
137
Feature Subset Feature Type
s1 simple structural features
s2 advanced structural features
n n-grams of words and POS tags
o simple orthographic features
a advanced orthographic features
g gazetteer features
Table 4: standard+g feature subsets
al., 2005) a procedure is described whereby the (nor-
malised) weights are explicitly trained. In this paper,
however, we only construct LOPs consisting of two
models in each case, one model with gazetteer fea-
tures and one without. We therefore do not require
the weight training procedure as we can easily fit the
two weights (only one of which is free) using the de-
velopment set.
To construct models for the gazetteer and non-
gazetteer features we first partition the feature set of
the standard+g model into the subsets outlined in
Table 4. The simple structural features model label-
label and label-word dependencies, while the ad-
vanced structural features include these features as
well as those modelling label-label-word conjunc-
tions. The simple orthographic features measure
properties of a word such as capitalisation, presence
of a digit, etc., while the advanced orthographic
properties model the occurrence of prefixes and suf-
fixes of varying length.
We create and train different models for the
gazetteer features by adding different feature sub-
sets to the gazetteer features. We regularise these
models in the usual way using a Gaussian prior. In
each case we then combine these models with the
standard model and decode under a LOP.
Table 5 gives results for LOP decoding for the
different model pairs. Results for the standard+g
model are included in the first row for comparison.
For each LOP the hyphen separates the two models
comprising the LOP. So, for example, in the second
row of the table we combine the gazetteer features
with simple structural features in a model, train and
decode with the standard model using a LOP. The
simple structural features are included so as to pro-
vide some basic support to the gazetteer features.
We see from Table 5 that the first two LOPs sig-
nificantly outperform the regularised standard+g
LOP Dev Set Test Set
standard+g 90.40 84.70
s1g-standard 91.34 85.98
s2g-standard 91.32 85.59
s2ng-standard 90.66 84.59
s2nog-standard 90.47 84.92
s2noag-standard 90.56 84.78
Table 5: Reg. LOP F scores
LOP LOP Weights
s1g-standard [0.39, 0.61]
s2g-standard [0.29, 0.71]
s2ng-standard [0.43, 0.57]
s2nog-standard [0.33, 0.67]
s2noag-standard [0.39, 0.61]
Table 6: Reg. LOP weights
model (at a significance level of p  0  01, on both
the test and development sets). By training the
gazetteer features separately we have avoided their
over-regularising effect on the non-gazetteer fea-
tures. This relies on training the gazetteer features
with a relatively small set of other features. This is
illustrated as we read down the table, below the top
two rows. As more features are added to the model
containing the gazetteer features we obtain decreas-
ing test set F scores because the advantage created
from separate training of the features is increasingly
lost.
Table 6 gives the corresponding weights for the
LOPs in Table 5, which are set using the develop-
ment data. We see that in every case the LOP al-
locates a smaller weight to the gazetteer features
model than the non-gazetteer features model and in
doing so restricts the influence that the gazetteer fea-
tures have in the LOP?s labelling decisions.
Table 7, similar to Table 2 earlier, shows test set
labelling errors for the standard model and one of
the LOPs. We take the s2g-standard LOP here for
illustration. We see from the table that the number
of errors in the top-right cell shows a reduction of
29% over the corresponding value in Table 2. We
have therefore reduced the number errors of the type
we were targeting with our approach. The approach
has also had the effect of reducing the number of er-
rors in the bottom-right cell, which further improves
model accuracy.
All the LOPs in Table 5 contain regularised mod-
138
s2g-standard LOP
 
7
sta
nda
rd   44,991 114
7 305 1,256
Table 7: Test set errors
LOP Dev Set Test Set
s1g-standard 90.58 84.87
s2g-standard 90.70 84.28
s2ng-standard 89.70 84.01
s2nog-standard 89.48 83.99
s2noag-standard 89.40 83.70
Table 8: Unreg. LOP F scores
els. Table 8 gives test set F scores for the cor-
responding LOPs constructed from unregularised
models. As we would expect, the scores are lower
than those in Table 5. However, it is interesting to
note that the s1g-standard LOP still outperforms
the regularised standard+g model.
In summary, by training the gazetteer features
and non-gazetteer features in separate models and
decoding using a LOP, we are able to overcome
the problems described in earlier sections and can
achieve much higher accuracy. This shows that
successfully deploying gazetteer features within
maxent-style models should involve careful consid-
eration of restrictions on how features interact with
each other, rather than simply considering the abso-
lute values of feature parameters.
8 Gazetteer-Like Features
So far our discussion has focused on gazetteer fea-
tures. However, we would expect that the problems
we have described and dealt with in the last sec-
tion also occur with other types of features that have
similar properties to gazetteer features. By applying
similar treatment to these features during training we
may be able harness their usefulness to a greater de-
gree than is currently the case when training in a sin-
gle model. So how can we identify these features?
The task of identifying the optimal partitioning
for creation of models in the previous section is in
general a hard problem as it relies on clustering the
features based on their explanatory power relative to
all other clusters. It may be possible, however, to de-
vise some heuristics that approximately correspond
to the salient properties of gazetteer features (with
respect to the clustering) and which can then be used
to identify other features that have these properties.
In this section we consider three such heuristics. All
of these heuristics are motivated by the observation
that gazetteer features are both highly discriminative
and generally very sparse.
Family Singleton Features We define a feature
family as a set of features that have the same con-
junction of predicates defined on the observations.
Hence they differ from each other only in the NER
label that they encode. Family singleton features
are features that have a count of 1 in the training
data when all other members of that feature family
have zero counts. These features have a flavour of
gazetteer features in that they represent the fact that
the conjunction of observation predicates they en-
code is highly predictive of the corresponding NER
label, and that they are also very sparse.
Family n-ton Features These are features that
have a count of n (greater than 1) in the training
data when all other members of that feature family
have zero counts. They are similar to family sin-
gleton features, but exhibit gazetteer-like properties
less and less as the value of n is increased because a
larger value of n represents less sparsity.
Loner Features These are features which occur
with a low mean number of other features in the
training data. They are similar to gazetteer features
in that, at the points where they occur, they are in
some sense being relied upon more than most fea-
tures to explain the data. To create loner feature sets
we rank all features in the standard+g model based
on the mean number of other features they are ob-
served with in the training data, then we take subsets
of increasing size. We present results for subsets of
size 500, 1000, 5000 and 10000.
For each of these categories of features we add
simple structural features (the s1 set from earlier),
to provide basic structural support, and then train a
regularised model. We also train a regularised model
consisting of all features in standard+g except the
features from the category in question. We decode
these model pairs under a LOP as described earlier.
Table 9 gives test set F scores for LOPs cre-
ated from each of the categories of features above
139
LOP Test Set
FSF 85.79
FnF 84.78
LF 500 85.80
LF 1000 85.70
LF 5000 85.77
LF 10000 85.62
Table 9: Reg. LOP F scores
(with abbreviated names derived from the category
names). The results show that for the family single-
ton features and each of the loner feature sets we
obtain LOPs that significantly outperform the reg-
ularised standard+g model (p  0  0002 in every
case). The family n-ton features? LOP does not do
as well, but that is probably due to the fact that some
of the features in this set have a large value of n and
so behave much less like gazetteer features.
In summary, we obtain the same pattern of results
using our quarantined training and LOP decoding
method with these categories of features that we do
with the gazetteer features. We conclude that the
problems with gazetteer features that we have iden-
tified in this paper are exhibited by general discrim-
inative features with gazetteer feature-like proper-
ties, and our method is also successful with these
more general features. Clearly, the heuristics that
we have devised in this section are very simple, and
it is likely that with more careful engineering better
feature partitions can be found.
9 Conclusion and future work
In this paper we have identified and analysed nega-
tive effects that can be introduced to maxent-style
models by the inclusion of highly discriminative
gazetteer features. We have shown that such ef-
fects manifest themselves through errors that gen-
erally result from the model?s over-dependence on
the gazetteer features for decision making. To over-
come this problem a more careful treatment of these
features is required during training. We have pro-
posed a solution that involves quarantining the fea-
tures and training them separately to the other fea-
tures in the model, then decoding the separate mod-
els with a logarithmic opinion pool. In fact, the LOP
provides a natural way to handle the problem, with
different constituent models for the different fea-
ture types. The method leads to much greater ac-
curacy, and allows the power of gazetteer features
to be more effectively harnessed. Finally, we have
identified other feature sets with gazetteer feature-
like properties and shown that similar results may be
obtained using our method with these feature sets.
In this paper we defined intuitively-motivated fea-
ture partitions (gazetteer feature-based or otherwise)
using heuristics. In future work we will focus on au-
tomatically determining such partitions.
References
James Curran and Stephen Clark. 2003. Language independent
NER using a maximum entropy tagger. In Proc. CoNLL-
2003.
Jenny Finkel, Shipra Dingare, Christopher D. Manning, Malv-
ina Nissim, Beatrice Alex, and Claire Grover. 2005. Ex-
ploring the boundaries: gene and protein identification in
biomedical text. BMC Bioinformatics, (6).
L. Gillick and Stephen Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing, volume 1, pages 532?535.
George R. Krupka and Kevin Hausman. 1998. Isoquest Inc:
Description of the NetOwl (TM) extractor system as used
for MUC-7. In Proc. MUC-7.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML 2001.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature in-
duction and web-enhanced lexicons. In Proc. CoNLL-2003.
Ryan McDonald and Fernando Pereira. 2005. Identifying gene
and protein mentions in text using conditional random fields.
BMC Bioinformatics, (6).
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers.
Fuchun Peng and Andrew McCallum. 2004. Accurate informa-
tion extraction from research papers using conditional ran-
dom fields. In Proc. HLT-NAACL 2004.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proc. CoNLL-2003.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. HLT-NAACL 2003.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Proc.
ACL 2005.
Charles Sutton, Michael Sindelar, and Andrew McCallum.
2006. Reducing weight undertraining in struxctured dis-
criminative learning. In Proc. HLT/NAACL 2006.
140
Proceedings of the Workshop on Statistical Machine Translation, pages 154?157,
New York City, June 2006. c?2006 Association for Computational Linguistics
Constraining the Phrase-Based, Joint Probability Statistical Translation
Model
Alexandra Birch Chris Callison-Burch Miles Osborne Philipp Koehn
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
a.c.birch-mayne@sms.ed.ac.uk
Abstract
The joint probability model proposed by
Marcu and Wong (2002) provides a strong
probabilistic framework for phrase-based
statistical machine translation (SMT). The
model?s usefulness is, however, limited by
the computational complexity of estimat-
ing parameters at the phrase level. We
present the first model to use word align-
ments for constraining the space of phrasal
alignments searched during Expectation
Maximization (EM) training. Constrain-
ing the joint model improves performance,
showing results that are very close to state-
of-the-art phrase-based models. It also al-
lows it to scale up to larger corpora and
therefore be more widely applicable.
1 Introduction
Machine translation is a hard problem because of
the highly complex, irregular and diverse nature
of natural languages. It is impossible to accurately
model all the linguistic rules that shape the trans-
lation process, and therefore a principled approach
uses statistical methods to make optimal decisions
given incomplete data.
The original IBM Models (Brown et al, 1993)
learn word-to-word alignment probabilities which
makes it computationally feasible to estimate
model parameters from large amounts of train-
ing data. Phrase-based SMT models, such as the
alignment template model (Och, 2003), improve
on word-based models because phrases provide
local context which leads to better lexical choice
and more reliable local reordering. However, most
phrase-based models extract their phrase pairs
from previously word-aligned corpora using ad-
hoc heuristics. These models perform no search
for optimal phrasal alignments. Even though this
is an efficient strategy, it is a departure from the
rigorous statistical framework of the IBM Models.
Marcu and Wong (2002) proposed the joint
probability model which directly estimates the
phrase translation probabilities from the corpus in
a theoretically governed way. This model neither
relies on potentially sub-optimal word alignments
nor on heuristics for phrase extraction. Instead, it
searches the phrasal alignment space, simultane-
ously learning translation lexicons for both words
and phrases. The joint model has been shown to
outperform standard models on restricted data sets
such as the small data track for Chinese-English in
the 2004 NIST MT Evaluation (Przybocki, 2004).
However, considering all possible phrases and
all their possible alignments vastly increases the
computational complexity of the joint model when
compared to its word-based counterpart. In this
paper, we propose a method of constraining the
search space of the joint model to areas where
most of the unpromising phrasal alignments are
eliminated and yet as many potentially useful
alignments as possible are still explored. The
joint model is constrained to phrasal alignments
which do not contradict a set high confidence word
alignments for each sentence. These high con-
fidence alignments could incorporate information
from both statistical and linguistic sources. In this
paper we use the points of high confidence from
the intersection of the bi-directional Viterbi word
alignments to constrain the model, increasing per-
formance and decreasing complexity.
154
2 Translation Models
2.1 Standard Phrase-based Model
Most phrase-based translation models (Och, 2003;
Koehn et al, 2003; Vogel et al, 2003) rely on
a pre-existing set of word-based alignments from
which they induce their parameters. In this project
we use the model described by Koehn et al (2003)
which extracts its phrase alignments from a corpus
that has been word aligned. From now on we re-
fer to this phrase-based translation model as the
standard model. The standard model decomposes
the foreign input sentence F into a sequence of
I phrases f1, . . . , f I . Each foreign phrase fi is
translated to an English phrase ei using the prob-
ability distribution ?(f i|ei). English phrases may
be reordered using a relative distortion probability.
This model performs no search for optimal
phrase pairs. Instead, it extracts phrase pairs
(f i, ei) in the following manner. First, it uses the
IBM Models to learn the most likely word-level
Viterbi alignments for English to Foreign and For-
eign to English. It then uses a heuristic to recon-
cile the two alignments, starting from the points
of high confidence in the intersection of the two
Viterbi alignments and growing towards the points
in the union. Points from the union are selected if
they are adjacent to points from the intersection
and their words are previously unaligned.
Phrases are then extracted by selecting phrase
pairs which are ?consistent? with the symmetrized
alignment, which means that all words within the
source language phrase are only aligned to the
words of the target language phrase and vice versa.
Finally the phrase translation probability distribu-
tion is estimated using the relative frequencies of
the extracted phrase pairs.
This approach to phrase extraction means that
phrasal alignments are locked into the sym-
metrized alignment. This is problematic because
the symmetrization process will grow an align-
ment based on arbitrary decisions about adjacent
words and because word alignments inadequately
represent the real dependencies between transla-
tions.
2.2 Joint Probability Model
The joint model (Marcu and Wong, 2002), does
not rely on a pre-existing set of word-level align-
ments. Like the IBM Models, it uses EM to align
and estimate the probabilities for sub-sentential
units in a parallel corpus. Unlike the IBM Mod-
els, it does not constrain the alignments to being
single words.
The joint model creates phrases from words and
commonly occurring sequences of words. A con-
cept, ci, is defined as a pair of aligned phrases
< ei, f i >. A set of concepts which completely
covers the sentence pair is denoted by C. Phrases
are restricted to being sequences of words which
occur above a certain frequency in the corpus.
Commonly occurring phrases are more likely to
lead to the creation of useful phrase pairs, and
without this restriction the search space would be
much larger.
The probability of a sentence and its translation
is the sum of all possible alignments C, each of
which is defined as the product of the probability
of all individual concepts:
p(F,E) =
?
C?C
?
<ei,f i>?C
p(< ei, f i >) (1)
The model is trained by initializing the trans-
lation table using Stirling numbers of the second
kind to efficiently estimate p(< ei, f i >) by cal-
culating the proportion of alignments which con-
tain p(< ei, f i >) compared to the total number
of alignments in the sentence (Marcu and Wong,
2002). EM is then performed by first discovering
an initial phrasal alignments using a greedy algo-
rithm similar to the competitive linking algorithm
(Melamed, 1997). The highest probability phrase
pairs are iteratively selected until all phrases are
are linked. Then hill-climbing is performed by
searching once for each iteration for all merges,
splits, moves and swaps that improve the proba-
bility of the initial phrasal alignment. Fractional
counts are collected for all alignments visited.
Training the IBM models is computationally
challenging, but the joint model is much more de-
manding. Considering all possible segmentations
of phrases and all their possible alignments vastly
increases the number of possible alignments that
can be formed between two sentences. This num-
ber is exponential with relation to the length of the
shorter sentence.
3 Constraining the Joint Model
The joint model requires a strategy for restricting
the search for phrasal alignments to areas of the
alignment space which contain most of the proba-
bility mass. We propose a method which examines
155
phrase pairs that are consistent with a set of high
confidence word alignments defined for the sen-
tence. The set of alignments are taken from the in-
tersection of the bi-directional Viterbi alignments.
This strategy for extracting phrase pairs is simi-
lar to that of the standard phrase-based model and
the definition of ?consistent? is the same. How-
ever, the constrained joint model does not lock
the search into a heuristically derived symmetrized
alignment. Joint model phrases must also occur
above a certain frequency in the corpus to be con-
sidered.
The constraints on the model are binding during
the initialization phase of training. During EM,
inconsistent phrase pairs are given a small, non-
zero probability and are thus not considered un-
less unaligned words remain after linking together
high probability phrase pairs. All words must be
aligned, there is no NULL alignment like in the
IBM models.
By using the IBM Models to constrain the joint
model, we are searching areas in the phrasal align-
ment space where both models overlap. We com-
bine the advantage of prior knowledge about likely
word alignments with the ability to perform a
probabilistic search around them.
4 Experiments
All data and software used was from the NAACL
2006 Statistical Machine Translation workshop
unless otherwise indicated.
4.1 Constraints
The unconstrained joint model becomes in-
tractable with very small amounts of training data.
On a machine with 2 Gb of memory, we were
only able to train 10,000 sentences of the German-
English Europarl corpora. Beyond this, pruning is
required to keep the model in memory during EM.
Table 1 shows that the application of the word con-
straints considerably reduces the size of the space
of phrasal alignments that is searched. It also im-
proves the BLEU score of the model, by guiding it
to explore the more promising areas of the search
space.
4.2 Scalability
Even though the constrained joint model reduces
complexity, pruning is still needed in order to scale
up to larger corpora. After the initialization phase
of the training, all phrase pairs with counts less
Unconstrained Constrained
No. Concepts 6,178k 1,457k
BLEU 19.93 22.13
Time(min) 299 169
Table 1. The impact of constraining the joint model
trained on 10,000 sentences of the German-English
Europarl corpora and tested with the Europarl test set
used in Koehn et al (2003)
than 10 million times that of the phrase pair with
the highest count, are pruned from the phrase ta-
ble. The model is also parallelized in order to
speed up training.
The translation models are included within a
log-linear model (Och and Ney, 2002) which al-
lows a weighted combination of features func-
tions. For the comparison of the basic systems
in Table 2 only three features were used for both
the joint and the standard model: p(e|f), p(f |e)
and the language model, and they were given equal
weights.
The results in Table 2 show that the joint model
is capable of training on large data sets, with a
reasonable performance compared to the standard
model. However, here it seems that the standard
model has a slight advantage. This is almost cer-
tainly related to the fact that the joint model results
in a much smaller phrase table. Pruning eliminates
many phrase pairs, but further investigations indi-
cate that this has little impact on BLEU scores.
BLEU Size
Joint Model 25.49 2.28
Standard Model 26.15 19.04
Table 2. Basic system comparisons: BLEU scores
and model size in millions of phrase pairs for Spanish-
English
The results in Table 3 compare the joint and the
standard model with more features. Apart from
including all Pharaoh?s default features, we use
two new features for both the standard and joint
models: a 5-gram language model and a lexical-
ized reordering model as described in Koehn et al
(2005). The weights of the feature functions, or
model components, are set by minimum error rate
training provided by David Chiang from the Uni-
versity of Maryland.
On smaller data sets (Koehn et al, 2003) the
joint model shows performance comparable to the
standard model, however the joint model does
not reach the level of performance of the stan-
156
EN-ES ES-EN
Joint
3-gram, dl4 20.51 26.64
5-gram, dl6 26.34 27.17
+ lex. reordering 26.82 27.80
Standard Model
5-gram, dl6
+ lex. reordering 31.18 31.86
Table 3. Bleu scores for the joint model and the stan-
dard model showing the effect of the 5-gram language
model, distortion length of 6 (dl) and the addition of
lexical reordering for the English-Spanish and Spanish-
English tasks.
dard model for this larger data set. This could
be due to the fact that the joint model results in
a much smaller phrase table. During EM only
phrase pairs that occur in an alignment visited dur-
ing hill-climbing are retained. Only a very small
proportion of the alignment space can be searched
and this reduces the chances of finding optimum
parameters. The small number of alignments vis-
ited would lead to data sparseness and over-fitting.
Another factor could be efficiency trade-offs like
the fast but not optimal competitive linking search
for phrasal alignments.
4.3 German-English submission
We also submitted a German-English system using
the standard approach to phrase extraction. The
purpose of this submission was to validate the syn-
tactic reordering method that we previously pro-
posed (Collins et al, 2005). We parse the Ger-
man training and test corpus and reorder it accord-
ing to a set of manually devised rules. Then, we
use our phrase-based system with standard phrase-
extraction, lexicalized reordering, lexical scoring,
5-gram LM, and the Pharaoh decoder.
On the development test set, the syntactic re-
ordering improved performance from 26.86 to
27.70. The best submission in last year?s shared
task achieved a score of 24.77 on this set.
5 Conclusion
We presented the first attempt at creating a system-
atic framework which uses word alignment con-
straints to guide phrase-based EM training. This
shows competitive results, to within 0.66 BLEU
points for the basic systems, suggesting that a
rigorous probabilistic framework is preferable to
heuristics for extracting phrase pairs and their
probabilities.
By introducing constraints to the alignment
space we can reduce the complexity of the joint
model and increase its performance, allowing it to
train on larger corpora and making the model more
widely applicable.
For the future, the joint model would benefit
from lexical weighting like that used in the stan-
dard model (Koehn et al, 2003). Using IBM
Model 1 to extract a lexical alignment weight for
each phrase pair would decrease the impact of data
sparseness, and other kinds smoothing techniques
will be investigated. Better search algorithms for
Viterbi phrasal alignments during EM would in-
crease the number and quality of model parame-
ters.
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation. In
Proceedings of ACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
and Chris Callison-Burch. 2005. Edinburgh system de-
scription. In IWSLT Speech Translation Evaluation.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine translation.
In Proceedings of EMNLP.
Dan Melamed. 1997. A word-to-word model of translational
equivalence. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In ACL.
Franz Josef Och. 2003. Statistical Machine Translation:
From Single-Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen Department of Computer Science,
Aachen, Germany.
Mark Przybocki. 2004. NIST 2004 machine translation eval-
uation results. Confidential e-mail to workshop partici-
pants, May.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003.
The CMU statistical machine translation system. In Ma-
chine Translation Summit.
157
Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
CCG Supertags in Factored Statistical Machine Translation
Alexandra Birch Miles Osborne Philipp Koehn
a.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
Abstract
Combinatorial Categorial Grammar (CCG)
supertags present phrase-based machine
translation with an opportunity to access
rich syntactic information at a word level.
The challenge is incorporating this informa-
tion into the translation process. Factored
translation models allow the inclusion of su-
pertags as a factor in the source or target lan-
guage. We show that this results in an im-
provement in the quality of translation and
that the value of syntactic supertags in flat
structured phrase-based models is largely
due to better local reorderings.
1 Introduction
In large-scale machine translation evaluations,
phrase-based models generally outperform syntax-
based models1. Phrase-based models are effective
because they capture the lexical dependencies be-
tween languages. However, these models, which
are equivalent to finite-state machines (Kumar and
Byrne, 2003), are unable to model long range word
order differences. Phrase-based models also lack the
ability to incorporate the generalisations implicit in
syntactic knowledge and they do not respect linguis-
tic phrase boundaries. This makes it difficult to im-
prove reordering in phrase-based models.
Syntax-based models can overcome some of the
problems associated with phrase-based models be-
cause they are able to capture the long range struc-
tural mappings that occur in translation. Recently
1www.nist.gov/speech/tests/mt/mt06eval official results.html
there have been a few syntax-based models that
show performance comparable to the phrase-based
models (Chiang, 2005; Marcu et al, 2006). How-
ever, reliably learning powerful rules from parallel
data is very difficult and prone to problems with
sparsity and noise in the data. These models also
suffer from a large search space when decoding with
an integrated language model, which can lead to
search errors (Chiang, 2005).
In this paper we investigate the idea of incorporat-
ing syntax into phrase-based models, thereby lever-
aging the strengths of both the phrase-based models
and syntactic structures. This is done using CCG
supertags, which provide a rich source of syntactic
information. CCG contains most of the structure of
the grammar in the lexicon, which makes it possi-
ble to introduce CCG supertags as a factor in a fac-
tored translation model (Koehn et al, 2006). Fac-
tored models allow words to be vectors of features:
one factor could be the surface form and other fac-
tors could contain linguistic information.
Factored models allow for the easy inclusion of
supertags in different ways. The first approach is to
generate CCG supertags as a factor in the target and
then apply an n-gram model over them, increasing
the probability of more frequently seen sequences
of supertags. This is a simple way of including syn-
tactic information in a phrase-based model, and has
also been suggested by Hassan et al (2007). For
both Arabic-English (Hassan et al, 2007) and our
experiments in Dutch-English, n-gram models over
CCG supertags improve the quality of translation.
By preferring more likely sequences of supertags,
it is conceivable that the output of the decoder is
9
more grammatical. However, its not clear exactly
how syntactic information can benefit a flat struc-
tured model: the constraints contained within su-
pertags are not enforced and relationships between
supertags are not linear. We perform experiments to
explore the nature and limits of the contribution of
supertags, using different orders of n-gram models,
reordering models and focussed manual evaluation.
It seems that the benefit of using n-gram supertag
sequence models is largely from improving reorder-
ing, as much of the gain is eroded by using a lexi-
calised reordering model. This is supported by the
manual evaluation which shows a 44% improvement
in reordering Dutch-English verb final sentences.
The second and novel way we use supertags is
to direct the translation process. Supertags on the
source sentence allows the decoder to make deci-
sions based on the structure of the input. The sub-
categorisation of a verb, for instance, might help se-
lect the correct translation. Using multiple depen-
dencies on factors in the source, we need a strat-
egy for dealing with sparse data. We propose using
a logarithmic opinion pool (Smith et al, 2005) to
combine the more specific models (which depend on
both words and supertags) with more general mod-
els (which only depends on words). This paper is the
first to suggest this approach for combining multiple
information sources in machine translation.
Although the addition of supertags to phrase-
based translation does show some improvement,
their overall impact is limited. Sequence models
over supertags clearly result in some improvements
in local reordering but syntactic information con-
tains long distance dependencies which are simply
not utilised in phrase-based models.
2 Factored Models
Inspired by work on factored language models,
Koehn et al (2006) extend phrase-based models to
incorporate multiple levels of linguistic knowledge
as factors. Phrase-based models are limited to se-
quences of words as their units with no access to
additional linguistic knowledge. Factors allow for
richer translation models, for example, the gender or
tense of a word can be expressed. Factors also allow
the model to generalise, for example, the lemma of a
word could be used to generalise to unseen inflected
forms.
The factored translation model combines features
in a log-linear fashion (Och, 2003). The most likely
target sentence t? is calculated using the decision rule
in Equation 1:
t? = argmax
t
{
M?
m=1
?mhm(s
Fs
1 , t
Ft
1 )
}
(1)
t? ?
M?
m=1
?mhm(s
Fs
1 , t
Ft
1 ) (2)
where M is the number of features, hm(s
Fs
1 , t
Ft
1 )
are the feature functions over the factors, and ? are
the weights which combine the features which are
optimised using minimum error rate training (Venu-
gopal and Vogel, 2005). Each function depends on a
vector sFs1 of source factors and a vector t
Ft
1 of tar-
get factors. An example of a factored model used in
upcoming experiments is:
t? ?
M?
m=1
?mhm(sw, twc) (3)
where sw means the model depends on (s)ource
(w)ords, and twc means the model generates (t)arget
(w)ords and (c)cg supertags. The model is shown
graphically in Figure 1.
WordWord
CCG
SOURCE TARGET
Figure 1. Factored translation with source words deter-
mining target words and CCG supertags
For our experiments we used the following fea-
tures: the translation probabilities Pr(sFs1 |t
Ft
1 ) and
Pr(tFt1 |s
Fs
1 ), the lexical weights (Koehn et al, 2003)
lex(sFs1 |t
Ft
1 ) and lex(t
Ft
1 |s
Fs
1 ), and a phrase penalty
e, which allows the model to learn a preference for
longer or shorter phrases. Added to these features
10
is the word penalty e?1 which allows the model to
learn a preference for longer or shorter sentences,
the distortion model d that prefers monotone word
order, and the language model probability Pr(t).
All these features are logged when combined in the
log-linear model in order to retain the impact of very
unlikely translations or sequences.
One of the strengths of the factored model is it
allows for n-gram distributions over factors on the
target. We call these distributions sequence models.
By analogy with language models, for example, we
can construct a bigram sequence model as follows:
p(f1, f2, . . . fn) = p(f1)
n?
i=2
p(fi|f(i?1))
where f is a factor (eg. CCG supertags) and n is
the length of the string. Sequence models over POS
tags or supertags are smaller than language models
because they have restricted lexicons. Higher or-
der, more powerful sequence models can therefore
be used.
Applying multiple factors in the source can lead to
sparse data problems. One solution is to break down
the translation into smaller steps and translate each
factor separately like in the following model where
source words are translated separately to the source
supertags:
t? ?
M?
m=1
?mhm(sw, tw) +
N?
n=1
?nhn(sc, tw)
However, in many cases multiple dependencies
are desirable. For instance translating CCG su-
pertags independently of words could introduce er-
rors. Multiple dependencies require some form of
backing off to simpler models in order to cover the
cases where, for instance, the word has been seen in
training, but not with that particular supertag. Dif-
ferent backoff paths are possible, and it would be
interesting but prohibitively slow to apply a strat-
egy similar to generalised parallel backoff (Bilmes
and Kirchhoff, 2003) which is used in factored lan-
guage models. Backoff in factored language mod-
els is made more difficult because there is no ob-
vious backoff path. This is compounded for fac-
tored phrase-based translation models where one has
to consider backoff in terms of factors and n-gram
lengths in both source and target languages. Fur-
thermore, the surface form of a word is probably the
most valuable factor and so its contribution must al-
ways be taken into account. We therefore did not use
backoff and chose to use a log-linear combination of
features and models instead.
Our solution is to extract two translation models:
t? ?
M?
m=1
?mhm(swc, tw) +
N?
n=1
?nhn(sw, tw) (4)
One model consists of more specific features m
and would return log probabilities, for example
log2Pr(tw|swc), if the particular word and supertag
had been seen before in training. Otherwise it re-
turns ?C, a negative constant emulating log2(0).
The other model consist of more general features
n and always returns log probabilities, for example
log2Pr(tw|sw).
3 CCG and Supertags
CCGs have syntactically rich lexicons and a small
set of combinatory operators which assemble the
parse-trees. Each word in the sentence is assigned a
category from the lexicon. A category may either be
atomic (S, NP etc.) or complex (S\S, (S\NP)/NP
etc.). Complex categories have the general form
?/? or ?\? where ? and ? are themselves cate-
gories. An example of a CCG parse is given:
Peter eats apples
NP (S\NP)/NP NP
>
S\NP
<
S
where the derivation proceeds as follows: ?eats?
is combined with ?apples? under the operation of
forward application. ?eats? can be thought of as a
function that takes a NP to the right and returns a
S\NP. Similarly the phrase ?eats apples? can be
thought of as a function which takes a noun phrase
NP to the left and returns a sentence S. This opera-
tion is called backward application.
A sentence together with its CCG categories al-
ready contains most of the information present in a
full parse. Because these categories are lexicalised,
11
they can easily be included into factored phrase-
based translation. CCG supertags are categories that
have been provided by a supertagger. Supertags
were introduced by Bangalore (1999) as a way of in-
creasing parsing efficiency by reducing the number
of structures assigned to each word. Clark (2002)
developed a suppertagger for CCG which uses a
conditional maximum entropy model to estimate the
probability of words being assigned particular cat-
egories. Here is an example of a sentence that has
been supertagged in the training corpus:
We all agree on that .
NP NP\NP (S[dcl]\NP)/PP PP/NP NP .
The verb ?agree? has been assigned a complex su-
pertag (S[dcl]\NP)/PP which determines the type
and direction of its arguments. This information can
be used to improve the quality of translation.
4 Experiments
The first set of experiments explores the effect of
CCG supertags on the target, translating from Dutch
into English. The last experiment shows the effect
of CCG supertags on the source, translating from
German into English. These language pairs present
a considerable reordering challenge. For example,
Dutch and German have SOVword order in subordi-
nate clauses. This means that the verb often appears
at the end of the clause, far from the position of the
English verb.
4.1 Experimental Setup
The experiments were run using Moses2, an open
source factored statistical machine translation sys-
tem. The SRILM language modelling toolkit (Stol-
cke, 2002) was used with modified Kneser-Ney dis-
counting and interpolation. The CCG supertag-
ger (Clark, 2002; Clark and Curran, 2004) was pro-
vided with the C&C Language Processing Tools3.
The supertagger was trained on the CCGBank in
English (Hockenmaier and Steedman, 2005) and in
German (Hockenmaier, 2006).
The Dutch-English parallel training data comes
from the Europarl corpus (Koehn, 2005) and ex-
cludes the proceedings from the last quarter of 2000.
2see http://www.statmt.org/moses/
3see http://svn.ask.it.usyd.edu.au/trac/candc/wiki
This consists of 855,677 sentences with a maximum
of 50 words per sentence. 500 sentences of tuning
data and the 2000 sentences of test data are taken
from the ACLWorkshop on Building and Using Par-
allel Texts4.
The German-English experiments use data from
the NAACL 2006 Workshop on Statistical Machine
Translation5. The data consists of 751,088 sentences
of training data, 500 sentences of tuning data and
3064 sentences of test data. The English and Ger-
man training sets were POS tagged and supertagged
before lowercasing. The language models and the
sequence models were trained on the Europarl train-
ing data. Where not otherwise specified, the POS
tag and supertag sequence models are 5-gram mod-
els and the language model is a 3-gram model.
4.2 Sequence Models Over Supertags
Our first Dutch-English experiment seeks to estab-
lish what effect sequence models have on machine
translation. We show that supertags improve trans-
lation quality. Together with Shen et al (2006) it is
one of the first results to confirm the potential of the
factored model.
Model BLEU
sw, tw 23.97
sw, twp 24.11
sw, twc 24.42
sw, twpc 24.43
Table 1. The effect of sequence models on Dutch-English
BLEU score. Factors are (w)ords, (p)os tags, (c)cg su-
pertags on the source s or the target t
Table 1 shows that sequence models over CCG su-
pertags in the target (model sw, twc) improves over
the baseline (model sw, tw) which has no supertags.
Supertag sequence models also outperform models
which apply POS tag sequence models (sw, twp)
and, interestingly do just as well as models which
apply both POS tag and supertag sequence mod-
els (sw, twps). Supertags are more informative than
POS tags as they contain the syntactic context of a
word.
These experiments were run with the distortion
limit set to 6. This means that at most 6 words in
4see http://www.statmt.org/wpt05/
5see http://www.statmt.org/wpt06/
12
the source sentence can be skipped. We tried setting
the distortion limit to 15 to see if allowing longer
distance reorderings with CCG supertag sequence
models could further improve performance, however
it resulted in a decrease in performance to a BLEU
score of 23.84.
4.3 Manual Analysis
The BLEU score improvement in Table 1 does not
explain how the supertag sequence models affect the
translation process. As suggested by Callison-Burch
et al(2006) we perform a focussed manual analysis
of the output to see what changes have occurred.
From the test set, we randomly selected 100
sentences which required reordering of verbs: the
Dutch sentences ended with a verb which had to be
moved forward in the English translation. We record
whether or not the verb was correctly translated and
whether it was reordered to the correct position in
the target sentence.
Model Translated Reordered
sw, tw 81 36
sw, twc 87 52
Table 2. Analysis of % correct translation and reordering
of verbs for Dutch-English translation
In Table 2 we can see that the addition of the CCG
supertag sequence model improved both the transla-
tion of the verbs and their reordering. However, the
improvement is much more pronounced for reorder-
ing. The difference in the reordering results is signif-
icant at p < 0.05 using the ?2 significance test. This
shows that the syntactic information in the CCG su-
pertags is used by the model to prefer better word
order for the target sentence.
In Figure 2 we can see two examples of Dutch-
English translations that have improved with the ap-
plication of CCG supertag sequence models. In the
first example the verb ?heeft? occurs at the end of the
source sentence. The baseline model (sw, tw) does
not manage to translate ?heeft?. The model with the
CCG supertag sequence model (sw, twc) translates it
correctly as ?has? and reorders it correctly 4 places
to the left. The second example also shows the se-
quence model correctly translating the Dutch verb at
the end of the sentence ?nodig?. One can see that it
is still not entirely grammatical.
The improvements in reordering shown here are
reorderings over a relatively short distance, two or
three positions. This is well within the 5-gram order
of the CCG supertag sequence model and we there-
fore consider this to be local reordering.
4.4 Order of the Sequence Model
The CCG supertags describe the syntactic context
of the word they are attached to. Therefore they
have an influence that is greater in scope than sur-
face words or POS tags. Increasing the order of
the CCG supertag sequence model should also in-
crease the ability to perform longer distance reorder-
ing. However, at some point the reliability of the
predictions of the sequence models is impaired due
to sparse counts.
Model None 1gram 3gram 5gram 7gram
sw, twc 24.18 23.96 24.19 24.42 24.32
sw, twpc 24.34 23.86 24.09 24.43 24.14
Table 3. BLUE scores for Dutch-English models which
apply CCG supertag sequence models of varying orders
In Table 3 we can see that the optimal order for
the CCG supertag sequence models is 5.
4.5 Language Model vs. Supertags
The language model makes a great contribution to
the correct order of the words in the target sentence.
In this experiment we investigate whether by using a
stronger language model the contribution of the se-
quence model will no longer be relevant. The rel-
ative contribution of the language mode and differ-
ent sequence models is investigated for different lan-
guage model n-gram lengths.
Model None 1gram 3gram 5gram 7gram
sw, tw - 21.22 23.97 24.05 24.13
sw, twp 21.87 21.83 24.11 24.25 24.06
sw, twc 21.75 21.70 24.42 24.67 24.60
sw, twpc 21.99 22.07 24.43 24.48 24.42
Table 4. BLEU scores for Dutch-English models which use
language models of increasing n-gram length. Column
None does not apply any language model. Model sw, tw
does not apply any sequence models, and model sw, twpc
applies both POS tag and supertag sequence models.
In Table 4 we can see that if no language model
is present(None), the system benefits slightly from
13
source:hij kan toch niet beweren dat hij daar geen exacte informatie over heeft !
reference: how can he say he does not have any precise information ?
sw, tw:he cannot say that he is not an exact information about .
sw, twc: he cannot say that he has no precise information on this !
source: wij moeten hun verwachtingen niet beschamen . meer dan ooit hebben al die landen thans onze bijstand nodig
reference: we must not disappoint them in their expectations , and now more than ever these countries need our help
sw, tw:we must not fail to their expectations , more than ever to have all these countries now our assistance necessary
sw, twc: we must not fail to their expectations , more than ever , those countries now need our assistance
Figure 2. Examples where the CCG supertag sequence model improves Dutch-English translation
having access to all the other sequence models.
However, the language model contribution is very
strong and in isolation contributes more to transla-
tion performance than any other sequence model.
Even with a high order language model, applying
the CCG supertag sequence model still seems to im-
prove performance. This means that even if we use
a more powerful language model, the structural in-
formation contained in the supertags continues to be
beneficial.
4.6 Lexicalised Reordering vs. Supertags
In this experiment we investigate using a stronger
reordering model to see how it compares to the con-
tribution that CCG supertag sequence models make.
Moses implements the lexicalised reordering model
described by Tillman (2004), which learns whether
phrases prefer monotone, inverse or disjoint orienta-
tions with regard to adjacent phrases. We apply this
reordering models to the following experiments.
Model None Lex. Reord.
sw, tw 23.97 24.72
sw, twc 24.42 24.78
Table 5. Dutch-English models with and without a lexi-
calised reordering model.
In Table 5 we can see that lexicalised reorder-
ing improves translation performance for both mod-
els. However, the improvement that was seen us-
ing CCG supertags without lexicalised reordering,
almost disappears when using a stronger reordering
model. This suggests that CCG supertags? contribu-
tion is similar to that of a reordering model. The lex-
icalised reordering model only learns the orientation
of a phrase with relation to its adjacent phrase, so its
influence is very limited in range. If it can replace
CCG supertags, it suggests that supertags? influence
is also within a local range.
4.7 CCG Supertags on Source
Sequence models over supertags improve the perfor-
mance of phrase-based machine translation. How-
ever, this is a limited way of leveraging the rich syn-
tactic information available in the CCG categories.
We explore the potential of letting supertags direct
translation by including them as a factor on the
source. This is similar to syntax-directed translation
originally proposed for compiling (Aho and Ullman,
1969), and also used in machine translation (Quirk et
al., 2005; Huang et al, 2006). Information about the
source words? syntactic function and subcategori-
sation can directly influence the hypotheses being
searched in decoding. These experiments were per-
formed on the German to English translation task,
in contrast to the Dutch to English results given in
previous experiments.
We use a model which combines more specific
dependencies on source words and source CCG su-
pertags, with a more general model which only has
dependancies on the source word, see Equation 4.
We explore two different ways of balancing the sta-
tistical evidence from these multiple sources. The
first way to combine the general and specific sources
of information is by considering features from both
models as part of one large log-linear model. How-
ever, by including more and less informative fea-
tures in one model, we may transfer too much ex-
planatory power to the more specific features. To
overcome this problem, Smith et al (2006) demon-
strated that using ensembles of separately trained
models and combining them in a logarithmic opin-
ion pool (LOP) leads to better parameter values.
This approach was used as the second way in which
14
we combined our models. An ensemble of log-linear
models was combined using a multiplicative con-
stant ? which we train manually using held out data.
t? ?
M?
m=1
?mhm(swc, tw) + ?
(
N?
n=1
?nhn(sw, tw)
)
Typically, the two models would need to be nor-
malised before being combined, but here the multi-
plicative constant fulfils this ro?le by balancing their
separate contributions. This is the first work sug-
gesting the application of LOPs to decoding in ma-
chine translation. In the future more sophisticated
translation models and ensembles of models will
need methods such as LOPs in order to balance sta-
tistical evidence from multiple sources.
Model BLEU
sw, tw 23.30
swc, tw 19.73
single 23.29
LOP 23.46
Table 6. German-English: CCG supertags are used as a
factor on the source. The simple models are combined in
two ways: either as a single log-linear model or as a LOP
of log-linear models
Table 6 shows that the simple, general model
(model sw, tw) performs considerably better than
the simple specific model, where there are multi-
ple dependencies on both words and CCG supertags
(model swc, tw). This is because there are words in
the test sentence that have been seen before but not
with the CCG supertag. Statistical evidence from
multiple sources must be combined. The first way
to combine them is to join them in one single log-
linear model, which is trained over many features.
This makes finding good weights difficult as the in-
fluence of the general model is greater, and its dif-
ficult for the more specific model to discover good
weights. The second method for combining the in-
formation is to use the weights from the separately
trained simple models and then combine them in a
LOP. Held out data is used to set the multiplicative
constant needed to balance the contribution of the
two models. We can see that this second approach is
more successful and this suggests that it is important
to carefully consider the best ways of combining dif-
ferent sources of information when using ensembles
of models. However, the results of this experiment
are not very conclusive. There is no uncertainty in
the source sentence and the value of modelling it us-
ing CCG supertags is still to be demonstrated.
5 Conclusion
The factored translation model allows for the inclu-
sion of valuable sources of information in many dif-
ferent ways. We have shown that the syntactically
rich CCG supertags do improve the translation pro-
cess and we investigate the best way of including
them in the factored model. Using CCG supertags
over the target shows the most improvement, espe-
cially when using targeted manual evaluation. How-
ever, this effect seems to be largely due to improved
local reordering. Reordering improvements can per-
haps be more reliably made using better reordering
models or larger, more powerful language models.
A further consideration is that supertags will always
be limited to the few languages for which there are
treebanks.
Syntactic information represents embedded
structures which are naturally incorporated into
grammar-based models. The ability of a flat struc-
tured model to leverage this information seems to be
limited. CCG supertags? ability to guide translation
would be enhanced if the constraints encoded in
the tags were to be enforced using combinatory
operators.
6 Acknowledgements
We thank Hieu Hoang for assistance with Moses, Ju-
lia Hockenmaier for access to CCGbank lexicons in
German and English, and Stephen Clark and James
Curran for providing the supertagger. This work was
supported in part under the GALE program of the
Defense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022 and in part under the
EuroMatrix project funded by the European Com-
mission (6th Framework Programme).
15
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Properties of syn-
tax directed translations. Journal of Computer and System
Sciences, 3(3):319?334.
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Proceedings of
the North American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine transla-
tion research. In Proceedings of the European Chapter of
the Association for Computational Linguistics, Trento, Italy.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 263?270, Ann
Arbor, Michigan.
Stephen Clark and James R. Curran. 2004. Parsing the wsj
using ccg and log-linear models. In Proceedings of the
Association for Computational Linguistics, pages 103?110,
Barcelona, Spain.
Stephen Clark. 2002. Supertagging for combinatory categorial
grammar. In Proceedings of the International Workshop on
Tree Adjoining Grammars, pages 19?24, Venice, Italy.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics, Prague, Czech Republic. (to appear).
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank man-
ual. Technical Report MS-CIS-05-09, Department of Com-
puter and Information Science, University of Pennsylvania.
Julia Hockenmaier. 2006. Creating a ccgbank and a wide-
coverage ccg lexicon for german. In Proceedings of the In-
ternational Conference on Computational Linguistics and of
the Association for Computational Linguistics, Sydney, Aus-
tralia.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of the Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language Pro-
cessing, pages 1?8, New York City, New York. Association
for Computational Linguistics.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the Human
Language Technology and North American Association for
Computational Linguistics Conference, pages 127?133, Ed-
monton, Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Richard Zens, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2006. Open source toolkit
for statistical machine translation. In Summer Workshop on
Language Engineering, John Hopkins University Center for
Language and Speech Processing.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit.
Shankar Kumar and William Byrne. 2003. A weighted finite
state transducer implementation of the alignment template
model for statistical machine translation. In Proceedings of
the Human Language Technology and North American As-
sociation for Computational Linguistics Conference, pages
63?70, Edmonton, Canada.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 44?52, Sydney, Australia.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the Associ-
ation for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proceedings of the Association for Computational
Linguistics, pages 271?279, Ann Arbor, Michigan.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Fed-
erico. 2006. The JHU workshop 2006 IWSLT system. In
Proceedings of the International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 59?63, Kyoto, Japan.
Andrew Smith and Miles Osborne. 2006. Using gazetteers in
discriminative information extraction. In The Conference on
Natural Language Learning, New York City, USA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Pro-
ceedings of the Association for Computational Linguistics,
pages 18?25, Ann Arbor, Michigan.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of Spoken Language Process-
ing, pages 901?904.
Christoph Tillman. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of the Hu-
man Language Technology and North American Association
for Computational Linguistics Conference, pages 101?104,
Boston, USA. Association for Computational Linguistics.
Ashish Venugopal and Stephan Vogel. 2005. Considerations
in MCE and MMI training for statistical machine transla-
tion. In Proceedings of the European Association for Ma-
chine Translation, Budapest, Hungary.
16
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 197?205,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Quantitative Analysis of Reordering Phenomena
Alexandra Birch Phil Blunsom Miles Osborne
a.c.birch-mayne@sms.ed.ac.uk pblunsom@inf.ed.ac.uk miles@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
Reordering is a serious challenge in sta-
tistical machine translation. We propose
a method for analysing syntactic reorder-
ing in parallel corpora and apply it to un-
derstanding the differences in the perfor-
mance of SMT systems. Results at recent
large-scale evaluation campaigns show
that synchronous grammar-based statisti-
cal machine translation models produce
superior results for language pairs such as
Chinese to English. However, for language
pairs such as Arabic to English, phrase-
based approaches continue to be competi-
tive. Until now, our understanding of these
results has been limited to differences in
BLEU scores. Our analysis shows that cur-
rent state-of-the-art systems fail to capture
the majority of reorderings found in real
data.
1 Introduction
Reordering is a major challenge in statistical ma-
chine translation. Reordering involves permuting
the relative word order from source sentence to
translation in order to account for systematic dif-
ferences between languages. Correct word order is
important not only for the fluency of output, it also
affects word choice and the overall quality of the
translations.
In this paper we present an automatic method
for characterising syntactic reordering found in a
parallel corpus. This approach allows us to analyse
reorderings quantitatively, based on their number
and span, and qualitatively, based on their relation-
ship to the parse tree of one sentence. The methods
we introduce are generally applicable, only requir-
ing an aligned parallel corpus with a parse over the
source or the target side, and can be extended to
allow for more than one reference sentence and
derivations on both source and target sentences.
Using this method, we are able to compare the re-
ordering capabilities of two important translation
systems: a phrase-based model and a hierarchical
model.
Phrase-based models (Och and Ney, 2004;
Koehn et al, 2003) have been a major paradigm
in statistical machine translation in the last few
years, showing state-of-the-art performance for
many language pairs. They search all possible re-
orderings within a restricted window, and their
output is guided by the language model and a
lexicalised reordering model (Och et al, 2004),
both of which are local in scope. However, the
lack of structure in phrase-based models makes it
very difficult to model long distance movement of
words between languages.
Synchronous grammar models can encode
structural mappings between languages which al-
low complex, long distance reordering. Some
grammar-based models such as the hierarchical
model (Chiang, 2005) and the syntactified target
language phrases model (Marcu et al, 2006) have
shown better performance than phrase-based mod-
els on certain language pairs.
To date our understanding of the variation in re-
ordering performance between phrase-based and
synchronous grammar models has been limited to
relative BLEU scores. However, Callison-Burch et
al. (2006) showed that BLEU score alone is insuffi-
cient for comparing reordering as it only measures
a partial ordering on n-grams. There has been little
direct research on empirically evaluating reorder-
ing.
We evaluate the reordering characteristics of
these two paradigms on Chinese-English and
Arabic-English translation. Our main findings are
as follows: (1) Chinese-English parallel sentences
exhibit many medium and long-range reorderings,
but less short range ones than Arabic-English, (2)
phrase-based models account for short-range re-
orderings better than hierarchical models do, (3)
197
by contrast, hierarchical models clearly outper-
form phrase-based models when there is signif-
icant medium-range reordering, and (4) none of
these systems adequately deal with longer range
reordering.
Our analysis provides a deeper understand-
ing of why hierarchical models demonstrate bet-
ter performance for Chinese-English translation,
and also why phrase-based approaches do well at
Arabic-English.
We begin by reviewing related work in Sec-
tion 2. Section 3 describes our method for ex-
tracting and measuring reorderings in aligned and
parsed parallel corpora. We apply our techniques
to human aligned parallel treebank sentences in
Section 4, and to machine translation outputs in
Section 5.We summarise our findings in Section 6.
2 Related Work
There are few empirical studies of reordering be-
haviour in the statistical machine translation lit-
erature. Fox (2002) showed that many common
reorderings fall outside the scope of synchronous
grammars that only allow the reordering of child
nodes. This study was performed manually and
did not compare different language pairs or trans-
lation paradigms. There are some comparative
studies of the reordering restrictions that can be
imposed on the phrase-based or grammar-based
models (Zens and Ney, 2003; Wellington et al,
2006), however these do not look at the reordering
performance of the systems. Chiang et al (2005)
proposed a more fine-grained method of compar-
ing the output of two translation systems by us-
ing the frequency of POS sequences in the output.
This method is a first step towards a better under-
standing of comparative reordering performance,
but neglects the question of what kind of reorder-
ing is occurring in corpora and in translation out-
put.
Zollmann et al (2008) performed an empiri-
cal comparison of the BLEU score performance
of hierarchical models with phrase-based models.
They tried to ascertain which is the stronger model
under different reordering scenarios by varying
distortion limits the strength of language models.
They show that the hierarchical models do slightly
better for Chinese-English systems, but worse for
Arabic-English. However, there was no analysis of
the reorderings existing in their parallel corpora,
or on what kinds of reorderings were produced in
their output. We perform a focused evaluation of
these issues.
Birch et al (2008) proposed a method for ex-
tracting reorderings from aligned parallel sen-
tences.We extend this method in order to constrain
the reorderings to a derivation over the source sen-
tence where possible.
3 Measuring Reordering
Reordering is largely driven by syntactic differ-
ences between languages and can involve complex
rearrangements between nodes in synchronous
trees. Modeling reordering exactly would be
sparse and heterogeneous and thus we make an
important simplifying assumption in order for the
detection and extraction of reordering data to be
tractable and useful. We assume that reordering
is a binary process occurring between two blocks
that are adjacent in the source. We extend the
methods proposed by Birch et al (2008) to iden-
tify and measure reordering. Modeling reordering
as the inversion in order of two adjacent blocks is
similar to the approach taken by the Inverse Trans-
duction Model (ITG) (Wu, 1997), except that here
we are not limited to a binary tree. We also detect
and include non-syntactic reorderings as they con-
stitute a significant proportion of the reorderings.
Birch et al (2008) defined the extraction pro-
cess for a sentence pair that has been word aligned.
This method is simple, efficient and applicable to
all aligned sentence pairs. However, if we have ac-
cess to the syntax tree, we can more accurately
determine the groupings of embedded reorder-
ings, and we can also access interesting informa-
tion about the reordering such as the type of con-
stituents that get reordered. Figure 1 shows the
advantage of using syntax to guide the extraction
process. Embedded reorderings that are extracted
without syntax assume a right branching structure.
Reorderings that are extracted using the syntac-
tic extraction algorithm reflect the correct sentence
structure. We thus extend the algorithm to extract-
ing syntactic reorderings. We require that syntac-
tic reorderings consist of blocks of whole sibling
nodes in a syntactic tree over the source sentence.
In Figure 2 we can see a sentence pair with an
alignment and a parse tree over the source. We per-
form a depth first recursion through the tree, ex-
tracting the reorderings that occur between whole
sibling nodes. Initially a reordering is detected be-
tween the leaf nodes P and NN. The block growing
algorithm described in Birch et al (2008) is then
used to grow block A to include NT and NN, and
block B to include P and NR. The source and tar-
get spans of these nodes do not overlap the spans
198
Figure 1. An aligned sentence pair which shows two
different sets of reorderings for the case without and
with a syntax tree.
of any other nodes, and so the reordering is ac-
cepted. The same happens for the higher level re-
ordering where block A covers NP-TMP and PP-
DIR, and block B covers the VP. In cases where
the spans do overlap spans of nodes that are not
siblings, these reorderings are then extracted us-
ing the algorithm described in Birch et al (2008)
without constraining them to the parse tree. These
non-syntactic reorderings constitute about 10% of
the total reorderings and they are a particular chal-
lenge to models which can only handle isomorphic
structures.
RQuantity
The reordering extraction technique allows us to
analyse reorderings in corpora according to the
distribution of reordering widths and syntactic
types. In order to facilitate the comparison of dif-
ferent corpora, we combine statistics about in-
dividual reorderings into a sentence level metric
which is then averaged over a corpus. This met-
ric is defined using reordering widths over the tar-
get side to allow experiments with multiple lan-
guage pairs to be comparable when the common
language is the target.
We use the average RQuantity (Birch et al,
2008) as our measure of the amount of reordering
in a parallel corpus. It is defined as follows:
RQuantity =
?
r?R |rAt | + |rBt |
I
where R is the set of reorderings for a sentence,
I is the target sentence length, A and B are the
two blocks involved in the reordering, and |rAs |
is the size or span of block A on the target side.
RQuantity is thus the sum of the spans of all the
reordering blocks on the target side, normalised
$ %
$
%
Figure 2. A sentence pair from the test corpus, with its
alignment and parse tree. Two reorderings are shown
with two different dash styles.
by the length of the target sentence. The minimum
RQuantity for a sentence would be 0. The max-
imum RQuantity occurs where the order of the
sentence is completely inverted and the RQuantity
is
?I
i=2 i. See, for example, Figure 1 where the
RQuantity is 94 .
4 Analysis of Reordering in Parallel
Corpora
Characterising the reordering present in different
human generated parallel corpora is crucial to un-
derstanding the kinds of reordering wemust model
in our translations. We first need to extract reorder-
ings for which we need alignments and deriva-
tions. We could use automatically generated an-
notations, however these contain errors and could
be biased towards the models which created them.
The GALE project has provided gold standard
word alignments for Arabic-English (AR-EN) and
Chinese-English (CH-EN) sentences.1 A subset of
these sentences come from the Arabic and Chi-
nese treebanks, which provide gold standard parse
trees. The subsets of parallel data for which we
have both alignments and parse trees consist of
1see LDC corpus LDC2006E93 version GALE-Y1Q4
199
ll
l l
l l
l l l
l
l
0.0
0.2
0.4
0.6
0.8
1.0
Sentence Length Bin
RQu
antit
y
0?9 20?29 40?49 60?69 80?89 >=100
l CH.EN.RQuantityAR.EN.RQuantity
Figure 3. Sentence level measures of RQuantity for the
CH-EN and AR-EN corpora for different English sen-
tence lengths.
l l
l
l
l
l
l
l
l
l
0
500
1000
1500
2000
2500
Reordering Width
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
l CH?ENAR?EN
Figure 4. Comparison of reorderings of different widths
for the CH-EN and AR-EN corpora.
3,380 CH-EN sentences and 4,337 AR-EN sen-
tences.
Figure 3 shows that the different corpora have
very different reordering characteristics. The CH-
EN corpus displays about three times the amount
of reordering (RQuantity) than the AR-EN cor-
pus. For CH-EN, the RQuantity increases with
sentence length and for AR-EN, it remains con-
stant. This seems to indicate that for longer CH-
EN sentences there are larger reorderings, but this
is not the case for AR-EN. RQuantity is low for
very short sentences, which indicates that these
sentences are not representative of the reordering
characteristics of a corpus. The measures seem
to stabilise for sentences with lengths of over 20
words.
The average amount of reordering is interesting,
but it is also important to look at the distribution
of reorderings involved. Figure 4 shows the re-
orderings in the CH-EN and AR-EN corpora bro-
l
l
l
l
l
l
l
l
l
l0
5
10
15
20
25
30
Widths of Reorderings
% N
umb
er o
f Re
orde
rings
 for W
idth
2 3 4 5 6 7?8 9?10 16?20
l NPDNPCPNP.PN
Figure 5. The four most common syntactic types being
reordered forward in target plotted as % of total syntac-
tic reorderings against reordering width (CH-EN).
ken down by the total width of the source span
of the reorderings. The figure clearly shows how
different the two language pairs are in terms of
reordering widths. Compared to the CH-EN lan-
guage pair, the distribution of reorderings in AR-
EN has many more reorderings over short dis-
tances, but many fewer medium or long distance
reorderings. We define short, medium or long dis-
tance reorderings to mean that they have a reorder-
ing of width of between 2 to 4 words, 5 to 8 and
more than 8 words respectively.
Syntactic reorderings can reveal very rich
language-specific reordering behaviour. Figure 5
is an example of the kinds of data that can be used
to improve reordering models. In this graph we se-
lected the four syntactic types that were involved
in the largest number of reorderings. They cov-
ered the block that was moved forward in the tar-
get (block A). We can see that different syntactic
types display quite different behaviour at different
reordering widths and this could be important to
model.
Having now characterised the space of reorder-
ing actually found in parallel data, we now turn
to the question of how well our translation models
account for them. As both the translation models
investigated in this work do not use syntax, in the
following sections we focus on non-syntactic anal-
ysis.
5 Evaluating Reordering in Translation
We are interested in knowing how current trans-
lation models perform specifically with regard to
reordering. To evaluate this, we compare the re-
orderings in the parallel corpora with the reorder-
ings that exist in the translated sentences. We com-
200
None Low Medium High
Average RQuantity
CH-EN 0 0.39 0.82 1.51
AR-EN 0 0.10 0.25 0.57
Number of Sentences
CH-EN 105 367 367 367
AR-EN 293 379 379 379
Table 1. The RQuantity and the number of sentences
for each reordering test set.
pare two state-of-the-art models: the phrase-based
system Moses (Koehn et al, 2007) (with lexi-
calised reordering), and the hierarchical model Hi-
ero (Chiang, 2007). We use default settings for
both models: a distortion limit of seven for Moses,
and a maximum source span limit of 10 words for
Hiero. We trained both models on subsets of the
NIST 2008 data sets, consisting mainly of news
data, totalling 547,420 CH-EN and 1,069,658 AR-
EN sentence pairs. We used a trigram language
model on the entire English side (211M words)
of the NIST 2008 Chinese-English training cor-
pus. Minimum error rate training was performed
on the 2002 NIST test for CH-EN, and the 2004
NIST test set for AR-EN.
5.1 Reordering Test Corpus
In order to determine what effect reordering has
on translation, we extract a test corpus with spe-
cific reordering characteristics from the manually
aligned and parsed sentences described in Sec-
tion 4. To minimise the impact of sentence length,
we select sentences with target lengths from 20 to
39 words inclusive. In this range RQuantity is sta-
ble. From these sentences we first remove those
with no detected reorderings, and we then divide
up the remaining sentences into three sets of equal
sizes based on the RQuantity of each sentence. We
label these test sets: ?none?, ?low?, ?medium? and
?high?.
All test sentences have only one reference En-
glish sentence. MT evaluations using one refer-
ence cannot make strong claims about any partic-
ular test sentence, but are still valid when used to
compare large numbers of hypotheses.
Table 1 and Figure 6 show the reordering char-
acteristics of the test sets. As expected, we see
more reordering for Chinese-English than for Ara-
bic to English.
It is important to note that although we might
name a set ?low? or ?high?, this is only relative
to the other groups for the same language pair.
The ?high? AR-EN set, has a lower RQuantity
than the ?medium? CH-EN set. Figure 6 shows
0
50
100
150
200
250
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
LowMediumHigh
Figure 6. Number of reorderings in the CH-EN test set
plotted against the total width of the reorderings.
none low med high all
MOSESHIERO
14
16
18
20
22
Figure 7. BLEU scores for the different CH-EN reorder-
ing test sets and the combination of all the groups for
the two translation models.The 95% confidence levels
as measured by bootstrap resampling are shown for
each bar.
that the CH-EN reorderings in the higher RQuan-
tity groups have more and longer reorderings. The
AR-EN sets show similar differences in reordering
behaviour.
5.2 Performance on Test Sets
In this section we compare the translation output
for the phrase-based and the hierarchical system
for different reordering scenarios. We use the test
sets created in Section 5.1 to explicitly isolate the
effect reordering has on the performance of two
translation systems.
Figure 7 and Figure 8 show the BLEU score
results of the phrase-based model and the hierar-
chical model on the different reordering test sets.
The 95% confidence intervals as calculated by
bootstrap resampling (Koehn, 2004) are shown for
each of the results. We can see that the models
show quite different behaviour for the different
test sets and for the different language pairs. This
demonstrates that reordering greatly influences the
201
none low med high all
MOSESHIERO
16
18
20
22
24
26
Figure 8. BLEU scores for the different AR-EN reorder-
ing test sets and the combination of all the groups for
the two translation models. The 95% confidence lev-
els as measured by bootstrap resampling are shown for
each bar.
BLEU score performance of the systems.
In Figure 7 we see that the hierarchical model
performs considerably better than Moses on the
?medium? CH-EN set, although the confidence
interval for these results overlap somewhat. This
supports the claim that Hiero is better able to cap-
ture longer distance reorderings than Moses.
Hiero performs significantly worse than Moses
on the ?none? and ?low? sets for CH-EN, and
for all the AR-EN sets, other than ?none?. All
these sets have a relatively low amount of reorder-
ing, and in particular a low number of medium
and long distance reorderings. The phrase-based
model could be performing better because it
searches all possible permutations within a certain
window whereas the hierarchical model will only
permit reorderings for which there is lexical evi-
dence in the training corpus. Within a small win-
dow, this exhaustive search could discover the best
reorderings, but within a bigger window, the more
constrained search of the hierarchical model pro-
duces better results. It is interesting that Hiero is
not always the best choice for translation perfor-
mance, and depending on the amount of reorder-
ing and the distribution of reorderings, the simpler
phrase-based approach is better.
The fact that both models show equally poor
performance on the ?high? RQuantity test set sug-
gests that the hierarchical model has no advantage
over the phrase-based model when the reorder-
ings are long enough and frequent enough. Nei-
ther Moses nor Hiero can perform long distance
reorderings, due to the local constraints placed on
their search which allows performance to be lin-
ear with respect to sentence length. Increasing the
window in which these models are able to perform
reorderings does not necessarily improve perfor-
l
l l
l l
l l l0
20
40
60
80
100
120
140
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7 8 >8
l NoneLowMediumHigh
Figure 9. Reorderings in the CH-EN MOSES transla-
tion of the reordering test set, plotted against the total
width of the reorderings.
mance, due to the number of hypotheses the mod-
els must discriminate amongst.
The performance of both systems on the ?high?
test set could be much worse than the BLEU score
would suggest. A long distance reordering that has
been missed, would only be penalised by BLEU
once at the join of the two blocks, even though it
might have a serious impact on the comprehension
of the translation. This flaw seriously limits the
conclusions that we can draw from BLEU score,
and motivates analysing translations specifically
for reordering as we do in this paper.
Reorderings in Translation
At best, BLEU can only partially reflect the re-
ordering performance of the systems. We therefore
perform an analysis of the distribution of reorder-
ings that are present in the systems? outputs, in or-
der to compare them with each other and with the
source-reference distribution.
For each hypothesis translation, we record
which source words and phrase pairs or rules were
used to produce which target words. From this we
create an alignment matrix from which reorder-
ings are extracted in the same manner as previ-
ously done for the manually aligned corpora.
Figure 9 shows the distribution of reorderings
that occur between the source sentence and the
translations from the phrase-based model. This
graph is interesting when compared with Figure 6,
which shows the reorderings that exist in the orig-
inal reference sentence pair. The two distribu-
tions are quite different. Firstly, as the models use
phrases which are treated as blocks, reorderings
which occur within a phrase are not recorded. This
reduces the number of shorter distance reorder-
ings in the distribution in Figure 6, as mainly short
202
l
l
l l
l
l
l l0
10
20
30
40
50
Widths of Reorderings
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7 8 >8
l NoneLowMediumHigh
Figure 10. Reorderings in the CH-EN Hiero translation
of the reordering test set, plotted against the total width
of the reorderings.
phrases pairs are used in the hypothesis. However,
even taking reorderings within phrase pairs into
account, there are many fewer reorderings in the
translations than in the references, and there are
no long distance reorderings.
It is interesting that the phrase-based model is
able to capture the fact that reordering increases
with the RQuantity of the test set. Looking at the
equivalent data for the AR-EN language pair, a
similar pattern emerges: there are many fewer re-
orderings in the translations than in the references.
Figure 10 shows the reorderings from the output
of the hierarchical model. The results are very dif-
ferent to both the phrase-based model output (Fig-
ure 9) and to the original reference reordering dis-
tribution (Figure 6). There are fewer reorderings
here than even in the phrase-based output. How-
ever, the Hiero output has a slightly higher BLEU
score than the Moses output. The number of re-
orderings is clearly not the whole story. Part of the
reason why the output seems to have few reorder-
ings and yet scores well, is that the output of hier-
archical models does not lend itself to the analysis
that we have performed successfully on the ref-
erence or phrase-based translation sentence pairs.
This is because the output has a large number of
non-contiguous phrases which prevent the extrac-
tion of reorderings from within their span. Only
4.6% of phrase-based words were blocked off due
to non-contiguous phrases but 47.5% of the hier-
archical words were. This problem can be amelio-
rated with the detection and unaligning of words
which are obviously dependent on other words in
the non-contiguous phrase.
Even taking blocked off phrases into account,
however, the number of reorderings in the hierar-
l l
l
l
l
l
l
l
l0
100
200
300
400
500
600
Reordering Width
Num
ber 
of R
eord
ering
s
2 3 4 5 6 7?8 9?10 16?20
l Test.SetPhrase.BasedHierarchical
Figure 11. Number of reorderings in the original CH-
EN test set, compared to the reorderings retained by
the phrase-based and hierarchical models. The data is
shown relative to the length of the total source width of
the reordering.
chical output is still low, especially for the medium
and long distance reorderings, as compared to the
reference sentences. The hierarchical model?s re-
ordering behaviour is very different to human re-
ordering. Even if human translations are freer and
contain more reordering than is strictly necessary,
many important reorderings are surely being lost.
Targeted Automatic Evaluation
Comparing distributions of reorderings is inter-
esting, but it cannot approach the question of how
many reorderings the system performed correctly.
In this section we identify individual reorderings
in the source and reference sentences and detect
whether or not they have been reproduced in the
translation.
Each reordering in the original test set is ex-
tracted. Then the source-translation alignment is
inspected to determine whether the blocks in-
volved in the original reorderings are in the reverse
order in the translation. If so, we say that these re-
orderings have been retained from the reference to
the translation.
If a reordering has been translated by one phrase
pair, we assume that the reordering has been re-
tained, because the reordering could exist inside
the phrase. If the segmentation is slightly differ-
ent, but a reordering of the correct size occurred at
the right place, it is also considered to be retained.
Figure 11 shows that the hierarchical model
retains more reorderings of all widths than the
phrase-based system. Both systems retain few re-
orderings, with the phrase-based model missing
almost all the medium distance reorderings, and
both models failing on all the long distance re-
203
Correct Incorrect NA
Retained 61 4 10
Not Retained 32 31 12
Table 2. Correlation between retaining reordering and it
being correct - for humans and for system
orderings. This is possibly the most direct evi-
dence of reordering performance so far, and again
shows how Hiero has a slight advantage over the
phrase-based systemwith regard to reordering per-
formance.
Targeted Manual Analysis
The relationship between targeted evaluation
and the correct reordering of the translation still
needs to be established. The translation system can
compensate for not retaining a reordering by us-
ing different lexical items. To judge the relevance
of the targeted evaluation we need to perform a
manual evaluation. We present evaluators with the
reference and the translation sentences. We mark
the target ranges of the blocks that are involved
in the particular reordering we are analysing, and
ask the evaluator if the reordering in the translation
is correct, incorrect or not applicable. The not ap-
plicable case is chosen when the translated words
are so different from the reference that their order-
ing is irrelevant. There were three evaluators who
each judged 25 CH-EN reorderings which were re-
tained and 25 CH-EN reorderings which were not
retained by the Moses translation model.
The results in Table 2 show that the retained
reorderings are generally judged to be correct. If
the reordering is not retained, then the evaluators
divided their judgements evenly between the re-
ordering being correct or incorrect. It seems that
the fact that a reordering is not retained does in-
dicate that its ordering is more likely to be incor-
rect. We used Fleiss? Kappa to measure the cor-
relation between annotators. It expresses the ex-
tent to which the amount of agreement between
raters is greater than what would be expected if
all raters made their judgements randomly. In this
case Fleiss? kappa is 0.357 which is considered to
be a fair correlation.
6 Conclusion
In this paper we have introduced a general and
extensible automatic method for the quantitative
analyse of syntactic reordering phenomena in par-
allel corpora.
We have applied our method to a systematic
analysis of reordering both in the training corpus,
and in the output, of two state-of-the-art transla-
tion models. We show that the hierarchical model
performs better than the phrase-based model in sit-
uations where there are many medium distance re-
orderings. In addition, we find that the choice of
translation model must be guided by the type of re-
orderings in the language pair, as the phrase-based
model outperforms the hierarchical model when
there is a predominance of short distance reorder-
ings. However, neither model is able to capture the
reordering behaviour of the reference corpora ad-
equately. These result indicate that there is still
much research to be done if statistical machine
translation systems are to capture the full range of
reordering phenomena present in translation.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2008.
Predicting success in machine translation. In Proceedings
of the Empirical Methods in Natural Language Process-
ing.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine trans-
lation research. In Proceedings of the European Chapter
of the Association for Computational Linguistics, Trento,
Italy.
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz,
Philip Resnik, and Michael Subotin. 2005. The Hiero
machine translation system: Extensions, evaluation, and
analysis. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical Methods
in Natural Language Processing, pages 779?786, Vancou-
ver, Canada.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 263?270,
Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics (to appear), 33(2).
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages 304?
311, Philadelphia, USA.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the
Human Language Technology and North American Asso-
ciation for Computational Linguistics Conference, pages
127?133, Edmonton, Canada. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Association for Computational Linguistics Companion
Demo and Poster Sessions, pages 177?180, Prague, Czech
Republic. Association for Computational Linguistics.
204
Philipp Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?
395, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, pages 44?52, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30(4):417?450.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Li-
bin Shen, David Smith, Katherine Eng, Viren Jain, Zhen
Jin, and Dragomir Radev. 2004. A smorgasbord of fea-
tures for statistical machine translation. In Proceedings of
Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing,
pages 161?168, Boston, USA. Association for Computa-
tional Linguistics.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the complex-
ity of translational equivalence. In Proceedings of the In-
ternational Conference on Computational Linguistics and
of the Association for Computational Linguistics, pages
977?984, Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative study
on reordering constraints in statistical machine translation.
In Proceedings of the Association for Computational Lin-
guistics, pages 144?151, Sapporo, Japan.
Andreas Zollmann, Ashish Venugopal, Franz Och, and Jay
Ponte. 2008. A systematic comparison of phrase-based,
hierarchical and syntax-augmented statistical mt. In Pro-
ceedings of International Conference On Computational
Linguistics.
205
Est imat ion of Stochast ic Attr ibute-Value Grammars using an 
Informative Sample 
Mi les  Osborne  
osborne~let . rug .n l  
R i jksun ivers i te i t  Gron ingen ,  The  Nether lands*  
Abst rac t  
We argue that some of the computational complexity 
associated with estimation of stochastic attribute- 
value grammars can be reduced by training upon an 
informative subset of the full training set. Results 
using the parsed Wall Street Journal corpus show 
that in some circumstances, it is possible to obtain 
better estimation results using an informative sam- 
ple than when training upon all the available ma- 
terial. Further experimentation demonstrates that 
with unlexicalised models, a Gaussian prior can re- 
duce overfitting. However, when models are lexi- 
ealised and contain overlapping features, overfitting 
does not seem to be a problem, and a Gmlssian prior 
makes minimal difference to performance. Our ap- 
proach is applicable for situal;ions when there are 
an infeasibly large mnnber of parses in the training 
set, or else for when recovery of these parses fl'om 
a packed representation is itself comi)utationally ex- 
pensive. 
1 I n t roduct ion  
Abney showed that attribute-value grammars can- 
not be modelled adequately using statistical tech- 
niques which assume that statistical dependencies 
are accidental (Ablmy, 1997). Instead of using a 
model class that assumed independence, Abney sug- 
gested using Random Fields Models (RFMs) tbr 
attribute-value grmnmars. RFMs deal with the 
graphical structure of a parse. Because they do not 
make independence assumptions about the stochas- 
tic generation process that might have produced 
some parse, they are able to model correctly depen- 
dencies that exist within parses. 
When estimating standardly-formulated RFMs, it 
is necessary to sum over all parses licensed by the 
grammar. For many broad coverage natural lan- 
guage grammars, this might involve summing over 
an exponential number of parses. This would make 
the task eomtmtationally intractable. Almey, fol- 
lowing the lead of Lafferty et al suggested a Monte 
* Current address: osborne@eogsei.ed.ae.uk, University of 
Edinburgh, Division of Informaties, 2 Bueeleuch Place, EII8 
9LW, Scotland. 
Carlo simulation as a way of reducing the computa- 
tional burden associated with RFM estimation (Laf- 
ferty et al, 1997). However, Johnson ct al consid- 
ered the form of sampling used in this sinmlation 
(Metropolis-Hastings) intractable (Johnson et M., 
1999). Instead, they proposed an Mternative strat- 
egy that redefined the estimation task. It was argued 
that this redefinition made estimation eomtmtation- 
Mly simple enough that a Monte Carlo simulation 
was unnecessary. They presented results obtained 
using a small unlexicalised model trained on a mod- 
est corlms. 
Unfortunately, Johnson et alassumed it was possi- 
ble to retrieve all parses licensed by a grmnmar when 
parsing a given training set. For us, this was not 
the case. In our experiments with a manually writ- 
ten broad coverage Definite Clause Grammar (DCG) 
(Briscoe and Carroll, 1996), we were only able to re- 
cover M1 parses for Wall Street .Journal sentences 
that were at most 13 tokens long within acceptable 
time and space bounds on comtmtation. When we 
used an incremental Minilnum Description Length 
(MDL) based learner to extend the coverage of our 
mmmally written gralnular (froul roughly 6()~ to 
around 90% of the parsed Wall Street .Jouriml), the 
situation became worse. Sentence ambiguity consid- 
erably increased. We were then only able to recover 
all parses for Wall Street Journal sentences that were 
at most 6 tokens long (Osborne, 1999). 
We can however, and usually in polynomial time, 
recover up to 30 parses for sentences up to 30 tokens 
long when we use a probabilistic unpacking mecha- 
nism (Carroll and Briscoe, 1992). (Longer sentences 
than 30 tokens can be parsed, but the nmnber of 
parses we can recover for them drops off rapidly). 1
However, 30 is far less tlmn the maximum number 
l"vVe made an attempt o determine the maximum num- 
ber of parses our grammar might assign to sentences. On 
a 450MIIz Ultra Spare 80 with 2 G'b of real memory, with 
a limit of at most 1000 parses per sentence, and allowing 
no more than 100 CPU seconds per sentence, we found that 
sentence ambiguity increased exponentially with respect to 
sentence l ngth. Sentences with 30 tokens had an estimated 
average of 866 parses (standard eviation 290.4). Without 
the limit of 1000 parses per sentence, it seems likely that this 
average would incrc, ase. 
586 
of parses per sentence o111' grammar mighl, assign to 
Wall Stl"ec't Journal sent;enees. Any training set we 
have a(:eess to will therefore be l|eeessarily limite(l 
in size. 
We therefore need an estimation strategy that 
takes seriously the issue of extracting the 1)esl, per- 
refinance fl 'om a limited size training Met. A limited 
size tra.ining sol; means one ereate(l )y retrieving at 
most n t)arses per Ment(mee. Although we (:annot re- 
cover all t)ossil)le i )arses~ we (lo }lave a choice as to 
which llarses estimation should 1)e based Ul)On. 
Our ai)proach to the prol)lem of making I{FM es- 
timation feasible, ibr our highly amt)iguous I)CG is 
to seek ol\]|; an ivformativc samt)le and train ui)on 
that. We (lo not redefine the estimation task in a 
non-s l ;a l~t la rd  w;~y, 1101' (lo we llSe a ~{o\] l te Car lo  s i ln -  
ulation. 
We (:all a salul)lc informative if it 1)oth leads to 
the select;ion of a 111ollol that does not mldertit or 
overfit, and also is typical of t'utm'e samples, l)esl)itc 
() l ie's intuitions, an infornmtive saml)le might be a 
prol)er subset of the fifll training set. This means 
that estinlation using the int'ornmtiv(; sample might 
yield 1)etter esults than estimation using all of the 
l;rainhlg Met;. 
The ):(;st of this 1)aper is as tbllows, l,'irstly we 
introduce RFMs. Then we show how they nlay be 
esl;imated and how an infbrmative saml)le might 1)e 
identified. Nexl;, we give details of the, a(;tribute- 
vahle gramnlar we use, all(t show \]lOW we ~o at)ot l t  
modelling it. We then i)resent two sets of experi- 
mel)ts. The first set is small scale, and art! de.signed 
to show the existent;e of ;m inti)rmative sample. The 
second ski of CXl)erilll(;llI, S al 'e larger in scale, an(1 
build upon the COml)utational savil|gS we are al)le 
to achieve using a probabilistic Unl)acking strategy. 
They show how large me(Ms (two orders of magni- 
tude larger than those reported by Johnson ctal) 
can 1)e estimated using the l)arsed Wall Street .lour- 
hal eort)us. Overlitting is shown to take place. They 
also show how this overfitting can be (partially) re- 
duced by using a Gaussian prior. Finally, we end 
with  SOllle COllllllelltS Oil Ollr WOl.k. 
2 Random F ie ld  Mode ls  
Here we show how attribute-wflue grammars may be 
modelle(1 using RFMs. Although our commentary is 
in terms of RFMs and grammars, it should t)e ol)- 
vious that RFM technology can be applied to other 
estimation see.narios. 
Let G be an attribute-value grammar, D the set 
of sentences within the string-set defined lly L(G) 
and ~ the union of the set of parses assigne(1 to 
each sentence in D by the gramnmr G. A Random 
Field Model, M, cons is t  of  two  cora l )o r ients :  a se t  o f  
features, F and a set; of 'wei.qhts, A. 
l?eatures are the basle building blocks of RFMs. 
They enable the system designer to spccit)~ {;lie key 
asl)ects of what it; takes to ditferentiate one 1)arse 
from a11other parse. Each feature is a t'lmetion from 
a 1)arse to an integer. Her(.', the integer value as- 
sociated with a feature is interpreted as the nmn- 
ber of times a feature 'matches' (is 'active') with 
a parse. Note features hould not be confllsed with 
features as found in feature-value t)undles (these will 
be called atl;ril)utes instead). \]Peatures are usually 
manually selected by the sysl;ein designer. 
The other component of a RFM, A, is a set of 
weights, hffornmlly, weights tell its how ti;atures are 
to be used when nlodellillg parses. For exanlple, an 
active feature with a large weight might indicate that 
some parse had a higtl prolmlfility. Each weight Ai is 
associated with a thatm'e fi. Weights arc' real-valued 
nmnl)ers an(l ~:H'O autonmtically deternfined 113: an es- 
timation process (for example using hnproved Itera- 
tire Scaling (LaflL'rty et al, 1997)). One of the nice 
l)roI)erties of Rl.i'Ms is that 1111o likelihood fiuw?ion 
of a RFM is strictly concave. This means 1;hat here 
~/t"e. 11o h)cal lllillillla~ and so  wc can  be, l)e sure  that 
sealing will result in estinmtion of a 11.1,'54 that is 
glol)ally ot)timal. 
The (unnormalised) total weight of a i)arse :c, 
'(J(:r), is a flulction of the. k feaLures that are 'active' 
on a 1)arse: 
k 
.,/,(.;) = (.)) (1) 
i=l 
The prol)ability of a parse, P(x I M), is simply 
the result of norm~dising the total weight associated 
with that parse: 
J'(:,, IM)  = 12) 
z -- (a) 
yGf~ 
The inl;erpretation of this I)robability depends upon 
the apt)lication of tile RFM. Here, we use parse prol)- 
abilities to rettect preferences for parses. 
When using RFMs for parse selection, we sin> 
ply select the parse that ma.ximises ~/;(:1:). In these 
circumstances, there is 11o need to nornlalise (com- 
pute Z). Also, when comtmting ,/~(:c) for comi)eting 
parses, there is no built-in bias towards shorter (or 
longer) derivations, and so no need to normalise with 
respect o deriw~tion length/  
2The reason there is no need to normalisc with respect to 
derivation length is that features can have positive o1" nega- 
tive weights. The weight of a parse will ttlcrcforc not always 
monotonical ly increase with respect to the re,tuber of active 
ti~atm'cs. 
587 
3 RFM Est imat ion  and  Se lec t ion  o f  
the  In fo rmat ive  Sample  
We now sketch how RFMs may be estimated and 
then outline how we seek out an informa.tive smnple. 
We use hnproved Iterative Scaling (IIS) to esti- 
mate RFMs. In outline, the IIS algorithm is as fol- 
lows: 
1. Start with a reference distribution H,, a set of 
features F and a set of weights A. Let M be 
the RFM defined using F and A. 
2. Initialise all weights to zero. This makes tile 
initial model uniform. 
3. Compute the expectation of each feature w.r.t 
R. 
4. For each feature fi 
(a) Find a weight ~; that equates the expecta- 
tion of fi w.r.t/?, and the expectation of fi 
w.r.t M. 
(b) Ileplace the old value of ki with 21. 
5. If the model has converged to/?, output M. 
6. Otherwise, go to step 4 
Tile key step here is 4a, computing the expectations 
? of features w.r.t the RFM. This involves calculating 
the probability of a parse, which, as we saw fronl 
equation 2, requires a summation over all parses in 
ft. 
We seek out an informative sample ~l (fh C ~) 
as follows: 
I. Pick out from ~ a sample of size n. 
2. Estimate a model using that smnple and evalu- 
ate it. 
3. If the model just estimated shows signs of over- 
fitting (with respect o an unseen held-out data 
set), halt and output the inodel. 
4. Otherwise, increase n and go back to step 1. 
Our approach is motivated by tile following (par- 
tially related) observations: 
? Because we use a non-Imrmnetric model class 
and select an instance of it in terlns of some 
sample (section 5 gives details), a stochastic 
complexity argument tells us that an overly sim- 
ple model (resulting from a small sample) is 
likely to underfit. Likewise, an overly complex 
model (resulting from a large sample) is likely 
to overfit. An informative samI)le will therefore 
relate to a model that does not under or overfit. 
? On average, an informative sample will be %yp- 
ical' of future samples. For many reaMife situ- 
ations, this set is likely to be small relative to 
the size of the full training set. 
We incorporate the first observation through our 
search mechanism. Because we start with small sam- 
pies and gradually increase their size, we remain 
within the donmin of etliciently recoverable samples. 
The second observation is (largely) incorporated 
in the way we pick samples. The experimental sec- 
tion of this paper goes into the relevant details. 
Note our approach is heuristic: we cmmot afford 
to evahmte all 21~1 possible training sets. The actual 
size of the informative sample fit will depend both 
tile Ill)On the model class used and the maximum 
sentence length we can de~,l with. We would ex- 
pect: richer, lexicalised models to exhibit overfitting 
with slnaller samples than would be the case with 
unlexicalised models. We would expect he size of 
an informative sample to increase as the maxilnum 
sentence length increased. 
There are similarities between our approach and 
with estimation using MDL (Rissanen, 1989). How- 
ever, our implementation does not explicitly attempt 
to minimise code lengths. Also, there are similari- 
ties with importance sampling approaches to RFM 
estimation (such as (Chen and ll,osenfeld, 1999a)). 
However, such attempts do not miifinfise under or 
overfitting. 
4 The Grammar  
The grammar we model with I/andom Fields, (called 
the Ta 9 Sequence Grammar (Briseoe and Carroll, 
1996), or TSG for short) was developed with regard 
to coverage, and when compiled consists of 455 Def- 
inite Clause Grammar (DCG) rules. It does not 
parse sequences of words directly, but instead as- 
signs derivations to sequences of part-of-speech tags 
(using the CLAWS2 tagset. The grammar is rela- 
tively shallow, (for exmnple, it does not fltlly anal- 
yse unbounded ependencies) but it does make an 
attelnpt o deal with coilunou constructions, uch as 
dates or names, commonly found in corpora, but of 
little, theoretical interest. Furthermore, it integrates 
into the syntax a text gramma.r, grouping utterances 
into units that reduce the overall ambiguity. 
5 Mode l l ing  the  Grammar  
Modelling the TSG with respect o the parsed Wall 
Street .\]ournal consists of two steps: creation of a 
feature set and definition of the reference distribu- 
tion. 
Our feature set is created by parsing sentences in 
the training set (~br), and using earl, parse to ill- 
stantiate templates. Each template defines a family 
of features. At present, the templates we use are 
somewhat ad-hoc. However, they are motivated by 
the observations that linguistically-stipulated units 
(DCG rules) are informative, trod that ninny DCG 
apl)lications in preferred parses can be predicted us- 
ing lexical information. 
588 
AP/al:unimlie{h.'d 
I 
A 1/aI) p i :unimpeded 
unimI)eded PP/t) I :by 
\[ 
P1/ iml  :by 
by N1/n:trafli{: 
I 
trafl\]{: 
Figure 1: TSG Parse Fragumnt 
The first template creates features that count 
{;lie numl)er of tinms a. DUG instantiationis i)resent 
within ,2 1}arse. a For examt}le , Sul)p{}s{~ we 1)arse{t 
the Wall Street Journal AP: 
1 unimpeded 1}y t:ra{lic 
A parse tree generated by TSG nfight lie as shown 
in figure 1. Here, to s:~ve on Sl}ace, wc have labdled 
each interior node in the parse tree with TSG rule 
names, and not attribut(;-valu(~ bun(lies. Further- 
more, we have mmota.t('xl each node with the lmad 
w(}rd of tim l}hrase in question. Within ollr gl;aill- 
mar, heads arc (usually) ext)lMtly marke{t. This 
1110;/,118 W(~ do  l\]ot \]l;~v{~ to Ill&k(~ ;lily g l lossos w}lcll 
identit\[ying the head of a. local tree. With head in- 
foi'mtd;io\]b we are alo/e to lexicalise models. \Ve haa;e 
suppressed taggillg information. 
For {'.xamp\]e, a \]hature (h'Jin(;d using this t(;nlplat{; 
might (:O1111t tho, nu inber  ()f t imes  th(! we saw:  
AP/at  
I 
A1/at/1)1 
111 a 1)arse. Such features r(~coi'd sore( 2(if the context 
of the rule a.t}p\]i(:ation, i  that rule al}t}Iication8 that 
differ ii1 terms of how attributes are bound will 1}e 
modelled by (litlhrent hatures. 
Our se{'ond total}late creates features that al'{'~ par- 
tially lexicalised. I~br each lo{:al tree (of depth one) 
that has a \]?P daughter, we create a feature that 
counts the lmmber of times that h)cal tree, de(:orated 
with the head-woM of the I ' l ' ,  was seen in a. parse. 
An cxmnple of such ;1 lexicMised feature would 1}e: 
A1/apt}l 
I 
PI)/til:l)y 
3Note, all (}111" fo.al;/ll'es Slll)i)r(?ss ;tlly t{!l'nlillals thgtl, al)i}em' 
in a h}caI 1,Fe(!. Lexical informaI;ioll is in{:luded when we decide 
to lexicalise features. 
These featm'cs are designed to model PP attach- 
ments that can be resolved using the head of the 
PP. 
The thh'd mid tinM template creates featuros that 
are again partiMly lexicalised. This time, we create 
local trees of det}th one that are, decorated with the 
head word. For example, here is one such feature: 
AP/al :mfimpeded 
I 
A1/appl  
Note the second and third templates result in fea- 
tures that overlap with features resulting fl'om at)- 
i}\]icati(ms of the first template. 
We create the reference distribution 1~ (an associ- 
ation of t)r{}l}al)i\]ities with TSG parses of sentences, 
such that the t}robabilities reflect 1}a.rse i)references) 
using the following process: 
1. Extra{;t some sami}le f~T (using the al)l)roach 
mentioned in sc(:tion 3). 
2. For each sentence in tim sample, for each l)arse 
of that sent;encc', {:Olnl)ute the '(lista.ncc' be- 
tween the TSG 1}mse and the WSJ refereuce 
parse. \]1\] our at)t)roach, dista.nce is cM{:lfla.tc(1 
in tcl7111s o f  a weighted Slltll o f  crossing rates, re- 
call and 1}recision. Mininlising it maximises our 
definition of parse plausibility. 4 However, there 
is nothing inherently crucial about this decision. 
Auy othc'r objective flmction (thaJ; can l)c ret)- 
r(~sent('.(l as an CXl}Oncntial distribution) couh\] 
1)e used instead. 
3. Normalise the distan('es, uch that for some ,sen- 
tcn(:e, tim sum of tim distances of all rt~cov- 
O,l.'od ~\[?SG t)al"S(~S \]\['(/1" that  soii|;(!ilCO, is a COllSt?tilt 
a.cross all sento.nces. Nornmlising in this man- 
ner ensures that each sentence is cquil)robal}le 
0"emcmber that \]{FM probabilities are in terms 
of I}a.rse lir{'.fl~r{'.nces, and not probability of oc- 
{:llrr{HIee ill 8{}111{~ (;orl)llS). 
4. Map the norinalised distances into 1}robabili- 
ties. If d(p) is the normalised {listance of TSG 
l/;}~l"Se p, then associate with parse 1) the refer- 
(race probability given by the maximum likeli- 
hood estimator: 
rl(1,) (4) 
Our approach therefore gives t}artial cl'e(lit (a 11oil- 
zero reference l)robability) to a.ll parses in ~z. /2, is 
thcreibr(; not as discontimlous as the equivalent dis- 
trit)ution used by Johnson at al. We therefl)re do not 
need to use simulated annea.ling o1' other numerically 
intensive techniques to cstiinate models. 
4Ore' distanc(~ mo.l;ric is the same one used I}y llekto{m 
(ltektoen, 19.97) 
589 
6 Exper iments  
Here we present wo sets of experiments. The first 
set demonstrate he existence of an informative sam- 
ple. It also shows some of the characteristics of three 
smnpling strategies. The second set of experiments 
is larger in scale, and show RFMs (both lexicalised 
and unlexicalised) estimated using sentences up to 
30 tokens long. Also, the effects of a Gaussian prior 
are demonstrated asa way of (partially) dealing with 
overfitting. 
6.1 Test ing the Var ious Sampl ing  
Strategies 
In order to see how various sizes of sample related to 
estimation accuracy and whether we could achieve 
similar levels of performm~ce without recovering all 
possible parses, we ran the following experiments. 
We used a model consisting of features that were 
defined using all three templates. We also threw 
away all features that occurred less than two times in 
the training set. We randomly split; the Wall Street 
Journal into disjoint training, held-out and testing 
sets. All sentences in the training and held-out sets 
were at most 14 tokens long. Sentences ill the test- 
tug set, were at most 30 tokens long. There were 
6626 sentences in the training set, 98 sentences in 
the held-out set and 441 sentences in tile testing set. 
Sentences in the held-out set had on average 12.6 
parses, whilst sentences in the testing-set had on av- 
erage 60.6 parses per sentence. 
The held-out set was used to decide which model 
performed best. Actual performmme of the models 
should be judged with rest)ect o the testing set. 
Evaluation was in terIns of exact match: tbr each 
sentence in the test set, we awarded ourselves a 
t)oint if the RFM ranked highest he same parse that 
was ranked highest using the reference probabilities. 
When evahmting with respect to the held-out set, 
we recovered all parses for sentences in the held-out 
set. When evaluating with respect o the testing-set, 
we recovered at most 100 parses per sentence. 
For each run, we ran IIS for the same number 
of iterations (20). In each case, we evaluated the 
RFM after each other iteration and recorded the best 
classification pertbrmance. This step was designed 
to avoid overfitting distorting our results. 
Figure 2 shows the results we obtained with pos- 
sible ways of picking 'typical' samples. The first 
column shows the maxinmm number of parses per 
sentences that we retrieved in each sample. 
The second column shows the size of the sample 
(in parses). 
The other cohmms give classification accuracy re- 
sults (a percentage) with respect o the testing set. 
In parentheses, we give performance with respect; to 
the held-out set. 
The column marked Rand shows the performance 
Max parses Size 
1 6626 
2 12331 
3 17026 
5 24878 
10 39581 
100 119694 
1000 246686 
oo 267400 
Rand SCFG Ref 
25.2 (51.7) 23.3 (59.0) 23.4 (50.0) 
37.9 (63.0) 40.4 (60.3) 40.4 (60.0) 
43.2 (65.5) 43.7 (63.8) 43.7 (63.8) 
43.7 (70.2) 45.8 (69.5) 45.8 (69.5) 
47.4 (72.0) 47.0 (70.0) 46.9 (70.0) 
45.0 (68.7) 45.0 (68.0) 45.0 (68.0) 
44.4 (67.4) 43.0 (67.0) 43.0 (67.0) 
43.0 (66.0) 43.0 (66.0) 43.0 (66.0) 
Figure 2: Results with various sampling strategies 
of runs that used a sample that contained parses 
which were randomly and uniformly selected out of 
the set, of all possible parses. The classification ac- 
curacy results for this sampler are averaged over 10 
runs.  
The column marked SCFG shows the results ob- 
tained when using a salnple that contained 1)arses 
that were retrieved using the probabilistic unI)acking 
strategy. This did not involve retrieving all possible 
parses for each sentence in the training set,. Since 
there is no random component, he results arc fl'om a 
single run. Here, parses were ranked using a stochas- 
tic context free backbone approximation ofTSG. Pa- 
rameters were estimated using simple counting. 
FinMly, the eohunn marked Ref shows the re- 
sults ol)tained when USillg a sample that contained 
the overall n-best parses per sentence, as defined in 
terms of the reference distril)ution. 
As a baseline, a nlodel containing randomly as- 
signed weights produced a classification accuracy of 
45% on the held-out sentences. These results were 
averaged over 10 runs. 
As can be seen, increasing the sainple size pro- 
duces better results (for ca& smnl)ling strategy). 
Around a smnple size of 40k parses, overfitting starts 
to manifest, and perIbrmance bottoms-out. One of 
these is therefore our inforinative sample. Note that 
the best smnple (40k parses) is less than 20% of the 
total possible training set. 
The ditference between the various samplers is 
marginal, with a slight preference for Rand. How- 
ever the fact that SUFG sampling seems to do ahnost 
as well as Rand sampling, and fllrthermore does not 
require unpacking all parses, makes it the sampling 
strategy of choice. 
SCFG sampling is biased in the sense that the 
sample produced using it will tend to concentrate 
around those parses that are all close to the best, 
parses. Rand smnpling is unbiased, and, apart 
h'om the practical problems of having to recover all 
parses, nfight in some circumstances be better than 
SCFG sampling. At the time of writing this paper, 
it was unclear whether we could combine SCFG with 
Rand sampling -sample parses from the flfll distribu- 
590 
lion without unpacking all parses. We suspect hat 
for i)robabilistic unt)acking to be efficient, it nmst 
\]:ely upon some non-uniform distribution. Unpack- 
ing randomly and uniformly would probably result 
in a large loss in computational e iiciency. 
6.2 Larger  Scale Eva luat ion  
Here we show results using a larger salnl)le and test- 
ing set. We also show the effects of lexicalisation, 
overtitting, and overfitting avoidance using a Gaus- 
sian prior. Strictly speaking this section could have 
been omitted fl'om the paper. However, if one views 
estimation using an informative sami)le as overfit- 
ling avoi(lance, then  estimation using a Gaussian 
l)rior Call be seen as another, complementary take 
on the problem. 
The experimental setup was as follows. We rall- 
domly split the Wall St, reel: Journal corpus into a 
training set and a testing set. Both sets contained 
sentence.s t;hat were at most 30 tokens hmg. When 
creating the set of parses used to estimate Ii.FMs, we 
used the SCFG approach, and retained the top 25 
parses per sentence. Within the training set (arising 
Dora 16, 200 sentences), there were 405,020 parses. 
The testing set consisted of 466 sentences, with an 
average of 60.6 parses per sentence. 
When evahmtillg, we retrieved at lllOSt 100 lmrscs 
per sentence in the tes t ing  set and  scored them using 
our reference distribution. As lmfore, we awarded 
ourselves a i)oinl; if the most probable testing parse 
(in terms of the I/.MF) coincided with the most t)rol)- 
able parse (in terms of the reference distribution). In 
all eases, we ran IIS tbr 100 iterations. 
For the tirst experiment, we used just the first 
telnp\]at('. (features that rc'la.t(;d to DC(I insl;antia- 
tions) to create model l; the second experiment uso.d 
the first and second teml)lat(~s (additional t'eatm'o.s 
relating to PP attachment) o create model 2. The 
linal experiment used all three templat('~s (additional 
fea,tl lres that were head-lexicalised) to create model 
3. 
The three mo(lels contained 39,230, 65,568 and 
278, 127 featm:es respectively, 
As a baseline, a model containing randomly as- 
signed weights achieved a 22% classification accu- 
racy. These results were averaged over 10 runs. Fig- 
ure 3 shows the classification accuracy using models 
1, 2 and 3. 
As can 1)e seen, the larger scale exl)erimental 
results were better than those achieved using the 
smaller samples (mentioned in section 6.1). The rea- 
Sell for this was because we used longer sentc,11ces. 
The. informative sainple derivable Kern such a train- 
ing set was likely to be larger (more representative of 
54 
52 
5O 
o>, 
~, 48 
o 
< 
46 
44 
42 
I I i . I ,  . A . .  I i . . .  I . .  
~'\ . , / ' , - - -  " ' .  . . '" . . . .  ..model1 . . . . . . . .  
" f f  ~ " ~  ~L model2 . . . . . .  
/ . . . . , '  ~ model3 . . . .  _ 
,, ,,,,- . . . . . . . . .  . ~_ \ / . .~  \ 
10 20 30 40 50 60 70 80 90 100 
Iterations 
Figure 3: Classification Accuracy tbr Three Models 
Estinmted using Basic IIS 
56 - - ~  r r \] l 1 l 1 7 
54 
52 
50 
o~ 
~: 4a 
46 
44 
42 
model1 -- - -  
model2 ..... 
0 10 20 30 40 50 60 70 80 90 1 O0 
Iterations 
Figure .l: Classification Accuracy for .\[hre(. Models 
Estinmted using a Gmlssian Prior and IIS 
the population) than the informative sample deriv- 
al)led from a training set using shorter, less syntat'- 
tically (Xmll)lex senten(:es. With the unle.xicalised 
model, we see (:lear signs of overfitting. Model 2 
overfits even more so. For reasons that are unclear, 
we see that the larger model 3 does not ai)pem: to 
exhibit overtitting. 
We next used the Gaussian Prior method of 
Chen and Rosenfeld to reduce overfitting (Chen 
and Rosenfeld, 1999b). This involved integrating 
a Gaussian prior (with a zero mean) into Ills and 
searching for the model that maximised the, prod- 
uct of the likelihood and prior prolmbilities. For the 
experiments reported here, we used a single wlri- 
ante over the entire model (better results might be 
achievable if multiple variances were used, i)erhaps 
with one variance per telnl)late type). The aetllal 
value of the variance was t'cmnd by trial-and-error. 
Itowever, optimisation using a held-out set is easy 
to achieve,. 
591 
We repeated the large-scale xperiment, but this 
time using a Gaussian prior. Figure 4 shows the 
classification accuracy of the models when using a 
Gmlssian Prior. 
When we used a Gaussian prior, we fotmd that all 
models showed signs of imt)rovenmnt (allbeit with 
varying degrees): performance ither increased, or 
else did not decrease with respect to the munber 
of iterations, Still, model 2 continued to underper- 
form. Model 3 seemed most resistent o the prior. 
It theretbre appears that a Gaussian prior is most 
useful for unlexicalised models, and that for mod- 
els built from complex, overlapping features, other 
forms of smoothing must be used instead. 
7 Comments 
We argued that RFM estimation tbr broad-coverage 
attribute-valued grammars could be made eompu- 
tationally tractable by training upon an inforlna- 
tive sample. Our small-scale xperiments suggested 
that using those parses that could be etliciently un- 
packed (SCFG sampling) was ahnost as effective as 
sampling from all possible parses (R~and salnplillg). 
Also, we saw that models should not be both built 
and also estimated using all possible parses. Better 
results can be obtained when models m'e built and 
trained using an intbrmative san@e. 
Given the relationshi I) between sample size and 
model complexity, we see that when there is a dan- 
ger of overfitting, one should build models on the ba- 
sis of all informative set. Itowever, this leaves open 
the possil)ility of training such a model upon a su- 
1)erset of the, informative set;. Although we ha.re not 
tested this scenario, we believe that this would lead 
to t)etter esults ttlan those achieved here. 
The larger scale experiments showed that I{FMs 
can be estimated using relatively long sentences. 
They also showed that a simple Gaussian prior could 
reduce the etfects of overfitting. However, they also 
showed that excessive overfitting probably required 
an alternative smoothing approach. 
The smaller and larger experiments can be both 
viewed as (complementary) ways of dealing with 
overfitting. We conjecture that of the two ap- 
proaches, the informative smnple al)proach is prefer- 
able as it deals with overfitting directly: overfitting 
results fi'om fitting to complex a model with too lit- 
tle data. 
Our ongoing research will concentrate upon 
stronger ways of dealing with overfitting in lexi- 
calised RFMs. One line we are pursuing is to com- 
bine a compression-based prior with an exponential 
model. This blends MDL with Maximum Entropy. 
We are also looking at alternative template sets. 
For example, we would probably benefit fi'om using 
templates that capture more of the syntactic ontext 
of a rule instantiation. 
Acknowledgments 
We would like to tliank Rob Malouf, Domfla Nie 
Gearailt and tim anonymous reviewers for com- 
ments. This work was supported by tile TMR 
Project Lcar'nin9 Computational Grammars. 
References  
St, even P. Atmey. 1997. Stochastic Attribute- 
Value Grmmnm:s. Computational Linguistics, 
23(4):597- 618, December. 
Miles Osborne 19?9. DCG induction using MDL 
and Parsed Corpora. In James Cussens, editor, 
Lcarnin9 Langua9 c in Logic, pages 63-71, Bled, 
Slovenia, June. 
Ted Briscoe and John Carroll. 1.996. Autolnatic 
Extraction of Subcategorization from Corpora. 
In Proceedings of the 5 th Conference on Applied 
NLP, p~ges 356-363, Washington, DC. 
John Carroll and Ted Briscoe. 1992. Probabilis- 
tic Normalisation and Unpacking of Paclmd Parse 
Forests for Unification-lmse, d Grmnmars. Ill Pro- 
cccdi,n9 s of the AAAI  Fall Symposi'u,m on P~vb- 
abilistic AppTvach, es to Natural Language , pages 
33-38, Cambridge, MA. 
Stanley Chen and Honald l{osenfeld. 1999a. Effi- 
cient Sampling and Feature Selection in Whole 
Sentence Maxinmin Entrol)y Language Models. In 
ICA SSP '99. 
Stanley F. Chen and Ronald Rosenfeld. 1999b. 
A Gaussian Prior for Smoothing Maxinmm \]211- 
tropy Models. Technical Rel)ort CMU-CS-99-108, 
Carnegie Mellon University. 
Eirik Hektoen. 1997. Probabilistic Parse Select;ion 
Based on Semantic Cooet:l lr l 'el lees. \]ill Pl'og('.cd- 
ings oJ" th, e 5th l'ntc, r'national Wo~wkh, op on Parsing 
Tcch, nolo.qics, Cambridge, Massach'usctts, 1)ages 
113 122. 
Marl< Johnson, Stuart Geman, Stephen Cannon, 
Zhiyi Chi, and Stephan Riezler. 1999. Esl, inmtors 
for Stochastic "Unification-based" (~rammars. In 
37 th Annual Meeting of the ACL, 
J. Latferty, S. Della Pietra, and V. Della Pietra. 
1997. Inducing Features of Random Fields. 1EEE 
Transactions on Pattern Analysis and Mach, inc 
Intclligcncc, 19(4):380 393, April. 
Jorma Rissanen. 1989. Stochastic Complezity in 
Statistical i'nquiry. Series in Computer Science -
Volmne 15. World Scientific. 
592 
Learning Computational Grammars
John Nerbonne   , Anja Belz  , Nicola Cancedda  , Herve? De?jean  ,
James Hammerton

, Rob Koeling

, Stasinos Konstantopoulos   ,
Miles Osborne   , Franck Thollard

and Erik Tjong Kim Sang 
Abstract
This paper reports on the LEARNING
COMPUTATIONAL GRAMMARS (LCG)
project, a postdoc network devoted to
studying the application of machine
learning techniques to grammars suit-
able for computational use. We were in-
terested in a more systematic survey to
understand the relevance of many fac-
tors to the success of learning, esp. the
availability of annotated data, the kind
of dependencies in the data, and the
availability of knowledge bases (gram-
mars). We focused on syntax, esp. noun
phrase (NP) syntax.
1 Introduction
This paper reports on the still preliminary, but al-
ready satisfying results of the LEARNING COM-
PUTATIONAL GRAMMARS (LCG) project, a post-
doc network devoted to studying the application
of machine learning techniques to grammars suit-
able for computational use. The member insti-
tutes are listed with the authors and also included
ISSCO at the University of Geneva. We were im-
pressed by early experiments applying learning
to natural language, but dissatisfied with the con-
centration on a few techniques from the very rich
area of machine learning. We were interested in

University of Groningen,  nerbonne,konstant  @let.
rug.nl, osborne@cogsci.ed.ac.uk
	
SRI Cambridge, anja.belz@cam.sri.com, Rob.Koe-
ling@netdecisions.co.uk

XRCE Grenoble, nicola.cancedda@xrce.xerox.com

University of Tu?bingen, Herve.Dejean@xrce.xerox.
com, thollard@sfs.nphil.uni-tuebingen.de

University College Dublin, james.hammerton@ucd.ie

University of Antwerp, erikt@uia.ua.ac.be
a more systematic survey to understand the rele-
vance of many factors to the success of learning,
esp. the availability of annotated data, the kind
of dependencies in the data, and the availability
of knowledge bases (grammars). We focused on
syntax, esp. noun phrase (NP) syntax from the
beginning. The industrial partner, Xerox, focused
on more immediate applications (Cancedda and
Samuelsson, 2000).
The network was focused not only by its sci-
entific goal, the application and evaluation of
machine-learning techniques as used to learn nat-
ural language syntax, and by the subarea of syn-
tax chosen, NP syntax, but also by the use of
shared training and test material, in this case ma-
terial drawn from the Penn Treebank. Finally, we
were curious about the possibility of combining
different techniques, including those from statisti-
cal and symbolic machine learning. The network
members played an important role in the organi-
sation of three open workshops in which several
external groups participated, sharing data and test
materials.
2 Method
This section starts with a description of the three
tasks that we have worked on in the framework of
this project. After this we will describe the ma-
chine learning algorithms applied to this data and
conclude with some notes about combining dif-
ferent system results.
2.1 Task descriptions
In the framework of this project, we have worked
on the following three tasks:
1. base phrase (chunk) identification
2. base noun phrase recognition
3. finding arbitrary noun phrases
Text chunks are non-overlapping phrases which
contain syntactically related words. For example,
the sentence:
 
He 
 
reckons 
 
the current
account deficit 
 
will narrow 
 
to 
 
only  1.8 billion 
 
in 
 
September  .
contains eight chunks, four NP chunks, two VP
chunks and two PP chunks. The latter only con-
tain prepositions rather than prepositions plus the
noun phrase material because that has already
been included in NP chunks. The process of
finding these phrases is called CHUNKING. The
project provided a data set for this task at the
CoNLL-2000 workshop (Tjong Kim Sang and
Buchholz, 2000)1. It consists of sections 15-18 of
the Wall Street Journal part of the Penn Treebank
II (Marcus et al, 1993) as training data (211727
tokens) and section 20 as test data (47377 tokens).
A specialised version of the chunking task is
NP CHUNKING or baseNP identification in which
the goal is to identify the base noun phrases. The
first work on this topic was done back in the
eighties (Church, 1988). The data set that has
become standard for evaluation machine learn-
ing approaches is the one first used by Ramshaw
and Marcus (1995). It consists of the same train-
ing and test data segments of the Penn Treebank
as the chunking task (respectively sections 15-18
and section 20). However, since the data sets
have been generated with different software, the
NP boundaries in the NP chunking data sets are
slightly different from the NP boundaries in the
general chunking data.
Noun phrases are not restricted to the base lev-
els of parse trees. For example, in the sentence In
early trading in Hong Kong Monday , gold was
quoted at $ 366.50 an ounce ., the noun phrase
  $ 366.50 an ounce  contains two embedded
noun phrases
  $ 366.50  and   an ounce  .
In the NP BRACKETING task, the goal is to find
all noun phrases in a sentence. Data sets for this
task were defined for CoNLL-992. The data con-
sist of the same segments of the Penn Treebank as
1Detailed information about chunking, the CoNLL-
2000 shared task, is also available at http://lcg-
www.uia.ac.be/conll2000/chunking/
2Information about NP bracketing can be found at
http://lcg-www.uia.ac.be/conll99/npb/
the previous two tasks (sections 15-18) as train-
ing material and section 20 as test material. This
material was extracted directly from the Treebank
and therefore the NP boundaries at base levels are
different from those in the previous two tasks.
In the evaluation of all three tasks, the accu-
racy of the learners is measured with three rates.
We compare the constituents postulated by the
learners with those marked as correct by experts
(gold standard). First, the percentage of detected
constituents that are correct (precision). Second,
the percentage of correct constituents that are de-
tected (recall). And third, a combination of pre-
cision and recall, the F ffHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 181?189,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Streaming First Story Detection with application to Twitter
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Victor Lavrenko
School of Informatics
University of Edinburgh
vlavrenk@inf.ed.ac.uk
Abstract
With the recent rise in popularity and size of
social media, there is a growing need for sys-
tems that can extract useful information from
this amount of data. We address the prob-
lem of detecting new events from a stream of
Twitter posts. To make event detection feasi-
ble on web-scale corpora, we present an algo-
rithm based on locality-sensitive hashing which
is able overcome the limitations of traditional
approaches, while maintaining competitive re-
sults. In particular, a comparison with a state-
of-the-art system on the first story detection
task shows that we achieve over an order of
magnitude speedup in processing time, while
retaining comparable performance. Event de-
tection experiments on a collection of 160 mil-
lion Twitter posts show that celebrity deaths
are the fastest spreading news on Twitter.
1 Introduction
In the recent years, the microblogging service Twit-
ter has become a very popular tool for express-
ing opinions, broadcasting news, and simply com-
municating with friends. People often comment on
events in real time, with several hundred micro-blogs
(tweets) posted each second for significant events.
Twitter is not only interesting because of this real-
time response, but also because it is sometimes ahead
of newswire. For example, during the protests fol-
lowing Iranian presidential elections in 2009, Iranian
people first posted news on Twitter, where they were
later picked up by major broadcasting corporations.
Another example was the swine flu outbreak when
the US Centre for disease control (CDC) used Twit-
ter to post latest updates on the pandemic. In ad-
dition to this, subjective opinion expressed in posts
is also an important feature that sets Twitter apart
from traditional newswire.
New event detection, also known as first story de-
tection (FSD)1 is defined within the topic detection
and tracking as one of the subtasks (Allan, 2002).
Given a sequence of stories, the goal of FSD is to
identify the first story to discuss a particular event.
In this context, an event is taken to be something
that happens at some specific time and place, e.g.,
an earthquake striking the town of L?Aquila in Italy
on April 6th 2009. Detecting new events from tweets
carries additional problems and benefits compared
to traditional new event detection from newswire.
Problems include a much higher volume of data to
deal with and also a higher level of noise. A major
benefit of doing new event detection from tweets is
the added social component ? we can understand the
impact an event had and how people reacted to it.
The speed and volume at which data is coming
from Twitter warrants the use of streaming algo-
rithms to make first story detection feasible. In
the streaming model of computation (Muthukrish-
nan, 2005), items (tweets in our case) arrive contin-
uously in a chronological order, and we have to pro-
cess each new one in bounded space and time. Recent
examples of problems set in the streaming model in-
clude stream-based machine translation (Levenberg
and Osborne, 2009), approximating kernel matrices
of data streams (Shi et al, 2009), and topic mod-
elling on streaming document collections (Yao et al,
2009). The traditional approach to FSD, where each
new story is compared to all, or a constantly grow-
ing subset, of previously seen stories, does not scale
to the Twitter streaming setting. We present a FSD
system that works in the streaming model and takes
constant time to process each new document, while
also using constant space. Constant processing time
is achieved by employing locality sensitive hashing
(LSH) (Indyk and Motwani, 1998), a randomized
technique that dramatically reduces the time needed
1We will be using the terms first story detection and new
event detection interchangeably.
181
to find a nearest neighbor in vector space, and the
space saving is achieved by keeping the amount of
stories in memory constant.
We find that simply applying pure LSH in a FSD
task yields poor performance and a high variance in
results, and so introduce a modification which vir-
tually eliminates variance and significantly improves
performance. We show that our FSD system gives
comparable results as a state-of-the-art system on
the standard TDT5 dataset, while achieving an order
of magnitude speedup. Using our system for event
detection on 160 million Twitter posts shows that i)
the number of users that write about an event is more
indicative than the volume of tweets written about
it, ii) spam tweets can be detected with reasonable
precision, and iii) news about deaths of famous peo-
ple spreads the fastest on Twitter.
2 First Story Detection
2.1 Traditional Approach
The traditional approach to first story detection is to
represent documents as vectors in term space, where
coordinates represent the (possibly IDF-weighted)
frequency of a particular term in a document. Each
new document is then compared to the previous ones,
and if its similarity to the closest document (or cen-
troid) is below a certain threshold, the new document
is declared to be a first story. For example, this ap-
proach is used in the UMass (Allan et al, 2000) and
the CMU system (Yang et al, 1998). Algorithm 1
shows the exact pseudocode used by the UMass sys-
tem. Note that dismin(d) is the novelty score as-
signed to document d. Often, in order to decrease
the running time, documents are represented using
only n features with the highest weights.
Algorithm 1: Traditional FSD system based on
nearest-neighbor search.
1 foreach document d in corpus do
2 foreach term t in d do
3 foreach document d? that contains t do
4 update distance(d, d?)
5 end
6 end
7 dismin(d) = mind?{distance(d, d?)}
8 add d to inverted index
9 end
2.2 Locality Sensitive Hashing
The problem of finding the nearest neighbor to a
given query has been intensively studied, but as the
dimensionality of the data increases none of the cur-
rent solutions provide much improvement over a sim-
ple linear search (Datar et al, 2004). More recently,
research has focused on solving a relaxed version of
the nearest neighbor problem, the approximate near-
est neighbor, where the goal is to report any point
that lies within (1 + ?)r distance of the query point,
where r is the distance to the nearest neighbor. One
of the first approaches to solving the approximate-
NN problem in sublinear time was described in Indyk
and Motwani (1998), where the authors introduced a
new method called locality sensitive hashing (LSH).
This method relied on hashing each query point into
buckets in such a way that the probability of collision
was much higher for points that are near by. When a
new point arrived, it would be hashed into a bucket
and the points that were in the same bucket were
inspected and the nearest one returned.
Because we are dealing with textual documents,
a particularly interesting measure of distance is the
cosine between two documents. Allan et al (2000)
report that this distance outperforms the KL diver-
gence, weighted sum, and language models as dis-
tance functions on the first story detection task. This
is why in our work we use the hashing scheme pro-
posed by Charikar (2002) in which the probability
of two points colliding is proportional to the cosine
of the angle between them. This scheme was used,
e.g., for creating similarity lists of nouns collected
from a web corpus in Ravichandran et al (2005). It
works by intersecting the space with random hyper-
planes, and the buckets are defined by the subspaces
formed this way. More precisely, the probability of
two points x and y colliding under such a hashing
scheme is
Pcoll = 1? ?(x, y)pi , (1)
where ?(x, y) is the angle between x and y. By us-
ing more than one hyperplane, we can decrease the
probability of collision with a non-similar point. The
number of hyperplanes k can be considered as a num-
ber of bits per key in this hashing scheme. In par-
ticular, if x ? ui < 0, i ? [1 . . . k] for document x and
hyperplane vector ui, we set the i-th bit to 0, and
1 otherwise. The higher k is, the fewer collisions
we will have in our buckets but we will spend more
time computing the hash values.2 However, increas-
ing k also decreases the probability of collision with
the nearest neighbor, so we need multiple hash ta-
bles (each with k independently chosen random hy-
perplanes) to increase the chance that the nearest
neighbor will collide with our point in at least one of
2Probability of collision under k random hyperplanes will
be Pkcoll .
182
them. Given the desired number of bits k, and the
desired probability of missing a nearest neighbor ?,
one can compute the number of hash tables L as
L = log1?Pkcoll ?. (2)
2.3 Variance Reduction Strategy
Unfortunately, simply applying LSH for nearest
neighbor search in a FSD task yields poor results
with a lot of variance (the exact numbers are given in
Section 6). This is because LSH only returns the true
near neighbor if it is reasonably close to the query
point. If, however, the query point lies far away
from all other points (i.e., its nearest neighbor is far
away), LSH fails to find the true near neighbor. To
overcome this problem, we introduce a strategy by
which, if the LSH scheme declares a document new
(i.e., sufficiently different from all others), we start a
search through the inverted index, but only compare
the query with a fixed number of most recent doc-
uments. We set this number to 2000; preliminary
experiments showed that values between 1000 and
3000 all yield very similar results. The pseudocode
shown in algorithm 2 summarizes the approach based
on LSH, with the lines 11 and 12 being the variance
reduction strategy.
Algorithm 2: Our LSH-based approach.
input: threshold t
1 foreach document d in corpus do
2 add d to LSH
3 S ? set of points that collide with d in LSH
4 dismin(d) ? 1
5 foreach document d? in S do
6 c = distance(d, d?)
7 if c < dismin(d) then
8 dismin(d) ? c
9 end
10 end
11 if dismin(d) >= t then
12 compare d to a fixed number of most
recent documents as in Algorithm 1 and
update dismin if necessary
13 end
14 assign score dismin(d) to d
15 add d to inverted index
16 end
3 Streaming First Story Detection
Although using LSH in the way we just described
greatly reduces the running time, it is still too expen-
sive when we want to deal with text streams. Text
streams naturally arise on the Web, where millions
of new documents are published each hour. Social
media sites like Facebook, MySpace, Twitter, and
various blogging sites are a particularly interesting
source of textual data because each new document
is timestamped and usually carries additional meta-
data like topic tags or links to author?s friends. Be-
cause this stream of documents is unbounded and
coming down at a very fast rate, there is usually a
limit on the amount of space/time we can spend per
document. In the context of first story detection,
this means we are not allowed to store all of the pre-
vious data in main memory nor compare the new
document to all the documents returned by LSH.
Following the previous reasoning, we present the
following desiderata for a streaming first story de-
tection system: we first assume that each day we
are presented with a large volume of documents
in chronological order. A streaming FSD system
should, for each document, say whether it discusses a
previously unseen event and give confidence in its de-
cision. The decision should be made in bounded time
(preferably constant time per document), and using
bounded space (also constant per document). Only
one pass over the data is allowed and the decision
has to be made immediately after a new document
arrives. A system that has all of these properties can
be employed for finding first stories in real time from
a stream of stories coming down from the Web.
3.1 A constant space and time approach
In this section, we describe our streaming FSD sys-
tem in more depth. As was already mentioned in
Section 2.2, we use locality sensitive hashing to limit
our search to a small number of documents. How-
ever, because there is only a finite number of buck-
ets, in a true streaming setting the number of docu-
ments in any bucket will grow without a bound. This
means that i) we would use an unbounded amount
of space, and ii) the number of comparisons we need
to make would also grow without a bound. To alle-
viate the first problem, we limit the number of doc-
uments inside a single bucket to a constant. If the
bucket is full, the oldest document in the bucket is
removed. Note that the document is removed only
from that single bucket in one of the L hash tables
? it may still be present in other hash tables. Note
that this way of limiting the number of documents
kept is in a way topic-specific. Luo et al (2007) use
a global constraint on the documents they keep and
show that around 30 days of data needs to be kept
in order to achieve reasonable performance. While
using this approach also ensures that the number of
comparisons made is constant, this constant can be
183
rather large. Theoretically, a new document can col-
lide with all of the documents that are left, and this
can be quite a large number (we have to keep a suffi-
cient portion of the data in memory to make sure we
have a representative sample of the stream to com-
pare with). That is why, in addition to limiting the
number of documents in a bucket, we also limit our-
selves to making a constant number of comparisons.
We do this by comparing each new document with
at most 3L documents it collided with. Unlike Datar
et al (2004), where any 3L documents were used, we
compare to the 3L documents that collide most fre-
quently with the new document. That is, if S is the
set of all documents that collided with a new doc-
ument in all L hash tables, we order the elements
of S according to the number of hash tables where
the collision occurred. We take the top 3L elements
of that ordered set and compare the new document
only to them.
4 Detecting Events in Twitter Posts
While doing first story detection on a newspaper
stream makes sense because all of the incoming doc-
uments are actual stories, this is not the case with
Twitter posts (tweets). The majority of tweets are
not real stories, but rather updates on one?s personal
life, conversations, or spam. Thus, simply running a
first story detection system on this data would yield
an incredible amount of new stories each day, most
of which would be of no interest to anyone but a few
people. However, when something significant hap-
pens (e.g., a celebrity dies), a lot of users write about
this either to share their opinion or just to inform
others of the event. Our goal here is to automati-
cally detect these significant events, preferably with
a minimal number of non-important events.
Threading. We first run our streaming FSD
system and assign a novelty score to each tweet. In
addition, since the score is based on a cosine dis-
tance to the nearest tweet, for each tweet we also
output which other tweet it is most similar to. This
way, we can analyze threads of tweets, i.e., a subset
of tweets which all discuss the same topic (Nallap-
ati et al, 2004). To explain how we form threads
of tweets, we first introduce the links relation. We
say that tweet a links to tweet b if b is the nearest
neighbor of a and 1? cos(a, b) < t, where t is a user-
specified threshold. Then, for each tweet a we either
assign it to an existing thread if its nearest neighbor
is within distance t, or say that a is the first tweet in
a new thread. If we assign a to an existing thread,
we assign it to the same thread to which its nearest
neighbor belongs. By changing t we can control the
granularity of threads. If t is set very high, we will
have few very big and broad threads, whereas setting
t very low will result in many very specific and very
small threads. In our experiments, we set t = 0.5.
We experimented with different values of t and found
that for t ? [0.5, 0.6] results are very much the same,
whereas setting t outside this interval starts to im-
pact the results in the way we just explained.
Once we have threads of tweets, we are interested
in which threads grow fastest, as this will be an indi-
cation that news of a new event is spreading. There-
fore, for each time interval we only output the fastest
growing threads. This growth rate also gives us a way
to measure a thread?s impact.
5 Related Work
In the recent years, analysis of social media has at-
tracted a lot of attention from the research commu-
nity. However, most of the work that uses social
media focuses on blogs (Glance et al, 2004; Bansal
and Koudas, 2007; Gruhl et al, 2005). On the other
hand, research that uses Twitter has so far only
focused on describing the properties of Twitter it-
self (Java et al, 2007; Krishnamurthy et al, 2008).
The problem of online new event detection in
a large-scale streaming setting was previously ad-
dressed in Luo et al (2007). Their system used the
traditional approach to FSD and then employed var-
ious heuristics to make computation feasible. These
included keeping only the first stories in memory,
limiting the number of terms per document, limiting
the number of total terms kept, and employing par-
allel processing. Our randomized framework gives us
a principled way to work out the errors introduced
and is more general than the previously mentioned
approach because we could still use all the heuris-
tics used by Luo et al (2007) in our system. Fi-
nally, while Luo et al (2007) achieved considerable
speedup over an existing system on a TDT corpus,
they never showed the utility of their system on a
truly large-scale task.
The only work we are aware of that analyzes so-
cial media in a streaming setting is Saha and Getoor
(2009). There, the focus was on solving the maxi-
mum coverage problem for a stream of blog posts.
The maximum coverage problem in their setting,
dubbed blog watch, was selecting k blogs that maxi-
mize the cover of interests specified by a user. This
work differs from Saha and Getoor (2009) in many
ways. Most notably, we deal with the problem of
detecting new events, and determining who was the
first to report them. Also, there is a difference in the
type and volume of data ? while Saha and Getoor
(2009) use 20 days of blog data totalling two million
posts, we use Twitter data from a timespan of six
184
months, totalling over 160 million posts.
6 Experiments
6.1 TDT5 Experimental Setup
Baseline. Before applying our FSD system on
Twitter data, we first compared it to a state-of-the-
art FSD system on the standard TDT5 dataset. This
way, we can test if our system is on par with the best
existing systems, and also accurately measure the
speedup that we get over a traditional approach. In
particular, we compare our system with the UMass
FSD system (Allan et al, 2000). The UMass system
has participated in the TDT2 and TDT3 competi-
tions and is known to perform at least as well as other
existing systems who also took part in the competi-
tion (Fiscus, 2001). Note that the UMass system
uses an inverted index (as shown in Algorithm 1)
which optimizes the system for speed and makes sure
a minimal number of comparisons is made. We com-
pare the systems on the English part of the TDT5
dataset, consisting of 221, 306 documents from a time
period spanning April 2003 to September 2003. To
make sure that any difference in results is due to
approximations we make, we use the same settings
as the UMass system: 1-NN clustering, cosine as a
similarity measure, and TFIDF weighted document
representation, where the IDF weights are incremen-
tally updated. These particular settings were found
by Allan et al (2000) to perform the best for the
FSD task. We limit both systems to keeping only
top 300 features in each document. Using more than
300 features barely improves performance while tak-
ing significantly more time for the UMass system.3
LSH parameters. In addition, our system has
two LSH parameters that need to be set. The num-
ber of hyperplanes k gives a tradeoff between time
spent computing the hash functions and the time
spent computing the distances. A lower k means
more documents per bucket and thus more distance
computations, whereas a higher k means less doc-
uments per bucket, but more hash tables and thus
more time spent computing hash functions. Given k,
we can use equation (2) to compute L. In our case,
we chose k to be 13, and L such that the probability
of missing a neighbor within the distance of 0.2 is
less than 2.5%. The distance of 0.2 was chosen as a
reasonable estimate of the threshold when two docu-
ments are very similar. In general, this distance will
depend on the application, and Datar et al (2004)
suggest guessing the value and then doing a binary
search to set it more accurately. We set k to 13 be-
3In other words, using more features only increases the
advantage of our system over the UMass system.
cause it achieved a reasonable balance between time
spent computing the distances and the time spent
computing the hash functions.
Evaluation metric. The official TDT evalua-
tion requires each system to assign a confidence score
for its decision, and this assignment can be made
either immediately after the story arrives, or after
a fixed number of new stories have been observed.
Because we assume that we are working in a true
streaming setting, systems are required to assign a
confidence score as soon as the new story arrives.
The actual performance of a system is measured
in terms of detection error tradeoff (DET) curves
and the minimal normalized cost. Evaluation is car-
ried out by first sorting all stories according to their
scores and then performing a threshold sweep. For
each value of the threshold, stories with a score above
the threshold are considered new, and all others are
considered old. Therefore, for each threshold value,
one can compute the probability of a false alarm, i.e.,
probability of declaring a story new when it is actu-
ally not, and the miss probability, i.e., probability
of declaring a new story old (missing a new story).
Having computed all the miss and false alarm prob-
abilities, we can plot them on a graph showing the
tradeoff between these two quantities ? such graphs
are called detection error tradeoff curves. The nor-
malized cost Cdet is computed as
Cdet = Cmiss ?Pmiss ?Ptarget+CFA?PFA?Pnon?target ,
where Cmiss and CFA are costs of miss and false
alarm, Pmiss and PFA are probabilities of a miss and
false alarm, and Ptarget and Pnon?target are the prior
target and non-target probabilities. Minimal nor-
malized cost Cmin is the minimal value of Cdet over
all threshold values (a lower value of Cmin indicates
better performance).
6.2 TDT5 Results
All the results on the TDT5 dataset are shown in
Table 1. In this section, we go into detail in explain-
ing them. As was mentioned in Section 2.2, simply
using LSH to find a nearest neighbor resulted in poor
performance and a high variance of results. In par-
ticular, the mean normalized cost of ten runs of our
system without the variance reduction strategy was
0.88, with a standard deviation of 0.046. When us-
ing the strategy explained in Section 2.2, the mean
result dropped to 0.70, with a standard deviation of
0.004. Therefore, the results were significantly im-
proved, while also reducing standard deviation by an
order of magnitude. This shows that there is a clear
advantage in using our variance reduction strategy,
185
Table 1: Summary of TDT5 results. Numbers next to LSH?ts indicate the maximal number of documents in a bucket,
measured in terms of percentage of the expected number of collisions.
Baseline Unbounded Bounded
Pure Variance Red. Time Space and Time
System UMass LSH LSH? LSH?t LSH?ts 0.5 LSH?ts 0.3 LSH?ts 0.1
Cmin 0.69 0.88 0.70 0.71 0.76 0.75 0.73
1
2
5
10
20
40
60
80
90
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
Miss 
proba
bility 
(in %)
False Alarms probability (in %)
Random PerformanceOur systemUMass system
Figure 1: Comparison of our system with the UMass FSD
system.
and all the following results we report were obtained
from a system that makes use of it.
Figure 1 shows DET curves for the UMass and for
our system. For this evaluation, our system was not
limited in space, i.e., buckets sizes were unlimited,
but the processing time per item was made constant.
It is clear that UMass outperforms our system, but
the difference is negligible. In particular, the min-
imal normalized cost Cmin was 0.69 for the UMass
system, and 0.71 for our system. On the other hand,
the UMass system took 28 hours to complete the
run, compared to two hours for our system. Figure 2
shows the time required to process 100 documents
as a function of number of documents seen so far.
We can see that our system maintains constant time,
whereas the UMass system processing time grows
without a bound (roughly linear with the number
of previously seen documents).
The last three columns in Table 1 show the effect
that limiting the bucket size has on performance.
Bucket size was limited in terms of the percent of
expected number of collisions, i.e., a bucket size of
0.5 means that the number of documents in a bucket
cannot be more than 50% of the expected number
of collisions. The expected number of collisions can
 0
 20
 40
 60
 80
 100
 120
 0  50000  100000  150000  200000  250000
Time 
per 10
0 doc
umen
ts (sec)
Number of documents processed
UMass systemOur system
Figure 2: Comparison of processing time per 100 docu-
ments for our and the UMass system.
be computed as n/2k, where n is the total number
of documents, and k is the LSH parameter explained
earlier. Not surprisingly, limiting the bucket size re-
duced performance compared to the space-unlimited
version, but even when the size is reduced to 10% of
the expected number of collisions, performance re-
mains reasonably close to the UMass system. Fig-
ure 3 shows the memory usage of our system on a
month of Twitter data (more detail about the data
can be found in Section 6.3). We can see that most
of the memory is allocated right away, after which
the memory consumption levels out. If we ran the
system indefinitely, we would see the memory usage
grow slower and slower until it reached a certain level
at which it would remain constant.
6.3 Twitter Experimental Setup
Corpus. We used our streaming FSD system to
detect new events from a collection of Twitter data
gathered over a period of six months (April 1st 2009
to October 14th 2009). Data was collected through
Twitter?s streaming API.4 Our corpus consists of
163.5 million timestamped tweets, totalling over 2
billion tokens. All the tweets in our corpus contain
4http://stream.twitter.com/
186
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  500  1000  1500  2000  2500  3000  3500  4000
Perce
nt of m
emor
y use
d
Minutes
Figure 3: Memory usage on a month of Twitter data.
X-axis shows how long the system has been running for.
only ASCII characters and we additionally stripped
the tweets of words beginning with the @ or # sym-
bol. This is because on Twitter words beginning with
@ indicate a reply to someone, and words beginning
with # are topic tags. Although these features would
probably be helpful for our task, we decided not to
use them as they are specific to Twitter and our ap-
proach should be independent of the stream type.
Gold standard. In order to measure how well
our system performs on the Twitter data, we em-
ployed two human experts to manually label all the
tweets returned by our system as either Event, Neu-
tral, or Spam. Note that each tweet that is returned
by our system is actually the first tweet in a thread,
and thus serves as the representative of what the
thread is about. Spam tweets include various ad-
vertisements, automatic weather updates, automatic
radio station updates, etc. For a tweet to be la-
beled as an event, it had to be clear from the tweet
alone what exactly happened without having any
prior knowledge about the event, and the event refer-
enced in the tweet had to be sufficiently important.
Important events include celebrity deaths, natural
disasters, major sports, political, entertainment, and
business events, shootings, plane crashes and other
disasters. Neutral tweets include everything not la-
beled as spam or event. Because the process of man-
ual labeling is tedious and time-consuming, we only
labeled the 1000 fastest growing threads from June
2009. Rate of growth of a thread is measured by the
number of tweets that belong to that thread in a win-
dow of 100,000 tweets, starting from the beginning
of the thread. Agreement between our two annota-
tors, measured using Cohen?s kappa coefficient, was
substantial (kappa = 0.65). We use 820 tweets on
which both annotators agreed as the gold standard.
Evaluation. Evaluation is performed by com-
puting average precision (AP) on the gold standard
sorted according to different criteria, where event
tweets are taken to be relevant, and neutral and spam
tweets are treated as non-relevant documents. Aver-
age precision is a common evaluation metric in tasks
like ad-hoc retrieval where only the set of returned
documents and their relevance judgements are avail-
able, as is the case here (Croft et al, 2009). Note
that we are not evaluating our FSD system here.
There are two main reasons for this: i) we already
have a very good idea about the first story detection
performance from the experiments on TDT5 data,
and ii) evaluating a FSD system on this scale would
be prohibitively expensive as it would involve hu-
man experts going through 30 million tweets looking
for first stories. Rather, we are evaluating different
methods of ranking threads which are output from a
FSD system for the purpose of detecting important
events in a very noisy and unstructured stream such
as Twitter.
6.4 Twitter Results
Results for the average precisions are given in Ta-
ble 2. Note that we were not able to compare our
system with the UMass FSD system on the Twit-
ter data, as the UMass system would not finish in
any reasonable amount of time. Different rows of
Table 2 correspond to the following ways of ranking
the threads:
? Baseline ? random ordering of threads
? Size of thread ? threads are ranked according to
number of tweets
? Number of users ? threads are ranked according
to number of unique users posting in a thread
? Entropy + users ? if the entropy of a thread is
< 3.5, move to the back of the list, otherwise
sort according to number of unique users
Results show that ranking according to size of thread
performs better than the baseline, and ranking ac-
cording to the number of users is slightly better.
However, a sign test showed that neither of the two
ranking strategies is significantly better than the
baseline. We perform the sign test by splitting the
labeled data into 50 stratified samples and ranking
each sample with different strategies. We then mea-
sure the number of times each strategy performed
better (in terms of AP) and compute the significance
levels based on these numbers. Adding the informa-
tion about the entropy of the thread showed to be
187
Table 2: Average precision for Events vs. Rest and for
Events and Neutral vs. Spam.
Ranking method events vs. rest spam vs. rest
Baseline 16.5 84.6
Size of thread 24.1 83.5
Number of users 24.5 83.9
Entropy + users 34.0 96.3
Table 3: Average precision as a function of the entropy
threshold on the Events vs. Rest task.
Entropy 2 2.5 3 3.5 4 4.5
AP 24.8 27.6 30.0 34.0 33.2 29.4
very beneficial. Entropy of a thread is computed as
Hthread = ?
?
i
ni
N log
ni
N ,
where ni is the number of times word i appears in
a thread, and N = ?i ni is the total number of
words in a thread. We move the threads with low
entropy (< 3.5) to the back of the list, while we or-
der other threads by the number of unique users.
A sign test showed this approach to be significantly
better (p ? 0.01) than all of the previous ranking
methods. Table 3 shows the effect of varying the en-
tropy threshold at which threads are moved to the
back of the list. We can see that adding informa-
tion about entropy improves results regardless of the
threshold we choose. This approach works well be-
cause most spam threads have very low entropy, i.e.,
contain very little information.
We conducted another experiment where events
and neutral tweets are considered relevant, and spam
tweets non-relevant documents. Results for this ex-
periment are given in the third column of Table 2.
Results for this experiment are much better, mostly
due to the large proportion of neutral tweets in the
data. The baseline in this case is very strong and
neither sorting according to the size of the thread
nor according to the number of users outperforms
the baseline. However, adding the information about
entropy significantly (p ? 0.01) improves the perfor-
mance over all other ranking methods.
Finally, in Table 4 we show the top ten fastest
growing threads in our data (ranked by the number
of users posting in the thread). Each thread is repre-
sented by the first tweet. We can see from the table
that events which spread the fastest on Twitter are
Table 4: Top ten fastest growing threads in our data.
# users First tweet
7814 TMZ reporting michael jackson has had a heart
attack. We r checking it out. And pulliing
video to use if confirmed
7579 RIP Patrick Swayze...
3277 Walter Cronkite is dead.
2526 we lost Ted Kennedy :(
1879 RT BULLETIN ? STEVE MCNAIR
HAS DIED.
1511 David Carradine (Bill in ?Kill Bill?)
found hung in Bangkok hotel.
1458 Just heard Sir Bobby Robson has died. RIP.
1426 I just upgraded to 2.0 - The professional
Twitter client. Please RT!
1220 LA Times reporting Manny Ramirez tested
positive for performance enhancing drugs.
To be suspended 50 games.
1057 A representative says guitar legend
Les Paul has died at 94
mostly deaths of famous people. One spam thread
that appears in the list has an entropy of 2.5 and
doesn?t appear in the top ten list when using the
entropy + users ranking.
7 Conclusion
We presented an approach to first story detection in a
streaming setting. Our approach is based on locality
sensitive hashing adapted to the first story detection
task by introducing a backoff towards exact search.
This adaptation greatly improved performance of the
system and virtually eliminated variance in the re-
sults. We showed that, using our approach, it is pos-
sible to achieve constant space and processing time
while maintaining very good results. A comparison
with the UMass FSD system showed that we gain
more than an order of magnitude speedup with only a
minor loss in performance. We used our FSD system
on a truly large-scale task of detecting new events
from over 160 million Twitter posts. To the best of
our knowledge, this is the first work that does event
detection on this scale. We showed that our system
is able to detect major events with reasonable preci-
sion, and that the amount of spam in the output can
be reduced by taking entropy into account.
Acknowledgments
The authors would like to thank Donnla Osborne for
her work on annotating tweets.
188
References
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000. Detections, bounds, and timelines:
Umass and tdt-3. In Proceedings of Topic Detection
and Tracking Workshop, pages 167?174.
James Allan. 2002. Topic detection and tracking: event-
based information organization. Kluwer Academic
Publishers.
Nilesh Bansal and Nick Koudas. 2007. Blogscope: a
system for online analysis of high volume text streams.
In VLDB ?07: Proceedings of the 33rd international
conference on Very large data bases, pages 1410?1413.
VLDB Endowment.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In STOC ?02: Pro-
ceedings of the thiry-fourth annual ACM symposium on
Theory of computing, pages 380?388, New York, NY,
USA. ACM.
W.B. Croft, D. Metzler, and T. Strohman. 2009. Search
Engines: Information Retrieval in Practice. Addison-
Wesley Publishing.
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Va-
hab Mirrokni. 2004. Locality-sensitive hashing scheme
based on p-stable distributions. In SCG ?04: Proceed-
ings of the twentieth annual symposium on Computa-
tional geometry, pages 253?262, New York, NY, USA.
ACM.
J. Fiscus. 2001. Overview of results (nist). In Proceed-
ings of the TDT 2001 Workshop.
N. Glance, M. Hurst, and T. Tomokiyo. 2004. BlogPulse:
Automated Trend Discovery for Weblogs. WWW 2004
Workshop on the Weblogging Ecosystem: Aggregation,
Analysis and Dynamics, 2004.
Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak,
and Andrew Tomkins. 2005. The predictive power
of online chatter. In KDD ?05: Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 78?87, New
York, NY, USA. ACM.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In STOC ?98: Proceedings of the thirti-
eth annual ACM symposium on Theory of computing,
pages 604?613, New York, NY, USA. ACM.
Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.
2007. Why we twitter: understanding microblogging
usage and communities. In WebKDD/SNA-KDD ?07:
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, pages 56?65, New York, NY, USA. ACM.
Balachander Krishnamurthy, Phillipa Gill, and Martin
Arlitt. 2008. A few chirps about twitter. In WOSP
?08: Proceedings of the first workshop on Online social
networks, pages 19?24, New York, NY, USA. ACM.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for smt. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 756?764.
Gang Luo, Chunqiang Tang, and Philip S. Yu. 2007.
Resource-adaptive real-time new event detection. In
SIGMOD ?07: Proceedings of the 2007 ACM SIG-
MOD international conference on Management of
data, pages 497?508, New York, NY, USA. ACM.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Now Publishers Inc.
Ramesh Nallapati, Ao Feng, Fuchun Peng, and James
Allan. 2004. Event threading within news topics. In
CIKM ?04: Proceedings of the thirteenth ACM interna-
tional conference on Information and knowledge man-
agement, pages 446?453, New York, NY, USA. ACM.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
622?629, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Barna Saha and Lise Getoor. 2009. On maximum cov-
erage in the streaming model & application to multi-
topic blog-watch. In 2009 SIAM International Con-
ference on Data Mining (SDM09), April.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, Alex Strehl, and Vishy Vish-
wanathan. 2009. Hash kernels. In Proceedings of
the 12th International Conference on Artificial Intelli-
gence and Statistics (AISTATS), pages 496?503.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998.
A study of retrospective and on-line event detection.
In SIGIR ?98: Proceedings of the 21st annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 28?36, New
York, NY, USA. ACM.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In KDD ?09: Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 937?946,
New York, NY, USA. ACM.
189
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 394?402,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Stream-based Translation Models for Statistical Machine Translation
Abby Levenberg
School of Informatics
University of Edinburgh
a.levenberg@ed.ac.uk
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
ccb@cs.jhu.edu
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Abstract
Typical statistical machine translation sys-
tems are trained with static parallel corpora.
Here we account for scenarios with a continu-
ous incoming stream of parallel training data.
Such scenarios include daily governmental
proceedings, sustained output from transla-
tion agencies, or crowd-sourced translations.
We show incorporating recent sentence pairs
from the stream improves performance com-
pared with a static baseline. Since frequent
batch retraining is computationally demand-
ing we introduce a fast incremental alternative
using an online version of the EM algorithm.
To bound our memory requirements we use
a novel data-structure and associated training
regime. When compared to frequent batch re-
training, our online time and space-bounded
model achieves the same performance with
significantly less computational overhead.
1 Introduction
There is more parallel training data available to-
day than there has ever been and it keeps increas-
ing. For example, the European Parliament1 releases
new parallel data in 22 languages on a regular basis.
Project Syndicate2 translates editorials into seven
languages (including Arabic, Chinese and Russian)
every day. Existing translation systems often get
?crowd-sourced? improvements such as the option
to contribute a better translation to GoogleTrans-
late3. In these and many other instances, the data can
be viewed as an incoming unbounded stream since
1http://www.europarl.europa.eu
2http://www.project-syndicate.org
3http://www.translate.google.com
the corpus grows continually with time. Dealing
with such unbounded streams of parallel sentences
presents two challenges: making retraining efficient
and operating within a bounded amount of space.
Statistical Machine Translation (SMT) systems
are typically batch trained, often taking many CPU-
days of computation when using large volumes of
training material. Incorporating new data into these
models forces us to retrain from scratch. Clearly,
this makes rapidly adding newly translated sen-
tences into our models a daunting engineering chal-
lenge. We introduce an adaptive training regime us-
ing an online variant of EM that is capable of in-
crementally adding new parallel sentences without
incurring the burdens of full retraining.
For situations with large volumes of incoming
parallel sentences we are also forced to consider
placing space-bounds on our SMT system. We in-
troduce a dynamic suffix array which allows us to
add and delete parallel sentences, thereby maintain-
ing bounded space despite processing a potentially
high-rate input stream of unbounded length.
Taken as a whole we show that online translation
models operating within bounded space can perform
as well as systems which are batch-based and have
no space constraints thereby making our approach
suitable for stream-based translation.
2 Stepwise Online EM
The EM algorithm is a common way of inducing
latent structure from unlabeled data in an unsuper-
vised manner (Dempster et al, 1977). Given a set
of unlabeled examples and an initial, often uniform
guess at a probability distribution over the latent
variables, the EM algorithm maximizes the marginal
394
log-likelihood of the examples by repeatedly com-
puting the expectation of the conditional probability
of the latent data with respect to the current distri-
bution, and then maximizing the expectations over
the observations into a new distribution used in the
next iteration. EM (and related variants such as vari-
ational or sampling approaches) form the basis of
how SMT systems learn their translation models.
2.1 Batch vs. Online EM
Computing an expectation for the conditional prob-
abilities requires collecting the sufficient statistics S
over the set of n unlabeled examples. In the case
of a multinomial distribution, S is comprised of the
counts over each conditional observation occurring
in the n examples. In traditional batch EM, we col-
lect the counts over the entire dataset of n unlabeled
training examples via the current ?best-guess? proba-
bility model ??t at iteration t (E-step) before normal-
izing the counts into probabilities ??(S) (M-step)4.
After each iteration all the counts in the sufficient
statistics vector S are cleared and the count collec-
tion begins anew using the new distribution ??t+1.
When we move to processing an incoming data
stream, however, the batch EM algorithm?s require-
ment that all data be available for each iteration be-
comes impractical since we do not have access to all
n examples at once. Instead we receive examples
from the input stream incrementally. For this reason
online EM algorithms have been developed to up-
date the probability model ?? incrementally without
needing to store and iterate through all the unlabeled
training data repeatedly.
Various online EM algorithms have been investi-
gated (see Liang and Klein (2009) for an overview)
but our focus is on the stepwise online EM (sOEM)
algorithm (Cappe and Moulines, 2009). Instead
of iterating over the full set of training examples,
sOEM stochastically approximates the batch E-step
and incorporates the information from the newly
available streaming observations in steps. Each step
is called a mini-batch and is comprised of one or
more new examples encountered in the stream.
Unlike in batch EM, in sOEM the expected counts
are retained between EM iterations and not cleared.
4As the M-step can be computed in closed form we desig-
nate it in this work as ??(S).
Algorithm 1: Batch EM for Word Alignments
Input: {F (source),E (target)} sentence-pairs
Output: MLE ??T over alignments a
??0 ?MLE initialization;
for iteration k = 0, . . . , T do
S ? 0; // reset counts
foreach (f, e) ? {F,E} do // E-step
S ? S +
?
a??a
Pr(f, a?|e; ??t);
end
??t+1 ? ??t(S) ; // M-step
end
That is, for each new example we interpolate its ex-
pected count with the existing set of sufficient statis-
tics. For each step we use a stepsize parameter ?
which mixes the information from the current ex-
ample with information gathered from all previous
examples. Over time the sOEM model probabilities
begin to stabilize and are guaranteed to converge to
a local maximum (Cappe and Moulines, 2009).
Note that the stepsize ? has a dependence on the
current mini-batch. As we observe more incoming
data the model?s current probability distribution is
closer to the true distribution so the new observa-
tions receive less weight. From Liang and Klein
(2009), if we set the stepsize as ?t = (t + 2)??,
with 0.5 < ? ? 1, we can guarantee convergence in
the limit as n ? ?. If we set ? low, ? weighs the
newly observed statistics heavily whereas if ? is low
new observations are down-weighted.
2.2 Batch EM for Word Alignments
Batch EM is used in statistical machine translation
to estimate word alignment probabilities between
parallel sentences. From these alignments, bilingual
rules or phrase pairs can be extracted. Given a set
of parallel sentence examples, {F,E}, with F the
set of source sentences and E the corresponding tar-
get sentences, we want to find the latent alignments
a for a sentence pair (f , e) ? {F,E} that defines
the most probable correspondence between words fj
and ei such that aj = i. We can induce these align-
ments using an HMM-based alignment model where
the probability of alignment aj is dependent only on
the previous alignment at aj?1 (Vogel et al, 1996).
395
We can write
Pr(f ,a | e) =
?
a??a
|f |
?
j=1
p(aj | aj?1, |e|) ? p(fj | eaj )
where we assume a first-order dependence on previ-
ously aligned positions.
To find the most likely parameter weights for
the translation and alignment probabilities for the
HMM-based alignments, we employ the EM algo-
rithm via dynamic programming. Since HMMs have
multiple local minima, we seed the HMM-based
model probabilities with a better than random guess
using IBM Model 1 (Brown et al, 1993) as is stan-
dard. IBM Model 1 is of the same form as the
HMM-based model except it uses a uniform distri-
bution instead of a first-order dependency. Although
a series of more complex models are defined, IBM
Models 2 to Model 6 (Brown et al, 1993; Och and
Ney, 2003), researchers typically find that extract-
ing phrase pairs or translation grammar rules using
Model 1 and the HMM-based alignments results in
equivalently high translation quality. Nevertheless,
there is nothing in our approach which limits us to
using just Model 1 and the HMM model.
A high-level overview of the standard, batch EM
algorithm applied to HMM-based word alignment
model is shown in Algorithm 1.
2.3 Stepwise EM for Word Alignments
Application of sOEM to HMM and Model 1 based
word aligning is straightforward. The process of
collecting the counts over the expected conditional
probabilities inside each iteration loop remains the
same as in the batch case. However, instead of clear-
ing the sufficient statistics between the iterations we
retain them and interpolate them with the batch of
counts gathered in the next iteration.
Algorithm 2 shows high level pseudocode of our
sOEM framework as applied to HMM-based word
alignments. Here we have an unbounded input
stream of source and target sentences {F,E} which
we do not have access to in its entirety at once.
Instead we observe mini-batches {M} comprised
of chronologically ordered strict subsets of the full
stream. To word align the sentences for each mini-
batch m ? M, we use the probability assigned by
the current model parameters and then interpolate
Algorithm 2: sOEM Algorithm for Word Align-
ments
Input: mini-batches of sentence pairs
{M : M ? {F (source), E(target)}}
Input: stepsize weight ?
Output: MLE ??T over alignments a
??0 ?MLE initialization;
S ? 0; k = 0;
foreach mini-batch {m : m ?M} do
for iteration t = 0, . . . , T do
foreach (f, e) ? {m} do // E-step
s??
?
a??a
Pr(f, a?|e; ??t);
end
? = (k + 2)??; k = k + 1; // stepsize
S ? ?s? + (1? ?)S; // interpolate
??t+1 ? ??t(S) ; // M-step
end
end
the newest sufficient statistics s? with our full count
vector S using an interpolation parameter ?. The in-
terpolation parameter ? has a dependency on how
far along the input stream we are processing.
3 Dynamic Suffix Arrays
So far we have shown how to incrementally retrain
translation models. We now consider how we might
bound the space we use for them when processing
(potentially) unbounded streams of parallel data.
Suffix arrays are space-efficient data structures for
fast searching over large text strings (Manber and
Myers, 1990). Treating the entire corpus as a sin-
gle string, a suffix array holds in lexicographical or-
der (only) the starting index of each suffix of the
string. After construction, since the corpus is now
ordered, we can query the suffix array quickly us-
ing binary search to efficiently find all occurrences
of a particular token or sequence of tokens. Then we
can easily compute, on-the-fly, the statistics required
such as translation probabilities for a given source
phrase. Suffix arrays can also be compressed, which
make them highly attractive structures for represent-
ing massive translation models (Callison-Burch et
al., 2005; Lopez, 2008).
We need to delete items if we wish to maintain
396
                                                                                         epoch 2           
epoch 1 epoch 2 model coverage
model coverage
input stream
Test Points
input stream
Test Points
Static 
Unbounded
input stream
Test Points
Bounded
model coverage
sliding windows
Figure 1: Streaming coverage conditions. In traditional
batch based modeling the coverage of a trained model
never changes. Unbounded coverage operates without
any memory constraints so the model is able to contin-
ually add data from the input stream. Bounded coverage
uses just a fixed window.
constant space when processing unbounded streams.
Standard suffix arrays are static, store a fixed corpus
and do not support deletions. Nevertheless, a dy-
namic variant of the suffix array does support dele-
tions as well as insertions and therefore can be used
in our stream-based approach (Salson et al, 2009).
Using a dynamic suffix array, we can compactly
represent the set of parallel sentences from which
we eventually extract grammar rules. Furthermore,
when incorporating new parallel sentences, we sim-
ply insert them into the array and, to maintain con-
stant space usage, we delete an equivalent number.
4 Experiments
In this section we describe the experiments con-
ducted comparing various batch trained translation
models (TMs) versus online incrementally retrained
TMs in a full SMT setting with different conditions
set on model coverage. We used publicly available
resources for all our tests. We start by showing that
recency motivates incremental retraining.
4.1 Effects of Recency on SMT
For language modeling, it is known that perfor-
mance can be improved using the criterion of re-
cency where training data is drawn from times
chronologically closer to the test data (Rosenfeld,
 0
 0.5
 1
 1.5
 2
 2.5
 5  10  15  20  25  30  35
D
el
ta
 in
 B
LE
U
 sc
or
es
epochs
Figure 2: Recency effects to SMT performance. De-
picted are the differences in BLEU scores for multiple
test points decoded by a static baseline system and a sys-
tem batched retrained on a fixed sized window prior to
the test point in question. The results are accentuated at
the end of the timeline when more time has passed con-
firming that recent data impacts translation performance.
1995). Given an incoming stream of parallel text,
we gauged the extent to which incorporating recent
data into a TM affects translation quality.
We used the Europarl corpus5 with the Fr-En lan-
guage pair using French as source and English as tar-
get. Europarl is released in the format of a daily par-
liamentary session per time-stamped file. The actual
dates of the full corpus are interspersed unevenly
(they do not convene daily) over a continuous time-
line corresponding to the parliament sessions from
April,1996 through October, 2006, but for concep-
tual simplicity we treated the corpus as a continual
input stream over consecutive days.
As a baseline we aligned the first 500k sentence
pairs from the beginning of the corpus timeline. We
extracted a grammar for and translated 36 held out
test documents that were evenly spaced along the re-
mainder of the Europarl timeline. These test docu-
ments effectively divided the remaining training data
into epochs and we used a sliding window over the
timeline to build 36 distinct, overlapping training
sets of 500k sentences each.
We then translated all 36 test points again using
a new grammar for each document extracted from
only the sentences contained in the epoch that was
before it. To explicitly test the effect of recency
5Available at http://www.statmt.org/europarl
397
on the TM all other factors of the SMT pipeline re-
mained constant including the language model and
the feature weights. Hence, the only change from
the static baseline to the epochs performance was the
TM data which was based on recency. Note that at
this stage we did not use any incremental retraining.
Results are shown in Figure 2 as the differences
in BLEU score (Papineni et al, 2001) between the
baseline TM versus the translation models trained
on material chronologically closer to the given test
point. The consistently positive deltas in BLEU
scores between the model that is never retrained and
the models that are retrained show that we achieve a
higher translation performance when using more up-
to-date TMs that incorporate recent sentence pairs.
As the chronological distance between the initial,
static model and the retrained models increases, we
see ever-increasing differences in translation perfor-
mance. This underlines the need to retrain transla-
tion models with timely material.
4.2 Unbounded and Bounded Translation
Model Retraining
Here we consider how to process a stream along two
main axes: by bounding time (batch versus incre-
mental retraining) and by bounding space (either us-
ing all the stream seen so far, or only using a fixed
sized sample of it).
To ensure the recency results reported above were
not limited to French-English, this time our paral-
lel input stream was generated from the German-
English language pair of Europarl with German as
source and English again as target. For testing we
held out a total of 22k sentences from 10 evenly
spaced intervals in the input stream which divided
the input stream into 10 epochs. Stream statistics for
three example epochs are shown in Table 1. We held
out 4.5k sentence pairs as development data to opti-
mize the feature function weights using minimum
error rate training (Och, 2003) and these weights
were used by all models. We used Joshua (Li et
al., 2009), a syntax-based decoder with a suffix array
implementation, and rule induction via the standard
Hiero grammar extraction heuristics (Chiang, 2007)
for the TMs. Note that nothing hinges on whether
we used a syntax or a phrase-based system.
We used a 5-gram, Kneser-Ney smoothed lan-
guage model (LM) trained on the initial segment of
Ep From?To Sent Pairs Source/Target
00 04/1996?12/2000 600k 15.0M/16.0M
03 02/2002?09/2002 70k 1.9M/2.0M
06 10/2003?03/2004 60k 1.6M/1.7M
10 03/2006?09/2006 73k 1.9M/2.0M
Table 1: Date ranges, total sentence pairs, and source and
target word counts encountered in the input stream for
example epochs. Epoch 00 is baseline data that is also
used as a seed corpus for the online models.
the target side parallel data used in the first base-
line as described further in the next subsection. As
our initial experiments aim to isolate the effect of
changes to the TM on overall translation system per-
formance, our in-domain LM remains static for ev-
ery decoding run reported below until indicated.
We used the open-source toolkit GIZA++ (Och
and Ney, 2003) for all word alignments. For the
online adaptation experiments we modified Model
1 and the HMM model in GIZA++ to use the sOEM
algorithm. Batch baselines were aligned using the
standard version of GIZA++. We ran the batch and
incremental versions of Model 1 and HMM for the
same number of iterations each in both directions.
4.3 Time and Space Bounds
For both batch and sOEM we ran a number of ex-
periments listed below corresponding to the differ-
ent training scenarios diagrammed in Figure 1.
1. Static: We used the first half of the in-
put stream, approximately 600k sentences and
15/16 million source/target words, as parallel
training data. We then translated each of the 10
test sets using the static model. This is the tradi-
tional approach and the coverage of the model
never changes.
2. Unbounded Space: Batch or incremental re-
training with no memory constraint. For each
epoch in the stream, we retrained the TM us-
ing all the data from the beginning of the in-
put stream until just before the present with re-
spect to a given test point. As more time passes
our training data set grows so each batch run
of GIZA++ takes more time. Overall this is the
most computationally expensive approach.
398
Baseline Unbounded Bounded
Epoch Test Date Test Sent. Train Sent. Rules Train Sent. Rules Train Sent. Rules
03 09/23/2002 1.0k 580k 4.0M 800k 5.0M 580k 4.2M
06 03/29/2004 1.5k 580k 5.0M 1.0M 7.0M 580k 5.5M
10 09/26/2006 3.5k 580k 8.5M 1.3M 14.0M 580k 10.0M
Table 2: Translation model statistics for example epochs and the next test dates grouped by experimental condition.
Test and Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique
Hiero grammar rules extracted for the corresponding test set.
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1  2  3  4  5  6  7  8  9  10
D
el
ta
 in
 B
LE
U
 sc
or
es
epochs
unbounded
bounded
Figure 3: Static vs. online TM performance. Gains in
translation performance measured by BLEU are achieved
when recent German-English sentence pairs are auto-
matically incorporated into the TM. Shown are relative
BLEU improvements for the online models against the
static baseline.
3. Bounded Space: Batch and incremental re-
training with an enforced memory constraint.
Here we batch or incrementally retrain using
a sliding window approach where the training
set size (the number of sentence pairs) remains
constant. In particular, we ensured that we
used the same number of sentences as the base-
line. Each batch run of GIZA++ takes approxi-
mately the same time.
The time for aligning in the sOEM model is unaf-
fected by the bounded/unbounded conditions since
we always only align the mini-batch of sentences
encountered in the last epoch. In contrast, for batch
EM we must realign all the sentences in our training
set from scratch to incorporate the new training data.
Similarly space usage for the batch training grows
with the training set size. For sOEM, in theory mem-
ory used is with respect to vocabulary size (which
grows slowly with the stream size) since we retain
count history for the entire stream. To make space
usage truly constant, we filter for just the needed
word pairs in the current epoch being aligned. This
effectively means that online EM is more mem-
ory efficient than the batch version. As our exper-
iments will show, the sufficient statistics kept be-
tween epochs by sOEM benefits performance com-
pared to the batch models which can only use infor-
mation present within the batch itself.
4.4 Incremental Retraining Procedure
Our incremental adaptation procedure was as fol-
lows: after the latest mini-batch of sentences had
been aligned using sOEM we added all newly
aligned sentence pairs to the dynamic suffix ar-
rays. For the experiments where our memory was
bounded, we also deleted an equal number of sen-
tences from the suffix arrays before extracting the
Hiero grammar for the next test point. For the un-
bounded coverage experiments we deleted nothing
prior to grammar extraction. Table 2 presents statis-
tics for the number of training sentence pairs and
grammar rules extracted for each coverage condition
for various test points.
4.5 Results
Figure 3 shows the results of the static baseline
against both the unbounded and bounded online EM
models. We can see that both the online models
outperform the static baseline. On average the un-
constrained model that contains more sentence pairs
for rule extraction slightly outperforms the bounded
condition which uses less data per epoch. However,
the static baseline and the bounded models both use
the same number of sentence-pairs for TM training.
We see there is a clear gain by incorporating recent
sentence-pairs made available by the stream.
399
Static Baseline Retrained (Unbounded) Retrained (Bounded)
Test Date Batch Batch Online Batch Online
09/23/2002 26.10 26.60 26.43 26.19 26.40
03/29/2004 27.40 28.33 28.42 28.06 28.38
09/26/2006 28.56 29.74 29.75 29.73 29.80
Table 3: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional
model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream
but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded
online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so
quickly adopt the new data (best results are italicized).
Table 3 gives results of the online models com-
pared to the batch retrained models. For presentation
clarity we show only a sample of the full set of ten
test points though all results follow the pattern that
using more aligned sentences to derive our gram-
mar set resulted in slightly better performance ver-
sus a restricted training set. However, for the same
coverage constraints not only do we achieve com-
parable performance to batch retrained models us-
ing the sOEM method of incremental adaptation, we
are able to align and adopt new data from the input
stream orders of magnitude quicker since we only
align the mini-batch of sentences collected from the
last epoch. In the bounded condition, not only do
we benefit from quicker adaptation, we also see that
sOEM models slightly outperform the batch based
models due to the online algorithm employing a
longer history of count-based evidence to draw on
when aligning new sentence pairs.
Figure 4 shows two example test sentences that
benefited from the online TM adaptation. Trans-
lations from the online model produce more and
longer matching phrases for both sentences (e.g.,
?creation of such a?, ?of the occupying forces?)
leading to more fluent output as well as the improve-
ments achieved in BLEU scores.
We experimented with a variety of interpolation
parameters (see Algorithm 2) but found no signifi-
cant difference between them (the biggest improve-
ment gained over all test points for all parameter set-
tings was less than 0.1% BLEU).
4.6 Increasing LM Coverage
A natural and interesting extension to the experi-
ments above is to use the target side of the incoming
stream to extend the LM coverage alongside the TM.
Test Date Static Unbounded Bounded
09/23/2002 26.46 27.11 26.96
03/29/2004 28.11 29.53 29.20
09/26/2006 29.53 30.94 30.88
Table 4: Unbounded LM coverage improvements. Shown
are the BLEU scores for each experimental conditional
when we allow the LM coverage to increase.
It is well known that more LM coverage (via larger
training data sets) is beneficial to SMT performance
(Brants et al, 2007) so we investigated whether re-
cency gains for the TM were additive with recency
gains afforded by a LM.
To test this we added all the target side data from
the beginning of the stream to the most recent epoch
into the LM training set before each test point. We
then batch retrained6 and used the new LM with
greater coverage for the next decoding run. Experi-
ments were for the static baseline and online models.
Results are reported in Table 4. We can see that
increasing LM coverage is complimentary to adapt-
ing the TM with recent data. Comparing Tables
3 and 4, for the bounded condition, adapting only
the TM achieved an absolute improvement of +1.24
BLEU over the static baseline for the final test point.
We get another absolute gain of +1.08 BLEU by al-
lowing the LM coverage to adapt as well. Using an
online, adaptive model gives a total gain of +2.32
BLEU over a static baseline that does not adapt.
6Although we batch retrain the LMs we could use an online
LM that incorporates new vocabulary from the input stream as
in Levenberg and Osborne (2009).
400
Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them.
Online: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles.
Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles.
Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen 
              Prinzipien mitzuwirken.
Static:  Our position is clear and we all know: we are against the war and the occupation of Iraq by the United States and the United    
             Kingdom, and we are calling for the immediate withdrawal of the besatzungsm?chte from this country.
Online: Our position is clear and well known: we are against the war and the occupation of Iraq by the United States and the United
             Kingdom, and we demand the immediate withdrawal of the occupying forces from this country .
Reference: Our position is clear and well known: we are against the war and the US-British occupation in Iraq and we demand the
                   immediate withdrawal of the occupying forces from that country.
Source: Unser Standpunkt ist klar und allseits bekannt: Wir sind gegen den Krieg und die Besetzung des Irak durch die USA und das   
              Vereinigte K?nigreich, und wir verlangen den unverz?glichen Abzug der Besatzungsm?chte aus diesem Land.
Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent
sentences. In both examples we get longer matching phrases in the online translation compared to the static one.
5 Related Work
5.1 Translation Model Domain Adaptation
Our work is related to domain adaptation for transla-
tion models. See, for example, Koehn and Schroeder
(2007) or Bertoldi and Federico (2009). Most tech-
niques center around using mixtures of translation
models. Once trained, these models generally never
change. They therefore fall under the batch training
regime. The focus of this work instead is on incre-
mental retraining and also on supporting bounded
memory consumption. Our experiments examine
updating model parameters in a single domain over
different periods in time. Naturally, we could also
use domain adaptation techniques to further improve
how we incorporate new samples.
5.2 Online EM for SMT
For stepwise online EM for SMT models, the only
prior work we are aware of is Liang and Klein
(2009), where variations of online EM were exper-
imented with on various NLP tasks including word
alignments. They showed application of sOEM can
produce quicker convergence compared to the batch
EM algorithm. However, the model presented does
not incorporate any unseen data, instead iterating
over a static data set multiple times using sOEM.
For Liang and Klein (2009) incremental retraining
is simply an alternative way to use a fixed training
set.
5.3 Streaming Language Models
Recent work in Levenberg and Osborne (2009) pre-
sented a streaming LM that was capable of adapt-
ing to an unbounded monolingual input stream in
constant space and time. The LM has the ability to
add or delete n-grams (and their counts) based on
feedback from the decoder after translation points.
The model was tested in an SMT setting and results
showed recent data benefited performance. How-
ever, adaptation was only to the LM and no tests
were conducted on the TM.
6 Conclusion and Future Work
We have presented an online EM approach for word
alignments. We have shown that, for a SMT system,
incorporating recent parallel data into a TM from an
input stream is beneficial to translation performance
compared to a traditional, static baseline.
Our strategy for populating the suffix array was
simply to use a first-in, first-out stack. For future
work we will investigate whether information pro-
vided by the incoming stream coupled with the feed-
back from the decoder allows for more sophisti-
cated adaptation strategies that reinforce useful word
alignments and delete bad or unused ones.
In the near future we also hope to test the online
EM setup in an application setting such as a com-
puter aided translation or crowdsourced generated
streams via Amazon?s Mechanical Turk.
401
Acknowledgements
Research supported by EuroMatrixPlus funded by
the European Commission, by the DARPA GALE
program under Contract Nos. HR0011-06-2-0001
and HR0011-06-C-0022, and the NSF under grant
IIS-0713448.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In WMT09: Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 182?189, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Olivier Cappe and Eric Moulines. 2009. Online EM al-
gorithm for latent data models. Journal Of The Royal
Statistical Society Series B, 71:593.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39:1?38.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In WMT09: Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, pages 135?139, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang and Dan Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319?327.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Ronald Rosenfeld. 1995. Optimizing lexical and n-gram
coverage via judicious use of linguistic data. In In
Proc. European Conf. on Speech Technology, pages
1763?1766.
Mikae?l Salson, Thierry Lecroq, Martine Le?onard, and
Laurent Mouchard. 2009. Dynamic extended suffix
arrays. Journal of Discrete Algorithms, March.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841, Morristown,
NJ, USA. Association for Computational Linguistics.
402
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338?346,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using paraphrases for improving first story detection in news and Twitter
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Victor Lavrenko
School of Informatics
University of Edinburgh
vlavrenk@inf.ed.ac.uk
Abstract
First story detection (FSD) involves identify-
ing first stories about events from a continuous
stream of documents. A major problem in this
task is the high degree of lexical variation in
documents which makes it very difficult to de-
tect stories that talk about the same event but
expressed using different words. We suggest
using paraphrases to alleviate this problem,
making this the first work to use paraphrases
for FSD. We show a novel way of integrat-
ing paraphrases with locality sensitive hashing
(LSH) in order to obtain an efficient FSD sys-
tem that can scale to very large datasets. Our
system achieves state-of-the-art results on the
first story detection task, beating both the best
supervised and unsupervised systems. To test
our approach on large data, we construct a cor-
pus of events for Twitter, consisting of 50 mil-
lion documents, and show that paraphrasing is
also beneficial in this domain.
1 Introduction
First story detection (FSD), sometimes also called
new event detection (NED), is the task of detecting
the first story about a new event from a stream of
documents. It began as one of the tasks in Topic
Detection and Tracking (TDT) (Allan, 2002) where
the overall goal of the project was to improve tech-
nologies related to event-based information organi-
zation tasks. Of the five TDT tasks, first story de-
tection is considered the most difficult one (Allan
et al, 2000a). A good FSD system would be very
useful for business or intelligence analysts where
timely discovery of events is crucial. With the sig-
nificant increase in the amount of information being
produced and consumed every day, a crucial require-
ment for a modern FSD system to be useful is effi-
ciency. This means that the system should be able
to work in a streaming setting where documents are
constantly coming in at a high rate, while still pro-
ducing good results. While previous work has ad-
dressed the efficiency (Petrovic? et al, 2010) aspect,
there has been little work on improving FSD perfor-
mance in the past few years. A major obstacle is the
high degree of lexical variation in documents that
cover the same event. Here we address this problem,
while keeping in mind the efficiency constraints.
The problem of lexical variation plagues many IR
and NLP tasks, and one way it has been addressed
in the past is through the use of paraphrases. Para-
phrases are alternative ways of expressing the same
meaning in the same language. For example, the
phrase he got married can be paraphrased as he tied
the knot. Paraphrases were already shown to help
in a number of tasks: for machine translation to
translate unknown phrases by translating their para-
phrases (Callison-Burch et al, 2006), for query ex-
pansion in information retrieval (Spa?rck Jones and
Tait, 1984; Jones et al, 2006), or for improving
question answering (Riezler et al, 2007). A much
more detailed discussion on the use of paraphrases
and ways to extract them is given in (Madnani and
Dorr, 2010). Here, we present the first work to use
paraphrases for improving first story detection. Us-
ing paraphrases, we are able to detect that some doc-
uments previously thought to be about new events
are actually paraphrases of the documents already
338
seen. Our approach is simple and we show a novel
way of integrating paraphrases with locality sen-
sitive hashing (LSH) (Indyk and Motwani, 1998).
This way we obtain a very efficient FSD system with
all the benefits of using paraphrases, while avoid-
ing computationally expensive topic modeling ap-
proaches such as Ahmed et al (2011).
First story detection was introduced as a task be-
fore the popularization of social media. Event de-
tection in social media, especially Twitter is a very
good fit: we cover a much larger set of events than
would be possible by using newswire, and the sto-
ries are reported in real time, often much sooner
than in news. Of course, social media carries ad-
ditional problems not found in traditional media: we
have to deal with huge amounts of data, the data is
very noisy (both due to spam and due to spelling
and grammar errors), and in the case of Twitter, doc-
uments are extremely short. There has been little
effort in solving these problems for FSD. Arguably
the main reason for this is the lack of a TDT-style
corpus for Twitter that researchers could use to test
their approaches. Here we build such a corpus and
use it to measure the performance of TDT systems
on Twitter.
Our main contributions are: i) we create a first
corpus of events on Twitter, ii) we show how to use
paraphrases in FSD, and how to combine it with
LSH to handle high-volume streams, iii) our unsu-
pervised system that uses paraphrases achieves the
highest reported results on the TDT5 corpus, beat-
ing both the supervised and unsupervised state of the
art, while still keeping a constant per-document time
complexity, and iv) we show that paraphrases also
help in Twitter, although less than in TDT.
2 Paraphrasing and FSD
2.1 Current approaches to efficient FSD
State-of-the-art FSD systems (Allan et al, 2000b)
use a fairly simple approach. Documents are repre-
sented as TF-IDF weighted vectors, their distance is
measured in terms of the cosine distance, and they
use a k-nearest neighbors clustering algorithm, with
k usually set to 1. The novelty score for a document
is the cosine distance to the nearest neighbor:
score(d) = 1? max
d??Dt
cos(d, d?). (1)
Dt is the set of all documents up to time t when
document d arrived.
Because the max in equation (1) takes O(|Dt|)
time to compute in the worst case, Petrovic? et al
(2010) introduced a way of using locality sensitive
hashing (LSH) to make this time O(1), while retain-
ing the same accuracy level. In particular, instead
of computing the max over the entire set Dt, like
in (1), they compute it over a smaller set S of poten-
tial nearest neighbors. The set S is the set of docu-
ments that collide with the current document under
a certain type of hash function:
S(x) = {y : hij(y) = hij(x), ?i ? [1..L],?j ? [1..k]},
(2)
where the hash functions hij are defined as:
hij(x) = sgn(uTijx), (3)
with the random vectors uij being drawn indepen-
dently for each i and j. The efficiency of this algo-
rithm stems from the fact that it can be shown that
the set S of potential nearest neighbors can be made
constant in size, while still containing the nearest
neighbor with high probability.
2.2 Paraphrases
There are several levels of paraphrasing ? lexical
paraphrases, where the relationship is restricted to
individual lexical items, phrasal paraphrases, where
longer phrases are considered, and sentential para-
phrases, where entire sentences are in a paraphrastic
relationship. Here we use the simplest form, lexi-
cal paraphrases, but our approach, described in sec-
tion 2.3, is general and it would be trivial to use
phrasal paraphrases in the same way ? we leave this
for future work.
We use three sources of paraphrases: Word-
net (Fellbaum, 1998), a carefully curated lexical
database of English containing synonym sets,
Microsoft Research paraphrase tables (Quirk et
al., 2004), a set of paraphrase pairs automatically
extracted from news texts, and syntactically-
constrained paraphrases from Callison-Burch
(2008) which are extracted from parallel text. We
also considered using paraphrases from Cohn et
al. (2008), but using them provided only minor
improvement over the baseline model. This is likely
due to the small size of that corpus (a total of 7
339
thousand pairs). We do not show results for this
paraphrase corpus in our results section.
Wordnet paraphrases contained 150 thousand
word pairs extracted from Wordnet?s synsets, where
all the pairs of words within one synset were con-
sidered to be paraphrases. MSR paraphrases were
extracted from the phrase tables provided by MSR.
Two words were considered paraphrases if they were
aligned at least once in the most probable alignment,
with the probability of both backward and forward
alignment of at least 0.2. In our initial experiments
we varied this threshold and found it has little ef-
fect on results. Using this method, we extracted
50 thousand paraphrase pairs. Finally, we use the
method of Callison-Burch (2008) to extract syntac-
tically constrained paraphrases from a parallel cor-
pus. This method requires that phrases and their
paraphrases be the same syntactic type, and has been
shown to substantially improve the quality of ex-
tracted paraphrases (Callison-Burch, 2008). We ex-
tracted paraphrases for all the words that appeared
in the MSR paraphrase corpus, and then kept all the
pairs that had the paraphrase probability of at least
0.2. This way, we extracted 48 thousand pairs. All
three resources we use are very different: they come
from different domains (news text, legal text, gen-
eral English), and they have very little overlap (less
than 5% of pairs are shared by any two resources).
2.3 Efficient paraphrasing in FSD
In this section, we explain how to use paraphrases
in a first story detection system. We account for
paraphrases by changing how we compute the co-
sine in equation (1). Because the cosine measure
depends on the underlying inner product, we change
the way the inner product is computed. We model
paraphrasing by using a binary word-to-word ma-
trix of paraphrases Q. An entry of 1 at row i and
column j in the matrix indicates that words i and j
are paraphrases of each other.1 Note, however, that
our approach is not limited to using single words ? if
the document representation includes n-grams with
n > 1, the matrix Q can contain phrases, and thus
we can capture non-compositional paraphrases like
1This is of course a simplification ? in general, one might
like the entries in the matrix to be real numbers corresponding
to the probability that the two words are paraphrases. We leave
this for future work.
he died ? he kicked the bucket. We use the matrix
Q to define a new inner product space:2
?x,y?Q = y
TQx. (4)
This way of using paraphrases basically achieves ex-
pansion of the terms in documents with their para-
phrases. Thus, if two documents have no terms in
common, but one has the term explosion and the
other has the term blast, by knowing that the two
terms are paraphrases, their similarity will be differ-
ent from zero, which would have been the case if no
paraphrasing was used. Alternatively, the new inner
product in equation (4) can also be seen as introduc-
ing a linear kernel.
One problem with using Q as defined in (4) is that
it is not very suitable for use in an online setting.
In particular, if documents come in one at a time
and we have to store each one, only for it to be re-
trieved at some later point, simply storing them and
computing the inner product as in (4) would lead to
frequent matrix-vector multiplications. Even though
Q is sparse, these multiplications become expen-
sive when done often, as is the case in first story
detection. We thus have to store a modified docu-
ment vector x, call it x?, such that when we compute
?x?,y?? we get ?x,y?Q. Note that the inner product
between x? and y? is computed in the original inner
product space. It is clear that by using:
x? = Q1/2x (5)
we have achieved our goal: ?x?,y?? = y?Tx? =
(Q1/2y)T (Q1/2x) = (yTQ1/2
T
)(Q1/2x) =
yTQx = ?x,y?Q. Again, if we view equation (4)
as defining a kernel, we can think of equation (5)
as performing an explicit mapping into the feature
space defined by the kernel. Because ours is a linear
kernel, performing this mapping is fairly efficient.
Unfortunately, the square root of the paraphrasing
matrix Q is in general dense (and can even contain
complex entries), which would make our approach
infeasible in practice because we would have to ex-
pand every document with all (or a very large num-
ber of) the words in the vocabulary. Thus, we have to
2Equation (4) does not define a proper inner product in the
strict technical sense because the positive definiteness property
does not hold. However, because vectors x and y in practice
always have positive entries, equation (4) behaves like a proper
inner product for all practical purposes.
340
approximate Q1/2 with a sparse matrix, preferably
one that is as sparse as the original Q matrix. To this
end, we introduce the following approximation:
Q?1/2ij =
Qij
??
k(Qik +Qkj)/2
(6)
To see how we arrive at this approximation, consider
the paraphrase matrix Q. If there was no polysemy
in the language, Q would be a block matrix, where
each non-zero submatrix would correspond to a sin-
gle meaning. The square root of such a matrix would
be given exactly by (6). While this approximation
is somewhat simplistic, it has two major advantages
over the exact Q1/2: i) it is very easy to compute
and, with proper implementation, takes O(n2) time,
as opposed to O(n3) for Q1/2, making it scalable to
very large matrices, and ii) matrix Q?1/2 is guaran-
teed to be as sparse as Q, whereas Q1/2 will in most
cases become dense, which would make it unusable
in real applications.
2.4 Locality-sensitive hashing with
paraphrasing
Here we explain how to integrate paraphrasing with
efficient FSD, using LSH described in section 2.1.
As we mentioned before, a single hash function hij
in the original LSH scheme hashes the vector x to:
h(x) = sgn(uTx), (7)
where u is a (dense) random vector. If we want to
use paraphrases with LSH, we simply change the
hash function to
h1(x) = sgn(uT (Q?1/2x)). (8)
It is not difficult to show that by doing this, the LSH
bounds for probability of collision hold in the new
inner product space defined by the matrix Q. We
omit this proof due to space constraints.
Space efficient LSH. While LSH can significantly
reduce the running time, it is fairly expensive
memory-wise. This memory overhead is due to the
random vectors u being very large. To solve this
problem, (Van Durme and Lall, 2010) used a hash-
ing trick for space-efficient storing of these vectors.
They showed that it is possible to project the vectors
onto a much smaller random subspace, while still re-
taining good properties of LSH. They proposed the
following hash function for a vector x:
h2(x) = sgn(uT (Ax)), (9)
where A is a random binary matrix with exactly one
non-zero element in each column. This approach
guarantees a constant space use which is bounded by
the number of rows in the A matrix. Here we show
that our paraphrasing approach can be easily used
together with this space-saving approach by defin-
ing the following hash function for x:
h3(x) = sgn(uT (AQ?1/2x)). (10)
This way we get the benefits of the hashing trick
(the constant space use), while also being able to use
paraphrases. The hash function in (10) is the actual
hash function we use in our system. Together with
the heuristics from Petrovic? et al (2010), it guaran-
tees that our FSD system will use constant space and
will take constant time to process each document.
3 Twitter Event Corpus
3.1 Event detection on Twitter
As we mentioned before, research on event detec-
tion in social media is hampered by the lack of a
corpus that could be used to measure performance.
The need for a standard corpus is evident from the
related work on event detection in Twitter. For ex-
ample, (Petrovic? et al, 2010) address the scaling
problem in social media and present a system that
runs in constant time per document, but the evalua-
tion of their system on Twitter data was limited to
very high-volume events. The only attempt in creat-
ing a corpus of events for Twitter that we are aware
of was presented in Becker et al (2011). Unfor-
tunately, that corpus is not suitable for FSD eval-
uation for two main reasons: i) the events were
picked from the highest-volume events identified by
the system (similar to what was done in Petrovic? et
al. (2010)), introducing not only a bias towards high-
volume events, but also a bias toward the kinds of
events that their system can detect, and ii) the au-
thors only considered tweets by users who set their
location to New York, which introduces a strong bias
towards the type of events that can appear in the cor-
pus. While these problems were not relevant to the
341
work of (Becker et al, 2011) because the corpus was
only used to compare different cluster representa-
tion techniques, they would certainly pose a serious
problem if we wanted to use the corpus to compare
FSD systems. In this paper we present a new corpus
of tweets with labeled events by taking a very sim-
ilar approach to that taken by NIST when creating
the TDT corpora.
3.2 Annotating the Tweets
In this section we describe the annotation process for
our event corpus. Note that due to Twitter?s terms of
service, we distribute the corpus as a set of tweet
IDs and the corresponding annotations ? users will
have to crawl the tweets themselves, but this can
be easily done using any one of the freely available
crawlers for Twitter. This is the same method that
the TREC microblog track3 used to distribute their
data. All our Twitter data was collected from the
streaming API4 and consists of tweets from begin-
ning of July 2011 until mid-September 2011. After
removing non-English tweets, our corpus consists of
50 million tweets.
In our annotation process, we have adopted the
approach used by the National Institute of Standards
and Technology (NIST) in labeling the data for TDT
competitions. First, we defined a set of events that
we want to find in the data, thus avoiding the bias of
using events that are the output of any particular sys-
tem. We choose the events from the set of important
events for our time period according to Wikipedia.5
Additionally, we used common knowledge of impor-
tant events at that time to define more events. In
total, we define 27 events, with an average of 112
on-topic tweets. This is comparable to the first TDT
corpus which contained 25 events and average of 45
on-topic documents. However, in terms of the to-
tal number of documents, our corpus is three orders
of magnitude larger than the first TDT corpus, and
two orders of magnitude larger than the biggest TDT
corpus (TDT5). Our corpus contains very different
events, such as the death of Amy Winehouse, down-
grading of US credit rating, increasing of US debt
ceiling, earthquake in Virginia, London riots, terror-
ist attacks in Norway, Google announcing plans to
3http://trec.nist.gov/data/tweets/
4https://stream.twitter.com/
5http://en.wikipedia.org/wiki/2011
buy Motorola Mobility, etc. The event with the most
on-topic tweets had over 1,000 tweets (death of Amy
Winehouse), and the smallest event had only 2 on-
topic tweets (arrest of Goran Hadzic).
We faced the same problems as NIST when label-
ing the events ? there were far too many stories to ac-
tually read each one and decide which (if any) events
it corresponds to. In order to narrow down the set
of candidates for each event, we use the same pro-
cedure as used by NIST. The annotator would first
read a description of the event, and from that de-
scription compile a set of keywords to retrieve pos-
sibly relevant tweets. He would then read through
this set, labeling each tweet as on- or off-topic, and
also adding new keywords for retrieving a new batch
of tweets. After labeling all the tweets in one batch,
the newly added keywords were used to retrieve the
next batch, and this procedure was repeated until no
new keywords were added. Unlike in TDT, however,
when retrieving tweets matching a keyword, we do
not search through the whole corpus, as this would
return far too many candidates than is feasible to la-
bel. Instead, we limit the search to a time window of
one day around the time the event happened.
Finally, the annotator guidelines contained some
Twitter-specific instructions. Links in tweets were
not taken into account (the annotator would not click
on links in the tweets), but retweets were (if the
retweet was cut off because of the 140 character
limit, the annotator would label the original tweet).
Furthermore, hashtags were taken into account, so
tweets like #Amywinehouseisdead were labeled as
normal sentences. Also, to be labeled on-topic, the
tweet would have to explicitly mention the event and
the annotator should be able to infer what happened
from the tweet alne, without any outside knowl-
edge. This means that tweets like Just heard about
Lokomotiv, this is a terrible summer for hockey! are
off topic, even though they refer to the plane crash
in which the Lokomotiv hockey team died.
In total, our corpus contains over 50 million
tweets, of which 3035 tweets were labeled as be-
ing on-topic for one of the 27 events. While search-
ing for first tweets (i.e., tweets that first mention an
event), fake first tweets were sometimes discovered.
For example, in the case of the death of Richard
Bowes (victim of London riots), a Telegraph jour-
nalist posted a tweet informing of the man?s death
342
more than 12 hours before he actually died. This
tweet was later retracted by the journalist for being
incorrect, but the man then died a few hours later.
Cases like this were labeled off-topic.
4 Experiments
4.1 Evaluation
In the official TDT evaluation, each FSD system is
required to assign a score between 0 and 1 to ev-
ery document upon its arrival. Lower scores corre-
spond to old stories, and vice versa. Evaluation is
then carried out by first sorting all stories accord-
ing to their scores and then performing a threshold
sweep. For each value of the threshold, stories with
a score above the threshold are considered new, and
all others are considered old. Therefore, for each
threshold value, one can compute the probability of
a false alarm, i.e., probability of declaring a story
new when it is actually not, and the miss probability,
i.e., probability of declaring a new story old (miss-
ing a new story). Using the false alarm and the miss
rate, the cost Cdet is defined as follows:
Cdet = Cmiss?Pmiss?Ptarget+CFA?PFA?Pnon?target ,
where Cmiss and CFA are costs of miss and false
alarm (0.02 and 0.98, respectively), Pmiss and PFA
are the miss and false alarm rate, and Ptarget and
Pnon?target are the prior target and non-target prob-
abilities. Different FSD systems are compared on
the minimal cost Cmin , which is the minimal value
of Cdet over all threshold values. This means that in
FSD evaluation, a lower value of Cmin indicates a
better system.
4.2 TDT results
For the TDT experiments, we use the English por-
tion of TDT-5 dataset, consisting of 126 topics in
278,108 documents. Similar to (Petrovic? et al,
2010), we compare our approach to a state-of-the-
art FSD system, namely the UMass system (Allan et
al., 2000b). This system always scored high in the
TDT competitions and is known to perform at least
as well as other systems that also took part in the
competition (Fiscus, 2001). Our system is based on
the streaming FSD system of (Petrovic? et al, 2010)
which has a constant per-document time complex-
ity. We use stemming (Porter, 1980) and, the same
as (Petrovic? et al, 2010), we use 13 bits per key
and 70 hash tables for LSH. Additionally, we use the
hashing trick described in section 2.4 with a pool of
size 218. Paraphrasing is implemented in this system
as described in section 2.4.
While the UMass system was among the best sys-
tems that took part in the TDT competitions, there
has been research in event detection since the com-
petitions stopped. Recent work on event detec-
tion includes a hybrid clustering and topic model
with rich features such as entities, time, and top-
ics (Ahmed et al, 2011). We do not compare our
system to Ahmed et al (2011) because in terms of
the numerical Cmin score, their approach does not
outperform the UMass system. This is not surpris-
ing as the primary goal in Ahmed et al (2011) was
not to improve FSD performance, but rather to cre-
ate storylines and support structured browsing.
We compare our approach to the best reported re-
sult in the literature on the TDT5 data. To the best of
our knowledge, the highest reported results in FSD
come from a supervised system described in Ku-
maran and Allan (2005). This system uses an SVM
classifier with the features being FSD scores from
unsupervised systems (the authors used scores com-
puted in the same way as is done in the UMass sys-
tem) computed using i) full text, ii) only named en-
tities in the document, and iii) only topic terms. The
classifier was trained on TDT3 and TDT4 corpora
and tested on TDT5.
Table 1 shows the results for TDT5 data. UMass
1000 is the run that was submitted as the official
run in the TDT competition.6 We can see that us-
ing paraphrases improves the results over the unsu-
pervised state of the art, regardless of which source
of paraphrasing is used. However, it is clear that
not all types of paraphrases are equally helpful. In
particular, the automatically extracted paraphrases
from Callison-Burch (2008) seem to be the most
helpful, and by using them our unsupervised sys-
tem is able to beat even the best known supervised
FSD system. This is a very promising result because
it indicates that we can use automatically extracted
paraphrases and do not have to rely on hand-crafted
resources like Wordnet as our source of paraphrases.
6Our experiments, and experiments in Allan et al (2000b)
showed that keeping full documents does not improve results,
while increasing running time.
343
System Cmin
UMass 100 0.721
UMass 1000 0.706
Best supervised system 0.661
Wordnet 0.657
MSR Paraphrases 0.642
Syntactic paraphrases 0.575
Table 1: TDT FSD results for different systems, lower is
better. The number next to UMass system indicates the
number of features kept for each document (selected ac-
cording to their TFIDF). All paraphrasing systems work
with full documents. Results for the best supervised sys-
tem were taken from Kumaran and Allan (2005).
The difference between our system and the UMass
system is significant at p = 0.05 using a paired t-test
over the individual topic costs. We were not able to
test significance against the supervised state-of-the-
art because we did not have access to this system. In
terms of efficiency, our approach is still O(1), like
the approach in Petrovic? et al (2010), but in practice
it is somewhat slower because hashing the expanded
documents takes more time. We measured the run-
ning time of our system, and it is 3.5 times slower
than the basic approach of Petrovic? et al (2010), but
also 3.5 times faster than the UMass system, while
outperforming both of these systems.
How does quality of paraphrases affect results?
We have shown that using automatically obtained
paraphrases to expand documents is beneficial in
first story detection. Because there are different
ways of extracting paraphrases, some of which are
targeted more towards recall, and some towards pre-
cision, we want to know which techniques would be
more suitable to extract paraphrases for use in FSD.
Here, precision is the ratio between extracted word
pairs that are actual paraphrases and all the word
pairs extracted, and recall is the ratio between ex-
tracted word pairs that are actual paraphrases, and
all the possible paraphrase pairs that could have been
extracted. In this experiment we focus on the syn-
tactic paraphrases which yielded the best results. To
lower recall, we randomly remove paraphrase pairs
from the corpus, and to lower precision, we add ran-
dom paraphrase pairs to our table. All the results
are shown in Table 2. Numbers next to precision
Paraphrasing resource Cmin
Precision 0.1 0.603
Precision 0.2 0.672
Precision 0.3 0.565
Precision 0.4 0.603
Precision 0.5 0.626
Recall 0.9 0.609
Recall 0.8 0.606
Recall 0.7 0.632
Recall 0.6 0.610
Recall 0.5 0.626
Table 2: Effect of paraphrase precision and recall on FSD
performance. Numbers next to recall and precision indi-
cate the sampling rate and the proportion of added ran-
dom pairs, respectively.
and recall indicate the proportion of added random
pairs and the proportion of removed pairs, respec-
tively (e.g., recall 0.4 means that 40% of pairs were
removed from the original resource). We can see
that the results are much more stable with respect to
recall ? there is an initial drop in performance when
we remove the first 10% of paraphrases, but after
that removing more paraphrases does not affect per-
formance very much. On the other hand, changing
the precision has a bigger impact on the results. For
example, we can see that our system using a para-
phrase corpus with 30% of pairs added at random
performs even better than the system that uses the
original corpus. On the other hand, adding 20% of
random pairs performs substantially worse than the
original corpus. These results show that it is more
important for the paraphrases to have good precision
than to have good recall.
4.3 Twitter results
Because the Twitter event corpus that we use con-
sists of over 50 million documents, we cannot
use the UMass system here due to its linear per-
document time complexity. Instead, our baseline
system here is the FSD system of (Petrovic? et
al., 2010), without any paraphrasing. This sys-
tem uses the same approach as the UMass system,
and (Petrovic? et al, 2010) showed that it achieves
very similar results. This means that our baseline, al-
344
though coming from a different system, is still state-
of-the-art. We make some Twitter-specific modifi-
cation to the baseline system that slightly improve
the results. Specifically, the baseline uses no stem-
ming, ignores links, @-mentions, and treats hash-
tags as normal words (i.e., removes the leading ?#?
character). While removing links and @-mentions
was also done in (Petrovic? et al, 2010), our pre-
liminary experiments showed that keeping hashtags,
only without the hash sign improves the results. Ad-
ditionally, we limit the number of documents in a
bucket to at most 30% of the expected number of
collisions for a single day (we assume one million
documents per day).
Results for the different systems are shown in Ta-
ble 3. First, we can see that not using stemming is
much better than using it, which is the opposite from
what is the case in TDT. Second, we can see that the
improvements from using paraphrases that we had in
TDT data are different here. Syntactic paraphrases
and the MSR paraphrases do not help, whereas the
paraphrases extracted from Wordnet did improve the
results, although the gains are not as large as in TDT.
A paired t-test revealed that none of the differences
between the baseline system and the systems that
use paraphrases were significant at p = 0.05.
To gain more insight into why the results are dif-
ferent here, we look at the proportion of words in
the documents that are being paraphrased, i.e., the
coverage of the paraphrasing resource. We can see
from Table 4 that the situation in TDT and Twit-
ter is very different. Coverage of MSR and syntac-
tic paraphrases was lower in Twitter than in TDT,
whereas Wordnet coverage was better on Twitter.
While it seems that the benefits of using paraphrases
in Twitter are not as clear as in news, our efficient
approach enables us to answer questions like these,
which could not be answered otherwise.
To illustrate how paraphrases help detect old
tweets, consider the tweet According to Russian avi-
ation officials, two passengers survived the crash,
but are in critical condition. Before paraphrasing,
the closest tweet returned by our system was Shaz-
aad Hussein has died in Birmingham after being
run over, two others are in critical condition, which
is not very related. After applying paraphrasing,
in particular knowing that officials is a paraphrase
of authorities, the closest tweet returned was Some
System Cmin
Baseline system (stemming) 0.756
Baseline system (no stemming) 0.694
Wordnet 0.679
MSR Paraphrases 0.739
Syntactic paraphrases 0.729
Table 3: Twitter FSD results for different systems, lower
is better. The baseline system is that of (Petrovic? et al,
2010).
Paraphrases Coverage TDT (%) Coverage Twitter (%)
Wordnet 52.5 56.1
MSR 33.5 31.0
Syntactic 35.6 31.7
Table 4: Coverage of different resources.
Russian authorities are reporting one survivor, oth-
ers are saying there are three. There were 37 total
on board, which is on the same event. There are also
cases where paraphrases hurt. For example, before
paraphrasing the tweet Top News #debt #deal #ceil-
ing #party had the nearest neighbor New debt ceiling
deal explained, whereas after paraphrasing, because
the word roof is a paraphrase of ceiling, the nearest
neighbor was The roof the roof the roof is on fire!.
Cases like this could be fixed by looking at the con-
text of the word, but we leave this for future work.
5 Conclusion
We present a way of incorporating paraphrase infor-
mation in a streaming first story detection system.
To the best of our knowledge, this is the first work
to use paraphrases in first story detection, and also
the first work to combine paraphrases with locality-
sensitive hashing to achieve fast retrieval of doc-
uments that are written with different words, but
talk about the same thing. We compare different
sources of paraphrases and show that our unsuper-
vised FSD system that uses syntactically constrained
paraphrases achieves state-of-the-art results, beating
both the best supervised and unsupervised systems.
To test our approach on very large data, we construct
a corpus of events for Twitter. Our approach scales
well on this data both in terms of time and mem-
ory, and we show that paraphrases again help, but
345
this time the paraphrase sources yield different im-
provements from TDT data. We find that this differ-
ence can be explained by the different coverage of
the paraphrasing resources.
Acknowledgments
The authors would like to thank Mirella Lapata
for her help with paraphrasing resources. We also
acknowledge financial support from EPSRC grant
EP/J020664/1.
References
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alex Smola, and Choon Hui Teo. 2011. Unified
analysis of streaming news. In Proceedings of WWW,
pages 267?276. ACM.
James Allan, Victor Lavrenko, and Hubert Jin. 2000a.
First story detection in tdt is hard. In Proceedings of
the CIKM, pages 374?381. ACM.
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000b. Detections, bounds, and timelines:
Umass and tdt-3. In Proceedings of Topic Detection
and Tracking Workshop, pages 167?174.
James Allan. 2002. Topic detection and tracking: event-
based information organization. Kluwer Academic
Publishers.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Se-
lecting quality twitter content for events. In Proceed-
ings of ICWSM.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of NAACL, pages
17?24. Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 196?205. Asso-
ciation for Computational Linguistics.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. The MIT press.
Jonathan Fiscus. 2001. Overview of results (nist). In
Proceedings of the TDT 2001 Workshop.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In STOC ?98: Proceedings of the thirti-
eth annual ACM symposium on Theory of computing,
pages 604?613, New York, NY, USA. ACM.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th international conference on
World Wide Web, pages 387?396. ACM.
Giridhar Kumaran and James Allan. 2005. Using names
and topics for new event detection. In Proceedings
of EMNLP, pages 121?128. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of the 11th annual confer-
ence of the North American Chapter of the ACL, pages
181?189.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In proceedings of EMNLP, pages 142?149.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL, volume 45, page 464.
Karen Spa?rck Jones and John Tait. 1984. Automatic
search term variant generation. Journal of Documen-
tation, 40(1):50?66.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In In Pro-
ceedings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 4?11.
346
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1027?1035,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Metrics for MT
Alexandra Birch Miles Osborne
a.birch@ed.ac.uk miles@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
One of the major challenges facing statistical
machine translation is how to model differ-
ences in word order between languages. Al-
though a great deal of research has focussed
on this problem, progress is hampered by the
lack of reliable metrics. Most current metrics
are based on matching lexical items in the
translation and the reference, and their abil-
ity to measure the quality of word order has
not been demonstrated. This paper presents
a novel metric, the LRscore, which explic-
itly measures the quality of word order by
using permutation distance metrics. We show
that the metric is more consistent with human
judgements than other metrics, including the
BLEU score. We also show that the LRscore
can successfully be used as the objective func-
tion when training translation model parame-
ters. Training with the LRscore leads to output
which is preferred by humans. Moreover, the
translations incur no penalty in terms of BLEU
scores.
1 Introduction
Research in machine translation has focused broadly
on two main goals, improving word choice and im-
proving word order in translation output. Current
machine translation metrics rely upon indirect meth-
ods for measuring the quality of the word order, and
their ability to capture the quality of word order is
poor (Birch et al, 2010).
There are currently two main approaches to eval-
uating reordering. The first is exemplified by the
BLEU score (Papineni et al, 2002), which counts
the number of matching n-grams between the refer-
ence and the hypothesis. Word order is captured by
the proportion of longer n-grams which match. This
method does not consider the position of match-
ing words, and only captures ordering differences
if there is an exact match between the words in the
translation and the reference. Another approach is
taken by two other commonly used metrics, ME-
TEOR (Banerjee and Lavie, 2005) and TER (Snover
et al, 2006). They both search for an alignment be-
tween the translation and the reference, and from
this they calculate a penalty based on the number
of differences in order between the two sentences.
When block moves are allowed the search space is
very large, and matching stems and synonyms in-
troduces errors. Importantly, none of these metrics
capture the distance by which words are out of order.
Also, they conflate reordering performance with the
quality of the lexical items in the translation, making
it difficult to tease apart the impact of changes. More
sophisticated metrics, such as the RTE metric (Pado?
et al, 2009), use higher level syntactic or semantic
analysis to determine the grammaticality of the out-
put. These approaches require annotation and can be
very slow to run. For most research, shallow metrics
are more appropriate.
We introduce a novel shallow metric, the Lexical
Reordering Score (LRscore), which explicitly mea-
sures the quality of word order in machine trans-
lations and interpolates it with a lexical metric.
This results in a simple, decomposable metric which
makes it easy for researchers to pinpoint the effect
of their changes. In this paper we show that the
LRscore is more consistent with human judgements
1027
than other metrics for five out of eight different lan-
guage pairs. We also apply the LRscore during Mini-
mum Error Rate Training (MERT) to see whether in-
formation on reordering allows the translation model
to produce better reorderings. We show that hu-
mans prefer the output of systems trained with the
LRscore 52.5% as compared to 43.9% when train-
ing with the BLEU score. Furthermore, training with
the LRscore does not result in lower BLEU scores.
The rest of the paper proceeds as follows. Sec-
tion 2 describes the reordering and lexical metrics
that are used and how they are combined. Section 3
presents the experiments on consistency with human
judgements and describes how to train the language
independent parameter of the LRscore. Section 4 re-
ports the results of the experiments on MERT. Fi-
nally we discuss related work and conclude.
2 The LRscore
In this section we present the LRscore which mea-
sures reordering using permutation distance metrics.
These reordering metrics have been demonstrated to
correlate strongly with human judgements of word
order quality (Birch et al, 2010). The LRscore com-
bines the reordering metrics with lexical metrics to
provide a complete metric for evaluating machine
translations.
2.1 Reordering metrics
The relative ordering of words in the source and tar-
get sentences is encoded in alignments. We can in-
terpret algnments as permutations which allows us
to apply research into metrics for ordered encodings
to measuring and evaluating reorderings. We use dis-
tance metrics over permutations to evaluate reorder-
ing performance. Figure 1 shows three permutations.
Each position represents a source word and each
value indicates the relative positions of the aligned
target words. In Figure 1 (a) represents the identity
permutation, which would result from a monotone
alignment, (b) represents a small reordering consist-
ing of two words whose orders are inverted, and (c)
represents a large reordering where the two halves
of the sentence are inverted in the target.
A translation can potentially have many valid
word orderings. However, we can be reasonably cer-
tain that the ordering of the reference sentence must
be acceptable. We therefore compare the ordering
(a) (1 2 3 4 5 6 7 8 9 10)
(b) (1 2 3 4 ?6 ?5 ?7 8 9 10)
(c) (6 7 8 9 10 ?1 2 3 4 5)
Figure 1. Three permutations: (a) monotone (b) with a
small reordering and (b) with a large reordering. Bullet
points highlight non-sequential neighbours.
of a translation with that of the reference sentence.
Where multiple references exist, we select the clos-
est, i.e. the one that gives the best score. The un-
derlying assumption is that most reasonable word
orderings should be fairly similar to the reference,
which is a necessary assumption for all automatic
machine translation metrics.
Permutations encode one-one relations, whereas
alignments contain null alignments and one-many,
many-one and many-many relations. We make some
simplifying assumptions to allow us to work with
permutations. Source words aligned to null are as-
signed the target word position immediately after
the target word position of the previous source word.
Where multiple source words are aligned to the same
target word or phrase, a many-to-one relation, the
target ordering is assumed to be monotone. When
one source word is aligned to multiple target words,
a one-to-many relation, the source word is assumed
to be aligned to the first target word. These simplifi-
cations are chosen so as to reduce the alignment to a
bijective relationship without introducing any extra-
neous reorderings, i.e. they encode a basic monotone
ordering assumption.
We choose permutation distance metrics which
are sensitive to the number of words that are out
of order, as humans are assumed to be sensitive to
the number of words that are out of order in a sen-
tence. The two permutations we refer to, pi and ?,
are the source-reference permutation and the source-
translation permutation. The metrics are normalised
so that 0 means that the permutations are completely
inverted, and 1 means that they are identical. We re-
port these scores as percentages.
2.1.1 Hamming Distance
The Hamming distance (Hamming, 1950) mea-
sures the number of disagreements between two per-
mutations. It is defined as follows:
dh(pi, ?) = 1?
?n
i=1 xi
n
, xi =
{
0 if pi(i) = ?(i)
1 otherwise
1028
Eg. BLEU METEOR TER dh dk
(a) 100.0 100.0 100.0 100.0 100.0
(b) 61.8 86.9 90.0 80.0 85.1
(c) 81.3 92.6 90.0 0.0 25.5
Table 1. Metric scores for examples in Figure 1 which are
calculated by comparing the permutations to the identity.
All metrics are adjusted so that 100 is the best score and
0 the worst.
where n is the length of the permutation. The
Hamming distance is the simplest permutation dis-
tance metric and is useful as a baseline. It has no
concept of the relative ordering of words.
2.1.2 Kendall?s Tau Distance
Kendall?s tau distance is the minimum number
of transpositions of two adjacent symbols necessary
to transform one permutation into another (Kendall,
1938). It represents the percentage of pairs of ele-
ments which share the same order between two per-
mutations. It is defined as follows:
dk(pi, ?) = 1?
?
?n
i=1
?n
j=1 zij
Z
where zij =
{
1 if pi(i) < pi(j) and ?(i) > ?(j)
0 otherwise
Z =
(n2 ? n)
2
Kendalls tau seems particularly appropriate for
measuring word order differences as the relative or-
dering words is taken into account. However, most
human and machine ordering differences are much
closer to monotone than to inverted. The range of
values of Kendall?s tau is therefore too narrow and
close to 1. For this reason we take the square root
of the standard metric. This adjusted dk is also
more correlated with human judgements of reorder-
ing quality (Birch et al, 2010).
We use the example in Figure 1 to highlight the
problem with current MT metrics, and to demon-
strate how the permutation distance metrics are cal-
culated. In Table 1 we present the metric results for
the example permutations. The metrics are calcu-
lated by comparing the permutation string with the
monotone permutation. (a) receives the best score
for all metrics as it is compared to itself. BLEU
and METEOR fail to recognise that (b) represents a
small reordering and (c) a large reordering and they
assign a lower score to (b). The reason for this is that
they are sensitive to breaks in order, but not to the
actual word order differences. BLEU matches more
n-grams for (c) and consequently assigns it a higher
score. METEOR counts the number of blocks that
the translation is broken into, in order to align it with
the source. (b) is aligned using four blocks, whereas
(c) is aligned using only two blocks. TER counts the
number of edits, allowing for block shifts, and ap-
plies one block shift for each example, resulting in
an equal score for (b) and (c). Both the Hamming
distance dh and the Kendall?s tau distance dk cor-
rectly assign (c) a worse score than (b). Note that
for (c), the Hamming distance was not able to re-
ward the permutation for the correct relative order-
ing of words within the two large blocks and gave
(c) a score of 0, whereas Kendall?s tau takes relative
ordering into account.
Wong and Kit (2009) also suggest a metric which
combines a word choice and a word order compo-
nent. They propose a type of F-measure which uses
a matching function M to calculate precision and
recall. M combines the number of matched words,
weighted by their tfidf importance, with their posi-
tion difference score, and finally subtracting a score
for unmatched words. Including unmatched words
in the M function undermines the interpretation of
the supposed F-measure. The reordering component
is the average difference of absolute and relative
word positions which has no clear meaning. This
score is not intuitive or easily decomposable and it is
more similar to METEOR, with synonym and stem
functionality mixed with a reordering penalty, than
to our metric.
2.2 Combined Metric
The LRscore consists of a reordering distance met-
ric which is linearly interpolated with a lexical score
to form a complete machine translation evaluation
metric. The metric is decomposable because the in-
dividual lexical and reordering components can be
looked at individually. The following formula de-
scribes how to calculate the LRscore:
LRscore = ?R+ (1? ?)L (1)
The metric contains only one parameter, ?, which
balances the contribution of the reordering metric,
R, and the lexical metric, L. Here we use BLEU as
1029
the lexical metric. R is the average permutation dis-
tance metric adjusted by the brevity penalty and it is
calculated as follows:
R =
?
s?S dsBPs
|S|
(2)
Where S is a set of test sentences, ds is the reorder-
ing distance for a sentence and BP is the brevity
penalty.
The brevity penalty is calculated as:
BP =
{
1 if t > r
e1?r/t if t ? r
(3)
where t is the length of the translation, and r is the
closest reference length. If the reference sentence is
slightly longer than the translation, then the brevity
penalty will be a fraction somewhat smaller than
1. This has the effect of penalising translations that
are shorter than the reference. The brevity penalty
within the reordering component is necessary as the
distance-based metric would provide the same score
for a one word translation as it would for a longer
monotone translation. R is combined with a system
level lexical score.
In this paper we apply the BLEU score as the lex-
ical metric, as it is well known and it measures lexi-
cal precision at different n-gram lengths. We experi-
ment with the full BLEU score and the 1-gram BLEU
score, BLEU1, which is purely a measure of the pre-
cision of the word choice. The 4-gram BLEU score
includes some measure of the local reordering suc-
cess in the precision of the longer n-grams. BLEU
is an important baseline, and improving on it by in-
cluding more reordering information is an interest-
ing result. The lexical component of the system can
be any meaningful metric for a particular target lan-
guage. If a researcher was interested in morpholog-
ically rich languages, for example, METEOR could
be used. We use the LRscore to return sentence level
scores as well system level scores, and when doing
so the smoothed BLEU (Lin and Och, 2004) is used.
3 Consistency with Human Judgements
Automatic metrics must be validated by compar-
ing their scores with human judgements. We train
the metric parameter to optimise consistency with
human preference judgements across different lan-
guage pairs and then we show that the LRscore is
more consistent with humans than other commonly
used metrics.
3.1 Experimental Design
Human judgement of rank has been chosen as the of-
ficial determinant of translation quality for the 2009
Workshop on Machine Translation (Callison-Burch
et al, 2009). We used human ranking data from this
workshop to evaluate the LRscore. This consisted
of German, French, Spanish and Czech translation
systems that were run both into and out of English.
In total there were 52,265 pairwise rank judgements
collected.
Our reordering metric relies upon word align-
ments that are generated between the source and the
reference sentences, and the source and the trans-
lated sentences. In an ideal scenario, the transla-
tion system outputs the alignments and the refer-
ence set can be selected to have gold standard hu-
man alignments. However, the data that we use to
evaluate metrics does not have any gold standard
alignments and we must train automatic alignment
models to generate them. We used version two of
the Berkeley alignment model (Liang et al, 2006),
with the posterior threshold set at 0.5. Our Spanish-,
French- and German-English alignment models are
trained using Europarl version 5 (Koehn, 2005). The
Czech-English alignment model is trained on sec-
tions 0-2 of the Czech-English Parallel Corpus, ver-
sion 0.9 (Bojar and Zabokrtsky, 2009).
The metric scores are calculated for the test set
from the 2009 workshop on machine translation. It
consists of 2525 sentences in English, French, Ger-
man, Spanish and Czech. These sentences have been
translated by different machine translation systems
and the output submitted to the workshop. The sys-
tem output along with human evaluations can be
downloaded from the web1.
The BLEU score has five parameters, one for each
n-gram, and one for the brevity penalty. These pa-
rameters are set to a default uniform value of one.
METEOR has 3 parameters which have been trained
for human judgements of rank (Lavie and Agarwal,
2008). METEOR version 0.7 was used. The other
baseline metric used was TER version 0.7.25. We
adapt TER by subtracting it from one, so that all
1http://www.statmt.org/wmt09/results.html
1030
metric increases mean an improvement in the trans-
lation. The TER metric has five parameters which
have not been trained.
Using rank judgements, we do not have absolute
scores and so we cannot compare translations across
different sentences and extract correlation statistics.
We therefore use the method adopted in the 2009
workshop on machine translation (Callison-Burch et
al., 2009). We ascertained how consistent the auto-
matic metrics were with the human judgements by
calculating consistency in the following manner. We
take each pairwise comparison of translation output
for single sentences by a particular judge, and we
recorded whether or not the metrics were consistent
with the human rank. I.e. we counted cases where
both the metric and the human judge agreed that one
system is better than another. We divided this by the
total number of pairwise comparisons to get a per-
centage. We excluded pairs which the human anno-
tators ranked as ties.
de-en es-en fr-en cz-en
dk 73.9 80.5 80.4 81.1
Table 2. The average Kendall?s tau reordering distance
between the test and reference sentences. 100 means
monotone thus de-en has the most reordering.
We present a novel method for setting the
LRscore parameter. Using multiple language pairs,
we train the parameter according to the amount of
reordering seen in each test set. The advantage of
this approach is that researchers do not need to train
the parameter for new language pairs or test do-
mains. They can simply calculate the amount of re-
ordering in the test set and adjust the parameter ac-
cordingly. The amount of reordering is calculated
as the Kendall?s tau distance between the source
and the reference sentences as compared to dummy
monotone sentences. The amount of reordering for
the test sentences is reported in Table 2. German-
English shows more reordering than other language
pairs as it has a lower dk score of 73.9. The language
independent parameter (?) is adjusted by applying
the reordering amount (dk) as an exponent. ? is al-
lowed to takes values of between 0 and 1. This works
in a similar way to the brevity penalty. With more re-
ordering, the dk becomes smaller which leads to an
increase in the final value of ?. ? represents the per-
centage contribution of the reordering component in
the LRscore:
? = ?dk (4)
The language independent parameter ? is trained
once, over multiple language pairs. This procedure
optimises the average of the consistency results
across the different language pairs. We use greedy
hillclimbing in order to find the optimal setting. As
hillclimbing can end up in a local minima, we per-
form 20 random restarts, and retaining only the pa-
rameter value with the best consistency result.
3.2 Results
Table 3 reports the optimal consistency of the
LRscore and baseline metrics with human judge-
ments for each language pair. The LRscore vari-
ations are named as follows: LR refers to the
LRscore, ?H? refers to the Hamming distance and
?K? to Kendall?s tau distance. ?B1? and ?B4? refer
to the smoothed BLEU score with the 1-gram and
the complete scores. Table 3 shows that the LRscore
is more consistent with human judgement for 5 out
of the 8 language pairs. This is an important result
which shows that combining lexical and reordering
information makes for a stronger metric than the
baseline metrics which do not have a strong reorder-
ing component.
METEOR is the most consistent for the Czech-
English and English-Czech language pairs, which
have the least amount of reordering. METEOR lags
behind for the language pairs with the most reorder-
ing, the German-English and English-German pairs.
Here LR-KB4 is the best metric, which shows that
metrics which are sensitive to the distance words are
out of order are more appropriate for situations with
a reasonable amount of reordering.
4 Optimising Translation Models
Automatic metrics are useful for evaluation, but they
are essential for training model parameters. In this
section we apply the LRscore as the objective func-
tion in MERT training (Och, 2003). MERT min-
imises translation errors according to some auto-
matic evaluation metric while searching for the best
parameter settings over the N-best output. A MERT
trained model is likely to exhibit the properties that
1031
Metric de-en es-en fr-en cz-en en-de en-es en-fr en-cz ave
METEOR 58.6 58.3 58.3 59.4 52.6 55.7 61.2 55.6 57.5
TER 53.2 50.1 52.6 47.5 48.6 49.6 58.3 45.8 50.7
BLEU1 56.1 57.0 56.7 52.5 52.1 54.2 62.3 53.3 55.6
BLEU 58.7 55.5 57.7 57.2 54.1 56.7 63.7 53.1 57.1
LR-HB1 59.7 60.0 58.6 53.2 54.6 55.6 63.7 54.5 57.5
LR-HB4 60.4 57.3 58.7 57.2 54.8 57.3 63.3 53.8 57.9
LR-KB1 60.4 59.7 58.0 54.0 54.1 54.7 63.4 54.9 57.5
LR-KB4 61.0 57.2 58.5 58.6 54.8 56.8 63.1 55.0 58.7
Table 3. The percentage consistency between human judgements of rank and metrics. The LRscore variations (LR-*)
are optimised for average consistency across language pair (shown in right hand column). The bold numbers represent
the best consistency score per language pair.
the metric rewards, but will be blind to aspects of
translation quality that are not directly captured by
the metric. We apply the LRscore in order to im-
prove the reordering performance of a phrase-based
translation model.
4.1 Experimental Design
We hypothesise that the LRscore is a good metric
for training translation models. We test this by eval-
uating the output of the models, first with automatic
metrics, and then by using human evaluation. We
choose to run the experiment with Chinese-English
as this language pair has a large amount of medium
and long distance reorderings.
4.1.1 Training Setup
The experiments are carried out with Chinese-
English data from GALE. We use the official test
set of the 2006 NIST evaluation (1994 sentences).
For the development test set, we used the evalu-
ation set from the GALE 2008 evaluation (2010
sentences). Both development set and test set have
four references. The phrase table was built from
1.727M parallel sentences from the GALE Y2 train-
ing data. The phrase-based translation model called
MOSES was used, with all the default settings. We
extracted phrases as in (Koehn et al, 2003) by run-
ning GIZA++ in both directions and merging align-
ments with the grow-diag-final heuristic. We used
the Moses translation toolkit, including a lexicalised
reordering model. The SRILM language modelling
toolkit (Stolcke, 2002) was used with interpolated
Kneser-Ney discounting. There are three separate 3-
gram language models trained on the English side
of parallel corpus, the AFP part of the Gigaword
corpus, and the Xinhua part of the Gigaword cor-
LR-HB1 LR-HB4 LR-KB1 LR-KB4
26.40 07.19 43.33 26.23
Table 4. The parameter setting representing the % impact
of the reordering component for the different versions of
the LRscore metric.
pus. A 4 or 5-gram language model would have
led to higher scores for all objective functions, but
would not have changed the findings in this paper.
We used the MERT code available in the MOSES
repository (Bertoldi et al, 2009).
The reordering metrics require alignments which
were created using the Berkeley word alignment
package version 1.1 (Liang et al, 2006), with the
posterior probability to being 0.5.
We first extracted the LRscore Kendall?s tau dis-
tance from the monotone for the Chinese-English
test set and this value was 66.1%. This is far more re-
ordering than the other language pairs shown in Ta-
ble 2. We then calculated the optimal parameter set-
ting, using the reordering amount as a power expo-
nent. Table 4 shows the parameter settings we used
in the following experiments. The optimal amount of
reordering for LR-HB4 is low, but the results show
it still makes an important contribution.
4.1.2 Human Evaluation Setup
Human judgements of translation quality are nec-
essary to determine whether humans prefer sen-
tences from models trained with the BLEU score
or with the LRscore. There have been some recent
studies which have used the online micro-market,
Amazons Mechanical Turk, to collect human anno-
tations (Snow et al, 2008; Callison-Burch, 2009).
While some of the data generated is very noisy, in-
valid responses are largely due to a small number
of workers (Kittur et al, 2008). We use Mechanical
1032
Turk and we improve annotation quality by collect-
ing multiple judgements, and eliminating workers
who do not achieve a certain level of performance
on gold standard questions.
We randomly selected a subset of sentences from
the test set. We use 60 sentences each for compar-
ing training with BLEU to training with LR-HB4
and with LR-KB4. These sentences were between
15 and 30 words long. Shorter sentences tend to have
uninteresting differences, and longer sentences may
have many conflicting differences.
Workers were presented with a reference sen-
tence and two translations which were randomly
ordered. They were told to compare the transla-
tions and select their preferred translation or ?Don?t
Know?. Workers were screened to guarantee reason-
able judgement quality. 20 sentence pairs were ran-
domly selected from the 120 test units and anno-
tated as gold standard questions. Workers who got
less than 60% of these gold questions correct were
disqualified and their judgements discarded.
After disagreeing with a gold annotation, a worker
is presented with the gold answer and an expla-
nation. This guides the worker on how to perform
the task and motivates them to be more accurate.
We used the Crowdflower2 interface to Mechanical
Turk, which implements the gold functionality.
Even though experts can disagree on preference
judgements, gold standard labels are necessary to
weed out the poor standard workers. There were 21
trusted workers who achieved an average accuracy
of 91% on the gold. There were 96 untrusted work-
ers who averaged 29% accuracy on the gold. Their
judgements were discarded. Three judgements were
collected from the trusted workers for each of the
120 test sentences.
4.2 Results
4.2.1 Automatic Evaluation of MERT
In this experiment we demonstrate that the re-
ordering metrics can be used as learning criterion in
minimum error rate training to improve parameter
estimation for machine translation.
Table 5 reports the average of three runs of MERT
training with different objective functions. The lexi-
cal metric BLEU is used as an objective function in
2http://www.crowdflower.com
MetricsPPPPObj.Func. BLEU LR-HB4 LR-KB4 TER MET.
BLEU 31.1 32.1 41.0 60.7 55.5
LRHB4 31.1 32.2 41.3 60.6 55.7
LRKB4 31.0 32.2 41.2 61.0 55.8
Table 5. Average results of three different MERT runs for
different objective functions.
isolation, and also as part of the LRscore together
with the Hamming distance and Kendall?s tau dis-
tance. We test with these metrics, and we also report
the TER and METEOR scores for comparison.
The first thing we note in Table 5 is that we would
expect the highest scores when training with the
same metric as that used for evaluation as MERT
maximises the objective function on the develop-
ment data set. Here, however, when testing with
BLEU, we see that training with BLEU and with
LR-HB4 leads to equally high BLEU scores. The
reordering component is more discerning than the
BLEU score. It reliably increases as the word order
approaches that of the reference, whereas BLEU can
reports the same score for a large number of different
alternatives. This might make the reordering metric
easier to optimise, leading to the joint best scores
at test time. This is an important result, as it shows
that by training with the LRscore objective function,
BLEU scores do not decrease, which is desirable as
BLEU scores are usually reported in the field.
The LRscore also results in better scores when
evaluated with itself and the other two baseline met-
rics, TER and METEOR. Reordering and the lexi-
cal metrics are orthogonal information sources, and
this shows that combining them results in better per-
forming systems. BLEU has shown to be a strong
baseline metric to use as an objective function (Cer
et al, 2010), and so the LRscore performance in Ta-
ble 5 is a good result.
Examining the weights that result from the dif-
ferent MERT runs, the only notable difference is
that the weight of the distortion cost is considerably
lower with the LRscore. This shows more trust in
the quality of reorderings. Although it is interesting
to look at the model weights, any final conclusion on
the impact of the metrics on training must depend on
human evaluation of translation quality.
1033
Type Sentence
Reference silicon valley is still a rich area in the united states. the average salary in the area was us
$62,400 a year, which was 64% higher than the american average.
LR-KB4 silicon valley is still an affluent area of the united states, the regional labor with an average
annual salary of 6.24 million us dollars, higher than the average level of 60 per cent.
BLEU silicon valley is still in the united states in the region in an affluent area of the workforce,
the average annual salary of 6.24 million us dollars, higher than the average level of 60 per
cent
Table 7. A reference sentence is compared with output from models trained with BLEU and with the LR-KB4 lrscore.
Prefer LR Prefer BLEU Don?t Know
LR-KB4 96 79 5
LR-HB4 93 79 8
Total 189 (52.5%) 158 (43.9%) 13
Table 6. The number of the times human judges preferred
the output of systems trained either with the LRscore or
with the BLEU score, or were unable to choose.
4.2.2 Human Evaluation
We collect human preference judgements for out-
put from systems trained using the BLEU score and
the LRscore in order to determine whether training
with the LRscore leads to genuine improvements in
translation quality. Table 6 shows the number of the
times humans preferred the LRscore or the BLEU
score output, or when they did not know. We can see
that humans have a greater preference for the out-
put for systems trained with the LRscore, which is
preferred 52.5% of the time, compared to the BLEU
score, which was only preferred 43.9% of the time.
The sign test can be used to determine whether
this difference is significant. Our null hypothesis
is that the probability of a human preferring the
LRscore trained output is the same as that of prefer-
ring the BLEU trained output. The one-tailed alter-
native hypothesis is that humans prefer the LRscore
output. If the null hypothesis is true, then there is
only a probability of 0.048 that 189 out of 347
(189 + 158) people will select the LRscore output.
We therefore discard the null hypothesis and the hu-
man preference for the output of the LRscore trained
system is significant to the 95% level.
In order to judge how reliable our judgements are
we calculate the inter-annotator agreement. This is
given by the Kappa coefficient (K), which balances
agreement with expected agreement. The Kappa co-
efficient is 0.464 which is considered to be a moder-
ate level of agreement.
In analysis of the results, we found that output
from the system trained with the LRscore tend to
produce sentences with better structure. In Table 7
we see a typical example. The word order of the
sentence trained with BLEU is mangled, whereas
the LR-KB4 model outputs a clear translation which
more closely matches the reference. It also garners
higher reordering and BLEU scores.
We expect that more substantial gains can be
made in the future by using models which have more
powerful reordering capabilities. A richer set of re-
ordering features, and a model capable of longer
distance reordering would better leverage metrics
which reward good word orderings.
5 Conclusion
We introduced the LRscore which combines a lexi-
cal and a reordering metric. The main motivation for
this metric is the fact that it measures the reorder-
ing quality of MT output by using permutation dis-
tance metrics. It is a simple, decomposable metric
which interpolates the reordering component with
a lexical component, the BLEU score. This paper
demonstrates that the LRscore metric is more con-
sistent with human preference judgements of ma-
chine translation quality than other machine trans-
lation metrics. We also show that when training a
phrase-based translation model with the LRscore as
the objective function, the model retains its perfor-
mance as measured by the baseline metrics. Cru-
cially, however, optimisation using the LRscore im-
proves subjective evaluation. Ultimately, the avail-
ability of a metric which reliably measures reorder-
ing performance should accelerate progress towards
developing more powerful reordering models.
1034
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Nicola Bertoldi, Barry Haddow, and Jean-Baptiste Fouet.
2009. Improved Minimum Error Rate Training in
Moses. The Prague Bulletin of Mathematical Linguis-
tics, 91:7?16.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT Evaluation: Evaluating Re-
ordering. Machine Translation, 24(1):15?26.
Ondrej Bojar and Zdenek Zabokrtsky. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92:63?
84.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and
Josh Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, pages 1?28, Athens, Greece, March. Association
for Computational Linguistics.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286?295, Singapore, August. Associa-
tion for Computational Linguistics.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical MT system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 555?563, Los An-
geles, California, June.
Richard Hamming. 1950. Error detecting and er-
ror correcting codes. Bell System Technical Journal,
26(2):147?160.
Maurice Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30:81?89.
A. Kittur, E. H. Chi, and B. Suh. 2008. Crowdsourcing
user studies with Mechanical Turk. In Proceeding of
the twenty-sixth annual SIGCHI conference on Human
factors in computing systems, pages 453?456. ACM.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada. Associ-
ation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT-
Summit.
Alon Lavie and Abhaya Agarwal. 2008. Meteor,
m-BLEU and m-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Workshop on Sta-
tistical Machine Translation at the Meeting of the As-
sociation for Computational Linguistics (ACL-2008),
pages 115?118.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Chin-Yew Lin and Franz Och. 2004. ORANGE: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the Conference
on Computational Linguistics, pages 501?507.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 160?
167, Sapporo, Japan.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, pages 181?193.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, USA.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings
of Association for Machine Translation in the Ameri-
cas, pages 223?231.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of Spoken Language
Processing, pages 901?904.
Billy Wong and Chunyu Kit. 2009. ATEC: automatic
evaluation of machine translation via word choice and
word order. Machine Translation, 23(2-3):141?155.
1035
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 753?758,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Variable Bit Quantisation for LSH
Sean Moran
School of Informatics
The University of Edinburgh
EH8 9AB, Edinburgh, UK
sean.moran@ed.ac.uk
Victor Lavrenko
School of Informatics
The University of Edinburgh
EH8 9AB, Edinburgh, UK
vlavrenk@inf.ed.ac.uk
Miles Osborne
School of Informatics
The University of Edinburgh
EH8 9AB, Edinburgh, UK
miles@inf.ed.ac.uk
Abstract
We introduce a scheme for optimally al-
locating a variable number of bits per
LSH hyperplane. Previous approaches as-
sign a constant number of bits per hyper-
plane. This neglects the fact that a subset
of hyperplanes may be more informative
than others. Our method, dubbed Variable
Bit Quantisation (VBQ), provides a data-
driven non-uniform bit allocation across
hyperplanes. Despite only using a fraction
of the available hyperplanes, VBQ outper-
forms uniform quantisation by up to 168%
for retrieval across standard text and image
datasets.
1 Introduction
The task of retrieving the nearest neighbours to a
given query document permeates the field of Nat-
ural Language Processing (NLP). Nearest neigh-
bour search has been used for applications as di-
verse as automatically detecting document transla-
tion pairs for the purposes of training a statistical
machine translation system (SMT) (Krstovski and
Smith, 2011), the large-scale generation of noun
similarity lists (Ravichandran et al, 2005) to an
unsupervised method for extracting domain spe-
cific lexical variants (Stephan Gouws and Metzle,
2011).
There are two broad approaches to nearest
neighbour based search: exact and approximate
techniques, which are differentiated by their abil-
ity to return completely correct nearest neighbours
(the exact approach) or have some possibility of
returning points that are not true nearest neigh-
bours (the approximate approach). Approximate
nearest neighbour (ANN) search using hashing
techniques has recently gained prominence within
NLP. The hashing-based approach maps the data
into a substantially more compact representation
referred to as a fingerprint, that is more efficient
for performing similarity computations. The re-
sulting compact binary representation radically re-
duces memory requirements while also permitting
fast sub-linear time retrieval of approximate near-
est neighbours.
Hashing-based ANN techniques generally com-
prise two main steps: a projection stage followed
by a quantisation stage. The projection stage
performs a neighbourhood preserving embedding,
mapping the input data into a lower-dimensional
representation. The quantisation stage subse-
quently reduces the cardinality of this represen-
tation by converting the real-valued projections
to binary. Quantisation is a lossy transformation
which can have a significant impact on the result-
ing quality of the binary encoding.
Previous work has quantised each projected di-
mension into a uniform number of bits (Indyk and
Motwani, 1998) (Kong and Li, 2012) (Kong et al,
2012) (Moran et al, 2013). We demonstrate that
uniform allocation of bits is sub-optimal and pro-
pose a data-driven scheme for variable bit alloca-
tion. Our approach is distinct from previous work
in that it provides a general objective function for
bit allocation. VBQ makes no assumptions on the
data and, in addition to LSH, it applies to a broad
range of other projection functions.
2 Related Work
Locality sensitive hashing (LSH) (Indyk and Mot-
wani, 1998) is an example of an approximate
nearest neighbour search technique that has been
widely used within the field of NLP to preserve the
Cosine distances between documents (Charikar,
2002). LSH for cosine distance draws a large
number of random hyperplanes within the input
feature space, effectively dividing the space into
non-overlapping regions (or buckets). Each hy-
perplane contributes one bit to the encoding, the
value (0 or 1) of which is determined by comput-
753
[a] 









[b]
 
	
	
   
  

		

	

		
	

		

	
	

	

Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 687?692,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exponential Reservoir Sampling for Streaming Language Models
Miles Osborne
?
School of Informatics
University of Edinburgh
Ashwin Lall
Mathematics and Computer Science
Denison University
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Abstract
We show how rapidly changing textual
streams such as Twitter can be modelled in
fixed space. Our approach is based upon
a randomised algorithm called Exponen-
tial Reservoir Sampling, unexplored by
this community until now. Using language
models over Twitter and Newswire as a
testbed, our experimental results based on
perplexity support the intuition that re-
cently observed data generally outweighs
that seen in the past, but that at times,
the past can have valuable signals enabling
better modelling of the present.
1 Introduction
Work by Talbot and Osborne (2007), Van Durme
and Lall (2009) and Goyal et al (2009) consid-
ered the problem of building very large language
models via the use of randomized data structures
known as sketches.
1
While efficient, these struc-
tures still scale linearly in the number of items
stored, and do not handle deletions well: if pro-
cessing an unbounded stream of text, with new
words and phrases being regularly added to the
model, then with a fixed amount of space, errors
will only increase over time. This was pointed
out by Levenberg and Osborne (2009), who inves-
tigated an alternate approach employing perfect-
hashing to allow for deletions over time. Their
deletion criterion was task-specific and based on
how a machine translation system queried a lan-
guage model.
?
Corresponding author: miles@inf.ed.ac.uk
1
Sketches provide space efficiencies that are measured on
the order of individual bits per item stored, but at the cost
of being lossy: sketches trade off space for error, where the
less space you use, the more likely you will get erroneous
responses to queries.
Here we ask what the appropriate selection
criterion is for streaming data based on a non-
stationary process, when concerned with an in-
trinsic measure such as perplexity. Using Twitter
and newswire, we pursue this via a sampling strat-
egy: we construct models over sentences based on
a sample of previously observed sentences, then
measure perplexity of incoming sentences, all on
a day by day, rolling basis. Three sampling ap-
proaches are considered: A fixed-width sliding
window of most recent content, uniformly at ran-
dom over the stream and a biased sample that
prefers recent history over the past.
We show experimentally that a moving window
is better than uniform sampling, and further that
exponential (biased) sampling is best of all. For
streaming data, recently encountered data is valu-
able, but there is also signal in the previous stream.
Our sampling methods are based on reser-
voir sampling (Vitter, 1985), a popularly known
method in some areas of computer science, but
which has seen little use within computational lin-
guistics.
2
Standard reservoir sampling is a method
for maintaining a uniform sample over a dynamic
stream of elements, using constant space. Novel
to this community, we consider a variant owing to
Aggarwal (2006) which provides for an exponen-
tial bias towards recently observed elements. This
exponential reservoir sampling has all of the guar-
antees of standard reservoir sampling, but as we
show, is a better fit for streaming textual data. Our
approach is fully general and can be applied to any
streaming task where we need to model the present
and can only use fixed space.
2
Exceptions include work by Van Durme and Lall (2011)
and Van Durme (2012), aimed at different problems than that
explored here.
687
2 Background
We address two problems: language changes over
time, and the observation that space is a problem,
even for compact sketches.
Statistical language models often assume either
a local Markov property (when working with ut-
terances, or sentences), or that content is gener-
ated fully i.i.d. (such as in document-level topic
models). However, language shows observable
priming effects, sometimes called triggers, where
the occurrence of a given term decreases the sur-
prisal of some other term later in the same dis-
course (Lau et al, 1993; Church and Gale, 1995;
Beeferman et al, 1997; Church, 2000). Conven-
tional cache and trigger models typically do not
deal with new terms and can be seen as adjusting
the parameters of a fixed model.
Accounting for previously unseen entries in a
language model can be naively simple: as they ap-
pear in new training data, add them to the model!
However in practice we are constrained by avail-
able space: how many unique phrases can we
store, given the target application environment?
Our work is concerned with modeling language
that might change over time, in accordance with
current trending discourse topics, but under a strict
space constraint. With a fixed amount of memory
available, we cannot allow our list of unique words
or phrases to grow over time, even while new top-
ics give rise to novel names of people, places, and
terms of interest. Thus we need an approach that
keeps the size of the model constant, but that is
geared to what is being discussed now, as com-
pared to some time in the past.
3 Reservoir Sampling
3.1 Uniform Reservoir Sampling
The reservoir sampling algorithm (Vitter, 1985) is
the classic method of sampling without replace-
ment from a stream in a single pass when the
length of the stream is of indeterminate or un-
bounded length. Say that the size of the desired
sample is k. The algorithm proceeds by retain-
ing the first k items of the stream and then sam-
pling each subsequent element with probability
f(k, n) = k/n, where n is the length of the stream
so far. (See Algorithm 1.) It is easy to show via in-
duction that, at any time, all the items in the stream
so far have equal probability of appearing in the
reservoir.
The algorithm processes the stream in a single
pass?that is, once it has processed an item in the
stream, it does not revisit that item unless it is
stored in the reservoir. Given this restriction, the
incredible feature of this algorithm is that it is able
to guarantee that the samples in the reservoir are a
uniformly random sample with no unintended bi-
ases even as the stream evolves. This makes it an
excellent candidate for situations when the stream
is continuously being updated and it is computa-
tionally infeasible to store the entire stream or to
make more than a single pass over it. Moreover,
it is an extremely efficient algorithm as it requires
O(1) time (independent of the reservoir size and
stream length) for each item in the stream.
Algorithm 1 Reservoir Sampling Algorithm
Parameters:
k: maximum size of reservoir
1: Initialize an empty reservoir (any container
data type).
2: n := 1
3: for each item in the stream do
4: if n < k then
5: insert current item into the reservoir
6: else
7: with probability f(n, k), eject an ele-
ment of the reservoir chosen uniformly
at random and insert current item into the
reservoir
8: n := n+ 1
3.2 Non-uniform Reservoir Sampling
Here we will consider generalizations of the reser-
voir sampling algorithm in which the sample
items in the reservoir are more biased towards the
present. Put another way, we will continuously
decay the probability that an older item will ap-
pear in the reservoir. Models produced using such
biases put more modelling stress on the present
than models produced using data that is selected
uniformly from the stream. The goal here is to
continuously update the reservoir sample in such
a way that the decay of older items is done consis-
tently while still maintaining the benefits of reser-
voir sampling, including the single pass and mem-
ory/time constraints.
The time-decay scheme we will study in this
paper is exponential bias towards newer items in
the stream. More precisely, we wish for items that
688
0 2000 4000 6000 8000 10000time
0.0
0.2
0.4
0.6
0.8
1.0
prob
abil
ity o
f ap
pea
ring
 in r
ese
rvoi
r
uniformexponential (various beta)
Figure 1: Different biases for sampling a stream
have age a in the stream to appear with probability
g(a) = c ? exp (?a/?),
where a is the age of the item, ? is a scale param-
eter indicating how rapidly older items should be
deemphasized, and c is a normalization constant.
To give a sense of what these time-decay proba-
bilities look like, some exponential distributions
are plotted (along with the uniform distribution)
in Figure 1.
Aggarwal (2006) studied this problem and
showed that by altering the sampling probability
(f(n, k) in Algorithm 1) in the reservoir sampling
algorithm, it is possible to achieve different age-
related biases in the sample. In particular, he
showed that by setting the sampling probability to
the constant function f(n, k) = k/?, it is possible
to approximately achieve exponential bias in the
sample with scale parameter ? (Aggarwal, 2006).
Aggarwal?s analysis relies on the parameter ? be-
ing very large. In the next section we will make
the analysis more precise by omitting any such as-
sumption.
3.3 Analysis
In this section we will derive an expression for the
bias introduced by an arbitrary sampling function
f in Algorithm 1. We will then use this expression
to derive the precise sampling function needed to
achieve exponential decay.
3
Careful selection of
f allows us to achieve anything from zero decay
(i.e., uniform sampling of the entire stream) to
exponential decay. Once again, note that since
we are only changing the sampling function, the
3
Specifying an arbitrary decay function remains an open
problem.
one-pass, memory- and time-efficient properties
of reservoir sampling are still being preserved.
In the following analysis, we fix n to be the size
of the stream at some fixed time and k to be the
size of the reservoir. We assume that the ith el-
ement of the stream is sampled with probability
f(i, k), for i ? n. We can then derive the proba-
bility that an element of age a will still be in the
reservoir as
g(a) = f(n? a, k)
n
?
t=n?a+1
(
1?
f(t, k)
k
)
,
since it would have been sampled with probability
f(n? a, k) and had independent chances of being
replaced at times t = n?a+1, . . . , n with proba-
bility f(t, k)/k. For instance, when f(x, k) =
k
x
,
the above formula simplifies down to g(a) =
k
n
(i.e., the uniform sampling case).
For the exponential case, we fix the sampling
rate to some constant f(n, k) = p
k
, and we wish
to determine what value to use for p
k
to achieve
a given exponential decay rate g(a) = ce
?a/?
,
where c is the normalization constant (to make g a
probability distribution) and ? is the scale param-
eter of the exponential distribution. Substituting
f(n, k) = p
k
in the above formula and equating
with the decay rate, we get that p
k
(1 ? p
k
/k)
a
?
ce
?a/?
, which must hold true for all possible val-
ues of a. After some algebra, we get that when
f(x, k) = p
k
= k(1 ? e
?1/?
), the probability
that an item with age a is included in the reser-
voir is given by the exponential decay rate g(a) =
p
k
e
?a/?
. Note that, for very large values of ?, this
probability is approximately equal to p
k
? k/?
(by using the approximation e
?x
? 1 ? x, when
|x| is close to zero), as given by Aggarwal, but our
formula gives the precise sampling probability and
works even for smaller values of ?.
4 Experiments
Our experiments use two streams of data to illus-
trate exponential sampling: Twitter and a more
conventional newswire stream. The Twitter data is
interesting as it is very multilingual, bursty (for ex-
ample, it talks about memes, breaking news, gos-
sip etc) and written by literally millions of differ-
ent people. The newswire stream is a lot more well
behaved and serves as a control.
4.1 Data, Models and Evaluation
We used one month of chronologically ordered
Twitter data and divided it into 31 equal sized
689
Stream Interval Total (toks) Test (toks)
Twitter Dec 2013 3282M 105M
Giga 1994 ? 2010 635.5M 12M
Table 1: Stream statistics
blocks (roughly corresponding with days). We
also used the AFP portion of the Giga Word corpus
as another source of data that evolves at a slower
pace. This data was divided into 50 equal sized
blocks. Table 1 gives statistics about the data. As
can be seen, the Twitter data is vastly larger than
newswire and arrives at a much faster rate.
We considered the following models. Each one
(apart from the exact model) was trained using the
same amount of data:
? Static. This model was trained using data
from the start of the duration and never var-
ied. It is a baseline.
? Exact. This model was trained using all
available data from the start of the stream and
acts as an upper bound on performance.
? Moving Window. This model used all data
in a fixed-sized window immediately before
the given test point.
? Uniform. Here, we use uniform reservoir
sampling to select the data.
? Exponential. Lastly, we use exponen-
tial reservoir sampling to select the data.
This model is parameterised, indicating how
strongly biased towards the present the sam-
ple will be. The ? parameter is a multiplier
over the reservoir length. For example, a ?
value of 1.1 with a sample size of 10 means
the value is 11. In general, ? always needs to
be bigger than the reservoir size.
We sample over whole sentences (or Tweets)
and not ngrams.
4
Using ngrams instead would
give us a finer-grained control over results, but
would come at the expense of greatly complicat-
ing the analysis. This is because we would need to
reason about not just a set of items but a multiset
of items. Note that because the samples are large
5
,
variations across samples will be small.
4
A consequence is that we do not guarantee that each sam-
ple uses exactly the same number of grams. This can be tack-
led by randomly removing sampled sentences.
5
Each day consists of approximately four million Tweets
and we evaluate on a whole day.
Day Uniform ? value
? 1.1 1.3 1.5 2.0
5 619.4 619.4 619.4 619.4 619.4
6 601.0 601.0 603.8 606.6 611.1
7 603.0 599.4 602.7 605.6 612.1
8 614.6 607.7 611.9 614.3 621.6
9 623.3 611.5 615.0 620.0 628.1
10 656.2 643.1 647.2 650.1 658.0
12 646.6 628.9 633.0 636.5 644.6
15 647.7 628.7 630.4 634.5 641.6
20 636.7 605.3 608.4 610.8 618.4
25 631.5 601.9 603.3 604.4 610.0
Table 2: Perplexities for different ? values over
Twitter (sample size = five days). Lower is better.
We test the model on unseen data from all of the
next day (or block). Afterwards, we advance to the
next day (block) and repeat, potentially incorpo-
rating the previously seen test data into the current
training data. Evaluation is in terms of perplexity
(which is standard for language modelling).
We used KenLM for building models and eval-
uating them (Heafield, 2011). Each model was
an unpruned trigram, with Kneser-Ney smoothing.
Increasing the language model order would not
change the results. Here the focus is upon which
data is used in a model (that is, which data is added
and which data is removed) and not upon making
it compact or making retraining efficient.
4.2 Varying the ? Parameter
Table 2 shows the effect of varying the ? param-
eter (using Twitter). The higher the ? value, the
more uniform the sampling. As can be seen, per-
formance improves when sampling becomes more
biased. Not shown here, but for Twitter, even
smaller ? values produce better results and for
newswire, results degrade. These differences are
small and do not affect any conclusions made here.
In practise, this value would be set using a devel-
opment set and to simplify the rest of the paper, all
other experiments use the same ? value (1.1).
4.3 Varying the Amount of Data
Does the amount of data used in a model affect re-
sults? Table 3 shows the results for Twitter when
varying the amount of data in the sample and us-
ing exponential sampling (? = 1.1). In paren-
theses for each result, we show the corresponding
moving window results. As expected, using more
data improves results. We see that for each sample
size, exponential sampling outperforms our mov-
ing window. In the limit, all sampling methods
would produce the same results.
690
Day Sample Size (Days)
1 2 3
5 652.5 (661.2) 629.1 (635.8) 624.8 (625.9)
6 635.4 (651.6) 611.6 (620.8) 604.0 (608.7)
7 636.0 (647.3) 611.0 (625.2) 603.7 (612.5)
8 654.8 (672.7) 625.6 (641.6) 614.6 (626.9)
9 653.9 (662.8) 628.3 (643.0) 618.8 (632.2)
10 679.1 (687.8) 654.3 (666.8) 646.6 (659.7)
12 671.1 (681.9) 645.8 (658.6) 633.8 (647.5)
15 677.7 (697.9) 647.4 (668.0) 636.4 (652.6)
20 648.1 (664.6) 621.4 (637.9) 612.2 (627.6)
25 657.5 (687.5) 625.3 (664.4) 613.4 (641.8)
Table 3: Perplexities for different sample sizes
over Twitter. Lower is better.
4.4 Alternative Sampling Strategies
Table 4 compares the two baselines against the two
forms of reservoir sampling. For Twitter, we see
a clear recency effect. The static baseline gets
worse and worse as it recedes from the current
test point. Uniform sampling does better, but it
in turn is beaten by the Moving Window Model.
However, this in turn is beaten by our exponential
reservoir sampling.
Day Static Moving Uniform Exp Exact
5 619.4 619.4 619.4 619.4 619.4
6 664.8 599.7 601.8 601.0 597.6
7 684.4 602.8 603.0 599.3 595.6
8 710.1 612.0 614.6 607.7 603.5
9 727.0 617.9 623.3 613.0 608.7
10 775.6 651.2 656.2 642.0 640.5
12 776.7 639.0 646.6 628.7 627.5
15 777.1 638.3 647.7 626.7 627.3
20 800.9 619.1 636.7 604.9 607.3
25 801.4 621.7 631.5 601.5 597.6
Table 4: Perplexities for differently selected sam-
ples over Twitter (sample size = five days, ? =
1.1). Results in bold are the best sampling results.
Lower is better.
4.5 GigaWord
Twitter is a fast moving, rapidly changing multi-
lingual stream and it is not surprising that our ex-
ponential reservoir sampling proves beneficial. Is
it still useful for a more conventional stream that
is drawn from a much smaller population of re-
porters? We repeated our experiments, using the
same rolling training and testing evaluation as be-
fore, but this time using newswire for data.
Table 5 shows the perplexities when using the
Gigaword stream. We see the same general trends,
albeit with less of a difference between exponen-
tial sampling and our moving window. Perplexity
values are all lower than for Twitter.
Block Static Moving Uniform Exp
11 416.5 381.1 382.0 382.0
15 436.7 353.3 357.5 352.8
20 461.8 347.0 354.4 344.6
25 315.6 214.9 222.2 211.3
30 319.1 200.5 213.5 199.5
40 462.5 304.4 313.2 292.9
Table 5: Perplexities for differently selected sam-
ples over Gigaword (sample size = 10 blocks, ? =
1.1). Lower is better.
4.6 Why does this work for Twitter?
Although the perplexity results demonstrate that
exponential sampling is on average beneficial, it
is useful to analyse the results in more detail. For
a large stream size (25 days), we built models us-
ing uniform, exponential (? = 1.1) and our moving
window sampling methods. Each approach used
the same amount of data. For the same test set
(four million Tweets), we computed per-Tweet log
likelihoods and looked at the difference between
the model that best explained each tweet and the
second best model (ie the margin). This gives us
an indication of how much a given model better
explains a given Tweet. Analysing the results, we
found that most gains came from short grams and
very few came from entire Tweets being reposted
(or retweeted). This suggests that the Twitter re-
sults follow previously reported observations on
how language can be bursty and not from Twitter-
specific properties.
5 Conclusion
We have introduced exponential reservoir sam-
pling as an elegant way to model a stream of un-
bounded size, yet using fixed space. It naturally al-
lows one to take account of recency effects present
in many natural streams. We expect that our lan-
guage model could improve other Social Media
tasks, for example lexical normalisation (Han and
Baldwin, 2011) or even event detection (Lin et
al., 2011). The approach is fully general and not
just limited to language modelling. Future work
should look at other distributions for sampling and
consider tasks such as machine translation over
Social Media.
Acknowledgments This work was carried out
when MO was on sabbatical at the HLTCOE and
CLSP.
691
References
Charu C Aggarwal. 2006. On biased reservoir sam-
pling in the presence of stream evolution. In Pro-
ceedings of the 32nd international conference on
Very large data bases, pages 607?618. VLDB En-
dowment.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A model of lexical attractions and repulsion.
In Proceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 373?380.
Association for Computational Linguistics.
K. Church and W. A. Gale. 1995. Poisson mixtures.
Natural Language Engineering, 1:163?190.
Kenneth W Church. 2000. Empirical estimates of
adaptation: the chance of two noriegas is closer to
p/2 than p 2. In Proceedings of the 18th conference
on Computational linguistics-Volume 1, pages 180?
186. Association for Computational Linguistics.
Amit Goyal, Hal Daum?e III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language Modeling. In Proceedings of NAACL.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 368?378, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Raymond Lau, Ronald Rosenfeld, and SaIim Roukos.
1993. Trigger-based language models: A maximum
entropy approach. In Acoustics, Speech, and Signal
Processing, 1993. ICASSP-93., 1993 IEEE Interna-
tional Conference on, volume 2, pages 45?48. IEEE.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756?764. Association for Compu-
tational Linguistics.
Jimmy Lin, Rion Snow, and William Morgan. 2011.
Smoothing techniques for adaptive online language
models: topic tracking in tweet streams. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 422?429. ACM.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2009. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2011. Effi-
cient online locality sensitive hashing via reservoir
counting. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 18?23. Association for Computa-
tional Linguistics.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 48?58. Association for
Computational Linguistics.
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37?57, March.
692
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37?42,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Real-Time Detection, Tracking, and Monitoring of Automatically
Discovered Events in Social Media
Miles Osborne
?
Edinburgh
Sean Moran
Edinburgh
Richard McCreadie
Glasgow
Alexander Von Lunen
Loughborough
Martin Sykora
Loughborough
Elizabeth Cano
Aston
Neil Ireson
Sheffield
Craig Macdonald
Glasgow
Iadh Ounis
Glasgow
Yulan He
Aston
Tom Jackson
Loughborough
Fabio Ciravegna
Sheffield
Ann O?Brien
Loughborough
Abstract
We introduce ReDites, a system for real-
time event detection, tracking, monitoring
and visualisation. It is designed to as-
sist Information Analysts in understand-
ing and exploring complex events as they
unfold in the world. Events are automat-
ically detected from the Twitter stream.
Then those that are categorised as be-
ing security-relevant are tracked, geolo-
cated, summarised and visualised for the
end-user. Furthermore, the system tracks
changes in emotions over events, sig-
nalling possible flashpoints or abatement.
We demonstrate the capabilities of ReD-
ites using an extended use case from the
September 2013 Westgate shooting inci-
dent. Through an evaluation of system la-
tencies, we also show that enriched events
are made available for users to explore
within seconds of that event occurring.
1 Introduction and Challenges
Social Media (and especially Twitter) has become
an extremely important source of real-time infor-
mation about a massive number of topics, ranging
from the mundane (what I had for breakfast) to the
profound (the assassination of Osama Bin Laden).
?
Corresponding author: miles@inf.ed.ac.uk
Detecting events of interest, interpreting and mon-
itoring them has clear economic, security and hu-
manitarian importance.
The use of social media message streams for
event detection poses a number of opportunities
and challenges as these streams are: very high in
volume, often contain duplicated, incomplete, im-
precise and incorrect information, are written in
informal style (i.e. short, unedited and conver-
sational), generally concern the short-term zeit-
geist; and finally relate to unbounded domains.
These characteristics mean that while massive and
timely information sources are available, domain-
relevant information may be mentioned very infre-
quently. The scientific challenge is therefore the
detection of the signal within that noise. This chal-
lenge is exacerbated by the typical requirement
that documents must be processed in (near) real-
time, such that events can be promptly acted upon.
The ReDites system meets these requirements
and performs event detection, tracking, summari-
sation, categorisation and visualisation. To the
best of our understanding, it is the first published,
large-scale, (near) real-time Topic Detection and
Tracking system that is tailored to the needs of in-
formation analysts in the security sector. Novel as-
pects of ReDites include the first large-scale treat-
ment of spuriously discovered events and tailoring
the event stream to the security domain.
37
Figure 1: System Diagram
2 Related Work
A variety of event exploration systems have previ-
ously been proposed within the literature. For in-
stance, Trend Miner
1
enables the plotting of term
times series, drawn from Social Media (Preot?iuc-
Pietro and Cohn, 2013). It has a summarisation
component and is also multilingual. In contrast,
our system is focussed instead upon documents
(Tweets) and is more strongly driven by real-
time considerations. The Social Sensor (Aiello et
al., 2013) system facilitates the tracking of pre-
defined events using social streams.
In contrast, we track all automatically discov-
ered events we find in the stream. The Twitci-
dent (Abel et al., 2012) project deals with user-
driven searching through Social Media with re-
spect to crisis management. However, unlike
ReDites, these crises are not automatically dis-
covered. The LRA Crisis Tracker
2
has a similar
purpose as ReDites. However, while LRA uses
crowdsourcing, our ReDites system is fully auto-
matic.
3 System Overview and Architecture
Figure 1 gives a high-level system description.
The system itself is loosely coupled, with ser-
vices from different development teams coordi-
nating via a Thrift interface. An important as-
pect of this decoupled design is that it enables ge-
ographically dispersed teams to coordinate with
each other. Event processing is comprised of the
following main 4 steps:
1) New events are detected. An event is described
by the first Tweet that we find discussing it and
is defined as something that is captured within a
single Tweet (Petrovic et al., 2010).
1
http://www.trendminer-project.eu/
2
http://www.lracrisistracker.com/
2) When an event is first discovered it may initially
have little information associated with it. Further-
more, events evolve over time. Hence, the sec-
ond step involves tracking the event ? finding new
posts relating to it as they appear and maintaining
a concise updated summary of them.
3) Not all events are of interest to our intended
audience, so we organise them. In particular, we
determine whether an event is security-related (or
otherwise), geolocate it, and detect how prominent
emotions relating to that event evolve.
4) Finally, we visualise the produced stream of
summarised, categorised and geolocated events
for the analyst(s), enabling them to better make
sense of the mass of raw information present
within the original Twitter stream.
Section 6 further describes these four steps.
4 Data and Statistics
For the deployment of ReDites, we use the Twit-
ter 1% streaming sample. This provides approx-
imately four million Tweets per day, at a rate of
about 50 Tweets a second. Table 1 gives some
illustrative statistics on a sample of data from
September 2013 to give a feel for the rate of data
and generated events we produce. Table 2 gives
timing information, corresponding with the major
components of our system: time to process and
time to transfer to the next component, which is
usually a service on another machine on the in-
ternet. The latency of each step is measured in
seconds over a 1000 event sample. ?Transfer? la-
tencies is the time between one step completing
and the output arriving at the next step to be pro-
cessed (Thrift transfer time). Variance is the aver-
age deviation from the mean latency over the event
sample.
When processing the live stream, we ingest data
at an average rate of 50 Tweets per second and de-
tect an event (having geolocated and filtered out
non-English or spam Tweets) with a per-Tweet la-
tency of 0.6?0.55 seconds. Figure 2 gives laten-
cies for the various major components of the sys-
tem. All processing uses commodity machines.
5 The Westgate Shopping Mall Attack
As an illustrative example of a complex recent
event, we considered a terrorist attack on the 21st
of September, 2013.
3
This event is used to demon-
strate how our system can be used to understand it.
3
https://en.wikipedia.org/wiki/Westgate shopping mall shooting
38
Measure Event Detection Tracking and Summ Emotion Ident Security Class
Detection Transfer Ranking Summ Transfer Ident Transfer Class
Latency (sec.) 0.6226 0.7929 2.2892 0.0409 0.0519 0.2881 0.1032 0.1765
Variance (sec.) 0.5518 0.2987 1.3079 0.0114 0.0264 0.1593 0.0195 0.0610
Table 2: Event exploration timing and timing variance (seconds)
Data Rate
Tweets 35 Million
Detected events 533k
Categorised (security-related) events 5795
Table 1: Data statistics, 1st September - 30th
September 2013
In summary, a shopping Mall in Kenya was at-
tacked from the 21st of September until the 24th
of September. This event was covered by tradi-
tional newswire, by victims caught up in it as well
as by terrorist sympathisers, often in real-time.
As we later show, even though we only operate
over 1% of the Twitter Stream, we are still able to
find many (sub) events connected with this attack.
There were 6657 mentions of Westgate in Septem-
ber 2013 in our 1% of sample Tweets.
6 Major Components
6.1 Event Detection
Building upon an established Topic Detection and
Tracking (TDT) methodology, which assumes that
each new event corresponds with seeing some
novel document. the event detection component
uses a hashing approach that finds novel events
4
in constant time (Petrovic et al., 2010). To make
it scale and process thousands of documents each
second, it can optionally be distributed over a clus-
ter of machines (via Storm
5
) (McCreadie et al.,
2013). The system favours recall over precision
and has been shown to have high recall, but a low
precision (Petrovic et al., 2013). Given that we are
presenting discovered events to a user and we do
not want to overload them with false positives, we
need to take steps to increase precision (ie present
fewer false positives).
We use a content categoriser to determine
whether a discovered event is worth reporting.
Using more than 100k automatically discovered
events from the Summer of 2011, we created a
training set and manually labelled each Tweet:
4
An event is defined as something happening at a given
time and place. Operationally, this means something that can
be described within a Tweet.
5
http://storm.incubator.apache.org/
was it content bearing (what you might want to
read about in traditional newswire) or irrelevant
/ not useful. With this labelled data, we used
a Passive-Aggressive algorithm to build a con-
tent classifier. Features were simply unigrams in
Tweets. This dramatically improves precision, to
70%, with a drop in recall to 25% (when tested
on 73k unseen events, manually labelled by two
annotators). We can change the precision-recall
boundary as needed by adjusting the associated
decision boundary. We do not consider non-
English language Tweets in this work and they are
filtered out (Lui and Baldwin, 2012).
Geolocation is important, as we are particu-
larly interested in events that occur at a spe-
cific location. We therefore additionally geolo-
cate any Tweets that were not originally ge-
olocated. To geotag those Tweets that do not
have any geo-location information we use the
Tweet text and additional Tweet metadata (lan-
guage, city/state/country name, user description
etc), to learn a L
1
penalised least squares regres-
sor (LASSO) to predict the latitude and longitude.
The model is learnt on nearly 20 million geo-
located Tweets collected from 2010-2014. Exper-
iments on a held-out test dataset show we can lo-
calise Tweets to within a mean distance of 433 km
of their true location. This performance is based
on the prediction of individual tweet location and
not, as in most previous work, on the location of a
user who is represented by a set of tweets. Further-
more we are not restricted to a single, well-defined
area (such as London) and we also evaluate over a
very large set of unfiltered tweets.
Turning to the Westgate example, the first men-
tion of it in our data was at 10:02 UTC. There were
57 mentions of Westgate in discovered events,
of which 42 mentioned Kenya and 44 mentioned
Nairobi. The first mention itself in Twitter was at
09:38 UTC. We declared it an event (having seen
enough evidence and post-processing it) less than
one second later:
Westgate under siege. Armed thugs. Gun-
shots reported. Called the managers, phones are
off/busy. Are cops on the way?
We also captured numerous informative sub-
39
events covering different aspects and sides of the
central Westgate siege event, four of these are il-
lustrated below:
Post Time Tweet
10:05am RT @ItsMainaKageni: My friend Ruhila Adatia
passed away together with her unborn child. Please
keep her family and new husband in your thou
10:13am RT howden africa: Kenya police firing tear gas and
warning shots at Kenyan onlookers. Crowd getting
angry #westgate
10:10am RT @BreakingNews: Live video: Local news cov-
erage of explosions, gunfire as smoke billows from
Nairobi, Kenya, mall - @KTNKenya
10:10am ?Purportedly official Twitter account for al-Shabaab
Tweeting on the Kenyan massacre HSM Press
(http://t.co/XnCz9BulGj)
6.2 Tracking and Summarisation
The second component of the event exploration
system is Tracking and Summarisation (TaS). The
aim of this component is to use the underlying
Tweet stream to produce an overview for each
event produced by the event detection stage, up-
dating this overview as the event evolves. Track-
ing events is important when dealing with live, on-
going disasters, since new information can rapidly
emerge over time.
TaS takes as input a Tweet representing an event
and emits a list of Tweets summarising that event
in more detail. TaS is comprised of two dis-
tinct sub-components, namely: real-time tracking;
and event summarisation. The real-time track-
ing component maintains a sliding window of
Tweets from the underlying Tweet stream. As
an event arrives, the most informative terms con-
tained
6
form a search query that is used to retrieve
new Tweets about the event. For example, tak-
ing the Tweet about the Westgate terrorist attack
used in the previous section as input on September
21st 2013 at 10:15am, the real-time tracking sub-
component retrieved the following related Tweets
from the Twitter Spritzer (1%) steam
7
(only 5/100
are shown):
ID Post Time Tweet Score
1 10:05am Westgate under siege. Armed thugs. Gun-
shots reported. Called the managers, phones are
off/busy. Are cops on the way?
123.7
2 10:13am DO NOT go to Westgate Mall. Gunshots and
mayhem, keep away until further notice.
22.9
3 10:13am RT DO NOT go to Westgate Mall. Gunshots
and mayhem, keep away until further notice.
22.9
4 10:10am Good people please avoid Westgate Mall. @Po-
liceKE @IGkimaiyo please act ASAP, reports
of armed attack at #WestgateMall
22.2
5 10:07am RT @steve enzo: @kirimiachieng these thugs
won?t let us be
11.5
6
Nouns, adjectives, verbs and cardinal numbers
7
https://dev.twitter.com/docs/streaming-
apis/streams/public
The second TaS sub-component is event sum-
marisation. This sub-component takes as input the
Tweet ranking produced above and performs ex-
tractive summarisation (Nenkova and McKeown,
2012) upon it, i.e. it selects a subset of the ranked
Tweets to form a summary of the event. The goals
of event summarisation are two-fold. First, to re-
move any Tweets from the above ranking that are
not relevant to the event (e.g. Tweet 5 in the exam-
ple above). Indeed when an event is first detected,
there may be few relevant Tweets yet posted. The
second goal is to remove redundancy from within
the selected Tweets, such as Tweets 2 and 3 in the
above example, thereby focussing the produced
summary on novelty. To tackle the first of these
goals, we leverage the score distribution of Tweets
within the ranking to identify those Tweets that are
likely background noise. When an event is first
detected, few relevant Tweets will be retrieved,
hence the mean score over the Tweets is indicative
of non-relevant Tweets. Tweets within the rank-
ing whose scores diverge from the mean score in
the positive direction are likely to be on-topic. We
therefore, make an include/exclude decision for
each Tweet t in the ranking R:
include(t, R) =
?
?
?
?
?
?
?
?
?
?
?
1 if score(t)? SD(R) > 0
and |SD(R)? score(t)| >
? ?
1
|R|
?
t
?
?R
|SD(R)? score(t
?
)|
0 otherwise
(1)
where SD(R) is the standard deviation of scores
inR, score(t) is the retrieval score for Tweet t and
? is a threshold parameter that describes the mag-
nitude of the divergence from the mean score that
a Tweet must have before it is included within the
summary. Then, to tackle the issue of redundancy,
we select Tweets in a greedy time-ordered man-
ner (earliest first). A similarity (cosine) threshold
between the current Tweet and each Tweet previ-
ously selected is used to remove those that are tex-
tually similar, resulting in the following extractive
summary:
ID Post Time Tweet Score
1 10:05am Westgate under siege. Armed thugs.
Gunshots reported. Called the man-
agers, phones are off/busy. Are cops
on the way?
123.7
2 10:13am DO NOT go to Westgate Mall. Gun-
shots and mayhem, keep away until
further notice.
22.9
4 10:10am Good people please avoid Westgate
Mall. @PoliceKE @IGkimaiyo please
act ASAP, reports of armed attack at
#WestgateMall
22.2
Finally, the TaS component can be used to track
40
events over time. In this case, instead of tak-
ing a new event as input from the event detec-
tion component, a previously summarised event
can be used as a surrogate. For instance, a user
might identify an event that they want to track.
The real-time search sub-component retrieves new
Tweets about the event posted since that event was
last summarised. The event summarisation sub-
component then removes non-relevant and redun-
dant Tweets with respect to those contained within
the previous summary, producing a new updated
summary.
6.3 Organising Discovered Events
The events we discover are not targeted at infor-
mation analysts. For example, they contain sports
updates, business acquisitions as well as those that
are genuinely relevant and can bear various opin-
ions and degrees of emotional expression. We
therefore take steps to filter and organise them for
our intended audience: we predict whether they
have a specific security-focus and finally predict
an emotional label for events (which can be useful
when judging changing viewpoints on events and
highlighting extreme emotions that could possibly
motivate further incidents).
6.3.1 Security-Related Event Detection
We are particularly interested in security-related
events such as violent events, natural disasters, or
emergency situations. Given a lack of in-domain
labelled data, we resort to a weakly supervised
Bayesian modelling approach based on the previ-
ously proposed Violence Detection Model (VDM)
(Cano et al., 2013) for identifying security events.
In order to differentiate between security and
non-security related events, we extract words re-
lating to security events from existing knowledge
sources such as DBpedia and incorporate them as
priors into the VDM model learning. It should be
noted that such a word lexicon only provides ini-
tial prior knowledge into the model. The model
will automatically discover new words describing
security-related events.
We trained the VDM model on a randomly
sampled 10,581 Tweets from the TREC Mi-
croblog 2011 corpus (McCreadie et al., 2012)
and tested the model on 1,759 manually labelled
Tweets which consist of roughly the same num-
ber of security-related and non-security related
Tweets. Our results show that the VDM model
achieved 85.8% in F-measure for the identification
of security-related Tweets, which is not far from
the F-measure of 90% obtained using the super-
vised Naive Bayes classifier despite using no la-
belled data in the model.
Here, we derived word priors from a total
of 32,174 documents from DBpedia and ex-
tracted 1,612 security-related words and 1,345
non-security-related words based on the measure-
ment of relative word entropy. We then trained
the VDM model by setting the topic number to
50 and using 7,613 event Tweets extracted from
the Tweets collected during July-August 2011 and
September 2013 in addition to 10,581 Tweets from
the TREC Microblog 2011 corpus. In the afore-
mentioned Westgate example, we classify 24% of
Tweets as security-related out of a total of 7,772
summary Tweets extracted by the TaS component.
Some of the security-related Tweets are listed be-
low
8
:
ID Post Time Tweet
1 9:46am Like Bin Laden kind of explosion?
?@The realBIGmeat:
There is an explosion at westgate!?
2 10:08am RT @SmritiVidyarthi: DO NOT go to Westgate
Mall. Gunshots and mayhem, keep away till further no-
tice.
3 10:10am RT @juliegichuru: Good people please avoid
Westgate. @PoliceKE @IGkimaiyo please act
ASAP, reports of armed attack at #WestgateMall.
4 10:13am there has bn shooting @ Westgate which is suspected
to b of gangs.......there is tension rt nw....
6.3.2 Emotion
Security-related events can be fraught, with emo-
tionally charged posts possibly evolving over time,
reflecting ongoing changes in underlying events.
Eight basic emotions, as identified in the psychol-
ogy literature (see (Sykora et al., 2013a) for a de-
tailed review of this literature) are covered, specif-
ically; anger, confusion, disgust, fear, happiness,
sadness, shame and surprise. Extreme values ?as
well as their evolution? can be useful to an ana-
lyst (Sykora et al., 2013b). We detect enotions in
Tweets and support faceted browing. The emotion
component assigns labels to Tweets representing
these emotions. It is based upon a manually con-
structed ontology, which captures the relationships
between these emotions and terms (Sykora et al.,
2013a).
We sampled the summarised Tweets of the
Westgate attack, starting from the event detection
and following the messages over a course of seven
days. In the relevant Tweets, we detected that
8
Note some Tweets happen on following days.
41
8.6% had emotive terms in them, which is in line
with the aforementioned literature. Some example
expressions of emotion include:
Time Tweet Emotions
03:34 -) Ya so were those gunshots outside Fear
of gables?! I?m terrified ?
06:27 -) I?m so impressed @ d way. Kenyans r handling d siege. Surprise
14:32 -) All you xenophobic idiots spewing anti-Muslim Fear
bullshit need to -get in one of these donation lines Disgust
and see how wrong you ?
For Westgate, the emotions of sadness, fear and
surprise dominated. Very early on the emotions of
fear and sadness were expressed, as Twitter users
were terrified by the event and saddened by the
loss of lives. Sadness and fear were ? over time ?
the emotions that were stated most frequently and
constantly, with expressions of surprise, as users
were shocked about what was going on, and some
happiness relating to when people managed to
escape or were rescued from the mall. Generally
speaking, factual statements in the Tweets were
more prominent than emotive ones. This coincides
with the emotive Tweets that represented fear and
surprise in the beginning, as it was not clear what
had happened and Twitter users were upset and
tried to get factual information about the event.
6.4 Visual Analytics
The visualisation component is designed to facili-
tate the understanding and exploration of detected
events. It offers faceted browsing and multiple vi-
sualisation tools to allow an information analyst
to gain a rapid understanding of ongoing events.
An analyst can constrain the detected events us-
ing information both from the original Tweets (e.g.
hashtags, locations, user details) and from the up-
dated summaries derived by ReDites. The ana-
lyst can also view events using facet values, loca-
tions/keywords in topic maps and time/keywords
in multiple timelines. By combining informa-
tion dimensions, the analyst can determine pat-
terns across dimensions to determine if an event
should be acted upon ? e.g the analyst can choose
to view Tweets, which summarise highly emotive
events, concerning middle eastern countries.
7 Discussion
We have presented ReDites, the first published
system that carries out large-scale event detection,
tracking summarisation and visualisation for the
security sector. Events are automatically identified
and those that are relevant to information analysts
are quickly made available for ongoing monitor-
ing. We showed how the system could be used
to help understand a complex, large-scale security
event. Although our system is initially specialised
to the security sector, it is easy to repurpose it to
other domains, such as natural disasters or smart
cities. Key aspects of our approach include scala-
bility and a rapid response to incoming data.
Acknowledgements
This work was funded by EPSRC grant
EP/L010690/1. MO also acknowledges sup-
port from grant ERC Advanced Fellowship
249520 GRAMPLUS.
References
F. Abel, C. Hauff, G.-J. Houben, R. Stronkman, and K. T.
Semantics + filtering + search = twitcident. exploring in-
formation in social web streams. In Proc. of HT, 2012.
L. M. Aiello et al. L. Aiello, G. Petkos, C. Martin, D. Corney,
S. Papadopoulos, R. Skraba, A. Goker, Y. Kompatsiaris,
A. Jaimes Sensing trending topics in Twitter. Transac-
tions on Multimedia Journal, 2012.
A.E. Cano, Y. He, K. Liu, and J. Zhao. A weakly supervised
bayesian model for violence detection in social media. In
Proc. of IJCNLP, 2013.
M. Lui and T. Baldwin. Langid.py: An off-the-shelf lan-
guage identification tool. In Proc. of ACL, 2012.
R. McCreadie, C. Macdonald, I. Ounis, M. Osborne, and S.
Petrovic. Scalable distributed event detection for twitter.
In Proc. of Big Data, 2013.
R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis and
D. McCullough. On building a reusable Twitter corpus. In
Proc. of SIGIR, 2012.
A. Nenkova and K. McKeown. A survey of text summariza-
tion techniques. In Mining Text Data Journal, 2012.
S. Petrovic, M. Osborne, and V. Lavrenko. Streaming first
story detection with application to Twitter. In Proc. of
NAACL, 2010.
S. Petrovic, M. Osborne, R. McCreadie, C. Macdonald, I.
Ounis, and L. Shrimpton. Can Twitter replace newswire
for breaking news? In Proc. of WSM, 2012.
D. Preot?iuc-Pietro and T. Cohn. A temporal model of text pe-
riodicities using gaussian processes. In Proc. of EMNLP,
2012.
M. D. Sykora, T. W. Jackson, A. O?Brien, and S. Elayan.
Emotive ontology: Extracting fine-grained emotions from
terse, informal messages. Computer Science and Informa-
tion Systems Journal, 2013.
M. D. Sykora, T. W. Jackson, A. O?Brien, and S. Elayan.
National security and social media monitoring. In Proc.
of EISIC, 2013.
42
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 25?26,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Edinburgh Twitter Corpus
Sas?a Petrovic?
School of Informatics
University of Edinburgh
sasa.petrovic@ed.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
Victor Lavrenko
School of Informatics
University of Edinburgh
vlavrenk@inf.ed.ac.uk
Abstract
We describe the first release of our corpus of
97 million Twitter posts. We believe that this
data will prove valuable to researchers working
in social media, natural language processing,
large-scale data processing, and similar areas.
1 Introduction
In the recent years, the microblogging service Twit-
ter has become a popular tool for expressing opin-
ions, broadcasting news, and simply communicating
with friends. People often comment on events in
real time, with several hundred micro-blogs (tweets)
posted each second for significant events. Despite
this popularity, there still does not exist a publicly
available corpus of Twitter posts. In this paper we
describe the first such corpus collected over a period
of two months using the Twitter streaming API.1
Our corpus contains 97 million tweets, and takes up
14 GB of disk space uncompressed. The corpus is
distributed under a Creative Commons Attribution-
NonCommercial-ShareAlike license2 and can be ob-
tained at http://demeter.inf.ed.ac.uk/. Each
tweet has the following information:
? timestamp ? time (in GMT) when the tweet was
written
? anonymized username ? the author of the tweet,
where the author?s original Twitter username
is replaced with an id of type userABC. We
anonymize the usernames in this way to avoid
malicious use of the data (e.g., by spammers).
Note that usernames are anonymized consis-
tently, i.e., every time user A is mentioned in
the stream, he is replaced with the same id.
1http://stream.twitter.com/
2http://creativecommons.org/licenses/by-nc-sa/3.0/
legalcode
Table 1: N-gram statistics.
N-grams tokens unique
Unigrams 2,263,886,631 31,883,775
Bigrams 2,167,567,986 174,785,693
Trigrams 2,072,595,131 948,850,470
4-grams 1,980,386,036 1,095,417,876
? posting method ? method used to publish the
tweet (e.g., web, API, some Twitter client).
Given that there are dozen of Twitter clients in
use today, we believe this information could be
very useful in determining, e.g., any differences
in content that comes through different clients.
The format of our data is very simple. Each line
has the following format:
timestamp \t username \t tweet \t client
where \t is the tab character, and client is the pro-
gram used for posting the tweet. Note that the ad-
ditional whitespaces seen above are only added for
readability, and don?t exist in the corpus.
2 Corpus statistics
We collected the corpus from a period spanning
November 11th 2009 until February 1st 2010. As was
already mentioned, the data was collected through
Twitter?s streaming API and is thus a representa-
tive sample of the entire stream. Table 1 shows the
basic n-gram statistics ? note that our corpus con-
tains over 2 billion words. We made no attempt to
distinguish between English and non-English tweets,
as we believe that a multilingual stream might be of
use for various machine translation experiments.
Table 2 shows some basic statistics specific to the
Twitter stream. In particular, we give the number
of users that posted the tweets, the number of links
(URLs) in the corpus, the number of topics and the
number of replies. From the first two rows of Table 2
25
Table 2: Twitter-specific statistics.
Unique Total
tweets - 96,369,326
users 9,140,015 -
links - 20,627,320
topics 1,416,967 12,588,431
replies 5,426,030 54,900,387
clients 33,860 -
Table 3: Most cited Twitter users
Username number of replies
@justinbieber 279,622
@nickjonas 95,545
@addthis 56,761
@revrunwisdom 51,203
@ 50,565
@luansantanaevc 49,106
@donniewahlberg 46,126
@eduardosurita 36,495
@fiuk 33,570
@ddlovato 32,327
we can see that the average number of tweets per user
is 10.5. Topics are defined as single word preceded by
a # symbol, and replies are single words preceded by
a @ symbol. This is the standard way Twitter users
add metadata to their posts. For topics and replies,
we give both the number of unique tokens and the
total number of tokens.
Table 3 shows a list of 10 users which received the
most replies. The more replies a user receives, more
influential we might consider him. We can see that
the two top ranking users are Justin Bieber and Nick
Jonas, two teenage pop-stars who apparently have a
big fan base on Twitter. In fact, six out of ten users
on the list are singers, suggesting that many artists
have turned to Twitter as a means of communicating
with their fans. Note also that one of the entries is
an empty username ? this is probably a consequence
of mistakes people make when posting a reply.
Similarly to Table 3, Table 4 shows the ten most
popular topics in our corpus. We can see that the
most popular topics include music (#nowplaying,
#mm ? music monday), jobs ads, facebook updates
(#fb), politics (#tcot ? top conservatives on Twit-
ter), and random chatter (#ff ? follow friday, #tiny-
chat, #fail, #formspringme). The topic #39;s is an
error in interpreting the apostrophe sign, which has
the ascii value 39 (decimal).
Table 4: Most popular topics on Twitter
Topic number of occurences
#nowplaying 255,715
#ff 220,607
#jobs 181,205
#fb 144,835
#39;s 110,150
#formspringme 85,775
#tcot 77,294
#fail 56,730
#tinychat 56,174
#mm 52,971
Figure 1: Different sources of tweets.
Figure 1 shows the ten most popular clients used
for posting to Twitter. Despite the large amount
of different Twitter clients used (over 33 thousand,
cf. Table 2), figure 1 shows that almost 80% of all the
tweets in our corpus were posted using one of the top
ten most popular clients. We can see that traditional
posting through the Twitter web site is still by far
the most popular method, while UberTwitter and
TweetDeck seem to be the next most popular choices.
3 Conclusion
In this paper we presented a corpus of almost 100
million tweets which we made available for public
use. Basic properties of the corpus are given and a
simple analysis of the most popular users and topics
revealed that Twitter is in large part used to talk
about music by communicating both with artists and
other fans. We believe that this corpus could be
a very useful resource for researchers dealing with
social media, natural language processing, or large-
scale data processing.
26
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 327?332,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LRscore for Evaluating Lexical and Reordering Quality in MT
Alexandra Birch
University of Edinburgh
United Kingdom
a.c.birch-mayne@s0454866.ed.ac.uk
Miles Osborne
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
The ability to measure the quality of word
order in translations is an important goal
for research in machine translation. Cur-
rent machine translation metrics do not
adequately measure the reordering perfor-
mance of translation systems. We present
a novel metric, the LRscore, which di-
rectly measures reordering success. The
reordering component is balanced by a
lexical metric. Capturing the two most im-
portant elements of translation success in
a simple combined metric with only one
parameter results in an intuitive, shallow,
language independent metric.
1 Introduction
The main purpose of MT evaluation is to de-
termine ?to what extent the makers of a system
have succeeded in mimicking the human transla-
tor? (Krauwer, 1993). But machine translation
has no ?ground truth? as there are many possi-
ble correct translations. It is impossible to judge
whether a translation is incorrect or simply un-
known and it is even harder to judge the the degree
to which it is incorrect. Even so, automatic met-
rics are necessary. It is nearly impossible to collect
enough human judgments for evaluating incre-
mental improvements in research systems, or for
tuning statistical machine translation system pa-
rameters. Automatic metrics are also much faster
and cheaper than human evaluation and they pro-
duce reproducible results.
Machine translation research relies heavily
upon automatic metrics to evaluate the perfor-
mance of models. However, current metrics rely
upon indirect methods for measuring the quality
of the word order, and their ability to capture re-
ordering performance has been demonstrated to be
poor (Birch et al, 2010). There are two main ap-
proaches to capturing reordering. The first way
to measure the quality of word order is to count
the number of matching n-grams between the ref-
erence and the hypothesis. This is the approach
taken by the BLEU score (Papineni et al, 2002).
This method discounts any n-gram which is not
identical to a reference n-gram, and also does not
consider the relative position of the strings. They
can be anywhere in the sentence. Another com-
mon approach is typified by METEOR (Banerjee
and Lavie, 2005) and TER (Snover et al, 2006).
They calculate an ordering penalty for a hypoth-
esis based on the minimum number of chunks the
translation needs to be broken into in order to align
it to the reference. The disadvantage of the second
approach is that aligning sentences with very dif-
ferent words can be inaccurate. Also there is no
notion of how far these blocks are out of order.
More sophisticated metrics, such as the RTE met-
ric (Pado? et al, 2009), use higher level syntactic or
even semantic analysis to determine the quality of
the translation. These approaches are useful, but
can be very slow, require annotation, they are lan-
guage dependent and their parameters are hard to
train. For most research work shallow metrics are
more appropriate.
Apart from failing to capture reordering perfor-
mance, another common criticism of most cur-
rent automatic MT metrics is that a particular
score value reported does not give insights into
quality (Przybocki et al, 2009). This is because
there is no intrinsic significance of a difference
in scores. Ideally, the scores that the metrics re-
port would be meaningful and stand on their own.
However, the most one can say is that higher is
better for accuracy metrics and lower is better for
error metrics.
We present a novel metric, the LRscore, which
explicitly measures the quality of word order in
machine translations. It then combines the re-
ordering metric with a metric measuring lexical
success. This results in a comprehensive met-
327
ric which measures the two most fundamental as-
pects of translation. We argue that the LRscore
is intuitive and meaningful because it is a simple,
decomposable metric with only one parameter to
train.
The LRscore has many of the properties that are
deemed to be desirable in a recent metric eval-
uation campaign (Przybocki et al, 2009). The
LRscore is language independent. The reorder-
ing component relies on abstract alignments and
word positions and not on words at all. The lex-
ical component of the system can be any mean-
ingful metric for a particular target language. In
our experiments we use 1-gram BLEU and 4-gram
BLEU, however, if a researcher was interested in
morphologically rich languages, a different met-
ric which scores partially correct words might be
more appropriate. The LRscore is a shallow met-
ric, which means that it is reasonably fast to run.
This is important in order to be useful for train-
ing of the translation model parameters. A final
advantage is that the LRscore is a sentence level
metric. This means that human judgments can be
directly compared to system scores and helps re-
searchers to understand what changes they are see-
ing between systems.
In this paper we start by describing the reorder-
ing metrics and then we present the LRscore. Fi-
nally we discuss related work and conclude.
2 Reordering Metrics
The relative ordering of words in the source and
target sentences is encoded in alignments. We
can interpret algnments as permutations. This
allows us to apply research into metrics for or-
dered encodings to our primary tasks of measur-
ing and evaluating reorderings. A word alignment
over a sentence pair allows us to transcribe the
source word positions in the order of the aligned
target words. Permutations have already been
used to describe reorderings (Eisner and Tromble,
2006), primarily to develop a reordering model
which uses ordering costs to score possible per-
mutations. Here we use permutations to evaluate
reordering performance based on the methods pre-
sented in (Birch et al, 2010).
The ordering of the words in the target sentence
can be seen as a permutation of the words in the
source sentence. The source sentence s of length
N consists of the word positions s0 ? ? ? si ? ? ? sN .
Using an alignment function where a source word
at position i is mapped to a target word at position
j with the function a : i ? j, we can reorder the
source word positions to reflect the order of the
words in the target. This gives us a permutation.
A permutation is a bijective function from a set
of natural numbers 1, 2, ? ? ? , N to itself. We will
name our permutations pi and ?. The ith symbol
of a permutation pi will be denoted as pi(i), and
the inverse of the permutation pi?1 is defined so
that if pi(i) = j then pi?1(j) = i. The identity, or
monotone, permutation id is the permutation for
which id(i) = i for all i. Table 1 shows the per-
mutations associated with the example alignments
in Figure 1. The permutations are calculated by
iterating over the source words, and recording the
ordering of the aligned target words.
Permutations encode one-one relations,
whereas alignments contain null alignments and
one-many, many-one and many-many relations.
For now, we make some simplifying assumptions
to allow us to work with permutations. Source
words aligned to null (a(i) ? null) are assigned
the target word position immediately after the
target word position of the previous source word
(pi(i) = pi(i ? 1) + 1). Where multiple source
words are aligned to the same target word or
phrase, a many-to-one relation, the target ordering
is assumed to be monotone. When one source
word is aligned to multiple target words, a one-to-
many relation, the source word is assumed to be
aligned to the first target word.
A translation can potentially have many valid
word orderings. However, we can be reason-
ably certain that the ordering of reference sentence
must be acceptable. We therefore compare the or-
dering of a translation with that of the reference
sentence. The underlying assumption is that most
reasonable word orderings should be fairly similar
to the reference. The assumption that the reference
is somehow similar to the translation is necessary
for all automatic machine translation metrics. We
propose using permutation distance metrics to per-
form the comparison.
There are many different ways of measuring
distance between two permutations, with different
solutions originating in different domains (statis-
tics, computer science, molecular biology, . . . ).
Real numbered data leads to measures such as Eu-
clidean distance, binary data to measures such as
Hamming distance. But for ordered sets, there
are many different options, and the best one de-
328
t1
t2
t3
t4
t5
t6
t7
t8
t9
t10
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(a)
t1
t2
t3
t4
t6
t5
t7
t8
t9
t10
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(b)
t6
t7
t8
t9
t10
t1
t2
t3
t4
t5
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(c)
t10
t1
t2
t3
t4
t5
t6
t7
t8
t9
s1 s2 s3 s4 s5 s6 s7 s8 s9 s1
0
(d)
Figure 1: Synthetic examples: a translation and three reference scenarios. (a) is a monotone translation,
(b) is a reference with one short distance word order difference, (c) is a reference where the order of the
two halves has been swapped, and (d) is a reference with a long distance reordering of the first target
word.
pends on the task at hand. We choose a few
metrics which are widely used, efficient to calcu-
late and capture certain properties of the reorder-
ing. In particular, they are sensitive to the num-
ber of words that are out of order. Three of the
metrics, Kendall?s tau, Spearman?s rho and Spear-
man?s footrule distances also take into account the
distance between positions in the reference and
translation sentences, or the size of the reordering.
An obvious disadvantage of this approach is the
fact that we need alignments, either between the
source and the reference, and the source and the
translation, or directly between the reference and
the translation. If accuracy is paramount, the test
set could include manual alignments and the sys-
tems could directly output the source-translation
alignments. Outputting the alignment informa-
tion should require a trivial change to the decoder.
Alignments can also be automatically generated
using the alignment model that aligns the training
data.
Distance metrics increase as the quality of trans-
lation decreases. We invert the scale of the dis-
(a) (1 2 3 4 5 6 7 8 9 10)
(b) (1 2 3 4 ?6 ?5 ?7 8 9 10)
(c) (6 7 8 9 10 ?1 2 3 4 5)
(d) (2 3 4 5 6 7 8 9 10 ?1)
Table 1: Permutations extracted from the sentence
pairs shown in Figure 1: (a) is a monotone permu-
tation and (b), (c) and (d) are permutations with
different amounts of disorder, where bullet points
highlight non-sequential neighbors.
tance metrics in order to easily compare them with
other metrics where increases in the metrics mean
increases in translation quality. All permutation
distance metrics are thus subtracted from 1. Note
that the two permutations we refer to pi and ? are
relative to the source sentence, and not to the ref-
erence: the source-reference permutation is com-
pared to the source-translation permutation.
2.1 Hamming Distance
The Hamming distance (Hamming, 1950) mea-
sures the number of disagreements between two
329
permutations. The Hamming distance for permu-
tations was proposed by (Ronald, 1998) and is also
known as the exact match distance. It is defined
as follows:
dH(pi, ?) = 1?
?n
i=1 xi
n
where xi =
{
0 if pi(i) = ?(i)
1 otherwise
Where pi, ? are the two permutations and the
normalization constant Z is n, the length of the
permutation. We are interested in the Hamming
distance for its ability to capture the amount of ab-
solute disorder that exists between two permuta-
tions. The Hamming distance is widely utilized in
coding theory to measure the discrepancy between
two binary sequences.
2.2 Kendall?s Tau Distance
Kendall?s tau distance is the minimum number
of transpositions of two adjacent symbols nec-
essary to transform one permutation into an-
other (Kendall, 1938; Kendall and Gibbons,
1990). This is sometimes known as the swap dis-
tance or the inversion distance and can be inter-
preted as a function of the probability of observing
concordant and discordant pairs (Kerridge, 1975).
It is defined as follows:
d? (pi, ?) = 1?
?n
i=1
?n
j=1 zij
Z
where zij =
{
1 if pi(i) < pi(j) and ?(i) > ?(j)
0 otherwise
Z =
(n2 ? n)
2
The Kendall?s tau metric is possibly the most in-
teresting for measuring reordering as it is sensitive
to all relative orderings. It consequently measures
not only how many reordering there are but also
the distance that words are reordered.
In statistics, Spearman?s rho and Kendall?s tau
are widely used non-parametric measures of as-
sociation for two rankings. In natural language
processing research, Kendall?s tau has been used
as a means of estimating the distance between
a system-generated and a human-generated gold-
standard order for the sentence ordering task (La-
pata, 2003). Kendall?s tau has also been used
in machine translation as a cost function in a re-
ordering model (Eisner and Tromble, 2006) and
an MT metric called ROUGE-S (Lin and Och,
2004) is similar to a Kendall?s tau metric on lexical
items. ROUGE-S is an F-measure of ordered pairs
of words in the translation. As far as we know,
Kendall?s tau has not been used as a reordering
metric before.
3 LRscore
The goal of much machine translation research is
either to improve the quality of the words used in
the output, or their ordering. We use the reordering
metrics and combine them with a measurement of
lexical performance to produce a comprehensive
metric, the LRscore. The LRscore is a linear in-
terpolation of a reordering metric with the BLEU
score. If we use the 1-gram BLEU score, BLEU1,
then the LRscore relies purely upon the reorder-
ing metric for all word ordering evaluation. We
also use the 4-gram BLEU score, BLEU4, as it is
an important baseline and the values it reports are
very familiar to machine translation researchers.
BLEU4 also contains a notion of word ordering
based on longer matching n-grams. However, it
is aware only of very local orderings. It does not
measure the magnitude of the orderings like the
reordering metrics do, and it is dependent on ex-
act lexical overlap which does not affect the re-
ordering metric. The two components are there-
fore largely orthogonal and there is a benefit in
combining them. Both the BLEU score and the
reordering distance metric apply a brevity penalty
to account for translations of different lengths.
The formula for calculating the LRscore is as
follows:
LRscore = ? ?R+ (1? ?)BLEU
Where the reordering metricR is calculated as fol-
lows:
R = d ?BP
Where we either take the Hamming distance dH
or the Kendall?s tau distance d? as the reordering
distance d and then we apply the brevity penalty
BP . The brevity penalty is calculated as:
BP =
{
1 if t > r
e1?r/t if t ? r
where t is the length of the translation, and r is
the closest reference length. R is calculated at the
sentence level, and the scores are averaged over a
test set. This average is then combined with the
330
system level lexical score. The Lexical metric is
the BLEU score which sums the log precision of
n-grams. In our paper we set the n-gram length to
either be one or four.
The only parameter in the metric ? balances the
contribution of reordering and the lexical compo-
nents. There is no analytic solution for optimizing
this parameter, and we use greedy hillclimbing in
order to find the optimal setting. We optimize the
sentence level correlation of the metric to human
judgments of accuracy as provided by the WMT
2010 shared task. As hillclimbing can end up in a
local minima, we perform 20 random restarts, and
retaining only the parameter value with the best
consistency result. Random-restart hill climbing is
a surprisingly effective algorithm in many cases. It
turns out that it is often better to spend CPU time
exploring the space, rather than carefully optimiz-
ing from an initial condition.
The brevity penalty applies to both the reorder-
ing metric and the BLEU score. We do not set
a parameter to regulate the impact of the brevity
penalty, as we want to retain BLEU scores that are
comparable with BLEU scores computed in pub-
lished research. And as we do not regulate the
brevity penalty in the BLEU score, we do not wish
to do so for the reordering metric either. It there-
fore impacts on both the reordering and the lexical
components equally.
4 Correlation with Human Judgments
It has been common to use seven-point fluency
and adequacy scores as the main human evalua-
tion task. These scores are intended to be absolute
scores and comparable across sentences. Seven-
point fluency and adequacy judgements are quite
unreliable at a sentence level and so it seems du-
bious that they would be reliable across sentences.
However, having absolute scores does have the ad-
vantage of making it easy to calculate the correla-
tion coefficients of the metric with human judge-
ments. Using rank judgements, we do not have
absolute scores and thus we cannot compare trans-
lations across different sentences.
We therefore take the method adopted in the
2009 workshop on machine translation (Callison-
Burch et al, 2009). We ascertained how consis-
tent the automatic metrics were with the human
judgements by calculating consistency in the fol-
lowing manner. We take each pairwise compari-
son of translation output for single sentences by a
Metric de-en es-en fr-en cz-en
BLEU4 58.72 55.48 57.71 57.24
LR-HB1 60.37 60.55 58.59 53.70
LR-HB4 60.49 58.88 58.80 57.74
LR-KB1 60.67 58.54 58.46 54.20
LR-KB4 61.07 59.86 58.59 58.92
Table 2: The percentage consistency between hu-
man judgements of rank and metrics. The LRscore
variations (LR-*) are optimised for consistency for
each language pair.
particular judge, and we recorded whether or not
the metrics were consistent with the human rank.
Ie. we counted cases where both the metric and the
human judged agree that one system is better than
another. We divided this by the total umber of pair-
wise comparisons to get a percentage. There were
many ties in the human data, but metrics rarely
give the same score to two different translations.
We therefore excluded pairs that the human anno-
tators ranked as ties. The human ranking data and
the system outputs from the 2009 Workshop on
Machine Translation (Callison-Burch et al, 2009)
have been used to evaluate the LRscore.
We optimise the sentence level consistency of
the metric. As hillclimbing can end up in a local
minima, we perform 20 random restarts, and re-
taining only the parameter value with the best con-
sistency result. Random-restart hill climbing is a
surprisingly effective algorithm in many cases. It
turns out that it is often better to spend CPU time
exploring the space, rather than carefully optimis-
ing from an initial condition.
Table 2 reports the optimal consistency of the
LRscore and baseline metrics with human judge-
ments for each language pair. The table also
reports the individual component results. The
LRscore variations are named as follows: LR
refers to the LRscore, ?H? refers to the Hamming
distance and ?K? to Kendall?s tau distance. ?B1?
and ?B4? refer to the smoothed BLEU score with
the 1-gram and 4-gram scores. The LRscore is the
metric which is most consistent with human judge-
ment. This is an important result which shows
that combining lexical and reordering information
makes for a stronger metric.
5 Related Work
(Wong and Kit, 2009) also suggest a metric which
combines a word choice and a word order com-
331
ponent. They propose a type of F-measure which
uses a matching function M to calculate precision
and recall. M combines the number of matched
words, weighted by their tfidf importance, with
their position difference score, and finally sub-
tracting a score for unmatched words. Includ-
ing unmatched words in the in M function un-
dermines the interpretation of the supposed F-
measure. The reordering component is the average
difference of absolute and relative word positions
which has no clear meaning. This score is not intu-
itive or easily decomposable and it is more similar
to METEOR, with synonym and stem functional-
ity mixed with a reordering penalty, than to our
metric.
6 Conclusion
We propose the LRscore which combines a lexi-
cal and a reordering metric. This results in a met-
ric which is both meaningful and accurately mea-
sures the word order performance of the transla-
tion model.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT Evaluation: Evaluating Re-
ordering. Machine Translation (to appear).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal
permutations in machine translation. In Proceed-
ings of the HLT-NAACL Workshop on Computation-
ally Hard Problems and Joint Inference in Speech
and Language Processing, pages 57?75, New York,
June.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
26(2):147?160.
M. Kendall and J. Dickinson Gibbons. 1990. Rank
Correlation Methods. Oxford University Press,
New York.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30:81?89.
D Kerridge. 1975. The interpretation of rank correla-
tions. Applied Statistics, 2:257?258.
S. Krauwer. 1993. Evaluation of MT systems: a pro-
grammatic view. Machine Translation, 8(1):59?66.
Mirella Lapata. 2003. Probabilistic text structur-
ing: Experiments with sentence ordering. Compu-
tational Linguistics, 29(2):263?317.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Juraf-
sky, and Christopher D. Manning. 2009. Measur-
ing machine translation quality as semantic equiva-
lence: A metric based on entailment features. Ma-
chine Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics,
pages 311?318, Philadelphia, USA.
Mark Przybocki, Kay Peterson, Se?bastien Bronsart,
and Gregory Sanders. 2009. The nist 2008 metrics
for machine translation challengeoverview, method-
ology, metrics, and results. Machine Translation.
S Ronald. 1998. More distance functions for order-
based encodings. In the IEEE Conference on Evolu-
tionary Computation, pages 558?563.
Matthew Snover, Bonnie Dorr, R Schwartz, L Micci-
ulla, and J Makhoul. 2006. A study of translation
edit rate with targeted human annotation. In AMTA.
B. Wong and C. Kit. 2009. ATEC: automatic eval-
uation of machine translation via word choice and
word order. Machine Translation, pages 1?15.
332
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177?186,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Multiple-stream Language Models for Statistical Machine Translation
Abby Levenberg
Dept. of Computer Science
University of Oxford
ablev@cs.ox.ac.uk
Miles Osborne
School of Informatics
University of Edinburgh
miles@inf.ed.ac.uk
David Matthews
School of Informatics
University of Edinburgh
dave.matthews@ed.ac.uk
Abstract
We consider using online language models for
translating multiple streams which naturally
arise on the Web. After establishing that us-
ing just one stream can degrade translations
on different domains, we present a series of
simple approaches which tackle the problem
of maintaining translation performance on all
streams in small space. By exploiting the dif-
fering throughputs of each stream and how
the decoder translates prior test points from
each stream, we show how translation perfor-
mance can equal specialised, per-stream lan-
guage models, but do this in a single language
model using far less space. Our results hold
even when adding three billion tokens of addi-
tional text as a background language model.
1 Introduction
There is more natural language data available today
than there has ever been and the scale of its produc-
tion is increasing quickly. While this phenomenon
provides the Statistic Machine Translation (SMT)
community with a potentially extremely useful re-
source to learn from, it also brings with it nontrivial
computational challenges of scalability.
Text streams arise naturally on the Web where
millions of new documents are published each day in
many different languages. Examples in the stream-
ing domain include the thousands of multilingual
websites that continuously publish newswire stories,
the official proceedings of governments and other
bureaucratic organisations, as well as the millions
of ?bloggers? and host of users on social network
services such as Facebook and Twitter.
Recent work has shown good results using an in-
coming text stream as training data for either a static
or online language model (LM) in an SMT setting
(Goyal et al, 2009; Levenberg and Osborne, 2009).
A drawback of prior work is the oversimplified sce-
nario that all training and test data is drawn from the
same distribution using a single, in-domain stream.
In a real world scenario multiple incoming streams
are readily available and test sets from dissimilar do-
mains will be translated continuously. As we show,
using stream data from one domain to translate an-
other results in poor average performance for both
streams. However, combining streams naively to-
gether hurts performance further still.
In this paper we consider this problem of multiple
stream translation. Since monolingual data is very
abundant, we focus on the subtask of updating an on-
line LM using multiple incoming streams. The chal-
lenges in multiple stream translation include dealing
with domain differences, variable throughput rates
(the size of each stream per epoch), and the need
to maintain constant space. Importantly, we impose
the key requirement that our model match transla-
tion performance reached using the single stream ap-
proach on all test domains.
We accomplish this using the n-gram history of
prior translations plus subsampling to maintain a
constant bound on memory required for language
modelling throughout all stream adaptation. In par-
ticular, when considering two test streams, we are
able to improve performance on both streams from
an average (per stream) BLEU score of 39.71 and
37.09 using a single stream approach (Tables 2 and
3) to an average BLEU score of 41.28 and 42.73 us-
ing multiple streams within a single LM using equal
memory (Tables 6 and 7). We also show additive im-
177
provements using this approach when using a large
background LM consisting of over one billion n-
grams. To our knowledge our approach is the first
in the literature to deal with adapting an online LM
to multiple streams in small space.
2 Previous Work
2.1 Randomised LMs
Randomised techniques for LMs from Talbot and
Osborne (2007) and Talbot and Brants (2008) are
currently industry state-of-the-art for fitting very
large datasets into much smaller amounts of mem-
ory than lossless representations for the data. Instead
of representing the n-grams exactly, the randomised
representation exchanges a small, one-sided error of
false positives for massive space savings.
2.2 Stream-based LMs
An unbounded text stream is an input source of natu-
ral language documents that is received sequentially
and so has an implicit timeline attached. In Leven-
berg and Osborne (2009) a text stream was used to
initially train and subsequently adapt an online, ran-
domised LM (ORLM) with good results. However,
a weakness of Levenberg and Osborne (2009) is that
the experiments were all conducted over a single in-
put stream. It is an oversimplification to assume that
all test material for a SMT system will be from a sin-
gle domain. No work was done on the multi-stream
case where we have more than one incoming stream
from arbitrary domains.
2.3 Domain Adaptation for SMT
Within MT there has been a variety of approaches
dealing with domain adaptation (for example (Wu et
al., 2008; Koehn and Schroeder, 2007)). Our work
is related to domain adaptation but differs in that we
are not skewing the distribution of an out-of-domain
LM to accommodate some test data for which we
have little or no training data for. Rather, we have
varying amounts of training data from all the do-
mains via the incoming streams and the LM must
account for each domain appropriately. However,
known domain adaptation techniques are potentially
applicable to multi-stream translation as well.
3 Multiple Streams and their Properties
Any source that provides a continuous sequence
of natural language documents over time can be
thought of as an unbounded stream which is time-
stamped and access to it is given in strict chronolog-
ical order. The ubiquity of technology and the In-
ternet means there are many such text streams avail-
able already and their number is increasing quickly.
For SMT, multiple text streams provide a potentially
abundant source of new training data that may be
useful for combating model sparsity.
Of primary concern is building models whose
space complexity is independent of the size of the
incoming stream. Allowing unbounded memory to
handle unbounded streams is unsatisfactory. When
dealing with more than one stream we must also
consider how the properties of single streams inter-
act in a multiple stream setting.
Every text stream is associated with a particular
domain. For example, we may draw a stream from
a newswire source, a daily web crawl of new blogs,
or the output of a company or organisation. Obvi-
ously the distribution over the text contained in these
streams will be very different from each other. As
is well-known from the work on domain adaptation
throughout the SMT literature, using a model from
one domain to translate a test document from an-
other domain would likely produce poor results.
Each stream source will also have a different
rate of production, or throughput, which may vary
greatly between sources. Blog data may be received
in abundance but the newswire data may have a sig-
nificantly lower throughput. This means that the text
stream with higher throughput may dominate and
overwhelm the more nuanced translation options of
the stream with less data in the LM during decod-
ing. This is bad if we want to translate well for all
domains in small space using a single model.
4 Multi-Stream Retraining
In a stream-based translation setting we can expect
to translate test points from various domains on any
number of incoming streams. Our goal is a single
unified LM that obtains equal performance in less
space than when using a separate LM per stream.
The underlying LMs could be exact, but here we use
randomised versions based on the ORLM.
178
?stream a1LM2m3
stream a1LM2mi
stream a1LM2mn
pum3
pumi
pumK
Naive Combination Approach
tLwmLroch tLwmLroch
Figure 1: In the naive approach all K streams are simply
combined into a single LM for each new epoch encoun-
tered.
Given an incoming number K of unbounded
streams over a potentially infinite timeline T , with
t ? T an epoch or windowed subset of the timeline,
the full set of n-grams in all K streams over all T
is denoted with S. By St we denote n-grams from
all K streams and Skt, k ? [1,K], as the n-grams
in the kth stream over epoch t. Since the streams
are unbounded, we do not have access to all the n-
grams in S at once. Instead we select n-grams from
each stream Skt ? S. We define the collection of
n-grams encoded in the LM at time t over all K
streams as Ct. Initially, at time t = 0 the LM is
composed of the n-grams in the stream so C0 = S0.
Since it is unsatisfactory to allow unbounded
memory usage for the model and more bits are
needed as we see more novel n-grams from the
streams, we enforce a memory constraint and use
an adaptation scheme to delete n-grams from the
LM Ct?1 before adding any new n-grams from the
streams to get the current n-gram set Ct. Below
we describe various approaches of updating the LM
with data from the streams.
4.1 Naive Combinations
Approach The first obvious approach for an online
LM using multiple input streams is to simply store
all the streams in one LM. That is, n-grams from
all the streams are only inserted into the LM once
and their stream specific counts are combined into a
single value in the composite LM.
Modelling the Stream In the naive case we retrain
the LM Ct in full at epoch t using all the new data
from the streams. We have simply
Ct =
K
?
k=1
Skt (1)
stream 1 LM 1
stream 1 LM 2
stream 1 LM 3
input stream 1
stream 2 LM 1
stream 2 LM 2
stream 2 LM 3
input stream 2
?
stream K LM 1
stream K LM 2
stream K LM 3
input stream K
Multiple LM Approach
new epoch new epoch
Figure 2: Each stream 1 . . . K gets its own stream-based
LM using the multiple LM approach.
where each of the K streams is combined into a sin-
gle model and the n-grams counts are merged lin-
early. Here we carry no n-grams over from the LM
Ct?1 from the previous epoch. The space needed is
the number of unique n-grams present in the com-
bined streams for each epoch.
Resulting LM To query the resulting LM Ct dur-
ing decoding with a test n-gram wni = (wi, . . . , wn)
we use a simple smoothing algorithm called Stupid
Backoff (Brants et al, 2007). This returns the
probability of an n-gram as
P (wi|wi?1i?n+1) :=
?
?
?
Ct(wii?n+1)
Ct(wi?1i?n+1)
if Ct(wii?n+1) > 0
?P (wi|wi?1i?n+2) otherwise
(2)
where Ct(.) denotes the frequency count returned by
the LM for an n-gram and ? is a backoff parameter.
The recursion ends once the unigram is reached in
which case the probability is P (wi) := wi/N where
N is the size of the current training corpus.
Each stream provides a distribution over the n-
grams contained in it and, for SMT, if a separate
LM was constructed for each domain it would most
likely cause the decoder to derive different 1-best
hypotheses than using a LM built from all the stream
data. Using the naive approach blurs the distribution
distinctions between streams and negates any stream
specific differences when the decoder produces a 1-
best hypothesis. It has been shown that doing lin-
ear combinations of this type produces poor perfor-
mance in theory (Mansour et al, 2008).
179
4.2 Weighted Interpolation
Approach An improved approach to using multi-
ple streams is to build a separate LM for each stream
and using a weighted combination of each during
decoding. Each stream is stored in isolation and we
interpolate the information contained within each
during decoding using a weighting on each stream.
Modelling the Stream Here we model the streams
by simply storing each stream at time t in its own
LM so Ckt = Skt for each stream Sk. Then the LM
after epoch t is
Ct = {C1t, . . . , CKt}.
We use more space here than all other approaches
since we must store each n-gram/count occurring in
each stream separately as well as the overhead in-
curred for each separate LM in memory.
Resulting LM During decoding, the probability of
a test n-gram wni is a weighted combination of all
the individual stream LMs. We can write
P (wni ) :=
K
?
k=1
fkPCkt(wni ) (3)
where we query each of the individual LMs Ckt to
get a score from each LM using Equation 2 and
combine them together using a weighting fk spe-
cific to each LM. Here we impose the restriction on
the weights that
?K
k=1 fk = 1. (We discuss specific
weight selections in the next section.)
By maintaining multiple stream specific LMs we
maintain the particular distribution of the individual
streams. This keeps the more nuanced translations
from the lower throughput streams available during
decoding without translations being dominated by a
stream with higher throughput. However using mul-
tiple distinct LMs is wasteful of memory.
4.3 Combining Models via History
Approach We want to combine the streams into
a single LM using less memory than when storing
each stream separately but still achieve at least as
good a translation for each test point. Naively com-
bining the streams removes stream specific transla-
tions but using the history of n-grams selected by the
decoder during the previous test point in the stream
was done in Levenberg and Osborne (2009) for the
single stream case with good results. This is appli-
cable to the multi-stream case as well.
Modelling the Stream For multiple streams and
epoch t > 0 we model the stream combination as
Ct = fT (Ct?1) ?
K
?
k=1
(Skt). (4)
where for each epoch a selected subset of the previ-
ous n-grams in the LM Ct?1 is merged with all the
newly arrived stream data to create the new LM set
Ct. The parameter fT denotes a function that filters
over the previous set of n-grams in the model. It
represents the specific adaptation scheme employed
and stays constant throughout the timeline T . In this
work we consider any n-grams queried by the de-
coder in the last test point as potentially useful to
the next point. Since all of the n-grams St in the
stream at time t are used the space required is of the
same order of complexity as the naive approach.
Resulting LM Since all the n-grams from the
streams are now encoded in a single LM Ct we can
query it using Equation 2 during decoding. The goal
of retraining using decoding history is to keep use-
ful n-grams in the current model so a better model
is obtained and performance for the next transla-
tion point is improved. Note that making use of the
history for hypothesis combination is theoretically
well-founded and is the same approach used here for
history based combination. (Mansour et al, 2008)
4.4 Subsampling
Approach The problem of multiple streams with
highly varying throughput rates can be seen as a type
of class imbalance problem in the machine learning
literature. Given a binary prediction problem with
two classes, for instance, the imbalance problem oc-
curs when the bulk of the examples in the training
data are instances of one class and only a much
smaller proportion of examples are available from
the other class. A frequently used approach to bal-
ancing the distribution for the statistical model is to
use random under sampling and select only a sub-
set of the dominant class examples during training
(Japkowicz and Stephen, 2002).
This approach is applicable to the multiple stream
translation problem with imbalanced throughput
rates between streams. Instead of storing the n-
grams from each stream separately, we can apply a
180
?stream a1LM2m3
stream a1LM2mi
stream a1LM2mn
pum3
LM 2 + (subset of LM 1)
LM 3 + (subset of LM 2)
Naive ComebnatAvaetoprr eAch
new epoch new epoch
SMT Decoder
Figure 3: Using decoding history all the streams are com-
bined into a unified LM.
subsampling selection scheme directly to the incom-
ing streams to balance each stream?s contribution in
the final LM. Note that subsampling is also related
to weighting interpolation. Since all returned LM
scores are based on frequency counts of the n-grams
and their prefixes, taking a weighting on a full prob-
ability of an n-gram is akin to having fewer counts
of the n-grams in the LM to begin with.
Modelling the Stream To this end we use the
weighted function parameter fk from Equation 3 to
serve as the sampling probability rate for accepting
an n-gram from a given stream k. The sampling
rate serves to limit the amount of stream data from
a stream that ends up in the model. For K > 1 we
have
Ct = fT (Ct?1) ?
K
?
k=1
fk(Skt) (5)
where fk is the probability a particular n-gram from
stream Sk at epoch t will be included in Ct. The
adaptation function fT remains the same as in Equa-
tion 4. The space used in this approach is now de-
pendent on the rate fk used for each stream.
Resulting LM Again, since we obtain a single LM
from all the streams, we use Equation 2 to get the
probability of an n-gram during decoding.
The subsampling method is applicable to all of the
approaches discussed in this section. However, since
we are essentially limiting the amount of data that
we store in the final LM we can expect to take a per-
formance hit based on the rate of acceptance given
by the parameters fk. By using subsampling with
the history combination approach we obtain good
performance for all streams in small space.
Stream 1-grams 3-grams 5-grams
EP 19K 520K 760K
GW (xie) 120K 3M 5M
RCV1 630K 21M 42M
Table 1: Sample statistics of unique n-gram counts from
the streams from epoch 2 of our timeline. The throughput
rate varies a lot between streams.
5 Experiments
Here we report on our SMT experiments with multi-
ple streams for translation using the approaches out-
lined in the previous section.
5.1 Experimental Setup
The SMT setup we employ is standard and all re-
sources used are publicly available. We translate
from Spanish into English using phrase-based de-
coding with Moses (Koehn and Hoang, 2007) as our
decoder. Our parallel data came from Europarl.
We use three streams (all are timestamped):
RCV1 (Rose et al, 2002), Europarl (EP) (Koehn,
2003), and Gigaword (GW) (Graff et al, 2007). GW
is taken from six distinct newswire sources but in
our initial experiments we limit the incoming stream
from Gigaword to one of the sources (xie). GW and
RCV1 are both newswire domain streams with high
rates of incoming data whereas EP is a more nu-
anced, smaller throughput domain of spoken tran-
scripts taken from sessions of the European Parlia-
ment. The RCV1 corpus only spans one calender
year from October, 1996 through September, 1997
so we selected only data in this time frame from
the other two streams so our timeline consists of the
same full calendar year for all streams.
For this work we use the ORLM. The crux of the
ORLM is an online perfect hash function that pro-
vides the ability to insert and delete from the data
structure. Consequently the ORLM has the abil-
ity to adapt to an unbounded input stream whilst
maintaining both constant memory usage and error
rate. All the ORLMs were 5-gram models built with
training data from the streams discussed above and
used Stupid Backoff smoothing for n-gram scoring
(Brants et al, 2007). All results are reported using
the BLEU metric (Papineni et al, 2001).
For testing we held-out three random test points
181
LM Type Test 1 Test 2 Test 3
RCV1 (Static) 39.30 38.28 33.06
RCV1 (Online) 39.30 40.64 39.19
EP (Online) 30.22 30.31 26.66
RCV1+EP (Online) 39.00 40.15 39.46
RCV1+EP+GW (Online) 41.29 41.73 40.41
Table 2: Results for the RCV1 test points. RCV1 and GW
streams are in-domain and EP is out-of-domain. Transla-
tion results are improved using more stream data since
most n-grams are in-domain to the test points.
from both the RCV1 and EP stream?s timeline for
a total of six test points. This divided the streams
into three epochs, and we updated the online LM
using the data encountered in the epoch prior to each
translation point. The n-grams and their counts from
the streams are combined in the LM using one of the
approaches from the previous section.
Using the notation from Section 4 we have the
RCV1, EP, and GW streams described above and
K = 3 as the number of incoming streams from two
distinct domains (newswire and spoken dialogue).
Our timeline T is one year?s worth of data split into
three epochs, t ? {1, 2, 3}, with test points at the
end of each epoch t. Since we have no test points
from the GW stream it acts as a background stream
for these experiments. 1
5.2 Baselines and Naive Combinations
In this section we report on our translation exper-
iments using a single stream and the naive linear
combination approach with multiple incoming data
streams from Section 4.1.
Using the RCV1 corpus as our input stream we
tested single stream translation first. Here we are
replicating the experiments from Levenberg and Os-
borne (2009) so both training and test data comes
from a single in-domain stream. Results are in Table
2 where each row represents a different LM type.
RCV1 (Static) is the traditional baseline with no
adaptation where we use the training data for the first
epoch of the stream. RCV1 (Online) is the online
LM adapted with data from the in-domain stream.
Confirming the previous work we get improvements
1A background stream is one that only serves as training
data for all other test domains.
LM Type Test 1 Test 2 Test 3
EP (Static) 42.09 44.15 36.42
EP (Online) 42.09 45.94 37.22
RCV1 (Online) 36.46 42.10 32.73
EP+RCV1 (Online) 40.82 44.07 35.01
EP+RCV1+GW (Online) 40.91 44.05 35.56
Table 3: EP results using in and out-of-domain streams.
The last two rows show that naive combination gets poor
results compared to single stream approaches.
when using an online LM that incorporates recent
data against a static baseline.
We then ran the same experiments using a stream
generated from the EP corpus. EP consists of the
proceedings of the European Parliament and is a sig-
nificantly different domain than the RCV1 newswire
stream. We updated the online LM using n-grams
from the latest stream epoch before translating each
in-domain EP test set. Results are in Table 3 and fol-
low the same naming convention as Table 2 (except
now in-domain is EP and out-of-domain is RCV1).
Using a single stream we also cross tested and
translated each test point using the online LM
adapted on the out-of-domain stream. As expected,
translation performance decreases (sometimes dras-
tically) in this case since the data of the out-of-
domain stream are not suited to the domain of the
current test point being translated.
We then tested the naive approach and combined
both streams into a single LM by taking the union of
the n-grams and adding their counts together. This
is the RCV1+EP (Online) row in Tables 2 and 3 and
clearly, though it contains more data compared to
each single stream LM, the naively combined LM
does not help the RCV1 test points much and de-
grades the performance of the EP translation results.
This translation hit occurs as the throughput of each
stream is significantly different. The EP stream con-
tains far less data per epoch than the RCV1 counter-
part (see Table 1) hence using a naive combination
means that the more abundant newswire data from
the RCV1 stream overrides the probabilities of the
more domain specific EP n-grams during decoding.
When we added a third newswire stream from a
portion of GW, shown in the last row of Tables 2
and 3, improvements are obtained for the RCV1 test
182
Weighting Test 1 Test 2 Test 3
.33R + .33E + .33G 38.97 39.78 35.66
.50R + .25E + .25G 39.59 40.40 37.22
.25R + .50E + .25G 36.57 38.03 34.23
.70R + 0.0E + .30G 40.54 41.46 39.23
Table 4: Weighted LM interpolation results for the RCV1
test points where E = Europarl, R = RCV1, and G =
Gigaword (xie).
points due to the addition of in-domain data but the
EP test performance still suffers.
This highlights why naive combination is unsat-
isfactory. While using more in-domain data aids
in the translation of the newswire tests, for the EP
test sets, naively combining the n-grams from all
streams means the hypotheses the decoder selects
are weighted heavily in favor of the out-of-domain
data. As the out-of-domain stream?s throughput is
significantly larger it swamps the model.
5.3 Interpolating Weighted Streams
Straightforward linear stream combination into a
single LM results in degradation of translations for
test points whose in-domain training data is drawn
from a stream with lower throughput than the other
data streams. We could maintain a separate MT sys-
tem for each streaming domain but intuitively some
combination of the streams may benefit average per-
formance since using all the data available should
benefit test points from streams with low through-
put. To test this we used an alternative approach de-
scribed in Section 4.2 and used a weighted combi-
nation of the single stream LMs during decoding.
We tested this approach using our three streams:
RCV1, EP and GW (xie). We used a separate
ORLM for each stream and then, during testing, the
result returned for an n-gram queried by the decoder
was a value obtained from some weighted interpola-
tion of each individual LM?s score for that n-gram.
To get the interpolation weights for each streaming
LM we minimised the perplexity of all the mod-
els on held-out development data from the streams.
2 Then we used the corresponding stream specific
2Due to the lossy nature of the encoding of the ORLM
means that the perplexity measures were approximations.
Nonetheless the weighting from this approach had the best per-
formance.
Weighting Test 1 Test 2 Test 3
.33E + .33R + .33G 40.75 45.65 35.77
.50E + .25R + .25G 41.46 46.37 36.94
.25E + .50R + .25G 40.57 44.90 35.77
.70E + .20R + .10G 42.47 46.83 38.08
Table 5: EP results in BLEU for the interpolated LMs.
weights to decode the test points from that domain.
Results are shown in Tables 4 and 5 using the
weighting scheme described above plus a selec-
tion of random parameter settings for comparison.
Using the notation from Section 4.2, a caption of
?.5R+ .25E+ .25G?, for example, denotes a weight-
ing of fRCV 1 = 0.5 for the scores returned from the
RCV1 stream LM while fEP and fGW = 0.25 for
the EP and GW stream LMs.
The weighted interpolation results suggest that
while naive combination of the streams may be mis-
guided, average translation performance can be im-
proved upon when using more than a single in-
domain stream. Comparing the best results in Tables
2 and 3 to the single stream baselines in Tables 4 and
5 we achieve comparable, if not improved, transla-
tion performance for both domains. This is espe-
cially true for test domains such as EP which have
low training data throughput from the stream. Here
adding some information from the out-of-domain
stream that contains a lot more data aids signifi-
cantly in the translation of in-domain test points.
However, the optimal weighting differs between
each test domain. For instance, the weighting that
gives the best results for the EP tests results in much
poorer translation performance for the RCV1 test
points requiring us to track which stream we are
decoding and then select the appropriate weighting.
This adds unnecessary complexity to the SMT sys-
tem. And, since we store each stream separately,
memory usage is not optimal using this scheme.
5.4 History and Subsampling
For space efficiency we want to represent multi-
ple streams non-redundantly instead of storing each
stream/domain in its own LM. Here we report on
experiments using both the history combination and
subsampling approaches from Sections 4.3 and 4.4.
Results are in Tables 6 and 7 for the RCV1 and
183
LM Type Test 1 Test 2 Test 3
Multi-fk 41.19 41.73 39.23
Multi-fT 41.29 42.23 40.51
Multi-fk + fT 41.19 42.52 40.12
Table 6: RCV1 test results using history and subsampling
approaches.
LM Type Test 1 Test 2 Test 3
Multi-fk 40.91 43.50 36.11
Multi-fT 40.91 47.84 39.29
Multi-fk + fT 40.91 48.05 39.23
Table 7: Europarl test results with history and subsam-
pling approaches.
EP test sets respectively with the column headers
denoting the test point. The row Multi-fk shows
results using only the random subsampling param-
eter fk and the rows Multi-fT show results with just
the time-based adaptation parameter without sub-
sampling. The final row Multi-fk + fT uses both
the f parameters with random subsampling as well
as taking decoding history into account.
Multi-fk uses the random subsampling parame-
ter fk to filter out higher order n-grams from the
streams. All n-grams that are sampled from the
streams are then combined into the joint LM. The
counts of n-grams sampled from more than one
stream are added together in the composite LM. The
parameter fk is set dependent on a stream?s through-
put rate, we only subsample from the streams with
high throughput, and the rate was chosen based on
the weighted interpolation results described previ-
ously. In Tables 6 and 7 the subsampling rate fk =
0.3 for the combined newswire streams RCV1 and
GW and we kept all of the EP data. We experi-
mented with various other values for the fk sampling
rates and found translation results only minorly im-
pacted. Note that the subsampling is truly random
so two adaptation runs with equal subsampling rates
may produce different final translations. Nonethe-
less, in our experiments we saw expected perfor-
mance, observing slight variation in performance for
all test points that correlated to the percentage of in-
domain data residing in the model.
The next row, Multi-fT , uses recency criteria to
keep potentially useful n-grams but uses no subsam-
pling and accepts all n-grams from all streams into
the LM. Here we get better results than naive combi-
nation or plain subsampling at the expense of more
memory for the same error rate for the ORLM.
The final row, Multi-fk + fT uses both the sub-
sampling function fk and fT so maintains a history
of the n-grams queried by the decoder for the prior
test points. This approach achieves significantly bet-
ter results than naive adaptation and compares to us-
ing all the data in the stream. Combining translation
history as well as doing random subsampling over
the stream means we match the performance of but
use far less memory than when using multiple online
LMs whilst maintaining the same error rate.
5.5 Experiments Summary
We have shown that using data from multiple
streams benefits SMT performance. Our best ap-
proach, using history based combination along with
subsampling, combines all incoming streams into a
single, succinct LM and obtains translation perfor-
mance equal to single stream, domain specific LMs
on all test domains. Crucially we do this in bounded
space, require less memory than storing each stream
separately, and do not incur translation degradations
on any single domain.
A note on memory usage. The multiple LM ap-
proach uses the most memory since this requires
all overlapping n-grams in the streams to be stored
separately. The naive and history combination ap-
proaches use less memory since they store all n-
grams from all the streams in a unified LM. For the
sampling the exact amount of memory is of course
dependent on the sampling rate used. For the results
in Tables 6 and 7 we used significantly less memory
(300MB) but still achieved comparable performance
to approaches that used more memory by storing the
full streams (600MB).
6 Scaling Up
The experiments described in the preceding section
used combinations of relatively small (compared to
current industry standards) input streams. The ques-
tion remains if using such approaches aids in the per-
formance of translation if used in conjunction with
large static LMs trained on large corpora. In this
section we describe scaling up the previous stream-
184
Order Count
1-grams 3.7M
2-grams 46.6M
3-grams 195.5M
4-grams 366.8M
5-grams 454.2M
Total 1067M
Table 8: Singleton-pruned n-gram counts (in millions)
for the GW3 background LM.
LM Type Test 1 Test 2 Test 3
GW (static) 41.69 42.40 35.48
+ RCV1 (online) 42.44 43.83 40.55
+ EP (online) 42.80 43.94 38.82
Table 9: Test results for the RCV1 stream using the large
background LM. Using stream data benefits translation.
based translation experiments using a large back-
ground LM trained on a billion n-grams.
We used the same setup described in Section 5.1.
However, instead of using only a subset of the GW
corpus as one of our incoming streams, we trained
a static LM using the full GW3 corpus of over three
billion tokens and used it as a background LM. As
the n-gram statistics for this background LM show
in Table 8, it contains far more data than each of the
stream specific LMs (Table 1). We tested whether
using streams atop this large background LM had a
positive effect on translation for a given domain.
Baseline results for all test points using only the
GW background LM are shown in the top row in
Tables 9 and 10. We then interpolated the ORLMs
with this LM. For each stream test point we interpo-
lated with the big GW LM an online LM built with
the most recent epoch?s data. Here we used sepa-
rate models per stream so the RCV1 test points used
the GW LM along with a RCV1 specific ORLM. We
used the same mechanism to obtain the interpolation
weights as described in Section 5.3 and minimised
the perplexity of the static LM along with the stream
specific ORLM. Interestingly, the tuned weights re-
turned gave approximately a 50-50 weighting be-
tween LMs and we found that simply using a 50-50
weighting for all test points resulted had no negative
effect on BLEU. In the third row of the Tables 9 and
10 we show the results of interpolating the big back-
LM Type Test 1 Test 2 Test 3
GW (static) 40.78 44.26 34.36
+ EP (online) 43.94 47.82 38.71
+ RCV1 (online) 43.07 47.72 39.15
Table 10: EP test results using the background GW LM.
ground LM with ORLMs built using the approach
described in Section 4.4. In this case all streams
were combined into a single LM using a subsam-
pling rate for higher order n-grams. As before our
sampling rate for the newswire streams was 30%
chosen by the perplexity reduction weights.
The results show that even with a large amount
of static data adding small amounts of stream spe-
cific data relevant to a given test point has an im-
pact on translation quality. Compared to only us-
ing the large background model we obtain signifi-
cantly better results when using a streaming ORLM
to compliment it for all test domains. However the
large amount of data available to the decoder in
the background LM positively impacts translation
performance compared to single-stream approaches
(Tables 2 and 3). Further, when we combine the
streams into a single LM using the subsampling ap-
proach we get, on average, comparable scores for all
test points. Thus we see that the patterns for multi-
ple stream adaptation seen in previous sections hold
in spite of big amounts of static data.
7 Conclusions and Future Work
We have shown how multiple streams can be effi-
ciently incorporated into a translation system. Per-
formance need not degrade on any of the streams.
As well, these results can be additive. Even when
using large amounts of additional background data,
adding stream specific data continues to improve
translation. Further, we achieve all results in
bounded space. Future work includes investigating
more sophisticated adaptation for multiple streams.
We also plan to explore alternative ways of sampling
the stream when incorporating data.
Acknowledgements
Special thanks to Adam Lopez and Conrad Hughes
and Phil Blunosm for helpful discussion and advice.
This work was sponsored in part by the GALE pro-
185
gram, DARPA Contract No. HR0011-06-C-0022
and by ESPRC Grant No. EP/I010858/1bb.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In North American Chapter of the
Association for Computational Linguistics (NAACL),
Boulder, CO.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium (LDC-2007T07).
Nathalie Japkowicz and Shaju Stephen. 2002. The class
imbalance problem: A systematic study. Intell. Data
Anal., 6:429?449, October.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn. 2003. Europarl: A multilingual corpus
for evaluation of machine translation. Available at:
http://www.statmt.org/europarl/.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Domain adaptation with multiple
sources. In NIPS, pages 1041?1048.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The reuters corpus volume 1 - from yester-
days news to tomorrows language resources. In In
Proceedings of the Third International Conference on
Language Resources and Evaluation, pages 29?31.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 993?
1000. Coling 2008 Organizing Committee, August.
186
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 401?409,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing
Matt Post? and Chris Callison-Burch?? and Miles Osborne?
?Human Langage Technology Center of Excellence, Johns Hopkins University
?Center for Language and Speech Processing, Johns Hopkins University
?School of Informatics, University of Edinburgh
Abstract
Recent work has established the efficacy of
Amazon?s Mechanical Turk for constructing
parallel corpora for machine translation re-
search. We apply this to building a collec-
tion of parallel corpora between English and
six languages from the Indian subcontinent:
Bengali, Hindi, Malayalam, Tamil, Telugu,
and Urdu. These languages are low-resource,
under-studied, and exhibit linguistic phenom-
ena that are difficult for machine translation.
We conduct a variety of baseline experiments
and analysis, and release the data to the com-
munity.
1 Introduction
The quality of statistical machine translation (MT)
systems is strongly related to the amount of paral-
lel text available for the language pairs. However,
most language pairs have little or no readily available
bilingual training data available. As a result, most
contemporary MT research tends to opportunisti-
cally focus on language pairs with large amounts of
parallel data.
A consequence of this bias is that language ex-
hibiting certain linguistic phenomena are underrep-
resented, including languages with complex mor-
phology and languages with divergent word order-
ings. In this paper, we describe our work gather-
ing and refining document-level parallel corpora be-
tween English and each of six verb-final languages
spoken on the Indian subcontinent: Bengali, Hindi,
Malayalam, Tamil, Telugu, and Urdu. This paper?s
contributions are as follows:
? We apply an established protocol for using
Amazon?s Mechanical Turk (MTurk) to collect
parallel data to train and evaluate translation
systems for six Indian languages.
? We investigate the relative performance of syn-
tactic translation models over hierarchical ones,
showing that syntax results in higher BLEU
scores in most cases.
? We explore the impact of training data quality
on the quality of the resulting model.
? We release the corpora to the research commu-
nity under the Creative Commons Attribution-
Sharealike 3.0 Unported License (CC BY-SA
3.0).1
2 Why Indian languages?
Indian languages are important objects of study for
a number of reasons. These languages are low-
resource languages in terms of the availability of
MT systems2 (and NLP tools in general) yet together
they represent nearly half a billion native speakers
(Table 1). Their speakers are well-educated, with
many of them speaking English either natively or as a
second language. Together with the degree of Inter-
net penetration in India, it is reasonably straightfor-
ward to find and hire non-expert translators through
crowdsourcing services like Amazon?s Mechanical
Turk.
1joshua-decoder.org/indian-parallel-corpora
2See sampark.iiit.ac.in/sampark/web/index.php/
content for a notable growing effort.
401
??????? ???? ???????????? ?????
senator her remarks prepared
Figure 1: An example of SOV word ordering in Tamil.
Translation: The senator prepared her remarks.
??? ?? ? ??
walk CONT PAST 1p
Figure 2: An example of the morphology of the Bengali
word ?????????, meaning [I] was walking. CONT denotes
the continuous aspect, while PAST denotes past tense.
In addition to a general desire to collect suitable
training corpora for low-resource languages, Indian
languages demonstrate a variety of linguistic phe-
nomena that are divergent from English and under-
studied. One example is head-finalness, exhibited
most obviously in a subject-object-verb (SOV) pat-
tern of sentence structure, in contrast to the gen-
eral SVO ordering of English sentences. One of
the motivations underlying linguistically-motivated
syntactic translation systems like GHKM (Galley et
al., 2004; Galley et al, 2006) or SAMT (Zollmann
and Venugopal, 2006) is to describe such transfor-
mations. This difference in word order has the po-
tential to serve as a better test bed for syntax-based
MT3 compared to translating between English and
European languages, most of which largely share its
word order. Figure 1 contains an example of SOV
reordering in Tamil.
A second important phenomenon present in these
languages is a high degree of morphological com-
plexity relative to English (Figure 2). Indian lan-
guages can be highly agglutinative, which means
that words are formed by concatenating morpholog-
ical affixes that convey information such as tense,
person, number, gender, mood, and voice. Mor-
phological complexity is a considerable hindrance at
all stages of the MT pipeline, but particularly align-
ment, where inflectional variations mask patterns
from alignment tools that treat words as atoms.
3Weuse hierarchical to denote translation grammars that use
only a single nonterminal (Chiang, 2007), in contrast to syntac-
tic systems, which make use of linguistic annotations (Zollmann
and Venugopal, 2006; Galley et al, 2006).
language script family L1
Bengali ????? Indo-Aryan 181M
Hindi ???? ?????? Indo-Aryan 180M
Malayalam ?????? Dravidian 35M
Tamil ????? Dravidian 65M
Telugu ?????? Dravidian 69M
Urdu ???? Indo-Aryan 60M
Table 1: Languages. L1 is the worldwide number of na-
tive speakers according to Lewis (2009).
3 Data collection
The source of the documents for our translation task
for each of the languages in Table 1 was the set of
the top-100 most-viewed documents from each lan-
guage?s Wikipedia. These lists were obtained us-
ing page view statistics compiled from dammit.lt/
wikistats over a one year period. We did not apply
any filtering for topic or content. Table 2 contains
a manually categorized list of documents for Hindi,
with some minimal annotations indicating how the
documents relate to those in the other languages.
These documents constitute a diverse set of topics,
including culture, the internet, and sex.
We collected the parallel corpora using a three-
step process designed to ensure the integrity of the
non-professional translations. The first step was to
build a bilingual dictionary (?3.1). These dictionar-
ies were used to bootstrap the experimental controls
in the collection of four translations of each source
sentence (?3.2). Finally, as a measure of data qual-
ity, we independently collect votes on the which of
the four redundant translations is the best (?3.3).
3.1 Dictionaries
A key component of managing MTurk workers is to
ensure that they are competently and conscientiously
undertaking the tasks. As non-speakers of all of the
Indian languages, we had no simple and scalable way
to judge the quality of the workers? translations. Our
solutionwas to bootstrap the process by first building
bilingual dictionaries for each of the datasets. The
dictionaries were then used to produce glosses of the
complete source sentences, which we compared to
the translations produced by the workers as a rough
means of manually gauging trust (?3.2).
The dictionaries were built in a separate MTurk
402
PLACES PEOPLE PEOPLE TECHNOLOGY LANGUAGE AND RELIGION
Agra A. P. J. Abdul Kalam Premchand Blog CULTURE Bhagavad Gita
Bihar Aishwarya Rai Rabindranath Tagore Google Ayurveda Diwali
China Akbar Rani Lakshmibai Hindi Web Resources Constitution of India Hanuman
Delhi Amitabh Bachchan Sachin Tendulkar Internet Cricket Hinduism
Himalayas Barack Obama Sarojini Naidu Mobile phone English language Hinduism
India Bhagat Singh Subhas Chandra Bose News aggregator Hindi Cable News Holi
Mumbai Dainik Jagran Surdas RSS Hindi literature Islam
Nepal Gautama Buddha Swami Vivekananda Wikipedia Hindi-Urdu grammar Mahabharata
Pakistan Harivansh Rai Bachchan Tulsidas YouTube Horoscope Puranas
Rajasthan Indira Gandhi Indian cuisine Quran
Red Fort Jaishankar Prasad THINGS SEX Sanskrit Ramayana
Taj Mahal Jawaharlal Nehru Air pollution Anal sex Standard Hindi Shiva
United States Kabir Earth Kama Sutra Shiva
Uttar Pradesh Kalpana Chawla Essay Masturbation EVENTS Taj Majal: Shiva Temple?
Mahadevi Varma Ganges Penis History of India Vedas
Meera General knowledge Sex positions World War II Vishnu
Mohammed Rafi Global warming Sexual intercourse
Mohandas Karamchand Gandhi Pollution Vagina
Mother Teresa Solar energy
Navbharat Times Terrorism
Table 2: The 100 most viewed Hindi Wikipedia articles (titles translated to English using inter-language links and
Google translate and manually categorized). Entries in bold were present in the top 100 lists of at least four of the
Indian top 100 lists. Earth, India,World War II, and Wikipedia were in the top 100 lists of all six languages.
language entries translations
Bengali 4,075 6,011
Hindi - -
Malayalam 41,502 144,505
Tamil 11,592 69,128
Telugu 12,193 38,532
Urdu 26,363 113,911
Table 3: Dictionary statistics. Entries is the number of
source-language types, while translations lists the num-
ber of words or phrases they translated to (i.e., the num-
ber of pairs in the dictionary). Controls for Hindi were
obtained using Google translate, the only one of these lan-
guages that were available at the outset of this project.
task, in which workers were asked to translate sin-
gle words and short phrases from the complete set of
Wikipedia documents. For each word, MTurk work-
ers were presented with three sentences containing
that word, which provided context. The control for
this task was obtained from the Wikipedia article ti-
tles which are linked across languages, and can thus
be assumed to be translations of each other. Workers
who performed too poorly on these known transla-
tions had their work rejected.
Table 3 lists the size of the dictionaries we con-
structed.
3.2 Translations
With the dictionaries in hand, we moved on to trans-
late the entireWikipedia documents. Each human in-
telligence task (HIT) posted onMTurk contained ten
sequential source-language sentences from a doc-
ument, and asked the worker to enter a free-form
translation for each. We collected four translations
from different translators for each source sentence.
To discourage cheating through cutting-and-pasting
into automatic translation systems, sentences were
presented as images. Workers were paid $0.70 per
HIT. We then manually determined whether to ac-
cept or reject a worker?s HITs based on a review of
each worker?s submissions, which included a com-
parison of the translations to a monotonic gloss (pro-
duced with the dictionary), the percentage of empty
translations, the amount of time the worker took to
complete the HIT, geographic location (self-reported
and geolocated by way of the worker?s IP address),
and by comparing different translations of the same
source segments against one another.
We obtained translations of the source-language
documents in a relatively short amount of time. Fig-
ure 3 depicts the number of translations collected as
a function of the amount of time from the posting of
the task. Malayalam provided the highest through-
put, generating half a million words in just under a
403
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 3: The total volume of translations (measured in
English words) as a function of elapsed days. For Malay-
alam, we collected half a million words of translations in
just under a week.
week. For comparison, the Europarl corpus (Koehn,
2005) has about 50million words of English for each
of the Spanish and French parallel corpora.
As has been previously reported (Zbib et al,
2012), cost is another advantage of building train-
ing data on Mechanical Turk. Germann (2001) puts
the cost of professionally translated English at about
$0.30 perword for translation fromTamil. Our trans-
lations were obtained for less than $0.01 per word.
The rate of collection could likely be increased by
raising these payments, but it is unclear whether
quality would be affected by raising the base pay
(although it could be improved by paying for sub-
sequent quality control HITs, like editing).
The tradeoff for low-cost translations is increased
variance in translation quality when compared to the
more consistently-good professional translations.
Figure 4 contains some hand-picked examples of the
sorts of translations we obtained. Later, in the Exper-
iments section (?4), we will investigate the effects
this variance in translation quality has on the qual-
ity of the models that can be constructed. For now,
the variancemotivated the collection of an additional
dataset, described in the next section.
3.3 Votes
A prevailing issue with translations collected on
MTurk is the prevalence of low-quality translations.
Quality suffers for a variety of reasons: Turkers
lack formal training, often translate into a nonna-
tive tongue, may give insufficient attention to the
task, and likely desire to maximize their throughput
(and thus their wage). Unlike Zaidan and Callison-
Burch (2011), who embed controls containing source
language sentences with known professional trans-
lations, we had no professionally translated data.
Therefore, we could not measure the BLEU score of
the Turkers.
Motivated by desire to have some measure of the
relative quality and variance of the translations, we
designed another task in which we presented an in-
dependent set of Turkers with an original sentence
and its four translations, and asked them to vote on
which was best.4 Five independent workers voted
on the translations of each source sentence. Tallying
the resulting votes, we found that roughly 65% of
the sentences had five votes cast on just one or two
of the translations, and about 95% of the sentences
had all the votes cast on one, two, or three sentences.
This suggests both (1) that there was a difference in
the quality of the translations, and (2) the voters were
able to discern these differences, and took their task
seriously enough to report them.
3.4 Data sets
For each parallel corpus, we created a standardized
test set in the following manner. We first manu-
ally assigned each of the Wikipedia documents for
each language into one of the following nine cate-
gories: EVENTS, LANGUAGE AND CULTURE,
PEOPLE, PLACES, RELIGION, SEX, TECHNOL-
OGY, THINGS, or MISC. We then assigned doc-
uments to training, development, development test,
and test sets in round-robin fashion using a ratio of
roughly 7:1:1:1. For training data, each source sen-
tence was repeated four times in order to allow it
to be paired with each of its translations. For the
development and test sets, the multiple translations
served as alternate references. Table 4 lists sentence-
and word-level statistics for the datasets for each lan-
guage pair (these counts are prior to any tokeniza-
tion).
4We did not collect votes for Malayalam.
404
?????? 15,2007??? ???????????? ?????? ?????? ???? ?????? ???????????.
In March 15,2007 Wiki got a place in Oxford English dictionary.
On March 15, 2007 wiki was included in the Oxford English dictionary. (5)
ON MARCH 15, 2007, WIKI FOUND A PLACE IN THE OXFORD ENGLISH DICTIONARY
March 15, 2007 oxford english index of wiki?s place.
Figure 4: An example of the variance in translation quality for the human translations of a Tamil sentence; the format-
ting of the translations has been preserved exactly. The parenthesized number indicates the number of votes received
in the voting task (?3.3).
language dict train dev devtest test
Bengali 16k 539k 63k 61k 69k
6k 20k 914 907 1k
Hindi 0 1,249k 67k 98k 74k
0 37k 1k 993 1k
Malayalam 410k 664k 61k 68k 70k
144k 29k 1k 1k 1k
Tamil 189k 747k 62k 53k 54k
69k 35k 1k 1k 1k
Telugu 106k 951k 52k 45k 49k
38k 43k 1k 916 1k
Urdu 253k 1,198k 67k 49k 42k
113k 33k 736 777 605
Table 4: Data set sizes for each language pair: words in
the first row, parallel sentences in the second. (The dictio-
naries contains short phrases in addition to words, which
accounts for the difference in dictionary word and line
counts.)
4 Experiments
In this section, we present experiments on the col-
lected data sets in order to quantify their perfor-
mance. The experiments aim to address the follow-
ing questions:
1. How well can we translate the test sets?
2. Do linguistically motivated translation models
improve translation results?
3. What is the effect of data quality onmodel qual-
ity?
4.1 Setup
A principal point of comparison in this paper is be-
tween Hiero grammars (Chiang, 2007) and SAMT
grammars (Zollmann and Venugopal, 2006), the lat-
ter of which make use of linguistic annotations to
improve nonterminal reordering. These grammars
were trained with the Thrax grammar extractor us-
ing its default settings, and translated using Joshua
(Weese et al, 2011). We tuned with minimum error-
rate training (Och, 2003) using Z-MERT (Zaidan,
2009) and present the mean BLEU score on test
data over three separate runs (Clark et al, 2011).
MBR reranking (Kumar and Byrne, 2004) was ap-
plied to Joshua?s 300-best (unique) output, and eval-
uation was conducted with case-insensitive BLEU
with four references.
The training data was produced by pairing a
source sentence with each of its four translations.
We also added the dictionaries to the training data.
We built five-gram language models from the target
side of the training data using interpolated Kneser-
Ney smoothing. We also experimented with a larger-
scale language model built from English Gigaword,
but, notably, found a drop of over a point in BLEU
score. This points forward to some of the difficul-
ties encountered with the lack of text normalization,
discussed in ?5.
4.2 Baseline translations
We begin by presenting BLEU scores for Hiero and
SAMT translations of each of the six Indian language
test sets (Table 5). For comparison purposes, we
also present BLEU scores from Google translations
of these languages (where available).
We observe that systems built with SAMT gram-
mars improve measurably above the Hiero models,
with the exception of Tamil and Telugu. As an ex-
ternal reference point, the Google baseline transla-
tion scores far surpass the results of any of our sys-
tems, but were likely constructed from much larger
datasets.
Table 6 lists some manually-selected examples of
405
language Hiero SAMT diff Google
Bengali 12.72 13.53 +0.81 20.01
Hindi 15.53 17.29 +1.76 25.21
Malayalam 13.72 14.28 +0.56 -
Tamil 9.81 9.85 +0.04 13.51
Telugu 12.46 12.61 +0.15 16.03
Urdu 19.53 20.99 +1.46 23.09
Table 5: BLEU scores translating into English (four ref-
erences). BLEU scores are the mean of three MERT runs.
the sorts of translations we obtained from our sys-
tems. While anecdotal and not characteristic of over-
all quality, together with the generally good BLEU
scores, these examples provide a measure of the abil-
ity to obtain good translations from this dataset.
4.3 Voted training data
We noted above the high variance in the quality of
the translations obtained on MTurk. For data col-
lection efforts, there is a question of how much time
and effort to invest in quality control, since it comes
at the expense of simply collecting more data. We
can either collect additional redundant translations
(to increase quality) or translate more foreign sen-
tences (to increase coverage).
To test this, we constructed two smaller datasets,
each making use of only one of the four translations
of each source sentence:
? Selected randomly
? Selected by choosing the translation that re-
ceived a plurality of the votes (?3.3), breaking
ties randomly (best)
We again included the dictionaries in the training
data (where available). Table 7 contains results on
the same test sets as before. These results do not
clearly indicate that quality control through redun-
dant translations are worth the extra expense. Novot-
ney and Callison-Burch (2010) had a similar finding
for crowdsourced transcriptions.
5 Further Analysis
The previous section has shown that reasonable
BLEU scores can be obtained from baseline transla-
tion systems built from these corpora. While trans-
lation quality is an issue (for example, very lit-
?????????? ????? ?????
in srilanka solar government
chola rule in sri lanka
in srilanka chozhas ruled
chola reign in sri lanka
Figure 5: An example of inconsistent orthography. Words
in bold are translations of the second Tamil word.
eral translations, etc), the previous section?s voted
dataset experiments suggest this is not one of the
most important issues to address.
In this section, we undertake a manual analysis of
the collected datasets to inform future work. There
are a number of issues that arise due to non-Roman
scripts, high-variance translation quality, and the rel-
atively small amount of training data.
5.1 Orthographic issues
Manual analysis demonstrates that inconsistencies
with orthography are a serious problem. An exam-
ple of this can be found in Figure 5, which contains
a set of translations of a Tamil sentence. In particu-
lar, the spelling of the Tamil word ????? has three
different realizations among the sentence?s transla-
tions. The discrepancy between zha and la is due
to phonetic variants (phonetic similarity may also
account for the word solar). This discrepancy is
present throughout the training and test data, where
the -la variant is preferred to -zha by about 6:1 (the
counts are 848 and 142, respectively).
In addition to mistakes potentially caused by for-
eign scripts, there are many mistakes that are sim-
ply spelling errors. Table 8 contains examples of
misspellings (along with their counts) in the train-
ing portion of the Urdu-English dataset. As a point
of comparison, there are no misspellings of the word
in Europarl.
Such errors are present in many collections, of
course, but they are particularly harmful in small
datasets, and they appear to be especially prevalent
in datasets like these, translated as they were by non-
native speakers. Whether caused by Turker care-
lessness or difficulty in translation from non-Roman
scripts, these are common issues, solutions for which
could yield significant improvement in translation
performance.
406
Bengali ?? ????? ???? ???? ???? ?????????????? ??????? ??? ?
Hiero in this time dhaka university was established on the year 1921 .
SAMT in this time dhaka university was established in 1921 .
Malayalam ????????? ???????????? ?????????? ? ?????? 5 , 700 ?k ?????? ??????????????? .
Hiero the surface temperature of sun 5 , 700 degree k to down to .
SAMT temperature in the surface of the sun 5 , 700 degree k to down to .
Table 6: Some example translations.
Hiero SAMT
language random best random best
Bengali 9.43 9.29 9.65 9.50
Hindi 11.74 12.18 12.61 12.69
Tamil 7.73 7.48 7.88 7.76
Telugu 10.49 10.61 10.75 10.72
Urdu 13.51 14.26 14.63 16.03
Table 7: BLEU scores translating into English on a quar-
ter of the training data (plus dictionary), selected in two
ways: best (result of vote), and random. There is little
difference, suggesting quality control may not be terribly
important. We did not collect votes for Malayalam.
misspelling count
japenese 91
japans 40
japenes 9
japenies 3
japeneses 3
japeneese 1
japense 1
Table 8: Misspellings of japanese (947) in the training
portion of the Urdu-English data, along with their counts.
5.2 Alignments
Inconsistent orthography fragments the training
data, exacerbating problems already present due to
morpohological richness. One place this is mani-
fested is during alignment, where different spellings
mask patterns from the standard alignment tech-
niques. We observe a large number of poor align-
ments, due to interactions among these problems,
as well as the small size of the training data, well-
documented alignment mistakes (such as garbage
collecting), and the divergent sentence structures. In
particular, it seems that the defacto alignment heuris-
tics may be particularly ill-suited to these language
pairs and data conditions. Figure 6 (top) contains an
example of a particularly poor alignment produced
by the default alignment heuristic, the grow-diag-
and method described in Koehn et al (2003).
As a means of testing this, we varied the align-
ment combination heuristics using five alternatives
described in Koehn et al (2003) and available in the
symal program distributed with Moses (Koehn et
al., 2007). Experiments on Tamil produce a range
of BLEU scores between 7.45 and 10.19 (each result
is the average of three MERT runs). If we plot gram-
mar size versus BLEU score, we observe a general
trend that larger grammars seem to positively cor-
relate with BLEU score. We tested this more gen-
erally across languages using the Berkeley aligner5
(Liang et al, 2006) instead of GIZA alignments, and
found a consistent increase in BLEU score for the
Hiero grammars, often putting them on par with the
original SAMT results (Table 9). Manual analysis
suggests that the Berkeley aligner produces fewer,
more reasonable-looking alignments than the Moses
heuristics (Figure 6). This suggest a fruitful ap-
proaches in revisiting assumptions underlying align-
ment heuristics.
6 Related Work
Crowdsourcing datasets has been found to be helpful
for many tasks in natural language processing. Ger-
mann (2001) showed that humans could perform sur-
prisingly well with very poor translations obtained
from non-expert translators, in part likely because
coarse-level translational adequacy is sufficient for
the tasks they evaluated. That work was also pitched
as a rapid resource acquisition task, meant to test our
ability to quickly build systems in emergency set-
tings. This work further demonstrates the ability to
quickly acquire training data for MT systems with
5code.google.com/p/berkeleyaligner/
407
X?
X X
?
X X
?
X
?
X X
X
?
?
X
?
?"#
$??'(
)?+
??./0
??3
???
.
a
a
s
a
i
w
a
s
t
h
e
f
i
r
s
t
s
u
c
c
e
s
s
f
u
l
l
m
o
v
i
e
f
o
r
a
j
i
t
h
k
u
m
a
r
.
?
X
?
?
?
?
?
X X
?
a
a
s
a
i
w
a
s
t
h
e
f
i
r
s
t
s
u
c
c
e
s
s
f
u
l
l
m
o
v
i
e
f
o
r
a
j
i
t
h
k
u
m
a
r
.
?"#
$??'(
)?+
??./0
??3
???
.
Figure 6: A bad Tamil alignment produced with the
grow-diag-and alignment combination heuristic (top); the
Berkeley aligner is better (bottom). A ? is a correct
guess, an X marks a false positive, and a ? denotes a false
negative. Hiero?s extraction heuristics yield 4 rules for
the top alignment and 16 for the bottom.
reasonable translation accuracy.
Closely related to our work here is that of Novot-
ney and Callison-Burch (2010), who showed that
transcriptions for training speech recognition sys-
tems could be obtained from Mechanical Turk with
near baseline recognition performance and at a sig-
nificantly lower cost. They also showed that redun-
dant annotation was not worthwhile, and suggested
that money was better spent obtaining more data.
Separately, Ambati and Vogel (2010) probed the
MTurk worker pool for workers capable of translat-
ing a number of low-resource languages, including
Hindi, Telugu, and Urdu, demonstrating that such
workers could be found and quantifying acceptable
grammar size
pair GIZA++ Berkeley BLEU gain
Bengali 15m 27m 13.54 +0.82
Hindi 34m 60m 16.47 +0.94
Malayalam 12m 27m 12.70 -1.02
Tamil 19m 30m 10.10 +0.29
Telugu 28m 46m 13.36 +0.90
Urdu 38m 58m 20.41 +0.88
Table 9: Hiero translation results using Berkeley align-
ments instead of GIZA++ heuristics. The gain columns
denotes improvements relative to the Hiero systems in Ta-
ble 5. In many cases (bold gains), the BLEU scores are
at or above even the SAMT models from that table.
wages and collection rates.
The techniques described here are similar to those
described in Zaidan and Callison-Burch (2011), who
showed that crowdsourcing with appropriate quality
controls could be used to produce professional-level
translations for Urdu-English translation. This pa-
per extends that work by applying their techniques
to a larger set of Indian languages and scaling it to
training-data-set sizes.
7 Summary
We have described the collection of six parallel cor-
pora containing four-way redundant translations of
the source-language text. The Indian languages of
these corpora are low-resource and understudied,
and exhibit markedly different linguistic properties
compared to English. We performed baseline exper-
iments quantifying the translation performance of a
number of systems, investigated the effect of data
quality on model quality, and suggested a number of
approaches that could improve the quality of models
constructed from the datasets. The parallel corpora
provide a suite of SOV languages for translation re-
search and experiments.
Acknowledgments We thank Lexi Birch for dis-
cussions about strategies for selecting and assem-
bling the data sets. This research was supported in
part by gifts from Google and Microsoft, the Euro-
MatrixPlus project funded by the EuropeanCommis-
sion (7th Framework Programme), and a DARPA
grant entitled ?Crowdsourcing Translation?. The
views in this paper are the authors? alone.
408
References
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, Los Angeles, California.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176?181. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, Sydney, Australia, July.
Ulrich Germann. 2001. Building a statistical ma-
chine translation system from scratch: how much
bang for the buck can we expect? In ACL work-
shop on Data-driven methods in machine translation,
Toulouse, France, July. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Edmonton, Alberta, Canada, May?June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond?ej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, Prague, Czech Republic,
June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InMachine translation
summit.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
M. Paul Lewis, editor. 2009. Ethnologue: Languages of
the World. SIL International, Dallas, TX, USA, six-
teenth edition.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL, pages 104?111.
Association for Computational Linguistics.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Proc. NAACL, Los
Angeles, California, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, Sapporo,
Japan, July.
JonathanWeese, Juri Ganitkevitch, Chris Callison-Burch,
Matt Post, and Adam Lopez. 2011. Joshua 3.0:
Syntax-based machine translation with the thrax gram-
mar extractor. InProceedings of the SixthWorkshop on
Statistical Machine Translation.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: professional quality from non-
professionals. In Proc. ACL, Portland, Oregon, USA,
June.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In Proc.
NAACL, Montreal, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, New York, New York, USA, June.
409
