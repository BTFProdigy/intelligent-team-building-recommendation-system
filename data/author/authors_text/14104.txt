Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 153?157, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
IIITH: A Corpus-Driven Co-occurrence Based Probabilistic Model for 
Noun Compound Paraphrasing  
 
Nitesh Surtani, Arpita Batra, Urmi Ghosh and Soma Paul 
Language Technologies Research Centre 
IIIT Hyderabad 
Hyderabad, Andhra Pradesh-500032 
{nitesh.surtaniug08, arpita.batra, urmi.ghosh}@students.iiit.ac.in, soma@iiit.ac.in 
  
Abstract 
This paper presents a system for automatically 
generating a set of plausible paraphrases for a 
given noun compound and rank them in de-
creasing order of their usage represented by 
the confidence value provided by the human 
annotators. Our system implements a corpus-
driven probabilistic co-occurrence based 
model for predicting the paraphrases, that uses 
a seed list of paraphrases extracted from cor-
pus to predict other paraphrases based on their 
co-occurrences. The corpus study reveals that 
the prepositional paraphrases for the noun 
compounds are quite frequent and well cov-
ered but the verb paraphrases, on the other 
hand, are scarce, revealing the unsuitability of 
the model for standalone corpus-driven ap-
proach. Therefore, to predict other paraphras-
es, we adopt a two-fold approach: (i) 
Prediction based on Verb-Verb co-
occurrences, in case the seed paraphrases are 
greater than threshold; and (ii) Prediction 
based on Semantic Relation of NC, otherwise. 
The system achieves a comparabale score of 
0.23 for the isomorphic system while main-
taining a score of 0.26 for the non-isomorphic 
system. 
1 Introduction 
Semeval 2013 Task 4 (Hendrickx et. al., 2013), 
?Free Paraphrases of Noun Compounds? is a pa-
raphrase generation task that requires the system to 
generate multiple paraphrases for a given noun 
compound and rank them to the best approxima-
tion of the human rankings, represented by the cor-
responding confidence value. The task is an 
extension of Semeval 2010 Task 9 (Butnariu et al, 
2010), where the participants were asked to rank 
the set of given paraphrases for each noun com-
pound. Although the ranking task is quite distinct 
from the task of generating paraphrases, however, 
we have taken many insights from the systems de-
veloped for the ranking task, and have reported 
them appropriately in our system description. 
This paper describes a system for generating a 
ranked set of paraphrases for a given NC. A pa-
raphrase can be Prepositional, Verb or Verb + Pre-
positional. Since the prepositional paraphrases are 
easily available in the corpus while the occurrences 
of verb or verb+prep paraphrases is scarce, the task 
of paraphrasing becomes significant in finding out 
a method for predicting reliable paraphrases with 
verbs for a given NC. Our system implements a 
model that is based on co-occurrences of the pa-
raphrases and selects those paraphrases that have a 
higher probability of co-occurring with a set of 
extracted paraphrases which are referred to as Seed 
Paraphrases. Keeping the verb-paraphrase scarcity 
issue in mind, we develop a two-way model: (i) 
Model 1 is used when the seed paraphrases are 
considerable in number i.e., greater than the thre-
shold value. In this case, other verb paraphrases are 
predicted based on their co-occurrence with the set 
of extracted verb paraphrases. (ii) Model 2 is used 
when the size of the seed list falls below the thre-
shold value, in which case, we make use of the 
prepositional paraphrases to predict the relation of 
the noun compound and select verbs that mostly 
co-occur with that relation. Our system achieves an 
isomorphic score of 0.23 with a non-isomorphic of 
0.26 with the human generated paraphrases. The 
next section discusses the system.  
2 System Description 
This section of the paper describes each module of 
the system in detail. The first module of the system 
153
talks about the Seed data extraction using corpus 
search. The next module uses the seed data for 
predicting more verbs that would be used in pa-
raphrasing. The third module uses these predicted 
verbs in template generation for generating NC 
Paraphrasing and the generated paraphrases are 
ranked in the last module. 
2.1 Seed Data Extraction Module 
We have relied mostly on the Google N-gram Cor-
pus for extracting the seed paraphrases. Google has 
publicly released their web data as n-grams, also 
known as Web-1T corpus, via the Linguistic Data 
Consortium (Brants and Franz, 2006). It contains 
sequences of n-terms that occur more than 40 times 
on the web. Since the corpus consists of raw data 
from the web, certain pre-processing steps are es-
sential before it can be used. We extract a set of 
POS templates from the training data, and general-
ize them enough to accommodate the legitimate 
paraphrases extracted from the corpus. The follow-
ing templates are used for extracting n-gram data: 
Head-Mod N-gram: This template includes both 
the head and the modifier in the same regular ex-
pression. A corresponding 5-gram template for a 
NC Amateur-Championship is shown in Table 1. 
Head <*> <*> 
<*>Mod 
championship conducted for the 
amateurs 
Head <*><*>  
Mod <*> 
championship for all amateur 
players 
Head <*>Mod 
<*><*> 
championship where amateur is 
competing 
Table 1: Templates for paraphrase extraction 
The paraphrases obtained from the above template 
are quite useful, but scarce. To overcome the issue 
of coverage of verb paraphrases, a loosely coupled 
analysis and representation of compounds can be 
employed, as suggested by (Li et.al, 2010). We 
retrieve the partial triplets from the n-gram corpus 
in the form of ?Head Para? and ?Para Modifier?. 
 
 
 
Head Template: Head <*> <*> 
Mod Template: <*> <*> Mod; <*> Mod <*> 
But the process of generating paraphrases from 
head and the modifier n-gram incorporates a huge 
amount of noise and produces a lot of irrelevant 
paraphrases. Therefore, these partial paraphrases 
are not directly used for generating the paraphrases 
but are instead used to diagnose the compatibility 
of the selected verb with the head and the modifier 
of the given NC in Section 2.2.2. We also extract 
paraphrases from ANC and BNC corpus. 
2.2 Verb Prediction Module 
This module is the heart of our system. It imple-
ments two models for predicting the verb paraph-
rases: a Verb Co-occurrence model and a Relation 
Prediction model. The decision of selection of 
model for verb prediction is based on the size of 
the seed list. If the number of seed paraphrases is 
above the threshold value, the verb co-occurrence 
model is used whereas the relation prediction mod-
el is used if it is below the threshold value. 
2.2.1 Verb Co-occurrence Model 
This model uses the seed paraphrases extracted 
from the corpus to predict other verb paraphrases 
by computing their co-occurrences. The model 
gains insights from the UCD-PN system (Nulty 
and Costello, 2010) which tries to identify a more 
general paraphrase by computing the co-
occurrence of a paraphrase with other paraphrases. 
But the task of generating paraphrases has two sub-
tle but significant differences: (i) The list of seed 
verb paraphrases for a given NC is usually small, 
with each seed verb having a corresponding proba-
bility of occurrence; and (ii) Not all the seed verbs 
have legitimate representation of the noun com-
pound. Our system incorporates these distinctions 
in the co-occurrence model discussed below. 
Using the training data at hand, we build a Verb-
Verb co-occurrence matrix, a 2-D matrix where 
each cell (i,j) represents the probability of occur-
rence of Vj when Vi has already occurred.  
? ??  ?? =
?(?? ,?? )
?(??)
=
?????(?? ,?? )
?????(??)
 
The verbs used in co-occurrence matrix are stored 
in a List A. Now, for a given test NC, the model 
extracts the seed list of verb paraphrases (referred 
as List B) from the corpus with their corresponding 
probabilities. The above model calculates a score 
for each verb in List A, by computing its co-
occurrence with the verbs in List B. 
???????? ?? =  ? ??  ?? ? ?(??)
???
 
(Head, Para, ?)  
(?, Para, Mod)  
(Head, Para, Mod)  
154
The term ?(??) in the above equation represents 
the relative occurrence of the verb ??  with the giv-
en NC. The relevance of this term becomes evident 
in the next model. The verbs achieving higher 
score are selected, suggesting a higher probability 
of co-occurrence with the seed verbs.  
2.2.2 Semantic Relation Prediction Model 
This module describes the second model of the 
two-way model, and is used by the system when 
the verbs extracted from the corpus are less than 
the threshold. In this model, we use prepositional 
paraphrases, having a pretty good coverage in the 
corpus, to predict the semantic relation of the com-
pound which helps us in predicting the other pa-
raphrases. The intuition behind using semantic 
class for predicting paraphrases is that they tend to 
capture the behavior of the noun compound and 
can be represented by general paraphrases.  
Noun Compound Relation Paraphrase Sel. 
Prep Verb 
Garden Party Location In, At Held 
Community Life Theme Of, In Made 
Advertising Agency Purpose For, Of, In Doing 
Table 2: Occurrence of Prepositional Paraphrases 
Relation Annotation: Since a supervised ap-
proach is used for identifying the semantic relation 
of the noun compound, we manually annotate the 
noun compounds with a semantic relation. We tag 
each noun compound with one semantic relation 
from the set used in (Moldovan et. al. 2004).  
Prep-Rel and Verb-Rel Co-occurrence: A Prep-
Rel co-occurrence matrix similar to Verb-Verb co-
occurrence matrix discussed in last subsection. 
This 2-D matrix consists of co-occurrence proba-
bilities between the prepositional paraphrases and 
the semantic relation of the compound, where each 
cell (i,j) represents the probability of occurrence of 
preposition Pj with relation Ri. This matrix is used 
as a model to identify semantic relation using pre-
positional paraphrases extracted from the corpus. 
The Verb-Relation co-occurrence matrix is used to 
predict the most co-occurring verbs with the identi-
fied relation. Each cell (i,j) in the matrix represents 
the probability of the verb Vj co-occurring with 
relation Ri. 
Relation Extraction: Research focusing on se-
mantic relation extraction has followed two direc-
tions: (i) Statistical approaches to using very large 
corpus (Berland and Charniak (1999); Hearst 
(1998)); and (ii) Ontology based approaches using 
hierarchical structure of wordnet (Moldovan et. al., 
2004). We employ a statistical model based on the 
Preposition-Relation co-occurrence for identifying 
the relation. The model is quite similar to the one 
used in Section 2.2, but it is here that the model 
reveals its actual power. Since two or more rela-
tions can be represented by same set of preposi-
tional paraphrases, as Theme and Purpose in Table 
2, it is important to take into account the probabili-
ties with which the extracted prepositions occur in 
the corpus. In Table 2, the NC Community Life 
(Theme) occurs frequently with preposition ?of? 
whereas the NC Advertising Agency (Purpose) is 
mostly represented by preposition ?for? in the cor-
pus. The term ?(??) in the equation below cap-
tures this phenomenon and classifies these two 
NCs in their respective classes. 
???????? ? =  ? ? ?? ? ?(??)
???
 
The relation with the highest score is selected as 
the semantic class of the noun compound. A set of 
verbs highly co-occurring with that class are se-
lected, and their compatibility with the correspond-
ing noun compound is judged from their 
occurrences with the partial head and the modifier 
paraphrases as discussed in Section 2.1. The above 
classifier performs moderately and classifies a giv-
en NC with 42.5% accuracy. We have also tried 
the Wordnet based Semantic Scattering model 
(Moldovan et. al., 2004), trained on a set of 400 
instances, but achieved an accuracy of 38%, the 
reason for which can be attributed to the small 
training set. Since the accuracy of identifying the 
correct relation is low, we select some paraphrases 
from the 2nd most probable relation, as assigned by 
the probabilistic classifier.  
2.3 Paraphrase Generator Module  
After predicting a set of verb for a test noun com-
pound, we use the following templates to generate 
the paraphrases: 
a) Head VP Mod 
b) Head VP PP Mod 
c) Head [that|which] VP PP Mod 
The paraphrases that are extracted from the corpus 
are also cleaned using the POS templates extracted 
from the training data. 
155
2.4 Paraphrase Ranker Module  
Motivated by the observations from Nulty and 
Costello (2010) that ?people tend to use general, 
semantically light paraphrases more often than de-
tailed, semantically heavy ones?, we perform rank-
ing of the paraphrases in two steps: (i) Assigning 
different weights to different type of paraphrases, 
i.e. a light weight prepositional paraphrases achiev-
ing higher score than the verb paraphrases; and (ii) 
Ranking a more general paraphrase with the same 
category higher. A paraphrase A is more general 
that paraphrase B (Nulty and Costello, 2010) if 
? ?|? > ?(?|?) 
For a list of paraphrases A generated for a given 
compound, each paraphrase b in that list is scored 
using the below eq., where more general paraph-
rase achieves a high score and is ranked higher. 
????? ? =  ? ? ? 
???
 
The seed paraphrases extracted from the corpus are 
ranked higher than the predicted paraphrases. 
3 Algorithm  
This section presents the implementation of the 
overall system.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4 Results 
The set of generated paraphrases are evaluated on 
two metrics: a) Isomorphic; b) Non-isomorphic. In 
the isomorphic setting, the test paraphrase is 
matched to the closest reference paraphrases, but 
the reference paraphrase is removed from the set 
whereas in non-isomorphic setting, the reference 
paraphrase which is mapped to a test paraphrase 
can still be used for matching other test paraphras-
es. Table 3 presents the scores of the 3 participat-
ing teams who have submitted total of 4 systems.  
Systems Isomorphic Non-Isomorphic 
SFS 0.2313 0.1794 
IIITH 0.2309 0.2583 
MELODI-Pri 0.1298 0.5484 
MELODI-Cont 0.1357 0.536 
Table 3: Results of the submitted systems 
Our system achieves an isomorphic score of 0.23, 
just below the SFS system maintaining a score of 
0.26 for the non-isomorphic system. The two va-
riants of MELODI system get a high score for the 
non-isomorphic metric but low scores for isomor-
phic metric as compared to other systems. 
5 Conclusion 
We have described a system for automatically ge-
nerating a set of paraphrases for a given noun 
compound, based on the co-occurrences of the pa-
raphrases. The system describes an approach for 
handling those 38% cases (calculated for optimum 
threshold value) of NCs where it is not convenient 
to predict the verbs using their co-occurrences with 
the seed verbs, because the size of the seed list is 
below a threshold value. For other cases, the verb 
co-occurrence model is used to predict the verbs 
for NC paraphrasing. The optimum value of thre-
shold parameter investigated from experiments is 
found to be 3, showing that atleast 3 verb paraph-
rases are necessary to capture the concept of a NC. 
// Training Phase ? Build Co-occurrence Matrices 
Verb_Co-occur = 2-D Matrix  
Prep-Rel_Co-occur = 2-D Matrix  
Verb-Rel_Co-occur = 2-D Matrix  
Verb_List = Verb List extracted from training corpus 
// Testing ? Extract paraphrases with probabilities 
Ext_Verb = List of extracted verb paraphrase  
VProb = Probability of each Ext_Verb 
Ext_Prep = List of extracted prepositional paraphrases 
PProb = Probability of each Ext_Prep 
Prob_Verb = List // Verbs with their selection score 
Prob_Rel = List // Relations with their selection score 
Threshold = 3 // Verb threshold for two-way model 
if count( Ext_Verb ) > Threshold  
    Candidate_Verbs = {Verb_List } - { Ext_Verbs }      
    foreach Candidate_Verbs Vi : 
        Prob_Verb[Vi] = 0 
        foreach Ext_Verb Vj : 
            Prob_Verb[Vi] += Verb_Co-occur [Vi][Vj] *   
   VProb[Vj] 
else       
    foreach Prep-Rel_Co-occur as rel : 
        Prob_Rel[rel] = 0 
             
            
       foreach Ext_Prep as prep : 
           Prob_Rel[rel] += Prep-Rel_Co-occur[rel][prep]
              * PProb[prep]              
           Rel=select highestProb(Prob_Rel) 
           Prob_Verb = Verb-Rel_Co-occur[Rel] 
sort(Prob_Verb)  
Verb_Predicted = select top(N)   
Paraphrase = generate_paraphrase(verb_predicted) 
rank(Paraphrase) 
156
References  
Matthew Berland and Eugene Charniak. 1999. Finding 
parts in very large. In Proceeding of ACL 1999 
T. Brants and A. Franz. 2006. Web 1T 5-gram Version1. 
Linguistic Data Consortium 
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O S? eaghdha, Stan Szpakowicz, and Tony-
Veale. 2010. Semeval-2 task 9: The interpreta-tion of 
noun compounds using paraphrasing verbs and pre-
positions. In Proceedings of the 5th SIGLEX Work-
shop on Semantic Evaluation 
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O S?eaghdha, Stan Szpakowicz, and Tony-
Veale. 2013. Semeval?13 task 4: Free Paraphrases of 
Noun Compounds. In Proceedings of the Internation-
al Workshop on Semantic Evaluation, Atlanta, Geor-
gia 
Marti Hearst. 1998. Automated Discovery of Word-Net 
relations. In An Electronic Lexical Database and-
Some of its Applications. MIT Press, Cambridge MA 
Mark Lauer. 1995. Designing Statistical Language-
Learners: Experiments on Noun Compounds. Ph.D. 
Thesis, Macquarie University 
Guofu Li, Alejandra Lopez-Fernandez and Tony Veale. 
2010. UCD-Goggle: A Hybrid System for Noun 
Compound Paraphrasing. In Proceedings of the 5th 
International Workshop on Semantic Evaluation 
(SemEval-2), Uppsala, Sweden 
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on 
Computational Lexical Semantics, pages 60?67, Bos-
ton, MA 
Paul Nulty and Fintan Costello. 2010. UCD-PN: Select-
ing general paraphrases using conditional probabili-
ty. In Proceedings of the 5th International Workshop 
on Semantic Evaluation (SemEval-2), Uppsala, Swe-
den 
 
157
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 32?38,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Syntactic Construct : An Aid for translating English Nominal Compound
into Hindi
Soma Paul
IIIT Hyderabad
soma@iiit.ac.in
Prashant Mathur
IIIT Hyderabad
mathur@research.iiit.ac.in
Sushant Kishore
IIIT Hyderabad
susanta@research.iiit.ac.in
Abstract
This paper illustrates a way of using para-
phrasal interpretation of English nominal
compound for translating them into Hindi.
Input Nominal compound is first para-
phrased automatically with the 8 preposi-
tions as proposed by Lauer (1995) for the
task. English prepositions have one-to-one
mapping to post-position in Hindi. The
English paraphrases are then translated
into Hindi using the mapping schema. We
have got an accuracy of 71% over a set of
gold data of 250 Nominal Compound. The
translation-strategy is motivated by the
following observation: It is only 50% of
the cases that English nominal compound
is translated into nominal compound in
Hindi. In other cases, they are translated
into varied syntactic constructs. Among
them the most frequent construction type
is ?Modifier + Postposition + Head?. The
translation module also attempts to deter-
mine when a compound is translated using
paraphrase and when it is translated into a
Nominal compound.
1 Introduction
Nominal Compounds are syntactically condensed
constructs which have extensively been attempted
to expand in order to unfold the meaning of the
constructions. Currently there exist two different
approaches in Computational Linguistics: (a) La-
beling the semantics of compound with a set of
abstract relations (Moldovan and Girju, 2003) (b)
Paraphrasing the compound in terms of syntac-
tic constructs. Paraphrasing, again, is done in
three ways: (1) with prepositions (?war story?
? ?story about war?) (Lauer 1995) (2) with
verb+preposition nexus (?war story? ? ?story
pertaining to war?, ?noise pollution? ? ?pol-
lution caused by noise?) (Finin 1980) (3) with
Copula (?tuna fish? ? ?fish that is tuna?) (Van-
derwende,1995). Nominal compound (henceforth
NC) is a frequently occurring construct in En-
glish1. A bigram or two word nominal compound,
is a construct of two nouns, the rightmost noun
being the head (H) and the preceding noun the
modifier (M) as found in ?cow milk?, ?road condi-
tion?, ?machine translation? and so on. Rackow et
al. (1992) has rightly observed that the two main
issues in translating the source language NC cor-
rectly into the target language involves (a) correct-
ness in the choice of the appropriate target lex-
eme during lexical substitution and (b) correctness
in the selection of the right target construct type.
The issue stated in (a) involves correct selection of
sense of the component words of NCs followed by
substitution of source language word with that of
target language that best fits for the selected sense
(see Mathur and Paul 2009).
From the perspective of machine translation, the
issue of selecting the right construct of target lan-
guage becomes very significant because English
NCs are translated into varied construct types in
Hindi. This paper motivates the advantage of ex-
panding English nominal compounds into ?para-
phrases with prepositions? for translating them
into Hindi. The English NCs are paraphrased us-
ing Lauer?s (1995) 8 prepositions. In many cases
prepositions are semantically overloaded. For ex-
ample, the NC ?Hindu law? can be paraphrased
as ?law of Hindu?. This paraphrase can mean
?Law made by Hindu? (not for Hindu people alone
though) or ?Law meant for Hindu? (law can be
made by anyone, not by the Hindus necessarily).
Such resolution of meaning is not possible from
?preposition paraphrase?. The paper argues that
this is not an issue from the point of view of trans-
1Kim and Baldwin (2005) reports that the BNC corpus (84
million words: Burnard (2000)) has 2.6% and the Reuters has
(108M words: Rose et al (2002)) 3.9% of bigram nominal
compound.
32
lation at least. It is because the Hindi correspon-
dent of ?of?, which is ?kA?, is equally ambiguous.
The translation of ?Hindu law? is ?hinduoM kA
kAnUn? and the construction can have both afore-
mentioned interpretations. Human users can se-
lect the right interpretation in the given context.
On the other hand, ?paraphrase with preposition?
approach has the following advantages: (a) An-
notation is simpler; (b) Learning is easier and (c)
Data sparseness is less; (d) Most importantly, En-
glish prepositions have one to one Hindi postpo-
sition correspondents most of the times. There-
fore we have chosen the strategy of ?paraphrasing
with prepositions? over other kind of paraphrasal
approach for the task of translation. The pa-
per explores the possibility of maintaining one to
one correspondence of English-Hindi preposition-
postpositions and examines the accuracy of trans-
lation. At this point it is worth mentioning that
translation of English NC as NC as well as differ-
ent syntactic constructs in Hindi is almost equal.
Therefore the task of translating English NCs into
Hindi is divided into two levels: (1) Paraphrases
for an NC are searched in the web corpus, (2) An
algorithm is devised to determine when the para-
phrase is to be ignored and the source language
NC to be translated as NC or transliterated in NC,
and (3) English preposition is replaced by Hindi
corresponding postposition. We have compared
our result with that of google translation system
on 250 that has been manually created.
The next section describes the data in some de-
tail. In section 3, we review earlier works that have
followed similar approaches as the present work.
Our approach is described in section 4. Finally the
result and analysis is discussed in section 5.
2 Data
We made a preliminary study of NCs in English-
Hindi parallel corpora in order to identify the dis-
tribution of various construct types in Hindi which
English NCs are aligned to. We took a paral-
lel corpora of around 50,000 sentences in which
we got 9246 sentences (i.e. 21% cases of the
whole corpus) that have nominal compounds. We
have found that English nominal compound can be
translated into Hindi in the following varied ways:
1. As Nominal Compound
?Hindu texts? ? hindU shAstroM
?milk production? ? dugdha utpAdana
2. M + Postposition + H Construction
?rice husk?? cAvala kI bhUsI,
?room temperature?? kamare ke tApa-
mAna
?wax work?? mom par citroM
?work on wax?
?body pain?? sharIra meM darda
?pain in body?
English NCs are frequently translated into
genitive2 construct in Hindi. In English ?of?
is heavily overloaded(very ambiguous), so
the genitives are in Hindi. The two other
postpositions that we see in the above data
are par ?on? and meM ?in/at? and they refer
to location.
3. As Adjective Noun Construction
?nature cure? ? prAkritika cikitsA
?hill camel? ? pahARI UMta
The words prAkrtik and pahARI being ad-
jectives derived from prakriti and pAhAR re-
spectively.
4. Single Word
?cow dung? ? gobara
The distribution of various translations is given
below:
Construction Type No. of Occurrence
Nominal Compound 3959
Genitive(of-kA/ke/kI) 1976
Purpose (for-ke liye) 22
Location (at/on-par) 34
Location (in-meM) 93
Adjective Noun Phrase 557
Single Word 766
Transliterated NC 1208
Table 1: Distribution of translations of English NC
from English Hindi parallel corpora.
There are 8% cases (see table 1) when an En-
glish NC becomes a single word form in Hindi.
For rest of the cases, they either remain as NC
(translated 43% or transliterated 13%) or corre-
spond to syntactic construct. When NC is trans-
lated as NC, they are mostly technical terms
2?of? corresponds to ?kA/ke/kI?, which are genitive
markers in Hindi.
33
or proper names. Our data shows that there are
around 40% cases when English NC is translated
as various kinds of syntactic constructs such M
+ Postposition + H, Adj + H or longer para-
phrases (?Hand luggage? ? hAth meM le jAne
vAle sAmAn ?luggage to be carried by hand?). Out
of these data, 70% cases are when English NC is
translated into M3 + postposition + H. Thus the
translation of NC into postpositional construction
is very common in Hindi.
For preparation of test data, we extracted nomi-
nal compound from BNC corpus (Burnard et al,
1995). BNC has varied amount of text ranging
from newspaper article to letters, books etc. We
extracted a sample of noun-noun bigrams from the
corpus and manually translated them into Hindi.
In this paper, we propose an algorithm that de-
termines when the syntactic paraphrase of English
NC is to be considered for translation and when it
is left for direct lexical substitution in Hindi.
3 Related Works
There exists no work which has attempted the
approach that we will be discussing here for
translating English NC into Hindi. From that per-
spective, the proposed approach is first of its kind
to be attempted. However, paraphrasing English
NCs is a widely studied issue. Scholars (Levi
1978; Finin 1980) agree there is a limited number
of relations that occur with high frequency in
noun compounds. However, the number and
the level of abstraction of these frequently used
semantic categories are not agreed upon. They
can vary from a few prepositional paraphrases
(Lauer, 1995) to hundreds and even thousands
more specific semantic relations (Finin, 1980).
Lauer (1995), for example, considers eight prepo-
sitional paraphrases as semantic classification
categories: of, for, with, in, on, at, about, and
from. According to this classification, the noun
compound ?bird sanctuary?, for instance, can
be classified both as ?sanctuary of bird? and
?sanctuary for bird?.
The automatic interpretation of noun compounds
is a difficult task for both unsupervised and super-
vised approaches. Currently, the best-performing
NC interpretation methods in computational lin-
guistics focus only on two-word noun compounds
and rely either on rather ad-hoc, domain-specific,
hand-coded semantic taxonomies, or on statistical
3M: Modifier, H: Head
models on large collections of unlabeled data.
The majority of corpus based statistical ap-
proaches to noun compound interpretation
collects statistics on the occurrence frequency
of the noun constituents and uses them in a
probabilistic model (Resnik, 1993; Lauer, 1995;
Lapata and Keller, 2004). Lauer (1995) was the
first to devise and test an unsupervised probabilis-
tic model for noun compound interpretation on
Grolier encyclopedia, an 8 million word corpus,
based on a set of 8 prepositional paraphrases. His
probabilistic model computes the probability of a
preposition p given a noun-noun pair n1-n2 and
finds the most likely prepositional paraphrase
p? = argmaxP (p|n1, n2) (1)
However, as Lauer noticed, this model requires
a very large training corpus to estimate these
probabilities. Lapata and Keller (2004) showed
that simple unsupervised models applied to the
noun compound interpretation task perform sig-
nificantly better when the n-gram frequencies are
obtained from the web (accuracy of 55.71% on Al-
tavista), rather than from a large standard corpus.
Our approach also uses web as a corpus and exam-
ines frequency of various preposition paraphrases
of a given NC. The next section describes our ap-
proach.
4 Approach
This section describes our procedure in details.
The system is comprised of the following stages:
(a) Web search of prepositional paraphrase for En-
glish NC; (b) mapping the English preposition to
corresponding Hindi postposition; (c) Evaluation
of correct paraphrasing on English side as well as
evaluation of translation.
4.1 Paraphrase Selection for Translation
Based on the observation from English-Hindi par-
allel corpus data that we examined as part of this
project, we have designed an algorithm to deter-
mine whether an English NC is to be translated
as an analytic construct or retained as an NC in
Hindi. We used Yahoo search engine to perform
a simple frequency search for ?M Preposition H?
in web corpus for a given input NC. For example,
the paraphrases obtained for the NC ?finance min-
ister? is given in table 2 and frequency of various
paraphrases is shown in the second column:
34
Paraphrase Web Frequency
minister about finance 2
minister from finance 16
minister on finance 34300
minister for finance 1370000
minister with finance 43
minister by finance 20
minister to finance 508
minister in finance 335
minister at finance 64
minister of finance 5420000
Table 2: Frequency of Paraphrases for ?finance
minister? after Web search.
In the table we notice that the distribution is
widely varied. For some paraphrase the count is
very low (minister about finance) while the high-
est count is 5420000 for ?minister of finance?. The
wide distribution is apparent even when the range
is not that high as shown in following table:
Paraphrase Web Frequency
agencies about welfare 1
agencies from welfare 16
agencies on welfare 64
agencies for welfare 707
agencies with welfare 34
agencies in welfare 299
agencies at welfare 0
agencies of welfare 92
Table 3: Frequency of Paraphrases for ?welfare
agencies? after Web search.
During our experiment we have come across
three typical cases: (a) No paraphrase is avail-
able when searched; (b) Frequency counts of some
paraphrases for a given NC is very low and (c) Fre-
quency of a number of paraphrases cross a thresh-
old limit. The threshold is set to be mean of all
the frequencies of paraphrases. Each of such cases
signifies something about the data and we build
our translation heuristics based on these observa-
tions. When no paraphrase is found in web corpus
for a given NC, we consider such NCs very close-
knit constructions and translate them as nominal
compound in Hindi. This generally happens when
the NC is a proper noun or a technical term. Sim-
ilarly when there exists a number of paraphrases
each of those crossing the threshold limit, it indi-
cates that the noun components of such NCs can
occur in various contexts and we select the first
3 paraphrase as probable paraphrase of NCs. For
example, the threshold value for the NC finance
minister is: Threshold = 6825288/8 = 853161.
The two paraphrases considered as probable para-
phrase of this NC is are therefore ?minister of fi-
nance? and ?minister for finance?. The remain-
ing are ignored. When count of a paraphrase is
less than the threshold, they are removed from the
data. We presume that such low frequency does
not convey any significance of paraphrase. On the
contrary, they add to the noise for probability dis-
tribution. For example, all paraphrases of ?ante-
lope species? except ?species of antelope? is very
low as shown in Table 4. They are not therefore
considered as probable paraphrases.
Paraphrase Web Frequency
species about antelope 0
species from antelope 44
species on antelope 98
species for antelope 8
species with antelope 10
species in antelope 9
species at antelope 8
species of antelope 60600
Table 4: Frequency of Paraphrases for antelope
species after Web search.
4.2 Mapping English Preposition to Hindi
Post-position
The strategy of mapping English preposition to
one Hindi post-position is a crucial one for the
present task of translation. The decision is mainly
motivated by a preliminary study of aligned paral-
lel corpora of English and Hindi in which we have
come across the distribution of Lauer?s 8 preposi-
tions as shown in table 5.
The table (Table 5) shows that English preposi-
tions are mostly translated into one Hindi postpo-
sition except for a few cases such as ?at?, ?with?
and ?for?. The probability of ?on? getting trans-
lating into ?ko? and ?of? into ?se? is very less
and therefore we are ignoring them in our map-
ping schema. The preposition ?at? can be trans-
lated into ?meM? and ?para? and both postposi-
tions in Hindi can refer to ?location?. However,
the two prepositions ?with? and ?for? can be trans-
lated into two distinct relations as shown in Ta-
ble 5. From our parallel corpus data, we therefore
35
Prep Post-Pos Sense Prob.
of
kA Possession 0.13
ke Possession 0.574
kI Possession 0.29
se Possession 0.002
from se Source .999
at
meM Location 0.748
par Location .219
with
se Instrument 0.628
ke sAtha Association 0.26
on
par Loc./Theme 0.987
ko Theme 0.007
about ke bAre meM Subj.Matter 0.68
in meM Location .999
for
ke lie Beneficiary 0.72
ke Possession 0.27
Table 5: Mapping of English Preposition to Hindi
postposition from alligned English-Hindi parallel
corpora.
find that these prepositions are semantically over-
loaded from Hindi language perspective. The right
sense and thereafter the right Hindi correspondent
can be selected in the context. In the present task,
we are selecting the mapping with higher prob-
ability. English Prepositions are mapped to one
Hindi Post-position for all cases except for ?at?
and ?about?.
Preposition Postposition
of kA/kI/ke
on para
for ke liye
at para/meM
in meM
from se
with ke sAtha
about
ke bAre meM
ke viSaya meM
ke sambaMdhi
Table 6: Preposition-Postposition Mapping
Post-positions in Hindi can be multi-word as in
?ke bAre meM?, ?ke liye? and so on. In the present
paper we are translating the English preposition to
the mostly probable postposition of Hindi. That
does not mean that the preposition cannot be trans-
lated into any other postposition. However, we are
taking the aforementioned stand as an preliminary
experiment and further refinement in terms of se-
lection of postposition will be done as future work.
For the present study, lexical substitution of head
noun and modifier noun are presumed to be cor-
rect.
5 Result and Analysis
In this section we will describe results of two
steps that are involved in our work: (a) Selection
of English preposition paraphrase for a given En-
glish NC; (b) Translation of English Preposition to
Hindi Post-position.
For a given NC we used a brute force method
to find the paraphrase structure. We used Lauer?s
prepositions (of, in, about, for, with, at, on, from,
to, by) for prepositional paraphrasing. Web search
is done on all paraphrases and frequency counts
are retrieved. Mean frequency (F) is calculated us-
ing all frequencies retrieved. All those paraphrases
that give frequency more than F are selected. We
first tested the algorithm on 250 test data of our
selection. The result of the top three paraphrases
are given below :
Selection Technique Precision
Top 1 61.6%
Top 2 67.20%
Top 3 71.6%
Table 7: Paraphrasing Accuracy
We have also tested the algorithm on Lauer?s
test data (first 218 compounds out 400 of NCs)
and got the following results (Table 8). Each of
the test data was marked with a preposition which
best explained the relationship between two noun
components. Lauer gives X for compounds which
cannot be paraphrased by using prepositions For
eg. tuna fish.
Prep OLauer OCI Percentage
Of 54 37 68.50%
For 42 20 47.62%
In 24 9 37.50%
On 6 2 33.33%
Table 8: Distribution of Preposition on Lauer test
data of 218 NC
OLauer : Number of occurrence of each prepo-
sition in Lauer test data
36
OCI : Number of correctly identified preposition
by our method
In Table 9 we compare our result with that of
Lauer?s on his data. We gave the results with cri-
teria: 1) only ?N prep N? is considered. 2) Non-
Prepositions (X) are also considered.
Case Our Method Lauer?s
N-prep-N 43.67% 39.87%
All 42.2% 28.8%
Table 9: Comparison of our approach with Lauer?s
Approach
Now that we have paraphrased NCs, we attempt
to translate the output into Hindi. We assume that
we have the right lexical substitution. In this pa-
per we have checked for the accuracy of the right
Hindi construction selection.
For a given NC we got the paraphrase as ?H
prep M? or ?MH?. We use English preposition
mapping as described in section 4.2 for translat-
ing NC in Hindi. For MH type compounds di-
rect lexical substitution is tried out. We tested our
approach on the gold data of 250 Nominal Com-
pounds. We translate the same 250 NCs using
google translation system in order to set up a base-
line. Google Translator could translate the data
with 68.8% accuracy.
Google returns only one translation which we
evaluated against our test data. In our case, we
have taken 3 top paraphrases as described in sec-
tion 4.1 and translated them into Hindi by using
the English preposition to Hindi postposition map-
ping schema. The following table presents the
accuracy of the translation of the top three para-
phrases
Case Precision
Top 1 61.6%
Top 2 68.4%
Top 3 70.8%
Table 10: Translation Accuracy
In this work we have not considered the context
of English NC while translating them into Hindi.
Table 11 gives the accuracy of each post-position
as translated from English preposition.
The other prepositions have occurred very less
in number and therefore not given in the table.
Preposition Post Position Accuracy
Of kA/ke/kI 94.3%
For ke liye 72.2%
In meM 42.9%
Table 11: Translation Accuracy for some individ-
ual prepositions
6 Conclusion and Future Work
This paper describes a preliminary approach for
translating English nominal compound into Hindi
using paraphrasing as a method of analysis of
source data. The result of translation is encourag-
ing as a first step towards this kind of work. This
work finds out a useful application for the task
of paraphrasing nominal compound using prepo-
sition. The next step of experiment includes the
following tasks: (a) Designing the test data in such
a way that all correspondents get equal represen-
tation in the data. (b) To examine if there are any
other prepositions (besides Lauer?s 8 preposition)
which can be used for paraphrasing (c) To use con-
text for translation.
References
Gildea. D. and Jurafsky. D. 2002. Automatic labeling
of semantic roles, Computational Linguistics 28 (3),
245-288.
Lapata, M. and Keller, F. 2004. The Web as a baseline:
evaluating the performance of unsupervised Web-
based models for a range of NLP tasks. In: Pro-
ceedings of the Human Language Technology con-
ference (HLT/NAACL), Boston, MA, pp. 121-128.
Lauer, M. 1995 Designing statistical language learn-
ers: experiments on noun compounds, Ph.D. Thesis,
Macquarie University, Australia
Moldovan, D. and Girju, R. 2003 Knowledge discov-
ery from text In: The Tutorial Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Sapporo, Japan.
Mathur, P. and Paul, S. 2009 Automatic Translation of
Nominal Compound into Hindi. In: Proceedings of
International Conference on Natural Language Pro-
cessing (ICON), Hyderabad
Moldovan, D., Girju, R., Tatu, M., and Antohe, D.
2005 On the semantics of noun compounds Com-
puter Speech & Language 19(4): 479-496
Girju, R. 2009 The Syntax and Semantics of Preposi-
tions in the Task of Automatic Interpretation of Nom-
inal Phrases and Compounds: A Cross-Linguistic
Study Computational Linguistics 35(2): 185-228
37
Vanderwende, L. 1995 The analysis of noun sequences
using semantic information extracted from on-line
dictionaries Ph.D. Dissertation, Georgetown Uni-
versity.
Barker, K. and Szpakowicz, S. 1998 Semi-automatic
recognition of noun modifier relationships In Proc.
of the 36th Annual Meeting of the ACL and 17th In-
ternational Conference on Computational Linguis-
tics (COLING/ACL-98) , pages 96-102, Montreal,
Canada.
Finin, T.W. 1980 The semantic interpretation of nom-
inal compounds In Proc. of the 1st Conference on
Artificial Intelligence (AAAI-80), 1980.
Isabelle, P. 1984 Another look at nominal com-
pounds In Proc. of the 10th International Confer-
ence on Computational Linguistics (COLING ?84),
Stanford, USA, 1984.
Kim, S.N. and Baldwin, T. 2005 Automatic Interpreta-
tion of Noun Compounds Using WordNet Similarity
IJCNLP 2005:945-956
Rackow, U., Dagan, I. and Schwall, U. 1992 Auto-
matic translation of noun compounds In Proc. of
the 14th International Conference on Computational
Linguistics (COLING ?92), Nantes, Frances, 1992
38
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 15?21,
Dublin, Ireland, August 23, 2014.
A Two-Stage Approach for Computing Associative Responses to a Set of
Stimulus Words
Urmi Ghosh, Sambhav Jain and Soma Paul
Language Technologies Research Center
IIIT-Hyderabad, India
{urmi.ghosh, sambhav.jain}@research.iiit.ac.in,
soma@iiit.ac.in
Abstract
This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task
on multiword association. The task involves generating a ranked list of responses to a set of
stimulus words. The two-stage approach combines the strength of neural network based word
embeddings and frequency based association measures. The system achieves an accuracy of
34.9% over the test set.
1 Introduction
Research in psychology gives evidence that word associations reveal the respondents? perception, learn-
ing and verbal memories and thus determine language production. Hence, it is possible to simulate
human derived word associations by analyzing the statistical distribution of words in a corpus. Church
and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by
utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that
corpus-based computations of word associations are similar to association norms collected from human
subjects.
The CogALex-2014 shared task on multi-word association involves generating a ranked list of re-
sponse words for a given set of stimulus words. For example, the stimulus word bank can invoke as-
sociative responses such as river, loan, finance and money. Priming
1
bank with bed and bridge, results
in strengthening association with the word river and it emerges as the best response amongst the afore-
mentioned response choices. This task is motivated by the tip-of-the-tongue problem, where associated
concepts from the memory can help recall the target word. Other practical applications include query ex-
pansion for information retrieval and natural language generation where missing words can be predicted
from their context.
The participating systems are distinguished into two categories - Unrestricted systems that allows
usage of any kind of data and Restricted systems that can only make use of the ukWaC (Baroni et al.,
2009) corpus, consisting of two billion tokens. Our proposed system falls in the restricted track since
we only used ukWaC for extracting information on word associations. It follows a two-staged approach:
Candidate Response Generation, which involves selection of words that are semantically similar to the
primes and Re-ranking by Association Measure, that re-ranks the responses using a proposed weighted
Pointwise Mutual Information (wPMI) measure. Our system was evaluated on test-datasets derived
from the Edinburgh Associative Thesaurus (Kiss et al., 1972) and it achieved an accuracy of 34.9%.
When ignoring the inflectional variations of the response word, an accuracy of 39.55% was achieved.
2 Observations on Training Data
The training set consists of 2000 sets of five words (multiword stimuli or primes) and the word that is
most closely associated to all of them (associative response). For example, a set of primes such as wheel,
driver, bus, drive and lorry are given along with the expected associative response - car.
In this section, our initial observations on the given training data are enlisted.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The phenomenon of providing multiple stimulus words is called priming.
15
2.1 Relation between the Associative Response and the Prime Words
It is observed that a response largely exhibits two kind of relations with a priming word.
Primes Associative Response
presents, Christmas, birthday, shops, present gifts
butterfly, light, ball, fly, insect moth
mouse, cat, catcher, race, tail rat
Table 1: Some examples of primes and their associative responses from the training set
Type A relation depicts a synonymous/antonymous behavior or ?of the same kind? nature. Word pairs
with paradigmatic relation are highly semantically related and belong to the same part of speech. And
hence, they tend to show a substitutive nature amongst themselves without affecting the grammar of the
sentence. From Table - 1, we observe that present/presents , butterfly/insect and mouse/cat can be substi-
tuted in place of gifts, moth and rat respectively. Type B relation depicts contextual co-occurrence, where
the words tend to occur together or form a collocation. This kind of relationship can be demonstrated by
taking examples from Table - 1, such as Christmas gifts, gift shops, birthday gifts, moth ball, rat catcher,
rat race and rat tail. In theory, the above have been formally categorized as paradigmatic (Type A) and
syntagmatic (Type B ) relations by De Saussure et al. (1916) and we will be referring to them accordingly
in rest of the paper.
Type C relation, depicting associations based on the phonological component of the words was also
observed. According to McCarthy (1990), responses can be affected by phonological shapes and or-
thographic patterns especially when instantaneous paradigmatic or syntagmatic association is difficult.
Examples from the training data set include ajar-Ajax, hypothalamus-hippopotamus and cravat-caravan.
Such examples were very few and hence, have not been dealt with in this paper.
2.2 Context Window Size
Words exhibiting syntagmatic associations often occur in close proximity in the corpus. We tested this
phenomenon on 500 randomly chosen sets of primes by calculating the distance of each prime from the
associative responses in the corpus. Figure - 1 testifies that a majority of primes occur within a context
window size of ?2 from the associative response.
1 2 3 4 5 6 7 8 9 10
0
500
1,000
1,500
2,000
d
f
Figure 1: Co-occurrence frequency f of an association at distance d from the response, averaged over the
2500 stimulus word and response word pairs from randomly chosen 500 training datasets
Next, a mechanism to interpret the above associations in a quantitative manner is required.
16
3 Word Representation
In order to have a quantitative comparison of association, first we need a representation for words in
a context. Traditionally co-occurrence vectors serve as a simple mechanism for such a representation.
However, such vectors are unable to effectively capture deeper semantics of words and also tend to suffer
from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been
made to represent word vectors in a lower dimensional space. Largely, these can be categorized into:
1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive
a vector based representation for each cluster, where semantically similar clusters are closer in
distance.
2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics.
Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls
in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation
of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors
over the probabilistic version of LSA (Hofmann, 1999).
3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector
corresponding to a word which effectively signifies its position in the semantic space. There has
been different suggestions on the nature of the neural-net and how the context needs to be fed to
the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008),
Turian et al. (2010) and Mikolov et al. (2013a).
4 Methodology
Our system follows a two-staged approach, where we first generate response candidates which are seman-
tically similar to prime words, followed by a re-ranking step where we give weightage to the responses
likely to occur in proximity.
4.1 Candidate Response Generation
The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word
embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by
the fact that this approach models semantic similarity and outperforms other approaches in terms of
accuracy as well as computational efficiency(Mikolov et al., 2013a; Mikolov et al., 2013c).
The word2vec
2
utility is used to learn this model and thereby create 300-dimensional word embed-
dings. word2vec implements two classification networks - the Skip-gram architecture and the Continuous
Bag-of-words (CBOW) architecture. We applied CBOW architecture as it works better on large corpora
and is significantly faster than Skip-gram(Mikolov et al., 2013b). The CBOW architecture predicts the
current word based on its context. The architecture employs a feed forward neural network, which con-
sists of:
1. An input layer, where the context words are fed to the network.
2. A projection layer, which projects words onto continuous space and reduces number of parameters
that are needed to be estimated.
3. An output layer.
This log-linear classifier learns to predict words based on its neighbors in a window of ?5. We also
applied a minimum word count of 25 so that infrequent words are filtered out.
With the vector representation available, a response r to a set of primes S, is searched in the vocabulary
by measuring its cosine similarity with each prime x
i
in S. The overall similarity of the response r, with
the prime word set S, is defined as the average of these similarities.
2
word2vec : https://code.google.com/p/word2vec/
17
sim(r, S) =
1
|S|
?
|S|
?
i=1
x
i
.r
|x
i
|.|r|
Using the best similarity score as the selection criterion for response, the approach resulted in an
accuracy of 20.8% over the test set. Error analysis revealed that the above approach is biased towards
finding a paradigmatic candidate. However, it is further observed that much of the correct answers
(> 80%) exist in a k-best(k=500) list but with a relatively lower similarity score. This confirmed that our
broader selection is correct but a better re-ranking approach is required.
4.2 Re-ranking by Association Measures
To give due weightage to responses with high syntagmatic associativity, we utilize word co-occurrences
from the corpus. Since we are dealing with semantically related candidates, applying even a basic lexical
association measure like Pointwise Mutual Information (PMI) (Church and Hanks, 1990) tend to improve
the results.
PMI
For each prime word, we calculate co-occurrence frequency information for its neighbors within a win-
dow of ?2 as mentioned in Section 2. Also, a threshold of 3 is set to the observed frequency measures
as PMI tends to give very high association score to infrequent words.
For each candidate response r, we calculate its PMI
i
with each of the primes (x
i
) in the set S. The
total association score Score
PMI
for a candidate is defined as the average of the individual measures.
PMI
i
=
p(x
i
r)
p(x
i
)p(r)
Score
PMI
=
1
|S|
?
?
i?S
PMI
i
Ranking the candidates based on PMI improved the results to 30.45%
Weighted PMI
It should be duly noted that only some primes exhibit a syntagmatic relation with the response, while
the rest exhibit a paradigmatic relation. For example, the expected response for primes Avenue, column,
dimension, sixth, fourth is fifth. The first three words share a syntagmatic relation with the response
while the last two words share a paradigmatic relation with the response. As PMI deals with word
co-occurrences, ideally, only primes exhibiting syntagmatic associations should be considered for re-
ranking. However, a clear distinction between the two categories of primes is a difficult task as the target
response is unknown.
In order to take effective contribution of each prime, we propose a weighed extension of PMI which
gives more weightage to syntagmatic primes as to the paradigmatic ones. Since, primes sharing a
paradigmatic relation with the response word are highly semantically related, they are expected to be
closer in the semantic space too. On the other hand, the primes showcasing syntagmatic relations are
expected to be distant.
Using the vector representation described in Section 4.1, we calculate an average vector of the five
primes, p
avg
, and compute its cosine distance from individual primes. The cosine distance thus obtained
is used as the weight w for the PMI associativity of a prime. In a nutshell, larger the distance of a
prime from p
avg
, the greater is its contribution in the PMI based re-ranking score. This ranking schema
assumes that the prime set consists of at least two words demonstrating paradigmatic relation with the
target response. Table - 2 displays the primes along with their distance from p
avg
.
Score
wPMI
=
1
|S|
?
?
i?S
w
i
PMI
i
Next, a ranked list of candidate responses for each set is generated by sorting the previously ranked
list according to the new score. The new ranking scheme based on weighted PMI (wPMI) improves the
results to 34.9%. Table -3 displays some sets which show improvement upon implementing the wPMI
18
Primes Cosine Distance
Avenue 0.612
column 0.422
dimension 0.390
sixth 0.270
fourth 0.212
Table 2: An example demonstrating Cosine Distance between the primes and the p
avg
of the prime set
ranking scheme. Taking a case from Table - 3, we observe that the correct response skeleton is generated
for primes cupboard, body, skull, bone and bones when ranked according to the wPMI scheme. This is
due to larger weights being assigned to primes cupboard and body which have a closer proximity to the
word skeleton than the word vertebral which is generated by the simple PMI ranking scheme.
cupboard 0.615 pit 0.553 boat 0.499
Primes(with weights) body 0.410 band 0.549 sailing 0.476
skull 0.248 hand 0.426 drab 0.338
bone 0.244 limb 0.0.340 dark 0.318
bones 0.172 leg 0.270 dull 0.307
PMI vertebral amputated drizzly
wPMI skeleton arm dingy
Expected Response skeleton arm dingy
Table 3: Comparison between results from PMI and wPMI re-ranking approaches
5 Results and Evaluation
The system was evaluated on the test set derived from the Edinburgh Associative Thesaurus (EAT) which
lists the associations to thousands of English stimulus words as collected from native speakers. For
example, for the stimulus word visual the top associations are aid, eyes, aids, see, eye, seen and sight.
For the shared task, top five associations for 2000 randomly selected stimulus words were provided as
prime sets and the system was evaluated based on its ability to predict the corresponding stimulus word
for each set. Table - 4 displays the top ten responses generated by our system for some prime sets and
their corresponding stimulus word.
Primes
knight, plate, soldier,
protection, sword
ants, flies, fly,
bees, bite
babies, baby, rash,
wet, washing
butterfly, moth, caterpillar,
cocoon, insect
Top 10
Responses
armor
armour
helmet
shield
guard
bulletproof
guards
warrior
enemy
gallant
mosquitoes
wasps
beetles
insects
spiders
sting
moths
butterflies
arachnids
bedbugs
nappy
shaving
nappies
clothes
skin
bathing
dry
eczema
bedding
dirty
larva
larvae
pupa
species
pests
beetle
silkworm
wings
pupate
pollinated
Target armour insects nappies chrysalis
Table 4: Top ten responses for some prime sets and their corresponding target response
19
As we have considered exact string match(ignoring capitalization), the evaluation does not account
for spelling variations. For example, the response output armor instead of the expected response armour
results in counting it as incorrect.
We achieved an accuracy of 34.9% by considering the top response for each list of ranked responses.
However, it was observed that the correct response was present within the top ten responses in 59.8%
of the cases. For example, the primes ants, flies, fly, bees, bite generate the response output mosquitoes.
The expected output insects ranks 4
th
in our list of responses.
For primes babies, baby, rash, wet, washing, our system outputs nappy while the expected response is
nappies. Such inflected forms of the responses are challenging to predict and hence, another evaluation
is presented which ignores the inflectional variation of the response word. Under this evaluation, we
achieved an accuracy of 39.55% for the best response and 63.15% if the expected response occurs in the
top ten responses. Table - 5 displays accuracy of our system when the target response lies within the
top-n responses for both evaluation methods.
Exact Match Ignoring Inflections
n=1 34.9 39.55
n=3 48.15 49.65
n=5 53.2 55.45
n=10 59.8 63.15
Table 5: Evaluation results in %
6 Conclusion
There exist some word associations that are asymmetric in nature. Rapp (2013) observed that the primary
response of a given stimulus word may have stronger association with another word and need not gen-
erate the stimulus word back. For example, the strongest association to bitter is sweet but the strongest
association to sweet is sour. Therefore, the EAT data set chosen for evaluation, may not be the best judge
for certain cases. Taking a case from our test data, for primes butterfly, moth, caterpillar, cocoon, insect,
our system outputs larva instead of the original stimulus word chrysalis which does not feature even in
the top ten responses (Refer Table - 4).
In this work, we proposed a system to generate a ranked list of responses for multiple stimulus words.
Candidate responses were generated by computing its semantic similarity with the stimulus words and
then re-ranked using a lexical association measure, PMI. This system scored 34.9% when the top ranked
response was considered and 59.8% when the top ten responses were taken into account. When ignoring
inflectional variations, the accuracy improved to 39.55% and 63.15% for the two evaluation methods
respectively.
In future, a more sophisticated re-ranking approach in place of PMI measure can be used such as
product-of-rank algorithm (Rapp, 2008). Since, the re-ranking methodologies discussed by far, take
into account word co-occurrences, it is biased towards syntagmatic responses. A better trade-off can be
worked out to give due weightage to paradigmatic responses too.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection
of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209?
226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
20
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Comput. Linguist., 16(1):22?29, March.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning,
pages 160?167. ACM.
Ferdinand De Saussure, Charles Bally, Albert Sechehaye, and Albert Riedlinger. 1916. Cours de linguistique
g?en?erale: Publi?e par Charles Bally et Albert Sechehaye avec la collaboration de Albert Riedlinger. Libraire
Payot & Cie.
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCI-
ENCE, 41(6):391?407.
Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence, pages 289?296. Morgan Kaufmann Publishers Inc.
George R. Kiss, Christine A. Armstrong, and Robert Milroy. 1972. An associative thesaurus of English. Medical
Research Council, Speech and Communication Unit, University of Edinburgh, Scotland.
Thomas K. Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211?240.
Michael McCarthy. 1990. Vocabulary. Oxford University Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word
representations. Proceedings of NAACL-HLT, pages 746?751.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages
1081?1088.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In Proceedings of the
workshop on Cognitive Aspects of the Lexicon, pages 102?109. Association for Computational Linguistics.
Reinhard Rapp. 2013. From stimulus to associations and back. Natural Language Processing and Cognitive
Science, page 78.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Manfred Wettler and Reinhard Rapp. 1989. A connectionist system to simulate lexical decisions in information
retrieval. Pfeifer, R., Schreter, Z., Fogelman, F. Steels, L.(eds.), Connectionism in perspective. Amsterdam:
Elsevier, 463:469.
Manfred Wettler and Reinhard Rapp. 1993. Computation of word associations based on the co-occurrences of
words in large corpora.
21
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 43?49,
Dublin, Ireland, August 23-29 2014.
A hybrid approach for automatic clause boundary identification in Hindi
Rahul Sharma, Soma Paul
Language Technology Research Centre, IIIT-Hyderabad, India
rahul.sharma@research.iiit.ac.in, soma@iiit.ac.in
Abstract
A complex sentence, divided into clauses, can be analyzed more easily than the complex sen-
tence itself. We present here, the task of clauses identification in Hindi text. To the best of our
knowledge, not much work has been done on clause boundary identification for Hindi, which
makes this task more important. We have built a Hybrid system which gives 90.804% F1-scores
and 94.697% F1-scores for identification of clauses? start and end respectively.
1 Introduction
Clause is the minimal grammatical unit which can express a proposition. It is a sequential group of
words, containing a verb or a verb group(verb and its auxiliary), and its arguments which can be explicit
or implicit in nature (Ram and Devi, 2008) . This makes clause an important unit in language grammars
and emphasis the need to identify and classify them as part of linguistic studies.
Analysis and processing of complex sentences is a far more challenging task as compared to a simple
sentence. NLP applications often perform poorly as the complexity of the sentence increases. ?It is im-
possible, to process a complex sentence if its clauses are not properly identified and classified according
to their syntactic function in the sentence? (Leffa, 1998). Further, identifying clauses, and processing
them separately are known to do better in many NLP tasks. The performance of many NLP systems like
Machine Translation, Parallel corpora alignment, Information Extraction, Syntactic parsing, automatic
summarization and speech applications etc improves by introducing clause boundaries in a sentence (e.g.,
Ejerhed, 1988; Abney, 1990; Leffa, 1998; Papageorgiou, 1997; Gadde et al., 2010).
We present a hybrid method which comprises of Conditional random fields(CRFs) (Lafferty et al., 2001)
based statistical learning followed by some rules to automatically determine ?clause? boundaries (be-
ginnings and ends) in complex or compound sentences. CRFs is a framework for building undirected
probabilistic graphical models to segment and label sequence data (Lafferty et al., 2001). In past, this
framework has proved to be successful for sequence labeling task (Sha and Pereira, 2003; McCallum and
Li, 2003). Van Nguyen et al. (2007) used CRFs for clause splitting task with some linguistic information
giving 84.09% F1-score.
Our system has minimum dependency on linguistic resources,only part of speech (POS) and chunk
information of lexical items is used with a fair performance of the system. As far as we know, not much
work has been done on clause boundary identification for Hindi and this makes this task more significant.
This paper is structured as follows: In Section 2, we discuss the related works that has been done earlier
on clause identification. Section 3 describes the creation of dataset for various system use. In Section
4, we talk about methodology of our system. Section 5 outlines the system performance. In section 6,
some issues related clause identification are discussed. In Section 7, we conclude and talk about future
works in this area.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
43
2 Related works
Studies in identifying clauses date back to (Ejerhed, 1988) work, where they showed how automatic
clause boundary identification in discourse can benefit a parser?s performance. However her experiments
could detect only basic clauses. Later (Abney, 1990) used clause filter as part of his CASS parser.
(Papageorgiou, 1997) used hand crafted rules to identify clause boundaries in a text. (Leffa, 1998) is
another rule based method which was implemented in an English-Portuguese MT system.
Some more recent works in this area are: (Puscasu, 2004), in which she proposed a multilingual
method of combining language independent ML techniques with language specific rules to detect
clause boundaries in unrestricted texts. The rules identify the finite verbs and clause boundaries not
included in learning process. (Ram and Devi, 2008) proposed a hybrid based approach for detecting
clause boundaries in a sentence. They have used a CRF based system which uses different linguistic
cues. After identifying the clause boundaries they run an error analyzer module to find false boundary
markings, which are then corrected by the rule based system, built using linguistic clues. (Ghosh et
al., 2010) is another rule based system for clause boundary identification for Bengali, where they use
machine learning approach for clause classification and dependency relations between verb and its
argument to find clause boundaries. Dhivya et al. (2012) use dependency trees from maltparser and
the dependency tag-set with 11 tags to identify clause boundaries. Similar to (Dhivya et al., 2012),
Sharma et al. (2013) showed how implicit clause information present in dependency trees can be used to
extract clauses in sentences. Their system have reported 94.44% accuracy for Hindi.Gadde et al. (2010)
reported improvement in parser performance by introducing automatic clause information in a sentence
for Hindi in ?Improving data driven dependency parsing using clausal information?. However their ap-
proach for identifying clause information has not been discussed. Thus a comparison is not possible here.
3 Dataset
In Hindi, We don?t have any data available annotated with clause boundary, So to generate clause anno-
tated corpora we have used (Sharma et al., 2013) technique where they have showed how implicit clause
information present in dependency trees can be used to extract clauses in sentences. By this technique
we have automatically generated 16000 sentences of Hindi treebank (Palmer et al., 2009) marked with
clause boudaries. Out of which, 14500 sentences were taken as training set, 500 for development set
and remaining 1000 sentences for testing set. As these sentences were generated automatically there are
chances of noises in form of wrongly marked clause boundaries, so for proper evaluation of the system,
we have manually corrected the wrongly marked clauses in development and testing sets.
4 Methodology
We propose a hybrid system which identifies the clause(s) in the input sentence and marks the ?clause
start position? (CSP) and ?clause end position? (CEP) with brackets.
Hindi usually follows the SOV word order, so ends of the clauses can be found by just using verb infor-
mation, in most of the cases. The language also has explicit relative pronouns, subordinating conjuncts,
coordinate conjunctions etc. which serve as cues that help to identify clause boundaries of the clauses.
Apart from the lexical cues we have also used POS tag and chunk information to built our system.
Our system comprise of two main modules; first modules is stochastic model which have been trained
on 14500 sentences, and second module which is built using hand crafted rules.
4.1 Stochastic Models
We have used two techniques to built two different models; 1) step-by-step model and 2) merged model,
using CRF machine learning approach. Both the models take word, word?s POS tag and its suffix as
word?s features for training. Table (1) shows the common features used for training models. These
feature are syntactic in nature, and relative pronoun, verb, conjunctions etc. plays important role in
identifying boundaries. suffixes help to learn morphological feature of the word.
44
Present word?s Lexicon, POS tag, last character, last two character, and last three character
Previous four words? Lexicon and POS tags
Next four words? Lexicon and POS tags
Next three words? last character, last two character, last three character
Table 1: Features
4.1.1 step-by-step model
This model comprises of two models; end model and start model. First one identifies the end of a clause
and then later one takes the output of former model as input and identifies the start of the clause. In this
technique we can notice that both models have to only mark whether a word is a boundary of a clause or
not. For example ?end model? has to check whether a given word is a end(boundary) of a clause or not.
Below example (1) explains this further.
(1) raam
Ram
jisne
who
khaanaa
food
khaayaa
eat+past
ghar
home
gayaaa
go+past
?Raam who ate food, went home?
In example (1), end model first marks ?gayaa? and ?khaayaa? as the end of clause. Then start model takes
this additional information also as the feature, and marks ?raam? and ?jisne? as the start of clause.
4.1.2 Merged Model
This model marks the clauses? start and end in one go. Unlike the step-by-step model, it check whether
a word is clause?s start, clause?s end or none. For above example (1), it will mark ?gayaa? and ?khaayaa?
as the end of clause, and ?raam? and ?jisne? as the start of clause respectively in one go.
-- Keeping post-processing module(discussed below) same, we have evaluated our system using both
stochastic models separately, and observed, system with step-by-step model gives high F1-score value
than the system with merged model.
4.2 Post-processing Module
This module processes the output from stochastic model, and mark the boundaries of clauses in sen-
tences. As we know, in a sentence CSPs should always be equal to CEPs. So on the basis of difference
between CSPs and CEPs, we have formalized our rules. Below is the description of rules used in this
module.
1. Rules, when CSPs are greater than CEPs are:
(a) Check for ?ki? complement clause: The verb in a sentence which contain ?ki? compliment
clause is not the end of its clause whereas its end is same as of end of ?ki? complement clause.
Below example (2) will make this rule more clearer.
(2) raam ne
Ram+arg
kahaa
say+past
ki
that
vaha
he
ghar
home
gayaa
go+past
?Raam said that he went home?
In this example (2), Stochastic models will mark ?raam? and ?ki? as the start of clause, and
?gayaa? as the end of clause, making CSPs more than the CEPs. We can notice that ?gayaa?
is the end for both the clauses in a sentence, so using this rule, we will add one more end of
clause to ?gayaa? word. The resultant sentence with clauses marked will be:
( raam ne kahaa ( ki vaha ghar gayaa ) )
(b) Check for finite verb: If a verb is finite and does not have any ?ki? complement clause in it
then that verb should be marked as the end of clause. So if this type verb is unmarked by the
stochastic model then this rule will handle this.
(c) Check for non-finite verb: If a non-finite verb is present in a sentence and word next to it does
not mark start of another clause then this rule will mark that word as the start of that clause.
45
?It should be noted that rules are applied in specific order, and once the number of CSPs and CEPs
become same at any point of rule we stop applying more rules from this type where CSPs and CEPs
are not same.
2. Rules, When CEPs are greater than CSPs are:
(a) If there is a ?non-finite? verb in a sentence then we check for its start and mark them using
regular expressions if not marked by stochastic models. for example:
(3) raam
Ram+arg
khaanaa
food
khakara
having eaten
ghar
home
gayaa
go+past
?having eaten food, Ram wen home?
In example (3), if stochastic models does not able to mark ?khaanaa? as the start of non-finite
clause ?khaanaa khakara?. Then this rules will capture these type of situations and add a new
CSP in a sentence.
(b) If a word before conjunction, not a verb, is marked as end of a clause then this rule will remove
that end, reducing number of CEP.
3. Rules, when CSPs and CEPs are same:
(a) If there are more than one clauses in one single ?ki? complement clause than this rules marks
one bigger boundary as clause which will contain all the embedded clauses. For example:
(4) raam ne
Ram+arg
kahaa
say+past
ki
that
shaam ne
Shaam+arg
khaanaa
food
khaayaa
eat+past
aur
and
paani
water
piyaa
drink+past
?Raam said that Shaam ate food and drank water?
The situation discussed in this rule can be observed in example (4). The system output before
this rule may be,
?( raam ne kahaa ( ki shaam ne khaanaa khaayaa ) aur ( paani piyaa ) )?, Which this rule will
convert to
?( raam ne kahaa ( ki ( shaam ne khaanaa khaayaa ) aur ( paani piyaa ) ) )?
? Having these rules applied, the output sentence will contain start and end of clauses in a sentence.
5 Evaluation and Results
As mentioned earlier we have used (Sharma et al., 2013) technique to automatically generate 16000
sentences of Hindi treebank marked with clause boundaries. Out of these 16000 sentences, a set of 1500
sentences with average length of 16 words was randomly selected. This set was then manually corrected
at the level of clause boundary for accurate evaluation of the system. It should be noted that this set
was not used in training of the models. Further we have divided this set into two set; development set
which consist of 500 sentences and testing set which consist of 1000 sentences. We have evaluated the
system with both models (step-by-step and merged) along with post-processing module, and we have
noticed system with step-by-step model performs better than the system with merged model. Table (2)
and Table (3) show the results on development set and testing set respectively.
Model Type Start of clause End of clause
Precision Recall F1-measure Precision Recall F1-measure
Step-by-step model 91.493 89.816 90.646 95.129 93.482 94.298
Merged Model 92.171 89.918 91.030 90.927 92.871 91.888
Table 2: Results on development set.
46
Model Type Start of clause End of clause
Precision Recall F1-measure Precision Recall F1-measure
Step-by-step model 92.051 89.590 90.804 95.969 93.458 94.697
Merged Model 91.779 88.907 90.320 90.919 92.263 91.586
Table 3: Results on testing set.
6 Error Analysis and Discussion
While evaluating our both systems (system with step-by-step model and system with merged model), we
come across some constructions which were not handled by them. which are:
1. Ellipses of verb: when a verb is omitted in a sentence then it is not possible for our system to mark
boundaries correctly. For example:
(5) raam ne
Ram+erg
kitaab
book
<V>
<read+past>
aur
and
maine
I+erg
kavitaa
poem
padhii
read+past
?Ram read a book and I read a poem?
In example (5), there is an ellipses of the verb ?padhi? in the clause ?raam ne kitaab?. Thus, though
the sentence has two clauses??raam ne kitaab? and ?maine kavitaa padhii?, our system incorrectly
identifies the whole sentence as one clause due to the ellipses of the verb (denoted by <V>).
2. Scrambling in the usual word order, which is SOV in Hindi, is likely to induce incorrect identifica-
tion of the clauses in our system. For Example:
(6) ghar
home
gayaa
go+past
raam,
Ram,
vaha
he
bolaa.
say+past
?He said Ram went home?
In example (6), Our system is unable to identify the clause boundaries correctly for any of the two
clauses, ?ghar gayaa raam? and ?ghar gayaa raam,vaha bolaa?, due to scrambling in the word order.
Its output for the sentence is ?(ghar) (gayaa raam, vaha bolaa)?, though the output should be ?( (ghar
(gayaa raam,) vaha bolaa)?.
3. Missing subordinate conjunction ?ki? in a sentence also leads to incorrect identification of clause
boundaries by our system. For example:
(7) raam ne
Ram+erg
kahaa
say+past
tum
you
ghar
home
jaao
go
?Ram said you go home?
The missing subordinate conjunction ?ki? in example (7) leads to incorrect marking of the clause
boundaries as: ?(raam ne kahaa ) ( tum ghar jaao)?. The correct clause boundaries for the sentence
are ?(raam ne kahaa ( tum ghar jaao) )?.
4. Start of non-finite clause: As we don?t find any syntactic cues for start of non-finite clause, our
systems does not perform much efficiently in finding start of non-finite clauses. For example:
(8) ab
now
hum
we
alag
different
maslon
matters/topics
para
on
khulkara
openly
baatchit
discussion
kar rahe hain
do+conti+present
?Now we are discussing openly on different matters?
47
Both system marks ?khulkara? and ?kar rahe hain? verbs as the end of clauses accurately but start of
non-finite clause which is ?alag? is not identified correctly. Output by the systems is, ?( ab hum alag
maslon para khulkara) (baatchit kar rahe hain )? , where as the correct output is, ?( ab hum ( alag
maslon para khulkara) baatchit kar rahe hain )?
-- Overall we observed that the system with step-by-step model which statistically first identifies end and
then start, followed by rules performs better than the system with merged model.
7 Conclusion and Future Work
We have discussed our work on clause boundary identification in Hindi and the issues pertaining to them,
in the course of this paper. Clausal information in a sentence is known to improve the performance of
many NLP systems, thus the need for this task. We observed that the system with step-by-step model
which statistically, first identifies end and then start of clauses, followed by rules, performs better than the
system with merged model. The step-by-step model system, showing a satisfactory performance in terms
of F1 scores of 91.53% for clause boundary identification, and the merged model system showing 80.63%
for the same. Since this task is a promising resource for NLP systems such as Machine Translation, Text-
to-Speech and so on, and can contribute to their better performance, applying this system for betterment
of NLP tools seems quite a favorable prospect as a future work. (Gadde et al., 2010) report that even
minimal clause boundary identification information leverages the performance of their system. We would
like to test the performance of our system in terms of leveraging the performance of other NLP systems
References
Steven Abney. 1990. Rapid incremental parsing with repair. pages 1?9.
R Dhivya, V Dhanalakshmi, M Anand Kumar, and KP Soman. 2012. Clause boundary identification for tamil
language using dependency parsing. pages 195?197. Springer.
Eva I Ejerhed. 1988. Finding clauses in unrestricted text by finitary and stochastic methods. pages 219?227.
Association for Computational Linguistics.
Phani Gadde, Karan Jindal, Samar Husain, Dipti Misra Sharma, and Rajeev Sangal. 2010. Improving data driven
dependency parsing using clausal information. pages 657?660. Association for Computational Linguistics.
Aniruddha Ghosh, Amitava Das, and Sivaji Bandyopadhyay. 2010. Clause identification and classification in
bengali. In 23rd International Conference on Computational Linguistics, page 17.
John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
Vilson J Leffa. 1998. Clause processing in complex sentences. volume 1, pages 937?943.
Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields,
feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language
learning at HLT-NAACL 2003-Volume 4, pages 188?191. Association for Computational Linguistics.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase structure. pages 14?17.
Harris V Papageorgiou. 1997. Clause recognition in the framework of alignment. pages 417?426.
Georgiana Puscasu. 2004. A multilingual method for clause splitting.
R Vijay Sundar Ram and Sobha Lalitha Devi. 2008. Clause boundary identification using conditional random
fields. In Computational Linguistics and Intelligent Text Processing, pages 140?150. Springer.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology-Volume 1, pages 134?141. Association for Computational Linguistics.
48
Rahul Sharma, Soma Paul, Riyaz Ahmad Bhat, and Sambhav Jain. 2013. Automatic clause boundary annotation
in the hindi treebank.
Vinh Van Nguyen, Minh Le Nguyen, and Akira Shimazu. 2007. Using conditional random fields for clause split-
ting. Proceedings of The Pacific Association for Computational Linguistics, University of Melbourne Australia.
49
