Proceedings of NAACL HLT 2007, pages 324?331,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Probability-Based Rankers for Action-Item Detection
Paul N. Bennett
Microsoft Research?
One Microsoft Way
Redmond, WA 98052
paul.n.bennett@microsoft.com
Jaime G. Carbonell
Language Technologies Institute, Carnegie Mellon
5000 Forbes Ave
Pittsburgh, PA 15213
jgc@cs.cmu.edu
Abstract
This paper studies methods that automat-
ically detect action-items in e-mail, an
important category for assisting users in
identifying new tasks, tracking ongoing
ones, and searching for completed ones.
Since action-items consist of a short span
of text, classifiers that detect action-items
can be built from a document-level or a
sentence-level view. Rather than com-
mit to either view, we adapt a context-
sensitive metaclassification framework to
this problem to combine the rankings pro-
duced by different algorithms as well as
different views. While this framework is
known to work well for standard classi-
fication, its suitability for fusing rankers
has not been studied. In an empirical eval-
uation, the resulting approach yields im-
proved rankings that are less sensitive to
training set variation, and furthermore, the
theoretically-motivated reliability indica-
tors we introduce enable the metaclassi-
fier to now be applicable in any problem
where the base classifiers are used.
1 Introduction
From business people to the everyday person, e-
mail plays an increasingly central role in a modern
lifestyle. With this shift, e-mail users desire im-
proved tools to help process, search, and organize
the information present in their ever-expanding in-
boxes. A system that ranks e-mails according to the
?This work was performed primarily while the first author
was supported by Carnegie Mellon University.
From: Henry Hutchins <hhutchins@innovative.company.com>
To: Sara Smith; Joe Johnson; William Woolings
Subject: meeting with prospective customers
Hi All,
I?d like to remind all of you that the group from GRTY will
be visiting us next Friday at 4:30 p.m. The schedule is:
+ 9:30 a.m. Informal Breakfast and Discussion in Cafeteria
+ 10:30 a.m. Company Overview
+ 11:00 a.m. Individual Meetings (Continue Over Lunch)
+ 2:00 p.m. Tour of Facilities
+ 3:00 p.m. Sales Pitch
In order to have this go off smoothly, I would like to practice
the presentation well in advance. As a result, I will need each
of your parts by Wednesday.
Keep up the good work!
?Henry
Figure 1: An E-mail with Action-Item (italics added).
likelihood of containing ?to-do? or action-items can
alleviate a user?s time burden and is the subject of
ongoing research throughout the literature.
In particular, an e-mail user may not always pro-
cess all e-mails, but even when one does, some
emails are likely to be of greater response urgency
than others. These messages often contain action-
items. Thus, while importance and urgency are not
equal to action-item content, an effective action-item
detection system can form one prominent subcom-
ponent in a larger prioritization system.
Action-item detection differs from standard text
classification in two important ways. First, the user
is interested both in detecting whether an email
contains action-items and in locating exactly where
these action-item requests are contained within the
email body. Second, action-item detection attempts
324
to recover the sender?s intent ? whether she means
to elicit response or action on the part of the receiver.
In this paper, we focus on the primary problem
of presenting e-mails in a ranked order according to
their likelihood of containing an action-item. Since
action-items typically consist of a short text span ?
a phrase, sentence, or small passage ? supervised
input to a learning system can either come at the
document-level where an e-mail is labeled yes/no
as to whether it contains an action-item or at the
sentence-level where each span that is an action-
item is explicitly identified. Then, a corresponding
document-level classifier or aggregated predictions
from a sentence-level classifier can be used to esti-
mate the overall likelihood for the e-mail.
Rather than commit to either view, we use a com-
bination technique to capture the information each
viewpoint has to offer on the current example. The
STRIVE approach (Bennett et al, 2005) has been
shown to provide robust combinations of heteroge-
neous models for standard topic classification by
capturing areas of high and low reliability via the
use of reliability indicators.
However, using STRIVE in order to produce im-
proved rankings has not been previously studied.
Furthermore, while they introduce some reliabil-
ity indicators that are general for text classification
problems as well as ones specifically tied to na??ve
Bayes models, they do not address other classifica-
tion models. We introduce a series of reliability in-
dicators connected to areas of high/low reliability in
kNN, SVMs, and decision trees to allow the combi-
nation model to include such factors as the sparse-
ness of training example neighbors around the cur-
rent example being classified. In addition, we pro-
vide a more formal motivation for the role these vari-
ables play in the resulting metaclassification model.
Empirical evidence demonstrates that the result-
ing approach yields a context-sensitive combination
model that improves the quality of rankings gener-
ated as well as reducing the variance of the ranking
quality across training splits.
2 Problem Approach
In contrast to related combination work, we focus on
improving rankings through the use of a metaclass-
ification framework. In addition, rather than sim-
ply focusing on combining models from different
classification algorithms, we also examine combin-
ing models that have different views, in that both the
qualitative nature of the labeled data and the applica-
tion of the learned base models differ. Furthermore,
we improve upon work on context-sensitive com-
bination by introducing reliability indicators which
model the sensitivity of a classifier?s output around
the current prediction point. Finally, we focus on the
application of these methods to action-item data ?
a growing area of interest which has been demon-
strated to behave differently than more standard text
classification problems (e.g. topic) in the literature
(Bennett and Carbonell, 2005).
2.1 Action-Item Detection
There are three basic problems for action-item de-
tection. (1) Document detection: Classify an e-mail
as to whether or not it contains an action-item. (2)
Document ranking: Rank the e-mails such that all
e-mail containing action-items occur as high as pos-
sible in the ranking. (3) Sentence detection: Classify
each sentence in an e-mail as to whether or not it is
an action-item.
Here we focus on the document ranking problem.
Improving the overall ranking not only helps users
find e-mails with action-items quicker (Bennett and
Carbonell, 2005) but can decrease response times
and help ensure that key e-mails are not overlooked.
Since a typical user will eventually process all
received mail, we assume that producing a quality
ranking will more directly measure the impact on
the user than accuracy or F1. Therefore, we focus on
ROC curves and area under the curve (AUC) since
both reflect the quality of the ranking produced.
2.2 Combining Classifiers with Metaclassifiers
One of the most common approaches to classi-
fier combination is stacking (Wolpert, 1992). In
this approach, a metaclassifier observes a past his-
tory of classifier predictions to learn how to weight
the classifiers according to their demonstrated ac-
curacies and interactions. To build the history,
cross-validation over the training set is used to ob-
tain predictions from each base classifier. Next, a
metalevel representation of the training set is con-
structed where each example consists of the class
label and the predictions of the base classifiers. Fi-
nally, a metaclassifier is trained on the metalevel rep-
resentation to learn a model of how to combine the
base classifiers.
However, it might be useful to augment the his-
tory with information other than the predicted prob-
abilities. For example, during peer review, reviewers
325
class class
 
 


 
 


 
Metaclassifier
Reliability
Indicators
SVM
Unigram
 
 


 
 


w1
w2
w3
wn
? ? ? ? ? ?
r1
r2
rn
Figure 2: Architecture of STRIVE. In STRIVE, an additional layer of learning is added where the metaclassifier can use the context
established by the reliability indicators and the output of the base classifiers to make an improved decision.
typically provide both a 1-5 acceptance rating and a
1-5 confidence. The first of these is related to an es-
timate of class membership, P (?accept?? | paper),
but the second is closer to a measure of expertise or
a self-assessment of the reviewer?s reliability on an
example-by-example basis.
Automatically deriving such self-assessments for
classification algorithms is non-trivial. The Stacked
Reliability Indicator Variable Ensemble framework,
or STRIVE, demonstrates how to extend stacking by
incorporating such self-assessments as a layer of re-
liability indicators and introduces a candidate set of
functions (Bennett et al, 2005).
The STRIVE architecture is depicted in Figure 2.
From left to right: (1) a bag-of-words representation
of the document is extracted and used by the base
classifiers to predict class probabilities; (2) reliabil-
ity indicator functions use the predicted probabili-
ties and the features of the document to characterize
whether this document falls within the ?expertise?
of the classifiers; (3) a metalevel classifier uses the
base classifier predictions and the reliability indica-
tors to make a more reliable combined prediction.
From the perspective of improving action-item
rankings, we are interested in whether stacking or
striving can improve the quality of rankings. How-
ever, we hypothesize that striving will perform better
since it can learn a model that varies the combination
rule based on the current example and thus, better
capture when a particular classifier at the document-
level or sentence-level, bag-of-words or n-gram rep-
resentation, etc. will produce a reliable prediction.
2.3 Formally Motivating Reliability Indicators
While STRIVE has been shown to provide robust
combination for topic classification, a formal moti-
vation is lacking for the type of reliability indicators
that are the most useful in classifier combination.
Assume we restrict our choice of metaclassifier to
a linear model. One natural choice is to rank the
e-mails according to the estimated posterior proba-
bility, P? (class = action item | x), but in a linear
combination framework it is actually more conve-
nient to work with the estimated log-odds or logit
transform which is monotone in the posterior, ?? =
log P? (class=action item|x)1?P? (class=action item|x) (Kahn, 2004).
Now, consider applying a metaclassifier to a sin-
gle base classifier. Given only a classifier?s probabil-
ity estimates, a metaclassifier cannot improve on the
estimates if they are well-calibrated (DeGroot and
Fienberg, 1986). Thus a metaclassifier applied to
a single base classifier corresponds to recalibration
(Kahn, 2004).
Assume each of the n base models gives an un-
calibrated log-odds estimate ??i. Then the com-
bination model would have the form ???(x) =
W0(x)+
?n
i=1 Wi(x)??i(x) where the Wi are exam-
ple dependent weight functions that the combination
model learns. The obvious implication is that our
reliability indicators can be informed by the optimal
values for the weighting functions.
We can determine the optimal weights in a sim-
plified case with a single base classifier by assuming
we are given ?true? log-odds values, ?, and a family
of distributions ?x such that ?x = p(z | x)
encodes what is local to x by giving the probability
of drawing a point z near to x. We use ? instead of
?x for notational simplicity. Since ? encodes the
example dependent nature of the weights, we can
drop x from the weight functions. To find weights
that minimize the squared difference between the
true log-odds and the estimated log-odds in the ?
vicinity of x, we can solve a standard regression
problem, argminw0,w1 E?
[
(
w1 ?? + w0 ? ?
)2
]
.
Under the assumption VAR?
[
??
]
6= 0, this yields:
326
w0 = E?[?] ? w1E?
[
??
]
(1)
w1 =
COV?
[
??, ?
]
VAR?
[
??
] = ?????
??,?? (2)
where ? and ? are the stdev and correlation co-
efficient under ?, respectively. The first parame-
ter is a measure of calibration that addresses the
question, ?How far off on average is the estimated
log-odds from the true log-odds in the local con-
text?? The second is a measure of correlation, ?How
closely does the estimated log-odds vary with the
true log-odds?? Note that the second parameter de-
pends on the local sensitivity of the base classifier,
VAR1/2?
[
??
]
= ???. Although we do not have true
log-odds, we can introduce local density models to
estimate the local sensitivity of the model.
In particular, we introduce a series of relia-
bility indicators by first defining a ? distribu-
tion and either computing VAR?
[
??
]
, E?
[
??
]
or
the closely related terms VAR?
[
??(z) ? ??(x)
]
,
E?
[
??(z) ? ??(x)
]
. We use the resulting values for
an example as features for a linear metaclassifier.
Thus we use a context-dependent bias term but leave
the more general model for future work.
2.4 Model-Based Reliability Indicators
As discussed in Section 2.3, we wish to define local
distributions in order to compute the local sensitivity
and similar terms for the base classification models.
To do so, we define local distributions that have the
same ?flavor? as the base classification model.
First, consider the kNN classifier. Since we are
concerned with how the decision function would
change as we move locally around the current pre-
diction point, it is natural to consider a set of shifts
defined by the k neighbors. In particular, let di de-
note the document that has been shifted by a factor
?i toward the ith neighbor, i.e. di = d+?i(ni?d).
We use the largest ?i such that the closest neighbor
to the new point is the original document, i.e. the
boundary of the Voronoi cell (see Figure 3). Clearly,
?i will not exceed 0.5, and we can find it efficiently
using a simple bisection algorithm. Now, let ? be
a uniform point-mass distribution over the shifted
points and ??kNN, the output score of the kNN model.
?1.5 ?1 ?0.5 0 0.5 1 1.5 2
?1.5
?1
?0.5
0
0.5
1
1.5
2
1
2 3
4
5
6
x
Figure 3: Illustration of the kNN shifts produced for a predic-
tion point x using the numbered points as its neighborhood.
Given this definition of ?, it is now straight-
forward to compute the kNN based reliabil-
ity indicators: E?[??kNN(z) ? ??kNN(x)] and
Var1/2? [??kNN(z) ? ??kNN(x)].
Similarly, we define variables for the SVM class-
ifier by considering a document?s locality in terms
of nearby support vectors from the set of support
vectors, V . To determine ?i, we define it in terms
of the closest support vector in V to d. Let  be
half the distance to the nearest point in V , i.e.  =
1
2 minv?V ?v ? d?. Then ?i = ?vi?d? .
1 Thus, the
shift vectors are all rescaled to have the same length.
Now, we must define a probability for the shift. We
use a simple exponential based on  and the rela-
tive distance from the document to the support vec-
tor defining this shift. Let di ? ? where P?(di) ?
exp(??vi ? d? + 2) and
?V
i=1 P?(di) = 1.2
Given this definition of ?, we compute the
SVM based reliability indicators: E?[??SVM(z) ?
??SVM(x)] and Var1/2? [??SVM(z) ? ??SVM(x)].
Space prevents us from presenting all the deriva-
tions here. However, we also define decision-tree
based variables where the locality distribution gives
high probability to documents that would land in
nearby leaves. For a multinomial na??ve Bayes model
(NB), we define a distribution of documents iden-
tical to the prediction document except having an
occurrence of a single feature deleted. For the
multivariate Bernoulli na??ve Bayes (MBNB) model
1We assume that the minimum distance is not zero. If it is
zero, then we return zero for all of the variables.
2As is standard to handle different document lengths, we
take the distance between documents after they have been nor-
malized to the unit sphere.
327
that models feature presence/absence, we use a
distribution over all documents that has one pres-
ence/absence bit flipped from the prediction docu-
ment. It is interesting to note that the variables from
the na??ve Bayes models can be shown to be equiva-
lent to variables introduced by Bennett et al (2005)
? although those were derived in a different fashion
by analyzing the weight a single feature carries with
respect to the overall prediction.
Furthermore, from this starting point, we go on to
define similar variables of possible interest. Includ-
ing the two for each model described here, we define
10 kNN variables, 5 SVM variables, 2 decision-tree
variables, 6 NB model based variables, and 6 MBNB
variables. We describe these variables as well as im-
plementation details and computational complexity
results in (Bennett, 2006).
3 Experimental Analysis
3.1 Data
Our corpus consists of e-mails obtained from vol-
unteers at an educational institution and covers
subjects such as: organizing a research work-
shop, arranging for job-candidate interviews, pub-
lishing proceedings, and talk announcements. Af-
ter eliminating duplicate e-mails, the corpus con-
tains 744 messages with a total of 6301 automat-
ically segmented sentences. A human panel la-
beled each phrase or sentence that contained an
explicit request for information or action. 416 e-
mails have no action-items and 328 e-mails con-
tain action-items. Additional information such
as annotator agreement, distribution of message
length, etc. can be found in (Bennett and Car-
bonell, 2005). An anonymized corpus is available
at http://www.cs.cmu.edu/?pbennett/action-item-dataset.html.
3.2 Feature Representation
We use two types of feature representation: a bag-
of-words representation which uses all unigram to-
kens as the feature pool; and a bag-of-n-grams
where n includes all n-grams where n ? 4. For
both representations at both the document-level and
sentence-level, we used only the top 300 features by
the chi-squared statistic.
3.3 Document-Level Classifiers
kNN
We used a s-cut variant of kNN common in text
classification (Yang, 1999) and a tfidf-weighting
of the terms with a distance-weighted vote of the
neighbors to compute the output. k was set to be
2(dlog2 Ne + 1) where N is the number of training
points. 3 The score used as the uncalibrated log-
odds estimate of being an action-item is:
??kNN(x) =
?
n?kNN(x)|c(n)=actionitem
cos(x,n) ?
?
n?kNN(x)|c(n) 6=actionitem
cos(x,n).
SVM
We used a linear SVM as implemented in the
SVMlight package v6.01 (Joachims, 1999) with a
tfidf feature representation and L2-norm. All de-
fault settings were used. SVM?s margin score,
?
?iyi K(xi,xj), has been shown to empirically
behave like an uncalibrated log-odds estimate (Platt,
1999).
Decision Trees
For the decision-tree implementation, we used the
WinMine toolkit and refer to this as Dnet below (Mi-
crosoft Corporation, 2001). Dnet builds decision
trees using a Bayesian machine learning algorithm
(Chickering et al, 1997; Heckerman et al, 2000).
The estimated log-odds is computed from a Laplace
correction to the empirical probability at a leaf node.
Na??ve Bayes
We use a multinomial na??ve Bayes (NB) and a mul-
tivariate Bernoulli na??ve Bayes classifier (MBNB)
(McCallum and Nigam, 1998). For these classifi-
ers, we smoothed word and class probabilities us-
ing a Bayesian estimate (with the word prior) and
a Laplace m-estimate, respectively. Since these are
probabilistic, they issue log-odds estimates directly.
3.4 Sentence-Level Classifiers
Each e-mail is automatically segmented into sen-
tences using RASP (Carroll, 2002). Since the cor-
pus has fine grained labels, we can train classifiers
to classify a sentence. Each classifier in Section 3.3
is also used to learn a sentence classifier. However,
we then must make a document-level prediction.
In order to produce a ranking score, the con-
fidence that the document contains an action-item is:
??(d) =
{ 1
n(d)
?
s?d|pi(s)=1 ??(s), ?s?d|pi(s) = 1
1
n(d) maxs?d ??(s) o.w.
3This rule is not guaranteed be optimal for a particular value
of N but is motivated by theoretical results which show such a
rule converges to the optimal classifier as the number of training
points increases (Devroye et al, 1996).
328
where s is a sentence in document d, pi is the class-
ifier?s 1/0 prediction, ?? is the score the classifier as-
signs as its confidence that pi(s) = 1, and n(d) is
the greater of 1 and the number of (unigram) to-
kens in the document. In other words, when any
sentence is predicted positive, the document score
is the length normalized sum of the sentence scores
above threshold. When no sentence is predicted pos-
itive, the document score is the maximum sentence
score normalized by length. The length normaliza-
tion compensates for the fact that we are more likely
to emit a false positive the longer a document is.
3.5 Stacking
To examine the hypothesis that the reliability in-
dicators provide utility beyond the information
present in the output of the 20 base classifiers
(2 representations?2 views?5 classifiers), we con-
struct a linear stacking model which uses only the
base classifier outputs and no reliability indicators as
a baseline. For the implementation, we use SVMlight
with default settings. The inputs to this classifier are
normalized to have zero mean and a scaled variance.
3.6 Striving
Since we are constructing base classifiers for both
the bag-of-words and bag-of-n-grams representa-
tions, this gives 58 reliability indicators from com-
puting the variables in Section 2.4 for the document-
level classifiers (58 = 2 ? [6 + 6 + 10 + 5 + 2]).
Although the model-based indicators are defined
for each sentence prediction, to use them at the
document-level we must somehow combine the re-
liability indicators over each sentence. The simplest
method is to average each classifier-based indicator
across the sentences in the document. We do so and
thus obtain another 58 reliability indicators.
Furthermore, our model might benefit from some
of the structure a sentence-level classifier offers
when combining document predictions. Analogous
to the sensitivity of each base model, we can con-
sider such indicators as the mean and standard de-
viation of the classifier confidences across the sen-
tences within a document. For each sentence-level
base classifier, these become two more indicators
which we can benefit from when combining docu-
ment predictions. This introduces 20 more variables
(20 = 2 representations ? 2 ? 5 classifiers).
Finally, we include the 2 basic voting statistic
reliability-indicators (PercentPredictingPositive and
PercentAgreeWBest) that Bennett et al (2005) found
useful for topic classification. This yields a total of
138 reliability-indicators (138 = 58+ 20+ 58+ 2).
With the 20 classifier outputs, there are a total of 158
input features for striving to handle.
As with stacking, we use SVMlight with default
settings and normalize the inputs to this classifier to
have zero mean and a scaled variance.
3.7 Performance Measures
We wish to improve the rankings of the e-mails in
the inbox such that action-item e-mails occur higher
in the inbox. Therefore, we use the area under the
curve (AUC) of an ROC curve as a measure of rank-
ing performance. AUC is a measure of overall model
and ranking quality that has gained wider adoption
recently and is equivalent to the Mann-Whitney-
Wilcoxon sum of ranks test (Hanley and McNeil,
1982). To put improvement in perspective, we can
write our relative reduction in residual area (RRA)
as 1?AUC1?AUCbaseline . We present gains relative to the
best AUC performer (bRRA), and relative to perfect
dynamic selection performance, (dRRA), which as-
sumes we could accurately dynamically choose the
best classifier per cross-validation run.
The F1 measure is the harmonic mean of preci-
sion and recall and is common throughout text class-
ification (Yang and Liu, 1999). Although we are not
concerned with F1 performance here, some users of
the system might be interested in improving rank-
ing while having negligible negative effect on F1.
Therefore, we examine F1 to ensure that an improve-
ment in ranking will not come at the cost of a statis-
tically significant decrease in F1.
3.8 Experimental Methodology
To evaluate performance of the combination sys-
tems, we perform 10-fold cross-validation and com-
pute the average performance. For significance tests,
we use a two-tailed t-test (Yang and Liu, 1999)
to compare the values obtained during each cross-
validation fold with a p-value of 0.05.
We examine two hypotheses: Stacking will out-
perform all of the base classifiers; Striving will out-
perform all the base classifiers and stacking.
3.9 Results & Discussion
Table 1 presents the summary of results. The best
performer in each column is in bold. If a combi-
nation method statistically significantly outperforms
all base classifiers, it is underlined.
329
F1 AUC bRRA dRRA
Document-Level, Bag-of-Words Representation
Dnet 0.7398 0.8423 1.41 1.78
NB 0.6905 0.7537 2.27 2.91
MBNB 0.6729 0.7745 2.00 2.49
SVM 0.6918 0.8367 1.48 1.87
kNN 0.6695 0.7669 2.17 2.74
Document-Level, Ngram Representation
Dnet 0.7412 0.8473 1.38 1.77
NB 0.7361 0.8114 1.75 2.23
MBNB 0.7534 0.8537 1.30 1.61
SVM 0.7392 0.8640 1.24 1.59
kNN 0.7021 0.8244 1.62 2.01
Sentence-Level, Bag-of-Words Representation
Dnet 0.7793 0.8885 1.00 1.27
NB 0.7731 0.8645 1.21 1.50
MBNB 0.7888 0.8699 1.14 1.42
SVM 0.6985 0.8548 1.34 1.70
kNN 0.6328 0.6823 2.98 3.88
Sentence-Level, Ngram Representation
Dnet 0.7521 0.8723 1.13 1.42
NB 0.8012 0.8723 1.15 1.46
MBNB 0.8010 0.8777 1.10 1.38
SVM 0.7842 0.8620 1.23 1.58
kNN 0.6811 0.8078 1.76 2.29
Metaclassifiers
Stacking 0.7765 0.8996 0.88 1.12
STRIVE 0.7813 0.9145 0.76 0.94
Table 1: Base classifier and combiner performance
Now, we turn to the issue of whether combination
improves the ranking of the documents. Examining
the results in Table 1, we see that STRIVE statistically
significantly beats every other classifier according to
AUC. Stacking outperforms the base classifiers with
respect to AUC but not statistically significantly.
Examining F1, we see that neither combination
method outperforms the best base classifier, NB
(sent,ngram). If we examine the hypothesis of
whether this base classifier significantly outperforms
either combination method, the hypothesis is re-
jected. Thus, STRIVE improves the overall ranking
with a negligible effect on F1.
Finally, we compare the ROC curves of striving,
stacking, and two of the most competitive base class-
ifiers in Figure 4. We see that striving loses by a
slight amount to stacking early in the curve but still
 0.5
 0.6
 0.7
 0.8
 0.9  1
 0
 0.2
 0.4
 0.6
 0.8
 1
True Positive Rate
False Positive R
ate
M
BN
B
(sent
,ng
ram)
SV
M
(sent
,ng
ram)
Stacking
STR
IV
E
Figure 4: ROC curves (rotated).
beats the base classifiers. Later in the curve, it dom-
inates all the classifiers. If we examine the curves
using error bars, we see that the variance of STRIVE
drops faster than the other classifiers as we move fur-
ther along the x-axis. Thus, STRIVE?s ranking quality
varies less with changes to the training set.
4 Related Work
Several researchers have considered text classifi-
cation tasks similar to action-item detection. Co-
hen et al (2004) describe an ontology of ?speech
acts?, such as ?Propose a Meeting?, and attempt
to predict when an e-mail contains one of these
speech acts. Corston-Oliver et al (2004) con-
sider detecting items in e-mail to ?Put on a To-Do
List? using a sentence-level classifier. In earlier
work (Bennett and Carbonell, 2005), we demon-
strated that sentence-level classifiers typically out-
perform document-level classifiers on this problem
and examined the underlying reasons why this was
330
the case. Furthermore, we presented user studies
demonstrating that users identify action-items more
rapidly when using the system.
In terms of classifier combination, a wide variety
of work has been done in the arena. The STRIVE
metaclassification approach (Bennett et al, 2005)
extended Wolpert?s stacking framework (Wolpert,
1992) to use reliability indicators. In recent work,
Lee et al (2006) derive variance estimates for na??ve
Bayes and tree-augmented na??ve Bayes and use
them in the combination model. Our work comple-
ments theirs by laying groundwork for how to com-
pute variance estimates for models such as kNN that
have no obvious probabilistic component.
5 Future Work and Conclusion
While there are many interesting directions for fu-
ture work, the most interesting is to directly integrate
the sensitivity and calibration quantities derived into
the more general model discussed in Section 2.3.
In this paper, we took an existing approach to
context-dependent combination, STRIVE, that used
many ad hoc reliability indicators and derived a
formal motivation for classifier model-based local
sensitivity indicators. These new reliability indi-
cators are efficiently computable, and the resulting
combination outperformed a vast array of alterna-
tive base classifiers for ranking in an action-item de-
tection task. Furthermore, the combination results
yielded a more robust performance relative to varia-
tion in the training sets. Finally, we demonstrated
that the STRIVE method could be successfully ap-
plied to ranking.
Acknowledgments
This work was supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract No.
NBCHD030010. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA), or the Depart-
ment of Interior-National Business Center (DOI-NBC).
References
Paul N. Bennett and Jaime Carbonell. 2005. Feature repre-
sentation for effective action-item detection. In SIGIR ?05,
Beyond Bag-of-Words Workshop.
Paul N. Bennett, Susan T. Dumais, and Eric Horvitz. 2005.
The combination of text classifiers using reliability indica-
tors. Information Retrieval, 8:67?100.
Paul N. Bennett. 2006. Building Reliable Metaclassifiers for
Text Learning. Ph.D. thesis, CMU. CMU-CS-06-121.
John Carroll. 2002. High precision extraction of grammatical
relations. In COLING ?02.
D.M. Chickering, D. Heckerman, and C. Meek. 1997. A
Bayesian approach to learning Bayesian networks with lo-
cal structure. In UAI ?97.
William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell.
2004. Learning to classify email into ?speech acts?. In
EMNLP ?04.
Simon Corston-Oliver, Eric Ringger, Michael Gamon, and
Richard Campbell. 2004. Task-focused summarization of
email. In Text Summarization Branches Out: Proceedings of
the ACL ?04 Workshop.
Morris H. DeGroot and Stephen E. Fienberg. 1986. Comparing
probability forecasters: Basic binary concepts and multivari-
ate extensions. In P. Goel and A. Zellner, editors, Bayesian
Inference and Decision Techniques. Elsevier.
Luc Devroye, La?szlo? Gyo?rfi, and Ga?bor Lugosi. 1996. A Prob-
abilistic Theory of Pattern Recognition. Springer-Verlag,
New York, NY.
James A. Hanley and Barbara J. McNeil. 1982. The meaning
and use of the area under a recever operating characteristic
(roc) curve. Radiology, 143(1):29?36.
D. Heckerman, D.M. Chickering, C. Meek, R. Rounthwaite,
and C. Kadie. 2000. Dependency networks for inference,
collaborative filtering, and data visualization. JMLR, 1:49?
75.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In Bernhard Scho?lkopf, Christopher J. Burges, and
Alexander J. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Joseph M. Kahn. 2004. Bayesian Aggregation of Probabil-
ity Forecasts on Categorical Events. Ph.D. thesis, Stanford
University, June.
Chi-Hoon Lee, Russ Greiner, and Shaojun Wang. 2006. Using
query-specific variance estimates to combine bayesian class-
ifiers. In ICML ?06.
Andrew McCallum and Kamal Nigam. 1998. A comparison
of event models for naive bayes text classification. In AAAI
?98, Workshops. TR WS-98-05.
Microsoft Corporation. 2001. WinMine
Toolkit v1.0. http://research.microsoft.com/
?dmax/WinMine/ContactInfo.html.
John C. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likelihood
methods. In Alexander J. Smola, Peter Bartlett, Bern-
hard Scholkopf, and Dale Schuurmans, editors, Advances in
Large Margin Classifiers. MIT Press.
David H. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5:241?259.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In SIGIR ?99.
Yiming Yang. 1999. An evaluation of statistical approaches to
text categorization. Information Retrieval, 1(1/2):67?88.
331
Proceedings of the Fifth Law Workshop (LAW V), pages 124?128,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Gold Standard Corpus of Early Modern German
Silke Scheible, Richard J. Whitt, Martin Durrell and Paul Bennett
School of Languages, Linguistics, and Cultures
University of Manchester
Silke.Scheible, Richard.Whitt@manchester.ac.uk
Martin.Durrell, Paul.Bennett@manchester.ac.uk
Abstract
This paper describes an annotated gold stan-
dard sample corpus of Early Modern German
containing over 50,000 tokens of text manu-
ally annotated with POS tags, lemmas, and
normalised spelling variants. The corpus is
the first resource of its kind for this variant of
German, and represents an ideal test bed for
evaluating and adapting existing NLP tools on
historical data. We describe the corpus for-
mat, annotation levels, and challenges, provid-
ing an example of the requirements and needs
of smaller humanities-based corpus projects.
1 Introduction
This paper describes work which is part of a larger
project whose goal is to develop a representative cor-
pus of Early Modern German from 1650-1800. The
GerManC corpus was born out of the need for a re-
source to facilitate comparative studies of the devel-
opment and standardisation of English and German
in the 17th and 18th centuries. One major goal is
to annotate GerManC with linguistic information in
terms of POS tags, lemmas, and normalised spelling
variants. However, due to the lexical, morpholog-
ical, syntactic, and graphemic peculiarities charac-
teristic of Early Modern German, automatic annota-
tion of the texts poses a major challenge. Most ex-
isting NLP tools are tuned to perform well on mod-
ern language data, but perform considerably worse
on historical, non-standardised data (Rayson et al,
2007). This paper describes a gold standard sub-
corpus of GerManC which has been manually anno-
tated by two human annotators for POS tags, lem-
mas, and normalised spelling variants. The corpus
will be used to test and adapt modern NLP tools on
historical data, and will be of interest to other current
corpus-based projects in historical linguistics (Jur-
ish, 2010; Fasshauer, 2011; Dipper, 2010).
2 Corpus design
2.1 GerManC
In order to enable corpus-linguistic investigations,
the GerManC corpus aims to be representative on
three different levels. First of all, the corpus includes
a range of text types: four orally-oriented genres
(dramas, newspapers, letters, and sermons), and four
print-oriented ones (narrative prose, and humanities,
scientific, and legal texts). Secondly, in order to en-
able historical developments to be traced, the pe-
riod has been divided into three fifty year sections
(1650-1700, 1700-1750, and 1750-1800). The com-
bination of historical and text-type coverage should
enable research on the evolution of style in differ-
ent genres (cf. Biber and Finegan, 1989). Finally,
the corpus also aims to be representative with re-
spect to region, including five broad dialect areas:
North German, West Central, East Central, West Up-
per (including Switzerland), and East Upper German
(including Austria). Per genre, period, and region,
three extracts of around 2000 words are selected,
yielding a corpus size of nearly a million words. The
structure of the GerManC corpus is summarised in
Table 1.
2.2 GerManC-GS
In order to facilitate a thorough linguistic inves-
tigation of the data, the final version of the Ger-
124
Periods Regions Genres
1650-1700 North Drama
1700-1750 West Central Newspaper
1750-1800 East Central Letter
West Upper Sermon
East Upper Narrative
Humanities
Scientific
Legal
Table 1: Structure of the GerManC corpus
ManC corpus aims to provide the following linguis-
tic annotations: 1.) Normalised spelling variants;
2.) Lemmas; 3.) POS tags. However, due to the
non-standard nature of written Early Modern Ger-
man, and the additional variation introduced by the
three variables of ?genre?, ?region?, and ?time?, au-
tomatic annotation of the corpus poses a major chal-
lenge. In order to assess the suitability of existing
NLP tools on historical data, and with a view to
adapting them to improve their performance, a man-
ually annotated gold standard subcorpus has been
developed, which aims to be as representative of
the main corpus as possible (GerManC-GS). To re-
main manageable in terms of annotation times and
cost, the subcorpus considers only two of the three
corpus variables, ?genre? and ?time?, as they alone
were found to display as much if not more varia-
tion than ?region?. GerManC-GS thus only includes
texts from the North German dialect region, with
one sample file per genre and time period. Table
2 provides an overview of GerManC-GS, showing
publication year, file name, and number of tokens for
each genre/period combination. It contains 57,845
tokens in total, which have been manually annotated
as described in the following sections.
2.3 Corpus format
As transcription of historical texts needs to be very
detailed with regard to document structure, glossing,
damaged or illegible passages, foreign language ma-
terial and special characters such as diacritics and
ligatures, the raw input texts have been annotated
according to the guidelines of the Text Encoding
Initiative (TEI)1 during manual transcription. The
TEI have published a set of XML-based encoding
conventions recommended for meta-textual markup
1http://www.tei-c.org
to minimise inconsistencies across projects and to
maximise mutual usability and data interchange.
The GerManC corpus has been marked up using
the TEI P5 Lite tagset, which serves as standard for
many humanities-based projects. Only the most rel-
evant tags have been selected to keep the document
structure as straightforward as possible. Figure 1
shows structural annotation of a drama excerpt, in-
cluding headers, stage directions, speakers, as well
as lines.
Figure 1: TEI annotation of raw corpus
3 Linguistic annotation
GerManC-GS has been annotated with linguistic in-
formation in terms of normalised word forms, lem-
mas, and POS tags. To reduce manual labour, a
semi-automatic approach was chosen whose output
was manually corrected by two trained annotators.
The following paragraphs provide an overview of
the annotation types and the main challenges en-
countered during annotation.
3.1 Tokenisation and sentence boundaries
As German orthography was not yet codified in the
Early Modern period, word boundaries were diffi-
cult to determine at times. Clitics and multi-word
tokens are particularly difficult issues: lack of stan-
dardisation means that clitics can occur in various
different forms, some of which are difficult to to-
kenise (e.g. wirstu instead of wirst du). Multi-word
tokens, on the other hand, represent a problem as the
same expression may be sometimes treated as com-
pound (e.g. obgleich), but written separately at other
times (ob gleich). Our tokenisation scheme takes cl-
itics into account, but does not yet deal with multi-
word tokens. This means that whitespace characters
usually act as token boundaries.
125
Genre P Year File name Tokens Genre P Year File name Tokens
DRAM
1 1673 Leonilda 2933
NARR
1 1659 Herkules 2345
2 1749 AlteJungfer 2835 2 1706 SatyrischerRoman 2379
3 1767 Minna 3037 3 1790 AntonReiser 2551
HUMA
1 1667 Ratseburg 2563
NEWS
1 1666 Berlin1 1132
2 1737 Ko?nigstein 2308 2 1735 Berlin 2273
3 1772 Ursprung 2760 3 1786 Wolfenbuettel1 1506
LEGA
1 1673 BergOrdnung 2534
SCIE
1 1672 Prognosticis 2323
2 1707 Reglement 2467 2 1734 Barometer 2438
3 1757 Rostock 2414 3 1775 Chemie 2303
LETT
1 1672 Guericke 2473
SERM
1 1677 LeichSermon 2585
2 1748 Borchward 2557 2 1730 JubelFeste 2523
3 1798 Arndt 2314 3 1770 Gottesdienst 2292
Total number of tokens 57,845
Table 2: GerManC-GS design
Annotation of sentence boundaries is also affected
by the non-standard nature of the data. Punctuation
is not standardised in Early Modern German and
varies considerably across the corpus. For example,
the virgule symbol ?/? was often used in place of
both comma and full-stop, which proves problem-
atic for sentence boundary detection.
3.2 Normalising spelling variants and
lemmatisation
One of the key challenges in working with histor-
ical texts is the large amount of spelling variation
they contain. As most existing NLP tools (such as
POS-taggers or parsers) are tuned to perform well
on modern language data, they are not usually able
to account for variable spelling, resulting in lower
overall performance (Rayson et al, 2007). Like-
wise, modern search engines do not take spelling
variation into account and are thus often unable to
retrieve all occurrences of a given historical search
word. Both issues have been addressed in previ-
ous work through the task of spelling normalisa-
tion. Ernst-Gerlach and Fuhr (2006) and Pilz and
Luther (2009) have created a tool that can gener-
ate variant spellings for historical German to retrieve
relevant instances of a given modern lemma, while
Baron and Rayson (2008) and Jurish (2010) have
implemented tools which normalise spelling vari-
ants in order to achieve better performance of NLP
tools such as POS taggers (by running the tools on
the normalised input). Our annotation of spelling
variants aims to compromise between these two ap-
proaches by allowing for historically accurate lin-
guistic searches, while also aiming to maximise the
performance of automatic annotation tools. We treat
the task of normalising spelling variation as a type
of pre-lemmatisation, where each word token occur-
ring in a text is labelled with a normalised head vari-
ant. As linguistic search requires a historically accu-
rate treatment of spelling variation, our scheme has a
preference for treating two seemingly similar tokens
as separate items on historical grounds (e.g. etwan
vs. etwa). However, the scheme normalises variants
to a modernised form even where the given lexical
item has since died out (e.g. obsolete verbs ending
in -iren are normalised to -ieren), in order to support
automatic tools using morphological strategies such
as suffix probabilities (Schmid, 1994).
Lemmatisation resolves the normalised variant to
a base lexeme in modern form, using Duden2 pre-
reform spelling. With obsolete words, the leading
form in Grimm?s Deutsches Wo?rterbuch3 is taken.
3.3 POS-Tagging
We introduce a modified version of the STTS tagset
(Schiller et al, 1999), the STTS-EMG tagset, to ac-
count for important differences between modern and
Early Modern German (EMG), and to facilitate more
accurate searches. The tagset merges two categories,
as the criteria for distinguishing them are not appli-
cable in EMG (1.), and provides a number of ad-
ditional ones to account for special EMG construc-
tions (2. to 6.):
2http://www.duden.de/
3http://www.dwb.uni-trier.de/
126
1. PIAT (merged with PIDAT): Indefinite deter-
miner (occurring on its own, or in conjunction
with another determiner), as in ?viele solche
Bemerkungen?
2. NA: Adjectives used as nouns, as in ?der
Gesandte?
3. PAVREL: Pronominal adverb used as relative,
as in ?die Puppe, damit sie spielt?
4. PTKREL: Indeclinable relative particle, as in
?die Fa?lle, so aus Schwachheit entstehen?
5. PWAVREL: Interrogative adverb used as
relative, as in ?der Zaun, woru?ber sie springt?
6. PWREL: Interrogative pronoun used as rela-
tive, as in ?etwas, was er sieht?
Around 2.0% (1132) of all tokens in the corpus
have been tagged with one of the above POS cate-
gories, of which the merged PIAT class contains the
majority (657 tokens). The remaining 475 cases oc-
cur as NA (291), or as one of the new relative mark-
ers PWAVREL (69), PWREL (57), PTKREL (38),
and PAVREL (20).
4 Annotation procedure and agreement
In order to produce the gold standard annotations in
GerManC-GS we used the GATE platform, which
facilitates automatic as well as manual annotation
(Cunningham et al 2002). Initially, GATE?s Ger-
man Language plugin4 was used to obtain word to-
kens and sentence boundaries. The output was man-
ually inspected and corrected by one annotator, who
manually added a layer of normalised spelling vari-
ants (NORM). This annotation layer was then used
as input for the TreeTagger (Schmid, 1994), obtain-
ing annotations in terms of lemmas (LEMMA) and
POS tags (POS). All annotations (NORM, LEMMA,
and POS) were subsequently corrected by two an-
notators, and all disagreements were reconciled to
produce the gold standard. Table 3 shows the over-
all agreement for the three annotation types across
GerManC-GS (measured in accuracy).
The agreement values demonstrate that nor-
malised word forms and lemmas are relatively easy
to determine for the annotators, with 96.9% and
95.5% agreement, respectively. POS tags, on the
other, represent more of a challenge with only 91.6%
4http://gate.ac.uk/sale/tao/splitch15.html
NORM LEMMA POS
Agreed tokens
(out of 57,845)
56,052 55,217 52,959
Accuracy (%) 96.9% 95.5% 91.6%
Table 3: Inter-annotator agreement
agreement between two annotators, which is consid-
erably lower than the agreement level reported for
annotating a corpus of modern German using STTS,
at 98.6% (Brants, 2000a). While a more detailed
analysis of the results remains to be carried out, an
initial study shows that POS agreement is lower in
earlier texts (89.3% in Period P1) compared to later
ones (93.1% in P3). It is likely that a substantial
amount of disagreements in the earlier texts are due
to the larger number of unfamiliar word forms and
variants on the one hand, and foreign word tokens
on the other. These represent a problem as from a
modern view point it is not always easy to decide
which words were ?foreign? to a language and which
ones ?native?.
5 Future work
The gold standard corpus described in this paper will
be used to test and adapt modern NLP tools on Early
Modern German data. Initial experiments focus on
utilising the layer of normalised spelling variants
to improve tagger performance, and investigating to
what extent normalisation can be reliably automated
(Jurish, 2010). We further plan to retrain state-of-
the-art POS taggers such as the TreeTagger and TnT
Tagger (Brants, 2000b) on our data.
Finally, we plan to investigate how linguistic an-
notations can be automatically integrated in the TEI-
annotated version of the corpus to produce TEI-
conformant output. Currently, both structural and
linguistic annotations are merged in GATE stand-off
XML format, which, as a consequence, is no longer
TEI-conformant. In the interest of interoperability
and comparative studies between corpora we aim to
contribute towards the development of clearer proce-
dures whereby structural and linguistic annotations
might be merged (Scheible et al, 2010).
127
References
Alistair Baron and Paul Rayson. 2008. VARD 2: A tool
for dealing with spelling variation in historical cor-
pora. Proceedings of the Postgraduate Conference in
Corpus Linguistics, Birmingham, UK.
Douglas Biber and Edward Finegan. 1989. Drift and the
evolution of English style: a history of three genres.
Language 65. 487-517.
Torsten Brants. 2000a. Inter-annotator agreement for
a German newspaper corpus. Second International
Conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Torsten Brants. 2000b. TnT ? a statistical part-of-speech
tagger. Proceedings of the 6th Applied NLP Confer-
ence, ANLP-2000, Seattle, WA.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Stefanie Dipper. 2010. POS-Tagging of historical lan-
guage data: First experiments in semantic approaches
in Natural Language Processing. Proceedings of
the 10th Conference on Natural Language Processing
(KONVENS-10). Saarbru?cken, Germany. 117-121.
Andrea Ernst-Gerlach and Norbert Fuhr. 2006. Gen-
erating search term variants for text collections with
historic spellings. Proceedings of the 28th European
Conference on Information Retrieval Research (ECIR
2006), London, UK.
Vera Fasshauer. 2011. http://www.indogermanistik.uni-
jena.de/index.php?auswahl=184
Accessed 30/03/2011.
Bryan Jurish. 2010. Comparing canonicalizations of his-
torical German text. Proceedings of the 11th Meeting
of the ACL Special Interest Group on Computational
Morphology and Phonology (SIGMORPHON), Upp-
sala, Sweden. 72-77.
Thomas Pilz and Wolfram Luther. 2009. Automated
support for evidence retrieval in documents with non-
standard orthography. The Fruits of Empirical Lin-
guistics. Sam Featherston and Susanne Winkler (eds.).
211?228.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
Bard: Evaluating the accuracy of a modern POS tagger
on Early Modern English corpora. Proceedings of the
Corpus Linguistics Conference (CL2007), University
of Birmingham, UK.
Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul
Bennett. 2010. Annotating a Historical Corpus of
German: A Case Study. Proceedings of the LREC
2010Workshop on Language Resources and Language
Technology Standards, Valletta, Malta.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1999. Guidelines fu?r das Tagging
deutscher Textcorpora mit STTS. Technical Report.
Institut fu?r maschinelle Sprachverarbeitung, Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. International Conference
on NewMethods in Language Processing, Manchester,
UK. 44?49.
128
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 19?23,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Evaluating an ?off-the-shelf? POS-tagger on Early Modern German text
Silke Scheible, Richard J. Whitt, Martin Durrell and Paul Bennett
School of Languages, Linguistics, and Cultures
University of Manchester
Silke.Scheible, Richard.Whitt@manchester.ac.uk
Martin.Durrell, Paul.Bennett@manchester.ac.uk
Abstract
The goal of this study is to evaluate an ?off-
the-shelf? POS-tagger for modern German on
historical data from the Early Modern period
(1650-1800). With no specialised tagger avail-
able for this particular stage of the language,
our findings will be of particular interest to
smaller, humanities-based projects wishing to
add POS annotations to their historical data
but which lack the means or resources to train
a POS tagger themselves. Our study assesses
the effects of spelling variation on the perfor-
mance of the tagger, and investigates to what
extent tagger performance can be improved by
using ?normalised? input, where spelling vari-
ants in the corpus are standardised to a mod-
ern form. Our findings show that adding such
a normalisation layer improves tagger perfor-
mance considerably.
1 Introduction
The work described in this paper is part of a larger
investigation whose goal is to create a representative
corpus of Early Modern German from 1650-1800.
The GerManC corpus, which is due to be completed
this summer, was developed to allow for compara-
tive studies of the development and standardisation
of English and German in the 17th and 18th cen-
turies. In order to facilitate corpus-linguistic inves-
tigations, one of the major goals of the project is
to annotate the corpus with POS tags. However,
no specialised tools are yet available for process-
ing data from this period. The goal of this study is
therefore to evaluate the performance of an ?off-the-
shelf? POS-tagger for modern German on data from
the Early Modern period, in order to assess if mod-
ern tools are suitable for a semi-automatic approach,
and how much manual post-processing work would
be necessary to obtain gold standard POS annota-
tions.
We report on our results of running the TreeTag-
ger (Schmid, 1994) on a subcorpus of GerManC
containing over 50,000 tokens of text annotated with
gold standard POS tags. This subcorpus is the first
resource of its kind for this variant of German, and
due to its complex structure it represents an ideal test
bed for evaluating and adapting existing NLP tools
on data from the Early Modern period. The study
described in this paper represents a first step towards
this goal. Furthermore, as spelling variants in our
corpus have been manually normalised to a modern
standard, this paper also aims to explore the extent
to which tagger performance is affected by spelling
variation, and to what degree performance can be
improved by using ?normalised? input. Our findings
promise to be of considerable interest to other cur-
rent corpus-based projects of earlier periods of Ger-
man (Jurish, 2010; Fasshauer, 2011; Dipper, 2010).
Before presenting the results in Section 4, we de-
scribe the corpus design (Section 2), and the prepro-
cessing steps necessary to create the gold standard
annotations, including adaptations to the POS tagset
(Section 3).
2 Corpus design
In order to be as representative of Early Modern Ger-
man as possible, the GerManC corpus design con-
siders three different levels. First, the corpus in-
cludes a range of text types: four orally-oriented
19
genres (dramas, newspapers, letters, and sermons),
and four print-oriented ones (narrative prose, and
humanities, scientific, and legal texts). Secondly, in
order to enable historical developments to be traced,
the period is divided into three fifty year sections
(1650-1700, 1700-1750, and 1750-1800). Finally,
the corpus also aims to be representative with re-
spect to region, including five broad areas: North
German, West Central, East Central, West Upper
(including Switzerland), and East Upper German
(including Austria). Three extracts of around 2000
words were selected per genre, period, and region,
yielding a corpus size of nearly a million words.
The experiments described in this paper were car-
ried out on a manually annotated gold standard sub-
corpus of GerManC, GerManC-GS. The subcorpus
was developed to enable an assessment of the suit-
ability of existing NLP tools on historical data, with
a view to adapting them to improve their perfor-
mance. For this reason, GerManC-GS aims to be as
representative of the main corpus as possible. How-
ever, to remain manageable in terms of annotation
times and cost, the subcorpus only considers two
of the three corpus variables, ?genre? and ?time?, as
they alone were found to display as much if not more
variation than ?region?. GerManC-GS thus includes
texts from the North German region, with one sam-
ple file per genre and time period. The corpus con-
tains 57,845 tokens in total, and was annotated with
gold standard POS tags, lemmas, and normalised
word forms (Scheible et al, to appear).
3 Creating the gold standard annotations
This section provides an overview of the preprocess-
ing work necessary to obtain the gold standard an-
notations in GerManC-GS. We used the GATE plat-
form to produce the initial annotations, which facil-
itates automatic as well as manual annotation (Cun-
ningham et al, 2002). First, GATE?s German Lan-
guage plugin1 was used to obtain word tokens and
sentence boundaries. The output was manually in-
spected and corrected by one annotator, who fur-
ther added a layer of normalised spelling variants.
This annotation layer was then used as input for the
TreeTagger (Schmid, 1994), obtaining annotations
in terms of POS tags and lemmas. All annotations
1http://gate.ac.uk/sale/tao/splitch15.html
were subsequently corrected by two annotators, and
disagreements were reconciled to produce the gold
standard.
3.1 Tokenisation
As German orthography was not yet codified in the
Early Modern period, a number of specific deci-
sions had to be made in respect of tokenisation. For
example, clitics can occur in various non-standard
forms. To allow for accurate POS tagging, clitics
should be tokenised as separate items, similar to the
negative particle n?t in can?t in English, which is
conventionally tokenised as ca|n?t. A case in point
is hastu, a clitic version of hast du (?have you?),
which we tokenise as has|tu. Furthermore, Ger-
man ?to-infinitive? verb forms are often directly ap-
pended to the infinitival marker zu without interven-
ing whitespace (e.g. zugehen instead of zu gehen,
?to go?). Such cases are tokenised as separate forms
(zu|gehen) to allow for their accurate tagging as
zu/PTKZU gehen/VVINF.
A further problem can be found in multi-word
tokens, where the same expression is sometimes
treated as a compound (e.g. obgleich), but at other
times written separately (ob gleich). Such cases rep-
resent a problem for POS-tagging as the variants
have to be treated differently even though their func-
tion in the sentence is the same. Our tokenisation
scheme deals with these in a similar way to nor-
mal conjunctions consisting of two words, where
the most suitable tags are assigned to each token
(e.g. als/KOKOM wenn/KOUS). Thus, the com-
pound obgleich is tagged KOUS, while the multi-
word variant ob gleich is tagged as ob/KOUS gle-
ich/ADV.
3.2 Normalising spelling variants
All spelling variants in GerManC-GS were nor-
malised to a modern standard. We view the task
of normalising spelling variation as a type of pre-
lemmatisation, where each word token occurring
in a text is labelled with a normalised head vari-
ant. As linguistic searches require a historically ac-
curate treatment of spelling variation, our scheme
has a preference for treating two seemingly simi-
lar tokens as separate items on historical grounds
(e.g. etwan vs. etwa). On the other hand, the
scheme normalises variants to a modernised form
20
even where the given lexical item has since died out
(e.g. obsolete verbs ending in -iren are normalised
to -ieren), in order to support automatic tools using
morphological strategies such as suffix probabilities
(Schmid, 1994). Inter-annotator agreement for an-
notating spelling variation was 96.9%, which indi-
cates that normalisation is a relatively easy task.
Figure 1 shows the proportion of normalised word
tokens in the individual corpus files plotted against
time. The graph clearly shows a decline of spelling
variants over time: while the earlier texts contain 35-
40% of normalised tokens, the proportion is lower
in later texts (11.3% in 1790, and 5.4% in 1798).
This suggests that by the end of the period (1800)
codification of the German language was already at
an advanced stage.
Figure 1: Proportion of normalised tokens (plotted
against time)
3.3 Adapting the POS tagset (STTS)
To account for important differences between mod-
ern and Early Modern German (EMG), and to facil-
itate more accurate searches, we adapted the STTS
tagset (Schiller et al, 1999). The STTS-EMG tagset
merges two categories, as the criteria for distinguish-
ing them are not applicable in EMG (1.), and pro-
vides a number of additional ones to account for spe-
cial EMG constructions (2. to 6.):
1. PIAT (merged with PIDAT): Indefinite de-
terminer, as in ?viele solche Bemerkungen?
(?many such remarks?)
2. NA: Adjectives used as nouns, as in ?der
Gesandte? (?the ambassador?)
3. PAVREL: Pronominal adverb used as relative,
as in ?die Puppe, damit sie spielt? (?the doll
with which she plays?)
4. PTKREL: Indeclinable relative particle, as in
?die Fa?lle, so aus Schwachheit entstehen? (?the
cases which arise from weakness?)
5. PWAVREL: Interrogative adverb used as
relative, as in ?der Zaun, woru?ber sie springt?
(?the fence over which she jumps?)
6. PWREL: Interrogative pronoun used as rel-
ative, as in ?etwas, was er sieht? (?something
which he sees?)
Around 2.0% (1132) of all tokens in the corpus
were tagged with one of the above POS categories.
Inter-annotator agreement for the POS tagging task
was 91.6%.
4 ?Off-the-shelf? tagger evaluation on
Early Modern German data
The evaluation described in this section aims to
complement the findings of Rayson et al (2007) for
Early Modern English, and a recent study by Dip-
per (2010), in which the TreeTagger is applied to a
corpus of texts from Middle High German (MHG)
- i.e. a period earlier than ours, from 1050-1350.
Both studies report considerable improvement of
POS-tagging accuracy on normalised data. How-
ever, unlike Dipper (2010), whose experiments in-
volve retraining the TreeTagger on a modified ver-
sion of STTS, our experiments assess the ?off-the-
shelf? performance of the modern tagger on histor-
ical data. We further explore the question of what
effect spelling variation has on the performance of a
tagger, and what improvement can be achieved when
running the tool on normalised data.
Table 1 shows the results of running the Tree-
Tagger on the original data vs. normalised data in
our corpus using the parameter file for modern Ger-
man supplied with the tagger2. The results show that
while overall accuracy for running the tagger on the
original input is relatively low at 69.6%, using the
normalised tokens as input results in an overall im-
provement of 10% (79.7%).
2http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
21
O N
Accuracy 69.6% 79.7%
Table 1: TreeTagger accuracy on original (O) vs. nor-
malised (N) input
However, improvement through normalisation is
not distributed evenly across the corpus. Figure 2
shows the performance curves of using TreeTagger
on original (O) and normalised (N) input plotted
against publication date. While both curves grad-
ually rise over time, the improvement curve (mea-
sured as difference in accuracy between N and O)
diminishes, a direct result of spelling variation be-
ing more prominent in earlier texts (cf. Figure 1).
Figure 2: Tagger performance plotted against publication
date
Compared with the performance of the TreeTag-
ger on modern data (ca. 97%; Schmid, (1995)), the
current results seem relatively low. However, two is-
sues should be taken into account when interpreting
these findings: First, the modern accuracy figures
result from an evaluation of the tagger on the text
type it was developed on (newspaper text), while
GerManC-GS includes a variety of genres, which
is bound to result in lower performance. Secondly,
inter-annotator agreement was also found to be con-
siderably lower in the present task (91.6%) than in
one reported for modern German (98.6%; Brants,
2000a). This is likely to be due to the large number
of unfamiliar word forms and variants in the corpus,
which represent a problem for human annotators.
Finally, Figure 3 provides a more detailed
overview of the effects of spelling variation on POS
tagger performance. Of 12,744 normalised tokens in
the corpus, almost half (5981; 47%) are only tagged
correctly when using the normalised variants as in-
put. Using the original word form as input results
in a false POS tag in these cases. Overall, this ac-
counts for an improvement of around 10.3% (5981
out of 57,845 tokens in the corpus). However, 32%
(4119) of normalised tokens are tagged correctly us-
ing both N and O input, while 18% (2339) of to-
kens are tagged incorrectly using both types of input.
This means that for 50% of all annotated spelling
variants, normalisation has no effect on POS tagger
performance. In a minority of cases (305; 3%) nor-
malisation has a negative effect on tagger accuracy.
Figure 3: Effect of using original (O)/normalised (N) in-
put on tagger accuracy for normalised tokens (+: cor-
rectly tagged; -: incorrectly tagged)
5 Conclusion and future work
The results of our study show that using an ?off-the
shelf? German POS tagger on data from the Early
Modern period achieves reasonable results (69.6%
on average), but requires a substantial amount of
manual post-editing. We further demonstrated that
adding a normalisation layer can improve results by
10%. However, using the current manual normalisa-
tion scheme only half of all annotations carried out
have a positive effect on tagger performance. In fu-
ture work we plan to investigate if the scheme can
be adapted to account for more cases, and to what
extent normalisation can be reliably automated (Jur-
ish, 2010). Finally, we plan to retrain state-of-the-art
POS taggers such as the TreeTagger and TnT Tagger
(Brants, 2000b) on our data and compare the results
to the findings of this study.
22
References
Torsten Brants. 2000a. Inter-annotator agreement for
a German newspaper corpus. Second International
Conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Torsten Brants. 2000b. TnT ? a statistical part-of-speech
tagger. Proceedings of the 6th Applied NLP Confer-
ence, ANLP-2000, Seattle, WA.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Stefanie Dipper. 2010. POS-Tagging of historical lan-
guage data: First experiments in semantic approaches
in Natural Language Processing. Proceedings of
the 10th Conference on Natural Language Processing
(KONVENS-10). Saarbru?cken, Germany. 117-121.
Vera Fasshauer. 2011. http://www.indogermanistik.uni-
jena.de/index.php?auswahl=184
Accessed 30/03/2011.
Bryan Jurish. 2010. Comparing canonicalizations of his-
torical German text. Proceedings of the 11th Meeting
of the ACL Special Interest Group on Computational
Morphology and Phonology (SIGMORPHON), Upp-
sala, Sweden. 72-77.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
Bard: Evaluating the accuracy of a modern POS tagger
on Early Modern English corpora. Proceedings of the
Corpus Linguistics Conference (CL2007), University
of Birmingham, UK.
Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul
Bennett. To appear. A Gold Standard Corpus of Early
Modern German. Proceedings of the Fifth Linguistic
Annotation Workshop (LAW V), Portland, Oregon.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1999. Guidelines fu?r das Tagging
deutscher Textcorpora mit STTS. Technical Report.
Institut fu?r maschinelle Sprachverarbeitung, Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. International Conference
on NewMethods in Language Processing, Manchester,
UK. 44?49.
Helmut Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. Proceedings
of the ACL SIGDAT-Workshop. 47?50.
23
