Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 16?19,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
TransAhead: A Writing Assistant for CAT and CALL 
 
*Chung-chi Huang  ++Ping-che Yang  *Mei-hua Chen *Hung-ting Hsieh  +Ting-hui Kao 
 
   
+Jason S. Chang 
*ISA, NTHU, HsinChu, Taiwan, R.O.C.  
++III, Taipei, Taiwan, R.O.C. +CS, NTHU, HsinChu, Taiwan, R.O.C. 
{u901571,maciaclark,chen.meihua,vincent732,maxis1718,jason.jschang}gmail.com 
 
Abstract 
We introduce a method for learning to 
predict the following grammar and text 
of the ongoing translation given a source 
text. In our approach, predictions are 
offered aimed at reducing users? burden 
on lexical and grammar choices, and 
improving productivity. The method 
involves learning syntactic phraseology 
and translation equivalents. At run-time, 
the source and its translation prefix are 
sliced into ngrams to generate subsequent 
grammar and translation predictions. We 
present a prototype writing assistant, 
TransAhead1, that applies the method to 
where computer-assisted translation and 
language learning meet. The preliminary 
results show that the method has great 
potentials in CAT and CALL (significant 
boost in translation quality is observed). 
1.  Introduction 
More and more language learners use the MT 
systems on the Web for language understanding 
or learning. However, web translation systems 
typically suggest a, usually far from perfect, one-
best translation and hardly interact with the user. 
Language learning/sentence translation could 
be achieved more interactively and appropriately 
if a system recognized translation as a 
collaborative sequence of the user?s learning and 
choosing from the machine-generated predictions 
of the next-in-line grammar and text and the 
machine?s adapting to the user?s accepting 
/overriding the suggestions. 
Consider the source sentence ????????
?????????? (We play an important role 
in closing this deal). The best learning 
environment is probably not the one solely 
                                                          
1Available at http://140.114.214.80/theSite/TransAhead/ 
which, for the time being, only supports Chrome browsers. 
providing the automated translation. A good 
learning environment might comprise a writing 
assistant that gives the user direct control over 
the target text and offers text and grammar 
predictions following the ongoing translations. 
We present a new system, TransAhead, that 
automatically learns to predict/suggest the 
grammatical constructs and lexical translations 
expected to immediately follow the current 
translation given a source text, and adapts to the 
user?s choices. Example TransAhead responses 
to the source ?????????????????? 
and the ongoing translation ?we? and ?we play 
an important role? are shown in Figure 12(a) and 
(b) respectively. TransAhead has determined the 
probable subsequent grammatical constructions 
with constituents lexically translated, shown in 
pop-up menus (e.g., Figure 1(b) shows a 
prediction ?IN[in] VBG[close, end, ?]? due to 
the history ?play role? where lexical items in 
square brackets are lemmas of potential 
translations). TransAhead learns these constructs 
and translations during training. 
At run-time, TransAhead starts with a source 
sentence, and iteratively collaborates with the 
user: by making predictions on the successive 
grammar patterns and lexical translations, and by 
adapting to the user?s translation choices to 
reduce source ambiguities (e.g., word 
segmentation and senses). In our prototype, 
TransAhead mediates between users and 
automatic modules to boost users? writing/ 
translation performance (e.g., productivity). 
2.  Related Work 
CAT has been an area of active research. Our 
work addresses an aspect of CAT focusing on 
language learning. Specifically, our goal is to 
build a human-computer collaborative writing 
assistant: helping the language learner with in- 
text  grammar  and  translation  and  at  the  same 
                                                          
2
 Note that grammatical constituents (in all-capitalized 
words) are represented using Penn parts-of-speech and the 
history based on the user input is shown in shades. 
16
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an important role?. Note 
that the grammar/text predictions of (a) and (b) are not placed directly under the current input focus for space limit. (c) and (d) 
depict predominant grammar constructs which follow and (e) summarizes the translations for the source?s character-based ngrams. 
 
time updating the system?s segmentation 
/translation options through the user?s word 
choices. Our intended users are different from 
those of the previous research focusing on what 
professional translator can bring for MT systems 
(e.g., Brown and Nirenburg, 1990). 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from analyses 
of the source text to the formation of the target 
translation. TransType project (Foster et al 2002) 
describes such pioneering system that supports 
next word predictions. Koehn (2009) develops 
caitra which displays one phrase translation at a 
time and offers alternative translation options. 
Both systems are similar in spirit to our work. 
The main difference is that we do not expect the 
user to be a professional translator and we 
provide translation hints along with grammar 
predictions to avoid the generalization issue 
facing phrase-based system. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al(2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al
(2004) and Ortiz-Martinez et al(2011) further 
exploit user feedbacks for better IMT systems 
and user experience. Instead of trigged by user 
correction, our method is triggered by word 
delimiter and assists in target language learning. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests 
subsequent grammar constructs with translations 
and interactively collaborates with learners, in 
view of reducing users? burden on grammar and 
word choice and enhancing their writing quality. 
3.  The TransAhead System 
3.1 Problem Statement 
For CAT and CALL, we focus on predicting a 
set of grammar patterns with lexical translations 
likely to follow the current target translation 
given a source text. The predictions will be 
examined by a human user directly. Not to 
overwhelm the user, our goal is to return a 
reasonable-sized set of predictions that contain 
suitable word choices and correct grammar to 
choose and learn from. Formally speaking, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus 
Cst, a source-language text S, and its target 
translation prefix Tp. Our goal is to provide a set 
of predictions based on Ct and Cst likely to 
further translate S in terms of grammar and text. 
For this, we transform S and Tp into sets of 
ngrams such that the predominant grammar 
constructs with suitable translation options 
following Tp are likely to be acquired. 
3.2  Learning to Find Pattern and Translation 
We attempt to find syntax-based phraseology and 
translation equivalents beforehand (four-staged) 
so that a real-time system is achievable. 
Firstly, we syntactically analyze the corpus Ct. 
In light of the phrases in grammar book (e.g., 
one?s in ?make up one?s mind?), we resort to 
parts-of-speech for syntactic generalization. 
Secondly, we build up inverted files of the words 
in Ct for the next stage (i.e., pattern grammar 
generation). Apart from sentence and position 
information, a word?s lemma and part-of-speech 
(POS) are also recorded. 
(b) 
Source text: 
???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..] , ? 
we VBP[play, act, ..] DT , ? 
we VBD[play, act, ..] DT , ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] , ? 
important role IN[in] VBG[close, end, ..] , ? 
role IN[in] VBG[close, end, ..] , ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB , ?, 
we VBP DT , ?, 
we VBD DT , ? 
Patterns for ?we play an important role?: 
play role IN[in] DT , 
play role IN[in] VBG , ?, 
important role IN[in] VBG , ?, 
role IN[in] VBG , ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
17
We then leverage the procedure in Figure 2 to 
generate grammar patterns for any given 
sequence of words (e.g., contiguous or not). 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation 
on the inverted file, InvList, of the current word 
wi and interInvList, a previous intersected results. 
Afterwards, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?,wordPosi(wn)], sentence 
number) denoting the positions of query?s words 
in the sentence, we generate grammar pattern 
involving replacing words with POS tags and 
words in wordPosi(wi) with lemmas, and 
extracting fixed-window3  segments surrounding 
query from the transformed sentence. The result 
is a set of grammatical, contextual patterns. 
The procedure finally returns top N 
predominant syntactic patterns associated with 
the query. Such patterns characterizing the 
query?s word usages follow the notion of pattern 
grammar in (Hunston and Francis, 2000) and are 
collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through 
leveraging IBM models to word-align the bitexts, 
?smoothing? the directional word alignments via 
grow-diagonal-final, and extracting translation 
equivalents using (Koehn et al 2003). 
3.3  Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, TransAhead then 
predicts/suggests the following grammar and text 
of a translation prefix given the source text using 
the procedure in Figure 3. 
We first slice the source text S and its 
translation prefix Tp into character-level and 
                                                          
3
 Inspired by (Gamon and Leacock, 2010). 
word-level ngrams respectively. Step (3) and (4) 
retrieve the translations and patterns learned 
from Section 3.2. Step (3) acquires the active 
target-language vocabulary that may be used to 
translate the source text. To alleviate the word 
boundary issue in MT raised by Ma et al(2007), 
TransAhead non-deterministically segments the 
source text using character ngrams and proceeds 
with collaborations with the user to obtain the 
segmentation for MT and to complete the 
translation. Note that a user vocabulary of 
preference (due to users? domain of knowledge 
or errors of the system) may be exploited for 
better system performance. On the other hand, 
Step (4) extracts patterns preceding with the 
history ngrams of {tj}. 
 
Figure 3. Predicting pattern grammar and translations. 
 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and 
t is one of the translation candidates under S and 
Tp. Subsequently, we incorporate the lemmatized 
translation candidates into grammar constituents 
in GramOptions. For example, we would include 
?close? in pattern ?play role IN[in] VBG? as 
?play role IN[in] VBG[close]?. 
At last, the algorithm returns the 
representative grammar patterns with confident 
translations expected to follow the ongoing 
translation and further translate the source. This 
algorithm will be triggered by word delimiter to 
provide an interactive environment where CAT 
and CALL meet. 
4.  Preliminary Results 
To train TransAhead, we used British National 
Corpus and Hong Kong Parallel Text and 
deployed GENIA tagger for POS analyses. 
To evaluate TransAhead in CAT and CALL, 
we introduced it to a class of 34 (Chinese) first-
year college students learning English as foreign 
language. Designed to be intuitive to the general 
public, esp. language learners, presentational 
tutorial lasted only for a minute. After the tutorial, 
the participants were asked to translate 15 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency (7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgram(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
18
Chinese texts from (Huang et al 2011a) one by 
one (half with TransAhead assistance, and the 
other without). Encouragingly, the experimental 
group (i.e., with the help of our system) achieved 
much better translation quality than the control 
group in BLEU (Papineni et al 2002) (i.e., 
35.49 vs. 26.46) and significantly reduced the 
performance gap between language learners and 
automatic decoder of Google Translate (44.82).  
We noticed that, for the source ?????????
?????????, 90% of the participants in the 
experimental group finished with more 
grammatical and fluent translations (see Figure 4) 
than (less interactive) Google Translate (?We 
conclude this transaction plays an important 
role?). In comparison, 50% of the translations of 
the source from the control group were erroneous. 
 
Figure 4. Example translations with TransAhead assistance. 
 
Post-experiment surveys indicate that a) the 
participants found TransAhead intuitive enough 
to collaborate with in writing/translation; b) the 
participants found TransAhead suggestions 
satisfying, accepted, and learned from them; c) 
interactivity made translation and language 
learning more fun and the participants found 
TransAhead very recommendable and would like 
to use the system again in future translation tasks. 
5.  Future Work and Summary 
Many avenues exist for future research and 
improvement. For example, in the linear 
combination, the patterns? frequencies could be 
considered and the feature weight could be better 
tuned. Furthermore, interesting directions to 
explore include leveraging user input such as 
(Nepveu et al 2004) and (Ortiz-Martinez et al 
2010) and serially combining a grammar checker 
(Huang et al 2011b). Yet another direction 
would be to investigate the possibility of using 
human-computer collaborated translation pairs to 
re-train word boundaries suitable for MT. 
In summary, we have introduced a method for 
learning to offer grammar and text predictions 
expected to assist the user in translation and 
writing (or even language learning). We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising, 
prompting us to further qualitatively and 
quantitatively evaluate our system in the near 
future (i.e., learners? productivity, typing speed 
and keystroke ratios of ?del? and ?backspace? 
(possibly hesitating on the grammar and lexical 
choices), and human-computer interaction, 
among others). 
Acknowledgement 
This study is conducted under the ?Project 
Digital Convergence Service Open Platform? of 
the Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs 
of the Republic of China. 
References  
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches to 
computer-assisted translation. Computer Linguistics, 
35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In Proceedings 
of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop on 
Innovative Use of NLP for Building Educational 
Applications, pages 37-44. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and J. 
S. Chang. 2011a. GRASP: grammar- and syntax-based 
pattern-finder in CALL. In Proceedings of ACL. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, and J. S. Chang. 
2011b. EdIt: a broad-coverage grammar checker using 
pattern grammar. In Proceedings of ACL. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer aided 
translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping word 
alignment via word packing. In Proceedings of ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004. 
Adaptive language and translation models for interactive 
machine translation. In Proceedings of EMNLP. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19-51. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-Varea, 
and F. Casacuberta. 2011. An interactive machine 
translation system with online learning. In Proceedings 
of ACL System Demonstrations, pages 68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: a 
method for automatic evaluation of machine translation. 
In Proceedings of ACL, pages 311-318. 
1. we play(ed) a critical role in closing/sealing this/the deal. 
2. we play(ed) an important role in ending/closing this/the deal. 
19
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 352?356,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
TransAhead: A Computer-Assisted Translation and Writing Tool 
G
G
*Chung-chi Huang      +Ping-che Yang **Keh-jiann Chen       ++Jason S. Chang 
  
*ISA, NTHU, HsinChu, Taiwan, R.O.C. **IIS, Academia Sinica, Taipei, Taiwan, R.O.C. 
+III, Taipei, Taiwan, R.O.C. ++CS, NTHU, HsinChu, Taiwan, R.O.C. 
{*u901571,+maciaclark,++jason.jschang}@gmail.com; **kchen@iis.sinica.edu.tw 
  
 
Abstract 
We introduce a method for learning to predict 
text completion given a source text and partial 
translation. In our approach, predictions are 
offered aimed at alleviating users? burden on 
lexical and grammar choices, and improving 
productivity. The method involves learning 
syntax-based phraseology and translation 
equivalents. At run-time, the source and its 
translation prefix are sliced into ngrams to 
generate and rank completion candidates, 
which are then displayed to users. We present 
a prototype writing assistant, TransAhead, that 
applies the method to computer-assisted 
translation and language learning. The 
preliminary results show that the method has 
great potentials in CAT and CALL with 
significant improvement in translation quality 
across users. 
1 Introduction 
More and more language workers and learners use 
the MT systems on the Web for information 
gathering and language learning. However, web 
translation systems typically offer top-1 
translations (which are usually far from perfect) 
and hardly interact with the user. 
Text translation could be achieved more 
interactively and effectively if a system considered 
translation as a collaborative between the machine 
generating suggestions and the user accepting or 
overriding on those suggestions, with the system 
adapting to the user?s action. 
Consider the source sentence ?????????
????????? (We play an important role in 
closing this deal). The best man-machine 
interaction is probably not the one used by typical 
existing MT systems. A good working 
environment might be a translation assistant that 
offers suggestions and gives the user direct control 
over the target text. 
We present a system, TransAhead1, that learns 
to predict and suggest lexical translations (and 
their grammatical patterns) likely to follow the 
ongoing translation of a source text, and adapts to 
the user?s choices. Example responses of 
TransAhead to the source sentence ????????
?????????? and two partial translations  
are shown in Figure 1. The responses include text 
and grammatical patterns (in all-cap labels 
representing parts-of-speech). TransAhead 
determines and displays the probable subsequent 
grammatical constructions and partial translations 
in the form of parts-of-speech and words (e.g., 
?IN[in] VBG[close,?]? for keywords ?play role? 
where lexical items in square brackets are lemmas 
of potential translations) in a pop-up. TransAhead 
learns these constructs and translations during 
training. 
At run-time, TransAhead starts with a source 
sentence, and iterates with the user, making 
predictions on the grammar patterns and lexical 
translations, while adapting to the user?s 
translation choices to resolve ambiguities in the 
source sentence related to word segmentation and 
word sense. In our prototype, TransAhead 
mediates between users and suggestion modules to 
translation quality and  productivity. 
2 Related Work 
Computer Assisted Translation (CAT) has been an 
area of active research. We focus on offering 
suggestions during the  translation process with  an 
                                                           
1
 http://140.114.214.80/theSite/TransAhead/ (Chrome only) 
352
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an 
important role?. Note that the grammar/text predictions of (a) and (b) are not placed directly under the caret (current 
input focus) for space limit. (c) and (d) depict predominant grammar constructs which follow and (e) summarizes 
the confident translations of the source?s character-based ngrams. The frequency of grammar pattern is shown in 
round brackets while the history (i.e., keyword) based on the user input is shown in shades. 
 
emphasis on language learning. Specifically, our 
goal is to build a translation assistant to help 
translator (or learner-translator) with inline 
grammar help and translation. Unlike recent 
research focusing on professional (e.g., Brown and 
Nirenburg, 1990), we target on both professional 
and student translators. 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from post-
editing machine output to collaborating with the 
machine to produce the target text. Foster et al
(2000) describe TransType, a pioneering system 
that supports next word predictions. Along the 
similar line, Koehn (2009) develops caitra which 
predicts and displays phrasal translation 
suggestions one phrase at a time. The main 
difference between their systems and TransAhead 
is that we also display grammar patterns to provide 
the general patterns of predicted translations so a 
student translator can learn and become more 
proficient. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al (2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al 
(2004) and Ortiz-Martinez et al (2011) further 
exploit user feedbacks for better IMT systems and 
user experience. Instead of triggered by user 
correction, our method is triggered by word 
delimiter and assists both translation and learning 
the target language. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests grammar 
constructs as well as lexical translations following 
users? partial translation, aiming to provide users 
with choice to ease mental burden and enhance 
performance. 
3 The TransAhead System 
3.1 Problem Statement 
We focus on predicting a set of grammar patterns 
with lexical translations likely to follow the current 
partial target translation of a source text. The 
predictions will be examined by a human user 
directly. Not to overwhelm the user, our goal is to 
return a reasonable-sized set of predictions that 
contain suitable word choices and grammatical 
patterns to choose and learn from. Formally, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus Cst, 
a source-language text S, and its translation prefix 
Tp. Our goal is to provide a set of predictions based 
on Ct and Cst likely to further translate S in terms of 
grammar and text. For this, we transform S and Tp 
into sets of ngrams such that the predominant 
grammar constructs with suitable translation 
options following Tp are likely to be acquired. 
(b) 
Source text: ???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..]  (41369), ? 
we VBP[play, act, ..] DT  (13138), ? 
we VBD[play, act, ..] DT  (8139), ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] (397), ? 
important role IN[in] VBG[close, end, ..]  (110), ? 
role IN[in] VBG[close, end, ..] (854), ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB (41369), ?, 
we VBP DT (13138), ?, 
we VBD DT (8139), ? 
Patterns for ?we play an important role?: 
play role IN[in] DT (599), 
play role IN[in] VBG (397), ?, 
important role IN[in] VBG (110), ?, 
role IN[in] VBG (854), ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
353
3.2 Learning to Find Pattern and Translation 
In the training stage, we find and store syntax-
based phraseological tendencies and translation 
pairs. These patterns and translations are intended 
to be used in a real-time system to respond to user 
input speedily. 
First, we part of speech tag sentences in Ct. 
Using common phrase patterns (e.g., the 
possessive noun one?s in ?make up one?s mind?) 
seen in grammar books, we resort to parts-of-
speech (POS) for syntactic generalization. Then, 
we build up inverted files of the words in Ct for the 
next stage (i.e., pattern grammar generation). Apart 
from sentence and position information, a word?s 
lemma and POS are also recorded. 
Subsequently, we use the procedure in Figure 2 
to generate grammar patterns following any given 
sequence of words, either contiguous or skipped. 
 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation on 
the inverted file, InvList, of the current word wi and 
interInvList, a previous intersected results. 
After that, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?, wordPosi(wn)], sentence 
number) denoting the positions of query?s words in 
the sentence, we generate grammar pattern 
involving replacing words in the sentence with 
POS tags and words in wordPosi(wi) with lemmas, 
and extracting fixed-window 2  segments 
surrounding query from the transformed sentence. 
The result is a set of grammatical patterns (i.e., 
syntax-based phraseology) for the query. The 
procedure finally returns top N predominant 
                                                           
2
 Inspired by (Gamon and Leacock, 2010). 
syntactic patterns of the query. Such patterns 
characterizing the query?s word usages in the spirit 
of pattern grammar in (Hunston and Francis, 2000) 
and are collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through a 
number of steps, namely, leveraging IBM models 
for bidirectional word alignments, grow-diagonal-
final heuristics to extract phrasal equivalences 
(Koehn et al, 2003). 
3.3 Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, they are stored for run-time 
reference. TransAhead then predicts/suggests the 
following grammar and text of a translation prefix 
given the source text using the procedure in Figure 
3. 
 
 
Figure 3. Predicting pattern grammar and 
translations at run-time. 
 
We first slice the source text S into character-
level ngrams, represented by {si}. We also find the 
word-level ngrams of the translation prefix Tp. But 
this time we concentrate on the ngrams, may 
skipped, ending with the last word of Tp (i.e., 
pivoted on the last word) since these ngrams are 
most related to the subsequent grammar patterns. 
Step (3) and (4) retrieve translations and patterns 
learned from Section 3.2. Step (3) acquires the 
target-language active vocabulary that may be used 
to translate the source. To alleviate the word 
boundary issue in MT (Ma et al (2007)), the word 
boundary in our system is loosely decided. Initially, 
TransAhead non-deterministically segments the 
source text using character ngrams for translations 
and proceeds with collaborations with the user to 
obtain the segmentation for MT and to complete 
the translation. Note that Tp may reflect some 
translated segments, reducing the size of the active 
vocabulary, and that a user vocabulary of 
preference (due to users? domain knowledge or 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency 
(7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgramWithPivot(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
354
errors of the system) may be exploited for better 
system performance. In addition, Step (4) extracts 
patterns preceding with the history ngrams of {tj}. 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and t 
is one of the translation candidates under S and Tp. 
Subsequently, we incorporate the lemmatized 
translation candidates according to their ranks into 
suitable grammar constituents in GramOptions. 
For example, we would include ?close? in pattern 
?play role IN[in] VBG? as ?play role IN[in] 
VBG[close]?. 
At last, the algorithm returns the representative 
grammar patterns with confident translations 
expected to follow the ongoing translation and 
further translate the source. This algorithm will be 
triggered by word delimiter to provide an 
interactive CAT and CALL environment. Figure 1 
shows example responses of our working prototype. 
4 Preliminary Results 
In developing TransAhead, we used British 
National Corpus and Hong Kong Parallel Text as 
target-language reference corpus and parallel 
training corpus respectively, and deployed GENIA 
tagger for lemma and POS analyses. 
To evaluate TransAhead in CAT and CALL, we 
introduced it to a class of 34 (Chinese) college 
freshmen learning English as foreign language. We 
designed TransAhead to be accessible and intuitive, 
so the user training tutorial took only one minute. 
After the tutorial, the participants were asked to 
translate 15 Chinese texts from (Huang et al, 2011) 
(half with TransAhead assistance called experi-
mental group, and the other without any system 
help whatsoever called control group). The 
evaluation results show that the experimental 
group achieved much better translation quality than 
the control group with an average BLEU score 
(Papineni et al, 2002) of 35.49 vs. 26.46. 
Admittedly, the MT system Google Translate 
produced translations with a higher BLEU score of 
44.82. 
Google Translate obviously has much more 
parallel training data and bilingual translation 
knowledge. No previous work in CAT uses Google 
Translate for comparison. Although there is a 
difference in average translation quality between 
the experimental TransAhead group and the 
Google Translate, it is not hard for us to notice the 
source sentences were better translated by 
language learners with the help of TransAhead. 
Take the sentence  ????????????????
?? for example. A total of 90% of the participants 
in the experimental group produced more 
grammatical and fluent translations (see Figure 4) 
than that (?We conclude this transaction plays an 
important role?) by Google Translate. 
 
 
Figure 4. Example translations with 
TransAhead assistance. 
 
Post-experiment surveys indicate that (a) the 
participants found Google Translate lack human-
computer interaction while TransAhead is intuitive 
to collaborate with in translation/writing; (b) the 
participants found TransAhead grammar and 
translation predictions useful for their immediate 
task and for learning; (c) interactivity made the 
translation and language learning a fun process 
(like image tagging game of (von Ahn and Dabbish, 
2004)) and the participants found TransAhead very 
recommendable and would like to use it again in 
future translation tasks. 
5 Summary 
We have introduced a method for learning to offer 
grammar and text predictions expected to assist the 
user in translation and writing. We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising. 
As for the further work, we intend to evaluate and 
improve our system further in learner productivity 
in terms of output quality, typing speed, and the 
amount of using certain keys such as delete and 
backspace. 
Acknowledgement 
This study is conducted under the ?Project Digital 
Convergence Service Open Platform? of the 
Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs of 
the Republic of China. 
1. we play(ed) a critical role in closing this/the deal. 
2. we play(ed) a critical role in sealing this/the deal. 
3. we play(ed) an important role in ending this/the deal. 
4. we play(ed) an important role in closing this/the deal. 
355
References 
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches 
to computer-assisted translation. Computational 
Linguistics, 35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In 
Proceedings of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and 
J. S. Chang. 2011. GRASP: grammar- and syntax-
based pattern-finder in CALL. In Proceedings of 
ACL Workshop. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer 
aided translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping 
word alignment via word packing. In Proceedings of 
ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 
2004. Adaptive language and translation models for 
interactive machine translation. In Proceedings of 
EMNLP. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-
Varea, and F. Casacuberta. 2011. An interactive 
machine translation system with online learning. In 
Proceedings of ACL System Demonstrations, pages 
68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: 
a method for automatic evaluation of machine 
translation. In Proceedings of ACL, pages 311-318. 
L. von Ahn and L. Dabbish. 2004. Labeling images with 
a computer game. In Proceedings of CHI. 
356
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Cross-Lingual Information to the Rescue in Keyword Extraction 
 
1Chung-Chi Huang 2Maxine Eskenazi 3Jaime Carbonell 4Lun-Wei Ku 5Ping-Che Yang 
1,2,3Language Technologies Institute, CMU, United States 
4Institute of Information Science, Academia Sinica, Taiwan 
5Institute for Information Industry, Taipei, Taiwan 
{1u901571,4lunwei.jennifer.ku}@gmail.com 
{2max+,3jgc}@cs.cmu.edu 5maciaclark@iii.org.tw 
 
  
  
Abstract 
We introduce a method that extracts keywords 
in a language with the help of the other. In our 
approach, we bridge and fuse conventionally 
irrelevant word statistics in languages. The 
method involves estimating preferences for 
keywords w.r.t. domain topics and generating 
cross-lingual bridges for word statistics 
integration. At run-time, we transform parallel 
articles into word graphs, build cross-lingual 
edges, and exploit PageRank with word 
keyness information for keyword extraction. 
We present the system, BiKEA, that applies 
the method to keyword analysis. Experiments 
show that keyword extraction benefits from 
PageRank, globally learned keyword 
preferences, and cross-lingual word statistics 
interaction which respects language diversity. 
1 Introduction 
Recently, an increasing number of Web services 
target extracting keywords in articles for content 
understanding, event tracking, or opinion mining. 
Existing keyword extraction algorithm (KEA) 
typically looks at articles monolingually and 
calculate word significance in certain language. 
However, the calculation in another language 
may tell the story differently since languages 
differ in grammar, phrase structure, and word 
usage, thus word statistics on keyword analysis. 
Consider the English article in Figure 1. Based 
on the English content alone, monolingual KEA 
may not derive the best keyword set. A better set 
might be obtained by referring to the article and 
its counterpart in another language (e.g., 
Chinese). Different word statistics in articles of 
different languages may help, due to language 
divergence such as phrasal structure (i.e., word 
order) and word usage and repetition (resulting 
from word translation or word sense) and so on. 
For example, bilingual phrases ?social 
reintegration? and ?????? in Figure 1 have 
inverse word orders (?social? translates into ??
? ? and ?reintegration? into ? ? ? ?), both 
?prosthesis? and ?artificial limbs? translate into 
????, and ?physical? can be associated with ??
? ? and ??? ? in ?physical therapist? and 
?physical rehabilitation? respectively. Intuitively, 
using cross-lingual statistics (implicitly 
leveraging language divergence) can help look at 
articles from different perspectives and extract 
keywords more accurately. 
We present a system, BiKEA, that learns to 
identify keywords in a language with the help of 
the other. The cross-language information is 
expected to reinforce language similarities and 
value language dissimilarities, and better 
understand articles in terms of keywords. An 
example keyword analysis of an English article 
is shown in Figure 1. BiKEA has aligned the 
parallel articles at word level and determined the 
scores of topical keyword preferences for words. 
BiKEA learns these topic-related scores during 
training by analyzing a collection of articles. We 
will describe the BiKEA training process in more 
detail in Section 3. 
At run-time, BiKEA transforms an article in a 
language (e.g., English) into PageRank word 
graph where vertices are words in the article and 
edges between vertices indicate the words? co-
occurrences. To hear another side of the story, 
BiKEA also constructs graph from its counterpart 
in another language (e.g., Chinese). These two 
independent graphs are then bridged over nodes 
1
  
 
 
 
 
 
 
 
 
 
Figure 1. An example BiKEA keyword analysis for an article.
that are bilingually equivalent or aligned. The 
bridging is to take language divergence into 
account and to allow for language-wise 
interaction over word statistics. BiKEA, then in 
bilingual context, iterates with learned word 
keyness scores to find keywords. In our 
prototype, BiKEA returns keyword candidates of 
the article for keyword evaluation (see Figure 1); 
alternatively, the keywords returned by BiKEA 
can be used as candidates for social tagging the 
article or used as input to an article 
recommendation system. 
2 Related Work 
Keyword extraction has been an area of active 
research and applied to NLP tasks such as 
document categorization (Manning and Schutze, 
2000), indexing (Li et al., 2004), and text mining 
on social networking services ((Li et al., 2010); 
(Zhao et al., 2011); (Wu et al., 2010)). 
The body of KEA focuses on learning word 
statistics in document collection. Approaches 
such as tfidf and entropy, using local document 
and/or across-document information, pose strong 
baselines. On the other hand, Mihalcea and 
Tarau (2004) apply PageRank, connecting words 
locally, to extract essential words. In our work, 
we leverage globally learned keyword 
preferences in PageRank to identify keywords. 
Recent work has been done on incorporating 
semantics into PageRank. For example, Liu et al. 
(2010) construct PageRank synonym graph to 
accommodate words with similar meaning. And 
Huang and Ku (2013) weigh PageRank edges 
based on nodes? degrees of reference. In contrast, 
we bridge PageRank graphs of parallel articles to 
facilitate statistics re-distribution or interaction 
between the involved languages. 
In studies more closely related to our work, 
Liu et al. (2010) and Zhao et al. (2011) present 
PageRank algorithms leveraging article topic 
information for keyword identification. The main 
differences from our current work are that the 
article topics we exploit are specified by humans 
not by automated systems, and that our 
PageRank graphs are built and connected 
bilingually. 
In contrast to the previous research in keyword 
extraction, we present a system that 
automatically learns topical keyword preferences 
and constructs and inter-connects PageRank 
graphs in bilingual context, expected to yield 
better and more accurate keyword lists for 
articles. To the best of our knowledge, we are the 
first to exploit cross-lingual information and take 
advantage of language divergence in keyword 
extraction. 
3 The BiKEA System 
Submitting natural language articles to keyword 
extraction systems may not work very well. 
Keyword extractors typically look at articles 
from monolingual points of view. Unfortunately, 
word statistics derived based on a language may 
The English Article: 
I've been in Afghanistan for 21 years. I work for the Red Cross and I'm a physical therapist. My job is to 
make arms and legs -- well it's not completely true. We do more than that. We provide the patients, the 
Afghan disabled, first with the physical rehabilitation then with the social reintegration. It's a very logical 
plan, but it was not always like this. For many years, we were just providing them with artificial limbs. It 
took quite many years for the program to become what it is now. ? 
 
Its Chinese Counterpart: ??????? 21 ?? ????????? ?????????? ??????????? -- ?????????? ?????????? ???????? ???????? ???????, ??????? ???????????? ?????????? ??????????? ????? ???????? ?????????????? 
 
Word Alignment Information: 
physical (??), therapist (???), social (??), reintegration (??), physical (??), rehabilitation  (??), prosthesis (??), ? 
 
Scores of Topical Keyword Preferences for Words: 
(English)    prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; ? 
(Chinese)   ??: 0.41; ?????: 0.15; ??:0.10; ???: 0.08, ? 
 
English Keywords from Bilingual Perspectives: 
prosthesis, artificial, leg, rehabilitation, orthopedic, ? 
2
be biased due to the language?s grammar, phrase 
structure, word usage and repetition and so on. 
To identify keyword lists from natural language 
articles, a promising approach is to automatically 
bridge the original monolingual framework with 
bilingual parallel information expected to respect 
language similarities and diversities at the same 
time.  
3.1 Problem Statement 
We focus on the first step of the article 
recommendation process: identifying a set of 
words likely to be essential to a given article. 
These keyword candidates are then returned as 
the output of the system. The returned keyword 
list can be examined by human users directly, or 
passed on to article recommendation systems for 
article retrieval (in terms of the extracted 
keywords). Thus, it is crucial that keywords be 
present in the candidate list and that the list not 
be too large to overwhelm users or the 
subsequent (typically computationally expensive) 
article recommendation systems. Therefore, our 
goal is to return reasonable-sized set of keyword 
candidates that, at the same time, must contain 
essential terms in the article. We now formally 
state the problem that we are addressing. 
Problem Statement: We are given a bilingual 
parallel article collection of various topics from 
social media (e.g., TED), an article ARTe in 
language e, and its counterpart ARTc in language 
c. Our goal is to determine a set of words that are 
likely to contain important words of ARTe. For 
this, we bridge language-specific statistics of 
ARTe and ARTc via bilingual information (e.g., 
word alignments) and consider word keyness 
w.r.t. ARTe?s topic such that cross-lingual 
diversities are valued in extracting keywords in e. 
In the rest of this section, we describe our 
solution to this problem. First, we define 
strategies for estimating keyword preferences for 
words under different article topics (Section 3.2). 
These strategies rely on a set of article-topic 
pairs collected from the Web (Section 4.1), and 
are monolingual, language-dependent 
estimations. Finally, we show how BiKEA 
generates keyword lists for articles leveraging 
PageRank algorithm with word keyness and 
cross-lingual information (Section 3.3). 
3.2 Topical Keyword Preferences 
We attempt to estimate keyword preferences 
with respect to a wide range of article topics. 
Basically, the estimation is to calculate word 
significance in a domain topic. Our learning 
process is shown in Figure 2. 
 
 
 
 
 
 
Figure 2. Outline of the process used 
to train BiKEA. 
In the first two stages of the learning process, we 
generate two sets of article and word information. 
The input to these stages is a set of articles and 
their domain topics. The output is a set of pairs 
of article ID and word in the article, e.g., 
(ARTe=1, we=?prosthesis?) in language e or 
(ARTc=1, wc=????) in language c, and a set of 
pairs of article topic and word in the article, e.g., 
(tpe=?disability?, we=?prosthesis?) in e and 
(tpe=?disability?, wc=????) in c. Note that the 
topic information is shared between the involved 
languages, and that we confine the calculation of 
such word statistics in their specific language to 
respect language diversities and the language-
specific word statistics will later interact in 
PageRank at run-time (See Section 3.3). 
The third stage estimates keyword preferences 
for words across articles and domain topics using 
aforementioned (ART,w) and (tp,w) sets. In our 
paper, two popular estimation strategies in 
Information Retrieval are explored. They are as 
follows. 
tfidf. tfidf(w)=freq(ART,w)/appr(ART?,w) where 
term frequency in an article is divided by its 
appearance in the article collection to distinguish 
important words from common words. 
ent. entropy(w)= -?
tp?
Pr(tp?|w)?log(Pr(tp?|w)) 
where  a word?s uncertainty in topics is used to 
estimate its associations with domain topics. 
These strategies take global information (i.e., 
article collection) into account, and will be used 
as keyword preference models, bilingually 
intertwined, in PageRank at run-time which 
locally connects words (i.e., within articles). 
3.3 Run-Time Keyword Extraction 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
(1) Generate article-word pairs in training data 
(2) Generate topic-word pairs in training data 
(3) Estimate keyword preferences for words w.r.t.  
      article topic based on various strategies 
(4) Output word-and-keyword-preference-score  
      pairs for various strategies 
3
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Extracting keywords at run-time. 
 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible.  
In Steps (1) and (2) we construct PageRank 
word graphs for the article ARTe in language e 
and its counterpart ARTc in language c. They are 
built individually to respect language properties 
(such as subject-verb-object or subject-object-
verb structure). Figure 4 shows the algorithm. In 
this algorithm, EW stores normalized edge 
weights for word wi and wj (Step (2)). And EW 
is a v by v matrix where v is the vocabulary size 
of ARTe and ARTc. Note that the graph is directed 
(from words to words that follow) and edge 
weights are words? co-occurrences within 
window size WS. Additionally we incorporate 
edge weight multiplier m>1 to propagate more 
PageRank scores to content words, with the 
intuition that content words are more likely to be 
keywords (Step (2)). 
 
 
 
 
 
 
 
Figure 4. Constructing PageRank word graph. 
Step (3) in Figure 3 linearly combines word 
graphs EWe and EWc using ?. We use ? to 
balance language properties or statistics, and 
BiKEA backs off to monolingual KEA if ? is one. 
In Step (4) of Figure 3 for each word 
alignment (wic, wje), we construct a link between 
the word nodes with the weight BiWeight. The 
inter-language link is to reinforce language 
similarities and respect language divergence 
while the weight aims to elevate the cross-
language statistics interaction. Word alignments 
are derived using IBM models 1-5 (Och and Ney, 
2003). The inter-language link is directed from 
wi
c
 to wj
e
, basically from language c to e based on 
the directional word-aligning entry (wic, wje). The 
bridging is expected to help keyword extraction 
in language e with the statistics in language c. 
Although alternative approach can be used for 
bridging, our approach is intuitive, and most 
importantly in compliance with the directional 
spirit of PageRank. 
Step (6) sets KP of keyword preference model 
using topical preference scores learned from 
Section 3.2, while Step (7) initializes KN of 
PageRank scores or, in our case, word keyness 
scores. Then we distribute keyness scores until 
the number of iteration or the average score 
differences of two consecutive iterations reach 
their respective limits. In each iteration, a word?s 
keyness score is the linear combination of its 
keyword preference score and the sum of the 
propagation of its inbound words? previous 
PageRank scores. For the word wje in ARTe, any 
edge (wie,wje) in ARTe, and any edge (wkc,wje) in 
WA, its new PageRank score is computed as 
below. 
procedure PredictKW(ARTe,ARTc,KeyPrefs,WA,?,N) 
//Construct language-specific word graph for PageRank 
(1)  EWe=constructPRwordGraph(ARTe) 
(2)  EWc=constructPRwordGraph(ARTc) 
//Construct inter-language bridges 
(3)  EW=?? EWe+(1-?) ? EWc 
       for each word alignment (wic, wje) in WA 
         if IsContWord(wic) and IsContWord(wje) 
(4a)      EW[i,j]+=1? BiWeightcont 
         else 
(4b)      EW[i,j]+=1? BiWeightnoncont 
(5)  normalize each row of EW to sum to 1 
//Iterate for PageRank 
(6)  set KP1 ?v to 
             [KeyPrefs(w1), KeyPrefs(w2), ?,KeyPrefs(wv)] 
(7)  initialize KN1 ?v to [1/v,1/ v, ?,1/v] 
       repeat 
(8a)  KN?=?? KN? EW+(1-?) ? KP 
(8b)  normalize KN? to sum to 1 
(8c)  update KN with KN? after the check of KN and KN? 
       until maxIter or avgDifference(KN,KN?) ? smallDiff 
(9)  rankedKeywords=Sort words in decreasing order of KN 
       return the N rankedKeywords in e with highest 
scores 
procedure constructPRwordGraph(ART) 
(1) EWv ?v=0v ?v 
      for each sentence st in ART 
         for each word wi in st 
            for each word wj in st where i<j and j-i ? WS 
         if not IsContWord(wi) and IsContWord(wj) 
(2a)            EW[i,j]+=1? m 
               elif not IsContWord(wi) and not IsContWord(wj) 
(2b)            EW[i,j]+=1 ? (1/m) 
               elif IsContWord(wi) and not IsContWord(wj) 
(2c)            EW[i,j]+=1? (1/m) 
               elif IsContWord(wi) and IsContWord(wj) 
(2d)            EW[i,j]+=1 ? m 
       return EW 
4
???[1, ?] =? ?
??
? ? ????[1, ?] ? ???[?, ?] +???(1 ? ?) ????[1, ?] ? ??[?, ?]??? ??
?
+ (1 ??) ? ??[1, ?] 
 
Once the iterative process stops, we rank 
words according to their final keyness scores and 
return top N ranked words in language e as 
keyword candidates of the given article ARTe. An 
example keyword analysis for an English article 
on our working prototype is shown in Figure 1. 
Note that language similarities and dissimilarities 
lead to different word statistics in articles of 
difference languages, and combining such word 
statistics helps to generate more promising 
keyword lists. 
4 Experiments 
BiKEA was designed to identify words of 
importance in an article that are likely to cover 
the keywords of the article. As such, BiKEA will 
be trained and evaluated over articles. 
Furthermore, since the goal of BiKEA is to 
determine a good (representative) set of 
keywords with the help of cross-lingual 
information, we evaluate BiKEA on bilingual 
parallel articles. In this section, we first present 
the data sets for training BiKEA (Section 4.1). 
Then, Section 4.2 reports the experimental 
results under different system settings. 
4.1 Data Sets 
We collected approximately 1,500 English 
transcripts (3.8M word tokens and 63K word 
types) along with their Chinese counterparts 
(3.4M and 73K) from TED (www.ted.com) for 
our experiments. The GENIA tagger (Tsuruoka 
and Tsujii, 2005) was used to lemmatize and 
part-of-speech tag the English transcripts while 
the CKIP segmenter (Ma and Chen, 2003) 
segment the Chinese. 
30 parallel articles were randomly chosen and 
manually annotated for keywords on the English 
side to examine the effectiveness of BiKEA in 
English keyword extraction with the help of 
Chinese. 
4.2 Experimental Results 
Table 1 summarizes the performance of the 
baseline tfidf and our best systems on the test set. 
The evaluation metrics are nDCG (Jarvelin and 
Kekalainen, 2002), precision, and mean 
reciprocal rank. 
(a) @N=5 nDCG P MRR 
tfidf .509 .213 .469 
PR+tfidf .676 .400 .621 
BiKEA+tfidf .703 .406 .655 
 
(b) @N=7 nDCG P MRR 
tfidf .517 .180 .475 
PR+tfidf .688 .323 .626 
BiKEA+tfidf .720 .338 .660 
 
(c) @N=10 nDCG P MRR 
tfidf .527 .133 .479 
PR+tfidf .686 .273 .626 
BiKEA+tfidf .717 .304 .663 
Table 1. System performance at 
(a) N=5 (b) N=7 (c) N=10. 
As we can see, monolingual PageRank (i.e., 
PR) and bilingual PageRank (BiKEA), using 
global information tfidf, outperform tfidf. They 
relatively boost nDCG by 32% and P by 87%. 
The MRR scores also indicate their superiority: 
their top-two candidates are often keywords vs. 
the 2nd place candidates from tfidf. 
Encouragingly, BiKEA+tfidf achieves better 
performance than the strong monolingual 
PR+tfidf across N?s. Specifically, it further 
improves nDCG relatively by 4.6% and MRR 
relatively by 5.4%. 
Overall, the topical keyword preferences, and 
the inter-language bridging and the bilingual 
score propagation in PageRank are simple yet 
effective. And respecting language statistics and 
properties helps keyword extraction. 
5 Summary 
We have introduced a method for extracting 
keywords in bilingual context. The method 
involves estimating keyword preferences, word-
aligning parallel articles, and bridging language-
specific word statistics using PageRank. 
Evaluation has shown that the method can 
identify more keywords and rank them higher in 
the candidate list than monolingual KEAs. As for 
future work, we would like to explore the 
possibility of incorporating the articles? reader 
feedback into keyword extraction. We would 
also like to examine the proposed methodology 
in a multi-lingual setting.  
5
Acknowledgement 
This study is conducted under the ?Online and 
Offline integrated Smart Commerce Platform 
(1/4)? of the Institute for Information Industry 
which is subsidized by the Ministry of Economy 
Affairs of the Republic of China. 
References  
Scott A. Golder and Bernardo A. Huberman. 
2006. Usage patterns of collaborative tagging 
systems. Information Science, 32(2): 198-208. 
Harry Halpin, Valentin Robu, and Hana 
Shepherd. 2007. The complex dynamics of 
collaborative tagging. In Proceedings of the 
WWW, pages 211-220. 
Chung-chi Huang and Lun-wei Ku. 2013. 
Interest analysis using semantic PageRank and 
social interaction content. In Proceedings of 
the ICDM Workshop on Sentiment Elicitation 
from Natural Text for Information Retrieval 
and Extraction, pages 929-936. 
Kalervo Jarvelin and Jaana Kekalainen. 2002. 
Cumulated gain-based evaluation of IR 
technologies. ACM Transactions on 
Information Systems, 20(4): 422-446. 
Philipp Koehn, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based 
translation. In Proceedings of the North 
American Chapter of the Association for 
Computational Linguistics, pages 48-54. 
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin 
Chen. 2004. Incorporating document 
keyphrases in search results. In Proceedings of 
the Americas Conference on Information 
Systems. 
Zhenhui Li, Ging Zhou, Yun-Fang Juan, and 
Jiawei Han. 2010. Keyword extraction for 
social snippets. In Proceedings of the WWW, 
pages 1143-1144. 
Marina Litvak and Mark Last. 2008. Graph-
based keyword extraction for single-document 
summarization. In Proceedings of the ACL 
Workshop on Multi-Source Multilingual 
Information Extraction and Summarization, 
pages 17-24. 
Zhengyang Liu, Jianyi Liu, Wenbin Yao, Cong 
Wang. 2010. Keyword extraction using 
PageRank on synonym networks. In 
Proceedings of the ICEEE, pages 1-4. 
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and 
Maosong Sun. 2010. Automatic keyphrase 
extraction via topic decomposition. In 
Proceedings of the EMNLP, pages 366-376. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. 
Introduction to CKIP Chinese word 
segmentation system for the first international 
Chinese word segmentation bakeoff. In 
Proceedings of the ACL Workshop on Chinese 
Language Processing. 
Chris D. Manning and Hinrich Schutze. 2000. 
Foundations of statistical natural language 
processing. MIT Press. 
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing orders into texts. In Proceedings of 
the EMNLP, pages 404-411. 
Franz Josef Och and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational Linguistics, 
29(1): 19-51. 
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. 
Bidirectional inference with the easiest-first 
strategy for tagging sequence data. In 
Proceedings of the EMNLP, pages 467-474. 
Peter D. Turney. 2000. Learning algorithms for 
keyphrase extraction. Information Retrieval, 
2(4): 303-336. 
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. 
Automatic generation of personalized 
annotation tags for Twitter users. In 
Proceedings of the NAACL, pages 689-692. 
Wayne Xin Zhao, Jing Jiang, Jing He, Yang 
Song, Palakorn Achananuparp, Ee-Peng Lim, 
and Xiaoming Li. 2011. Topical keyword 
extraction from Twitter. In Proceedings of the 
ACL, pages 379-388. 
 
6
Reducing the False Alarm Rate of Chinese Character Error Detection 
and Correction 
 
Shih-Hung Wu, Yong-Zhi Chen  
Chaoyang University of Technol-
ogy, Taichung Country 
shwu@cyut.edu.tw 
 
Ping-che Yang, Tsun Ku 
Institute for information industry,
Taipei City 
maciaclark@iii.org.tw, 
cujing@iii.org.tw 
Chao-Lin Liu  
National Chengchi Universi-
ty, Taipei City 
chaolin@nccu.edu.tw 
 
 
Abstract 
The main drawback of previous Chinese cha-
racter error detection systems is the high false 
alarm rate. To solve this problem, we propose 
a system that combines a statistic method and 
template matching to detect Chinese character 
errors. Error types include pronunciation-
related errors and form-related errors. Possible 
errors of a character can be collected to form a 
confusion set. Our system automatically gene-
rates templates with the help of a dictionary 
and confusion sets. The templates can be used 
to detect and correct errors in essays. In this 
paper, we compare three methods proposed in 
previous works. The experiment results show 
that our system can reduce the false alarm sig-
nificantly and give the best performance on f-
score. 
1 Introduction 
Since many Chinese characters have similar forms 
and similar or identical pronunciation, improperly 
used characters in Chinese essays are hard to be de-
tectted. Previous works collected these hard-to-
distinguish characters and used them to form confu-
sion sets. Confusion sets are critical for detecting and 
correcting improperly used Chinese characters. A 
confusion set of a Chinese character consists of cha-
racters with similar pronunciation, similar forms, and 
similar meaning. Most Chinese character detection 
systems were built based on confusion sets and a lan-
guage model. Ren et.al proposed a rule-based method 
that was also integrated with a language model to 
detect character errors in Chinese (Ren, Shi, & Zhou, 
1994). Chang used confusion sets to represent all 
possible errors to reduce the amount of computation. 
A language model was also used to make decisions. 
The confusion sets were edited manually. Zhang et al 
proposed a way to automatically generate confusion 
sets based on the Wubi input method (Zhang, Zhou, 
Huang, & Sun, 2000). The basic assumption was that 
characters with similar input sequences must have 
similar forms. Therefore, by replacing one code in the 
input sequence of a certain character, the system 
could generate characters with similar forms. In the 
following work, Zhang et al designed a Chinese cha-
racter detection system based on the confusion sets 
(Zhang, Zhou, Huang, & Lu, 2000). Another input 
method was also used to generate confusion sets. Lin 
et al used the Cangjie input method to generate con-
fusion sets (Lin, Huang, & Yu, 2002). The basic as-
sumption was the same. By replacing one code in the 
input sequence of a certain character, the system 
could generate characters with similar forms. Since 
the two input methods have totally different represen-
tations of the same character, the confusion set of any 
given character will be completely different. 
In recent years, new systems have been incorporat-
ing more NLP technology for Chinese character error 
detection. Huang et al proposed that a word segmen-
tation tool can be used to detect character error in 
Chinese (Huang, Wu, & Chang, 2007). They used a 
new word detection function in the CKIP word seg-
mentation toolkit to detect error candidates (CKIP, 
1999). With the help of a dictionary and confusion set, 
the system can decide whether a new word is a cha-
racter error or not. Hung et al proposed a system that 
can detect character errors in student essays and then 
suggest corrections (Hung & Wu, 2008). The system 
was based on common error templates which were 
manually edited. The precision of this system is the 
highest, but the recall remains average. The main 
drawback of this approach is the cost of editing com-
mon error templates. Chen et al proposed an automat-
ic method for common error template generation 
(Chen, Wu, Lu, & Ku, 2009). The common errors 
were collected from a large corpus automatically. The 
template is a short phrase with one error in it. The 
assumption is the frequency of a correct phrase must 
be higher than the frequency of the corresponding 
template, with one error character. Therefore, a statis-
tical test can be used to decide weather there is a 
common error or not. 
The main drawback of previous systems is the high 
false alarm rate. The drawback is found by comparing 
the systems with sentences without errors. As we will 
show in our experiments, the systems in previous 
works tent to report more errors in an essay than the 
real ones, thus, cause false alarms.  
In this paper, we will further improve upon the 
Chinese character checker using a new error model 
and a simplified common error template generation 
method. The idea of error model is adopted from the 
noise channel model, which is used in many natural 
language processing applications, but never on Chi-
nese character error detection. With the help of error 
model, we can treat the error detection problem as a 
kind of translation, where a sentence with errors can 
be translated into a sentence without errors. The sim-
plified template generation is based on given confu-
sion sets and a lexicon.  
The paper is organized as follows. We introduce 
briefly the methods in previous works in section 2. 
Section 3 reports the necessary language resources 
used to build such systems. Our approach is described 
in section 4. In section 5, we report the experiment 
settings and results of our system, as well as give the 
comparison of our system to the three previous sys-
tems. Finally, we give the conclusions in the final 
section. 
2 Previous works 
In this paper, we compare our method to previous 
works. Since they are all not open source systems, we 
will reconstruct the systems proposed by Chang 
(1995), Lin, Huang, & Yu (2002), and Huang, Wu, & 
Chang (2007). We cannot compare our system to the 
system proposed by Zhang, Zhou, Huang, & Sun 
(2000), since the rule-based system is not available. 
We describe the systems below. 
Chang?s system (1995) consists of five steps. First, 
the system segments the input article into sentences. 
Second, each character in the sentence is replaced by 
the characters in the corresponding confusion set. 
Third, the probability of a sentence is calculated ac-
cording to a bi-gram language model. Fourth, the 
probability of the sentences before and after replace-
ment is compared. If the replacement causes a higher 
probability, then the replacement is treated as a cor-
rection of a character error. Finally, the results are 
outputted. There are 2480 confusion sets used in this 
system. Each confusion set consists of one to eight 
characters with similar forms or similar pronunciation. 
The system uses OCR results to collect characters 
with similar forms. The average size of the confusion 
sets was less than two. The language model was built 
from a 4.7 million character news corpus. 
The system proposed by Lin, Huang, & Yu (2002) 
has two limitations. First, there is only one spelling 
error in one sentence. Second, the error was caused by 
the Cangjie input method. The system also has five 
steps. First, sentences are inputted. Second, a search is 
made of the characters in a sentence that have similar 
input sequences. Third, a language model is used to 
determine whether the replacement improves the 
probability of the sentence or not. Fourth, the three 
steps for all input sentences are repeated. Finally, the 
results are outputted. The confusion sets of this sys-
tem were constructed from the Cangjie input method. 
Similarity of characters in a confusion set is ranked 
according to the similarity of input sequences. The 
language model was built from a 59 million byte news 
corpus. 
The system by Huang, Wu, & Chang (2007) con-
sists of six steps. First, the input sentences are seg-
mented into words according to the CKIP word seg-
mentation toolkit. Second, each of the characters in 
the new words is replaced by the characters in the 
confusion sets. Third, a word after replacement 
checked in the dictionary. Fourth, a language model is 
used to assess the replacement. Fifth, the probability 
of the sentence before and after replacement is com-
pared. Finally, the result with the highest probability 
is outputted. The confusion set in this system, which 
also consists of characters with similar forms or simi-
lar pronunciation, was edited manually.  
Since the test data in the papers were all different 
test sets, it is improper to compare their results direct-
ly, therefore; there was no comparison available in the 
literature on this problem. To compare these systems 
with our method, we used a fixed dictionary, inde-
pendently constructed confusion sets, and a fixed lan-
guage model to reconstruct the systems. We per-
formed tests on the same test set. 
3 Data in Experiments  
3.1 Confusion sets 
Confusion sets are a collection of sets for each indi-
vidual Chinese character. A confusion set of a certain 
character consists of phonologically or logographical-
ly similar characters. For example, the confusion set 
of ??? might consist of the following characters with 
the same pronunciation????????? or with 
similar forms????????????????
??????. In this study, we use the confusion sets 
used by Liu, Tien, Lai, Chuang, & Wu (2009). The 
similar Cangjie (SC1 and SC2) sets of similar forms, 
and both the same-sound-same-tone (SSST) and 
same-sound-different-tone (SSDT) sets for similar 
pronunciation were used in the experiments. There 
were 5401 confusion sets for each of the 5401 high 
frequency characters. The size of each confusion set 
was one to twenty characters. The characters in each 
confusion set were ranked according to Google search 
results. 
3.2 Language model 
Since there is no large corpus of student essays, we 
used a news corpus to train the language model. The 
size of the news corpus is 1.5 GB, which consists of 
1,278,787 news articles published between 1998 and 
2001. The n-gram language model was adopted to 
calculate the probability of a sentence p(S). The gen-
eral n-gram formula is: 
)|()( 1 1
?
+?= n Nnn wwpSp    (1) 
Where N was set to two for bigram and N was set to 
one for unigram. The Maximum Likelihood Estima-
tion (MLE) was used to train the n-gram model. We 
adopted the interpolated Kneser-Ney smoothing me-
thod as suggested by Chen & Goodman (1996). As 
following: 
 
)()1()|(
)|(
1
1int
wpwwp
wwp
unigramibigram
ierpolate
?? ?+= ?
?
  (2)
 
To determine whether a replacement is good or not, 
our system use the modified perplexity:  
 
NSpPerplexity /))(log(2?=   (3) 
Where N is the length of a sentence and p(S) is the bi-
gram probability of a sentence after smoothing. 
3.3 Dictionary and test set 
We used a free online dictionary provided by Tai-
wan?s Ministry of Education, MOE (2007). We fil-
tered out one character words and used the remaining 
139,976 words which were more than one character as 
our lexicon in the following experiments. 
The corpus is 5,889 student essays collected from a 
primary high school. The students were 13 to 15 years 
old. The essays were checked by teachers manually, 
and all the errors were identified and corrected. Since 
our algorithm needed a training set, we divided the 
essays into two sets to test our method. The statistics 
is given in Table 1. There are less than two errors in 
an essay on average. We find that most (about 97%) 
of characters in the essays were among the 5,401 most 
common characters, and most errors were characters 
of similar forms or pronunciation. Therefore, the 
5,401 confusion sets constructed according to form 
and pronunciation were suitable for error detection. 
Table 2 shows the error types of errors in students? 
essays.  More than 70% errors are characters with 
similar pronunciation, 40% errors are characters with 
similar form, and there are 20% errors are characters 
with both similar pronunciation and similar form. 
Only 10% errors are in other types. Therefore, in this 
study, our system aimed to identify and correct the 
errors of the two common types. 
 
Table 1. Training set and test set statistics 
 # of Essays 
Average 
length 
of essay 
Average 
# of 
errors 
% of 
common 
characters
Training 
set 5085 403.18 1.76 96.69% 
Test set 804 387.08 1.2 97.11% 
 
Table 2. Error type analysis 
 Similar form Similar pronunciation Both Other
Training set 41.54% 72.60% 24.24% 10.10%
Test set 40.36% 76.98% 27.66% 10.30%
4 System Architecture 
4.1 System flowchart 
Figure 1 shows the flowchart of our system. First, the 
input essays are segmented into words. Second, the 
words are sent to two different error detection mod-
ules. The first one is the template module, which can 
detect character errors based on the stored templates 
as in the system proposed by Chen, Wu, Lu, & Ku, 
(2009). The second module is the new language mod-
el module, which treats error detection as a kind of 
translation. Third, the results of the two modules can 
be merged to get a better system result. The details 
will be described in the following subsections. 
 
Figure 1. System flowchart 
 
4.2 Word segmentation 
The first step in our system uses word segmentation to 
find possible errors. In this study, we do not use the 
CKIP word segmentation tool (CKIP, 1999) as Huang, 
Wu, & Chang (2007) did, since it has a merge 
algorithm that might merge error charactersto form 
new words (Ma & Chen, 2003).  We use a backward 
longest first approach to build our system. The lex-
icon is taken from an online dictionary (MOE, 2007).  
We consider an input sentence with an error, ????
????????????, as an example. The 
sentence will be segmented into ??|??|??|?
??|?|?|?|?|???. The sequence of single 
characters will be our focus. In this case, it is ????
??. These kinds of sequences will be the output of 
the first step and will be sent to the following two 
modules. The error character can be identified and 
corrected by a ????-???? template. 
4.3 Template Module 
The template module in this study is a simplified ver-
sion of a module from a previous work (Chen, Wu, 
Lu, & Ku, 2009), which collects templates from a 
corpus. The simplified approach replaces one 
character of each word in a dictionary with one 
character in the corresponding confusion set. For 
example, a correct word ???? might be written with 
an error character ???? since ??(bian4)? is in the 
confusion set of ??(ban4)??. This method generates 
all possible error words with the help of confusion 
sets. Once the error template ???? is matched in an 
essay, our system can conclude that the character is an 
error and make a suggestion on correction ???? 
based on the ????-???? template. 
4.4 Translate module 
To improve the n-gram language model method, we 
use a statistical machine translation formula (Brown, 
1993) as a new way to detect character error. We treat 
the sentences with/without errors as a kind of transla-
tion. Given a sentence S that might have character 
errors in the sentence in the source language, the out-
put sentence C
~
 is the sentence in the target language 
with the highest probability of different replacements 
C. The replacement of each character is treated as a 
translation without alignment. 
)|(maxarg
~
SCpC
c
=   (4) 
From the Bayesian rule and when the fixed value of 
p(w) is ignored, this equation can be rewritten as (5): 
)()|(maxarg
)(
)()|(
maxarg
~
CpCSp
Sp
CpCSp
C
c
c
?
=
  (5) 
The formula is known as noisy channel model. We 
call p(S|C) an ?error model?, that is,  the probability 
which a character can be incorrect. It can be defined 
as the product of the error probability of each charac-
ter in the sentence. 
?
=
=
n
i
jiij cspCWp
1
 )|()|(    (6) 
where n is the length of the sentence S, and si ith cha-
racter of input sentence S. Cj is the jth replacement 
and cij.is the ith character at the jth replacement. The 
error model was built from the training set of student 
essays. Where p(C) is the n-gram language model as 
was described in section 3.2. Note that the number of 
replacements is not fixed, since the number of re-
placements depends on the size of all possible errors 
in the training set. 
For example, consider a segmented sentence with 
an error: ??|??|?|?|?|???, we will use the 
error model to evaluate the replacement of each cha-
racter in the subsequence: ?????. Here p(?|?) 
and p(?|?) are 0.0456902 and 0.025729 respective-
ly, which are estimated according to the training cor-
pus. And in training corpus, no one write the character 
?, therefore, there is no any replacement. Therefore, 
the probability of our error model and the n-gram lan-
guage model can be shown in the following table. Our 
system then multiplies the two probabilities and gets 
the perplexity of each replacement. The replacement 
????? gets the lowest perplexity, therefore, it is 
the output of our system and is both a correct error 
detection and correction. 
 
Table 3. An example of calculating perplexity 
according the new error model 
  Error Model LM multiply Perplexity
??? 0.025728988 1.88E-05 4.83E-07 127.442812
??? 0.001175563 1.05E-04 1.24E-07 200.716961
??? 1 2.09E-09 2.09E-09 782.669809
??? 0.045690212 1.17E-08 5.34E-10 1232.6714
 
4.5 Merge corrections 
Since the two modules detect errors using an inde-
pendent information source, we can combine the deci-
sions of the two modules to get a higher precision or a 
higher recall on the detection and correction of errors. 
We designed two working modes, the Precision Mode 
(PM) and the Detection Mode (DM). The output of 
PM is the intersection of the output of the template 
module and translation module, while the output of 
DM is the union of the two modules. 
5 Experiment Settings and Results  
Since there is no open source system in previous 
works and the data in use is not available, we repro-
duced the systems with the same dictionary, the same 
confusion set, and the same language model. Then we 
performed a test on the same test set. Since the confu-
sion sets are quite large, to reduce the number of 
combinations during the experiment, the size must be 
limited. Since Liu?s experiments show that it takes 
about 3 candidates to find the correct character, we 
use the top 1 to top 10 similar characters as the candi-
dates only in our experiments. That is, we take 1 to 10 
characters from each of the SC1, SC2, SSST, and 
SSDT sets. Thus, the size of each confusion set is 
limited to 4 for the top 1 mode and 40 for the top 10 
mode. 
The evaluation metrics is the same as Chang?s 
(1995). We also define the precision rate, detection 
rate, and correction rate as follows: 
Precision = C / B * 100%  (7) 
Detection = C / A * 100%  (8) 
Correction = D / A * 100%  (9) 
where A is the number of all spelling errors, B is 
the number of errors detected by be system, C is the 
number of errors detected correctly by the system, and 
D is the number of spelling errors that is detected and 
corrected. Note that some errors can be detected but 
cannot be corrected. Since the correction is more im-
portant in an error detection and correction system, 
we define the corresponding f-score as: 
CorrectionPrecision
Correction*Precision*2
scoreF +=?  (10) 
 
 
 
 
 
Figure 2. The comparison of different methods on 
full test set 
5.1 Results of our initial system 
Table 4 shows the initial results of the template mod-
ule (TM), the translation module (LMM) and the 
combined results of the precision mode (PM) and 
detection mode (DM). We find that the precision 
mode gets the highest precision and f-score, while the 
detection mode gets the highest correction rate, as 
expected. The precision and detection rate improved 
dramatically. The precision improved from 14.28% to 
61.68% for the best setting and to 58.82% for the best 
f-score setting. The detection rate improved from 
58.06% to around 72%. The f-score improved from 
22.28% to 43.80%. The result shows that combining 
two independent methods yield better performance 
than each single method does. 
5.2 Results of our system when more know-
ledge and enlarged training sets are added 
The templates used in the initial system were the sim-
plified automatic generated templates, as described in 
section 4.3. Since there were many manually edited 
templates in previous works, we added the 6,701 ma-
nually edited templates and the automatically generat-
ed templates into our system. The results are shown in 
Table 5. All the performance increased for both the 
template module and the translation module. The best 
f-score increased from 43.80% to 45.03%. We believe 
that more knowledge will increase the performance of 
our system. 
5.3 Results of methods in previous works 
We compared the performance of our method to the 
methods in previous works. The result is shown in 
Table 6. Chang?s method has the highest detection 
rate, at 91.79%. Note that the price of this high detec-
tion rate is the high false alarm. The corresponding 
precision is only 0.94%. The precision mode in our 
method has the highest precision, correction, and f-
score. The comparison is shown in Figure 2. The ho-
rizontal axis is the size of confusion sets in our expe-
riment. We can find that the performances converge. 
That is, the size of confusion sets is large enough to 
detect and correct errors in students? essays. 
5.4 Comparison to methods in previous works 
related to sentences with errors 
The numbers in Table 6 are much lower than that in 
the original paper. The reason is the false alarms in 
sentences without any errors, since most previous 
works tested their systems on sentences with errors 
only. In addition, our test set was built on real essays, 
and there were only one or two errors in an essay. 
Most of the sentences contained no errors. The pre-
vious methods tend to raise false alarms.  
To clarify this point, we designed the last experiment 
to test the methods on sentences with at least one er-
ror. We extracted 949 sentences from our test set. 
Among them, 883 sentences have one error, 61 sen-
tences have two errors, 2 sentences have three errors, 
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Precision
Chang Lin Huang PM DM
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Detection
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Correction
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
F-Score
and 3 sentences that have four errors. The result is 
shown in Table 7. All the methods have better per-
formance. The precision of Chang?s method rose from 
3% to 43%. The precision of Lin?s method rose from 
3.5% to 61%. The precision of Huang?s method rose 
from 27% to 84%, while PM?s precision rose from 
60% to 97% and DM?s precision rose from 7% to 
62%. The detection mode of our system still has the 
highest f-score.  
The differences of performances in Table 7 and Table 
6 show that, systems in previous works tent to have 
false alarms in sentences without errors.  
5.5 Processing time comparison 
Processing complexity was not discussed in previous 
works. Since all the systems require different re-
sources, it is hard to compare the time or space com-
plexity. We list the average time it takes to process an 
essay for each method on our server as a reference. 
The processing time is less than 0.5 second for both 
our method and Huang?s method. Lin?s method re-
quired 3.85 sec and Chang?s method required more 
than 237 seconds. 
6 Conclusions 
In this paper, we proposed a new Chinese character 
checker that combines two kinds of technology and 
compared it to three previous methods. Our system 
achieved the best F-score performance by reducing 
the false alarm significantly. An error model adopted 
from the noisy channel model was proposed to make 
use of the frequency of common errors that we col-
lected from a training set. A simplified version of 
automatic template generation was also proposed to 
provide high precision character error detection. Fine 
tuning of the system can be done by adding more 
templates manually.  
The experiment results show that the main draw-
back of previous works is false alarms. Our systems 
have fewer false alarms. The combination of two in-
dependent methods gives the best results on real 
world data. In the future, we will find a way to com-
bine the independent methods with theoretical foun-
dation. 
Acknowledgement 
This study is conducted under the ?Intelligent Web - 
enabled Service Research and Development Project? 
of the Institute for Information Industry which is sub-
sidized by the Ministry of Economy Affairs of the 
Republic of China. 
References  
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. (1993). The ma-
thematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics 19 (pp. 
263-311). 
Chang, C.-H. (1995). A New Approach for Automatic 
Chinese Spelling Correction. In Proceedings of 
Natural Language Processing Pacific Rim Sympo-
sium, (pp. 278-283). Korea. 
Chen, S. F., & Goodman, J. (1996). An Empirical 
Study of Smoothing Techniques for Language 
Modeling. Proc. of the 34th annual meeting on As-
sociation for Computational Linguistics, (pp. 310-
318). Santa Cruz, California. 
Chen, Y.-Z., Wu, S.-H., Lu, C.-C., & Ku, T. (2009). 
Chinese Confusion Word Set for Automatic Gen-
eration of Spelling Error Detecting Template. The 
21th Conference on Computational Linguistics and 
Speech Processing (Rocling 2009), (pp. 359-372). 
Taichung. 
CKIP. (1999). AutoTag. Academia Sinica. 
Huang, C.-M., Wu, M.-C., & Chang, C.-C. (2007). 
Error Detection and Correction Based on Chinese 
Phonemic Alphabet in Chinese Text. Proceedings of 
the Fourth Conference on Modeling Decisions for 
Artificial Intelligence (MDAI IV), (pp. 463-476). 
Hung, T.-H., & Wu, S.-H. (2008). Chinese Essay Er-
ror Detection and Suggestion System. Taiwan E-
Learning Forum.  
Lin, Y.-J., Huang, F.-L., & Yu, M.-S. (2002). A 
CHINESE SPELLING ERROR CORRECTION 
SYSTEM. Proceedings of the Seventh Conference 
on Artificial Intelligence and Applications (TAAI).  
Liu, C.-L., Tien, K.-W., Lai, M.-H., Chuang, Y.-H., & 
Wu, S.-H. (2009). Capturing errors in written Chi-
nese words. Proceedings of the Forty Seventh An-
nual Meeting of the Association for Computational 
Linguistics (ACL'09), (pp. 25-28). Singapore. 
Liu, C.-L., Tien, K.-W., Lai, M.-H., Chuang, Y.-H., & 
Wu, S.-H. (2009). Phonological and logographic in-
fluences on errors in written Chinese words. Pro-
ceedings of the Seventh Workshop on Asian Lan-
guage Resources (ALR7), the Forty Seventh An-
nual Meeting of the Association for Computational 
Linguistics (ACL'09), (pp. 84-91). Singapore. 
Ma, W.-Y., & Chen, K.-J. (2003). A Bottom-up 
Merging Algorithm for Chinese. Proceedings of 
ACL workshop on Chinese Language Processing, 
(pp. 31-38). 
MOE. (2007). MOE Dictionary new edition. Taiwan: 
Ministry of Education. 
Ren, F., Shi, H., & Zhou, Q. (1994). A hybrid ap-
proach to automatic Chinese text checking and er-
ror correction. In Proceedings of the ARPA Work 
shop on Human Language Technology, (pp. 76-81). 
Zhang, L., Zhou, M., Huang, C., & Lu, M. (2000). 
Approach in automatic detection and correction of 
errors in Chinese text based on feature and learning. 
Proceedings of the 3rd world congress on Intelli-
gent Control and Automation, (pp. 2744-2748). He-
fei. 
Zhang, L., Zhou, M., Huang, C., & Sun, M. (2000). 
Automatic Chinese Text Error Correction Approach 
Based-on Fast Approximate Chinese Word-
Matching Algorithm. Proceedings of the 3rd world 
congress on Intelligent Control and Automation, (pp. 
2739-2743). Hefei. 
 
Table 4. Results of our initial system  
  Top 1 2 3 4 5 6 7 8 9 10 
TM 
P 5.74% 5.63% 5.21% 5.02% 4.90% 4.65% 4.36% 4.12% 4.06% 3.95% 
D 29.23% 41.25% 45.36% 49.17% 52.00% 53.47% 54.94% 55.13% 56.79% 57.09% 
C 26.00% 36.75% 40.08% 43.40% 45.65% 46.33% 46.92% 46.82% 48.00% 48.58% 
F 9.40% 9.76% 9.23% 8.99% 8.85% 8.46% 7.99% 7.58% 7.49% 7.31% 
LMM 
P 14.28% 
D 58.06% 
C 50.63% 
F 22.28% 
PM 
P 55.52% 60.03% 60.60% 61.58% 60.65% 61.68% 60.51% 61.19% 58.82% 59.03% 
D 21.60% 29.52% 31.28% 32.74% 34.21% 34.31% 34.31% 33.91% 35.19% 34.79% 
C 21.60% 29.42% 31.18% 32.64% 34.01% 34.11% 34.01% 33.62% 34.89% 34.50% 
F 31.10% 39.49% 41.17% 42.67% 43.58% 43.93% 43.55% 43.40% 43.80% 43.55% 
DM 
P 7.32% 6.15% 5.64% 5.33% 5.11% 4.87% 4.62% 4.42% 4.30% 4.19% 
D 62.75% 65.59% 67.44% 69.40% 70.38% 71.06% 71.94% 72.23% 72.62% 72.72%
C 54.05% 56.69% 58.16% 59.62% 60.60% 60.99% 61.68% 61.77% 61.58% 61.68%
F 12.89% 11.10% 10.28% 9.79% 9.43% 9.02% 8.60% 8.25% 8.04% 7.85% 
 
Table 5. Results of our system after adding more knowledge and enlarged the train set 
  Top 1 2 3 4 5 6 7 8 9 10 
TM 
P 7.31% 6.45% 5.73% 5.41% 5.12% 4.83% 4.51% 4.26% 4.20% 4.08% 
D 37.93% 47.70% 50.15% 53.18% 54.45% 55.62% 56.89% 57.09% 58.75% 59.04% 
C 34.70% 43.21% 44.87% 47.41% 47.70% 48.48% 48.88% 48.78% 49.95% 50.54% 
F 12.08% 11.23% 10.17% 9.70% 9.25% 8.79% 8.26% 7.84% 7.74% 7.55% 
LMM 
P 14.03% 
D 63.14% 
C 55.52% 
F 22.40% 
PM 
P 59.95% 62.72% 62.50% 62.88% 60.66% 61.72% 59.51% 60.29% 58.08% 58.54% 
D 27.66% 34.21% 35.19% 36.26% 35.58% 35.77% 35.77% 35.48% 36.85% 36.85% 
C 27.66% 34.11% 35.09% 36.16% 35.48% 35.67% 35.58% 35.28% 36.65% 36.65% 
F 37.85% 44.19% 44.95% 45.92% 44.77% 45.21% 44.53% 44.51% 44.94% 45.08%
DM 
P 7.76% 6.46% 5.85% 5.51% 5.28% 5.04% 4.78% 4.57% 4.45% 4.33% 
D 69.50% 71.26% 72.04% 73.50% 74.48% 75.26% 75.95% 76.05% 76.34% 76.34%
C 60.50% 62.17% 62.65% 63.73% 64.71% 65.29% 65.78% 65.68% 65.39% 65.39%
F 13.76% 11.70% 10.70% 10.14% 9.76% 9.36% 8.91% 8.55% 8.33% 8.12% 
 
Table 6. Results of methods in previous works 
  Top 1 2 3 4 5 6 7 8 9 10 
Chang 
P 2.82% 1.95% 1.63% 1.43% 1.25% 1.13% 1.07% 0.98% 0.94% 0.91% 
D 72.04% 81.72% 84.55% 88.27% 89.54% 90.32% 91.50% 91.50% 91.79% 91.59%
C 27.66% 39.10% 43.30% 45.45% 44.77% 45.16% 46.33% 45.26% 43.30% 44.28%
F 5.11% 3.71% 3.14% 2.77% 2.43% 2.21% 2.08% 1.92% 1.83% 1.77% 
Lin 
P 3.59% 3.19% 2.93% 2.82% 2.60% 2.51% 2.39% 2.35% 2.32% 2.31% 
D 25.12% 28.93% 29.91% 31.18% 30.98% 31.37% 31.18% 31.57% 32.16% 32.74%
C 19.45% 25.51% 26.78% 27.95% 28.05% 28.15% 28.25% 28.25% 28.73% 29.42%
F 6.06% 5.67% 5.28% 5.12% 4.76% 4.61% 4.41% 4.34% 4.29% 4.28% 
Huang 
P 27.02% 25.81% 25.02% 24.05% 23.30% 22.54% 22.04% 21.16% 20.98% 20.62%
D 10.75% 17.79% 23.06% 26.00% 28.54% 30.49% 31.37% 31.86% 33.33% 33.43%
C 8.30% 12.02% 15.54% 17.00% 17.39% 18.57% 19.64% 18.76% 17.69% 18.27%
F 12.70% 16.40% 19.17% 19.92% 19.92% 20.36% 20.77% 19.89% 19.20% 19.37%
 
Table 7. Results of methods in previous works on sentences with errors 
  Top 1 2 3 4 5 6 7 8 9 10 
Chang 
P 42.94% 37.21% 33.30% 31.18% 29.31% 27.19% 25.98% 24.48% 23.61% 23.14%
D 72.33% 81.62% 84.55% 88.26% 89.63% 90.51% 91.78% 91.79% 92.08% 91.89%
C 27.95% 39.58% 43.98% 46.23% 45.65% 46.04% 47.31% 46.14% 44.28% 45.26%
F 33.86% 38.36% 37.90% 37.24% 35.70% 34.19% 33.54% 31.99% 30.80% 30.62%
Lin 
P 60.59% 59.33% 57.32% 57.19% 55.10% 55.35% 54.27% 53.88% 53.80% 53.97%
D 25.70% 29.52% 30.59% 31.86% 31.67% 32.35% 32.25% 32.55% 33.13% 33.82%
C 19.55% 25.80% 27.37% 28.64% 29.03% 29.52% 29.61% 29.52% 30.00% 30.69%
F 29.56% 35.96% 37.05% 38.17% 38.03% 38.50% 38.32% 38.14% 38.52% 39.13%
Huang 
P 84.16% 76.99% 78.51% 76.11% 73.66% 74.07% 73.21% 70.19% 66.23% 66.66%
D 9.87% 16.03% 20.72% 23.36% 25.70% 27.37% 28.05% 28.54% 29.91% 29.91%
C 7.62% 10.85% 14.17% 15.64% 15.83% 16.71% 17.79% 17.20% 16.12% 16.61%
F 13.97% 19.02% 24.01% 25.95% 26.06% 27.27% 28.62% 27.63% 25.93% 26.59%
PM 
P 96.72% 96.66% 96.76% 96.57% 96.51% 96.54% 96.54% 96.23% 96.11% 96.10%
D 25.90% 31.09% 32.16% 33.04% 32.45% 32.75% 32.75% 32.45% 33.82% 33.72%
C 25.90% 30.98% 32.06% 32.94% 32.36% 32.65% 32.55% 32.26% 33.63% 33.53%
F 40.86% 46.92% 48.16% 49.13% 48.46% 48.80% 48.69% 48.32% 49.82% 49.71%
DM 
P 61.83% 58.45% 56.46% 54.75% 54.21% 53.48% 52.80% 51.53% 51.15% 50.45%
D 69.20% 70.97% 71.74% 73.22% 74.19% 74.98% 75.66% 75.76% 76.05% 76.05%
C 55.62% 57.28% 57.77% 58.84% 59.82% 60.41% 60.90% 60.80% 60.51% 60.51%
F 58.56% 57.86% 57.11% 56.72% 56.88% 56.73% 56.56% 55.78% 55.44% 55.03%
 
