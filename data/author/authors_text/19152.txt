Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590?1601,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Prior Disambiguation of Word Tensors
for Constructing Sentence Vectors
Dimitri Kartsaklis
University of Oxford
Department of
Computer Science
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary University of London
School of Electronic Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrs@eecs.qmul.ac.uk
Abstract
Recent work has shown that compositional-
distributional models using element-wise op-
erations on contextual word vectors benefit
from the introduction of a prior disambigua-
tion step. The purpose of this paper is to
generalise these ideas to tensor-based models,
where relational words such as verbs and ad-
jectives are represented by linear maps (higher
order tensors) acting on a number of argu-
ments (vectors). We propose disambiguation
algorithms for a number of tensor-based mod-
els, which we then test on a variety of tasks.
The results show that disambiguation can pro-
vide better compositional representation even
for the case of tensor-based models. Further-
more, we confirm previous findings regarding
the positive effect of disambiguation on vec-
tor mixture models, and we compare the ef-
fectiveness of the two approaches.
1 Introduction
Distributional models of meaning have been proved
extremely useful for a number of natural language
processing tasks, ranging from thesaurus extraction
(Curran, 2004) to topic modelling (Landauer and
Dumais, 1997) and information retrieval (Manning
et al, 2008), to name just a few. These models
are based on the distributional hypothesis of Har-
ris (1968), which states that the meaning of a word
depends on its context. This idea allows the words
to be represented by vectors of statistics collected
from a sufficiently large corpus of text; each ele-
ment of the vector reflects how many times a word
co-occurs in the same context with another word
of the vocabulary. However, due to the generative
power of natural language, which is able to pro-
duce infinite new structures from a finite set of re-
sources (words), no text corpus, regardless of its
size, can provide reliable distributional representa-
tions for anything longer than single words or per-
haps very short phrases consisting of two words; in
other words, this technique cannot scale up to the
phrase or sentence level.
Much research activity has been recently dedi-
cated to provide a solution to this problem: although
the direct construction of a sentence vector is not
possible, we might still be able to synthetically cre-
ate such a vectorial representation by somehow com-
posing the vectors of the words that comprise the
sentence. Towards this goal, researchers have em-
ployed a variety of approaches that roughly fall into
two general categories. Following an influential
work (Mitchell and Lapata, 2008), the models in the
first category compute a sentence vector as a mix-
ture of the original word vectors, using simple oper-
ations such as element-wise multiplication and ad-
dition; we refer to these models as vector mixtures.
The main characteristic of these models is that they
do not distinguish between the type-logical identi-
ties of the different words: an intransitive verb, for
example, is of the same order as its subject (a noun),
and both will contribute equally to the composite
sentence vector.
However, this symmetric treatment of composi-
tion seems unjustified from a formal semantics point
of view. Words with special meanings, such as verbs
and adjectives, are usually seen as functions acting
on, hence modifying, a number of arguments rather
than lexical units of the same order as them; an
adjective, for example, is a function that returns a
modified version of its input noun. Inspired from
1590
this more-aligned-to-formal-semantics view, a sec-
ond research direction aims to represent relational
words as linear maps (tensors of various orders)
that can be applied to one or more arguments (vec-
tors). Baroni and Zamparelli (2010), for example,
model adjectives as matrices which, when matrix-
multiplied with a noun vector, will produce a vec-
torial representation of the specific adjective-noun
compound. The notion of a framework where re-
lational words are entities living in vector spaces of
higher order than nouns, which are simple vectors,
has been formalized by Coecke et al (2010) in the
context of the abstract mathematical framework of
compact closed categories. We refer to this class of
models as tensor-based.
Regardless of the way they approach the repre-
sentation of relational words and their composition
operation, however, most current compositional-
distributional models do share a common feature:
they all rely on ambiguous vector representations,
where all the senses of a polysemous word, such as
the verb ?file? (which can mean register or smooth),
are merged into the same vector or tensor. At least
for the vector mixture approach, this practice has
been proved suboptimal: Reddy et al (2011) and
Kartsaklis et al (2013) test a number of simple mul-
tiplicative and additive models using disambiguated
vector representations on various tasks, showing that
the introduction of a disambiguation step prior to ac-
tual composition can indeed increase the quality of
the composite vectors. However, the fact that disam-
biguation can be beneficial for models based on vec-
tor mixtures is not very surprising. Both additive and
multiplicative compositions are but a kind of average
of the vectors of the words in the sentence, hence
can directly benefit from the provision of more ac-
curate starting points. Perhaps a more interesting
question, and one that the current paper aims to ad-
dress, is to what extent disambiguation can also pro-
vide benefits for tensor-based approaches, which in
general constitute more powerful models for natural
language (see discussion in Section 2).
Specifically, this paper aims to: (a) propose dis-
ambiguation algorithms for a number of tensor-
based distributional models; (b) examine the effect
of disambiguation on tensors for relational words;
and (c) meaningfully compare the effectiveness of
tensor-based against vector mixture models in a
number of tasks. Based on the generic procedure of
Schu?tze (1998), we propose algorithms for a num-
ber of tensor-based models, where the composition
is modelled as the application of linear maps (ten-
sor contractions). Following Mitchell and Lapata
(2008) and many others, we test our models on
two disambiguation tasks similar to that of Kintsch
(2001), and on the phrase similarity task introduced
in (Mitchell and Lapata, 2010). In almost every
case, the results show that disambiguation can make
a great difference in the case of tensor-based models;
they also reconfirm previous findings regarding the
effectiveness of the method for simple vector mix-
ture models.
2 Vectors vs tensors
The simple models of Mitchell and Lapata (2008)
constitute the easiest and perhaps the most intuitive
way of composing two or more vectors: each ele-
ment of the resulting vector is computed as the sum
or the product of the corresponding elements in the
input vectors (left part in Figure 1). In the case of
addition, the components of the output vector are
simply the cumulative scores of the corresponding
input components. So in a sense the output element
embraces both input elements, resembling a union of
the input features. On the other hand, the element-
wise multiplication of two vectors can be seen as the
intersection of their features: a zero element in one
of the input vectors will eliminate the corresponding
feature in the output, no matter how high the other
input component was. In addition to failing to iden-
tify the special roles of words in a sentence, vector
mixture models disregard grammar in another way:
the commutativity of operators make them a bag-
of-words approach, where the meaning of sentence
?dog bites man? is equated to that of ?man bites dog?.
On the contrary to the above element-wise treat-
ment, a compositional approach based on linear
maps computes each element of the resulting vec-
= =
Vector mixture Tensor-based
Figure 1: Vector mixture and tensor-based models for
composition. In the latter approach, the ith element of
the output vector is the linear combination of the input
vector with the ith row of the matrix.
1591
tor via a linear combination of all the elements of
the input vector (right part of Figure 1); in other
words, possible interdependencies between differ-
ent features are also taken into account, offering (in
principle) more power. Furthermore, by design, the
bag-of-words problem is not present here. Over-
all, tensor-based models offer a more complete and
linguistically motivated solution to the problem of
composition. For example, one can consider build-
ing linear maps for prepositions and logical words,
rather than treating them as noise and discard them,
as commonly done in the vector mixture models.
3 Disambiguation in vector mixtures
For a compositional model based on vector mix-
tures, polysemy of words can be a critical factor.
Pulman (2013) and Kartsaklis et al (2013) point out
that the element-wise combination of ?ambiguous?
vectors produces results that are hard to interpret;
the composed vector is not a purely compositional
representation but a product of two tasks that take
place in parallel: composition and some amount of
disambiguation that emerges as a side-effect of the
compositional process, leaving the resulting vector
in an intermediate state.
This effect is demonstrated in Figure 2, which
shows the composition of the ambiguous verb ?run?
(with meanings moving fast and dissolving) with the
subject ?horse?. The first three components of our
toy vector space are related to the dissolving mean-
ing, while the last three of them to the moving fast
meaning. An ambiguous vector for ?run? will have
non-zero values for every component. On the other
hand, we would expect the vector for ?horse? to have
high values for the ?race?, ?gallop?, and ?move? com-
ponents, and very low values (but not necessarily
zero) for the dissolving-related ones?it is always
possible for the word ?horse? to appear in the same
c o l o u r
d i s s o l v e
p a i n t i n g
r a c e
g a l l o p
m o v e
5
9
4
1 1
8
1 5
1
0
2
6
1 3
7
A mbig uous 
runhorse
=
5
0
8
6 6
1 0 4
1 0 5
0
0
0
1 1
8
1 5
1
0
2
6
1 3
7
D isambig uated
runhorse
=
0
0
0
6 6
1 0 4
1 0 5
Figure 2: The effect of disambiguation on vector compo-
sition. The numbers are (artificial) co-occurrence counts
of each target word with the 6 basis words on the left.
context with the word ?painting?, for example. The
left part of Figure 2 shows what happens when the
ambiguous ?run? vector is used; the multiplication
with the ?horse? vector will produce an impure re-
sult, half affected by composition and half by disam-
biguation. However, what we really want is a vec-
tor where all the dissolving-related components will
be eliminated, since they are irrelevant to the way
the word ?run? is used in the sentence. In order to
achieve this, we have to introduce a disambiguation
step prior to composition (right part of Figure 2).
These ideas are experimentally verified in the
works of Reddy et al (2011) and Kartsaklis et al
(2013); Pulman (2013) also presents a comprehen-
sive analysis of the problem. What remains to be
seen is if disambiguation can also provide bene-
fits for the linguistically motivated setting of tensor-
based models, the principles of which are shortly
discussed in the next section.
4 Tensors as multilinear maps
A tensor is a geometric object that can be seen as the
generalization of the familiar notion of a vector in
higher dimensions. The order of a tensor is the num-
ber of its dimensions; in other words, the number of
indices we need to fully describe a random element
of the tensor. Hence, a vector is a tensor of order
1, a matrix is a tensor of order 2, and so on. Ten-
sors and multilinear maps stand in one-to-one cor-
respondence, as stated by the following well-known
?map-state? isomorphism (Bourbaki, 1989):
f : V1 ? . . .? Vj ? Vk ?= Vk?Vj?. . .?V1 (1)
This offers an elegant way to adopt a formal se-
mantics view of natural language in vector spaces.
Let nouns live in a basic vector space N ? RD; re-
turning to our previous example, an adjective then
can be seen as a map f : N ? N which is isomor-
phic to N ?N (that is, to a matrix). In general, the
order of the tensor is equal to the number of argu-
ments plus one dimension that carries the result; so
a unary function (e.g. adjectives, intransitive verbs)
is represented by a tensor of order 2 (a matrix), a bi-
nary function (e.g. a transitive verb) as an order 3
tensor, and so on. Due to the above isomorphism,
function application (and hence our compositional
operation) becomes a generalisation of matrix mul-
tiplication, formalised in terms of the inner product.
In the case of a unary relational word, such as an
adjective, this is nothing more than the usual notion
1592
of matrix multiplication between a matrix and a vec-
tor. The generalization of this process to tensors of
higher order is known as tensor contraction. Given
two tensors of orders n and m, the tensor contrac-
tion operation will always produce a tensor of order
n+m? 2.
Let us see an example of how this works for a
simple transitive sentence. Let V ? RI?J?K be the
tensor of order 3 for the verb and S ? RI , O ? RK
the tensors of order 1 (vectors) for the subject and
the object of the verb, respectively. Then V ?O will
return a new tensor living in RI?J (i.e. a matrix)1;
a further interaction of this result with the subject
will return a vector for the whole transitive sentence
living in RJ . We should note that the order in which
the verb is applied to its arguments is not important;
so in general the meaning of a transitive sentence is
given by:
(V ?O)T ? S = (VT ? S)?O (2)
where T denotes a transpose and makes indices
match, since subject precedes the verb.
5 Creating verb tensors
In this section we review a number of proposals re-
garding concrete methods of constructing tensors for
relational words in the context of the frameworks
of Coecke et al (2010) and Baroni and Zamparelli
(2010), which both comply to the setting of Section
4.2
Relational Following ideas from the set-theoretic
view of formal semantics, Grefenstette and
Sadrzadeh (2011a) suggest that the meaning of a
relational word should be represented as the sum
of its arguments. The meaning of adjective ?red?,
for example, becomes the sum of the vectors of
all the nouns that ?red? modifies in the corpus; so
??
red =
?
i
????nouni, where i iterates through all the
occurrences of ?red?. This can be generalised to
relational words of any arity, by summing the tensor
product of their arguments. So for a transitive verb
we have:
verb
2
=
?
i
(
???
subji ?
???
obji) (3)
1The symbol ? denotes tensor contraction.
2In what follows we use the case of a transitive verb as an
example; however the descriptions apply to any relational word
of any arity. A vector (an order-1 tensor) is denoted as ??x ; ten-
sors of order n > 1 are shown as xn for clarity.
where i again iterates over all occurrences of the spe-
cific verb in the corpus and the superscript denotes
the order of the tensor.
In order to achieve a more expressive represen-
tation for the sentences, the authors used the con-
vention that the arity of the head word in a sentence
will also determine the order of the sentence space;
that is, the space of intransitive sentences will be of
order 1, of transitive ones will be of order 2, and
so on. Recall from Section 4 that for the transitive
case this increases the order of the verb tensor to 4
(2 dimensions for the arguments plus another 2 for
the result). In spite of this, however, note that the
method of Equation 3 produces a matrix. The other
two dimensions of the tensor remain empty (filled
with zeros), a fact that simplifies the calculations but
also considerably weakens the expressive power of
the model. This simplification transforms Equation
2 to the following:
subj verb obj
2
= (
???
subj ?
??
obj) verb
2
(4)
where ? denotes the tensor product and  element-
wise multiplication.
Kronecker In a subsequent work (Grefenstette
and Sadrzadeh, 2011b), the same team proposes the
creation of a verb matrix as the Kronecker product
of the verb?s contextual vector with itself:
verb
2
=
???
verb?
???
verb (5)
Again in this model the sentence space is of order
2, and the meaning of a transitive sentence is calcu-
lated using Equation 4.
Frobenius The previous models bring the impor-
tant limitation that only sentences of the same struc-
ture can be meaningfully compared; it is not pos-
sible, for example, to compare an intransitive sen-
tence (e.g. ?kids play?) with a transitive one (?chil-
dren play football?), since the former is a vector and
the latter a matrix. Using Frobenius algebras, Kart-
saklis et al (2012) provide a unified sentence space
for every sentence regardless of its type. These mod-
els turn the matrix of Equation 3 to a tensor of or-
der 3 (as required by the type-logical identities) by
copying one of the existing dimensions. When the
dimension of rows (corresponding to subjects) is
copied, the calculation of a vector for a transitive
sentence becomes:
??????????
subj verb obj =
???
subj  (verb
2
?
??
obj) (6)
1593
Copying the column dimension (objects) gives:
??????????
subj verb obj =
??
obj 
(
(verb
2
)T ?
???
subj
)
(7)
Linear regression None of the above models cre-
ate tensors that are fully populated: one or more
dimensions will always remain empty. Following
an idea first introduced by Baroni and Zamparelli
(2010) for the creation of adjective matrices, Grefen-
stette et al (2013) use linear regression in order
to learn full tensors of order 3 for transitive verbs.
Linear regression is a supervised method of learn-
ing, so it needs a number of exemplar data points.
In the case of the adjective ?red?, for example, we
would need a set of the form ???car,
?????
red car?, ?
???
shirt,
??????
red shirt?, ?
???
shoe,
??????
red shoe? and so on, where the sec-
ond vector in each pair is the contextual vector of the
whole phrase created exactly as if it were a single
word. The goal of the learning process is to find the
parameters adj
2
and
??
b such that:
???????
adj noun ? adj
2
?????noun+
??
b (8)
for all nouns modified by the specific adjective. In
practice, the bias
??
b is embedded in adj
2
, hence
the above procedure provides us with a matrix for
the adjective. One can generalize this procedure to
tensors of higher order by proceeding step-wise, as
done by Grefenstette et al (2013). For the case
of a transitive verb, they first use exemplar pairs
of the form ?
???
subj,
?????????
subj verb obj? to learn a matrix
verb obj
2
for the verb phrase; then, they perform
a new training session with exemplars of the form
?
??
obj, verb obj
2
?, the result of which is an order 3
tensor for the verb.
6 Generic context-based disambiguation
In all of the models of Section 5, the training of a
relational word tensor is based on the set of contexts
where this word occurs. Hence, in these models the
problem of creating disambiguated versions of ten-
sors can be recasted to that of further breaking the
set of contexts in a way that each subset reflects a
different sense of the word in the corpus. If, for ex-
ample, S is the whole set of sentences for a word
w that occurs in the corpus under n different senses,
then the goal is to create n subsets S1, . . . Sn such
that S1 contains the sentences where w appears un-
der the first sense, S2 the sentences where w occurs
under the second sense, and so on. Each one of these
subsets can then be used to train a tensor for a spe-
cific sense of the target relational word.
Towards this purpose we use a variation of the ef-
fective procedure of Schu?tze (1998): first, each con-
text for a target word wt is represented by a context
vector of the form 1n(
??w1 + . . . +
??wn), where
??wi is
the lexical vector of some other word wi 6= wt in the
same context. Next, we apply a clustering method
on this set of vectors in order to discover the latent
senses of wt. The assumption is that the contexts
of wt will vary according to the specific sense this
word is used: ?bank? as a financial institution should
appear in quite different contexts than as land.
The above procedure will give us a number of
clusters, each consisting of context vectors; we use
the centroid of each cluster as a vectorial repre-
sentation of the corresponding sense. So in our
model each wordw is initially represented by a tuple
???w ,S?, where??w is the lexical vector of the word as
created by the usual distributional practice, and S
is a set of sense vectors (centroids of context vec-
tor clusters) produced by the above procedure. The
disambiguation of a new word w under a context C
can now be accomplished as follows: we create a
context vector ??c for C as above, and we compare it
with every sense vector of w; the word is assigned to
the sense corresponding to the closest sense vector.
Specifically, if Sw is the set of sense vectors for w,
??c the context vector for C, and d(??v ,??u ) our vector
distance measure, the preferred sense s? is given by:
s? = argmin
??vs?Sw
d(??vs ,
??c ) (9)
For the actual clustering step we follow the set-
ting of Kartsaklis et al (2013), which worked well in
tasks very similar to ours. Specifically, we perform
hierarchical agglomerative clustering (HAC) using
Ward?s method as the inter-cluster distance, while
the distance between vectors is measured with Pear-
son?s correlation.3 In the above work, this configura-
tion has been found to return the highest V-measure
(Rosenberg and Hirschberg, 2007) on the noun set
of SEMEVAL 2010 Word Sense Induction & Disam-
biguation Task (Manandhar et al, 2010). As con-
text for a word, we consider the sentence in which
this word occurs. The output of HAC is a dendro-
gram embedding all the possible partitionings of the
3Informal experimentation with more robust probabilistic
techniques, such as Dirichlet process gaussian mixture models,
revealed no significant benefits for our setting.
1594
data. In order to select the optimal partitioning, we
rely on the Calin?ski/Harabasz index (Calin?ski and
Harabasz, 1974), also known as variance ratio cri-
terion (VRC). VRC is calculated as the ratio of the
sum of the inter-cluster variances over the sum of
the intra-cluster variances, bearing the intuition that
the optimal partitioning should be the one that re-
sults in the most compact and maximally separated
clusters. We compute the VRC for a range of differ-
ent partitionings (from 2 to 10 clusters) and keep the
partitioning with the highest score.
7 Constructing unambiguous verb tensors
The procedure described in Section 6 provides us
with n clusters of context vectors for a target word.
Since in our case each context vector corresponds
to a distinct sentence, the output of the clustering
scheme can also be seen as n subsets of sentences,
where the word appears under different senses. It
is now quite straightforward to use this partitioning
of the training corpus in order to learn unambiguous
versions of verb tensors, as detailed below.
Relational/Frobenius Both the Relational and the
Frobenius models use the same way of creating an
initial verb matrix (Equation 3) which then they ex-
pand to a higher order tensor. Let S1 . . . Sn be the
sets of sentences returned by the clustering step for
a verb. Then, the verb tensor for the ith sense is:
verb
2
i =
?
s?Si
(
????
subjs ?
???
objs) (10)
where subjs and objs refer to the subject/object pair
that occurred with the verb in sentence s. This can
be generalized to any arity n as follows:
word
n
i =
?
s?Si
n?
k=1
????argk,s (11)
where argk,s denotes the kth argument of the target
word in sentence s.
Kronecker For a given verb v in a context C, let
??vi be the sense vector of v given C corresponding to
the sense i returned by Equation 9. Then we have:
verb
2
i =
??vi ?
??vi (12)
The generalized version to arity n is given by:
word
n
i =
n?
k=1
??vi (13)
Linear regression Creating unambiguous full ten-
sors using linear regression is also quite straightfor-
ward. Let us assume again that the clustering step
for a verb v returns n sets of sentences S1 . . . Sn,
where each sentence set corresponds to a different
sense. Then, we have n different regression prob-
lems, each one of which will be trained on exemplar
pairs derived exclusively from the sentences of the
corresponding set. This will result in n verb tensors,
which will correspond to the different senses of the
verb. Generalization to higher arities is a straightfor-
ward extension of the step-wise process in Section 5
for transitive verbs.
8 Experiments
In this section we will test the effect of disambigua-
tion on the models of Section 5 in a variety of tasks.
Due to the significant methodological differences
of the linear regression model from the other ap-
proaches and the variety of its set of parameters, we
decided that it would be better if this was left as the
subject of a distinct work.
Experimental setting We train our vectors using
ukWaC (Ferraresi et al, 2008), a corpus of English
text with 2 billion words (100m sentences). We use
2000 dimensions, with weights calculated as the ra-
tio of the probability of the context word given the
target word to the probability of the context word
overall. The context here is a 5-word window on
both sides of the target word. The vectors are disam-
biguated both syntactically and semantically: first,
separate vectors have been created for different syn-
tactic usages of the same word in the corpus; for ex-
ample, the word ?book? has two vectors, one for its
noun sense and one for its verb sense. Furthermore,
each word is semantically disambiguated according
to the method of Section 6.
Models We compare the tensor-based models of
Section 5 with the multiplicative and additive mod-
els of Mitchell and Lapata (2008), reporting results
for both ambiguous and disambiguated versions.
For all the disambiguated models, the best sense for
each word in the sentence or phrase is first selected
by applying the procedure of Section 6 and Equa-
tion 9. If the model is based on a vector mixture, the
sense vectors corresponding to these senses are mul-
tiplied or added to form the composite representa-
tion for the sentence or phrase. For the tensor-based
models, the composite meanings are calculated ac-
1595
cording to the equations of Section 5, using verb
tensors created by the procedures of Section 7. The
semantic similarity of two phrases or sentences is
measured as the cosine distance between their com-
posite vectors. For models that return a matrix (e.g.
Relational, Kronecker), the distance is based on the
Frobenius inner product.
Implementation details Our code is mainly writ-
ten in Python and C++, and for the actual cluster-
ing step we use the Python interface of the efficient
FASTCLUSTER library (Mu?llner, 2013). In a shared
24-core Xeon machine with 72 GB of memory, and
with a fair amount of parallelism applied, the aver-
age processing time per word was about 4 minutes;
this is roughly translated to 12-13 hours of training
on average per dataset.
8.1 Verb disambiguation task
Perhaps surprisingly, one of the most popular tasks
for testing compositionality in distributional models
is based on disambiguation. This task, originally in-
troduced by Kintsch (2001), has been adopted by
Mitchell and Lapata (2008) and others for evaluating
the quality of composition in vector spaces. Given
an ambiguous verb such as ?file?, the goal is to find
out to what extent the presence of an appropriate
context will disambiguate its intended meaning. The
context (e.g. a subject/object pair) is composed with
two landmark verbs corresponding to the different
senses (?smooth? and ?register?) to create simple sen-
tences. The assumption is that a good compositional
model should be able to reflect that ?woman files ap-
plication? is closer to ?woman registers application?
than to ?woman smooths application?.
In this paper we test our models on two different
datasets of transitive sentences, that of Grefenstette
and Sadrzadeh (2011a) and Kartsaklis et al (2013)4.
Specific details about the creation of the datasets can
be found in the above papers; for the purposes of
the current work it is sufficient to mention that their
main difference is that in the former the verbs and
their alternative meanings have been selected auto-
matically using the JCN metric of semantic similar-
ity (Jiang and Conrath, 1997), while in the latter the
selection was based on human judgements from the
work of Pickering and Frisson (2001). So, while
4This dataset has been created by Mehrnoosh Sadrzadeh in
collaboration with Edward Grefenstette, but remained unpub-
lished until (Kartsaklis et al, 2013).
in the first dataset many verbs cannot be consid-
ered as genuinely ambiguous (e.g. ?say? with mean-
ings state and allege or ?write? with meanings pub-
lish and spell), the landmarks in the second dataset
correspond to clearly separated senses (e.g. ?file?
with meanings register and smooth or ?charge? with
meanings accuse and bill). Furthermore, subjects
and objects of this latter case are modified by appro-
priate adjectives, overall creating a richer and more
linguistically balanced dataset.
In both cases the evaluation methodology is the
same: each entry of the dataset has the form
?subject, verb, object, high-sim landmark, low-sim
landmark?. The context is combined with the verb
and the two landmarks, creating three simple tran-
sitive sentences. The main-verb sentence is paired
with both the landmark sentences, and these pairs
are randomly presented to human evaluators, the
duty of which is to evaluate the similarity of the sen-
tences within a pair in a scale from 1 to 7. The scores
of the compositional models are the cosine distances
(or the Frobenius inner products, in the case of ma-
trices) between the composite representations of the
sentences of each pair. As an overall score for each
model, we report its Spearman?s ? correlation with
the human judgements. Both datasets consist of 200
pairs of sentences (10 main verbs ? 2 landmarks ?
10 contexts).
Results The results for the G&S dataset are shown
in Table 1.5 The verbs-only model (BL) refers to a
non-compositional evaluation, where the similarity
between two sentences is solely based on the dis-
tance between the two verbs, without applying any
compositional step with subject and object.
The tensor-based models present much better per-
formance than the vector mixture ones, with the dis-
ambiguated version of the copy-object model sig-
nificantly higher than the relational model. By de-
sign, the copy-object model retains more informa-
tion about the objects; so this result confirms pre-
vious findings, that in this certain dataset the role
of objects is more important than that of subjects
(Kartsaklis et al, 2012). In general, the disambigua-
tion step improves the results of all the tensor-based
models except Kronecker; the effect is reversed for
the vector mixture models, where the disambiguated
versions present much worse performance (these
5For all tables in this section,  and denote highly sta-
tistically significant differences with p < 0.001.
1596
Model Ambig. Disamb.
BL Verbs only 0.198  0.132
M1 Multiplicative 0.137  0.044
M2 Additive 0.127  0.047
T1 Relational 0.219 < 0.223
T2 Kronecker 0.207  0.061
T3 Copy-subject 0.070  0.122
T4 Copy-object 0.241  0.262
Human agreement 0.599
Difference between T4 and T1 is s.s. with p < 0.001
Table 1: Results for the G&S dataset.
Model Ambig. Disamb.
BL Verbs only 0.151  0.217
M1 Multiplicative 0.131 < 0.137
M2 Additive 0.085  0.193
T1 Relational 0.036  0.121
T2 Kronecker 0.159 < 0.166
T3 Copy-subject 0.035  0.117
T4 Copy-object 0.033  0.095
Human agreement 0.383
Difference between BL and M2 is s.s. with p < 0.001
Table 2: Results for the Kartsaklis et al dataset.
findings are further discussed in Section 9).
The result of disambiguation is clearer for the
dataset of Kartsaklis et al (Table 2). The longer
context in combination with genuinely ambiguous
verbs produces two effects: first, disambiguation is
now helpful for all models, either vector mixtures
or tensor-based; second, the disambiguation of just
the verb (verbs-only model), without any interac-
tion with the context, is sufficient to provide the best
score (0.22) with a difference statistically significant
from the second model (0.19 for disambiguated ad-
ditive). In fact, further composition of the verb with
the context decreases performance, confirming the
results reported by Kartsaklis et al (2013) for vec-
tors trained using BNC. Given the nature of the spe-
cific task, which is designed around the ambiguity of
the verb, this result is not surprising: a direct disam-
biguation of the verb based on the rest of the con-
text should naturally constitute the best method to
achieve top performance?no composition is neces-
sary for this task to be successful.
However, when one does use a task like this in
order to evaluate compositional models (as we do
here and as is commonly the case), they implic-
itly correlate the strength of the disambiguation ef-
fect that takes place during the composition with the
quality of composition, essentially assuming that the
stronger the disambiguation, the better the composi-
tional model that produced this side-effect. Unfor-
tunately, the extent to which this assumption is valid
or not is still not quite clear; the subject is addressed
in more detail in (Kartsaklis et al, 2013). Keeping a
note of this observation, we now proceed to examine
the performance of our models in a task that does not
use disambiguation as a criterion of composition.
8.2 Phrase/sentence similarity task
Our second set of experiments is based on the phrase
similarity task of Mitchell and Lapata (2010). On
the contrary with the task of Section 8.1, this one
does not involve any assumptions about disambigua-
tion, and thus it seems like a more genuine test of
models aiming to provide appropriate phrasal or sen-
tential semantic representations; the only criterion is
the degree to which these models correctly evaluate
the similarity between pairs of sentences or phrases.
We work on the verb-phrase part of the dataset, con-
sisting of 72 short verb phrases (verb-object struc-
tures). These 72 phrases have been paired in three
different ways to form groups exhibiting various
degrees of similarity: the first group contains 36
pairs of highly similar phrases (e.g. produce effect-
achieve result), the pairs of the second group are
of medium similarity (e.g. write book-hear word),
while a last group contains low-similarity pairs (use
knowledge-provide system). The task is again to
compare the similarity scores given by the various
models for each phrase pair with those of human an-
notators. Additionally to the verb phrases task, we
also perform a richer version of the experiment us-
ing transitive sentences.
Verb phrases It can be shown that for simple verb
phrases the relational model reduces itself to the
copy-subject model; for both of these methods, the
meaning of the verb phrase is calculated according
to Equation 6. Furthermore, according to the copy-
object model the meaning of a verb phrase computed
by a verb matrix
?
ij vij(
??ni?
??nj) and an object vec-
tor
?
j oj
??nj becomes:
verb object
2
=
?
ij
vijoj(
??ni ?
??nj) (14)
Finally, the Kronecker model has no meaning for
verb phrases, since the vector of a verb phrase
will become (??vs ?
??vs) ?
??
obj, which is equal to
???vs |
??
obj???vs , where ?
??vs |
??
obj? denotes the inner prod-
uct between vectors of verb and object. Hence, the
1597
Model Ambig. Disamb.
BL Verbs only 0.310  0.420
M1 Multiplicative 0.315  0.448
M2 Additive 0.291  0.436
T1 Rel./Copy-sbj 0.340  0.367
T2 Copy-object 0.290  0.393
Human agreement 0.550
Difference between M1 and M2 is not s.s.
Difference between M1 and BL is s.s. with p < 0.001
Table 3: Results for the original M&L task.
meaning of a verb phrase becomes a scalar multipli-
cation of the meaning of its verb. As a result, the
cosine distance (used for measuring similarity) be-
tween the meanings of two verb phrases is reduced
to the distance between the vectors of their verbs,
completely dismissing the role of their objects.
Hence our models are limited to those of Table
3. The effects of disambiguation for this task are
quite impressive: the differences between the scores
of all disambiguated models and those of the am-
biguous versions are highly statistically significant
(with p < 0.001), while 4 of the 5 models present
an improvement greater than 10 units of correla-
tion. The models that benefit the most from disam-
biguation are the vector mixtures; both of these ap-
proaches perform significantly better than the best
tensor-based model (copy-object). In fact, the score
of M1 (0.45) is quite high, given that the inter-
annotator agreement is 0.55 (best score reported by
Mitchell and Lapata was 0.41 for their LDA-dilation
model).
Transitive sentences The second part of this ex-
periment aims to examine the extent to which the
above picture can change for the case of text struc-
tures longer than verb phrases. In order to achieve
this, we extend each one of the 72 verb phrases to
a full transitive sentence by adding an appropriate
subject such that the similarity relationships of the
original dataset are retained as much as possible,
so the human judgements for the verb phrase pairs
could as well be used for the transitive cases. We
worked pair-wise: for each pair of verb phrases, we
first selected one of the 5 most frequent subjects for
the first phrase; then, the subject of the other phrase
was selected by a list of synonyms of the first sub-
ject in a way that the new pair of transitive sen-
tences constitutes the least more specific version of
the given verb-phrase pair. So, for example, the pair
produce effect/achieve result became drug produce
effect/medication achieve result, while the pair pose
problem/address question became study pose prob-
lem/paper address question.6
The restrictions of the verb-phrase version do not
hold here, so we evaluate on the full set of models
(Table 4). Once more disambiguation produces bet-
ter results in all cases, with highly statistically sig-
nificant differences for all but one model. Further-
more, now the best score is delivered by one of the
tensor-based models (Kronecker), with a difference
not statistically significant from disambiguated ad-
ditive. In any case, the result suggests that as the
length of the text segments increases, the perfor-
mance of vector mixtures and tensor-based models
converges. Indeed, note how the performance of the
vector mixture models are significantly decreased
compared to the verb phrase task.
9 Discussion
The purpose of this work was twofold: our main ob-
jective was to investigate how disambiguation can
affect the compositional models which are based on
higher order vector spaces; a second, but not less
important goal, was to compare this more linguisti-
cally motivated approach to the simpler vector mix-
ture methods. Based on the experimental work pre-
sented here, we can say with enough confidence that
disambiguation as an additional step prior to com-
position is indeed very beneficial for tensor-based
models. Furthermore, our experiments confirm and
strengthen previous work (Reddy et al, 2011; Kart-
saklis et al, 2013) that showed better performance of
disambiguated vector mixture models compared to
their ambiguous versions. The positive effect of dis-
ambiguation is more evident for the vector mixture
models (especially for the additive model) than for
6The dataset will be available at http://www.cs.ox.
ac.uk/activities/compdistmeaning/.
Model Ambig. Disamb.
BL Verbs only 0.310  0.341
M1 Multiplicative 0.325  0.404
M2 Additive 0.368  0.410
T1 Relational 0.368  0.397
T2 Kronecker 0.404 < 0.412
T3 Copy-subject 0.310  0.337
T4 Copy-object 0.321  0.368
Human agreement 0.550
Difference between T2 and M2 is not s.s.
Table 4: Transitive version of M&L task.
1598
the tensor-based ones. This is expected: composite
representations created by element-wise operations
are averages, and a prior step of disambiguation can
make a great difference.
From a task perspective, the effect of disambigua-
tion was much more definite in the phrase/sentence
similarity task. This observation is really interest-
ing, since the words of that dataset were not se-
lected in order to be ambiguous in any way. The
superior performance of the disambiguated models,
therefore, implies that the proposed methodology
can improve tasks based on phrase or sentence sim-
ilarity regardless of the level of ambiguity in the
vocabulary. For these cases, the proposed disam-
biguation algorithm acts as a fine-tuning process, the
outcome of which seems to be always positive; it
can only produce better composite representations,
not worse. In general, the positive effect of dis-
ambiguation in the phrase/sentence similarity task is
quite encouraging, especially given the fact that this
task constitutes a more appropriate test for evaluat-
ing compositional models, avoiding the pitfalls of
disambiguation-based experiments (as shortly dis-
cussed in Section 8.1).
For disambiguation-based tasks similar to those
of Section 8.1, the form of dataset is very important;
hence the inferior performance of disambiguated
models in the G&S dataset, compared to the dataset
of Kartsaklis et al. In fact, the G&S dataset was
the only one where disambiguation was not helpful
for some cases (specifically, for vector mixtures and
the Kronecker model). We believe the reason behind
this lies in the fact that the automatic selection of
landmark verbs using the JCN metric (as done with
the G&S dataset) was not very efficient for certain
cases. Note, for example, that the bare baseline of
comparing just ambiguous versions of verbs (with-
out any composition) in that dataset aleady achieves
a very high correlation of 0.198 with human judge-
ments (Table 1).7. This number is only 0.15 for the
Kartsaklis et al dataset, due to the more efficient
verb selection procedure. In general, we consider
the results gained by this latter experiment more re-
liable for the specific task, the successful evaluation
of which requires genuinely ambiguous verbs.
The results are less conclusive for the second
question we posed in the beginning of this section,
regarding the comparison of the two classes of mod-
7The reported number for this baseline by Grefenstette and
Sadrzadeh (2011a) was 0.16 using vectors trained from BNC.
els. Despite the obvious benefits of the tensor-based
approaches, this work suggests for one more time
that vector mixture models might constitute a hard-
to-beat baseline; similar observations have been
made, for example, in the comparative study of Bla-
coe and Lapata (2012). However, when trying to in-
terpret the mixing results regarding the effectiveness
of the tensor-based models compared to vector mix-
tures, we need to take into account that the tensor-
based models tested in this work were all ?hybrid?,
in the sense that they all involved some element
of point-wise operation; in other words, they con-
stituted a trade-off between transformational power
and complexity.
Even with this compromise, though, the study
presented in Section 8.2 implies that the effective-
ness of each method depends to some extent on the
length of the text segment: when more words are
involved, vector mixture models tend to be less ef-
fective; on the contrary, the performance of tensor-
based models seems to be proportional to the length
of the phrase or sentence?the more, the better.
These observations comply with the nature of the
approaches: ?averaging? larger numbers of points
results in more general (hence less accurate) repre-
sentations; on the other hand, a larger number of
arguments makes a function (such as a verb) more
accurate.
10 Conclusion and future work
In the present paper we showed how to improve
a number of tensor-based compositional distribu-
tional models of meaning by introducing a step
of disambiguation prior to composition. Our sim-
ple algorithm (based on the procedure of Schu?tze
(1998)) creates unambiguous versions of tensors be-
fore these are composed with vectors of nouns in
order to construct vectors for sentences and phrases.
This algorithm is quite generic, and can be applied
to any model that follows the tensor contraction pro-
cess described in Section 4. As for future work, we
aim to investigate the application of this procedure
to the regression model of Grefenstette et al (2013).
Acknowledgements
We would like to thank Edward Grefenstette for his
comments on the first draft of this paper, as well as
the three anonymous reviewers for their fruitful sug-
gestions. Support by EPSRC grant EP/F042728/1 is
gratefully acknowledged by the authors.
1599
References
Baroni, M. and Zamparelli, R. (2010). Nouns are
Vectors, Adjectives are Matrices. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Blacoe, W. and Lapata, M. (2012). A comparison of
vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Bourbaki, N. (1989). Commutative Algebra: Chap-
ters 1-7. Srpinger Verlag, Berlin/New York.
Calin?ski, T. and Harabasz, J. (1974). A Dendrite
Method for Cluster Analysis. Communications in
Statistics-Theory and Methods, 3(1):1?27.
Coecke, B., Sadrzadeh, M., and Clark, S.
(2010). Mathematical Foundations for Dis-
tributed Compositional Model of Meaning. Lam-
bek Festschrift. Linguistic Analysis, 36:345?384.
Curran, J. (2004). From Distributional to Seman-
tic Similarity. PhD thesis, School of Informatics,
University of Edinburgh.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukWaC, a very large web-derived corpus of En-
glish. In Proceedings of the 4th Web as Corpus
Workshop (WAC-4) Can we beat Google, pages
47?54.
Grefenstette, E., Dinu, G., Zhang, Y.-Z., Sadrzadeh,
M., and Baroni, M. (2013). Multi-step regres-
sion learning for compositional distributional se-
mantics. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS
2013).
Grefenstette, E. and Sadrzadeh, M. (2011a). Exper-
imental Support for a Categorical Compositional
Distributional Model of Meaning. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Grefenstette, E. and Sadrzadeh, M. (2011b). Exper-
imenting with Transitive Verbs in a DisCoCat. In
Proceedings of Workshop on Geometrical Models
of Natural Language Semantics (GEMS).
Harris, Z. (1968). Mathematical Structures of Lan-
guage. Wiley.
Jiang, J. and Conrath, D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics, pages
19?33, Taiwan.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2012). A unified sentence space for categori-
cal distributional-compositional semantics: The-
ory and experiments. In Proceedings of 24th
International Conference on Computational Lin-
guistics (COLING 2012): Posters, pages 549?
558, Mumbai, India. The COLING 2012 Orga-
nizing Committee.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2013). Separating Disambiguation from Com-
position in Distributional Semantics. In Proceed-
ings of 17th Conference on Computational Nat-
ural Language Learning (CoNLL-2013), Sofia,
Bulgaria.
Kintsch, W. (2001). Predication. Cognitive Science,
25(2):173?202.
Landauer, T. and Dumais, S. (1997). A Solution to
Plato?s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representa-
tion of Knowledge. Psychological Review.
Manandhar, S., Klapaftis, I., Dligach, D., and Prad-
han, S. (2010). Semeval-2010 task 14: Word
sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68. Association for
Computational Linguistics.
Manning, C., Raghavan, P., and Schu?tze, H. (2008).
Introduction to Information Retrieval. Cambridge
University Press.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics, pages 236?244.
Mitchell, J. and Lapata, M. (2010). Composition
in distributional models of semantics. Cognitive
Science, 34(8):1388?1439.
Mu?llner, D. (2013). fastcluster: Fast Hierarchical
Clustering Routines for R and Python. Journal of
Statistical Software, 9(53):1?18.
Pickering, M. and Frisson, S. (2001). Processing
ambiguous verbs: Evidence from eye movements.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 27(2):556.
1600
Pulman, S. (2013). Combining Compositional and
Distributional Models of Semantics. In Heunen,
C., Sadrzadeh, M., and Grefenstette, E., editors,
Quantum Physics and Linguistics: A Composi-
tional, Diagrammatic Discourse. Oxford Univer-
sity Press.
Reddy, S., Klapaftis, I., McCarthy, D., and Man-
andhar, S. (2011). Dynamic and static prototype
vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural
Language Processing, pages 705?713.
Rosenberg, A. and Hirschberg, J. (2007). V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 410?420.
Schu?tze, H. (1998). Automatic Word Sense Dis-
crimination. Computational Linguistics, 24:97?
123.
1601
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708?719,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Evaluating Neural Word Representations in
Tensor-Based Compositional Settings
Dmitrijs Milajevs
1
Dimitri Kartsaklis
2
Mehrnoosh Sadrzadeh
1
Matthew Purver
1
1
Queen Mary University of London
School of Electronic Engineering
and Computer Science
Mile End Road, London, UK
{d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk
2
University of Oxford
Department of Computer Science
Parks Road, Oxford, UK
dimitri.kartsaklis@cs.ox.ac.uk
Abstract
We provide a comparative study be-
tween neural word representations and
traditional vector spaces based on co-
occurrence counts, in a number of com-
positional tasks. We use three differ-
ent semantic spaces and implement seven
tensor-based compositional models, which
we then test (together with simpler ad-
ditive and multiplicative approaches) in
tasks involving verb disambiguation and
sentence similarity. To check their scala-
bility, we additionally evaluate the spaces
using simple compositional methods on
larger-scale tasks with less constrained
language: paraphrase detection and di-
alogue act tagging. In the more con-
strained tasks, co-occurrence vectors are
competitive, although choice of composi-
tional method is important; on the larger-
scale tasks, they are outperformed by neu-
ral word embeddings, which show robust,
stable performance across the tasks.
1 Introduction
Neural word embeddings (Bengio et al., 2006;
Collobert and Weston, 2008; Mikolov et al.,
2013a) have received much attention in the dis-
tributional semantics community, and have shown
state-of-the-art performance in many natural lan-
guage processing tasks. While they have been
compared with co-occurrence based models in
simple similarity tasks at the word level (Levy et
al., 2014; Baroni et al., 2014), we are aware of
only one work that attempts a comparison of the
two approaches in compositional settings (Blacoe
and Lapata, 2012), and this is limited to additive
and multiplicative composition, compared against
composition via a neural autoencoder.
The purpose of this paper is to provide a more
complete picture regarding the potential of neu-
ral word embeddings in compositional tasks, and
meaningfully compare them with the traditional
distributional approach based on co-occurrence
counts. We are especially interested in investi-
gating the performance of neural word vectors in
compositional models involving general mathe-
matical composition operators, rather than in the
more task- or domain-specific deep-learning com-
positional settings they have generally been used
with so far (for example, by Socher et al. (2012),
Kalchbrenner and Blunsom (2013) and many oth-
ers).
In particular, this is the first large-scale study
to date that applies neural word representations in
tensor-based compositional distributional models
of meaning similar to those formalized by Coecke
et al. (2010). We test a range of implementations
based on this framework, together with additive
and multiplicative approaches (Mitchell and Lap-
ata, 2008), in a variety of different tasks. Specif-
ically, we use the verb disambiguation task of
Grefenstette and Sadrzadeh (2011a) and the tran-
sitive sentence similarity task of Kartsaklis and
Sadrzadeh (2014) as small-scale focused experi-
ments on pre-defined sentence structures. Addi-
tionally, we evaluate our vector spaces on para-
phrase detection (using the Microsoft Research
Paraphrase Corpus of Dolan et al. (2005)) and di-
alogue act tagging using the Switchboard Corpus
(see e.g. (Stolcke et al., 2000)).
In all of the above tasks, we compare the neural
word embeddings of Mikolov et al. (2013a) with
two vector spaces both based on co-occurrence
counts and produced by standard distributional
techniques, as described in detail below. The gen-
eral picture we get from the results is that in almost
all cases the neural vectors are more effective than
the traditional approaches.
We proceed as follows: Section 2 provides a
concise introduction to distributional word repre-
sentations in natural language processing. Section
708
3 takes a closer look to the subject of composi-
tionality in vector space models of meaning and
describes the range of compositional operators ex-
amined here. In Section 4 we provide details about
the vector spaces used in the experiments. Our ex-
perimental work is described in detail in Section 5,
and the results are discussed in Section 6. Finally,
Section 7 provides conclusions.
2 Meaning representation
There are several approaches to the representation
of word, phrase and sentence meaning. As nat-
ural languages are highly creative and it is very
rare to see the same sentence twice, any practical
approach dealing with large text segments must
be compositional, constructing the meaning of
phrases and sentences from their constituent parts.
The ideal method would therefore express not
only the similarity in meaning between those con-
stituent parts, but also between the results of their
composition, and do this in ways which fit with
linguistic structure and generalisations thereof.
Formal semantics Formal approaches to the
semantics of natural language have long built
upon the classical idea of compositionality ?
that the meaning of a sentence is a function
of the meanings of its parts (Frege, 1892). In
compositional type-logical approaches, predicate-
argument structures representing phrases and sen-
tences are built from their constituent parts by ?-
reduction within the lambda calculus framework
(Montague, 1970): for example, given a represen-
tation of John as john
?
and sleeps as ?x.sleep
?
(x),
the meaning of the sentence ?John sleeps?
can be constructed as ?x.sleep
?
(x)(john
?
) =
sleep
?
(john
?
). Given a suitable pairing between
words and semantic representations of them, this
method can produce structured sentential repre-
sentations with broad coverage and good gener-
alisability (see e.g. (Bos, 2008)). The above logi-
cal approach is extremely powerful because it can
capture complex aspects of meaning such as quan-
tifiers and their interaction (see e.g. (Copestake et
al., 2005)), and enables inference using well stud-
ied and developed logical methods (see e.g. (Bos
and Gabsdil, 2000)).
Distributional hypothesis However, such for-
mal approaches are less able to express similar-
ity in meaning. We would like to capture the
intuition that while John and Mary are distinct,
they are rather similar to each other (both of them
are humans) and dissimilar to words such as dog,
pavement or idea. The same applies at the phrase
and sentence level: ?dogs chase cats? is similar in
meaning to ?hounds pursue kittens?, but less so to
?cats chase dogs? (despite the lexical overlap).
Distributional methods provide a way to address
this problem. By representing words and phrases
as vectors or tensors in a (usually highly dimen-
sional) vector space, one can express similarity
in meaning via a suitable distance metric within
that space (usually cosine distance); furthermore,
composition can be modelled via suitable linear-
algebraic operations.
Co-occurrence-based word representations
One way to produce such vectorial representa-
tions is to directly exploit Harris (1954)?s intuition
that semantically similar words tend to appear in
similar contexts. We can construct a vector space
in which the dimensions correspond to contexts,
usually taken to be words as well. The word
vector components can then be calculated from
the frequency with which a word has co-occurred
with the corresponding contexts in a window of
words, with a predefined length.
Table 1 shows 5 3-dimensional vectors for the
words Mary, John, girl, boy and idea. The words
philosophy, book and school signify vector space
dimensions. As the vector for John is closer to
Mary than it is to idea in the vector space?a di-
rect consequence of the fact that John?s contexts
are similar to Mary?s and dissimilar to idea?s?we
can infer that John is semantically more similar to
Mary than to idea.
Many variants of this approach exist: perfor-
mance on word similarity tasks has been shown
to be improved by replacing raw counts with
weighted values (e.g. mutual information)?see
(Turney et al., 2010) and below for discussion, and
(Kiela and Clark, 2014) for a detailed comparison.
philosophy book school
Mary 0 10 22
John 4 60 59
girl 0 19 93
boy 0 12 164
idea 10 47 39
Table 1: Word co-occurrence frequencies ex-
tracted from the BNC (Leech et al., 1994).
709
Neural word embeddings Deep learning tech-
niques exploit the distributional hypothesis dif-
ferently. Instead of relying on observed co-
occurrence frequencies, a neural language model
is trained to maximise some objective function re-
lated to e.g. the probability of observing the sur-
rounding words in some context (Mikolov et al.,
2013b):
1
T
T
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
) (1)
Optimizing the above function, for example, pro-
duces vectors which maximise the conditional
probability of observing words in a context around
the target word w
t
, where c is the size of the
training window, and w
1
w
2
, ? ? ?w
T
a sequence of
words forming a training instance. Therefore, the
resulting vectors will capture the distributional in-
tuition and can express degrees of lexical similar-
ity.
This method has an obvious advantage com-
pared to co-occurrence method: since now the
context is predicted, the model in principle can
be much more robust in data sparsity prob-
lems, which is always an important issue for co-
occurrence word spaces. Additionally, neural vec-
tors have also proven successful in other tasks
(Mikolov et al., 2013c), since they seem to en-
code not only attributional similarity (the degree to
which similar words are close to each other), but
also relational similarity (Turney, 2006). For ex-
ample, it is possible to extract the singular:plural
relation (apple:apples, car:cars) using vector sub-
traction:
????
apple ?
?????
apples ?
??
car ?
???
cars
Perhaps even more importantly, semantic relation-
ships are preserved in a very intuitive way:
???
king ?
???
man ?
????
queen ?
?????
woman
allowing the formation of analogy queries similar
to
???
king ?
???
man +
?????
woman = ?, obtaining
????
queen as
the result.
1
Both neural and co-occurrence-based ap-
proaches have advantages over classical formal
approaches in their ability to capture lexical se-
mantics and degrees of similarity; their success at
1
Levy et al. (2014) improved Mikolov et al. (2013c)?s
method of retrieving relational similarities by changing the
underlying objective function.
extending this to the sentence level and to more
complex semantic phenomena, though, depends
on their applicability within compositional mod-
els, which is the subject of the next section.
3 Compositional models
Compositional distributional models represent
meaning of a sequence of words by a vector, ob-
tained by combining meaning vectors of the words
within the sequence using some vector composi-
tion operation. In a general classification of these
models, one can distinguish between three broad
cases: simplistic models which combine word
vectors irrespective of their order or relation to one
another, models which exploit linear word order,
and models which use grammatical structure.
The first approach combines word vectors
by vector addition or point-wise multiplication
(Mitchell and Lapata, 2008)?as this is indepen-
dent of word order, it cannot capture the differ-
ence between the two sentences ?dogs chase cats?
and ?cats chase dogs?. The second approach has
generally been implemented using some form of
deep learning, and captures word order, but not by
necessarily caring about the grammatical structure
of the sentence. Here, one works by recursively
building and combining vectors for subsequences
of words within the sentence using e.g. autoen-
coders (Socher et al., 2012) or convolutional fil-
ters (Kalchbrenner et al., 2014). We do not con-
sider this approach in this paper. This is because,
as mentioned in the introduction, their vectors and
composition operators are task-specific. These are
trained directly to achieve specific objectives in
certain pre-determined tasks. We are interested
in vector and composition operators that work for
any compositional task, and which can be com-
bined with results in linguistics and formal se-
mantics to provide generalisable models that can
canonically extend to complex semantic phenom-
ena. The third (i.e. the grammatical) approach
promises a way to achieve this, and has been in-
stantiated in various ways in the work of Baroni
and Zamparelli (2010),Grefenstette and Sadrzadeh
(2011a), and Kartsaklis et al. (2012).
General framework Formally, we can spec-
ify the vector representation of a word sequence
w
1
w
2
? ? ?w
n
as the vector
??
s =
??
w
1
?
??
w
2
? ? ? ??
??
w
n
,
where ? is a vector operator, such as addition +,
point-wise multiplication , tensor product ?, or
matrix multiplication ?.
710
In the simplest compositional models (the first
approach described above), ? is + or , e.g. see
(Mitchell and Lapata, 2008). Grammar-based
compositional models (the third approach) are
based on a generalisation of the notion of vectors,
known as tensors. Whereas a vector
??
v is an ele-
ment of an atomic vector space V , a tensor z is an
element of a tensor space V ?W ? ? ? ? ? Z. The
number of tensored spaces is referred to by the or-
der of the space. Using a general duality theorem
from multi-linear algebra (Bourbaki, 1989), it fol-
lows that tensors are in one-one correspondence
with multi-linear maps, that is we have:
z ? V ?W?? ? ??Z
?
=
f
z
: V ?W ? ? ? ? ? Z
In such a tensor-based formalism, meanings of
nouns are vectors and meanings of predicates such
as adjectives and verbs are tensors. Meaning of a
string of words is obtained by applying the compo-
sitions of multi-linear map duals of the tensors to
the vectors. For the sake of demonstration, take
the case of an intransitive sentence ?Sbj Verb?;
the meaning of the subject is a vector
??
Sbj ? V
and the meaning of the intransitive verb is a ten-
sor Verb ? V ?W . Meaning of the sentence is
obtained by applying f
V erb
to
??
Sbj, as follows:
??????
Sbj Verb = f
V erb
(
??
Sbj)
By tensor-map duality, the above becomes
equivalent to the following, where composition
has now become the familiar notion of matrix mul-
tiplication, that is ? is ?:
Verb?
??
Sbj
In general and for words with tensors of order
higher than two, ? becomes a generalisation of ?,
referred to by tensor contraction, see e.g. Kartsak-
lis and Sadrzadeh (2013). Since the creation and
manipulation of tensors of order higher than 2 is
difficult, one can work with simplified versions of
tensors, faithful to their underlying mathematical
basis; these have found intuitive interpretations,
e.g. see Grefenstette and Sadrzadeh (2011a), Kart-
saklis and Sadrzadeh (2014). In such cases, ? be-
comes a combination of a range of operations such
as ?, ?, , and +.
Specific models In the current paper we will ex-
periment with a variety of models. In Table 2, we
present these models in terms of their composi-
tion operators and a reference to the main paper in
which each model was introduced. For the sim-
ple compositional models the sentence is a string
of any number of words; for the grammar-based
models, we consider simple transitive sentences
?Sbj Verb Obj? and introduce the following abbre-
viations for the concrete method used to build a
tensor for the verb:
1. Verb is a verb matrix computed using the for-
mula
?
i
???
Sbj
i
?
???
Obj
i
, where
???
Sbj
i
and
???
Obj
i
are
the subjects and objects of the verb across the
corpus. These models are referred to by rela-
tional (Grefenstette and Sadrzadeh, 2011a);
they are generalisations of predicate seman-
tics of transitive verbs, from pairs of individ-
uals to pairs of vectors. The models reduce
the order 3 tensor of a transitive verb to an
order 2 tensor (i.e. a matrix).
2.
?
Verb is a verb matrix computed using the for-
mula
???
Verb ?
???
Verb, where
???
Verb is the distri-
butional vector of the verb. These models are
referred to by Kronecker, which is the term
sometimes used to denote the outer prod-
uct of tensors (Grefenstette and Sadrzadeh,
2011b). This models also reduces the order
3 tensor of a transitive verb to an order 2 ten-
sor.
3. The models of the last five lines of the table
use the so-called Frobenius operators from
categorical compositional distributional se-
mantics (Kartsaklis et al., 2012) to expand
the relational matrices of verbs from order 2
to order 3. The expansion is obtained by ei-
ther copying the dimension of the subject into
the space provided by the third tensor, hence
referred to by Copy-Sbj, or copying the di-
mension of the object in that space, hence re-
ferred to by Copy-Obj; furthermore, we can
take addition, multiplication, or outer product
of these, which are referred to by Frobenius-
Add, Frobenius-Mult, and Frobenius-Outer
(Kartsaklis and Sadrzadeh, 2014).
4 Semantic word spaces
Co-occurrence-based vector space instantiations
have received a lot of attention from the scientific
community (refer to (Kiela and Clark, 2014; Pola-
jnar and Clark, 2014) for recent studies). We in-
stantiate two co-occurrence-based vectors spaces
with different underlying corpora and weighting
schemes.
711
Method Sentence Linear algebraic formula Reference
Addition w
1
w
2
? ? ?w
n
??
w
1
+
??
w
2
+ ? ? ?+
??
w
n
Mitchell and Lapata (2008)
Multiplication w
1
w
2
? ? ?w
n
??
w
1

??
w
2
 ? ? ? 
??
w
n
Mitchell and Lapata (2008)
Relational Sbj Verb Obj Verb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011a)
Kronecker Sbj Verb Obj V?erb (
??
Sbj?
??
Obj) Grefenstette and Sadrzadeh (2011b)
Copy object Sbj Verb Obj
??
Sbj (Verb?
??
Obj) Kartsaklis et al. (2012)
Copy subject Sbj Verb Obj
??
Obj (Verb
T
?
??
Sbj) Kartsaklis et al. (2012)
Frob. add. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) + (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. mult. Sbj Verb Obj (
??
Sbj (Verb?
??
Obj)) (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Frob. outer Sbj Verb Obj (
??
Sbj (Verb?
??
Obj))? (
??
Obj (Verb
T
?
??
Sbj)) Kartsaklis and Sadrzadeh (2014)
Table 2: Compositional methods.
GS11 Our first word space is based on a typ-
ical configuration that has been used in the past
extensively for compositional distributional mod-
els (see below for details), so it will serve as a
useful baseline for the current work. In this vec-
tor space, the co-occurrence counts are extracted
from the British National Corpus (BNC) (Leech et
al., 1994). As basis words, we use the most fre-
quent nouns, verbs, adjectives and adverbs (POS
tags SUBST, VERB, ADJ and ADV in the BNC
XML distribution
2
). The vector space is lemma-
tized, that is, it contains only ?canonical? forms of
words.
In order to weight the raw co-occurrence counts,
we use positive point-wise mutual information
(PPMI). The component value for a target word
t and a context word c is given by:
PPMI(t, c) = max
(
0, log
p(c|t)
p(c)
)
where p(c|t) is the probability of word c given t
in a symmetric window of length 5 and p(c) is the
probability of c overall.
Vector spaces based on point-wise mutual in-
formation (or variants thereof) have been success-
fully applied in various distributional and compo-
sitional tasks; see e.g. Grefenstette and Sadrzadeh
(2011a), Mitchell and Lapata (2008), Levy et al.
(2014) for details. PPMI has been shown to
achieve state-of-the-art results (Levy et al., 2014)
and is suggested by the review of Kiela and Clark
(2014). Our use here of the BNC as a corpus
and the window length of 5 is based on previ-
ous use and better performance of these param-
eters in a number of compositional experiments
(Grefenstette and Sadrzadeh, 2011a; Grefenstette
2
http://www.natcorp.ox.ac.uk/
and Sadrzadeh, 2011b; Mitchell and Lapata, 2008;
Kartsaklis et al., 2012).
KS14 In this variation, we train a vector space
from the ukWaC corpus
3
(Ferraresi et al., 2008),
originally using as a basis the 2,000 content words
with the highest frequency (but excluding a list of
stop words as well as the 50 most frequent content
words since they exhibit low information content).
The vector space is again lemmatized. As context
we consider a 5-word window from either side of
the target word, while as our weighting scheme we
use local mutual information (i.e. point-wise mu-
tual information multiplied by raw counts). In a
further step, the vector space was normalized and
projected onto a 300-dimensional space using sin-
gular value decomposition (SVD).
In general, dimensionality reduction produces
more compact word representations that are robust
against potential noise in the corpus (Landauer and
Dumais, 1997; Sch?utze, 1997). SVD has been
shown to perform well on a variety of tasks similar
to ours (Baroni and Zamparelli, 2010; Kartsaklis
and Sadrzadeh, 2014).
Neural word embeddings (NWE) For our neu-
ral setting, we used the skip-gram model of
Mikolov et al. (2013b) trained with negative sam-
pling. The specific implementation that was tested
in our experiments was a 300-dimensional vec-
tor space learned from the Google News corpus
and provided by the word2vec
4
toolkit. Fur-
thermore, the gensim library (
?
Reh?u?rek and So-
jka, 2010) was used for accessing the vectors.
On the contrary with the previously described co-
3
http://wacky.sslmit.unibo.it/
4
https://code.google.com/p/word2vec/
712
occurrence vector spaces, this version is not lem-
matized.
The negative sampling method improves the ob-
jective function of Equation 1 by introducing neg-
ative examples to the training algorithm. Assume
that the probability of a specific (c, t) pair of words
(where t is a target word and c another word in
the same context with t), coming from the training
data, is denoted as p(D = 1|c, t). The objective
function is then expressed as follows:
?
(c,t)?D
p(D = 1|c, t) (2)
That is, the goal is to set the model parameters in
a way that maximizes the probability of all obser-
vations coming from the training data. Assume
now that D
?
is a set of randomly selected incorrect
(c
?
, t
?
) pairs that do not occur in D, then Equation
2 above can be recasted in the following way:
?
(c,t)?D
p(D = 1|c, t)
?
(c
?
,t
?
)?D
?
p(D = 0|c
?
, t
?
)
(3)
In other words, the model tries to distinguish a tar-
get word t from random draws that come from a
noise distribution. In the implementation we used
for our experiments, c is always selected from
a 5-word window around t. More details about
the negative sampling approach can be found in
(Mikolov et al., 2013b); the note of Goldberg and
Levy (2014) also provides an intuitive explanation
of the underlying setting.
5 Experiments
Our experiments explore the use of the vector
spaces above, together with the compositional op-
erators described in Section 3, in a range of tasks
all of which require semantic composition: verb
sense disambiguation; sentence similarity; para-
phrasing; and dialogue act tagging.
5.1 Disambiguation
We use the transitive verb disambiguation dataset
described in Grefenstette and Sadrzadeh (2011a)
5
.
This dataset consists of ambiguous transitive verbs
together with their arguments, landmark verbs
that identify one of the verb senses, and human
judgements that specify how similar is the disam-
biguated sense of the verb in the given context to
5
This and the sentence similarity dataset are avail-
able at http://www.cs.ox.ac.uk/activities/
compdistmeaning/
one of the landmarks. This is similar to the in-
transitive dataset described in (Mitchell and Lap-
ata, 2008). Consider the sentence ?system meets
specification?; here, meets is the ambiguous tran-
sitive verb, and system and specification are its ar-
guments in this context. Possible landmarks for
meet are satisfy and visit; for this sentence, the
human judgements show that the disambiguated
meaning of the verb is more similar to the land-
mark satisfy and less similar to visit.
The task is to estimate the similarity of the sense
of a verb in a context with a given landmark. To
get our similarity measures, we compose the verb
with its arguments using one of our compositional
models; we do the same for the landmark and then
compute the cosine similarity of the two vectors.
We evaluate the performance by averaging the hu-
man judgements for the same verb, argument and
landmark entries, and calculating the Spearman?s
correlation between the average values and the co-
sine scores. As a baseline, we compare this with
the correlation produced by using only the verb
vector, without composing it with its arguments.
Table 3 shows the results of the experiment.
NWE copy-object composition yields the best cor-
relation with the human judgements, and top per-
formance across all vector spaces and models with
a Spearman ? of 0.456. For the KS14 space, the
best result comes from Frobenius outer (0.350),
Method GS11 KS14 NWE
Verb only 0.212 0.325 0.107
Addition 0.103 0.275 0.149
Multiplication 0.348 0.041 0.095
Kronecker 0.304 0.176 0.117
Relational 0.285 0.341 0.362
Copy subject 0.089 0.317 0.131
Copy object 0.334 0.331 0.456
Frobenius add. 0.261 0.344 0.359
Frobenius mult. 0.233 0.341 0.239
Frobenius outer 0.284 0.350 0.375
Table 3: Spearman ? correlations of models with
human judgements for the word sense disam-
biguation task. The best result (NWE Copy ob-
ject) outperforms the nearest co-occurrence-based
competitor (KS14 Frobenius outer) with a statisti-
cally significant difference (p < 0.05, t-test).
713
while the best operator for the GS11 space is
point-wise multiplication (0.348).
For simple point-wise composition, only mul-
tiplicative GS11 and additive NWE improve over
their corresponding verb-only baselines (but both
perform worse than the KS14 baseline). With
tensor-based composition in co-occurrence based
spaces, copy subject yields lower results than
the corresponding baselines. Other composition
methods, except Kronecker for KS14, improve
over the verb-only baselines. Finally we should
note that, despite the small training corpus, the
GS11 vector space performs comparatively well:
for instance, Kronecker model improves the pre-
viously reported score of 0.28 (Grefenstette and
Sadrzadeh, 2011b).
5.2 Sentence similarity
In this experiment we use the transitive sen-
tence similarity dataset described in Kartsaklis and
Sadrzadeh (2014). The dataset consists of transi-
tive sentence pairs and a human similarity judge-
ment
6
. The task is to estimate a similarity measure
between two sentences. As in the disambiguation
task, we first compose word vectors to obtain sen-
tence vectors, then compute cosine similarity of
them. We average the human judgements for iden-
tical sentence pairs to compute a correlation with
cosine scores.
Table 4 shows the results. Again, the best
performing vector space is KS14, but this time
with addition: the Spearman ? correlation score
with averaged human judgements is 0.732. Addi-
tion was the means for the other vector spaces to
achieve top performance as well: GS11 and NWE
got 0.682 and 0.689 respectively.
None of the models in tensor-based composi-
tion outperformed addition. KS14 performs worse
with tensor-based methods here than in the other
vector spaces. However, GS11 and NWE, except
copy subject for both of them and Frobenius multi-
plication for NWE, improved over their verb-only
baselines.
5.3 Paraphrasing
In this experiment we evaluate our vector spaces
on a mainstream paraphrase detection task.
6
The textual content of this dataset is the same as that of
(Kartsaklis and Sadrzadeh, 2013), the difference is that the
dataset of (Kartsaklis and Sadrzadeh, 2014) has updated hu-
man judgements whereas the previous dataset used the orig-
inal annotations of the intransitive dataset of (Mitchell and
Lapata, 2010).
Method GS11 KS14 NWE
Verb only 0.491 0.602 0.561
Addition 0.682 0.732 0.689
Multiplication 0.597 0.321 0.341
Kronecker 0.581 0.408 0.561
Relational 0.558 0.437 0.618
Copy subject 0.370 0.448 0.405
Copy object 0.571 0.306 0.655
Frobenius add. 0.566 0.460 0.585
Frobenius mult. 0.525 0.226 0.387
Frobenius outer 0.560 0.439 0.622
Table 4: Results for sentence similarity. There
is no statistically significant difference between
KS14 addition and NWE addition (the second best
result).
Specifically, we get classification results on the
Microsoft Research Paraphrase Corpus paraphrase
corpus (Dolan et al., 2005) working in the follow-
ing way: we construct vectors for the sentences
of each pair; if the cosine similarity between the
two sentence vectors exceeds a certain threshold,
the pair is classified as a paraphrase, otherwise as
not a paraphrase. For this experiment and that of
Section 5.4 below, we investigate only the addi-
tion and point-wise multiplication compositional
models, since at their current stage of development
tensor-based models can only efficiently handle
sentences of fixed structure. Nevertheless, the
simple point-wise compositional models still al-
low for a direct comparison of the vector spaces,
which is the main goal of this paper.
For each vector space and model, a number of
different thresholds were tested on the first 2000
pairs of the training set, which we used as a de-
velopment set; in each case, the best-performed
threshold was selected for a single run of our
?classifier? on the test set (1726 pairs). Addition-
ally, we evaluate the NWE model with a lemma-
tized version of the corpus, so that the experimen-
tal setup is maximally similar for all vector spaces.
The results are shown in the first part of Table 5.
Additive NWE gives the highest performance,
with both lemmatized and un-lemmatized versions
outperforming the GS11 and KS14 spaces. In
the un-lemmatized case, the accuracy of our sim-
ple ?classifier? (0.73) is close to state-of-the-art
range. The state-of-the art result (0.77 accuracy
714
Co-occurrence Neural word embeddings
Baseline GS11 KS14 Unlemmatized Lemmatized
Model Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score
MSR addition
0.65 0.75
0.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81
MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36
SWDA addition
0.60 0.58
0.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40
SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38
Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results
significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, ?
2
test.
and 0.84 F-score
7
) by the time of this writing has
been obtained using 8 machine translation metrics
and three constituent classifiers (Madnani et al.,
2012).
The multiplicative model gives lower results
than the additive model across all vector spaces.
The KS14 vector space shows the steadiest per-
formance, with a drop in accuracy of only 0.04
and no drop in F-score, while for the GS11 and
NWE spaces both accuracy and F-score experi-
enced drops by more than 0.20.
5.4 Dialogue act tagging
As our last experiment, we evaluate the word
spaces on a dialogue act tagging task (Stolcke et
al., 2000) over the Switchboard corpus (Godfrey
et al., 1992). Switchboard is a collection of ap-
proximately 2500 dialogs over a telephone line by
500 speakers from the U.S. on predefined topics.
8
The experiment pipeline follows (Milajevs and
Purver, 2014). The input utterances are prepro-
cessed so that the parts of interrupted utterances
are concatenated (Webb et al., 2005). Disfluency
markers and commas are removed from the utter-
ance raw texts. For GS11 and KS14 the utterance
tokens are POS-tagged and lemmatized; for NWE,
we test the vectors in both a lemmatized and an
un-lemmatized version of the corpus.
9
We split
the training and testing utterances as suggested by
Stolcke et al. (2000). Utterance vectors are then
obtained as in the previous experiments; they are
reduced to 50 dimensions using SVD and a k-
nearest-neighbour classifier is trained on these re-
duced utterance vectors (the 5 closest neighbours
by Euclidean distance are retrieved to make a clas-
7
F-scores use the standard definition F = 2(precision ?
recall)/(precision + recall).
8
The dataset and a Python interface to it are available
at http://compprag.christopherpotts.net/
swda.html
9
We use WordNetLemmatizer of the NLTK library
(Bird, 2006).
sification decision). The results are shown in the
second part of Table 5.
Un-lemmatized NWE addition gave the best ac-
curacy (0.63) and F-score (0.60) (averaged over
tag classes), i.e. similar results to (Milajevs and
Purver, 2014)?although note that the dimension-
ality of our NWE vectors is 10 times lower than
theirs. Multiplicative NWE outperformed the cor-
responding model in (Milajevs and Purver, 2014).
In general, addition consistently outperforms mul-
tiplication for all the models. Lemmatization
dramatically lowers tagging accuracy: the lem-
matized GS11, KS14 and NWE models perform
much worse than un-lemmatized NWE, suggest-
ing that morphological features are important for
this task.
6 Discussion
Previous comparisons of co-occurrence-based and
neural word vector representations vary widely
in their conclusions. While Baroni et al. (2014)
conclude that ?context-predicting models obtain
a thorough and resounding victory against their
count-based counterparts?, this seems to contra-
dict, at least at the first consideration, the more
conservative conclusion of Levy et al. (2014) that
?analogy recovery is not restricted to neural word
embeddings [. . . ] a similar amount of relational
similarities can be recovered from traditional dis-
tributional word representations? and the findings
of Blacoe and Lapata (2012) that ?shallow ap-
proaches are as good as more computationally in-
tensive alternatives? on phrase similarity and para-
phrase detection tasks.
It seems clear that neural word embeddings
have an advantage when used in tasks for which
they have been trained; our main questions here
are whether they outperform co-occurrence based
alternatives across the board; and which ap-
proach lends itself better to composition using
general mathematical operators. To partially an-
715
swer this question, we can compare model be-
haviour against the baselines in isolation.
For the disambiguation and sentence similarity
tasks the baseline is the similarity between verbs
only, ignoring the context?see above. For the
paraphrase task, we take the global vector-based
similarity reported in (Mihalcea et al., 2006): 0.65
accuracy and 0.75 F-score. For the dialogue act
tagging task the baseline is the accuracy of the
bag-of-unigrams model in (Milajevs and Purver,
2014): 0.60.
Sections 5.1 and 5.2 show that although the best
choice of vector representation might vary, for
small-scale tasks all methods give fairly compet-
itive results. The choice of compositional oper-
ator seems to be more important and more task-
specific: while a tensor-based operation (Frobe-
nius copy-object) performs best for verb disam-
biguation, the best result for sentence similarity
is achieved by a simple additive model, with all
other compositional methods behaving worse than
the verb-only baseline in the KS14 case. GS11 and
NWE, on the other hand, outperform their base-
lines with a number of compositional methods, al-
though both of them achieve lower performance
than KS14 overall.
Based on only small-scale experiment results,
one could conclude that there is little significant
difference between the two ways of obtaining vec-
tors. GS11 and NWE show similar behaviour in
comparison to their baselines, while it is possible
to tune a co-occurrence based vector space (KS14)
and obtain the best result. Large scale tasks reveal
another pattern: the GS11 vector space, which be-
haves stably on the small scale, drags behind the
KS14 and NWE spaces in the paraphrase detec-
tion task. In addition, NWE consistently yields
best results. Finally, only the NWE space was able
to provide adequate results on the dialogue act tag-
ging task. Table 6 summarizes model performance
with regard to baselines.
7 Conclusion
In this work we compared the performance of two
co-occurrence-based semantic spaces with vectors
learned by a neural network in compositional set-
tings. We carried out two small-scale tasks (word
sense disambiguation and sentence similarity) and
two large-scale tasks (paraphrase detection and di-
alogue act tagging).
Task GS11 KS14 NWE
Disambiguation + + +
Sentence similarity + ? +
Paraphrase ? + +
Dialog act tagging ? ? +
Table 6: Summary of vector space performance
against baselines. General improvement (cases
where more than a half of the models perform bet-
ter) and decrease with regard to a corresponding
baseline is respectively marked by + and ?. A
bold value means that the model gave the best re-
sult in the task.
On small-scale tasks, where the sentence struc-
tures are predefined and relatively constrained,
NWE gives better or similar results to count-based
vectors. Tensor-based composition does not al-
ways outperform simple compositional operators,
but for most of the cases gives results within the
same range.
On large-scale tasks, neural vectors are more
successful than the co-occurrence based alterna-
tives. However, this study does not reveal whether
this is because of their neural nature, or just be-
cause they are trained on a larger amount of data.
The question of whether neural vectors outper-
form co-occurrence vectors therefore requires fur-
ther detailed comparison to be entirely resolved;
our experiments suggest that this is indeed the case
in large-scale tasks, but the difference in size and
nature of the original corpora may be a confound-
ing factor. In any case, it is clear that the neural
vectors of word2vec package perform steadily
off-the-shelf across a large variety of tasks. The
size of the vector space (3 million words) and the
available code-base that simplifies the access to
the vectors, makes this set a good and safe choice
for experiments in the future. Of course, even bet-
ter performances can be achieved by training neu-
ral language models specifically for a given task
(see e.g. Kalchbrenner et al. (2014)).
The choice of compositional operator (tensor-
based or a simple point-wise operation) depends
strongly on the task and dataset: tensor-based
composition performed best with the verb dis-
ambiguation task, where the verb senses depend
strongly on the arguments of the verb. However, it
seems to depend less on the nature of the vectors
itself: in the disambiguation task, tensor-based
716
composition proved best for both co-occurrence-
based and neural vectors; in the sentence similar-
ity task, where point-wise operators proved best,
this was again true across vector spaces.
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Sup-
port by EPSRC grant EP/F042728/1 is grate-
fully acknowledged by Milajevs, Kartsaklis and
Sadrzadeh. Purver is partly supported by Con-
CreTe: the project ConCreTe acknowledges the fi-
nancial support of the Future and Emerging Tech-
nologies (FET) programme within the Seventh
Framework Programme for Research of the Eu-
ropean Commission, under FET grant number
611733.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69?72. Asso-
ciation for Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546?556. Association for Compu-
tational Linguistics.
Johan Bos and Malte Gabsdil. 2000. First-order infer-
ence and the interpretation of questions and answers.
Proceedings of Gotelog, pages 43?50.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
N. Bourbaki. 1989. Commutative Algebra: Chapters
1-7. Srpinger Verlag, Berlin/New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2-3):281?332.
Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-
crosoft research paraphrase corpus. Retrieved May,
29:2013.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563?584.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Yoav Goldberg and Omer Levy. 2014. word2vec
Explained: deriving Mikolov et al.?s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404.
Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of the GEMS 2011 Work-
shop on GEometrical Models of Natural Language
Semantics, pages 62?66, Edinburgh, UK, July. As-
sociation for Computational Linguistics.
Z.S. Harris. 1954. Distributional structure. Word.
717
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNL), pages 1590?1601, Seat-
tle, USA, October. Association for Computational
Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL), Kyoto,
Japan, June.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING 2012: Posters, pages 549?558, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21?30, Gothenburg, Sweden, April.
Association for Computational Linguistics.
T. Landauer and S. Dumais. 1997. A Solution
to Plato?s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representation
of Knowledge. Psychological Review.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182?190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC),
pages 40?47, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36(3):373?398.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230?
238, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.
Hinrich Sch?utze. 1997. Ambiguity resolution in natu-
ral language learning. csli. Stanford, CA, 4:12?36.
718
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
719
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 212?217,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Resolving Lexical Ambiguity in
Tensor Regression Models of Meaning
Dimitri Kartsaklis
University of Oxford
Department of
Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Nal Kalchbrenner
University of Oxford
Department of
Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
nkalch@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary Univ. of London
School of Electronic Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrnoosh.sadrzadeh@qmul.ac.uk
Abstract
This paper provides a method for improv-
ing tensor-based compositional distribu-
tional models of meaning by the addition
of an explicit disambiguation step prior to
composition. In contrast with previous re-
search where this hypothesis has been suc-
cessfully tested against relatively simple
compositional models, in our work we use
a robust model trained with linear regres-
sion. The results we get in two experi-
ments show the superiority of the prior dis-
ambiguation method and suggest that the
effectiveness of this approach is model-
independent.
1 Introduction
The provision of compositionality in distributional
models of meaning, where a word is represented as
a vector of co-occurrence counts with every other
word in the vocabulary, offers a solution to the
fact that no text corpus, regardless of its size, is
capable of providing reliable co-occurrence statis-
tics for anything but very short text constituents.
By composing the vectors for the words within
a sentence, we are still able to create a vectorial
representation for that sentence that is very useful
in a variety of natural language processing tasks,
such as paraphrase detection, sentiment analysis
or machine translation. Hence, given a sentence
w
1
w
2
. . . w
n
, a compositional distributional model
provides a function f such that:
??
s = f(
??
w
1
,
??
w
2
, . . . ,
??
w
n
) (1)
where
??
w
i
is the distributional vector of the ith
word in the sentence and
??
s the resulting compos-
ite sentential vector.
An interesting question that has attracted the at-
tention of researchers lately refers to the way in
which these models affect ambiguous words; in
other words, given a sentence such as ?a man was
waiting by the bank?, we are interested to know to
what extent a composite vector can appropriately
reflect the intended use of word ?bank? in that con-
text, and how such a vector would differ, for exam-
ple, from the vector of the sentence ?a fisherman
was waiting by the bank?.
Recent experimental evidence (Reddy et al,
2011; Kartsaklis et al, 2013; Kartsaklis and
Sadrzadeh, 2013) suggests that for a number of
compositional models the introduction of a dis-
ambiguation step prior to the actual composi-
tional process results in better composite represen-
tations. In other words, the suggestion is that Eq.
1 should be replaced by:
??
s = f(?(
??
w
1
), ?(
??
w
2
), . . . , ?(
??
w
n
)) (2)
where the purpose of function ? is to return a dis-
ambiguated version of each word vector given the
rest of the context (e.g. all the other words in the
sentence). The composition operation, whatever
that could be, is then applied on these unambigu-
ous representations of the words, instead of the
original distributional vectors.
Until now this idea has been verified on rela-
tively simple compositional functions, usually in-
volving some form of element-wise operation be-
tween the word vectors, such as addition or mul-
tiplication. An exception to this is the work of
Kartsaklis and Sadrzadeh (2013), who apply Eq.
2 on partial tensor-based compositional models.
In a tensor-based model, relational words such
as verbs and adjectives are represented by multi-
linear maps; composition takes place as the ap-
plication of those maps on vectors representing
the arguments (usually nouns). What makes the
models of the above work ?partial? is that the au-
thors used simplified versions of the linear maps,
projected onto spaces of order lower than that re-
quired by the theoretical framework. As a result,
a certain amount of transformational power was
traded off for efficiency.
A potential explanation then for the effective-
ness of the proposed prior disambiguation method
can be sought on the limitations imposed by the
compositional models under test. After all, the
idea of having disambiguation emerge as a direct
212
consequence of the compositional process, with-
out the introduction of any explicit step, seems
more natural and closer to the way the human
mind resolves lexical ambiguities.
The purpose of this paper is to investigate
the hypothesis whether prior disambiguation is
important in a pure tensor-based compositional
model, where no simplifying assumptions have
been made. We create such a model by using lin-
ear regression, and we explain how an explicit dis-
ambiguation step can be introduced to this model
prior to composition. We then proceed by com-
paring the composite vectors produced by this ap-
proach with those produced by the model alone in
a number of experiments. The results show a clear
superiority of the priorly disambiguated models
following Eq. 2, confirming previous research and
suggesting that the reasons behind the success of
this approach are more fundamental than the form
of the compositional function.
2 Composition in distributional models
Compositional distributional models of meaning
vary in sophistication, from simple element-wise
operations between vectors such as addition and
multiplication (Mitchell and Lapata, 2008) to deep
learning techniques based on neural networks
(Socher et al, 2011; Socher et al, 2012; Kalch-
brenner and Blunsom, 2013a). Tensor-based mod-
els, formalized by Coecke et al (2010), comprise
a third class of models lying somewhere in be-
tween these two extremes. Under this setting rela-
tional words such as verbs and adjectives are rep-
resented by multi-linear maps (tensors of various
orders) acting on a number of arguments. An ad-
jective for example is a linear map f : N ? N
(where N is our basic vector space for nouns),
which takes as input a noun and returns a mod-
ified version of it. Since every map of this sort
can be represented by a matrix living in the ten-
sor product space N ? N , we now see that the
meaning of a phrase such as ?red car? is given by
red ?
??
car, where red is an adjective matrix and
? indicates matrix multiplication. The same con-
cept applies for functions of higher order, such as
a transitive verb (a function of two arguments, so
a tensor of order 3). For these cases, matrix mul-
tiplication generalizes to the more generic notion
of tensor contraction. The meaning of a sentence
such as ?kids play games? is computed as:
???
kids
T
? play ?
?????
games (3)
where play here is an order-3 tensor (a ?cube?)
and ? now represents tensor contraction. A con-
cise introduction to compositional distributional
models can be found in (Kartsaklis, 2014).
3 Disambiguation and composition
The idea of separating disambiguation from com-
position first appears in a work of Reddy et al
(2011), where the authors show that the intro-
duction of an explicit disambiguation step prior
to simple element-wise composition is beneficial
for noun-noun compounds. Subsequent work by
Kartsaklis et al (2013) reports very similar find-
ings for verb-object structures, again on additive
and multiplicative models. Finally, in (Kartsaklis
and Sadrzadeh, 2013) these experiments were ex-
tended to include tensor-based models following
the categorical framework of Coecke et al (2010),
where again all ?unambiguous? models present
superior performance compared to their ?ambigu-
ous? versions.
However, in this last work one of the dimen-
sions of the tensors was kept empty (filled in
with zeros). This simplified the calculations but
also weakened the effectiveness of the multi-linear
maps. If, for example, instead of using an order-3
tensor for a transitive verb, one uses some of the
matrix instantiations of Kartsaklis and Sadrzadeh,
Eq. 3 is reduced to one of the following forms:
play  (
???
kids?
?????
games) ,
???
kids (play ?
?????
games)
(
???
kids
T
? play)
?????
games
(4)
where symbol  denotes element-wise multipli-
cation and play is a matrix. Here, the model does
not fully exploit the space provided by the theo-
retical framework (i.e. an order-3 tensor), which
has two disadvantages: firstly, we lose space that
could hold valuable information about the verb in
this case and relational words in general; secondly,
the generally non-commutative tensor contraction
operation is now partly relying on element-wise
multiplication, which is commutative, thus forgets
(part of the) order of composition.
In the next section we will see how to apply lin-
ear regression in order to create full tensors for
verbs and use them for a compositional model that
avoids these pitfalls.
4 Creating tensors for verbs
The essence of any tensor-based compositional
model is the way we choose to create our sentence-
producing maps, i.e. the verbs. In this paper we
adopt a method proposed by Baroni and Zampar-
elli (2010) for building adjective matrices, which
can be generally applied to any relational word.
213
In order to create a matrix for, say, the intransi-
tive verb ?play?, we first collect all instances of
the verb occurring with some subject in the train-
ing corpus, and then we create non-compositional
holistic vectors for these elementary sentences fol-
lowing exactly the same methodology as if they
were words. We now have a dataset with instances
of the form ?
????
subj
i
,
???????
subj
i
play? (e.g. the vector of
?kids? paired with the holistic vector of ?kids play?,
and so on), that can be used to train a linear regres-
sion model in order to produce an appropriate ma-
trix for verb ?play?. The premise of a model like
this is that the multiplication of the verb matrix
with the vector of a new subject will produce a re-
sult that approximates the distributional behaviour
of all these elementary two-word exemplars used
in training.
We present examples and experiments based
on this method, constructing ambiguous and dis-
ambiguated tensors of order 2 (that is, matrices)
for verbs taking one argument. In principle, our
method is directly applicable to tensors of higher
order, following a multi-step process similar to
that of Grefenstette et al (2013) who create order-
3 tensors for transitive verbs using similar means.
Instead of using subject-verb constructs as above
we concentrate on elementary verb phrases of the
form verb-object (e.g. ?play football?, ?admit stu-
dent?), since in general objects comprise stronger
contexts for disambiguating the usage of a verb.
5 Experimental setting
Our basic vector space is trained from the ukWaC
corpus (Ferraresi et al, 2008), originally using as
a basis the 2,000 content words with the highest
frequency (but excluding a list of stop words as
well as the 50 most frequent content words since
they exhibit low information content). We cre-
ated vectors for all content words with at least
100 occurrences in the corpus. As context we
considered a 5-word window from either side of
the target word, while as our weighting scheme
we used local mutual information (i.e. point-wise
mutual information multiplied by raw counts).
This initial semantic space achieved a score of
0.77 Spearman?s ? (and 0.71 Pearson?s r) on the
well-known benchmark dataset of Rubenstein and
Goodenough (1965). In order to reduce the time of
regression training, our vector space was normal-
ized and projected onto a 300-dimensional space
using singular value decomposition (SVD). The
performance of the reduced space on the R&G
dataset was again very satisfying, specifically 0.73
Spearman?s ? and 0.72 Pearson?s r.
In order to create the vector space of the holistic
verb phrase vectors, we first collected all instances
where a verb participating in the experiments ap-
peared at least 100 times in a verb-object relation-
ship with some noun in the corpus. As context of
a verb phrase we considered any content word that
falls into a 5-word window from either side of the
verb or the object. For the 68 verbs participating
in our experiments, this procedure resulted in 22k
verb phrases, a vector space that again was pro-
jected into 300 dimensions using SVD.
Linear regression For each verb we use simple
linear regression with gradient descent directly ap-
plied on matrices X and Y, where the rows of X
correspond to vectors of the nouns that appear as
objects for the given verb and the rows ofY to the
holistic vectors of the corresponding verb phrases.
Our objective function then becomes:
?
W = argmin
W
1
2m
(
?WX
T
?Y
T
?
2
+ ??W?
2
)
(5)
wherem is the number of training examples and ?
a regularization parameter. The matrix W is used
as the tensor for the specific verb.
6 Supervised disambiguation
In our first experiment we test the effectiveness
of a prior disambiguation step for a tensor-based
model in a ?sandbox? using supervised learning.
The goal is to create composite vectors for a num-
ber of elementary verb phrases of the form verb-
object with and without an explicit disambiguation
step, and evaluate which model approximates bet-
ter the holistic vectors of these verb phrases.
The verb phrases of our dataset are based on the
5 ambiguous verbs of Table 1. Each verb has been
combined with two different sets of nouns that ap-
pear in a verb-object relationship with that verb
in the corpus (a total of 343 verb phrases). The
nouns of each set have been manually selected in
order to explicitly represent a different meaning of
the verb. As an example, in the verb ?play? we im-
pose the two distinct meanings of using a musical
instrument and participating in a sport; so the first
Verb Meaning 1 Meaning 2
break violate (56) break (22)
catch capture (28) be on time (21)
play musical instrument (47) sports (29)
admit permit to enter (12) acknowledge (25)
draw attract (64) sketch (39)
Table 1: Ambiguous verbs for the supervised task.
The numbers in parentheses refer to the collected
training examples for each case.
214
set of objects contains nouns such as ?oboe?, ?pi-
ano?, ?guitar?, and so on, while in the second set
we see nouns such as ?football?, ?baseball? etc.
In more detail, the creation of the dataset was
done in the following way: First, all verb entries
with more than one definition in the Oxford Junior
Dictionary (Sansome et al, 2000) were collected
into a list. Next, a linguist (native speaker of En-
glish) annotated the semantic difference between
the definitions of each verb in a scale from 1 (sim-
ilar) to 5 (distinct). Only verbs with definitions
exhibiting completely distinct meanings (marked
with 5) were kept for the next step. For each one
of these verbs, a list was constructed with all the
nouns that appear at least 50 times under a verb-
object relationship in the corpus with the specific
verb. Then, each object in the list was manually
annotated as exclusively belonging to one of the
two senses; so, an object could be selected only if
it was related to a single sense, but not both. For
example, ?attention? was a valid object for the at-
tract sense of verb ?draw?, since it is unrelated to
the sketch sense of that verb. On the other hand,
?car? is not an appropriate object for either sense
of ?draw?, since it could actually appear under both
of them in different contexts. The verbs of Table
1 were the ones with the highest numbers of ex-
emplars per sense, creating a dataset of significant
size for the intended task (each holistic vector is
compared with 343 composite vectors).
We proceed as follows: We apply linear regres-
sion in order to train verb matrices using jointly
the object sets for both meanings of each verb, as
well as separately?so in this latter case we get
two matrices for each verb, one for each sense. For
each verb phrase, we create a composite vector by
matrix-multiplying the verb matrix with the vector
of the specific object. Then we use 4-fold cross
validation to evaluate which version of composite
vectors (the one created by the ambiguous tensors
or the one created by the unambiguous ones) ap-
proximates better the holistic vectors of the verb
phrases in our test set. This is done by comparing
each holistic vector with all the composite ones,
and then evaluating the rank of the correct com-
posite vector within the list of results.
In order to get a proper mixing of objects from
both senses of a verb in training and testing sets,
we set the cross-validation process as follows: We
first split both sets of objects in 4 parts. For each
fold then, our training set is comprised by
3
4
of set
#1 plus
3
4
of set #2, while the test set consists of
the remaining
1
4
of set #1 plus
1
4
of set #2. The
data points of the training set are presented in the
Accuracy MRR Avg Sim
Amb. Dis. Amb. Dis. Amb. Dis.
break 0.19 0.28 0.41 0.50 0.41 0.43
catch 0.35 0.37 0.58 0.61 0.51 0.57
play 0.20 0.28 0.41 0.49 0.60 0.68
admit 0.33 0.43 0.57 0.64 0.41 0.46
draw 0.24 0.29 0.45 0.51 0.40 0.44
Table 2: Results for the supervised task. ?Amb.?
refers to models without the explicit disambigua-
tion step, and ?Dis.? to models with that step.
learning algorithm in random order.
We measure approximation in three different
metrics. The first one, accuracy, is the strictest,
and evaluates in how many cases the composite
vector of a verb phrase is the closest one (the first
one in the result list) to the corresponding holistic
vector. A more relaxed and perhaps more repre-
sentative method is to calculate the mean recipro-
cal rank (MRR), which is given by:
MRR =
1
m
m
?
i=1
1
rank
i
(6)
where m is the number of objects and rank
i
refers
to the rank of the correct composite vector for the
ith object.
Finally, a third way to evaluate the efficiency of
each model is to simply calculate the average co-
sine similarity between every holistic vector and
its corresponding composite vector. The results
are presented in Table 2, reflecting a clear supe-
riority (p < 0.001 for average cosine similarity)
of the prior disambiguation method for every verb
and every metric.
7 Unsupervised disambiguation
In Section 6 we used a controlled procedure to col-
lect genuinely ambiguous verbs and we trained our
models from manually annotated data. In this sec-
tion we briefly outline how the process of creat-
ing tensors for distinct senses of a verb can be au-
tomated, and we test this idea on a generic verb
phrase similarity task.
First, we use unsupervised learning in order to
detect the latent senses of each verb in the corpus,
following a procedure first described by Sch?utze
(1998). For every occurrence of the verb, we cre-
ate a vector representing the surrounding context
by averaging the vectors of every other word in
the same sentence. Then, we apply hierarchical
agglomerative clustering (HAC) in order to cluster
these context vectors, hoping that different groups
of contexts will correspond to the different senses
under which the word has been used in the corpus.
The clustering algorithm uses Ward?s method as
215
inter-cluster measure, and Pearson correlation for
measuring the distance of vectors within a clus-
ter. Since HAC returns a dendrogram embedding
all possible groupings, we measure the quality of
each partitioning by using the variance ratio crite-
rion (Cali?nski and Harabasz, 1974) and we select
the partitioning that achieves the best score (so the
number of senses varies from verb to verb).
The next step is to classify every noun that has
been used as an object with that verb to the most
probable verb sense, and then use these sets of
nouns as before for training tensors for the vari-
ous verb senses. Being equipped with a number of
sense clusters created as above for every verb, the
classification of each object to a relevant sense is
based on the cosine distance of the object vector
from the centroids of the clusters.
1
Every sense
with less than 3 training exemplars is merged to
the dominant sense of the verb. The union of all
object sets is used for training a single unambigu-
ous tensor for the verb. As usual, data points are
presented to learning algorithm in random order.
No objects in our test set are used for training.
We test this system on a verb phase similarity
task introduced in (Mitchell and Lapata, 2010).
The goal is to assess the similarity between pairs
of short verb phrases (verb-object constructs) and
evaluate the results against human annotations.
The dataset consists of 72 verb phrases, paired
in three different ways to form groups of various
degrees of phrase similarity?a total of 108 verb
phrase pairs.
The experiment has the following form: For ev-
ery pair of verb phrases, we construct composite
vectors and then we evaluate their cosine similar-
ity. For the ambiguous regression model, the com-
position is done by matrix-multiplying the am-
biguous verb matrix (learned by the union of all
object sets) with the vector of the noun. For the
disambiguated version, we first detect the most
probable sense of the verb given the noun, again
by comparing the vector of the noun with the
centroids of the verb clusters; then, we matrix-
multiply the corresponding unambiguous tensor
created exclusively from objects that have been
classified as closer to this specific sense of the
verb with the noun. We also test a number
of baselines: the ?verbs-only? model is a non-
compositional baseline where only the two verbs
are compared; ?additive? and ?multiplicative? com-
pose the word vectors of each phrase by applying
simple element-wise operations.
1
In general, our approach is quite close to the multi-
prototype models of Reisinger and Mooney (2010).
Model Spearman?s ?
Verbs-only 0.331
Additive 0.379
Multiplicative 0.301
Linear regression (ambiguous) 0.349
Linear regression (disamb.) 0.399
Holistic verb phrase vectors 0.403
Human agreement 0.550
Table 3: Results for the phrase similarity task. The
difference between the ambiguous and the disam-
biguated version is s.s. with p < 0.001.
The results are presented in Table 3, where
again the version with the prior disambiguation
step shows performance superior to that of the am-
biguous version. There are two interesting obser-
vations that can be made on the basis of Table
3. First of all, the regression model is based on
the assumption that the holistic vectors of the ex-
emplar verb phrases follow an ideal distributional
behaviour that the model aims to approximate as
close as possible. The results of Table 3 confirm
this: using just the holistic vectors of the corre-
sponding verb phrases (no composition is involved
here) returns the best correlation with human an-
notations (0.403), providing a proof that the holis-
tic vectors of the verb phrases are indeed reli-
able representations of each verb phrase?s mean-
ing. Next, observe that the prior disambiguation
model approximates this behaviour very closely
(0.399) on unseen data, with a difference not sta-
tistically significant. This is very important, since
a regression model can only perform as well as its
training dataset alows it; and in our case this is
achieved to a very satisfactory level.
8 Conclusion and future work
This paper adds to existing evidence from previ-
ous research that the introduction of an explicit
disambiguation step before the composition im-
proves the quality of the produced composed rep-
resentations. The use of a robust regression model
rejects the hypothesis that the proposed methodol-
ogy is helpful only for relatively ?weak? composi-
tional approaches. As for future work, an interest-
ing direction would be to see how a prior disam-
biguation step can affect deep learning composi-
tional settings similar to (Socher et al, 2012) and
(Kalchbrenner and Blunsom, 2013b).
Acknowledgements
We would like to thank the three anonymous
reviewers for their fruitful comments. Support
by EPSRC grant EP/F042728/1 is gratefully ac-
knowledged by D. Kartsaklis and M. Sadrzadeh.
216
References
M. Baroni and R. Zamparelli. 2010. Nouns are Vec-
tors, Adjectives are Matrices. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
T. Cali?nski and J. Harabasz. 1974. A Dendrite Method
for Cluster Analysis. Communications in Statistics-
Theory and Methods, 3(1):1?27.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for Distributed Compositional
Model of Meaning. Lambek Festschrift. Linguistic
Analysis, 36:345?384.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47?54.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
N. Kalchbrenner and P. Blunsom. 2013a. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Compo-
sitionality, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA, October. Association for Computational Lin-
guistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2013.
Separating Disambiguation from Composition in
Distributional Semantics. In Proceedings of 17th
Conference on Computational Natural Language
Learning (CoNLL-2013), Sofia, Bulgaria, August.
Dimitri Kartsaklis. 2014. Compositional operators in
distributional semantics. Springer Science Reviews,
April. DOI: 10.1007/s40362-014-0017-z.
J. Mitchell and M. Lapata. 2008. Vector-based Mod-
els of Semantic Composition. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 705?713.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109?117. Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of
the ACM, 8(10):627?633.
R. Sansome, D. Reid, and A. Spooner. 2000. The Ox-
ford Junior Dictionary. Oxford University Press.
H. Sch?utze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:97?123.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and
C.D. Manning. 2011. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. Advances in Neural Information Processing
Systems, 24.
R. Socher, B. Huval, C. Manning, and Ng. A.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Conference on Empirical
Methods in Natural Language Processing 2012.
217
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 114?123,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Separating Disambiguation from Composition
in Distributional Semantics
Dimitri Kartsaklis
University of Oxford
Dept of Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
dimitri.kartsaklis@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary Univ. of London
School of Electr. Engineering
and Computer Science
Mile End Road
London, E1 4NS, UK
mehrs@eecs.qmul.ac.uk
Stephen Pulman
University of Oxford
Dept of Computer Science
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
stephen.pulman@cs.ox.ac.uk
Abstract
Most compositional-distributional models
of meaning are based on ambiguous vec-
tor representations, where all the senses
of a word are fused into the same vec-
tor. This paper provides evidence that the
addition of a vector disambiguation step
prior to the actual composition would be
beneficial to the whole process, produc-
ing better composite representations. Fur-
thermore, we relate this issue with the
current evaluation practice, showing that
disambiguation-based tasks cannot reli-
ably assess the quality of composition. Us-
ing a word sense disambiguation scheme
based on the generic procedure of Sch?tze
(1998), we first provide a proof of con-
cept for the necessity of separating dis-
ambiguation from composition. Then we
demonstrate the benefits of an ?unambigu-
ous? system on a composition-only task.
1 Introduction
Compositional and distributional semantic mod-
els seem to provide complementary solutions for
solving the same problem, that of assigning a
proper ?meaning? to a text segment. Specifically,
while compositional models deal with the recur-
sive nature of the language, providing a way to
address its inherent ability to create infinite sen-
tences from finite resources (words), they leave
words as unexplained primitives whose meanings
have somehow already been set before the compo-
sitional process. On the other hand, distributional
models have been especially successful in provid-
ing concrete representations for the meaning of
words as vectors in a vector space, created by tak-
ing into account the context in which each word
appears. Despite its success for smaller language
units, the distributional hypothesis does not natu-
rally lend itself to compounds of words. Hence
these models do not canonically scale in tasks re-
quiring the creation of vector representations for
text constituents larger than words, i.e. for phrases
and sentences.
Given the complementary nature of those two
semantic models, it is not surprising that consider-
able research activity has been dedicated on com-
bining them into a single framework that would
benefit from the best of both worlds in a uni-
fied manner: Mitchell and Lapata (2008) exper-
iment with intransitive sentences, applying sim-
ple compositional models based on vector ad-
dition and point-wise multiplication in a disam-
biguation task; Baroni and Zamparelli (2010) and
Guevara (2010) use regression models in order to
build vectors for adjective-noun compounds; Erk
and Pad? (2008) work on transitive sentences us-
ing structured vector spaces; Socher et al (2010,
2011, 2012) use neural networks to combine vec-
tors following the grammatical structure; Grefen-
stette and Sadrzadeh (2011a,b) apply the categori-
cal framework of Coecke et al (2010) on the dis-
ambiguation task of Mitchell and Lapata (2008);
and Kartsaklis et al (2012) and Grefenstette et al
(2013) build upon previous implementations by
adding specific algebraic operations and machine
learning techniques to further improve the con-
crete abilities of the abstract categorical models.
A common strand in all of the above models is
that they are based on ?ambiguous? vector rep-
resentations, where a polysemous word is repre-
sented by a single vector regardless of the number
of its actual senses. For example, the word ?bank?
has at least two meanings (financial institution and
land alongside a river), both of which will be fused
into a single vector representation. And, although
it is generally true that compositional models fol-
lowing the formal semantics view of Montague do
not care about disambiguation (meanings of words
in such models are represented by logical con-
stants explicitly set before the compositional pro-
cess), the story changes when one moves to a vec-
tor space model with ambiguous vector represen-
tations. The main problem is that, when acting on
ambiguous vector spaces, compositional models
114
seem to perform two tasks at the same time, com-
position and disambiguation, leaving the resulting
vector hard to interpret: it is not clear if this vector
is a proper meaning representation for the com-
posed compound or just a disambiguated version
of one of the words therein. This problem escapes
the evaluation schemes, especially when disam-
biguation tasks are used as a criterion for evaluat-
ing compositional models?a common practice in
current research for compositional-distributional
semantics. Indeed, Pulman (2013) argues that al-
though disambiguation can emerge as a welcome
side-effect of the compositional process, it is not
clear if compositionality is either a necessary or
sufficient condition for disambiguation to happen.
On the contrary, it seems that the form of most
current vector space models and the compositional
operations used on them (quite often some form of
vector point-wise multiplication) mainly achieve
disambiguation, but not composition.
The purpose of this paper is to further investi-
gate the potential of a compositional-distributional
model based on disambiguated vector represen-
tations, where each word can have one or more
distinct senses. More specifically, we aim to
show that (a) compositionality is not a neces-
sary condition for disambiguation, so the quite
common practice of using a disambiguation task
as a criterion for evaluating the performance of
compositional-distributional models is question-
able; and (b) the introduction of a separate disam-
biguation step in the compositional process of dis-
tributional models can be indeed beneficial for the
quality of the resulting composed vectors.
We train our models from BNC, a 100-million
words corpus created from samples of written and
spoken English. We perform word sense induc-
tion by following the generic algorithm of Sch?tze
(1998), in which the senses of a word are repre-
sented by distinct clusters created by taking into
account the various contexts in which this specific
word occur in the corpus. For the actual cluster-
ing step we use a combination of hierarchical ag-
glomerative clustering and the Calin?ski-Harabasz
index (Calin?ski and Harabasz, 1974). The param-
eters of the models are fine-tuned on the noun set
of SEMEVAL 2010 Word Sense Induction and Dis-
ambiguation task (Manandhar et al, 2010).
Equipped with a disambiguated vector space,
we use it on a verb disambiguation experiment,
similar in style to that of Mitchell and Lapata
(2008), but applied on a more linguistically mo-
tivated dataset, based on the work of Pickering
and Frisson (2001). We find that the application
of a simple disambiguation algorithm, without any
compositional steps, is proven more effective than
a number of compositional models. We consider
this as an indication for the necessity of separat-
ing disambiguation from composition, since it im-
plies that the latter is not necessary for achiev-
ing the former. Next, we demonstrate that a com-
positional model based on disambiguated vectors
can indeed produce composite vector representa-
tions of better quality, by applying the model on a
phrase similarity task (Mitchell and Lapata, 2010).
The goal here is to evaluate the similarity of short
verb phrases, based on the distance of their com-
posite vectors.
2 Composition in distributional models
The transition from word meaning to sentence
meaning, a task easily done by human subjects
based on the rules of grammar, implies the exis-
tence of a composition operation applied to prim-
itive text units in order to build compound ones.
Various solutions have been proposed with differ-
ent levels of sophistication for this problem in the
context of vector space models of meaning.
At one end of the spectrum the simple models
of Mitchell and Lapata (2008) address composi-
tion as the point-wise multiplication or addition
of the involved word vectors. This bag-of-words
approach has been proven a hard-to-beat baseline
for many of the more sophisticated models. At the
other end, composition in the work of Socher et al
(2010, 2011, 2012) is served by the advanced ma-
chinery of recurring neural networks, where the
output of the network is used again as input in a
recurring fashion, for composing vectors of larger
constituents. Following a different path, the cat-
egorical framework of Coecke et al (2010) ex-
ploits a structural homomorphism between gram-
mar and vector spaces in order to treat words with
special meanings, such as verb and adjectives, as
functions (tensors of rank-n) that apply to their ar-
guments. This application has the form of inner
product, generalising the familiar notion of matrix
multiplication to tensors of higher rank.
Regardless of their level of sophistication, most
of the models which aim to apply composition-
ality on word vector representations fail to ad-
dress the problem of handling the polysemous na-
ture of words. Even more importantly, many of
the models are evaluated on their ability to dis-
ambiguate the meaning of specific words, follow-
ing an idea first introduced by Kintsch (2001) and
later adopted by Mitchell and Lapata (2008) and
others. For example, in this latter work the au-
115
thors test their multiplicative and additive models
as follows: given an ambiguous intransitive verb,
say ?run? (with the two senses to be those of mov-
ing fast and of a liquid dissolving), they examine
to what extent the composition of the verb with
an appropriate subject (e.g. ?horse? or ?colour?)
will disambiguate the intended sense of the verb
within the specific context. Each row in the dataset
consists of a subject (e.g. ?horse?), a verb (?run?),
a high-similarity landmark verb (?gallop?), and a
low-similarity landmark verb (?dissolve?). The
subject is combined with the main verb to form a
simple intransitive sentence, and the vector of this
sentence is then compared with the vectors of the
landmark verbs. The goal is to evaluate the degree
to which the composed sentence vector is closer
to the high landmark than to the vector of the low
landmark, and this is considered an indication of
successful composition.
However, although it is generally true that mul-
tiplying ???run with ????horse will filter out most of the
components of???run that are irrelevant to ?dissolve?
(since the ?dissolve?-related elements of ????horse
should have values close to zero) and will pro-
duce a disambiguated version of this verb under
the context of ?horse?, it is not at all clear if this
vector will also constitute an appropriate repre-
sentation for the meaning of the intransitive sen-
tence ?horse runs?. In other words, here we have
two tasks taking place at the same time: (a) dis-
ambiguation of the ambiguous word given its con-
text; and (b) composition that produces a mean-
ing vector for the whole sentence. The extent to
which the latter is a necessary condition for the
former remains unclear, and constitutes a factor
that complicates the evaluation and assessment of
such systems. In this paper we argue that as long
as the above distinct tasks are interwoven into a
single step, claims of compositionality in distri-
butional systems cannot be reliably assessed. We
therefore propose the addition of a disambiguation
step in the generic methodology of compositional-
distributional models.
3 Related work
Although in general word sense induction is a
popular topic in the natural language processing
literature, little has been done to address poly-
semy specifically in the context of compositional-
distributional models of meaning. In fact, the only
works relevant to ours we are aware of are that of
Erk and Pad? (2008) and Reddy et al (2011). The
structured vector space of Erk and Pad? (2008) is
designed to handle ambiguity in an implicit way,
showing promising results on the Mitchell and
Lapata (2008) task. The work of Reddy et al
(2011) is closer to our research: the authors eval-
uate two word sense disambiguation approaches
on the noun-noun compound similarity task intro-
duced by Mitchell and Lapata (2010), using sim-
ple multiplicative and additive models for compo-
sition. The reported results are also promising,
where at least one of their models performs bet-
ter than the current practice of using ambiguous
vector representations.
Compared to both of the above works, the
scope of the current paper is broader: it does not
solely aim to demonstrate the positive effect of a
?cleaner? vector space on the compositional pro-
cess, but it also proceeds one step further and re-
lates this issue with the current evaluation prac-
tice, showing that a number of verb disambigua-
tion tasks that have been invariantly used for the
assessment of compositional-distributional mod-
els might be in fact based on a wrong criterion.
4 Disambiguation scheme
Our word sense induction method is based on
the effective procedure first presented by Sch?tze
(1998). For the ith occurrence of a target word wt
in the corpus with context Ci = {w1, . . . , wn},
we calculate the centroid of the context as ??ci =
1
n(
??w1 + . . . + ??wn), where ??w is the lexical (or
first order) vector of word w as it is created by the
usual distributional practice (more details in Sec-
tion 5). Then, we cluster these centroids in order
to form a number of sense clusters. Each sense
of the word is represented by the centroid of the
corresponding cluster. Following Sch?tze, we will
refer to these sense vectors as second-order vec-
tors, in order to distinguish them from the lexical
(first-order) vectors. So, in our model each word is
represented by a tuple ???w ,S?, where??w is the 1st-
order vector of the word and S the set of 2nd-order
vectors created by the above procedure.
We are now able to disambiguate the sense of a
target word wt given a context C by calculating a
context vector ??c for C as above, and then com-
paring this with every 2nd-order vector of wt; the
word is assigned to the sense that corresponds to
the closest 2nd-order vector. That is,
???spref = arg min??s ?S
d(??s ,??c ) (1)
where S is the set of 2nd-order vectors for wt and
d(??u ,??v ) the vector distance metric we use.
For the clustering step, we use an iterative
bottom-up approach known as hierarchical ag-
glomerative clustering (HAC). Hierarchical clus-
116
1 0 1 2 3 4 51.0
0.50.0
0.51.0
1.52.0
2.53.0
3.5
23 27 28 20 29 24 22 25 21 26 14 13 16 15 11 19 17 18 10 12 8 1 3 6 9 4 2 7 0 50.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 1: Hierarchical agglomerative clustering.
tering has been invariably applied to unsupervised
word sense induction on a variety of languages,
generally showing good performance?see, for
example, the comparative study of Broda and
Mazur (2012) for English and Polish. Compared
to k-means clustering, this approach has the ma-
jor advantage that it does not require us to define
in advance a specific number of clusters. Com-
pared to more advanced probabilistic techniques,
such as Bayesian mixture models, it is much
more straightforward and simple to implement,
yet powerful enough to demonstrate the necessity
of factoring out ambiguity from compositional-
distributional models.
HAC is a bottom-up method of cluster analy-
sis, starting with each data point (context vector in
our case) forming its own cluster; then, in each it-
eration the two closest clusters are merged into a
new cluster, until all points are finally merged un-
der the same cluster. This process produces a den-
drogram (i.e. a tree diagram), which essentially
embeds every possible clustering of the dataset.
As an example, Figure 1 shows a small dataset
produced by three distinct Gaussian distributions,
and the dendrogram derived by the above algo-
rithm. Implementation-wise, the clustering part in
this work is served by the efficient FASTCLUSTER
library (M?llner, 2013).
Choosing a number of senses In HAC, one still
needs to decide where exactly to cut the tree in or-
der to get the best possible partitioning of the data.
Although the right answer to this problem might
depend on many factors, we can safely assume that
the optimal partitioning is the one that provides
the most compact and maximally separated clus-
ters. One way to measure the quality of a cluster-
ing based on this criterion is the Calin?ski/Harabasz
index (Calin?ski and Harabasz, 1974), also known
as variance ratio criterion (VRC). Given a set ofN
data points and a partitioning of k disjoint clusters,
VRC is computed as follows:
V RCk =
trace(B)
trace(W ) ?
N ? k
k ? 1 (2)
Here, W and B are the intra-cluster and inter-
cluster dispersion matrices, respectively:
W =
k?
i=1
Ni?
l=1
(??xi(l)? x?i)(??xi(l)? x?i)T (3)
B =
k?
i=1
Ni(x?i ? x?)(x?i ? x?)T (4)
where Ni is the number of data points assigned to
cluster i,??xi(l) is the lth point assigned to this clus-
ter, x?i is the centroid of ith cluster (the mean), and
x? is the data centroid of the overall dataset. Given
the above formulas, the trace of B is the sum of
inter-cluster variances, while the trace of W is the
sum of intra-cluster variances. A good partitioning
should have high values for B (which is an indi-
cation for well-separated clusters) and low values
for W (an indication for compact clusters), so the
higher the quality of the partitioning the greater
the value of this ratio.
Compared to other criteria, VRC has been
found to be one of the most effective approaches
for clustering validity?see the comparative stud-
ies of Milligan and Cooper (1985) and Vendramin
et al (2009). Furthermore, it has been previously
applied to word sense discrimination successfully,
returning the best results among a number of other
measures (Savova et al, 2006). For this work, we
calculate VRC for a number of different partition-
ings (ranged from 2 to 10 clusters), and we keep
the partitioning that results in the highest VRC
value as the optimal number of senses for the spe-
cific word. Note that since the HAC dendrogram
already embeds all possible clusterings, the cut-
ting of the tree in order to get a different partition-
ing is performed in constant time.
5 Experimental setting
The choice of our 1st-order vector space is based
on empirical tests, where we found out that a basis
with elements of the form ?word, class? presents
the right balance for our purposes among sim-
pler techniques, such as word-based spaces, and
more complex ones, such as dependency-based
approaches. In our vector space, each word has a
distinct vector representation for every word class
under which occurs in the corpus (e.g. ?suit? will
have a noun vector and a verb vector). As our ba-
sis elements we use the 2000 most frequent con-
tent words in BNC, with weights being calculated
as the ratio of the probability of the context word
given the target word to the probability of the con-
text word overall. The context here is a 5-word
window on both sides of the target word.
The parameters of the clustering scheme are op-
timized on the noun set of SEMEVAL 2010 Word
117
Sense Induction & Disambiguation Task (Man-
andhar et al, 2010). Specifically, when using HAC
one has to decide how to measure the distance
between the clusters, which is the merging crite-
rion applied in every iteration of the algorithm,
as well as the measure between the data points,
i.e. the individual vectors. Based on empirical
tests we limit our options to two inter-cluster mea-
sures: complete-link and Ward?s methods. In the
complete-link method the distance between two
clustersX and Y is the distance between their two
most remote elements:
D(X,Y ) = max
x?X,y?Y
d(x, y) (5)
In Ward?s method, two clusters are selected for
merging if the new partitioning exhibits the mini-
mum increase in the overall intra-cluster variance.
The cluster distance is given by:
D(X,Y ) = 2|X||Y ||X|+ |Y |?
??cX ???cY ?2 (6)
where ??cX and ??cY are the centroids of X and Y .
We test these linkage methods in combination
with three vector distance measures: euclidean,
cosine, and Pearson?s correlation (6 models in to-
tal). The metrics were chosen to represent pro-
gressively more relaxed forms of vector compar-
ison, with the strictest form to be the euclidean
distance and correlation as the most relaxed. For
sense detection we use the disambiguation algo-
rithm described in Section 4, considering as con-
text the whole sentence in which a target word
appears. The distance metric used for the dis-
ambiguation process in each model is identical
to the metric used for the clustering process, so
in the Ward/euclidean model the disambiguation
is based on the euclidean distance, in complete-
link/cosine model on the cosine distance, and so
on. We evaluate the models using V-measure,
an entropy-based metric that addresses the so-
Model V-Meas. Avg clust.
Ward/Euclidean 0.05 1.44
Ward/Correlation 0.14 3.14
Ward/Cosine 0.08 1.94
Complete/Euclidean 0.00 1.00
Complete/Correlation 0.11 2.66
Complete/Cosine 0.06 1.74
Most frequent sense 0.00 1.00
1 cluster/instance 0.36 89.15
Gold standard 1.0 4.46
Table 1: Results on the noun set of SEMEVAL
2010 WSI&D task.
keyboard: 1105 contexts, 2 senses
COMPUTER (665 contexts): program dollar disk power
enter port graphic card option select language drive
pen application corp external editor woman price
page design sun cli amstrad lock interface lcd slot
notebook
MUSIC (440 contexts): drummer instrumental singer
german father fantasia english generation wolfgang
wayne cello body join ensemble mike chamber gary
saxophone sax ricercarus apply form son metal guy
clean roll barry orchestra
Table 2: Derived senses for word ?keyboard?.
called matching problem of F-score (Rosenberg
and Hirschberg, 2007). Table 1 shows the results.
Ward?s method in combination with correla-
tion distance provided the highest V-measure, fol-
lowed by the combination of complete-link with
(again) correlation. Although a direct compari-
son of our models with the models participating
in this task would not be quite sound (since these
models were trained on a special corpus provided
by the organizers, while our model was trained
on the BNC), it is nevertheless enlightening to
mention that the 0.14 V-measure places the Ward-
correlation model at the 4th rank among 28 sys-
tems for the noun set of the task, while at the
same time provides a reasonable average number
of clusters per word (3.14), close to that of the
human-annotated gold standard (4.46). Compare
this, for example, with the best-performing sys-
tem that achieved a V-measure of 0.21, a score
that was largely due to the fact that the model as-
signed the unrealistic number of 11.54 senses per
word on average (since V-measure tends to favour
higher numbers of senses, as the baseline 1 clus-
ter/instance shows in Table 1).1
Table 2 provides an example of the results,
showing the senses for the noun ?keyboard? learnt
by the best model of Ward?s method and correla-
tion measure. Each sense is visualized as a list of
the most dominant words in the cluster, ranked by
their TF-ICF values. Furthermore, Figure 2 shows
the dendrograms produced by four linkage meth-
ods for the word ?keyboard?, demonstrating the su-
periority of Ward?s method.
6 Disambiguation vs composition
A number of models that aim to equip distribu-
tional semantics with compositionality are evalu-
ated on some form of the disambiguation task pre-
sented in Section 2. Versions of this task can be
found, for example, in Mitchell and Lapata (2008),
1The results of SEMEVAL 2010 can be found online at
http://www.cs.york.ac.uk/semeval2010_WSI/task_14
_ranking.html.
118
0.0
0.1
0.2
0.3
0.4
0.5
0.6
keyboard (single/cosine)
(a) Single-link
0.0
0.2
0.4
0.6
0.8
keyboard (average/cosine)
(b) Average-link
0
1
2
3
4
5
keyboard (ward/cosine)
(c) Ward?s method
0.0
0.2
0.4
0.6
0.8
1.0 keyboard (complete/cosine)
(d) Complete-link
Figure 2: Dendrograms produced for word ?key-
board? according to 4 different linkage methods.
Erk and Pad? (2008), Grefenstette and Sadrzadeh
(2011a,b), Kartsaklis et al (2012) and Grefenstette
et al (2013). We briefly remind that the goal is to
assess how well a compositional model can disam-
biguate the meaning of an ambiguous verb, given
a specific context. This kind of evaluation involves
two distinct tasks: the composition of sentence
vectors, and the disambiguation of the verbs. And,
although the evaluation of a model against human
judgements provides some indication for the suc-
cess of the latter task, it leaves unclear to what ex-
tent the former has been achieved. In this section
we perform two experiments in order to address
this question. The first of them aims to support the
following argument: that although disambiguation
can emerge as a side-effect of a compositional pro-
cess, compositionality is not a necessary condition
for this to happen. The second experiment is based
on a more appropriate task that requires genuine
compositional abilities, and demonstrates the good
performance of a compositional model based on
the disambiguated vector space of Section 5.
As our compositional method for the follow-
ing tasks we use the multiplicative and additive
models of Mitchell and Lapata (2008). Despite
the simple nature of these models, there is a num-
ber of reasons that make them good candidates for
demonstrating the main ideas of this paper. First,
for better or worse ?simple? does not necessar-
ily mean ?ineffective?. The comparative study of
Blacoe and Lapata (2012) shows that for certain
tasks these ?baselines? perform equally well or
even better than other more sophisticated models.
And second, it is reasonable to expect that better
compositional models would only work in favour
of our arguments, and not the other way around.
6.1 Evaluating disambiguation
One potential problem with the datasets used for
the disambiguation task of Section 2, similar to
the one of Grefenstette and Sadrzadeh (2011a), is
that ambiguous verbs are usually collected from a
corpus based on some automated method. And,
although they do exhibit variations in their senses
(as most verbs do), in many cases these meanings
are actually related?for example, the meanings of
?write? in G&S dataset are spell and publish. To
overcome this problem, we used the work of Pick-
ering and Frisson (2001), which provides a list of
genuinely ambiguous verbs obtained from careful
manual selection and ranking from human evalu-
ators. The evaluators assessed the relatedness of
each verb?s different meanings using a scale of
0 (totally unrelated) to 7 (highly related). From
these verbs, we picked 10 with an average mark
< 1. An example is ?file?, which means ?smooth?
in ?file nails? and ?register? as in ?file an applica-
tion?. For each verb we picked the 10 most oc-
curring subjects and objects from the BNC (5 for
each landmark). In the case of verb ?file?, for ex-
ample, among these were ?woman? and ?nails? for
landmark ?smooth?, and ?union? and ?lawsuit? for
landmark ?register?. Each subject and object was
modified by its most occurring adjective in the cor-
pus. This resulted in triples of sentences of the
following form:
(1) main: young woman filed long nails
high: young woman smoothed long nails
low: young woman registered long nails
(2) main: monetary union filed civil lawsuit
high: mon. union registered civil lawsuit
low: mon. union smoothed civil lawsuit
The main sentence was paired with both high
and low landmark sentences, creating a dataset2 of
200 sentence pairs (10 main verbs ? 10 contexts
? 2 landmarks)3. These were randomly presented
to 43 human annotators, whose duty was to judge
the similarity between the sentences of each pair.
The human scores were compared with scores pro-
duced by a number of models (Table 3).
The most successful model (M1) does not ap-
ply any form of composition. Instead, the com-
parison of a sentence with a ?landmark? sentence
is simply based on disambiguated versions of the
2The dataset will be available at http://www.cs.ox.
ac.uk/activities/compdistmeaning/.
3As a comparison, the Mitchell and Lapata (2008) dataset
consists of 15 main verbs? 4 contexts? 2 landmarks = 120
sentence pairs, while the Grefenstette and Sadrzadeh (2011a)
dataset has the same configuration and size with ours.
119
verbs alone. Specifically, the main verb and the
landmark verb are disambiguated given the con-
text (subjects, objects, and adjectives that mod-
ify them) according to Equation 1; this produces
two 2nd-order vectors, one for the main verb and
one for the landmark. The degree of similarity be-
tween the two sentences is then calculated by mea-
suring the similarity between the two sense vec-
tors of the verbs, without any compositional step.
The score of 0.28 achieved by this model is im-
pressive, given that the inter-annotator agreement
(which serves as an upper-bound) is 0.38.
A number of interesting observations can be
made based on the results of Table 3. First of
all, the ?verbs-only? model outperforms the two
baselines (which use composition but not disam-
biguation) by a large margin, and indeed also the
other compositional models. This is an indica-
tion that this kind of disambiguation task might
not be the best way to evaluate a compositional
model. The fact that the most important condi-
tion for success is the proper disambiguation of
the verb, means that the good performance of a
compositional model demonstrates only this: how
well the model is able to disambiguate an am-
biguous verb. This is different from how well the
composed representation reflects the meaning of
the larger constituent; that is, it has very little to
say about the extent to which an operation like
??????woman???file????nails ( denotes point-wise mul-
tiplication) results in a faithful representation of
the meaning of sentence ?woman filed nails?.
M2 to M5 represent different versions of the
compositional models that use disambiguation in
a distinct step. All these models compose both the
main verb and the landmark with a given context,
and then perform the comparison at sentence level.
In M2 and M3 all words are first disambiguated
prior to composition, while in M4 and M5 the 2nd-
Disambig. Composition ?
M1 Only verbs No 0.282 ?
M2 All words Multiplicative 0.118
M3 All words Additive 0.210
M4 Only verbs Multiplicative 0.110
M5 Only verbs Additive 0.234 ?
B1 No Multiplicative 0.143
B2 No Additive 0.042
Inter-annotator agreement 0.383
? The difference between M1 and M5 is highly
statistically significant with p < 0.0001
Table 3: Spearman?s ? for the Pickering and Fris-
son dataset.
order vector of the verb is composed with the 1st-
order vectors of the other words. The most im-
pressive observation here is that the separation of
disambiguation results in a tremendous improve-
ment for the additive model, from 0.04 to 0.21.
This is not surprising since, when using magni-
tude invariant measures between vectors (such as
cosine distance), the resulting vector is nothing
more than the average of the involved word vec-
tors. The introduction of the disambiguation step
before the composition, therefore, makes a great
difference since it provides much more accurate
starting points to be averaged.
On the other hand, the disambiguated version
of multiplicative model (M2) presents inferior per-
formance compared to the ?ambiguous? version
(B1). We argue that the reason behind this is that
the two models perform different jobs: the result
of B1 is a ?mixing? of composition and disam-
biguation of the most ambiguous word (i.e. the
verb), since this is the natural effect of the point-
wise multiplication operation (see discussion in
Section 2); on the other hand, M2 is designed to
construct an appropriate composite meaning for
the whole sentence. We will try to support this
argument by the experiment of the next section.
6.2 A better test of compositionality
Although there might not exists such a thing
as the best evaluation method for compositional-
distributional semantics, it is safe to assume that
a phrase similarity task avoids many of the pitfalls
of tasks such as the one of Section 6.1. Given pairs
of short phrases, the goal is to assess the similar-
ity of the phrases by constructing composite vec-
tors for them and computing their distance. No as-
sumptions about disambiguation abilities regard-
ing a specific word (e.g. the verb) are made here;
the only criterion is to what extent the composite
vector representing the meaning of a phrase is sim-
ilar or dissimilar to the vector of another phrase.
From this perspective, this task seems the ideal
choice for evaluating a model aiming to provide
appropriate phrasal semantics. The scores given
by the models are compared to those of human
evaluators using Spearman?s ?.
For this experiment, we use the ?verb-object?
part of the dataset presented in the work of
Mitchell and Lapata (2010), which consists of 108
pairs of short verb phrases exhibiting three de-
grees of similarity. A high similarity pair for ex-
ample, is produce effect/achieve result, a medium
one is pour tea/join party, and a low one is close
eye/achieve end. The original dataset alo con-
120
Disambig. Composition ?
M1 Only verbs No 0.318
M2 All words Multiplicative 0.412 ?
M3 All words Additive 0.414 ?
M4 Only verbs Multiplicative 0.352
M5 Only verbs Additive 0.324
B1 No Multiplicative 0.379 ??
B2 No Additive 0.334
Inter-annotator agreement 0.550
? Difference between M2/B1 is stat. sign. with p ? 0.07
? Difference between M3/B1 is stat. sign. with p ? 0.06
Table 4: Phrase similarity results.
tains noun-noun and adjective-noun compounds.
However, the verb-object part serves the pur-
poses of this paper much better, for two reasons.
First, since by definition the proposed methodol-
ogy suits better circumstances involving at least
some level of word ambiguity, a dataset based on
the most ambiguous part of speech (verbs) seems a
reasonable choice. Second, this part of the dataset
allows us to do some meaningful comparisons
with the task of Section 6.1, which is again around
verb structures. The results are shown in Table 4.
This time, the disambiguation step provides
solid benefits for both multiplicative (M2) and
additive (M3) models, with differences that are
statistically significant from the best baseline B1
(with p ? 0.07 and p ? 0.06, respectively).
Note that the ?verbs-only? model (M1), which was
by a large margin the most successful for the
task of Section 6.1, now shows the worst perfor-
mance. For comparison, the best result reported by
Mitchell and Lapata (2010) on a 1st-order space
similar to ours (regarding dimensions and weights)
was 0.38 (?dilation? model).
7 Discussion
This paper is based on the observation that any
compositional operation between two vectors is
essentially a hybrid process consisting of two
?components? that, depending on the form of the
underlying vector space, can have different ?mag-
nitudes?. One of the components results in a cer-
tain amount of disambiguation for the most am-
biguous original word, while the other one works
towards a composite representation for the mean-
ing of the whole phrase or sentence. The tasks of
Section 6 are designed so that each one of them as-
sesses a different aspect of this hybrid process: the
task of Section 6.1 is focused on the disambigua-
tion aspect, while the task of Section 6.2 addresses
the compositionality part. One of our main argu-
ments is the observation that, in order the get bet-
ter compositional representations, it is essential to
first eliminate (or at least reduce as much as pos-
sible the magnitude of) the disambiguation ?com-
ponent? that might show up as a by-product of the
compositional process, so that the result is mainly
a product of pure composition?this is what the
?unambiguous? models do achieve in the task of
Section 6.2. Based on the experimental work con-
ducted in this paper, our first concluding remark is
that the elimination of the ambiguity factor can be
essential for the quality of the composed vectors.
But, if Table 4 provides a proof that the sep-
aration of disambiguation and composition can
indeed produce better compositional representa-
tions, what is the meaning of the inferior perfor-
mance of all ?unambiguous? models (M2 to M5)
compared to verbs-only version (M1) in the task
of Section 6.1? Why disambiguation is not always
effective (as in the case of multiplicative model)
for that task? These are strong indications that the
quality of composition is not crucial for disam-
biguation tasks of this sort, whose only achieve-
ment is that they measure the disambiguation side-
effects generated by the compositional process. In
other words, the practice of evaluating the qual-
ity of composition by using disambiguation tasks
is problematic. As the topic of compositionality
in distributional models of meaning increasingly
gains popularity in the recent years, this second
concluding remark is equally important since it
can contribute towards better evaluation schemes
of such models.
8 Future work
A next step to take in the future is the appli-
cation of these ideas on more complex spaces,
such as those based on the categorical framework
of Coecke et al (2010). The challenge here is
the effective generalization of a disambiguation
scheme on tensors of rank greater than 1. Ad-
ditionally, we would expect this method to bene-
fit from more robust probabilistic clustering tech-
niques. An appealing option is the use of a non-
parametric method, such as a hierarchical Dirich-
let process (Yao and Van Durme, 2011).
Acknowledgements
We would like to thank Daniel M?llner for his
comments on the use of FASTCLUSTER library,
as well as the three anonymous reviewers for their
fruitful suggestions. Support by EPSRC grant EP/
F042728/1 is gratefully acknowledged by the first
two authors.
121
References
Baroni, M. and Zamparelli, R. (2010). Nouns
are Vectors, Adjectives are Matrices. In Pro-
ceedings of Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Blacoe, W. and Lapata, M. (2012). A compari-
son of vector-based representations for seman-
tic composition. In Proceedings of the 2012
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning, pages 546?556,
Jeju Island, Korea. Association for Computa-
tional Linguistics.
Broda, B. and Mazur, W. (2012). Evaluation
of clustering algorithms for word sense disam-
biguation. International Journal of Data Anal-
ysis Techniques and Strategies, 4(3):219?236.
Calin?ski, T. and Harabasz, J. (1974). A Dendrite
Method for Cluster Analysis. Communications
in Statistics-Theory and Methods, 3(1):1?27.
Coecke, B., Sadrzadeh, M., and Clark, S.
(2010). Mathematical Foundations for Dis-
tributed Compositional Model of Meaning.
Lambek Festschrift. Linguistic Analysis,
36:345?384.
Erk, K. and Pad?, S. (2008). A Structured Vector-
Space Model for Word Meaning in Context. In
Proceedings of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 897?906.
Grefenstette, E., Dinu, G., Zhang, Y.-Z.,
Sadrzadeh, M., and Baroni, M. (2013). Multi-
step regression learning for compositional dis-
tributional semantics.
Grefenstette, E. and Sadrzadeh, M. (2011a). Ex-
perimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Grefenstette, E. and Sadrzadeh, M. (2011b). Ex-
perimenting with Transitive Verbs in a DisCo-
Cat. In Proceedings of Workshop on Geomet-
rical Models of Natural Language Semantics
(GEMS).
Guevara, E. (2010). A Regression Model of
Adjective-Noun Compositionality in Distribu-
tional Semantics. In Proceedings of the ACL
GEMS Workshop.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2012). A unified sentence space for categorical
distributional-compositional semantics: Theory
and experiments. In Proceedings of 24th Inter-
national Conference on Computational Linguis-
tics (COLING 2012): Posters, pages 549?558,
Mumbai, India. The COLING 2012 Organizing
Committee.
Kintsch, W. (2001). Predication. Cognitive Sci-
ence, 25(2):173?202.
Manandhar, S., Klapaftis, I., Dligach, D., and
Pradhan, S. (2010). Semeval-2010 task 14:
Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68. Associa-
tion for Computational Linguistics.
Milligan, G. and Cooper, M. (1985). An Exami-
nation of Procedures for Determining the Num-
ber of Clusters in a Data Set. Psychometrika,
50(2):159?179.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 236?
244.
Mitchell, J. and Lapata, M. (2010). Composition
in distributional models of semantics. Cognitive
Science, 34(8):1388?1439.
M?llner, D. (2013). fastcluster: Fast Hierarchical
Clustering Routines for R and Python. Journal
of Statistical Software, 9(53):1?18.
Pickering, M. and Frisson, S. (2001). Process-
ing ambiguous verbs: Evidence from eye move-
ments. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 27(2):556.
Pulman, S. (2013). Combining Compositional and
Distributional Models of Semantics. In Heunen,
C., Sadrzadeh, M., and Grefenstette, E., editors,
Quantum Physics and Linguistics: A Composi-
tional, Diagrammatic Discourse. Oxford Uni-
versity Press.
Reddy, S., Klapaftis, I., McCarthy, D., and Man-
andhar, S. (2011). Dynamic and static prototype
vectors for semantic composition. In Proceed-
ings of 5th International Joint Conference on
Natural Language Processing, pages 705?713.
Rosenberg, A. and Hirschberg, J. (2007). V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning, pages
410?420.
122
Savova, G., Therneau, T., and Chute, C. (2006).
Cluster Stopping Rules for Word Sense Dis-
crimination. In Proceedings of the workshop
on Making Sense of Sense: Bringing Psy-
cholinguistics and Computational Linguistics
Together, pages 9?16.
Sch?tze, H. (1998). Automatic Word Sense Dis-
crimination. Computational Linguistics, 24:97?
123.
Socher, R., Huang, E., Pennington, J., Ng, A., and
Manning, C. (2011). Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase
Detection. Advances in Neural Information
Processing Systems, 24.
Socher, R., Huval, B., Manning, C., and A., N.
(2012). Semantic compositionality through re-
cursive matrix-vector spaces. In Conference on
Empirical Methods in Natural Language Pro-
cessing 2012.
Socher, R., Manning, C., and Ng, A. (2010).
Learning Continuous Pphrase Representations
and Syntactic Parsing with recursive neural net-
works. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning
Workshop.
Vendramin, L., Campello, R., and Hruschka, E.
(2009). On the Comparison of Relative Clus-
tering Validity Criteria. In Proceedings of the
SIAM International Conference on Data Min-
ing, SIAM, pages 733?744.
Yao, X. and Van Durme, B. (2011). Nonparamet-
ric bayesian word sense induction. ACL HLT
2011, page 10.
123
