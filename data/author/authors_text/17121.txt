Proceedings of NAACL-HLT 2013, pages 32?40,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Simultaneous Word-Morpheme Alignment for
Statistical Machine Translation
Elif Eyigo?z
Computer Science
University of Rochester
Rochester, NY 14627
Daniel Gildea
Computer Science
University of Rochester
Rochester, NY 14627
Kemal Oflazer
Computer Science
Carnegie Mellon University
PO Box 24866, Doha, Qatar
Abstract
Current word alignment models for statisti-
cal machine translation do not address mor-
phology beyond merely splitting words. We
present a two-level alignment model that dis-
tinguishes between words and morphemes, in
which we embed an IBM Model 1 inside an
HMM based word alignment model. The
model jointly induces word and morpheme
alignments using an EM algorithm. We eval-
uated our model on Turkish-English parallel
data. We obtained significant improvement of
BLEU scores over IBM Model 4. Our results
indicate that utilizing information from mor-
phology improves the quality of word align-
ments.
1 Introduction
All current state-of-the-art approaches to SMT rely
on an automatically word-aligned corpus. However,
current alignment models do not take into account
the morpheme, the smallest unit of syntax, beyond
merely splitting words. Since morphology has not
been addressed explicitly in word alignment models,
researchers have resorted to tweaking SMT systems
by manipulating the content and the form of what
should be the so-called ?word?.
Since the word is the smallest unit of translation
from the standpoint of word alignment models, the
central focus of research on translating morphologi-
cally rich languages has been decomposition of mor-
phologically complex words into tokens of the right
granularity and representation for machine transla-
tion. Chung and Gildea (2009) and Naradowsky and
Toutanova (2011) use unsupervised methods to find
word segmentations that create a one-to-one map-
ping of words in both languages. Al-Onaizan et al
(1999), C?mejrek et al (2003), and Goldwater and
McClosky (2005) manipulate morphologically rich
languages by selective lemmatization. Lee (2004)
attempts to learn the probability of deleting or merg-
ing Arabic morphemes for Arabic to English trans-
lation. Niessen and Ney (2000) split German com-
pound nouns, and merge German phrases that cor-
respond to a single English word. Alternatively,
Yeniterzi and Oflazer (2010) manipulate words of
the morphologically poor side of a language pair
to mimic having a morphological structure similar
to the richer side via exploiting syntactic structure,
in order to improve the similarity of words on both
sides of the translation.
We present an alignment model that assumes in-
ternal structure for words, and we can legitimately
talk about words and their morphemes in line with
the linguistic conception of these terms. Our model
avoids the problem of collapsing words and mor-
phemes into one single category. We adopt a two-
level representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment in the scope of a given word
alignment. The model jointly induces word and
morpheme alignments using an EM algorithm.
We develop our model in two stages. Our initial
model is analogous to IBM Model 1: the first level
is a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. In this manner,
we embed one IBM Model 1 in the scope of another
IBM Model 1. At the second stage, by introducing
distortion probabilities at the word level, we develop
an HMM extension of the initial model.
We evaluated the performance of our model on the
32
Turkish-English pair both on hand-aligned data and
by running end-to-end machine translation experi-
ments. To evaluate our results, we created gold word
alignments for 75 Turkish-English sentences. We
obtain significant improvement of AER and BLEU
scores over IBM Model 4. Section 2.1 introduces
the concept of morpheme alignment in terms of its
relation to word alignment. Section 2.2 presents
the derivation of the EM algorithm and Section 3
presents the results of our experiments.
2 Two-level Alignment Model (TAM)
2.1 Morpheme Alignment
Following the standard alignment models of Brown
et al (1993), we assume one-to-many alignment for
both words and morphemes. A word alignment aw
(or only a) is a function mapping a set of word po-
sitions in a source language sentence to a set of
word positions in a target language sentence. A mor-
pheme alignment am is a function mapping a set of
morpheme positions in a source language sentence
to a set of morpheme positions in a target language
sentence. A morpheme position is a pair of integers
(j, k), which defines a word position j and a relative
morpheme position k in the word at position j. The
alignments below are depicted in Figures 1 and 2.
aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1
Figure 1 shows a word alignment between two sen-
tences. Figure 2 shows the morpheme alignment be-
tween same sentences. We assume that all unaligned
morphemes in a sentence map to a special null mor-
pheme.
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the fol-
lowing conditions: If the morpheme alignment am
maps a morpheme of e to a morpheme of f , then the
word alignment aw maps e to f . If the word align-
ment aw maps e to f , then the morpheme alignment
am maps at least one morpheme of e to a morpheme
of f . If the word alignment aw maps e to null, then
all of its morphemes are mapped to null. In sum, a
morpheme alignment am and a word alignment aw
are compatible if and only if:
? j, k,m, n ? N+, ? s, t ? N+
[am(j, k) = (m,n)? aw(j) = m] ?
[aw(j) = m? am(j, s) = (m, t)] ?
[aw(j) = null? am(j, k) = null] (1)
Please note that, according to this definition of com-
patibility, ?am(j, k) = null? does not necessarily im-
ply ?aw(j) = null?.
A word alignment induces a set of compati-
ble morpheme alignments. However, a morpheme
alignment induces a unique word alignment. There-
fore, if a morpheme alignment am and a word align-
ment aw are compatible, then the word alignment is
aw is recoverable from the morpheme alignment am.
The two-level alignment model (TAM), like
IBM Model 1, defines an alignment between words
of a sentence pair. In addition, it defines a mor-
pheme alignment between the morphemes of a sen-
tence pair.
The problem domain of IBM Model 1 is defined
over alignments between words, which is depicted
as the gray box in Figure 1. In Figure 2, the smaller
boxes embedded inside the main box depict the new
problem domain of TAM. Given the word align-
ments in Figure 1, we are presented with a new
alignment problem defined over their morphemes.
The new alignment problem is constrained by the
given word alignment. We, like IBM Model 1, adopt
a bag-of-morphemes approach to this new problem.
We thus embed one IBM Model 1 into the scope of
another IBM Model 1, and formulate a second-order
interpretation of IBM Model 1.
TAM, like IBM Model 1, assumes that words and
morphemes are translated independently of their
context. The units of translation are both words and
morphemes. Both the word alignment aw and the
morpheme alignment am are hidden variables that
need to be learned from the data using the EM algo-
rithm.
In IBM Model 1, p(e|f), the probability of trans-
lating the sentence f into e with any alignment is
computed by summing over all possible word align-
ments:
p(e|f) =
?
a
p(a, e|f)
33
Figure 1: Word alignment Figure 2: Morpheme alignment
In TAM, the probability of translating the sentence
f into e with any alignment is computed by sum-
ming over all possible word alignments and all pos-
sible morpheme alignments that are compatible with
a given word alignment aw:
p(e|f) =
?
aw
p(aw, e|f)
?
am
p(am, e|aw, f) (2)
where am stands for a morpheme alignment. Since
the morpheme alignment am is in the scope of a
given word alignment aw, am is constrained by aw.
In IBM Model 1, we compute the probability of
translating the sentence f into e by summing over
all possible word alignments between the words of f
and e:
p(e|f) = R(e, f)
|e|?
j=1
|f |?
i=0
t(ej |fi) (3)
where t(ej | fi) is the word translation probability
of ej given fi. R(e, f) substitutes
P (le|lf )
(lf+1)
le for easy
readability.1
In TAM, the probability of translating the sen-
tence f into e is computed as follows:
Word
R(e, f)
|e|?
j=1
|f |?
i=0
(
t(ej |fi)
R(ej , fi)
|ej |?
k=1
|fi|?
n=0
t(ekj |f
n
i )
)
Morpheme
where fni is the n
th morpheme of the word at po-
sition i. The right part of this equation, the con-
tribution of morpheme translation probabilities, is
1le = |e| is the number of words in sentence e and lf = |f |.
in the scope of the left part. In the right part, we
compute the probability of translating the word fi
into the word ej by summing over all possible mor-
pheme alignments between the morphemes of ej and
fi. R(ej , fi) is equivalent to R(e, f) except for the
fact that its domain is not the set of sentences but
the set of words. The length of words ej and fi in
R(ej , fi) are the number of morphemes of ej and fi.
The left part, the contribution of word transla-
tion probabilities alone, equals Eqn. 3. Therefore,
canceling the contribution of morpheme translation
probabilities reduces TAM to IBM Model 1. In
our experiments, we call this reduced version of
TAM ?word-only? (IBM). TAM with the contribu-
tion of both word and morpheme translation proba-
bilities, as the equation above, is called ?word-and-
morpheme?. Finally, we also cancel out the con-
tribution of word translation probabilities, which is
called ?morpheme-only?. In the ?morpheme-only?
version of TAM, t(ej |fi) equals 1. Bellow is the
equation of p(e|f) in the morpheme-only model.
p(e|f) =
R(e, f)
|e|?
j=1
|f |?
i=0
|ej |?
k=1
|fi|?
n=0
R(ej , fi)t(e
k
j |f
n
i ) (4)
Please note that, although this version of the two-
level alignment model does not use word translation
probabilities, it is also a word-aware model, as mor-
pheme alignments are restricted to correspond to a
valid word alignment according to Eqn. 1. When
presented with words that exhibit no morphology,
the morpheme-only version of TAM is equivalent to
IBM Model 1, as every single-morpheme word is it-
self a morpheme.
Deficiency and Non-Deficiency of TAM We
present two versions of TAM, the word-and-
34
morpheme and the morpheme-only versions. The
word-and-morpheme version of the model is defi-
cient whereas the morpheme-only model is not.
The word-and-morpheme version is deficient, be-
cause some probability is allocated to cases where
the morphemes generated by the morpheme model
do not match the words generated by the word
model. Moreover, although most languages exhibit
morphology to some extent, they can be input to the
algorithm without morpheme boundaries. This also
causes deficiency in the word-and-morpheme ver-
sion, as single morpheme words are generated twice,
as a word and as a morpheme.
Nevertheless, we observed that the deficient ver-
sion of TAM can perform as good as the non-
deficient version of TAM, and sometimes performs
better. This is not surprising, as deficient word align-
ment models such as IBM Model 3 or discriminative
word alignment models work well in practice.
Goldwater and McClosky (2005) proposed a mor-
pheme aware word alignment model for language
pairs in which the source language words corre-
spond to only one morpheme. Their word alignment
model is:
P (e|f) =
K?
k=0
P (ek|f)
where ek is the kth morpheme of the word e. The
morpheme-only version of our model is a general-
ization of this model. However, there are major dif-
ferences in their and our implementation and exper-
imentation. Their model assumes a fixed number of
possible morphemes associated with any stem in the
language, and if the morpheme ek is not present, it
is assigned a null value.
The null word on the source side is also a null
morpheme, since every single morpheme word is it-
self a morpheme. In TAM, the null word is the null
morpheme that all unaligned morphemes align to.
2.2 Second-Order Counts
In TAM, we collect counts for both word translations
and morpheme translations. Unlike IBM Model 1,
R(e, f) = P (le|lf )
(lf+1)
le does not cancel out in the counts
of TAM. To compute the conditional probability
P (le|lf ), we assume that the length of word e (the
number of morphemes of word e) varies according
to a Poisson distribution with a mean that is linear
with length of the word f .
P (le|lf ) = FPoisson(le, r ? lf )
=
exp(?r ? lf )(r ? lf )le
le!
FPoisson(le, r ? lf ) expresses the probability that there
are le morphemes in e if the expected number of
morphemes in e is r ? lf , where r =
E[le]
E[lf ]
is the rate
parameter. Since lf is undefined for null words, we
omit R(e, f) for null words.
We introduce T (e|f), the translation probability
of e given f with all possible morpheme alignments,
as it will occur frequently in the counts of TAM:
T (e|f) = t(e|f)R(e, f)
|e|?
k=1
|f |?
n=0
t(ek|fn)
The role of T (e|f) in TAM is very similar to the
role of t(e|f) in IBM Model 1. In finding the Viterbi
alignments, we do not take max over the values in
the summation in T (e|f).
2.2.1 Word Counts
Similar to IBM Model 1, we collect counts for
word translations over all possible alignments,
weighted by their probability. In Eqn. 5, the count
function collects evidence from a sentence pair
(e, f) as follows: For all words ej of the sentence e
and for all word alignments aw(j), we collect counts
for a particular input word f and an output word e
iff ej = e and faw(j) = f .
cw(e|f ; e, f , aw) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
T (e|f)
|f |?
i=0
T (e|fi)
(5)
2.2.2 Morpheme Counts
As for morpheme translations, we collect counts
over all possible word and morpheme alignments,
weighted by their probability. The morpheme count
function below collects evidence from a word pair
(e, f) in a sentence pair (e, f) as follows: For all
words ej of the sentence e and for all word align-
ments aw(j), for all morphemes ekj of the word ej
and for all morpheme alignments am(j, k), we col-
lect counts for a particular input morpheme g and an
35
output morpheme h iff ej = e and faw(j) = f and
h = ekj and g = fam(j,k).
cm(h|g; e, f , aw, am) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?
1?k?|e|
s.t.
h=ekj
g=fam(j,k)
T (e|f)
|f |?
i=0
T (e|fi)
t(h|g)
|f |?
i=1
t(h|f i)
The left part of the morpheme count function is the
same as the word-counts in Eqn. 5. Since it does not
contain h or g, it needs to be computed only once for
each word. The right part of the equation is familiar
from the IBM Model 1 counts.
2.3 HMM Extension
We implemented TAM with the HMM extension
(Vogel et al, 1996) at the word level. We redefine
p(e|f) as follows:
R(e, f)
?
aw
|e|?
j=1
(
p(s(j ) |C (faw (j?1 ))) t(ej |faw(j))
R(ej , faw(j))
?
am
|ej |?
k=1
t(ekj |fam(j,k))
)
where the distortion probability depends on the rel-
ative jump width s(j) = aw(j ? 1) ? aw(j),
as opposed to absolute positions. The distortion
probability is conditioned on class of the previous
aligned word C (faw(j?1)). We used the mkcls
tool in GIZA (Och and Ney, 2003) to learn the word
classes.
We formulated the HMM extension of TAM only
at the word level. Nevertheless, the morpheme-only
version of TAM also has an HMM extension, as it
is also a word-aware model. To obtain the HMM
extension of the morpheme-only version, substitute
t(ej |faw(j)) with 1 in the equation above.
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions. We
learn the probabilities of jumping to a null position
from the data. To compute the jump probability from
a null position, we keep track of the nearest previous
source word that does not align to null, and use the
position of the previous non-null word to calculate
the jump width. For this reason, we use a total of
2lf ? 1 words for the HMM model, the positions
> lf stand for null positions between the words of f
(Och and Ney, 2003). We do not allow null to null
jumps. In sum, we enforce the following constraints:
P (i+ lf + 1|i
?) = p(null|i?)
P (i+ lf + 1|i
? + lf + 1) = 0
P (i|i? + lf + 1) = p(i|i
?)
In the HMM extension of TAM, we perform
forward-backward training using the word counts in
Eqn. 5 as the emission probabilities. We calculate
the posterior word translation probabilities for each
ej and fi such that 1 ? j ? le and 1 ? i ? 2lf ? 1
as follows:
?j(i) =
?j(i)?j(i)
2lf?1?
m=1
?j(m)?j(m)
where ? is the forward and ? is the backward prob-
abilities of the HMM. The HMM word counts, in
turn, are the posterior word translation probabilities
obtained from the forward-backward training:
cw(e|f ; e, f , aw) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?j(aw(j))
Likewise, we use the posterior probabilities in HMM
morpheme counts:
cm(h|g; e, f , aw, am) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?
1?k?|e|
s.t.
h=ekj
g=fam(j,k)
?j(aw(j))
t(h|g)
|f |?
i=1
t(h|f i)
The complexity of the HMM extension of TAM is
O(n3m2), where n is the number of words, and m
is the number of morphemes per word.
2.4 Variational Bayes
Moore (2004) showed that the EM algorithm is par-
ticularly susceptible to overfitting in the case of rare
words when training IBM Model 1. In order to pre-
vent overfitting, we use the Variational Bayes ex-
tension of the EM algorithm (Beal, 2003). This
36
(a) Kas?m 1996?da, Tu?rk makamlar?, I?c?is?leri Bakanl?g?? bu?nyesinde bir kay?p kis?ileri arama birimi olus?turdu.
(b) Kas?m+Noun 1996+Num?Loc ,+Punc Tu?rk+Noun makam+Noun?A3pl?P3sg ,+Punc I?c?is?i+Noun?A3pl?
P3sg Bakanl?k+Noun?P3sg bu?nye+Noun?P3sg?Loc bir+Det kay?p+Adj kis?i+Noun?A3pl?Acc ara+Verb?
Inf2 birim+Noun?P3sg olus?+Verb?Caus?Past .+Punc
(c) In November 1996 the Turkish authorities set up a missing persons search unit within the Ministry of the
Interior.
(d) in+IN November+NNP 1996+CD the+DT Turkish+JJ author+NN?ity+N|N.?NNS set+VB?VBD up+RP
a+DT miss+VB?VBG+JJ person+NN?NNS search+NN unit+NN within+IN the+DT minister+NN?
y+N|N. of+IN the+DT interior+NN .+.
(e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ
persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+.
Figure 3: Turkish-English data examples
amounts to a small change to the M step of the orig-
inal EM algorithm. We introduce Dirichlet priors ?
to perform an inexact normalization by applying the
function f(v) = exp(?(v)) to the expected counts
collected in the E step, where ? is the digamma
function (Johnson, 2007).
?x|y =
f(E[c(x|y)] + ?)
f(
?
j E[c(xj |y)] + ?)
We set ? to 10?20, a very low value, to have the ef-
fect of anti-smoothing, as low values of ? cause the
algorithm to favor words which co-occur frequently
and to penalize words that co-occur rarely.
3 Experimental Setup
3.1 Data
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences, which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such as
the Turkish Ministry of Foreign Affairs, EU, etc. We
followed a heavily supervised approach in morpho-
logical analysis. The Turkish data was first morpho-
logically parsed (Oflazer, 1994), then disambiguated
(Sak et al, 2007) to select the contextually salient in-
terpretation of words. In addition, we removed mor-
phological features that are not explicitly marked by
an overt morpheme ? thus each feature symbol be-
yond the root part-of-speech corresponds to a mor-
pheme. Line (b) of Figure 3 shows an example of
a segmented Turkish sentence. The root is followed
by its part-of-speech tag separated by a ?+?. The
derivational and inflectional morphemes that follow
the root are separated by ???s. In all experiments,
we used the same segmented version of the Turkish
data, because Turkish is an agglutinative language.
For English, we used the CELEX database
(Baayen et al, 1995) to segment English words into
morphemes. We created two versions of the data:
a segmented version that involves both derivational
and inflectional morphology, and an unsegmented
POS tagged version. The CELEX database provides
tags for English derivational morphemes, which in-
dicate their function: the part-of-speech category the
morpheme attaches to and the part-of-speech cate-
gory it returns. For example, in ?sparse+ity? = ?spar-
sity?, the morpheme -ity attaches to an adjective to
the right and returns a noun. This behavior is repre-
sented as ?N|A.? in CELEX, where ?.? indicates the
attachment position. We used these tags in addition
to the surface forms of the English morphemes, in
order to disambiguate multiple functions of a single
surface morpheme.
The English sentence in line (d) of Figure 3 ex-
hibits both derivational and inflectional morphology.
For example, ?author+ity+s?=?authorities? has both
an inflectional suffix -s and a derivational suffix -ity,
whereas ?person+s? has only an inflectional suffix -s.
For both English and Turkish data, the dashes in
Figure 3 stand for morpheme boundaries, therefore
the strings between the dashes are treated as indi-
37
Words Morphemes
Tokens Types Tokens Types
English Der+Inf 1,033,726 27,758 1,368,188 19,448
English POS 1,033,726 28,647 1,033,726 28,647
Turkish Der+Inf 812,374 57,249 1,484,673 16,713
Table 1: Data statistics
visible units. Table 1 shows the number of words,
the number of morphemes and the respective vocab-
ulary sizes. The average number of morphemes in
segmented Turkish words is 2.69, and the average
length of segmented English words is 1.57.
3.2 Experiments
We initialized our baseline word-only model with 5
iterations of IBM Model 1, and further trained the
HMM extension (Vogel et al, 1996) for 5 iterations.
We call this model ?baseline HMM? in the discus-
sions. Similarly, we initialized the two versions of
TAM with 5 iterations of the model explained in
Section 2.2, and then trained the HMM extension of
it as explained in Section 2.3 for 5 iterations.
To obtain BLEU scores for TAM models and
our implementation of the word-only model, i.e.
baseline-HMM, we bypassed GIZA++ in the Moses
toolkit (Och and Ney, 2003). We also ran GIZA++
(IBM Model 1?4) on the data. We translated 1000
sentence test sets.
4 Results and Discussion
We evaluated the performance of our model in two
different ways. First, we evaluated against gold
word alignments for 75 Turkish-English sentences.
Second, we used the word Viterbi alignments of our
algorithm to obtain BLEU scores.
Table 2 shows the AER (Och and Ney, 2003) of
the word alignments of the Turkish-English pair and
the translation performance of the word alignments
learned by our models. We report the grow-diag-
final (Koehn et al, 2003) of the Viterbi alignments.
In Table 2, results obtained with different versions
of the English data are represented as follows: ?Der?
stands for derivational morphology, ?Inf? for inflec-
tional morphology, and ?POS? for part-of-speech
tags. ?Der+Inf? corresponds to the example sen-
tence in line (d) in Figure 3, and ?POS? to line (e).
?DIR? stands for models with Dirichlet priors, and
?NO DIR? stands for models without Dirichlet pri-
ors. All reported results are of the HMM extension
of respective models.
Table 2 shows that using Dirichlet priors hurts
the AER performance of the word-and-morpheme
model in all experiment settings, and benefits the
morpheme-only model in the POS tagged experi-
ment settings.
In order to reduce the effect of nondeterminism,
we run Moses three times per experiment setting,
and report the highest BLEU scores obtained. Since
the BLEU scores we obtained are close, we did a sig-
nificance test on the scores (Koehn, 2004). Table 2
visualizes the partition of the BLEU scores into sta-
tistical significance groups. If two scores within the
same column have the same background color, or the
border between their cells is removed, then the dif-
ference between their scores is not statistically sig-
nificant. For example, the best BLEU scores, which
are in bold, have white background. All scores in a
given experiment setting without white background
are significantly worse than the best score in that ex-
periment setting, unless there is no border separating
them from the best score.
In all experiment settings, the TAM Models per-
form better than the baseline-HMM. Our experi-
ments showed that the baseline-HMM benefits from
Dirichlet priors to a larger extent than the TAM mod-
els. Dirichlet priors help reduce the overfitting in
the case of rare words. The size of the word vo-
cabulary is larger than the size of the morpheme
vocabulary. Therefore the number of rare words is
larger for words than it is for morphemes. Conse-
quently, baseline-HMM, using only the word vocab-
38
                                                
BLEU
EN to TR
BLEU
TR to EN
AER
Der+Inf POS Der+Inf POS Der+Inf POS
NO
 
DIR
TAM
Morph only 22.57 22.54 29.30 29.45 0.293 0.276
Word & Morph 21.95 22.37 28.81 29.01 0.286 0.282
WORD
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
Base-HMM 21.78 21.38 28.22 28.02 0.381 0.375
IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/A
DIR
TAM
Morph only 22.18 22.52 29.32 29.98 0.304 0.256
Word & Morph 22.43 21.62 29.21 29.11 0.338 0.317
WORD
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
Base-HMM 21.69 21.95 28.76 29.13 0.381 0.377
IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/A
Table 2: AER and BLEU Scores
ulary, benefits from the use of Dirichlet priors more
than the TAM models.
In four out of eight experiment settings, the
morpheme-only model performs better than the
word-and-morpheme version of TAM. However,
please note that our extensive experimentation
with TAM models revealed that the superiority
of the morpheme-only model over the word-and-
morpheme model is highly dependent on segmenta-
tion accuracy, degree of segmentation, and morpho-
logical richness of languages.
Finally, we treated morphemes as words and
trained IBM Model 4 on the morpheme segmented
versions of the data. To obtain BLEU scores, we
had to unsegment the translation output: we con-
catenated the prefixes to the morpheme to the right,
and suffixes to the morpheme to the left. Since this
process creates malformed words, the BLEU scores
obtained are much lower than the scores obtained by
IBM Model 4, the baseline and the TAM Models.
5 Conclusion
We presented two versions of a two-level alignment
model for morphologically rich languages. We ob-
served that information provided by word transla-
tions and morpheme translations interact in a way
that enables the model to be receptive to the par-
tial information in rarely occurring words through
their frequently occurring morphemes. We obtained
significant improvement of BLEU scores over IBM
Model 4. In conclusion, morphologically aware
word alignment models prove to be superior to their
word-only counterparts.
Acknowledgments Funded by NSF award IIS-
0910611. Kemal Oflazer acknowledges the gen-
erous support of the Qatar Foundation through
Carnegie Mellon University?s Seed Research pro-
gram. The statements made herein are solely the
responsibility of this author(s), and not necessarily
that of Qatar Foundation.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
39
Technical report, Final Report, JHU Summer Work-
shop.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2) [CD-ROM].
Linguistic Data Consortium, University of Pennsylva-
nia [Distributor], Philadelphia, PA.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718?726.
Martin C?mejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83?90, Morristown, NJ, USA.
Association for Computational Linguistics.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
HLT-EMNLP.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In EMNLP-CoNLL, pages 296?305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL, pages 57?
60.
Robert C. Moore. 2004. Improving IBM word alignment
model 1. In ACL, pages 518?525, Barcelona, Spain,
July.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich Hidden Semi-Markov Models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 895?904, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sonja Niessen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Computa-
tional Linguistics, pages 1081?1085, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kemal Oflazer. 1994. Two-level description of Turkish
morphology. Literary and Linguistic Computing, 9(2).
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107?118,
Berlin, Heidelberg. Springer-Verlag.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
ACL, pages 454?464, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
40
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 494?502,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Multi-rate HMMs for Word Alignment
Elif Eyigo?z
Computer Science
University of Rochester
Rochester, NY 14627
Daniel Gildea
Computer Science
University of Rochester
Rochester, NY 14627
Kemal Oflazer
Computer Science
Carnegie Mellon University
PO Box 24866, Doha, Qatar
Abstract
We apply multi-rate HMMs, a tree struc-
tured HMM model, to the word-alignment
problem. Multi-rate HMMs allow us to
model reordering at both the morpheme
level and the word level in a hierarchical
fashion. This approach leads to better ma-
chine translation results than a morpheme-
aware model that does not explicitly model
morpheme reordering.
1 Introduction
We present an HMM-based word-alignment
model that addresses transitions between mor-
pheme positions and word positions simultane-
ously. Our model is an instance of a multi-scale
HMM, a widely used method for modeling dif-
ferent levels of a hierarchical stochastic process.
In multi-scale modeling of language, the deepest
level of the hierarchy may consist of the phoneme
sequence, and going up in the hierarchy, the next
level may consist of the syllable sequence, and
then the word sequence, the phrase sequence, and
so on. By the same token, in the hierarchical word-
alignment model we present here, the lower level
consists of the morpheme sequence and the higher
level the word sequence.
Multi-scale HMMs have a natural application in
language processing due to the hierarchical nature
of linguistic structures. They have been used for
modeling text and handwriting (Fine et al, 1998),
in signal processing (Willsky, 2002), knowledge
extraction (Skounakis et al, 2003), as well as in
other fields of AI such as vision (Li et al, 2006;
Luettgen et al, 1993) and robotics (Theocharous
et al, 2001). The model we propose here is most
similar to multi-rate HMMs (C?etin et al, 2007),
which were applied to a classification problem in
industrial machine tool wear.
The vast majority of languages exhibit morphol-
ogy to some extent, leading to various efforts in
machine translation research to include morphol-
ogy in translation models (Al-Onaizan et al, 1999;
Niessen and Ney, 2000; C?mejrek et al, 2003;
Lee, 2004; Chung and Gildea, 2009; Yeniterzi and
Oflazer, 2010). For the word-alignment problem,
Goldwater and McClosky (2005) and Eyigo?z et al
(2013) suggested word alignment models that ad-
dress morphology directly.
Eyigo?z et al (2013) introduced two-level align-
ment models (TAM), which adopt a hierarchi-
cal representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment. TAMs jointly induce word
and morpheme alignments using an EM algorithm.
TAMs can align rarely occurring words through
their frequently occurring morphemes. In other
words, they use morpheme probabilities to smooth
rare word probabilities.
Eyigo?z et al (2013) introduced TAM 1, which is
analogous to IBM Model 1, in that the first level is
a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. By introducing
distortion probabilities at the word level, Eyigo?z
et al (2013) defined the HMM extension of TAM
1, the TAM-HMM. TAM-HMM was shown to
be superior to its single-level counterpart, i.e., the
HMM-based word alignment model of Vogel et al
(1996).
The alignment example in Figure 1 shows a
Turkish word aligned to an English phrase. The
morphemes of the Turkish word are aligned to
the English words. As the example shows, mor-
phologically rich languages exhibit complex re-
ordering phenomena at the morpheme level, which
is left unutilized in TAM-HMMs. In this paper,
we add morpheme sequence modeling to TAMs
to capture morpheme level distortions. The ex-
ample also shows that the Turkish morpheme or-
494
Figure 1: Turkish word aligned to an English
phrase.
der is the reverse of the English word order. Be-
cause this pattern spans several English words, it
can only be captured by modeling morpheme re-
ordering across word boundaries. We chose multi-
rate HMMs over other hierarchical HMM mod-
els because multi-rate HMMs allow morpheme se-
quence modeling across words over the entire sen-
tence.
It is possible to model the morpheme sequence
by treating morphemes as words: segmenting the
words into morphemes, and using word-based
word alignment models on the segmented data.
Eyigo?z et al (2013) showed that TAM-HMM per-
forms better than treating morphemes as words.
Since the multi-rate HMM allows both word
and morpheme sequence modeling, it is a gener-
alization of TAM-HMM, which allows only word
sequence modeling. TAM-HMM in turn is a gen-
eralization of the model suggested by Goldwater
and McClosky (2005) and TAM 1. Our results
show that multi-rate HMMs are superior to TAM-
HMMs. Therefore, multi-rate HMMs are the best
two-level alignment models proposed so far.
2 Two-level Alignment Model (TAM)
The two-level alignment model (TAM) takes the
approach of assigning probabilities to both word-
to-word translations and morpheme-to-morpheme
translations simultaneously, allowing morpheme-
level probabilities to guide alignment for rare word
pairs. TAM is based on a concept of alignment
defined at both the word and morpheme levels.
2.1 Morpheme Alignment
A word alignment aw is a function mapping a set
of word positions in a target language sentence e
to a set of word positions in a source language sen-
tence f , as exemplified in Figure 2. A morpheme
alignment am is a function mapping a set of mor-
pheme positions in a target language sentence to
a set of morpheme positions in a source language
sentence. A morpheme position is a pair of inte-
gers (j, k), which defines a word position j and a
relative morpheme position k in the word at posi-
tion j, as shown in Figure 3. The word and mor-
pheme alignments below are depicted in Figures 2
and 3.
aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the
following conditions: If the morpheme alignment
am maps a morpheme of e to a morpheme of f ,
then the word alignment aw maps e to f . If the
word alignment aw maps e to f , then the mor-
pheme alignment am maps at least one morpheme
of e to a morpheme of f . If the word align-
ment aw maps e to null, then all of its morphemes
are mapped to null. Figure 3 shows a morpheme
alignment that is compatible with, i.e., restricted
by, the word alignment in Figure 2. The smaller
boxes embedded inside the main box in Figure 3
depict the embedding of the morpheme level in-
side the word level in two-level alignment models
(TAM).
2.2 TAM 1
We call TAM without sequence modeling TAM 1,
because it defines an embedding of IBM Model 1
(Brown et al, 1993) for morphemes inside IBM
Model 1 for words. In TAM 1, p(e|f), the prob-
ability of translating the sentence f into e is com-
puted by summing over all possible word align-
ments and all possible morpheme alignments that
are compatible with a given word alignment aw:
Word Morpheme
Rw
|e|?
j=1
|f |?
i=0
?
?t(ej |fi) Rm
|ej |?
k=1
|fi|?
n=0
t(ekj |fni )
?
?
(1)
where fni is the nth morpheme of the word at po-
sition i. The probability of translating the word fi
into the word ej is computed by summing over all
possible morpheme alignments between the mor-
phemes of ej and fi. Rw substitutes P (le|lf )(lf+1)le for
easy readability.1 Rm is equivalent to Rw except
1le = |e| is the number of words in sentence e and lf =
|f |.
495
Figure 2: Word alignment Figure 3: Morpheme alignment
for the fact that its domain is not the set of sen-
tences but the set of words. The length of a word is
the number of morphemes in the word. The length
of words ej and fi in R(ej , fi) are the number of
morphemes of ej and fi. We assume that all un-
aligned morphemes in a sentence map to a special
null morpheme.
TAM 1 with the contribution of both word and
morpheme translation probabilities, as in Eqn. 1, is
called ?word-and-morpheme? version of TAM 1.
The model is technically deficient probabilisti-
cally, as it models word and morpheme transla-
tion independently, and assigns mass to invalid
word/morpheme combinations. We can also de-
fine the ?morpheme-only? version of TAM 1 by
canceling out the contribution of word translation
probabilities and assigning 1 to t(ej |fi) in Eqn. 1.
Please note that, although this version of the two-
level alignment model does not use word transla-
tion probabilities, it is also a word-aware model, as
morpheme alignments are restricted to correspond
to a valid word alignment. As such, it also allows
for word level sequence modeling by HMMs. Fi-
nally, canceling out the contribution of morpheme
translation probabilities reduces TAM 1 to IBM
Model 1. Just as IBM Model 1 is used for initial-
ization before HMM-based word-alignment mod-
els (Vogel et al, 1996; Och and Ney, 2003), TAM
Model 1 is used to initialize its HMM extensions,
which are described in the next section.
3 Multi-rate HMM
Like other multi-scale HMM models such as hi-
erarchical HMM?s (Fine et al, 1998) and hidden
Markov trees (Crouse et al, 1998), the multi-rate
HMM characterizes the inter-scale dependencies
by a tree structure. As shown in Figure 5, scales
are organized in a hierarchical manner from coarse
to fine, which allows for efficient representation of
both short- and long-distance context simultane-
ously.
We found that 51% of the dependency relations
in the Turkish Treebank (Oflazer et al, 2003) are
between the last morpheme of a dependent word
and the first morpheme (the root) of the head word
that is immediately to its right, which is exempli-
fied below. The following examples show English
sentences in Turkish word/morpheme order. The
pseudo Turkish words are formed by concatena-
tion of English morphemes, which are indicated
by the ?+? between the morphemes.
? ? I will come from X.
? X+ABL come+will+I
? ? I will look at X.
? X+DAT look+will+I
In English, the verb ?come? subcategorizes for
a PP headed by ?from? in the example above.
In the pseudo Turkish version of this sentence,
?come? subcategorizes for a NP marked with abla-
tive case (ABL), which corresponds to the prepo-
sition ?from?. Similarly, ?look? subcategorizes for
a PP headed by ?at? in English, and a NP marked
with dative case (DAT) in Turkish. Just as the verb
and the preposition that it subcategorizes for are
frequently found adjacent to each other in English,
the verb and the case that it subcategorizes for are
frequently found adjacent to each other in Turk-
ish. Thus, we have a pattern of three correspond-
ing morphemes appearing in reverse order in En-
glish and Turkish, spanning two words in Turkish
and three words in English. In order to capture
such regularities, we chose multi-rate HMMs over
other hierarchically structured HMM models be-
cause, unlike other models, multi-rate HMMs al-
low morpheme sequence modeling across words
over the entire sentence. This allows us to capture
morpheme-mediated syntactic relations between
words (Eryig?it et al, 2008), as exemplified above.
Morpheme sequence modeling across words is
shown in Figure 4 by the arrows after the nodes
496
Figure 4: Multi-rate HMM graph.
representing fam(0,2) and fam(1,2). The circles
represent the words and morphemes of the source
language, the squares represent the words and
morphemes of the target language. e0,2 is the last
morpheme of word e0, and e1,0 is the first mor-
pheme of the next word e1. fam(1,0) is conditioned
on fam(0,2), which is in the previous word.
In order to model the morpheme sequence
across words, we define the function prev(j, k),
which maps the morpheme position (j, k) to the
previous morpheme position:
prev(j, k) =
{
(j, k ? 1) if k > 1
(j ? 1, |ej?1|) if k = 1
If a morpheme is the first morpheme of a word,
then the previous morpheme is the last morpheme
of the previous word.
3.1 Transitions
3.1.1 Morpheme transitions
Before introducing the morpheme level transition
probabilities, we first restrict morpheme level tran-
sitions according to the assumptions of our model.
We consider only the morpheme alignment func-
tions that are compatible with a word alignment
function. If we allow unrestricted transitions be-
tween morphemes, then this would result in some
morpheme alignments that do not allow a valid
word alignment function.
To avoid this problem, we restrict the transi-
tion function as follows: at each time step, we
allow transitions between morphemes in sentence
f if the morphemes belong to the same word.
This restriction reduces the transition matrix to a
block diagonal matrix. The block diagonal matrix
Ab below is a square matrix which has blocks of
square matrices A1 ? ? ?An on the main diagonal,
and the off-diagonal values are zero.
Ab =
?
????
A0 0 ? ? ? 0
0 A1 ? ? ? 0... ... . . . ...
0 0 ? ? ? An
?
????
The square blocks A0, . . . ,An have the dimen-
sions |f0|, . . . , |fn|, the length of the words in sen-
tence f . In each step of the forward-backward al-
gorithm, multiplying the forward (or backward)
probability vectors with the block diagonal ma-
trix restricts morpheme transitions to occur only
within the words of sentence f .
In order to model the morpheme sequence
across words, we also allow transitions between
morphemes across the words in sentence f . How-
ever, we allow cross-word transitions only at cer-
tain time steps: between the last morpheme of a
word in sentence e and the first morpheme of the
next word in sentence e. This does not result in
morpheme alignments that do not allow a valid
word alignment function. Instead of the block di-
agonal matrix Ab, we use a transition matrix A
which is not necessarily block diagonal, to model
morpheme transitions across words.
In sum, we multiply the forward (or backward)
probability vectors with either the transition ma-
trix Ab or the transition matrix A, depending on
whether the transition is occurring at the last mor-
pheme of a word in e. We introduce the function
?(p, q, r, s) to indicate whether a transition is al-
lowed from source position (p, q) to source posi-
497
tion (r, s) when advancing one target position:
?(p, q, r, s) =
{
1 if p = r or s = 1
0 otherwise
Morpheme transition probabilities have four
components. First, the ? function as described
above. Second, the jump width:
J (p, q, r, s) = abs(r, s)? abs(p, q)
where abs(j, k) maps a word-relative morpheme
position to an absolute morpheme position, i.e., to
the simple left-to-right ordering of a morpheme in
a sentence. Third, the morpheme class of the pre-
vious morpheme:2
M(p, q) = Class(f qp )
Fourth, as the arrow from faw(0) to fam(0,0) in Fig-
ure 4 shows, there is a conditional dependence on
the word class that the morpheme is in:
W(r) = Class(fr)
Putting together these components, the morpheme
transitions are formulated as follows:
p(am(j, k) = (r, s) | am(prev(j, k)) = (p, q)) ?
p
(
J (p, q, r, s)|M(p, q),W(r)
)
?(p, q, r, s)
(2)
The block diagonal matrix Ab consists of mor-
pheme transition probabilities.
3.1.2 Word transitions
In the multi-rate HMM, word transition probabili-
ties have two components. First, the jump width:
J (p, r) = r ? p
Second, the word class of the previous word:
W(p) = Class(fp)
The jump width is conditioned on the word class
of the previous word:
p(aw(j) = r | aw(j ? 1) = p) ?
p(J (p, r) | W(p)) (3)
The transition matrix A, which is not necessarily
block diagonal, consists of values which are the
product of a morpheme transition probability, as
defined in Eqn. 2, and a word transition probabil-
ity, as defined in Eqn. 3.
2We used the mkcls tool in GIZA (Och and Ney, 2003)
to learn the word and the morpheme classes.
3.2 Probability of translating a sentence
Finally, putting together Eqn. 1, Eqn. 2 and Eqn. 3,
we formulate the probability of translating a sen-
tence p(e|f) as follows:
Rw
?
aw
|e|?
j=1
(
t(ej |faw(j))p(aw(j)|aw(j?1))
Rm
?
am
|ej |?
k=1
t(ej,k|fam(j,k))
p(am(j,k)|am(prev(j,k)))
)
Rw is the same as it is in Eqn. 1, whereas
Rm = P (le|lf ). If we cancel out morpheme tran-
sitions by setting p(am(j, k)|am(prev(j, k))) =
1/|fam(j,k)|, i.e., with a uniform distribution, then
we get TAM with only word-level sequence mod-
eling, which we call TAM-HMM.
The complexity of the multi-rate HMM is
O(m3n3), where n is the number of words, and
m is the number of morphemes per word. TAM-
HMM differs from multi-rate HMM only by the
lack of morpheme-level sequence modeling, and
has complexity O(m2n3).
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions.
We learn the probabilities of jumping to a null po-
sition from the data. To compute the transition
probability from a null position, we keep track of
the nearest previous source word (or morpheme)
that does not align to null, and use the position of
the previous non-null word to calculate the jump
width. In order to keep track of the previous non-
null word, we insert a null word between words
(Och and Ney, 2003). Similarly, we insert a null
morpheme after every non-null morpheme.
3.3 Counts
We use Expectation Maximization (EM) to learn
the word and morpheme translation probabili-
ties, as well as the transition probabilities of the
reordering model. This is done with forward-
backward training at the morpheme level, collect-
ing translation and transition counts for both the
word and the morphemes from the morpheme-
level trellis.
In Figure 5, the grid on the right depicts the
morpheme-level trellis. The grid on the left is
the abstraction of the word-level trellis over the
498
Figure 5: Multi-rate HMM trellis
morpheme-level trellis. For each target word e and
for each source word f , there is a small HMM trel-
lis with dimensions |e|?|f | inside the morpheme-
level trellis, as shown by the shaded area inside the
grid on the right. We collect counts for words by
summing over the values in the small HMM trellis
associated with the words.
3.3.1 Translation counts
Morpheme translation counts We compute ex-
pected counts over the morpheme-level trellis.
The morpheme translation count function below
collects expected counts for a morpheme pair
(h, g) in a sentence pair (e, f):
cm(h|g; e, f) =
?
(j,k)
s.t.
h=ekj
?
(p,q)
s.t.
g=fqp
?j,k(p, q)
where ?j,k(p, q) stands for the posterior mor-
pheme translation probabilities for source position
(p, q) and target position (i, j) that are computed
with the forward-backward algorithm.
Word translation counts For each target word
e and source word f , we collect word transla-
tion counts by summing over posterior morpheme
translation probabilities that are in the small trellis
associated with e and f .
Since ? allows only within-word transitions to
occur inside the small trellis, the posterior proba-
bility of observing the word e given the word f
is preserved across time points within the small
trellis associated with e and f . In other words,
the sum of the posterior probabilities in each col-
umn of the small trellis is the same. Therefore, we
collect word translation counts only from the last
morphemes of the words in e.
The word translation count function below col-
lects expected counts from a sentence pair (e, f)
for a particular source word f and target word e:
cw(e|f ; e, f) =
?
j
s.t.
e=ej
?
p
s.t.
f=fp
?
1?q?|f |
?j,|e|(p, q)
3.3.2 Transition counts
Morpheme transition counts For all target po-
sitions (j, k) and all pairs of source positions (p, q)
and (r, s), we compute morpheme transition pos-
teriors:
?j,k((p, q), (r, s))
using the forward-backward algorithm. These
expected counts are accumulated to esti-
mate the morpheme jump width probabilities
p
(
J (p, q, r, s)|M(p, q),W(r)
) used in Eqn. 2.
Word transition counts We compute posterior
probabilities for word transitions by summing over
morpheme transition posteriors between the mor-
phemes of the words fl and fn:
?j(p, r) =
?
1?q?|fp|
?
1?s?|fr|
?j,|ej |((p, q), (r, s))
Like the translation counts, the transition counts
are collected from the last morphemes of words
in e. These expected counts are accumulated
to estimate the word jump width probabilities
p(J (p, r) | W(p)) used in Eqn. 3.
Finally, Rm = P (le|lf ) does not cancel out in
the counts of the multi-rate HMM. To compute the
conditional probability P (le|lf ), we assume that
the length of word e varies according to a Poisson
distribution with a mean that is linear with length
of the word f (Brown et al, 1993).
3.4 Variational Bayes
In order to prevent overfitting, we use the Varia-
tional Bayes extension of the EM algorithm (Beal,
2003). This amounts to a small change to the
M step of the original EM algorithm. We in-
troduce Dirichlet priors ? to perform an inexact
normalization by applying the function f(v) =
exp(?(v)) to the expected counts collected in the
E step, where ? is the digamma function (John-
son, 2007). The M-step update for a multinomial
parameter ?x|y becomes:
?x|y =
f(E[c(x|y)] + ?)
f(
?
j E[c(xj |y)] + ?)
499
Multi-rate
HMM
TAM-HMM WORD
Word-
Morph 
Morph 
only IBM 4 Baseline
BLEU TR to EN 30.82 29.48 29.98 29.13 27.91EN to TR 23.09 22.55  22.54 21.95 21.82
AER 0.254 0.255 0.256 0.375 0.370
Table 1: AER and BLEU Scores
We set ? to 10?20, a very low value, to have the
effect of anti-smoothing, as low values of ? cause
the algorithm to favor words which co-occur fre-
quently and to penalize words that co-occur rarely.
We used Dirichlet priors on morpheme translation
probabilities.
4 Experiments and Results
4.1 Data
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such
as the Turkish Ministry of Foreign Affairs, EU,
etc. The Turkish data was first morphologically
parsed (Oflazer, 1994), then disambiguated (Sak
et al, 2007) to select the contextually salient inter-
pretation of words. In addition, we removed mor-
phological features that are not explicitly marked
by an overt morpheme. For English, we use part-
of-speech tagged data. The number of English
words is 1,033,726 and the size of the English vo-
cabulary is 28,647. The number of Turkish words
is 812,374, the size of the Turkish vocabulary is
57,249. The number of Turkish morphemes is
1,484,673 and the size of the morpheme vocab-
ulary is 16,713.
4.2 Experiments
We initialized our implementation of the single
level ?word-only? model, which we call ?baseline?
in Table 1, with 5 iterations of IBM Model 1, and
further trained the HMM extension (Vogel et al,
1996) for 5 iterations. Similarly, we initialized
TAM-HMM and multi-rate HMM with 5 iterations
of TAM 1 as explained in Section 2.2. Then we
trained TAM-HMM and the multi-rate HMM for 5
iterations. We also ran GIZA++ (IBM Model 1?4)
on the data. We translated 1000 sentence test sets.
We used Dirichlet priors in both IBM Model 1
and TAM 1 training. We experimented with using
Dirichlet priors on the HMM extensions of both
IBM-HMM and TAM-HMM. We report the best
results obtained for each model and translation di-
rection.
We evaluated the performance of our model in
two different ways. First, we evaluated against
gold word alignments for 75 Turkish-English sen-
tences. Table 1 shows the AER (Och and Ney,
2003) of the word alignments; we report the grow-
diag-final (Koehn et al, 2003) of the Viterbi align-
ments. Second, we used the Moses toolkit (Koehn
et al, 2007) to train machine translation systems
from the Viterbi alignments of our various models,
and evaluated the results with BLEU (Papineni et
al., 2002).
In order to reduce the effect of nondetermin-
ism, we run Moses three times per experiment set-
ting, and report the highest BLEU scores obtained.
Since the BLEU scores we obtained are close,
we did a significance test on the scores (Koehn,
2004). In Table 1, the colors partition the table
into equivalence classes: If two scores within the
same row have different background colors, then
the difference between their scores is statistically
significant. The best scores in the leftmost column
were obtained from multi-rate HMMs with Dirich-
let priors only during the TAM 1 training. On the
contrary, the best scores for TAM-HMM and the
baseline-HMM were obtained with Dirichlet pri-
ors both during the TAM 1 and the TAM-HMM
500
training. In Table 1, as the scores improve grad-
ually towards the left, the background color gets
gradually lighter, depicting the statistical signifi-
cance of the improvements. The multi-rate HMM
performs better than the TAM-HMM, which in
turn performs better than the word-only models.
5 Conclusion
We presented a multi-rate HMM word alignment
model, which models the word and the morpheme
sequence simultaneously. We have tested our
model on the Turkish-English pair and showed
that our model is superior to the two-level word
alignment model which has sequence modeling
only at the word level.
Acknowledgments Partially funded by NSF
award IIS-0910611. Kemal Oflazer acknowledges
the generous support of the Qatar Foundation
through Carnegie Mellon University?s Seed Re-
search program. The statements made herein are
solely the responsibility of this author(s), and not
necessarily that of Qatar Foundation.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, Final Report, JHU Summer Work-
shop.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
O?zgu?r C?etin, Mari Ostendorf, and Gary D. Bernard.
2007. Multirate coupled Hidden Markov Models
and their application to machining tool-wear clas-
sification. IEEE Transactions on Signal Processing,
55(6):2885?2896, June.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718?726.
Martin C?mejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83?90.
Matthew Crouse, Robert Nowak, and Richard Bara-
niuk. 1998. Wavelet-based statistical signal pro-
cessing using Hidden Markov Models. IEEE Trans-
actions on Signal Processing, 46(4):886?902.
Gu?ls?en Eryig?it, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Computa-
tional Linguistics, 34(3):357?389.
Elif Eyigo?z, Daniel Gildea, and Kemal Oflazer. 2013.
Simultaneous word-morpheme alignment for statis-
tical machine translation. In NAACL.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical Hidden Markov model: Analysis
and applications. Machine Learning, 32(1):41?62,
July.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In HLT-EMNLP.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In EMNLP-CoNLL, pages
296?305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Young-suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In HLT-NAACL, pages
57?60.
Jia Li, Robert Gray, and Richard Olshen. 2006.
Multiresolution image classification by hierarchi-
cal modeling with two-dimensional Hidden Markov
Models. IEEE Transactions on Information Theory,
46(5):1826?1841, September.
Mark R. Luettgen, William C. Karl, Alan S. Willsky,
and Robert R. Tenney. 1993. Multiscale representa-
tions of Markov Random Fields. IEEE Transactions
on Signal Processing, 41(12):3377?3396.
Sonja Niessen and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
COLING, pages 1081?1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
Models. Computational Linguistics, 29(1):19?51.
501
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r. 2003. Building a Turkish treebank. In
A. Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, pages 261?277. Kluwer, London.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02), pages 311?
318.
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107?118.
Marios Skounakis, Mark Craven, and Soumya Ray.
2003. Hierarchical Hidden Markov Models for in-
formation extraction. In International Joint Con-
ference on Artificial Intelligence, volume 18, pages
427?433.
Georgios Theocharous, Khashayar Rohanimanesh, and
Sridhar Maharlevan. 2001. Learning hierarchi-
cal observable Markov decision process Models for
robot navigation. In ICRA 2001, volume 1, pages
511?516.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING, pages 836?841.
Alan S. Willsky. 2002. Multiresolution Markov Mod-
els for signal and image processing. In Proceedings
of the IEEE, pages 1396?1458.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from English to Turkish.
In ACL 2010, pages 454?464.
502
