Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 21?24,
New York, June 2006. c?2006 Association for Computational Linguistics
Class Model Adaptation for Speech Summarisation
Pierre Chatain, Edward W.D. Whittaker, Joanna Mrozinski and Sadaoki Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, Japan
{pierre, edw, mrozinsk, furui}@furui.cs.titech.ac.jp
Abstract
The performance of automatic speech
summarisation has been improved in pre-
vious experiments by using linguistic
model adaptation. We extend such adapta-
tion to the use of class models, whose ro-
bustness further improves summarisation
performance on a wider variety of objec-
tive evaluation metrics such as ROUGE-2
and ROUGE-SU4 used in the text sum-
marisation literature. Summaries made
from automatic speech recogniser tran-
scriptions benefit from relative improve-
ments ranging from 6.0% to 22.2% on all
investigated metrics.
1 Introduction
Techniques for automatically summarising written
text have been actively investigated in the field of
natural language processing, and more recently new
techniques have been developed for speech sum-
marisation (Kikuchi et al, 2003). However it is
still very hard to obtain good quality summaries.
Moreover, recognition accuracy is still around 30%
on spontaneous speech tasks, in contrast to speech
read from text such as broadcast news. Spontaneous
speech is characterised by disfluencies, repetitions,
repairs, and fillers, all of which make recognition
and consequently speech summarisation more diffi-
cult (Zechner, 2002). In a previous study (Chatain
et al, 2006), linguistic model (LiM) adaptation us-
ing different types of word models has proved use-
ful in order to improve summary quality. However
sparsity of the data available for adaptation makes it
difficult to obtain reliable estimates of word n-gram
probabilities. In speech recognition, class models
are often used in such cases to improve model ro-
bustness. In this paper we extend the work previ-
ously done on adapting the linguistic model of the
speech summariser by investigating class models.
We also use a wider variety of objective evaluation
metrics to corroborate results.
2 Summarisation Method
The summarisation system used in this paper is es-
sentially the same as the one described in (Kikuchi
et al, 2003), which involves a two step summarisa-
tion process, consisting of sentence extraction and
sentence compaction. Practically, only the sentence
extraction part was used in this paper, as prelimi-
nary experiments showed that compaction had little
impact on results for the data used in this study.
Important sentences are first extracted accord-
ing to the following score for each sentence
W = w1, w2, ..., wn, obtained from the automatic
speech recognition output:
S(W ) =
1
N
N?
i=1
{?CC(wi)+?II(wi)+?LL(wi)},
(1)
where N is the number of words in the sentence
W , and C(wi), I(wi) and L(wi) are the confidence
score, the significance score and the linguistic score
of word wi, respectively. ?C , ?I and ?L are the
respective weighting factors of those scores, deter-
mined experimentally.
For each word from the automatic speech recogni-
21
tion transcription, a logarithmic value of its posterior
probability, the ratio of a word hypothesis probabil-
ity to that of all other hypotheses, is calculated using
a word graph obtained from the speech recogniser
and used as a confidence score.
For the significance score, the frequencies of oc-
currence of 115k words were found using the WSJ
and the Brown corpora.
In the experiments in this paper we modified the
linguistic component to use combinations of dif-
ferent linguistic models. The linguistic component
gives the linguistic likelihood of word strings in
the sentence. Starting with a baseline LiM (LiMB)
we perform LiM adaptation by linearly interpolat-
ing the baseline model with other component mod-
els trained on different data. The probability of a
given n-gram sequence then becomes:
P (wi|wi?n+1..wi?1) = ?1P1(wi|wi?n+1..wi?1)
+... + ?nPn(wi|wi?n+1..wi?1), (2)
where
?
k ?k = 1 and ?k and Pk are the weight and
the probability assigned by model k.
In the case of a two-sided class-based model,
Pk(wi|wi?n+1..wi?1) = Pk(wi|C(wi)) ?
Pk(C(wi)|C(wi?n+1)..C(wi?1)), (3)
where Pk(wi|C(wi)) is the probability of the
word wi belonging to a given class C, and
Pk(C(wi)|C(wi?n+1)..C(wi?1)) the probability of
a certain word class C(wi) to appear after a history
of word classes, C(wi?n+1), ..., C(wi?1).
Different types of component LiM are built, com-
ing from different sources of data, either as word
or class models. The LiMB and component LiMs
are then combined for adaptation using linear inter-
polation as in Equation (2). The linguistic score is
then computed using this modified probability as in
Equation (4):
L(wi) = logP (wi|wi?n+1..wi?1). (4)
3 Evaluation Criteria
3.1 Summarisation Accuracy
To automatically evaluate the summarised speeches,
correctly transcribed talks were manually sum-
marised, and used as the correct targets for evalua-
tion. Variations of manual summarisation results are
merged into a word network, which is considered to
approximately express all possible correct summari-
sations covering subjective variations. The word ac-
curacy of automatic summarisation is calculated as
the summarisation accuracy (SumACCY) using the
word network (Hori et al, 2003):
Accuracy = (Len?Sub?Ins?Del)/Len?100[%],
(5)
where Sub is the number of substitution errors, Ins
is the number of insertion errors, Del is the number
of deletion errors, and Len is the number of words
in the most similar word string in the network.
3.2 ROUGE
Version 1.5.5 of the ROUGE scoring algorithm
(Lin, 2004) is also used for evaluating results.
ROUGE F-measure scores are given for ROUGE-
2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4
(skip-bigram), using the model average (average
score across all references) metric.
4 Experimental Setup
Experiments were performed on spontaneous
speech, using 9 talks taken from the Translanguage
English Database (TED) corpus (Lamel et al, 1994;
Wolfel and Burger, 2005), each transcribed and
manually summarised by nine different humans for
both 10% and 30% summarization ratios. Speech
recognition transcriptions (ASR) were obtained for
each talk, with an average word error rate of 33.3%.
A corpus consisting of around ten years of con-
ference proceedings (17.8M words) on the subject
of speech and signal processing is used to generate
the LiMB and word classes using the clustering al-
gorithm in (Ney et al, 1994).
Different types of component LiM are built and
combined for adaptation as described in Section 2.
The first type of component linguistic models are
built on the small corpus of hand-made summaries
described above, made for the same summarisation
ratio as the one we are generating. For each talk
the hand-made summaries of the other eight talks
(i.e. 72 summaries) were used as the LiM training
corpus. This type of LiM is expected to help gener-
ate automatic summaries in the same style as those
made manually.
22
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 34.4 0.104 0.055 0.142 - - - -
Word 63.1 0.186 0.130 0.227 67.8 0.193 0.140 0.228
Class 65.1 0.195 0.131 0.226 72.6 0.210 0.143 0.234
Mixed 63.6 0.186 0.128 0.218 71.8 0.211 0.139 0.231
30% Random 71.2 0.294 0.198 0.331 - - - -
Word 81.6 0.365 0.271 0.395 83.3 0.365 0.270 0.392
Class 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Mixed 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Table 1: TRS baseline and adapted results.
The second type of component linguistic models
are built from the papers in the conference proceed-
ings for the talk we want to summarise. This type
of LiM, used for topic adaptation, is investigated be-
cause key words and important sentences that appear
in the associated paper are expected to have a high
information value and should be selected during the
summarisation process.
Three sets of experiments were made: in the first
experiment (referred to as Word), LiMB and both
component models are word models, as introduced
in (Chatain et al, 2006). For the second one (Class),
both LiMB and the component models are class
models built using exactly the same data as the word
models. For the third experiment (Mixed), the LiMB
is an interpolation of class and word models, while
the component LiMs are class models.
To optimise use of the available data, a rotating
form of cross-validation (Duda and Hart, 1973) is
used: all talks but one are used for development, the
remaining talk being used for testing. Summaries
from the development talks are generated automati-
cally by the system using different sets of parameters
and the LiMB . These summaries are evaluated and
the set of parameters which maximises the develop-
ment score for the LiMB is selected for the remain-
ing talk. The purpose of the development phase is
to choose the most effective combination of weights
?C , ?I and ?L. The summary generated for each
talk using its set of optimised parameters is then
evaluated using the same metric, which gives us our
baseline for this talk. Using the same parameters as
those that were selected for the baseline, we gener-
ate summaries for the lectures in the development set
for different LiM interpolation weights ?k. Values
between 0 and 1 in steps of 0.1, were investigated
for the latter, and an optimal set of ?k is selected.
Using these interpolation weights, as well as the set
of parameters determined for the baseline, we gen-
erate a summary of the test talk, which is evaluated
using the same evaluation metric, giving us our fi-
nal adapted result for this talk. Averaging those re-
sults over the test set (i.e. all talks) gives us our final
adapted result.
This process is repeated for all evaluation metrics,
and all three experiments (Word, Class, and Mixed).
Lower bound results are given by random sum-
marisation (Random) i.e. randomly extracting sen-
tences and words, without use of the scores present
in Equation (1) for appropriate summarisation ratios.
5 Results
5.1 TRS Results
Initial experiments were made on the human tran-
scriptions (TRS), and results are given in Table 1.
Experiments on word models (Word) show relative
improvements in terms of SumACCY of 7.5% and
2.1% for the 10% and 30% summarisation ratios, re-
spectively. ROUGE metrics, however, do not show
any significant improvement.
Using class models (Class and Mixed), for all
ROUGE metrics, relative improvements range from
3.5% to 13.4% for the 10% summarisation ratio, and
from 8.6% to 16.5% on the 30% summarisation ra-
tio. For SumACCY, relative improvements between
11.5% to 12.9% are observed.
5.2 ASR Results
ASR results for each experiment are given in Ta-
ble 2 for appropriate summarisation ratios. As for
23
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 33.9 0.095 0.042 0.140 - - - -
Word 48.6 0.143 0.064 0.182 49.8 0.129 0.060 0.173
Class 50.0 0.133 0.063 0.170 55.1 0.156 0.077 0.193
Mixed 48.5 0.134 0.068 0.176 56.2 0.142 0.077 0.191
30% Random 56.1 0.230 0.124 0.283 - - - -
Word 66.7 0.265 0.157 0.314 68.7 0.271 0.161 0.328
Class 66.1 0.277 0.165 0.324 71.1 0.300 0.180 0.348
Mixed 64.9 0.268 0.160 0.312 70.5 0.304 0.192 0.351
Table 2: ASR baseline and adapted results.
the TRS, LiM adaptation showed improvements in
terms of SumACCY, but ROUGE metrics do not cor-
roborate those results for the 10% summarisation ra-
tio. Using class models, for all ROUGE metrics, rel-
ative improvements range from 6.0% to 22.2% and
from 7.4% to 20.0% for the 10% and 30% summari-
sation ratios, respectively. SumACCY relative im-
provements range from 7.6% to 15.9%.
6 Discussion
Compared to previous experiments using only word
models, improvements obtained using class models
are larger and more significant for both ROUGE and
SumACCY metrics. This can be explained by the
fact that the data we are performing adaptation on
is very sparse, and that the nine talks used in these
experiments are quite different from each other, es-
pecially since the speakers also vary in style. Class
models are more robust to this spontaneous speech
aspect than word models, since they generalise bet-
ter to unseen word sequences.
There is little difference between the Class and
Mixed results, since the development phase assigned
most weight to the class model component in the
Mixed experiment, making the results quite similar
to those of the Class experiment.
7 Conclusion
In this paper we have investigated linguistic model
adaptation using different sources of data for an au-
tomatic speech summarisation system. Class mod-
els have proved to be much more robust than word
models for this process, and relative improvements
ranging from 6.0% to 22.2% were obtained on a va-
riety of evaluation metrics on summaries generated
from automatic speech recogniser transcriptions.
Acknowledgements: The authors would like to
thank M. Wo?lfel for the recogniser transcriptions
and C. Hori for her work on two stage summarisa-
tion and gathering the TED corpus data. This work
is supported by the 21st Century COE Programme.
References
P. Chatain, E.W.D. Whittaker, J. Mrozinski, and S. Fu-
rui. 2006. Topic and Stylistic Adaptation for Speech
Summarization. Proc. ICASSP, Toulouse, France.
R. Duda and P. Hart. 1973. Pattern Classification and
Scene Analysis. Wiley, New York.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation
Method for Automatic Speech Summarization. Proc.
Eurospeech, Geneva, Switzerland, 4:2825?2828.
T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic
Speech Summarization based on Sentence Extraction
and Compaction. Proc. ICASSP, Hong Kong, China,
1:236?239.
L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-
mann. 1994. The Translanguage English Database
(TED). Proc. ICSLP, Yokohama, Japan, 4:1795?1798.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. Proc. WAS, Barcelona,
Spain.
H. Ney, U. Essen, and R. Kneser. 1994. On Structur-
ing Probabilistic Dependences in Stochastic Language
Modelling. Computer Speech and Language, (8):1?
38.
M. Wolfel and S. Burger. 2005. The ISL Baseline Lec-
ture Transcription System for the TED Corpus. Tech-
nical report, Karlsruhe University.
K. Zechner. 2002. Summarization of Spoken Language-
Challenges, Methods, and Prospects. Speech Technol-
ogy Expert eZine, Issue.6.
24
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 288?291,
New York City, June 2006. c?2006 Association for Computational Linguistics
Factoid Question Answering with Web, Mobile and Speech Interfaces
E.W.D. Whittaker J. Mrozinski S. Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1, Ookayama, Meguro-ku
Tokyo 152-8552 Japan
 
edw,mrozinsk,furui  @furui.cs.titech.ac.jp
Abstract
In this paper we describe the web and
mobile-phone interfaces to our multi-
language factoid question answering (QA)
system together with a prototype speech
interface to our English-language QA sys-
tem. Using a statistical, data-driven ap-
proach to factoid question answering has
allowed us to develop QA systems in five
languages in a matter of months. In the
web-based system, which is accessible
at http://asked.jp, we have com-
bined the QA system output with standard
search-engine-like results by integrating it
with an open-source web search engine.
The prototype speech interface is based
around a VoiceXML application running
on the Voxeo developer platform. Recog-
nition of the user?s question is performed
on a separate speech recognition server
dedicated to recognizing questions. An
adapted version of the Sphinx-4 recog-
nizer is used for this purpose. Once the
question has been recognized correctly it
is passed to the QA system and the re-
sulting answers read back to the user by
speech synthesis. Our approach is mod-
ular and makes extensive use of open-
source software. Consequently, each com-
ponent can be easily and independently
improved and easily extended to other lan-
guages.
1 Introduction
The approach to factoid question answering (QA)
that we adopt was first described in (Whittaker et
al., 2005b) where the details of the mathematical
model and how it was trained for English were
given. The approach has been successfully evalu-
ated in the 2005 text retrieval conference (TREC)
question answering track evaluations (Voorhees and
Trang Dang, 2005) where our group placed eleventh
out of thirty participants (Whittaker et al, 2005a).
Although the TREC QA task is substantially differ-
ent to web-based QA this evaluation showed that our
approach works and provides an objective assess-
ment of its quality. Similarly, for our Japanese lan-
guage system we have evaluated the performance of
our approach on the NTCIR-3 QAC-1 task (Whit-
taker et al, 2005c). Although our Japanese ex-
periments were applied retrospectively, the results
would have placed us in the mid-range of partici-
pating systems. In (Whittaker et al, 2006b) we de-
scribed how our approach could be used for the rapid
development of web-based QA systems in five very
different languages. It was shown that a developer
proficient with the tools, and with access to suitable
training data, could build a system in a new language
in around 10 hours. In (Whittaker et al, 2006a) we
evaluated the performance of the systems for four of
our five languages. We give a brief summary of our
approach to QA in Section 2.
In this paper we introduce our web-based
QA system which is publicly accessible at
http://asked.jp, permitting questions in En-
glish, Japanese, Chinese, Russian and Swedish and
288
is discussed in Section 3. Since answers in factoid
QA are inherently well-suited to display on small
screens we have also made a mobile-phone interface
which is accessible at the same address when using
an HTML browser from a mobile phone. This is dis-
cussed in Section 4. There are several other QA sys-
tems on the web including Brainboost (Brainboost,
2005) and Lexxe (Lexxe, 2005) but they only try to
answer questions in English and do not have conve-
nient mobile interfaces.
Entering whole questions rather than just key-
words is tedious especially on a mobile-phone so
we have also begun to look at speech interfaces. In
this paper we describe a prototype speech interface
to our English-language QA system. This prototype
is currently intended primarily as a platform for fur-
ther research into speech recognition and answering
of questions from an acoustic modelling point-of-
view (e.g. low-bandwidth, low-quality VoIP chan-
nel), from a language modelling perspective (e.g. ir-
regular word order in questions vs. text, and very
large out-of-vocabulary problem) and also in terms
of dialog modelling. There have been several at-
tempts at speech interfaces to QA systems in the lit-
erature e.g. (Schofield and Zheng, 2003) but as far
as we know ours is the only system that is publicly
accessible. We discuss this interface in Section 5.
2 Statistical pattern classification
approach to QA
The answer to a question depends primarily on the
question itself but also on many other factors such
as the person asking the question, the location of the
person, what questions the person has asked before,
and so on. For simplicity, we choose to consider
only the dependence of an answer   on the question

. In particular, we hypothesize that the answer  
depends on two sets of features extracted from

:


	
and  

	
as follows:

 

	

 


	
 (1)
where

can be thought of as a set of Proceedings of NAACL HLT 2009: Short Papers, pages 273?276,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Chinese Abbreviation Generation Using Conditional Random
Field
Dong Yang, Yi-cheng Pan, and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552 Japan
{raymond,thomas,furui}@furui.cs.titech.ac.jp
Abstract
This paper presents a new method for au-
tomatically generating abbreviations for Chi-
nese organization names. Abbreviations are
commonly used in spoken Chinese, especially
for organization names. The generation of
Chinese abbreviation is much more complex
than English abbreviations, most of which are
acronyms and truncations. The abbreviation
generation process is formulated as a character
tagging problem and the conditional random
field (CRF) is used as the tagging model. A
carefully selected group of features is used in
the CRF model. After generating a list of ab-
breviation candidates using the CRF, a length
model is incorporated to re-rank the candi-
dates. Finally the full-name and abbreviation
co-occurrence information from a web search
engine is utilized to further improve the per-
formance. We achieved top-10 coverage of
88.3% by the proposed method.
1 Introduction
Long named entities are frequently abbreviated in
oral Chinese language for efficiency and simplic-
ity. Therefore, abbreviation modeling is an impor-
tant building component for many systems that ac-
cept spoken input, such as directory assistance and
voice search systems.
While English abbreviations are usually formed
as acronyms, Chinese abbreviations are much more
complex, as shown in Figure 1. Most of the Chi-
nese abbreviations are formed by selecting several
characters from full-names, which are not necessar-
ily the first character of each word. Usually the orig-
inal character order in the full-name is preserved in
???? ?? T s i n g h u a U n i v e r s i t y
??????? ?? C h i n a c e n t r a l t e l e v i s i o n
F u l l ? n a m e  a b b r e v i a t i o n  E n g l i s h  e x p l a n a t i o n
?????? ?? ???? P e k i n g  U n i v e r s i t y  N o . 3  h o s p i t a l
Figure 1: Chinese abbreviation examples
the abbreviation. However, re-ordering of charac-
ters as shown in the third example in Figure 1 where
characters ?n? and ??? are swapped in the abbre-
viation, also happens.
There has been a considerable amount of research
on extracting full-name and abbreviation pairs in
the same document for obtaining abbreviations (Li
and Yarowsky, 2008; Sun et al, 2006; Fu et al,
2006). However, generation of abbreviations given
a full-name is still a non-trivial problem. Chang
and Lai (Chang and Lai, 2004) have proposed using
a hidden Markov model to generate abbreviations
from full-names. However, their method assumes
that there is no word-to-null mapping, which means
that every word in the full-name has to contribute at
least one character to the abbreviation. This assump-
tion does not hold for organizations? names which
have many word skips in the abbreviation genera-
tion.
The CRF was first introduced to natural language
processing (NLP) by (Lafferty et al, 2001) and has
been widely used in word segmentation, part-of-
speech (POS) tagging, and some other NLP tasks.
In this paper, we convert the Chinese abbreviation
generation process to a CRF tagging problem. The
key problem here is how to find a group of discrim-
273
inant and robust features. After using the CRF, we
get a list of abbreviation candidates with associate
probability scores. We also use the prior condi-
tional probability of the length of the abbreviations
given the length of the full-names to complement the
CRF probability scores. Such global information is
hard to include in the CRF model. In addition, we
apply the full-name and abbreviation candidate co-
occurrence statistics obtained on the web to increase
the correctness of the abbreviation candidates.
2 Chinese Abbreviation Introduction
Chinese abbreviations are generated by three meth-
ods (Lee, 2005): reduction, elimination, and gener-
alization.
Both in the reduction and elimination methods,
characters are selected from the full-name, and the
order of the characters is sometime changed. Note
that this paper does not cover the case when the or-
der is changed. The elimination means that one or
more words in the full-name are ignored completely,
while the reduction requires that at least one char-
acter is selected from each word. All the three ex-
amples in Figure 1 are produced by the elimination,
where at least one word is skipped.
Generalization, which is used to abbreviate a list
of similar terms, is usually composed of the number
of terms and a shared character across the terms. A
example is ?n? (three forces) for ?????
?? (land force, sea force, air force). This is the
most difficult scenario for the abbreviations and is
not considered in this paper.
3 CRF Model for Abbreviation Modeling
3.1 CRF model
A CRF is an undirected graphical model and assigns
the following probability to a label sequence L =
l1l2 . . . lT , given an input sequence C = c1c2 . . . cT ,
P (L|C) = 1Z(C)exp(
T?
t=1
?
k
?kfk(lt, lt?1, C, t))
(1)
Here, fk is the feature function for the k-th fea-
ture, ?k is the parameter which controls the weight
of the k-th feature in the model, and Z(C) is the nor-
malization term that makes the summation of the
probability of all label sequences to 1. CRF training
is usually performed through the typical L-BFGS al-
gorithm (Wallach, 2002) and decoding is performed
by Viterbi algorithm (Viterbi, 1967). In this paper,
we use an open source toolkit ?crf++?.
3.2 Abbreviation modeling as a tagging
problem
In order to use the CRF method in abbreviation gen-
eration, the abbreviation generation problem was
converted to a tagging problem. The character is
used as a tagging unit and each character in a full-
name is tagged by a binary variable with the values
of either Y or N: Y stands for a character used in the
abbreviation and N means not. An example is given
in Figure 2.
??????? ??
?/ N ?/ N ?/ N ?/ Y ?/ N ?/ Y ?/ N
Figure 2: Abbreviation in the CRF tagging format
3.3 Feature selection for the CRF
In the CRF method, feature function describes
a co-occurrence relation, and it is defined as
fk(lt, lt?1, C, t) (Eq. 1). fk is usually a binary func-
tion, and takes the value 1 when both observation ct
and transition lt?1 ? lt are observed. In our ab-
breviation generation model, we use the following
features:
1. Current character The character itself is the
most important feature for abbreviation as it will be
either retained or discarded. For example, ??? (bu-
reau) and ??? (institue), indicating a government
department, are very common characters used in ab-
breviations. When they appear in full-names, they
are likely to be kept in abbreviations.
2. Current word In the full name of ??I??
??? (China Agricultural university), the word ??
I? (China) is usually ignored in the abbreviation,
but the word ???? (agriculture) is usually kept.
The length (the number of characters) is also an im-
portant feature of the current word.
3. Position of the current character in the cur-
rent word Previous work (Chang and Lai, 2004)
showed that the first character of a word has high
possibility to form part of the abbreviation and this
is also true for the last character of a three-character
word.
4. Combination of feature 2. and 3. above
Combination of the features 2 and 3 is expected to
improve the performance, since the position infor-
274
mation affects the abbreviation along with the cur-
rent word. For example, ending character in ????
(university) and that in ???? (research institute)
have very different possibilities to be selected for ab-
breviations.
Besides the features above, we have examined
context information (previous word, previous char-
acter, next character, etc.) and other local features
like the length of the word, but these features did
not improve the performance. The reason may be
due to the sparseness of the training data.
4 Improvement by a Length Model and a
Web Search Engine
4.1 Length model
There is a strong correlation between the length of
organizations? full-names and their abbreviations.
We use the length modeling based on discrete prob-
ability of P (M |L), in which the variables M and
L are lengths of abbreviations and full-names, re-
spectively. Since it is difficult to incorporate length
information into the CRF model explicitly, we use
P (M |L) to rescore the output of the CRF.
In order to use the length information, we model
the abbreviation process with two steps:
? 1st step: evaluate the length in abbreviation ac-
cording to the length model P (M |L);
? 2nd step: choose the abbreviation, given the
length and full-name.
We assume the following approximation:
P (A|F ) ? P (M |L) ? P (A|M,F ) (2)
in which variable A is the abbreviation and F is the
full-name; P (M |L) is the length model, and the sec-
ond probability can be calculated according to the
Bayesian rule:
P (A|M,F ) = P (A,M |F )P (M |F )
= P (A,M |F )?
length(A?)=M P (A?,M |F )
(3)
It is obvious that P (A,M |F ) = P (A|F ) (as A
contains the information M implicitly) and P (A|F )
can be obtained from the output of the CRF.
4.2 Web search engine
Co-occurrence of a full-name and an abbreviation
candidate can be a clue of the correctness of the ab-
breviation. We use the ?abbreviation candidate?+
?full-name? as queries and input them to the most
popular Chinese search engine (www.baidu.com),
and then we use the number of hits as the metric
to perform re-ranking. The hits is theoretically re-
lated to the number of pages which contain both the
full-name and abbreviation. The bigger the value of
hits, the higher probability that the abbreviation is
correct.
We then simply multiply the previous probability
score, obtained from Eq. 2, by the number of hits
and re-rank the top-30 candidates accordingly.
There are some other ways to use information re-
trieval methods (Mandala et al, 2000). Our method
has an advantage that the access load to the web
search engine is relatively small.
5 Experiment
5.1 Data introduction
The corpus we use in this paper comes from two
sources: one is the book ?modern Chinese abbre-
viation dictionary? (Yuan and Ruan, 2002) and the
other is the wikipedia. Altogether we collected 1945
pairs of organization full-names and their abbrevia-
tions.
The data is randomly divided into two parts, a
training set with 1298 pairs and a test set with 647
pairs. Table 1 shows the length mapping statistics
of the training set. It can be seen that the average
length of full-names is about 7.29. We know that for
a full-name with length N, the number of abbrevia-
tion candidates is about 2N ? 2?N (exclude length
of 0, 1, and N) and we can conclude that the average
number of candidates for organization names in this
corpus is more than 100.
5.2 Results
The abbreviation method described is part of a
project to develop a voice-based search application.
For our name abbreviation system we plan to add 10
abbreviation candidates for each organization name
into the vocabulary of our voice search application,
hence here we consider top-10 coverage.
275
length of length of abbreviation
full-name 2 3 4 5 >5 sum
4 107 1 0 0 0 108
5 89 140 0 0 0 229
6 96 45 46 0 0 187
7 60 189 49 16 0 314
8 48 29 60 3 6 146
9 10 47 35 12 2 106
10 18 11 29 8 6 73
others 21 43 38 17 14 133
average length of the full-name 7.27
average length of the abbreviation 3.01
Table 1: Length statistics on the training set
Proceedings of ACL-08: HLT, pages 443?451,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Collecting a Why-question corpus for development and evaluation of an
automatic QA-system
Joanna Mrozinski Edward Whittaker
Department of Computer Science
Tokyo Institute of Technology
2-12-1-W8-77 Ookayama, Meguro-ku
Tokyo 152-8552 Japan
{mrozinsk,edw,furui}@furui.cs.titech.ac.jp
Sadaoki Furui
Abstract
Question answering research has only recently
started to spread from short factoid questions
to more complex ones. One significant chal-
lenge is the evaluation: manual evaluation is a
difficult, time-consuming process and not ap-
plicable within efficient development of sys-
tems. Automatic evaluation requires a cor-
pus of questions and answers, a definition of
what is a correct answer, and a way to com-
pare the correct answers to automatic answers
produced by a system. For this purpose we
present a Wikipedia-based corpus of Why-
questions and corresponding answers and arti-
cles. The corpus was built by a novel method:
paid participants were contacted through a
Web-interface, a procedure which allowed dy-
namic, fast and inexpensive development of
data collection methods. Each question in the
corpus has several corresponding, partly over-
lapping answers, which is an asset when es-
timating the correctness of answers. In ad-
dition, the corpus contains information re-
lated to the corpus collection process. We be-
lieve this additional information can be used to
post-process the data, and to develop an auto-
matic approval system for further data collec-
tion projects conducted in a similar manner.
1 Introduction
Automatic question answering (QA) is an alternative
to traditional word-based search engines. Instead of
returning a long list of documents more or less re-
lated to the query parameters, the aim of a QA sys-
tem is to isolate the exact answer as accurately as
possible, and to provide the user only a short text
clip containing the required information.
One of the major development challenges is eval-
uation. The conferences such as TREC1, CLEF2
and NTCIR3 have provided valuable QA evaluation
methods, and in addition produced and distributed
corpora of questions, answers and corresponding
documents. However, these conferences have fo-
cused mainly on fact-based questions with short an-
swers, so called factoid questions. Recently more
complex tasks such as list, definition and discourse-
based questions have also been included in TREC in
a limited fashion (Dang et al, 2007). More complex
how- and why-questions (for Asian languages) were
also included in the NTCIR07, but the provided data
comprised only 100 questions, of which some were
also factoids (Fukumoto et al, 2007). Not only is
the available non-factoid data quite limited in size,
it is also questionable whether the data sets are us-
able in development outside the conferences. Lin
and Katz (2006) suggest that training data has to be
more precise, and, that it should be collected, or at
least cleaned, manually.
Some corpora of why-questions have been col-
lected manually: corpora described in (Verberne et
al., 2006) and (Verberne et al, 2007) both com-
prise fewer than 400 questions and corresponding
answers (one or two per question) formulated by na-
tive speakers. However, we believe one answer per
question is not enough. Even with factoid questions
it is sometimes difficult to define what is a correct
1http://trec.nist.gov/
2http://www.clef-campaign.org/
3http://research.nii.ac.jp/ntcir/
443
answer, and complex questions result in a whole new
level of ambiguity. Correctness depends greatly on
the background knowledge and expectations of the
person asking the question. For example, a correct
answer to the question ?Why did Mr. X take Ms. Y
to a coffee shop?? could be very different depending
on whether we knew that Mr. X does not drink cof-
fee or that he normally drinks it alone, or that Mr. X
and Ms. Y are known enemies.
The problem of several possible answers and, in
consequence, automatic evaluation has been tackled
for years within another field of study: automatic
summarisation (Hori et al, 2003; Lin and Hovy,
2003). We believe that the best method of provid-
ing ?correct? answers is to do what has been done in
that field: combine a multitude of answers to ensure
both diversity and consensus among the answers.
Correctness of an answer is also closely related to
the required level of detail. The Internet FAQ pages
were successfully used to develop QA-systems (Jijk-
oun and de Rijke, 2005; Soricut and Brill, 2006), as
have the human-powered question sites such as An-
swers.com, Yahoo Answers and Google Answers,
where individuals can post questions and receive an-
swers from peers (Mizuno et al, 2007). Both re-
sources can be assumed to contain adequately error-
free information. FAQ pages are created so as to
answer typical questions well enough that the ques-
tions do not need to be repeated. Question sites typ-
ically rank the answers and offer bonuses for peo-
ple providing good ones. However, both sites suffer
from excess of information. FAQ-pages tend to also
answer questions which are not asked, and also con-
tain practical examples. Human-powered answers
often contain unrelated information and discourse-
like elements. Additionally, the answers do not al-
ways have a connection to the source material from
which they could be extracted.
One purpose of our project was to take part in
the development of QA systems by providing the
community with a new type of corpus. The cor-
pus includes not only the questions with multiple
answers and corresponding articles, but also certain
additional information that we believe is essential to
enhance the usability of the data.
In addition to providing a new QA corpus, we
hope our description of the data collection process
will provide insight, resources and motivation for
further research and projects using similar collection
methods. We collected our corpus through Amazon
Mechanical Turk service 4 (MTurk). The MTurk
infrastructure allowed us to distribute our tasks to
a multitude of workers around the world, without
the burden of advertising. The system also allowed
us to test the workers suitability, and to reward the
work without the bureaucracy of employment. To
our knowledge, this is the first time that the MTurk
service has been used in equivalent purpose.
We conducted the data collection in three steps:
generation, answering and rephrasing of questions.
The workers were provided with a set of Wikipedia
articles, based on which the questions were created
and the answers determined by sentence selection.
The WhyQA-corpus consists of three parts: original
questions along with their rephrased versions, 8-10
partly overlapping answers for each question, and
the Wikipedia articles including the ones corre-
sponding to the questions. The WhyQA-corpus is
in XML-format and can be downloaded and used
under the GNU Free Documentation License from
www.furui.cs.titech.ac.jp/ .
2 Setup
Question-answer pairs have previously been gen-
erated for example by asking workers to both ask
a question and then answer it based on a given
text (Verberne et al, 2006; Verberne et al, 2007).
We decided on a different approach for two reasons.
Firstly, based on our experience such an approach is
not optimal in the MTurk framework. The tasks that
were welcomed by workers required a short atten-
tion span, and reading long texts was negatively re-
ceived with many complaints, sloppy work and slow
response times. Secondly, we believe that the afore-
mentioned approach can produce unnatural ques-
tions that are not actually based on the information
need of the workers.
We divided the QA-generation task into two
phases: question-generation (QGenHIT) and an-
swering (QAHIT). We also trimmed the amount of
the text that the workers were required to read to cre-
ate the questions. These measures were taken both
in order to lessen the cognitive burden of the task
4http://www.mturk.com
444
and to produce more natural questions.
In the first phase the workers generated the ques-
tions based on a part of Wikipedia article. The re-
sulting questions were then uploaded to the system
as new HITs with the corresponding articles, and
answered by available (different) workers. Our hy-
pothesis is that the questions are more natural if their
answer is not known at the time of the creation.
Finally, in an additional third phase, 5 rephrased
versions of each question were created in order to
gain variation (QRepHIT). The data quality was en-
sured by requiring the workers to achieve a certain
result from a test (or a Qualification) before they
could work on the aforementioned tasks.
Below we explain the MTurk system, and then our
collection process in detail.
2.1 Mechanical Turk
Mechanical Turk is a Web-based service, offered by
Amazon.com, Inc. It provides an API through which
employers can obtain a connection to people to per-
form a variety of simple tasks. With tools provided
by Amazon.com, the employer creates tasks, and up-
loads them to the MTurk Web-site. Workers can then
browse the tasks and, if they find them profitable
and/or interesting enough, work on them. When the
tasks are completed, the employer can download the
results, and accept or reject them. Some key con-
cepts of the system are listed below, with short de-
scriptions of the functionality.
? HIT Human Intelligence Task, the unit of a
payable chore in MTurk.
? Requester An ?employer?, creates and uploads
new HITs and rewards the workers. Requesters
can upload simple HITs through the MTurk Re-
quester web site, and more complicated ones
through the MTurk Web Service APIs.
? Worker An ?employee?, works on the hits
through the MTurk Workers? web site.
? Assignment. One HIT consists of one or more
assignments. One worker can complete a sin-
gle HIT only once, so if the requester needs
multiple results per HIT, he needs to set the
assignment-count to the desired figure. A HIT
is considered completed when all the assign-
ments have been completed.
? Rewards At upload time, each HIT has to be
assigned a fixed reward, that cannot be changed
later. Minimum reward is $0.01. Amazon.com
collects a 10% (or a minimum of $0.05) service
fee per each paid reward.
? Qualifications To improve the data quality,
a HIT can also be attached to certain tests,
?qualifications? that are either system-provided
or created by the requester. An example of
a system-provided qualification is the average
approval ratio of the worker.
Even if it is possible to create tests that workers
have to pass before being allowed to work on a HIT
so as to ensure the worker?s ability, it is impossible
to test the motivation (for instance, they cannot be
interviewed). Also, as they are working through the
Web, their working conditions cannot be controlled.
2.2 Collection process
The document collection used in our research was
derived from the Wikipedia XML Corpus by De-
noyer and Gallinari (2006). We selected a total of
84 articles, based on their length and contents. A
certain length was required so that we could expect
the article to contain enough interesting material to
produce a wide selection of natural questions. The
articles varied in topic, degree of formality and the
amount of details; from ?Horror film? and ?Christ-
mas worldwide? to ?G-Man (Half-Life)? and ?His-
tory of London?. Articles consisting of bulleted lists
were removed, but filtering based on the topic of the
article was not performed. Essentially, the articles
were selected randomly.
2.2.1 QGenHIT
The first phase of the question-answer generation
was to generate the questions. In QGenHIT we pre-
sented the worker with only part of a Wikipedia ar-
ticle, and instructed them to think of a why-question
that they felt could be answered based on the origi-
nal, whole article which they were not shown. This
approach was expected to lead to natural curiosity
and questions. Offering too little information would
have lead to many questions that would finally be
left unanswered, and it also did not give the workers
enough to work on. Giving too much information
445
Qualification The workers were required to pass a test before working on the HITs.
QGenHIT Questions were generated based on partial Wikipedia articles. These questions were
then used to create the QAHITs.
QAHIT Workers were presented with a question and a corresponding article. The task was to
answer the questions (if possible) through sentence selection.
QRepHIT To ensure variation in the questions, each question was rephrased by 5 different workers.
Table 1: Main components of the corpus collection process.
Article topic: Fermi paradox
Original question Why is the moon crucial to the rare earth hypothesis?
Rephrased Q 1 How does the rare earth theory depend upon the moon?
Rephrased Q 2 What makes the moon so important to rare earth theory?
Rephrased Q 3 What is the crucial regard for the moon in the rare earth hypothesis?
Rephrased Q 4 Why is the moon so important in the rare earth hypothesis?
Rephrased Q 5 What makes the moon necessary, in regards to the rare earth hypothesis?
Answer 1. Sentence ids: 20,21. Duplicates: 4. The moon is important because its gravitational pull
creates tides that stabilize Earth?s axis. Without this stability, its variation, known as precession of
the equinoxes, could cause weather to vary so dramatically that it could potentially suppress the more
complex forms of life.
Answer 2. Sentence ids: 18,19,20. Duplicates: 2. The popular Giant impact theory asserts that it
was formed by a rare collision between the young Earth and a Mars-sized body, usually referred to as
Orpheus or Theia, approximately 4.45 billion years ago. The collision had to occur at a precise angle,
as a direct hit would have destroyed the Earth, and a shallow hit would have deflected the Mars-sized
body. The moon is important because its gravitational pull creates tides that stabilize Earth?s axis.
Answer 3. Sentence ids: 20,21,22. Duplicates: 2. The moon is important because its gravitational
pull creates tides that stabilize Earth?s axis. Without this stability, its variation, known as precession
of the equinoxes, could cause weather to vary so dramatically that it could potentially suppress the
more complex forms of life. The heat generated by the Earth/Theia impact, as well as subsequent
Lunar tides, may have also significantly contributed to the total heat budget of the Earth?s interior,
thereby both strengthening and prolonging the life of the dynamos that generate Earth?s magnetic field
Dynamo 1.
Answer 4. Sentence ids: 18,20,21. No duplicates. The popular Giant impact theory asserts that
it was formed by a rare collision between the young Earth and a Mars-sized body, usually referred
to as Orpheus or Theia, approximately 4.45 billion years ago. The moon is important because its
gravitational pull creates tides that stabilize Earth?s axis. Without this stability, its variation, known
as precession of the equinoxes, could cause weather to vary so dramatically that it could potentially
suppress the more complex forms of life.
Answer 5. Sentence ids: 18,21. No duplicates. The popular Giant impact theory asserts that it
was formed by a rare collision between the young Earth and a Mars-sized body, usually referred to as
Orpheus or Theia, approximately 4.45 billion years ago. Without this stability, its variation, known
as precession of the equinoxes, could cause weather to vary so dramatically that it could potentially
suppress the more complex forms of life.
Table 2: Data example: Question with rephrased versions and answers.
446
(long excerpts from the articles) was severely dis-
liked among the workers simply because it took a
long time to read.
We finally settled on a solution where the partial
content consisted of the title and headers of the arti-
cle, along with the first sentences of each paragraph.
The instructions to the questions demanded rigidly
that the question starts with the word ?Why?, as it
was surprisingly difficult to explain what we meant
by why-questions if the question word was not fixed.
The reward per HIT was $0.04, and 10 questions
were collected for each article. We did not force the
questions to be different, and thus in the later phase
some of the questions were removed manually as
they were deemed to mean exactly the same thing.
However, there were less than 30 of these duplicate
questions in the whole data set.
2.2.2 QAHIT
After generating the questions based on partial ar-
ticles, the resulting questions were uploaded to the
system as HITs. Each of these QAHITs presented a
single question with the corresponding original arti-
cle. The worker?s task was to select either 1-3 sen-
tences from the text, or a No-answer-option (NoA).
Sentence selection was conducted with Javascript
functionality, so the workers had no chance to in-
clude freely typed information within the answer (al-
though a comment field was provided). The reward
per HIT was $0.06. At the beginning, we collected
10 answers per question, but we cut that down to 8
because the HITs were not completed fast enough.
The workers for QAHITs were drawn from the
same pool as the workers for QGenHIT, and it was
possible for the workers to answer the questions they
had generated themselves.
2.2.3 QRepHIT
As the final step 5 rephrased versions of each
question were generated. This was done to com-
pensate the rigid instructions of the QGenHIT and
to ensure variation in the questions. We have not yet
measured how well the rephrased questions match
the answers of their original versions. In the final
QRepHIT questions were grouped into groups of 5.
Each HIT consisted of 5 assignments, and a $0.05
reward was offered for each HIT.
QRepHIT required the least amount of design and
trials, and workers were delighted with the task. The
HITs were completed fast and well even in the case
when we accidentally uploaded a set of HITs with
no reward.
As with QAHIT, the worker pool for creating and
rephrasing questions was the same. The questions
were rephrased by their creator in 4 cases.
2.3 Qualifications
To improve the data quality, we used the qualifi-
cations to test the workers. For the QGenHITs we
only used the system-provided ?HIT approval rate?-
qualification. Only workers whose previous work
had been approved in 80% of the cases were able to
work on our HITs.
In addition to the system-provided qualification,
we created a why-question-specific qualification.
The workers were presented with 3 questions, and
they were to answer each by either selecting 1-
3 most relevant sentences from a list of about 10
sentences, or by deciding that there is no answer
present. The possible answer-sentences were di-
vided into groups of essential, OK and wrong, and
one of the questions did quite clearly have no an-
swer. The scoring was such that it was impossible
to get approved results if not enough essential sen-
tences were included. Selecting sentences from the
OK-group only was not sufficient, and selecting sen-
tences from the wrong-group was penalized. A min-
imum score per question was required, but also the
total score was relevant ? component scores could
compensate each other up to a point. However, if
the question with no answer was answered, the score
could not be of an approvable level. This qualifica-
tion was, in addition to the minimum HIT approval
rate of 80%, a prerequisite for both the QRepHITs
and the QAHITs.
A total of 2355 workers took the test, and 1571
(67%) of them passed it, thus becoming our avail-
able worker pool. However, in the end the actual
number of different workers was only 173.
Examples of each HIT, their instructions and the
Qualification form are included in the final corpus.
The collection process is summarised in Table 1.
447
3 Corpus description
The final corpus consists of questions with their
rephrased versions and answers. There are total of
695 questions, of which 159 were considered unan-
swerable based on the articles, and 536 that have 8-
10 answers each. The total cost of producing the
corpus was about $350, consisting of $310 paid in
workers rewards and $40 in Mechanical Turk fees,
including all the trials conducted during the devel-
opment of the final system.
Also included is a set of Wikipedia documents
(WikiXML, about 660 000 articles or 670MB in com-
pressed format), including the ones corresponding to
the questions (84 documents). The source of Wik-
iXML is the English part of the Wikipedia XML
Corpus by Denoyer and Gallinari (2006). In the
original data some of the HTML-structures like lists
and tables occurred within sentences. Our sentence-
selection approach to QA required a more fine-
grained segmentation and for our purpose, much
of the HTML-information was redundant anyway.
Consequently we removed most of the HTML-
structures, and the table-cells, list-items and other
similar elements were converted into sentences.
Apart from sentence-information, only the section-
title information was maintained. Example data is
shown in Table 2.
3.1 Task-related information
Despite the Qualifications and other measures taken
in the collection phase of the corpus, we believe the
quality of the data remains open to question. How-
ever, the Mechanical Turk framework provided addi-
tional information for each assignment, for example
the time workers spent on the task. We believe this
information can be used to analyse and use our data
better, and have included it in the corpus to be used
in further experiments.
? Worker Id Within the MTurk framework, each
worker is assigned a unique id. Worker id can
be used to assign a reliability-value to the work-
ers, based on the quality of their previous work.
It was also used to examine whether the same
workers worked on the same data in different
phases: Of the original questions, only 7 were
answered and 4 other rephrased by the same
worker they were created by. However, it has
to be acknowledged that it is also possible for
one worker to have had several accounts in the
system, and thus be working under several dif-
ferent worker ids.
? Time On Task The MTurk framework also
provides the requester the time it took for the
worker to complete the assignment after ac-
cepting it. This information is also included in
the corpus, although it is impossible to know
precisely how much time the workers actually
spent on each task. For instance, it is possible
that one worker had several assignments open
at the same time, or that they were not concen-
trating fully on working on the task. A high
value of Time On Task thus does not necessar-
ily mean that the worker actually spent a long
time on it. However, a low value indicates that
he/she did only spend a short time on it.
? Reward Over the period spent collecting the
data, we changed the reward a couple of times
to speed up the process. The reward is reported
per HIT.
? Approval Status Within the collection pro-
cess we encountered some clearly unacceptable
work, and rejected it. The rejected work is also
included in the corpus, but marked as rejected.
The screening process was by no means per-
fect, and it is probable that some of the ap-
proved work should have been rejected.
? HIT id, Assignment id, Upload Time HIT and
assignment ids and original upload times of the
HITs are provided to make it possible to retrace
the collection steps if needed.
? Completion Time Completion time is the
timestamp of the moment when the task was
completed by a worker and returned to the sys-
tem. The time between the completion time
and the upload time is presumably highly de-
pendent on the reward, and on the appeal of the
task in question.
3.2 Quality experiments
As an example of the post-processing of the data,
we conducted some preliminary experiments on the
answer agreement between workers.
448
Out of the 695 questions, 159 were filtered out in
the first part of QAHIT. We first uploaded only 3 as-
signments, and the questions that 2 out of 3 work-
ers deemed unanswerable were filtered out. This
left 536 questions which were considered answered,
each one having 8-10 answers from different work-
ers. Even though in the majority of cases (83% of the
questions) one of the workers replied with the NoA,
the ones that answered did agree up to a point: of
all the answers, 72% were such that all of their sen-
tences were selected by at least two different work-
ers. On top of this, an additional 17% of answers
shared at least one sentence that was selected by
more than one worker.
To understand the agreement better, we also cal-
culated the average agreement of selected sentences
based on sentence ids and N-gram overlaps between
the answers. In both of these experiments, only
those 536 questions that were considered answer-
able were included.
3.2.1 Answer agreement on sentence ids
As the questions were answered by means of sen-
tence selection, the simplest method to check the
agreement between the workers was to compare
the ids of the selected sentences. The agreement
was calculated as follows: each answer was com-
pared to all the other answers for the same ques-
tion. For each case, the agreement was defined as
Agreement = CommonIdsAllIds , where CommonIds
is the number of sentence ids that existed in both
answers, and AllIds is the number of different ids
in both answers. We calculated the overall average
agreement ratio (Total Avg) and the average of the
best matches between two assignments within one
HIT (Best Match). We ran the test for two data sets:
The most typical case of the workers cheating was
to mark the question unaswerable. Because of this
the first data set included only the real answers, and
the NoAs were removed (NoA not included, 3872
answers). If an answer was compared with a NoA,
the agreement was 0, and if two NoAs were com-
pared, the agreement was 1. We did, however, also
include the figures for the whole data set (NoA in-
cluded, 4638 answers). The results are shown in Ta-
ble 3.
The Best Match -results were quite high com-
pared to the Total Avg. From this we can conclude
Total Avg Best Match
NoA not included 0.39 0.68
NoA included 0.34 0.68
Table 3: Answer agreement based on sentence ids.
that in the majority of cases, there was at least one
quite similar answer among those for that HIT. How-
ever, comparing the sentence ids is only an indica-
tive measure, and it does not tell the whole story
about agreement. For each document there may ex-
ist several separate sentences that contain the same
kind of information, and so two answers can be alike
even though the sentence ids do not match.
3.2.2 Answer agreement based on ROUGE
Defining the agreement over several passages of
texts has for a long time been a research prob-
lem within the field of automatic summarisation.
For each document it is possible to create several
summarisations that can each be considered cor-
rect. The problem has been approached by using
the ROUGE-metric: calculating the N-gram over-
lap between manual, ?correct? summaries, and the
automatic summaries. ROUGE has been proven to
correlate well with human evaluation (Lin and Hovy,
2003).
Overlaps of higher order N-grams are more usable
within speech summarisation as they take the gram-
matical structure and fluency of the summary into
account. When selecting sentences, this is not an is-
sue, so we decided to use only unigram and bigram
counts (Table 4: R-1, R2), as well as the skip-bigram
values (R-SU) and the longest common N-gram met-
ric R-L. We calculated the figures for two data sets
in the same way as in the case of sentence id agree-
ment. Finally, we set a lower bound for the results
by comparing the answers to each other randomly
(the NoAs were also included).
The final F-measures of the ROUGE results are
presented in Table 4. The figures vary from 0.37 to
0.56 for the first data set, and from 0.28 to 0.42 to
the second. It is debatable how the results should
be interpreted, as we have not defined a theoretical
upper bound to the values, but the difference to the
randomised results is substantial. In the field of au-
tomatic summarisation, the overlap of the automatic
449
results and corresponding manual summarisations is
generally much lower than the overlap between our
answers (Chali and Kolla, 2004). However, it is dif-
ficult to draw detailed conclusions based on compar-
ison between these two very different tasks.
R-1 R-2 R-SU R-L
NoA not included 0.56 0.46 0.37 0.52
NoA included 0.42 0.35 0.28 0.39
Random Answers 0.13 0.01 0.02 0.09
Table 4: Answer agreement: ROUGE-1, -2, -SU and -L.
The sentence agreement and ROUGE-figures do
not tell us much by themselves. However, they are
an example of a procedure that can be used to post-
process the data and in further projects of similar
nature. For example, the ROUGE similarity could
be used in the data collection phase as a tool of au-
tomatic approval and rejection of workers? assign-
ments.
4 Discussion and future work
During the initial trials of data collection we encoun-
tered some unexpected phenomena. For example,
increasing the reward did have a positive effect in
reducing the time it took for HITs to be completed,
however it did not correlate in desirable way with
data quality. Indeed the quality actually decreased
with increasing reward. We believe that this unex-
pected result is due to the distributed nature of the
worker pool in Mechanical Turk. Clearly the moti-
vation of some workers is other than monetary re-
ward. Especially if the HIT is interesting and can
be completed in a short period of time, it seems that
there are people willing to work on them even for
free.
MTurk requesters cannot however rely on this
voluntary workforce. From MTurk Forums it is clear
that some of the workers rely on the money they
get from completing the HITs. There seems to be a
critical reward-threshold after which the ?real work-
force?, i.e. workers who are mainly interested in per-
forming the HITs as fast as possible, starts to partic-
ipate. When the motivation changes from voluntary
participation to maximising the monetary gain, the
quality of the obtained results often understandably
suffers.
It would be ideal if a requester could rely on the
voluntary workforce alone for results, but in many
cases this may result either in too few workers and/or
too slow a rate of data acquisition. Therefore it is of-
ten necessary to raise the reward and rely on efficient
automatic validation of the data.
We have looked into the answer agreement of
the workers as an experimental post-processing step.
We believe that further work in this area will provide
the tools required for automatic data quality control.
5 Conclusions
In this paper we have described a dynamic and inex-
pensive method of collecting a corpus of questions
and answers using the Amazon Mechanical Turk
framework. We have provided to the community
a corpus of questions, answers and corresponding
documents, that we believe can be used in the de-
velopment of QA-systems for why-questions. We
propose that combining several answers from dif-
ferent people is an important factor in defining the
?correct? answer to a why-question, and to that goal
have included several answers for each question in
the corpus.
We have also included data that we believe is
valuable in post-processing the data: the work his-
tory of a single worker, the time spent on tasks, and
the agreement on a single HIT between a set of dif-
ferent workers. We believe that this information, es-
pecially the answer agreement of workers, can be
successfully used in post-processing and analysing
the data, as well as automatically accepting and re-
jecting workers? submissions in similar future data
collection exercises.
Acknowledgments
This study was funded by the Monbusho Scholar-
ship of Japanese Government and the 21st Century
COE Program ?Framework for Systematization and
Application of Large-scale Knowledge Resources
(COE-LKR)?
References
Yllias Chali and Maheedhar Kolla. 2004. Summariza-
tion Techniques at DUC 2004. In DUC2004.
Hoa Trang Dang, Diane Kelly, and Jimmy Lin. 2007.
Overview of the TREC 2007 Question Answering
450
Track. In E. Voorhees and L. P. Buckland, editors, Six-
teenth Text REtrieval Conference (TREC), Gaithers-
burg, Maryland, November.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An Overview of the 4th Ques-
tion Answering Challenge (QAC-4) at NTCIR work-
shop 6. In Proceedings of the Sixth NTCIR Workshop
Meeting, pages 433?440.
Chiori Hori, Takaaki Hori, and Sadaoki Furui. 2003.
Evaluation Methods for Automatic Speech Summa-
rization. In In Proc. EUROSPEECH, volume 4, pages
2825?2828, Geneva, Switzerland.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
Answers from Frequently Asked Questions Pages on
the Web. In CIKM ?05: Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 76?83, New York, NY, USA.
ACM Press.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Eval-
uation of Summaries Using N-gram Co-occurrence
Statistics. In Human Technology Conference (HLT-
NAACL), Edmonton, Canada.
Jimmy Lin and Boris Katz. 2006. Building a Reusable
Test Collection for Question Answering. J. Am. Soc.
Inf. Sci. Technol., 57(7):851?861.
Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, and
Katunobu Itou. 2007. Non-factoid Question Answer-
ing Experiments at NTCIR-6: Towards Answer Type
Detection for Realworld Questions. In Proceedings of
the 6th NTCIR Workshop Meeting on Evaluation of In-
formation Access Technologies, pages 487?492.
Radu Soricut and Eric Brill. 2006. Automatic Question
Answering Using the Web: Beyond the Factoid. Inf.
Retr., 9(2):191?206.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2006. Data for Question Answering:
the Case of Why. In LREC.
Susan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Discourse-based Answer-
ing of Why-questions. Traitement Automatique des
Langues, 47(2: Discours et document: traitements
automatiques):21?41.
451
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 755?763,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Discriminative Lexicon Adaptation for Improved Character Accuracy ?
A New Direction in Chinese Language Modeling
Yi-cheng Pan
Speech Processing Labratory
National Taiwan University
Taipei, Taiwan 10617
thomashughPan@gmail.com
Lin-shan Lee
Speech Processing Labratory
National Taiwan University
Taipei, Taiwan 10617
lsl@speech.ee.ntu.edu.tw
Sadaoki Furui
Furui Labratory
Tokyo Institute of Technology
Tokyo 152-8552 Japan
furui@furui.cs.titech.ac.jp
Abstract
While OOV is always a problem for most
languages in ASR, in the Chinese case the
problem can be avoided by utilizing char-
acter n-grams and moderate performances
can be obtained. However, character n-
gram has its own limitation and proper
addition of new words can increase the
ASR performance. Here we propose a dis-
criminative lexicon adaptation approach for
improved character accuracy, which not
only adds new words but also deletes some
words from the current lexicon. Different
from other lexicon adaptation approaches,
we consider the acoustic features and make
our lexicon adaptation criterion consistent
with that in the decoding process. The pro-
posed approach not only improves the ASR
character accuracy but also significantly
enhances the performance of a character-
based spoken document retrieval system.
1 Introduction
Generally, an automatic speech recognition (ASR)
system requires a lexicon. The lexicon defines the
possible set of output words and also the building
units in the language model (LM). Lexical words
offer local constraints to combine phonemes into
short chunks while the language model combines
phonemes into longer chunks by more global con-
straints. However, it?s almost impossible to include
all words into a lexicon both due to the technical
difficulty and also the fact that new words are cre-
ated continuously. The missed out words will never
be recognized, which is the well-known OOV prob-
lem. Using graphemes for OOV handling is pro-
posed in English (Bisani and Ney, 2005). Although
this sacrifices some of the lexical constraints and in-
troduces a further difficulty to combine graphemes
back into words, it is compensated by its ability for
5.8K characters 61.5K full lexicon
bigram 63.55% 73.8%
trigram 74.27% 79.28%
Table 1: Character recognition accuracy under dif-
ferent lexicons and the order of language model.
open vocabulary ASR. Morphs are another possi-
bility, which are longer than graphemes but shorter
than words, in other western languages (Hirsima?ki
et al, 2005).
Chinese language, on the other hand, is quite
different from western languages. There are no
blanks between words and the definition for words
is vague. Since almost all characters in Chinese
have their own meanings and words are composed
of the characters, there is an obvious solution for
the OOV problem: simply using all characters as
the lexicon. In Table 1 we see the differences in
character recognition accuracy by using only 5.8K
characters and a full set of 61.5K lexicon. The train-
ing set and testing set are the same as those that
will be introduced in Section 4.1. It is clear that
characters alone can provide moderate recognition
accuracies while augmenting new words signifi-
cantly improves the performance. If the words?
semantic functionality can be abandoned, which
definitely can not be replaced by characters, we can
treat words as a means to enhance character recog-
nition accuracy. Such arguments stand at least for
Chinese ASR since they evaluate on character error
rate and do not add explicit blanks between words.
Here we formulate a lexicon adaptation problem
and try to discriminatively find out not only OOV
words beneficial for ASR but also those existing
words that can be deleted.
Unlike previous lexicon adaptation or construc-
tion approaches (Chien, 1997; Fung, 1998; Deligne
and Sagisaka, 2000; Saon and Padmanabhan, 2001;
Gao et al, 2002; Federico and Bertoldi, 2004), we
755
consider the acoustic signals and also the whole
speech decoding structure. We propose to use
a simple approximation for the character poste-
rior probabilities (PPs), which combines acoustic
model and language model scores after decoding.
Based on the character PPs, we adapt the current
lexicon. The language model is then re-trained ac-
cording the new lexicon. Such procedure can be
iterated until convergence.
Characters, are not only the output units in Chi-
nese ASR but also have their roles in spoken docu-
ment retrieval (SDR). It has been shown that char-
acters are good indexing units. Generally, char-
acters can at least help OOV query handling; in
the subword-based confusion network (S-CN) pro-
posed by Pan et al (2007), characters are even
better than words for in-vocabulary (IV) queries.
In addition to evaluating the proposed approach on
ASR performance, we investigate its helpfulness
when integrated with an S-CN framework.
2 Related Work
Previous works for lexicon adaptation were focused
on OOV rate reduction. Given an adaptation cor-
pus, the standard way is to first identify OOVwords.
These OOV words are selected into the current lex-
icon based on the criterion of frequency or recency
(Federico and Bertoldi, 2004). The language model
is also re-estimated according to the new corpus
and new derived words.
For Chinese, it is more difficult to follow the
same approach since OOV words are not readily
identifiable. Several methods have been proposed
to extract OOV words from the new corpus based
on different statistics, which include associate norm
and context dependency (Chien, 1997), mutual in-
formation (Gao et al, 2002), morphological and
statistical rules (Chen and Ma, 2002), and strength
and spread measure (Fung, 1998). The used statis-
tics generally help find sequences of characters
that are consistent to the general concept of words.
However, if we focus on ASR performance, the
constraint of the extracted character strings to be
word-like is unnecessary.
Yang et al (1998) proposed a way to select new
character strings based on average character per-
plexity reduction. The word-like constraint is not
required and they show a significant improvement
on character-based perplexity. Similar ideas were
found to use mutual probability as an effective mea-
sure to combine two existing lexicon words into a
new word (Saon and Padmanabhan, 2001). Though
proposed for English, this method is effective for
Chinese ASR (Chen et al, 2004). Gao et al (2002)
combined an information gain-like metric and the
perplexity reduction criterion for lexicon word se-
lection. The application is on Chinese pinyin-to-
character conversion, which has very good correla-
tion with the underlying language model perplexity.
The above works actually are all focused on the
text level and only consider perplexity effect. How-
ever, as pointed by Rosenfeld (2000), lower per-
plexity does not always imply lower ASR error rate.
Here we try to face the lexicon adaptation problem
from another aspect and take the acoustic signals
involved in the decoding procedure into account.
3 Proposed Approach
3.1 Overall Pictureord
Chara
cter-b
ased  
Confu
sion 
Autom
atic 
Speec
h Rec
ogniti
on
(ASR
)
Chara
cter-b
ased 
Confu
sion N
etwor
k
(CCN
) cons
tructio
n
word lattice
s
Netw
ork (C
CN)
Adap
tation Corpu
s
Lexic
on Ad
aptati
on
for Im
prove
d 
Chara
cter A
ccurac
y
Add/D
elete w
ords
Lexic
on (Lex i)
Langu
age 
Mode
l(LM
i)
y
(LAIC
A)
Word
 
Segm
entati
on
LM Traini
ng 
(Lex i)
Mode
l (LM
i)
Manu
al 
Trans
criptio
n
Segm
entati
on 
and L
M Tra
ining
g
Corpo
ra 
Figure 1: The flow chart of the proposed approach.
We show the complete flow chart in Figure 1. At
the beginning we are given an adaptation spoken
corpus and manual transcriptions. Based on a base-
line lexicon (Lex0) and a language model (LM0)
we perform ASR on the adaptation corpus and con-
struct corresponding word lattices. We then build
character-based confusion networks (CCNs) (Fu
et al, 2006; Qian et al, 2008). On the CCNs we
perform the proposed algorithm to add and delete
words into/from the current lexicon. The LM train-
ing corpora joined with the adaptation corpus is
then segmented using Lex1 and the language model
is in turn re-trained, which gives LM1. This pro-
cedure can be iterated to give Lexi and LMi until
convergence.
3.2 Character Posterior Probability and
Character-based Confusion Network
(CCN)
Consider a word W as shown in Figure 2 with
characters {c1c2c3} corresponding to the edge e
starting at time ? and ending at time t in a word
lattice. During decoding the boundaries between c1
756
Figure 2: An edge e of word W composed of char-
acters c1c2c3 starting at time ? and ending at time
t.
and c2, and c2 and c3 are recorded respectively as t1
and t2. The posterior probability (PP) of the edge e
given the acoustic features A, P (e|A), is (Wessel
et al, 2001):
P (e|A) =
?(?) ? P (xt? |W ) ? PLM (W ) ? ?(t)
?start
,
(1)
where ?(?) and ?(t) denote the forward and back-
ward probability masses accumulated up to time ?
and t obtained by the standard forward-backward
algorithm, P (xt? |W ) is the acoustic likelihood
function, PLM (W ) the language model score, and
?start the sum of all path scores in the lattice. Equa-
tion (1) can be extended to the PP of a character of
W , say c1 with edge e1:
P (e1|A) =
?(?) ? P (xt1? |c1) ? PLM (c1) ? ?(t1)
?start
.
(2)
Here we need two new probabilities, PLM (c1)
and ?(t1). Since neither is easy to estimate, we
make some approximations. First, we assume
PLM (c1) ? PLM (W ). Of course this is not true,
the actual relation being PLM (c1) ? PLM (W ),
since the set of events having c1 given its his-
tory includes a set of events having W given the
same history. We used the above approximation
for easier implementation. Second, we assume
that after c1 there is only one path from t1 to
t: through c2 and c3. This is more reasonable
since we restrain the hypotheses space to be in-
side the word lattice, and pruned paths are sim-
ply neglected. With this approximation we have
?(t1) = P (xtt1 |c2c3) ? ?(t). Substituting these
two approximate values for PLM (c1) and ?(t1) in
Equation (2), the result turns out to be very sim-
ple: P (e1|A) ? P (e|A). With similar assump-
tions for the character edges e2 and e3, we have
P (e2|A) ? P (e3|A) ? P (e|A). Similar results
were obtained by Yao et al (2008) from a different
point of view.
The result that P (ei|A) ? P (e|A) seems to
diverge from the intuition: approximating an
n-segment word by splitting the probability of
the entire edge over the segments ? P (ei|A) ?
n
?
P (e|A). The basic meaning of Equation (1) is
to calculate the ratio of the paths going through a
specific edge divided by the total paths while each
path is weighted properly. Of course the paths go-
ing through a sub-edge ei should be definitely more
than the paths through the corresponding full-edge
e. As a result, P (ei|A) should usually be greater
than P (e|A), as implied by the intuition. However,
the inter-connectivity between all sub-edges and
the proper weights of them are not easy to be han-
dled well. Here we constrain the inter-connectivity
of sub-edges to be only inside its own word edge
and also simplify the calculation of the weights
of paths. This offers a tractable solution and the
performance is quite acceptable.
After we obtain the PPs for each character arc
in the lattice, such as P (ei|A) as mentioned above,
we can perform the same clustering method pro-
posed by Mangu et al (2000) to convert the word
lattice to a strict linear sequence of clusters, each
consisting of a set of alternatives of character hy-
potheses, or a character-based confusion network
(CCN) (Fu et al, 2006; Qian et al, 2008). In CCN
we collect the PPs for all character arc c with begin-
ning time ? and end time t as P ([c; ?, t]|A) (based
on the above mentioned approximation):
P ([c; ?, t]|A) =
?
H = w1 . . . wN ? lattice :
?i ? {1 . . . N} :
wi contains [c; ?, t]
P (H)P (A|H)
?
pathH? ? lattice
P (H ?)P (A|H ?)
,
(3)
whereH stands for a path in the word lattice. P (H)
is the language model score of H (after proper scal-
ing) and P (A|H) is the acoustic model score. CCN
was known to be very helpful in reducing character
error rate (CER) since it minimizes the expected
CER (Fu et al, 2006; Qian et al, 2008). Given
a CCN, we simply choose the characters with the
highest PP from each cluster as the recognition
results.
3.3 Lexicon Adaptation with Improved
Character Accuracy (LAICA)
In Figure 3 we show a piece of a character-based
confusion network (CCN) aligned with the corre-
sponding manual transcription characters. Such
alignment can be implemented by an efficient dy-
namic programming method. The CCN is com-
posed of several strict linear ordering clusters of
757
R m-1
R m
Refe
renc
e 
Char
acter
s ?
R m+1
R m+2
R m+3
n ||
o ||
p ||
q ||
r ||
?
Char
acter
-bas
ed 
Con
fusio
n Ne
twor
k 
(CC
N)
?
?
n
s
t
u
?
?
??
??.C
align
(m)
C alig
n(m+
2)
C alig
n(m+
3)
o
q
?
??..
?
?
?
?
C alig
n(m-
1)
C alig
n(m+
1)a
lign(
m+2
)
p
R m: 
char
acter
 vari
able
 at th
e mt
h po
sitio
n in 
the r
efere
nce 
char
acter
s
m
p
C alig
n(m)
: a c
luste
r of 
CCN
 alig
ned 
with
 the 
mth c
hara
cter 
in th
e ref
eren
ce
n~u:
 sym
bols
 for 
Chin
ese c
hara
cters
Figure 3: A character-based confusion network
(CCN) and corresponding reference manual tran-
scription characters.
character alternatives. In the figure, Calign(m)
is a specific cluster aligned with the mth char-
acter in the reference, which contains characters
{s . . . o . . .} (The alphabets n, o . . . u are symbols
for specific Chinese characters) . The characters in
each cluster of CCN are well sorted according to
the PP, and in each cluster a special null character
 with its PP being equal to 1 minus the summation
of PPs for all character hypotheses in that cluster.
The clusters with  ranked first are neglected in the
alignment.
After the alignment, there are only three pos-
sibilities corresponding to each reference charac-
ter. (1) The reference character is ranked first in
the corresponding cluster (Rm?1 and the cluster
Calign(m?1)). In this case the reference charac-
ter can be correctly recognized. (2) The refer-
ence character is included in the corresponding
cluster but not ranked first. ([Rm . . . Rm+2] and
{Calign(m), . . . , Calign(m+2)}) (3) The reference
character is not included in the corresponding clus-
ter (Rm+3 and Calign(m+3)). For cases (2) and (3),
the reference character will be incorrectly recog-
nized.
The basic idea of the proposed lexicon adapta-
tion with an improved character accuracy (LAICA)
approach is to enhance the PPs of those incorrectly
recognized characters by adding new words and
deleting existing words in the lexicon. Here we
only focus on those characters of case (2) men-
tioned above. This is primarily motivated by the
minimum classification error (MCE) discriminative
training approach proposed by Juang et al (1997),
where a sigmoid function was used to suppress the
impacts of those perfectly and very poorly recog-
nized training samples. In our approach, the case
(1) is the perfect case and case (3) is the very poor
one. Another motivation is that for characters in
case (1), since they are already correctly recognized
we do not try to enhance their PPs.
The procedure of LAICA then becomes simple.
Among the aligned reference characters and clus-
ters of CCN, case (1) and (3) are anchors. The
reference characters between two anchors then be-
come our focus segment and their PPs should be en-
hanced. By investigating Equation (3), to enhance
the PP of a specific character we can adjust the
language model (P (H)), and the acoustic model
(P (A|H)), or we can simply modify the lexicon
(the constraint under summation). We should add
new words to cover the characters of case (2) to
enlarge the numerator of Equation (3) and at the
same time delete some existing words to suppress
the denominator.
In Figure 3, reference characters
[RmRm+1Rm+2=opq] and the clusters
{Calign(m), . . . , Calign(m+2)} show an exam-
ple of our focus segment. For each such segment,
we at most add one new word and delete an
existing word. From the string [opq] we choose
the longest OOV part from it as a new word.
To select a word to be deleted, we choose the
longest in-vocabulary (IV) part from the top
ranked competitors of [opq], which are then [stu]
in clusters {Calign(m), . . . , Calign(m+2)}. This is
also motivated by MCE that we only suppress the
strongest competitors? probabilities. Note that we
do not delete single-characters in the procedure.
The ?at most one? constraint here is motivated
by previous language model adaptation works (Fed-
erico, 1999) which usually try to introduce new ev-
idences in the adaptation corpus but with the least
modification of the original model. Of course the
modification of language models led by the addi-
tion and deletion of words is hard to quantify and
we choose to add and delete as fewer words as pos-
sible, which is just a simple heuristic. On the other
hand, adding fewer words means that longer words
are added. It has been shown that longer words are
more helpful for ASR (Gao et al, 2004; Saon and
Padmanabhan, 2001).
The proposed LAICA approach can be regarded
as a discriminative one since it not only considers
the reference characters but also those wrongly rec-
ognized characters. This can be beneficial since it
reduces potential ambiguities existing in the lexi-
con.
758
The Expectation-Maximization algorithm
1. Bootstrap initial word segmentation by
maximum-matching algorithm
(Wong and Chan, 1996)
2. Estimate unigram LM
3. Expectation: Re-segment according
to the unigram LM
4. Maximization: Estimate the n-gram LM
5. Expectation: Re-segment according to
the n-gram LM
6. Go to step 4 until convergence
Table 2: EM algorithm for word segmentation and
LM estimation
3.4 Word Segmentation and Language
Model Training
If we regard the word segmentation process as a
hidden variable, then we can apply EM algorithm
(Dempster et al, 1977) to train the underlying n-
gram language model. The procedure is described
in Table 2. In the algorithm we can see two ex-
pectation phases. This is natural since at the be-
ginning the bootstrap segmentation can not give
reliable statistics for higher order n-gram and we
choose to only use the unigram marginal probabili-
ties. The procedure was well established by Hwang
et al (2006).
Actually, the EM algorithm proposed here is sim-
ilar to the n-multigram model training procedure
proposed by Deligne and Sagisaka (2000). The role
of multigrams can be regarded as the words here,
except that multigrams begin from scratch while
here we have an initial lexicon and use maximum-
matching algorithm to offer an acceptable initial
unigram probability distributions. If the initial lex-
icon is not available, the procedure proposed by
Deligne and Sagisaka (2000) is preferred.
4 Experimental Results
4.1 Baseline Lexicon, Corpora and Language
Models
The baseline lexicon was automatically constructed
from a 300 MB Chinese news text corpus ranging
from 1997 to 1999 using the widely applied PAT-
tree-based word extraction method (Chien, 1997).
It includes 61521 words in total, of which 5856
are single-characters. The key principles of the
PAT-tree-based approach to extract a sequence of
characters as a word are: (1) high enough frequency
count; (2) high enough mutual information between
component characters; (3) large enough number of
context variations on both sides; (4) not dominated
by the most frequent context among all context
variations. In general the words extracted have high
frequencies and clear boundaries, thus very often
they have good semantic meanings. Since all the
above statistics of all possible character sequences
in a raw corpus are combinatorially too many, we
need an efficient data structure such as the PAT-tree
to record and access all such information.
With the baseline lexicon, we performed the EM
algorithm as in Table 2 to train the trigram LM.
Here we used a 313 MB LM training corpus, which
contains text news articles in 2000 and 2001. Note
that in the following Sections, the pronunciations
of the added words were automatically labeled by
exhaustively generating all possible pronunciations
from all component characters? canonical pronun-
ciations.
4.2 ASR Character Accuracy Results
A set of broadcast news corpus collected from a
Chinese radio station from January to September,
2001 was used as the speech corpus. It contained
10K utterances. We separated these utterances into
two parts randomly: 5K as the adaptation corpus
and 5K as the testing set. We show the ASR char-
acter accuracy results after lexicon adaptation by
the proposed approach in Table 3.
LAICA-1 LAICA-2
A D A+D A D A+D
Baseline +1743 -1679 +1743 +409 -112 +314
-1679 -88
79.28 80.48 79.31 80.98 80.58 79.33 81.21
Table 3: ASR character accuracies for the baseline
and the proposed LAICA approach. Two iterations
are performed, each with three versions. A: only
add new words, D: only delete words and A+D: si-
multaneously add and delete words. + and - means
the number of words added and deleted, respec-
tively.
For the proposed LAICA approach, we show
the results for one (LAICA-1) and two (LAICA-
2) iterations respectively, each of which has three
different versions: (A) only add new words into
the current lexicon, (D) only delete words, (A+D)
simultaneously add and delete words. The num-
ber of added or deleted words are also included in
Table 3.
There are some interesting observations. First,
we see that deletion of current words brought much
759
less benefits than adding new words. We try to give
some explanations. Deleting existing words in the
lexicon actually is a passive assistance for recog-
nizing reference characters correctly. Of course
we eliminate some strong competitive characters
in this way but we can not guarantee that refer-
ence characters will then have high enough PP
to be ranked first in its own cluster. Adding new
words into the lexicon, on the other hand, offers
explicit reinforcement in PP of the reference char-
acters. Such reinforcement offers the main positive
boosting for the PP of reference characters. These
boosted characters are under some specific con-
texts which normally correspond to OOV words
and sometimes in-vocabulary (IV) words that are
hard to be recognized.
From the model training aspect, adding new
words gives the maximum-likelihood flavor while
deleting existing words provides discriminant abil-
ity. It has been shown that discriminative train-
ing does not necessarily outperform maximum-
likelihood training until we have enough training
data (Ng and Jordan, 2001). So it is possible that
discriminatively trained model performs worse than
that trained by maximum likelihood. In our case,
adding and deleting words seem to compliment
each other well. This is an encouraging result.
Another good property is that the proposed ap-
proach converged quickly. The number of words to
be added or deleted dropped significantly in the sec-
ond iteration, compared to the first one. Generally
the fewer words to be changed the fewer recogni-
tion improvement can be expected. Actually we
have tried the third iteration and simply obtained
dozens of words to be added and no words to be
deleted, which resulted in negligible changes in
ASR recognition accuracy.
4.3 Comparison with other Lexicon
Adaptation Methods
In this section we compare our method with two
other traditionally used approaches: one is the PAT-
tree-based as introduced in Section 4.1 and the
other is based on mutual probability (Saon and Pad-
manabhan, 2001), which is the geometrical average
of the direct and reverse bigram:
PM (wi, wj) =
?
Pf (wj |wi)Pr(wi|wj),
where the direct (Pf (?) and reverse bigram (Pr(?))
can be estimated as:
Pf (wj |wi) =
P (Wt+1 = wj ,Wt = wi)
P (Wt = wi)
,
Pr(wj |wi) =
P (Wt+1 = wj ,Wt = wi)
P (Wt+1 = wj)
.
PM (wi, wj) is used as a measure about whether to
combine wi and wj as a new word. By properly
setting a threshold, we may iteratively combine
existing characters and/or words to produce the re-
quired number of new words. For both the PAT-tree-
and mutual-information-based approaches, we use
the manual transcriptions of the development 5K
utterances to collect the required statistics and we
extract 2159 and 2078 words respectively to match
the number of added words by the proposed LAICA
approach after 2 iterations (without word deletion).
The language model is also re-trained as described
in Section 3.4. The results are shown in Table 4,
where we also include the results of our approach
with 2 iterations and adding words only for refer-
ence.
PAT-
tree
Mutual
Probability LAICA-2(A)
Character
Accuracy 79.33 80.11 80.58
Table 4: ASR character accuracies on the lexicon
adapted by different approaches.
From the results we observe that the PAT-tree-
based approach did not give satisfying improve-
ments while the mutual probability-based one
worked well. This may be due to the sparse adap-
tation data, which includes only 81K characters.
PAT-tree-based approach relies on the frequency
count, and some terms which occur only once in
the adaptation data will not be extracted. Mutual
probability-based approach, on the other hand, con-
siders two simple criterion: the components of a
new word occur often together and rarely in con-
junction with other words (Saon and Padmanabhan,
2001). Compared with the proposed approach, both
PAT-tree and mutual probability do not consider the
decoding structure.
Some new words are clearly good for human
sense and definitely convey novel semantic infor-
mation, but they can be useless for speech recogni-
tion. That is, character n-gram may handle these
words equally well due to the low ambiguities with
other words. The proposed LAICA approach tries
to focus on those new words which can not be han-
dled well by simple character n-grams. Moreover,
the two methods discussed here do not offer pos-
sible ways to delete current words, which can be
considered as a further advantage of the proposed
LAICA approach.
760
4.4 Application: Character-based Spoken
Document Indexing and Retrieval
Pan et al (2007) recently proposed a new Subword-
based Confusion Network (S-CN) indexing struc-
ture for SDR, which significantly outperforms
word-based methods for IV or OOV queries. Here
we apply S-CN structure to investigate the effec-
tiveness of improved character accuracy for SDR.
Here we choose characters as the subword units,
and then the S-CN structure is exactly the same as
CCN, which was introduced in Section 3.2.
For the SDR back-end corpus, the same 5K test
utterances as used for the ASR experiment in Sec-
tion 4.2 were used. The previously mentioned lexi-
con adaptation approaches and corresponding lan-
guage models were used in the same speech recog-
nizer for the spoken document indexing. We auto-
matically choose 139 words and terms as queries
according to the frequency (at least six times in the
5K utterances). The SDR performance is evaluated
by mean average precision (MAP) calculated by
the trec eval1 package. The results are shown
in Table 5.
Character
Accuracy MAP
Baseline 79.28 0.8145
PAT-tree 79.33 0.8203
Mutual
Probability 80.11 0.8378
LAICA-2(A+D) 81.21 0.8628
Table 5: ASR character accuracies and SDR MAP
performances under S-CN structure.
From the results, we see that generally the
increasing of character recognition accuracy im-
proves the SDR MAP performance. This seems
trivial but we have to note the relative improve-
ments. Actually the transformation ratios from the
relative increased character accuracy to the relative
increased MAP for the three lexicon adaptation ap-
proaches are different. A key factor making the
proposed LAICA approach advantageous is that
we try to extensively raise the incorrectly recog-
nized character posterior probabilities, by means
of adding effective OOV words and deleting am-
biguous words. Actually S-CN is relying on the
character posterior probability for indexing, which
is consistent with our criterion and makes our ap-
proach beneficial. The degree of the raise of char-
acter posterior probabilities can be visualized more
clearly in the following experiment.
1http://trec.nist.gov/
4.5 Further Investigation: the Improved
Rank in Character-based Confusion
Networks
In this experiment, we have the same setup as in
Section 4.2. After decoding, we have character-
based confusion networks (CCNs) for each test
utterance. Rather than taking the top ranked char-
acters in each cluster as the recognition result, we
investigate the ranks of the reference characters in
these clusters. This can be achieved by the same
alignment as we did in Section 3.3. The results are
shown in Table 6.
# of ranked
reference
characters
Average
Rank
baseline 70993 1.92
PAT-tree 71038 1.89
Mutual
Probability
71054 1.81
LAICA-2(A+D) 71083 1.67
Table 6: Average ranks of reference characters in
the confusion networks constructed by different
lexicons and corresponding language models
In Table 6 we only evaluate ranks on those ref-
erence characters that can be found in its corre-
sponding confusion network cluster (case (1) and
(2) as described in Section 3.3). The number of
those evaluated reference characters depends on
the actual CCN and is also included in the results.
Generally, over 93% of reference characters are in-
cluded (the total number is 75541). Such ranks are
critical for lattice-based spoken document indexing
approaches such as S-CN since they directly affect
retrieval precision. The advantage of the proposed
LAICA approach is clear. The results here provide
a more objective point of view since SDR evalua-
tion is inevitably effected by the selected queries.
5 Conclusion and Future Work
Characters together is an interesting and distinct
language unit for Chinese. They can be simultane-
ously viewed as words and subwords, which offer
a special means for OOV handling. While relying
only on characters gives moderate performances in
ASR, properly augmenting new words significantly
increases the accuracy. An interesting question
would then be how to choose words to augment.
Here we formulate the problem as an adaptation
one and try to find the best way to alter the current
761
lexicon for improved character accuracy.
This is a new perspective for lexicon adaptation.
Instead of identifying OOV words from adaptation
corpus to reduce OOV rate, we try to pick out word
fragments hidden in the adaptation corpus that help
ASR. Furthermore, we delete some existing words
which may result in ambiguities. Since we directly
match our criterion with that in decoding, the pro-
posed approach is expected to have more consistent
improvements than perplexity based criterions.
Characters also play an important role in spoken
document retrieval. This extends the applicability
of the proposed approach and we found that the
S-CN structure proposed by Pan et al for spoken
document indexing fitted well with the proposed
LAICA approach.
However, there still remain lots to be improved.
For example, considering Equation 3, the language
model score and the summation constraint are not
independent. After we alter the lexicon, the LM is
different accordingly and there is no guarantee that
the obtained posterior probabilities for those incor-
rectly recognized characters would be increased.
We increased the path alternatives for those refer-
ence characters but this can not guarantee to in-
crease total path probability mass. This can be
amended by involving the discriminative language
model adaptation in the iteration, which results in
a unified language model and lexicon adaptation
framework. This can be our future work. Moreover,
the same procedure can be used in the construction.
That is, beginning with only characters in the lexi-
con and using the training data to alter the current
lexicon in each iteration. This is also an interesting
direction.
References
Maximilian Bisani and Hermann Ney. 2005. Open vo-
cabulary speech recognition with flat hybrid models.
In Interspeech, pages 725?728.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown
word extraction for chinese documents. In COLING,
pages 169?175.
Berlin Chen, Jen-Wei Kuo, and Wen-Hung Tsai. 2004.
Lightly supervised and data-driven approaches to
mandarin broadcast news transcription. In ICASSP,
pages 777?780.
Lee-Feng Chien. 1997. Pat-tree-based keyword ex-
traction for Chinese information retrieval. In SIGIR,
pages 50?58.
Sabine Deligne and Yoshinori Sagisaka. 2000. Sta-
tistical language modeling with a class-based n-
multigram model. Comp. Speech and Lang.,
14(3):261?279.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistics Soci-
ety, 39(1):1?38.
Marcello Federico and Nicola Bertoldi. 2004. Broad-
cast news LM adaptation over time. Comp. Speech
Lang., 18:417?435.
Marcello Federico. 1999. Efficient language model
adaptation through MDI estimation. In Intersspech,
pages 1583?1586.
Yi-Sheng Fu, Yi-Cheng Pan, and Lin-Shan Lee.
2006. Improved large vocabulary continuous Chi-
nese speech recognition by character-based consen-
sus networks. In ISCSLP, pages 422?434.
Pascale Fung. 1998. Extracting key terms from chi-
nese and japanese texts. Computer Processing of
Oriental Languages, 12(1):99?121.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statis-
tical language modeling for Chinese. ACM Trans-
action on Asian Language Information Processing,
1(1):3?33.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2004. Chinese word segmentation: A prag-
matic approach. In MSR-TR-2004-123.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen.
2005. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Comp. Speech Lang.
Mei-Yuh Hwang, Xin Lei, Wen Wang, and Takahiro
Shinozaki. 2006. Investigation on mandarin
broadcast news speech recognition. In Interspeech-
ICSLP, pages 1233?1236.
Bing-Hwang Juang, Wu Chou, and Chin-Hui Lee.
1997. Minimum classification error rate methods for
speech recognition. IEEE Trans. Speech Audio Pro-
cess., 5(3):257?265.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word er-
ror minimization and other applications of confusion
networks. Comp. Speech Lang., 14(2):373?400.
Andrew Y. Ng and Michael I. Jordan. 2001. On
discriminative vs. generative classifiers: A compar-
ison of logistic regression and naive bayes. In Ad-
vances in Neural Information Processing Systems
(14), pages 841?848.
762
Yi-Cheng Pan, Hung-Lin Chang, and Lin-Shan Lee.
2007. Analytical comparison between position spe-
cific posterior lattices and confusion networks based
on words and subword units for spoken document
indexing. In ASRU.
Yao Qian, Frank K. Soong, and Tan Lee. 2008. Tone-
enhanced generalized character posterior probabil-
ity (GCPP) for Cantonese LVCSR. Comp. Speech
Lang., 22(4):360?373.
Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here?
Proceeding of IEEE, 88(8):1270?1278.
George Saon and Mukund Padmanabhan. 2001. Data-
driven approach to designing compound words for
continuous speech recognition. IEEE Trans. Speech
and Audio Process., 9(4):327?332, May.
Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and Her-
mann Ney. 2001. Confidence measures for large
vocabulary continuous speech recognition. IEEE
Trans. Speech Audio Process., 9(3):288?298, Mar.
Pak-kwong Wong and Chorkin Chan. 1996. Chinese
word segmentation based on maximum matching
and word binding force. In Proceedings of the 16th
International Conference on Computational Linguis-
tic, pages 200?203.
Kae-Cherng Yang, Tai-Hsuan Ho, Lee-Feng Chien, and
Lin-Shan Lee. 1998. Statistics-based segment pat-
tern lexicon: A new direction for chinese language
modeling. In ICASSP, pages 169?172.
763
Hybrid Statistical and Structural Semantic Modeling for Thai Multi-
Stage Spoken Language Understanding 
 
Chai Wutiwiwatchai   and   Sadaoki Furui 
Department of Computer Science, Tokyo Institute of Technology 
2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8552 Japan. 
{chai, furui}@furui.cs.titech.ac.jp 
 
Abstract 
This article proposes a hybrid statistical and 
structural semantic model for multi-stage 
spoken language understanding (SLU). The 
first stage of this SLU utilizes a weighted fi-
nite-state transducer (WFST)-based parser, 
which encodes the regular grammar of con-
cepts to be extracted. The proposed method 
improves the regular grammar model by in-
corporating a well-known n-gram semantic 
tagger. This hybrid model thus enhances the 
syntax of n-gram outputs while providing 
robustness against speech-recognition errors. 
With applications to a Thai hotel reservation 
domain, it is shown to outperform both indi-
vidual models at every stage of the SLU sys-
tem. Under the probabilistic WFST 
framework, the use of N-best hypotheses 
from the speech recognizer instead of the 1-
best can further improve performance requir-
ing only a small additional processing time. 
1 Introduction 
Automatic speech recognition (ASR) for Thai lan-
guage is still in the first stage, where Thai researchers 
in related fields have worked towards creating funda-
mental tools for language processing such as phono-
logical and morphological analyzers. Although Thai 
writing is an alphabetic system, a problem of writing 
without sentence markers or spaces between words has 
obstructed initiation of development of ASR. Pioneer-
ing a Thai spoken dialogue system has therefore be-
come a challenging task, where several unique 
components need to be developed specifically for a 
Thai system. 
Our prototype dialogue system, namely Thai Inter-
active Hotel Reservation Agent (TIRA), was created 
mainly by handcrafted rules. The first user evaluation 
(Wutiwiwatchai and Furui, 2003a) showed that the 
spoken language understanding (SLU) part of the sys-
tem proved the most problematic as it could not cover 
the variety of contents supplied by the users, especially 
when they talked in a mixed-initiative style. 
To rapidly improve performance, a trainable SLU 
model is preferable and it needs to be able to learn 
from a partially annotated corpus, where only essential 
keywords are given. This is particularly important for 
Thai where no large corpus is available. 
Recently, a novel multi-stage SLU model has been 
developed (Wutiwiwatchai and Furui, 2003b), which 
combines two different practices used for SLU-related 
tasks, robust semantic parsing and topic classification. 
The former paradigm was implemented in the concept 
extraction and concept-value recognition component, 
whereas the latter was applied for the goal identifica-
tion component. The concept extraction utilizes a set 
of weighted finite-state transducers (WFST) to encode 
possible word-syntax (or regular grammar) expressed 
for each concept. The concept WFST not only deter-
mines the existence of a concept in an input utterance, 
but also labels keywords used to construct its value in 
the concept-value recognition component. Given the 
extracted concepts, the goal of the utterance can be 
identified in the goal identification component using a 
generalized pattern classifier. 
This article reports an improvement of the concept 
extraction and concept-value recognition parts by con-
ducting a well-known statistical n-gram parser to com-
pensate for the concept expressions, which cannot be 
recognized by the ordinary concept WFST. The n-
gram modeling alone lacks structural information as it 
captures only up to n-word dependencies. Combining 
the statistical and structural model for SLU hence be-
comes a better alternative. Motivated by B?chet et al  
(2002), we propose a strategic way called logical n-
gram modeling, which combines the statistical n-gram 
with the existing regular grammar. In contrast to the 
regular-grammar approach, the probabilistic model 
allows the SLU to deal with ASR N-best hypotheses, 
resulting in an increment of the overall performance. 
Some related works are reviewed in the next sec-
tion, followed by a description of our multi-stage SLU 
model. Section 4 explains the proposed hybrid model. 
Section 5 shows the evaluation results with a conclu-
sion in section 6. 
2 Related Works 
In the technology of trainable or data-driven SLU, two 
different practices for different applications have been 
widely investigated. The first practice aims to tag the 
words (or group of words) in the utterance with se-
mantic labels, which are later converted to a certain 
format of semantic representation. To generate such a 
semantic frame, words in the utterance are usually 
aligned to a semantic tree by a parsing algorithm such 
as a probabilistic context free grammar or a recursive 
network whose nodes represent semantic symbols of 
the words and arcs consist of transition probabilities. 
During parsing, these probabilities are summed up, 
and used to determine the most likely parsed tree. 
Many understanding engines have been successfully 
implemented based on this paradigm (Seneff, 1992; 
Potamianos et al, 2000; Miller et al, 1994). A draw-
back of this method is, however, the requirement of a 
large, fully annotated corpus, i.e. a corpus with seman-
tic tags on every word, to ensure training reliability. 
The second practice has been utilized in applica-
tions such as call classification (Gorin et al, 1997). In 
this application, the understanding module aims to 
classify an input utterance to one of predefined user 
goals (if an utterance is supposed to have one goal) 
directly from the words contained in the utterance. 
This problem can be considered a simple pattern clas-
sification task. An advantage of this method is the 
need for training utterances tagged only with their 
goals, one for each utterance. However, another proc-
ess is required if one needs to obtain more detailed 
information. Our motivation for combining the two 
practices described above is that this allows the use of 
an only partially annotated corpus, while still allowing 
the system to capture sufficient information. The idea 
of combination has also been investigated in other 
works such as Wang et al (2002). 
Another issue related to this article is the combina-
tion of a statistical and rule-based approach for SLU, a 
system which is expected to improve the overall per-
formance over both individual approaches. The closest 
approach to our work was proposed by B?chet et al  
(2002), aiming to extract named-entities (NEs) from 
an input utterance. NE extraction is performed in two 
steps, detecting the NEs by a statistical tagger and ex-
tracting NE values using local models. Est?ve et al 
(2003) proposed a tighter coupling method that em-
beds conceptual structures into the ASR decoding 
network. Wang et al (2000), and Hacioglu and Ward 
(2001) proposed similar ideas for unified models that 
incorporated domain-specific context-free grammars 
(CFGs) into domain-independent n-gram models. The 
hybrid models thus improved the generalized ability of 
the CFG and specificity of the n-gram. With the exist-
ing regular grammar model in a weighted finite-state 
transducer (WFST) framework, we propose another 
strategy to incorporate the statistical n-gram model 
into the concept extraction and concept-value recogni-
tion components of our multi-stage SLU. 
3 Multi-Stage SLU 
In the design of our spoken dialogue system, the dia-
logue manager decides to respond to the user after 
perceiving the user goal. In some types of goal, infor-
mation items contained in the utterance are required 
for communication. For example the goal ?request for 
facilities? must come with the facilities the user is ask-
ing for, and the goal ?request for prerequisite keys? 
aims to have the user state the reserved date and the 
number of participants. Hence, the SLU module must 
be able to identify the goal and extract the required 
information items. 
We proposed a novel SLU model (Wutiwiwatchai 
and Furui, 2003b) that processes an input utterance in 
three stages, concept extraction, goal identification, 
and concept-value recognition. Figure 1 illustrates the 
overall architecture of the SLU model, in which its 
components are described in detail as follows: 
 
 
Figure 1. Overall architecture of the multi-stage SLU. 
3.1 Concept extraction 
The function of concept extraction is similar to that of 
other works, aiming to extract a set of concepts from 
an input utterance. However, our way to define a con-
cept is rather different. 
? A concept has a unique semantic meaning. 
? The order of concepts is not important. 
? Each type of concept occurs only once in an ut-
terance. 
? The semantic meaning of a concept can be inter-
preted from a sequence of words arbitrarily 
placed in the utterance (the sequence can overlap 
or cross each other). 
Examples of utterances and concepts contained in the 
utterances are shown in Table 1. A word sequence or 
 
 
Concept-value 
recognition 
Accepted  
substrings 
 
Goal Concept-values 
Concept extraction 
Concepts 
Word string 
Goal 
identification 
substring corresponding to the concept is presented in 
the form of a label sequence. The ??? and two-alphabet 
symbols such as ?fd? denote the words required to in-
dicate the concept. The two-alphabet symbols addi-
tionally specify keywords used for concept-value 
recognition. The ?-? is for other words not related to 
the concept. As defined above, a concept such as 
?reqprovide? (asking whether something is provided) is 
expressed by the substring ?there is ? right?, which 
contains two separated strings, ?there is? and ?right?. 
In the same utterance, another concept ?yesnoq? (ask-
ing by a yes-no question) also possesses the word 
?right?. We considered this method of definition to 
have more impact for presenting the meaning of con-
cepts, compared to what has been defined in other 
works. It must be noted that some concepts contain 
values such as the concept ?numperson? (the number of 
people), whereas some do not, such as the concept 
?yesnoq?. 
 
 
 
Figure 2. A portion of regular grammar WFST for the 
concept ?numperson? (the number of people). 
We implemented the concept extraction component 
by using weighted finite-state transducers (WFSTs). 
Similar to the implementation of salient grammar 
fragments in Gorin et al (1997), the possible word 
sequences expressed for a concept are encoded in a 
WFST, one for each type of concept. Figure 2 demon-
strates a portion of WFST for the concept ?numperson?. 
Each arc or transition of the WFST is labeled with an 
input word (or word class) followed after a colon by 
an output semantic label, and enclosed after a slash by 
a weight. A special symbol ?NIL? represents any word 
not included in the concept. The transitions, linking 
between the start and end node, characterize the ac-
ceptable word syntax. Weights of these transitions, 
except those containing ?NIL?, are assigned to be -1. 
The rest are assigned to have zero weights. The output 
labels indicate keywords as shown in Table 1. These 
labels will be used later by the concept-value recogni-
tion component. 
In the training step, each concept WFST was cre-
ated separately. The training utterances were tagged by 
marking just the words required by the concept. Then 
the WFST was constructed by: 
1. replacing the unmarked words in each training  
utterance by the symbols ?NIL?, 
2. making an individual FST for the preprocessed 
utterance, 
3. performing the union operation of all FSTs and 
determinizing the resulting FST, 
4. attaching the recursive-arcs of every word to 
the start and end node as illustrated in Fig. 2, 
5. assigning the weights to the transitions as  
described previously. 
In the parsing step, an input utterance is fed to 
every concept WFST in parallel. For each WFST, the 
words in the utterance that are not included in the 
WFST are replaced by the symbols ?NIL? and the pre-
processed word string is parsed by the WFST using the 
composition operation. By minimizing the cumulative 
weight, the longest accepted substring is chosen. A 
concept is considered to exist if at least one substring 
is accepted. Since this model is a kind of word-
grammar representation for a particular concept, we 
have called it the concept regular grammar or ?Reg? 
model in short. 
 
                               ?two  nights  from  the  sixth  of  July? 
Concept Keyword labels of accepted substring 
(1) reservedate 
    -        -          ?      ?       fd     ?    fm 
(2) numnight 
   nn      ?          -       -       -       -     - 
Goal inform_prerequisite-keys 
Label sequence 
 2:nn    2:?     1:?    1:?   1:fd  1:?  1:fm 
 
                               ?there   is    a   pool,  right?? 
Concept Keyword labels of accepted substring 
(1) reqprovide 
     ?      ?      -      -        ? 
(2) facility 
     -       -     ?      fc       -  
(3) yesnoq 
     -       -      -      -        ? 
Goal request_facility 
Label sequence 
   1:?   1:?   2:?  2:fc  1:?,3:? 
 
Table 1. Examples of defined goals, concepts and their 
corresponding substrings presented by keyword labels. 
3.2 Goal identification 
Having extracted the concepts, the goal of the utter-
ance can be identified. The goal in our case can be 
considered as a derivative of the dialogue act coupled 
with additional information. As the examples show in 
Table 1, the goal ?request_facility? means a request 
(dialogue act) for some facilities (additional informa-
tion). Since we observed in our largest corpus that 
only 1.1% were multiple-goal utterances, an utterance 
could be supposed to have only one goal. 
The goal identification task can be viewed as a 
simple pattern classification problem, where a goal is 
identified given an input vector of binary values indi-
cating the existence of predefined concepts. Our previ-
ous work (Wutiwiwatchai and Furui, 2003b) showed 
that this task could be efficiently achieved by the sim-
ple multi-layer perceptron type of artificial neural net-
work (ANN). 
DGT: np /-1 
I: ac /-1 DGT: nc /-1 
person: ? /-1 
NIL: ? /0 
S E 
friend: ac /-1 
I: ? /0 
NIL: ? /0 
DGT: ? /0 
? 
I :? /0 
NIL: ? /0 
DGT: ? /0 
? 
3.3 Concept-value recognition 
Recall again that some concepts contain values such as 
the concept ?numperson?, whose value is the number 
of people, whereas some concepts do not, such as the 
concept ?yesnoq?. Given an input utterance, the SLU 
module must be able to identify the goal and extract 
information items such as the reserved date, the num-
ber of people, the name of facility, etc. The concepts 
extracted in the first stage are not only used to identify 
the goal, but also strongly related to the described in-
formation items, that is, the values of concepts are 
actually the required information items. Hence, ex-
tracting the information items is to recognize the con-
cept values. 
Since the keywords within a concept have already 
been labeled by WFST composition in the concept 
extraction step, recognizing the concept-value is just a 
matter of converting the labeled keywords to a certain 
format. For sake of explanation, let?s consider the ut-
terance ?two nights from the sixth of July? in Table 1. 
After parsing by the ?reservedate? (the reserved date) 
concept WFST, the substring ?from the sixth of July? 
is accepted with the words ?sixth? and ?July? labeled 
by the symbols ?fd? and ?fm? respectively. These label 
symbols are specifically defined for each type of con-
cept and have their unique meanings, e.g. ?fd? for the 
check-in date, ?fm? for the check-in month, etc. The 
labeled keywords are then converted to a predefined 
format for the concept value. The value of ?reserve-
date? concept is in a form of <fy-fm-fd_ty-tm-td>, and 
thus the labeled keywords ?sixth(fd) July(fm)? is con-
verted to <04-07-06_ty-tm-td>. It must be noted that 
although the check-in year is not stated in the utterance, 
the concept-value recognition process under its knowl-
edge-base inherently assigns the value ?04? (the year 
2004) to the ?fy?. This process can greatly help in solv-
ing anaphoric expressions in natural conversation. Ta-
ble 2 gives more examples of substrings accepted and 
labeled by ?reservedate? WFST, and their correspond-
ing values. Currently, this conversion task is per-
formed by simple rules. 
 
Accepted substring Concept-value 
 
?sixth(fd) to eighth(td) of July(tm)? 
 
?check-in tomorrow(fd)? 
 
?until next Tuesday(td)? 
 
<04-07-06_04-07-08> 
 
<04-06-10_ty-tm-td> 
 
<fy-fm-fd_04-06-18> 
 
 
Table 2. Examples of substrings accepted by the ?re-
servedate? WFST with their corresponding values. 
4 Hybrid Statistical and Structural Se-
mantic Modeling 
Although the Reg model described in Sect. 3.1 has an 
ability to capture long-distant dependencies for seen 
grammar, it certainly fails to parse an unseen-grammar 
utterance, especially when it is distorted by speech 
recognition errors. This article thus presents an effort 
to improve concept extraction and concept-value 
recognition by incorporating a statistical approach. 
4.1 N-gram modeling 
We can view the concept extraction process as a se-
quence labeling task, where a label sequence L = (l1 ? 
lT) as shown in the ?Label sequence? lines of Table 1 
is determined given a word string W = (w1?wT). Each 
label, in the form of {c:l}, refers to the cth-concept 
with keyword label l. A word is allowed to be in mul-
tiple concepts, hence having multiple keyword labels 
such as {1:?,3: ?} as shown in the last line of Table 1. 
Finding the most probable sequence L is equivalent to 
maximizing the joint probability P(W,L), which can be 
simplified using n-gram modeling (n = 2 for bigram) 
as follows: 
?
=
??
==
T
t
tttt
LL
lwlwPLWPL
1
11 ),|,(maxarg),(maxarg~  
 (1) 
The described n-gram model, called ?Ngram? 
hereafter, can be implemented also by a WFST, whose 
weights are the smoothed n-gram probabilities. Parsing 
an utterance by the Ngram WFST is performed simply 
by applying the WFST composition in the same way 
as operated with the Reg model. 
4.2 Logical n-gram modeling 
Although the n-gram model can assign a likelihood 
score to any input utterance, it cannot distinguish be-
tween valid and invalid grammar structure. On the 
other hand, the regular grammar model can give se-
mantic tags to an utterance that is permitted by the 
grammar, but always rejects an ungrammatical utter-
ance. Thus, another probabilistic approach that inte-
grates the advantages of both models is optimum. 
Our proposed model, motivated mainly by (B?chet 
et al 2002), combines the statistical and structural 
models in two-pass processing. Firstly, the conven-
tional n-gram model is used to generate M-best hy-
potheses of label sequences given an input word string. 
The likelihood score of each hypothesis is then en-
hanced once its word-and-label syntax is permitted by 
the regular grammar model. By rescoring the M-best 
list using the modified scores, the syntactically valid 
sequence that has the highest n-gram probability is 
reordered to the top. Even if no label sequence is per-
mitted by the regular grammar, the hybrid model is 
still able to output the best sequence based on the 
original n-gram scores. Since the proposed model aims 
to enhance the logic of n-gram outputs, it is named the 
logical n-gram model. 
This idea can be implemented efficiently in the 
framework of WFST as depicted in Fig. 3. At first, the 
concept-specific Reg WFST is modified from the one 
shown in Fig. 2 by replacing the weight -1 by a vari-
able -?, which can be empirically adjusted to gain the 
best result. An unknown word string in the form of a 
finite state machine is parsed by the Ngram WFST, 
producing a WFST of M-best label-sequence hypothe-
ses. Concepts are detected in the top hypothesis. Then, 
the concept-value recognition process is applied for 
each detected concept separately. In the concept-value 
recognition process, the M-best WFST is intersected 
by the concept-specific Reg WFST. Rescoring the 
result offers a new WFST of P-best (P < M) hypothe-
ses with a score in logarithmic domain for each hy-
pothesis assigned by 
 
?
=
??
+=
T
t
ttttt lwlwPScore
1
11 )),|,((log ? , (2) 
where }0,{?? ?t . If ? is set to 0, the intersection op-
eration is just to filter out the hypotheses that violate 
the regular grammar, while the original scores from n-
gram model are left unaltered. If a larger ? is used, the 
hypothesis that contains a longer valid syntax is given 
a higher score. When no hypothesis in the M-best list 
is permitted by the grammar (P = 0), the top hypothe-
sis of the M-best list is outputted. It is noted that the 
strategy of eliminating unacceptable paths of n-gram 
due to syntactical violation has also successfully been 
used in a WFST-based speech recognition system 
(Szarvas and Furui, 2003). Hereafter, we will refer to 
the logical n-gram modeling as ?LNgram?. 
4.3 The use of ASR N-best hypotheses 
The probabilistic model allows the use of N-best hy-
potheses from the automatic speech recognition (ASR) 
engine. As described in Sect. 4.1, our Ngram semantic 
model produces a joint probability P(W,L), which in-
dicates the chance that the semantic-label sequence L 
occurs with the word hypothesis W. When the N-best 
word hypotheses generated from the ASR are fed into 
the Ngram semantic parser, the parsed scores are 
combined with the ASR likelihood scores in a log-
linear interpolation fashion (Klakow, 1998) as shown 
in Eq. 3. 
?? ?
??
?
1
,
),(),(maxarg~ LWPWAPL
NWL
 (3) 
where A is an acoustic speech signal, and P(A,W) is a 
product of an acoustic score P(A|W) and a language 
score P(W). ?N denotes the N-best list and ? is an in-
terpolation weight, which can be adjusted experimen-
tally to give the best result. This interpolation method 
can be easily implemented in a WFST framework 
compared to normal linear interpolation. 
An N-best list can be used in the LNgram using 
the same criterion as well. The only necessary precau-
tion is an appropriate size of M in the M-best seman-
tic-label list, which is rescored in the second pass to 
improve the concept-value result. 
 
 
Figure 5. Logical n-gram modeling. 
5 Evaluation and Discussion 
5.1 Corpora 
Collecting and annotating a corpus is an especially 
serious problem for language like Thai, where only 
few databases are available. To shorten the collection 
time, we created a specific web page simulating our 
expected conversational dialogues, and asked Thai 
native users to answer the dialogue questions by typ-
ing. As we asked the users to try answering the ques-
tions using spoken language, we could obtain a fairly 
good corpus for training the SLU. 
Currently, 5,869 typed-in utterances from 150 us-
ers have been completely annotated. To reduce the 
effort of manual annotation, we conducted a semi-
automatic annotation method. The prototype rule-
based SLU was used to roughly tag each utterance 
with a goal and concepts, which were then manually 
corrected. Words or phases that were relevant to the 
concept were marked automatically based on their 
frequencies and information mutual to the concept. 
Finally the tags were manually checked and the key-
words within each concept were additionally marked 
by the defined label symbols. 
All 5,869 utterances described above were used as 
a training set (TR) for the SLU system. We also col-
lected a set of speech utterances during an evaluation 
of our prototype dialogue system. It contained 1,101 
speech utterances from 96 dialogues. By balancing the 
 Semantic-label tagging 
by the Ngram model 
The top hypothesis 
M-best hypotheses 
 
Rescoring by each 
concept Reg model 
Extracted 
concepts 
 
Converting keyword seq. 
to concept values 
Concept values 
Concept-value 
recognition 
Word string 
Concept 
extraction 
Concept 
Reg models 
occurrence of goals, we reserved 500 utterances for a 
development set (DS), which was used for tuning pa-
rameters. The remaining 601 utterances were used for 
an evaluation set (ES). Table 3 shows the characteris-
tics of each data set. From the TR set, 75 types of con-
cepts and 42 types of goals were defined. The out-of-
goal and out-of-concept denote goals and concepts that 
are not defined in the TR set, and thus cannot be rec-
ognized by the trained SLU. Since concepts that con-
tain no value are not counted for concept-value 
evaluation, Table 3 also shows the number of concepts 
that contain values in the line ?# Concept-values?. 
 
Characteristic TR DS ES 
# Utterances 5,869 500 601 
# Words / utterance 7.3 6.2 5.8 
# Goal types 42 40 40 
# Concept types 75 58 57 
# Concept-value types 20 18 18 
# Concepts 10,041 791 949 
# Concept-values 6,365 366 439 
% Out-of-goal  5.2 5.3 
% Out-of-concept  2.8 3.3 
% Word accuracy  77.2 79.0 
 
Table 3. Characteristics of data sets 
5.2 Evaluation measures 
Four measures were used for evaluation: 
1. Word accuracy (WAcc) ? the standard measure 
for evaluating the ASR, 
2. Concept F-measure (ConF) ? the F-measure of 
detected concepts, 
3. Goal accuracy (GAcc) ? the number of 
utterances with correctly identified goals, 
divided by the total number of test utterances, 
4. Concept-value accuracy (CAcc) ? the number 
of concepts, whose values are correctly 
matched to their references, divided by the total 
number of concepts that contain values. 
5.3 The use of logical n-gram modeling 
The first experiment was to inspect improvement 
gained after conducting the statistical approaches for 
concept extraction and concept-value recognition. 
Only the 1-best word hypothesis from the ASR was 
experimented in this section. The AT&T generalized 
FSM library (Mohri et al, 1997) was used to construct 
and operate all WFSTs, and the SNNS toolkit (Zell et 
al., 1994) was used to create the ANN classifiers for 
the goal identification task. 
The baseline system utilized the Reg model for 
concept extraction and concept-value recognition, and 
the multi-layer perceptron ANN for goal identification. 
75 WFSTs corresponding to the number of defined 
concepts were created from the TR set. The ANN con-
sisted of a 75-node input layer, a 100-node hidden 
layer (Wutiwiwatchai and Furui, 2003b), and a 42-
node output layer equal to the number of goals to be 
identified. 
66
68
70
72
74
76
10 20 30 40 50 60 70 80 90 100
M -best
CA
c
c 
(%
)
 
Figure 4. CAcc results with respect to values of M in 
an oracle test for the DS set. 
58
59
60
61
62
63
64
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
1.
1
1.
2
CA
cc
 
(%
)
 
          ? 
Figure 5. CAcc results with variation of ? for the DS 
set when M is set to 80. 
Recognition Orthography Measure 
Reg Ngram LNgram Reg LNgram 
ConF 76.5 88.6 78.9 91.4 
GAcc 71.4 76.0 81.2 83.5 
CAcc 65.1 52.4 67.2 75.7 76.8 
 
Table 4. Evaluation results for the ES set using the 
Reg, Ngram, and LNgram models. 
Another WFST was constructed for the n-gram 
semantic parser (n = 2 in our experiment), which was 
used for the Ngram model and the first pass of the 
LNgram model. Two parameters, M and ?, in the 
LNgram approach need to be adjusted. To determine 
an appropriate value of M, we plotted in an oracle 
mode the CAcc of the DS set with respect to M, as 
shown in Figure 4. According to the graph, an M of 80 
was considered optimum and set for the rest of the 
experiments. Figure 5 then shows the CAcc obtained 
for rescored M-best hypotheses when the weight ? as 
defined in Eq. 2 is varied. Here, the larger value of ? 
means to assign a higher score to the hypothesis that 
contains longer valid word-and-label syntax. Hence, 
we concluded by Fig. 5 that reordering the hypotheses, 
which contain longer valid syntaxes, could improve 
the CAcc significantly. Since the CAcc results become 
steady when the value of ? is greater than 0.7, a ? of 
1.0 is used henceforth to ensure the best performance. 
The overall evaluation results on the ES set are 
shown in Table 4, where M and ? in the LNgram 
model are set to 80 and 1.0 respectively. ?Recognition? 
denotes the experiments on automatic speech-
recognized utterances (at 79% WAcc), whereas ?Or-
thography? means their exact manual transcriptions. It 
is noted that the LNgram approach utilizes the same 
process of Ngram in its first pass, where the concepts 
are determined. Therefore, the ConF and GAcc results 
of both approaches are the same. 
According to the results, the Ngram tagger worked 
well for the concept extraction task as it increased the 
ConF by over 10%. The improvement mainly came 
from reduction of redundant concepts often accepted 
by the Reg model. The better extraction of concepts 
could give better goal identification accuracy reasona-
bly. However, as we expected, the conventional 
Ngram model itself had no syntactic information and 
thus often produced a confusing label sequence, espe-
cially for ill-formed utterances. A typical error oc-
curred for words that could be tagged with one of 
several semantic labels, such as the word ?MNT? (re-
ferring to the name of the month), which could be 
identified as ?check-in month? or ?check-out month?. 
These two alternatives could only be clarified by a 
context word, which sometimes located far from the 
word ?MNT?. This problem could be solved by using 
the Reg model. The Reg model, however, could not 
provide a label sequence to any out-of-syntax sentence. 
The LNgram as an integration of both models thus 
obviously outperformed the others. 
In conclusion, the LNgram model could improve 
the ConF, GAcc, and CAcc by 15.8%, 6.4%, and 3.2% 
relative to the baseline Reg model. Moreover, if we 
considered the orthography result an upperbound of 
the underlying model, the GAcc and CAcc results pro-
duced by the LNgram model are relatively closer to 
their upperbounds compared to the Reg model. This 
verifies robustness improvement of the proposed 
model against speech-recognition errors. 
5.4 The use of ASR N-best hypotheses 
To incorporate N-best hypotheses from the ASR to the 
LNgram model, we need to firstly determine an ap-
propriate value of N. An oracle test that measures 
WAcc and ConF for the DS set with variation of N is 
shown in Fig. 6. Although we can select a proper value 
of N by considering only the WAcc, we also examine 
the ConF to ensure that the selected N provides possi-
bility to improve the understanding performance as 
well. According to Fig. 6, the ConF highly correlates 
to the WAcc, and an N of 50 is considered optimum 
for our task. At this operating point, we plot another 
curve of ConF for the DS set with a variation of ?, the 
interpolation weight in Eq. 3, as shown in Fig. 7. The 
appropriate value of ? is 0.6, as the highest ConF is 
obtained at this point. The last parameter we need to 
adjust is the value of M. Although we have tuned the 
value of M for the case of 1-best word hypothesis, the 
appropriate value of M may change when the N-best 
hypotheses are used instead. However, in our trial, we 
found that the optimum value of M is again in the 
same range as that operated for the 1-best case. A 
probable reason is that rescoring the N-best word hy-
potheses by the Ngram model can reorder the good 
hypotheses to a certain upper portion of the N-best list, 
and thus rescoring in the second pass of the LNgram 
is independent to the value of N. Consequently, an M 
of 80 as that selected for the 1-best hypothesis is also 
used for the N-best case. 
75
80
85
90
95
100
1 11 21 31 41 51 61 71 81 91
N -best
%
WAcc
ConF
 
Figure 6. WAcc and ConF results with respect to val-
ues of N in an oracle test for the DS set. 
85
86
87
88
89
90
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
Co
n
F 
(%
)
50-best
1-best
 
   ? 
Figure 7. ConF results with variation of ? for the DS 
set when N is set to 50. 
Given all tuned parameters, an evaluation on the 
ES set is carried out as shown in Fig. 8. With the Reg 
model as a baseline system, the use of N-best hypothe-
ses further improves the the ConF, GAcc, and CAcc 
by 0.9%, 0.6%, and 3.9% from the only 1-best, and 
hence reduces the gap between the speech-recognized 
test set and the orthography test set by 25%, 5.3%, and 
26% respectively. 
Finally, we would like to note that the proposed 
LNgram approach provided the significant advantage 
of a much smaller computational time compared to the 
original Reg approach. While the Reg model requires 
C times (C denotes the number of defined concepts) of 
WFST operations to determine concepts, the LNgram 
needs only D+1 times (D << C), where D is the num-
ber of concepts appearing in the top hypothesis pro-
duced by the n-gram semantic model. Moreover, under 
the framework of WFST, incorporating ASR N-best 
hypotheses required only a small increment of addi-
tional processing time compared to the use of 1-best. 
60
65
70
75
80
85
90
95
ConF GAcc CAcc
%
Reg
LNgram (1-best)
LNgram (50-best)
Orthgraphy
 
Figure 8. Comparative results for the ES set between 
the use of ASR 1-best and N-best (N = 50) hypotheses. 
6 Conclusion and Future Works 
Recently, a multi-stage spoken language understanding 
(SLU) approach has been proposed for the first Thai 
spoken dialogue system. This article reported an im-
provement on the SLU system by replacing the regular 
grammar-based semantic model by a hybrid n-gram 
and regular grammar approach, which not only cap-
tures long-distant dependencies of word syntax, but 
also provides robustness against speech-recognition 
errors. The proposed model, called logical n-gram 
modeling, obviously improved the performance in 
every SLU stage, while reducing the computational 
time compared to the original regular-grammar ap-
proach. Under the probabilistic WFST framework, the 
system was improved further by using N-best word-
hypotheses from the ASR, requiring only a small addi-
tional processing time compared to the use of 1-best. 
Further improvement of overall speech understand-
ing as well as a spoken dialogue system in the future 
can be expected by introducing dialogue-state depend-
ent modeling in the ASR and/or the SLU. A better way 
to utilize the first P-best goal hypotheses produced by 
the goal identifier instead of 1-best would also en-
hance the understanding performance. 
 
References 
B?chet, F., Gorin, A., Wright, J., and Tur, D. H. 2002. 
Named entity extraction from spontaneous speech in How 
May I Help You. Proc. ICSLP 2002, 597-600. 
Est?ve, Y., Raymond, C., B?chet, F., and De Mori, R. 2003. 
Conceptual decoding for spoken dialogue systems. Proc. 
Eurospeech 2003, 617-620. 
Gorin, A. L., Riccardi, G., and Wright, J. H. 1997. How May 
I Help You. Speech Communication, 23, 113-127. 
Hacioglu, K., and Ward, W. 2001. Dialog-context dependent 
language modeling combining n-grams and stochastic 
context-free grammars. Proc. ICASSP 2001, 537-540. 
Klakow, D. 1998. Log-linear interpolation of language mod-
els. Proc. ICSLP 1998, 1695-1699. 
Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. 1994. 
Hidden understanding models of natural language. Proc. 
ACL 1994, 25-32. 
Mohri, M., Pereira, F., and Riley, M. 1997. General-purpose 
finite-state machine software tools. 
http://www.research.att.com/sw/tools/fsm, AT&T Labs ? 
Research. 
Potamianos, A., Kwang, H., and Kuo, J. 2000. Statistical 
recursive finite state machine parsing for speech under-
standing. Proc. ICSLP 2000, vol.3, 510-513. 
Seneff, S. 1992. TINA: A natural language system for spoken 
language applications. Computational Linguistics, 18(1), 
61-86. 
Szarvas, M. and Furui, S. Finite-state transducer based 
modeling of morphosyntax with applications to Hungar-
ian LVCSR. Proc. ICASSP 2003, 368-371. 
Wang, Y. Y., Mahajan, M., and Huang, X. 2000. A unified 
context-free grammar and n-gram model for spoken lan-
guage processing. Proc. ICASSP 2000, 1639-1642. 
Wang, Y. Y., Acero, A., Chelba, C., Frey, B., and Wong, L. 
2002. Combination of statistical and rule-based ap-
proaches for spoken language understanding. Proc. 
ICSLP 2002, 609-612. 
Wutiwiwatchai, C. and Furui, S. 2003a. Pioneering a Thai 
Language Spoken Dialogue System. Spring Meeting of 
Acoustic Society of Japan, 2-4-15, 87-88. 
Wutiwiwatchai, C., and Furui, S. 2003b. Combination of 
finite state automata and neural network for spoken lan-
guage understanding. Proc. EuroSpeech 2003, 2761-2764. 
Zell, A., Mamier, G., Vogt, M., Mach, N., Huebner, R., 
Herrmann, K. U., Doering, S., and Posselt, D. SNNS 
Stuttgart neural network simulator, user manual. Univer-
sity of Stuttgart. 
Monolingual Web-based Factoid Question Answering in Chinese,
Swedish, English and Japanese
E.W.D. Whittaker J. Hamonic D. Yang T. Klingberg S. Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1, Ookayama, Meguro-ku
Tokyo 152-8552 Japan
 edw,yuuki,raymond,tor,furui@furui.cs.titech.ac.jp
Abstract
In this paper we extend the application
of our statistical pattern classification ap-
proach to question answering (QA) which
has previously been applied successfully
to English and Japanese to develop two
prototype QA systems in Chinese and
Swedish. We show what data is necessary
to achieve this and also evaluate the per-
formance of the two new systems using a
translation of the TREC 2003 factoid QA
task. While performance for Chinese and
Swedish is found to be lower than that for
the more developed English and Japanese
systems we explain why this is the case
and offer solutions for their improvement.
All systems form the basis of our pub-
licly accessible web-based multilingual
QA system at http://asked.jp.
1 Introduction
Much of the research into automatic question an-
swering (QA) has understandably concentrated on
the English language with little regard to portabil-
ity or efficacy in other languages. It is only rela-
tively recently, with the introduction of the CLEF
and NTCIR QA evaluations, that researchers have
started to look at porting and evaluating the tech-
niques that have been shown to work well for En-
glish to other languages.
One of the major drawbacks of porting an En-
glish language QA system or approach to other
languages is often the lack of the corresponding
NLP tools in the target language. For instance,
parsers and named-entity (NE)-taggers, which are
typical components in many QA systems, are cer-
tainly not available for all the world?s languages.
Trainable parsers and NE-taggers similarly require
appropriate training data which, if not available,
is costly and requires specialized knowledge to
produce. Language-specific databases are also a
common feature of many systems, some of which
have taken many man-years to construct and ver-
ify. Such a component in many English-language
systems, for example, is WordNet. While porting
WordNet to other languages has been started in the
Euro and Global WordNet projects they still only
cover a relatively small number of languages.
In this paper we describe the application of our
data-driven approach to QA which was developed
right from the outset with the aim of portability
and robustness in mind. This statistical pattern
classification approach to QA is essentially lan-
guage independent and trainable given appropriate
language-specific training data. No assumptions
about the language are made by the model except
that some notion of words or space-separated to-
kens must exist or be introduced where it is ab-
sent. Our only other requirements to build a QA
system in a new target language are: a large col-
lection of text data in the target language that can
be searched for answers (e.g. the web), and a list
of example questions-and-answers (q-and-a) in the
target language. Given these data sources the re-
maining components can be obtained automati-
cally for each language.
So, in contrast to other contemporary ap-
proaches to QA our English language system
does not use WordNet as in (Hovy et al, 2001;
Moldovan et al, 2002), NE extraction, or any
other linguistic information e.g. from semantic
analysis (Hovy et al, 2001) or from question pars-
ing (Hovy et al, 2001; Moldovan et al, 2002) and
uses capitalised (where appropriate for the lan-
guage) word tokens as the only features for mod-
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
45
elling. For our Japanese system, although we cur-
rently use Chasen to segment Japanese charac-
ter sequences into units that resemble words, we
make no use of any morphological information
as used for example in (Fuchigami et al, 2004).
Moreover, it should be noted that our approach
is at the same time very different to other purely
web-based approaches such as askMSR (Brill et
al., 2002) and Aranea (Lin et al, 2002). For exam-
ple, we use entire documents rather than the snip-
pets of text returned by web search engines; we do
not use structured document sources or databases
and we do not transform the query in any way ei-
ther by term re-ordering or by modifying the tense
of verbs. These basic principles apply to each of
our language-specific QA systems thus simplify-
ing and accelerating development.
The approach to QA that we adopt has previ-
ously been described in (Whittaker et al, 2005a;
Whittaker et al, 2005b; Whittaker et al, 2005c)
where the details of the mathematical model and
how it was trained for English and Japanese were
given. Our approach has also been successfully
evaluated in the text retrieval conference (TREC)
2005 QA track evaluation (Voorhees, 2003) where
our group placed eleventh out of thirty partici-
pants (Whittaker et al, 2005a). Although the
TREC QA task is substantially different to web-
based QA the TREC evaluation confirmed that our
approach works and also provides an objective as-
sessment of its quality. Similarly, for our Japanese
language system we have previously evaluated the
performance of our approach on the NTCIR-3
QAC-1 task (Whittaker et al, 2005c). Although
our Japanese experiments were applied retrospec-
tively, the results would have placed us in the mid-
range of participating systems in that year?s eval-
uation. In this paper we present additional ex-
periments on Chinese and Swedish and explain
how our statistical pattern classification approach
to QA was successfully applied to these two new
languages. Using our approach and given ap-
propriate training data it is found that a reason-
ably proficient developer can build a QA system
in a new language in around 10 hours. Evalua-
tion of the Chinese and Swedish systems is per-
formed using a translation of the first 200 fac-
toid questions from the TREC 2003 evaluation
which we have also made available online. We
compare these results both qualitatively and quan-
titatively against results obtained previously for
English and Japanese. The systems, built us-
ing this method, form the basis of our multi-
language web demo which is publicly available at
http://asked.jp.
An outline of the remainder of this paper is as
follows: we briefly describe our statistical pattern
classification approach to QA in Section 2 repeat-
ing the important elements of our approach as nec-
essary to understand the remainder of the paper.
In Section 3 we describe the basic building blocks
of our QA system and how they can typically be
trained. We also give a breakdown of the data
used to train each language specific QA system.
In Section 4 we present the results of experiments
on Chinese, Swedish, English and Japanese and in
Section 5 we compare and analyse these results.
We wrap up with a conclusion and further work in
Sections 6 and 7.
2 Statistical pattern classification
approach to QA
The answer to a question depends primarily on
the question itself but also on many other factors
such as the person asking the question, the loca-
tion of the person, what questions the person has
asked before, and so on. Although such factors
are clearly relevant in a real-world scenario they
are difficult to model and also to test in an off-
line mode, for example, in the context of the NT-
CIR and TREC evaluations. We therefore choose
to consider only the dependence of an answer  
on the question , where each is considered to
be a string of 
 
words     
 
     

 
and 

words    
 
     


, respectively. In particu-
lar, we hypothesize that the answer   depends on
two sets of features      and     
as follows:
	       	     (1)
where    

 
     



can be thought of as a
set of 

features describing the ?question-type?
part of  such as who, when, where, which, etc.
and    
 
     


is a set of 

features
comprising the ?information-bearing? part of 
i.e. what the question is actually about and what it
refers to. For example, in the questions, ?Where
was Tom Cruise married?? and ?When was
Tom Cruise married?? the information-bearing
component is identical in both cases whereas the
question-type component is different.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
46
Finding the best answer   involves a search
over all   for the one which maximizes the prob-
ability of the above model:

    
 
	     (2)
Using Bayes rule, making further conditional
independence assumptions and assuming uniform
prior probabilities, which therefore do not affect
the optimisation criterion, we obtain the final op-
timisation criterion (see (Whittaker et al, 2005a)
for more details):

 
	    
   
	


 	    
   


 (3)
The 	     model is essentially a language
model which models the probability of an answer
sequence   given a set of information-bearing fea-
tures  . It models the proximity of   to features
in  . We call this model the retrieval model and
examine it further in Section 2.1.
The 	     model matches an answer  
with features in the question-type set  . Roughly
speaking this model relates ways of asking a ques-
tion with classes of valid answers. For example, it
associates dates, or days of the week with when-
type questions. In general, there are many valid
and equiprobable   for a given  so this compo-
nent can only re-rank candidate answers retrieved
by the retrieval model. Consequently, we call it the
filter model and examine it further in Section 2.2.
2.1 Retrieval model
The retrieval model essentially models the prox-
imity of   to features in  . Since    

 
     

 
we are actually modelling the distri-
bution of multi-word sequences. This should be
borne in mind in the following discussion when-
ever   is used. As mentioned above, we currently
use a deterministic information-feature mapping
function     . This mapping only gener-
ates word -tuples (   	 
   ) from single
words in  that are not present in a stoplist of 50-
100 high-frequency words. For more details on
the exact form of the retrieval model please refer
to (Whittaker et al, 2005a).
2.2 Filter model
A set of 
 
 single-word features is extracted
based on frequency of occurrence in question data.
Some examples include: HOW, MANY, WHEN,
WHO, UNTIL etc. The question-type mapping
function   extracts -tuples (   	 
   ) of
question-type features from the question , such
as HOW MANY and UNTIL WHEN.
Modelling the complex relationship between 
and   directly is non-trivial. We therefore intro-
duce an intermediate variable representing classes
of example questions-and-answers (q-and-a) 

for
   	    

 drawn from the set 

, and to fa-
cilitate modelling we say that  is conditionally
independent of   given 

as follows:
	      




 
	   

  	 

   (4)
Given a set  of example q-and-a 

for
   	     where 

  

 

  


 
     




 

 
     


 

 we define a mapping
function     

by 

   . Each
class 

  


 
     





 

 
     


 

 is then
obtained by 

 




 



 


 



. In all the
experiments in this paper no clustering of the q-
and-a is actually performed so each q-and-a ex-
ample forms its own unique class i.e. each 

cor-
responds to a single 

and vice-versa.
Assuming conditional independence of the an-
swer words in class 

given   and making the
modelling assumption that the th answer word 

in the example class 

is dependent only on the
th answer word in   we obtain:
	      




 
	   

 

 


 
	 


 


 




 
	   



 


 

 



 
	 


 


	 


 


(5)
where 


is a concrete class in the set of 
 
 an-
swer classes 
 
, and assuming 

is conditionally
independent of 


given 

. The system using the
formulation of filter model given by Equation (5)
is referred to as model ONE. The model given by
Equation (4) is referred to as model TWO, how-
ever, we are only concerned with model ONE in
this paper.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
47
Answer typing, such as it exists in our model
ONE system, is performed by the filter model and
is effected by matching the input query against
our set of example questions 

. Each one of
these example questions 

has an answer asso-
ciated with it which in turn is expanded via the

 
classes into a set of possible answers for each
question. At each step this matching process is
entirely probabilistic as given by Equation (5). To
take a simple example, if we are presented with
the input query ?Who was the U.S. President who
first used Camp David?? the first step effectively
matches the words (and bigrams, trigrams) in that
query against the words (and bigrams, trigrams)
in our example questions with a probability of its
match assigned to all example questions. Suppose,
for example, that the top-scoring example question
in our set is ?Who was the U.S. President who re-
signed as a result of the Watergate affair?? since
the first six words in each question match each
other and will likely result in a high probability
being assigned. The corresponding answer to this
example question is ?Richard Nixon?. So the next
step expands ?Richard? using 
 
and results in
a high score being assigned to a class of words
which tend to share the feature of being male first
names, one of which is ?Franklin?. Expanding
the second word in the answer ?Nixon? using 
 
will possibly result in a high score being assigned
to a class of words that share the feature of being
the surnames of U.S. presidents, one of which is
?Roosevelt?. In this way, we end up with a high
score assigned by the filter model to ?Franklin
Roosevelt? but also to ?Abraham Lincoln?, ?Bill
Clinton?, etc. In combination with the retrieval
model, we hope that the documents obtained for
this query assign a higher retrieval model score
to ?Franklin Roosevelt? over the names of other
U.S. presidents and thus output it in first place
with the highest overall probability. While this ap-
proach works well for names, dates and short place
names it does fall down, for example, on names
of books, plays and films where there is typically
less of a clear correspondence between the words
in any given position of two answers. This situa-
tion could be avoided by using multi-word answer
strings and not making the position-dependence
modelling assumption that was made to arrive at
Equation (5) but this has its own drawbacks.
The above description of the operation of the
filter model highlights the need for homogeneous
classes of 
 
of sufficiently wide coverage. In the
next section we describe a way in which this can
be achieved in an efficient, data-driven manner for
essentially any language.
2.3 Obtaining 
 
As we saw in the previous section the 
 
are the
closest thing we have to named entities since they
define classes of words that share some similarity
with each other. However, in some sense they are
more flexible than named entities since any word
can actually belong to any class but with a certain
probability. In this way we don?t rule out with a
zero probability the possibility of a word belong-
ing to some class, just that a word is more likely
to belong to some classes than others. In addition,
the entities are not actually named i.e. we do not
impose our own label on the classes so we do not
explicitly have a class of first names, or a class of
days of the week. Although we hope that we will
end up with classes containing such similar words
we do not make it explicit and we do not label what
each class of words is supposed to represent.
In keeping with our data-driven philosophy
and related objective to make our approach as
language-independent as possible we use an ag-
glomerative clustering algorithm to derive classes
automatically from data. The set of potential an-
swer words 

 
that are clustered, should ideally
cover all possible words that might ever be an-
swers to questions. We therefore take the most fre-
quent 

 
 words from a language-specific cor-
pus  comprising   word tokens as our set of
potential answers. The ?seeds? for the clusters are
chosen to be the most frequent 
 
 words in  .
The algorithm then uses the co-occurrence proba-
bilities of words in  to group together words with
similar co-occurrence statistics. For each word 
in 

 
the co-occurrence probability 	 

  
is the probability of 

given  occurring  words
away. If  is positive, 

occurs after , and if
negative, before . We then construct a vector
of co-occurrences with maximum separation be-
tween words , as follows:
     
 
         

     
 
      
 

           
 
        

  
 
 
        

   (6)
Rather than storing 
 

 
elements we can com-
pute most terms efficiently and on-the-fly using
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
48
Language    

 
 

 
 

Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000
Swedish PAROLE (University, 1997) 19M 367k 5k 1000
English AQUAINT (Voorhees, 2002) 300M 215k 290k 500
Japanese MAINICHI (Fukumoto et al, 2002) 150M 300k 270k 5000
Table 1: Description of each of the four monolingual QA systems.
the Katz back-off method (Katz, 1987) and ab-
solute discounting for estimating the probabilities
of unobserved events. To find the distance be-
tween two vectors, for efficiency, we use an 
 
distance metric: Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 72?75,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Combining a Two-step Conditional Random Field Model and a Joint
Source Channel Model for Machine Transliteration
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi
Masanobu Nakamura and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Techonology
{raymond,dixonp,thomas,oonishi,masa,furui}@furui.cs.titech.ac.jp
Abstract
This paper describes our system for
?NEWS 2009 Machine Transliteration
Shared Task? (NEWS 2009). We only par-
ticipated in the standard run, which is a
direct orthographical mapping (DOP) be-
tween two languages without using any
intermediate phonemic mapping. We
propose a new two-step conditional ran-
dom field (CRF) model for DOP machine
transliteration, in which the first CRF seg-
ments a source word into chunks and the
second CRF maps the chunks to a word
in the target language. The two-step CRF
model obtains a slightly lower top-1 ac-
curacy when compared to a state-of-the-
art n-gram joint source-channel model.
The combination of the CRF model with
the joint source-channel leads to improve-
ments in all the tasks. The official re-
sult of our system in the NEWS 2009
shared task confirms the effectiveness of
our system; where we achieved 0.627 top-
1 accuracy for Japanese transliterated to
Japanese Kanji(JJ), 0.713 for English-to-
Chinese(E2C) and 0.510 for English-to-
Japanese Katakana(E2J) .
1 Introduction
With the increasing demand for machine transla-
tion, the out-of-vocabulary (OOV) problem caused
by named entities is becoming more serious.
The translation of named entities from an alpha-
betic language (like English, French and Spanish)
to a non-alphabetic language (like Chinese and
Japanese) is usually performed through transliter-
ation, which tries to preserve the pronunciation in
the source language.
For example, in Japanese, foreign words im-
ported from other languages are usually written
H a r r i n g t o n ? ? ? ? ? English-to-Japanese
T i m o t h y ??? English-to-Chinese
Source Name       Target Name          Note
ti mo   xi                     Chinese Romanized writing
ha  ri n   to  n Japanese Romanized writing
Figure 1: Transliteration examples
in a special syllabary called Katakana; in Chi-
nese, foreign words accepted to Chinese are al-
ways written by Chinese characters; examples are
given in Figure 1.
An intuitive transliteration method is to first
convert a source word into phonemes, then find the
corresponding phonemes in the target language,
and finally convert to the target language?s writ-
ing system (Knight and Graehl, 1998; Oh et al,
2006). One major limitation of this method is that
the named entities are usually OOVs with diverse
origins and this makes the grapheme-to-phoneme
conversion very difficult.
DOP is gaining more attention in the transliter-
ation research community which is also the stan-
dard evaluation of NEWS 2009.
The source channel and joint source-channel
models (Li et al, 2004) have been proposed for
DOP, which try to model P (T |S) and P (T, S) re-
spectively, where T and S denotes the words in
the target and source languages. (Ekbal et al,
2006) modified the joint source-channel model to
incorporate different context information into the
model for the Indian languages. Here we propose
a two-step CRF model for transliteration, and the
idea is to make use of the discriminative ability of
CRF. For example, in E2C transliteration, the first
step is to segment an English name into alphabet
chunks and after this step the number of Chinese
characters is decided. The second step is to per-
form a context-dependent mapping from each En-
glish chunk into one Chinese character. Figure 1
shows that this method is applicable to many other
72
transliteration tasks including E2C and E2J.
Our CRF method and the n-gram joint source-
channel model use different information in pre-
dicting the corresponding Chinese characters and
therefore in combination better results are ex-
pected. We interpolate the two models linearly
and use this as our final system for NEWS 2009.
The rest of the paper is organized as follows: Sec-
tion 2 introduces our system in detail including the
alignment and decoding modules, Section 3 ex-
plains our experiments and finally Section 4 de-
scribes conclusions and future work.
2 System Description
Our system starts from a joint source channel
alignment to train the CRF segmenter. The CRF
is used to re-segment and align the training data,
and from this alignment we create a Weighted Fi-
nite State Transducer (WFST) based n-gram joint
source-channel decoder and a CRF E2C converter.
The following subsections explain the structure of
our system shown in Figure 2.
N-gram joint source-channel Alignment
CRF segmenter
N-gram WFST decoder CRF E2C converter
Each pair in the training corpus
New Alignment
N-gram WFST decoder
CRF E2C converter
Linear combination
Each source name in the test corpus
CRF segmenter
Tr
ai
ni
ng
Te
st
in
g
Output
Figure 2: System structure
2.1 Theoretical background
2.1.1 Joint source channel model
The source channel model represents the condi-
tional probability of target names given a source
name P (T |S). The joint source channel model
calculates how the source words and target names
are generated simultaneously (Li et al, 2004):
P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)
= P (< s, t >1, < s, t >2, ..., < s, t >k)
=
K?
k=1
P (< s, t >k | < s, t >k?11 ) (1)
where, S = (s1, s2, ..., sk) and T =
(t1, t2, ..., tk).
2.1.2 CRF
A CRF (Lafferty et al, 2001) is an undirected
graphical model which assigns a probability to a
label sequence L = l1l2 . . . lT , given an input se-
quence C = c1c2 . . . cT ,
P (L|C) = 1
Z(C)
exp(
T?
t=1
?
k
?kfk(lt, lt?1, C, t))
(2)
For the kth feature, fk denotes the feature function
and ?k is the parameter which controls the weight-
ing. Z(C) is a normalization term that ensure the
distribution sums to one. CRF training is usually
performed through the L-BFGS algorithm (Wal-
lach, 2002) and decoding is performed by Viterbi
algorithm (Viterbi, 1967). In this paper, we use an
open source toolkit ?crf++?1.
2.2 N-gram joint source-channel alignment
To calculate the probability in Equation 1, the
training corpus needs to be aligned first. We use
the Expectation-Maximization(EM) algorithm to
optimize the alignment A between the source S
and target T pairs, that is:
A? = arg max
A
P (S, T,A) (3)
The procedure is summarized as follows:
1. Initialize a random alignment
2. E-step: update n-gram probability
3. M-step: apply the n-gram model to realign
each entry in corpus
4. Go to step 2 until the alignment converges
2.3 CRF alignment & segmentation
The performance of EM algorithm is often af-
fected by the initialization. Fortunately, we can
correct mis-alignments by using the discriminative
ability of the CRF. The alignment problem is con-
verted into a tagging problem that doesn?t require
the use of the target words at all. Figure 3 is an
example of a segmentation and alignment, where
the labels B and N indicate whether the character
is in the starting position of the chunk or not.
In the CRF method the feature function de-
scribes a co-occurrence relation, and it is formally
1crfpp.sourceforge.net
73
T i m o t h y ???
T/B i/N m/B o/N t/B h/N y/N
Ti/? mo/? thy/?
Figure 3: An example of the CRF segmenter for-
mat and E2C converter
defined as fk(lt, lt?1, C, t) (Eq. 2). fk is usually a
binary function, and takes the value 1 when both
observation ct and transition lt?1 ? lt are ob-
served. In our segmentation tool, we use the fol-
lowing features
? 1. Unigram features: C?2, C?1, C0, C1, C2
? 2. Bigram features:C?1C0, C0C1
Here, C0 is the current character, C?1 and C1 de-
note the previous and next characters and C?2 and
C2 are the characters two positions to the left and
right of C0.
In the alignment process, we use the CRF seg-
menter to split each English word into chunks.
Sometimes a problem occurs in which the num-
ber of chunks in the segmented output will not be
equal to the number of Chinese characters. In such
cases our solution is to choose from the n-best list
the top scoring segmentation which contains the
correct number of chunks.
In the testing process, we use the segmenter in
the similar way, but only take top-1 output seg-
mented English chunks for use in the following
CRF E2C conversion.
2.4 CRF E2C converter
Similar to the CRF segmenter, the CRF E2C con-
verter has the format shown in Figure 3. For this
CRF, we use the following features:
? 1. Unigram features: C?1, C0, C1
? 2. Bigram features:C?1C0, C0C1
where C represents the English chunks and the
subscript notation is the same as the CRF seg-
menter.
2.5 N-gram WFST decoder for joint source
channel model
Our decoding approach makes use of WFSTs to
represent the models and simplify the develop-
ment by utilizing standard operations such as com-
position and shortest path algorithms.
After the alignments are generated, the first
step is to build a corpus to train the translit-
eration WFST. Each aligned word is converted
to a sequence of transliteration alignment pairs
?s, t?1 , ?s, t?2 , ... ?s, t?k, where each s can be a
chunk of one or more characters and t is assumed
to be a single character. Each of the pairs is
treated as a word and the entire set of alignments is
used to train an n-gram language model. In these
evaluations we used the MITLM toolkit (Hsu and
Glass, 2008) to build a trigram model with modi-
fied Kneser-Ney smoothing.
We then use the procedure described in (Caseiro
et al, 2002) and convert the n-gram to a weighted
acceptor representation where each input label be-
longs to the set of transliteration alignment pairs.
Next the pairs labels are broken down into the in-
put and output parts and the acceptor is converted
to a transducer M . To allow transliteration from a
sequence of individual characters, a second WFST
T is constructed. T has a single state and for each
s a path is added to allow a mapping from the
string of individual characters.
To perform the actual transliteration, the input
word is converted to an acceptor I which has one
arc for each of the characters in the word. I is
then combined with T and M according to O =
I ?T ?M where ? denotes the composition opera-
tor. The n?best paths are extracted from O by pro-
jecting the output, removing the epsilon labels and
applying the n-shortest paths algorithm with de-
terminization from the OpenFst Toolkit(Allauzen
et al, 2007).
2.6 Linear combination
We notice that there is a significant difference be-
tween the correct answers of the n-gram WFST
and CRF decoders. The reason may be due to
the different information utilized in the two de-
coding methods. Since their performance levels
are similar, the overall performance is expected
to be improved by the combination. From the
CRF we compute the probability PCRF (T |S) and
from the list of scores output from the n-gram de-
coder we calculate the conditional probability of
Pn?gram(T |S). These are used in our combina-
tion method according to:
P (T |S) = ?PCRF (T |S)+(1??)Pn?gram(T |S)
(4)
where ? denotes the interpolation weight (0.3 in
this paper).
74
3 Experiments
We use the training and development sets of
NEWS 2009 data in our experiments as detailed
in Table 12. There are several measure metrics in
the shared task and due to limited space in this pa-
per we provide the results for top-1 accuracy.
Task Training data size Test data size
E2C 31961 2896
E2J 23808 1509
Table 1: Corpus introduction
n-gram+CRF
Task Alignment interpolation
WFST CRF
E2C 70.3 67.3 71.5
E2J 44.9 44.8 46.7
Table 2: Top-1 accuracies(%)
The results are listed in Table 2. For E2C
task the top-1 accuracy of the joint source-channel
model is 70.3% and 67.3% for the two-step CRF
model. After combining the two results together
the top-1 accuracy increases to 71.5% correspond-
ing to a 1.2% absolute improvement over the state-
of-the-art joint source-channel model. Similarly,
we get 1.8% absolute improvement for E2J task.
4 Conclusions and future work
In this paper we have presented our new hybrid
method for machine transliteration which com-
bines a new two-step CRF model with a state-of-
the-art joint source-channel model. In compari-
son to the joint source-channel model the combi-
nation approach achieved 1.2% and 1.8% absolute
improvements for E2C and E2J task respectively.
In the first step of the CRF method we only
use the top-1 segmentation, which may propagate
transliteration errors to the following step. In fu-
ture work we would like to optimize the 2-step
CRF jointly. Currently, we are also investigating
minimum classification error (MCE) discriminant
training as a method to further improve the joint
source channel model.
2For the JJ task the submitted results
are only based on the joint source
channel model. Unfortunately, we were
unable to submit a combination result
because the training time for the CRF
was too long.
Acknowledgments
The corpora used in this paper are from ?NEWS
2009 Machine Transliteration Shared Task? (Li et
al., 2004; CJK, website)
References
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration, 1998 Association for Computa-
tional Linguistics.
Li Haizhou, Zhang Min and Su Jian. 2004. A joint
source-channel model for machine transliteration,
2004 Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics.
Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration, Proceedings of the COL-
ING/ACL, pages 191-198.
Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models , Journal of Artificial Intelligence Re-
search, 27, pages 119-151.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data., Proceedings of International Confer-
ence on Machine Learning, 2001, pages 282-289.
Hanna Wallach 2002. Efficient Training of Condi-
tional Random Fields. M. Thesis, University of Ed-
inburgh, 2002.
Andrew J. Viterbi 1967. Error Bounds for Convolu-
tional Codes and an Asymptotically Optimum De-
coding Algorithm. IEEE Transactions on Informa-
tion Theory, Volume IT-13, 1967,pages 260-269.
Bo-June Hsu and James Glass 2008. Iterative Lan-
guage Model Estimation: Efficient Data Structure
& Algorithms. Proceedings Interspeech, pages 841-
844.
Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira
and Ceu Viana 2002. Grapheme-to-phone using
finite state transducers. Proceedings 2002 IEEE
Workshop on Speech Synthesis.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut and Mehryar Mohri 2002. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. Proceedings of the Ninth Interna-
tional Conference on Implementation and Applica-
tion of Automata, (CIAA 2007), pages 11-23.
http://www.cjk.org
75
Proceedings of the ACL 2010 Conference Short Papers, pages 236?240,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimizing Question Answering Accuracy by Maximizing Log-Likelihood
Matthias H. Heie, Edward W. D. Whittaker and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552, Japan
{heie,edw,furui}@furui.cs.titech.ac.jp
Abstract
In this paper we demonstrate that there
is a strong correlation between the Ques-
tion Answering (QA) accuracy and the
log-likelihood of the answer typing com-
ponent of our statistical QA model. We
exploit this observation in a clustering al-
gorithm which optimizes QA accuracy by
maximizing the log-likelihood of a set of
question-and-answer pairs. Experimental
results show that we achieve better QA ac-
curacy using the resulting clusters than by
using manually derived clusters.
1 Introduction
Question Answering (QA) distinguishes itself
from other information retrieval tasks in that the
system tries to return accurate answers to queries
posed in natural language. Factoid QA limits it-
self to questions that can usually be answered with
a few words. Typically factoid QA systems em-
ploy some form of question type analysis, so that
a question such as What is the capital of Japan?
will be answered with a geographical term. While
many QA systems use hand-crafted rules for this
task, such an approach is time-consuming and
doesn?t generalize well to other languages. Ma-
chine learning methods have been proposed, such
as question classification using support vector ma-
chines (Zhang and Lee, 2003) and language mod-
eling (Merkel and Klakow, 2007). In these ap-
proaches, question categories are predefined and a
classifier is trained on manually labeled data. This
is an example of supervised learning. In this pa-
per we present an unsupervised method, where we
attempt to cluster question-and-answer (q-a) pairs
without any predefined question categories, hence
no manually class-labeled questions are used.
We use a statistical QA framework, described in
Section 2, where the system is trained with clusters
of q-a pairs. This framework was used in several
TREC evaluations where it placed in the top 10
of participating systems (Whittaker et al, 2006).
In Section 3 we show that answer accuracy is
strongly correlated with the log-likelihood of the
q-a pairs computed by this statistical model. In
Section 4 we propose an algorithm to cluster q-a
pairs by maximizing the log-likelihood of a dis-
joint set of q-a pairs. In Section 5 we evaluate the
QA accuracy by training the QA system with the
resulting clusters.
2 QA system
In our QA framework we choose to model only
the probability of an answer A given a question Q,
and assume that the answer A depends on two sets
of features: W = W (Q) and X = X(Q):
P (A|Q) = P (A|W,X), (1)
where W represents a set of |W | features describ-
ing the question-type part of Q such as who, when,
where, which, etc., and X is a set of features
which describes the ?information-bearing? part of
Q, i.e. what the question is actually about and
what it refers to. For example, in the questions
Where is Mount Fuji? and How high is Mount
Fuji?, the question type features W differ, while
the information-bearing features X are identical.
Finding the best answer A? involves a search over
all A for the one which maximizes the probability
of the above model, i.e.:
A? = arg max
A
P (A|W,X). (2)
Given the correct probability distribution, this
will give us the optimal answer in a maximum
likelihood sense. Using Bayes? rule, assuming
uniform P (A) and that W and X are indepen-
dent of each other given A, in addition to ignoring
P (W,X) since it is independent of A, enables us
to rewrite Eq. (2) as
236
A? = arg max
A
P (A | X)
? ?? ?
retrieval
model
? P (W | A)
? ?? ?
filter
model
. (3)
2.1 Retrieval Model
The retrieval model P (A|X) is essentially a lan-
guage model which models the probability of an
answer sequence A given a set of information-
bearing features X = {x1, . . . , x|X|}. This set
is constructed by extracting single-word features
from Q that are not present in a stop-list of high-
frequency words. The implementation of the re-
trieval model used for the experiments described
in this paper, models the proximity of A to fea-
tures in X . It is not examined further here;
see (Whittaker et al, 2005) for more details.
2.2 Filter Model
The question-type feature set W = {w1, . . . , w|W |}
is constructed by extracting n-tuples (n = 1, 2, . . .)
such as where, in what and when were from the
input question Q. We limit ourselves to extracting
single-word features. The 2522 most frequent
words in a collection of example questions are
considered in-vocabulary words; all other words
are out-of-vocabulary words, and substituted with
?UNK?.
Modeling the complex relationship between
W and A directly is non-trivial. We there-
fore introduce an intermediate variable CE =
{c1, . . . , c|CE |}, representing a set of classes of
example q-a pairs. In order to construct these
classes, given a set E = {t1, . . . , t|E|} of ex-
ample q-a pairs, we define a mapping function
f : E 7? CE which maps each example q-a pair tj
for j = 1 . . . |E| into a particular class f(tj) = ce.
Thus each class ce may be defined as the union of
all component q-a features from each tj satisfy-
ing f(tj) = ce. Hence each class ce constitutes a
cluster of q-a pairs. Finally, to facilitate modeling
we say that W is conditionally independent of A
given ce so that,
P (W | A) =
|CE |?
e=1
P (W | ceW ) ? P (ceA | A), (4)
where ceW and ceA refer to the subsets of question-
type features and example answers for the class ce,
respectively.
P (W | ceW ) is implemented as trigram langu-
age models with backoff smoothing using absolute
discounting (Huang et al, 2001).
Due to data sparsity, our set of example q-a
pairs cannot be expected to cover all the possi-
ble answers to questions that may ever be asked.
We therefore employ answer class modeling rather
than answer word modeling by expanding Eq. (4)
as follows:
P (W | A) =
|CE |?
e=1
P (W | ceW )?
|KA|?
a=1
P (ceA | ka)P (ka | A),
(5)
where ka is a concrete class in the set of |KA|
answer classes KA. These classes are generated
using the Kneser-Ney clustering algorithm, com-
monly used for generating class definitions for
class language models (Kneser and Ney, 1993).
In this paper we restrict ourselves to single-
word answers; see (Whittaker et al, 2005) for the
modeling of multi-word answers. We estimate
P (ceA | kA) as
P (ceA | kA) =
f(kA, ceA)
|CE |?
g=1
f(kA, c
g
A)
, (6)
where
f(kA, ceA) =
?
?i:i?ceA
?(i ? kA)
|ceA|
, (7)
and ?(?) is a discrete indicator function which
equals 1 if its argument evaluates true and 0 if
false.
P (ka | A) is estimated as
P (ka | A) =
1
?
?j:j?Ka
?(A ? j) . (8)
3 The Relationship between Mean
Reciprocal Rank and Log-Likelihood
We use Mean Reciprocal Rank (MRR) as our
metric when evaluating the QA accuracy on a set
of questions G = {g1...g|G|}:
MRR =
?|G|
i=1 1/Ri
|G| , (9)
237
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
-1.18 -1.16 -1.14 -1.12
M
R
R
LL
? = 0.86
Figure 1: MRR vs. LL (average per q-a pair) for
100 random cluster configurations.
where Ri is the rank of the highest ranking correct
candidate answer for gi.
Given a set D = (d1...d|D|) of q-a pairs disjoint
from the q-a pairs in CE , we can, using Eq. (5),
calculate the log-likelihood as
LL =
|D|
?
d=1
logP (Wd|Ad)
=
|D|
?
d=1
log
|CE |?
e=1
P (Wd | ceW )?
|KA|?
a=1
P (ceA | ka)P (ka | Ad).
(10)
To examine the relationship between MRR and
LL, we randomly generate configurations CE ,
with a fixed cluster size of 4, and plot the result-
ing MRR and LL, computed on the same data set
D, as data points in a scatter plot, as seen in Fig-
ure 1. We find that LL and MRR are strongly
correlated, with a correlation coefficient ? = 0.86.
This observation indicates that we should be
able to improve the answer accuracy of the QA
system by optimizing the LL of the filter model
in isolation, similar to how, in automatic speech
recognition, the LL of the language model can
be optimized in isolation to improve the speech
recognition accuracy (Huang et al, 2001).
4 Clustering algorithm
Using the observation that LL is correlated with
MRR on the same data set, we expect that opti-
mizing LL on a development set (LLdev) will also
improve MRR on an evaluation set (MRReval).
Hence we propose the following greedy algorithm
to maximize LLdev:
init: c1 ? CE contains all training pairs |E|
while improvement > threshold do
best LLdev ? ??
for all j = 1...|E| do
original cluster = f(tj)
Take tj out of f(tj)
for e = ?1, 1...|CE |, |CE |+ 1 do
Put tj in ce
Calculate LLdev
if LLdev > best LLdev then
best LLdev ? LLdev
best cluster ? e
best pair ? j
end if
Take tj out of ce
end for
Put tj back in original cluster
end for
Take tbest pair out of f(tbest pair)
Put tbest pair into cbest cluster
end while
In this algorithm, c?1 indicates the set of train-
ing pairs outside the cluster configuration, thus ev-
ery training pair will not necessarily be included
in the final configuration. c|C|+1 refers to a new,
empty cluster, hence this algorithm automatically
finds the optimal number of clusters as well as the
optimal configuration of them.
5 Experiments
5.1 Experimental Setup
For our data sets, we restrict ourselves to questions
that start with who, when or where. Furthermore,
we only use q-a pairs which can be answered with
a single word. As training data we use questions
and answers from the Knowledge-Master collec-
tion1. Development/evaluation questions are the
questions from TREC QA evaluations from TREC
2002 to TREC 2006, the answers to which are to
be retrieved from the AQUAINT corpus. In total
we have 2016 q-a pairs for training and 568 ques-
tions for development/evaluation. We are able to
retrieve the correct answer for 317 of the devel-
opment/evaluation questions, thus the theoretical
upper bound for our experiments is an answer ac-
curacy of MRR = 0.558.
Accuracy is evaluated using 5-fold (rotating)
cross-validation, where in each fold the TREC
QA data is partitioned into a development set of
1http://www.greatauk.com/
238
Configuration LLeval MRReval #clusters
manual -1.18 0.262 3
all-in-one -1.32 0.183 1
one-in-each -0.87 0.263 2016
automatic -0.24 0.281 4
Table 1: LLeval (average per q-a pair) and
MRReval (over all held-out TREC years), and
number of clusters (median of the cross-evaluation
folds) for the various configurations.
4 years? data and an evaluation set of one year?s
data. For each TREC question the top 50 doc-
uments from the AQUAINT corpus are retrieved
using Lucene2. We use the QA system described
in Section 2 for QA evaluation. Our evaluation
metric is MRReval, and LLdev is our optimiza-
tion criterion, as motivated in Section 3.
Our baseline system uses manual clusters.
These clusters are obtained by putting all who q-a
pairs in one cluster, all when pairs in a second and
all where pairs in a third. We compare this baseline
with using clusters resulting from the algorithm
described in Section 4. We run this algorithm until
there are no further improvements in LLdev. Two
other cluster configurations are also investigated:
all q-a pairs in one cluster (all-in-one), and each q-
a pair in its own cluster (one-in-each). The all-in-
one configuration is equivalent to not using the fil-
ter model, i.e. answer candidates are ranked solely
by the retrieval model. The one-in-each configura-
tion was shown to perform well in the TREC 2006
QA evaluation (Whittaker et al, 2006), where it
ranked 9th among 27 participants on the factoid
QA task.
5.2 Results
In Table 1, we see that the manual clusters (base-
line) achieves an MRReval of 0.262, while the
clusters resulting from the clustering algorithm
give an MRReval of 0.281, which is a relative
improvement of 7%. This improvement is sta-
tistically significant at the 0.01 level using the
Wilcoxon signed-rank test. The one-in-each clus-
ter configuration achieves an MRReval of 0.263,
which is not a statistically significant improvement
over the baseline. The all-in-one cluster configura-
tion (i.e. no filter model) has the lowest accuracy,
with an MRReval of 0.183.
2http://lucene.apache.org/
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0  400  800  1200  1600  2000
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
LL M
R
R
# iterations
LLdevMRRdev
(a) Development set, 4 year?s TREC.
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0  400  800  1200  1600  2000
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
LL M
R
R
# iterations
LLevalMRReval
(b) Evaluation set, 1 year?s TREC.
Figure 2: MRR and LL (average per q-a pair)
vs. number of algorithm iterations for one cross-
validation fold.
6 Discussion
Manual inspection of the automatically derived
clusters showed that the algorithm had constructed
configurations where typically who, when and
where q-a pairs were put in separate clusters, as in
the manual configuration. However, in some cases
both who and where q-a pairs occurred in the same
cluster, so as to better answer questions like Who
won the World Cup?, where the answer could be a
country name.
As can be seen from Table 1, there are only 4
clusters in the automatic configuration, compared
to 2016 in the one-in-each configuration. Since
the computational complexity of the filter model
described in Section 2.2 is linear in the number of
clusters, a beneficial side effect of our clustering
procedure is a significant reduction in the compu-
tational requirement of the filter model.
In Figure 2 we plot LL and MRR for one of
the cross-validation folds over multiple iterations
(the while loop) of the clustering algorithm in Sec-
239
tion 4. It can clearly be seen that the optimization
of LLdev leads to improvement in MRReval, and
that LLeval is also well correlated with MRReval.
7 Conclusions and Future Work
In this paper we have shown that the log-likelihood
of our statistical model is strongly correlated with
answer accuracy. Using this information, we have
clustered training q-a pairs by maximizing log-
likelihood on a disjoint development set of q-a
pairs. The experiments show that with these clus-
ters we achieve better QA accuracy than using
manually clustered training q-a pairs.
In future work we will extend the types of ques-
tions that we consider, and also allow for multi-
word answers.
Acknowledgements
The authors wish to thank Dietrich Klakow for his
discussion at the concept stage of this work. The
anonymous reviewers are also thanked for their
constructive feedback.
References
[Huang et al2001] Xuedong Huang, Alex Acero and
Hsiao-Wuen Hon. 2001. Spoken Language Pro-
cessing. Prentice-Hall, Upper Saddle River, NJ,
USA.
[Kneser and Ney1993] Reinhard Kneser and Hermann
Ney. 1993. Improved Clustering Techniques for
Class-based Statistical Language Modelling. Pro-
ceedings of the European Conference on Speech
Communication and Technology (EUROSPEECH).
[Merkel and Klakow2007] Andreas Merkel and Diet-
rich Klakow. 2007. Language Model Based Query
Classification. Proceedings of the European Confer-
ence on Information Retrieval (ECIR).
[Whittaker et al2005] Edward Whittaker, Sadaoki Fu-
rui and Dietrich Klakow. 2005. A Statistical Clas-
sification Approach to Question Answering using
Web Data. Proceedings of the International Con-
ference on Cyberworlds.
[Whittaker et al2006] Edward Whittaker, Josef Novak,
Pierre Chatain and Sadaoki Furui. 2006. TREC
2006 Question Answering Experiments at Tokyo In-
stitute of Technology. Proceedings of The Fifteenth
Text REtrieval Conference (TREC).
[Zhang and Lee2003] Dell Zhang and Wee Sun Lee.
2003. Question Classification using Support Vec-
tor Machines. Proceedings of the Special Interest
Group on Information Retrieval (SIGIR).
240
Proceedings of the ACL 2010 Conference Short Papers, pages 275?280,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Jointly optimizing a two-step conditional random field model for machine
transliteration and its fast decoding algorithm
Dong Yang, Paul Dixon and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552 Japan
{raymond,dixonp,furui}@furui.cs.titech.ac.jp
Abstract
This paper presents a joint optimization
method of a two-step conditional random
field (CRF) model for machine transliter-
ation and a fast decoding algorithm for
the proposed method. Our method lies in
the category of direct orthographical map-
ping (DOM) between two languages with-
out using any intermediate phonemic map-
ping. In the two-step CRF model, the first
CRF segments an input word into chunks
and the second one converts each chunk
into one unit in the target language. In this
paper, we propose a method to jointly op-
timize the two-step CRFs and also a fast
algorithm to realize it. Our experiments
show that the proposed method outper-
forms the well-known joint source channel
model (JSCM) and our proposed fast al-
gorithm decreases the decoding time sig-
nificantly. Furthermore, combination of
the proposed method and the JSCM gives
further improvement, which outperforms
state-of-the-art results in terms of top-1 ac-
curacy.
1 Introduction
There are more than 6000 languages in the world
and 10 languages of them have more than 100 mil-
lion native speakers. With the information revolu-
tion and globalization, systems that support mul-
tiple language processing and spoken language
translation become urgent demands. The transla-
tion of named entities from alphabetic to syllabary
language is usually performed through translitera-
tion, which tries to preserve the pronunciation in
the original language.
For example, in Chinese, foreign words are
written with Chinese characters; in Japanese, for-
eign words are usually written with special char-
G o o g l e ?? ? ? English-to-Japanese
G o o g l e ?? English-to-Chinese
Source Name       Target Name          Note
gu ge Chinese Romanized writing         
guu gu ru Japanese Romanized writing
Figure 1: Transliteration examples
acters called Katakana; examples are given in Fig-
ure 1.
An intuitive transliteration method (Knight and
Graehl, 1998; Oh et al, 2006) is to firstly convert
a source word into phonemes, then find the corre-
sponding phonemes in the target language, and fi-
nally convert them to the target language?s written
system. There are two reasons why this method
does not work well: first, the named entities have
diverse origins and this makes the grapheme-to-
phoneme conversion very difficult; second, the
transliteration is usually not only determined by
the pronunciation, but also affected by how they
are written in the original language.
Direct orthographical mapping (DOM), which
performs the transliteration between two lan-
guages directly without using any intermediate
phonemic mapping, is recently gaining more at-
tention in the transliteration research community,
and it is also the ?Standard Run? of the ?NEWS
2009 Machine Transliteration Shared Task? (Li et
al., 2009). In this paper, we try to make our system
satisfy the standard evaluation condition, which
requires that the system uses the provided parallel
corpus (without pronunciation) only, and cannot
use any other bilingual or monolingual resources.
The source channel and joint source channel
models (JSCMs) (Li et al, 2004) have been pro-
posed for DOM, which try to model P (T |S) and
P (T, S) respectively, where T and S denote the
words in the target and source languages. Ekbal
et al (2006) modified the JSCM to incorporate
different context information into the model for
275
Indian languages. In the ?NEWS 2009 Machine
Transliteration Shared Task?, a new two-step CRF
model for transliteration task has been proposed
(Yang et al, 2009), in which the first step is to
segment a word in the source language into char-
acter chunks and the second step is to perform a
context-dependent mapping from each chunk into
one written unit in the target language.
In this paper, we propose to jointly optimize a
two-step CRF model. We also propose a fast de-
coding algorithm to speed up the joint search. The
rest of this paper is organized as follows: Sec-
tion 2 explains the two-step CRF method, fol-
lowed by Section 3 which describes our joint opti-
mization method and its fast decoding algorithm;
Section 4 introduces a rapid implementation of a
JSCM system in the weighted finite state trans-
ducer (WFST) framework; and the last section
reports the experimental results and conclusions.
Although our method is language independent, we
use an English-to-Chinese transliteration task in
all the explanations and experiments.
2 Two-step CRF method
2.1 CRF introduction
A chain-CRF (Lafferty et al, 2001) is an undi-
rected graphical model which assigns a probability
to a label sequence L = l1l2 . . . lT , given an input
sequence C = c1c2 . . . cT . CRF training is usually
performed through the L-BFGS algorithm (Wal-
lach, 2002) and decoding is performed by the
Viterbi algorithm. We formalize machine translit-
eration as a CRF tagging problem, as shown in
Figure 2.
T i m o t h y ???
T/B i/N m/B o/N t/B h/N y/N
Ti/? mo/? thy/?
Figure 2: An pictorial description of a CRF seg-
menter and a CRF converter
2.2 CRF segmenter
In the CRF, a feature function describes a co-
occurrence relation, and it is usually a binary func-
tion, taking the value 1 when both an observa-
tion and a label transition are observed. Yang et
al. (2009) used the following features in the seg-
mentation tool:
? Single unit features: C?2, C?1, C0, C1, C2
? Combination features: C?1C0, C0C1
Here, C0 is the current character, C?1 and C1 de-
note the previous and next characters, and C?2 and
C2 are the characters located two positions to the
left and right of C0.
One limitation of their work is that only top-1
segmentation is output to the following CRF con-
verter.
2.3 CRF converter
Similar to the CRF segmenter, the CRF converter
has the format shown in Figure 2.
For this CRF, Yang et al (2009) used the fol-
lowing features:
? Single unit features: CK?1, CK0, CK1
? Combination features: CK?1CK0,
CK0CK1
where CK represents the source language chunk,
and the subscript notation is the same as the CRF
segmenter.
3 Joint optimization and its fast decoding
algorithm
3.1 Joint optimization
We denote a word in the source language by S, a
segmentation of S by A, and a word in the target
langauge by T . Our goal is to find the best word T?
in the target language which maximizes the prob-
ability P (T |S).
Yang et al (2009) used only the best segmen-
tation in the first CRF and the best output in the
second CRF, which is equivalent to
A? = arg max
A
P (A|S)
T? = arg max
T
P (T |S, A?), (1)
where P (A|S) and P (T |S,A) represent two
CRFs respectively. This method considers the seg-
mentation and the conversion as two independent
steps. A major limitation is that, if the segmenta-
tion from the first step is wrong, the error propa-
gates to the second step, and the error is very dif-
ficult to recover.
In this paper, we propose a new method to
jointly optimize the two-step CRF, which can be
276
written as:
T? = arg max
T
P (T |S)
= arg max
T
?
A
P (T,A|S)
= arg max
T
?
A
P (A|S)P (T |S,A)
(2)
The joint optimization considers all the segmen-
tation possibilities and sums the probability over
all the alternative segmentations which generate
the same output. It considers the segmentation and
conversion in a unified framework and is robust to
segmentation errors.
3.2 N-best approximation
In the process of finding the best output using
Equation 2, a dynamic programming algorithm for
joint decoding of the segmentation and conversion
is possible, but the implementation becomes very
complicated. Another direction is to divide the de-
coding into two steps of segmentation and conver-
sion, which is this paper?s method. However, exact
inference by listing all possible candidates explic-
itly and summing over all possible segmentations
is intractable, because of the exponential computa-
tion complexity with the source word?s increasing
length.
In the segmentation step, the number of possible
segmentations is 2N , where N is the length of the
source word and 2 is the size of the tagging set. In
the conversion step, the number of possible candi-
dates is MN ? , where N ? is the number of chunks
from the 1st step and M is the size of the tagging
set. M is usually large, e.g., about 400 in Chinese
and 50 in Japanese, and it is impossible to list all
the candidates.
Our analysis shows that beyond the 10th candi-
date, almost all the probabilities of the candidates
in both steps drop below 0.01. Therefore we de-
cided to generate top-10 results for both steps to
approximate the Equation 2.
3.3 Fast decoding algorithm
As introduced in the previous subsection, in the
whole decoding process we have to perform n-best
CRF decoding in the segmentation step and 10 n-
best CRF decoding in the second CRF. Is it really
necessary to perform the second CRF for all the
segmentations? The answer is ?No? for candidates
with low probabilities. Here we propose a no-loss
fast decoding algorithm for deciding when to stop
performing the second CRF decoding.
Suppose we have a list of segmentation candi-
dates which are generated by the 1st CRF, ranked
by probabilities P (A|S) in descending order A :
A1, A2, ..., AN and we are performing the 2nd
CRF decoding starting from A1. Up to Ak,
we get a list of candidates T : T1, T2, ..., TL,
ranked by probabilities in descending order. If
we can guarantee that, even performing the 2nd
CRF decoding for all the remaining segmentations
Ak+1, Ak+2, ..., AN , the top 1 candidate does not
change, then we can stop decoding.
We can show that the following formula is the
stop condition:
Pk(T1|S) ? Pk(T2|S) > 1 ?
k
?
j=1
P (Aj |S). (3)
The meaning of this formula is that the prob-
ability of all the remaining candidates is smaller
than the probability difference between the best
and the second best candidates; on the other hand,
even if all the remaining probabilities are added to
the second best candidate, it still cannot overturn
the top candidate. The mathematical proof is pro-
vided in Appendix A.
The stop condition here has no approximation
nor pre-defined assumption, and it is a no-loss fast
decoding algorithm.
4 Rapid development of a JSCM system
The JSCM represents how the source words and
target names are generated simultaneously (Li et
al., 2004):
P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)
= P (< s, t >1, < s, t >2, ..., < s, t >k)
=
K
?
k=1
P (< s, t >k | < s, t >k?11 ) (4)
where S = (s1, s2, ..., sk) is a word in the source
langauge and T = (t1, t2, ..., tk) is a word in the
target language.
The training parallel data without alignment is
first aligned by a Viterbi version EM algorithm (Li
et al, 2004).
The decoding problem in JSCM can be written
as:
T? = arg max
T
P (S, T ). (5)
277
After the alignments are generated, we use the
MITLM toolkit (Hsu and Glass, 2008) to build a
trigram model with modified Kneser-Ney smooth-
ing. We then convert the n-gram to a WFST
M (Sproat et al, 2000; Caseiro et al, 2002). To al-
low transliteration from a sequence of characters,
a second WFST T is constructed. The input word
is converted to an acceptor I , and it is then com-
bined with T and M according to O = I ? T ?M
where ? denotes the composition operator. The
n?best paths are extracted by projecting the out-
put, removing the epsilon labels and applying the
n-shortest paths algorithm with determinization in
the OpenFst Toolkit (Allauzen et al, 2007).
5 Experiments
We use several metrics from (Li et al, 2009) to
measure the performance of our system.
1. Top-1 ACC: word accuracy of the top-1 can-
didate
2. Mean F-score: fuzziness in the top-1 candi-
date, how close the top-1 candidate is to the refer-
ence
3. MRR: mean reciprocal rank, 1/MRR tells ap-
proximately the average rank of the correct result
5.1 Comparison with the baseline and JSCM
We use the training, development and test sets of
NEWS 2009 data for English-to-Chinese in our
experiments as detailed in Table 1. This is a paral-
lel corpus without alignment.
Training data Development data Test data
31961 2896 2896
Table 1: Corpus size (number of word pairs)
We compare the proposed decoding method
with the baseline which uses only the best candi-
dates in both CRF steps, and also with the well
known JSCM. As we can see in Table 2, the pro-
posed method improves the baseline top-1 ACC
from 0.670 to 0.708, and it works as well as, or
even better than the well known JSCM in all the
three measurements.
Our experiments also show that the decoding
time can be reduced significantly via using our fast
decoding algorithm. As we have explained, with-
out fast decoding, we need 11 CRF n-best decod-
ing for each word; the number can be reduced to
3.53 (1 ?the first CRF?+2.53 ?the second CRF?)
via the fast decoding algorithm.
We should notice that the decoding time is sig-
nificantly shorter than the training time. While
testing takes minutes on a normal PC, the train-
ing of the CRF converter takes up to 13 hours on
an 8-core (8*3G Hz) server.
Measure Top-1 Mean MRR
ACC F-score
Baseline 0.670 0.869 0.750
Joint optimization 0.708 0.885 0.789
JSCM 0.706 0.882 0.789
Table 2: Comparison of the proposed decoding
method with the previous method and the JSCM
5.2 Further improvement
We tried to combine the two-step CRF model and
the JSCM. From the two-step CRF model we get
the conditional probability PCRF (T |S) and from
the JSCM we get the joint probability P (S, T ).
The conditional probability of PJSCM(T |S) can
be calculuated as follows:
PJSCM (T |S) =
P (T, S)
P (S) =
P (T, S)
?
T P (T, S)
. (6)
They are used in our combination method as:
P (T |S) = ?PCRF (T |S) + (1 ? ?)PJSCM (T |S)
(7)
where ? denotes the interpolation weight (? is set
by development data in this paper).
As we can see in Table 3, the linear combination
of two sytems further improves the top-1 ACC to
0.720, and it has outperformed the best reported
?Standard Run? (Li et al, 2009) result 0.717. (The
reported best ?Standard Run? result 0.731 used
target language phoneme information, which re-
quires a monolingual dictionary; as a result it is
not a standard run.)
Measure Top-1 Mean MRR
ACC F-score
Baseline+JSCM 0.713 0.883 0.794
Joint optimization
+ JSCM 0.720 0.888 0.797
state-of-the-art 0.717 0.890 0.785
(Li et al, 2009)
Table 3: Model combination results
6 Conclusions and future work
In this paper we have presented our new joint
optimization method for a two-step CRF model
and its fast decoding algorithm. The proposed
278
method improved the system significantly and out-
performed the JSCM. Combining the proposed
method with JSCM, the performance was further
improved.
In future work we are planning to combine our
system with multilingual systems. Also we want
to make use of acoustic information in machine
transliteration. We are currently investigating dis-
criminative training as a method to further im-
prove the JSCM. Another issue of our two-step
CRF method is that the training complexity in-
creases quadratically according to the size of the
label set, and how to reduce the training time needs
more research.
Appendix A. Proof of Equation 3
The CRF segmentation provides a list of segmen-
tations: A : A1, A2, ..., AN , with conditional
probabilities P (A1|S), P (A2|S), ..., P (AN |S).
N
?
j=1
P (Aj |S) = 1.
The CRF conversion, given a segmenta-
tion Ai, provides a list of transliteration out-
put T1, T2, ..., TM , with conditional probabilities
P (T1|S,Ai), P (T2|S,Ai), ..., P (TM |S,Ai).
In our fast decoding algorithm, we start per-
forming the CRF conversion from A1, then A2,
and then A3, etc. Up to Ak, we get a list of can-
didates T : T1, T2, ..., TL, ranked by probabili-
ties Pk(T |S) in descending order. The probability
Pk(Tl|S)(l = 1, 2, ..., L) is accumulated probabil-
ity of P (Tl|S) over A1, A2, ..., Ak , calculated by:
Pk(Tl|S) =
k
?
j=1
P (Aj |S)P (Tl|S,Aj)
If we continue performing the CRF conversion
to cover all N (N ? k) segmentations, eventually
we will get:
P (Tl|S) =
N
?
j=1
P (Aj |S)P (Tl|S,Aj)
?
k
?
j=1
P (Aj |S)P (Tl|S,Aj)
= Pk(Tl|S) (8)
If Equation 3 holds, then for ?i 6= 1,
Pk(T1|S) > Pk(T2|S) + (1 ?
k
?
j=1
P (Aj |S))
? Pk(Ti|S) + (1 ?
k
?
j=1
P (Aj |S))
= Pk(Ti|S) +
N
?
j=k+1
P (Aj |S)
? Pk(Ti|S)
+
N
?
j=k+1
P (Aj |S)P (Ti|S,Aj)
= P (Ti|S) (9)
Therefore, P (T1|S) > P (Ti|S)(i 6= 1), and T1
maximizes the probability P (T |S).
279
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut and Mehryar Mohri 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. Proceedings of the Ninth Interna-
tional Conference on Implementation and Applica-
tion of Automata, (CIAA), pages 11-23.
Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira
and Ceu Viana 2002. Grapheme-to-phone using fi-
nite state transducers. Proceedings IEEE Workshop
on Speech Synthesis.
Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration, Proceedings of the COL-
ING/ACL, pages 191-198.
Bo-June Hsu and James Glass 2008. Iterative Lan-
guage Model Estimation: Efficient Data Structure
& Algorithms. Proceedings Interspeech, pages 841-
844.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration, Association for Computational Lin-
guistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data., Proceedings of International Confer-
ence on Machine Learning, pages 282-289.
Haizhou Li, Min Zhang and Jian Su. 2004. A joint
source-channel model for machine transliteration,
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Haizhou Li, A. Kumaran, Vladimir Pervouchine and
Min Zhang 2009. Report of NEWS 2009 Ma-
chine Transliteration Shared Task, Proceedings of
the 2009 Named Entities Workshop: Shared Task on
Transliteration (NEWS 2009), pages 1-18
Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models , Journal of Artificial Intelligence Re-
search, 27, pages 119-151.
Richard Sproat 2000. Corpus-Based Methods and
Hand-Built Methods. Proceedings of International
Conference on Spoken Language Processing, pages
426-428.
Andrew J. Viterbi 1967. Error Bounds for Convolu-
tional Codes and an Asymptotically Optimum De-
coding Algorithm. IEEE Transactions on Informa-
tion Theory, Volume IT-13, pages 260-269.
Hanna Wallach 2002. Efficient Training of Condi-
tional Random Fields. M. Thesis, University of Ed-
inburgh.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura and Sadaoki Furui 2009.
Combining a Two-step Conditional Random Field
Model and a Joint Source Channel Model for Ma-
chine Transliteration, Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 72-75
280
