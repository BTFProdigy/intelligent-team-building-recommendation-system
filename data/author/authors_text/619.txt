	








 	

Proceedings of NAACL HLT 2007, pages 105?112,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Random Text Model for the Generation of  
Statistical Language Invariants 
 
Chris Biemann 
 
 
NLP Dept., University of Leipzig 
 
 
Johannisgasse 26  
04103 Leipzig, Germany 
 
 
biem@informatik.uni-leipzig.de 
 
 
 
Abstract 
A novel random text generation model is  
introduced. Unlike in previous random 
text models, that mainly aim at producing 
a Zipfian distribution of word frequencies, 
our model also takes the properties of 
neighboring co-occurrence into account 
and introduces the notion of sentences in 
random text. After pointing out the defi-
ciencies of related models, we provide a 
generation process that takes neither the 
Zipfian distribution on word frequencies 
nor the small-world structure of the 
neighboring co-occurrence graph as a 
constraint. Nevertheless, these distribu-
tions emerge in the process. The distribu-
tions obtained with the random generation 
model are compared to a sample of natu-
ral language data, showing high agree-
ment also on word length and sentence 
length. This work proposes a plausible 
model for the emergence of large-scale 
characteristics of language without as-
suming a grammar or semantics. 
1 Introduction 
G. K. Zipf (1949) discovered that if all words in a 
sample of natural language are arranged in de-
creasing order of frequency, then the relation be-
tween a word?s frequency and its rank in the list 
follows a power-law. Since then, a significant 
amount of research in the area of quantitative lin-
guistics has been devoted to the question how this 
property emerges and what kind of processes gen-
erate such Zipfian distributions. 
The relation between the frequency of a word at 
rank r and its rank is given by f(r) ? r-z, where z is 
the exponent of the power-law that corresponds to 
the slope of the curve in a log plot (cf. figure 2). 
The exponent z was assumed to be exactly 1 by 
Zipf; in natural language data, also slightly differ-
ing exponents in the range of about 0.7 to 1.2 are 
observed (cf. Zanette and Montemurro 2002). B. 
Mandelbrot (1953) provided a formula with a 
closer approximation of the frequency distributions 
in language data, noticing that Zipf?s law holds 
only for the medium range of ranks, whereas the 
curve is flatter for very frequent words and steeper 
for high ranks. He also provided a word generation 
model that produces random words of arbitrary 
average length in the following way: With a prob-
ability w, a word separator is generated at each 
step, with probability (1-w)/N, a letter from an al-
phabet of size N is generated, each letter having 
the same probability. This is sometimes called the 
?monkey at the typewriter? (Miller, 1957). The 
frequency distribution follows a power-law for 
long streams of words, yet the equiprobability of 
letters causes the plot to show a step-wise rather 
than a smooth behavior, as examined by Ferrer i 
Cancho and Sol? (2002), cf. figure 2. In the same 
study, a smooth rank distribution could be obtained 
by setting the letter probabilities according to letter 
frequencies in a natural language text. But the 
question of how these letter probabilities emerge 
remains unanswered.  
Another random text model was given by 
Simon (1955), which does not take an alphabet of 
single letters into consideration. Instead, at each 
time step, a previously unseen new word is added 
to the stream with a probability a, whereas with 
probability (1-a), the next word is chosen amongst 
the words at previous positions. As words with 
higher frequency in the already generated stream 
105
have a higher probability of being added again, this 
imposes a strong competition among different 
words, resulting in a frequency distribution that 
follows a power-law with exponent z=(1-a). This 
was taken up by Zanette and Montemurro (2002), 
who slightly modify Simon?s model. They intro-
duce sublinear vocabulary growth by additionally 
making the new word probability dependent on the 
time step. Furthermore, they introduce a threshold 
on the maximal probability a previously seen word 
can be assigned to for generation, being able to 
modify the exponent z as well as to model the flat-
ter curve for high frequency words. In (Ha et al, 
2002), Zipf?s law is extended to words and 
phrases, showing its validity for syllable-class 
based languages when conducting the extension. 
Neither the Mandelbrot nor the Simon genera-
tion model take the sequence of words into ac-
count. Simon treats the previously generated 
stream as a bag of words, and Mandelbrot does not 
consider the previous stream at all. This is cer-
tainly an over-simplification, as natural language 
exhibits structural properties within sentences and 
texts that are not grasped by bags of words. 
The work by Kanter and Kessler (1995) is, to 
our knowledge, the only study to date that takes the 
word order into account when generating random 
text. They show that a 2-parameter Markov process 
gives rise to a stationary distribution that exhibits 
the word frequency distribution and the letter fre-
quency distribution characteristics of natural lan-
guage. However, the Markov process is initialized 
such that any state has exactly two successor 
states, which means that after each word, only two 
other following words are possible. This certainly 
does not reflect natural language properties, where 
in fact successor frequencies of words follow a 
power-law and more successors can be observed 
for more frequent words. But even when allowing 
a more realistic number of successor states, the 
transition probabilities of a Markov model need to 
be initialized a priori in a sensible way. Further, 
the fixed number of states does not allow for infi-
nite vocabulary. 
In the next section we provide a model that 
does not suffer from all these limitations. 
2 The random text generation model 
When constructing a random text generation 
model, we proceed according to the following 
guidelines (cf. Kumar et al 1999 for web graph 
generation): 
? simplicity: a generation model should reach 
its goal using the simplest mechanisms pos-
sible but results should still comply to char-
acteristics of real language 
? plausibility: Without claiming that our 
model is an exhaustive description of what 
makes human brains generate and evolve 
language, there should be at least a possibil-
ity that similar mechanisms could operate in 
human brains. For a discussion on the sensi-
tivity of people to bigram statistics, see e.g. 
(Thompson and Newport, 2007). 
? emergence: Rather than constraining the 
model with the characteristics we would like 
to see in the generated stream, these features 
should emerge in the process. 
Our model is basically composed of two parts 
that will be described separately: A word generator 
that produces random words composed of letters 
and a sentence generator that composes random 
sentences of words. Both parts use an internal 
graph structure, where traces of previously gener-
ated words and sentences are memorized. The 
model is inspired by small-world network genera-
tion processes, cf. (Watts and Strogatz 1998, 
Barab?si and Albert 1999, Kumar et al 1999, 
Steyvers and Tenenbaum 2005). A key notion is 
the strategy of following beaten tracks: Letters, 
words and sequences of words that have been gen-
erated before are more likely to be generated again 
in the future - a strategy that is only fulfilled for 
words in Simon?s model.  
But before laying out the generators in detail, 
we introduce ways of testing agreement of our ran-
dom text model with natural language text. 
2.1 Testing properties of word streams 
All previous approaches aimed at reproducing a 
Zipfian distribution on word frequency, which is a 
criterion that we certainly have to fulfill. But there 
are more characteristics that should be obeyed to 
make a random text more similar to natural lan-
guage than previous models: 
? Lexical spectrum: The smoothness or step-
wise shape of the rank-frequency distribu-
tion affects the lexical spectrum, which is 
the probability distribution on word fre-
106
quency. In natural language texts, this distri-
bution follows a power-law with an expo-
nent close to 2 (cf. Ferrer i Cancho and Sol?, 
2002).  
? Distribution of word length: According to 
(Sigurd et al, 2004), the distribution of word 
frequencies by length follows a variant of 
the gamma distribution 
? Distribution of sentence length: The random 
text?s sentence length distribution should re-
semble natural language. In (Sigurd et al, 
2004), the same variant of the gamma distri-
bution as for word length is fit to sentence 
length. 
? Significant neighbor-based co-occurrence: 
As discussed in (Dunning 1993), it is possi-
ble to measure the amount of surprise to see 
two neighboring words in a corpus at a cer-
tain frequency under the assumption of in-
dependence. At random generation without 
word order awareness, the number of such 
pairs that are significantly co-occurring in 
neighboring positions should be very low. 
We aim at reproducing the number of sig-
nificant pairs in natural language as well as 
the graph structure of the neighbor-based co-
occurrence graph. 
The last characteristic refers to the distribution 
of words in sequence. Important is the notion of 
significance, which serves as a means to distin-
guish random sequences from motivated ones. We 
use the log-likelihood ratio for determining signifi-
cance as in (Dunning, 1993), but other measures 
are possible as well. Note that the model of Kanter 
and Kessler (1995) produces a maximal degree of 
2 in the neighbor-based co-occurrence graph. 
As written language is rather an artifact of the 
most recent millennia then a realistic sample of 
everyday language, we use the beginning of the 
spoken language section of the British National 
Corpus (BNC) to test our model against. For sim-
plicity, all letters are capitalized and special char-
acters are removed, such that merely the 26 letters 
of the English alphabet are contained in the sam-
ple. Being aware that a letter transcription is in 
itself an artifact of written language, we chose this 
as a good-enough approximation, although operat-
ing on phonemes instead of letters would be pref-
erable. The sample contains 1 million words in 
125,395 sentences with an average length of 7.975 
words, which are composed of 3.502 letters in av-
erage. 
2.2 Basic notions of graph theory 
As we use graphs for the representation of memory 
in both parts of the model, some basic notions of 
graph theory are introduced. A graph G(V,E) 
consists of a set of vertices V and a set of 
weighted, directed edges between two vertices 
E?V?V?R with R real numbers. The first vertex 
of an edge is called startpoint, the second vertex is 
called endpoint. A function weight: V?V?R 
returns the weight of edges. The indegree 
(outdegree) of a vertex v is defined as the number 
of edges with v as startpoint (endpoint). The 
degree of a vertex is equal to its indegree and 
outdegree if the graph is undirected, i.e. (u,v,w)?E 
implies (v,u,w)?E. The neighborhood neigh(v) of 
a vertex v is defined as the set of vertices s?S 
where (v,s,weight(v,s))?E. 
The clustering coefficient is the probability that 
two neighbors X and Y of a given vertex Z are 
themselves neighbors, which is measured for 
undirected graphs (Watts and Strogatz, 1998). The 
amount of existing edges amongst the vertices in 
the neighborhood of a vertex v is divided by the 
number of possible edges. The average over all 
vertices is defined as the clustering coefficient C.  
The small-world property holds if the average 
shortest path length between pairs of vertices is 
comparable to a random graph (Erd?s and R?nyi, 
1959), but its clustering coefficient is much higher. 
A graph is called scale-free (cf. Barab?si and 
Albert, 1999), if the degree distribution of vertices 
follows a power-law. 
2.3 Word Generator 
The word generator emits sequences of letters, 
which are generated randomly in the following 
way: The word generator starts with a graph of all 
N letters it is allowed to choose from. Initially, all 
vertices are connected to themselves with weight 1. 
When generating a word, the generator chooses a 
letter x according to its probability P(x), which is 
computed as the normalized weight sum of 
outgoing edges: 
107
?
?
=
Vv
vweightsum
xweightsum
xP )(
)()(   
 .),()(
)(
?
?
=
yneighu
uyweightyweightsum  
After the generation of the first letter, the word 
generator proceeds with the next position. At every 
position, the word ends with a probability w?(0,1) 
or generates a next letter according to the letter 
production probability as given above. For every 
letter bigram, the weight of the directed edge 
between the preceding and current letter in the 
letter graph is increased by one. This results in 
self-reinforcement of letter probabilities: the more 
often a letter is generated, the higher its weight 
sum will be in subsequent steps, leading to an 
increased generation probability. Figure 1 shows 
how a word generator with three letters A,B,C 
changes its weights during the generation of the 
words AA, BCB and ABC. 
    
Figure 1: Letter graph of the word generator. Left: 
initial state. Right.: State after generating AA, 
BCB and ABC. The numbers next to edges are 
edge weights. The probability for the letters for the 
next step are P(A)=0.4, P(B)=0.4 and P(C)=0.2. 
 
The word end probability w directly influences 
the average word length, which is given by 
1+(1/w). For random number generation, we use 
the Mersenne Twister (Masumoto and Nishimura, 
1998). 
The word generator itself does produce a 
smooth Zipfian distribution on word frequencies 
and a lexical spectrum following a power-law. 
Figure 2 shows frequency distribution and lexical 
spectrum of 1 million words as generated by the 
word generator with w=0.2 on 26 letters in 
comparison to a Mandelbrot generator with the 
same parameters. The reader might note that a 
similar behaviour could be reached by just setting 
the probability of generating a letter according to 
its relative frequency in previously generated 
words. The graph seems an unnecessary 
complication for that reason. But retaining the 
letter graph with directed edges gives rise to model 
the sequence of letters for a more plausible 
morphological production in future extensions of 
this model, probably in a similar way than in the 
sentence generator as described in the following 
section.  
As depicted in figure 2, the word generator 
fulfills the requirements on Zipf?s law and the 
lexical spectrum, yielding a Zipfian exponent of 
around 1 and a power-law exponent of 2 for a large 
regime in the lexical spectrum, both matching the 
values as observed previously in natural language 
in e.g. (Zipf, 1949) and (Ferrer i Cancho and Sol?, 
2002). In contrast to this, the Mandelbrot model 
shows to have a step-wise rank-frequency 
distribution and a distorted lexical spectrum. 
Hence, the word generator itself is already an 
improvement over previous models as it produces 
a smooth Zipfian distribution and a lexical 
spectrum following a power-law. But to comply to 
the other requirements as given in section 2.1, the 
process has to be extended by a sentence generator.  
           
 1
 10
 100
 1000
 10000
 1  10  100  1000  10000
fre
qu
e
n
cy
rank
rank-frequency
word generator w=0.2
power law z=1
Mandelbrot model
 
 1e-006
 1e-005
 0.0001
 0.001
 0.01
 0.1
 1
 1  10  100  1000
P(
fre
qu
en
cy
)
frequency
lexical spectrum
word generator w=0.2
power law z=2
Mandelbrot model
 
Figure 2: rank-frequency distribution and lexical 
spectrum for the word generator in comparison to 
the Mandelbrot model 
initial state state after 3 words 
108
2.4 Sentence Generator 
The sentence generator model retains another di-
rected graph, which memorizes words and their 
sequences. Here, vertices correspond to words and 
edge weights correspond to the number of times 
two words were generated in a sequence. The word 
graph is initialized with a begin-of-sentence (BOS) 
and an end-of-sentence (EOS) symbol, with an 
edge of weight 1 from BOS to EOS. When gener-
ating a sentence, a random walk on the directed 
edges starts at the BOS vertex. With a new word 
probability (1-s), an existing edge is followed from 
the current vertex to the next vertex according to 
its weight: the probability of choosing endpoint X 
from the endpoints of all outgoing edges from the 
current vertex C is given by  
?
?
==
)(
),(
),()(
CneighN
NCweight
XCweightXwordP . 
Otherwise, with probability s?(0,1), a new 
word is generated by the word generator model, 
and a next word is chosen from the word graph in 
proportion to its weighted indegree: the probability 
of choosing an existing vertex E as successor of a 
newly generated word N is given by 
.),()(
,)(
)()(
?
?
?
?
=
==
Vv
Vv
XvweightXindgw
vindgw
EindgwEwordP
 
For each sequence of two words generated, the 
weight of the directed edge between them is in-
creased by 1. Figure 3 shows the word graph for 
generating in sequence: (empty sentence), AA, AA 
BC, AA, (empty sentence), AA CA BC AA, AA 
CA CA BC. 
 
During the generation process, the word graph 
grows and contains the full vocabulary used so far 
for generating in every time step. It is guaranteed 
that a random walk starting from BOS will finally 
reach the EOS vertex. It can be expected that sen-
tence length will slowly increase during the course 
of generation as the word graph grows and the ran-
dom walk has more possibilities before finally ar-
riving at the EOS vertex. The sentence length is 
influenced by both parameters of the model: the 
word end probability w in the word generator and 
the new word probability s in the sentence genera-
tor. By feeding the word transitions back into the 
generating model, a reinforcement of previously 
generated sequences is reached. Figure 4 illustrates 
the sentence length growth for various parameter 
settings of w and s.  
 
Figure 3: the word graph of the sentence generator 
model. Note that in the last step, the second CA 
was generated as a new word from the word gen-
erator. The generation of empty sentences happens 
frequently. These are omitted in the output. 
 1
 10
 100
 10000  100000  1e+006
a
vg
.
 
se
n
te
n
ce
 
le
n
gt
h
text interval
sentence length growth
w=0.4 s=0.08
w=0.4 s=0.1
w=0.17 s=0.22
w=0.3 s=0.09
x^(0.25);
 
Figure 4: sentence length growth, plotted in aver-
age sentence length per intervals of 10,000 sen-
tences. The straight line in the log plot indicates a 
polynomial growth. 
 
It should be noted that the sentence generator 
produces a very diverse sequence of sentences 
which does not deteriorate in repeating the same 
sentence all over again in later stages. Both word 
and sentence generator can be viewed as weighted 
finite automata (cf. Allauzen et al, 2003) with self-
training.  
109
After having defined our random text genera-
tion model, the next section is devoted to testing it 
according to the criteria given in section 2.1. 
3 Experimental results 
To measure agreement with our BNC sample, we 
generated random text with the sentence generator 
using w=0.4 and N=26 to match the English aver-
age word length and setting s to 0.08 for reaching a 
comparable sentence length. The first 50,000 sen-
tences were skipped to reach a relatively stable 
sentence length throughout the sample. To make 
the samples comparable, we used 1 million words 
totaling 125,345 sentences with an average sen-
tence length of 7.977.   
3.1 Word frequency 
The comparison between English and the sentence 
generator w.r.t the rank-frequency distribution is 
depicted in figure 5.  
Both curves follow a power-law with z close to 
1.5, in both cases the curve is flatter for high fre-
quency words as observed by Mandelbrot (1953). 
This effect could not be observed to this extent for 
the word generator alone (cf. figure 2).  
 1
 10
 100
 1000
 10000
 1  10  100  1000  10000
fre
qu
en
cy
rank
rank-frequency
sentence generator
English
power law z=1.5
 
Figure 5: rank-frequency plot for English and the 
sentence generator 
3.2 Word length 
While the word length in letters is the same in both 
samples, the sentence generator produced more 
words of length 1, more words of length>10 and 
less words of medium length. The deviation in sin-
gle letter words can be attributed to the writing 
system being a transcription of phonemes and few 
phonemes being expressed with only one letter. 
However, the slight quantitative differences do not 
oppose the similar distribution of word lengths in 
both samples, which is reflected in a curve of simi-
lar shape in figure 6 and fits well the gamma dis-
tribution variant of (Sigurd et al, 2004). 
 1
 10
 100
 1000
 10000
 100000
 1  10
fre
qu
e
n
cy
length in letters
word length
sentence generator
English
gamma distribution
 
Figure 6: Comparison of word length distributions. 
The dotted line is the function as introduced in 
(Sigurd et al, 2004) and given by f(x) ?x1.5?0.45x. 
3.3 Sentence length 
The comparison of sentence length distribution 
shows again a high capability of the sentence gen-
erator to model the distribution of the English 
sample. As can be seen in figure 7, the sentence 
generator produces less sentences of length>25 but 
does not show much differences otherwise. In the 
English sample, there are surprisingly many two-
word sentences. 
 1
 10
 100
 1000
 10000
 1  10  100
n
u
m
be
r 
o
f s
en
te
n
ce
s
length in words
sentence length
sentence generator
English
 
Figure 7: Comparison of sentence length distribu-
tion. 
3.4 Neighbor-based co-occurrence 
In this section, the structure of the significant 
neighbor-based co-occurrence graphs is examined. 
110
The significant neighbor-based co-occurrence 
graph contains all words as vertices that have at 
least one co-occurrence to another word exceeding 
a certain significance threshold. The edges are un-
directed and weighted by significance. Ferrer i 
Cancho and Sol? (2001) showed that the neighbor-
based co-occurrence graph of the BNC is scale-
free and the small-world property holds.  
For comparing the sentence generator sample to 
the English sample, we compute log-likelihood 
statistics (Dunning, 1993) on neighboring words 
that at least co-occur twice. The significance 
threshold was set to 3.84, corresponding to 5% 
error probability when rejecting the hypothesis of 
mutual independence. For both graphs, we give the 
number of vertices, the average shortest path 
length, the average degree, the clustering coeffi-
cient and the degree distribution in figure 8. Fur-
ther, the characteristics of a comparable random 
graph as defined by (Erd?s and R?nyi, 1959) are 
shown. 
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 10000
 1  10  100  1000
n
r 
o
f v
e
rti
ce
s
degree interval
degree distribution
sentence generator
English
word generator
power law z=2
 
 English 
sample 
sentence 
gen. 
word 
gen. 
random 
graph 
# of ver. 7154 15258 3498 10000 
avg. sht. 
path 
2.933 3.147 3.601 4.964 
avg. 
deg. 
9.445 6.307 3.069 7 
cl.coeff. 0.2724 0.1497 0.0719 6.89E-4 
z 1.966 2.036 2.007 - 
Figure 8: Characteristics of the neighbor-based co-
occurrence graphs of English and the generated 
sample. 
 
From the comparison with the random graph it 
is clear that both neighbor-based graphs exhibit the 
small-world property as their clustering coefficient 
is much higher than in the random graph while the 
average shortest path lengths are comparable. In 
quantity, the graph obtained from the generated 
sample has about twice as many vertices but its 
clustering coefficient is about half as high as in the 
English sample. This complies to the steeper rank-
frequency distribution of the English sample (see 
fig. 5), which is, however, much steeper than the 
average exponent found in natural language. The 
degree distributions clearly match with a power-
law exponent of 2, which does not confirm the two 
regimes of different slopes as in (Ferrer i Cancho 
and Sol? 2001). The word generator data produced 
an number of significant co-occurrences that lies in 
the range of what can be expected from the 5% 
error of the statistical test. The degree distribution 
plot appears shifted downwards about one decade, 
clearly not matching the distribution of words in 
sequence of natural language. 
 Considering the analysis of the significant 
neighbor-based co-occurrence graph, the claim is 
supported that the sentence generator model repro-
duces the characteristics of word sequences in 
natural language on the basis of bigrams. 
4 Conclusion 
In this work we introduced a random text genera-
tion model that fits well with natural language with 
respect to frequency distribution, word length, sen-
tence length and neighboring co-occurrence. The 
model was not constrained by any a priori distribu-
tion ? the characteristics emerged from a 2-level 
process involving one parameter for the word gen-
erator and one parameter for the sentence genera-
tor. This is, to our knowledge, the first random text 
generator that models sentence boundaries beyond 
inserting a special blank character at random: 
rather, sentences are modeled as a path between 
sentence beginning and sentence end which im-
poses restrictions on the words possible at sentence 
beginnings and endings. Considering its simplicity, 
we have therefore proposed a plausible model for 
the emergence of large-scale characteristics of lan-
guage without assuming a grammar or semantics. 
After all, our model produces gibberish ? but gib-
berish that is well distributed. 
The studies of Miller (1957) rendered Zipf?s 
law un-interesting for linguistics, as it is a mere 
artifact of language rather than playing an impor-
111
tant role in its production, as it emerges when put-
ting a monkey in front of a typewriter. Our model 
does not only explain Zipf?s law, but many other 
characteristics of language, which are obtained 
with a monkey that follows beaten tracks. These 
additional characteristics can be thought of as arti-
facts as well, but we strongly believe that the study 
of random text models can provide insights in the 
process that lead to the origin and the evolution of 
human languages. 
For further work, an obvious step is to improve 
the word generator so that it produces morphologi-
cally more plausible sequences of letters and to 
intertwine both generators for the emergence of 
word categories. Furthermore, it is desirable to 
embed the random generator in models of commu-
nication where speakers parameterize language 
generation of hearers and to examine, which struc-
tures are evolutionary stable (see J?ger, 2003). 
This would shed light on the interactions between 
different levels of human communication. 
Acknowledgements 
The author would like to thank Colin Bannard,  
Reinhard Rapp and the anonymous reviewers for 
useful comments. 
References 
C. Allauzen, M. Mohri, and B. Roark. 2003. General-
ized algorithms for constructing language models. In 
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 40?47 
A.-L. Barab?si and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286:509-512 
T. Dunning. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational Linguis-
tics, 19(1), pp. 61-74 
P. Erd?s and A. R?nyi. 1959. On Random Graphs I. 
Publicationes Mathematicae (Debrecen) 
R. Ferrer i Cancho and R. V. Sol?. 2001. The small-
world of human language. Proceedings of the Royal 
Society of London B 268 pp. 2261-2266 
R. Ferrer i Cancho and R. V. Sol?. 2002. Zipf?s law and 
random texts. Advances in Complex Systems, Vol.5 
No. 1 pp. 1-6  
L. Q. Ha, E. Sicilia-Garcia, J. Ming and F.J. Smith. 
2002. Extension of Zipf's law to words and phrases. 
Proceedings of 19th International Conference on 
Computational Linguistics (COLING-2002), pp. 315-
320. 
G. J?ger. 2003. Evolutionary Game Theory and Linguis-
tic Typology: A Case Study. Proceedings of the 14th 
Amsterdam Colloquium, ILLC, University of Am-
sterdam, 2003. 
I. Kanter and D. A. Kessler. 1995. Markov Processes: 
Linguistics and Zipf?s law. Physical review letters, 
74:22 
S. R. Kumar, P. Raghavan, S. Rajagopalan and A. Tom-
kins. 1999. Extracting Large-Scale Knowledge Bases 
from the Web. The VLDB Journal, pp. 639-650 
B. B. Mandelbrot. 1953. An information theory of the 
statistical structure of language. In Proceedings of 
the Symposium on Applications of Communications 
Theory, London 
M. Matsumoto and T. Nishimura. 1998. Mersenne 
Twister: A 623-dimensionally equidistributed uni-
form pseudorandom number generator. ACM Trans. 
on Modeling and Computer Simulation, Vol. 8, No. 
1, pp.3-30 
G. A. Miller. 1957. Some effects of intermittent silence,. 
American Journal of Psychology, 70, pp. 311-314 
H. A. Simon. 1955. On a class of skew distribution 
functions. Biometrika, 42, pp. 425-440 
B. Sigurd, M. Eeg-Olofsson and J. van de Weijer. 2004. 
word length, sentence length and frequency ? Zipf 
revisited. Studia Linguistica, 58(1), pp. 37-52 
M. Steyvers and J. B. Tenenbaum. 2005. The large-
scale structure of semantic networks: statistical 
analyses and a model of semantic growth. Cognitive 
Science, 29(1) 
S. P. Thompson and E. L. Newport. 2007. Statistical 
learning of syntax: The role of transitional probabil-
ity.  Language Learning and Development,  3, pp. 1-
42. 
D. J. Watts and S. H. Strogatz. 1998. Collective dynam-
ics of small-world networks. Nature, 393 pp. 440-
442 
D. H. Zanette and M. A. Montemurro. 2002. Dynamics 
of text generation with realistic Zipf distribution. 
arXiv:cond-mat/0212496  
G. K. Zipf. 1949. Human Behavior and the Principle of 
least Effort. Cambridge, MA: Addison Wesley 
112
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 37?40,
Rochester, April 2007. c?2007 Association for Computational Linguistics
Unsupervised Natural Language Processing using Graph Models 
 
Chris Biemann 
NLP Dept., University of Leipzig 
Johannisgasse 26  
04103 Leipzig, Germany 
biem@informatik.uni-leipzig.de 
  
Abstract 
In the past, NLP has always been based 
on the explicit or implicit use of linguistic 
knowledge. In classical computer linguis-
tic applications explicit rule based ap-
proaches prevail, while machine learning 
algorithms use implicit knowledge for 
generating linguistic knowledge. The 
question behind this work is: how far can 
we go in NLP without assuming explicit 
or implicit linguistic knowledge? How 
much efforts in annotation and resource 
building are needed for what level of so-
phistication in text processing? This work 
tries to answer the question by experi-
menting with algorithms that do not pre-
sume any linguistic knowledge in the 
system. The claim is that the knowledge 
needed can largely be acquired by know-
ledge-free and unsupervised methods. 
Here, graph models are employed for rep-
resenting language data. A new graph 
clustering method finds related lexical 
units, which form word sets on various 
levels of homogeneity. This is exempli-
fied and evaluated on language separation 
and unsupervised part-of-speech tagging, 
further applications are discussed. 
1 Introduction 
1.1 Unsupervised and Knowledge-Free 
A frequent remark on work dealing with unsuper-
vised methods in NLP is the question: ?Why not 
take linguistic knowledge into account?? While for 
English, annotated corpora, classification exam-
ples, sets of rules and lexical semantic word nets of 
high coverage do exist, this does not reflect the 
situation for most of even the major world lan-
guages. Further, as e.g. Lin (1997) notes, hand-
made and generic resources often do not fit the 
application domain, whereas resources created 
from and for the target data will not suffer from 
these discrepancies.  
Shifting the workload from creating resources 
manually to developing generic methods, a one-
size-fits-all solution needing only minimal adapta-
tion to new domains and other languages comes 
into reach. 
1.2 Graph Models 
The interest in incorporating graph models into 
NLP arose quite recently, and there is still a high 
potential exploiting this combination (cf. Wid-
dows, 2005). An important parallelism between 
human language and network models is the small 
world structure of lexical networks both built 
manually and automatically (Steyvers and 
Tenenbaum, 2005), providing explanation for 
power-law distributions like Zipf?s law and others, 
see Biemann (2007). For many problems in NLP, a 
graph representation is an intuitive, natural and 
direct way to represent the data.  
The pure vector space model (cf. Sch?tze, 
1993) is not suited to highly skewed distributions 
omni-present in natural language. Computationally 
expensive, sometimes lossy transformations have 
to be applied for effectiveness and efficiency in 
processing. Graph models are a veritable alterna-
tive, as the equivalent of zero-entries in the vector 
representation are neither represented nor have to 
37
be processed, rendering dimensionality reduction 
techniques unnecessary while still retaining the 
exact information. 
1.3 Roadmap 
For the entirety of this research, nothing more is 
required as input data than plain, tokenized text, 
separated into sentences. This is surely quite a bit 
of knowledge that is provided to the system, but 
unsupervised word boundary and sentence bound-
ary detection is left for future work. Three steps are 
undertaken to identify similar words on different 
levels of homogeneity: same language, same part-
of-speech, or same distributional properties. Figure 
1 shows a coarse overview of the processing steps 
discussed in this work. 
 
Figure 1: Coarse overview: From multilingual in-
put to typed relations and instances 
2 Methods in Unsupervised Processing 
Having at hand neither explicit nor implicit knowl-
edge, but in turn the goal of identifying structure of 
equivalent function, the only possibility that is left 
in unsupervised and knowledge-free processing is 
statistics and clustering.  
2.1 Co-occurrence Statistics 
As a building block, co-occurrence statistics are 
used in several components of the system de-
scribed here. A significance measure for co-
occurrence is a means to distinguish between ob-
servations that are there by chance and effects that 
take place due to an underlying structure. 
Throughout, the likelihood ratio (Dunning, 1993) 
is used as significance measure because of its sta-
ble performance in various evaluations, yet many 
more measures are possible. Dependent on the con-
text range in co-occurrence calculation, they will 
be called sentence-based or neighbor-based co-
occurrences in the remainder of this paper. The 
entirety of all co-occurrences of a corpus is called 
its co-occurrence graph. Edges are weighted by co-
occurrence significance; often a threshold on edge 
weight is applied. 
2.2 Graph Clustering 
For clustering graphs, a plethora of algorithms ex-
ist that are motivated from a graph-theoretic view-
point, but often optimize NP-complete measures 
(cf. ??ma and Schaeffer, 2005), making them non-
applicable to lexical data that is naturally repre-
sented in graphs with millions of vertices. In Bie-
mann and Teresniak (2005) and more detailed in 
Biemann (2006a), the Chinese Whispers (CW) 
Graph Clustering algorithm is described, which is a 
randomized algorithm with edge-linear run-time. 
The core idea is that vertices retain class labels 
which are inherited along the edges: In an update 
step, a vertex gets assigned the predominant label 
in its neighborhood. For initialization, all vertices 
get different labels, and after a handful of update 
steps per vertex, almost no changes in the labeling 
are observed ? especially small world graphs con-
verge fast. CW can be viewed as a more efficient 
modification and simplification of Markov Chain 
Clustering (van Dongen, 2000), which requires full 
matrix multiplications. 
CW is parameter-free, non-deterministic and 
finds the number of clusters automatically ? a fea-
ture that is welcome in NLP, where the number of 
desired clusters (e.g. in word sense induction) is 
often unknown. 
3 Results  
3.1 Language Separation 
Clustering the sentence-based co-occurrence graph 
of a multilingual corpus with CW, a language 
separator with almost perfect performance is im-
plemented in the following way: The clusters rep-
resent languages; a sentence gets assigned the label 
of the cluster with the highest lexical overlap be-
tween sentence and cluster. The method is evalu-
ated in (Biemann and Teresniak, 2005) by sorting 
monolingual material that has been artificially 
mixed together. Dependent on similarities of lan-
guages, the method works almost error-free from 
about 100-1,000 sentences per language on. For 
38
languages with different encoding, it is possible to 
un-mix corpora of size factors up to 10,000 for the 
monolingual parts.  
In a nutshell, comparable scores to supervised 
language identifiers are reached without training. 
Notice that the number of languages in a multilin-
gual chunk of text is unknown. This prohibits any 
clustering method that needs the number of clus-
ters to be specified be-forehand. 
3.2 Unsupervised POS Tagging 
Unlike in standard POS tagging, there is neither a 
set of predefined categories, nor annotation in a 
text. As POS tagging is not a system for its own 
sake, but serves as a preprocessing step for systems 
building upon it, the names and the number of 
categories are very often not important.  
The system presented in Biemann (2006b) uses 
CW clustering on graphs constructed by distribu-
tional similarity to induce a lexicon of supposedly 
non-ambiguous words w.r.t. POS by selecting only 
safe bets and excluding questionable cases from 
the lexicon. In this implementation, two clusterings 
are combined, one for high and medium frequency 
words, the other collecting medium and low fre-
quency words. High and medium frequency words 
are clustered by similarity of their stop word con-
text feature vectors: a graph is built, including only 
words that are involved in highly similar pairs. 
Clustering this graph of typically 5,000 vertices 
results in several hundred clusters, which are fur-
ther used as POS categories. To extend the lexicon, 
words of medium and low frequency are clustered 
using a graph that encodes similarity of neighbor-
based co-occurrences. Both clusterings are mapped 
by overlapping elements into a lexicon that pro-
vides POS information for some 50,000 words. For 
obtaining a clustering on datasets of this size, an 
effective algorithm like CW is crucial. Using this 
lexicon, a trigram tagger with a morphological ex-
tension is trained, which assigns a tag to every to-
ken in the corpus. 
The tagsets obtained with this method are usu-
ally more fine-grained than standard tagsets and 
reflect syntactic as well as semantic similarity. 
Figure 2 demonstrates the domain-dependence on 
the tagset for MEDLINE: distinguishing e.g. ill-
nesses and error probabilities already in the tagset 
might be a valuable feature for relation extraction 
tasks. 
Size Sample words 
1613 colds, apnea, aspergilloma, ACS, 
breathlessness, lesions, perforations, ... 
1383 proven, supplied, engineered, distin-
guished, constrained, omitted, ? 
589 dually, circumferentially, chronically, 
rarely, spectrally, satisfactorily, ... 
124 1-min, two-week, 4-min, 2-day, ? 
6 P<0.001, P<0.01, p<0.001, p<0.01, ... 
Figure 2: Some examples for MEDLINE tagset: 
Number of lex. entries per tag and sample words.  
 
In Biemann (2006b), the tagger output was di-
rectly compared to supervised taggers for English, 
German and Finnish via information-theoretic 
measures. While it is possible to compare the con-
tribution of different components of a system rela-
tively along this scale, it only gives a poor 
impression on the utility of the unsupervised tag-
ger?s output. Therefore, the tagger was evaluated 
indirectly in machine learning tasks, where POS 
tags are used as features. Biemann et al (2007) 
report that for standard Named Entity Recognition, 
Word Sense Disambiguation and Chunking tasks, 
using unsupervised POS tags as features helps 
about as much as supervised tagging: Overall, al-
most no significant differences between results 
could be observed, supporting the initial claim. 
3.3 Word Sense Induction (WSI) 
Co-occurrences are a widely used data source for 
WSI. The methodology of Dorow and Widdows 
(2003) was adopted: for the focus word, obtain its 
graph neighborhood (all vertices that are connected 
via edges to the focus word vertex and edges be-
tween these). Clustering this graph with CW and 
regarding clusters as senses, this method yields 
comparable results to Bordag (2006), tested using 
the unsupervised evaluation framework presented 
there. More detailed results are reported in Bie-
mann (2006a). 
4 Further Work 
4.1 Word Sense Disambiguation (WSD) 
The encouraging results in WSI enable support in 
automatic WSD systems. As described by Agirre et 
al. (2006), better performance can be expected if 
the WSI component distinguishes between a large 
number of so-called micro-senses. This illustrates a 
39
principle of unsupervised NLP: It is not important 
to reproduce word senses found by introspection; 
rather, it is important that different usages of a 
word can be reliably distinguished, even if the cor-
responding WordNet sense is split into several sub-
senses. 
4.2 Distributional Thesaurus with Relations 
It is well understood that distributional similarity 
reflects semantic similarity and can be used to 
automatically construct a distributional thesaurus 
for frequent words (Lin, 1997; inter al). Until now, 
most works aiming at semantic similarity rely on a 
parser that extracts dependency relations. The 
claim here again is that similarity on parser output 
might be replaced by similarity on a pattern basis, 
(cf. Davidov and Rappoport 2006). For class-based 
generalization in these patterns, the system de-
scribed in section 3.2 might prove useful. Prelimi-
nary experiments revealed that similarity on 
significantly co-occurring patterns is able to pro-
duce very promising similarity rankings. A cluster-
ing of these with CW leads to thesaurus entries 
comparable to thesauri like Roget?s.  
Clustering not only words based on similarity 
of patterns, but also patterns based on similarity of 
words enables us to identify clusters of patterns 
with different relations they manifest.  
5 Conclusion 
The claim of this work is that unsupervised NLP 
can support and/or replace preprocessing steps in 
NLP that have previously been achieved by a large 
amount of manual work, i.e. annotation, rule con-
struction or resource building. This is proven em-
pirically on the tasks of language identification and 
part-of-speech tagging, exemplified on WSD and 
discussed for thesaurus construction and relation 
extraction. The main contributions of the disserta-
tion that is summarized here are: 
? A framework for unsupervised NLP 
? An efficient graph clustering algorithm 
? An unsupervised language separator 
? An unsupervised POS tagger 
The main advantage of unsupervised NLP, 
namely language independence, will enable the 
immediate processing of all languages and do-
mains for which a large amount of text is elec-
tronically available. 
References  
E. Agirre, D. Mart?nez, O. L?pez de Lacalle and A. So-
roa. 2006. Evaluating and optimizing the parameters 
of an unsupervised graph-based WSD algorithm. 
Proceedings of Textgraphs-06, New York, USA 
C. Biemann and S. Teresniak. 2005. Disentangling from 
Babylonian Confusion ? Unsupervised Language 
Identification. Proc. CICLing-2005, Mexico City 
C. Biemann. 2006a. Chinese Whispers - an Efficient 
Graph Clustering Algorithm and its Application to 
Natural Language Processing Problems. Proceedings 
of Textgraphs-06, New York, USA 
C. Biemann. 2006b. Unsupervised Part-of-Speech Tag-
ging Employing Efficient Graph Clustering. Proceed-
ings of COLING/ACL-06 SRW, Sydney, Australia 
C. Biemann. 2007. A Random Text Model for the Gen-
eration of Statistical Language Invariants. Proceed-
ings of HLT-NAACL-07, Rochester, USA 
C. Biemann, C. Giuliano and A. Gliozzo. 2007. Unsu-
pervised POS tagging supporting supervised meth-
ods. Manuscript to appear 
S. Bordag. 2006. Word Sense Induction: Triplet-Based 
Clustering and Automatic Evaluation. Proceedings of 
EACL-06. Trento, Italy 
D. Davidov, A. Rappoport. 2006. Efficient Unsuper-
vised Discovery of Word Categories Using Symmet-
ric Patterns and High Frequency Words. Proceedings 
of COLING/ACL-06, Sydney, Australia 
S. van Dongen. 2000. A cluster algorithm for graphs. 
Technical Report INS-R0010, CWI, Amsterdam. 
B. Dorow and D. Widdows. 2003. Discovering Corpus-
Specific Word Senses. In EACL-2003 Conference 
Companion, Budapest, Hungary 
T. Dunning. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational Linguis-
tics, 19(1), pp. 61-74 
D. Lin. 1997. Automatic Retrieval and Clustering of 
Similar Words. Proc. COLING-97, Montreal, Canada  
H. Sch?tze. 1993. Word Space. Proceedings of NIPS-5, 
Morgan Kaufmann, San Francisco, CA, USA 
J. ??ma and S.E. Schaeffer. 2005. On the NP-
completeness of some graph cluster measures. Tech-
nical Report cs.CC/0506100, http://arxiv.org/. 
M. Steyvers, J. B. Tenenbaum. 2005. The large-scale 
structure of semantic networks. Cog. Science, 29(1) 
D. Widdows. 2005. Geometry and Meaning. CSLI Lec-
ture notes #172, Stanford, USA 
40
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 7?12,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Part-of-Speech Tagging  
Employing Efficient Graph Clustering 
Chris Biemann 
University of Leipzig, NLP Department 
Augustusplatz 10/11, 04109 Leipzig, Germany 
biem@informatik.uni-leipzig.de 
 
Abstract 
An unsupervised part-of-speech (POS) 
tagging system that relies on graph 
clustering methods is described. Unlike 
in current state-of-the-art approaches, the 
kind and number of different tags is 
generated by the method itself. We 
compute and merge two partitionings of 
word graphs: one based on context 
similarity of high frequency words, 
another on log-likelihood statistics for 
words of lower frequencies. Using the 
resulting word clusters as a lexicon, a 
Viterbi POS tagger is trained, which is 
refined by a morphological component. 
The approach is evaluated on three 
different languages by measuring 
agreement with existing taggers.  
1 Introduction 
1.1 Motivation 
Assigning syntactic categories to words is an 
important pre-processing step for most NLP 
applications.  
Essentially, two things are needed to construct 
a tagger: a lexicon that contains tags for words 
and a mechanism to assign tags to running words 
in a text. There are words whose tags depend on 
their use. Further, we also need to be able to tag 
previously unseen words. Lexical resources have 
to offer the possible tags, and our mechanism has 
to choose the appropriate tag based on the 
context.  
Given a sufficient amount of manually tagged 
text, several approaches have demonstrated the 
ability to learn the instance of a tagging 
mechanism from manually labelled data and 
apply it successfully to unseen data. Those high-
quality resources are typically unavailable for 
many languages and their creation is labour-
intensive. We will describe an alternative 
needing much less human intervention. 
In this work, steps are undertaken to derive a 
lexicon of syntactic categories from unstructured 
text without prior linguistic knowledge. We 
employ two different techniques, one for high-
and medium frequency terms, one for medium- 
and low frequency terms. The categories will be 
used for the tagging of the same text where the 
categories were derived from. In this way, 
domain- or language-specific categories are 
automatically discovered. 
1.2 Existing Approaches 
There are a number of approaches to derive 
syntactic categories. All of them employ a 
syntactic version of Harris? distributional 
hypothesis: Words of similar parts of speech can 
be observed in the same syntactic contexts. 
Contexts in that sense are often restricted to the 
most frequent words. The words used to describe 
syntactic contexts will be called feature words in 
the remainder. Target words, as opposed to this, 
are the words that are to be grouped into 
syntactic clusters.  
The general methodology (Finch and Chater, 
1992; Sch?tze, 1995; inter al.) for inducing word 
class information can be outlined as follows: 
1. Collect global context vectors for target 
words by counting how often feature 
words appear in neighbouring positions. 
2. Apply a clustering algorithm on these 
vectors to obtain word classes 
Throughout, feature words are the 150-250 
words with the highest frequency. Contexts are 
the feature words appearing in the immediate 
neighbourhood of a word. The word?s global 
context is the sum of all its contexts. 
For clustering, a similarity measure has to be 
defined and a clustering algorithm has to be 
chosen. Finch and Chater (1992) use the 
Spearman Rank Correlation Coefficient and a 
hierarchical clustering, Sch?tze (1995) uses the 
cosine between vector angles and Buckshot 
clustering.  
An extension to this generic scheme is 
presented in (Clark, 2003), where morphological 
7
information is used for determining the word 
class of rare words. Freitag (2004) does not sum 
up the contexts of each word in a context vector, 
but the most frequent instances of four-word 
windows are used in a co-clustering algorithm. 
Regarding syntactic ambiguity, most 
approaches do not deal with this issue while 
clustering, but try to resolve ambiguities at the 
later tagging stage.  
A severe problem with most clustering 
algorithms is that they are parameterised by the 
number of clusters. As there are as many 
different word class schemes as tag sets, and the 
exact amount of word classes is not agreed upon 
intra- and interlingually, inputting the number of 
desired clusters beforehand is clearly a 
drawback. In that way, the clustering algorithm 
is forced to split coherent clusters or to join 
incompatible sub-clusters. In contrast, 
unsupervised part-of-speech induction means the 
induction of the tag set, which implies finding 
the number of classes in an unguided way. 
1.3 Outline 
This work constructs an unsupervised POS 
tagger from scratch. Input to our system is a 
considerable amount of unlabeled, monolingual 
text bar any POS information. In a first stage, we 
employ a clustering algorithm on distributional 
similarity, which groups a subset of the most 
frequent 10,000 words of a corpus into several 
hundred clusters (partitioning 1). Second, we use 
similarity scores on neighbouring co-occurrence 
profiles to obtain again several hundred clusters 
of medium- and low frequency words 
(partitioning 2). The combination of both 
partitionings yields a set of word forms 
belonging to the same derived syntactic category. 
To gain on text coverage, we add ambiguous 
high-frequency words that were discarded for 
partitioning 1 to the lexicon. Finally, we train a 
Viterbi tagger with this lexicon and augment it 
with an affix classifier for unknown words.  
The resulting taggers are evaluated against 
outputs of supervised taggers for various 
languages. 
2 Method 
The method employed here follows the coarse 
methodology as described in the introduction, 
but differs from other works in several respects. 
Although we use 4-word context windows and 
the top frequency words as features (as in 
Sch?tze 1995), we transform the cosine 
similarity values between the vectors of our 
target words into a graph representation. 
Additionally, we provide a methdology to 
identify and incorporate POS-ambiguous words 
as well as low-frequency words into the lexicon. 
2.1 The Graph-Based View 
Let us consider a weighted, undirected graph 
G(V,E) (v?V vertices, (vi,vj,wij)?E edges with 
weights wij). Vertices represent entities (here: 
words); the weight of an edge between two 
vertices indicates their similarity.  
As the data here is collected in feature vectors, 
the question arises why it should be transformed 
into a graph representation. The reason is, that 
graph-clustering algorithms such as e.g. (van 
Dongen, 2000; Biemann 2006), find the number 
of clusters automatically1. Further, outliers are 
handled naturally in that framework, as they are 
represented as singleton nodes (without edges) 
and can be excluded from the clustering. A 
threshold s on similarity serves as a parameter to 
influence the number of non-singleton nodes in 
the resulting graph.  
For assigning classes, we use the Chinese 
Whispers (CW) graph-clustering algorithm, 
which has been proven useful in NLP 
applications as described in (Biemann 2006). It is 
time-linear with respect to the number of edges, 
making its application viable even for graphs 
with several million nodes and edges. Further, 
CW is parameter-free, operates locally and 
results in a partitioning of the graph, excluding 
singletons (i.e. nodes without edges). 
2.2 Obtaining the lexicon 
Partitioning 1: High and medium frequency 
words 
Four steps are executed in order to obtain 
partitioning 1: 
1. Determine 200 feature and 10.000 target 
words from frequency counts 
2. construct graph from context statistics 
3. Apply CW on graph. 
4. Add the feature words not present in the 
partitioning as one-member clusters. 
The graph construction in step 2 is conducted 
by adding an edge between two words a and b 
                                                 
1 This is not an exclusive characteristic for graph 
clustering algorithms. However, the graph model 
deals with that naturally while other models usually 
build some meta-mechanism on top for determining 
the optimal number of clusters. 
8
with weight w=1/(1-cos(a,b)), if w exceeds a 
similarity threshold s. The latter influences the 
number of words that actually end up in the 
graph and get clustered. It might be desired to 
cluster fewer words with higher confidence as 
opposed to running in the danger of joining two 
unrelated clusters because of too many 
ambiguous words that connect them. 
After step 3, we already have a partition of a 
subset of our target words. The distinctions are 
normally more fine-grained than existing tag 
sets. 
As feature words form the bulk of tokens in 
corpora, it is clearly desired to make sure that 
they appear in the final partitioning, although 
they might form word classes of their own2. This 
is done in step 4. We argue that assigning 
separate word classes for high frequency words 
is a more robust choice then trying to 
disambiguate them while tagging.  
Lexicon size for partitioning 1 is limited by 
the computational complexity of step 2, which is 
time-quadratic in the number of target words. For 
adding words with lower frequencies, we pursue 
another strategy.  
Partitioning 2: Medium and low frequency 
words 
As noted in (Dunning, 1993), log-likelihood 
statistics are able to capture word bi-gram 
regularities. Given a word, its neighbouring co-
occurrences as ranked by the log-likelihood 
reflect the typical immediate contexts of the 
word. Regarding the highest ranked neighbours 
as the profile of the word, it is possible to assign 
similarity scores between two words A and B 
according to how many neighbours they share, 
i.e. to what extent the profiles of A and B 
overlap. This directly induces a graph, which can 
be again clustered by CW.  
This procedure is parametrised by a log-
likelihood threshold and the minimum number of 
left and right neighbours A and B share in order 
to draw an edge between them in the resulting 
graph. For experiments, we chose a minimum 
log-likelihood of 3.84 (corresponding to 
statistical dependence on 5% level), and at least 
four shared neighbours of A and B on each side.  
Only words with a frequency rank higher than 
2,000 are taken into account. Again, we obtain 
several hundred clusters, mostly of open word 
classes. For computing partitioning 2, an 
efficient algorithm like CW is crucial: the graphs 
                                                 
2 This might even be desired, e.g. for English not. 
as used for the experiments consisted of 
52,857/691,241 (English), 85,827/702,349 
(Finnish) and 137,951/1,493,571 (German) 
nodes/edges. 
The procedure to construct the graphs is faster 
than the method used for partitioning 1, as only 
words that share at least one neighbour have to 
be compared and therefore can handle more 
words with reasonable computing time. 
Combination of  partitionings 1 and 2 
Now, we have two partitionings of two different, 
yet overlapping frequency bands. A large portion 
of these 8,000 words in the overlapping region is 
present in both partitionings. Again, we construct 
a graph, containing the clusters of both 
partitionings as nodes; weights of edges are the 
number of common elements, if at least two 
elements are shared. And again, CW is used to 
cluster this graph of clusters. This results in 
fewer clusters than before for the following 
reason: While the granularities of partitionings 1 
and 2 are both high, they capture different 
aspects as they are obtained from different 
sources. Nodes of large clusters (which usually 
consist of open word classes) have many edges 
to the other partitioning?s nodes, which in turn 
connect to yet other clusters of the same word 
class. Eventually, these clusters can be grouped 
into one.  
Clusters that are not included in the graph of 
clusters are treated differently, depending on 
their origin: clusters of partition 1 are added to 
the result, as they are believed to contain 
important closed word class groups. Dropouts 
from partitioning 2 are left out, as they mostly 
consist of small, yet semantically motivated 
word sets. Combining both partitionings in this 
way, we arrive at about 200-500 clusters that will 
be further used as a lexicon for tagging. 
Lexicon construction 
A lexicon is constructed from the merged 
partitionings, which contains one possible tag 
(the cluster ID) per word. To increase text 
coverage, it is possible to include those words 
that dropped out in the distributional step for 
partitioning 1 into the lexicon. It is assumed that 
these words dropped out because of ambiguity. 
From a graph with a lower similarity threshold s 
(here: such that the graph contained 9,500 target 
words), we obtain the neighbourhoods of these 
words one at a time. The tags of those 
neighbours ? if known ? provide a distribution of 
possible tags for these words.  
9
2.3 Constructing the tagger 
Unlike in supervised scenarios, our task is not to 
train a tagger model from a small corpus of 
hand-tagged data, but from our clusters of 
derived syntactic categories and a considerably 
large, yet unlabeled corpus.  
Basic Trigram Model 
We decided to use a simple trigram model 
without re-estimation techniques. Adopting a 
standard POS-tagging framework, we maximize 
the probability of the joint occurrence of tokens 
(ti) and categories (ci) for a sequence of length n: 
?
=
??=
n
i
iiiii tcPcccPCTP
1
21 )|(),|(),( . 
The transition probability P(ci|ci-1,ci-2) is 
estimated from word trigrams in the corpus 
whose elements are all present in our lexicon.  
The last term of the product, namely P(ci|ti), is 
dependent on the lexicon3. If the lexicon does not 
contain (ti), then (ci) only depends on 
neighbouring categories. Words like these are 
called out-of-vocabulary (OOV) words.  
Morphological Extension 
Morphologically motivated add-ons are used e.g. 
in (Clark, 2003) and (Freitag 2004) to guess a 
more appropriate category distribution based on 
a word?s suffix or its capitalization for OOV 
words. Here, we examine the effects of Compact 
Patricia Trie classifiers (CPT) trained on prefixes 
and suffixes.  We use the implementation of 
(Witschel and Biemann, 2005). For OOV words, 
the category-wise product of both classifier?s 
distributions serve as probabilities P(ci|ti): Let 
w=ab=cd be a word, a be the longest common 
prefix of w that can be found in all lexicon 
words, and d be the longest common suffix of w 
that can be found in all lexicon words. Then 
}|{
})(class|{
}|{
})(class|{
)|(
ydvv
icvydvv
axuu
icuaxuuwicP =
=?=
?
=
=?== . Table 1: Characteristics of corpora: number of sentences, tokens, tagger and tagset size, corpus 
coverage of top 200 and 10,000 words. 
CPTs do not only smoothly serve as a 
substitute lexicon component, they also realize 
capitalization, camel case and suffix endings 
naturally. 
                                                 
3 Although (Charniak et al 1993) report that using  
P(ti|ci) instead leads to superior results in the 
supervised setting, we use the ?direct? lexicon 
probability. Note that our training material size is 
considerably larger than hand-labelled POS corpora. 
3 Evaluation methodology 
We adopt the methodology of (Freitag 2004) and 
measure cluster-conditional tag perplexity PP as 
the average amount of uncertainty to predict the 
tags of a POS-tagged corpus, given the tagging 
with classes from the unsupervised method. Let  
??=
x
X xPxPI )(ln)(  
be the entropy of a random variable X and  
?=
xy
XY yPxP
yxPyxPM
)()(
),(ln),(  
be the mutual information between two 
random variables X and Y. Then the cluster-
conditional tag perplexity for a gold-standard 
tagging T and a tagging resulting from clusters C 
is computed as  
)exp()exp( | TCTCT MIIPP ?== . 
Minimum PP is 1.0, connoting a perfect 
congruence on gold standard tags.  
In the experiment section we report PP on 
lexicon words and OOV words separately. The 
objective is to minimize the total PP.  
4 Experiments 
4.1 Corpora 
For this study, we chose three corpora: the 
British National Corpus (BNC) for English, a 10 
Million sentences newspaper corpus from 
Projekt Deutscher Wortschatz4 for German, and 
3 million sentences from a Finnish web corpus 
(from the same source). Table 1 summarizes 
some characteristics. 
lang. sent. tok. tagger nr. 
tags 
200 
cov. 
10K 
cov. 
en 6M 100M BNC5 84 55% 90%
fi 3M 43M Connexor6 31 30% 60%
ger 10M 177M (Schmid,1994) 54 49% 78%
 
Since a high coverage is reached with few 
words in English, a strategy that assigns only the 
most frequent words to sensible clusters will take 
us very far here. In the Finnish case, we can 
expect a high OOV rate, hampering performance 
                                                 
4 See http://corpora.informatik.uni-leipzig.de. 
5 Semi-automatic tags as provided by BNC. 
6 Thanks goes to www.connexor.com for an academic 
license; the tags do not include interpunctuation 
marks, which are treated seperately. 
10
of strategies that cannot cope well with low 
frequency or unseen words. 
4.2 Baselines 
To put our results in perspective, we computed 
the following baselines on random samples of 
the same 1000 randomly chosen sentences that 
we used for evaluation: 
? 1: the trivial top clustering: all words are in 
the same cluster 
? 200: The most frequent 199 words form 
clusters of their own; all the rest is put into 
one cluster.  
? 400: same as 200, but with 399 most 
frequent words 
Table 2 summarizes the baselines. We give PP 
figures as well as tag-conditional cluster 
perplexity PPG (uncertainty to predict the 
clustering from the gold standard tags, inverse 
direction of PP):  
lang English Finnish German 
base 1 200 400 1 200 400 1 200 400
PP 29 3.6 3.1 20 6.1 5.3 19 3.4 2.9 
PPG 1.0 2.6 3.5 1.0 2.0 2.5 1.0 2.5 3.1 
Table 2: Baselines for various tag set sizes 
4.3 Results 
We measured the quality of the resulting taggers 
for combinations of several substeps:  
? O: Partitioning 1  
? M: the CPT morphology extension  
? T: merging partitioning 1 and 2 
? A: adding ambiguous words to the lexicon 
Figure 2 illustrates the influence of the 
similarity threshold s for O,  OM and OMA for 
German ? the other languages showed similar 
results. Varying s influences coverage on the 
10,000 target words. When clustering very few 
words, tagging performance on these words 
reaches a PP as low as 1.25 but the high OOV 
rate impairs the total performance. Clustering too 
many words results in deterioration of results - 
most words end up in one big partition. In the 
medium ranges, higher coverage and lower 
known PP compensate each other, optimal total 
PPs were observed at target coverages 4,000-
8,000. Adding ambiguous words results in a 
worse performance on lexicon words, yet 
improves overall performance, especially for 
high thresholds. 
For all further experiments we fixed the 
threshold in a way that partitioning 1 consisted of 
5,000 words, so only half of the top 10,000 
words are considered unambiguous. At this 
value, we found the best performance averaged 
over all corpora.  
 
 
Fig 2. Influence of threshold s on tagger 
performance: cluster-conditional tag perplexity 
PP as a function of target word coverage.  
 
lang  O OM OMA TM TMA
total 2.66 2.43 2.08 2.27 2.05
lex 1.25 1.51 1.58 1.83
oov 6.74 6.70 5.82 9.89 7.64
oov% 28.07 14.25 14.98 4.62
 
 
EN 
tags 619 345 
total 4.91 3.96 3.79 3.36 3.22
lex 1.60 2.04 1.99 2.29
oov 8.58 7.90 7.05 7.54 6.94
oov% 47.52 36.31 32.01 23.80
 
 
FI 
tags 625 466 
total 2.53 2.18 1.98 1.84 1.79
lex 1.32 1.43 1.51 1.57
oov 3.71 3.12 2.73 2.97 2.57
oov% 31.34 23.60 19.12 13.80
 
 
GER
tags 781 440 
Table 3: results for English, Finnish, German. 
oov% is the fraction of non-lexicon words. 
 
Overall results are presented in table 3. The 
combined strategy TMA reaches the lowest PP 
for all languages. The morphology extension (M) 
always improves the OOV scores. Adding 
ambiguous words (A) hurts the lexicon 
performance, but largely reduces the OOV rate, 
which in turn leads to better overall performance. 
Combining both partitionings (T) does not 
always decrease the total PP a lot, but lowers the 
number of tags significantly. Finnish figures are 
generally worse than for the other languages, 
akin to higher baselines.  
The high OOV perplexities for English in 
experiment TM and TMA can be explained as 
follows: The smaller the OOV rate gets, the more 
likely it is that the corresponding words were 
also OOV in the gold standard tagger. A remedy 
11
would be to evaluate on hand-tagged data. 
Differences between languages are most obvious 
when comparing OMA and TM: whereas for 
English it pays off much more to add ambiguous 
words than to merge the two partitionings, it is 
the other way around in the German and Finnish 
experiments.   
To wrap up: all steps undertaken improve the 
performance, yet their influence's strength varies. 
As a flavour of our system's output, consider the 
example in table 4 that has been tagged by our 
English TMA model: as in the introductory 
example, "saw" is disambiguated correctly. 
 
Word cluster ID cluster members (size) 
I 166 I (1) 
saw 2 past tense verbs (3818) 
the 73 a, an, the (3) 
man 1 nouns (17418) 
with 13 prepositions (143) 
a 73 a, an, the (3) 
saw 1 nouns (17418) 
. 116 . ! ? (3) 
Table 4: Tagging example 
 
We compare our results to (Freitag, 2004), as 
most other works use different evaluation 
techniques that are only indirectly measuring 
what we try to optimize here. Unfortunately, 
(Freitag 2004) does not provide a total PP score 
for his 200 tags. He experiments with an hand-
tagged, clean English corpus we did not have 
access to (the Penn Treebank). Freitag reports a 
PP for known words of 1.57 for the top 5,000 
words (91% corpus coverage, baseline 1 at 23.6), 
a PP for unknown words without morphological 
extension of 4.8. Using morphological features 
the unknown PP score is lowered to 4.0. When 
augmenting the lexicon with low frequency 
words via their distributional characteristics, a 
PP as low as 2.9 is obtained for the remaining 
9% of tokens. His methodology, however, does 
not allow for class ambiguity in the lexicon, the 
low number of OOV words is handled by a 
Hidden Markov Model.  
5 Conclusion and further work 
We presented a graph-based approach to 
unsupervised POS tagging. To our knowledge, 
this is the first attempt to leave the decision on 
tag granularity to the tagger. We supported the 
claim of language-independence by validating 
the output of our system against supervised 
systems in three languages.  
The system is not very sensitive to parameter 
changes: the number of feature words, the 
frequency cutoffs, the log-likelihood threshold 
and all other parameters did not change overall 
performance considerably when altered in 
reasonable limits. In this way it was possbile to 
arrive at a one-size-fits-all configuration that 
allows the parameter-free unsupervised tagging 
of large corpora.  
To really judge the benefit of an unsupervised 
tagging system, it should be evaluated in an 
application-based way. Ideally, the application 
should tell us the granularity of our tagger: e.g. 
semantic class learners could greatly benefit 
from the high-granular word sets arising in both 
of our partitionings, which we endeavoured to 
lump into a coarser tagset here.  
References 
C. Biemann. 2006. Chinese Whispers - an Efficient 
Graph Clustering Algorithm and its Application to 
Natural Language Processing Problems. 
Proceedings of the HLT-NAACL-06 Workshop on 
Textgraphs-06, New York, USA 
E. Charniak, C. Hendrickson, N. Jacobson and M. 
Perkowitz. 1993. Equations for part-of-speech 
tagging. In Proceedings of the 11th National 
Conference on AI, pp. 784-789, Menlo Park 
A. Clark. 2003. Combining Distributional and 
Morphological Information for Part of Speech 
Induction, Proceedings of EACL-03 
T. Dunning. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence, Computational 
Linguistics 19(1), pp. 61-74 
S. Finch and N. Chater. 1992. Bootstrapping Syntactic 
Categories Using Statistical Methods. In Proc. 1st 
SHOE Workshop. Tilburg, The Netherlands 
D. Freitag. 2004. Toward unsupervised whole-corpus 
tagging. Proc. of COLING-04, Geneva, 357-363. 
H. Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In: Proceedings of 
the International Conference on New Methods in 
Language Processing, Manchester, UK, pp. 44-49 
H. Sch?tze. 1995. Distributional part-of-speech 
tagging. In EACL 7, pages 141?148 
S. van Dongen. 2000. A cluster algorithm for graphs. 
Technical Report INS-R0010, National Research 
Institute for Mathematics and Computer Science in 
the Netherlands, Amsterdam. 
F. Witschel, and C. Biemann. 2005. Rigorous 
dimensionality reduction through linguistically 
motivated feature selection for text categorisation. 
Proc. of NODALIDA 2005, Joensuu,  Finland 
12
Named Entity Learning and Verification:
Expectation Maximization in Large Corpora
Uwe QUASTHOFF, Christian BIEMANN, Christian WOLFF
CS Institute, Leipzig University
Augustusplatz 10/11
Leipzig, Germany, 04109
Abstract
The regularity of named entities is used to learn
names and to extract named entities. Having only
a few name elements and a set of patterns the al-
gorithm learns new names and its elements. A
verification step assures quality using a large
background corpus. Further improvement is
reached through classifying the newly learnt
elements on character level. Moreover, unsuper-
vised rule learning is discussed.
1 Introduction
The task of recognizing person names in text
corpora is well examined in the field of Infor-
mation Extraction. Most of the approaches, us-
ing machine-learning or statistical methods
make excessive use of annotated data or large
gazetteers. We present a method that needs little
input knowledge and performs unsupervised
learning on unlabeled data, restricting ourselves
to person names.
In most languages, named entities form regular
patterns. Usually, a surname has a preceding
first name, which in turn might have a preceding
title or profession. Similar rules hold for differ-
ent kinds of named entities consisting of more
than one word. Moreover, some elements of
such multiwords (like president) are high fre-
quency items, well known and of high signifi-
cance for identifying a named entity. On the
other hand, there are elements that often appear
in named entities, but are not characteristic for
them (like the first name Isra l).
Therefore, a verification step is included in the
learning algorithm.
2 The Algorithm
Our learning algorithm starts with a set of pat-
terns and initial name elements. A large corpus
of more then 10 million sentences [cf. Quasthoff
& Wolff 2000], taken from newspapers of the
last 10 years is used for both, the identification
of candidates for new name elements as well as
for verifying the candidates found. The algo-
rithm stops, if no more new name elements are
found.
The algorithm implements xpectation maximi-
zation (EM) [cf. Dempster, 1977, Collins, 1999]
in the following way: The combination of a
learning step and a verification step are iterated.
If more name elements are found, the recall of
the verification step increases. The key property
of this algorithm is to assure high precision and
stil  get massive recall.
From another point of view our algorithm im-
plements bootstrapping [cf. Riloff 99], as it
starts from a small number of seed words and
uses knowledge found during the run to find
more candidates.
2.1 Patterns and Pattern Rules
In a first step the text to be analysed is tagged in
he ollowing way: We have two types of tags.
The first type is problem dependent. In the case
of persons, we have tags for title or profession
(TI), first name (FN) and surname (LN). The
second tag set is problem independent, but lan-
guage dependent. In our experiments, we
marked words as lower case (LC) or upper case
(UC) depending on the first letter. Punctuation
marks are marked as PM, determiners as DET.
Words can have multiple tags, e.g. UC and FN
at the same time.
The next step is to find tag sequences which are
typical for names, like TI-FN-LN . From here,
we can create rules like
TI-UC-LN ? TI-FN-LN ,
which means that an upper case word between
title and last name is a candidate for a first name.
An overview of handmade start rules is given in
appendix 1.
Looking at the rules, it is possible to argue that a
rule like UC-LN ? FN-LN  is a massive over-
generalization. This would be true if we would
learn new name elements simply by applying
rules. However, the verification step ensures that
false friends are eliminated at high rate.
2.2 The Outer Loop
The algorithm is described as follows:
Load pattern rules.
Let unused name elements = in i tial
             set of name elements
Loop:
   For each unused name entity
Do the learning step and
             collect new cand i dates
   For each new candidate
Do the verification step
   Output verified candidates
   Let unused name elements =
             ver i fied candidates
2.3 The Learning Step: Finding Candi-
dates
Using the pattern rules and the current name
elements, new candidates are searched. Here we
use the corpus.
Search 255 random sentences co n-
taining the unused name entity (or
all, if <255).
Use the pattern rules to identify
new candidates as d escribed above.
2.4 The Verification Step
In a verification step, each candidate is tested
before it is used to generate new candidates. We
test the following property: Does the candidate
appear often enough together with verified name
elements? Again, we use the corpus.
Search 30 random sentences contai n-
ing the name element to be ver i -
fied (or all, if <30).
If the ratio fulfilling at least
one right side of a pattern rule
is above some threshold, the ca n-
didate is a ccepted.
3 The Exhaustion Cycle
The overall performance of the algorithm can be
estimated as follows: For simplicity let us as-
sume that the average number of items (in our
task: name elements) findable by any unused
item equals N. Then the number of items starts
to grow exponentially. Sooner or later, the total
number of unseen entities decreases. Hence,
most of the N items found are known already.
The numbers of new items found in each turn
decreases, until no more items can be reached.
So we discriminate between a phase of growth
and a phase of exhaustion.
The following figures visualize the number of
new items per turn and the accumulated total
number of items for each turn. Data was taken
from an experiment with 19 items of knowledge
(see appendix 2). The test was performed on the
German corpus and designed to find first and
last names only. The phase of growth lasts until
the 5th cycle, then exhaustion takes over, as can
be seen in figure 11.
0
5000
10000
15000
20000
25000
30000
0 1 2 3 4 5 6 7 8 9 1 0 1 1
cycle
it
em
s
New Items Total Items
Figure 1: total and new items vs. cycles2
                                            
1 Additional runs with more start items produced the
same amount of  total items in less cycles.
2 Note that 25'000 name elements are responsible for
the detection of over 150'000 full names.
Natural growth in the number of items takes
place under the following conditions:
? Sufficient size of corpus
? Sufficient frequency of start items
? Suitable relation, e.g. names
If the corpus is not large enough or it is not pos-
sible to find enough candidates from the start
items, exhaustion takes place immediately.
4 Examples
Let us closely examine the learning of items by
example: From the known first name John, the
candidate for being a last name H uberg was
found in the fragment "...by John Hauberg and.."
by the rule FN-UC-LC => FN-LN-LC  and
verified in occurrences like "Robert Hauberg,
...", "Robert Hauberg urges..." using the already
known first name Robert.
Errors occur in the case of words, which are
mainly used in positions which are often occu-
pied by first names. In German, the algorithm
extracts and verifies "?ra" (e ) and "Transport-
panzer" (Army transportation tank) because of
the common usage "?ra Kohl" and the proper
name "Transportpanzer Fuchs" (fox tank). In the
case of "?ra", this false first name supports the
classifications of the proper last names Hinrichs,
Strau?, Bangemann, Albrecht, Gorbatchow,
Jelzin and many more.
5 Precision and Recall
5.1 Precision
Note that precision will be different for the dif-
ferent types of name elements. Usually surnames
are recognized with high precision. First names
may be confused with titles, for instance.
Moreover, precision is language dependent
mainly due to the different usage of capital let-
ters: In German, nouns start with capital letters
and can much easier be confused with names.
For German first names in the run mentioned
above, the algorithm yields a precision of
84.1%. Noise items mainly are titles and profes-
sion names, which are spelled with a capital
letter in German. Using the additional fact that
first names usually do not exceed 10 letters in
length, the precision for first names rose to
92.7%.
For last names, results were excellent with a
precision of more than 99%. The same holds for
titles, as further experiments showed.
The ratio number of first names vs. number of
last names happens to be about 1:3, overall pre-
cision for German scored 97.5%.
Because of the fewer capitalized words in Eng-
lish the precision for English first names is
higher, scoring 92.6% without further filtering.
Overall precision for English first and last names
was 98.7%.
5.2 Recall
Recall mainly depends on the pattern rules used.
The experiments were performed with the 14
handmade rules given in appendix 1, which
surely are not sufficient.
Calculating the recall is not at all straightfor-
ward, because we do not know how many names
are contained in our corpora and experiments on
small corpora fail to show the natural growth of
items described in the previous section. Further,
recall will rise with a growing knowledge size.
So we modified the algorithm in a way that it
takes plain text as input, applies the rules to find
candidates and checks them in the big corpus.
Providing a large set of knowledge items, in an
experiment processing 1000 sentences, 71.4% of
the person names were extracted correctly.
To increase the coverage of the rules it is possi-
ble to add rules manually or start a process of
rule learning as described below.
5.3. Propagation of Errors
During the run the error rate increases due to
finding candidates and verification through mis-
classified items. However, as the "era" example
(see section 4) illustrates, misclassified items
support the classification of goal items.
The amount of deterioration highly depends on
the pattern rules. Strict rules mean low recall but
high precision, whereas general rules have
greater coverage but find too much, resulting in
a trade-off between precision and recall.
Table 1 shows the error rate for first names for
the illustrated run (see section 3) over the course
of time.
From this we conclude that the algorithm is ro-
bust against errors and the quality of the classifi-
cations remains relatively stable during the run
when using appropriate rules.
total items
interval
Precision for
FN without
length filter
Precision for
FN with
length filter
1-1000 87.1% 93.8%
1001-2000 90.0% 95.3%
4001-5000 88.1% 97.1%
9001-10000 83.2% 94.4%
19001-20000 83.7% 91.2%
21001-22000 86.2% 92.4%
24001-25000 83.0% 87.9%
Table 1: Propagation of Errors
6 Classification on character level
In German, most words misclassified as first
names were titles and professions. While they
cannot be distinguished by the rules used, they
differ strongly from the morphological view.
German titles are usually longer because they
are compounds, and parts of compounds are
used very frequently.
In this section, we introduce a m thod to distin-
guish between titles and first names at character
level, using the fact that the formation of words
follows language-dependent rules.
This procedure is implemented in the following
classifier A: Assume the property we are inter-
ested in is visible at the ending of a word (this is
basically true for different word classes in lan-
guages like English, French or German). We
build a decision tree [cf. McCarthy & Lehnert
1995] reading the words character-by-character,
starting from the end. We stop if the feature is
uniquely determined.
Moreover, we could as well start from the be-
ginning of a word (classifier B). Finally, we can
use any connected substring of the word instead
of substrings containing the end or the beginning
(classifier C).
If the training set is large enough and the algo-
rithm of the classifier is appropriate, it will cover
both general rules as well as many exceptions.
Classifier A and B only differ on the direction a
word is analyzed. We build decision trees with
additional default child nodes as follows.
6.1 Classifier A: Considering Prefixes
Step 1:Building the ordinary decision tree:
Given the training word list we construct
a prefix tree [cf. Gusfield 1999, Navarro
2001:38ff]. The leaves in the tree corre-
spond to word endings; here we store the
feature of the corresponding word.
Step 2:Reduction of the decision tree: If all
children of a given node have the same
feature, this feature is lifted to the parent
node and the children are deleted.
Step 3:Insertion of default features: If a node
does not yet have a feature, but one of
the features is very dominant (say, pres-
ent in 80% of the children), this feature
will be assigned as default feature.
For classification, the decision tree is used as
follows:
Step 1:Searching the tree: Reading the given
word from left to right we follow the
tree as far as possible. The reading proc-
ess stops in a certain node N.
Step 2:Classification: If the node N has an as-
signed feature F then return F. Other-
wise return no decision.
Figure 2 shows a part of the decision tree built
using first names Theoardis, Theobald, Theo-
derich, Theodor, Theresa, Therese, ? and the
singular title Theologe (which should be the only
title in our training list starting with Theo). As a
result, all children of Theo will be first names;
hence they get the feature firstname. The node
Theologe gets the feature titl .
This turns out to be singular; hence their parent
Theo gets the default feature firstname. As a
consequence, Theophil will correctly be classi-
fied as firstname, while the exception Theologe
will still be classified as title.
 Theologe 
(TI) 
Theodor 
(FN) 
Theobald 
(FN) 
Theo [default] 
(FN) 
... 
... 
T A Z 
(root) 
Figure 2: Prefix Decision Tree for Proper Names
As mentioned above, algorithm B works the
same way as algorithm A, using suffixes instead
of prefixes for the decision tree.
6.2 Classifier C: Considering Substrings
Instead of concentrating on prefixes or suffixes,
we consider all relevant continuous substrings of
a given word. Unfortunately, there is no natural
tree structure for this set. Hence, we will con-
struct a decision list without default features.
Given is a training list containing pairs (word,
feature):
Construction of the decision list
Step 1:Collect all substring information. We
produce the following list L: For all
pairs (wordN, featureN) from the train-
ing list we generate all possible pairs of
the kind (continuous substring of
wordN, featureN). If wordN has length
n, we have n(n+1)/2 continuous sub-
strings. Finally the list is sorted alpha-
betically and duplicates are removed.
Step 2:Removing contradictions: If a substring
occurs with more then one feature, these
lines are deleted from L.
Step 3:Removing tails: If a certain string now
has a unique feature, all extensions of
this string should have the same feature
and the corresponding entries are re-
moved from L.
For classification, the decision list is used as
follows:
Step 1:Look-up of substrings: For a word to be
classified we generate its continuous
substrings and collect their features from
L.
Step 2:Classification: If all collected features
are equal, then return this feature. Oth-
erwise, return no decision.
6.3 Properties of the classifiers
In the following, we assume that the classifiers
are trained with non-contradictory data. The
classifiers now have the following properties:
? The classifiers reproduce the results given in
the training set. Hence, they can also be
trained with rare exceptions.
? It is necessary to have a training set covering
all aspects of the data, otherwise the deci-
sion tree will be confused.
? It is appropriate to return no decision if the
classifier stops in the decision tree at a point
where children have mixed features.
Bagging [cf. Breiman 1996] the three classifiers,
we achieved a precision of 94.7% with 94.5%
recall, using merely a training set of 1368 exam-
ples on a test set of 683 items, distinguishing
between the three classes:
? First name (FN)
? Title (TI)
? None of these.
This method of postprocessing is applicable to
all features visible by the three classifiers, which
are:
? Features represented by word suffixes or
prefixes like inflection and some word for-
mation rules.
? Words carrying the same feature if they are
similar as strings. Candidates are all kinds of
proper names, as well as distinguishing
parts-of-speech.
? Words of languages for special purposes,
which are often built by combining parts
where some of them are very typical for a
given domain. Examples are chemical sub-
stances, professions and titles, or industrial
goods.
7 Rule Learning
Unlike most tasks in Inductive Logic Program-
ming (ILP) [cf. Dzeroski, 2001], our method
needs rules-of-thumb that find many candidates
like in boosting [cf. Collins, 1999], rather then a
rule precision of 100%.
For automatic rule induction we used a training
set of 236 sentences found automatically by
taking sentences containing known first names
and last names from the corpus. After excessive
annotation, all possible rules were built accord-
ing to the contexts of known items and after-
wards tested on the training set. To avoid rules
too general like UC-UC?FN-UC, the patterns
had to contain at least one problem specific tag
(i.e. FN, LN, TIT). The rules performing above a
certain precision threshold (in our experiments
we used 0.7) were taken as input for our algo-
rithm.
We obtained 106 rules for first names, 67 for
last names and 4 for titles, ranging from very
specific rules like
PM-PM-UC-LN ? PM-PM-FN-LN
to very general ones like
TI-UC ? TI-FN.
In the table below some rules found by auto-
matic induction are shown.
Rule example context
FN-UC-LN
? FN-FN-LN
Herbert Archibald
Miller
FN-LC-FN-UC
? FN-LC-FN-LN
Ilse und Maria Bode-
mann
UC-UC-LN
? UC-FN-LN
Pr?sident Bill Clinton
FN-FN-UC
? FN-FN-LN
Hans Christian A -
derson
TI-PM-UC-UC
? TI-FS-FN-UC
Dr. Helmut Kohl
Table 2: Rules Found by Automatic Induction
Using those rules as input for our algorithm, we
gained both, higher recall as well as higher pre-
cision compared to the handmade rules when
starting with the same knowledge. Table 3
shows precision rates for the three classes of
name elements, data was taken from a run with
19 start elements, the length filter for first names
was applied, and the string classifiers were not.
Due to less strict rules, precision decreases.
total items
interval
Prec. FN Prec. LN Prec.  TIT
1-1000 94,6% 99,6% 100%
1001-2000 94,8% 98,6% 100%
2001-3000 94,7% 98,4% 100%
4001-5000 84,7% 99,1% 100%
9001-10000 86,6% 98,6% 100%
24001-2500074,0% 89,7% 100%
Table 2: Propagation of errors for inferred rules
Percentage of first name items from the number
of total items was 23,3%, last name items made
75,2% of total items and title items yielded only
1,4%, because to the low number of title rules.
8 Future work
Despite of the good results when using inferred
rules as described above for our algorithm, we
hope to improve the method as a whole with
respect to the size of the input knowledge.
Natural growth behaviour can be observed from
some 10 frequent start items, the string classifier
requires a couple of hundred words for training
whereas rule learning needs some 200 fully an-
notated sentences containing names. Experi-
ments with sparsely annotated training sentences
(100 knowledge items) yielded too specific and
too weak rules with poor performance w.r.t.
recall.
Another possibility would be to start with a
small set of seed rules [cf. Riloff 1999] and to
construct-by-example and rate rules during the
classification task.
Another interesting issue is the understanding of
relations suitable for this method from a the-
retical viewpoint.
9 Acknowledgements
The authors would like to thank Martin L?uter
for providing, implementing and testing the
three string classifiers.
10 References
Apte, C.; Damerau, F.; Weiss, S. M. (1998) Text
Mining with Decision Trees and Decision Rules.
Proc. Conference on Automated Learning and Dis-
covery, Carnegie-Mellon University, June 1998.
Breiman, L. (1996) Bagging Predictors, Machine
Learning, Vol. 24, No. 2, pp. 123-140
Califf, M. E.; Mooney, R. J. (1997) Relational
Learning of Pattern-match Rules for Information
Extraction. Working Papers of the ACL-97 Work-
shop in NLP, 1997, 6-11.
Collins, M.; Singer, Y. (1999) Unsupervised Models
for Named Entity Classification. In: Proc. Of the
Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and very Large
Corpora.
Dempster, A.P.; Laird, N. M.; Rubin, D.B. (1977)
Maximum Likelihood from Incomplete Data via the
EM Algorithm, Journal of the Royal Statistical So-
ciety, Ser B, 39, 1-38.
Dzeroski, S.; Lavrac, N. (2001) Introduction to In-
ductive Logic Programming. I  Saso Dzeroski and
Nada Lavrac, editors, Relational Data Mining,
pages 48-73. Springer-Verlag, Berlin
Freitag, D. (1998) Multistrategy Learning for Info-
mation Extraction. Proc. 15th International Conf.
on Machine Learning, 161-169.
Gusfield, Dan (1999) Algorithms on Strings, Trees,
and Sequences. Cambridge University Press, UK.
McCarthy, J.; Lehnert, W. (1995) Using Decision
Trees for Coreference Resolution. In: Mellish, C.
(ed.) (1995). Proc. Fourteenth International Con-
ference on Artificial Intelligence, 1050-1055.
Nahm, U. Y.; Mooney, R. J. (2002) Text Mining with
Information Extraction. To appear in AAAI 2002
Spring Symposium on Mining Answers from Texts
and Knowledge Bases, Stanford, CA.
Navarro, G. (2001) A guided tour to approximate
string matching. ACM Computing Surveys 33(1)
(2001), 31-88.
Ng, H.; Lee, H. (1996) Integrating Multiple Knowl-
edge Sources to Disambiguate Word Sense: An Ex-
emplar-Based Approach. Proc. of the 34th Annual
Meeting of the ACL, 40-47.
Quasthoff, U.; Wolff, Ch. (2000) An Infrastructure
for Corpus-Based Monolingual Dictionaries. Proc.
LREC-2000. Second International Conference on
Language Resources and Evaluation. Athens, May
/ June 2000, Vol. I, 241-246.
Riloff, E.; Jones, R. (1999) Learning Dictionaries for
Information Extraction by Multi-Level Bootstrap-
ping. Proceedings of the sixteenth National Con-
ference on Artificial Intellig nce (AAAI-99)
Roth, D. (1998) Learning to Resolve Natural Lan-
guage Ambiguities: A Unified Approach. Pro . of
the American Association of Artificial Intelligence,
806-813.
Witten, I. H.; Frank, E. (1999) Data Mining: Practi-
cal Machine Learning Tools and Techniques with
Java Implementations. San Francisco, CA: Morgan
Kaufman.
Appendix 1: Initial Handmade Rule Set
UC-LN ? FN-LN
PM-FN-PM-UC ? PM-FN-PM-FN
TI-PM-UC-LN ? TI-PM-FN-LN
FN-LN-PM-UC-LN ? FN-LN-PM-FN-LN
FN-UC-PM ? FN-LN-PM
FN-UC-LC ? FN-LN-LC
TI-UC-LC ? TI-LN-LC
TI-PM-UC-LC ? TI-PM-LN-LC
LN-PM-FN-UC-PM ? LN-PM-FN-LN-PM
UC-PM-FN-LN ? TI-PM-FN-LN
UC-PM-LN ? TI-PM-LN
DET-UC-FN-LN ? DET-TI-FN-LN
DET-UC-FN-FN-LN ? DET-TI-FN-FN-LN
DET-UC-LN ? DET-TI-LN
Note that the last three rules are specific for German
because titles are in upper case in this language.
Appendix 2: 19 Start items used in the
experiments
Name elment Class
Schmidt LN
Reuter LN
Wagner LN
Sch?uble LN
Vogts LN
Hoffmann LN
Schulz LN
M?ller LN
Meyer LN
Beck LN
Michael FN
Thomas FN
Klaus FN
Wolfgang FN
Hans FN
Werner FN
Martin FN
Walter FN
Karl FN
Workshop on TextGraphs, at HLT-NAACL 2006, pages 73?80,
New York City, June 2006. c?2006 Association for Computational Linguistics
Chinese Whispers - an Efficient Graph Clustering Algorithm  
and its Application to Natural Language Processing Problems 
Chris Biemann 
University of Leipzig, NLP Department  
Augustusplatz 10/11  
04109 Leipzig, Germany 
biem@informatik.uni-leipzig.de 
 
Abstract 
We introduce Chinese Whispers, a 
randomized graph-clustering algorithm, 
which is time-linear in the number of 
edges. After a detailed definition of the 
algorithm and a discussion of its strengths 
and weaknesses, the performance of 
Chinese Whispers is measured on Natural 
Language Processing (NLP) problems as 
diverse as language separation, 
acquisition of syntactic word classes and 
word sense disambiguation. At this, the 
fact is employed that the small-world 
property holds for many graphs in NLP.  
1 Introduction 
Clustering is the process of grouping together 
objects based on their similarity to each other. In 
the field of Natural Language Processing (NLP), 
there are a variety of applications for clustering. 
The most popular ones are document clustering in 
applications related to retrieval and word clustering 
for finding sets of similar words or concept 
hierarchies.   
Traditionally, language objects are 
characterized by a feature vector. These feature 
vectors can be interpreted as points in a 
multidimensional space. The clustering uses a 
distance metric, e.g. the cosine of the angle 
between two such vectors. As in NLP there are 
often several thousand features, of which only a 
few correlate with each other at a time ? think 
about the number of different words as opposed to 
the number of words occurring in a sentence ? 
dimensionality reduction techniques can greatly 
reduce complexity without considerably losing 
accuracy.  
An alternative representation that does not deal 
with dimensions in space is the graph 
representation. A graph represents objects (as 
nodes) and their relations (as edges). In NLP, there 
are a variety of structures that can be naturally 
represented as graphs, e.g. lexical-semantic word 
nets, dependency trees, co-occurrence graphs and 
hyperlinked documents, just to name a few. 
Clustering graphs is a somewhat different task 
than clustering objects in a multidimensional 
space: There is no distance metric; the similarity 
between objects is encoded in the edges. Objects 
that do not share an edge cannot be compared, 
which gives rise to optimization techniques. There 
is no centroid or ?average cluster member? in a 
graph, permitting centroid-based techniques. 
As data sets in NLP are usually large, there is a 
strong need for efficient methods, i.e. of low 
computational complexities. In this paper, a very 
efficient graph-clustering algorithm is introduced 
that is capable of partitioning very large graphs in 
comparatively short time. Especially for small-
world graphs (Watts, 1999), high performance is 
reached in quality and speed. After explaining the 
algorithm in the next section, experiments with 
synthetic graphs are reported in section 3. These 
give an insight about the algorithm?s performance. 
In section 4, experiments on three NLP tasks are 
reported, section 5 concludes by discussing 
extensions and further application areas. 
2 Chinese Whispers Algorithm 
In this section, the Chinese Whispers (CW) 
algorithm is outlined. After recalling important 
concepts from Graph Theory (cf. Bollob?s 1998), 
we describe two views on the algorithm. The 
73
second view is used to relate CW to another graph 
clustering algorithm, namely MCL (van Dongen, 
2000). 
We use the following notation throughout this 
paper: Let G=(V,E) be a weighted graph with 
nodes (vi)?V and weighted edges (vi, vj, wij) ?E 
with weight wij. If (vi, vj, wij)?E implies (vj, vi, 
wij)?E, then the graph is undirected. If all weights 
are 1, G is called unweighted.  
The degree of a node is the number of edges a 
node takes part in. The neighborhood of a node v 
is defined by the set of all nodes v? such that 
(v,v?,w)?E or (v?,v,w)?E; it consists of all nodes 
that are connected to v.  
The adjacency matrix AG of a graph G with n 
nodes is an n?n matrix where the entry aij denotes 
the weight of the edge between vi and vj , 0 
otherwise. 
The class matrix DG of a Graph G with n nodes is 
an n?n matrix where rows represent nodes and 
columns represent classes (ci)?C. The value dij at 
row i and column j represents the amount of vi as 
belonging to a class cj. For convention, class 
matrices are row-normalized; the i-th row denotes 
a distribution of vi over C. If all rows have exactly 
one non-zero entry with value 1, DG denotes a hard 
partitioning of V, soft partitioning otherwise. 
2.1 Chinese Whispers algorithm 
CW is a very basic ? yet effective ? algorithm to 
partition the nodes of weighted, undirected graphs. 
It is motivated by the eponymous children?s game, 
where children whisper words to each other. While 
the game?s goal is to arrive at some funny 
derivative of the original message by passing it 
through several noisy channels, the CW algorithm 
aims at finding groups of nodes that broadcast the 
same message to their neighbors. It can be viewed 
as a simulation of an agent-based social network; 
for an overview of this field, see (Amblard 2002).  
The algorithm is outlined in figure 1: 
 
initialize:  
 forall vi in V: class(vi)=i; 
 
while changes: 
 forall v in V, randomized order: 
 class(v)=highest ranked class  
            in neighborhood of v; 
Figure 1: The Chinese Whispers algorithm 
 
Intuitively, the algorithm works as follows in a 
bottom-up fashion: First, all nodes get different 
classes. Then the nodes are processed for a small 
number of iterations and inherit the strongest class 
in the local neighborhood. This is the class whose 
sum of edge weights to the current node is 
maximal. In case of multiple strongest classes, one 
is chosen randomly. Regions of the same class 
stabilize during the iteration and grow until they 
reach the border of a stable region of another class. 
Note that classes are updated immediately: a node 
can obtain classes from the neighborhood that were 
introduced there in the same iteration.  
Figure 2 illustrates how a small unweighted 
graph is clustered into two regions in three 
iterations. Different classes are symbolized by 
different shades of grey.  
 
 
 
Figure 2: Clustering an 11-nodes graph with CW in 
two iterations 
 
It is possible to introduce a random mutation 
rate that assigns new classes with a probability 
decreasing in the number of iterations as described 
in (Biemann & Teresniak 2005). This showed 
having positive effects for small graphs because of 
slower convergence in early iterations.  
The CW algorithm cannot cross component 
boundaries, because there are no edges between 
nodes belonging to different components. Further, 
nodes that are not connected by any edge are 
discarded from the clustering process, which 
possibly leaves a portion of nodes unclustered.  
Formally, CW does not converge, as figure 3 
exemplifies: here, the middle node?s neighborhood 
0. 
1. 
2. 
74
consists of a tie which can be decided in assigning 
the class of the left or the class of the right nodes in 
any iteration all over again. Ties, however, do not 
play a major role in weighted graphs. 
 
Figure 3: The middle node gets the grey or the 
black class. Small numbers denote edge weights.   
 
Apart from ties, the classes usually do not 
change any more after a handful of iterations. The 
number of iterations depends on the diameter of 
the graph: the larger the distance between two 
nodes is, the more iterations it takes to percolate 
information from one to another. 
The result of CW is a hard partitioning of the 
given graph into a number of partitions that 
emerges in the process ? CW is parameter-free. It 
is possible to obtain a soft partitioning by assigning 
a class distribution to each node, based on the 
weighted distribution of (hard) classes in its 
neighborhood in a final step. 
The outcomes of CW resemble those of Min-
Cut (Wu & Leahy 1993): Dense regions in the 
graph are grouped into one cluster while sparsely 
connected regions are separated. In contrast to 
Min-Cut, CW does not find an optimal hierarchical 
clustering but yields a non-hierarchical (flat) 
partition. Furthermore, it does not require any 
threshold as input parameter and is more efficient. 
Another algorithm that uses only local contexts 
for time-linear clustering is DBSCAN as, described 
in (Ester et al 1996), needing two input parameters 
(although the authors propose an interactive 
approach to determine them). DBSCAN is 
especially suited for graphs with a geometrical 
interpretation, i.e. the objects have coordinates in a 
multidimensional space. A quite similar algorithm 
to CW is MAJORCLUST (Stein & Niggemann 
1996), which is based on a comparable idea but 
converges slower. 
2.2 Chinese Whispers as matrix operation 
As CW is a special case of Markov-Chain-
Clustering (MCL) (van Dongen, 2000), we spend a 
few words on explaining it. MCL is the parallel 
simulation of all possible random walks up to a 
finite length on a graph G. The idea is that random 
walkers are more likely to end up in the same 
cluster where they started than walking across 
clusters. MCL simulates flow on a graph by 
repeatedly updating transition probabilities 
between all nodes, eventually converging to a 
transition matrix after k steps that can be 
interpreted as a clustering of G. This is achieved by 
alternating an expansion step and an inflation step. 
The expansion step is a matrix multiplication of 
MG with the current transition matrix. The inflation 
step is a column-wise non-linear operator that 
increases the contrast between small and large 
transition probabilities and normalizes the column-
wise sums to 1. The k matrix multiplications of the 
expansion step of MCL lead to its time-complexity 
of O(k?n?).  
It has been observed in (van Dongen, 2000), 
that only the first couple of iterations operate on 
dense matrices ? when using a strong inflation 
operator, matrices in the later steps tend to be 
sparse. The author further discusses pruning 
schemes that keep only some of the largest entries 
per column, leading to drastic optimization 
possibilities. But the most aggressive sort of 
pruning is not considered: only keeping one single 
largest entry. Exactly this is conducted in the basic 
CW process. Let maxrow(.) be an operator that 
operates row-wise on a matrix and sets all entries 
of a row to zero except the largest entry, which is 
set to 1. Then the algorithm is denoted as simple as 
this: 
 
D0 = In 
for t=1 to iterations 
 Dt-1 = maxrow(Dt-1) 
 Dt  = Dt-1AG  
Figure 4: Matrix Chinese Whispers process. t is 
time step, In is the identity matrix of size n?n, AG is 
the adjacency matrix of graph G. 
 
By applying maxrow(.), Dt-1 has exactly n 
non-zero entries. This causes the time-complexity 
to be dependent on the number of edges, namely 
O(k?|E|). In the worst case of a fully connected 
graph, this equals the time-complexity of MCL.  
A problem with the matrix CW process is that it 
does not necessarily converge to an iteration-
invariant class matrix D, but rather to a pair of 
oscillating class matrices. Figure 5 shows an 
example.  
 1 
 1  1 
 1 
 2 
 2 
75
 Figure 5: oscillating states in matrix CW for an 
unweighted graph 
 
This is caused by the stepwise update of the 
class matrix. As opposed to this, the CW algorithm 
as outlined in figure 1 continuously updates D after 
the processing of each node. To avoid these 
oscillations, one of the following measures can be 
taken: 
? Random mutation: with some probability, the 
maxrow-operator places the 1 for an otherwise 
unused class 
? Keep class: with some probability, the row is 
copied from Dt-1 to Dt 
? Continuous update (equivalent to CW as 
described in section 2.1.) 
While converging to the same limits, the 
continuous update strategy converges the fastest 
because prominent classes are spread much faster 
in early iterations.  
3 Experiments with synthetic graphs 
The analysis of the CW process is difficult due to 
its nonlinear nature. Its run-time complexity 
indicates that it cannot directly optimize most 
global graph cluster measures because of their NP-
completeness (??ma and Schaeffer, 2005). 
Therefore we perform experiments on synthetic 
graphs to empirically arrive at an impression of our 
algorithm's abilities. All experiments were 
conducted with an implementation following 
figure 1. For experiments with synthetic graphs, 
we restrict ourselves to unweighted graphs, if not 
stated explicitly. 
3.1 Bi-partite cliques 
A cluster algorithm should keep dense regions 
together while cutting apart regions that are 
sparsely connected. The highest density is reached 
in fully connected sub-graphs of n nodes, a.k.a. n-
cliques. We define an n-bipartite-clique as a graph 
of two n-cliques, which are connected such that 
each node has exactly one edge going to the clique 
it, does not belong to.  
Figures 5 and 6 are n-partite cliques for n=4,10. 
 
Figure 6: The 10-bipartite clique. 
  
We clearly expect a clustering algorithm to cut 
the two cliques apart. As we operate on 
unweighted graphs, however, CW is left with two 
choices: producing two clusters or grouping all 
nodes into one cluster. This is largely dependent on 
the random choices in very early iterations - if the 
same class is assigned to several nodes in both 
cliques, it will finally cover the whole graph.  
Figure 7 illustrates on what rate this happens on n-
bipartite-cliques for varying n. 
 
Figure 7: Percentage of obtaining two clusters 
when applying CW on n-bipartite cliques 
 
It is clearly a drawback that the outcome of CW 
is non-deterministic. Only half of the experiments 
with 4-bipartite cliques resulted in separation. 
However, the problem is most dramatic on small 
graphs and ceases to exist for larger graphs as 
demonstrated in figure 7. 
3.2 Small world graphs 
A structure that has been reported to occur in an 
enormous number of natural systems is the small 
world (SW) graph. Space prohibits an in-depth 
discussion, which can be found in (Watts 1999). 
Here, we restrict ourselves to SW-graphs in 
language data. In (Ferrer-i-Cancho and Sole, 
2001), co-occurrence graphs as used in the 
experiment section are reported to possess the 
small world property, i.e. a high clustering co-
efficient and short average path length between 
76
arbitrary nodes. Steyvers and Tenenbaum (2005) 
show that association networks as well as semantic 
resources are scale-free SW-graphs: their degree 
distribution follows a power law.  A generative 
model is provided that generates undirected, scale-
free SW-graphs in the following way: We start 
with a small number of fully connected nodes. 
When adding a new node, an existing node v is 
chosen with a probability according to its degree. 
The new node is connected to M nodes in the 
neighborhood of v. The generative model is 
parameterized by the number of nodes n and the 
network's mean connectivity, which approaches 
2M for large n. 
Let us assume that we deal with natural systems 
that can be characterized by small world graphs. If 
two or more of those systems interfere, their 
graphs are joined by merging some nodes, 
retaining their edges. A graph-clustering algorithm 
should split up the resulting graph in its previous 
parts, at least if not too many nodes were merged.  
We conducted experiments to measure CW's 
performance on SW-graph mixtures: We generated 
graphs of various sizes, merged them by twos to a 
various extent and measured the amount of cases 
where clustering with CW leads to the 
reconstruction of the original parts. When 
generating SW-graphs with the Steyvers-
Tenenbaum model, we fixed M to 10 and varied n 
and the merge rate r, which is the fraction of nodes 
of the smaller graph that is merged with nodes of 
the larger graph.  
 
Figure 8: Rate of obtaining two clusters for mix-
tures of SW-graphs dependent on merge rate r.  
 
Figure 8 summarizes the results for equisized 
mixtures of 300, 3,000 and 30,000 nodes and 
mixtures of 300 with 30,000 nodes.   
It is not surprising that separating the two parts 
is more difficult for higher r. Results are not very 
sensitive to size and size ratio, indicating that CW 
is able to identify clusters even if they differ 
considerably in size ? it even performs best at the 
skewed mixtures. At merge rates between 20% and 
30%, still more then half of the mixtures are 
separated correctly and can be found when 
averaging CW?s outcome over several runs.  
3.3 Speed issues 
As formally, the algorithm does not converge, it is 
important to define a stop criterion or to set the 
number of iterations. To show that only a few 
iterations are needed until almost-convergence, we 
measured the normalized Mutual Information 
(MI)1 between the clustering in the 50th iteration 
and the clusterings of earlier iterations. This was 
conducted for two unweighted SW-graphs with 
1,000 (1K) and 10,000 (10K) nodes, M=5 and a 
weighted 7-lingual co-occurrence graph (cf. 
section 4.1) with 22,805 nodes and 232,875 edges. 
Table 1 indicates that for unweighted graphs, 
changes are only small after 20-30 iterations. In 
iterations 40-50, the normalized MI-values do not 
improve any more. The weighted graph converges 
much faster due to fewer ties and reaches a stable 
plateau after only 6 iterations. 
 
Iter 1 2 3 5 10 20 30 40 49 
1K 1 8 13 20 37 58 90 90 91 
10K 6 27 46 64 79 90 93 95 96 
7ling 29 66 90 97 99.5 99.5 99.5 99.5 99.5 
Table 1: normalized Mutual Information values for 
three graphs and different iterations in %. 
4 NLP Experiments 
In this section, some experiments with graphs 
originating from natural language data are 
presented. First, we define the notion of co-
occurrence graphs, which are used in sections 4.1 
and 4.3: Two words co-occur if they can both be 
found in a certain unit of text, here a sentence. 
Employing a significance measure, we determine 
whether their co-occurrences are significant or 
random. In this case, we use the log-likelihood 
measure as described in (Dunning 1993). We use 
the words as nodes in the graph. The weight of an 
                                                          
1
 defined for two random variables X and Y as (H(X)+H(Y)-
H(X,Y))/max(H(X),H(Y)) with H(X) entropy. A value of 0 
denotes indepenence, 1 is perfect congruence. 
77
edge between two words is set to the significance 
value of their co-occurrence, if it exceeds a certain 
threshold. In the experiments, we used sig-
nificances from 15 on. The entirety of words that 
are involved in at least one edge together with 
these edges is called co-occurrence graph (cf. 
Biemann et al 2004). 
In general, CW produces a large number of 
clusters on real-world graphs, of which the 
majority is very small. For most applications, it 
might be advisable to define a minimum cluster 
size or something alike. 
4.1 Language Separation 
This section shortly reviews the results of 
(Biemann and Teresniak, 2005), where CW was 
first described. The task was to separate a 
multilingual corpus by languages, assuming its 
tokenization in sentences.  
The co-occurrence graph of a multilingual 
corpus resembles the synthetic SW-graphs: Every 
language forms a separate co-occurrence graph, 
some words that are used in more than one 
language are members of several graphs, 
connecting them. By CW-partitioning, the graph is 
split into its monolingual parts. These parts are 
used as word lists for word-based language 
identification. (Biemann and Teresniak, 2005) 
report almost perfect performance on getting 7-
lingual corpora with equisized parts sorted apart as 
well as highly skewed mixtures of two languages.  
In the process, language-ambiguous words are 
assigned to only one language, which did not hurt 
performance due to the high redundancy of the 
task. However, it would have been possible to use 
the soft partitioning to acquire a distribution over 
languages for each word.  
4.2 Acquisition of Word Classes 
For the acquisition of word classes, we use a 
different graph: the second-order graph on 
neighboring co-occurrences. To set up the graph, a 
co-occurrence calculation is performed which 
yields significant word pairs based on their 
occurrence as immediate neighbors. This can be 
perceived as a bipartite graph, figure 9a gives a toy 
example. Note that if similar words occur in both 
parts, they form two distinct nodes. 
This graph is transformed into a second-order 
graph by comparing the number of common right 
and left neighbors for two words. The similarity 
(edge weight) between two words is the sum of 
common neighbors. Figure 9b depicts the second-
order graph derived from figure 9a and its 
partitioning by CW. The word-class-ambiguous 
word ?drink? (to drink the drink) is responsible for 
all intra-cluster edges. The hypothesis here is that 
words sharing many neighbors should usually be 
observed with the same part-of-speech and get 
high weights in the second order graph. In figure 9, 
three clusters are obtained that correspond to 
different parts-of-speech (POS).  
 
   
      (a)             (b) 
Figure 9: Bi-partite neighboring co-occurrence 
graph (a) and second-order graph on neighboring 
co-occurrences (b) clustered with CW. 
 
To test this on a large scale, we computed the 
second-order similarity graph for the British 
National Corpus (BNC), excluding the most 
frequent 2000 words and drawing edges between 
words if they shared at least four left and right 
neighbors. The clusters are checked against a 
lexicon that contains the most frequent tag for each 
word in the BNC. The largest clusters are 
presented in table 2 . 
 
size tags:count sample words 
18432 NN:17120 
AJ: 631 
secret, officials, transport, 
unemployment, farm, county, 
wood, procedure, grounds, ... 
4916 AJ: 4208 
V: 343 
busy, grey, tiny, thin, sufficient, 
attractive, vital, ... 
4192 V: 3784 
AJ: 286 
filled, revealed,  experienced, 
learned, pushed, occurred, ... 
3515 NP: 3198 
NN: 255 
White, Green, Jones, Hill, Brown, 
Lee, Lewis, Young, ... 
2211 NP: 1980 
NN: 174 
Ian, Alan, Martin, Tony, Prince, 
Chris, Brian, Harry, Andrew, 
 1 
 1 
 1  1 
 2 
 2 
 4 
 2 
 2 
 1 
1 
 1 
1 
 1 
1 
left right 
78
Christ, Steve, ... 
1855 NP: 1670 
NN: 148 
Central, Leeds, Manchester, 
Australia,  Yorkshire, Belfast, 
Glasgow, Middlesbrough,  ... 
Table 2: the largest clusters from partitioning the 
second order graph with CW. 
 
In total, CW produced 282 clusters, of which 26 
exceed a size of 100. The weighted average of 
cluster purity (i.e. the number of predominant tags 
divided by cluster size) was measured at 88.8%, 
which exceeds significantly the precision of 53% 
on word type as reported by Sch?tze (1995) on a 
related task. How to use this kind of word clusters 
to improve the accuracy of POS-taggers is outlined 
in (Ushioda, 1996).  
4.3 Word Sense Induction 
The task of word sense induction (WSI) is to find 
the different senses of a word. The number of 
senses is not known in advance, therefore has to be 
determined by the method.  
Similar to the approach as presented in (Dorow 
and Widdows, 2003) we construct a word graph. 
While there, edges between words are drawn iff 
words co-occur in enumerations, we use the co-
occurrence graph. Dorow and Widdows construct a 
graph for a target word w by taking the sub-graph 
induced by the neighborhood of w (without w) and 
clustering it with MCL. We replace MCL by CW. 
The clusters are interpreted as representations of 
word senses.  
To judge results, the methodology of (Bordag, 
2006) is adopted: To evaluate word sense 
induction, two sub-graphs induced by the 
neighborhood of different words are merged. The 
algorithm's ability to separate the merged graph 
into its previous parts can be measured in an 
unsupervised way. Bordag defines four measures:  
? retrieval precision (rP): similarity of the 
found sense with the gold standard sense 
? retrieval recall (rR): amount of words that 
have been correctly assigned to the gold 
standard sense 
? precision (P): fraction of correctly found 
disambiguations 
? recall (R): fraction of correctly found 
senses 
We used the same program to compute co-
occurrences on the same corpus (the BNC). 
Therefore it is possible to directly compare our 
results to Bordag?s, who uses a triplet-based 
hierarchical graph clustering approach. The 
method was chosen because of its appropriateness 
for unlabelled data: without linguistic pre-
processing like tagging or parsing, only the 
disambiguation mechanism is measured and not 
the quality of the preprocessing steps. We provide 
scores for his test 1 (word classes separately) and 
test 3 (words of different frequency bands). Data 
was obtained from BNC's raw text; evaluation was 
performed for 45 test words. 
 
% (Bordag, 2006) Chinese Whispers 
POS P R rP rR P R rP rR 
N 87.0 86.7 90.9 64.2 90.0 79.5 94.8 71.3 
V 78.3 64.3 80.2 55.2 77.6 67.1 87.3 57.9 
A 88.6 71.0 88.0 65.4 92.2 61.9 89.3 71.9 
Table 3: Disambiguation results in % dependent on 
word class (nouns, verbs, adjectives)  
 
% (Bordag, 2006) Chinese Whispers 
freq P R rP rR P R rP rR 
high 93.7 78.1 90.3 80.7 93.7 72.9 95.0 73.8 
med 84.6 85.2 89.9 54.6 80.7 83.8 91.0 55.7 
low 74.8 49.5 71.0 41.7 74.1 51.4 72.9 56.2 
Table 4: Disambiguation results in % dependent on 
frequency 
 
Results (tables 3 and 4) suggest that both 
algorithms arrive at about equal overall 
performance (P and R). Chinese Whispers 
clustering is able to capture the same information 
as a specialized graph-clustering algorithm for 
WSI, given the same input. The slightly superior 
performance on rR and rP indicates that CW leaves 
fewer words unclustered, which can be 
advantageous when using the clusters as clues in 
word sense disambiguation. 
5 Conclusion 
Chinese Whispers, an efficient graph-clustering 
algorithm was presented and described in theory 
and practice. Experiments with synthetic graphs 
showed that for small graphs, results can be 
inconclusive due to its non-deterministic nature. 
But while there exist plethora of clustering 
approaches that can deal well with small graphs, 
the power of CW lies in its capability of handling 
very large graphs in reasonable time. The 
79
application field of CW rather lies in size regions, 
where other approaches? solutions are intractable.  
On the NLP data discussed, CW performs 
equally or better than other clustering algorithms. 
As CW ? like other graph clustering algorithms ? 
chooses the number of classes on its own and can 
handle clusters of different sizes, it is especially 
suited for NLP problems, where class distributions 
are often highly skewed and the number of classes 
(e.g. in WSI) is not known beforehand. 
To relate the partitions, it is possible to set up a 
hierarchical version of CW in the following way: 
The nodes of equal class are joined to hyper-nodes. 
Edge weights between hyper-nodes are set 
according to the number of inter-class edges 
between the corresponding nodes. This results in 
flat hierarchies.  
In further works it is planned to apply CW to 
other graphs, such as the co-citation graph of 
Citeseer, the co-citation graph of web pages and 
the link structure of Wikipedia.  
Acknowledgements 
Thanks go to Stefan Bordag for kindly 
providing his WSI evaluation framework. Further, 
the author would like to thank Sebastian Gottwald 
and Rocco Gwizdziel for a platform-independent 
GUI implementation of CW, which is available for 
download from the author?s homepage. 
References 
F. Amblard. 2002. Which ties to choose? A survey of 
social networks models for agent-based social 
simulations. In Proc. of the 2002 SCS International 
Conference On Artificial Intelligence, Simulation 
and Planning in High Autonomy Systems, pp.253-
258, Lisbon, Portugal. 
C. Biemann, S. Bordag, G. Heyer, U. Quasthoff,  C. 
Wolff. 2004. Language-independent Methods for 
Compiling Monolingual Lexical Data, Proceedings 
of CicLING 2004, Seoul, Korea and Springer LNCS 
2945, pp. 215-228, Springer,  Berlin Heidelberg 
B. Bollob?s. 1998. Modern graph theory, Graduate 
Texts in Mathematics, vol. 184, Springer, New York 
S. Bordag. 2006. Word Sense Induction: Triplet-Based 
Clustering and Automatic Evaluation. Proceedings of 
EACL-06. Trento 
C. Biemann and S. Teresniak. 2005. Disentangling from 
Babylonian Confusion ? Unsupervised Language 
Identification. Proceedings of CICLing-2005, 
Mexico City, Mexico and Springer LNCS 3406, pp. 
762-773 
S. van Dongen. 2000. A cluster algorithm for graphs. 
Technical Report INS-R0010, National Research 
Institute for Mathematics and Computer Science in 
the Netherlands, Amsterdam. 
T. Dunning. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence, Computational 
Linguistics 19(1), pp. 61-74 
M. Ester, H.-P. Kriegel, J. Sander and X. Xu. 1996. A 
Density-Based Algorithm for Discovering Clusters in 
Large Spatial Databases with Noise. In Proceedings 
of the 2nd Int. Conf. on Knowledge Discovery and 
Datamining (KDD'96) Portland, USA, pp. 291-316.  
B. Dorow and D. Widdows. 2003. Discovering Corpus-
Specific Word Senses. In EACL-2003 Conference 
Companion (research notes and demos), pp. 79-82, 
Budapest, Hungary 
R. Ferrer-i-Cancho and R.V. Sole. 2001. The small 
world of human language. Proceedings of The Royal 
Society of London. Series B, Biological Sciences, 
268(1482):2261-2265 
H. Sch?tze. 1995. Distributional part-of-speech 
tagging. In EACL 7, pages 141?148 
J. ??ma and S.E. Schaeffer. 2005. On the np-
completeness of some graph cluster measures. 
Technical Report cs.CC/0506100, arXiv.org e-Print 
archive, http://arxiv.org/. 
B. Stein and O. Niggemann. 1999. On the Nature of 
Structure and Its Identification. Proceedings of 
WG'99, Springer LNCS 1665, pp. 122-134, Springer 
Verlag Heidelberg 
M. Steyvers, J. B. Tenenbaum. 2005. The large-scale 
structure of semantic networks: statistical analyses 
and a model of semantic growth. Cognitive Science, 
29(1). 
Ushioda, A. (1996). Hierarchical clustering of words 
and applications to NLP tasks. In Proceedings of the 
Fourth Workshop on Very Large Corpora, pp. 28-41. 
Somerset, NJ, USA 
D. J. Watts. 1999. Small Worlds: The Dynamics of 
Networks Between Order and Randomness, Princeton 
Univ. Press, Princeton, USA 
Z. Wu and R. Leahy (1993): An optimal graph theoretic 
approach to data clustering: Theory and its 
application to image segmentation. IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence 
80
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245?248,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Syntax is from Mars while Semantics from Venus!
Insights from Spectral Analysis of Distributional Similarity Networks
Chris Biemann
Microsoft/Powerset, San Francisco
Chris.Biemann@microsoft.com
Monojit Choudhury
Microsoft Research Lab India
monojitc@microsoft.com
Animesh Mukherjee
Indian Institute of Technology Kharagpur, India
animeshm@cse.iitkgp.ac.in
Abstract
We study the global topology of the syn-
tactic and semantic distributional similar-
ity networks for English through the tech-
nique of spectral analysis. We observe that
while the syntactic network has a hierar-
chical structure with strong communities
and their mixtures, the semantic network
has several tightly knit communities along
with a large core without any such well-
defined community structure.
1 Introduction
Syntax and semantics are two tightly coupled, yet
very different properties of any natural language
? as if one is from ?Mars? and the other from
?Venus?. Indeed, this exploratory work shows that
the distributional properties of syntax are quite dif-
ferent from those of semantics. Distributional hy-
pothesis states that the words that occur in the
same contexts tend to have similar meanings (Har-
ris, 1968). Using this hypothesis, one can define a
vector space model for words where every word
is a point in some n-dimensional space and the
distance between them can be interpreted as the
inverse of the semantic or syntactic similarity be-
tween their corresponding distributional patterns.
Usually, the co-occurrence patterns with respect to
the function words are used to define the syntactic
context, whereas that with respect to the content
words define the semantic context. An alternative,
but equally popular, visualization of distributional
similarity is through graphs or networks, where
each word is represented as nodes and weighted
edges indicate the extent of distributional similar-
ity between them.
What are the commonalities and differences be-
tween the syntactic and semantic distributional
patterns of the words of a language? This study is
an initial attempt to answer this fundamental and
intriguing question, whereby we construct the syn-
tactic and semantic distributional similarity net-
work (DSN) and analyze their spectrum to un-
derstand their global topology. We observe that
there are significant differences between the two
networks: the syntactic network has well-defined
hierarchical community structure implying a sys-
tematic organization of natural classes and their
mixtures (e.g., words which are both nouns and
verbs); on the other hand, the semantic network
has several isolated clusters or the so called tightly
knit communities and a core component that lacks
a clear community structure. Spectral analysis
also reveals the basis of formation of the natu-
ral classes or communities within these networks.
These observations collectively point towards a
well accepted fact that the semantic space of nat-
ural languages has extremely high dimension with
no clearly observable subspaces, which makes the-
orizing and engineering harder compared to its
syntactic counterpart.
Spectral analysis is the backbone of several
techniques, such as multi-dimensional scaling,
principle component analysis and latent semantic
analysis, that are commonly used in NLP. In re-
cent times, there have been some work on spec-
tral analysis of linguistic networks as well. Belkin
and Goldsmith (2002) applied spectral analysis to
understand the struture of morpho-syntactic net-
works of English words. The current work, on
the other hand, is along the lines of Mukherjee et
al. (2009), where the aim is to understand not only
the principles of organization, but also the global
topology of the network through the study of the
spectrum. The most important contribution here,
however, lies in the comparison of the topology
of the syntactic and semantic DSNs, which, to the
best of our knowledge, has not been explored pre-
viously.
245
2 Network Construction
The syntactic and semantic DSNs are constructed
from a raw text corpus. This work is restricted to
the study of English DSNs only
1
.
Syntactic DSN: We define our syntactic net-
work in a similar way as previous works in unsu-
pervised parts-of-speech induction (cf. (Sch?utze,
1995; Biemann, 2006)): The most frequent 200
words in the corpus (July 2008 dump of English
Wikipedia) are used as features in a word window
of ?2 around the target words. Thus, each target
word is described by an 800-dimensional feature
vector, containing the number of times we observe
one of the most frequent 200 words in the respec-
tive positions relative to the target word. In our
experiments, we collect data for the most frequent
1000 and 5000 target words, arguing that all syn-
tactic classes should be represented in those. A
similarity measure between target words is defined
by the cosine between the feature vectors. The
syntactic graph is formed by inserting the target
words as nodes and connecting nodes with edge
weights equal to their cosine similarity if this sim-
ilarity exceeds a threshold t = 0.66.
Semantic DSN: The construction of this net-
work is inspired by (Lin, 1998). Specifically,
we parsed a dump of English Wikipedia (July
2008) with the XLE parser (Riezler et al, 2002)
and extracted the following dependency relations
for nouns: Verb-Subject, Verb-Object, Noun-
coordination, NN-compound, Adj-Mod. These
lexicalized relations act as features for the nouns.
Verbs are recorded together with their subcatego-
rization frame, i.e. the same verb lemmas in dif-
ferent subcat frames would be treated as if they
were different verbs. We compute log-likelihood
significance between features and target nouns (as
in (Dunning, 1993)) and keep only the most signif-
icant 200 features per target word. Each feature f
gets a feature weight that is inversely proportional
to the logarithm of the number of target words it
applies on. The similarity of two target nouns is
then computed as the sum of the feature weights
they share. For our analysis, we restrict the graph
to the most frequent 5000 target common nouns
and keep only the 200 highest weighted edges per
target noun. Note that the degree of a node can
1
As shown in (Nath et al, 2008), the basic structure
of these networks are insensitive to minor variations in the
parameters (e.g., thresholds and number of words) and the
choice of distance metric.
Figure 1: The spectrum of the syntactic and se-
mantic DSNs of 1000 nodes.
still be larger than 200 if this node is contained in
many 200 highest weighted edges of other target
nouns.
3 Spectrum of DSNs
Spectral analysis refers to the systematic study of
the eigenvalues and eigenvectors of a network. Al-
though here we study the spectrum of the adja-
cency matrix of the weighted networks, it is also
quite common to study the spectrum of the Lapla-
cian of the adjacency matrix (see for example,
Belkin and Goldsmith (2002)). Fig. 1 compares
the spectrum of the syntactic and semantic DSNs
with 1000 nodes, which has been computed as fol-
lows. First, the 1000 eigenvalues of the adjacency
matrix are sorted in descending order. Then we
compute the spectral coverage till the ith eigen-
value by adding the squares of the first i eigenval-
ues and normalizing it by the sum of the squares
of all the eigenvalues - a quantity also known as
the Frobenius norm of the matrix.
We observe that for the semantic DSN the first
10 eigenvalues cover only 40% of the spectrum
and the first 500 together make up 75% of the
spectrum. On the other hand, for the syntactic
DSN, the first 10 eigenvalues cover 75% of the
spectrum while the first 20 covers 80%. In other
words, the structure of the syntactic DSN is gov-
erned by a few (order of 10) significant principles,
whereas that of the semantic DSN is controlled by
a large number of equally insignificant factors.
The aforementioned observation has the fol-
lowing alternative, but equivalent interpretations:
(a) the syntactic DSN can be clustered in lower
dimensions (e.g., 10 or 20) because, most of
the rows in the matrix can be approximately ex-
pressed as a linear combination of the top 10 to 20
246
Figure 2: Plot of corpus frequency based rank vs.
eigenvector centrality of the words in the DSNs of
5000 nodes.
eigenvectors. Furthermore, the graceful decay of
the eigenvalues of the syntactic DSN implies the
existence of a hierarchical community structure,
which has been independently verified by Nath et
al. (2008) through analysis of the degree distribu-
tion of such networks; and (b) a random walk con-
ducted on the semantic DSN will have a high ten-
dency to drift away very soon from the semantic
class of the starting node, whereas in the syntactic
DSN, the random walk is expected to stay within
the same syntactic class for a long time. There-
fore, it is reasonable to advocate that characteriza-
tion and processing of syntatic classes is far less
confusing than that of the semantic classes ? a fact
that requires no emphasis.
4 Eigenvector Analysis
The first eigenvalue tells us to what extent the
rows of the adjacency matrix are correlated and
therefore, the corresponding eigenvector is not a
dimension pointing to any classificatory basis of
the words. However, as we shall see shortly, the
other eigenvectors corresponding to the signifi-
cantly high eigenvalues are important classifica-
tory dimensions.
Fig 2 shows the plot of the first eigenvector
component (aka eigenvector centrality) of a word
versus its rank based on the corpus frequency. We
observe that the very high frequency (i.e., low
rank) nodes in both the networks have low eigen-
vector centrality, whereas the medium frequency
nodes display a wide range of centrality values.
However, the most striking difference between the
networks is that while in the syntactic DSN the
centrality values are approximately normally dis-
tributed for the medium frequency words, the least
frequent words enjoy the highest centrality for the
semantic DSN. Furthermore, we observe that the
most central nodes in the semantic DSN corre-
spond to semantically unambiguous words of sim-
ilar nature (e.g., deterioration, abandonment, frag-
mentation, turmoil). This indicates the existence
of several ?tightly knit communities consisting of
not so high frequency words? which pull in a sig-
nificant fraction of the overall centrality. Since
the high frequency words are usually polysemous,
they on the other hand form a large, but non-
cliqueish structure at the core of the network with
a few connections to the tightly knit communities.
This is known as the tightly knit community ef-
fect (TKC effect) that renders very low central-
ity values to the ?truly? central nodes of the net-
work (Lempel and Moran, 2000). The structure
of the syntactic DSN, however, is not governed by
the TKC effect to such an extreme extent. Hence,
one can expect to easily identify the natural classes
of the syntactic DSN, but not its semantic counter-
part.
In fact, this observation is further corroborated
by the higher eigenvectors. Fig. 3 shows the plot
of the second eigenvector component versus the
fourth one for the two DSNs consisting of 5000
words. It is observed that for the syntactic net-
work, the words get neatly clustered into two sets
comprised of words with the positive and negative
second eigenvector components. The same plot
for the semantic DSN shows that a large number of
words have both the components close to zero and
only a few words stand out on one side of the axes
? those with positive second eigenvector compo-
nent and those with negative fourth eigenvector
component. In essence, none of these eigenvec-
tors can neatly classify the words into two sets ?
a trend which is observed for all the higher eigen-
vectors (we conducted experiments for up to the
twentieth eigenvector).
Study of the individual eignevectors further re-
veals that the nodes with either the extreme pos-
itive or the extreme negative components have
strong linguistic correlates. For instance, in the
syntactic DSN, the two ends of the second eigen-
247
Figure 3: Plot of the second vs. fourth eigenvector
components of the words in the DSNs.
vector correspond to nouns and adjectives; one of
the ends of the fourth, fifth, sixth and the twelfth
eigenvectors respectively correspond to location
nouns, prepositions, first names and initials, and
verbs. In the semantic DSN, one of the ends of
the second, third, fourth and tenth eigenvectors
respectively correspond to professions, abstract
terms, food items and body parts. One would ex-
pect that the higher eigenvectors (say the 50
th
one)
would show no clear classificatory basis for the
syntactic DSN, while for the semantic DSN those
could be still associated with prominent linguistic
correlates.
5 Conclusion and Future Work
Here, we presented some initial investigations into
the nature of the syntactic and semantic DSNs
through the method of spectral analysis, whereby
we could observe that the global topology of the
two networks are significantly different in terms
of the organization of their natural classes. While
the syntactic DSN seems to exhibit a hierarchi-
cal structure with a few strong natural classes and
their mixtures, the semantic DSN is composed of
several tightly knit small communities along with
a large core consisting of very many smaller ill-
defined and ambiguous sets of words. To visual-
ize, one could draw an analogy of the syntactic
and semantic DSNs respectively to ?crystalline?
and ?amorphous? solids.
This work can be furthered in several directions,
such as, (a) testing the robustness of the findings
across languages, different network construction
policies, and corpora of different sizes and from
various domains; (b) clustering of the words on the
basis of eigenvector components and using them in
NLP applications such as unsupervised POS tag-
ging and WSD; and (c) spectral analysis of Word-
Net and other manually constructed ontologies.
Acknowledgement
CB and AM are grateful to Microsoft Research
India, respectively for hosting him while this re-
search was conducted, and financial support.
References
M. Belkin and J. Goldsmith 2002. Using eigenvec-
tors of the bigram graph to infer morpheme identity.
In Proceedings of the ACL-02Workshop onMorpho-
logical and Phonological Learning, pages 4147, As-
sociation for Computational Linguistics.
Chris Biemann 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL-06 Student Re-
search Workshop.
Ted Dunning 1993. Accurate methods for the statis-
tics of surprise and coincidence. In Computational
Linguistics 19, 1, pages 61?74
Z.S. Harris 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
R. Lempel and S. Moran 2000. The stochastic ap-
proach for link-structure analysis (SALSA) and the
TKC effect. In Computer Networks, 33, pages 387-
401
Dekang Lin 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING?98.
Animesh Mukherjee, Monojit Choudhury and Ravi
Kannan 2009. Discovering Global Patterns in Lin-
guistic Networks through Spectral Analysis: A Case
Study of the Consonant Inventories. In The Pro-
ceedings of EACL 2009, pages 585-593.
Joydeep Nath, Monojit Choudhury, Animesh Mukher-
jee, Christian Biemann and Niloy Ganguly 2008.
Unsupervised parts-of-speech induction for Bengali.
In The Proceedings of LREC?08, ELRA.
S. Riezler, T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell, M. Johnson 2002. Parsing the Wall Street
Journal using a lexical-functional grammar and dis-
criminative estimation techniques. In Proceedings
of the 40th Annual Meeting of the ACL, pages 271-
278.
Hinrich Sch?utze 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141-148.
248
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1435?1446, Dublin, Ireland, August 23-29 2014.
Combining Supervised and Unsupervised Parsing
for Distributional Similarity
Martin Riedl, Irina Alles and Chris Biemann
FG Language Technology
Computer Science Department, Technische Universit?at Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biem}@cs.tu-darmstadt.de, ialles@gmx.de
Abstract
In this paper, we address the role of syntactic parsing for distributional similarity. On the one
hand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers.
On the other hand, we explore whether single unsupervised parsers, or their combination, can
contribute to better distributional similarities, or even replace supervised parsing as a prepro-
cessing step for word similarity. We evaluate distributional thesauri against manually created
taxonomies both for English and German for five unsupervised parsers. While for English, a
supervised parser is the best single parser in this evaluation, we find an unsupervised parser to
work best for German. For both languages, we show significant improvements in word similarity
when combining features from supervised and unsupervised parsers. To our knowledge, this is
the first work where unsupervised parsers are systematically evaluated extrinsically in a seman-
tic task, and the first work to show that unsupervised parsing can complement and even replace
supervised parsing, when used as a pre-processing feature.
1 Introduction
While the field has seen increased interest in automatically inducing syntactic structures from raw or part-
of-speech (POS) tagged text, the evaluation of unsupervised data-driven parsers has almost exclusively
been conducted either by introspection or by automatic comparison to treebanks. It might be due to
comparatively low scores on reproducing a treebank?s syntactic annotation that hardly anyone has yet
attempted to use the output of unsupervised parsers for an NLP task other than parsing itself.
A further complication with unsupervised parsers ? be it dependency parsers, constituency parsers
or combinatory categorial grammar parsers ? is that the categories induced by such parsers cannot be
straightforwardly mapped to linguistically-inspired categories as defined in a treebank. But also when
considering only unlabeled syntactic annotations, an unsupervised parser is hardly to blame if it does not
adhere to sometimes arbitrary conventions: e.g. for dependencies, it is not a priori clear how to connect
auxiliary and main verbs, where to attach the complementizer of subordinate clauses, how to represent
a conjunction and its conjuncts, how to relate the preposition and the nominal in prepositional phrases,
and how to handle punctuation, cf. Nivre and K?ubler (2006), Schwartz et al. (2011).
When it comes to utilizing syntactic structures, however, it is more important that they are consistent
across different sentences than that they adhere to specific syntactic theories and conventions. Here, we
choose a task that makes only intermediary use of syntactic structures: we employ unsupervised parsing
for preprocessing corpora for the purpose of computing distributional similarities. Since it is generally
accepted (e.g. (Lin, 1997; Curran and Moens, 2002)), that syntactic preprocessing plays an important
role for the quality of distributional thesauri, and comparing words along their syntactic contexts does
rely on the existence of such a structure rather than its actual representation, we believe that distribu-
tional similarities are an excellent test bed for addressing the following two research questions: (1) How
do unsupervised parsers compare to supervised parsers when used as feature providers for building Dis-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1435
tributional Thesauri (DTs) in comparison to supervised parsers? (2) Can the combination of syntactic
parsers increase DT quality?
2 Related Work
2.1 Unsupervised Parser Evaluation
As with other unsupervised approaches, the premise of unsupervised induction of syntactic structure
is to alleviate the bottleneck of expensive manual annotations for improving NLP applications. For
grammar induction, the potential is extremely high due to the complexity of the subject matter: treebanks
belong to the most work-intensive NLP datasets. On the other hand, this complexity is hard to grasp for
unsupervised systems, which is probably the reason why unsupervised parsing technology is still in its
infancy, despite more than a decade of work on this topic.
One of the early works inducing structure from raw sentences and yielding better performance than
a random baseline was achieved by van Zaanen and of Leeds. School of Computer Studies (2001),
who used an Alignment Based Learning (ABL) approach. This algorithm compares all sentences of
a given set and considers matching sequences as constituents. Klein and Manning (2002) presented
another approach focusing on constituent sequences called the Constituent-Context Model (CCM). It is
an EM-based iterative approach that makes use of the linguistic phenomenon that long constituents often
have shorter representations of the same grammatical function that occur in similar contexts. A hybrid
approach combining CCM with a dependency model, called Dependency Model with Valence (DMV),
shows even better performance and is the first unsupervised system to outperform the right-branching
baseline (Klein and Manning, 2004). A great number of recent works are based on DMV, such as the
system by Headden III et al. (2009), who improved DMV by adding lexical information, and Gillenwater
et al. (2010) who added posterior regularization during the training process. Bod (2007) takes a slightly
different direction by following an ?all subtrees approach?, where all possible binary trees are generated
for each sentence. It generates all possible binary trees for each sentence. The parse of a new sentence is
determined by selecting the most probable tree based on the previously accumulated subtree frequencies.
Most of the evaluation of these parsers was performed against a treebank, offering manually annotated
and linguistically motivated parse trees. Schwartz et al. (2011) underline the fact that treebanks contain
linguistically problematic annotations, cases without linguistic consensus, such as the decision on the
head of a verb phrase or a sequence of nouns. They show that the neglectance of these cases has a
significant but unjustified negative influence on the evaluation outcomes and propose a new measure,
Neutral Edge Direction (NED), which alleviates this problem. Bod (2007) argues that parser evaluation
against a treebank favors supervised approaches and therefore measures the parser quality on the outcome
of a syntax based Machine Translation (MT) task where the dependency parsers are evaluated as language
models. In Motazedi et al. (2012), a single unsupervised parser is evaluated in an extrinsic evaluation
for realisation ranking, and does not compare favorably against a supervised parser. Other extrinsic
evaluations with supervised dependency parsers have been performed in information extraction systems
(Miyao et al., 2008; Buyko and Hahn, 2010) or semantic role labeling (Johansson and Nugues, 2008).
2.2 Evaluating Distributional Similarity
Distributional thesauri have been evaluated both extrinsically and intrinsically. Extrinsic evaluations have
been performed e.g. for automatic set expansion (Pantel et al., 2009) or phrase polarity identification
(Goyal and Daum?e, 2011). In this work, we will conduct an intrinsic evaluation, which is more common
for the evaluation of DTs and lexical semantic similarity. Lin (1997; 1998) introduced two measures
using WordNet (Miller, 1995) and Roget?s Thesaurus. Using WordNet, he defines context (synsets a
word occurs in Wordnet or subsets when using Roget?s Thesaurus) and then builds a gold standard
thesaurus using a similarity measure on these contexts. Then he evaluates his automatically computed
Distributional Thesaurus (DT) with respect to the gold standard thesauri. Weeds et al. (2004) evaluate
various similarity measures based on 1000 frequent and 1000 infrequent target terms. Curran (2004)
created a gold standard thesaurus by manually extracting entries from several English thesauri for 70
words. His automatically generated DTs are evaluated against this gold standard thesaurus. All these
1436
systems employ context representations based on syntactic parsing for computing word similarity.
We are going to use a comparatively simple WordNet-based measure, which calculates the similarity
between two terms using the WordNet::Similarity path measure (Pedersen et al., 2004), and averages
path scores between a target term and its n most similar terms. The score between two terms is inversely
proportional to the shortest path between all the synsets of both terms. If two terms share a synset,
the highest possible score of one is assigned. The score is 0.5 for terms that stand in a direct hypernym
relation, and so on. While the absolute scores are hard to interpret due to inhomogeneity in the granularity
of WordNet, they are well-suited for relative comparison when operating on the same set of target terms.
The evaluation in this work is performed by comparing the average score of the top ten entries in the
DT for each of the target terms and report separately on frequent and rare words. Riedl and Biemann
(2013) also show that the results, using the WordNet based approach, are highly correlated to the results
observed with Curran?s approach using a manually created thesaurus. This justifies the usage of manually
created taxonomies for this evaluation.
3 Methodology
3.1 Parsers
In our evaluation, we use five unsupervised parsers, which we will describe briefly. They have been
selected to span several paradigms of unsupervised syntax induction, and due to software availability.
Gillenwater et al. (2010)
1
use a model based on the DMV (Klein and Manning, 2004) and improve
performance by adding sparsity biases on dependency types. They assume a corpus annotated with POS
tags. The aim of this bias is to limit unique head-dependent tag pairs, which is achieved by a constraint
on model posteriors during the learning process.
The work of Marecek and Straka (2013)
2
is another enhancement of the DMV and is subsequently
referred to as Unsupervised Dependency Parser (UDP). It additionally uses prior knowledge in the form
of stop estimates that are computed on a large raw corpus using the reducibility principle: a sequence
of words is considered as reducible if a word can be removed from the phrase and the remaining part
appears another time in the corpus. The assumed property, that the first word of a reducible sequence
does not have any left children and the last word of this sequence does not have any right children, is
used for the calculation of such stop estimates. The authors show that estimates computed on a large
corpus such as Wikipedia can be used for the parsing of new text.
Bisk and Hockenmaier (2013) use an EM approach to induce a Combinatory Categorial Grammar
(CCG), based on very general linguistic assumptions. It creates a model that can be used to parse un-
seen data. The algorithm requires a corpus, previously assigned with POS tags, in order to be able to
distinguish between word classes (mainly to find the verb), and employs general knowledge such as that
sentences are headed by verbs. Further language-specific properties are induced from the training data.
Seginer (2007)
3
takes an incremental parsing and learning approach. It operates directly on the plain
text without the need for POS tags, by using Common Cover Links (CCL), which can be directly con-
verted to dependency arcs. This parser learns during parsing and can be used without a prior learning
step. This should result in increased parsing quality towards later stages, which suggests several passes
over the training data. The obtained model can then be reused to parse unseen data.
The approach of S?gaard (2012) is different from all other approaches discussed here: This algorithm
does not require any training data and can operate with or without POS tags. For this reason, it can be
applied to arbitrary amounts of data, since it operates sentence-wise without memorizing previous inputs,
and produces non-projective dependency parses. The words of a phrase are ordered by centrality and a
parse is determined by the ranking of a parsing algorithm, which uses general linguistic knowledge for
grammar induction. This knowledge is inspired by the rules of Naseem et al. (2010), and the approach
has been tuned (once and for all, for all languages) on development data from the Penn Treebank.
1
http://code.google.com/p/pr-toolkit/
2
http://ufal.mff.cuni.cz/udp/
3
http://www.seggu.net/ccl/
1437
Figure 1: Parses for an example sentence for several parsers. Here, Bisk?s parser looks most similar to
the parses extracted from the Stanford parser. Gillenwater and UDP seem to have some problems with
the full stop. S?gaards parser mostly connects neighbors.
Baseline S?gaard Gillenwater UDP Bisk Seginer Seginer
English 53.2 59.9 64.4 55.4 70.3 55.6 (WSJ 40) 74.2 (WSJ 10)
German 33.7 57.6 35.7 52.4 68.4 38.2 (Negra 40) 48.0 (Negra 10)
Table 1: Unlabeled accuracy values of different unsupervised parsers based on the CoNLL-X shared task
(Buchholz and Marsi, 2006). Seginers results show F-measure values for the Negra and the WSJ corpus,
used with maximum sentence of lengths of 10 and 40.
An example sentence and the according parses coming from the 10M model, except for UDP, where
the 1M model is used (cf. Table 2 in Section 4.3.1), are shown in Figure 1.
Table 1 reports the accuracy of four parsers for the English and the German treebanks from the CoNLL-
X shared task (Buchholz and Marsi, 2006) predicting unlabeled dependency parses for sentences with
length equal and smaller than 10 tokens. Seginer reports only F-scores for WSJ and Negra considering
sentences with a maximum length of 10 and 40. The best baselines reported in Canisius et al. (2006) are
a left branching method for English and a nearest neighbor branching method for German, which is a
combination of left and right branching.
3.2 Computing Distributional Thesauri
The extraction of context features, used to calculate similarities between terms, is performed in accor-
dance with the generic scheme proposed in (Biemann and Riedl, 2013): A (typed or untyped) parser
arc is split into term and context feature, which consists of the edge direction and label (if any), and the
connected term. Similarity between terms is subsequently computed on the overlap of their most salient
context features. We represent the term t and the context feature c as a pair < t, c > and extract a depen-
dency triple (or dependency pair, as most unsupervised dependency parsers do not label the edges). For
the dependency between I and gave (nsub;gave;I) in I gave her the book, terms and context features
would look like <gave,(nsub,I,@)> and <I,(nsub,@,gave)>. In this example, the term gave
is characterized by the context information that I is its nominal subject, and term I is characterized by
being the subject of gave. We build distributional thesauri using the JoBimText
4
open-source framework.
This framework scales to large data and has proven to outperform other methods, when using large data
(Riedl and Biemann, 2013). The computation of the distributional thesaurus within this framework is
following the MapReduce paradigm and scales to very large corpora. This is achieved by applying a
significance measure between term and context feature, retaining only the most salient 1000 context fea-
tures per term, and computing the cardinality of the set overlap between the respective context features
4
www.jobimtext.org, (Biemann and Riedl, 2013)
1438
per term, which defines the similarity between terms. Per term, the most similar terms are subsequently
ranked, resulting in a distributional thesaurus as introduced by Lin (1997).
4 Evaluation
We report experimental results on German and English corpora. Both corpora are compiled from 10
million sentences (about 2 Gigawords) each from the Leipzig Corpora Collection
5
, randomly sampled
from online newspapers. The semantic similarity in English DTs is assessed using WordNet 3.1 as a
lexical resource, as proposed by Riedl and Biemann (2013). For evaluating the German DTs, we replace
WordNet by its German counterpart, GermaNet 8 (Hamp and Feldweg, 1997). We report results sepa-
rately for frequent and infrequent targets and average the path scores for the most similar 10 words per
entry. The evaluation of the English DTs is performed using 1000 frequent and 1000 infrequent nouns,
as previously employed by Weeds et al. (2004). These nouns are randomly sampled from the British
National Corpus (BNC) and all occur in WordNet. For the evaluation of German DTs, we randomly
selected 1000 frequent and 1000 infrequent nouns from our German corpus that all occur in GermaNet.
4.1 Experimental Settings
The DTs are calculated using the dependencies from the unsupervised parsers, one at a time. To show
the impact of corpus size, we down-sampled our corpora, and used 1 million (1M), 100,000 (100K) and
10,000 (10K) sentences (raw or automatically POS-tagged with the TreeTagger
6
) for training/inducing
the parsers. Not all parsers were able to deal with the large training sets in feasible runtime, which might
either be due to their computational complexity or their implementation. While it would be preferable to
keep the corpus size for DT computations constant, this was not possible since some of our unsupervised
parsers cannot be applied to unseen text. Hence, we decided to report DT quality results for two setups:
Setup A uses the same data for training the parsers and the DT computation. Setup B uses the full
corpus of 10M sentences for DT computation, parsed with unsupervised parsers induced on differently
sized corpus samples. We feel that Setup B is better reflecting the possible utilization of unsupervised
parsers for semantic similarity, since DT quality is known to increase with corpus size. However, we
still wanted to assess the quality of parsers that cannot be operated on unseen text in their current state
of development.
4.2 Four Baselines
We compare the results of unsupervised parsers against four baselines. As a lower-bound baseline, we
use a random dependency parser that connects each word in a sentence with a randomly chosen other
word. As a supervised upper-bound baseline, we use Stanford collapsed dependencies (Marneffe et al.,
2006) for the English data and dependencies coming from the Mate tools (Bohnet, 2010) for the German
corpus. Finally, to gauge whether the potential of unsupervised parsing to model long-range depen-
dencies ? as opposed to local n-gram contexts ? lead to better distributional similarities, we use word
bigrams and trigrams as n-gram-based systems. The bigram system simulates left- and right-branching.
We characterize the word in the first and in the second position of two neighboring words, which re-
sults to the following term feature pairs according to the example in Section 3.2: <I,(@,gave)> and
<gave,(I,@)>. Using the trigram, we characterize the word in the second position with the context
feature formed by the pair of words in first and third position. The term-feature pair for gave would be
<gave, (I,@,her)>.
While we expect the scores of any reasonable unsupervised parser to fall somewhere between the
lower bound and the upper bound when compared in the same setting, the n-gram baselines serve as a
measure for whether it is worth the trouble to induce and run the unsupervised parser for our evaluation
application, as opposed to relying on an arguably simpler system for this purpose.
5
corpora.uni-leipzig.de, (Richter et al., 2006)
6
www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/, (Schmid, 1997)
1439
4.3 Results
4.3.1 Single Parser Results for English
We summarize the results for the English evaluation for Setup A and Setup B in Table 2. All unsu-
pervised parsers beat the random baseline in all setups, with higher improvements observed using more
training data, which somewhat validates their approaches. Also, more data for DT computation results
in higher similarity scores, and rare words generally receive lower scores on average, which is expected
and validates the DT computation framework.
10k 100k 1M 10M
Parser freq rare freq rare freq rare freq rare
Setup A
Random 0.115 0.029 0.128 0.082 0.145 0.103 0.159 0.113
Trigram 0.133 0.020 0.179 0.082 0.200 0.120 0.236 0.151
Bigram 0.140 0.029 0.173 0.088 0.208 0.129 0.246 0.159
Stanford 0.151 0.028 0.209 0.128 0.261 0.176 0.280 0.209
Seginer 0.136 0.027 0.176 0.085 0.211 0.127 0.240 0.155
Gillenwater 0.135 0.026 0.159 0.077 0.195 0.117 0.223 0.141
S?gaard 0.120 0.027 0.147 0.083 0.185 0.117 0.227 0.144
UDP 0.127 0.017 0.169 0.063 0.204 0.119 * *
Bisk 0.118 0.017 * * * * * *
Setup B
Seginer 0.200 0.063 0.236 0.139 0.241 0.156 0.240 0.155
Gillenwater 0.220 0.140 0.221 0.142 0.221 0.141 0.223 0.141
S?gaard 0.227 0.144 0.227 0.144 0.227 0.144 0.227 0.144
Bisk 0.220 0.139 * * * * * *
Table 2: Setup A English: Parser induction and DT computation on the same corpus. Wordnet path
scores averaged on top 10 similar words, for 1000 frequent and 1000 rare nouns. A * denotes that the
evaluation failed because of computational constraints. Setup B English: Parser induction on different
corpus sizes, and DT computation on 10M sentences.
In comparison to the n-gram baselines, only the parser by Seginer yields a higher score for frequent
words and 1M sentences training in Setup A. However, the difference is very small and is confirmed on
the 10M sentences only in comparison to the Trigram baseline. It seems that Seginer?s training procedure
saturates somewhere between 100K and 1M sentences, and shows even slightly worse performance on
10M sentences of training in Setup B. All parsers do not seem to be particularly useful as preprocessing
steps for DT computation, since better similarity can consistently be reached by context features based
on bigram statistics.
Comparing the unsupervised parsers, we note that Seginer?s approach consistently scores highest in
Setup A, while UDP comes in second for frequent words but not for rare words. While Gillenwater?s
approach reaches comparably high scores for small corpora in Setup A, it is beaten by S?gaard?s no-
training approach for larger corpora: It seems that Gillenwater?s training procedure can hardly make use
of additional training, which is shown in Setup B, where practically no differences are observed between
10K and 10M sentences of parser training. Differences in Setup A are thus solely due to increased corpus
size for DT computation for the Gillenwater experiments.
UDP did not finish parsing 10M sentences after running for 157 days, and it is not trivial to disable
its update procedure, which is why we could not include UDP in Setup B. Bisk?s parser is a special
case in this evaluation, since it only selects sentences shorter than 15 tokens for training, and hence was
effectively trained on a 5400 sentence subset of the 10K corpus. While we did not manage to train it on
larger corpora, we could apply this model on 10M sentences in Setup B, where it lands slightly below
the no-training S?gaard parser, but clearly above Seginer?s approach for 10K training.
4.3.2 Single Parser Results for German
A different picture is drawn for the German evaluation (see Setup A in Table 3). Comparing the results
of the unsupervised parsers, Seginer?s parser does not only outperform the trigram and bigram baseline
for frequent nouns but also the supervised Mate parser for all corpus sizes. Yet, the improvements over
1440
the Mate parser are not significant for all results using a paired t-test
7
. Also, S?gaards parser exceeds the
trigram and bigram baseline for 10 million sentences. The remaining unsupervised parsers can beat the
random baseline for frequent nouns but none of the n-gram baselines. Again we are not able to parse the
10 million sentences using UDP and also Gillenwater?s parser failed, parsing this corpus. Comparing the
baselines in Setup A (see Table 3), we observe a difference between the sophisticated baselines and the
random baseline only for frequent words.
10k 100k 1M 10M
Parser freq rare freq rare freq rare freq rare
Setup A
Random 0.097 0.002 0.108 0.010 0.123 0.051 0.143 0.077
Trigram 0.102 0.002 0.130 0.014 0.159 0.067 0.179 0.086
Bigram 0.112 0.003 0.130 0.009 0.163 0.053 0.192 0.082
Mate 0.111 0.004 0.126 0.014 0.170 0.027 0.204 0.090
Seginer 0.113? 0.002 0.137? 0.012 0.171 0.068 0.208 0.091
Gillenwater 0.104 0.002 0.118 0.009 0.132 0.040 * *
S?gaard 0.104 0.002 0.123 0.010 0.161 0.054 0.193 0.077
UDP 0.107 0.001 0.129 0.004 0.151 0.021 * *
Bisk 0.101 0.002 * * * * * *
Setup B
Seginer 0.153 0.004 0.186 0.021 0.200 0.092 0.208 0.091
Gillenwater 0.189 0.080 0.190 0.082 0.189 0.080 * *
S?gaard 0.193 0.077 0.193 0.077 0.193 0.077 0.193 0.077
Bisk 0.185 0.069 * * * * * *
Table 3: Setup A and B for German corpora.
Furthermore, we see that the supervised Mate parser results in worse scores for the frequent nouns
using the 10k and 100k dataset in comparison to the bigram baseline. This could be attributed to the
heavier tail in German?s word frequency distribution, which results in sparser context features for small
data
8
. For the 1M and 10M datasets, the supervised parser yields the best similarities for frequent nouns.
The results for Setup B for the German corpora are shown at the bottom in Table 3. We observe
similar trends to the ones for the English data: using more data for the training does not seem to help the
performance of Gillenwater?s algorithm. Noticeable is the increase of Seginer?s results for rare words as
training data size increases. Seginer?s algorithm even beats both n-gram baselines for the 10M corpus
when trained only on 1 million sentences.
4.3.3 Combining Parsers for DT Quality Improvement
To clarify the best practice for building a DT of high quality, we combine different parsers: the two
best-performing unsupervised parsers (S?gaard?s and Seginer?s), the baselines and the supervised parser.
Additionally, these two parsers where the only ones which could be applied to the largest dataset for both
languages.
For English (see Table 4), we observe a boost in performance when combining unsupervised parsers.
Combining the supervised Stanford parser with the bigram and the trigram baselines also leads to a sig-
nificant improvement (p < 0.01)
9
over the Stanford parser alone, which is about the same as combining
the supervised parser with the two unsupervised parsers, and combining all five types of features for
DT construction. Overall, a relative improvement of 3.5% on the average WordNet::Path measure for
frequent words and a relative 4% improvement for rare words is obtained over the Stanford parser alone.
The results for German (see Table 5) show a similar trend. It is remarkable that merging the two un-
supervised parsers already outperforms the supervised Mate parser significantly
9
with p < 0.01 (6.7%
for frequent and 8% relative improvement for rare words). The combination of the supervised and unsu-
pervised parsers again leads to further improvement, which is also significant over the supervised parser
alone, and again, adding the bigram and trigram baselines to the three parsers does not help.
7
Significant improvements (p < 0.01) against the Mate parser are marked with the symbol ? in Table 3 for frequent nouns.
8
Within the 10M sentences, there are 22 million word types in the German corpus and 10 million word types in the English
corpus.
9
We use a paired t-test to compare the DTs built using the supervised parser and the combinations.
1441
Parser frequent rare
Stanford (supervised) 0.280 0.209
Seginer 0.240 0.155
S?gaard 0.227 0.144
Seginer & S?gaard 0.248 0.162
Stanford & Bigram & Trigram 0.290? 0.217?
Stanford & Seginer & S?gaard 0.291? 0.217?
Stanford & Seginer & S?gaard &
Bigram & Trigram
0.290? 0.218?
Table 4: Combinations of different parsers
for computing English thesauri. The cross (?)
indicates significant improvements over the
supervised parser.
Parser frequent rare
Mate (supervised) 0.204 0.090
Seginer 0.208 0.091
S?gaard 0.193 0.077
Seginer & S?gaard 0.218? 0.097?
Mate & Bigram & Trigram 0.204 0.091
Mate & Seginer & S?gaard 0.222? 0.100?
Mate & Seginer & S?gaard &
Bigram & Trigram
0.222? 0.100?
Table 5: Combinations of different parsers
for computing German thesauri
4.3.4 Discussion
Overall, it is surprising how well S?gaard?s parser performs in comparison to others on this task, since it
neither uses training nor relies on POS tags. This hints at either unsupervised parsing being simpler than
commonly assumed or rather the immaturity of all unsupervised parsers tested. Further, we would have
expected that trained unsupervised parsers, as most unsupervised methods, would exhibit a better per-
formance when trained on larger corpora. This could not be confirmed for both systems that we trained
on various corpus sizes, i.e. Seginer?s and Gillenwater?s approach. The findings are only moderately
correlated with evaluations on treebanks, cf Table 1: Whereas Seginer?s and S?gaard?s parsers perform
favorably in our evaluation, they are outperformed by Bisk?s parser on treebanks, which currently does
not scale to large data. Gillenwater?s parser seems to be overly tuned to English treebanks, but cannot
capitalize on this in our evaluation for English.
POS information does not seem beneficial for unsupervised parser induction in noun similarity evalu-
ation, since the highest-scoring approach by Seginer does not use POS tags and a version of S?gaard?s
parser with POS tags scored slightly but consistently lower than the version without POS, as we found
in further experiments. This is in line with the findings of Cramer (2007), who reports no benefit from
manually corrected or unsupervised POS tags for a range of unsupervised parsers.
Comparing the results of previous intrinsic evaluations (see Table 1) and the results of our extrinsic
evaluation (see Table 2 and 3), we observe that the ranking of parsers is only mildly correlated. Thus,
our proposed evaluation covers different aspects than the adherence to (partially arbitrary) conventions
of manually labeled dependency data. Also, our current evaluation disregards all arcs that do not involve
nouns.
When combining parsers, we observe that we can improve the quality of DTs significantly. This leads
us to conclude that unsupervised parsers should at least be used for generating features when computing
distributional thesauri of high quality. In case no high-quality supervised parser is available for the
language or domain of interest, it might suffice to use combinations of unsupervised parsers.
We also report the computation times of the different parsers, for the English dataset for Setup A (see
Table 6). The results were computed on a server with 80 GB and 16 cores. Whereas all parsers require
different amounts of memory, all parsers are single-threaded
10
. While S?gaard?s parser is the fastest
for small datasets, Seginer?s runs faster on 10 million sentences. Whereas Gillenwater?s and Seginer?s
10k 100k 1M 10M
Seginer 210 224 261 508
Gillenwater 3248 3248 3280 5546
S?gaard 3 21 182 975
UDP 183 1220 11316 -
Table 6: Computation time in minutes for parsing the data according to the English corpora used in Setup
A, cf. Table 2
10
As S?gaards algorithm parses sentence-wise without storing any information, it could be easily run multi-threaded.
1442
algorithm require almost the same time for parsing 10k, 100k or 1M sentences, the runtime of the UDP
and S?gaard?s parser is linear in time with the number of sentences to be parsed. We cannot report the
parsing times for the Bisk algorithm, as the parsing was not performed by us. Again it is noticeable that
the best two parsers are also the two unsupervised parsers that run quickest.
5 Conclusion
The contribution of this paper is two-fold: First, we have proposed and conducted a comparative extrin-
sic evaluation procedure for unsupervised parsers based on noun similarity in DTs. Second, we have
explored how to improve DT quality by combining features from several parsers. The transparency of
this method with respect to the kind of induced structures (dependencies, constituent trees, combinatory
categorial grammar) and with respect to labels of nodes and edges in the parse graph makes it possible to
compare different unsupervised parsers without having to rely on treebanks. Since semantic similarity,
especially for nouns, is a building block for many NLP applications, we feel that removing the depen-
dency on high-quality supervised parsers can give rise to semantic technologies in many languages. We
have conducted this evaluation with five different unsupervised parsers, and examined the influence of
corpus size for parser training and for the similarity computation in a series of experiments. Using estab-
lished methods for evaluating distributional similarity against lexical semantic resources, we were able
to measure differences between parsers in this task that are not reflected by intrinsic evaluations that
compare their induced structures to treebanks. These include the influence of corpus size on the training
procedure and the consistency of parse fragments on ?frequent versus rare words? as well as different
languages. Further, we were able to pinpoint a crucial point in unsupervised parsers that has not received
much attention: approaches that do not induce an actual parser that can be run on unseen sentences but
merely produce syntactic annotations for a given fixed training corpus are hardly useful in applications.
Our evaluation results can be summarized as follows: For English, with its relatively fixed order,
Seginer?s parser achieves very scarce to no improvements compared to a simple n-gram baseline when
used to compute distributional similarities. But for German, Seginer?s parser outperforms all baselines
including a state-of-the-art supervised parser, and S?gaard?s simplistic approach compares favorably to
the n-gram baselines. Furthermore, we demonstrate that the quality of noun similarity can be improved
significantly when combining the features from supervised and unsupervised parsers.
While today?s unsupervised parsers might not be ready for their utilization for semantic similarity for
the English language, they can be applied to a large number of other languages where highly optimized
supervised parsers are not available. Additionally, we feel that our proposed evaluation method exhibits
enough sensitivity to be a meaningful test bed for future unsupervised parsers.
6 Outlook
Where do we go from here? We strongly argue that in times of availability of very large monolingual
corpora from the web, we should strive at unsupervised parser induction systems that can make use of
large training data, as opposed to focussing our efforts on complex models that scale poorly, and thus
cannot elevate to the performance levels needed in order to make unsupervised parsing a building block
in natural language processing applications.
For further work, we want to proceed in several ways: we would like to extend our evaluation frame-
work from nouns to other parts of speech. Furthermore, we will explore whether unsupervised parsers
can be tuned towards the task of computing a distributional thesaurus, e.g. by using only assignments
with a certain confidence, type, or from sentences with limited length. Additionally, we would like to ex-
plore the interaction of unsupervised POS induction and grammar induction (Headden, III et al., 2008),
in order to entirely remove language-dependent preprocessing for the purpose of semantic similarity
computations, while at the same time being able to leverage the advantages of structured representations,
cf. Erk and Pad?o (2008). Finally, we would like to test whether we can also detect a different ranking
for different supervised parsers when comparing their scores in the normal treebank setting versus using
them for building distributional thesauri.
1443
Acknowledgments
This work has been supported by the German Federal Ministry of Education and Research (BMBF)
within the context of the Software Campus project LiCoRes under grant No. 01IS12054, by IBM under
a Shared University Research Grant and by DFG under the SemSch project grant.
References
Chris Biemann and Martin Riedl. 2013. Text: Now in 2D! A Framework for Lexical Expansion with Contextual
Similarity. Journal of Language Modelling, 1(1):55?95.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP Model for Inducing Combinatory Categorial Grammars. In
Transactions of the Association for Computational Linguistics, pages 75?88, Atlanta, GA, USA.
Rens Bod. 2007. Is the end of supervised parsing in sight? In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 400?407, Prague, Czech Republic.
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics, COLING ?10, pages 89?97, Beijing, China.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ?06, pages 149?164, New
York City, New York.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating the impact of alternative dependency graph encodings on solv-
ing event extraction tasks. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 982?992, Cambridge, Massachusetts.
Sander Canisius, Toine Bogers, Antal van den Bosch, Jeroen Geertzen, and Erik Tjong Kim Sang. 2006. Depen-
dency parsing by inference over high-recall dependency predictions. In Proceedings of the Tenth Conference on
Computational Natural Language Learning, CoNLL-X ?06, pages 176?180, New York City, New York.
Bart Cramer. 2007. Limitations of Current Grammar Induction Algorithms. In Proceedings of the ACL 2007
Student Research Workshop, pages 43?48, Prague, Czech Republic.
James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acquisition - Volume 9, ULA ?02, pages 59?66, Philadelphia,
Pennsylvania, USA.
James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,
Honolulu, Hawaii.
Jennifer Gillenwater, Kuzman Ganchev, Jo?ao Grac?a, Fernando Pereira, and Ben Taskar. 2010. Sparsity in depen-
dency grammar induction. In Proceedings of the ACL 2010 Conference Short Papers, pages 194?199, Uppsala,
Sweden.
Amit Goyal and Hal Daum?e, III. 2011. Generating semantic orientation lexicon using large data and thesaurus.
In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,
WASSA ?11, pages 37?43, Portland, Oregon, USA.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a Lexical-Semantic Net for German. In In Proceedings of
ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Appli-
cations, pages 9?15, Madrid, Spain.
William P. Headden, III, David McClosky, and Eugene Charniak. 2008. Evaluating unsupervised part-of-speech
tagging for grammar induction. In Proceedings of the 22nd International Conference on Computational Lin-
guistics - Volume 1, COLING ?08, pages 329?336, Manchester, United Kingdom.
William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109,
Boulder, CO, USA.
1444
Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling.
In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ?08,
pages 393?400, Manchester, United Kingdom.
Dan Klein and Christopher D Manning. 2002. A generative constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages
128?135, Philadelphia, PA,USA.
Dan Klein and Christopher D Manning. 2004. Corpus-based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational
Linguistics, pages 478?485, Barcelona, Spain.
Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the
European Chapter of the Association for Computational Linguistics, ACL ?98, pages 64?71, Madrid, Spain.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics - Volume 2, COLING ?98, pages 768?774, Montreal, Quebec, Canada.
David Marecek and Milan Straka. 2013. Stop-probability estimates computed on a large corpus improve Unsu-
pervised Dependency Parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics, pages 281?290, Sofia, Bulgaria.
Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the International Conference on Language Resources
and Evaluation, LREC 2006, pages 449?454, Genova, Italy.
George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38:39?41.
Yusuke Miyao, Rune Stre, Kenji Sagae, Takuya Matsuzaki, and Jun?ichi Tsujii. 2008. Task-oriented evaluation
of syntactic parsers and their representations. In Proceeding of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 46?54, Columbus, Ohio.
Yasaman Motazedi, Mark Dras, and Franc?ois Lareau. 2012. Is bad structure better than no structure?: Unsuper-
vised parsing for realisation ranking. In Proceedings of the 24th International Conference on Computational
Linguistics, COLING ?12, pages 1811?1830, Mumbai,India.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to
guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 1234?1244, Cambridge, MA, USA.
Joakim Nivre and Sandra K?ubler. 2006. Dependency parsing. In Tutorial at COLING-ACL, Sydney, Australia.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas. 2009. Web-scale distri-
butional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume 2, EMNLP ?09, pages 938?947, Singapore.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity: measuring the relatedness
of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL?Demonstrations ?04, pages 38?41,
Boston, Massachusetts, USA.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd?ottir, and Chris Biemann. 2006. Exploiting the Leipzig Corpora
Collection. In Proceedings of the IS-LTC 2006, pages 68?73, Ljubljana, Slovenia.
Martin Riedl and Chris Biemann. 2013. Scaling to large
3
data: An efficient and effective method to compute
distributional thesauri. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2013, pages 884?890, Seattle, WA, USA.
Helmut Schmid. 1997. Probabilistic part-of-speech tagging using decision trees. In Daniel Jones and Harold
Somers, editors, New Methods in Language Processing, Studies in Computational Linguistics, pages 154?164.
UCL Press, London, GB.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing evaluation. In Proceedings of the 49nd Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pages 663?672, Portland, Oregon,
USA.
1445
Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 384?391, Prague, Czech Republic.
Anders S?gaard. 2012. Unsupervised dependency parsing without training. Natural Language Engineering,
18(02):187?203.
Menno van Zaanen and University of Leeds. School of Computer Studies. 2001. Building Treebanks Using a
Grammar Induction System. Research report series. University of Leeds, School of Computer Studies.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of the 20th international conference on Computational Linguistics, COLING ?04, pages 1015?
1021, Geneva, Switzerland.
1446
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 884?890,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Scaling to Large3 Data: An efficient and effective method
to compute Distributional Thesauri
Martin Riedl and Chris Biemann
FG Language Technology
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biem}@cs.tu-darmstadt.de
Abstract
We introduce a new highly scalable approach
for computing Distributional Thesauri (DTs).
By employing pruning techniques and a dis-
tributed framework, we make the computation
for very large corpora feasible on comparably
small computational resources. We demon-
strate this by releasing a DT for the whole vo-
cabulary of Google Books syntactic n-grams.
Evaluating against lexical resources using two
measures, we show that our approach pro-
duces higher quality DTs than previous ap-
proaches, and is thus preferable in terms of
speed and quality for large corpora.
1 Introduction
Using larger data to estimate models for machine
learning applications as well as for applications of
Natural Language Processing (NLP) has repeatedly
shown to be advantageous, see e.g. (Banko and
Brill, 2001; Brants et al, 2007). In this work, we
tackle the influence of corpus size for building a
distributional thesaurus (Lin, 1998). Especially, we
shed light on the interaction of similarity measures
and corpus size, as well as aspects of scalability.
We shortly introduce the JoBimText framework
for distributional semantics and show its scalability
for large corpora. For the computation of the data
we follow the MapReduce (Dean and Ghemawat,
2004) paradigm. The computation of similarities
between terms becomes challenging on large cor-
pora, as both the numbers of terms to be compared
and the number of context features increases. This
makes standard similarity calculations as proposed
in (Lin, 1998; Curran, 2002; Lund and Burgess,
1996; Weeds et al, 2004) computationally infeasi-
ble. These approaches first calculate an informa-
tion measure between each word and the accord-
ing context and then calculate the similarity between
all words, based on the information measure for all
shared contexts.
2 Related Work
A variety of approaches to compute DTs have been
proposed to tackle issues regarding size and run-
time. The reduction of the feature space seems to
be one possibility, but still requires the computation
of such reduction cf. (Blei et al, 2003; Golub and
Kahan, 1965). Other approaches use randomised in-
dexing for storing counts or hashing functions to ap-
proximate counts and measures (Gorman and Cur-
ran, 2006; Goyal et al, 2010; Sahlgren, 2006). An-
other possibility is the usage of distributed process-
ing like MapReduce. In (Pantel et al, 2009; Agirre
et al, 2009) a DT is computed using MapReduce
on 200 quad core nodes (for 5.2 billion sentences)
respectively 2000 cores (1.6 Terawords), an amount
of hardware only available to commercial search en-
gines. Whereas Agirre uses a ?2 test to measure the
information between terms and context, Pantel uses
the Pointwise Mutual Information (PMI). Then, both
approaches use the cosine similarity to calculate the
similarity between terms. Furthermore, Pantel de-
scribes an optimization for the calculation of the co-
sine similarity. Whereas Pantel and Lin (2002) de-
scribe a method for sense clustering, they also use
a method to calculate similarities between terms.
Here, they propose a pruning scheme similar to ours,
but do not explicitly evaluate its effect.
The evaluation of DTs has been performed in ex-
trinsic and intrinsic manner. Extrinsic evaluations
have been performed using e.g. DTs for automatic
884
set expansion (Pantel et al, 2009) or phrase polar-
ity identification (Goyal and Daume?, 2011). In this
work we will concentrate on intrinsic evaluations:
Lin (1997; 1998) introduced two measures using
WordNet (Miller, 1995) and Roget?s Thesaurus. Us-
ing WordNet, he defines context features (synsets a
word occurs in Wordnet or subsets when using Ro-
get?s Thesaurus) and then builds a gold standard the-
saurus using a similarity measure. Then he evaluates
his generated Distributional Thesaurus (DT) with re-
spect to the gold standard thesauri. Weeds et al
(2004) evaluate various similarity measures based
on 1000 frequent and 1000 infrequent words. Curran
(2004) created a gold standard thesaurus by manu-
ally extracting entries from several English thesauri
for 70 words. His automatically generated DTs are
evaluated against this gold standard thesaurus using
several measures. We will report on his measure and
additionally propose a measure based on WordNet
paths.
3 Building a Distributional Thesaurus
Here we present our scalable DT algorithm using
the MapReduce paradigm, which is divided into
two parts: The holing system and a computational
method to calculate distributional similarities. A
more detailed description, especially for the MapRe-
duce steps, can be found in (Biemann and Riedl,
2013).
3.1 Holing System
The holing operation splits an observation (e.g. a
dependency relation) into a pair of two parts: a
term and a context feature. This captures their first-
order relationship. These pairs are subsequently
used for the computation of the similarities between
terms, leading to a second-order relation. The rep-
resentation can be formalized by the pair <x,y>
where x is the term and y represents the context
feature. The position of x in y is denoted by the
hole symbol ?@?. As an example the dependency
relation (nsub;gave2;I1) could be transferred to
<gave2,(nsub;@;I1)> and <I1,(nsub;gave2;@)>.
This representation scheme is more generic then the
schemes introduced in (Lin, 1998; Curran, 2002),
as it allows to characterise pairs by several holes,
which could be used to learn analogies, cf. (Turney
and Littman, 2005).
3.2 Distributional Similarity
First, we count the frequency for each first-order
relation and remove all features that occur with
more than w terms, as these context features tend to
be too general to characterise the similarity between
other words (Rychly? and Kilgarriff, 2007; Goyal
et al, 2010, cmp.). From this. we calculate a sig-
nificance score for all first-order relations. For this
work, we implemented two different significance
measures: Pointwise Mutual Information (PMI):
PMI(term, feature) = log2(
f(term,feature)
f(term)f(feature))
(Church and Hanks, 1990) and Lexicographer?s Mu-
tual Information (LMI): LMI(term, feature) =
f(term, feature) log2(
f(term,feature)
f(term)f(feature)) (Evert,
2005).
We then prune all negatively correlated pairs
(s<0). The maximum number of context features
per term are defined with p, as we argue that it is
sufficient to keep only the p most salient (ordered
descending by their significance score) context fea-
tures per term. Features of low saliency generally
should not contribute much to the similarity of terms
and also could lead to spurious similarity scores. Af-
terwards, all terms are aggregated by their features,
which allows us to compute similarity scores be-
tween all terms that share at least one such feature.
Whereas the method introduced by (Pantel and
Lin, 2002) is very similar to the one proposed in
this paper (the similarity between terms is calculated
solely by the number of features two terms share),
they use PMI to rank features and do not use pruning
to scale to large corpora, as they use a rather small
corpus. Additionally, they do not evaluate the effect
of such pruning.
In contrast to the best measures proposed by Lin
(1998; Curran (2002; Pantel et al (2009; Goyal et
al. (2010) we do not calculate any information mea-
sure using frequencies of features and terms (we use
significance ranking instead), as shown in Table 1.
Additionally, we avoid any similarity measure-
ment using the information measure, as also done in
these approaches, to calculate the similarity over the
feature counts of each term: we merely count how
many salient features two terms share. All these con-
straints makes this approach more scalable to larger
corpora, as we do not need to know the full list of
885
Information Measures
Lin?s formula I(term, feature) = lin(term, feature) = log f(term,feature)?f(relation(feature))P(f(word,relation(feature))f(word)
Curran?s TTest I(term, feature) = ttest(term, feature) = p(term,feature)?p(feature)?p(term)?
p(feature)?p(term)
Similarity Measures
Lin?s formula sim(t1, t2) =
P
f?features(t1)?features(t2)
(I(t1,f)+I(t2,f))
P
f?features(t1)
I(t1,f)+
P
f?features(w2)
I(w2,f)
Curran?s Dice sim(t1, t2) =
P
f?features(t1)?features(t2)
min(I(t1,f),I(t2,f))
P
f?features(t1)?features(t2)
(I(t1,f)+I(t2,f))
with I(t, f) > 0
Our Measure sim(t1, t2) =
?
f?features(t1)?features(t2)
1 with s > 0
Table 1: Similarity measures used for computing the distributional similarity between terms.
features for a term pair at any time. While our com-
putations might seem simplistic, we demonstrate its
adequacy for large corpora in Section 5.
4 Evaluation
The evaluation is performed using a recent dump of
English Wikipedia, containing 36 million sentences
and a newspaper corpus, compiled from 120 million
sentences (about 2 Gigawords) from Leipzig Cor-
pora Collection (Richter et al, 2006) and the Giga-
word corpus (Parker et al, 2011). The DTs are based
on collapsed dependencies from the Stanford Parser
(Marneffe et al, 2006) in the holing operation. For
all DTs we use the pruning parameters s=0, p=1000
and w=1000. In a final evaluation, we use the syn-
tactic n-grams built from Google Books (Goldberg
and Orwant, 2013).
To show the impact of corpus size, we down-
sampled our corpora to 10 million, 1 million and
100,000 sentences. We compare our results against
DTs calculated using Lin?s (Lin, 1998) measure and
the best measure proposed by Curran (2002) (see Ta-
ble 1).
Our evaluation is performed using the same 1000
frequent and 1000 infrequent nouns as previously
employed by Weeds et al (2004). We create a gold
standard, by extracting reasonable entries of these
2000 nouns using Roget?s 1911 thesaurus, Moby
Thesaurus, Merriam Webster?s Thesaurus, the Big
Huge Thesaurus and the OpenOffice Thesaurus and
employ the inverse ranking measure (Curran, 2002)
to evaluate the DTs.
Furthermore, we introduce a WordNet-based
method. To calculate the similarity between two
terms, we use the WordNet::Similarity path (Peder-
sen et al, 2004) measure. While its absolute scores
are hard to interpret due to inhomogenity in the gran-
ularity of WordNet, they are well-suited for relative
comparison. The score between two terms is in-
versely proportional to the shortest path between all
the synsets of both terms. The highest possible score
is one, if two terms share a synset. We compare the
average score of the top five (or ten) entries in the
DT for each of the 2000 selected words for our com-
parison.
5 Results
First, we inspect the results of Curran?s measure us-
ing the Wikipedia and newspaper corpus for the fre-
quent nouns, shown in Figure 1.
Both graphs show the inverse ranking score
against the size of the corpus. Our method scores
consistently higher when using LMI instead of PMI
for ranking the features per term. The PMI measure
declines when the corpus becomes larger. This can
be attributed to the fact that PMI favors term-context
pairs involving rare contexts (Bordag, 2008). Com-
puting similarities between terms should not be per-
formed on the basis of rare contexts, as these do not
generalize well because of their sparseness.
All other measures improve with larger corpora.
It is surprising that recent works use PMI to calcu-
late similarities between terms (Goyal et al, 2010;
Pantel et al, 2009), who, however evaluate their ap-
proach only with respect to their own implementa-
tion or extrinsically, and do not prune on saliency.
Apart from the PMI measure, Curran?s measure
leads to the weakest results. We could not confirm
that his measure outperforms Lin?s measure as stated
in (Curran, 2002)1. An explanation for this results
1Regarding Curran?s Dice formula, it is not clear whether to
use the intersection or the union of the features. We use an inter-
section, as it is unclear how to interpret the minimum function
otherwise, and the alternatives performed worse.
886
Figure 1: Inverse ranking for 1000 frequent nouns (Wikipedia left, Newspaper right) for different sized corpora. The
4 lines represent the scores of following DTs: our method using LMI (dashed black line) and the PMI significance
measure (solid black line) and Curran?s (dash bray line) and Lin?s measure (solid tray line).
might be the use of a different parser, very few test
words and also a different gold standard thesaurus
in his evaluation. Comparing our method using LMI
to Lin?s method, we achieve lower scores with our
method using small corpora, but surpass Lin?s mea-
sure from 10 million sentences onwards.
Next, we show the results of the WordNet eval-
uation measure in Figure 2. Comparing the top 10
(upper) to the top 5 words (lower) used for the eval-
uation, we can observe higher scores for the top 5
words, which validates the ranking. These results
are highly correlated to the results achieved with the
inverse ranking measure. This is a positive result,
as the WordNet measure can be performed automat-
ically using a single public resource2. In Figure 3,
we show results for the 1000 infrequent nouns using
the inverse ranking (upper) and the WordNet mea-
sure (lower).
We can see that our method using PMI does not
decline for larger corpora, as the limit on first-order
features is not reached and frequent features are still
being used. Comparing our LMI DT is en par with
Lin?s measure for 10 million sentences, and makes
better use of large data when using the complete
dataset. Again, the inverse ranking and the Word-
Net Path measure are highly correlated.
2Building a gold standard thesaurus following Curran
(2002) needs access to all the used thesauri. Whereas for some,
programming interfaces exist, often with limited access and li-
cence restrictions, others have to be extracted manually.
Figure 2: Results, using the WordNet:Path measure for
frequent nouns using the newspaper corpus.
887
Figure 3: WordNet::Path results for 1000 infrequent
nouns
The results shown here validate our pruning ap-
proach. Whereas Lin and Curran propose ap-
proaches to filter features that have low word feature
scores, they do not remove features that occur with
too many words, which is done in this work. Using
these pruning steps, a simplistic similarity measure
does not only lead to reduced computation times, but
also to better results, when using larger corpora.
5.1 Using a large3 corpus
We demonstrate the scalability of our method using
the very large Google Books dataset (Goldberg and
Orwant, 2013), consisting of dependencies extracted
from 17.6 billion sentences. The evaluation results,
using different measures, are given in Table 2.
Comparing the results for the Google Books DT
to the ones achieved using Wikipedia and the news-
Corpus Inv. P@1 Path@5 Path@10
frequent
nouns
Newspaper 2.0935 0.709 0.3277 0.2906
Wikipedia 2.1213 0.703 0.3365 0.2968
Google Books 2.3171 0.764 0.3712 0.3217
infrequent
nouns
Newspaper 1.4097 0.516 0.2577 0.2269
Wikipedia 1.3832 0.514 0.2565 0.2265
Google Books 1.8125 0.641 0.2989 0.2565
Table 2: Comparing results for different corpora.
paper, we can observe a boost in the performance,
both for the inverse ranking and the WordNet mea-
sures. Additionally, we show results for the P@1
measure, which indicates the percentage of entries,
whose first entry is in the gold standard thesaurus.
Remarkably, we get a P@1 against our gold stan-
dard thesaurus of 76% for frequent and 64% for in-
frequent nouns using the Google Books DT.
The most computation time was needed for the
dependency parsing and took two weeks on a small
cluster (64 cores on 8 nodes) for the 120 million
Newspaper sentences. The DT for the Google Books
was calculated in under 30 hours on a Hadoop clus-
ter (192 cores on 16 nodes) and could be calculated
within 10 hours for the Newspaper corpus. The com-
putation of a DT using this huge corpus would be in-
tractable with standard vector-based measurements.
Even computing Lin?s and Curran?s vector-based
similarity measure for the whole vocabulary of the
newspaper corpus was not possible with our Hadoop
cluster, as too much memory would have been re-
quired and thus we computed similarities only for
the 2000 test nouns on a server with 92GB of main
memory.
6 Conclusion
We have introduced a highly scalable approach
to DT computation and showed its adequacy for
very large corpora. Evaluating against thesauri and
WordNet, we demonstrated that our similarity mea-
sure yields better-quality DTs and scales to corpora
of billions of sentences, even on comparably small
compute clusters. We achieve this by a number of
pruning operations, and distributed processing. The
framework and the DTs for Google Books, News-
paper and Wikipedia are available online3 under the
ASL 2.0 licence.
3https://sf.net/projects/jobimtext/
888
Acknowledgments
This work has been supported by the Hessian re-
search excellence program ?Landes-Offensive zur
Entwicklung Wissenschaftlich-konomischer Exzel-
lenz? (LOEWE) as part of the research center ?Dig-
ital Humanities?. We would also thank the anony-
mous reviewers for their comments, which greatly
helped to improve the paper.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, NAACL ?09,
pages 19?27, Boulder, Colorado, USA.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting on
Association for Computational Linguistics, ACL ?01,
pages 26?33, Toulouse, France.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! a framework for lexical expansion with contextual
similarity. Journal of Language Modelling, 1(1):55?
95.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Stefan Bordag. 2008. A comparison of co-occurrence
and similarity measures as simulations of context. In
CICLing?08 Proceedings of the 9th international con-
ference on Computational linguistics and intelligent
text processing, pages 52?63, Haifa, Israel.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867, Prague, Czech Republic.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
James R. Curran. 2002. Ensemble methods for au-
tomatic thesaurus extraction. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, EMNLP ?02, pages
222?229, Philadelphia, PA, USA.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified Data Processing on Large Clusters. In Pro-
ceedings of Operating Systems, Desing & Implementa-
tion (OSDI) ?04, pages 137?150, San Francisco, CA,
USA.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung, University
of Stuttgart.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of english books. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume 1:
Proceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 241?247, At-
lanta, Georgia, USA.
Gene H. Golub and William M. Kahan. 1965. Calcu-
lating the singular values and pseudo-inverse of a ma-
trix. J. Soc. Indust. Appl. Math.: Ser. B, Numer. Anal.,
2:205?224.
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, ACL-44, pages
361?368, Sydney, Australia.
Amit Goyal and Hal Daume?, III. 2011. Generating se-
mantic orientation lexicon using large data and the-
saurus. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, WASSA ?11, pages 37?43, Portland, Ore-
gon, USA.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?, III, and
Suresh Venkatasubramanian. 2010. Sketch techniques
for scaling distributional similarity to the web. In Pro-
ceedings of the 2010 Workshop on GEometrical Mod-
els of Natural Language Semantics, GEMS ?10, pages
51?56, Uppsala, Sweden.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Compu-
tational Linguistics, ACL ?98, pages 64?71, Madrid,
Spain.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics - Vol-
ume 2, COLING ?98, pages 768?774, Montreal, Que-
bec, Canada.
889
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28(2):203?
208.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation, LREC 2006, Genova, Italy.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
Edmonton, Alberta, Canada.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2
- Volume 2, EMNLP ?09, pages 938?947, Singapore.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia, PA,
USA.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Boston, Massachusetts, USA.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsdo?ttir,
and Chris Biemann. 2006. Exploiting the leipzig cor-
pora collection. In Proceedings of the IS-LTC 2006,
Ljubljana, Slovenia.
Pavel Rychly? and Adam Kilgarriff. 2007. An efficient
algorithm for building a distributional thesaurus (and
other sketch engine developments). In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, ACL ?07, pages
41?44, Prague, Czech Republic.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
pages 1015?1021, Geneva, Switzerland.
890
Book Reviews
Graph-Based Natural Language Processing and Information Retrieval
Rada Mihalcea and Dragomir Radev
(University of North Texas and University of Michigan)
Cambridge, UK: Cambridge University Press, 2011, viii+192 pp; hardbound,
ISBN 978-0-521-89613-9, $65.00
Reviewed by
Chris Biemann
Technische Universita?t Darmstadt
Graphs are ubiquitous. There is hardly any domain in which objects and their relations
cannot be intuitively represented as nodes and edges in a graph. Graph theory is a
well-studied sub-discipline of mathematics, with a large body of results and a large
number of efficient algorithms that operate on graphs. Like many other disciplines, the
fields of natural language processing (NLP) and information retrieval (IR) also deal
with data that can be represented as a graph. In this light, it is somewhat surprising
that only in recent years the applicability of graph-theoretical frameworks to language
technology became apparent and increasingly found its way into publications in the
field of computational linguistics. Using algorithms that take the overall graph structure
of a problem into account, rather than characteristics of single objects or (unstructured)
sets of objects, graph-based methods have been shown to improve a wide range of NLP
tasks. In a short but comprehensive overview of the field of graph-based methods for
NLP and IR, Rada Mihalcea and Dragomir Radev list an extensive number of techniques
and examples from a wide range of research papers by a large number of authors.
This book provides an excellent review of this research area, and serves both as an
introduction and as a survey of current graph-based techniques in NLP and IR. Because
the few existing surveys in this field concentrate on particular aspects, such as graph
clustering (Lancichinetti and Fortunato 2009) or IR (Liu 2006), a textbook on the topic
was very much needed and this book surely fills this gap.
The book is organized in four parts and contains a total of nine chapters. The
first part gives an introduction to notions of graph theory, and the second part covers
natural and random networks. The third part is devoted to graph-based IR, and part
IV covers graph-based NLP. Chapter 1 lays the groundwork for the remainder of the
book by introducing all necessary concepts in graph theory, including the notation,
graph properties, and graph representations. In the second chapter, a glimpse is offered
into the plethora of graph-based algorithms that have been developed independently
of applications in NLP and IR. Sacrificing depth for breadth, this chapter does a
great job in touching on a wide variety of methods, including minimum spanning
trees, shortest-path algorithms, cuts and flows, subgraph matching, dimensionality
reduction, random walks, spreading activation, and more. Algorithms are explained
concisely, using examples, pseudo-code, and/or illustrations, some of which are very
well suited for classroom examples. Network theory is presented in Chapter 3. The
term network is here used to refer to naturally occurring relations, as opposed to graphs
being generated by an automated process. After presenting the classical Erdo?s-Re?nyi
random graph model and showing its inadequacy to model power-law degree distri-
butions following Zipf?s law, scale-free small-world networks are introduced. Further,
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
several centrality measures, as well as other topics in network theory, are defined
and exemplified.
Establishing the connection to NLP, Chapter 4 introduces networks constructed
from natural language. Co-occurrence networks and syntactic dependency networks
are examined quantitatively. Results on the structure of semantic networks such as
WordNet are presented, as well as a range of similarity networks between lexical units.
This chapter will surely inspire the reader to watch out for networks in his/her own
data. Chapter 5 turns to link analysis for the Web. The PageRank algorithm is de-
scribed at length, variants for undirected and weighted graphs are introduced, and the
algorithm?s application to topic-sensitive analysis and query-dependent link analysis
is discussed. This chapter is the only one that touches on core IR, and this is also the
only chapter with content that can be found in other textbooks (e.g., Liu 2011). Still,
this chapter is an important prerequisite for the chapter on applications. It would have
been possible to move the description of the algorithms to Chapter 2, however, omitting
this part.
The topic of Chapter 6 is text clustering with graph-based methods, outlining
the Fiedler method, the Kernighan?Lin method, min-cut clustering, betweenness, and
random walk clustering. After defining measures on cluster quality for graphs, spectral
and non-spectral graph clustering methods are briefly introduced. Most of the chapter
is to be understood as a presentation of general graph clustering methods rather than
their application to language. For this, some representative methods for different core
ideas were selected. Part IV on graph-based NLP contains the chapters probably most
interesting to readers working in computational linguistics. In Chapter 7, graph-based
methods for lexical semantics are presented, including detection of semantic classes,
synonym detection using random walks on semantic networks, semantic distance on
WordNet, and textual entailment using graph matching. Methods for word sense and
name disambiguation with graph clustering and random walks are described. The chap-
ter closes with graph-based methods for sentiment lexicon construction and subjectivity
classification.
Graph-based methods for syntactic processing are presented in Chapter 8: an
unsupervised part-of-speech tagging algorithm based on graph clustering, minimum
spanning trees for dependency parsing, PP-attachment with random walks over syn-
tactic co-occurrence graphs, and coreference resolution with graph cuts. In the final
chapter, many of the algorithms introduced in the previous chapters are applied to
NLP applications as diverse as summarization, passage retrieval, keyword extraction,
topic identification and segmentation, discourse, machine translation, cross-language
IR, term weighting, and question answering.
As someone with a background in graph-based NLP, I enjoyed reading this book.
The writing style is concise and clear, and the authors succeed in conveying the most
important points from an incredibly large number of works, viewed from the graph-
based perspective. I also liked the extensive use of examples?throughout, almost half
of the space is used for figures and tables illustrating the methods, which some readers
might perceive as unbalanced, however. With just under 200 pages and a topic as broad
as this, it necessarily follows that many of the presented methods are exemplified and
touched upon rather than discussed in great detail. Although this sometimes leads to
the situation that some passages can only be understood with background knowledge,
it is noteworthy that every chapter includes a section on further reading. In this way,
the book serves as an entry point to a deeper engagement with graph-based methods
for NLP and IR, and it encourages readers to see their NLP problem from a graph-based
view.
220
Book Reviews
For a future edition, however, I have a few wishes: It would be nice if the figures and
examples were less detached from the text and explained more thoroughly. At times, it
would be helpful to present deeper insights and to connect the methodologies, rather
than just presenting them next to each other. Also, some of the definitions in Chapter 2
could be less confusing and structured better.
Because this book emphasizes graph-based aspects for language processing rather
than aiming at exhaustively treating the numerous tasks that benefit from graph-based
methods, it cannot replace a general introduction to NLP or IR: For students without
prior knowledge in NLP and IR, a more guided and focused approach to the topic
would be required. The target audience is, rather, NLP researchers and professionals
who want to add the graph-based view to their arsenal of methods, and to become
inspired by this rapidly growing research area. It is equally suited for people working
in graph algorithms to learn about graphs in language as a field of application for their
work. I will surely consult this volume in the future to supplement the preparation of
lectures because of its comprehensive references and its richness in examples.
References
Lancichinetti, Andrea and Santo Fortunato.
2009. Community detection algorithms:
A comparative analysis. Physical
Review E, 80:056117.
Liu, Bing. 2011. Web Data Mining:
Exploring Hyperlinks, Contents,
and Usage Data (second edition).
Berlin, Springer.
Liu, Yi. 2006. Graph-based learning
models for information retrieval:
A survey. Available at: www.cse.msu.
edu/?rongjin/semisupervised/
graph.pdf.
Chris Biemann is Juniorprofessor (assistant professor) for Language Technology at Darmstadt
University of Technology. His current research interests include statistical semantics, graph-
based methods for unsupervised acquisition, and topic modeling. Biemann?s address is
UKP lab, Computer Science Department, Hochschulstr. 10, 64289 Darmstadt, Germany; e-mail:
biemann@tk.informatik.tu-darmstadt.de.
221
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553?557,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
How Text Segmentation Algorithms Gain from Topic Models
Martin Riedl and Chris Biemann
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
riedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
Abstract
This paper introduces a general method to in-
corporate the LDA Topic Model into text seg-
mentation algorithms. We show that seman-
tic information added by Topic Models signifi-
cantly improves the performance of two word-
based algorithms, namely TextTiling and C99.
Additionally, we introduce the new TopicTil-
ing algorithm that is designed to take better
advantage of topic information. We show con-
sistent improvements over word-based meth-
ods and achieve state-of-the art performance
on a standard dataset.
1 Introduction
Texts are often structured into segments to ease un-
derstanding and readability of texts. Knowing about
sentence boundaries is advantageous for natural lan-
guage processing (NLP) tasks such as summariza-
tion or indexing. While many genres such as en-
cyclopedia entries or scientific articles follow rather
formal conventions of breaking up a text into mean-
ingful units, there are plenty of electronically avail-
able texts without defined segments, e.g. web doc-
uments. Text segmentation is the task of automati-
cally segmenting texts into parts. Viewing a well-
written text as sequence of subtopics and assuming
that subtopics correspond to segments, a segmenta-
tion algorithm needs to find changes of subtopics to
identify the natural division of an unstructured text.
In this work, we utilize semantic information
from Topic Models (TMs) to inform text segmen-
tation algorithms. For this, we compare two early
word-based algorithms with their topic-based vari-
ants, and construct our own algorithm called Topic-
Tiling. We show that using topics estimated by La-
tent Dirichlet Allocation (LDA) in lieu of words sub-
stantially improves earlier segmentation algorithms.
In comparison to TextTiling (TT), neither smoothing
nor a blocksize or window size is needed. TT using
TMs and our own algorithm improve on the state-of-
the-art for a standard dataset, while being conceptu-
ally simpler and computationally more efficient than
other topic-based segmentation algorithms.
2 Related Work
Based on the observation of Halliday and Hasan
(1976) that the density of coherence relations is
higher within segments than between segments,
most algorithms compute a coherence score to mea-
sure the difference of textual units for informing
a segmentation decision. TextTiling (TT) (Hearst,
1994) relies on the simplest coherence relation ?
word repetition ? and computes similarities between
textual units based on the similarities of word space
vectors. With C99 (Choi, 2000) an algorithm was
introduced that uses a matrix-based ranking and a
clustering approach in order to relate the most sim-
ilar textual units and to cluster groups of consecu-
tive units into segments. Both TT and C99 charac-
terize textual units by the words they contain. Gal-
ley et al (2003) showed that using TF-IDF term
weights in the term vector improves the performance
of TT. Proposals using Dynamic Programming (DP)
are given in (Utiyama and Isahara, 2001; Fragkou et
al., 2004). Related to our work are the approaches
described in (Misra et al, 2009; Sun et al, 2008):
here, TMs are also used to alleviate the sparsity of
word vectors. Misra et al (2009) extended the DP
algorithm U00 from Utiyama and Isahara (2001) us-
553
ing TMs. At this, the topic assignments have to be
inferred for each possible segment, resulting in high
computational cost. In addition to these linear topic
segmentation algorithms, there are hierarchical seg-
mentation algorithms, see (Yaari, 1997; Hsueh et al,
2006; Eisenstein, 2009).
For topic modeling, we use the widely applied
LDA (Blei et al, 2003). This generative probabilis-
tic model uses a training corpus of documents to cre-
ate document-topic and topic-word distributions and
is parameterized by the number of topics N as well
as by two hyperparameters. To generate a document
d the topic proportions are drawn using a Dirichlet
distribution with hyperparameter ?. Adjacent for
each word i a topic zdi is chosen according to a
multinomial distribution using hyperparameter ?zdi .
Unseen documents can be annotated with an existing
TM using Bayesian inference methods (here: Gibbs
sampling).
3 Method: From Words to Topics
The underlying mechanism described here is very
simple: Instead of using words directly as features
to characterize textual units, we use the topic IDs
assigned by Bayesian inference. LDA inference as-
signs a topic ID to each word in the test document
in each inference iteration step, based on a TM es-
timated on a training corpus. We use the topic ID,
lastly assigned to each word. This might lead to in-
stabilities as a word with high probabilities for sev-
eral topics could be assigned to different topics in
different inference iterations. To avoid these insta-
bilities, we save all topic IDs assigned to a word for
each inference iteration. Finally, the most frequent
topic ID is assigned to each word. This mechanism
we call the mode method. Both word replacements
can be applied to most segmentation algorithms.
In this work, we use this general setup to imple-
ment topic-based versions of TT and C99 and de-
velop a new TextTiling-based method called Topic-
Tiling.
4 Topic-based Segmentation Algorithms
4.1 TextTiling using Topic Models
In TextTiling (TT) (Hearst, 1994) using topic IDs
(TTLDA), a document D, which is subject to seg-
mentation, is represented as a sequence of n topic
IDs1. TT splits the document into topic-sequences,
instead of sentences, where each sequence consists
of w topic IDs. To calculate the similarity between
two topic-sequences, called sequence-gap, TT uses
k topic-sequences, named block, to the left and to
the right of the sequence gap. This parameter k de-
fines the so-called blocksize. The cosine similarity
is applied to computed a similarity score based on
the topic frequency of the adjacent blocks at each
sequence-gap. A value close to 1 indicates a high
similarity among two blocks, a value close to zero
denotes a low similarity. Then for each sequence-
gap a depth score di is calculated for describing the
sharpness of a gap, by di = 1/2(hl(i)?si+hr(i)?
si). The function hl(i) returns the highest similarity
score on the left side of the sequence-gap index i that
does not increase and hr(i) returns the highest score
on the right side. Then all local maxima positions
are searched based on the depth scores.
In the next step, these obtained maxima scores are
sorted. If the number of segments n is given as input
parameter, the n highest depth scores are used, oth-
erwise a cut-off function is used that applies a seg-
ment only if the depth score is larger than ? ? ?/2,
where mean ? and the standard deviation ? are cal-
culated based on the entirety of depth scores. As TT
calculates the depth on every topic-sequence using
the highest gap, this could lead to a segmentation
in the middle of a sentence. To avoid this, a final
step ensures that the segmentation is positioned at
the nearest sentence boundary.
4.2 C99 using Topic Models
For the C99 algorithm (Choi, 2000), named
(C99LDA) when using topic IDs, the text is divided
into minimal units on sentence boundaries. A sim-
ilarity matrix Sm?m is computed, where m denotes
the number of units (sentences). Every element sij
is calculated using the cosine similarity between unit
i and j. Next, a rank matrix R is computed to im-
prove the contrast of S: Each element rij contains
the number of neighbors of sij that have lower simi-
larity scores then sij itself. In a final step a top-down
clustering algorithm is performed to split the docu-
ment into m segments B = b1, . . . , bm. This algo-
1words instead of topic IDs are utilized in the original ap-
proach.
554
rithm starts with the whole document considered as
one segment and splits off segments until the stop
criteria are met, e.g. the number of segments or a
similarity threshold.
4.3 TopicTiling
TopicTiling is a new TextTiling-based algorithm and
is adjusted to use TMs. As we have found in data
analysis, it is frequently the case that a topic dom-
inates within a sampling unit (sentence), and that
units from the same segment frequently are domi-
nated by the same topic. In contrast to word-based
representations, we expect no need to face sparsity
issues that require smoothing methods (see TT) and
ranking methods (see C99), which allows us to sim-
plify the algorithm. Initially, the document is split
into minimal units on sentence boundaries. To mea-
sure the coherence between units, the cosine similar-
ity (vector dot product) between two adjacent sen-
tences is computed. Each sentences s is represented
as a N -dimensional vector, where N is the number
of topics defined in the TMs. The i-th element of the
vector contains the number of times the i-th topic
is observed in the sentence. In comparison to TT
we search all local minima based on these similar-
ity scores and calculate for these positions the depth
score as described in TT. If the number of segments
is known in advance, the segments of the n-highest
depth-scores are used, otherwise the cut-off score
criteria used in TT is adapted.
5 Evaluation
As laid out in Section 3, a LDA Model is estimated
on a training dataset and used for inference on the
test set. To ensure that we do no use informa-
tion from the test set, we perform a 10-fold Cross
Validation (CV) for all reported results. To reduce
the variance of the shown results, derived by the ran-
dom nature of sampling and inference, the results
for each fold are calculated 30 times using different
LDA models.
The LDA model is trained with N=100 top-
ics, 500 sampling iterations and symmetric hy-
perparameters as recommended by Griffiths and
Steyvers (2004)(?=50/N and ?=0.01), using JGibb-
sLda (Phan and Nguyen, 2007). For the annota-
tion of unseen data with topic information, we use
LDA inference, sampling 100 iterations. Inference
is executed sentence-wise, since sentences form the
minimal unit of our segmentation algorithms and we
cannot use document information in the test setting.
The performance of the algorithms is measured us-
ing Pk and WindowDiff (WD) metrics (Beeferman
et al, 1999; Pevzner and Hearst, 2002). The C99 al-
gorithm is initialized with a 11?11 ranking mask, as
recommended in Choi (2000). TT is configured ac-
cording to Choi (2000) with sequence length w=20
and block size k=6.
5.1 Data Set
For evaluation, we rely on the Choi data set (Choi,
2000), which has been used in several other text seg-
mentation approaches to ensure comparability. This
data set is generated artificially using the Brown cor-
pus and consists of 700 documents. Each docu-
ment consists of 10 segments. For its generation,
3?11 sentences are sequentially extracted from a
randomly selected document and merged together.
While our CV evaluation setting is designed to avoid
using the same documents for training and testing,
this cannot be guaranteed as the segments within the
documents generated by Choi are included in sev-
eral documents. This problem also occurs in other
approaches, but has not be described in (Fragkou et
al., 2004; Misra et al, 2009; Galley et al, 2003),
where parts or the whole dataset are used for train-
ing either TF-IDF values or topic models.
5.2 Results
For the experiments the C99 and TT implementa-
tions2 are executed in two settings: using words and
using topics. When using words, TT and C99 use
stemmed words and filter out words using a stop-
word list. C99 additional removes words using pre-
defined regular expressions. In the case of topic IDs,
no stopword filtering was deemed necessary. Table
1 shows the result of the different algorithms with all
combination of provided segment number and using
the mode method.
We note that WD values are always higher than
the Pk values, and these measures are highly corre-
lated. First we discuss results for the setting with
number of segments provided (see column 2-5 of
2We use the implementations by Choi available at http:
//code.google.com/p/uima-text-segmenter/.
555
Method Segments provided Segments unprovided
mode=false mode=true mode=false mode=true
Pk WD Pk WD Pk WD Pk WD
C99 11.20 12.07 12.73 14.57
C99LDA 4.16 4.89 2.67 3.08 8.69 10.52 3.24 4.08
TT 44.48 47.11 49.51 66.16
TTLDA 1.85 2.10 1.04 1.18 16.41 21.40 2.89 3.67
TopicTiling 2.65 3.02 2.12 2.42 4.12 5.75 2.30 3.08
TopicTiling 1.50 1.72 1.06 1.21 3.24 4.58 1.39 1.84
(filtered)
Table 1: Results by segment length for TT with
words and topics (TTLDA), C99 with words and topics
(C99LDA) and TopicTiling using all sentences and using
only sentences with more then 5 word tokens (filtered).
Table 1). A significant improvement for C99 and
TT can be achieved when using topic IDs. In case
of C99LDA, the error rate is at least halved and for
TTLDA the error rate is reduced by a factor of 20.
Using the most frequent topic ID assigned during
the Bayesian inference (mode method) reduces the
error rates further for the TM-based approaches, as
the probability for randomly assigned topic IDs is
decreased. The newly introduced algorithm Top-
icTiling as described above does not improve over
TTLDA. Analysis revealed that the Choi corpus in-
cludes also captions and other ?non-sentences? that
are marked as sentences, which causes TopicTil-
ing to introduce false positive segments since the
topic vectors are too sparse for these short ?non-
sentences?. We therefore filter out ?sentences? with
less than 5 words (see bottom line in Table 1).
This leads to errors values that are close to the re-
sults achieved with TTLDA when the mode is used.
When the number of segments is not given in ad-
vance (see columns 6-9 in Table 1), we again ob-
serve significantly better results comparing topic-
based methods to word-based methods. But the er-
ror rates of TTLDA are unexpectedly high when the
mode method is not used. We discovered in data
analysis that TT estimates too many segments, as the
topic ID distributions between adjacent sentences
within a segment are often too diverse, especially
in face of random fluctuations from the topic assign-
ments. Estimating the number of segments is better
achieved using TopicTiling instead of TTLDA.
In Table 2, we compare TTLDA, C99LDA and
our TopicTiling algorithm to other published results
on the same dataset. We can see that all introduced
topic-based methods outperform the yet best pub-
Method Segments
provided unprovided
TT 44.48 49.51
C99 11.20 12.73
U00 (Utiyama and Isahara, 2001) 9 10
F04 (Fragkou et al, 2004) 5.39
M09 (Misra et al, 2009) 2.73
C99LDA (mode = true) 2.67 3.24
TTLDA (mode=true) 1.04 2.89
TopicTiling (mode=true, filtered) 1.06 1.39
Table 2: List of lowest Pk values for the Choi data set for
different algorithms in the literature.
lished M09 algorithm (Misra et al, 2009). The
improvements of C99, TTLDA and TopicTiling in
comparison to M09 are significant3.
TopicTiling and TTLDA are computationally
more efficient than M09. Whereas our linear method
has a complexity of O(T ) (T is the number of
sentences), dynamic algorithms like M09 have a
complexity of O(T 2) (cf. Fragkou et al (2004)),
which also applies to the number of topic inference
runs. When the number of segments is not given
in advance, TopicTiling outperforms TTLDA sig-
nificantly. As an additional benefit, TopicTiling is
even simpler than TT, as no smoothing parameter is
needed and the depth scores are only calculated for
the minima of the similarity scores.
6 Conclusion
The method introduced in this paper shows that us-
ing semantic information, provided by TMs, can im-
prove existing algorithm significantly. This is at-
tested modifying the algorithm TT and C99. With
TopicTiling a new simplistic topic based algorithm
is developed that can produce state-of-the-art results
based on the Choi corpus and outperform TTLDA
when the number of segments is unknown. Addi-
tionally this method is computationally more effi-
cient in comparison to other topic based segmenta-
tion algorithms. Another contribution is the mode
method for stabilizing topic ID assignments.
7 Acknowledgments
This work has been supported by LOEWE as part of
the research center ?Digital Humanities?. We would
like to thank the anonymous reviewers for their com-
ments, which truly helped to improve the paper.
3using a one sampled t-test with ? = 0.05
556
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine learning, 34(1):177?210.
David M. Blei, Andrew Y Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, pages 26?33,
Seattle, WA, USA.
Jacob Eisenstein. 2009. Hierarchical text segmenta-
tion from multi-scale lexical cohesion. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 353?
361, Boulder, CO, USA.
Pavlina Fragkou, Vassilios Petridis, and Athanasios Ke-
hagias. 2004. A Dynamic Programming Algorithm
for Linear Text Segmentation. Journal of Intelligent
Information Systems, 23(2):179?197.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation
of multi-party conversation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1, pages 562?569, Sapporo,
Japan.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
M A K Halliday and Ruqaiya Hasan. 1976. Cohesion in
English, volume 1 of English Language Series. Long-
man.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd annual
meeting on Association for Computational Linguistics,
pages 9?16, Las Cruces, NM, USA.
P.-Y. Hsueh, J. D. Moore, and S. Renals. 2006. Auto-
matic segmentation of multiparty dialogue. AMI-156.
Hemant Misra, Joemon M Jose, and Olivier Cappe?. 2009.
Text Segmentation via Topic Modeling : An Analyti-
cal Study. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, pages
1553?1556, Hong Kong.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistic, 28(1):19?36.
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. Gibb-
sLDA++: A C/C++ implementation of latent Dirichlet
allocation (LDA). http://jgibblda.sourceforge.net/.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher ker-
nel. Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Human
Language Technologies, pages 269?272.
Masao Utiyama and Hitoshi Isahara. 2001. A statisti-
cal model for domain-independent text segmentation.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 499?506,
Toulouse, France.
Yaakov Yaari. 1997. Segmentation of expository texts
by hierarchical agglomerative clustering. In Proceed-
ings of the Conference on Recent Advances in Natural
Language Processing, Tzigov Chark, Bulgaria.
557
Proceedings of NAACL-HLT 2013, pages 989?999,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Three Knowledge-Free Methods
for Automatic Lexical Chain Extraction
Steffen Remus and Chris Biemann
FG Language Technology
Department of Computer Science
Technische Universita?t Darmstadt
remus@kdsl.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
Abstract
We present three approaches to lexical chain-
ing based on the LDA topic model and eval-
uate them intrinsically on a manually anno-
tated set of German documents. After motivat-
ing the choice of statistical methods for lexi-
cal chaining with their adaptability to different
languages and subject domains, we describe
our new two-level chain annotation scheme,
which rooted in the concept of cohesive har-
mony. Also, we propose a new measure
for direct evaluation of lexical chains. Our
three LDA-based approaches outperform two
knowledge-based state-of-the art methods to
lexical chaining by a large margin, which can
be attributed to lacking coverage of the knowl-
edge resource. Subsequent analysis shows that
the three methods yield a different chaining
behavior, which could be utilized in tasks that
use lexical chaining as a component within
NLP applications.
1 Introduction
A text that is understandable by its nature exhibits
an underlying structure which makes the text co-
herent; that is, the structure is responsible for mak-
ing the text ?hang? together (Halliday and Hasan,
1976). The theoretic foundation of this structure is
defined as coherence and cohesion. While the for-
mer is concerned with the meaning of a text, the lat-
ter can be seen as a collection of devices for cre-
ating it. Cohesion and coherence build the basis
for most of the current natural language processing
problems that deal with text understanding. Lex-
ical cohesion ties together words or phrases that
are semantically related. Once all the cohesive ties
are identified the involved items can be grouped to-
gether to form so-called lexical chains, which form a
theoretically well-founded building block in various
natural language processing applications, such as
word sense disambiguation (Okumura and Honda,
1994), summarization (Barzilay and Elhadad, 1997),
malapropism detection and correction (Hirst and St-
Onge, 1998), document hyperlinking (Green, 1996),
text segmentation (Stokes et al, 2004), topic track-
ing (Carthy, 2004), and others. The performance of
the individual task heavily depends on the quality of
the identified lexical chains.
1.1 Motivation for Corpus-driven Approach
Previous approaches mainly focus on the use of
knowledge resources like lexical semantic databases
(Hirst and St-Onge, 1998) or thesauri (Morris and
Hirst, 1991) as background information in order to
resolve possible semantic relations. A major draw-
back of this strategy is the dependency on the cov-
erage of the resource, which has a direct impact on
the lexical chains. Their quality can be expected to
be poor for resource-scarce languages or specialized
application domains.
Statistical methods to modeling language seman-
tics have proven to deliver good results in many nat-
ural language processing applications. In particu-
lar, probabilistic topic models have been success-
fully used for tasks such as summarization (Gong
and Liu, 2001; Hennig, 2009), text segmentation
(Misra et al, 2009), lexical substitution (Dinu and
Lapata, 2010) or word sense disambiguation (Cai et
al., 2007; Boyd-Graber et al, 2007).
989
In this work, we address the question, whether
statistical methods for the extraction of lexi-
cal chains can yield better results than existing
knowledge-based methods, especially for underre-
sourced languages or domains, following principles
of Structure Discovery (Biemann, 2012). To address
this, we have developed a methodology for evaluat-
ing the quality of lexical chains intrinsically, have
carried out an annotation study, and report results on
a corpus of manually annotated German news docu-
ments.
After defining a measure for the comparison of
(manually or automatically created) lexical chains
in Section 2, Section 3 describes our annotation
methodology and discusses issues regarding the in-
herent subjectivity of lexical chain annotation. In
Section 4, three statistical approaches for lexical
chaining are developed on the basis of the LDA topic
model. Experiments that demonstrate the advantage
of these approaches over a knowledge-baseline are
conducted and evaluated in Section 5, and Section 6
concludes and provides an outlook future directions.
1.2 Previous Work on Lexical Chains
Morris and Hirst (1991) initially proposed an al-
gorithm for lexical chaining based on Roget?s the-
saurus (Roget, 1852), and manually assessed the
quality of their algorithm. Hirst and St-Onge (1998)
first presented a computational approach to lexical
chaining based on WordNet showing that the lexi-
cal database is a reasonable replacement to Roget?s.
The basic idea behind these algorithms is that se-
mantically close words should be connected to form
chains. Subsequent approaches mainly concentrated
on disambiguation of words to WordNet concepts
(WSD), since ambiguous words can lead to the over-
generation of connections. Barzilay and Elhadad
(1997) improved the implicit word sense disam-
biguation (WSD) by keeping a list of different inter-
pretations of the text and finally choosing the most
plausible senses for chaining. Silber and McCoy
(2002) introduced an efficient variant of the algo-
rithm with linear complexity in the number of can-
didate terms. Galley and McKeown (2003) further
improved accuracy by first performing WSD, and
then using the remaining links between the disam-
biguated concepts only. They also introduced a so-
called disambiguation graph, a representation that
has also been utilized by the method of Medelyan
(2007), where she applied a graph clustering algo-
rithm to the disambiguation graph to cut weak links,
performing implicit WSD. A combination of statis-
tical and knowledge-based methods is presented by
Marathe and Hirst (2010), who combine distribu-
tional co-occurrence information with semantic in-
formation from a lexicographic resource for extract-
ing lexical chains and evaluate them by text segmen-
tation. We are not aware of previous lexical chain-
ing algorithms that do not rely on a lexicographic
resource at all.
A major issue in developing a new lexical chain-
ing algorithm is the comparison to previous systems.
Most of previous approaches are validated by the
evaluation in a certain task like summarization, word
sense disambiguation, keyphrase extraction or infor-
mation retrieval (Stairmand, 1996). Hence, these ex-
trinsic evaluations are heavily influenced by the par-
ticular task at hand. We propose to re-consider lexi-
cal chaining as a task on its own, and propose objec-
tive criteria for directly comparing lexical chains to
this end.
2 Comparing Lexical Chains
The comparison of lexical chains is a non-trivial
task. We adopt the idea of interpreting lexical chains
as clusters and a particular set of lexical chains as
a clustering, and develop a suitable cluster com-
parison measure. As stated by Meila? (2005) and
Amigo? et al (2009), a best clustering comparison
measure for the general case does not exist. It should
be stressed that the appropriate clustering measure
highly depends on the task at hand.
After exploring a number of measures1, we de-
cided on a combination of the adjusted Rand in-
dex (ARI , Hubert and Arabie (1985)) and the basic
merge distance (BMD, Menestrina et al (2010))
for our new measure. Menestrina et al (2010) in-
troduced a linear time algorithm for computing the
generalized merge distance (GMD), which counts
1Explored measures which are unsatisfactory for the given
task are: Closest Cluster F1 (Benjelloun et al, 2009), K (Ajmera
et al, 2002), Pairwise F1 (Manning et al, 2008), Variation of
Information (Meila?, 2005), B3(Bagga and Baldwin, 1998), V-
Measure (Rosenberg and Hirschberg, 2007), Normalized Mu-
tual Information (Strehl, 2002). The last two measures are
equal. A proof of this can be found in the appendix.
990
split and merge cluster editing operations. Using a
constant factor of 1 for both splits and merges gives
the basic merge distance (BMD): Considering >
as the most general clustering of a dataset D, where
all elements are grouped into the same cluster, and
further considering ? as the most specific cluster-
ing of D, where each element builds its own clus-
ter, the lattice between > and ? spans all possible
clusterings and the BMD can be interpreted as the
shortest path from a clustering C to a clustering C ?
in the lattice with some restrictions (see Menestrina
et al (2010) for details). We normalize the BMD
score by the maximum BMD2 to the normalized
basic merge distance (NBMD). ARI is is based
on pair comparisons, and is computed as3:
index = TP
expected index =
(TP + FP )? (TP + FN)
TP + TN + FP + FN
max index = TP +
1
2
(FP + FN)
ARI(C,C ?) =
index? expected index
max index? expected index
The reasons for choosing these two particular
measures are the following: ARI is a well known
measure which is adjusted (corrected) for decisions
made by chance. But since it is based on pairwise
element comparison it completely disregards single-
ton clusters (chains) and some types of errors are not
adequately penalized. The NBMD on the other hand
penalizes various errors almost equally.
We combine the two single measures into a
new lccm (lexical chain comparison measure), de-
fined as the arithmetic mean between ARI and
1?NBMD. An lccm of 1 indicates perfect con-
gruence and an lccm = 0 indicates that not a single
pair of items in C is found in a cluster together in
C ?.
lccm(C,C ?) =
1
2
[
1?NBMD(C,C ?) +ARI(C,C ?)
]
.
2BMD(>,?) for |D| ? 2, BMD(>,?) + 1 otherwise
3TP : pairs in D and D?, FP : pairs in D? but not in D, FN :
pairs in D but not in D?, TN : pairs not in D and not in D?, where
D is the underlying dataset of C, D? is the underlying dataset of
C?, and pairs means all unique combinations of elements that
are in the same cluster.
3 Annotating Lexical Chains
A challenge with the annotation of lexical chains is
the subjective interpretation of the text by individ-
ual annotators (Morris and Hirst, 2004), which also
substantiates the fact that currently no gold stan-
dard exist, and all previous automatic approaches
are evaluated by performing a certain NLP task.
Hollingsworth and Teufel (2005) as well as Cramer
et al (2008) conclude from their lexical chain anno-
tation projects that high inter-annotator agreement is
very hard to achieve. We argue that directly evalu-
ating on lexical chains should enable us to optimize
towards higher-quality chain annotations, which is
a task of its own right and which has the potential
to improve all subsequent applications. For this, we
devise an annotation scheme that gets us reasonable
inter-annotator agreement, inspired by the concept
of cohesive harmony (Hasan, 1984), and report on
an annotation project for German newswire texts.
Documents from the SALSA 2.0 (Burchardt et al,
2006) corpus were chosen to form the basis for the
annotation of lexical chain information. SALSA is
based on the semi-automatically annotated TIGER
Treebank 2.1 (Brants et al, 2002). The TIGER
treebank provides manual annotations, such as lem-
mas, part-of-speech tags, and syntactic structure, the
SALSA part of the corpus is also partially annotated
with FrameNet-style (Baker et al, 1998) frame an-
notation. The documents are general domain news
articles from a German newspaper comprising about
1,550 documents and around 50,000 sentences in to-
tal, with a median document length of 275 tokens.
3.1 Annotation Scheme
In order to minimize the subjectiveness of choices
by different annotators, annotation guidelines were
developed comprising a total of ten pages. We
decided to consider only nouns, noun compounds
and non-compositional adjective noun phrases like
?dirty money? as candidate terms for lexical chain-
ing, which is consistent with the procedures of
Hollingsworth and Teufel (2005) and Cramer et al
(2008). For annotation, we used the MMAX24
(Mu?ller and Strube, 2006) tool.
We introduce the term dense chain, which refers
to a type of lexical chain in which every element is
4http://mmax2.sourceforge.net
991
related to every other element in that chain. Terms
are considered to be related if they share the same
topic, i.e. common sense and knowledge of the lan-
guage is needed to decide which terms belong to-
gether in the same topic and whether a chosen topic
is neither too broad nor too narrow. A single dense
chain can thus be assigned a definite topical descrip-
tion of its items. Whereas Hollingsworth and Teufel
(2005) dealt with the inherent fuzziness of member-
ship of terms to lexical chains by allowing terms
to occur in different lexical chains, we follow the
concept of cohesive harmony introduced by Hasan
(1984) here, where complete chains can be linked
to others. For this purpose, we introduce so-called
level two links, which are cohesive ties between lex-
ical items in distinct dense chains. Having such
a link between two chains, both chains can be as-
signed a topical description which is broader than
the description of the individual chains. This results
in a two-level representation of chains. We report
on dense lexical chains and merged lexical chains
(dense chains are merged into a common chain if a
level two link exists between them) separately.
In total, 100 documents were annotated by two
expert annotators. Documents were chosen around
the length median and consist of 248 ? 304 tokens.
The two rightmost columns of Table 3 show the
characteristics of the annotated data set. It can be
concluded that there is a moderate to high agree-
ment regarding the annotator selections of candidate
terms, which is ensured by preselection of candidate
terms by part-of-speech patterns. A value of 81% in
the average agreement on lexical items (cf. Figure 1)
shows that even though the choice of lexical items
is limited to nouns and adjective noun phrases only,
the decision on candidate termhood is somewhat dif-
ferent between the annotators, but compares favor-
ably with previous findings of 63% average pairwise
agreement (Morris and Hirst, 2004).
Figure 2 shows the annotator agreement on the
individual documents using the lccm (cf. Sec. 2),
sorted in the same way as in Figure 1. In order to use
the level two link information the figure also shows
a second agreement score, which was computed on
merged chains.
The agreement scores of the assignment of lexical
items to lexical chains depend partially on the agree-
ment scores of the identified lexical items them-
0600 2068 1069 0337 0364 0897 1323 0100 1388 0962 0545 1156 1190 0122 1194 1855 1808 0356 1779 2040 0219 1734 1216 0947 0937 2026 1548 1232 1826 1224 1251 1239 2044 1119 0174 1650 2014 0630 0199 0655 1818 0310 1150 1726 2035 0382 1639 0275 1047 1707 0228 1050 1607 0984 0755 0752 0747 1162 1018 0182 0942 0492 0675 0928 1399 1072 1310 0268 1033 1168 1523 1737 0524 0338 0976 0639 0299 1524 1387 0694 0278 1692 1534 1037 0499 0721 0283 0736 0258 1130 0988 0649 1777 0309 0110 1319 0729 0730 0390 0925Document ID02040
6080
100%
Figure 1: Agreement of lexical items annotated by anno-
tator A and annotator B as a percentage of lexical items
annotated by annotator A or annotator B. The average
agreement is 81%.
0600 2068 1069 0337 0364 0897 1323 0100 1388 0962 0545 1156 1190 0122 1194 1855 1808 0356 1779 2040 0219 1734 1216 0947 0937 2026 1548 1232 1826 1224 1251 1239 2044 1119 0174 1650 2014 0630 0199 0655 1818 0310 1150 1726 2035 0382 1639 0275 1047 1707 0228 1050 1607 0984 0755 0752 0747 1162 1018 0182 0942 0492 0675 0928 1399 1072 1310 0268 1033 1168 1523 1737 0524 0338 0976 0639 0299 1524 1387 0694 0278 1692 1534 1037 0499 0721 0283 0736 0258 1130 0988 0649 1777 0309 0110 1319 0729 0730 0390 0925Document ID0.00.20.4
0.60.8
1.0lccm
Figure 2: Individual annotator agreement scores on 100
documents sorted by their agreement on candidate terms.
The red circles show the agreement of both annotators on
the dense lexical chains disregarding the cohesive links,
and the green dots show the agreement of both annotators
on the merged lexical chains (via the cohesive links) both
using the proposed lexical chain comparison measure.
selves, which is a desired property. Across all doc-
uments, a perfect agreement was never achieved,
which confirms the difficulty of annotating such a
subjective task: The average lccm per document on
the manual annotations is 0.56 (dense chains), re-
spectively 0.54 (merged chains). However, the con-
siderable overlap between the annotators still en-
ables us to evaluate automatic chaining methods,
and the lccm agreement score serves as an upper
bound. Note that by performing no reconciliation
of the annotations we explicitly allow the possibil-
ity of different interpretations which is in our opin-
ion appropriate here due to the subjectiveness of the
task itself. By doing so, we evaluate our algorithms
against individual annotator interpretations.
4 Statistical Methods for Lexical Chaining
This work employs a well-studied statistical method
for creating something that Barzilay (1997) called
an automatic thesaurus which will then be adapted
for lexical chaining. For our automatic approaches,
candidate lexical items in a text are preselected by
the same heuristic that is also applied in Section 3
for the annotation process.
Topic models (TMs) are a suite of unsuper-
992
vised algorithms designed for unveiling some hid-
den structure in large data collections. The key idea
is that documents can be represented as compos-
ites of so-called topics where a topic itself repre-
sents as a composite of words. Hofmann (1999)
defined a topic to be a probability distribution over
words and a document to be a probability distribu-
tion over a fixed set of topics. We use the latent
Dirichlet alocation (LDA, Blei et al (2003)) topic
model for estimating the semantic closeness of can-
didate terms, and explore different ways of utilizing
LDA?s topic information in automatic lexical chain-
ers. Specifically, we use the GibbsLDA++5 frame-
work for topic model estimation and inference, and
examine the following LDA parameters: number of
topics T , Dirichlet hyperparameters for document-
topic distribution ? and topic-term distribution ?.
We now describe three LDA-based approaches to
lexical chaining.
4.1 LDA Mode Method (LDA-MM)
The LDA-MM approach places all word tokens that
share the same topic ID into the same chain. The
point is now how to decide to which topic a word
belongs to. Since single samples of topics per word
exhibit a large variance (Riedl and Biemann, 2012),
we follow these authors by sampling several times
and using the mode (most frequently assigned) topic
ID per word as the topic assignment. This strategy
reduced the variance in the lccm to a tenth6.
More formally, let samples(d,w) be the vector
of assignments that have been collected for a cer-
tain word w in a certain document d with each
samples(d,w)i referring to the i-th sampled topic ID
for (d,w). In other words, samples(d,w) can be seen
as the Markov chain for a particular word in a par-
ticular document. Further let z(d,w) be the topic ID
that was most assigned to the word w with respect
to the samples in samples(d,w). Precisely, z(d,w) is
defined to be the sampled mode in samples(d,w) ?
in case of multiple modes a random mode is chosen,
5http://gibbslda.sourceforge.net
6Preliminary experiments yielded a variance of 2.6 ? 10?6
in lccm using the mode method and 3.07?10?5 using a single
sample for lexical chain assignment.
which never happened in our experiments.
z(d,w) = mode (samples(d,w))
? argmax
j
(P (z = j|w, d))
The LDA-MM assigns for every word w which
is a candidate lexical item of a certain document d
which is assigned the same topic z(d,w) to the same
chain; hence implicitly disambiguating the terms.
The possibility to create level two links is given
by taking the second most occurring topic for a given
word if it exceeds a certain threshold.
4.2 LDA Graph Method (LDA-GM)
The LDA-GM algorithm creates a similarity graph
based on the comparison of topic distributions for
given words and then applies a clustering algorithm
in order to find semantically related words.
Let ?(d,w) be the per-word topic distribution
P (z|w, d). Analogously to the LDA-MM, ?(d,w)
can be obtained by counting the occurrences of
a certain topic ID z in the sample collection
samples(d,w) for a particular word w and document
d.
The semantic relatedness between any two words
wi and wj can then be measured by their similarity
score of the topic distributions ?(d,wi) and ?(d,wj),
which is stored in a term similarity matrix. This
matrix can also be interpreted as an adjacency ma-
trix of a graph, with candidate items being nodes
and edges being weighted with the similarity value
simij for any two nodes i, j : i 6= j ? i, j ?
{1, 2, . . . , Nd}. We test two similarity measures:
Euclidian (dis-)similarity and cosine similarity.
Let G = (V,E) be the graph represen-
tation of a document with term vertices
V = {v1, . . . , vNd} and weighted edges E =
{(v1, v2, sim12), . . . (vNd, vNd?1, simNdNd?1)},
where simij is either the cosine or Euclidean
similarity of term vectors. For simplicity, we reduce
this representation to an unweighted graph by
only retaining edges (of unit weight) that have a
similarity above a parameter threshold sim. To
identify chains as clusters in this graph, we follow
Medelyan (2007) and apply the Chinese Whispers
graph clustering algorithm (CW, Biemann (2006)),
which finds the number of clusters automatically.
The CW algorithm implementation comes with
993
three parameters to regulate the node weight based
on its degree, which influences cluster size and
granularity. We test options ?top?, ?dist log? and
?dist lin?.
The final chaining procedure is straightforward:
The LDA-GM algorithm assigns every candidate
lexical item wi of a certain document d which is
assigned the same class label ci to the same chain.
Level two links are drawn using the second domi-
nant class of a vertex?s neighborhood, which is pro-
vided by the CW implementation.
4.3 LDA Top-N Method (LDA-TM)
The LDA-TM method is different to the others in
that it uses the information of the per-topic word dis-
tribution ?(z) = P (w|z) and the per-document topic
distribution ?(d) = P (z|d). Given a parameter n re-
ferring to the top n topics to choose from ?(d) and a
parameter m referring to the top m words to choose
from ?(z) the main procedure can be described as
follows: for all z ? top n topics in ?(d): chain the
top m words in ?(z) .
Note that although the number of chains and chain
members for each chain is bound and could lead to
the same number and sizes of chains, in practice the
number of generated chains as well as the number of
chain members still varies considerably across doc-
uments: often some of the top m words for a (glob-
ally computed) topic do not even occur in a partic-
ular document. This implies that the parameters n
and m must not be set globally but dependent on
the particular document. To overcome this to some
extent, additional thresholding parameters ? and ?
are used for further bounding the respective n or m
parameter. The procedure works like this: for all z
? top n topics in ?(d) ? ?(d)z < ?: chain the top m
words w in ?(z) ? ?(z)w < ?.
Level two links are created by computing the co-
sine similarity between every pair of the top n topic
distributions, and thresholding with a link parame-
ter.
4.4 Repetition Heuristic
All methods described above can be applied to new
unseen documents that are not in the training set. To
alleviate a possible vocabulary mismatch between
training set and test set, which happens when terms
in the test set have not been contained in our training
documents, we add a heuristic that chains repetitions
of (previously unknown) words as a post-processing
step to all methods.
5 Empirical Analysis
In order to provide a realistic estimate of the qual-
ity of our methods to unseen material, we randomly
split our annotated documents in two parts of 50
documents each. One part is used as a development
set for optimizing the parameters of the methods (i.e.
model selection), the other part forms our test set for
evaluation.
The training corpus, on the other hand, consists
of all 1,211 SALSA/Tiger documents that are not
part of the development and test corpus and nei-
ther very long nor very short. These documents
are taken from the German newspaper ?Frankfurter
Rundschau? around 1992. Additionally the training
corpus is enriched with 12,264 news texts from the
same newspaper around 1997 with similar charac-
teristics7, making up a total of 13,457 training doc-
uments for the estimation of topic models.
Input to the LDA model training are verbs, nouns
and adjectives, as well as candidate terms as de-
scribed in Section 3.1, all in their lemmatized form.
We further filter words that occur in more than 1/3
of the training documents, as well as known stop-
words, and words that occur in less than two doc-
uments which results in a vocabulary size of about
100K words.
5.1 Experimental Setup
For comparison, we implemented three baselines,
which we describe below. One baseline is trivial,
two baselines are state-of-the art knowledge-based
systems adapted to German.
Random: Candidate lexical items are randomly
tied together to form sets of lexical chains.
Level two links are created analogously. We
regulate the process to yield the same average
number of chains and links as in the develop-
ment and test data.
S&M GermaNet: Algorithm by Silber and McCoy
(2002) with GermaNet as its knowledge re-
source.
7as provided by Projekt Deutscher Wortschatz,
http://wortschatz.uni-leipzig.de/
994
G&M GermaNet: Algorithm by Galley and McK-
eown (2003), also using GermaNet.
GermaNet (Hamp and Feldweg, 1997) is a large
WordNet-like resource for German, containing al-
most 100,000 lexical units and over 87,000 concep-
tual relations between synsets. While its size is only
about half of WordNet, it is one of the largest non-
English lexical semantic resources.
5.2 Model Selection
We optimize two sets of parameters: parameters for
the LDA topic model (number of topics K, Dirich-
let hyperparameters ? and ?) are optimized for the
LDA-MM method only, and the same LDA model
is used in the other two LDA-based methods. Pa-
rameters particular to the respective method are op-
timized individually. For LDA, we tested sensible
combinations in the ranges K = 50..1000, ? =
0.05/K..50/K and ? = 0.001..0.1. The highest
performance of the LDA-MM method was found for
K = 500, ? = 50/K, ? = 0.001, and the result-
ing topic model is used across all methods. The final
parameter values for the other methods, found by ex-
haustive search, are summarized in Table 1.
Method Parameter
LDA-GM similarityfunction = cosine similarity
labelweightscheme = dist log
sim = 0.95
LDA-TM n = 10, m = 20, ? = 0.2, ? = 0.2
Table 1: Final parameter values.
5.3 Evaluation
For evaluation purposes, terms that consist of multi-
ple words are mapped to its rightmost term which
is assumed to be the head, e.g. ?dirty money? is
mapped to ?money?. Additionally, singleton chains,
i.e. chains that contain only a single lexical item
are omitted unless the respective lexical item is not
linked by a level two link.
Dense Chains Comparative results of the ap-
proaches in terms of lccm for both annotators are
summarized in Table 2 (upper half). We observe
that all our new methods beat the random baseline
and the two knowledge-based baselines by a large
margin. The knowledge-based baselines, both using
Anno A Anno B Average
LDA-MM 0.320 0.306 0.313
LDA-TM 0.307 0.299 0.303
LDA-GM 0.328 0.314 0.321
G&M 0.255 0.215 0.235
S&M 0.248 0.209 0.229
Random 0.126 0.145 0.135
LDA-MM 0.316 0.300 0.308
LDA-TM 0.303 0.280 0.291
LDA-GM 0.279 0.267 0.273
G&M 0.184 0.166 0.176
S&M 0.179 0.159 0.169
Random 0.196 0.205 0.201
Table 2: Results of the evaluation based on dense chains
(upper half) and merged chains (lower half). The annota-
tor agreement on the test set?s chains = 0.585; on merged
chains = 0.553
GermaNet, produce very similar lccm scores, which
highlights the important role of the knowledge re-
source. Data analysis revealed that while chains pro-
duced by knowledge-based baselines are sensible,
the main problem is a lack of coverage in terms of
vocabulary and relations in GermaNet. Comparing
the statistical methods, the LDA-GM method excels
over the others.
Level Two Links Table 2 (lower half) summarizes
the evaluation results of the merged chains via level
two links. Because of merging, a text now contains
fewer chains with more lexical items each. Note that
knowledge-based baselines do not construct level
two links, which is why they are heavily penalized
in this setup.
Again, the statistical methods beat the baselines
by a substantial amount. In this evaluation, the ran-
dom baseline performs above the knowledge-based
methods, which is rooted in the fact that lccm penal-
izes small, correct chains, whereas the random base-
line with linking often produces very large chains
containing most of the terms ? something that we
also observe for many manually annotated docu-
ments. The large overlap in the biggest chain then
leads to the comparatively high random baseline
score. In this evaluation, the LDA-MM is the clear
winner, with LDA-GM being clearly inferior this
time.
995
LDA-MM LDA-GM LDA-TM S&M G&M Anno A Anno B
avg. num. of lexical items per doc. 38.20 29.32 30.82 14.40 15.29 38.66 38.96
avg. num. of chains per doc. 13.80 9.12 7.32 5.83 5.71 11.25 7.38
avg. num. of links per doc. 8.60 2.06 1.44 ? ? 5.47 2.41
avg. size lexical chains 2.82 3.41 4.61 2.48 2.68 3.69 5.57
avg. num. of merged lexical chains 5.76 7.06 5.98 ? ? 6.10 4.99
avg. size merged lexical chains 8.29 4.45 5.57 ? ? 7.60 8.91
Table 3: Quantitiative characteristics of automatic and manual lexical chains. In average, a document contains 51.58
candidate terms as extracted by our noun phrase patterns
Davud Bouchehri,
[Davud Bouchehri,]
seit
[since]
der
[the]
letzten
[last]
Spielzeit
[playing period]
als
[as]
Dramaturg
[dramaturg]
in
[in]
Basel
[Basle]
ta?tig,
[acting,]
wechselt
[switches]
zur
[to the]
Saison 1996 / 97
[1996 / 97 season]
als
[as]
ku?nstlerischer
[art]
Gescha?ftsfu?hrer
[director]
des
[of the]
Schauspiels
[play]
an
[to]
das
[the]
Staatstheater
[state theater]
Darmstadt.
[Darmstadt.]
Der
[The]
aus
[from]
dem
[the]
Iran
[Iran]
stammende
[coming]
34ja?hrige
[34-year-old]
soll
[shall]
daneben
[besides]
auch
[also]
fu?r
[for]
spartenu?bergreifende
[multi discipline]
Projekte
[projects]
zusta?ndig
[responsible]
sein,
[be,]
teilte
[aquainted]
das
[the]
Basler
[Basle?s]
Theater
[theather]
am
[on]
Donnerstag
[Thursday]
mit.
[with.]
LDA-MM:
c1: {Spielzeit, Schauspiels, Staatstheater}
c2: {Dramaturg, Theater}
c3: {Saison}
l1: (Theater? Spielzeit)
l2: (Spielzeit? Saison)
LDA-GM:
c1: {Dramaturg, Theater}
c2: {Schauspiels, Staatstheater}
LDA-TM:
c1: {Schauspiels, Staatstheater, Theater}
c2: {Dramaturg}
c3: {Spielzeit, Saison}
l1: (Theater? Dramaturg)
S&M-GermaNet:
?
G&M-GermaNet:
c1: {Staatstheater, Theater}
Figure 3: Diverse output of the various lexical chaining systems after applying them on a short German example text
from the used TIGER/SALSA corpus. For a better understanding the text is calqued. Candidate items are highlighted
and the ci are the resulting dense lexical chains and the li are the level two links produced by the various methods.
Data Analysis Table 3 shows quantitative num-
bers of the extracted lexical chains in the test set.
The LDA-MM approach chains and links a lot
more items than the other statistical methods: it cre-
ates a lot more links between items that would oth-
erwise be removed because they form unlinked sin-
gleton chains. As opposed to this, the graph method
(LDA-GM), as well as the top-n method (LDA-TM)
perform an implicit filtering on the candidate lexi-
cal items by creating less level two links, yet larger
dense chains. The knowledge based algorithms by
Silber and McCoy (2002) and Galley and McKeown
(2003) extract fewer and smaller chains than the sta-
tistical approaches, which reflects GermaNet?s spar-
sity issues. While higher lexical coverage in the
underlying resource would increase the coverage of
our knowledge-based systems, this is only one part
of the story. The other part is rooted in the fact
that lexical cohesion relations, which are used in
lexical chains, encompass many more semantic re-
lations than listed in today?s lexical semantic net-
works. This especially holds for cases where sev-
eral expressions refer to the same event or theme for
which no well-defined relation exists, such as e.g.
?captain? and ?harbor?.
Comparing the three LDA-based approaches, no
overall best method could be determined. the LDA-
MM seems especially suited for a high coverage
and coarse (level two) chains, the LDA-GM appears
most suited for dense chains, and LDA-TM pro-
duces the longest chains on average.
Figure 3 shows the resulting dense lexical chains
and level two links after applying our chainers to a
short example text from our corpus. In the exam-
ple the LDA-TM produces the most adequate lexi-
cal chains, at least in our intuition. The LDA-GM
and the LDA-MM produce slightly wrong chains,
yet the LDA-MM additionally creates some mean-
ingfull level two links which the LDA-GM does not.
Both knowledge-based approaches perform poorly
compared to the knowledge-free approaches, where
the S&M algorithm creates no chains at all and the
996
G&M algorithm produces only a single chain con-
taining only two words. This is mostly due to Ger-
maNet?s lacking lexical and relational coverage and
the scope of the algorithms for finding relations be-
tween the words.
6 Conclusion
In this paper, we presented experiments for auto-
matic lexical chain annotation and evaluated them
directly on a manually annotated dataset for Ger-
man. A new two-level annotation scheme for lexi-
cal chains was proposed and motivated by the con-
cept of cohesive harmony. We further proposed a
new measure for comparing lexical chain annota-
tions that is especially suited for the characteristics
of lexical chain annotations. Three variants of sta-
tistical lexical chaining methods based on the LDA
topic model were proposed and evaluated against
two knowledge-based baseline systems. Our sta-
tistical methods exhibit a substantially higher per-
formance than the knowledge-based systems on our
dataset. This can partially be attributed to miss-
ing relations, partially to the lack of lexical cov-
erage of GermaNet, which was used in these sys-
tems. Since GermaNet is a large lexical-semantic
net, however, this strengthens our main point: Espe-
cially for under-resourced languages or subject do-
mains, statistical and data-driven methods should be
preferred over their knowledge-based counterparts,
since they do not require the development of lexical-
semantic nets and adopt easily to subject domains by
training their unsupervised models on an in-domain
collection.
In future work, we would like to explore better
ways of selecting candidate items. While our POS-
pattern-based selection mechanism works for practi-
cal purposes, it currently only extracts noun phrases
and over-generates on compositional adjective mod-
ifiers. We would like to define a better filter to re-
duce over-generation. Further, especially for com-
pounding languages such as German, we would like
to decompose one-word compounds as to be able to
link their heads in lexical chains.
While we found it important to directly evalu-
ate our lexical chaining algorithms on manually an-
notated data, a natural next step in this line of re-
search is to use our lexical chaining methods as
pre-processing steps for applications such as sum-
marization, text segmentation or word sense disam-
biguation. This would enable to find out advantages
and disadvantages of our three variants with respect
to an application.
The manually annotated data, the open source an-
notation tool, the annotation guidelines and the im-
plementations of all described methods and base-
lines are available for download8.
Acknowledgments
This work has been supported by the Hessian re-
search excellence program Landes-Offensive zur
Entwicklung Wissenschaftlich-o?konomischer Exzel-
lenz (LOEWE) as part of the research center Digital
Humanities.
Proof: Equality of NMI and V
Using the standard notation from information retrievalH(X)= Entropy,
I(X,Y )= Information, H(X|Y )= Conditional Entropy, NMI(X,Y)=
Normalized Mutual Information, V(X,Y)= V-Measure:
V (C,K) = 2?
h? c
h+ c
(1)
h = 1?
H(C|K)
H(C)
, c = 1?
H(K|C)
H(K)
(2)
and
NMI(C,K) =
I(C,K)
H(C)+H(K)
2
= 2?
I(C,K)
H(C) +H(K)
(3)
reformulate h and c using the fact that I(C,K) = H(C) ?
H(C|K) = H(K)?H(K|C):
h = 1?
H(C|K)
H(C)
=
H(C)
H(C)
?
H(C|K)
H(C)
=
I(C,K)
H(C)
(4)
c = 1?
H(K|C)
H(K)
=
H(K)
H(K)
?
H(K|C)
H(K)
=
I(C,K)
H(K)
(5)
simplifying h? c using (4) and (5):
h? c =
I(C,K)
H(C)
?
I(C,K)
H(K)
=
I(C,K)2
H(C)H(K)
(6)
simplifying h+ c using (4) and (5):
h+ c =
I(C,K)
H(C)
+
I(C,K)
H(K)
=
I(C,K)H(K) + I(C,K)H(C)
H(C)H(K)
=
I(C,K)[(H(K) +H(C)]
H(C)H(K)
(7)
simplifying h?ch+c using (6) and (7):
h? c
h+ c
=
I(C,K)2
H(C)H(K)
?
H(C)H(K)
I(C,K)[H(K) +H(C)]
=
I(C,K)
H(K) +H(C)
(8)
8http://www.ukp.tu-darmstadt.de/
data/lexical-chains-for-german/
997
substituting (8) into (1) shows that NMI and V are equal:
V (C,K) = 2?
h? c
h+ c
= 2?
I(C,K)
H(K) +H(C)
= NMI(C,K) (9)
References
Jitendra Ajmera, Herve? Bourlard, and I. Lapidot. 2002.
Unknown-Multiple Speaker clustering using HMM.
In Proceedings of the International Conference of Spo-
ken Language Processing, ICSLP ?02, Denver, Col-
orado, USA.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12:461?486.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Linguistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Evalu-
ation, LREC ?98, pages 563?566, Granada, Spain.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In COLING
?98: Proceedings of the 17th International Conference
on Computational Linguistics, volume 1, pages 86?90,
Montreal, Quebec, Canada.
Regina Barzilay and Michael Elhadad. 1997. Using Lex-
ical Chains for Text Summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17, Madrid, Spain.
Regina Barzilay. 1997. Lexical Chains for Summariza-
tion. Master?s thesis, Ben-Gurion University of the
Negev, Beersheva, Israel.
Omar Benjelloun, Hector Garcia-Molina, David Men-
estrina, Qi Su, Steven Whang, and Jennifer Widom.
2009. Swoosh: a generic approach to entity resolu-
tion. The VLDB Journal, 18:255?276.
Chris Biemann. 2006. Chinese Whispers ? an Effi-
cient Graph Clustering Algorithm and its Application
to Natural Language Processing. In Proceedings of
TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, USA.
Chris Biemann. 2012. Structure Discovery in Natural
Language. Theory and Applications of Natural Lan-
guage Processing. Springer Berlin / Heidelberg.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A Topic Model for Word Sense Disambigua-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033, Prague, Czech
Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. TIGER Treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories (TLT02), Sozopol, Bulgaria.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA Corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th interna-
tional conference on Language Resources and evalua-
tion (LREC-2006), Genoa, Italy.
Junfu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving Word Sense Disambiguation Using Topic
Features. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1015?1023, Prague, Czech
Republic.
Joe Carthy. 2004. Lexical Chains versus Keywords
for Topic Tracking. In A. Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 2945 of Lecture Notes in Computer Science,
pages 507?510. Springer, Berlin / Heidelberg.
Irene Cramer, Marc Finthammer, Alexander Kurek,
Lukas Sowa, Melina Wachtling, and Tobias Claas.
2008. Experiments on Lexical Chaining for Ger-
man Corpora: Annotation, Extraction, and Applica-
tion. Journal for Language Technology and Computa-
tional Linguistics (JLCL), 23(2):34?48.
Georgiana Dinu and Mirella Lapata. 2010. Topic Mod-
els for Meaning Similarity in Context. In COLING
?10: Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 250?
258, Beijing, China.
Michel Galley and Kathleen McKeown. 2003. Im-
proving word sense disambiguation in lexical chain-
ing. In IJCAI?03: Proceedings of the 18th interna-
tional joint conference on Artificial intelligence, pages
1486?1488, Acapulco, Mexico.
Yihong Gong and Xin Liu. 2001. Generic Text Sum-
marization Using Relevance Measure and Latent Se-
mantic Analysis. In SIGIR 2001: Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 19?25, New Orleans, Louisiana, USA.
Stephen J. Green. 1996. Using Lexical Chains to Build
Hypertext Links in Newspaper Articles. In AAAI-
96 Workshop on Internet-based Information Systems,
pages 115?141, Portland, Oregon, USA.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. English language series. Longman,
London.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet
- a Lexical-Semantic Net for German. In Proceed-
998
ings of the ACL/EACL-97 workshop Automatic Infor-
mation Extraction and Building of Lexical Semantic
Resources for NLP Applications, Madrid, Spain.
Ruqaiya Hasan. 1984. Coherence and Cohesive Har-
mony. In James Flood, editor, Understanding Reading
Comprehension, Cognition, Language, and the Struc-
ture of Prose, pages 181?220. International Reading
Association, Newark, Delaware, USA.
Leonhard Hennig. 2009. Topic-based multi-document
summarization with probabilistic latent semantic anal-
ysis. In Proceedings of the International Conference
RANLP-2009, pages 144?149, Borovets, Bulgaria.
Graeme Hirst and David St-Onge. 1998. Lexical Chains
as representation of context for the detection and cor-
rection malapropisms. In Christiane Fellbaum, edi-
tor, WordNet: An Electronic Lexical Database, Lan-
guage, Speech, and Communication, pages 305?332.
The MIT Press, Cambridge, Massachusetts, USA.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Analysis. In Proceedings of the Fifteenth Confer-
ence on Uncertainty in Artificial Intelligence, UAI ?99,
pages 289?296, Stockholm, Sweden.
William Hollingsworth and Simone Teufel. 2005. Hu-
man annotation of lexical chains: Coverage and agree-
ment measures. In Proceedings of the Workshop
ELECTRA: Methodologies and Evaluation of Lexical
Cohesion Techniques in Real-world Applications, In
Association with SIGIR ?05, Salvador, Brazil.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
partitions. Journal of Classification, 2(1):193?218.
Christopher Manning, Prabhakar Raghavan, and Hinrich
Schu?tze. 2008. An Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Meghana Marathe and Graeme Hirst. 2010. Lexical
Chains Using Distributional Measures of Concept D.
In Proceedings of the 11th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, CICLing?10, pages 291?302, Ias?i, Romania.
Olena Medelyan. 2007. Computing lexical chains with
graph clustering. In Proceedings of the 45th Annual
Meeting of the ACL: Student Research Workshop, ACL
?07, pages 85?90, Prague, Czech Republic.
Marina Meila?. 2005. Comparing clusterings: an ax-
iomatic view. In Proceedings of the 22nd Interna-
tional Conference on Machine Learning, ICML ?05,
pages 577?584, Bonn, Germany.
David Menestrina, Steven Euijong Whang, and Hec-
tor Garcia-Molina. 2010. Evaluating entity resolu-
tion results. Proceedings of the VLDB Endowment,
3(1):208?219.
Hemant Misra, Franc?ois Yvon, Joemon Jose, and Olivier
Cappe?. 2009. Text Segmentation via Topic Mod-
eling: An Analytical Study. In Proceedings of the
18th ACM Conference on Information and Knowledge
Management, CIKM 2009, pages 1553?1556, Hong
Kong, China.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21?
48.
Jane Morris and Graeme Hirst. 2004. The Subjectivity of
Lexical Cohesion in Text. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, Palo Alto, California,
USA.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang, Frankfurt a.M., Germany.
Manabu Okumura and Takeo Honda. 1994. Word sense
disambiguation and text segmentation based on lexical
cohesion. In COLING ?94: Proceedings of the 15th
Conference on Computational Linguistics, volume 2,
pages 755?761, Kyoto, Japan.
Martin Riedl and Chris Biemann. 2012. Sweeping
through the Topic Space: Bad luck? Roll again! In
ROBUS-UNSUP 2012: Joint Workshop on Unsuper-
vised and Semi-Supervised Learning in NLP held in
conjunction with EACL 2012, pages 19?27, Avignon,
France.
Peter Mark Roget. 1852. Roget?s Thesaurus of English
Words and Phrases. Longman Group Ltd., Harlow,
UK.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420,
Prague, Czech Republic.
H. Gregory Silber and Kathleen F. McCoy. 2002. Effi-
ciently computed lexical chains as an intermediate rep-
resentation for automatic text summarization. Compu-
tational Linguistics, 28(4):487?496.
Mark A. Stairmand. 1996. A Computational Analysis
of Lexical Cohesion with Applications in Information
Retrieval. Ph.D. thesis, Center for Computational Lin-
guistics, UMIST, Manchester.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004.
SeLeCT: A Lexical Cohesion Based News Story Seg-
mentation System. AI Communications, 17(1):3?12.
Alexander Strehl. 2002. Relationship-based Cluster-
ing and Cluster Ensembles for High-dimensional Data
Mining. Ph.D. thesis, University of Texas, Austin.
999
Proceedings of NAACL-HLT 2013, pages 1131?1141,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supervised All-Words Lexical Substitution using Delexicalized Features
Gyo?rgy Szarvas1 Chris Biemann2 Iryna Gurevych3,4
(1) Nuance Communications Deutschland GmbH
Kackertstrasse 10, D-52072 Aachen, Germany
(2) FG Language Technology
Department of Computer Science, Technische Universita?t Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
(4) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.nuance.com , http://www.ukp.tu-darmstadt.de
Abstract
We propose a supervised lexical substitu-
tion system that does not use separate clas-
sifiers per word and is therefore applicable
to any word in the vocabulary. Instead of
learning word-specific substitution patterns, a
global model for lexical substitution is trained
on delexicalized (i.e., non lexical) features,
which allows to exploit the power of super-
vised methods while being able to general-
ize beyond target words in the training set.
This way, our approach remains technically
straightforward, provides better performance
and similar coverage in comparison to unsu-
pervised approaches. Using features from lex-
ical resources, as well as a variety of features
computed from large corpora (n-gram counts,
distributional similarity) and a ranking method
based on the posterior probabilities obtained
from a Maximum Entropy classifier, we im-
prove over the state of the art in the LexSub
Best-Precision metric and the Generalized Av-
erage Precision measure. Robustness of our
approach is demonstrated by evaluating it suc-
cessfully on two different datasets.
1 Introduction
In recent years, the task of automatically providing
lexical substitutions in context (McCarthy and Nav-
igli, 2007) received much attention. The premise
to be able to replace words in a sentence with-
out changing its meaning gave rise to applications
like linguistic steganography (Topkara et al, 2006;
Chang and Clark, 2010), semantic text similarity
(Agirre et al, 2012), and plagiarism detection (Gipp
et al, 2011).
Lexical substitution, a special form of contex-
tual paraphrasing where only a single word is re-
placed, is closely related to word sense disambigua-
tion (WSD): polysemous words have possible sub-
stitutions reflecting several senses, and the correct
sense has to be picked to avoid spurious system be-
havior. However, no explicit word sense inventory is
required for lexical substitution (Dagan et al, 2006).
The prominent tasks in a lexical substitution sys-
tem are generation and ranking, i.e. to generate a set
of possible substitutions for the target word and then
to rank this set of possible substitutions according to
their contextual fitness. The task to generate a high
quality set of possible substitutions is challenging in
itself, for two reasons. First, the available lexical
resources are seldom complete in listing synonyms.
Second, manually annotated substitutions show that
not all synonyms of a word are appropriate in a given
context, and many good substitutions have other lex-
ical relation than synonymy to the original word.
In this work, we present a supervised lexical sub-
stitution system that, unlike the usual lexical sam-
ple supervised approaches, can produce substitu-
tions for targets that are not contained in the train-
ing material. We reach this by using non-lexical
features from heterogeneous evidence, including
lexical-semantic resources and distributional simi-
larity, n-gram and shallow syntactic features based
on large, unannotated background corpora. In light
of the existence of lexical resources such as Word-
Net (Fellbaum, 1998) or machine readable dictio-
naries that can serve as the source for lexical infor-
mation, and with the ever-increasing availability of
large unannotated corpora for many languages and
1131
domains, our proposal enables us to leverage the
quality gain of supervised machine learning while
generalizing over a large vocabulary through the
avoidance of lexicalized features. Using a single
classifier for all substitution targets in this way re-
sults in an all-words substitution system. As our re-
sults demonstrate, our model improves over the state
of the art in lexical substitution with practically no
open parameters that have to be optimized and se-
lected carefully according to the dataset at hand.
2 Related Work
Previous works in lexical substitution either ad-
dress both the generation and the ranking tasks, and
are therefore applicable to any word without pre-
labeled data (c.f. the Semeval 2007 task (McCarthy
and Navigli, 2007) and related work) or focus on
the more challenging ranking step only (c.f. Erk
and Pado? (2008) and related work). The latter ap-
proaches take the list of possible substitutions di-
rectly from the testing data as a workaround to gen-
erating the possible substitutions, and merely evalu-
ate the ranking capabilities of these methods.
The most accurate lexical substitution systems
use supervised machine learning to train (and test)
a separate classifier per target word, using lexical
and shallow syntactic features. These systems rely
on the existence of a large number of annotated
examples (i.e. sentences together with the con-
textually valid substitutions) for each word. Bie-
mann (2012) describes a supervised lexical sub-
stitution system for frequent nouns. Exploiting a
large amount of sense tagged examples and (sense-
specific) data annotated with substitutions, an ac-
curate coarse-grained WSD model is trained and
then the most frequent substitutions of the predicted
sense are assigned to the new occurrences of the tar-
get words. The results demonstrate that lexical sub-
stitution of noun targets can be attained with very
high precision (over 90%) if sufficient training ma-
terial is available. However, due to high annotation
costs, methods that do not require labeled training
data per target scale better to a large vocabulary.
Knowledge-based systems like e.g. by Hassan et
al. (2007), who use a number of knowledge-based
and unsupervised methods and combine these clues
using a voting scheme, do not need training data per
target. The combination of different signals, how-
ever, has to be done manually. Unsupervised sys-
tems that rely on distributional similarity (Thater et
al., 2011) or topic models (Li et al, 2010) are single
signals in this sense, and their development is guided
by the performance and observations on standard
datasets. Such signals, however, can also be kept
simple avoiding any task-specific optimization and
can be integrated in a single model for all words us-
ing a limited amount of training data and delexical-
ized features, as in Senselearner (Mihalcea and Cso-
mai, 2005) for weakly supervised all-words disam-
biguation. This way, task specific development can
be replaced by a machine learning component and
the resulting model applies also to unseen words,
similar to the knowledge-based approaches.
2.1 Full Lexical Substitution Systems
Related works that address the lexical substitution
problem according to the settings established by the
English Lexical Substitution Task (McCarthy and
Navigli, 2007) at Semeval 2007 (LexSub) typically
employ a simple ranking strategy based on local
n-gram frequencies and focus on finding an opti-
mal source of possible substitutions, as the selec-
tion of lexical resources has largest impact on the
overall system performance: Sinha and Mihalcea
(2009) systematically explored the benefits of mul-
tiple lexical resources and found that a supervised
combination of several resources lead to statisti-
cally significant improvements in accuracy (about
3.5% points over the best single resource, WordNet).
They tested LSA (Deerwester et al, 1990), ESA
(Gabrilovich and Markovitch, 2007) and n-gram fre-
quencies for contextualization and found n-gram fre-
quencies to be more effective than dimensionality
reduction techniques by a large margin. Their im-
provements were obtained by supervised learning on
the combination of several lexical resources. Our
work, on the other hand, is concerned with using
more advanced features and we obtain significant
improvements based on a diverse set of features and
a different learning setup: we train a model for con-
textualization, rather than to combine substitutions
from several different resources.
A recent work by Sinha and Mihalcea (2011) used
an approach based on graph centrality to rank the
candidates and achieved comparable performance
1132
to n-gram-frequency-based ranking. To summarize,
the use of n-gram frequencies for ranking and Word-
Net as the (most appropriate single) source of syn-
onyms is competitive to more complex solutions and
provides a simple and strong lexical substitution sys-
tem. This motivated the follow-up work by Chang
and Clark (2010) to use WordNet and n-grams in a
linguistic steganography application and this moti-
vates us to use this method as our baseline.
2.2 Ranking Word Meaning in Context
Another prominent line of related work focused
solely on the accurate ranking of a pre-given set of
possible synonyms, according to their plausibility as
a substitution in a given context. Typically, lexi-
cal substitution data is used for evaluation purposes,
taking the candidate substitutions directly from the
test data. This choice is motivated by the assump-
tion that better semantic models should rank near-
synonyms more accurately according to how they fit
in the original word?s context.
Erk and Pado? (2008) proposed the use of multiple
vector representations of words, where the basic rep-
resentation corresponds to a standard co-occurrence
vector, while further vectors are used to characterize
words according to their inverse selectional prefer-
ence statistics for typical dependency relations. The
representation of a word in its context is computed
via combining the basic representation of a word
with the inverse selectional preference vectors of its
related words from the context. Ranking is done by
comparing vectors of possible substitutions with the
substitution target. Thater et al (2010) took a sim-
ilar approach but used second order co-occurrence
vectors and report improved performance.
An exemplar-based approach is presented by Erk
and Pado? (2010) and Reisinger and Mooney (2010b)
to model word meaning with respect to its context:
instead of representing the word and the context as
separate vectors and combining them, a set of word
occurrences in similar contexts is picked first, and
then only these exemplars are used to represent the
word in context. While this approach provides good
results with relatively simple and transparent mod-
els, each occurrence of a word has a unique repre-
sentation (that can only be computed at testing time),
and it is computationally expensive to scale these
models to a large number of examples.
Dinu and Lapata (2010) used a bag of words la-
tent variable model to characterize the meaning of a
word as a distribution over a set of latent variables
(that is, probabilistic senses). Contextualized repre-
sentation of word meaning is then attained by con-
ditioning the model on the context words in which
the target word occurs. A similar approach has
been evaluated for word similarity (Reisinger and
Mooney, 2010a) and word sense disambiguation (Li
et al, 2010).
Although our main goal here is to develop a full-
fledged lexical substitution system, we mainly fo-
cus on the construction of better ranking models
based on supervised machine learning and delexi-
calized features that scale well for unseen words.
This approach has similar properties (applicability
to all words without word-specific training data) to
the knowledge-based and unsupervised models de-
scribed above, so we will also refer to these systems
for comparison.
3 Datasets
In our work, we use two major freely available
datasets that contain human-annotated substitutions
for single words in their full-sentence context.
3.1 LexSub dataset
This dataset was introduced in the Lexical Substi-
tution task at Semeval 20071. It consists of 2002
sentences for a total of 201 words (10 sentences
per word, but 8 sentences does not have gold stan-
dard labels). Each sentence was assigned to 5 na-
tive speaker annotators, who entered as many para-
phrases or substitutions as they found appropriate
for the word in context. Paraphrases are assigned a
weight (or frequency) that denotes how many anno-
tators suggested that particular word as a substitute.
3.2 TWSI
A similar, but larger dataset is the Turk Bootstrap
Word Sense Inventory (TWSI2, (Biemann, 2012)).
The data was collected through a three-step crowd-
sourcing process and comprises 24,647 sentences
1download at http://nlp.cs.swarthmore.edu/
semeval/tasks/task10/data.shtml
2http://www.ukp.tu-darmstadt.de/data/
lexical-resources/twsi-lexical-substitutions/
1133
for a total of 1,012 target nouns, where crowdwork-
ers have provided substitutions for a target word in
context. We did not use the roughly 150,000 sense-
labeled contexts and the sense inventory of this re-
source, i.e. this dataset ? as used in this study ? is
transparent to the LexSub data. For the majority of
the data, responses from 3 annotators were collected
per context, and there are on average 24 sentences
per target word in the dataset. Due to this, the aver-
age weight of good substitutions is somewhat lower
than in the LexSub dataset (1.27 vs. 1.58 in Lex-
Sub), but the average number of unique substitutions
per target word is slightly higher in TWSI (average
of 22 words / target vs. 17 in LexSub).
3.3 Source of Possible Substitutions
In our lexical substitution system, we used WordNet
as the source for candidate synonyms. For each sub-
stitution target, we took all synonyms from all of the
word?s WordNet synsets as candidates, together with
the words from synsets in similar to, entailment and
also see relation to these synsets3. In order to evalu-
ate and compare our ranking methodology in a trans-
parent way with those studies that focused just on
the candidate ranking task, we also performed exper-
iments where we pooled the set of candidates from
the gold standard dataset. This setting ensures that
each set contains a positive candidate, and that all
human-suggested paraphrases are available as posi-
tive examples for a given sentence.
The main characteristics of the datasets (with both
WordNet or the gold standard as the source of candi-
date substitutions) are summarized in Table 1. The
rows in the table indicate the source of possible sub-
stitutions, number of target words, instances with at
least one non-multiword possible substitution, aver-
age size of candidate sets, and number of instances
with no good candidate and frequency of different
labels. The labels denote how many annotators pro-
posed a particular word as substitution in the given
context and can be interpreted as a measure of good-
ness: the higher the value, the better the candidate
fits in the context. Similarly, the label 0 denotes the
total number of negative examples in our datasets,
i.e. bad substitutions ? words that belong to the can-
3This candidate set was found best for WordNet by Martinez
et al (2007).
LexSub TWSI
source WN Gold St. WN Gold St.
# words 201 201 908 1007
#inst 2002 2002 22543 24643
avg. set 21 17 7.5 22
# empty 508 17 11165 620
#0 39465 27300 151538 443993
#1 1302 4698 10678 77417
#2 582 1251 4171 17585
#3 308 571 2069 5629
#4 212 319 74 325
#5+ 129 179 121 411
Table 1: Details of the datasets: WN=WordNet
didate set for a particular target word, but are not
listed as good substitutions in the given context in
the dataset.
4 Methodology
4.1 Experimental Setup and Evaluation
We follow previous works in lexical substitution and
evaluate our models using the Generalized Average
Precision (GAP) (Kishida, 2005) measure which as-
sesses the quality of the entire ranked list. In addi-
tion, we also provide the precision of our system at
the first rank (P@1), i.e. the percentage of correct
paraphrases at rank 1. This is a realistic evaluation
criterion for many applications, such as paraphras-
ing for linguistic steganography: it is the highest-
ranked candidate that can be used to replace the orig-
inal word (the manipulated text should preserve the
original meaning) and there is no straightforward
way to exploit multiple correct answers. In addition,
we also provide the Semeval 2007 best precision4
metric (McCarthy and Navigli, 2007) for the Lex-
Sub dataset for comparison to Semeval 2007 partic-
ipants. This metric also evaluates the first guess of
a system (per context), but gives less credit to easier
contexts, where several good options exist. This fact
motivates us to use P@1 rather than the best preci-
sion metric in all other experiments.
4Since our system always provides an answer, the Semeval
2007 best recall equals best precision.
1134
4.2 Machine Learning on Delexicalized
Features
After the list of potential substitutions is obtained,
lexical substitution is cast as a ranking task where
the goal is to prefer contextually plausible substitu-
tions over implausible ones. The goal of this study
is to learn a ranking model that is applicable to any
word, for which a list of synonyms is available. A
supervised model can generalize over the example
target words in the datasets, if aggregate features
can be defined that have the same semantics regard-
less of the actual context, target word or candidate
substitution they are computed from. Having such a
representation, one can expect to learn patterns that
generalize over the words/contexts seen in the train-
ing dataset, and thus the setup constitutes a super-
vised all-word system.
To simulate an all-word scenario, we perform a
10-fold cross validation in our experiments, splitting
the dataset into equal-sized folds randomly on the
target word level. That is, all sentences for a particu-
lar target word fall into the same fold and thus either
the training or the test set (but never both). This way
we always train and test the model on disjoint sets of
words and as such, the learnt models cannot exploit
word-specific properties. This makes our results re-
alistic estimates of an open vocabulary paraphrasing
system, where we would apply the models (mostly)
to words that were not in the training material.
4.2.1 Machine Learning Model
In our experiments, we used a Maximum Entropy
(MaxEnt) classifier model implemented in the Mal-
let (McCallum, 2002) package and trained a binary
classifier to predict if a given substitution is valid in
a particular context or not.
We chose to use Maximum Entropy models for
two main reasons: MaxEnt is not sensitive to param-
eter settings and handles correlated features well,
which is crucial in our situation where many features
are highly correlated.
Due to the low number of positive examples in the
datasets (see Table 1, labels 1-5+) and to emphasize
better paraphrases suggested by several annotators,
we assigned a weight to positive instances during the
training process equal to their score (the number of
annotators suggesting that paraphrase; the weight of
negative instances was set to 1).
The output of the MaxEnt classifier is a posterior
probability distribution for each target/substitution
pair, denoting the probabilities of the instance to
be a good or a bad substitution, given the feature
values that describe both the words and their con-
text. The ranking over a set of candidates can be
naturally induced based on their posterior scores for
the positive class, i.e. a number that denotes ?how
good the candidate is, given the context?. That is,
the best substitution candidate s (characterized by a
set of features F) from a set of candidates S is ob-
tained as argmaxs?S[P (good|F)], the next best as
the argmax of the remaining elements, and so on.
This pointwise approach to subset ranking (Cos-
sock and Zhang, 2008) is arguably simplistic, but
several studies (c.f. Li et al (2007; Busa-Fekete
et al (2011)) found this approach to perform rea-
sonably well given that the model provides accurate
probability estimates, which is the case for MaxEnt.
4.3 Delexicalized Features
We use heterogeneous sources of information to de-
scribe each target word/candidate substitution pair
in its context. The most important features describe
the syntagmatic coherence of the substitute in con-
text, measured as local n-gram frequencies obtained
from web data, in a sliding window around the tar-
get word. In addition we use features to describe the
(non-positional, i.e. non-local) distributional simi-
larity of the target and its candidate substitution in
terms of sentence level co-occurrence statistics col-
lected from newspaper texts. A further set of fea-
tures captures the properties of the target and can-
didate word in a lexical resource (WordNet), such
as their number of senses, how frequent senses are
synonymous, etc. Lastly, we use part of speech pat-
terns to describe the target word in context. This
way, unlike many other methods suggested in previ-
ous works (Thater et al, 2011; Erk and Pado?, 2008),
our model does not require deep syntactic analysis
of the test sentences in order to rank the candidates.
Even though we make intensive use of WordNet to
compute some of our feature functions, this is not
a severe restriction for a practical paraphrasing sys-
tem: one has to have a decent lexical resource in or-
der to mine a reasonable set of candidate synonyms
and such a resource can also serve as a source for
features in the classifier. The rest of the feature func-
1135
tions exploit only large unannotated corpora and a
POS tagger at application time.
For a target word t, and candidate substitution si
from a set of candidates S, we used the features be-
low. Each numeric feature is used both in the form
given below, and set-wise scaled to [0, 1] (we leave
it to the classifier to pick the more useful form of
information). For the LexSub dataset, each feature
is defined once for all instances, and once specific
to the four POS categories in the dataset. That is
each instance would have the described features de-
fined twice, once the general form (defined for every
instance) and once the form according to the pre-
dicted POS category of the target word. This allows
the model to learn general and also POS-specific
patterns based on the information described below
(i.e. frequency thresholds, distributional properties
etc. for nouns or verbs etc. in particular). We denote
the left and right contexts around t and all words in
the sentence except t with cl, cr and c, respectively)
4.3.1 Lexical Resource Features
We used Wordnet 3.0 as the source for substi-
tution candidates and as a source for delexicalized
features. We found the measure of ambiguity and
the sense number to provide useful information in
a more general context: it is informative how many
senses a word has, and it is informative from which
sense number of the substitution target the substitu-
tion candidate came from, since they are ordered by
corpus frequency. In addition, we used the synsets
IDs of the words? hypernyms as features, which can
capture more general semantics (the word to replace
is ?animate?, ?abstract?, etc.). The following features
were extracted from WordNet:
? number of senses of t and si in WordNet
? the sense numbers of t and si which are syn-
onymous (in case they are direct synonyms, c.f.
WN sense numbers encode sense frequencies)
? binary features for synset IDs of the hypernyms
of the synset containing t and si (this feature
type did not significantly improve results)
4.3.2 Corpus-based Features
In order to create a Distributional Thesaurus (DT)
similar to Lin (1998), we parsed a source corpus
of 120M sentence English newspaper texts from
the LCC5 (Richter et al, 2006) with the Stanford
parser (de Marneffe et al, 2006) and used depen-
dencies to extract features for words: each depen-
dency triple (w1, r, w2) denoting a dependency of
type r between words w1 and w2 results in a fea-
ture (r, w2) characterizing w1, and a feature (w1, r)
characterizing w26. After counting the frequency
of each feature for each word, we apply a signifi-
cance measure (log-likelihood test (LL), (Dunning,
1993)), rank features per word according to their
significance, and prune the data, keeping only the
1000 most salient features (Fw) per word7. The sim-
ilarity of two words is then given by the number
of their common features. Our distributional the-
saurus provides a list of the 1000 most salient fea-
tures and a ranked list of up to 200 similar words
(simw, based on the number of shared features) for
all words above a certain frequency in the source
corpus. We compute the following features to char-
acterize a target word / substitution pair:
? To what extent the context c characterizes si:?
c?Fsi
LL(Fsi (c))
?
sj?S
?
c?Fsj
LL(Fsj (c))
? percentage of shared words among
the top k similar words to t and
to si:
|simt|k?|simsi |k
max(|simt|k,|simsi |k)
, for k =
1, 5, 10, 20, 50, 100, 2008
? percentage of shared salient features among the
top k features of t and si, globally and re-
stricted to the words from the target sentence:
|Ft|k?|Fsi |k
max(|Ft|k,|Fsi |k)
and
|Ft|k?|Fsi |k?|c|
|c| , for k =
1, 5, 10, 20, 50, 100, 1000
? boolean feature indicating whether si ? simt
or not (in top 100 similar words)
5http://corpora.informatik.uni-leipzig.de/
6open source implementation and data available at
http://sourceforge.net/p/jobimtext
7The pruning operation greatly reduces runtime at the-
saurus collection, rendering memory reduction techniques like
(Charikar et al, 2004) as unnecessary.
8The various values for k trade off the salience of this fea-
ture for coverage: only very few substitutions have overlap in
the top 1-5 similar words set, but if this happens, it is a very
strong indicator of contextual fitness, whereas overlap within
the top 100-200 similar words is present for much more tar-
get/substitution pairs, but it is a weaker indicator of fitness.
1136
4.3.3 Local n-gram Features (from Web 1T)
Syntagmatic coherence, measured as the n-gram
frequency of the context with the candidate substi-
tution serves as the basis of ranking in the best Se-
meval 2007 system (Giuliano et al, 2007), which is
also our baseline method here. We use the same n-
grams as features in our supervised model:
? 1-5-gram frequencies in a sliding window
around t: freq(clsicr)/freq(cltcr), normal-
ized w.r.t t
? 1-5-gram frequencies in a sliding window
around t: freq(clsicr)/
?
freq(clScr), nor-
malized w.r.t. S
? for each of x in {?and?, ?or?, ?,?}, 3-5-
gram frequencies in a sliding window around
t: freq(cltxsicr)/freq(cltcr) (how frequently
the target and candidate are part of a list or con-
junctive phrase)
4.3.4 Shallow Syntactic Features
We also use part of speech information (from
TreeTagger (Schmid, 1994)) as features, in order
to enable the model to learn POS-specific patterns.
This is especially important for the LexSub dataset,
which contains examples from all major parts of
speech (the TWSI dataset contains only noun tar-
gets). Specifically, we use:
? 1-3-grams of main POS categories in a window
around t, e.g. NVV for a noun, verb, verb con-
text
? Penn Treebank POS code of t
4.3.5 Example
For clarity, we exemplify our delexicalized fea-
tures briefly. Using WordNet as a source for the
word bright, we considered the 11 words brilliant,
vivid, smart, burnished, lustrous, shining, shiny,
undimmed, brilliant, hopeful, promising from the
synsets of bright, and 64 further words from its re-
lated synsets (e.g. intelligent, glimmery, polished,
happy, ...) as potential paraphrases. That is, for
the sentence ?He was bright and independent and
proud.?, where the human annotators listed intelli-
gent, clever as suitable paraphrases, our system had
1 correct (intelligent) and 74 incorrect substituions
in the candidate set (that is, clever is not found in
WordNet in the above described way). The substitu-
tion intelligent in this context is characterized by a
total of 178 active features. Of those, 112 features
are based on local n-gram features (Sect. 4.3.3),
where the large number stems from different n in
n-gram, as well as the different variants of normal-
ization and copies for the particular POS (here: JJ)
and for all POS. For instance, ?bright? and ?intelli-
gent? are frequently occurring in comma-separated
enumerations, and ?intelligent? fits well in the target
context based on n-gram probabilities. The second
largest block of features is constituted by 48 active
distributional similarity features (Sect. 4.3.2), which
are also available per POS and for different normal-
izations. Here is e.g. captured that the candidate
has a high distributional similarity to the target with
respect to our background corpus. The 12 shallow
syntatic features (Sect 4.3.4) capture various present
POS patterns around the target, and the 6 resource-
based features (Sect. 4.3.1) e.g. inform about the
number of senses of the target (10) and the candi-
date (4).
4.4 Results
Now, we describe our results in detail. First we com-
pare our system on two datasets with a competitive
baseline, which uses the same candidate set as our
ML-based model, and the simple and effective rank-
ing function based on Google n-grams described by
Giuliano et al (2007). Later on we analyze how the
four major feature groups contribute to the results in
a feature ablation experiment, and then we provide
a detailed and thorough comparison to earlier works
that are similar to the model presented here and used
the same dataset (LexSub) for evaluation.
4.4.1 Semeval 2007 Lexical Substitution
In Table 2 we report results on the LexSub dataset.
As can be seen, our model outperforms the baseline
by a significant margin (p < 0.01 for all measures,
using a paired t-test for significance). Both the over-
all rankings and the P@1 scores are of higher quality
than the rankings based only on n-grams.
4.4.2 Turk Bootstrap Word Sense Inventory
The results on the TWSI dataset are provided in
Table 3. Our model outperforms the baseline in all
1137
cand. from WN from Gold St.
GAP P@1 GAP P@1
Baseline 36.8 31.1 46.9 49.5
Our model 43.8 40.2 52.4 57.7
Table 2: Comparison to the baseline on LexSub 2007.
cand. from WN from Gold St.
GAP P@1 GAP P@1
Baseline 33.8 28.2 44.4 44.5
Our model 36.6 32.4 47.2 49.5
Table 3: Comparison to the baseline on the TWSI dataset.
the comparisons similar to the LexSub dataset. The
differences are not so pronounced but still highly
significant (p < 0.01). This is consistent with the
observation by several Semeval 2007 participants
and with a per-POS analysis of our results on Lex-
Sub: the ranking task seems to be more challenging
for nouns than for other parts of speech. When us-
ing WordNet, for about half (11165/22543) of the
instances, individual scores are 0 (cf. Table 1). For
the other half, avg. P@1 score is around 0.7, which
results in 0.324 overall. Note that the task of ranking
in avg. 7.5 items is considerably easier than rank-
ing in avg. 22 items, which explains the high P@1
scores for cases where good candidates exist ? also,
a random ranker would score higher in this case.
These results demonstrate that the proposed
delexicalized approach is superior to a competitive
baseline across two datasets.
4.5 Feature Exploration
We explored the contribution of our various fea-
ture types on the LexSub dataset with candidate set
from the gold standard. Our MaxEnt model rely-
ing only on local n-gram frequency features, i.e. the
GAP P@1
w/o n-gram features 47.3 48.9
w/o distr. thesaurus 49.8 55.0
w/o POS features 51.6 56.3
w/o WN features 51.7 57.0
Our model (all) 52.4 57.7
Table 4: Feature ablation experiment (on LexSub dataset,
with candidates from Gold Standard).
same information as the baseline model, achieved a
GAP score of 48.3 and P@1 of 52.1, respectively.
This result is significantly better than the baseline
(p < 0.01), i.e. the machine learnt ranking model
is better than a state-of-the-art handcrafted ranker
based on the same data. All single feature groups,
when combined with n-grams, lead to significant im-
provements (p < 0.01), which proves the usefulness
of each feature group. In order to assess the contri-
bution of each group to the overall system perfor-
mance, we performed a feature ablation experiment.
That is, we trained the MaxEnt model with using
all feature groups (this equals the model in Table 2)
and then with leaving each of the feature groups out
once. As can be seen, all feature groups improve the
overall results in a noticeable way, i.e. their contri-
bution is complementary.
4.5.1 Comparison to Previous Works
In Table 5 we compare our method with previous
works in the field, using the LexSub dataset.
candidates from WN from Gold Standard
Best-P GAP
Pado?Erk10 38.6
Giuliano 12.93 DinuLapata 42.9
Martinez 12.68 Thater10 46.0
Sinha 13.60 Thater11 51.7
Baseline 11.75 Baseline 46.9
Our model 15.94 Our model 52.4
Table 5: Comparison to previous works (LexSub dataset).
In the left column of Table 5, we compare the per-
formance of our system to representative Semeval
2007 participants, namely Martinez et al (2007) and
Giuliano et al (2007). In order to make a fair com-
parison, we report scores for the official test data
of Semeval 2007, using a 10-fold cross-validation
scheme. Martinez et al (2007) developed their sys-
tem based on WordNet and we use the same can-
didate set here that they proposed in their system
description. Our reimplementation of (Giuliano et
al., 2007) performs below the original scores, due
to the more restricted source of substitution can-
didates (they use more lexical resources), yet uses
the same ranking methodology based on Google n-
grams that we adopted here as our baseline. We also
report the best previous result for this task, which
1138
was achieved via the (supervised) combination of
lexical resources to improve the performance (Sinha
and Mihalcea, 2009). Our model outperforms this
result by a large margin for the best-precision eval-
uation (mode-P, precision measured on those exam-
ples where there is a clear best substitution provided
by humans was 26.3%, compared to 21.3% reported
by Sinha and Mihalcea (2009). This is especially
promising in light of the fact that we use only a sin-
gle source (WordNet) for synonyms and achieve our
improvements through more advanced delexicalized
features in an improved ranking model. Sinha and
Mihalcea (2009), on the other hand, used compara-
bly simple features for contextualization, of which
n-gram features were deemed most successful. As
Sinha and Mihalcea (2009) showed improvements
through utilizing several synonym sources, a combi-
nation of their approach with ours should allow for
further improvements in the future.
In the right column of Table 5, we compare our
model to previous works that addressed only the
ranking task, and report performance on the whole
dataset (i.e. trial and test). As can be seen, the
methodology proposed here outperforms previous
ranking models, without the need to develop a high-
quality ranking model by hand, and without the need
to parse the test sentences. Our delexicalized super-
vised model only requires the development of fea-
tures, and achieves excellent results without major
task-specific tuning or customization: we omitted
the optimization of the feature set and the parame-
ters of the learning model. This fact makes us as-
sume that the proposed model can be applied more
quickly and easily than previous models that have
several important design aspects to choose from.
5 Conclusion and Future Work
In this study, we presented a supervised approach to
all-words lexical substitution based on delexicalized
features, which enables us to fully exploit the power
of supervised models while ensuring applicability to
a large, open vocabulary.
Results demonstrate the feasibility of this method:
our MaxEnt-based ranking approach improved over
the baseline in all settings and yielded ? to our
knowledge ? the best scores for lexical substitu-
tion with automatically gathered synonyms on the
Semeval 2007 LexSub dataset. Also, it performed
slightly better than the state of the art for candidates
pooled from the gold standard without any parame-
ter tuning or empirical design choices.
In this study, we established transparency be-
tween Semeval-style and ranking-only studies in
lexical substitution ? two lines of work that were dif-
ficult to compare in the past. Further, we observe
similar improvements on two different datasets,
showing the robustness of the approach.
While previous works showed the potential of
more/improved lexical resources for lexical substi-
tution, we improved over the best Semeval-style per-
formance just by exploiting an improved ranking
model over a standard WordNet-based candidate set.
These results indicate that improvements from lexi-
cal resources and better ranking models are additive,
thus we want to add more lexical resources in our
system in the future.
Of course there are several other ways to improve
further the work described here. First of all, simi-
lar to the best ranking approaches (e.g. Thater et al
(2011)), one could use contextualized feature func-
tions to make global information from the distri-
butional thesaurus more accurate. Instead of using
globally calculated similarities, information from
the distributional thesaurus could be contextualized
via constraining the statistics with words from the
context.
Other natural ways to improve the model de-
scribed here are to make heavier use of parser infor-
mation or to employ pair-wise or list-wise machine
learning models (Cao et al, 2007), which are specif-
ically designed for subset ranking. Lastly, while in-
trinsic evaluation of lexical substitution is important,
we would like to show its practicability in tasks such
as steganography or information retrieval.
Acknowledgements
This work has been supported by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-o?konomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities, and by the German Ministry of
Education and Research under grant SiDiM (grant
no. 01IS10054G).
1139
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
Ro?bert Busa-Fekete, Bala?zs Ke?gl, E?lteto? Yama?s, and
Gyo?rgi Szarvas. 2011. A robust ranking method-
ology based on diverse calibration of adaboost. In
European Conference on Machine Learning, volume
LNCS, 6911, pages 263?279.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24rd International Conference on Machine Learning,
pages 129?136.
Ching-Yun Chang and Stephen Clark. 2010. Practi-
cal linguistic steganography using contextual synonym
substitution and vertex colour coding. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1194?1203, Cam-
bridge, MA.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312(1):3?15.
D. Cossock and T. Zhang. 2008. Statistical analysis of
Bayes optimal subset ranking. IEEE Transactions on
Information Theory, 54(11):5140?5154.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 449?456, Sydney, Australia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, Genova, Italy.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92?
97, Uppsala, Sweden.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Bela Gipp, Norman Meuschke, and Joeran Beel. 2011.
Comparative Evaluation of Text- and Citation-based
Plagiarism Detection Approaches using GuttenPlag.
In Proceedings of 11th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries (JCDL?11), pages 255?258,
Ottawa, Canada. ACM New York, NY, USA. Avail-
able at http://sciplore.org/pub/.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. FBK-irst: Lexical substitution task exploit-
ing domain and syntagmatic coherence. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 145?148, Prague,
Czech Republic.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. UNT: SubFinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 410?413, Prague, Czech Republic.
Kazuaki Kishida. 2005. Property of Average Precision
and Its Generalization: An Examination of Evaluation
Indicator for Information Retrieval Experiments. NII
technical report. National Institute of Informatics.
Ping Li, Christopher J.C. Burges, and Qiang Wu. 2007.
McRank: Learning to rank using multiple classifica-
tion and gradient boosting. In Advances in Neural In-
formation Processing Systems, volume 19, pages 897?
904. The MIT Press.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
1140
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 1138?1147.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and the 17th International Conference on Compu-
tational Linguistics, volume 2 of ACL ?98, pages 768?
774, Montreal, Quebec, Canada.
David Martinez, Su Nam Kim, and Timothy Bald-
win. 2007. MELB-MKB: Lexical substitution system
based on relatives in context. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 237?240, Prague, Czech
Republic.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 48?53,
Prague, Czech Republic.
Rada Mihalcea and Andras Csomai. 2005. Senselearner:
word sense disambiguation for all words in unre-
stricted text. In Proceedings of the ACL 2005 on Inter-
active poster and demonstration sessions, ACLdemo
?05, pages 53?56.
Joseph Reisinger and Raymond Mooney. 2010a. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173?
1182, Cambridge, MA.
Joseph Reisinger and Raymond J. Mooney. 2010b.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
109?117, Los Angeles, California.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proceesings of the IS-LTC 2006. Ljubljana, Slove-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404?410, Borovets, Bulgaria.
Ravi Som Sinha and Rada Flavia Mihalcea. 2011. Using
centrality algorithms on directed graphs for synonym
expansion. In R. Charles Murray and Philip M. Mc-
Carthy, editors, FLAIRS Conference. AAAI Press.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948?957, Uppsala,
Sweden.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of the Fifth Interna-
tional Joint Conference on Natural Language Process-
ing : IJCNLP 2011, pages 1134?1143, Chiang Mai,
Thailand. MP, ISSN 978-974-466-564-5.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th workshop on Multimedia and security, pages 164?
174, New York, NY, USA. ACM.
1141
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?6,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
WebAnno: A Flexible, Web-based and Visually Supported
System for Distributed Annotations
Seid Muhie Yimam1,3 Iryna Gurevych2,3 Richard Eckart de Castilho2 Chris Biemann1
(1) FG Language Technology, Dept. of Computer Science, Technische Universita?t Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universita?t Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
We present WebAnno, a general pur-
pose web-based annotation tool for a wide
range of linguistic annotations. Web-
Anno offers annotation project manage-
ment, freely configurable tagsets and the
management of users in different roles.
WebAnno uses modern web technology
for visualizing and editing annotations in
a web browser. It supports arbitrarily
large documents, pluggable import/export
filters, the curation of annotations across
various users, and an interface to farming
out annotations to a crowdsourcing plat-
form. Currently WebAnno allows part-of-
speech, named entity, dependency parsing
and co-reference chain annotations. The
architecture design allows adding addi-
tional modes of visualization and editing,
when new kinds of annotations are to be
supported.
1 Introduction
The creation of training data precedes any sta-
tistical approach to natural language processing
(NLP). Linguistic annotation is a process whereby
linguistic information is added to a document,
such as part-of-speech, lemmata, named entities,
or dependency relations. In the past, platforms
for linguistic annotations were mostly developed
ad-hoc for the given annotation task at hand, used
proprietary formats for data exchange, or required
local installation effort. We present WebAnno, a
browser-based tool that is immediately usable by
any annotator with internet access. It supports an-
notation on a variety of linguistic levels (called an-
notation layers in the remainder), is interoperable
with a variety of data formats, supports annotation
project management such as user management, of-
fers an adjudication interface, and provides qual-
ity management using inter-annotator agreement.
Furthermore, an interface to crowdsourcing plat-
forms enables scaling out simple annotation tasks
to a large numbers of micro-workers. The added
value of WebAnno, as compared to previous an-
notation tools, is on the one hand its web-based
interface targeted at skilled as well as unskilled
annotators, which unlocks a potentially very large
workforce. On the other hand, it is the support for
quality control, annotator management, and adju-
dication/curation, which lowers the entrance bar-
rier for new annotation projects. We created Web-
Anno to fulfill the following requirements:
? Web-based: Distributed work, no installation
effort, increased availability.
? Interface to crowdsourcing: unlocking a very
large distributed workforce.
? Quality and user management: Integrated
different user roles support (administra-
tor, annotator, and curator), inter-annotator
agreement measurement, data curation, and
progress monitoring.
? Flexibility: Support of multiple annotation
layers, pluggable import and export formats,
and extensibility to other front ends.
? Pre-annotated and un-annotated documents:
supporting new annotations, as well as man-
ual corrections of existing, possibly auto-
matic annotations.
? Permissive open source: Usability of our tool
in future projects without restrictions, under
the Apache 2.0 license.
In the following section, we revisit related work
on annotation tools, which only partially fulfill the
aforementioned requirements. In Section 3, the ar-
chitecture as well as usage aspects of our tool are
lined out. The scope and functionality summary
1
of WebAnno is presented in Section 4. Section 5
elaborates on several use cases of WebAnno, and
Section 6 concludes and gives an outlook to fur-
ther directions.
2 Related Work
GATE Teamware (Bontcheva et al, 2010) is prob-
ably the tool that closely matches our requirements
regarding quality management, annotator manage-
ment, and support of a large set of annotation lay-
ers and formats. It is mostly web-based, but the
annotation is carried out with locally downloaded
software. An interface to crowdsourcing platforms
is missing. The GATE Teamware system is heav-
ily targeted towards template-based information
extraction. It sets a focus on the integration of au-
tomatic annotation components rather than on the
interface for manual annotation. Besides, the over-
all application is rather complex for average users,
requires considerable training and does not offer
an alternative simplified interface as it would be
required for crowdsourcing.
General-purpose annotation tools like MMAX2
(Mu?ller and Strube, 2006) or WordFreak (Morton
and LaCivita, 2003) are not web-based and do not
provide annotation project management. They are
also not sufficiently flexible regarding different an-
notation layers. The same holds for specialized
tools for single annotation layers, which we can-
not list here for the sake of brevity.
With the brat rapid annotation tool (Stenetorp
et al, 2012), for the first time a web-based open-
source annotation tool was introduced, which sup-
ports collaborative annotation for multiple anno-
tation layers simultaneously on a single copy of
the document, and is based on a client-server ar-
chitecture. However, the current version of brat
has limitations such as: (i) slowness for docu-
ments of more than 100 sentences, (ii) limits re-
garding file formats, (iii) web-based configuration
of tagsets/tags is not possible and (iv) configuring
the display of multiple layers is not yet supported.
While we use brat?s excellent visualization front
end in WebAnno, we decided to replace the server
layer to support the user and quality management,
and monitoring tools as well as to add the interface
to crowdsourcing.
3 System Architecture of WebAnno
The overall architecture of WebAnno is depicted
in Figure 1. The modularity of the architecture,
Figure 1: System architecture, organized in user,
front end, back end and persistent data storage.
which is mirrored in its open-source implementa-
tion1, makes it possible to easily extend the tool or
add alternative user interfaces for annotation lay-
ers that brat is less suited for, e.g. for constituent
structure. In Section 3.1, we illustrate how differ-
ent user roles are provided with different graphical
user interfaces, and show the expressiveness of the
annotation model. Section 3.2 elaborates on the
functionality of the back end, and describes how
data is imported and exported, as well as our im-
plementation of the persistent data storage.
3.1 Front End
All functionality of WebAnno is accessible via
a web browser. For annotation and visualiza-
tion of annotated documents, we adapted the brat
rapid annotation tool. Changes had to be made to
make brat interoperate with the Apache Wicket,
on which WebAnno is built, and to better integrate
into the WebAnno experience.
3.1.1 Project Definition
The definition and the monitoring of an annota-
tion project is conducted by a project manager (cf.
Figure 1) in a project definition form. It supports
creating a project, loading un-annotated or pre-
annotated documents in different formats2, adding
annotator and curator users, defining tagsets, and
configuring the annotation layers. Only a project
manager can administer a project. Figure 2 illus-
trates the project definition page with the tagset
editor highlighted.
1Available for download at (this paper is based on v0.3.0):
webanno.googlecode.com/
2Formats: plain text, CoNLL (Nivre et al, 2007), TCF
(Heid et al, 2010), UIMA XMI (Ferrucci and Lally, 2004)
2
Figure 2: The tagset editor on the project definition page
3.1.2 Annotation
Annotation is carried out with an adapted ver-
sion of the brat editor, which communicates with
the server via Ajax (Wang et al, 2008) using the
JSON (Lin et al, 2012) format. Annotators only
see projects they are assigned to. The annotation
page presents the annotator different options to set
up the annotation environment, for customization:
? Paging: For heavily annotated documents or
very large documents, the original brat vi-
sualization is very slow, both for displaying
and annotating the document. We use a pag-
ing mechanism that limits the number of sen-
tences displayed at a time to make the perfor-
mance independent of the document size.
? Annotation layers: Annotators usually work
on one or two annotations layers, such as
part-of-speech and dependency or named en-
tity annotation. Overloading the annota-
tion page by displaying all annotation layers
makes the annotation and visualization pro-
cess slower. WebAnno provides an option to
configure visible/editable annotation layers.
? Immediate persistence: Every annotation is
sent to the back end immediately and per-
sisted there. An explicit interaction by the
user to save changes is not required.
3.1.3 Workflow
WebAnno implements a simple workflow to track
the state of a project. Every annotator works on a
separate version of the document, which is set to
the state in progress the first time a document is
opened by the annotator. The annotator can then
mark it as complete at the end of annotation at
which point it is locked for further annotation and
can be used for curation. Such a document cannot
be changed anymore by an annotator, but can be
used by a curator. A curator can mark a document
as adjudicated.
3.1.4 Curation
The curation interface allows the curator to open a
document and compare annotations made by the
annotators that already marked the document as
complete. The curator reconciles the annotation
with disagreements. The curator can either decide
on one of the presented alternatives, or freely re-
annotate. Figure 3 illustrates how the curation in-
terface detects sentences with annotation disagree-
ment (left side of Figure 3) which can be used to
navigate to the sentences for curation.
3.1.5 Monitoring
WebAnno provides a monitoring component, to
track the progress of a project. The project man-
ager can check the progress and compute agree-
ment with Kappa and Tau (Carletta, 1996) mea-
sures. The progress is visualized using a matrix of
annotators and documents displaying which docu-
ments the annotators have marked as complete and
which documents the curator adjudicated. Fig-
ure 4 shows the project progress, progress of in-
dividual annotator and completion statistics.
3
Figure 3: Curation user interface (left: sentences
with disagreement; right: merging editor)
3.1.6 Crowdsourcing
Crowdsourcing is a way to quickly scale annota-
tion projects. Distributing a task that otherwise
will be performed by a controlled user group has
become much easier. Hence, if quality can be en-
sured, it is an alternative to high quality annotation
using a large number of arbitrary redundant anno-
tations (Wang et al, 2013). For WebAnno, we
have designed an approach where a source doc-
ument is split into small parts that get presented
to micro-workers in the CrowdFlower platform3.
The crowdsourcing component is a separate mod-
ule that handles the communication via Crowd-
Flower?s API, the definition of test items and job
parameters, and the aggregation of results. The
crowdsourced annotation appears as a virtual an-
notator in the tool.
Since it is not trivial to express complex anno-
tation tasks in comparably simple templates suit-
able for crowdsourcing (Biemann, 2013), we pro-
ceed by working out crowdsourcing templates and
strategies per annotation layer. We currently only
support named entity annotation with predefined
templates. However, the open and modular archi-
tecture allows to add more crowdsourced annota-
tion layers.
3.2 Back End
WebAnno is a Java-based web application that
may run on any modern servlet container. In mem-
ory and on the file system, annotations are stored
3www.crowdflower.com
Figure 4: Project monitoring
as UIMA CAS objects (Ferrucci and Lally, 2004).
All other data is persisted in an SQL database.
3.2.1 Data Conversion
WebAnno supports different data models that re-
flect the different communication of data between
the front end, back end, and the persistent data
storage. The brat data model serves exchanging
data between the front end and the back end.
The documents are stored in their original for-
mats. For annotations, we use the type system
from the DKPro Core collection of UIMA compo-
nents (Eckart de Castilho and Gurevych, 2009)4.
This is converted to the brat model for visualiza-
tion. Importing documents and exporting anno-
tations is implemented using UIMA reader and
writer components from DKPro Core as plug-ins.
Thus, support for new formats can easily be added.
To provide quick reaction times in the user inter-
face, WebAnno internally stores annotations in a
binary format, using the SerializedCasReader and
SerializedCasWriter components.
3.2.2 Persistent Data Storage
Project definitions including project name and de-
scriptions, tagsets and tags, and user details are
kept in a database, whereas the documents and an-
notations are stored in the file system. WebAnno
supports limited versioning of annotations, to pro-
tect against the unforeseen loss of data. Figure 5
shows the database entity relation diagram.
4code.google.com/p/dkpro-core-asl/
4
Figure 5: WebAnno database scheme
4 Scope and Functionality Summary
WebAnno supports the production of linguistically
annotated corpora for different natural language
processing applications. WebAnno implements
ease of usage and simplicity for untrained users,
and provides:
? Annotation via a fast, and easy-to-use web-
based user interface.
? Project and user management.
? Progress and quality monitoring.
? Interactive curation by adjudicating disagree-
ing annotations from multiple users.
? Crowdsourcing of annotation tasks.
? Configurable annotation types and tag sets.
5 Use Cases
WebAnno currently allows to configure different
span and arc annotations. It comes pre-configured
with the following annotation layers from the
DKPro Core type system:
Span annotations
? Part-of-Speech (POS) tags: an annotation
task on tokens. Currently, POS can be added
to a token, if not already present, and can be
modified. POS annotation is a prerequisite of
dependency annotation (Figure 6).
Figure 6: Parts-of-speech & dependency relations
Figure 7: Co-reference & named entites
? Named entities: a multiple span annotation
task. Spans can cover multiple adjacent to-
kens, nest and overlap (Figure 7), but cannot
cross sentence boundaries.
Arc Annotations
? Dependency relations: This is an arc annota-
tion which connects two POS tag annotations
with a directed relation (Figure 6).
? Co-reference chains: The co-reference chain
is realized as a set of typed mention spans
linked by typed co-reference relation arcs.
The co-reference relation annotation can
cross multiple sentences and is represented in
co-reference chains (Figure 7).
The brat front end supports tokens and sub-
tokens as a span annotation. However, tokens are
currently the minimal annotation units in Web-
Anno, due to a requirement of supporting the TCF
file format (Heid et al, 2010). Part-of-speech an-
notation is limited to singles token, while named
entity and co-reference chain annotations may
span multiple tokens. Dependency relations are
implemented in such a way that the arc is drawn
from the governor to the dependent (or the other
way around, configurable), while co-reference
chains are unidirectional and a chain is formed by
referents that are transitively connected by arcs.
Based on common practice in manual annota-
tion, every user works on their own copy of the
same document so that no concurrent editing oc-
curs. We also found that displaying all annotation
layers at the same time is inconvenient for anno-
tators. This is why WebAnno supports showing
5
and hiding of individual annotation layers. The
WebAnno curation component displays all anno-
tation documents from all users for a given source
document, enabling the curator to visualize all of
the annotations with differences at a time. Unlike
most of the annotation tools which rely on config-
uration files, WebAnno enables to freely configure
all parameters directly in the browser.
6 Conclusion and Outlook
WebAnno is a new web-based linguistic annota-
tion tool. The brat annotation and GUI front end
have been enhanced to support rapidly process-
ing large annotation documents, configuring the
annotation tag and tagsets in the browser, speci-
fying visible annotation layers, separating anno-
tation documents per user, just to name the most
important distinctions. Besides, WebAnno sup-
ports project definition, import/export of tag and
tagsets. Flexible support for importing and ex-
porting different data formats is handled through
UIMA components from the DKPro Core project.
The monitoring component of WebAnno helps the
administrator to control the progress of annotators.
The crowdsourcing component of WebAnno pro-
vides a unique functionality to distribute the an-
notation to a large workforce and automatically
integrate the results back into the tool via the
crowdsourcing server. The WebAnno annotation
tool supports curation of different annotation doc-
uments, displaying annotation documents created
by users in a given project with annotation dis-
agreements. In future work, WebAnno will be en-
hanced to support several other front ends to han-
dle even more annotation layers, and to provide
more crowdsourcing templates. Another planned
extension is a more seamless integration of lan-
guage processing tools for pre-annotation.
Acknowledgments
We would like to thank Benjamin Milde and Andreas
Straninger, who assisted in implementing WebAnno, as well
as Marc Reznicek, Nils Reiter and the whole CLARIN-D F-
AG 7 for testing and providing valuable feedback. The work
presented in this paper was funded by a German BMBF grant
to the CLARIN-D project, the Hessian LOEWE research ex-
cellence program as part of the research center ?Digital Hu-
manities? and by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No. I/82806.
References
Chris Biemann. 2013. Creating a system for lexical substi-
tutions from scratch using crowdsourcing. Lang. Resour.
Eval., 47(1):97?122, March.
Kalina Bontcheva, Hamish Cunningham, Ian Roberts, and
Valentin Tablan. 2010. Web-based collaborative corpus
annotation: Requirements and a framework implementa-
tion. In New Challenges for NLP Frameworks workshop
at LREC-2010, Malta.
Jean Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. In Computational Linguistics,
Volume 22 Issue 2, pages 249?254.
Richard Eckart de Castilho and Iryna Gurevych. 2009.
DKPro-UGD: A Flexible Data-Cleansing Approach to
Processing User-Generated Discourse. In Online-
proceedings of the First French-speaking meeting around
the framework Apache UIMA, LINA CNRS UMR 6241 -
University of Nantes, France.
David Ferrucci and Adam Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Processing in
the Corporate Research Environment. In Journal of Natu-
ral Language Engineering 2004, pages 327?348.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Erhard
Hinrichs. 2010. A Corpus Representation Format for
Linguistic Web Services: the D-SPIN Text Corpus Format
and its Relationship with ISO Standards. In Proceedings
of LREC 2010, Malta.
Boci Lin, Yan Chen, Xu Chen, and Yingying Yu. 2012.
Comparison between JSON and XML in Applications
Based on AJAX. In Computer Science & Service System
(CSSS), 2012, Nanjing, China.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak:
an open tool for linguistic annotation. In Proceedings of
NAACL-2003, NAACL-Demonstrations ?03, pages 17?18,
Edmonton, Canada.
Christoph Mu?ller and Michael Strube. 2006. Multi-level an-
notation of linguistic data with MMAX2. In S. Braun,
K. Kohn, and J. Mukherjee, editors, Corpus Technology
and Language Pedagogy: New Resources, New Tools,
NewMethods, pages 197?214. Peter Lang, Frankfurt a.M.,
Germany.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Re-
public.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proceedings of the Demonstrations at EACL-2012, Avi-
gnon, France.
Qingling Wang, Qin Liu, Na Li, and Yan Liu. 2008. An
Automatic Approach to Reengineering Common Website
with AJAX. In 4th International Conference on Next Gen-
eration Web Services Practices, pages 185?190, Seoul,
South Korea.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013.
Perspectives on Crowdsourcing Annotations for Natural
Language Processing. In Language Resources And Eval-
uation, pages 9?31. Springer Netherlands.
6
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1020?1029,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
That?s sick dude!:
Automatic identification of word sense change across different timescales
Sunny Mitra
1
, Ritwik Mitra
1
, Martin Riedl
2
,
Chris Biemann
2
, Animesh Mukherjee
1
, Pawan Goyal
1
1
Dept. of Computer Science and Engineering,
Indian Institute of Technology Kharagpur, India ? 721302
2
FG Language Technology, Computer Science Department, TU Darmstadt, Germany
1
{sunnym,ritwikm,animeshm,pawang}@cse.iitkgp.ernet.in
2
{riedl,biem}@cs.tu-darmstadt.de
Abstract
In this paper, we propose an unsupervised
method to identify noun sense changes
based on rigorous analysis of time-varying
text data available in the form of millions
of digitized books. We construct distribu-
tional thesauri based networks from data
at different time points and cluster each
of them separately to obtain word-centric
sense clusters corresponding to the differ-
ent time points. Subsequently, we com-
pare these sense clusters of two different
time points to find if (i) there is birth of
a new sense or (ii) if an older sense has
got split into more than one sense or (iii)
if a newer sense has been formed from the
joining of older senses or (iv) if a partic-
ular sense has died. We conduct a thor-
ough evaluation of the proposed method-
ology both manually as well as through
comparison with WordNet. Manual eval-
uation indicates that the algorithm could
correctly identify 60.4% birth cases from
a set of 48 randomly picked samples and
57% split/join cases from a set of 21 ran-
domly picked samples. Remarkably, in
44% cases the birth of a novel sense is
attested by WordNet, while in 46% cases
and 43% cases split and join are respec-
tively confirmed by WordNet. Our ap-
proach can be applied for lexicography, as
well as for applications like word sense
disambiguation or semantic search.
1 Introduction
Two of the fundamental components of a natu-
ral language communication are word sense dis-
covery (Jones, 1986) and word sense disambigua-
tion (Ide and Veronis, 1998). While discovery
corresponds to acquisition of vocabulary, disam-
biguation forms the basis of understanding. These
two aspects are not only important from the per-
spective of developing computer applications for
natural languages but also form the key compo-
nents of language evolution and change.
Words take different senses in different contexts
while appearing with other words. Context plays
a vital role in disambiguation of word senses as
well as in the interpretation of the actual mean-
ing of words. For instance, the word ?bank? has
several distinct interpretations, including that of a
?financial institution? and the ?shore of a river.?
Automatic discovery and disambiguation of word
senses from a given text is an important and chal-
lenging problem which has been extensively stud-
ied in the literature (Jones, 1986; Ide and Vero-
nis, 1998; Sch?utze, 1998; Navigli, 2009). How-
ever, another equally important aspect that has not
been so far well investigated corresponds to one
or more changes that a word might undergo in its
sense. This particular aspect is getting increas-
ingly attainable as more and more time-varying
text data become available in the form of millions
of digitized books (Goldberg and Orwant, 2013)
gathered over the last centuries. As a motivat-
ing example one could consider the word ?sick?
? while according to the standard English dictio-
naries the word is normally used to refer to some
sort of illness, a new meaning of ?sick? refer-
ring to something that is ?crazy? or ?cool? is cur-
rently getting popular in the English vernacular.
This change is further interesting because while
traditionally ?sick? has been associated to some-
thing negative in general, the current meaning as-
sociates positivity with it. In fact, a rock band
by the name of ?Sick Puppies? has been founded
which probably is inspired by the newer sense of
the word sick. The title of this paper has been
motivated by the above observation. Note that
this phenomena of change in word senses has ex-
isted ever since the beginning of human commu-
nication (Bamman and Crane, 2011; Michel et
1020
al., 2011; Wijaya and Yeniterzi, 2011; Mihalcea
and Nastase, 2012); however, with the advent of
modern technology and the availability of huge
volumes of time-varying data it now has become
possible to automatically track such changes and,
thereby, help the lexicographers in word sense dis-
covery, and design engineers in enhancing vari-
ous NLP/IR applications (e.g., disambiguation, se-
mantic search etc.) that are naturally sensitive to
change in word senses.
The above motivation forms the basis of the
central objective set in this paper, which is to de-
vise a completely unsupervised approach to track
noun sense changes in large texts available over
multiple timescales. Toward this objective we
make the following contributions: (a) devise a
time-varying graph clustering based sense induc-
tion algorithm, (b) use the time-varying sense
clusters to develop a split-join based approach for
identifying new senses of a word, and (c) evalu-
ate the performance of the algorithms on various
datasets using different suitable approaches along
with a detailed error analysis. Remarkably, com-
parison with the English WordNet indicates that
in 44% cases, as identified by our algorithm, there
has been a birth of a completely novel sense, in
46% cases a new sense has split off from an older
sense and in 43% cases two or more older senses
have merged in to form a new sense.
The remainder of the paper is organized as fol-
lows. In the next section we present a short re-
view of the literature. In Section 3 we briefly
describe the datasets and outline the process of
co-occurrence graph construction. In Section 4
we present an approach based on graph cluster-
ing to identify the time-varying sense clusters and
in Section 5 we present the split-merge based ap-
proach for tracking word sense changes. Evalu-
ation methods are summarized in Section 6. Fi-
nally, conclusions and further research directions
are outlined in Section 7.
2 Related work
Word sense disambiguation as well as word sense
discovery have both remained key areas of re-
search right from the very early initiatives in nat-
ural language processing research. Ide and Vero-
nis (1998) present a very concise survey of the his-
tory of ideas used in word sense disambiguation;
for a recent survey of the state-of-the-art one can
refer to (Navigli, 2009). Some of the first attempts
to automatic word sense discovery were made by
Karen Sp?arck Jones (1986); later in lexicography,
it has been extensively used as a pre-processing
step for preparing mono- and multi-lingual dictio-
naries (Kilgarriff and Tugwell, 2001; Kilgarriff,
2004). However, as we have already pointed out
that none of these works consider the temporal as-
pect of the problem.
In contrast, the current study, is inspired by
works on language dynamics and opinion spread-
ing (Mukherjee et al, 2011; Maity et al, 2012;
Loreto et al, 2012) and automatic topic detection
and tracking (Allan et al, 1998). However, our
work differs significantly from those proposed in
the above studies. Opinion formation deals with
the self-organisation and emergence of shared vo-
cabularies whereas our work focuses on how the
different senses of these vocabulary words change
over time and thus become ?out-of-vocabulary?.
Topic detection involves detecting the occurrence
of a new event such as a plane crash, a murder, a
jury trial result, or a political scandal in a stream
of news stories from multiple sources and track-
ing is the process of monitoring a stream of news
stories to find those that track (or discuss) the
same event. This is done on shorter timescales
(hours, days), whereas our study focuses on larger
timescales (decades, centuries) and we are inter-
ested in common nouns, verbs and adjectives as
opposed to events that are characterized mostly by
named entities. Other similar works on dynamic
topic modelling can be found in (Blei and Laf-
ferty, 2006; Wang and McCallum, 2006). Google
books n-gram viewer
1
is a phrase-usage graphing
tool which charts the yearly count of selected letter
combinations, words, or phrases as found in over
5.2 million digitized books. It only reports fre-
quency of word usage over the years, but does not
give any correlation among them as e.g., in (Heyer
et al, 2009), and does not analyze their senses.
A few approaches suggested by (Bond et al,
2009; P?a?akk?o and Lind?en, 2012) attempt to aug-
ment WordNet synsets primarily using methods
of annotation. Another recent work by Cook et
al. (2013) attempts to induce word senses and then
identify novel senses by comparing two different
corpora: the ?focus corpora? (i.e., a recent version
of the corpora) and the ?reference corpora? (older
version of the corpora). However, this method
is limited as it only considers two time points to
1
https://books.google.com/ngrams
1021
identify sense changes as opposed to our approach
which is over a much larger timescale, thereby, ef-
fectively allowing us to track the points of change
and the underlying causes. One of the closest
work to what we present here has been put forward
by (Tahmasebi et al, 2011), where the authors an-
alyze a newspaper corpus containing articles be-
tween 1785 and 1985. The authors mainly report
the frequency patterns of certain words that they
found to be candidates for change; however a de-
tailed cause analysis as to why and how a particu-
lar word underwent a sense change has not been
demonstrated. Further, systematic evaluation of
the results obtained by the authors has not been
provided.
All the above points together motivated us to
undertake the current work where we introduce,
for the first time, a completely unsupervised and
automatic method to identify the change of a word
sense and the cause for the same. Further, we also
present an extensive evaluation of the proposed al-
gorithm in order to test its overall accuracy and
performance.
3 Datasets and graph construction
In this section, we outline a brief description
of the dataset used for our experiments and
the graph construction procedure. The primary
source of data have been the millions of digitized
books made available through the Google Book
project (Goldberg and Orwant, 2013). The Google
Book syntactic n-grams dataset provides depen-
dency fragment counts by the years. However, in-
stead of using the plain syntactic n-grams, we use
a far richer representation of the data in the form of
a distributional thesaurus (Lin, 1997; Rychl?y and
Kilgarriff, 2007). In specific, we prepare a distri-
butional thesaurus (DT) for each of the time peri-
ods separately and subsequently construct the re-
quired networks. We briefly outline the procedure
of thesauri construction here referring the reader
to (Riedl and Biemann, 2013) for further details.
In this approach, we first extract each word and a
set of its context features, which are formed by la-
beled and directed dependency parse edges as pro-
vided in the dataset. Following this, we compute
the frequencies of the word, the context and the
words along with their context. Next we calculate
the lexicographer?s mutual information LMI (Kil-
garriff, 2004) between a word and its features and
retain only the top 1000 ranked features for ev-
ery word. Finally, we construct the DT network as
follows: each word is a node in the network and
the edge weight between two nodes is defined as
the number of features that the two corresponding
words share in common.
4 Tracking sense changes
The basic idea of our algorithm for tracking sense
changes is as follows. If a word undergoes a
sense change, this can be detected by comparing
its senses obtained from two different time pe-
riods. Since we aim to detect this change au-
tomatically, we require distributional representa-
tions corresponding to word senses for different
time periods. We, therefore, utilize the basic hy-
pothesis of unsupervised sense induction to in-
duce the sense clusters over various time periods
and then compare these clusters to detect sense
change. The basic premises of the ?unsupervised
sense induction? are briefly described below.
4.1 Unsupervised sense induction
We use the co-occurrence based graph clustering
framework introduced in (Biemann, 2006). The
algorithm proceeds in three basic steps. Firstly,
a co-occurrence graph is created for every target
word found in DT. Next, the neighbourhood/ego
graph is clustered using the Chinese Whispers
(CW) algorithm (see (McAuley and Leskovec,
2012) for similar approaches). The algorithm, in
particular, produces a set of clusters for each target
word by decomposing its open neighborhood. We
hypothesize that each different cluster corresponds
to a particular sense of the target word. For a de-
tailed description, the reader is referred to (Bie-
mann, 2011).
If a word undergoes sense change, this can be
detected by comparing the sense clusters obtained
from two different time periods by the algorithm
outlined above. For this purpose, we use statis-
tics from the DT corresponding to two different
time intervals, say tv
i
and tv
j
. We then run the
sense induction algorithm over these two different
datasets. Now, for a given word w that appears
in both the datasets, we get two different set of
clusters, say C
i
and C
j
. Without loss of gener-
ality, let us assume that our algorithm detects m
sense clusters for the word w in tv
i
and n sense
clusters in tv
j
. Let C
i
= {s
i1
, s
i2
, . . . , s
im
} and
C
j
= {s
j1
, s
j2
, . . . , s
jn
}, where s
kz
denotes z
th
sense cluster for word w during time interval tv
k
.
1022
We next describe our algorithm for detecting sense
change from these sets of sense clusters.
4.2 Split, join, birth and death
We hypothesize that word w can undergo sense
change from one time interval (tv
i
) to another
(tv
j
) as per one of the following scenarios:
Split A sense cluster s
iz
in tv
i
splits into two (or
more) sense clusters, s
jp
1
and s
jp
2
in tv
j
Join Two sense clusters s
iz
1
and s
iz
2
in tv
i
join to
make a single cluster s
jp
in tv
j
Birth A new sense cluster s
jp
appears in tv
j
,
which was absent in tv
i
Death A sense cluster s
iz
in tv
i
dies out and does
not appear in tv
j
To detect split, join, birth or death, we build an
(m+1)? (n+1) matrix I to capture the intersec-
tion between sense clusters of two different time
periods. The first m rows and n columns corre-
spond to the sense clusters in tv
i
and tv
j
espec-
tively. We append an additional row and column to
capture the fraction of words, which did not show
up in any of the sense clusters in another time in-
terval. So, an element I
kl
of the matrix
? 1 ? k ? m, 1 ? l ? n: denotes the frac-
tion of words in a newer sense cluster s
jl
,
that were also present in an older sense clus-
ter s
ik
.
? k = m + 1, 1 ? l ? n: denotes the fraction
of words in the sense cluster s
jl
, that were not
present in any of the m clusters in tv
i
.
? 1 ? k ? m, l = n + 1: denotes the fraction
of words in the sense cluster s
ik
, that did not
show up in any of the n clusters in tv
j
.
Thus, the matrix I captures all the four possible
scenarios for sense change. Since we can not
expect a perfect split, birth etc., we used certain
threshold values to detect if a candidate word is
undergoing sense change via one of these four
cases. In Figure 1, as an example, we illustrate
the birth of a new sense for the word ?compiler?.
4.3 Multi-stage filtering
To make sure that the candidate words obtained
via our algorithm are meaningful, we applied
multi-stage filtering to prune the candidate word
list. The following criterion were used for the fil-
tering:
Stage 1 We utilize the fact that the CW algorithm
is non-deterministic in nature. We apply CW
three times over the source and target time inter-
vals. We obtain the candidate word lists using our
algorithm for the three runs, then take the inter-
section to output those words, which came up in
all the three runs.
Stage 2 From the above list, we retain only those
candidate words, which have a part-of-speech tag
?NN? or ?NNS?, as we focus on nouns for this
work.
Stage 3 We sort the candidate list obtained in
Stage 2 as per their occurrence in the first time
period. Then, we remove the top 20% and the
bottom 20% words from this list. Therefore, we
consider the torso of the frequency distribution
which is the most informative part for this type
of an analysis.
5 Experimental framework
For our experiments, we utilized DTs created for
8 different time periods: 1520-1908, 1909-1953,
1954-1972, 1973-1986, 1987-1995, 1996-2001,
2002-2005 and 2006-2008 (Riedl et al, 2014).
The time periods were set such that the amount
of data in each time period is roughly the same.
We will also use T
1
to T
8
to denote these time pe-
riods. The parameters for CW clustering were set
as follows. The size of the neighbourhood (N )
to be clustered was set to 200. The parameter n
regulating the edge density in this neighbourhood
was set to 200 as well. The parameter a was set to
lin, which corresponds to favouring smaller clus-
ters by hub downweighing
2
. The threshold values
used to detect the sense changes were as follows.
For birth, at least 80% words of the target cluster
should be novel. For split, each split cluster should
have at least 30% words of the source cluster and
the total intersection of all the split clusters should
be > 80%. The same parameters were used for the
join and death case with the interchange of source
and target clusters.
5.1 Signals of sense change
Making comparisons between all the pairs of time
periods gave us 28 candidate words lists. For
2
data available at http://sf.net/p/jobimtext/
wiki/LREC2014_Google_DT/
1023
Figure 1: Example of the birth of a new sense for the word ?compiler?
each of these comparison, we applied the multi-
stage filtering to obtain the pruned list of candidate
words. Table 1 provides some statistics about the
number of candidate words obtained correspond-
ing to the birth case. The rows correspond to the
source time-period and the columns correspond to
the target time periods. An element of the table
shows the number of candidate words obtained
by comparing the corresponding source and target
time periods.
Table 1: Number of candidate birth senses be-
tween all time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
8
T
1
2498 3319 3901 4220 4238 4092 3578
T
2
1451 2330 2789 2834 2789 2468
T
3
917 1460 1660 1827 1815
T
4
517 769 1099 1416
T
5
401 818 1243
T
6
682 1107
T
7
609
The table clearly shows a trend. For most of
the cases, the number of candidate birth senses
tends to increase as we go from left to right. Sim-
ilarly, this number decreases as we go down in
the table. This is quite intuitive since going from
left to right corresponds to increasing the gap be-
tween two time periods while going down cor-
responds to decreasing this gap. As the gap in-
creases (decreases), one would expect more (less)
new senses coming in. Even while moving diago-
nally, the candidate words tend to decrease as we
move downwards. This corresponds to the fact
that the number of years in the time periods de-
creases as we move downwards, and therefore, the
gap also decreases.
5.2 Stability analysis & sense change location
Formally, we consider a sense change from tv
i
to tv
j
stable if it was also detected while com-
paring tv
i
with the following time periods tv
k
s.
This number of subsequent time periods, where
the same sense change is detected, helps us to de-
termine the age of a new sense. Similarly, for a
candidate sense change from tv
i
to tv
j
, we say that
the location of the sense change is tv
j
if and only
if that sense change does not get detected by com-
paring tv
i
with any time interval tv
k
, intermediate
between tv
i
and tv
j
.
Table 1 gives a lot of candidate words for sense
change. However, not all the candidate words
were stable. Thus, it was important to prune these
results using stability analysis. Also, it is to be
noted that these results do not pin-point to the ex-
act time-period, when the sense change might have
taken place. For instance, among the 4238 candi-
date birth sense detected by comparing T
1
and T
6
,
many of these new senses might have come up in
between T
2
to T
5
as well. We prune these lists fur-
ther based on the stability of the sense, as well as
to locate the approximate time interval, in which
the sense change might have occurred.
Table 2 shows the number of stable (at least
twice) senses as well as the number of stable
sense changes located in that particular time pe-
riod. While this decreases recall, we found this to
be beneficial for the accuracy of the method.
Once we were able to locate the senses as well
as to find the age of the senses, we attempted to
1024
Table 2: Number of candidate birth senses ob-
tained for different time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
1
2498 3319 3901 4220 4238 4092
stable 537 989 1368 1627 1540 1299
located 537 754 772 686 420 300
T
2
1451 2330 2789 2834 2789
stable 343 718 938 963 810
located 343 561 517 357 227
select some representative words and plotted them
on a timeline as per the birth period and their age
in Figure 2. The source time period here is 1909-
1953.
6 Evaluation framework
During evaluation, we considered the clusters ob-
tained using the 1909-1953 time-slice as our refer-
ence and attempted to track sense change by com-
paring these with the clusters obtained for 2002-
2005. The sense change detected was categorized
as to whether it was a new sense (birth), a single
sense got split into two or more senses (split) or
two or more senses got merged (join) or a particu-
lar sense died (death). We present a few instances
of the resulting clusters in the paper and refer the
reader to the supplementary material
3
for the rest
of the results.
6.1 Manual evaluation
The algorithm detected a lot of candidate words
for the cases of birth, split/join as well as death.
Since it was difficult to go through all the candi-
date sense changes for all the comparisons man-
ually, we decided to randomly select some can-
didate words, which were flagged by our algo-
rithm as undergoing sense change, while compar-
ing 1909-1953 and 2002-2005 DT. We selected 48
random samples of candidate words for birth cases
and 21 random samples for split/join cases. One
of the authors annotated each of the birth cases
identifying whether or not the algorithm signalled
a true sense change while another author did the
same task for the split/join cases. The accuracy as
per manual evaluation was found to be 60.4% for
the birth cases and 57% for the split/join cases.
Table 3 shows the evaluation results for a few
candidate words, flagged due to birth. Columns
3
http://cse.iitkgp.ac.in/resgrp/cnerg/
acl2014_wordsense/
correspond to the candidate words, words obtained
in the cluster of each candidate word (we will use
the term ?birth cluster? for these words, hence-
forth), which indicated a new sense, the results
of manual evaluation as well as the possible sense
this birth cluster denotes.
Table 4 shows the corresponding evaluation re-
sults for a few candidate words, flagged due to
split or join.
A further analysis of the words marked due
to birth in the random samples indicates that
there are 22 technology-related words, 2 slangs,
3 economics related words and 2 general words.
For the split-join case we found that there are
3 technology-related words while the rest of the
words are general. Therefore one of the key ob-
servations is that most of the technology related
words (where the neighborhood is completely
new) could be extracted from our birth results. In
contrast, for the split-join instances most of the re-
sults are from the general category since the neigh-
borhood did not change much here; it either got
split or merged from what it was earlier.
6.2 Automated evaluation with WordNet
In addition to manual evaluation, we also per-
formed automated evaluation for the candidate
words. We chose WordNet for automated evalua-
tion because not only does it have a wide coverage
of word senses but also it is being maintained and
updated regularly to incorporate new senses. We
did this evaluation for the candidate birth, join and
split sense clusters obtained by comparing 1909-
1953 time period with respect to 2002-2005. For
our evaluation, we developed an aligner to align
the word clusters obtained with WordNet senses.
The aligner constructs a WordNet dictionary for
the purpose of synset algnment. The CW clus-
ter is then aligned to WordNet synsets by compar-
ing the clusters with WordNet graph and the synset
with the maximum alignment score is returned as
the output. In summary, the aligner tool takes as
input the CW cluster and returns a WordNet synset
id that corresponds to the cluster words. The eval-
uation settings were as follows:
Birth: For a candidate word flagged as birth, we
first find out the set of all WordNet synset ids for
its CW clusters in the source time period (1909-
1953 in this case). Let S
init
denote the union of
these synset ids. We then find WordNet synset id
for its birth-cluster, say s
new
. Then, if s
new
/?
1025
Figure 2: Examples of birth senses placed on a timeline as per their location as well as age
Table 3: Manual evaluation for seven randomly chosen candidate birth clusters between time periods
1909-1953 and 2002-2005
Sl Candidate birth cluster Evaluation judgement,
No. Word comments
1 implant gel, fibre, coatings, cement, materials, metal, filler No, New set of words but
silicone, composite, titanium, polymer, coating similar sense already existed
2 passwords browsers, server, functionality, clients, workstation Yes, New sense related
printers, software, protocols, hosts, settings, utilities to ?a computer sense?
3 giants multinationals, conglomerates, manufacturers Yes, New sense as ?an
corporations, competitors, enterprises, companies organization with very great
businesses, brands, firms size or force?
4 donation transplantation, donation, fertilization, transfusions Yes, The new usage of donation
transplant, transplants, insemination, donors, donor ... associated with body organs etc.
5 novice negro, fellow, emigre, yankee, realist, quaker, teen No, this looks like a false
male, zen, lady, admiring, celebrity, thai, millionaire ... positive
6 partitions server, printers, workstation, platforms, arrays Yes, New usage related to
modules, computers, workstations, kernel ... the ?computing? domain
7 yankees athletics, cubs, tigers, sox, bears, braves, pirates Yes, related to the ?New
cardinals, dodgers, yankees, giants, cardinals ... York Yankees? team
S
init
, it implies that this is a new sense that was
not present in the source clusters and we call it a
?success? as per WordNet.
Join: For the join case, we find WordNet synset
ids s
1
and s
2
for the clusters obtained in the
source time period and s
new
for the join cluster
in the target time period. If s
1
6= s
2
and s
new
is
either s
1
or s
2
, we call it a ?success?.
Split: For the split case, we find WordNet synset
id s
old
for the source cluster and synset ids s
1
and s
2
for the target split clusters. If s
1
6= s
2
and either s
1
, or s
2
retains the id s
old
, we call it a
?success?.
Table 5 show the results of WordNet based eval-
uation. In case of birth we observe a success of
Table 5: Results of the automatic evaluation using
WordNet
Category No. of Candidate Words Success Cases
Birth 810 44%
Split 24 46%
Join 28 43%
44% while for split and join we observe a success
of 46% and 43% respectively. We then manually
verified some of the words that were deemed as
successes, as well as investigated WordNet sense
they were mapped to. Table 6 shows some of the
words for which the evaluation detected success
along with WordNet senses. Clearly, the cluster
words correspond to a newer sense for these words
1026
Table 4: Manual evaluation for five randomly chosen candidate split/join clusters between time periods
1909-1953 and 2002-2005
Sl Candidate Source and target clusters
No. Word
1 intonation S: whisper, glance, idioms, gesture, chant, sob, inflection, diction, sneer, rhythm, accents ...
(split) T
1
: nod, tone, grimace, finality, gestures, twang, shake, shrug, irony, scowl, twinkle ...
T
2
: accents, phrase, rhythm, style, phonology, diction, utterance, cadence, harmonies ...
Yes, T
1
corresponds to intonation in normal conversations while T
2
corresponds to the use of accents in
formal and research literature
2 diagonal S: coast, edge, shoreline, coastline, border, surface, crease, edges, slope, sides, seaboard ...
(split) T
1
: circumference, center, slant, vertex, grid, clavicle, margin, perimeter, row, boundary ..
T
2
: border, coast, seaboard, seashore, shoreline, waterfront, shore, shores, coastline, coasts
Yes, the split T
1
is based on mathematics where as T
2
is based on geography
3 mantra S
1
: sutra, stanza, chanting, chants, commandments, monologue, litany, verse, verses ...
(join) S
2
: praise, imprecation, benediction, praises, curse, salutation, benedictions, eulogy ...
T : blessings, spell, curses, spells, rosary, prayers, blessing, prayer, benediction ...
Yes, the two seemingly distinct senses of mantra - a contextual usage for chanting and prayer (S
1
)
and another usage in its effect - salutations, benedictions (S
2
) have now merged in T .
4 continuum S: circumference, ordinate, abscissa, coasts, axis, path, perimeter, arc, plane axis ...
(split) T
1
: roadsides, corridors, frontier, trajectories, coast, shore, trail, escarpment, highways ...
T
2
: arc, ellipse, meridians, equator, axis, axis, plane, abscissa, ordinate, axis, meridian ....
Yes, the split S
1
denotes the usage of ?continuum? with physical objects while the
the split S
2
corresponds to its usages in mathematics domain.
5 headmaster S
1
: master, overseer, councillor, chancellor, tutors, captain, general, principal ...
(join) S
2
: mentor, confessor, tutor, founder, rector, vicar, graduate, counselor, lawyer ...
T : chaplain, commander, surveyor, coordinator, consultant, lecturer, inspector ...
No, it seems a false positive
and the mapped WordNet synset matches the birth
cluster to a very high degree.
6.3 Evaluation with a slang list
Slangs are words and phrases that are regarded as
very informal, and are typically restricted to a par-
ticular context. New slang words come up every
now and then, and this plays an integral part in the
phenomena of sense change. We therefore decided
to perform an evaluation as to how many slang
words were being detected by our candidate birth
clusters. We used a list of slangs available from
the slangcity website
4
. We collected slangs for the
years 2002-2005 and found the intersection with
our candidate birth words. Note that the website
had a large number of multi-word expressions that
we did not consider in our study. Further, some
of the words appeared as either erroneous or very
transient (not existing more than a few months) en-
tires, which had to be removed from the list. All
these removal left us with a very little space for
comparison; however, despite this we found 25
slangs from the website that were present in our
birth results, e.g. ?bum?, ?sissy?, ?thug?, ?dude? etc.
4
http://slangcity.com/email_archive/
index_2003.htm
6.4 Evaluation of candidate death clusters
Much of our evaluation was focussed on the birth
sense clusters, mainly because these are more in-
teresting from a lexicographic perspective. Addi-
tionally, the main theme of this work was to de-
tect new senses for a given word. To detect a
true death of a sense, persistence analysis was re-
quired, that is, to verify if the sense was persist-
ing earlier and vanished after a certain time period.
While such an analysis goes beyond the scope of
this paper, we selected some interesting candidate
?death? senses. Table 7 shows some of these inter-
esting candidate words, their death cluster along
with the possible vanished meaning, identified by
the authors. While these words are still used in a
related sense, the original meaning does not exist
in the modern usage.
7 Conclusions
In this paper, we presented a completely unsu-
pervised method to detect word sense changes
by analyzing millions of digitized books archived
spanning several centuries. In particular, we con-
structed DT networks over eight different time
windows, clustered these networks and compared
these clusters to identify the emergence of novel
1027
Table 6: Example of randomly chosen candidate birth clusters mapped to WordNet
Sl Candidate birth cluster Synset Id,
No. Word WordNet sense
1 macro code, query, handler, program, procedure, subroutine 6582403, a set sequence of steps,
module, script part of larger computer program
2 caller browser, compiler, sender, routers, workstation, cpu 4175147, a computer that
host, modem, router, server provides client stations with access to files
3 searching coding, processing, learning, computing, scheduling 1144355, programming: setting an
planning, retrieval, routing, networking, navigation order and time for planned events
4 hooker bitch, whore, stripper, woman slut, prostitute 10485440, a woman who
girl, dancer ... engages in sexual intercourse for money
5 drones helicopters, fighters, rockets, flights, planes 4264914, a craft capable of
vehicles, bomber, missions, submarines ... traveling in outer space
6 amps inverters, capacitor, oscillators, switches, mixer 2955247, electrical device characterized
transformer, windings, capacitors, circuits ... by its capacity to store an electric charge
7 compilers interfaces, algorithms, programming, software 6566077, written programs pertaining
modules, libraries, routines, tools, utilities ... to the operation of a computer system
Table 7: Some representative examples for candidate death sense clusters
Sl Candidate death cluster Vanished meaning
No. Word
1 slop jeans, velveteen, tweed, woollen, rubber, sealskin, wear clothes and bedding supplied to
oilskin, sheepskin, velvet, calico, deerskin, goatskin, cloth ... sailors by the navy
2 blackmail subsidy, rent, presents, tributes, money, fine, bribes Origin: denoting protection money
dues, tolls, contributions, contribution, customs, duties ... levied by Scottish chiefs
3 repertory dictionary, study, compendium, bibliography, lore, directory Origin: denoting an index
catalogues, science, catalog, annals, digest, literature ... or catalog: from late Latin repertorium
4 phrasing contour, outline, construction, handling, grouping, arrangement in the sense ?style or manner of
structure, modelling, selection, form ... expression?: via late Latin Greek phrasis
senses. The performance of our method has been
evaluated manually as well as by comparison with
WordNet and a list of slang words. Through man-
ual evaluation we found that the algorithm could
correctly identify 60.4% birth cases from a set of
48 random samples and 57% split/join cases from
a set of 21 randomly picked samples. Quite strik-
ingly, we observe that (i) in 44% cases the birth of
a novel sense is attested by WordNet, (ii) in 46%
cases the split of an older sense is signalled on
comparison with WordNet and (iii) in 43% cases
the join of two senses is attested by WordNet.
These results might have strong lexicographic im-
plications ? even if one goes by very moderate es-
timates almost half of the words would be candi-
date entries in WordNet if they were not already
part of it. This method can be extremely useful
in the construction of lexico-semantic networks
for low-resource languages, as well as for keeping
lexico-semantic resources up to date in general.
Future research directions based on this work
are manifold. On one hand, our method can be
used by lexicographers in designing new dictio-
naries where candidate new senses can be semi-
automatically detected and included, thus greatly
reducing the otherwise required manual effort.
On the other hand, this method can be directly
used for various NLP/IR applications like seman-
tic search, automatic word sense discovery as well
as disambiguation. For semantic search, taking
into account the newer senses of the word can in-
crease the relevance of the query result. Similarly,
a disambiguation engine informed with the newer
senses of a word can increase the efficiency of
disambiguation, and recognize senses uncovered
by the inventory that would otherwise have to be
wrongly assigned to covered senses. In addition,
this method can be also extended to the ?NNP?
part-of-speech (i.e., named entities) to identify
changes in role of a person/place. Furthermore,
it would be interesting to apply this method to lan-
guages other than English and to try to align new
senses of cognates across languages.
Acknowledgements
AM would like to thank DAAD for supporting the
faculty exchange programme to TU Darmstadt.
PG would like to thank Google India Private Ltd.
for extending travel support to attend the confer-
ence. MR and CB have been supported by an IBM
SUR award and by LOEWE as part of the research
center Digital Humanities.
1028
References
J. Allan, R. Papka and V. Lavrenko. 1998. On-line
new event detection and tracking. In proceedings of
SIGIR, 37?45, Melbourne, Australia.
D. Bamman and G. Crane. 2011. Measuring Historical
Word Sense Variation. In proceedings of JCDL, 1?
10, New York, NY, USA.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, 73?80, New York, USA.
C. Biemann. 2011. Structure Discovery in Natural
Language. Springer Heidelberg Dordrecht London
New York. ISBN 978-3-642-25922-7.
D. Blei and J. Lafferty. 2006. Dynamic topic mod-
els. In proceedings of ICML, 113?120, Pittsburgh,
Pennsylvania.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayash and K. Kanzaki. 2009. Enhancing the
Japanese WordNet. In proceedings of workshop on
Asian Language Resources, 1?8, Suntec, Singapore.
P. Cook, J. H. Lau, M. Rundell, D. McCarthy, T. Bald-
win. 2013. A lexicographic appraisal of an auto-
matic approach for detecting new word senses. In
proceedings of eLex, 49-65, Tallinn, Estonia.
Y. Goldberg and J. Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In proceedings of the Joint
Conference on Lexical and Computational Seman-
tics (*SEM), 241?247, Atlanta, GA, USA.
G. Heyer, F. Holz and S. Teresniak. 2009. Change of
topics over time ? tracking topics by their change of
meaning. In proceedings of KDIR, Madeira, Portu-
gal.
N. Ide and J. Veronis. 1998. Introduction to the special
issue on word sense disambiguation: The state of the
art. Computational Linguistics, 24(1):1?40.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The sketch engine. In Proceedings of EU-
RALEX, 105?116, Lorient, France.
A. Kilgarriff and D. Tugwell. 2001. Word sketch: Ex-
traction and display of significant collocations for
lexicography. In proceedings of COLLOCATION:
Computational Extraction, Analysis and Exploita-
tion, 32?38, Toulouse, France.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In pro-
ceedings of ACL/EACL, 64?71, Madrid, Spain.
V. Loreto, A. Mukherjee and F. Tria. 2012. On the ori-
gin of the hierarchy of color names. PNAS, 109(18),
6819?6824.
S. K. Maity, T. M. Venkat and A. Mukherjee. 2012.
Opinion formation in time-varying social networks:
The case of the naming game. Phys. Rev. E, 86,
036110.
J. McAuley and J. Leskovec. 2012. Learning to dis-
cover social circles in ego networks. In proceedings
of NIPS, 548?556, Nevada, USA.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak and E. L. Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science, 331(6014):176?182.
R. Mihalcea and V. Nastase. 2012. Word epoch disam-
biguation: finding how words change over time. In
proceedings of ACL, 259?263, Jeju Island, Korea.
A. Mukherjee, F. Tria, A. Baronchelli, A. Puglisi and V.
Loreto. 2011. Aging in language dynamics. PLoS
ONE, 6(2): e16677.
R. Navigli. 2009. Word sense disambiguation: a sur-
vey. ACM Computing Surveys, 41(2):1?69.
P. P?a?akk?o and K. Lind?en. 2012. Finding a location
for a new word in WordNet. In proceedings of the
Global WordNet Conference, Matsue, Japan.
M. Riedl and C. Biemann. 2013. Scaling to large
3
data: An efficient and effective method to compute
distributional thesauri. In proceedings of EMNLP,
884?890, Seattle, Washington, USA.
M. Riedl, R. Steuer and C. Biemann. 2014. Distributed
distributional similarities of Google books over the
centuries. In proceedings of LREC, Reykjavik, Ice-
land.
P. Rychl?y and A. Kilgarriff. 2007. An efficient al-
gorithm for building a distributional thesaurus (and
other sketch engine developments). In proceedings
of ACL, poster and demo sessions, 41?44, Prague,
Czech Republic.
H. Sch?utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
K. Sp?ark-Jones. 1986. Synonymy and Semantic Clas-
sification. Edinburgh University Press. ISBN 0-
85224-517-3.
N. Tahmasebi, T. Risse and S. Dietze. 2011. Towards
automatic language evolution tracking: a study on
word sense tracking. In proceedings of EvoDyn, vol.
784, Bonn, Germany.
X. Wang and A. McCallum. 2006. Topics over time:
a non-Markov continuous-time model of topical
trends. In proceedings of KDD, 424?433, Philadel-
phia, PA, USA.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In proceed-
ings of the workshop on Detecting and Exploiting
Cultural Diversity on the Social Web, 35?40, Glas-
gow, Scotland, UK.
1029
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 91?96,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Automatic Annotation Suggestions and
Custom Annotation Layers in WebAnno
Seid Muhie Yimam
1
Richard Eckart de Castilho
2
Iryna Gurevych
2,3
Chris Biemann
1
(1) FG Language Technology, Dept. of Computer Science, Technische Universit?at Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit?at Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.{lt,ukp}.tu-darmstadt.de
Abstract
In this paper, we present a flexible ap-
proach to the efficient and exhaustive man-
ual annotation of text documents. For this
purpose, we extend WebAnno (Yimam et
al., 2013) an open-source web-based an-
notation tool.
1
While it was previously
limited to specific annotation layers, our
extension allows adding and configuring
an arbitrary number of layers through a
web-based UI. These layers can be an-
notated separately or simultaneously, and
support most types of linguistic annota-
tions such as spans, semantic classes, de-
pendency relations, lexical chains, and
morphology. Further, we tightly inte-
grate a generic machine learning compo-
nent for automatic annotation suggestions
of span annotations. In two case studies,
we show that automatic annotation sug-
gestions, combined with our split-pane UI
concept, significantly reduces annotation
time.
1 Introduction
The annotation of full text documents is a costly
and time-consuming task. Thus, it is important to
design annotation tools in such a way that the an-
notation process can happen as swiftly as possible.
To this end, we extend WebAnno with the capabil-
ity of suggesting annotations to the annotator.
A general-purpose web-based annotation tool
can greatly lower the entrance barrier for linguistic
annotation projects, as tool development costs and
preparatory work are greatly reduced. WebAnno
1.0 only partially fulfilled desires regarding gen-
erality: Although it covered already more kinds
1
WebAnno is open-source software under the terms of the
Apache Software License 2.0. This paper describes v1.2:
http://webanno.googlecode.com
of annotations than most other tools, it supported
only a fixed set of customizable annotation lay-
ers (named entities, part-of-speech, lemmata, co-
reference, dependencies). Thus, we also remove a
limitation of the tool, which was previously bound
to specific, hardcoded annotation layers.
We have generalized the architecture to support
three configurable generic structures: spans, rela-
tions, and chains. These support all of the original
layers and allow the user to define arbitrary custom
annotation layers based on either of these struc-
tures. Additionally, our approach allows maintain-
ing multiple properties on annotations, e.g. to sup-
port morphological annotations, while previously
only one property per annotation was supported.
Automatic suggestion of annotations is based
on machine learning, which is common practice
in annotation tools. However, most of existing
web-based annotation tools, such as GATE (Cun-
ningham et al., 2011) or brat (Stenetorp et al.,
2012), depend on external preprocessing and post-
processing plugins or on web services. These tools
have limitations regarding adaptability (difficulty
to adapt to other annotation tasks), reconfigurabil-
ity (generating a classifier when new features and
training documents are available is complicated),
and reusability (requires manual intervention to
add newly annotated documents into the iteration).
For our approach, we assume that an annota-
tor actually does manually verify all annotations
to produce a completely labeled dataset. This task
can be sped up by automatically suggesting anno-
tations that the annotator may then either accept
or correct. Note that this setup and its goal differs
from an active learning scenario, where a system
actively determines the most informative yet unan-
notated example to be labeled, in order to quickly
arrive at a high-quality classifier that is then to be
applied to large amounts of unseen data.
Our contribution is the integration of machine
learning into the tool to support exhaustive an-
91
notation of documents providing a shorter loop
than comparable tools (Cunningham et al., 2011;
Stenetorp et al., 2012), because new documents
are added to the training set as soon as they are
completed by the annotators. The machine learn-
ing support currently applies to sequence classifi-
cation tasks only. It is complemented by our ex-
tension allowing to define custom annotation lay-
ers, making it applicable to a wide range of anno-
tation tasks with only little configuration effort.
Section 2 reviews related work about the uti-
lization of automatic supports and customiza-
tion of annotation schemes in existing annotation
tools. The integration of automatic suggestions
into WebAnno, the design principles followed, and
two case studies are explained in Section 3. Sec-
tion 4 presents the implementation of customiz-
able annotation layers into the tool. Finally, Sec-
tion 5 summarizes the main contributions and fu-
ture directions of our work.
2 Related Work
Automatic annotation support The impact of
using lexical and statistical resources to produce
pre-annotation automatically to increase the anno-
tation speed has been studied widely for various
annotation tasks. For the task of medical named
entity labeling, Lingren et al. (2013) investigate
the impact of automatic suggestions on annotation
speed and potential biases using dictionary-based
annotations. This technique results in 13.83% to
21.5% time saving and in an inter-annotator agree-
ment (IAA) increase by several percentage points.
WordFreak (Morton and LaCivita, 2003) in-
cludes an automation component, where instances
with a low machine learning confidence are pre-
sented for annotation in an active learning setup.
Beck et al. (2013) demonstrate that the use of ac-
tive learning for machine translation reduces the
annotation effort and show a reduced annotation
load on three out of four datasets.
The GoldenGATE editor (Sautter et al., 2007)
integrates NLP tools and assistance features for
manual XML editing. The tool is used in correct-
ing/editing an automatically annotated document
with an editor where both text and XML markups
are modified. GoldenGATE is merely used to fa-
cilitate the correction of an annotation while pre-
annotation is conducted outside of the tool.
Automatic annotation support in brat (Stenetorp
et al., 2012) was carried out for a semantic class
disambiguation task to investigate how such au-
tomation facilitates the annotators? progress. They
report a 15.4% reduction in total annotation time.
However, the automation process in brat 1) de-
pends on bulk annotation imports and web service
configurations, which is labor intensive, 2) is task
specific so that it requires a lot of effort to adapt it
to different annotation tasks, 3) there is no way of
using the corrected result for the next iteration of
training the automatic tool.
The GATE Teamware (Bontcheva et al., 2013)
automation component is most similar to our
work. It is based either on plugins and externally
trained classification models, or uses web services.
Thus, it is highly task specific and requires exten-
sive configuration. The automatic annotation sug-
gestion component in our tool, in contrast, is easily
configurable and adaptable to different annotation
tasks and allows the use of annotations from the
current annotation project.
Custom annotation layers Generic annotation
data models are typically directed graph models
(e.g. GATE, UIMA CAS (G?otz and Suhre, 2004),
GrAF (Ide and Suderman, 2007)). In addition, an
annotation schema defines possible kinds of anno-
tations, their properties and relations. While these
models offer great expressiveness and flexibility, it
is difficult to adequately transfer their power into
a convenient annotation editor. For example, one
schema may prescribe that the part-of-speech tag
is a property on a Token annotation, another one
may prescribe that the tag is a separate annotation,
which is linked to the token. An annotator should
not be exposed to these details in the UI and should
be able to just edit a part-of-speech tag, ignorant of
the internal representation.
This problem is typically addressed in two
ways. Either, the full complexity of the annota-
tion model is exposed to the annotator, or the an-
notation editor uses a simplified model. The first
approach can easily lead to an unintuitive UI and
make the annotation an inconvenient task. The
second approach (e.g. as advocated by brat) re-
quires the implementation of specific import and
export filters to transform between the editor data
model and the generic annotation data models.
We propose a third approach integrating a con-
figurable mapping between a generic annotation
model (UIMA CAS) and a simplified editing
model (brat) directly into the annotation tool.
Thus, we avoid exposing the full complexity of
92
the generic model to the user and also avoid the
necessity for implementing import/export filters.
Similar approaches have already been used to map
annotation models to visualization modules (cf.
(Zeldes et al., 2009)), but have, to our knowledge,
not been used in an annotation editor. Our ap-
proach is different from schema-based annotation
editors (e.g. GATE), which employ a schema as
a template of properties and controlled vocabular-
ies that can be used to annotate documents, but
which do not allow to map structures inherent in
annotations, like relations or chains, to respective
concepts in the UI.
3 Automatic Annotation Suggestions
It is the purpose of the automatic annotation sug-
gestion component to increase the annotation ef-
ficiency, while maintaining the quality of annota-
tions. The key design principle of our approach is
a split-pane (Figure 1) that displays automatic an-
notation suggestions in the suggestion pane (lower
part) and only verified or manual ones in the anno-
tation pane (upper part). In this way, we force the
annotators to review each automatic suggestion as
to avoid overlooking wrong suggestions.
Figure 1: Split-pane UI. Upper: the annotation
pane, which should be completed by the annotator.
Lower: the suggestion pane, displaying predic-
tions or automatic suggestions, and coding their
status in color. This examples shows automatic
suggestions for parts-of-speech. Unattended anno-
tations are rendered in blue, accepted annotations
in grey and rejected annotations in red. Here, the
last five POS annotations have been attended, four
have been accepted by clicking on the suggestion,
and one was rejected by annotating it in the anno-
tation pane.
3.1 Suggestion modes
We distinguish three modes of automatic annota-
tion suggestion:
Correction mode In this mode, we import doc-
uments annotated by arbitrary external tools and
present them to the user in the suggestion pane
of the annotation page. This mode is specifi-
cally appropriate for annotation tasks where a pre-
annotated document contains several possibilities
for annotations in parallel, and the user?s task is
to select the correct annotation. This allows to
leverage specialized external automatic annotation
components, thus the tool is not limited to the in-
tegrated automation mechanism.
Repetition mode In this mode, further occur-
rences of a word annotated by the user are high-
lighted in the suggestion pane. To accept sugges-
tions, the user can simply click on them in the sug-
gestion pane. This basic ? yet effective ? sugges-
tion is realized using simple string matching.
Learning mode For this mode, we have inte-
grated MIRA (Crammer and Singer, 2003), an ex-
tension of the perceptron algorithm for online ma-
chine learning which allows for the automatic sug-
gestions of span annotations. MIRA was selected
because of its relatively lenient licensing, its good
performance even on small amounts of data, and
its capability of allowing incremental classifier up-
dates. Results of automatic tagging are displayed
in the suggestion pane. Our architecture is flexible
to integrate further machine learning tools.
3.2 Suggestion Process
The workflow to set up an automatically supported
annotation project consists of the following steps.
Importing annotation documents We can im-
port documents with existing annotations (manual
or automatic). The annotation pane of the automa-
tion page allows users to annotate documents and
the suggestion pane is used for the automatic sug-
gestion as shown in Figure 1. The suggestion pane
facilitates accepting correct pre-annotations with
minimal effort.
Configuring features For the machine learning
tool, it is required to define classification features
to train a classifier. We have designed a UI where
a range of standard classification features for se-
quence tagging can be configured. The features
include morphological features (prefixes, suffixes,
and capitalization), n-grams, and other layers as a
feature (for example POS annotation as a feature
93
Figure 2: Configuring an annotation suggestion: 1) layers for automation, 2) different features, 3) training
documents, 4) start training classifier.
for named entity recognition). While these stan-
dard features do not lead to state-of-the-art per-
formance on arbitrary tasks, we have found them
to perform very well for POS tagging, named en-
tity recognition, and chunking. Figure 2 shows the
feature configuration in the project settings.
Importing training documents We offer two
ways of providing training documents: importing
an annotated document in one of the supported file
formats, such as CoNLL, TCF, or UIMA XMI; or
using existing annotation documents in the same
project that already have been annotated.
Starting the annotation suggestion Once fea-
tures for a training layer are configured and train-
ing documents are available, automatic annotation
is possible. The process can be started manually
by the administrator from the automation settings
page, and it will be automatically re-initiated when
additional documents for training become avail-
able in the project. While the automatic annotation
is running in the background, users still can work
on the annotation front end without being affected.
Training and creating a classifier will be repeated
only when the feature configuration is changed or
when a new training document is available.
Display results on the monitoring page Af-
ter the training and automatic annotation are com-
pleted, detailed information about the training data
such as the number of documents (sentence, to-
kens), features used for each layer, F-score on
held-out data, and classification errors are dis-
played on the monitoring page, allowing an esti-
mation whether the automatic suggestion is use-
ful. The UI also shows the status of the training
process (not started, running, or finished).
3.3 Case Studies
We describe two case studies that demonstrate lan-
guage independence and flexibility with respect to
sequence label types of our automatic annotation
suggestions. In the first case study, we address the
task of POS tagging for Amharic as an example of
an under-resourced language. Second, we explore
German named entity recognition.
3.3.1 Amharic POS tagging
Amharic is an under-resourced language in the
Semitic family, mainly spoken in Ethiopia. POS
tagging research for Amharic is mostly conducted
as an academic exercise. The latest result re-
ported by Gebre (2009) was about 90% accuracy
using the Walta Information Center (WIC) corpus
of about 210,000 tokens (1065 news documents).
We intentionally do not use the corpus as training
data because of the reported inconsistencies in the
tagging process (Gebre, 2009). Instead, we man-
ually annotate Amharic documents for POS tag-
ging both to test the performance of the automa-
tion module and to produce POS-tagged corpora
for Amharic. Based upon the work by Petrov et al.
(2012) and Ethiopian Languages Research Cen-
ter (ELRC) tagset, we have designed 11 POS tags
equivalent to the Universal POS tags. The tag DET
is not included as Amharic denotes definiteness as
noun suffixes.
We collected some Amharic documents from an
online news portal.
2
Preprocessing of Amharic
documents includes the normalization of charac-
ters and tokenization (sentence and word bound-
2
http://www.ethiopianreporter.com/
94
Figure 3: Example Amharic document. The red
tags in the suggestion pane have not been con-
firmed by the annotator.
ary detection). Initially, we manually annotated 21
sentences. Using these, an iterative automatic an-
notation suggestion process was started until 300
sentences were fully annotated. We obtained an
F-score of 0.89 with the final model. Hence the
automatic annotation suggestion helps in decreas-
ing the total annotation time, since the user has
to manually annotate only one out of ten words,
while being able to accept most automatic sugges-
tions. Figure 3 shows such an Amharic document
in WebAnno.
3.3.2 German Named Entity Recognition
A pilot Named Entity Recognition (NER) project
for German was conducted by Benikova et al.
(2014). We have used the dataset ? about 31,000
sentences, over 41,000 NE annotations ? for train-
ing NER. Using this dataset, an F-score of about
0.8 by means of automatic suggestions was ob-
tained, which leads to an increase in annotation
speed of about 21% with automatic suggestion.
4 Customs Annotation Layers
The tasks in which an annotation editor can be em-
ployed depends on the expressiveness of the un-
derlying annotation model. However, fully expos-
ing the expressive power in the UI can make the
editor inconvenient to use.
We propose an approach that allows the user
to configure a mapping of an annotation model to
concepts well-supported in a web-based UI. In this
way, we can avoid to expose all details of the an-
notation model in the UI, and remove the need to
implement custom import/export filters.
WebAnno 1.0 employs a variant of the annota-
tion UI provided by brat, which offers the concepts
of spans and arcs. Based on these, WebAnno 1.2
implements five annotation layers: named entity,
part-of-speech, lemmata, co-reference, and depen-
dencies. In the new WebAnno version, we gener-
alized the support for these five layers into three
Figure 4: UI for custom annotation layers.
structural categories: span, relation (arc), and
chain. Each of these categories is handled by a
generic adapter which can be configured to sim-
ulate any of the original five layers. Based on
this generalization, the user can now define cus-
tom layers (Figure 4).
Additionally, we introduced a new concept of
constraints. For example, NER spans should not
cross sentence boundaries and attach to whole to-
kens (not substrings of tokens). Such constraints
not only help preventing the user from making in-
valid annotations, but can also offer extra conve-
nience. We currently support four hard-coded con-
straints:
Lock to token offsets Defines if annotation
boundaries must coincide with token boundaries,
e.g. named entities, lemmata, part-of-speech, etc.
For the user?s convenience, the annotation is auto-
matically expanded to include the full token, even
if only a part of a token is selected during annota-
tion (span/chain layers only).
Allow multiple tokens Some kinds of annota-
tions may only cover a single token, e.g. part-of-
speech, while others may cover multiple tokens,
e.g. named entities (span/chain layers only).
Allow stacking Controls if multiple annotations
of the same kind can be at the same location, e.g.
if multiple lemma annotations are allowed per to-
ken. For the user?s convenience, an existing an-
notation is replaced if a new annotation is created
when stacking is not allowed.
Allow crossing sentence boundaries Certain
annotations, e.g. named entities or dependency de-
lations, may not cross sentence boundaries, while
others need to, e.g. coreference chains.
Finally, we added the ability to define multiple
properties for annotations to WebAnno. For exam-
ple, this can be use to define a custom span-based
morphology layer with multiple annotation prop-
erties such as gender, number, case, etc.
95
5 Conclusion and Outlook
We discussed two extensions of WebAnno: the
tight and generic integration of automatic annota-
tion suggestions for reducing the annotation time,
and the web-based addition and configuration of
custom annotation layers.
While we also support the common practice
of using of external tools to automatically pre-
annotate documents, we go one step further by
tightly integrating a generic sequence classifier
into the tool that can make use of completed an-
notation documents from the same project. In two
case studies, we have shown quick convergence
for Amharic POS tagging and a substantial reduc-
tion in annotation time for German NER. The key
concept here is the split-pane UI that allows to dis-
play automatic suggestions, while forcing the an-
notator to review all of them.
Allowing the definition of custom annotation
layers in a web-based UI is greatly increasing
the number of annotation projects that potentially
could use our tool. While it is mainly an engineer-
ing challenge to allow this amount of flexibility
and to hide its complexity from the user, it is a ma-
jor contribution in the transition from specialized
tools towards general-purpose tools.
The combination of both ? custom layers and
automatic suggestions ? gives rise to the rapid
setup of efficient annotation projects. Adding to
existing capabilities in WebAnno, such as cura-
tion, agreement computation, monitoring and fine-
grained annotation project definition, our contri-
butions significantly extend the scope of annota-
tion tasks in which the tool can be employed.
In future work, we plan to support annota-
tion suggestions for non-span structures (arcs and
chains), and to include further machine learning
algorithms.
Acknowledgments
The work presented in this paper was funded by a German
BMBF grant to the CLARIN-D project, the Hessian LOEWE
research excellence program as part of the research center
?Digital Humanities? and by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant
No. I/82806.
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013. Re-
ducing annotation effort for quality estimation via active
learning. In Proc. ACL 2013 System Demonstrations,
Sofia, Bulgaria.
Darina Benikova, Chris Biemann, and Marc Reznicek. 2014.
NoSta-D Named Entity Annotation for German: Guide-
lines and Dataset. In Proc. LREC 2014, Reykjavik, Ice-
land.
Kalina Bontcheva, H. Cunningham, I. Roberts, A. Roberts,
V. Tablan, N. Aswani, and G. Gorrell. 2013. GATE
Teamware: a web-based, collaborative text annota-
tion framework. Language Resources and Evaluation,
47(4):1007?1029.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. In Journal of
Machine Learning Research 3, pages 951 ? 991.
Hamish Cunningham, D. Maynard, K. Bontcheva, V. Tablan,
N. Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,
D. Damljanovic, T. Heitz, M. A. Greenwood, H. Saggion,
J. Petrak, Y. Li, and W. Peters. 2011. Text Processing with
GATE (Version 6). University of Sheffield Department of
Computer Science, ISBN 978-0956599315.
Binyam Gebrekidan Gebre. 2009. Part-of-speech tagging for
Amharic. In ISMTCL Proceedings, International Review
Bulag, PUFC.
T. G?otz and O. Suhre. 2004. Design and implementation
of the UIMA Common Analysis System. IBM Systems
Journal, 43(3):476 ?489.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-based
format for linguistic annotations. In Proc. Linguistic An-
notation Workshop, pages 1?8, Prague, Czech Republic.
Todd Lingren, L. Deleger, K. Molnar, H. Zhai, J. Meinzen-
Derr, M. Kaiser, L. Stoutenborough, Q. Li, and I. Solti.
2013. Evaluating the impact of pre-annotation on anno-
tation speed and potential bias: natural language process-
ing gold standard development for clinical named entity
recognition in clinical trial announcements. In Journal of
the American Medical Informatics Association, pages 951
? 991.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak: an
open tool for linguistic annotation. In Proc. NAACL 2003,
demonstrations, pages 17?18, Edmonton, Canada.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A
universal part-of-speech tagset. In Proc LREC 2012, Is-
tanbul, Turkey.
Guido Sautter, Klemens B?ohm, Frank Padberg, and Walter
Tichy. 2007. Empirical Evaluation of Semi-automated
XML Annotation of Text Documents with the GoldenGATE
Editor. Budapest, Hungary.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proc. EACL 2012 Demo Session, Avignon, France.
Seid Muhie Yimam, Iryna Gurevych, Richard Eckart
de Castilho, and Chris Biemann. 2013. WebAnno: A
flexible,web-based and visually supported system for dis-
tributed annotations. In Proc. ACL 2013 System Demon-
strations, pages 1?6, Sofia, Bulgaria.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian Chiar-
cos. 2009. ANNIS: A search tool for multi-layer anno-
tated corpora. In Proc. Corpus Linguistics 2009, Liver-
pool, UK.
96
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435?440,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UKP: Computing Semantic Textual Similarity by
Combining Multiple Content Similarity Measures
Daniel Ba?r?, Chris Biemann?, Iryna Gurevych??, and Torsten Zesch??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
We present the UKP system which performed
best in the Semantic Textual Similarity (STS)
task at SemEval-2012 in two out of three met-
rics. It uses a simple log-linear regression
model, trained on the training data, to combine
multiple text similarity measures of varying
complexity. These range from simple char-
acter and word n-grams and common sub-
sequences to complex features such as Ex-
plicit Semantic Analysis vector comparisons
and aggregation of word similarity based on
lexical-semantic resources. Further, we em-
ploy a lexical substitution system and statisti-
cal machine translation to add additional lex-
emes, which alleviates lexical gaps. Our final
models, one per dataset, consist of a log-linear
combination of about 20 features, out of the
possible 300+ features implemented.
1 Introduction
The goal of the pilot Semantic Textual Similarity
(STS) task at SemEval-2012 is to measure the de-
gree of semantic equivalence between pairs of sen-
tences. STS is fundamental to a variety of tasks
and applications such as question answering (Lin
and Pantel, 2001), text reuse detection (Clough et
al., 2002) or automatic essay grading (Attali and
Burstein, 2006). STS is also closely related to tex-
tual entailment (TE) (Dagan et al, 2006) and para-
phrase recognition (Dolan et al, 2004). It differs
from both tasks, though, insofar as those operate on
binary similarity decisions while STS is defined as
a graded notion of similarity. STS further requires a
bidirectional similarity relationship to hold between
a pair of sentences rather than a unidirectional en-
tailment relation as for the TE task.
A multitude of measures for computing similar-
ity between texts have been proposed in the past
based on surface-level and/or semantic content fea-
tures (Mihalcea et al, 2006; Landauer et al, 1998;
Gabrilovich and Markovitch, 2007). The exist-
ing measures exhibit two major limitations, though:
Firstly, measures are typically used in separation.
Thereby, the assumption is made that a single
measure inherently captures all text characteristics
which are necessary for computing similarity. Sec-
ondly, existing measures typically exclude similar-
ity features beyond content per se, thereby implying
that similarity can be computed by comparing text
content exclusively, leaving out any other text char-
acteristics. While we can only briefly tackle the sec-
ond issue here, we explicitly address the first one by
combining several measures using a supervised ma-
chine learning approach. With this, we hope to take
advantage of the different facets and intuitions that
are captured in the single measures.
In the following section, we describe the feature
space in detail. Section 3 describes the machine
learning setup. After describing our submitted runs,
we discuss the results and conclude.
2 Text Similarity Measures
We now describe the various features we have tried,
also listing features that did not prove useful.
2.1 Simple String-based Measures
String Similarity Measures These measures op-
erate on string sequences. The longest common
435
substring measure (Gusfield, 1997) compares the
length of the longest contiguous sequence of char-
acters. The longest common subsequence measure
(Allison and Dix, 1986) drops the contiguity re-
quirement and allows to detect similarity in case
of word insertions/deletions. Greedy String Tiling
(Wise, 1996) allows to deal with reordered text parts
as it determines a set of shared contiguous sub-
strings, whereby each substring is a match of maxi-
mal length. We further used the following measures,
which, however, did not make it into the final mod-
els, since they were subsumed by the other mea-
sures: Jaro (1989), Jaro-Winkler (Winkler, 1990),
Monge and Elkan (1997), and Levenshtein (1966).
Character/word n-grams We compare character
n-grams following the implementation by Barro?n-
Ceden?o et al (2010), thereby generalizing the orig-
inal trigram variant to n = 2, 3, . . . , 15. We also
compare word n-grams using the Jaccard coefficient
as previously done by Lyon et al (2001), and the
containment measure (Broder, 1997). As high n led
to instabilities of the classifier due to their high in-
tercorrelation, only n = 1, 2, 3, 4 was used.
2.2 Semantic Similarity Measures
Pairwise Word Similarity The measures for
computing word similarity on a semantic level op-
erate on a graph-based representation of words and
the semantic relations among them within a lexical-
semantic resource. For this system, we used the al-
gorithms by Jiang and Conrath (1997), Lin (1998a),
and Resnik (1995) on WordNet (Fellbaum, 1998).
In order to scale the resulting pairwise word sim-
ilarities to the text level, we applied the aggregation
strategy by Mihalcea et al (2006): The sum of the
idf -weighted similarity scores of each word with the
best-matching counterpart in the other text is com-
puted in both directions, then averaged. In our ex-
periments, the measure by Resnik (1995) proved to
be superior to the other measures and was used in all
word similarity settings throughout this paper.
Explicit Semantic Analysis We also used the vec-
tor space model Explicit Semantic Analysis (ESA)
(Gabrilovich and Markovitch, 2007). Besides Word-
Net, we used two additional lexical-semantic re-
sources for the construction of the ESA vector space:
Wikipedia and Wiktionary1.
Textual Entailment We experimented with using
the BIUTEE textual entailment system (Stern and
Dagan, 2011) for generating entailment scores to
serve as features for the classifier. However, these
features were not selected by the classifier.
Distributional Thesaurus We used similarities
from a Distributional Thesaurus (similar to Lin
(1998b)) computed on 10M dependency-parsed sen-
tences of English newswire as a source for pairwise
word similarity, one additional feature per POS tag.
However, only the feature based on cardinal num-
bers (CD) was selected in the final models.
2.3 Text Expansion Mechanisms
Lexical Substitution System We used the lexical
substitution system based on supervised word sense
disambiguation (Biemann, 2012). This system au-
tomatically provides substitutions for a set of about
1,000 frequent English nouns with high precision.
For each covered noun, we added the substitutions
to the text and computed the pairwise word similar-
ity for the texts as described above. This feature al-
leviates the lexical gap for a subset of words.
Statistical Machine Translation We used the
Moses SMT system (Koehn et al, 2007) to trans-
late the original English texts via three bridge lan-
guages (Dutch, German, Spanish) back to English.
Thereby, the idea was that in the translation pro-
cess additional lexemes are introduced which allevi-
ate potential lexical gaps. The system was trained on
Europarl made available by Koehn (2005), using the
following configuration which was not optimized for
this task: WMT112 baseline without tuning, with
MGIZA alignment. The largest improvement was
reached for computing pairwise word similarity (as
described above) on the concatenation of the origi-
nal text and the three back-translations.
2.4 Measures Related to Structure and Style
In our system, we also used measures which go
beyond content and capture similarity along the
structure and style dimensions inherent to texts.
However, as we report later on, for this content-
1www.wiktionary.org
20-5-grams, grow-diag-final-and alignment, msd-bidirec-
tional-fe reodering, interpolation and kndiscount
436
oriented task they were not selected by the classifier.
Nonetheless, we briefly list them for completeness.
Structural similarity between texts can be de-
tected by computing stopword n-grams (Sta-
matatos, 2011). Thereby, all content-bearing words
are removed while stopwords are preserved. Stop-
word n-grams of both texts are compared using the
containment measure (Broder, 1997). In our experi-
ments, we tested n-gram sizes for n = 2, 3, . . . , 10.
We also compute part-of-speech n-grams for
various POS tags which we then compare using the
containment measure and the Jaccard coefficient.
We also used two similarity measures between
pairs of words (Hatzivassiloglou et al, 1999): Word
pair order tells whether two words occur in the
same order in both texts (with any number of words
in between), word pair distance counts the number
of words which lie between those of a given pair.
To compare texts along the stylistic dimension,
we further use a function word frequencies mea-
sure (Dinu and Popescu, 2009) which operates on a
set of 70 function words identified by Mosteller and
Wallace (1964). Function word frequency vectors
are computed and compared by Pearson correlation.
We also include a number of measures which
capture statistical properties of texts, such as type-
token ratio (TTR) (Templin, 1957) and sequential
TTR (McCarthy and Jarvis, 2010).
3 System Description
We first run each of the similarity measures intro-
duced above separately. We then use the resulting
scores as features for a machine learning classifier.
Pre-processing Our system is based on DKPro3,
a collection of software components for natural
language processing built upon the Apache UIMA
framework. During the pre-processing phase, we to-
kenize the input texts and lemmatize using the Tree-
Tagger implementation (Schmid, 1994). For some
measures, we additionally apply a stopword filter.
Feature Generation We now compute similarity
scores for the input texts with all measures and for
all configurations introduced in Section 2. This re-
sulted in 300+ individual score vectors which served
as features for the following step.
3http://dkpro-core-asl.googlecode.com
Run Features
1 Greedy String Tiling
Longest common subsequence (2 normalizations)
Longest common substring
Character 2-, 3-, and 4-grams
Word 1- and 2-grams (Containment, w/o stopwords)
Word 1-, 3-, and 4-grams (Jaccard)
Word 2- and 4-grams (Jaccard, w/o stopwords)
Word Similarity (Resnik (1995) on WordNet
aggregated according to Mihalcea et al (2006);
2 variants: complete texts + difference only)
Explicit Semantic Analysis (Wikipedia, Wiktionary)
Distributional Thesaurus (POS: Cardinal numbers)
2 All Features of Run 1
Lexical Substitution for Word Sim. (complete texts)
SMT for Word Sim. (complete texts as above)
3 All Features of Run 2
Random numbers from [4.5, 5] for surprise datasets
Table 1: Feature sets of our three system configurations
Feature Combination The feature combination
step uses the pre-computed similarity scores, and
combines their log-transformed values using a linear
regression classifier from the WEKA toolkit (Hall et
al., 2009). We trained the classifier on the training
datasets of the STS task. During the development
cycle, we evaluated using 10-fold cross-validation.
Post-processing For Runs 2 and 3, we applied a
post-processing filter which stripped all characters
off the texts which are not in the character range
[a-zA-Z0-9]. If the texts match, we set their similar-
ity score to 5.0 regardless of the classifier?s output.
4 Submitted Runs
Run 1 During the development cycle, we identi-
fied 19 features (see Table 1) which achieved the
best performance on the training data. For each
of the known datasets, we trained a separate clas-
sifier and applied it to the test data. For the surprise
datasets, we trained the classifier on a joint dataset
of all known training datasets.
Run 2 For the Run 2, we were interested in the
effects of two additional features: lexical substitu-
tion and statistical machine translation. We added
the corresponding measures to the feature set of Run
1 and followed the same evaluation procedure.
Run 3 For the third run, we used the same feature
set as for Run 2, but returned random numbers from
[4.5, 5] for the sentence pairs in the surprise datasets.
437
Dim. Text Similarity Features PAR VID SE
Best Feature Set, Run 1 .711 .868 .735
Best Feature Set, Run 2 .724 .868 .742
Content Pairwise Word Similarity .564 .835 .527
Character n-grams .658 .771 .554
Explicit Semantic Analysis .427 .781 .619
Word n-grams .474 .782 .619
String Similarity .593 .677 .744
Distributional Thesaurus .494 .481 .365
Lexical Substitution .228 .554 .483
Statistical Machine Translation .287 .652 .516
Structure Part-of-speech n-grams .193 .265 .557
Stopword n-grams .211 .118 .379
Word Pair Order .104 .077 .295
Style Statistical Properties .168 .225 .325
Function Word Frequencies .179 .142 .189
Table 2: Best results for single measures, grouped by di-
mension, on the training datasets MSRpar, MSRvid, and
SMTeuroparl, using 10-fold cross-validation
5 Results on Training Data
Evaluation was carried out using the official scorer
which computes Pearson correlation of the human
rated similarity scores with the the system?s output.
In Table 2, we report the results achieved on
each of the training datasets using 10-fold cross-
validation. The best results were achieved for the
feature set of Run 2, with Pearson?s r = .724,
r = .868, and r = .742 for the datasets MSR-
par, MSRvid, and SMTeuroparl, respectively. While
individual classes of content similarity measures
achieved good results, a different class performed
best for each dataset. However, text similarity mea-
sures related to structure and style achieved only
poor results on the training data. This was to be ex-
pected due to the nature of the data, though.
6 Results on Test Data
Besides the Pearson correlation for the union of all
datasets (ALL), the organizers introduced two addi-
tional evaluation metrics after system submission:
ALLnrm computes Pearson correlation after the sys-
tem outputs for each dataset are fitted to the gold
standard using least squares, and Mean refers to the
weighted mean across all datasets, where the weight
depends on the number of pairs in each dataset.
In Table 3, we report the offical results achieved
on the test data. The best configuration of our system
was Run 2 which was ranked #1 for the evaluation
#1 #2 #3 Sys. r1 r2 r3 PAR VID SE WN SN
1 2 1 UKP2 .823 .857 .677 .683 .873 .528 .664 .493
2 3 5 TL .813 .856 .660 .698 .862 .361 .704 .468
3 1 2 TL .813 .863 .675 .734 .880 .477 .679 .398
4 4 4 UKP1 .811 .855 .670 .682 .870 .511 .664 .467
5 6 13 UNT .784 .844 .616 .535 .875 .420 .671 .403
...
...
...
...
...
...
...
...
...
...
...
...
87 85 70 B/L .311 .673 .435 .433 .299 .454 .586 .390
Table 3: Official results on the test data for the top 5
participating runs out of 89 which were achieved on the
known datasets MSRpar, MSRvid, and SMTeuroparl, as
well as on the surprise datasets OnWN and SMTnews. We
report the ranks (#1: ALL, #2: ALLnrm, #3: Mean) and
the corresponding Pearson correlation r according to the
three offical evaluation metrics (see Sec. 6). The provided
baseline is shown at the bottom of this table.
metrics ALL (r = .823)4 and Mean (r = .677), and
#2 for ALLnrm (r = .857). An exhaustive overview
of all participating systems can be found in the STS
task description (Agirre et al, 2012).
7 Conclusions and Future Work
In this paper, we presented the UKP system, which
performed best across the three official evaluation
metrics in the pilot Semantic Textual Similarity
(STS) task at SemEval-2012. While we did not
reach the highest scores on any of the single datasets,
our system was most robust across different data. In
future work, it would be interesting to inspect the
performance of a system that combines the output
of all participating systems in a single linear model.
We also propose that two major issues with the
datasets are tackled in future work: (a) It is unclear
how to judge similarity between pairs of texts which
contain contextual references such as on Monday
vs. after the Thanksgiving weekend. (b) For several
pairs, it is unclear what point of view to take, e.g. for
the pair An animal is eating / The animal is hopping.
Is the pair to be considered similar (an animal is do-
ing something) or rather not (eating vs. hopping)?
Acknowledgements This work has been sup-
ported by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No.
I/82806, and by the Klaus Tschira Foundation under
project No. 00.133.2008.
499% confidence interval: .807 ? r ? .837
438
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion, in conjunction with the 1st Joint Conference on
Lexical and Computational Semantics.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305?310.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism Detection across
Distant Language Pairs. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 37?45.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. Proceedings of the Compres-
sion and Complexity of Sequences, pages 21?29.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
152?159.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. In Machine Learning Challenges, Lecture
Notes in Computer Science, pages 177?190. Springer.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62?66.
William B. Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News Sources.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 350?356.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combina-
tions via machine learning. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
203?212.
Matthew A. Jaro. 1989. Advances in record linkage
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Associa-
tion, 84(406):414?420.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the 10th International Conference
on Research in Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
10th Machine Translation Summit, pages 79?86.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2):259?284.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998a. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296?304.
Dekang Lin. 1998b. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 768?774.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of Conference
on Empirical Methods in Natural Language Process-
ing, pages 118?125.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophisti-
439
cated approaches to lexical diversity assessment. Be-
havior research methods, 42(2):381?92.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775?780.
Alvaro Monge and Charles Elkan. 1997. An efficient
domain-independent algorithm for detecting approxi-
mately duplicate database records. In Proceedings of
the SIGMOD Workshop on Data Mining and Knowl-
edge Discovery, pages 23?29.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and disputed authorship: The Federalist.
Addison-Wesley.
Philip Resnik. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence, pages 448?453.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512?2527.
Asher Stern and Ido Dagan. 2011. A Confidence
Model for Syntactically-Motivated Entailment Proofs.
In Proceedings of the International Conference on Re-
cent Advances in Natural Language Processing, pages
455?462.
Mildred C. Templin. 1957. Certain language skills in
children. University of Minnesota Press.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the Sec-
tion on Survey Research Methods, pages 354?359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE technical symposium
on Computer science education, pages 130?134.
440
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 39?47, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 5: Evaluating Phrasal Semantics
Ioannis Korkontzelos
National Centre for Text Mining
School of Computer Science
University of Manchester, UK
ioannis.korkontzelos@man.ac.uk
Torsten Zesch
UKP Lab, CompSci Dept.
Technische Universita?t Darmstadt
Germany
zesch@ukp.informatik.tu-darmstadt.de
Fabio Massimo Zanzotto
Department of Enterprise Engineering
University of Rome ?Tor Vergata?
Italy
zanzotto@info.uniroma2.it
Chris Biemann
FG Language Technology, CompSci Dept.
Technische Universita?t Darmstadt
Germany
biem@cs.tu-darmstadt.de
Abstract
This paper describes the SemEval-2013 Task
5: ?Evaluating Phrasal Semantics?. Its first
subtask is about computing the semantic simi-
larity of words and compositional phrases of
minimal length. The second one addresses
deciding the compositionality of phrases in a
given context. The paper discusses the impor-
tance and background of these subtasks and
their structure. In succession, it introduces the
systems that participated and discusses evalu-
ation results.
1 Introduction
Numerous past tasks have focused on leveraging the
meaning of word types or words in context. Exam-
ples of the former are noun categorization and the
TOEFL test, examples of the latter are word sense
disambiguation, metonymy resolution, and lexical
substitution. As these tasks have enjoyed a lot suc-
cess, a natural progression is the pursuit of models
that can perform similar tasks taking into account
multiword expressions and complex compositional
structure. In this paper, we present two subtasks de-
signed to evaluate such phrasal models:
a. Semantic similarity of words and compositional
phrases
b. Evaluating the compositionality of phrases in
context
For example, the first subtask addresses computing
how similar the word ?valuation? is to the compo-
sitional sequence ?price assessment?, while the sec-
ond subtask addresses deciding whether the phrase
?piece of cake? is used literally or figuratively in the
sentence ?Labour was a piece of cake!?.
The aim of these subtasks is two-fold. Firstly,
considering that there is a spread interest lately in
phrasal semantics in its various guises, they provide
an opportunity to draw together approaches to nu-
merous related problems under a common evalua-
tion set. It is intended that after the competition,
the evaluation setting and the datasets will comprise
an on-going benchmark for the evaluation of these
phrasal models.
Secondly, the subtasks attempt to bridge the
gap between established lexical semantics and full-
blown linguistic inference. Thus, we anticipate that
they will stimulate an increased interest around the
general issue of phrasal semantics. We use the no-
tion of phrasal semantics here as opposed to lexi-
cal compounds or compositional semantics. Bridg-
ing the gap between lexical semantics and linguis-
tic inference could provoke novel approaches to cer-
tain established tasks, such as lexical entailment and
paraphrase identification. In addition, it could ul-
39
timately lead to improvements in a wide range of
applications in natural language processing, such
as document retrieval, clustering and classification,
question answering, query expansion, synonym ex-
traction, relation extraction, automatic translation,
or textual advertisement matching in search engines,
all of which depend on phrasal semantics.
The remainder of this paper is structured as fol-
lows: Section 2 presents details about the data
sources and the variety of sources applicable to the
task. Section 3 discusses the first subtask, which
is about semantic similarity of words and compo-
sitional phrases. In subsection 3.1 the subtask is
described in detail together with some information
about its background. Subsection 3.2 discusses the
data creation process and subsection 3.3 discusses
the participating systems and their results. Section 4
introduces the second subtask, which is about eval-
uating the compositionality of phrases in context.
Subsection 4.1 explains the data creation process for
this subtask. In subsection 4.2 the evaluation statis-
tics of participating systems are presented. Section
5 is a discussion about the conclusions of the entire
task. Finally, in section 6 we summarize this presen-
tation and discuss briefly our vision about challenges
in distributional semantics.
2 Data Sources & Methodology
Data instances of both subtasks are drawn from the
large-scale, freely available WaCky corpora (Baroni
et al, 2009). The resource contains corpora in 4 lan-
guages: English, French, German and Italian. The
English corpus, ukWaC, consists of 2 billion words
and was constructed by crawling to the .uk domain
of the web and using medium-frequency words from
the BNC as seeds. The corpus is part-of-speech
(PoS) tagged and lemmatized using the TreeTagger
(Schmid, 1994). The French corpus, frWaC, con-
tains 1.6 billion word corpus and was constructed
by web-crawling the .fr domain and using medium-
frequency words from the Le Monde Diplomatique
corpus and basic French vocabulary lists as seeds.
The corpus was PoS tagged and lemmatized with
the TreeTagger. The French corpus, deWaC, con-
sists of 1.7 billion word corpus and was constructed
by crawling the .de domain and using medium-
frequency words from the SudDeutsche Zeitung cor-
pus and basic German vocabulary lists as seeds. The
corpus was PoS tagged and lemmatized with the
TreeTagger. The Italian corpus, itWaC, is a 2 billion
word corpus constructed from the .it domain of the
web using medium-frequency words from the Re-
pubblica corpus and basic Italian vocabulary lists as
seeds. The corpus was PoS tagged with the Tree-
Tagger, and lemmatized using the Morph-it! lexicon
(Zanchetta and Baroni, 2005). Several versions of
the WaCky corpora, with various extra annotations
or modifications are also available1.
We ensured that data instances occur frequently
enough in the WaCky corpora, so that participat-
ing systems could gather statistics for building dis-
tributional vectors or other uses. As the evalua-
tion data only contains very small annotated sam-
ples from freely available web documents, and the
original source is provided, we could provide them
without violating copyrights.
The size of the WaCky corpora is suitable for
training reliable distributional models. Sentences
are already lemmatized and part-of-speech tagged.
Participating approaches making use of distribu-
tional methods, part-of-speech tags or lemmas, were
strongly encouraged to use these corpora and their
shared preprocessing, to ensure the highest possi-
ble comparability of results. Additionally, this had
the potential to considerably reduce the workload of
participants. For the first subtask, data were pro-
vided in English, German and Italian and for the sec-
ond subtask in English and German.
The range of methods applicable to both subtasks
was deliberately not limited to any specific branch of
methods, such as distributional or vector models of
semantic compositionality. We believe that the sub-
tasks can be tackled from different directions and we
expect a great deal of the scientific benefit to lie in
the comparison of very different approaches, as well
as how these approaches can be combined. An ex-
ception to this rule is the fact that participants in the
first subtask were not allowed to use directly defini-
tions extracted from dictionaries or lexicons. Since
the subtask is considered fundamental and its data
were created from online knowledge resources, sys-
tems using the same tools to address it would be of
limited use. However, participants were allowed to
1WaCky website: wacky.sslmit.unibo.it
40
use other information residing in dictionaries, such
as Wordnet synsets or synset relations.
Participating systems were allowed to attempt one
or both subtasks, in one or all of the languages sup-
ported. However, it was expected that systems per-
forming well at the first basic subtask would pro-
vide a good starting point for dealing with the sec-
ond subtask, which is considered harder. Moreover,
language-independent models were of special inter-
est.
3 Subtask 5a: Semantic Similarity of
Words and Compositional Phrases
The aim of this subtask is to evaluate the compo-
nent of a semantic model that computes the simi-
larity between word sequences of different length.
Participating systems are asked to estimate the se-
mantic similarity of a word and a short sequence of
two words. For example, they should be able to fig-
ure out that contact and close interaction are similar
whereas megalomania and great madness are not.
This subtask addresses a core problem, since sat-
isfactory performance in computing the similarity of
full sentences depends on similarity computations
on shorter sequences.
3.1 Background and Description
This subtask is based on the assumption that we
first need a basic set of functions to compose the
meaning of two words, in order to construct more
complex models that compositionally determine the
meaning of sentences, as a second step. For compo-
sitional distributional semantics, the need for these
basic functions is discussed in Mitchell and Lapata
(2008). Since then, many models have been pro-
posed for addressing the task (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010; Guevara, 2010),
but still comparative analysis is in general based on
comparing sequences that consist of two words.
As in Zanzotto et al (2010), this subtask proposes
to compare the similarity of a 2-word sequence and
a single word. This is important as it is the basic
step to analyse models that can compare any word
sequences of different length.
The development and testing set for this subtask
were built based on the idea described in Zanzotto
et al (2010). Dictionaries were used as sources of
contact/[kon-takt]
1. the act or state of touching;
a touching or meeting, as of
two things or people.
2. close interaction
3. an acquaintance, colleague,
or relative through whom a
person can gain access to
information, favors, influ-
ential people, and the like.
Figure 1: The definition of contact in a sample dictionary
positive training examples. Dictionaries are natural
repositories of equivalences between words under
definition and sequences of words used for defining
them. Figure 1 presents the definition of the word
contact, from which the pair (contact, close interac-
tion) can be extracted. Such equivalences extracted
from dictionaries can be seen as natural and unbi-
ased data instances. This idea opens numerous op-
portunities:
? Since definitions in dictionaries are syntacti-
cally rich, we are able to create examples for
different syntactic relations.
? We have the opportunity to extract positive ex-
amples for languages for which dictionaries
with sufficient entries are available.
Negative examples were generated by matching
words under definition with randomly chosen defin-
ing sequences. In the following subsection, we pro-
vide details about the application of this idea to build
the development and testing set for subtask 5a.
3.2 Data Creation
Data for this subtask were provided in English, Ger-
man and Italian. Pairs of words under definitions and
defining sequences were extracted from the English,
German and Italian part of Wiktionary, respectively.
In particular, for each language, all Wiktionary en-
tries were downloaded and part-of-speech tagged us-
ing the Genia tagger (Tsuruoka et al, 2005). In
succession, definitions that start with noun phrases
41
Language Train set Test set Total
English 5,861 3,907 9,768
German 1,516 1,010 2,526
Italian 1,275 850 2,125
German - no names 1,101 733 1,834
Table 1: Quantitative characteristics of the datasets
were kept, only. For the purpose of extracting word
and sequence pairs for this subtask, we consider as
noun phrases, sequences that consist of adjectives
or noun and end with a noun. In cases where the
extracted noun phrase was longer than two words,
the right-most two sequences were kept, since in
most cases noun phrases are governed by their right-
most component. Subsequently, we discarded in-
stances whose words occur too infrequently in the
WaCky corpora (Baroni et al, 2009) of each lan-
guage. WaCky corpora are available freely and are
large enough for participating systems to extract dis-
tributional statistics. Taking the numbers of ex-
tracted instances into account, we set the frequency
thresholds at 10 occurrences for English and 5 for
German and Italian.
Data instances extracted following this process
were then checked by a computational linguist. Can-
didate pairs in which the definition sequence was not
judged to be a precise and adequate definition of the
word under definition were discarded. These cases
were very limited and mostly account for shortcom-
ings of the very simple pattern used for extraction.
For example, the pair (standard, transmission vehi-
cle) coming from the definition of ?standard? as ?A
manual transmission vehicle? was discarded. Simi-
larly in German, the pair (Fremde (Eng. stranger),
weibliche Person (Eng. female person)) was dis-
carded. ?Fremde?, which is of female grammat-
ical genre, was defined as ?weibliche Person, die
man nicht kennt (Eng. female person, one does not
know)?. In Italian, the pair (paese (Eng. land, coun-
try, region), grande estensione (Eng. large tract))
was discarded, since the original definition was
?grande estensione di terreno abitato e generalmente
coltivato (Eng. large tract of land inhabited and cul-
tivated in general)?.
The final data sets were divided into training and
held-out testing sets, according to a 60% and 40%
ratio, respectively. The first three rows of table 1
present the numbers of the train and test sets for the
three languages chosen. It was identified that a fair
percentage of the German instances (approximately
27%) refer to the definitions of first names or family
names. This is probably a flaw of the German part of
Wiktionary. In addition, the pattern used for extrac-
tion happens to apply to the definitions of names.
Name instances were discarded from the German
data set to produce the data set described in the last
row of table 1.
The training set was released approximately 3
months earlier than the test data. Instances in both
set ware annotated as positive or negative. Test set
annotations were not released to the participants, but
were used for evaluation, only.
3.3 Results
Participating systems were evaluated on their ability
to predict correctly whether the components of each
test instance, i.e. word-sequence pair, are semanti-
cally similar or distinct. Participants were allowed
to use or ignore the training data, i.e. the systems
could be supervised or unsupervised. Unsupervised
systems were allowed to use the training data for de-
velopment and parameter tuning. Since this is a core
task, participating systems were not be able to use
dictionaries or other prefabricated lists. Instead, they
were allowed to use distributional similarity models,
selectional preferences, measures of semantic simi-
larity etc.
Participating system responses were scored in
terms of standard information retrieval measures:
accuracy (A), precision (P), recall (R) and F1 score
(Radev et al, 2003). Systems were encouraged to
submit at most 3 solutions for each language, but
submissions for fewer languages were accepted.
Five research teams participated. Ten system runs
were submitted for English, one for German (on data
set: German - no names) and one for Italian. Table 2
illustrates the results of the evaluation process. The
teams of (HsH) (Wartena, 2013), CLaC (Siblini and
Kosseim, 2013), UMCC DLSI-(EPS) (Da?vila et al,
2013), and ITNLP, the Harbin Institute of Technol-
ogy, approached the task in a supervised way, while
MELODI (Van de Cruys et al, 2013) participated
with two unsupervised approaches. Interestingly,
42
Language Rank Participant Id run Id A R P rej. R rej. P F1
1 HsH 1 .803 .752 .837 .854 .775 .792
3 CLaC 3 .794 .707 .856 .881 .750 .774
2 CLaC 2 .794 .695 .867 .893 .745 .771
4 CLaC 1 .788 .638 .910 .937 .721 .750
English 5 MELODI lvw .748 .614 .838 .882 .695 .709
6 UMCC DLSI-(EPS) 1 .724 .613 .787 .834 .683 .689
7 ITNLP 3 .703 .501 .840 .904 .645 .628
8 MELODI dm .689 .481 .825 .898 .634 .608
9 ITNLP 1 .663 .392 .857 .934 .606 .538
10 ITNLP 2 .659 .427 .797 .891 .609 .556
German 1 HsH 1 .825 .765 .870 .885 .790 .814
Italian 1 UMCC DLSI-(EPS) 1 .675 .576 .718 .774 .646 .640
Table 2: Task 5a: Evaluation results. A, P, R, rej. and F1 stand for accuracy, precision, recall, rejection and F1 score,
respectively.
these approaches performed better than some super-
vised ones for this experiment. Below, we sum-
marise the properties of participating systems.
(HsH) (Wartena, 2013) used distributed similarity
and especially random indexing to compute similar-
ities between words and possible definitions, under
the hypothesis that a word and its definition are dis-
tributionally more similar than a word and an arbi-
trary definition. Considering all open-class words,
context vectors over the entire WaCky corpus were
computed for the word under definition, the defining
sequence, its component words separately, the ad-
dition and multiplication of the vectors of the com-
ponent words and a general context vector. Then,
various similarity measures were computed on the
vectors, including an innovative length-normalised
version of Jensen-Shannon divergence. The similar-
ity values are used to train a Support Vector Machine
(SVM) classifier (Cortes and Vapnik, 1995).
The first approach (run 1) of CLaC (Siblini and
Kosseim, 2013) is based on a weighted semantic
network to measure semantic relatedness between
the word and the components of the phrase. A
PART classifier is used to generate a partial decision
trained on the semantic relatedness information of
the labelled training set. The second approach uses
a supervised distributional method based on words
frequently occurring in the Web1TB corpus to cal-
culate relatedness. A JRip classifier is used to gen-
erate rules trained on the semantic relatedness infor-
mation of the training set. This approach was used
in conjunction with the first one as a backup method
(run 2). In addition, features generated by both ap-
proaches were used to train the JRIP classifier col-
lectively (run 3).
The first approach of MELODI (Van de Cruys
et al, 2013), called lvw, uses a dependency-based
vector space model computed over the ukWaC cor-
pus, in combination with Latent Vector Weighting
(Van de Cruys et al, 2011). The system computes
the similarity between the first noun and the head
noun of the second phrase, which was weighted ac-
cording to the semantics of the modifier. The second
approach, called dm, used a dependency-based vec-
tor space model, but, unlike the first approach, disre-
garded the modifier in the defining sequence. Since
both systems are unsupervised, the training data was
used to train a similarity threshold parameter, only.
UMCC DLSI-(EPS) (Da?vila et al, 2013) locates
the synsets of words in data instances and computes
the semantic distances between each synset of the
word under definition and each synsets of the defin-
ing sequence words. In succession, a classifier is
trained using features based on distance and Word-
Net relations.
The first attempt of ITNLP (run 1) consisted of an
SVM classifier trained on semantic similarity com-
putations between the word under definition and
43
the defining sequence in each instance. Their sec-
ond attempt also uses an SVM, however trained on
WordNet-based similarities. The third attempt of
ITNLP is a combination of the previous two; it com-
bines their features to train an SVM classifier.
4 Subtask 5b: Semantic Compositionality
in Context
An interesting sub-problem of semantic composi-
tionality is to decide whether a target phrase is used
in its literal or figurative meaning in a given con-
text. For example ?big picture? might be used lit-
erally as in Click here for a bigger picture or figura-
tively as in To solve this problem, you have to look at
the bigger picture. Another example is ?old school?
which can also be used literally or figuratively: He
will go down in history as one of the old school, a
true gentlemen. vs. During the 1970?s the hall of the
old school was converted into the library.
Being able to detect whether a phrase is used lit-
erally or figuratively is e.g. especially important for
information retrieval, where figuratively used words
should be treated separately to avoid false positives.
For example, the example sentence He will go down
in history as one of the old school, a true gentle-
men. should probably not be retrieved for the query
?school?. Rather, the insights generated from sub-
task 5a could be utilized to retrieve sentences using
a similar phrase such as ?gentleman-like behavior?.
The task may also be of interest to the related re-
search fields of metaphor detection and idiom iden-
tification.
There were no restrictions regarding the array of
methods, and the kind of resources that could be
employed for this task. In particular, participants
were allowed to make use of pre-fabricated lists of
phrases annotated with their probability of being
used figuratively from publicly available sources, or
to produce these lists from corpora. Assessing how
well the phrase suits its context might be tackled
using e.g. measures of semantic relatedness as well
as distributional models learned from the underlying
corpus.
Participants of this subtask were provided with
real usage examples of target phrases. For each us-
age example, the task is to make a binary decision
whether the target phrase is used literally or figu-
ratively in this context. Systems were tested in two
different disciplines: a known phrases task where all
target phrases in the test set were contained in the
training, and an unknown phrases setting, where all
target phrases in the test set were unseen.
4.1 Data Creation
The first step in creating the corpus was to compile
a list of phrases that can be used either literally or
metaphorically. Thus, we created an initial list of
several thousand English idioms from Wiktionary by
listing all entries under the category ENGLISH ID-
IOMS using the JWKTL Wiktionary API (Zesch et
al., 2008). We manually filtered the list removing
most idioms that are very unlikely to be ever used
literally (anymore), e.g. to knock on heaven?s door.
For each of the resulting list of phrases, we extracted
usage contexts from the ukWaC corpus (Baroni et
al., 2009). Each usage context contains 5 sentences,
where the sentence with the target phrase appears in
a randomized position. Due to segmentation errors,
some usage contexts actually might contain less than
5 sentences, but we manually filtered all usage con-
texts where the remaining context was insufficient.
This was done in the final cleaning step where we
also manually removed (near) duplicates, obvious
spam, encoding problems etc.
The target phrases in context were annotated for
figurative, literal, both or impossible to tell usage,
using the CrowdFlower2 crowdsourcing annotation
platform. We used about 8% of items as ?gold?
items for quality assurance, and had each example
annotated by three crowdworkers. The task was
comparably easy for crowdworkers, who reached
90%-94% pairwise agreement, and 95% success on
the gold items. About 5% of items with low agree-
ment and marked as impossible were removed. Ta-
ble 3 summarizes the quantitative characteristics of
all datasets resulting from this process. We took care
in sampling the data as to keep similar distributions
across the training, development and testing parts.
4.2 Results
Training and development datasets were made avail-
able in advance, test data was provided during the
evaluation period without labels. System perfor-
2www.crowdflower.com
44
Task Dataset # Phrases # Items Items per phrase # Liter. # Figur. # Both
known
train 10 1,424 68?188 702 719 3
dev 10 358 17?47 176 181 1
test 10 594 28?78 294 299 1
unseen
train 31 1,114 4?75 458 653 3
dev 9 342 4?74 141 200 1
test 15 518 8?73 198 319 1
Table 3: Quantitative characteristics of the datasets
Rank System Run Accuracy
1 IIRG 3 .779
2 UNAL 2 .754
3 UNAL 1 .722
5 IIRG 1 .530
4 Baseline MFC - .503
6 IIRG 2 .502
Table 4: Task 5b: Evaluation results for the known
phrases setting
Rank System Run Accuracy
1 UNAL 1 .668
2 UNAL 2 .645
3 Baseline MFC - .616
4 CLaC 1 .550
Table 5: Task 5b: Evaluation results for the unseen
phrases setting
mance was measured in accuracy. Since all partic-
ipants provided classifications for all test items, the
accuracy score is equivalent to precision/recall/F1.
Participants were allowed to enter up to three dif-
ferent runs for evaluation. We also provide baseline
accuracy scores, which are obtained by always as-
signing the most frequent class (figurative).
Table 4 provides the evaluation results for the
known phrases task, while Table 5 ranks participants
for the unseen phrases task. As expected, the un-
seen phrases setting is much harder than the known
phrases setting, as for unseen phrases it is not possi-
ble to learn lexicalised contextual clues. In both set-
tings, the winning entries were able to beat the MFC
baseline. While performance in the known phrases
setting is close to 80% and thus acceptable, the gen-
eral task of recognizing the literal or figurative use of
unseen phrases remains very challenging, with only
a small improvement over the baseline. We refer to
the system descriptions for more details on the tech-
niques used for this subtask: UNAL (Jimenez et al,
2013), IIRG (Byrne et al, 2013) and CLaC (Siblini
and Kosseim, 2013).
5 Task Conclusions
In this section, we further discuss the findings and
conclusion of the evaluation challenge in the task of
?Phrasal Semantics?.
Looking at the results of both subtasks, one ob-
serves that the maximum performance achieved is
higher for the first than the second subtask. For
this comparison to be fair, trivial baselines should be
taken into account. A system randomly assigning an
output value would be on average 50% correct in the
first subtask, since the numbers of positive and neg-
ative instances in the testing set are equal. Similarly,
a system assigning the most frequent class, i.e. the
figurative use of any phrase, would be 50.3% and
61.6% accurate in the second subtask for seen and
unseen test instances, respectively. It should also be
noted that the testing instances in the first subtask
are unseen in the respective training set. As a result,
in terms of baselines, the second subtask on unseen
data (Table 5) should be considered easier than the
first subtask (Table 2). However, the best perform-
ing systems achieved much higher accuracy in the
first than in the second subtask. This contradiction
confirms our conception that the first subtask is less
complex than the second.
In the first subtask, it is evident that no method
performs much better or much worse than the others.
45
Although the participating systems have employed a
wide variety of approaches and tools, the difference
between the best and worst accuracy achieved is
relatively limited, in particular approximately 14%.
Even more interestingly, unsupervised approaches
performed better than some supervised ones. This
observation suggests that no ?golden recipe? has
been identified so far for this task. Thus, probably
different processing tools take advantage of different
sources of information. It is a matter of future re-
search to identify these sources and the correspond-
ing tools, and then develop hybrid methods of im-
proved performance.
In the second subtask, the results of evaluation
on known phrases are much higher than on unseen
phrases. This was expected, as for unseen phrases it
is not possible to learn lexicalised contextual clues.
Thus, the second subtask has succeeded in identify-
ing the complexity threshold up to which the cur-
rent state-of-the-art can address the computational
problem. Further than this threshold, i.e. for unseen
phrases, current systems have not yet succeeded in
addressing it. In conclusion, the difficulty in eval-
uating the compositionality of previously unseen
phrases in context highlights the overall complexity
of the second subtask.
6 Summary and Future Work
In this paper we have presented the 5th task of Se-
mEval 2013, ?Evaluating Phrasal Semantics?, which
consists of two subtasks: (1) semantic similarity of
words and compositional phrases, and (2) compo-
sitionality of phrases in context. The former sub-
task, which focussed on the first step of composing
the meaning of phrases of any length, is less com-
plex than the latter subtask, which considers the ef-
fect of context to the semantics of a phrase. The
paper presents details about the background and im-
portance of these subtasks, the data creation process,
the systems that took part in the evaluation and their
results.
In the future, we expect evaluation challenges on
phrasal semantics to progress towards two direc-
tions: (a) the synthesis of semantics of sequences
longer than two words, and (b) aiming to improve
the performance of systems that determine the com-
positionality of previously unseen phrases in con-
text. The evaluation results of the first task sug-
gest that state-of-the-art systems can compose the
semantics of two word sequences with a promising
level of success. However, this task should be seen
as the first step towards composing the semantics
of sentence-long sequences. As far as subtask 5b
is concerned, the accuracy achieved by the partici-
pating systems on unseen testing data was low, only
slightly better than the most frequent class baseline,
which assigns the figurative use to all test phrases.
Thus, the subtask cannot be considered well ad-
dressed by the state-of-the-art and further progress
should be sought.
Acknowledgements
The work relevant to subtask 5a described in this pa-
per is funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
We would like to thank Tristan Miller for help-
ing with the subtleties of English idiomatic ex-
pressions, and Eugenie Giesbrecht for support
in the organization of subtask 5b. This work
has been supported by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Pro-
gram under grant No. I/82806, and by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-o?konomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities.
46
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA. Association for Compu-
tational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Lorna Byrne, Caroline Fenlon, and John Dunnion. 2013.
IIRG: A naive approach to evaluating phrasal seman-
tics. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
He?ctor Da?vila, Antonio Ferna?ndez Orqu??n, Alexander
Cha?vez, Yoan Gutie?rrez, Armando Collazo, Jose? I.
Abreu, Andre?s Montoyo, and Rafael Mun?oz. 2013.
UMCC DLSI-(EPS): Paraphrases detection based on
semantic distance. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?37, Uppsala, Sweden. Association for Com-
putational Linguistics.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. UNAL: Discriminating between literal
and figurative phrasal usage using distributional statis-
tics and POS tags. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio. As-
sociation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda C?elebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, ACL ?03, pages
375?382, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic Part-of-Speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Reda Siblini and Leila Kosseim. 2013. CLaC: Semantic
relatedness of words and phrases. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Atlanta, Georgia, USA.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Panayiotis
Bozanis and Elias N. Houstis, editors, Advances in In-
formatics, volume 3746, chapter 36, pages 382?392.
Springer Berlin Heidelberg, Berlin, Heidelberg.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1012?1022, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tim Van de Cruys, Stergos Afantenos, and Philippe
Muller. 2013. MELODI: Semantic similarity of words
and compositional phrases using latent vector weight-
ing. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.
Christian Wartena. 2013. HsH: Estimating semantic sim-
ilarity of words and short phrases with frequency nor-
malized distance measures. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Atlanta, Georgia, USA.
Eros Zanchetta and Marco Baroni. 2005. Morph-it!: A
free corpus-based morphological resource for the ital-
ian language. Corpus Linguistics 2005, 1(1).
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING).
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
Wikipedia and Wiktionary. Proceedings of the Confer-
ence on Language Resources and Evaluation (LREC),
15:60.
47
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 55?59,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Co-occurrence Cluster Features for Lexical Substitutions in Context
Chris Biemann
Powerset (a Microsoft company)
475 Brannan St Ste. 330
San Francisco, CA 94107, USA
cbiemann@microsoft.com
Abstract
This paper examines the influence of fea-
tures based on clusters of co-occurrences
for supervised Word Sense Disambigua-
tion and Lexical Substitution. Co-
occurrence cluster features are derived
from clustering the local neighborhood of
a target word in a co-occurrence graph
based on a corpus in a completely un-
supervised fashion. Clusters can be as-
signed in context and are used as features
in a supervised WSD system. Experi-
ments fitting a strong baseline system with
these additional features are conducted on
two datasets, showing improvements. Co-
occurrence features are a simple way to
mimic Topic Signatures (Mart??nez et al,
2008) without needing to construct re-
sources manually. Further, a system is de-
scribed that produces lexical substitutions
in context with very high precision.
1 Introduction
Word Sense Disambiguation (WSD, see (Agirre
and Edmonds, 2006) for an extensive overview)
is commonly seen as an enabling technology for
applications like semantic parsing, semantic role
labeling and semantic retrieval. Throughout re-
cent years, the Senseval and Semeval competitions
have shown that a) WordNet as-is is not an ade-
quate semantic resource for reaching high preci-
sion and b) supervised WSD approaches outper-
form unsupervised (i.e. not using sense-annotated
examples) approaches. Due to the manual effort
involved in creating more adequate word sense in-
ventories and sense-annotated training data, WSD
has yet to see its prime-time in real world applica-
tions.
Since WordNet?s sense distinctions are often too
fine-grained for allowing reliable distinctions by
machines and humans, the OntoNotes project
(Hovy et al, 2006) conflated similar WordNet
senses until 90% inter-annotator agreement on
sense-labelling was reached. The SemEval 2007
lexical sample task employs this ?coarse-grained?
inventory, which allows for higher system perfor-
mance.
To alleviate the bottleneck of sense-labelled sen-
tences, (Biemann and Nygaard, 2010) present
an approach for acquiring a sense inventory
along with sense-annotated example usages using
crowdsourcing, which makes the acquisition pro-
cess cheaper and potentially quicker.
Trying to do away with manual resources entirely,
the field of Word Sense Induction aims at induc-
ing the inventory from text corpora by clustering
occurrences or senses according to distributional
similarity, e.g. (Veronis, 2004). While such unsu-
pervised and knowledge-free systems are capable
of discriminating well between different usages, it
is not trivial to link their distinctions to existing se-
mantic resources, which is often necessary in ap-
plications.
Topic Signatures (Mart??nez et al, 2008) is an at-
tempt to account for differences in relevant topics
per target word. Here, a large number of contexts
for a given sense inventory are collected automat-
ically using relations from a semantic resource,
sense by sense. The most discriminating content
words per sense are used to identify a sense in
an unseen context. This approach is amongst the
most successful methods in the field. It requires,
however, a semantic resource of sufficient detail
and size and a sense-labeled corpus to estimate
priors from the sense distribution. Here, a sim-
ilar approach is described that uses an unlabeled
55
corpus alone for unsupervised topic signature ac-
quisition using graph clustering, not relying on the
existence of a WordNet. Unlike in previous eval-
uations like (Agirre et al, 2006), parameters for
word sense induction are not optimized globally,
but instead several parameter settings are offered
as features to a Machine Learning setup.
Experimental results are provided for two datasets:
the Semeval-2007 lexical sample task (Pradhan et
al., 2007) and the Turk bootstrap Word Sense In-
ventory (TWSI1, (Biemann and Nygaard, 2010) ).
2 Cluster Co-occurrence Features
2.1 Graph Preperation and Parameterization
Similar to the approach in (Widdows and Dorow,
2002), a word graph around each target word
is constructed. In this work, sentence-based
co-occurrence statistics from a large corpus are
used as a basis to to construct several word
graphs for different parameterizations. Significant
co-occurrences between all content words (nouns,
verbs, adjectives as identified by POS tagging) are
computed from a large corpus using the tinyCC2
tool. The full word graph for a target word is de-
fined as all words significantly co-occurring with
the target as nodes, with edge weights set to the
log-likelihood significance of the co-occurrence
between the words corresponding to nodes. Edges
between words that co-occur only once or with
significance smaller than 6.63 (1% confidence
level) are omitted.
Aiming at different granularities of usage clusters,
the graph is parameterized by a size parameter
t and a density parameter n: Only the most
significant t co-occurrences of the target enter the
graph as nodes, and an edge between nodes is
drawn only if one of the corresponding words is
contained in the most significant n co-occurrences
of the other.
2.2 Graph Clustering Parameterization
As described in (Biemann, 2006), the neighbor-
hood graph is clustered with Chinese Whispers.
This efficient graph clustering algorithm finds the
numbers of clusters automatically and returns a
partition of the nodes. It is initialized by assigning
different classes to all nodes in the graph. Then,
1full dataset available for download at
http://aclweb.org/aclwiki/index.php?title=Image:TWSI397.zip
2http://beam.to/biem/software/TinyCC2.html
a number of local update steps are performed, in
which a node inherits the predominant class in its
neighborhood. At this, classes of adjacent nodes
are weighted by edge weight and downweighted
by the degree (number of adjacent nodes) of the
neighboring node. This results in hard clusters of
words per target, which represent different target
usages.
Downweighting nodes by degree is done accord-
ing to the following intuition: nodes with high
degrees are probably very universally used words
and should be less influential for clustering. Three
ways of node weighting are used: (a) dividing the
influence of a node in the update step by the degree
of the node, (b) dividing by the natural logarithm
of the degree + 1 and (c) not doing node weight-
ing. The more aggressive the downweighting, the
higher granularity is expected for the clustering.
It is emphasized that no tuning techniques are ap-
plied to arrive at the ?best? clustering. Rather, sev-
eral clusterings of different granularities as fea-
tures are made available to a supervised system.
Note that this is different from (Agirre et al,
2006), where a single global clustering was used
directly in a greedy mapping to senses.
2.3 Feature Assignment in Context
For a given occurrence of a target word, the
overlap in words between the textual context
and all clusters from the neighborhood graph is
measured. The cluster ID of the cluster with the
highest overlap is assigned as a feature. This can
be viewed as a word sense induction system in its
own right.
At this, several clusterings from different param-
eterizations are used to form distinct features,
which enables the machine learning algorithm to
pick the most suitable cluster features per target
word when building the classification model.
2.4 Corpora for Cluster Features
When incorporating features that are induced us-
ing large unlabeled corpora, it is important to en-
sure that the corpus for feature induction and the
word sense labeled corpus are from the same do-
main, ideally from the same source.
Since TWSI has been created from Wikipedia, an
English Wikipedia dump from January 2008 is
used for feature induction, comprising a total of 60
million sentences. The source for the lexical sam-
ple task is the Wall Street Journal, and since the
56
76,400 sentences from the WSJ Penn Treebank are
rather small for co-occurrence analysis, a 20 Mil-
lion sentence New York Times corpus was used
instead.
For each corpus, a total of 45 different clus-
terings were prepared for all combinations of
t={50,100,150,200,250}, n={50,100,200} and
node degree weighting options (a), (b) and (c).
3 Experimental Setup
3.1 Machine Learning Setup
The classification algorithm used throughout this
work is the AODE (Webb et al, 2005) classifier
as provided by the WEKA Machine Learning
software (Hall et al, 2009). This algorithm is
similar to a Na??ve Bayes classifier. As opposed
to the latter, AODE does not assume mutual
independence of features but models correlations
between them explicitly, which is highly desirable
here since both baseline and co-occurrence cluster
features are expected to be highly correlated.
Further, AODE handles nominal features, so it is
directly possible to use lexical features and cluster
IDs in the classifier. AODE showed superior
performance to other classifiers handling nominal
features in preliminary experiments.
3.2 Baseline System
The baseline system relies on 15 lexical and POS-
based nominal features: word forms left and right
from target, POS sequences left and right bigram
around target, POS tags of left and right word from
target, and POS tag of target, two left and two right
nouns from target, left and right verbs from target
and left and right adjectives from target.
3.3 Feature Selection
To determine the most useful cluster co-
occurrence features, they were added to the
baseline features one at the time, measuring the
contribution using 10-fold cross validation on
the training set. Then, the best k single cluster
features for k={2,3,5,10} were added together
to account for a range of different granularities.
The best performing system on the lexical sample
training data resulted in a 10-fold accuracy of
88.5% (baseline: 87.1%) for k=3. On the 204
ambiguous words (595 total senses with 46
sentences per sense on average) of the TWSI
only, the best system was found at k=5 with a
System F1
NUS-ML 88.7% ? 1.2
top3 cluster, optimal F1 88.0% ? 1.2
top3 cluster, max recall 87.8% ? 1.2
baseline, optimal F1 87.5% ? 1.2
baseline, max recall 87.3% ? 1.2
UBC-ALM 86.9% ? 1.2
Table 1: Cluster co-occurrence features and base-
line in comparison to the best two systems in the
SemEval 2007 Task 17 Lexical Sample evaluation
(Pradhan et al, 2007). Error margins provided by
the task organizers.
10-fold accuracy of 83.0% (baseline: 80.7%,
MFS: 71.5%). Across the board, all single
co-occurrence features improve over the baseline,
most of them significantly.
4 Results
4.1 SemEval 2007 lexical sample task
The system in the configuration determined above
was trained on the full training set and applied it
to the test data provided bt the task organizers.
Since the AODE classifier reports a confidence
score (corresponding to the class probability for
the winning class at classification time), it is possi-
ble to investigate a tradeoff between precision and
recall to optimize the F1-value3 used for scoring
in the lexical sample task.
It is surprising that the baseline system outper-
forms the second-best system in the 2007 evalua-
tion, see Table 1. This might be attributed to the
AODE classifier used, but also hints at the power
of nominal lexical features in general.
The co-occurrence cluster system outperforms the
baseline, but does not reach the performance of the
winning system. However, all reported systems
fall into each other?s error margins, unlike when
evaluating on training data splits. In conclusion,
the WSD setup is competitive to other WSD sys-
tems in the literature, while using only minimal
linguistic preprocessing and no word sense inven-
tory information beyond what is provided by train-
ing examples.
3F1 = (2 ? precision ? recall)/(precision + recall)
57
Substitutions
Gold System Random
YES 469 (93.8%) 456 (91.2%) 12 (2.4%)
NO 14 (2.8%) 27 (5.4%) 485 (97.0%)
SOMEWHAT 17 (3.4%) 17 (3.4%) 3 (0.6%)
Table 2: Substitution acceptability as measured by
crowdsourcing for TWSI gold assignments, sys-
tem assignments and random assignments.
4.2 Substitution Acceptability
For evaluating substitution acceptability, 500
labeled sentences from the overall data (for all
397 nouns, not just the ambiguous nouns used in
the experiments above) were randomly selected.
The 10-fold test classifications as described above
were used for system word sense assignment. The
three highest ranked substitutions per sense from
the TWSI are supplied as substitutions.
In a crowdsourcing task, workers had to state
whether the substitutions provided for a target
word in context do not change the meaning of
the sentence. Each assignment was given to three
workers.
Since this measures both substitution quality of
the TWSI and the system?s capability of assigning
the right sense, workers were also asked to score
the substitutions for the gold standard assignments
of this data set. For control, random substitution
quality for all sentences is measured.
Table 2 shows the results for averaging over
the worker?s responses. For being counted as
belonging to the YES or NO class, the majority of
workers had to choose this option, otherwise the
item was counted into the SOMEWHAT class.
The substitution quality of the gold standard is
somewhat noisy, containing 2.8% errors and 3.4%
questionable cases. Despite this, the system is able
to assign acceptable substitutions in over 91% of
cases, questionable substitutions for 3.4% at an
error rate of only 5.4%. Checking the positively
judged random assignments, an acceptable substi-
tution was found in about half of the cases by the
author, which allows to estimate the worker noise
at about 1%.
When using confidence values of the AODE clas-
sifier to control recall as reported in Table 3, it is
possible to further reduce error rates, which might
e.g. improve retrieval applications.
coverage YES NO
100% 91.2% 5.4%
95% 91.8% 3.4%
90% 93.8% 2.9%
80% 94.8% 2.0%
70% 95.7% 0.9%
Table 3: Substitution acceptability in reduced cov-
erage settings. SOMEWHAT class accounts for
percentage points missing to 100%.
5 Conclusion
A way to improve WSD accuracy using a family of
co-occurrence cluster features was demonstrated
on two data sets. Instead of optimizing parameters
globally, features corresponding to different gran-
ularities of induced word usages are made avail-
able in parallel as features in a supervised Machine
Learning setting.
Whereas the contribution of co-occurrence fea-
tures is significant on the TWSI, it is not signif-
icantly improving results on the SemEval 2007
data. This might be attributed to a larger number
of average training examples in the latter, making
smoothing over clusters less necessary due to less
lexical sparsity.
We measured performance of our lexical substi-
tution system by having the acceptability of the
system-provided substitutions in context manually
judged. With error rates in the single figures and
the possibility to reduce error further by sacrific-
ing recall, we provide a firm enabling technology
for semantic search.
For future work, it would be interesting to evaluate
the full substitution system based on the TWSI in
a semantic retrieval application.
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations, volume 33 of Text, Speech and Language
Technology. Springer, July.
Eneko Agirre, David Mart??nez, Oier L. de Lacalle, and
Aitor Soroa. 2006. Evaluating and optimizing the
parameters of an unsupervised graph-based wsd al-
gorithm. In Proceedings of TextGraphs: the Sec-
ond Workshop on Graph Based Methods for Natural
Language Processing, pages 89?96, New York City.
Association for Computational Linguistics.
58
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing WordNet. In Proceedings of the 5th Global
WordNet conference, Mumbai, India. ACL Data and
Code Repository, ADCR2010T005.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of the HLT-NAACL-06 Workshop on Textgraphs-06,
New York, USA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
HLT-NAACL 2006, pages 57?60.
David Mart??nez, Oier Lopez de Lacalle, and Eneko
Agirre. 2008. On the use of automatically acquired
examples for all-nouns word sense disambiguation.
J. Artif. Intell. Res. (JAIR), 33:79?107.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 87?
92, Prague, Czech Republic, June. Association for
Computational Linguistics.
Jean Veronis. 2004. Hyperlex: lexical cartography
for information retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
G. Webb, J. Boughton, and Z. Wang. 2005. Not so
Naive Bayes: Aggregating one-dependence estima-
tors. Machine Learning, 58(1):5?24.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Pro-
ceedings of the 19th international conference on
Computational linguistics, pages 1?7, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
59
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 21?28,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Distributional Semantics and Compositionality 2011:
Shared Task Description and Results
Chris Biemann
UKP lab, Technical University of Darmstadt
Hochschulstr. 10
64289 Darmstadt, Germany
biemann@tk.informatik.tu-darmstadt.de
Eugenie Giesbrecht
FZI Forschungszentrum Informatik
Haid-und-Neu-Str. 10-14
76131 Karlsruhe, Germany
giesbrecht@fzi.de
Abstract
This paper gives an overview of the shared
task at the ACL-HLT 2011 DiSCo (Distribu-
tional Semantics and Compositionality) work-
shop. We describe in detail the motivation
for the shared task, the acquisition of datasets,
the evaluation methodology and the results
of participating systems. The task of assign-
ing a numerical score for a phrase accord-
ing to its compositionality showed to be hard.
Many groups reported features that intuitively
should work, yet showed no correlation with
the training data. The evaluation reveals that
most systems outperform simple baselines, yet
have difficulties in reliably assigning a compo-
sitionality score that closely matches the gold
standard. Overall, approaches based on word
space models performed slightly better than
methods relying solely on statistical associa-
tion measures.
1 Introduction
Any NLP system that does semantic processing re-
lies on the assumption of semantic compositionality:
the meaning of a phrase is determined by the mean-
ings of its parts and their combination. However,
this assumption does not hold for lexicalized phrases
such as idiomatic expressions, which causes troubles
not only for semantic, but also for syntactic process-
ing (Sag et al, 2002). In particular, while distribu-
tional methods in semantics have proved to be very
efficient in tackling a wide range of tasks in natural
language processing, e.g., document retrieval, clus-
tering and classification, question answering, query
expansion, word similarity, synonym extraction, re-
lation extraction, textual advertisement matching in
search engines, etc. (see Turney and Pantel (2010)
for a detailed overview), they are still strongly lim-
ited by being inherently word-based. While dictio-
naries and other lexical resources contain multiword
entries, these are expensive to obtain and not avail-
able for all languages to a sufficient extent. Fur-
thermore, the definition of a multiword varies across
resources, and non-compositional phrases are often
merely a subclass of multiword units.
This shared task addressed researchers that are
interested in extracting non-compositional phrases
from large corpora by applying distributional mod-
els that assign a graded compositionality score to a
phrase, as well as researchers interested in express-
ing compositional meaning with such models. The
score denotes the extent to which the composition-
ality assumption holds for a given expression. The
latter can be used, for example, to decide whether
the phrase should be treated as a single unit in ap-
plications. We emphasized that the focus is on au-
tomatically acquiring semantic compositionality and
explicitly did not invite approaches that employ pre-
fabricated lists of non-compositional phrases.
It is often the case that compositionality of a
phrase depends on the context. Though we have
used a sentence context in the process of construct-
ing the gold standard, we have decided not to pro-
vide it with the dataset: we have asked for a sin-
gle compositionality score per phrase. In an appli-
cation, this could play the role of a compositional-
ity prior that could, e.g., be stored in a dictionary.
There is a long-living tradition within the research
21
community working on multiword units (MWUs) to
automatically classify MWUs into either composi-
tional or non-compositional ones. However, it has
been often noted that compositionality comes in de-
grees, and a binary classification is not valid enough
in many cases (Bannard et al, 2003; Katz and Gies-
brecht, 2006). To the best of our knowledge, this has
been the first attempt to offer a dataset and a shared
task that allows to explicitly evaluate the models of
graded compositionality.
2 Shared Task Description
For the shared task, we aimed to get composition-
ality scores for phrases frequently occurring in cor-
pora. Since distributional models need large corpora
to perform reliable statistics, and these statistics are
more reliable for frequent items, we chose to restrict
the candidate set to the most frequent phrases from
the freely available WaCky1 web corpora (Baroni et
al., 2009). Those are currently downloadable for En-
glish, French, German and Italian. They have al-
ready been automatically sentence-split, tokenized,
part-of-speech (POS) tagged and lemmatized, which
reduces the load on both organizers and participants
that decide to make use of these corpora. Further,
WaCky corpora provide a good starting point for ex-
perimenting with distributional models due to their
size, ranging between 1-2 billion tokens, and exten-
sive efforts to make these corpora as clean as possi-
ble.
2.1 Candidate Selection
There is a wide range of subsentential units that can
function as a non-compositional construction. These
units do not have to be realized continuously in the
surface realization and can consist of an arbitrary
number of lexical items. While it would be interest-
ing to examine unrestricted forms of multiwords and
compositional phrases, we decided to restrict candi-
date selection to certain grammatical constructions
to make the task more tangible. Specifically, we use
word pairs in the following relations:
? ADJ NN: Adjective modifying a noun, e.g.
?red herring? or ?blue skies?
1http://wacky.sslmit.unibo.it
? V SUBJ: Noun in subject position and verb,
e..g. ?flies fly? or ?people transfer (sth.)?
? V OBJ: Noun in object position and verb, e.g.
?lose keys?, ?play song?
While it is possible to extract the relations fairly ac-
curately from parsed English text, there is ? to our
knowledge ? no reliable, freely available method
that can tell verb-subjects from verb-objects for Ger-
man. Thus, we employed a three-step selection
procedure for producing a set of candidate phrases
per grammatical relation and language that involved
heavy manual intervention.
1. Extract candidates using (possibly over-
generating) patterns over part-of-speech
sequences and sort by frequency
2. Manually select plausible candidates for the
target grammatical relation in order of decreas-
ing frequency
3. Balance the candidate set to select enough non-
compositional phrases
For English, we used the following POS pat-
terns: ADJ NN: ?JJ* NN*?; V SUBJ: ?NN* VV*?;
V OBJ: ?VV* DT|CD NN*? and ?VV* NN*?. The
star * denotes continuation of tag labels: e.g. VV*
matches all tags starting with ?VV?, such as VV,
VVD, VVG, VVN, VVP and VVZ.
For German, we used ?ADJ* NN*? for ADJ NN.
For relations involving nouns and verbs, we ex-
tracted all noun-verb pairs in a window of 4 tokens
and manually filtered by relation on the aggregated
frequency list. Frequencies were computed on the
lemma forms.
This introduces a bias on the possible construc-
tions that realize the target relations, especially for
the verb-noun pairs. Further, the selection procedure
is biased by the intuition of the person that performs
the selection. We only admitted what we thought
were clear-cut cases (only nouns that are typically
found in subject respectively object position) to the
candidate set at this stage.
Since non-compositional phrases are much less
in numbers than compositional phrases, we tried to
somewhat balance this in the third step in the se-
lection. If the candidates would have been randomly
22
selected, an overwhelming number of compositional
phrases would have rendered the task very hard to
evaluate, since a baseline system predicting high
compositionality in all cases would have achieved
a very high score. We argue that since we are es-
pecially interested in non-compositional phrases in
this competition, it is valid to bias the dataset in this
way.
After we collected a candidate list, we randomly
selected seven sentences per candidate from the cor-
pus. Through manual filtering, we checked whether
the target word pair was in fact found in the target
relation in these sentences. Further we removed in-
complete and too long sentences, so that we ended
up with five sentences per target phrase. Some can-
didate phrases that only occurred in very fixed con-
texts (e.g. disclaimers) or did not have enough well-
formed sentences were removed in this step.
Figure 1 shows the sentences for ?V OBJ: buck
trend? as an example output of this procedure.
2.2 Annotation
The sample usages of target phrases now had to be
annotated for compositionality. We employed the
crowdsourcing service Amazon Turk2 for realizing
these annotations. The advantage of crowdsourc-
ing is its scalability through the large numbers of
workers that are ready to perform small tasks for
pay. The disadvantage is that tasks usually cannot be
very complex, since quality issues (scammers) have
to be addressed either with test items or redundancy
or both ? mechanisms that only work for types of
tasks where there is clearly a correct answer.
Previous experiences in constructing linguistic
annotations with Amazon Turk (Biemann and Ny-
gaard, 2010) made us stick to the following two-step
procedure that more or less ensured the quality of
annotation by hand-picking workers:
1. Gather high quality workers: In an open task
for a small data sample with unquestionable de-
cisions, we collected annotations from a large
number of workers. Workers were asked to
provide reasons for their decisions. Workers
that performed well, gave reasons that demon-
strated their understanding of the task and com-
pleted a significant amount of the examples
2http://www.mturk.com
were invited for a closed task. Net pay was 2
US cents for completing a HIT.
2. Get annotations for the real task: In the closed
task, only invited workers were admitted and
redundancy was reduced to four workers per
HIT. Net pay was 3 US cents for completing
a HIT.
Figure 2 shows a sample HIT (human intelligence
task) for English on Amazon turk, including in-
structions. Workers were asked to enter a judgment
from 0-10 about the literacy of the highlighted tar-
get phrase in the respective context. For the German
data, we used an equivalent task definition in Ger-
man.
All five contexts per target phrase were scored by
four workers each. A few items were identified as
problematic by the workers (e.g. missing highlight-
ing, too little context), and one worker was excluded
during the English experiment for starting to delib-
erately scam. For this worker, all judgments were re-
moved and not repeated. Thus, the standard number
of judgments per target phrase was 20, with some
targets receiving less judgments because of these
problems. The minimum number of judgments per
target phrase was 12: four HITs with three judg-
ments each.
From this, we computed a score by averaging over
all judgments per phrase and multiplying the over-
all score by 10 to get scores in the range of 0-100.
This score cannot help in discriminating moderately
compositional phrases like ?V OBJ: make decision?
from phrases that are dependent on the context like
?V OBJ: wait minute? which had two HITs for the
idiomatic use of ?wait a minute!? and three HITs
with literally minutes to spend idling.
As each HIT was annotated by a possibly differ-
ent set of workers, it is not possible to compute inter-
annotator agreement. Eyeballing the scores revealed
that some workers generally tend to give higher re-
spectively lower scores than others. Overall, work-
ers agreed more for clearly compositional or clearly
non-compositional HITs. We believe that using this
comparatively high number of judgments per target,
averaged over several contexts, should give us fairly
reliable judgments, as worker biases should cancel
out each other.
23
? I would like to buck the trend of complaint !
? One company that is bucking the trend is Flowcrete Group plc located in Sandbach , Cheshire .
? ? We are now moving into a new phase where we are hoping to buck the trend .
? With a claimed 11,000 customers and what look like aggressive growth plans , including recent acquisitions of
Infinium Software , Interbiz and earlier also Max international , the firm does seem to be bucking the trend of
difficult times .
? Every time we get a new PocketPC in to Pocket-Lint tower , it seems to offer more features for less money and
the HP iPaq 4150 is n?t about to buck the trend .
Figure 1: sentences for V OBJ: buck trend after manual filtering and selection. The target is highlighted.
How literal is this phrase?
Can you infer the meaning of a given phrase by only considering their parts literally, or does the phrase carry a ?special? meaning?
In the context below, how literal is the meaning of the phrase in bold?
Enter a number between 0 and 10.
? 0 means: this phrase is not to be understood literally at all.
? 10 means: this phrase is to be understood very literally.
? Use values in between to grade your decision. Please, however, try to take a stand as often as possible.
In case the context is unclear or nonsensical, please enter ?66? and use the comment field to explain. However, please try to make sense of it even if the sentences are incomplete.
Example 1 :
There was a red truck parked curbside. It looked like someone was living in it.
YOUR ANSWER: 10
reason: the color of the truck is red, this can be inferred from the parts ?red? and ?truck? only - without any special knowledge.
? Example 2 :
What a tour! We were on cloud nine when we got back to headquarters but we kept our mouths shut.
YOUR ANSWER: 0
reason: ?cloud nine? means to be blissfully happy. It does NOT refer to a cloud with the number nine.
Example 3 :
Yellow fever is found only in parts of South America and Africa.
YOUR ANSWER: 7
reason: ?yellow fever? refers to a disease causing high body temperature. However, the fever itself is not yellow. Overall, this phrase is fairly literal, but not totally, hence answering with a value
between 5 and 8 is appropriate.
We take rejection seriously and will not reject a HIT unless done carelessly. Entering anything else but numbers between 0 and 10 or 66 in the judgment field will automatically trigger rejection.
YOUR CONTEXT with big day
Special Offers : Please call FREEPHONE 0800 0762205 to receive your free copy of ? Groom ? the full
colour magazine dedicated to dressing up for the big day and details of Moss Bros Hire rates .
How literal is the bolded phrase in the context above between 0 and 10?
[ ]
OPTIONAL: leave a comment, tell us about what is broken, help us to improve this type of HIT:
[ ]
Figure 2: Sample Human Intelligence Task on Amazon Turk with annotation instructions
24
EN ADJ NN V SUBJ V OBJ Sum
Train 58 (43) 30 (23) 52 (41) 140 (107)
Vali. 10 (7) 9 (6) 16 (13) 35 (26)
Test 77 (52) 35 (26) 62 (40) 174 (118)
All 145 (102) 74 (55) 130 (94) 349 (251)
Table 1: English dataset: number of target phrases (with
coarse scores)
DE ADJ NN V SUBJ V OBJ Sum
Train 49 (42) 26 (23) 44 (33) 119 (98)
Vali. 11 (8) 9 (8) 9 (7) 29 (23)
Test 63 (48) 29 (28) 57 (44) 149 (120)
All 123 (98) 64 (59) 110 () 297 (241)
Table 2: German dataset: number of target phrases (with
coarse scores)
Additionally to the numerical scores, we?ve also
provided coarse-grained labels. This is motivated
by the following: for some applications, it is prob-
ably enough to decide whether a phrase is always
compositional, somewhat compositional or usually
not compositional, without the need of more fine-
grained distinctions. For this, we?ve transformed the
numerical scores in the range of 0-25 to coarse la-
bel ?low?, those between 38-62 have been labeled
as ?medium?, and the ones from 75 to 100 have re-
ceived the value ?high?. All other phrases have been
excluded from the corresponding training and test
datasets for ?coarse evaluation? (s. Section 2.4.2):
28.1% of English and 18.9% of German phrases.
2.3 Datasets
Now we describe the datasets in detail. Table 1 sum-
marizes the English data, Table 2 describes the Ger-
man data quantitatively. Per language and relation,
the data was randomly split in approximatively 40%
training, 10% validation and 50% test.
2.4 Scoring of system responses
We provided evaluation scripts along with the train-
ing and validation data. Additionally, we report cor-
relation values (Spearman?s rho and Kendall?s tau)
in Section 4.
2.4.1 Numerical Scoring
For numerical scoring, the evaluation script com-
putes the distance between the system responses
S = {starget1, starget2, ...stargetN} and the gold
standard G = {gtarget1, gtarget2, ...gtargetN} in
points, averaged over all items:
NUMSCORE(S,G) = 1N
?
i=1..N |gi ? si|.
Missing values in the system scores are filled
with the default value of 50. A perfect score is
0, indicating no difference between the system
responses and the gold standard.
2.4.2 Coarse Scoring
We use precision on coarse label predictions for
coarse scoring:
COARSE(S,G) =
1
N
?
i=1..N
{ si == gi : 1
otherwise : 0.
As with numerical scoring, missing system re-
sponses are filled with a default value, in this case
?medium?. A perfect score would be 1.00, connot-
ing complete congruence of gold standard and sys-
tem response labels.
3 Participants
Seven teams participated in the shared task. Table 3
summarizes the participants and their systems. Four
of the teams (Duluth, UoY, JUCSE, SCSS-TCD)
submitted three runs for the whole English test set.
One team participated with two systems, one of
which was for the entire English dataset and an-
other one included entries only for English V SUBJ
and V OBJ relations. A team from UNED provided
scores solely for English ADJ NN pairs. UCPH was
the only team that delivered results for both English
and German.
Systems can be split into approaches based on sta-
tistical association measures and approaches based
on word space models. On top, some systems used
a machine-learned classifier to predict numerical
scores or coarse labels.
4 Results
The results of the official evaluation for English are
shown in Tables 4 and 5.
Table 4 reports the results for numerical scor-
ing. UCPH-simple.en performed best with the score
of 16.19. The second best system UoY: Exm-Best
achieved 16.51, and the third was UoY:Pro-Best
with 16.79. It is worth noting that the top six systems
25
Systems Institution Team Approach
Duluth-1 Dept. of Computer Science, Ted Pedersen statistical
Duluth-2 University of Minnesota association measures:
Duluth-3 t-score and pmi
JUCSE-1 Jadavpur University Tanmoy Chakraborty, Santanu Pal mix of statistical
JUCSE-2 Tapabrata Mondal, Tanik Saikh, association measures
JUCSE-3 Sivaju Bandyopadhyay
SCSS-TCD:conf1 SCSS, Alfredo Maldonado-Guerra, unsupervised WSM,
SCSS-TCD:conf2 Trinity College Dublin Martin Emms cosine similarity
SCSS-TCD:conf3
submission-ws Gavagai Hillevi Ha?gglo?f, random indexing
submission-pmi Lisa Tengstrand association measures (pmi)
UCPH-simple.en University of Copenhagen Anders Johannsen, Hector Martinez, support vector regression
Christian Rish?j, Anders S?gaard with COALS-based
endocentricity features
UoY: Exm University of York, UK; Siva Reddy, Diana McCarthy, exemplar-based WSMs
UoY: Exm-Best Lexical Computing Ltd., UK Suresh Manandhar,
UoY: Pro-Best Spandana Gella prototype-based WSM
UNED-1: NN NLP and IR Group at UNED Guillermo Garrido, syntactic VSM,
UNED-2: NN Anselmo Peas dependency-parsed UKWaC,
UNED-3: NN SVM classifier
Table 3: Participants of DiSCo?2011 Shared Task
in the numerical evaluation are all based on different
variations of word space models.
The outcome of evaluation for coarse scores is
displayed in Table 5. Here, Duluth-1 performs high-
est with 0.585, followed closely by UoY:ExmBest
with 0.576 and UoY: ProBest with 0.567. Duluth-
1 is an approach purely based on association mea-
sures.
Both tables also report ZERO-response and
RANDOM-response baselines. ZERO-response
means that, if no score is reported for a phrase, it
gets a default value of 50 (fifty) points in numerical
evaluation and ?medium? in coarse evaluation. Ran-
dom baselines were created by using random labels
from a uniform distribution. Most systems beat the
RANDOM-response baseline, only about half of the
systems are better than ZERO-response.
Apart from the officially announced scoring meth-
ods, we provide Spearman?s rho and Kendall?s tau
rank correlations for numerical scoring. Rank cor-
relation scores that are not significant are noted in
parentheses. With correlations, the higher the score,
the better is the system?s ability to order the phrases
according to their compositionality scores. Here,
systems UoY: Exm-Best, UoY: Pro-Best / JUCSE-
1 and JUCSE-2 achieved the first, second and third
best results respectively.
Overall, there is no clear winner for the English
dataset. However, across different scoring mecha-
nisms, UoY: Exm-Best is the most robust of the sys-
tems. The UCPH-simple.en system has a stellar per-
formance on V OBJ but apparently uses a subopti-
mal way of assigning coarse labels. The Duluth-1
system, on the other hand, is not able to produce a
numerical ranking that is significant according to the
correlation measures, but excels in the coarse scor-
ing.
When comparing word space models and asso-
ciation measures, it seems that the former do a
slightly better job on modeling graded composition-
ality, which is especially obvious in the numerical
evaluation.
Since word space models and statistical associa-
tion measures are language-independent approaches
and most teams have not used syntactic preprocess-
ing other than POS tagging, it is a pity that only one
team has tried the German task (see Tables 6 and
7). The comparison to the baselines shows that the
UCPH system is robust across languages and per-
forms (relatively speaking) equally well in the nu-
merical scoring both for the German and the English
tasks.
26
numerical scores responses ? ? EN all EN ADJ NN EN V SUBJ EN V OBJ
number of phrases 174 77 35 62
0-response baseline 0 - - 23.42 24.67 17.03 25.47
random baseline 174 (0.02) (0.02) 32.82 34.57 29.83 32.34
UCPH-simple.en 174 0.27 0.18 16.19 14.93 21.64 14.66
UoY: Exm-Best 169 0.35 0.24 16.51 15.19 15.72 18.6
UoY: Pro-Best 169 0.33 0.23 16.79 14.62 18.89 18.31
UoY: Exm 169 0.26 0.18 17.28 15.82 18.18 18.6
SCSS-TCD: conf1 174 0.27 0.19 17.95 18.56 20.8 15.58
SCSS-TCD: conf2 174 0.28 0.19 18.35 19.62 20.2 15.73
Duluth-1 174 (-0.01) (-0.01) 21.22 19.35 26.71 20.45
JUCSE-1 174 0.33 0.23 22.67 25.32 17.71 22.16
JUCSE-2 174 0.32 0.22 22.94 25.69 17.51 22.6
SCSS-TCD: conf3 174 0.18 0.12 25.59 24.16 32.04 23.73
JUCSE-3 174 (-0.04) (-0.03) 25.75 30.03 26.91 19.77
Duluth-2 174 (-0.06) (-0.04) 27.93 37.45 17.74 21.85
Duluth-3 174 (-0.08) (-0.05) 33.04 44.04 17.6 28.09
submission-ws 173 0.24 0.16 44.27 37.24 50.06 49.72
submission-pmi 96 - - - - 52.13 50.46
UNED-1: NN 77 - - - 17.02 - -
UNED-2: NN 77 - - - 17.18 - -
UNED-3: NN 77 - - - 17.29 - -
Table 4: Numerical evaluation scores for English: average point difference and correlation measures (not significant
values in parentheses)
coarse values responses EN all EN ADJ NN EN V SUBJ EN V OBJ
number of phrases 118 52 26 40
zero-response baseline 0 0.356 0.288 0.654 0.250
random baseline 118 0.297 0.288 0.308 0.300
Duluth-1 118 0.585 0.654 0.385 0.625
UoY: Exm-Best 114 0.576 0.692 0.500 0.475
UoY: Pro-Best 114 0.567 0.731 0.346 0.500
UoY: Exm 114 0.542 0.692 0.346 0.475
SCSS-TCD: conf2 118 0.542 0.635 0.192 0.650
SCSS-TCD: conf1 118 0.534 0.64 0.192 0.625
JUCSE-3 118 0.475 0.442 0.346 0.600
JUCSE-2 118 0.458 0.481 0.462 0.425
SCSS-TCD: conf3 118 0.449 0.404 0.423 0.525
JUCSE-1 118 0.441 0.442 0.462 0.425
submission-ws 117 0.373 0.346 0.269 0.475
UCPH-simple.en 118 0.356 0.346 0.500 0.275
Duluth-2 118 0.322 0.173 0.346 0.500
Duluth-3 118 0.322 0.135 0.577 0.400
submission-pmi - - - 0.346 0.550
UNED-1-NN 52 - 0.289 - -
UNED-2-NN 52 - 0.404 - -
UNED-3-NN 52 - 0.327 - -
Table 5: Coarse evaluation scores for English
27
numerical scores responses ? ? DE all DE ADJ NN DE V SUBJ DE V OBJ
number of phrases 149 63 29 57
0-response baseline 0 - - 32.51 32.21 38.00 30.05
random baseline 149 (0.005) (0.004) 37.79 36.27 47.45 34.54
UCPH-simple.de 148 0.171 0.116 24.03 27.09 15.55 24.06
Table 6: Numerical evaluation scores for German
heightcoarse values responses DE all DE ADJ NN DE V SUBJ DE V OBJ
number of phrases 120 48 28 44
0-response baseline 0 0.158 0.208 0.071 0.159
random baseline 120 0.283 0.313 0.214 0.295
UCPH-simple.de 119 0.283 0.375 0.286 0.182
Table 7: Coarse evaluation scores for German
For more details on the systems as well as fine-
grained analysis of the results, please consult the
corresponding system description papers.
5 Conclusion
DiSCo Shared Task attracted seven groups that sub-
mitted results for 19 systems. We consider this
a success, taking into consideration that the task
is new and difficult. The opportunity to evaluate
language-independent models for languages other
than English was unfortunately not taken up by most
participants.
The teams applied a variety of approaches that
can be classified into lexical association measures
and word space models of various flavors. From
the evaluation, it is hard to decide what method is
currently more suited for the task of automatic ac-
quisition of compositionality, with a slight favor for
approaches based on word space model.
A takeaway message is that a pure corpus-based
acquisition of graded compositionality is a hard task.
While some approaches clearly outperform base-
lines, further advances are needed for automatic sys-
tems to be able to reproduce semantic composition-
ality.
Acknowledgments
We thank Emiliano Guevara for helping with the
preparation of the evaluation scripts and the ini-
tial task description. This work was partially sup-
ported by the German Federal Ministry of Eco-
nomics (BMWi) under the project Theseus (number
01MQ07019).
References
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, pages 65?72, Sapporo, Japan.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion.
Chris Biemann and Valerie Nygaard. 2010. Crowdsourc-
ing WordNet. In Proc. of the 5th International Confer-
ence of the Global WordNet Association (GWC-2010),
Mumbai, India.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, pages 12?19, Sydney, Australia.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proc. of
the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002), pages 1?15, Mexico City, Mexico.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
28
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 19?27,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Sweeping through the Topic Space:
Bad luck? Roll again!
Martin Riedl and Chris Biemann
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
riedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
Abstract
Topic Models (TM) such as Latent Dirich-
let Allocation (LDA) are increasingly used
in Natural Language Processing applica-
tions. At this, the model parameters and
the influence of randomized sampling and
inference are rarely examined ? usually,
the recommendations from the original pa-
pers are adopted. In this paper, we ex-
amine the parameter space of LDA topic
models with respect to the application of
Text Segmentation (TS), specifically target-
ing error rates and their variance across dif-
ferent runs. We find that the recommended
settings result in error rates far from opti-
mal for our application. We show substan-
tial variance in the results for different runs
of model estimation and inference, and give
recommendations for increasing the robust-
ness and stability of topic models. Run-
ning the inference step several times and se-
lecting the last topic ID assigned per token,
shows considerable improvements. Similar
improvements are achieved with the mode
method: We store all assigned topic IDs
during each inference iteration step and se-
lect the most frequent topic ID assigned to
each word. These recommendations do not
only apply to TS, but are generic enough to
transfer to other applications.
1 Introduction
With the rise of topic models such as pLSI (Hof-
mann, 2001) or LDA (Blei et al, 2003) in Nat-
ural Language Processing (NLP), an increasing
number of works in the field use topic models to
map terms from a high-dimensional word space
to a lower-dimensional semantic space. TMs
are ?the new Latent Semantic Analysis? (LSA),
(Deerwester et al, 1990), and it has been shown
that generative models like pLSI and LDA not
only have a better mathematical foundation rooted
in probability theory, but also outperform LSA in
document retrieval and classification, e.g. (Hof-
mann, 2001; Blei et al, 2003; Biro et al, 2008).
To estimate the model parameters in LDA, the ex-
act computation that was straightforward in LSA
(matrix factorization) is replaced by a randomized
Monte-Carlo sampling procedure (e.g. variational
Bayes or Gibbs sampling).
Aside from the main parameter, the number
of topics or dimensions, surprisingly little atten-
tion has been spent to understand the interac-
tions of hyperparameters, the number of sam-
pling iterations in model estimation and inter-
ference, and the stability of topic assignments
across runs using different random seeds. While
progress in the field of topic modeling is mainly
made by adjusting prior distributions (e.g. (Sato
and Nakagawa, 2010; Wallach et al, 2009)), or
defining more complex model mixtures (Heinrich,
2011), it seems unclear whether improvements,
reached on intrinsic measures like perplexity or
on application-based evaluations, are due to an
improved model structure or could originate from
sub-optimal parameter settings or literally ?bad
luck? due to the randomized nature of the sam-
pling process.
In this paper, we address these issues by sys-
tematically sweeping the parameter space. For
this, we pick LDA since it is the most commonly
used TM in the field of NLP. To evaluate the con-
tribution of the TM, we choose the task of TS:
this task has received considerable interest from
the NLP community, standard datasets and eval-
uation measures are available for testing, and it
19
has been shown that this task considerably bene-
fits from the use of TMs, see (Misra et al, 2009;
Sun et al, 2008; Eisenstein, 2009).
This paper is organized as follows: In the next
section, we present related work regarding text
segmentation using topic models and topic model
parameter evaluations. Section 3 defines the Top-
icTiling text segmentation algorithm, which is a
simplified version of TextTiling (Hearst, 1994),
and makes direct use of topic assignments. Its
simplicity allows us to observe direct conse-
quences of LDA parameter settings. Further, we
describe the experimental setup, our application-
based evaluation methodology including the data
set and the LDA parameters we vary in Section 4.
Results of our experiments in Section 5 indi-
cate that a) there is an optimal range for the num-
ber of topics, b) there is considerable variance in
performance for different runs for both model es-
timation and inference, c) increasing the number
of sampling iterations stabilizes average perfor-
mance but does not make TMs more robust, but d)
combining the output of several independent sam-
pling runs does, and additionally leads to large er-
ror rate reductions. Similar results are obtained by
e) the mode method with less computational costs
using the most frequent topic ID that is assigned
during different inference iteration steps. In the
conclusion, we give recommendations to add sta-
bility and robustness for TMs: aside from opti-
mization of the hyperparameters, we recommend
combining the topic assignments of different in-
ference iterations, and/or of different independent
inference runs.
2 Related Work
2.1 Text Segmentation with Topic Models
Based on the observation of Halliday and Hasan
(1976) that the density of coherence relations is
higher within segments than between segments,
most algorithms compute a coherence score to
measure the difference of textual units for inform-
ing a segmentation decision. TextTiling (Hearst,
1994) relies on the simplest coherence relation ?
word repetition ? and computes similarities be-
tween textual units based on the similarities of
word space vectors. The task of text segmenta-
tion is to decide, for a given text, how to split this
text into segments.
Related to our algorithm (see Section 3.1) are
the approaches described in Misra et al (2009)
and Sun et al (2008): topic modeling is used to
alleviate the sparsity of word vectors by mapping
words into a topic space. This is done by extend-
ing the dynamic programming algorithms from
(Utiyama and Isahara, 2000; Fragkou et al, 2004)
using topic models. At this, the topic assignments
have to be inferred for each possible segment.
2.2 LDA and Topic Model Evaluation
For topic modeling, we use the widely applied
LDA (Blei et al, 2003), This model uses a train-
ing corpus of documents to create document-topic
and topic-word distributions and is parameterized
by the number of topics T as well as by two
hyperparameters. To generate a document, the
topic proportions are drawn using a Dirichlet dis-
tribution with hyperparameter ?. Adjacent for
each word w a topic zdw is chosen according to
a multinomial distribution using hyperparameter
?zdw . The model is estimated using m itera-
tions of Gibbs sampling. Unseen documents can
be annotated with an existing topic model using
Bayesian inference methods. At this, Gibbs sam-
pling with i iterations is used to estimate the topic
ID for each word, given the topics of the other
words in the same sentential unit. After inference,
every word in every sentence receives a topic ID,
which is the sole information that is used by the
TopicTiling algorithm to determine the segmenta-
tion. We use the GibbsLDA implementation by
Phan and Nguyen (2007) for all our experiments.
The article of Blei et al (2003) compares LDA
with pLSI and Mixture Unigram models using the
perplexity of the model. In a collaborative filter-
ing evaluation for different numbers of topics they
observe that using too many topics leads to over-
fitting and to worse results.
In the field of topic model evaluations, Griffiths
and Steyvers (2004) use a corpus of abstracts pub-
lished between 1991 and 2001 and evaluate model
perplexity. For this particular corpus, they achieve
the lowest perplexity using 300 topics. Further-
more, they compare different sampling methods
and show that the perplexity converges faster with
Gibbs sampling than with expectation propaga-
tion and variational Bayes. On a small artificial
testset, small variations in perplexity across dif-
ferent runs were observed in early sampling itera-
tions, but all runs converged to the same limit.
20
In Wallach et al (2009) topic models are eval-
uated with symmetric and asymmetric hyperpa-
rameters based on the perplexity. They observe
a benefit using asymmetric parameters for ?, but
cannot show improvement with asymmetric priors
for ?.
3 Method
3.1 TopicTiling
For the evaluation of the topic models, a text seg-
mentation algorithm called TopicTiling is used
here. This algorithm is a newly developed al-
gorithm based on TextTiling (Hearst, 1994) and
achieves state of the art results using the Choi
dataset, which is a standard dataset for TS eval-
uation. The algorithm uses sentences as minimal
units. Instead of words, we use topic IDs that
are assigned to each word using the LDA infer-
ence running on sentence units. The LDA model
should be estimated on a corpus of documents that
is similar to the to-be-segmented documents.
To measure the coherence cp between two sen-
tences around position p, the cosine similarity
(vector dot product) between these two adjacent
sentences is computed. Each sentence is repre-
sented as a T -dimensional vector, where T is the
number of topic IDs defined in the topic model.
The t-th element of the vector contains the num-
ber of times the t-th topic is observed in the sen-
tence. Similar to the TextTiling algorithm, lo-
cal minima calculated from these similarity scores
are taken as segmentation candidates.
This is illustrated in Figure 1, where the simi-
larity scores between adjacent sentences are plot-
ted. The vertical lines in this plot indicate all local
minima found.
0 5 10 15 20 25 300.0
0.2
0.4
Sentence
cosine
 simila
rity
Figure 1: Cosine similarity scores of adjacent sen-
tences based on topic distribution vectors. Vertical
lines (solid and dashed) indicate local minima. Solid
lines mark segments that have a depth score above a
chosen threshold.
Following the TextTiling definition, not the
minimum score cp at position p itself is used, but
a depth score dp for position p computed by
di = 1/2 ? (cp?1 ? cp + cp+1 ? cp). (1)
In contrast to TextTiling, the directly neighboring
similarity scores of the local minima are used, if
they are higher than cp. When using topics instead
of words, it can be expected that sentences within
one segment have many topics in common, which
leads to cosine similarities close to 1. Further, us-
ing topic IDs instead of words greatly increases
sparsity. A minimum in the curve indicates a
change in topic distribution. Segment boundaries
are set at the positions of the n highest depth-
scores, which is common practice in text segmen-
tation algorithms. An alternative to a given n
would be the selection of segments according to
a depth score threshold.
4 Experimental Setup
As dataset the Choi dataset (Choi, 2000) is used.
This dataset is an artificially generated corpus that
consists of 700 documents. Each document con-
sists of 10 segments and each segment has 3?
11 sentences extracted from a document of the
Brown corpus. For the first setup, we perform a
10-fold Cross Validation (CV) for estimating the
TM (estimating on 630 documents at a time), for
the other setups we use 600 documents for TM
estimation and the remaining 100 documents for
testing. While we aim to neglect using the same
documents for training and testing, it is not guar-
anteed that all testing data is unseen, since the
same source sentences can find their way in sev-
eral artificially crafted ?documents?. This prob-
lem, however, applies for all evaluations on this
dataset that use any kind of training, be it LDA
models in Misra et al (2009) or TF-IDF values in
Fragkou et al (2004).
For the evaluation of the Topic Model in combi-
nation of Text Segmentation, we use the Pk mea-
sure (Beeferman et al, 1999), which is a stan-
dard measure for error rates in the field of TS.
This measure compares the gold standard seg-
mentation with the output of the algorithm. A
Pk value of 0 indicates a perfect segmentation,
the averaged state of the art on the Choi Dataset
is Pk = 0.0275 (Misra et al, 2009). To assess
the robustness of the TM, we sweep over varying
21
configurations of the LDA model, and plot the re-
sults using Box-and-Whiskers plots: the box in-
dicates the quartiles and the whiskers are maxi-
mal 1.5 times of the Interquartile Range (IQR) or
equal to the data point that is no greater to the 1.5
IQR. The following parameters are subject to our
exploration:
? T : Number of topics used in the LDA model.
Common values vary between 50 and 500.
? ? : Hyperparameter that regulates the sparse-
ness topic-per-document distribution. Lower
values result in documents being represented
by fewer topics (Heinrich, 2004). Recom-
mended: ? = 50/T (Griffiths and Steyvers,
2004)
? ? : Reducing ? increases the sparsity of
topics, by assigning fewer terms to each
topic, which is correlated to how related
words need to be, to be assigned to a topic
(Heinrich, 2004). Recommended: ? =
{0.1, 0.01} (Griffiths and Steyvers, 2004;
Misra et al, 2009)
? m Model estimation iterations. Recom-
mended / common settings: m = 500?5000
(Griffiths and Steyvers, 2004; Wallach et al,
2009; Phan and Nguyen, 2007)
? i Inference iterations. Recommended / com-
mon settings: 100 (Phan and Nguyen, 2007)
? d Mode of topic assignments. At each in-
ference iteration step, a topic ID is assigned
to each word within a document (represented
as a sentence in our application). With this
option, we count these topic assignments for
each single word in each iteration. After all i
inference iterations, the most frequent topic
ID is chosen for each word in a document.
? r Number of inference runs: We repeat the
inference r times and assign the most fre-
quently assigned topic per word at the fi-
nal inference run for the segmentation algo-
rithm. High r values might reduce fluctua-
tions due to the randomized process and lead
to a more stable word-to-topic assignment.
All introduced parameters parameterize the TM.
We are not aware of any research that has used
several inference runs r and the mode of topic as-
signments d to increase stability and varying TM
parameters in combinations with measures other
then perplexity.
5 Results
In this section, we present the results we obtained
from varying the parameters under examination.
5.1 Number of Topics T
To provide a first impression of the data, a 10-fold
CV is calculated and the segmentation results are
visualized in Figure 2.
Topic Number
P_k 
valu
e
0.0
0.1
0.2
0.3
0.4
0.5
3 10 20 50 100 250 500
l
l
l l
l
l
l
l l l l l l
l l l
ll
lll l
l
l
ll
l
ll
l
l
l
l
l
l
ll
l
ll l
l l
l
l
lll l
l
l
ll
l
l
ll
l
ll
l
l
l
l
ll
l
l
lll
l
l l
l
l
l
l
l
l
l
l
l
llll
l ll l
l
l
Figure 2: Box plots for different number of topics T .
Each box plot is generated from the average Pk value
of 700 documents, ? = 50/T , ? = 0.1, m = 1000,
i = 100, r = 1. These documents are segmented with
TopicTiling using a 10-folded CV.
Each box plot is generated from the Pk values
of 700 documents. As expected, there is a contin-
uous range of topic numbers, namely between 50
and 150 topics, where we observe the lowest Pk
values. Using too many topics leads to overfitting
of the data and too few topics result in too gen-
eral distinctions to grasp text segments. This is in
line with other studies, that determine an optimum
for T , cf. (Griffiths and Steyvers, 2004), which is
specific to the application and the data set.
5.2 Estimation and Inference iterations
The next step examines the robustness of the topic
model according to the number of model estima-
tion iterations m needed to achieve stable results.
600 documents are used to train the LDA model
22
that is applied by TopicTiling to segment the re-
maining 100 documents. From Figure 2 we know
that sampling 100 topics leads to good results.
To have an insight into unstable topic regions we
also inspect performance at different sampling it-
erations using 20 and 250 topics. To assess sta-
bility across different model estimation runs, we
trained 30 LDA models using different random
seeds. Each box plot in Figures 3 and 4 is gen-
erated from 30 mean values, calculated from the
Pk values of the 100 documents. The variation
indicates the score variance for the 30 different
models.
Number of topics: 100
number of sample iterations
P_k 
valu
e
0.0
0.1
0.2
0.3
0.4
2 3 5 10 20 50 100 300 500 1000
l l l l l l
l
l
l
l
l
l l l l l l l l l l l l l l
l
l
l
l
l
l l
0.02
0.04
0.06
0.08
0.10
50 100 300 500 1000
l
l
l
l l l l l l l l l l l l l
l
l
l l
Figure 3: Box plots with different model estimation
iterations m, with T=100, ? = 50/T , ? = 0.1, i =
100, r = 1. Each box plot is generated from 30 mean
values calculated from 100 documents.
Using 100 topics (see Figure 3), the burn-in
phase starts with 8?10 iterations and the mean Pk
values stabilize after 40 iterations. But looking
at the inset for large m values, significant vari-
ations between the different models can be ob-
served: note that the Pk error rates are almost
double between the lower and the upper whisker.
These remain constant and do not disappear for
largerm values: The whiskers span error rates be-
tween 0.021 - 0.037 for model estimation on doc-
ument units
With 20 topics, the Pk values are worse as with
100 topics, as expected from Figure 2. Here the
convergence starts at 100 sample iterations. More
interesting results are achieved with 250 topics.
A robust range for the error rates can be found be-
tween 20 and 100 sample iterations. With more
iterations m, the results get both worse and un-
stable: as the ?natural? topics of the collection
have to be split in too many topics in the model,
perplexity optimizations that drive the estimation
process lead to random fluctuations, which the
TopicTiling algorithm is sensitive to. Manual in-
spection of models for T = 250 revealed that in
fact many topics do not stay stable across estima-
tion iterations.
number of inference iterations
P_k 
valu
e
0.01
0.02
0.03
0.04
2 3 5 10 20 50 100
l
l l l l l l l l l l l l l l l
l
l
l l
Figure 5: Figure of box plots for different inference
iterations i and m = 1000, T = 100, ? = 50/T ,
? = 0.1, r = 1 .
In the next step we sweep over several infer-
ence iterations i. Starting from 5 iterations, error
rates do not change much, see Figure 5. But there
is still substantial variance, between about 0.019 -
0.038 for inference on sentence units.
5.3 Number of inference runs r
To decrease this variance, we assign the topic not
only from a singe inference run, but repeat the in-
ference calculations several times, denoted by the
parameter r. Then the frequency of assigned topic
IDs per token is counted across the r runs, and we
assign the most frequent topic ID (frequency ties
are broken randomly). The box plot for several
evaluated values of r is shown in Figure 6.
This log-scaled plot shows that both variance
and Pk error rate can be substantially decreased.
Already for r = 3, we observe a significant im-
provement in comparison to the default setting of
r = 1 and with increasing r values, the error rates
are reduced even more: for r = 20, variance and
error rates are is cut in less than half of their orig-
inal values using this simple operation.
23
Number of topics: 20
number of sample iterations
P_k 
valu
e
0.1
0.2
0.3
0.4
2 3 5 10 20 50 100 300 500 1000
l l l l l l l
l
l
l
l
l
l
l l l l l l l l l l l l
l
l l l
l
l
l
l
l l l l
l
l
0.02
0.04
0.06
0.08
0.10
50 100 300 500 1000
l
l
l l l l l l l l l l l l
l
l
l
l
l l l l
l
l
Number of topics: 250
number of sample iterations
P_k 
valu
e
0.1
0.2
0.3
0.4
2 3 5 10 20 50 100 300 500 1000
l l l l l l
l
l
l
l
l
l l l l l l l
l l l l l l l
l
l
l
l
l
l
l
l
l
l
0.02
0.04
0.06
0.08
0.10
50 100 300 500 1000
l
l
l l l l l l l
l l l l l l l
l
ll
l
l
Figure 4: Box plots with varying model estimation iterations m applied with T = 20 (left) and T = 250 (right)
topics, ? = 50/T , ? = 0.1, i = 100, r = 1
number of repeated inferences
P_k
 valu
e
0.01
0.02
0.03
0.04
1 3 5 10 20
l
l
l
l l
l
Figure 6: Box plot for several inference runs r, to as-
sign the topics to a word with m = 1000, i = 100,
T = 100, ? = 50/T , ? = 0.1.
5.4 Mode of topic assignment d
In the previous experiment, we use the topic IDs
that have been assigned most frequently at the last
inference iteration step. Now, we examine some-
thing similar, but for all i inference steps of a sin-
gle inference run: we select the mode of topic
ID assignments for each word across all inference
steps. The impact of this method on error and
variance is illustrated in Figure 7. Using a sin-
gle inference iteration, the topic IDs are almost
assigned randomly. After 20 inference iterations
Pk values below 0.02 are achieved. Using further
iterations, the decrease of the error rate is only
number of inference iterations
P_k 
valu
e
0.01
0.02
0.03
0.04
2 3 5 10 20 50 100
l
l
l l l ll
l
l
Figure 7: Box plot using the mode method d = true
with several inference iterations i with m = 500, T =
100, ? = 50/T , ? = 0.1.
marginal. In comparison to the repeated inference
method, the additional computational costs of this
method are much lower as the inference iterations
have to be carried out anyway in the default appli-
cation setting.
5.5 Hyperparameters ? and ?
In many previous works, hyperparameter settings
? = 50/T and ? = {0.1, 0.01} are commonly
used. In the next series of experiments we inves-
tigate how different parameters of these both pa-
rameters can change the TS task.
For ? values, shown in Figure 8, we can see
that the recommended value for T = 100 , ? =
24
0.5 leads to sub-optimal results, and an error rate
reduction of about 40% can be realized by setting
? = 0.1.
alpha values
P_k 
valu
e
0.01
0.02
0.03
0.04
0.01 0.02 0.03 0.05 0.1 0.2 0.5 1
l l l l
l l
l
l
l l
l
l
l
l
l
l
l
l
Figure 8: Box plot for several alpha values ?withm =
500, i = 100, T = 100, ? = 0.1, r = 1.
Regarding values of ?, we find that Pk rates
and their variance are relatively stable between
the recommended settings of 0.1 and 0.01. Values
larger than 0.1 lead to much worse performance.
Regarding variance, no patterns within the stable
range emerge, see Figure 9.
beta values
P_k 
valu
e
0.05
0.10
0.15
0.01 0.02 0.03 0.05 0.1 0.2 0.5
l l l l l
l l l l
l
l
l
l
l
l
l
l
l
l
Figure 9: Box plot for several beta values ? with m =
500, i = 100, T = 100, ? = 50/T , r = 1.
5.6 Putting it all together
Until this point, we have examined different pa-
rameters with respect to stability and error rates
one at the time. Now, we combine what we have
System Pk error ?2 var.
red. red.
default 0.0302 0.00% 2.02e-5 0.00%
? = 0.1 0.0183 39.53% 1.22e-5 39.77%
r = 20 0.0127 57.86% 4.65e-6 76.97%
d = true 0.0137 54.62% 3.99e-6 80.21%
combined 0.0141 53.45% 9.17e-6 54.55%
Table 1: Comparison of single parameter optimiza-
tions, and combined system. Pk averages and variance
are computed over 30 runs, together with reductions
relative to the default setting. Default: ? = 0.5, r = 1.
combined: ? = 0.1, r = 20, d = true
learned from this and strive at optimal system per-
formance. For this, we contrast TS results ob-
tained with the default LDA configuration with
the best systems obtained by optimization of sin-
gle parameters, as well as to a system that uses
these optimal settings for all parameters. Table 1
shows Pk error rates for the different systems. At
this, we fixed the following parameters: T = 100,
m = 500, i = 100, ? = 0.1. For the computa-
tions we use 600 documents for the LDA model
estimation, apply TopicTiling and compute the er-
ror rate for the 100 remaining documents and re-
peat this 30 times with different random seeds.
We can observe a massive improvement for op-
timized single parameters. The ?-tuning tuning
results in an error rate reduction of 39.77% in
comparison to the default configurations. Using
r = 20, the error rate is cut in less than half
its original value. Also for the mode mechanism
(d = true) the error rate is halved but slightly
worse than than when using the repeated infer-
ence. Using combined optimized parameters does
not result to additional error decreases. We at-
tribute the slight decline of the combined method
in both in the error rate Pk and in the variance to
complex parameter interactions that shall be ex-
amined in further work. In Figure 10, we visual-
ize these results in a density plot. It becomes clear
that repeated inference leads to slightly better and
more robust performance (higher peak) than the
mode method. We attribute the difference to sit-
uations, where there are several highly probable
topics in our sampling units, and by chance the
same one is picked for adjacent sentences that be-
long to different segments, resulting in failure to
recognize the segmentation point. However, since
the differences are miniscule, only using the mode
method might be more suitable for practical pur-
poses since its computational cost is lower.
25
0.00 0.01 0.02 0.03 0.04 0.05
0
50
100
150
P_k values
Den
sity
default valuesalpha=0.01r=20d=truecombined
Figure 10: Density plot of the error distributions for
the systems listed in Table 1
6 Conclusion
In this paper, we examined the robustness of LDA
topic models with respect to the application of
Text Segmentation by sweeping through the topic
model parameter space. To our knowledge, this is
the first attempt to systematically assess the sta-
bility of topic models in a NLP task.
The results of our experiments are summarized
as follows:
? Perform the inference r times using the same
model and choosing the assigned topic ID
per word token taken from the last infer-
ence iteration, improves both error rates and
stability across runs with different random
seeds.
? Almost equal performance in terms of er-
ror and stability is achieved with the mode
mechanism: choose the most frequent topic
ID assignment per word across inference
steps. While error rates were slightly higher
for our data set, this method is probably
preferable in practice because of its lower
computation costs.
? As found in other studies, there is a range for
the number of topics T , where optimal re-
sults are obtained. In our task, performance
showed to be robust in the range of 50 - 150
topics.
? The default setting for LDA hyperparameters
? and ? can lead to sub-optimal results. Es-
pecially ? should be optimized for the task at
hand, as the utility of the topic model is very
sensitive to this parameter.
? While the number of iterations for model es-
timation and inference needed for conver-
gence is depending on the number of topics,
the size of the sampling unit (document) and
the collection, it should be noted that after
convergence the variance between different
sampling runs does not decrease for a larger
number of iterations.
Equipped with the insights gained from exper-
iments on single parameter variation, we were
able to implement a very simple algorithm for text
segmentation that improves over the state of the
art on a standard dataset by a large margin. At
this, the combination of the optimal ?, and a high
number of inference repetitions r and the mode
method (d = true) produced slightly more errors
than a high r alone. While the purpose of this pa-
per was mainly to address robustness and stability
issues of topic models, we are planning to apply
the segmentation algorithm to further datasets.
The most important takeaway, however, is that
especially for small sampling units like sentences,
tremendous improvements in applications can be
obtained when looking at multiple inference as-
signments and using the most frequently assigned
topic ID in subsequent processing ? either across
diffeent inference steps or across diffeent infer-
ence runs. These two new strategies seem to be
able to offset sub-optimal hyperparameters to a
certain extent. This scheme is not only applica-
ble to Text Segmentation, but in all applications
where performance crucially depends on stable
topic ID assignments per token. Extensions to
this scheme, like ignoring tokens with a high topic
variability (stop words or general terms) or dy-
namically deciding to conflate several topics be-
cause of their per-token co-occurrence, are left for
future work.
7 Acknowledgments
This work has been supported by the Hessian
research excellence program ?Landes-Offensive
zur Entwicklung Wissenschaftlich-konomischer
Exzellenz? (LOEWE) as part of the research cen-
ter ?Digital Humanities?. We would also thank
the anonymous reviewers for their comments,
which greatly helped to improve the paper.
26
References
D. Beeferman, A. Berger, and J. Lafferty. 1999.
Statistical models for text segmentation. Machine
learning, 34(1):177?210.
Istvan Biro, Andras Benczur, Jacint Szabo, and Ana
Maguitman. 2008. A comparative analysis of la-
tent variable models for web page classification. In
Proceedings of the 2008 Latin American Web Con-
ference, pages 23?28, Washington, DC, USA. IEEE
Computer Society.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 26?33, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Sci-
ence, 41(6):391?407.
Jacob Eisenstein. 2009. Hierarchical text segmenta-
tion from multi-scale lexical cohesion. Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics on -
NAACL ?09, page 353.
P. Fragkou, V. Petridis, and Ath. Kehagias. 2004. A
Dynamic Programming Algorithm for Linear Text
Segmentation. Journal of Intelligent Information
Systems, 23(2):179?197, September.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. PNAS, 101(suppl. 1):5228?
5235.
M A K Halliday and Ruqaiya Hasan. 1976. Cohesion
in English, volume 1 of English Language Series.
Longman.
Marti a. Hearst. 1994. Multi-paragraph segmentation
of expository text. Proceedings of the 32nd annual
meeting on Association for Computational Linguis-
tics, (Hearst):9?16.
Gregor Heinrich. 2004. Parameter estimation for text
analysis. Technical report.
Gregor Heinrich. 2011. Typology of mixed-
membership models: Towards a design method.
In Machine Learning and Knowledge Discovery in
Databases, volume 6912 of Lecture Notes in Com-
puter Science, pages 32?47. Springer Berlin / Hei-
delberg. 10.1007/978-3-642-23783-6 3.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Computer,
pages 177?196.
Hemant Misra, Joemon M Jose, and Olivier Cappe?.
2009. Text Segmentation via Topic Modeling : An
Analytical Study. In Proceeding of the 18th ACM
Conference on Information and Knowledge Man-
agement - CIKM ?09, pages 1553?-1556.
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007.
GibbsLDA++: A C/C++ implementa-
tion of latent Dirichlet alocation (LDA).
http://jgibblda.sourceforge.net/.
Issei Sato and Hiroshi Nakagawa. 2010. Topic mod-
els with power-law using pitman-yor process cate-
gories and subject descriptors. Science And Tech-
nology, (1):673?681.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher
kernel. Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies Short Papers - HLT
?08, (June):269.
Masao Utiyama and Hitoshi Isahara. 2000. A Statis-
tical Model for Domain-Independent Text Segmen-
tation. Communications.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
NIPS.
27
Proceedings of the 2012 Student Research Workshop, pages 37?42,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
TopicTiling: A Text Segmentation Algorithm based on LDA
Martin Riedl and Chris Biemann
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biemann}@cs.tu-darmstadt.de
Abstract
This work presents a Text Segmentation al-
gorithm called TopicTiling. This algorithm
is based on the well-known TextTiling algo-
rithm, and segments documents using the La-
tent Dirichlet Allocation (LDA) topic model.
We show that using the mode topic ID as-
signed during the inference method of LDA,
used to annotate unseen documents, improves
performance by stabilizing the obtained top-
ics. We show significant improvements over
state of the art segmentation algorithms on two
standard datasets. As an additional benefit,
TopicTiling performs the segmentation in lin-
ear time and thus is computationally less ex-
pensive than other LDA-based segmentation
methods.
1 Introduction
The task tackled in this paper is Text Segmentation
(TS), which is to be understood as the segmentation
of texts into topically similar units. This implies,
viewing the text as a sequence of subtopics, that a
subtopic change marks a new segment. The chal-
lenge for a text segmentation algorithm is to find the
sub-topical structure of a text.
In this work, this semantic information is gained
from Topic Models (TMs). We introduce a newly
developed TS algorithm called TopicTiling. The
core algorithm is a simplified version of TextTil-
ing (Hearst, 1994), where blocks of text are com-
pared via bag-of-word vectors. TopicTiling uses
topic IDs, obtained by the LDA inference method,
instead of words. As some of the topic IDs ob-
tained by the inference method tend to change for
different runs, we recommend to use the most prob-
able topic ID assigned during the inference. We de-
note this most probable topic ID as the mode (most
frequent across all inference steps) of the topic as-
signment. These IDs are used to calculate the co-
sine similarity between two adjacent blocks of sen-
tences, represented as two vectors, containing the
frequency of each topic ID. Without parameter opti-
mization we obtain state-of-the-art results based on
the Choi dataset (Choi, 2000). We show that the
mode assignment improves the results substantially
and improves even more when parameterizing the
size of sampled blocks using a window size param-
eter. Using these optimizations, we obtain signif-
icant improvements compared to other algorithms
based on the Choi dataset and also on a more diffi-
cult Wall Street Journal (WSJ) corpus provided by
Galley et al (2003). Not only does TopicTiling
deliver state-of-the-art segmentation results, it also
performs the segmentation in linear time, as opposed
to most other recent TS algorithms.
The paper is organized as follows: The next sec-
tion gives an overview of text segmentation algo-
rithms. Section 3 introduces the TopicTiling TS al-
gorithm. The Choi and the Galley datasets used
to measure the performance of TopicTiling are de-
scribed in Section 4. In the evaluation section, the
results of TopicTiling are demonstrated on these
datasets, followed by a conclusion and discussion.
2 Related Work
TS can be divided into two sub-fields: (i) linear
TS and (ii) hierarchical TS. Whereas linear TS
deals with the sequential analysis of topical changes,
37
hierarchical segmentation is concerned with find-
ing more fine grained subtopic structures in texts.
One of the first unsupervised linear TS algorithms
was introduced by Hearst (1994): TextTiling seg-
ments texts in linear time by calculating the sim-
ilarity between two blocks of words based on the
cosine similarity. The calculation is accomplished
by two vectors containing the number of occur-
ring terms of each block. LcSeg (Galley et al,
2003), a TextTiling-based algorithm, uses tf-idf term
weights and improved TS results compared to Text-
Tiling. Utiyama and Isahara (2001) introduced one
of the first probabilistic approaches using Dynamic
Programming (DP) called U00. Related to our work
are the DP approaches described in Misra et al
(2009) and Sun et al (2008): here, topic modeling is
used to alleviate the sparsity of word vectors. This
approach was extended by (Misra et al, 2009) and
(Sun et al, 2008) using topic information achieved
from the LDA topic model. The first hierarchical
algorithm was proposed by Yaari (1997), using the
cosine similarity and agglomerative clustering ap-
proaches. A hierarchical Bayesian algorithm based
on LDA is introduced with Eisenstein (2009). In our
work, however, we focus on linear TS.
LDA was introduced by Blei et al (2003) and is
a generative model that discovers topics based on a
training corpus. Model training estimates two dis-
tributions: A topic-word distribution and a topic-
document distribution. As LDA is a generative prob-
abilistic model, the creation process follows a gen-
erative story: First, for each document a topic distri-
bution is sampled. Then, for each document, words
are randomly chosen, following the previously sam-
pled topic distribution. Using the Gibbs inference
method, LDA is used to apply a trained model for
unseen documents. Here, words are annotated by
topic IDs by assigning a topic ID sampled by the
document-word and word-topic distribution. Note
that the inference procedure, in particular, marks the
difference between LDA and earlier dimensionality
reduction techniques such as Latent Semantic Anal-
ysis.
3 TopicTiling
This section introduces the TopicTiling algorithm,
first introduced in (Riedl and Biemann, 2012a).
In contrast to the quite similar TextTiling algo-
rithm, TopicTiling is not based on words, but on
the last topic IDs assigned by the Bayesian Infer-
ence method of LDA. This increases sparsity since
the word space is reduced to a topic space of much
lower dimension. Therefore, the documents that are
to be segmented have first to be annotated with topic
IDs. For useful topic distinctions, however, the topic
model must be trained on documents similar in con-
tent to the test documents. Preliminary experiments
have shown that repeating the Bayesian inference,
often leads to different topic distributions for a given
sentence in several runs. Memorizing each topic ID
assigned to a word in a document during each in-
ference step can alleviate this instability, which is
rooted in the probabilistic nature of LDA. After fin-
ishing the inference on the unseen documents, we
select the most frequent topic ID for each word and
assign it to the word. We call this method the mode
of a topic assignment, denoted with d = true in
the remainder (Riedl and Biemann, 2012b). Note
that this is different from using the overall topic dis-
tribution as determined by the inference step, since
this winner-takes-it-all approach reduces noise from
random fluctuations. As this parameter stabilizes
the topic IDs at low computational costs, we rec-
ommend using this option in all setups where subse-
quent steps rely on a single topic assignment.
TopicTiling assumes a sentence si as the small-
est basic unit. At each position p, located between
two adjacent sentences, a coherence score cp is cal-
culated. With w we introduce a so-called window
parameter that specifies the number of sentences to
the left and to the right of position p that define two
blocks: sp?w, . . . , sp and sp+1, . . . , sp+w+1. In con-
trast to the mode topic assignment parameter d, we
cannot state a recommended value for w, as this pa-
rameter is dependent on the number of sentences a
segment should contain. This is conditioned on the
corpus that is segmented.
To calculate the coherence score, we exclusively
use the topic IDs assigned to the words by infer-
ence: Assuming an LDA model with T topics, each
block is represented as a T -dimensional vector. The
t-th element of each vector contains the frequency
of the topic ID t obtained from the according block.
The coherence score is calculated by the vector dot
product, also referred to as cosine similarity. Val-
38
ues close to zero indicate marginal relatedness be-
tween two adjacent blocks, whereas values close to
one denote a substantial connectivity. Next, the co-
herence scores are plotted to trace the local minima.
These minima are utilized as possible segmentation
boundaries. But rather using the cp values itself, a
depth score dp is calculated for each minimum (cf.
TextTiling, (Hearst, 1994)). In comparison to Topic-
Tiling, TextTiling calculates the depth score for each
position and than searches for maxima. The depth
score measures the deepness of a minimum by look-
ing at the highest coherence scores on the left and on
the right and is calculated using following formula:
dp = 1/2(hl(p)? cp + hr(p)? cp).
The function hl(p) iterates to the left as long as
the score increases and returns the highest coherence
score value. The same is done, iterating in the other
direction with the hr(p) function. If the number of
segments n is given as input, the n highest depth
scores are used as segment boundaries. Otherwise, a
threshold is applied (cf. TextTiling). This threshold
predicts a segment if the depth score is larger than
? ? ?/2, with ? being the mean and ? being the
standard variation calculated on the depth scores.
The algorithm runtime is linear in the number of
possible segmentation points, i.e. the number of sen-
tences: for each segmentation point, the two adja-
cent blocks are sampled separately and combined
into the coherence score. This, and the parameters d
and w, are the main differences to the dynamic pro-
gramming approaches for TS described in (Utiyama
and Isahara, 2001; Misra et al, 2009).
4 Data Sets
The performance of the introduced algorithm is
demonstrated using two datasets: A dataset pro-
posed by Choi and another more challenging one as-
sembled by Galley.
4.1 Choi Dataset
The Choi dataset (Choi, 2000) is commonly used in
the field of TS (see e.g. (Misra et al, 2009; Sun et
al., 2008; Galley et al, 2003)). It is a corpus, gen-
erated artificially from the Brown corpus and con-
sists of 700 documents. For document generation,
ten segments of 3-11 sentences each, taken from dif-
ferent documents, are combined forming one doc-
ument. 400 documents consist of segments with a
sentence length of 3-11 sentences and there are 100
documents each with sentence lengths of 3-5, 6-8
and 9-11.
4.2 Galley Dataset
Galley et al (2003) present two corpora for writ-
ten language, each having 500 documents, which are
also generated artificially. In comparison to Choi?s
dataset, the segments in its ?documents? vary from 4
to 22 segments, and are composed by concatenat-
ing full source documents. One dataset is gener-
ated based on WSJ documents of the Penn Treebank
(PTB) project (Marcus et al, 1994) and the other is
based on Topic Detection Track (TDT) documents
(Wayne, 1998). As the WSJ dataset seems to be
harder (consistently higher error rates across several
works), we use this dataset for experimentation.
5 Evaluation
The performance of TopicTiling is evaluated using
two measures, commonly used in the TS task: The
Pk measure and the WindowDiff (WD) measure
(Beeferman et al, 1999; Pevzner and Hearst, 2002).
Besides the training corpus, the following parame-
ters need to be specified for LDA: The number of
topics T , the number of sample iterations for the
model m and two hyperparameters ? and ?, spec-
ifying the sparseness of the topic-document and the
topic-word distribution. For the inference method,
the number of sampling iterations i is required. In
line with Griffiths and Steyvers (2004), the follow-
ing standard parameters are used: T = 100, ? =
50/T , ? = 0.01, m = 500, i = 100. We use the
JGibbsLDA implementation described in Phan and
Nguyen (2007).
5.1 Evaluation of the Choi Dataset
For the evaluation we use a 10-fold Cross Validation
(CV): the full dataset of 700 documents is split into
630 documents for training the topic model and 70
documents that are segmented. These two steps are
repeated ten times to have all 700 documents seg-
mented. For this dataset, no part-of-speech based
word filtering is necessary. The results for different
parameter settings are listed in Table 1.
When using only the window parameter without
the mode (d=false), the results demonstrate a sig-
39
seg. size 3-5 6-8 9-11 3-11
Pk WD Pk WD Pk WD Pk WD
d=false,w=1 2.71 3.00 3.64 4.14 5.90 7.05 3.81 4.32
d=true,w=1 3.71 4.16 1.97 2.23 2.42 2.92 2.00 2.30
d=false,w=2 1.46 1.51 1.05 1.20 1.13 1.31 1.00 1.15
d=true,w=2 1.24 1.27 0.76 0.85 0.56 0.71 0.95 1.08
d=false,w=5 2.78 3.04 1.71 2.11 4.47 4.76 3.80 4.46
d=true,w=5 2.34 2.65 1.17 1.35 4.39 4.56 3.20 3.54
Table 1: Results based on the Choi dataset with varying
parameters.
nificant error reduction when using a window of 2
sentences. An impairment is observed when using
a too large window (w=5). This is expected, as the
size of the segments is in a range of 3-11 sentences:
A window of 5 sentences therefore leads to blocks
that contain segment boundaries. We can also see
that the mode method improves the results when
using a window of one, except for the documents
having small segments ranging from 3-5 sentences.
The lowest error rates are obtained with the mode
method and a window size of 2.
As described above, the algorithm is also able to
automatically estimate the number of segments us-
ing a threshold value (see Table 2).
3-5 6-8 9-11 3-11
Pk WD Pk WD Pk WD Pk WD
d=false,w=1 2.39 2.45 4.09 5.85 9.20 15.44 4.87 6.74
d=true,w=1 3.54 3.59 1.98 2.57 3.01 5.15 2.04 2.62
d=false,w=2 15.53 15.55 0.79 0.88 1.98 3.23 1.03 1.36
d=true,w=2 14.65 14.69 0.62 0.62 0.67 0.88 0.66 0.78
d=false,w=5 21.47 21.62 16.30 16.30 6.01 6.14 14.31 14.65
d=true,w=5 21.57 21.67 17.24 17.24 6.44 6.44 15.51 15.74
Table 2: Results on the Choi dataset without given num-
ber of segments as parameter.
The results show that for small segments, the
number of segments is not correctly estimated, as
the error rates are much higher than with given seg-
ments. As the window parameter has a smoothing
effect on the coherence score function, less possible
boundary candidates are detected. We can also see
that the usage of the mode parameter leads to worse
results with w=1 compared to the results where the
mode is deactivated for the documents containing
segments of length 3-5. Especially, results on these
documents suffer when not providing the number of
segments. But for the other documents, results are
much better. Some results (see segment lengths 6-
8 and 3-11 with d=true and w=2) are even better
than the results with segments provided (see Table
1). The threshold method can outperform the setup
with given a number of segments, since not recog-
nizing a segment produces less error in the measures
than predicting a wrong segment.
Table 3 presents a comparison of the performance
of TopicTilig compared to different algorithms in the
literature.
Method 3-5 6-8 9-11 3-11
TT (Choi, 2000) 44 43 48 46
C99 (Choi, 2000) 12 9 9 12
U00 (Utiyama and Isahara, 2001) 9 7 5 10
LCseg (Galley et al, 2003) 8.69
F04 (Fragkou et al, 2004) 5.5 3.0 1.3 7.0
M09 (Misra et al, 2009) 2.2 2.3 4.1 2.3
TopicTiling (d=true, w=2) 1.24 0.76 0.56 0.95
Table 3: Lowest Pk values for the Choi data set for vari-
ous algorithms in the literature with number of segments
provided
It is obvious that the results are far better than cur-
rent state-of-the-art results. Using a one-sampled t-
test with ? = 0.05 we can state significant improve-
ments in comparison to all other algorithms.
While we aim not using the same documents for
training and testing by using a CV scheme, it is not
guaranteed that all testing data is unseen, since the
same source sentences can find their way in several
artificially crafted ?documents?. We could detect re-
occurring snippets in up to 10% of the documents
provided by Choi. This problem, however, applies
for all evaluations on this dataset that use any kind
of training, be it LDA models in Misra et al (2009)
or tf-idf values in Fragkou et al (2004) and Galley
et al (2003).
5.2 Evaluation on Galley?s WSJ Dataset
For the evaluation on Galley?s WSJ dataset, a topic
model is created from the WSJ collection of the PTB
project. The dataset for model estimation consists
of 2499 WSJ articles, and is the same dataset Galley
used as a source corpus. The evaluation generally
leads to higher error rates than in the evaluation for
the Choi dataset, as shown in Table 4.
This table shows results of the WSJ data when us-
ing all words of the documents for training a topic
model and assigning topic IDs to new documents
and also filtered results, using only nouns (proper
40
Parameters All words Filtered
Pk WD Pk WD
d=false,w=1 37.31 43.20 37.01 43.26
d=true,w=1 35.31 41.27 33.52 39.86
d=false,w=2 22.76 28.69 21.35 27.28
d=true,w=2 21.79 27.35 19.75 25.42
d=false,w=5 14.29 19.89 12.90 18.87
d=true,w=5 13.59 19.61 11.89 17.41
d=false,w=10 14.08 22.60 14.09 22.22
d=true,w=10 13.61 21.00 13.48 20.59
Table 4: Results for Galley?s WSJ dataset using differ-
ent parameters with using unfiltered documents and with
filtered documents using only verbs, nouns (proper and
common) and adjectives.
and common), verbs and adjectives1. Considering
the unfiltered results we observe that results improve
when using the mode assigned topic ID and a win-
dow of larger than one sentence. In case of the WSJ
dataset, we find the optimal setting for w=5. As the
test documents contain whole articles, which con-
sist of at least 4 sentences, a larger window is ad-
vantageous here, yet a value of 10 is too large. Fil-
tering the documents for parts of speech leads to ?
1% absolute error rate reduction, as can be seen in
the last two columns of Table 4. Again, we observe
that the mode assignment always leads to better re-
sults, gaining at least 0.6%. Especially the window
size of 5 helps TopicTiling to decrease the error rate
to a third of the value observed with d=false and
w=1. Similar to the previous findings, results de-
cline when using a too large window.
Table 5 shows the results we achieve with the
threshold-based estimation of segment boundaries
for the unfiltered and filtered data.
Parameters All words Filtered
Pk WD Pk WD
d=false,w=1 53.07 72.78 52.63 72.66
d=true,w=1 53.42 74.12 51.84 72.57
d=false,w=2 46.68 65.01 44.81 63.09
d=true,w=2 46.08 64.41 43.54 61.18
d=false,w=5 30.68 43.73 28.31 40.36
d=true,w=5 28.29 38.90 26.96 36.98
d=false,w=10 19.93 32.98 18.29 29.29
d=true,w=10 17.50 26.36 16.32 24.75
Table 5: Table with results the WSJ dataset without num-
ber of segments given, using all words and content words
only.
1The Treetagger http://code.google.com/p/
tt4j/ is applied to POS-tag the data
In contrast to the results obtained with the Choi
dataset (see Table 2) no decline is observed when the
threshold approach is used in combination with the
window approach. We attribute this due to the small
segments and documents used in the Choi setting.
Comparing the all-words data with pos-filtered data,
an improvement is always observed. Also a contin-
uous decreasing of both error rates, Pk and WD,
is detected when using the mode and using a larger
window size, even for w=10. The reason for this is
that too many boundaries are detected when using
small windows. As the window approach smoothes
the similarity scores, this leads to less segmentation
boundaries, which improve results.
For comparison, we present the evaluation results
of other algorithms, shown in Table 6, as published
in Galley et al (2003).
Method Pk WD
C99 (Choi, 2000) 19.61 26.42
U00 (Utiyama and Isahara, 2001) 15.18 21.54
LCseg (Galley et al, 2003) 12.21 18.25
TopicTiling (d=true,w=5) 11.89 17.41
Table 6: List of results based on the WSJ dataset. Values
for C99, U00 and LCseg as stated in (Galley et al, 2003).
Again, TopicTiling improves over the state of the
art. The improvements with respect to LCseg are
significant using a one-sample t-test with ? = 0.05.
6 Conclusion and Further Work
We introduced TopicTiling, a new TS algorithm
that outperforms other algorithms as shown on two
datasets. The algorithm is based on TextTiling and
uses the topic model LDA to find topical changes
within documents. A general result with implica-
tions to other algorithms that use LDA topic IDs is
that using the mode of topic assignments across the
different inference steps is recommended to stabilize
the topic assignments, which improves performance.
As the inference method is relatively fast in compar-
ison to building a model, this mechanism is a useful
and simple improvement, not only restricted to the
field of TS. Using more than a single sentence in in-
ference blocks leads to further stability and less spar-
sity, which improves the results further. In contrast
to other TS algorithms using topic models (Misra
et al, 2009; Sun et al, 2008), the runtime of Top-
icTiling is linear in the number of sentences. This
41
makes TopicTiling a fast algorithm with complex-
ity of O(n) (n denoting the number of sentences)
as opposed to O(n2) of the dynamic programming
approach as discussed in Fragkou et al (2004).
Text segmentation benefits from the usage of topic
models. As opposed to general-purpose lexical re-
sources, topic models can also find fine-grained sub-
topical changes, as shown with the segmentation re-
sults of the WSJ dataset. Here, most articles have
financial content and the topic model can e.g. dis-
tinguish between commodity and stock trading. The
topic model adapts to the subtopic distribution of the
target collection, in contrast e.g. to static WordNet
domain labels as in Bentivogli et al (2004).
For further work, we would like to devise a
method to detect the optimal setting for the window
parameter w automatically, especially in a setting
where the number of target segments is not known in
advance. This is an issue that is shared with the orig-
inal TextTiling algorithm. Moreover, we will extend
the usage of our algorithm to more realistic corpora.
Another direction of research that is more generic
for approaches based on topic models is the ques-
tion of how to automatically select appropriate data
for topic model estimation, given only a small target
collection. Since topic model estimation is computa-
tionally expensive, and topic models for generic col-
lections (think Wikipedia) might not suit the needs
of a specialized domain (such as with the WSJ data),
it is a promising direction to look at target-domain-
driven automatic corpus synthesis.
Acknowledgments
This work has been supported by the Hessian re-
search excellence program ?Landes-Offensive zur
Entwicklung Wissenschaftlich-konomischer Exzel-
lenz? (LOEWE) as part of the research center ?Dig-
ital Humanities?.
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Sta-
tistical models for text segmentation. Mach. learn.,
34(1):177?210.
L. Bentivogli, P. Forner, B. Magnini, and E. Pianta. 2004.
Revising the wordnet domains hierarchy: semantics,
coverage and balancing. In Proc. COLING 2004 MLR,
pages 101?108, Geneva, Switzerland.
D. M. Blei, A. Y Ng, and M. I. Jordan. 2003. Latent
Dirichlet Allocation. JMLR ?03, 3:993?1022.
F. Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proc 1st NAACL ?00,
pages 26?33, Seattle, WA, USA.
J. Eisenstein. 2009. Hierarchical text segmentation from
multi-scale lexical cohesion. In Proc. NAACL-HLT
?09, pages 353?361, Boulder, CO, USA.
P. Fragkou, V. Petridis, and A. Kehagias. 2004. A Dy-
namic Programming Algorithm for Linear Text Seg-
mentation. JIIS ?04, 23(2):179?197.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proc 41st ACL ?03, volume 1, pages 562?
569, Sapporo, Japan.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific
topics. PNAS, 101:5228?5235.
M. A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proc. 32nd ACL ?94, pages 9?16,
Las Cruces, NM, USA.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. Macintyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: Annotating predicate ar-
gument structure. In Proc. ARPA-HLT Workshop ?94,
pages 114?119, Plainsboro, NJ, USA.
Hemant Misra, Joemon M Jose, and Olivier Cappe?. 2009.
Text Segmentation via Topic Modeling : An Analyti-
cal Study. In Proc. 18th CIKM ?09, pages 1553?1556,
Hong Kong.
L. Pevzner and M. A. Hearst. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics, 28.
X.-H. Phan and C.-T. Nguyen. 2007. GibbsLDA++: A
C/C++ implementation of latent Dirichlet alocation
(LDA). http://jgibblda.sourceforge.net/.
M. Riedl and C. Biemann. 2012a. How text segmen-
tation algorithms gain from topic models. In Proc.
NAACL-HLT ?12, Montreal, Canada.
M. Riedl and C. Biemann. 2012b. Sweeping through
the Topic Space: Bad luck? Roll again! In ROBUS-
UNSUP at EACL ?12, Avignon, France.
Q. Sun, R. Li, D. Luo, and X. Wu. 2008. Text segmen-
tation with LDA-based Fisher kernel. In Proc. 46th
ACl-HLT ?08, pages 269?272, Columbus, OH, USA.
M. Utiyama and H. Isahara. 2001. A statistical model for
domain-independent text segmentation. In Proc. 39th
ACL ?00, pages 499?506, Toulouse, France.
C. Wayne. 1998. Topic detection and tracking (TDT):
Overview & perspective. In Proc. DARPA BNTUW,
Lansdowne, Virginia.
Y. Yaari. 1997. Segmentation of expository texts by hi-
erarchical agglomerative clustering. In Proc. RANLP
?97, Tzigov Chark, Bulgaria.
42
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 61?71,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Exploring Cities in Crime: Significant Concordance and Co-occurrence in
Quantitative Literary Analysis
Janneke Rauscher1 Leonard Swiezinski2 Martin Riedl2 Chris Biemann2
(1) Johann Wolfgang Goethe University
Gru?neburgplatz 1, 60323 Frankfurt am Main, Germany
(2) FG Language Technology, Dept. of Computer Science
Technische Universita?t Darmstadt, 64289 Darmstadt, Germany
j.rauscher@em.uni-frankfurt.de, floppy35@web.de,
{riedl,biem}@cs.tu-darmstadt.de
Abstract
We present CoocViewer, a graphical analy-
sis tool for the purpose of quantitative lit-
erary analysis, and demonstrate its use on
a corpus of crime novels. The tool dis-
plays words, their significant co-occurrences,
and contains a new visualization for signif-
icant concordances. Contexts of words and
co-occurrences can be displayed. After re-
viewing previous research and current chal-
lenges in the newly emerging field of quan-
titative literary research, we demonstrate how
CoocViewer allows comparative research on
literary corpora in a project-specific study, and
how we can confirm or enhance our hypothe-
ses through quantitative literary analysis.
1 Introduction
Recent years have seen a surge in Digital Human-
ities research. This area, touching on both the
fields of computer science and the humanities, is
concerned with making data from the humanities
analysable by digitalisation. For this, computational
tools such as search, visual analytics, text mining,
statistics and natural language processing aid the hu-
manities researcher. On the one hand, software per-
mits processing a larger set of data in order to assess
traditional research questions. On the other hand,
this gives rise to a transformation of the way re-
search is conducted in the humanities: the possibil-
ity of analyzing a much larger amount of data ? yet
in a quantitative fashion with all its necessary aggre-
gation ? opens the path to new research questions,
and different methodologies for attaining them.
Although the number of research projects in Dig-
ital Humanities is increasing at fast pace, we still
observe a gap between the traditional humanities
scholars on the one side, and computer scientists on
the other. While computer science excels in crunch-
ing numbers and providing automated processing
for large amounts of data, it is hard for the com-
puter scientist to imagine what research questions
form the discourse in the humanities. In contrast to
this, humanities scholars have a hard time imagining
the possibilities and limitations of computer technol-
ogy, how automatically generated results ought to
be interpreted, and how to operationalize automatic
processing in a way that its unavoidable imperfec-
tions are more than compensated by the sheer size
of analysable material.
This paper resulted from a successful co-
operation between a natural language processing
(NLP) group and a literary researcher in the field
of Digital Humanities. We present the CoocViewer
analysis tool for literary and other corpora, which
supports new angles in literary research through
quantitative analysis.
In the Section 2, we describe the CoocViewer
tool and review the landscape of previously available
tools for our purpose. As a unique characteristic,
CoocViewer contains a visualisation of significant
concordances, which is especially useful for target
terms of high frequency. In Section 3, we map the
landscape of previous and current quantitative re-
search in literary analysis, which is still an emerging
and somewhat controversial sub-discipline. A use-
case for the tool in the context of a specific project
is laid out in Section 4, where a few examples illus-
61
trate how CoocViewer is used to confirm and gen-
erate hypotheses in literary analysis. Section 5 con-
cludes and provides an outlook to further needs in
tool support for quantative literary research.
2 CoocViewer - a Visual Corpus Browser
This section describes our CoocViewer visual cor-
pus browsing tool. After shortly outlining neces-
sary pre-processing steps, we illustrate and moti-
vate the functionality of the graphical user interface.
The tool was specifically designed to aid researchers
from the humanities that do not have a background
in computational linguistics.
2.1 Related Work
Whereas there exist a number of tools for visualizing
co-occurrences, there is, to the best of our knowl-
edge, no tool to visualize positional co-occurrences,
or as we also call them, significant concordances.
In (Widdows et al, 2002) tools are presented that
visualize meanings of nouns as vector space repre-
sentation, using LSA (Deerwester et al, 1990) and
graph models using co-occurrences. There is also
a range of text-based tools, without any quantita-
tive statistics, e.g. Textpresso (Mu?ller et al, 2004),
PhraseNet1 and Walden2. For searching words in
context, Luhn (1960) introduced KWIC (Key Word
in Context) which allows us to search for concor-
dances and is also used in several corpus linguis-
tic tools e.g. (Culy and Lyding, 2011), BNCWeb3,
Sketch Engine (Kilgarriff et al, 2004), Corpus
Workbench4 and MonoConc (Barlow, 1999). Al-
though several tools for co-occurrences visualiza-
tion exist (see e.g. co-occurrences for over 200 lan-
guages at LCC5), they often have different aims, and
e.g. do not deliver the functionality to filter on dif-
ferent part-of-speech tags.
2.2 Corpus Preprocessing
To make a natural language corpus accessible in the
tool, a number of preprocessing steps have to be car-
1http://www-958.ibm.com/software/data/
cognos/manyeyes/page/Phrase_Net.html
2http://infomotions.com/sandbox/
network-diagrams/bin/walden/
3http://bncweb.lancs.ac.uk/bncwebSignup/
user/login.php
4http://cwb.sourceforge.net
5http://corpora.uni-leipzig.de/
ried out for producing the contents of CoocViewer?s
database. These steps consist of a fairly standard
natural language processing pipeline, which we de-
scribe shortly.
After tokenizing, part-of-speech tagging (Schmid,
1994) and indexing the input data by document,
sentence and paragraph within the document, we
compute signficant sentence-wide and paragraph-
wide co-occurrences, using the tinyCC6 tool. Here,
the log-likelihood test (Dunning, 1993) is employed
to determine the significance sig(A,B) of the co-
occurrence of two tokens A and B. To sup-
port the significant concordance view (described in
the next section), we have extended the tool to
also produce positional significant co-occurrences,
where sig(A,B, offset) is computed by the log-
likelihood significance of the co-occurrence of A
and B in a token-distance of offset. Since the sig-
nificance measure requires the single frequencies of
A and B, as well as their joint frequency per po-
sitional offset in this setup, this adds considerable
overhead during preprocessing. To our knowledge,
we are the first to extend the notion of positional co-
occurrence beyond direct neighbors, cf. (Richter et
al., 2006). We apply a sigificance threshold of 3.847
and a frequency threshold of 2 to only keep ?interest-
ing? pairs. The outcome of preprocessing is stored in
a MySQL database schema similar to the one used
by LCC (Biemann et al, 2007). We store sentence-
and paragraph-wide co-occurrences and positional
co-occurrences in separate database tables, and use
one database per corpus. The database tables are in-
dexed accordingly to optimize the queries issued by
the CoocViewer tool. Additionally, we map the part-
of-speeches to E (proper names), N (proper nouns),
A (adjectives), V (verbs), R (all other part-of-speech
tags) for an uniform representation for different lan-
guages.
2.3 Graphical User Interface
The graphical user interface (UI) is built with com-
mon web technologies, such as HTML, CSS and
JavaScript. The UI communicates via AJAX with
a backend, which utilizes PHP and a MySQL
6http://wortschatz.uni-leipzig.de/
?cbiemann/software/TinyCC2.html, (Richter et
al., 2006)
7corresponding to 5% error probability
62
database. This makes the approach flexible regard-
ing the platform. It can run on client computers
using XAMP8, a portable package of various Web
technologies, including an Apache web server and a
MySQL server. Alternatively, the tool can operate as
a client-server application over a network. In partic-
ular, we want to highlight the JavaScript data visual-
ization framework D3 (Bostock et al, 2011), which
was used to layout and draw the graphs. We deliber-
ately designed the tool to match the requirements of
literary researchers, who are at times overwhelmed
by general-purpose visualisation tools such as e.g.
Gephi9. The UI is split into three parts: At the top a
menu bar, including a search input field and search
options, a graph drawing panel and display options
at the bottom of the page.
Figure 1: Screenshot of the Coocviewer application using
the concordance view.
The menu bar allows switching between co-
occurrence and concordance views (see Figure
1). The search field supports wildcards and
type-ahead autocompletion, immediately displaying
which words exist in the corpus and match the cur-
rent input. Additionally, there are functionalities to
export the shown graph as SVG or PNG image, or
as plain text, containing all relations, including their
frequencies and significance scores. Within the ad-
vanced configuration windows (shown on the right
side) one can select different corpora, enable case
sensitive/insensitive searches or filter words accord-
8http://www.apachefriends.org/en/index.
html
9https://gephi.org/
ing their part-of-speech tags (as described in Sec-
tion 2.2). The graph drawing panel visualizes the
queried term and its significant co-occurrences resp.
concordances, significancy being visualized by the
thickness of the lines. In Figure 1, showing the con-
cordances for bread, we can directly see words that
occur often with bread in the context: E.g. bread
is often used in combination with butter, cheese,
margarine (offset +2), but also the kind of different
breads is described by the adjectives at offset -1. For
the same information, using the normal KWIC view,
one has to count the words with different offset by
hand to find properties for the term bread. At the
bottom, the maximal number of words shown in the
graph can be specified. For the concordances display
there is an additional option to specify the maximal
offset. The original text (with provenance informa-
tion) containing the nodes (words) or edges (word
pairs) shown in the graph can be retrieved by either
clicking on a word itself or on the edge connecting
two words, in a new window (see Figure 2) within
the application. This window also provides informa-
Figure 2: Occurrences of a significant concordance
tion about the frequencies of single words as well as
their co-occurrence, and also displays relative single
word frequencies in parts-per-million (ppm) to en-
able comparisons between corpora of different sizes.
Words in focus are highlighted and the contents of
this window can also be exported as plain text.
3 Quantitative Literary Research
Quantitative research in literary analysis, although
being conducted and discussed since at least the
1960s, (Hoover, 2008), is still far from being a clear
field of research with a verified and acknowledged
methodology. Studies in this field vary widely with
respect to scope, methods applied and theoretical
63
background. Until now, only the most basic defi-
nition can be given that applies to these approaches:
Quantitative research in literary analysis is generally
concerned with the application of methods from cor-
pus linguistics (and statistics) to the field of litera-
ture to investigate and quantify general grammatical
and lexical features of texts.
Most studies applying such methods to literary anal-
ysis are carried out in the field of stylistics, building
a relatively new research area of corpus stylistics,
also called stylometry (Mahlberg, 2007; Hoover,
2008; Biber, 2011). The quantitative exploration
of stylistic features and patterns is used for author-
ship attribution, e.g. (Burrows, 1992; Burrows,
2007; Craig, 2004; Hoover, 2001; Hoover, 2002),
exploring the specificity of one author?s style, e.g.
(Burrows, 1987; Hori, 2004; Fischer-Starcke, 2010;
Mahlberg, 2012) or one certain text, often compared
to other texts of the same author or period, e.g.
(Craig, 1999; McKenna and Antonia, 2001; Stubbs,
2005; Clement, 2008; Fischer-Starcke, 2009). Some
studies focus on content-related questions such as
the analysis of plot or characterization and the ex-
ploration of relations between and role of different
characters, e.g. (Mahlberg, 2007; Culpeper, 2002;
Culpeper, 2009), developing new ways of exploring
these literary features, e.g. via the application of so-
cial network analysis (Elson et al, 2010; Moretti,
2011; Agarwal et al, 2012). Besides this area, there
are numerous other approaches, like the attempt to
investigate the phenomenon of ?literary creativity?
(Hoey, 2007) or ways for automatic recognition of
literary genres (Allison et al, 2011).
Major methodological approaches of this field are,
according to Biber (2011), Mahlberg (2007) and
Hoover (2008), the study of keywords and word-
frequencies, co-occurrences, lexical clusters (also
called bundles or n-grams) and collocational as well
as concordance analysis. Additionally, the need for
cross-investigating and comparing the results with
other corpora (be it a general corpus of one language
or other small, purpose-built corpora) is emphasized
to discuss the uniqueness of the results.
But while especially the studies of Moretti (2000;
2007; 2009), taking a quantitative approach of ?dis-
tant reading? on questions of literary history and the
evolution of literary genres, are often received as
groundbreaking for the case, and despite the rising
interest in this field of research in the last decades,
there still is much reluctance towards the imple-
mentation of such methods. The general arguments
raised frequently from the point of view of ?classi-
cal? literary analysis against a quantitative or compu-
tational approach can be grouped around four cen-
tral points: The uniqueness of each literary text
that quantitative analysis seems to underscore when
treating the texts just as examples of style or period,
focussing on very general patterns; the emphasize
of technology and the relatively high threshold that
the application, analysis and interpretation of the
generated data contains (Potter, 1988); and the gen-
eral notion that meaning in literary texts is highly
context-related and context-dependent in different
ways (Hoover, 2008). Last but not least there is
what can be called the ?so-what-argument?: Quan-
titative methods tend to produce sparse significant
new information compared with the classical ap-
proach of close reading, generating insights and in-
terpretations that could as well be reached by simply
reading the book (Mahlberg, 2007; Rommel, 2008).
But the possibilities and advantages of corpus lin-
guistics come to the foreground especially if one is
not interested in aspects of uniqueness or particu-
larity but in commonalities and differences between
large amounts of literary texts too many to be read
and compared in the classical way. This especially
holds when it comes to questions of topics, themes,
discourse analysis and the semantisation of certain
words.
4 Empirical Analysis
This section describes a few exemplary analysis
which we carried out within our ongoing project ?At
the crime scene: The intrinsic logic of cities in con-
temporary crime novels?. Settled between the dis-
ciplines of sociology and literature, the project is
embedded in the urban sociological research area of
the ?Eigenlogik? (intrinsic logic) of cities (Berking,
2012; Lo?w, 2012; Lo?w, forthcoming). The basic hy-
pothesis is that there is no such thing as ?the? city or
?the? urban experience in general, but that every city
forms its own contexts and complexes of meaning,
the unquestioned and often subconsciously operat-
ing knowledge of how things are done, respectively
making sense of the city. To put it another way, we
64
want to find out if and in what way the respective
city makes a difference and is forming distinctive
structures of thought, action and feeling. This is ex-
plored simultaneously in four different projects in-
vestigating different fields (economic practices, city
marketing, problem discourses and literary field and
texts) in four different cities that are compared with
each other (Birmingham (UK), Glasgow, Frankfurt
on the Main and Dortmund). If the hypothesis is
right, the four different investigated fields should
have more in common within one city and across the
fields than within one field across different cities.
Our subproject is mainly concerned with the literary
and cultural imagination and representation of the
cities in question. One crucial challenge is the ex-
ploration, analysis and comparison of 240 contem-
porary crime novels, each of them set in one of the
cities under examination. The aim of this explorative
study is to analyze the possibility and characteristics
of city-specific structures within the realm of liter-
ary representations of cities.
Dealing with such comparably large amounts of lit-
erary texts, a tool was needed that facilitated us
(laypeople in the field of corpus linguistics) to ex-
plore the city-specific content and structures within
these corpora, enabling a connection of qualitative
close reading and quantitative methods. Visualiza-
tion was a major concern, apparently lowering the
resistance of the literary research community to-
wards charts and numbers and making the results
readable and interpretable without having much ex-
pertise in corpus linguistics. Moreover, the option of
generating significant concordances instead of sim-
ple concordance lines (as e.g. with KWIC) is very
promising: Confronted with very high word fre-
quencies for some of our search terms, e.g. more
than 2200 occurrences of ?Frankfurt? in our Frank-
furt corpus, completely manual analysis turned out
to be painstaking and very time-consuming. Auto-
mated or manual reduction of the number of lines
according to standard practices, as e.g. suggested
by Tribble (2010), is not possible without potential
loss of information. CoocViewer enables a sophis-
ticated and automated analysis with concentration
on statistically significant findings through cluster-
ing co-occurring words according to their statistical
significance in concordance lines. Additionally, the
positionality of these re-occurring co-occurrences in
City lang. #novels #tokens #sent. #para.
Birmingham engl. 41 4.8M 336K 142K
Glasgow engl. 61 7.7M 496K 222K
Dortmund ger. 59 5.0M 361K 127K
Frankfurt ger. 79 8.0M 546K 230K
Table 1: Quantitative characteristics of our corpora
relation to the search term (with a maximum range
from -10 to +10 around the node) gives a clear
and immediate picture of patterns of usage within
a corpus. Via exploring the references of the re-
sults we are still able to take account of the context-
specificity of literary texts, as well as distinguishing
author-specific results from those distributed more
equally across a corpus.
After describing the corpus resources, we conduct
two exemplary analysis to show how the quantita-
tive tool as described in Section 2 can be used to aid
complex qualitative research interests in the human-
ities through supporting the exploration and compar-
ison of large corpora (Sect. 4.2), as well as investi-
gating and comparing the semantization and seman-
tic preference of words (Sect. 4.3). The discussion
of results shows how CoocViewer can support hy-
pothesis building and testing on a quantitative basis,
linking qualitative and quantitative approaches.
4.1 Corpus
The selection of the crime novels was based on three
criteria: contemporariness (written and published
within the past 30 years until 2010), the city in ques-
tion (should play a major role resp. be used as major
setting), and genre (crime fiction in any variety). In
a first step, the 240 novels (gathered as paperback-
editions) had to be scanned and digitalized10. Meta-
data was removed and the remaining texts were pre-
processed as described in Section 2.2. The nov-
els were compiled in different corpora according to
the city they are set in, and the database underly-
ing them (sentence or paragraph). Table 1 provides
an overview of the quantitative characteristics of the
four city-specific corpora we discuss here.
10We used ABBY FineReader 10 professional for optical
character recognition, which generated tolerable but not perfect
results, making extensive proof reading and corrections neces-
sary.
65
4.2 Analysis 1: Exploring the Use of the City?s
Name
The occurrence of the name of a city in crime nov-
els can serve different purposes and functions in
the text. It can be used, for example, to simply
?place? the plot (instead of or additionally to de-
scribing the setting in further detail) or to indicate
the direction of movement of figures (?they drove to
Glasgow?). Often it is surrounded by information
about city-specific aspects, e.g. of history or mate-
riality. Searching for the respective proper names of
the cities in the four corpora therefore seems to be
a promising start to explore the possibility of city-
specific structures of meaning in literary representa-
tion. If the ?Eigenlogik?-hypothesis is right, not only
the content that is associated with the name (what
would generally be expected) but also its frequent
usages and functions (as pointer or marker, as start-
ing point for further explanations of city life, etc.)
should differ systematically across cities.
A first close reading of some exemplary crime nov-
els already suggested that this could be the case. To
check this qualitatively derrived impression we con-
ducted CoocViewer searches for the top-15 signif-
icant co-occurrences across all parts of speech for
each proper name in the respective corpus on sen-
tence level (see Figure 3 for the cases of Glasgow
and Frankfurt). To interpret and compare these find-
ings, we additionally looked at the significant con-
cordances (with the same search parameters and an
offset from the node of -3/+3), which helps to ana-
lyze and refine the findings in more depth. In the fol-
lowing, we discuss, compare and interpret the results
with respect to our overriding project-hypothesis to
verify or falsify some of our qualitative first impres-
sions quantitatively.
The corpora indeed tend not only to vary signif-
icantly with respect to the sheer frequency of the
usage of the proper name (with relative frequen-
cies ranging from Glasgow (324ppm) and Frankfurt
(286ppm) to Dortmund (187ppm) and Birmingham
(154ppm)), but also in the usages and functions that
the naming fulfills. The graphs reveal not only dif-
fering co-occurrences, but also differing proportions
of co-occurring word classes, each city revealing its
own distinct pattern.
Especially the English cities tend to co-occur with
Figure 3: Significant co-occurrences of ?Glasgow? (up-
per) and ?Frankfurt? (lower) in their respective corpora
proper names and common nouns (ten proper
names, four common nouns in the case for Glas-
gow, eight names and six nouns for Birmingham).11
For Glasgow, these comprise parts of the inven-
tory of the city (with ?City? (sig. of 695.57) as
either part of the name or city-specific institution
(?City of Glasgow?, ?City Orchestra?) or to refer
to different crime-genre specific institutions (as the
?City of Glasgow Police? or ?Glasgow City Mortu-
ary?)), the ?University? (sig. of 380.42), or the park
?Glasgow Green? (233.46). There is also the name
of another city, the Scottish capital (and rival city)
Edinburgh. As the statistical concordances reveal
quickly, the ?Port? (350.88) is, despite Glasgow?s
history as shipbuilding capital, not used to refer to
the cities industrial past. Instead, as can bee seen
from its positioning on -1 directly left to the node,
it refers to the small nearby town Port Glasgow (see
11The noun ?accent? which both English cities names co-
occur with (and for which no equivalent term can be found on
the German side) can be explained by a different lexicalization
of the concept, which is realized through derivation in German.
66
Fig. 4). The co-occurrence of ?Royal? and Glas-
gow (being not a royal city) can also be easily ex-
plained via the concordance view, showing that this
is mainly due to the ?Royal Concert Hall? (forming
a strong triangle on positions +1, +2 and +3 from the
node). Besides these instances of places and institu-
tions within and around the city, especially the con-
nection to the pronoun ?its? (82 instances with a sig.
of 144) is interesting. None of the other cities shows
a top-significant co-occurrence with a comparable
pronoun. A look at the corresponding references in
the corpus shows that it is mainly used in statements
about the quality or speciality of certain aspects of
the city (indicated on graphic level through the con-
nections between ?its? and ?city? or ?area?) and in
personifications (e.g. ?Glasgow could still reach out
with its persistent demands?). This implies that the
literary device of anthropomorphization of the city
(in direct connection with the proper name) occurs
more often within Glasgow-novels than within those
of the other cities, and that there are many explicit
statements about ?how this city (or a special part of
it) is?, showing a tendency to explain the city. Fur-
thermore, the exploration of the different references
indicates a relatively ?coherent corpus? (and, there-
fore, relatively stable representation) with recurring
instances across many authors.
Figure 4: Significant concordances of ?Glasgow? in
Glasgow corpus
In contrast to this, Birmingham?s co-occurring
proper names mainly refer to (fictive) names of
newspapers (the Birmingham ?Sentinel?, ?Post? and
?News?). The inventory of the city is not very
prominently represented, with ?University? (sig. of
152.52) and ?Airport? (80.63) as the only instances.
Furthermore, the University tends to be represented
as region-, not city-specific (with a stronger connec-
tion between ?University? and ?Midlands? (sig. of
200.49) than between both words to the city itself
(?Midlands? co-occurring with a sig. of 68.68)).
The rest of the proper names relates to not fur-
ther specified parts of the city (?East? (71.62) and
?North-East? (73.43)). The word ?south? appears as
adverb, reflecting on graphic level that it is more of-
ten used as in ?heading south? than referring to the
?south of Birmingham?. Also, the noun ?city? (sig.
of 154.53) is often related to the ?city centre? (indi-
cated through the very strong link between those two
words), but also to make statements like ?Birming-
ham is a city that? or ?like other cities, Birmingham
has?. The references reveal the quality of this expla-
nations, rather stressing its ordinariness as city in-
stead of personalizing it or emphasizing its unique-
ness. This indicates that the city itself is not stand-
ing prominently in the foreground in its crime nov-
els (in contrast to Glasgow and in accordance with
our qualitatively derived prior results). The proper
name is mainly used as part of other proper names
(e.g. ?Birmingham Sentinel?), fulfilling the function
of simply placing the plot, and there is very little
city-specific information given on a statistical sig-
nificant re-occurring level in the closer surroundings
of it. Even the statements about Birmingham as a
city tend to downplay its singularity.
On the German side, the cities names co-occur with
words from a wider range of word classes. For
both cities, we find less co-occurring proper names:
five for Frankfurt, only one of them referring to a
city-specific aspect (the long version of the name
?Frankfurt on the Main? (sig. of 585.09)); four for
Dortmund (again, only one city-specific, the name
of its soccer club ?Borussia? (with only seven in-
stances and a sig. of 41.93)). For both cities, the
rest of the proper names is composed of names of
other cities (in Frankfurt the two nearby cities ?Of-
fenbach? (139.49) and ?Darmstadt? (105.73), and
?Berlin?, ?Hamburg?); for Dortmund only cities
from the same metropolitan area (the Ruhrgebiet),
?Du?sseldorf? (41.95), ?Werne? (41.78) and ?Duis-
burg? (39.42)). It seems that Dortmund is closely
connected within the metropolitan area it is a part
of, but looking at the references shows that only
Du?sseldorf plays a role across different crime novel
series, while the rest mainly feature in one certain
series (being rated as author-specifc).
67
In the case of Frankfurt, the nouns that co-
occur (seven) either denote city-specific aspects
(Flughafen (airport) (96.83) and Eintracht (the lo-
cal soccer club with a sig. of 192.36)) or very
general instances (December, Jahren (years)). A
look at the statistical concordances, ordered accord-
ing not only to their position around the node but
also to their significance, displays that the noun
?Kripo? (short form for crime investigation unit)
on the -1 position is more often used than the first
city-specific instance, with a significance of 564.58
(while ?police? for Glasgow on the +1 position is
relatively ranked lower). This prominent position
of the crime investigation unit (interpreted as im-
pact of genre-related aspects) indicates that there are
many ?police-procedural? crime novels in Frankfurt
(which is true), giving insight into the sub-genre
composition of the corpus. As with the English
cities, the word ?Stadt? (city; sig. of 245.63) co-
occurs frequently, and as the references reveal it
serves similar purposes: either to denote the politi-
cal administration (the ?Stadt Frankfurt?) or in com-
bination with further explanations of ?how this city
is? (as in Glasgow, but without personalization), or
?Frankfurt is a city that?, but in contrast to Birming-
ham not with a frequent downplaying of uniqueness.
Additionally, we find instances where other cities
are compared to Frankfurt (?a city that, unlike/like
Frankfurt?). This seems to point towards a more
flexible use of this combination resp. to a variety
of ways of representation. Frankfurt is represented
as a city allowing for different semantizations and
different ways of depicting it without posing contra-
dictions (as the differing uses occur not only across
a wide range of authors, but within the same texts).
Finally, taking a closer look at Dortmund, the
frequently co-occurring nouns nearly all are re-
lated to genre-specific instances, referring to crime
investigation-related institutions (again ?Kripo?
(sig. of 88.91); ?Polizeipra?sidium? (police head-
quarters; sig. of 35.15), ?Landgericht? (dis-
trict court; 37.25) and ?Sonderstaatsanwaltschaft?
(34.63)). This indicates that in this corpus the genre-
specific structures seem to imprint themselves more
than the city-specific ones, putting the city itself
into the background (similar to the case of Birm-
ingham but with a highly differing pattern). But
we also have to consider the comparably low rel-
ative frequency rates (ppm) that demand an expla-
nation. There might be another similarity between
Dortmund and Birmingham, both showing low rel-
ative frequencies for their respective proper names.
But as we take a closer look on the references of
the occurrences of the names, we can see that the
one series of crime novels that represents the biggest
share of the corpus (with 21 novels belonging to this
series) does not mention ?Dortmund? at all, while
for Birmingham the use of the proper name is quite
equally distributed across all authors and series. A
look inside one of this books of the series in ques-
tion reveals a possible answer to the low frequencies:
instead of using the proper name, the author conse-
quently uses the nickname ?Bierstadt? (Beer-city).
Therefore, while it is possible to show that each city
under investigation reveals a specific pattern of co-
occurrences and differing uses and functions of its
proper name, as our hypothesis has suggested, the
search for the proper name alone seems not suffi-
cient to get the overall picture of the literary repre-
sentation of a city, demanding further analysis.
4.3 Analysis 2: Investigating Genre Aspects
When it comes to questions of genre-conventions vs.
city-conventions, the investigation of the semantic
preference of typically crime-related words is inter-
esting. If the specific city has an impact on genre-
aspects, the graphs should show clear differences.
Close reading of exemplary novels of Glasgow and
Birmingham indicated that violence plays a greater
role in Glasgow crime fiction than in that of Birm-
ingham, therefore we expect to find differing attri-
butions towards and meanings of ?violence?, show-
ing a higher vocabulary richness in Glasgow than in
Birmingham, taking into account its semantic pref-
erence (for more details about this aspect see e.g.
(Hoey, 2007)). We examine this hypothesis through
making ?violence? the node of a search for signif-
icant concordances, searching for the top-30 adjec-
tives directly altering the noun within a range of -3
to +3 around the node.
As depicted in Figure 5, our initial hypothesis can
be verified. While Glasgow (upper) has nine sig-
nificantly co-occurring adjectives (six directly alter-
ing the noun ?violence? on pos. -1), Birmingham
(lower) only has five (four on pos. -1). Those that di-
rectly alter the noun show a slightly differing seman-
68
Figure 5: Significant adjective concordances of ?vio-
lence?, comparing Glasgow (upper) and Birmingham
(lower) corpora
tic preference, with adjectives of ?kind of violence?
(domestic, physical) standing on top in both corpora.
Next, we look at adjectives that bear a notion of
?quality or intensity of violence?: while Birming-
ham only discriminates between mindless and latent
violence, the vocabulary of Glasgow is much richer
(thuggish, mindless, sudden), one of them also bear-
ing a notion of expectability (sudden). Additionally,
a temporal adjective is used to refer to ?past vio-
lence?. If we look at the instances on the -3 position
for Glasgow (a position that is not filled for Birm-
ingham), we can add random to the list of ?qual-
ity of violence?, and find some instances of ?being
afraid of (physical) violence? (as the link between
those words implies). This verifies our close reading
interpretations.
The adjectives to the right of the node (?own?
on position +3 in Glasgow, ?old? on position +2
in Birmingham) pose a puzzle. Through a look at
the references for this instances, we can see that
in the case of Birmingham, old is referring to vic-
tims of violence (old people), while the picture for
Glasgow is split between violence of its own type
(which then could be added to the list of quality-
adjectives) and violence that one experienced on his
own. Through the interconnectedness of the adjec-
tives settled on different positions for the case of
Glasgow and a look at the resources of the instances,
we conclude that the patterns seem to be more estab-
lished on city level (showing instances from varying
authors for all adjective-noun combinations) than
they are in Birmingham, where there are no cross-
connections and the authors differ more among each
other (with ?physical violence? being the only com-
bination that occurs across different authors, while
all other adjective-noun combinations only appear
within the work of a single author).
5 Conclusion and Further Work
To conclude the exemplary analysis, CoocViewer
helps not only to explore large corpora but also to
verify or relativize impressions from classical quali-
tative literary research. It opens up new ways of ex-
ploring topics, themes and relationships within large
sets of literary texts. Especially the combination
and linkage of co-occurrences and significant con-
cordances simplifies the analysis and allows a finer-
grained and more focused analysis than KWIC con-
cordances or simple frequency counts. The possi-
bility to distinguish between these two viewpoints
on the data accelerates and improves the interpre-
tation of results. Additionally, the comparison be-
tween corpora is much facilitated through the imme-
diate visibility of differing patterns. Further work
can proceed along a few lines. We would like
to enable investigations of the wide context of co-
occurrences through access from the references back
to the whole crime-novel document. Further, we
would like to automatically compare corpora of the
same language on the level of local co-occurrence
and concordance graphs to aid generating hypothe-
ses. This will make a change in the interface nec-
essary to support a comparative view. Furthermore,
we want to extend the view of the original text (see
Figure 2) in our tool by centering the sentences ac-
cording to the selected word or words, as done in
KWIC views. When clicking on a single word, this
would lead to the normal KWIC view, but selecting
an edge we then want to center the sentences accord-
ing to the two words connected by the edge, which
might be useful especially for the concordances.
The tool and the pre-processing software is avail-
able as an open source project12 and as a web demo.
Acknowledgments
This work has been funded by the Hessian re-
search excellence program Landes-Offensive zur
Entwicklung Wissenschaftlich-O?konomischer Exzel-
lenz (LOEWE) as part of the research center Digital
Humanities.
12https://sourceforge.net/p/coocviewer
69
References
Apoorv Agarwal, Augusto Corvalan, Jacob Jensen, and
Owen Rambow. 2012. Social network analysis of
alice in wonderland. In Workshop on Computational
Linguistics for Literature, pages 88?96, Montre?al,
Canada.
Sahra Allison, Ryan Heuser, Mathhew Jockers, Franco
Moretti, and Michael Witmore. 2011. Quantitative
Formalism: an Experiment. Stanford Literary Lab.
M. Barlow. 1999. Monoconc 1.5 and paraconc. Interna-
tional journal of corpus linguistics, 4(1):173?184.
Helmuth Berking. 2012. The distinctiveness of cities:
outline of a research programme. Urban Research &
Practice, 5(3):316?324.
Douglas Biber. 2011. Corpus linguistics and the study
of literature. back to the future? Scientific Study of
Literature, 1(1):15?23.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora Col-
lection - monolingual corpora of standard size. In
Proceedings of Corpus Linguistics 2007, Birmingham,
UK.
Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer.
2011. D3: Data-driven documents. IEEE Trans. Visu-
alization & Comp. Graphics (Proc. InfoVis).
John F. Burrows. 1987. Computation into Criticism. A
Study of Jane Austen?s Novels and an Experiment in
Method. Clarendon, Oxford.
John F. Burrows. 1992. Computers and the study of liter-
ature. In Christopher S. Butler, editor, Computers and
Written Texts, pages 167?204, Oxford. Blackwell.
John F. Burrows. 2007. All the way through: Testing for
authorship in different frequency strata. Literary and
Linguistic Computing, 22(1):27?47.
Tanya E. Clement. 2008. ?A thing not beginning and not
ending?: using digital tools to distand-read Gertrude
Stein?s The Making of Americans. Literary and Lin-
guistic Computing, 23(3):361?381.
Hugh Craig. 1999. Jonsonian chronology and the styles
of a tale of a tub. In Martin Butler, editor, Re-
Presenting Ben Jonson: Text, History, Performance,
pages 210?232, Houndmills. Macmillan.
Hugh Craig. 2004. Stylistic analysis and authorship
studies. In Susan Schreibman, Ray Siemens, and John
Unsworth, editors, A Companion to Digital Humani-
ties. Blackwell.
Jonathan Culpeper. 2002. Computers, language and
characterisation: An analysis of six characters in
Romeo and Juliet. In Ulla Melander-Marttala, Carin
Ostman, and Merja Kyt, editors, Conversation in Life
and Literature: Papers from the ASLA Symposium,
volume 15, pages 11?30, Uppsala. Association Sue-
doise de Linguistique Appliquee.
Jonathan Culpeper. 2009. Keyness: Words, parts-of-
speech and semantic categories in the character-talk of
shakespeare?s romeo and juliet. International Journal
of Corpus Linguistics, 14(1):29?59.
Chris Culy and Verena Lyding. 2011. Corpus clouds - fa-
cilitating text analysis by means of visualizations. In
Zygmunt Vetulani, editor, Human Language Technol-
ogy. Challenges for Computer Science and Linguistics,
volume 6562 of Lecture Notes in Computer Science,
pages 351?360. Springer Berlin Heidelberg.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. In 48th Annual Meeting of the Association for
Computer Linguistics, pages 138?147, Uppsala, Swe-
den.
Bettina Fischer-Starcke. 2009. Keywords and fre-
quent phrases of Jane Austen?s Pride and Prejudice: A
corpus-stylistc analysis. International Journal of Cor-
pus Linguistics, 14(4):492?523.
Bettina Fischer-Starcke. 2010. Corpus linguistics in lit-
erary analysis: Jane Austen and her contemporaries.
Continuum, London.
Carsten Go?rg, Hannah Tipney, Karin Verspoor, Jr. Baum-
gartner, William A., K. Bretonnel Cohen, John Stasko,
and Lawrence E. Hunter. 2010. Visualization and
language processing for supporting analysis across the
biomedical literature. In Knowledge-Based and Intel-
ligent Information and Engineering Systems, volume
6279 of Lecture Notes in Computer Science, pages
420?429. Springer Berlin Heidelberg.
Michael Hoey. 2007. Lexical priming and literary cre-
ativity. In Michael Hoey, Michaela Mahlberg, Michael
Stubbs, and Wolfgang Teubert, editors, Text, Dis-
course and Corpora. Theory and Analysis, pages 31?
56, London. Continuum.
David L. Hoover. 2001. Statistical stylistics and author-
ship attribution: an emprirical investigation. Literary
and Linguistic Computing, 16(4):421?444.
David L. Hoover. 2002. Frequent word sequences and
statistical stylistics. Literary and Linguistic Comput-
ing, 17(2):157?180.
David L. Hoover. 2008. Quantitative analysis and liter-
ary studies. In Ray Siemens and Susan Schreibman,
editors, A Companion to Digital Literary Studies, Ox-
ford. Blackwell.
70
Masahiro Hori. 2004. Investigating Dicken?s Style:
A Collocational Analysis. Palgrave Macmillan, Bas-
ingstoke.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The sketch engine. In Proceedings of
EURALEX, pages 105?116, Lorient, France.
Martina Lo?w. 2012. The intrinsic logic of cities: towards
a new theory on urbanism. Urban Research & Prac-
tice, 5(3):303?315.
Martina Lo?w. forthcoming. The city as experential
space: The production of shared meaning. Interna-
tional Journal of Urban and Regional Research.
H. P. Luhn. 1960. Key word-in-context index for tech-
nical literature (KWIC index). American Documenta-
tion, 11(4):288?295.
Michaela Mahlberg. 2007. Corpus stylistics: bridg-
ing the gap between linguistic and literary studies. In
Michael Hoey, Michaela Mahlberg, Michael Stubbs,
and Wolfgang Teubert, editors, Text, Discourse and
Corpora. Theory and Analysis, pages 217?246, Lon-
don. Continuum.
Michaela Mahlberg. 2012. Corpus Stylistics and
Dicken?s Fiction. Routledge advances in corpus lin-
guistics. Routledge, London.
C. W. F. McKenna and Alexis Antonia. 2001. The sta-
tistical analysis of style: Reflections on fom, meaning,
and ideology in the ?Nausicaa? episode of Ulysses. Lit-
erary and Linguistic Computing, 16(4):353?373.
Franco Moretti. 2000. Conjectures on world literature.
New Left Review, 1(January/February):54?68.
Franco Moretti. 2007. Graphs, Maps, Trees. Abstract
Models for Literary History. Verso, London / New
York.
Franco Moretti. 2009. Style, Inc. reflections on seven
thousand titels (British novels, 1740-1850). Critical
Inquiry, 36(1):134?158.
Franco Moretti. 2011. Network Theory, Plot Analysis.
Stanford Literary Lab.
Hans-Michael Mu?ller, Eimear E. Kenny, and Paul W.
Sternberg. 2004. Textpresso: An ontology-based in-
formation retrieval and extraction system for biologi-
cal literature. Plos Biology, 2(11).
Rosanne G. Potter. 1988. Literary criticism and literary
computing: The difficulties of a synthesis. Computers
and Humanities, 22:91?97.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsdo?ttir,
and Chris Biemann. 2006. Exploiting the Leipzig
Corpora Collection. In Proceesings of the IS-LTC
2006, Ljubljana, Slovenia.
Thomas Rommel. 2008. Literary studies. In Ray
Siemens and Susan Schreibman, editors, A Compan-
ion to Digital Literary Studies, Oxford. Blackwell.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
Michael Stubbs. 2005. Conrad in the computer: exam-
ples of quantitative stylistic methods. Language and
Literature, 14(1):5?24.
Christopher Tribble. 2010. What are concordances and
how are they used? In Anne O?Keeffe and Michael
McCarthy, editors, The Routledge Handbook of Cor-
pus Linguistics, pages 167?183, Abingdon. Routledge.
Dominic Widdows, Scott Cederberg, and Beate Dorow.
2002. Visualisation Techniques for Analysing Mean-
ing. In Fifth International Conference on Text, Speech
and Dialogue (TSD-02), pages 107?114. Springer.
71
Proceedings of the TextGraphs-8 Workshop, pages 6?10,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
JoBimText Visualizer:
A Graph-based Approach to Contextualizing Distributional Similarity
Alfio Gliozzo1 Chris Biemann2 Martin Riedl2
Bonaventura Coppola1 Michael R. Glass1 Matthew Hatem1
(1) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
(2) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
{gliozzo,mrglass,mhatem}@us.ibm.com coppolab@gmail.com
{biem,riedl}@cs.tu-darmstadt.de
Abstract
We introduce an interactive visualization com-
ponent for the JoBimText project. JoBim-
Text is an open source platform for large-scale
distributional semantics based on graph rep-
resentations. First we describe the underly-
ing technology for computing a distributional
thesaurus on words using bipartite graphs of
words and context features, and contextualiz-
ing the list of semantically similar words to-
wards a given sentential context using graph-
based ranking. Then we demonstrate the ca-
pabilities of this contextualized text expan-
sion technology in an interactive visualization.
The visualization can be used as a semantic
parser providing contextualized expansions of
words in text as well as disambiguation to
word senses induced by graph clustering, and
is provided as an open source tool.
1 Introduction
The aim of the JoBimText1 project is to build a
graph-based unsupervised framework for computa-
tional semantics, addressing problems like lexical
ambiguity and variability, word sense disambigua-
tion and lexical substitutability, paraphrasing, frame
induction and parsing, and textual entailment. We
construct a semantic analyzer able to self-adapt to
new domains and languages by unsupervised learn-
ing of semantics from large corpora of raw text. At
the moment, this analyzer encompasses contextual-
ized similarity, sense clustering, and a mapping of
senses to existing knowledge bases. While its pri-
mary target application is functional domain adap-
tation of Question Answering (QA) systems (Fer-
1http://sf.net/projects/jobimtext/
rucci et al, 2013), output of the semantic analyzer
has been successfully utilized for word sense disam-
biguation (Miller et al, 2012) and lexical substitu-
tion (Szarvas et al, 2013). Rather than presenting
the different algorithms and technical solutions cur-
rently implemented by the JoBimText community in
detail, in this paper we will focus on available func-
tionalities and illustrate them using an interactive vi-
sualization.
2 Underlying Technologies
While distributional semantics (de Saussure, 1959;
Harris, 1951; Miller and Charles, 1991) and the
computation of distributional thesauri (Lin, 1998)
has been around for decades, its full potential has yet
to be utilized in Natural Language Processing (NLP)
tasks and applications. Structural semantics claims
that meaning can be fully defined by semantic oppo-
sitions and relations between words. In order to per-
form a reliable knowledge acquisition process in this
framework, we gather statistical information about
word co-occurrences with syntactic contexts from
very large corpora. To avoid the intrinsic quadratic
complexity of the similarity computation, we have
developed an optimized process based on MapRe-
duce (Dean and Ghemawat, 2004) that takes advan-
tage of the sparsity of contexts, which allows scal-
ing the process through parallelization. The result of
this computation is a graph connecting the most dis-
criminative contexts to terms and explicitly linking
the most similar terms. This graph represents local
models of semantic relations per term rather than a
model with fixed dimensions. This representation
departs from the vector space metaphor (Schu?tze,
1993; Erk and Pado?, 2008; Baroni and Zamparelli,
6
2010), commonly employed in other frameworks for
distributional semantics such as LSA (Deerwester et
al., 1990) or LDA (Blei et al, 2003).
The main contribution of this paper is to de-
scribe how we operationalize semantic similarity in
a graph-based framework and explore this seman-
tic graph using an interactive visualization. We de-
scribe a scalable and flexible computation of a dis-
tributional thesaurus (DT), and the contextualization
of distributional similarity for specific occurrences
of language elements (i.e. terms). For related works
on the computation of distributional similarity, see
e.g. (Lin, 1998; Lin and Dyer, 2010).
2.1 Holing System
To keep the framework flexible and abstract with re-
spect to the pre-processing that identifies structure
in language material, we introduce the holing op-
eration, cf. (Biemann and Riedl, 2013). It is ap-
plied to observations over the structure of text, and
splits these observations into a pair of two parts,
which we call the ?Jo? and the ?Bim?2. All JoBim
pairs are maintained in the bipartite First-Order Jo-
Bim graph TC(T,C,E) with T set of terms (Jos),
C set of contexts (Bims), and e(t, c, f) ? E edges
between t ? T , c ? C with frequency f . While
these parts can be thought of as language elements
referred to as terms, and their respective context fea-
tures, splits over arbitrary structures are possible (in-
cluding pairs of terms for Jos), which makes this
formulation more general than similar formulations
found e.g. in (Lin, 1998; Baroni and Lenci, 2010).
These splits form the basis for the computation of
global similarities and for their contextualization. A
Holing System based on dependency parses is illus-
trated in Figure 1: for each dependency relation, two
JoBim pairs are generated.
2.2 Distributed Distributional Thesaurus
Computation
We employ the Apache Hadoop MapReduce Fram-
work3, and Apache Pig4, for parallelizing and dis-
tributing the computation of the DT. We describe
this computation in terms of graph transformations.
2arbitrary names to emphasize the generality, should be
thought of as ?term? and ?context?
3http://hadoop.apache.org
4http://pig.apache.org/
Figure 1: Jos and Bims generated applying a dependency
parser (de Marneffe et al, 2006) to the sentence I suffered
from a cold and took aspirin. The @@ symbolizes the
hole.
Staring from the JoBim graph TC with counts as
weights, we first apply a statistical test5 to com-
pute the significance of each pair (t, c), then we only
keep the p most significant pairs per t. This consti-
tutes our first-order graph for Jos FOJO. In analogy,
when keeping the p most significant pairs per c, we
can produce the first-order graph for Bims FOBIM .
The second order similarity graph for Jos is defined
as SOJO(T,E) with Jos t1, t2 ? T and undirected
edges e(t1, t2, s) with similarity s = |{c|e(t1, c) ?
FOJO, e(t2, c) ? FOJO}|, which defines similar-
ity between Jos as the number of salient features
two Jos share. SOJO defines a distributional the-
saurus. In analogy, SOBIM is defined over the
shared Jos for pairs of Bims and defines similar-
ity of contexts. This method, which can be com-
puted very efficiently in a few MapReduce steps, has
been found superior to other measures for very large
datasets in semantic relatedness evaluations in (Bie-
mann and Riedl, 2013), but could be replaced by any
other measure without interfering with the remain-
der of the system.
2.3 Contextualization with CRF
While the distributional thesaurus provides the sim-
ilarity between pairs of terms, the fidelity of a par-
ticular expansion depends on the context. From the
term-context associations gathered in the construc-
tion of the distributional thesaurus we effectively
have a language model, factorized according to the
holing operation. As with any language model,
smoothing is critical to performance. There may be
5we use log-likelihood ratio (Dunning, 1993) or LMI (Evert,
2004)
7
many JoBim (term-context) pairs that are valid and
yet under represented in the corpus. Yet, there may
be some similar term-context pair that is attested in
the corpus. We can find similar contexts by expand-
ing the term arguments with similar terms. However,
again we are confronted with the fact that the simi-
larity of these terms depends on the context.
This suggests some technique of joint inference
to expand terms in context. We use marginal in-
ference in a conditional random field (CRF) (Laf-
ferty et al, 2001). A particular world, x is defined
as single, definite sequence of either original or ex-
panded words. The weight of the world, w(x) de-
pends on the degree to which the term-context as-
sociations present in this sentence are present in the
corpus and the general out-of-context similarity of
each expanded term to the corresponding term in the
original sentence. Therefore the probability associ-
ated with any expansion t for any position xi is given
by Equation 1. Where Z is the partition function, a
normalization constant.
P (xi = t) =
1
Z
?
{x | xi=t}
ew(x) (1)
The balance between the plausibility of an ex-
panded sentence according to the language model,
and its per-term similarity to the original sentence is
an application specific tuning parameter.
2.4 Word Sense Induction, Disambiguation
and Cluster Labeling
The contextualization described in the previous sub-
section performs implicit word sense disambigua-
tion (WSD) by ranking contextually better fitting
similar terms higher. To model this more explicitly,
and to give rise to linking senses to taxonomies and
domain ontologies, we apply a word sense induction
(WSI) technique and use information extracted by
IS-A-patterns (Hearst, 1992) to label the clusters.
Using the aggregated context features of the clus-
ters, the word cluster senses are assigned in con-
text. The DT entry for each term j as given in
SOJO(J,E) induces an open neighborhood graph
Nj(Vj , Ej) with Vj = {j?|e(j, j?, s) ? E) and Ej
the projection of E regarding Vj , consisting of sim-
ilar terms to j and their similarities, cf. (Widdows
and Dorow, 2002).
We cluster this graph using the Chinese Whispers
graph clustering algorithm (Biemann, 2010), which
finds the number of clusters automatically, to ob-
tain induced word senses. Running shallow, part-
of-speech-based IS-A patterns (Hearst, 1992) over
the text collection, we obtain a list of extracted IS-
A relationships between terms, and their frequency.
For each of the word clusters, consisting of similar
terms for the same target term sense, we aggregate
the IS-A information by summing the frequency of
hypernyms, and multiplying this sum by the number
of words in the cluster that elicited this hypernym.
This results in taxonomic information for labeling
the clusters, which provides an abstraction layer for
terms in context6. Table 1 shows an example of this
labeling from the model described below. The most
similar 200 terms for ?jaguar? have been clustered
into the car sense and the cat sense and the high-
est scoring 6 hypernyms provide a concise descrip-
tion of these senses. This information can be used
to automatically map these cluster senses to senses
in an taxonomy or ontology. Occurrences of am-
biguous words in context can be disambiguated to
these cluster senses comparing the actual context
with salient contexts per sense, obtained by aggre-
gating the Bims from the FOJO graph per cluster.
sense IS-A labels similar terms
jaguar
N.0
car, brand,
company,
automaker,
manufacturer,
vehicle
geely, lincoln-mercury,
tesla, peugeot, ..., mit-
subishi, cadillac, jag, benz,
mclaren, skoda, infiniti,
sable, thunderbird
jaguar
N.1
animal, species,
wildlife, team,
wild animal, cat
panther, cougar, alligator,
tiger, elephant, bull, hippo,
dragon, leopard, shark,
bear, otter, lynx, lion
Table 1: Word sense induction and cluster labeling exam-
ple for ?jaguar?. The shortened cluster for the car sense
has 186 members.
3 Interactive Visualization
3.1 Open Domain Model
The open domain model used in the current vi-
sualization has been trained from newspaper cor-
6Note that this mechanism also elicits hypernyms for unam-
biguous terms receiving a single cluster by the WSI technique.
8
Figure 2: Visualization GUI with prior expansions for
?cold?. Jobims are visualized on the left, expansions on
the right side.
pora using 120 million sentences (about 2 Giga-
words), compiled from LCC (Richter et al, 2006)
and the Gigaword (Parker et al, 2011) corpus. We
constructed a UIMA (Ferrucci and Lally, 2004)
pipeline, which tokenizes, lemmatizes and parses
the data using the Stanford dependency parser (de
Marneffe et al, 2006). The last annotator in the
pipeline annotates Jos and Bims using the collapsed
dependency relations, cf. Fig. 1. We define the lem-
matized forms of the terminals including the part-
of-speech as Jo and the lemmatized dependent word
and the dependency relation name as Bim.
3.2 Interactive Visualization Features
Evaluating the impact of this technology in applica-
tions is an ongoing effort. However, in the context
of this paper, we will show a visualization of the ca-
pabilities allowed by this flavor of distributional se-
mantics. The visualization is a GUI as depicted in
Figure 2, and exemplifies a set of capabilities that
can be accessed through an API. It is straightfor-
ward to include all shown data as features for seman-
tic preprocessing. The input is a sentence in natural
language, which is processed into JoBim pairs as de-
scribed above. All the Jos can be expanded, showing
their paradigmatic relations with other words.
We can perform this operation with and without
taking the context into account (cf. Sect. 2.3). The
latter performs an implicit disambiguation by rank-
ing similar words higher if they fit the context. In
the example, the ?common cold? sense clearly dom-
inates in the prior expansions. However, ?weather?
and ?chill? appear amongst the top-similar prior ex-
pansions.
We also have implemented a sense view, which
displays sense clusters for the selected word, see
Figure 3. Per sense, a list of expansions is pro-
vided together with a list of possible IS-A types. In
this example, the algorithm identified two senses of
?cold? as a temperature and a disease (not all clus-
ter members shown). Given the JoBim graph of the
context (as displayed left in Fig. 2), the particular
occurrence of ?cold? can be disambiguated to Clus-
ter 0 in Fig. 3, since its Bims ?amod(@@,nasty)?
and ?-dobj(catch, @@)? are found in FOJO for far
more members of cluster 0 than for members of clus-
ter 1. Applications of this type of information in-
clude knowledge-based word sense disambiguation
(Miller et al, 2012), type coercion (Kalyanpur et al,
2011) and answer justification in question answering
(Chu-Carroll et al, 2012).
4 Conclusion
In this paper we discussed applications of the Jo-
BimText platform and introduced a new interactive
visualization which showcases a graph-based unsu-
pervised technology for semantic processing. The
implementation is operationalized in a way that it
can be efficiently trained ?off line? using MapRe-
duce, generating domain and language specific mod-
els for distributional semantics. In its ?on line? use,
those models are used to enhance parsing with con-
textualized text expansions of terms. This expansion
step is very efficient and runs on a standard laptop,
so it can be used as a semantic text preprocessor. The
entire project, including pre-computed data models,
is available in open source under the ASL 2.0, and
allows computing contextualized lexical expansion
on arbitrary domains.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Comp. Ling., 36(4):673?721.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: representing adjective-noun
constructions in semantic space. In Proc. EMNLP-
2010, pages 1183?1193, Cambridge, Massachusetts.
C. Biemann and M. Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual simi-
larity. Journal of Language Modelling, 1(1):55?95.
C. Biemann. 2010. Co-occurrence cluster features for
lexical substitutions in context. In Proceedings of
TextGraphs-5, pages 55?59, Uppsala, Sweden.
9
Figure 3: Senses induced for the term ?cold?.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022,
March.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: search and candidate generation. IBM
J. Res. Dev., 56(3):300?311.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. LREC-2006, Genova,
Italy.
Ferdinand de Saussure. 1916. Cours de linguistique
ge?ne?rale. Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. OSDI
?04, San Francisco, CA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proc. EMNLP-
2008, pages 897?906, Honolulu, Hawaii.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
D. Ferrucci and A. Lally. 2004. UIMA: An Architectural
Approach to Unstructured Information Processing in
the Corporate Research Environment. In Nat. Lang.
Eng. 2004, pages 327?348.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T.
Mueller. 2013. Watson: Beyond Jeopardy! Artificial
Intelligence, 199-200:93?105.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING-
1992, pages 539?545, Nantes, France.
A. Kalyanpur, J.W. Murdock, J. Fan, and C. Welty. 2011.
Leveraging community-built knowledge for type co-
ercion in question answering. In Proc. ISWC 2011,
pages 144?156. Springer.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML 2001, pages 282?289, San Francisco, CA, USA.
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan & Claypool Publishers, San
Rafael, CA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768?774,
Montre?al, Quebec, Canada.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych. 2012.
Using distributional similarity for lexical expansion
in knowledge-based word sense disambiguation. In
Proc. COLING-2012, pages 1781?1796, Mumbai, In-
dia.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proc. IS-LTC 2006, Ljubljana, Slovenia.
H. Schu?tze. 1993. Word space. In Advances in Neu-
ral Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.
G. Szarvas, C. Biemann, and I. Gurevych. 2013. Super-
vised all-words lexical substitution using delexicalized
features. In Proc. NAACL-2013, Atlanta, GA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING-
2002, pages 1?7, Taipei, Taiwan.
10
Proceedings of the TextGraphs-8 Workshop, pages 39?43,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
From Global to Local Similarities:
A Graph-Based Contextualization Method using Distributional Thesauri
Chris Biemann and Martin Riedl
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biem}@cs.tu-darmstadt.de
Abstract
After recasting the computation of a distribu-
tional thesaurus in a graph-based framework
for term similarity, we introduce a new con-
textualization method that generates, for each
term occurrence in a text, a ranked list of terms
that are semantically similar and compatible
with the given context. The framework is in-
stantiated by the definition of term and con-
text, which we derive from dependency parses
in this work. Evaluating our approach on a
standard data set for lexical substitution, we
show substantial improvements over a strong
non-contextualized baseline across all parts of
speech. In contrast to comparable approaches,
our framework defines an unsupervised gener-
ative method for similarity in context and does
not rely on the existence of lexical resources as
a source for candidate expansions.
1 Introduction
Following (de Saussure, 1916) we consider two dis-
tinct viewpoints: syntagmatic relations consider the
assignment of values to a linear sequence of terms,
and the associative (also: paradigmatic) viewpoint
assigns values according to the commonalities and
differences to other terms in the reader?s memory.
Based on these notions, we automatically expand
terms in the linear sequence with their paradigmati-
cally related terms.
Using the distributional hypothesis (Harris,
1951), and operationalizing similarity of terms
(Miller and Charles, 1991), it became possible to
compute term similarities for a large vocabulary
(Ruge, 1992). Lin (1998) computed a distributional
thesaurus (DT) by comparing context features de-
fined over grammatical dependencies with an ap-
propriate similarity measure for all reasonably fre-
quent words in a large collection of text, and evalu-
ated these automatically computed word similarities
against lexical resources. Entries in the DT consist
of a ranked list of the globally most similar terms for
a target term. While the similarities are dependent
on the instantiation of the context feature as well as
on the underlying text collection, they are global in
the sense that the DT aggregates over all occurrences
of target and its similar elements. In our work, we
will use a DT in a graph representation and move
from a global notion of similarity to a contextual-
ized version, which performs context-dependent text
expansion for all word nodes in the DT graph.
2 Related Work
The need to model semantics just in the same way
as local syntax is covered by the n-gram-model, i.e.
trained from a background corpus sparked a large
body of research on semantic modeling. This in-
cludes computational models for topicality (Deer-
wester et al, 1990; Hofmann, 1999; Blei et al,
2003), and language models that incorporate topical
(as well as syntactic) information, see e.g. (Boyd-
Graber and Blei, 2008; Tan et al, 2012). In the
Computational Linguistics community, the vector
space model (Schu?tze, 1993; Turney and Pantel,
2010; Baroni and Lenci, 2010; Pucci et al, 2009;
de Cruys et al, 2013) is the prevalent metaphor for
representing word meaning.
While the computation of semantic similarities on
the basis of a background corpus produces a global
model, which e.g. contains semantically similar
words for different word senses, there are a num-
ber of works that aim at contextualizing the infor-
mation held in the global model for particular oc-
currences. With his predication algorithm, Kintsch
(2001) contextualizes LSA (Deerwester et al, 1990)
for N-VP constructions by spreading activation over
neighbourhood graphs in the latent space.
In particular, the question of operationalizing se-
mantic compositionality in vector spaces (Mitchell
39
and Lapata, 2008) received much attention. The lex-
ical substitution task (McCarthy and Navigli, 2009)
(LexSub) sparked several approaches for contextual-
ization. While LexSub participants and subsequent
works all relied on a list of possible substitutions
as given by one or several lexical resources, we de-
scribe a graph-based system that is knowledge-free
and unsupervised in the sense that it neither requires
an existing resource (we compute a DT graph for
that), nor needs training for contextualization.
3 Method
3.1 Holing System
For reasons of generality, we introduce the holing
operation (cf. (Biemann and Riedl, 2013)), to split
any sort of observations on the syntagmatic level
(e.g. dependency relations) into pairs of term and
context features. These pairs are then both used for
the computation of the global DT graph similarity
and for the contextualization. This holing system
is the only part of the system that is dependent on
a pre-processing step; subsequent steps operate on
a unified representation. The representation is given
by a list of pairs <t,c> where t is the term (at a cer-
tain offset) and c is the context feature. The position
of t in c is denoted by a hole symbol ?@?. As an ex-
ample, the dependency triple (nsub;gave2;I1)
could be transferred to <gave2,(nsub;@;I1)>
and <I1,(nsub;gave2;@)>.
3.2 Distributional Similarity
Here, we present the computation of the distribu-
tional similarity between terms using three graphs.
For the computation we use the Apache Hadoop
Framework, based on (Dean and Ghemawat, 2004).
We can describe this operation using a bipartite
?term?-?context feature? graph TC(T,C,E) with
T the set terms, C the set of context features and
e(t, c, f) ? E edges between t ? T , c ? C
with f = count(t, c) frequency of co-occurrence.
Additionally, we define count(t) and count(c) as
the counts of the term, respectively as the count
of the context feature. Based on the graph TC
we can produce a first-order graph FO(T,C,E),
with e(t, c, sig) ? E. First, we calculate a signif-
icance score sig for each pair (t, c) using Lexicog-
rapher?s Mutual Information (LMI): score(t, c) =
LMI(t, c, ) = count(t, c) log2(
count(t,c)
count(t)count(c))
(Evert, 2004). Then, we remove all edges with
score(t, c) < 0 and keep only the p most signif-
icant pairs per term t and remove the remaining
edges. Additionally, we remove features which co-
occur with more then 1000 words, as these features
do not contribute enough to similarity to justify the
increase of computation time (cf. (Rychly? and Kil-
garriff, 2007; Goyal et al, 2010)). The second-
order similarity graph between terms is defined as
SO(T,E) for t1, t2 ? T with the similarity score
s = |{c|e(t1, c) ? FO, e(t2, c) ? FO}|, which is
the number of salient features two terms share. SO
defines a distributional thesaurus.
In contrast to (Lin, 1998) we do not count how of-
ten a feature occurs with a term (we use significance
ranking instead), and do not use cosine or other sim-
ilarities (Lee, 1999) to calculate the similarity over
the feature counts of each term, but only count sig-
nificant common features per term. This constraint
makes this approach more scalable to larger data, as
we do not need to know the full list of features for
a term pair at any time. Seemingly simplistic, we
show in (Biemann and Riedl, 2013) that this mea-
sure outperforms other measures on large corpora in
a semantic relatedness evaluation.
3.3 Contextual Similarity
The contextualization is framed as a ranking prob-
lem: given a set of candidate expansions as pro-
vided by the SO graph, we aim at ranking them such
that the most similar term in context will be ranked
higher, whereas non-compatible candidates should
be ranked lower.
First, we run the holing system on the lexical
material containing our target word tw ? T ? ?
T and select all pairs <tw,ci> ci ? C ? ? C
that are instantiated in the current context. We
then define a new graph CON(T ?, C ?, S) with con-
text features ci ? C ?. Using the second-order
similarity graph SO(T,E) we extract the top n
similar terms T ?={ti, . . . , tn}?T from the second-
order graph SO for tw and add them to the graph
CON . We add edges e(t, c, sig) between all tar-
get words and context features and label the edge
with the significance score from the first order graph
FO. Edges e(t, c, sig), not contained in FO, get
a significance score of zero. We can then calcu-
40
late a ranking score for each ti with the harmonic
mean, using a plus one smoothing: rank(ti) =?
cj
(sig(ti,cj)+1)/count(term(cj))
?
cj
(sig(ti,cj)+1)/count(term(cj))
(term(cj) extracts
the term out of the context notation). Using that
ranking score we can re-order the entries t1, . . . , tn
according to their ranking score.
In Figure 1, we exemplify this, using the tar-
get word tw= ?cold? in the sentence ?I caught
a nasty cold.?. Our dependency parse-based
Figure 1: Contextualized ranking for target ?cold? in the
sentence ?I caught a nasty cold? for the 10 most similar
terms from the DT.
holing system produced the following pairs for
?cold?: <cold5 ,(amod;@;nasty4)>,
<cold5,(dobj;caught2;@)>. The top 10
candidates for ?cold? are T ?={heat, weather, tem-
perature, rain, flue, wind, chill, disease}. The scores
per pair are e.g. <heat, (dobj;caught;@)>
with an LMI score of 42.0, <weather
,(amod;@;nasty)> with a score of 139.4.
The pair <weather, (dobj;caught;@)>
was not contained in our first-order data. Ranking
the candidates by their overall scores as given in the
figure, the top three contextualized expansions are
?disease, flu, heat?, which are compatible with both
pairs. For the top 200 words, the ranking of fully
compatible candidates is: ?virus, disease, infection,
flu, problem, cough, heat, water?, which is clearly
preferring the disease-related sense of ?cold? over
the temperature-related sense.
In this way, each candidate t? gets as many
scores as there are pairs containing c? in the holing
system output. An overall score per t? then given by
the harmonic mean of the add-one-smoothed single
scores ? smoothing is necessary to rank candidates
t? that are not compatible to all pairs. This scheme
can easily be extended to expand all words in a given
sentence or paragraph, yielding a two-dimensional
contextualized text, where every (content) word is
expanded by a list of globally similar words from the
distributional thesaurus that are ranked according to
their compatibility with the given context.
4 Evaluation
The evaluation of contextualizing the thesaurus (CT)
was performed using the LexSub dataset, introduced
in the Lexical Substitution task at Semeval 2007
(McCarthy and Navigli, 2009). Following the setup
provided by the task organizers, we tuned our ap-
proach on the 300 trial sentences, and evaluate it
on the official remaining 1710 test sentences. For
the evaluation we used the out of ten (oot) preci-
sion and oot mode precision. Both measures cal-
culate the number of detected substitutions within
ten guesses over the complete subset. Whereas en-
tries in the oot precision measures are considered
correct if they match the gold standard, without pe-
nalizing non-matching entries, the oot mode preci-
sion includes also a weighting as given in the gold
standard1. For comparison, we use the results of the
DT as a baseline to evaluate the contextualization.
The DT was computed based on newspaper corpora
(120 million sentences), taken from the Leipzig Cor-
pora Collection (Richter et al, 2006) and the Giga-
word corpus (Parker et al, 2011). Our holing system
uses collapsed Stanford parser dependencies (Marn-
effe et al, 2006) as context features. The contextual-
ization uses only context features that contain words
with part-of-speech prefixes V,N,J,R. Furthermore,
we use a threshold for the significance value of the
LMI values of 50.0, p=1000, and the most similar 30
terms from the DT entries.
5 Results
Since out contextualization algorithm is dependent
on the number of context features containing the tar-
get word, we report scores for targets with at least
two and at least three dependencies separately. In
the Lexical Substitution Task 2007 dataset (LexSub)
test data we detected 8 instances without entries in
the gold standard and 19 target words without any
1The oot setting was chosen because it matches the expan-
sions task better than e.g. precision@1
41
dependency, as they are collapsed into the depen-
dency relation. The remaining entries have at least
one, 49.2% have at least two and 26.0% have at least
three dependencies. Furthermore, we also evalu-
ated the results broken down into separate part-of-
speeches of the target. The results on the LexSub
test set are shown in Table 1.
Precision Mode Precision
min. # dep. 1 2 3 1 2 3
POS Alg.
noun CT 26.64 26.55 28.36 38.68 38.24 37.68
noun DT 25.35 25.09 28.07 34.96 34.31 36.23
verb CT 23.39 23.75 23.05 32.05 33.09 33.33
verb DT 22.46 22.13 21.32 29.17 28.78 28.25
adj. CT 32.65 34.75 36.08 45.09 48.24 46.43
adj. DT 32.13 33.25 35.02 43.56 43.53 42.86
adv. CT 20.47 29.46 36.23 30.14 40.63 100.00
adv. DT 28.91 26.75 29.88 41.63 34.38 66.67
ALL CT 26.46 26.43 26.61 37.21 37.40 37.38
ALL DT 27.06 24.83 25.24 36.96 33.06 33.11
Table 1: Results of the LexSub test dataset.
Inspecting the results for all POS (denoted as
ALL), we only observe a slight decline for the preci-
sion score with at least only one dependency, which
is caused by adverbs. For targets with more than
one dependency, we observe overall improvements
of 1.6 points in precision and more than 4 points in
mode precision.
Regarding the results of different part-of-speech
tags, we always improve over the DT ranking, ex-
cept for adverbs with only one dependency. Most
notably, the largest relative improvements are ob-
served on verbs, which is a notoriously difficult
word class in computational semantics. For adverbs,
at least two dependencies seem to be needed; there
are only 7 adverb occurrences with more than two
dependencies in the dataset. Regarding performance
on the original lexical substitution task (McCarthy
and Navigli, 2009), we did not come close to the per-
formance of the participating systems, which range
between 32?50 precision points, respectively 43?66
mode precision points (only taking systems with-
out duplicate words in the result set into account).
However, all participants used one or several lexical
resources for generating substitution candidates, as
well as a large number of features. Our system, on
the other hand, merely requires a holing system ? in
this case based on a dependency parser ? and a large
amount of unlabeled text, and a very small number
of contextual clues.
For an insight of the coverage for the entries deliv-
ered by the DT graph, we extended the oot precision
measure, to consider not only the first 10 entries, but
the first X={1,10,50,100,200} entries (see Figure 2).
Here we also show the coverage for different sized
Figure 2: Coverage on the LexSub test dataset for differ-
ent DT graphs, using out of X entries.
datasets (10 and 120 million sentences). Amongst
the 200 most similar words from the DT, a cover-
age of up to 55.89 is reached. DT quality improves
with corpus size, especially due to increased cover-
age. This shows that there is considerable headroom
for optimization for our contextualization method,
but also shows that our automatic candidate expan-
sions can provide a coverage that is competitive to
lexical resources.
6 Conclusion
We have provided a way of operationalizing seman-
tic similarity by splitting syntagmatic observations
into terms and context features, and representing
them a first-order and second-order graph. Then,
we introduced a conceptually simple and efficient
method to perform a contextualization of semantic
similarity. Overall, our approach constitutes an un-
supervised generative model for lexical expansion
in context. We have presented a generic method
on contextualizing distributional information, which
retrieves the lexical expansions from a target term
from the DT graph, and ranks them with respect to
their context compatibility. Evaluating our method
on the LexSub task, we were able to show improve-
ments, especially for expansion targets with many
informing contextual elements. For further work,
we will extend our holing system and combine sev-
eral holing systems, such as e.g. n-gram contexts.
42
Additionally, we would like to adapt more advanced
methods for the contextualization (Viterbi, 1967;
Lafferty et al, 2001) that yield an all-words simulta-
neous expansion over the whole sequence, and con-
stitutes a probabilistic model of lexical expansion.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! a framework for lexical expansion with contextual
similarity. Journal of Language Modelling, 1(1):55?
95.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022.
J. Boyd-Graber and D. M. Blei. 2008. Syntactic topic
models. In Neural Information Processing Systems,
Vancouver, BC, USA.
T. Van de Cruys, T. Poibeau, and A. Korhonen. 2013. A
tensor-based factorization model of semantic compo-
sitionality. In Proc. NAACL-HLT 2013, Atlanta, USA.
F. de Saussure. 1916. Cours de linguistique ge?ne?rale.
Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. of
Operating Systems, Design & Implementation, pages
137?150, San Francisco, CA, USA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
A. Goyal, J. Jagarlamudi, H. Daume?, III, and T. Venkata-
subramanian. 2010. Sketch techniques for scaling dis-
tributional similarity to the web. In Proc. of the 2010
Workshop on GEometrical Models of Nat. Lang. Se-
mantics, pages 51?56, Uppsala, Sweden.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago, USA.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. 22nd ACM SIGIR, pages 50?57,
New York, NY, USA.
W. Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
the 18th Int. Conf. on Machine Learning, ICML ?01,
pages 282?289, San Francisco, CA, USA.
L. Lee. 1999. Measures of distributional similarity. In
Proc. of the 37th ACL, pages 25?32, College Park,
MD, USA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING?98, pages 768?774,
Montreal, Quebec, Canada.
M.-C. De Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. of the Int. Conf. on
Language Resources and Evaluation, Genova, Italy.
D. McCarthy and R. Navigli. 2009. The english lexical
substitution task. Language Resources and Evalua-
tion, 43(2):139?159.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, OH, USA.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia, USA.
D. Pucci, M. Baroni, F. Cutugno, and R. Lenci. 2009.
Unsupervised lexical substitution with a word space
model. In Workshop Proc. of the 11th Conf. of the
Italian Association for Artificial Intelligence, Reggio
Emilia, Italy.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proceesings of the IS-LTC 2006, Ljubljana, Slove-
nia.
G. Ruge. 1992. Experiments on linguistically-based
term associations. Information Processing & Manage-
ment, 28(3):317 ? 332.
P. Rychly? and A. Kilgarriff. 2007. An efficient algo-
rithm for building a distributional thesaurus (and other
sketch engine developments). In Proc. 45th ACL,
pages 41?44, Prague, Czech Republic.
Hinrich Schu?tze. 1993. Word space. In Advances in
Neural Information Processing Systems 5, pages 895?
902. Morgan Kaufmann.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic, and
lexical language model. Computational Linguistics,
38(3):631?671.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: vector space models of semantics. J. Artif.
Int. Res., 37(1):141?188.
A. J. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, 13(2):260?
269.
43
