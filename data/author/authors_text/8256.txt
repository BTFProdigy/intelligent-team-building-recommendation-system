Proceedings of NAACL HLT 2007, Companion Volume, pages 133?136,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An integrated architecture for speech-input multi-target machine translation
Alicia Pe?rez, M. Ine?s Torres
Dep. of Electricity and Electronics
University of the Basque Country
manes@we.lc.ehu.es
M. Teresa Gonza?lez, Francisco Casacuberta
Dep. of Information Systems and Computation
Technical University of Valencia
fcn@dsic.upv.es
Abstract
The aim of this work is to show the abil-
ity of finite-state transducers to simultane-
ously translate speech into multiple lan-
guages. Our proposal deals with an ex-
tension of stochastic finite-state transduc-
ers that can produce more than one out-
put at the same time. These kind of de-
vices offer great versatility for the inte-
gration with other finite-state devices such
as acoustic models in order to produce a
speech translation system. This proposal
has been evaluated in a practical situation,
and its results have been compared with
those obtained using a standard mono-
target speech transducer.
1 Introduction
Finite-state models constitute an important frame-
work both in syntactic pattern recognition and in
language processing. Specifically, stochastic finite-
state transducers (SFSTs) have proved to be useful
for machine translation tasks within restricted do-
mains; they usually offer high speed during the de-
coding step and they provide competitive results in
terms of error rates (Mohri et al, 2002). Moreover,
SFSTs have proved to be versatile models, which
can be easily integrated with other finite-state mod-
els (Pereira and Riley, 1997).
The article (Casacuberta and Vidal, 2004) ex-
plored an automatic method to learn an SFST from a
bilingual set of samples for machine translation pur-
poses, the so-called GIATI (Grammar Inference and
Alignments for Transducers Inference). It described
how to learn both the structural and the probabilistic
components of an SFST making use of underlying
alignment models.
A multi-target SFST is a generalization of stan-
dard SFSTs, in such a way that every input string
in the source language results in a tuple of output
strings each being associated to a different target
language. An extension of GIATI that allowed to in-
fer a multi-target SFST from a multilingual corpus
was proposed in (Gonza?lez and Casacuberta, 2006).
A syntactic variant of this method (denoted as GI-
AMTI) has been used in this work in order to infer
the models from training samples as it is summa-
rized in section 3.
On the other hand, speech translation has been al-
ready carried out by integrating acoustic models into
a SFST (Casacuberta et al, 2004). Our main goal
in this work is to extend and assess these method-
ologies to accomplish spoken language multi-target
translation. Section 2 deals with this proposal by
presenting a new integrated architecture for speech-
input multi-target translation. Under this approach
spoken language can be simultaneously decoded and
translated into m languages using a unique network.
In section 4, the performance of the system has
been experimentally evaluated over a trilingual task
which aims to translate TVweather forecast into two
languages at the same time.
2 An integrated architecture for
speech-input multi-target translation
The classical architecture for spoken language
multi-target translation involves a speech recogni-
133
tion system in a serial architecture withm decoupled
text-to-text translators. Thus, the whole process in-
volves m + 1 searching stages, a first one for the
speech signal transcription into the source language
text string, and further m for the source language
translation into the m target languages. If we re-
placed the m translators by the multi-target SFST,
the problem would be reduced to 2 searching stages.
Nevertheless, in this paper we propose a natural way
for acoustic models to be integrated in the same net-
work. As a result, the input speech-signal can be
simultaneously decoded and translated into m target
languages just in a single searching stage.
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i ? {1, . . . ,m}). This approach is summarized
in eq. (1), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
t?m = argmax
tm
P (tm|x) = argmax
tm
?
s
P (tm, s|x)
(1)
Making use of Bayes? rule, the former expression
turns into:
t?m = argmax
tm
?
s
P (tm, s)P (x|tm, s) (2)
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string: i.e., that P (x|tm, s) is in-
dependent of tm. In this sense, eq. (2) can be rewrit-
ten as:
t?m = argmax
tm
?
s
P (tm, s)P (x|s) (3)
Equation (3) combines a standard acoustic model,
P (x|s), and a multi-target translation model,
P (tm, s), both of whom can be integrated on the fly
during the searching routine. Nevertheless, the outer
maximization is computationally very expensive to
search for the optimal tuple of target strings tm in
an effective way. Thus we make use of the so called
Viterbi approximation, which finds the best path.
3 Inference
Given a multilingual corpus, that is, a finite set of
multilingual samples (s, t1, . . . , tm) ? ?? ? ??1 ?
? ? ? ? ??m, where ti denotes the translation of the
source sentence s (formed by words of the input vo-
cabulary ?) into the i-th target language, which, in
its turn, has a vocabulary ?i, the GIAMTI method
can be outlined as follows:
1. Each multilingual sample is transformed into a
single string from an extended vocabulary (? ?
????1 ? ? ? ? ??
?
m) using a labelling function
(Lm). This transformation searches an ade-
quate monotonous segmentation for each of the
m source-target language pairs. A monotonous
segmentation copes with monotonous align-
ments, that is, j < k ? aj < ak following
the notation of (Brown et al, 1993). Each
source word is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a word from the source language plus zero
or more words from each target language.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z ? ??), a stochastic regular grammar can be
inferred.
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input word and m output phrases
(w/p?1/ . . . /p?m) by the inverse labeling func-
tion (L?m), leading to the required transducer.
In this work, the first step of the algorithm (as
described above), which is the one that handles
the alignment and segmentation routines, relies on
statistical alignments obtained with GIZA++ (Och,
2000). The second step was implemented us-
ing our own language modeling toolkit, which
learns stochastic k-testable in the string-sense gram-
mars (Torres and Varona, 2001), and allows for
back-off smoothing.
4 Experimental results
4.1 Task and corpus
We have implemented a highly practical application
that could be used to translate on-line TV weather
forecasts into several languages, taking the speech
of the presenter as the input and producing as output
text-strings, or sub-titles, in several languages. For
134
this purpose, we used the corpus METEUS (see Ta-
ble 1) which consists of a set of trilingual sentences,
in English, Spanish and Basque, as extracted from
weather forecast reports that had been published on
the Internet. Basque language is a minority lan-
guage, spoken in a small area of Europe and also
within some small American communities (such as
that in Boise, Idaho). In the Basque Country it has
an official status along with Spanish. However both
languages differs greatly in syntax and in semantics.
The differences in the size of the vocabulary (see
Table 1), for instance, are due to the agglutinative
nature of the Basque language.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
independent if it is to be realistic.
Spanish Basque English
T
ra
in
in
g Sentences 14,615
Different Sent. 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Te
st
Different Sent. 500
Words 8,706 8,274 9,150
Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
Table 1: Main features of the METEUS corpus.
4.2 System evaluation
The experimental setup was as follows: the multi-
target SFST was learned from the training set in Ta-
ble 1 using the GIAMTI algorithm described in sec-
tion 1; then, the speech test was translated, and the
output provided by the system in each language was
compared to the corresponding reference sentence.
Additionally, two mono-target SFST were inferred
from the same training set with their outputs for the
aforementioned test to be taken as baseline.
4.2.1 Computational cost
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
input machine translation applications. These values
can be objectively measured based on the size and on
the average branching factor of the model displayed
in Table 2.
multi-target mono-targetS2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Braching factor 3.30 3.13 3.46
Table 2: Features of multi-target model and the two
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the states and the edges up for the two
mono-target SFSTs that take part in the decoupled
architecture (see Table 2), we conclude that the de-
coupled model needs a total of 185, 216 edges to be
allocated in memory, which represents an increment
of 13% in memory-space with respect to the multi-
target model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which means that searching
for a translation can be faster. In fact, experimental
results in Table 3 show that the mono-target archi-
tecture works%11more slowly than the multi-target
one.
multi-target mono-targetS2B S2E S2B+S2E
Time (s) 30,514 24,398 9,501 33,899
Table 3: Time needed to translate the speech-test
into two languages.
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems have been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In order
to assess the performance of the system in a quan-
titative manner, the following evaluation parameters
135
were computed for each scenario: bilingual evalua-
tion under study (BLEU), position independent er-
ror rate (PER) and word error rate (WER).
As can be derived from the Speech-input trans-
lation results shown in Table 4, slightly better re-
sults are obtained with the classical mono-target SF-
STs, compared with the multi-target approach. From
Spanish into English the improvement is around
3.4% but from Spanish into Basque, multi-target ap-
proach works better with an improvement of a 0.8%.
multi-target mono-target
S2B S2E S2B S2E
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
Table 4: Speech-input translation results for Spanish
into Basque (S2B) and Spanish into English (S2E)
using a multi-target SFST or two mono-target SF-
STs.
The process of speech signal decoding is itself
introducing some errors. In an attempt to measure
these errors, the text transcription of the recognized
input signal was extracted and compared to the input
reference in terms of WER as shown in Table 5.
multi-target mono-targetS2B S2E
WER 10.7 9.3 9.1
Table 5: Spanish speech decoding results for the
multi-target SFST and the two mono target SFSTs.
5 Concluding remarks and further work
A fully embedded architecture that integrates the
acoustic model into the multi-target translation
model for multiple speech translation has been pro-
posed. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm. The most significant feature of this
approach is its ability to carry out both the recogni-
tion and the translation into multiple languages inte-
grated in a unique model.
In contrast to the classical decoupled systems,
multi-target SFSTs enable the translation from one
source language simultaneously into several target
languages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults.
In future work we intend to make a deeper study
on the performance of the multi-target system as the
amount of targets increase, since the amount of pa-
rameters to be estimated also increases.
Acknowledgements
This work has been partially supported by the Uni-
versity of the Basque Country and by the Spanish
CICYT under grants 9/UPV 00224.310-15900/2004
and TIC2003-08681-C02-02 respectively.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205?
225.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M.
Vilar, S. Barrachina, I. Garc??a-Varea, D. Llorens,
C. Mart??nez, S. Molau, F. Nevado, M. Pastor, D. Pico?,
A. Sanchis, and C. Tillmann. 2004. Some approaches
to statistical and finite-state speech-to-speech transla-
tion. Computer Speech and Language, 18:25?47, Jan-
uary.
M. Teresa Gonza?lez and Francisco Casacuberta. 2006.
Multi-Target Machine Translation using Finite-State
Transducers. In Proceedings of TC-Star Speech to
Speech Translation Workshop, pages 105?110.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88, January.
Franz J. Och. 2000. GIZA++: Training of statistical
translation models.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431?
453. The MIT Press, Cambridge, Massachusetts.
M. Ine?s Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127?149.
136
Proceedings of the Second Workshop on Statistical Machine Translation, pages 56?63,
Prague, June 2007. c?2007 Association for Computational Linguistics
Speech-input multi-target machine translation
Alicia Pe?rez, M. Ine?s Torres
Dep. of Electricity and Electronics
University of the Basque Country
manes@we.lc.ehu.es
M. Teresa Gonza?lez, Francisco Casacuberta
Dep. of Information Systems and Computation
Technical University of Valencia
fcn@dsic.upv.es
Abstract
In order to simultaneously translate speech
into multiple languages an extension of
stochastic finite-state transducers is pro-
posed. In this approach the speech trans-
lation model consists of a single network
where acoustic models (in the input) and the
multilingual model (in the output) are em-
bedded.
The multi-target model has been evaluated
in a practical situation, and the results have
been compared with those obtained using
several mono-target models. Experimental
results show that the multi-target one re-
quires less amount of memory. In addition, a
single decoding is enough to get the speech
translated into multiple languages.
1 Introduction
In this work we deal with finite-state models which
constitute an important framework in syntactic pat-
tern recognition for language and speech processing
applications (Mohri et al, 2002; Pereira and Riley,
1997). One of their outstanding characteristics is the
availability of efficient algorithms for both optimiza-
tion and decoding purposes.
Specifically, stochastic finite-state transducers
(SFSTs) have proved to be useful for machine trans-
lation tasks within restricted domains. There are
several approaches implemented over SFSTs which
range from word-based systems (Knight and Al-
Onaizan, 1998) to phrase-based systems (Pe?rez et
al., 2007). SFSTs usually offer high speed during
the decoding step and they provide competitive re-
sults in terms of error rates. In addition, SFSTs have
proved to be versatile models, which can be easily
integrated with other finite-state models, such as a
speech recognition system for speech-input transla-
tion purposes (Vidal, 1997). In fact, the integrated
architecture has proved to work better than the de-
coupled one. Our main goal is, hence, to extend
and assess these methodologies to accomplish spo-
ken language multi-target translation.
As far as multilingual translation is concerned,
there are two main trends in machine translation de-
voted to translate an input string simultaneously into
m languages (Hutchins and Somers, 1992): inter-
lingua and parallel transfer. The former has his-
torically been a knowledge-based technique that re-
quires a deep-analysis effort, and the latter consists
on m decoupled translators in a parallel architec-
ture. These translators can be either knowledge or
example-based. On the other hand, in (Gonza?lez
and Casacuberta, 2006) an example based technique
consisting of a single SFST that cope with multiple
target languages was presented. In that approach,
when translating an input sentence, only one search
through the multi-target SFST is required, instead of
the m independent decoding processes required by
the mono-target translators.
The classical layout for speech-input multi-target
translation includes a speech recognition system in
a serial architecture with m decoupled text-to-text
translators. Thus, this architecture entails a decod-
ing stage of the speech signal into the source lan-
guage text, and m further decoding stages to trans-
late the source text into each of the m target lan-
56
guages. If we supplant the m translators with the
multi-target SFST, the problem would be reduced to
2 searching stages. Nevertheless, in this paper we
propose a natural way for acoustic models to be in-
tegrated in the multilingual network itself, in such
a way that the input speech signal can be simulta-
neously decoded and translated into m target lan-
guages. As a result, due to the fact that there is just
a single searching stage, this novel approach entails
less computational cost.
The remainder of the present paper is structured
as follows: section 2 describes both multi-target SF-
STs and the inference algorithm from training ex-
amples; in section 3 a novel integrated architecture
for speech-input multi-target translation is proposed;
section 4 presents a practical application of these
methods, including the experimental setup and the
results they produced; finally, section 5 summarizes
the main conclusions of this work.
2 Multi-target stochastic finite-state
transducers
A multi-target SFST is a generalization of standard
SFSTs, in such a way that every input string in the
source language results in a tuple of output strings
each being associated to a different target language.
2.1 Definition
A multi-target stochastic finite-state transducer is a
tuple T = ??,?1 . . .?m, Q, q0, R, F, P ?, where:
? is a finite set of input symbols (source vocabu-
lary);
?1 . . .?m are m finite sets of output symbols (tar-
get vocabularies);
Q is a finite set of states;
q0 ? Q is the initial state;
R ? Q?????1 . . .?
?
m?Q is a set of transitions
such as (q, w, p?1, . . . , p?m, q?), which is a tran-
sition from the state q to the state q?, with the
source symbol w and producing the substrings
(p?1, . . . , p?m);
P : R ? [0, 1] is the transition probability distri-
bution;
F : Q ? [0, 1] is the final state probability distri-
bution;
The probability distributions satisfy the stochastic
constraint:
?q ? Q (1)
F (q)+
?
w,p?1,...,p?m,q?
P (q, w, p?1, . . . , p?m, q?) = 1
2.2 Training the multilingual translation model
Both topology and parameters of an SFST can
be learned fully automatically from bilingual ex-
amples making use of underlying alignment mod-
els (Casacuberta and Vidal, 2004). Furthermore,
a multi-target SFST can be inferred from a multi-
lingual set of samples (Gonza?lez and Casacuberta,
2006). Even though in realistic situations multilin-
gual corpora are too scarce, recent works (Popovic?
et al, 2005) show that bilingual corpora covering the
same domain are sufficient to obtain generalized cor-
pora based on which one can subsequently create the
required collections of aligned tuples.
The inference algorithm, GIAMTI (grammatical
inference and alignments for multi-target transducer
inference), requires a multilingual corpus, that is, a
finite set of multilingual samples (s, t1, . . . , tm) ?
?????1?? ? ???
?
m, where ti denotes the translation
of the source sentence s into the i-th target language;
? denotes the source language vocabulary, and ?i
the i-th target language vocabulary; the algorithm
can be outlined as follows:
1. Each multilingual sample is transformed into
a single string from an extended vocabulary
(? ? ? ? ??1 ? ? ? ? ? ?
?
m) using a labeling
function (Lm). This transformation searches an
adequate monotonic segmentation for each of
the m source-target language pairs on the basis
of bilingual alignments such as those given by
GIZA++ (Och, 2000). A monotonic segmen-
tation copes with monotonic alignments, that
is, j < k ? aj < ak following the notation
of (Brown et al, 1993). Each source token,
which can be either a word or a phrase (Pe?rez
et al, 2007), is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a token from the source language plus zero
57
Alignment #0
0:tenperatura
1:minimoa
2:jeitsiko
3:da
0:
te
mp
er
at
ur
as
1:
mi
ni
ma
s
2:
en
3:
de
sc
en
so
(a) Spanish-Basque
Alignment #0
0:low
1:temperatures
2:falling
0:
te
mp
er
at
ur
as
1:
mi
ni
ma
s
2:
en
3:
de
sc
en
so
(b) Spanish-English
0 1temperaturas | temperatura | NIL 2maximas | maximoak | high temperaturesminimas | minimoak | low temperatures 3en | NIL | NIL 5descenso | jaitsiko da | fallingascenso | igoko da | rising
(c) Multi-target SFST from Spanish into English and Basque.
Figure 1: Example of a trilingual alignment over a trilingual sentence extracted from the task under consid-
eration;the related multi-target SFST (with Spanish as input, and English and Basque as output).
or more words from each target language in
their turn.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z ? ??), a stochastic regular grammar can be
inferred. Specifically, in this work we deal with
k-testable in the string-sense grammars (Garc??a
and Vidal, 1990), which are considered to be
a syntactic approach of the n-gram models. In
addition, they allow the integration of several
order models in a single smoothed automa-
ton (Torres and Varona, 2001).
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input token and m output phrases
(w/p?1| . . . |p?m) by the inverse labeling function
(L?m), leading to the required transducer.
Example An illustration of the inference of the
multi-target SFST can be shown over a couple of
simple trilingual sentences from the corpus (where
?B? stands for Basque, ?S? for Spanish and ?E? for
English):
1-B tenperatura maximoa jaitsiko da
1-S temperaturas ma?ximas en descenso
1-E high temperatures falling
2-B tenperatura minimoa igoko da
2-S temperaturas m??nimas en ascenso
2-E low temperatures rising
From the alignments, depicted in Figures 1(a)
and 1(b), an input-language-synchronized
monotonous segmentation can be built (bear in
mind that we are considering Spanish as the input
language). The corresponding extended strings with
the following constituents for the first and second
samples respectively are the following ones:
1 temperaturas|tenperatura|?
m??nimas|minimoa|low temperatures
en|?|?
descenso|jaitsiko da|falling
58
2 temperaturas|tenperatura|?
ma?ximas|maximoa|high temperatures
en|?|?
ascenso|igoko da|rising
Finally, from this representation of the data, the
multi-target SFST can be built as shown in Fig-
ure 1(c).
2.3 Decoding
Given an input string s (a sentence in the source lan-
guage), the decoding module has to search the opti-
mal m output strings tm ? ??1 ? ? ? ? ??
?
m (a sen-
tence in each of the target language) according to the
underlying translation model (T ):
t?m = arg max
tm???1??????
?
m
PT (s, t
m) (2)
Solving equation (2) is a hard computational prob-
lem, however, it can be efficiently computed under
the so called maximum approach as follows:
PT (s, t
m) ? max
?(s,tm)
PT (?(s, t
m)) (3)
where ?(s, tm) is a translation form, that is, a se-
quence of transitions in the multi-target SFST com-
patible with both the input and the m output strings.
?(s, tm) : (q0, w1, p?m1 , q1) ? ? ? (qJ?1, wJ , p?
m
J , qJ)
The input string (s) is a sequence of J input sym-
bols, s = wJ1 , and each of the m output strings
consists of J phrases in its corresponding language
tm = (t1, ? ? ? , tm) = (p?1)J1 , ? ? ? , (p?m)
J
1 . Thus, the
probability supplied by the multi-target SFST to the
translation form is given by:
PT (?(s, t
m)) = F (qJ)
J?
j=1
P (qj?1, wj , p?
m
j , qj)
(4)
In this context, the Viterbi algorithm can be used
to obtain the optimal sequence of states through the
multi-target SFST for a given input string. As a
result, the established m translations are built con-
catenating the (J) output phrases for each language
through the optimal path.
3 An embedded architecture for
speech-input multi-target translation
3.1 Statistical framework
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i ? {1, . . . ,m}). This approach is summarized
in eq. (5), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
t?m = arg max
tm
P (tm|x) = arg max
tm
?
s
P (tm, s|x)
(5)
Making use of Bayes? rule, the former expression
turns into:
t?m = arg max
tm
?
s
P (tm, s)P (x|tm, s) (6)
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string, i.e. P (x|tm, s) is inde-
pendent of tm. In this sense, eq. (6) can be rewritten
as:
t?m = arg max
tm
?
s
P (tm, s)P (x|s) (7)
Equation (7) combines a standard acoustic model,
P (x|s), and a multi-target translation model,
P (tm, s), both of whom can be integrated on the fly
during the searching routine as shown in Figure 2.
That is, each acoustic sub-network is only expanded
at decoding time when it is required.
The outer sum is computationally very expensive
to search for the optimal tuple of target strings tm
in an effective way. Thus we make use of the so
called Viterbi approximation, which finds the best
path over the whole transducer.
3.2 Practical issues
The underlying recognizer used in this work is our
own continuous-speech recognition system, which
implements stochastic finite-state models at all lev-
els: acoustic-phonetic, lexical and syntactic, and
which allows to infer them based on samples.
The signal analysis was carried out in a stan-
dard way, based on the classical Mel-cepstrum
parametrization. Each phone-like unit was modeled
59
1 /e/ | NIL | NIL 2/n/ | NIL | NIL
Figure 2: Integration on the fly of acoustic models in one edge of the SFST shown in Figure 1(c)
by a typical left to right hidden Markov model. A
phonetically-balanced Spanish database, called Al-
bayzin (Moreno et al, 1993), was used to train these
models.
The lexical model consisted of the extended to-
kens of the multi-target SFST instead of running
words. The acoustic transcription for each extended
token was automatically obtained on the basis of the
input projection of each unit, that is, the Spanish vo-
cabulary in this case.
Instead of the usual language model, we make use
of the multi-target SFST itself, which had the syn-
tactic structure provided by a k-testable in the strict
sense model, with k=3, and Witten-Bell smoothing.
Note that the SFST implicitly involves both input
and output language models.
4 Experimental results
4.1 Task and corpus
The described general methodology has been put
into practice in a highly practical application that
aims to translate on-line TV weather forecasts into
several languages, taking the speech of the presen-
ter as the input and producing as output text-strings,
or sub-titles, in several languages. For this purpose,
we used the corpus METEUS which consists of a
set of trilingual sentences, in English, Spanish and
Basque, as extracted from weather forecast reports
that had been published on the Internet. Let us no-
tice that it is a real trilingual corpus, which they are
usually quite scarce.
Basque is a pre-Indoeuropean language of still
unknown origin. It is a minority language, spo-
ken in a small area of Europe and also within some
small American communities (such as that in Reno,
Nevada). In the Basque Country (located in the
north of Spain) it has an official status along with
Spanish. However, despite having coexisted for cen-
turies in the same area, they differ greatly both in
syntax and in semantics. Hence, efforts are being
devoted nowadays to machine translation tools in-
volving these two languages (Alegria et al, 2004),
although they are still scarce. With regard to the or-
der of the phrases within a sentence, the most com-
mon one in Basque is Subject plus Objects plus Verb
(even though some alternative structures are also ac-
cepted), whereas in Spanish and English other con-
structions such as Subject plus Verb plus Objects are
more frequent (see Figures 1(a) and 1(b)). Another
difference between Basque and Spanish or English
is that Basque is an extremely inflected language.
In this experiment we intend to translate Span-
ish speech simultaneously into both Basque and En-
glish. Just by having a look at the main features of
the corpus in Table 1, we can realize that there are
substantial differences among these three languages,
in terms both of the size of the vocabulary and of the
amount of running words. These figures reveal the
agglutinant nature of the Basque language in com-
parison with English or Spanish.
Spanish Basque English
T
ra
in
in
g Total sentences 14,615
Different sentences 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Te
st
Sentences 500
Words 8,706 8,274 9,150
Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
Table 1: Main features of the METEUS corpus.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
60
independent if it is to be realistic.
4.2 System evaluation
The performance obtained by the acoustic integra-
tion has been experimentally tested for both multi-
target and mono-target devices. As a matter of com-
parison, text-input translation results are also re-
ported.
The multi-target SFST was learned from the train-
ing set described in Table 1 using the previously de-
scribed GIAMTI algorithm. The 500 test sentences
were then translated by the multi-target SFST. The
translation provided by the system in each language
was compared to the corresponding reference sen-
tence. Additionally, two mono-target SFSTs were
inferred with their outputs for the aforementioned
test to be taken as baseline. The evaluation includes
both computational cost and performance of the sys-
tem.
4.2.1 Computational cost
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
input machine translation applications. These val-
ues can be objectively measured in terms of the size
and on the average branching factor of the model
displayed in Table 2.
multi-target mono-targetS2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Branching factor 3.30 3.13 3.46
Table 2: Features of multi-target model and the two
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the edges up for the two mono-target SF-
STs that take part in the decoupled architecture (see
Table 2), we conclude that the decoupled model
needs a total of 185, 216 edges to be allocated in
memory, which represents an increment of 13%
in memory-space with respect to the multi-target
model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which suggests that search-
ing for a translation might be faster. As a matter of
fact, experimental results in Table 3 show that the
mono-target architecture works 11% more slowly
than the multi-target one for speech-input machine
translation and decoding, and 30% for text to text
translation.
Time (s)
multi-target mono-targetS2B+S2E
Text-input 0.36 0.47
Speech-input 16.9 18.9
Table 3: Average time needed to translate each input
sentence into two languages.
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems has been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In or-
der to determine the performance of the system in
a quantitative manner, the following evaluation pa-
rameters were computed for each scenario: bilingual
evaluation under study (BLEU), position indepen-
dent error rate (PER) and word error rate (WER).
Both text and speech-input translation results pro-
vided by the multi-target and the mono-target mod-
els respectively are shown in Table 4.
As can be derived from the translation results,
for text-input translation the classical approach per-
forms slightly better than the multi-target one, but
for speech-input translation from Spanish into En-
glish is the other way around. In any case, the dif-
ferences in performance are marginal.
Comparing the text-input with the speech-input
results we realize that, as could be expected, the pro-
cess of speech signal decoding is itself introducing
some errors. In an attempt to measure these errors,
the text transcription of the recognized input signal
was extracted and compared to the input reference
in terms of WER as shown in the last row of the Ta-
ble 4. Note that even though the input sentences are
the same the three results differ due to the fact that
61
we are making use of different SFST models that de-
code and translate at the same time.
multi-target mono-target
S2B S2E S2B S2E
Te
xt
BLEU 42.7 66.7 43.4 67.8
PER 39.9 19.9 38.2 19.0
WER 48.0 27.5 46.2 26.6
Sp
ee
ch
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
recognition WER 10.7 9.3 9.1
Table 4: Text-input and speech-input translation re-
sults for Spanish into Basque (S2B) and Spanish into
English (S2E) using a multi-target SFST (columns
on the left) or two mono-target SFSTs (columns on
the right). The last row shows Spanish speech de-
coding results using each of the three devices.
In these series of experiments the same task has
been compared with two extremely different lan-
guage pairs under the same conditions. There is a
noticeable difference in terms of quality between the
English and the Basque translations. The underlying
reason might be due to the fact that SFST models
do not capture properly the rich morphology of the
Basque as they have to face long-distance reordering
issues. These differences in the performance of the
system when translating into English or into Basque
have been previously detected in other works (Or-
tiz et al, 2003). In our case, a manual review of the
models and the obtained translations encourage us to
make use of reordering models in future work, since
they have proved to report good results in a similar
framework (Kanthak et al, 2005).
5 Concluding remarks and further work
The main contribution of this paper is the proposal
of a fully embedded architecture for multiple speech
translation. Thus, acoustic models are integrated on
the fly into a multi-target translation model. The
most significant feature of this approach is its abil-
ity to carry out both the recognition and the transla-
tion into multiple languages integrated in a unique
model. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm.
In contrast to the mono-target systems, multi-
target SFSTs enable the translation from one source
language simultaneously into several target lan-
guages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults. Moreover, the integration of several languages
and acoustic models is straightforward on means of
finite-state devices.
Nevertheless, the integrated architecture needs
more parameters to be estimated. In fact, as the
amount of targets increase the data sparseness might
become a difficult problem to cope with. In future
work we intend to make a deeper study on the per-
formance of the multi-target system with regard to
the amount of parameters to be estimated. In ad-
dition, as the first step of the learning algorithm is
decisive, we are planning to make use of reordering
models in an attempt to face up to with long dis-
tance reordering and in order to homogenize all the
languages involved.
Acknowledgments
This work has been partially supported by the Uni-
versity of the Basque Country and by Spanish CI-
CYT under grants 9/UPV 00224.310-15900/2004,
TIC2003-08681-C02-02, and CICYT es TIN2005-
08660-C04-03 respectively.
References
In?aki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,
Koldo Gojenola, and Ruben Urizar. 2004. Repre-
sentation and treatment of multiword expressions in
basque. In Takaaki Tanaka, Aline Villavicencio, Fran-
cis Bond, and Anna Korhonen, editors, Second ACL
Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 48?55, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205?
225.
P. Garc??a and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12(9):920?925.
62
M.T. Gonza?lez and F. Casacuberta. 2006. Multi-Target
Machine Translation using Finite-State Transducers.
In Proceedings of TC-Star Speech to Speech Transla-
tion Workshop, pages 105?110.
John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press,
Cambridge, MA.
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard
Zens, and Hermann Ney. 2005. Novel reordering ap-
proaches in phrase-based statistical machine transla-
tion. In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 167?174, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In 4th AMTA (Association for Ma-
chine Translation in the Americas).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88, January.
A. Moreno, D. Poch, A. Bonafonte, E. Lleida, J. Llisterri,
J. B. Mario, and C. Nadeu. 1993. Albayzin speech
database: Design of the phonetic corpus. In Proc. of
the European Conference on Speech Communications
and Technology (EUROSPEECH), Berl??n, Germany.
Franz J. Och. 2000. GIZA++: Train-
ing of statistical translation models.
http://www.fjoch.com/GIZA++.html.
Daniel Ortiz, Ismael Garc??a-Varea, Francisco Casacu-
berta, Antonio Lagarda, and Jorge Gonza?lez. 2003.
On the use of statistical machine translation techniques
within a memory-based translation system (AME-
TRA). In Proc. of Machine Translation Summit IX,
pages 115?120, New Orleans, USA, September.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431?
453. The MIT Press, Cambridge, Massachusetts.
Alicia Pe?rez, M. Ine?s Torres, and Francisco Casacuberta.
2007. Speech translation with phrase based stochas-
tic finite-state transducers. In Proceedings of the 32nd
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP 2007), Honolulu, Hawaii
USA, April 15-20. IEEE.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran S?aric?. 2005. Augmenting a small
parallel text with morpho-syntactic language. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, pages 41?48, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
M. Ine?s Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127?149.
Enrique Vidal. 1997. Finite-state speech-to-speech
translation. In Proc. IEEE International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 111?114, Munich, Germany, April.
63
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 99?107,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Finite-state acoustic and translation model composition in statistical
speech translation: empirical assessment
Alicia Pe?rez(1), M. Ine?s Torres(2)
(1)Dep. Computer Languages and Systems
(2)Dep. Electricidad y Electro?nica
University of the Basque Country UPV/EHU
Bilbao (Spain)
(1)alicia.perez@ehu.es
(2)manes.torres@ehu.es
Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Valencia (Spain)
fcn@iti.upv.es
Abstract
Speech translation can be tackled by
means of the so-called decoupled ap-
proach: a speech recognition system fol-
lowed by a text translation system. The
major drawback of this two-pass decod-
ing approach lies in the fact that the trans-
lation system has to cope with the er-
rors derived from the speech recognition
system. There is hardly any cooperation
between the acoustic and the translation
knowledge sources. There is a line of re-
search focusing on alternatives to imple-
ment speech translation efficiently: rang-
ing from semi-decoupled to tightly in-
tegrated approaches. The goal of inte-
gration is to make acoustic and transla-
tion models cooperate in the underlying
decision problem. That is, the transla-
tion is built by virtue of the joint ac-
tion of both models. As a side-advantage
of the integrated approaches, the transla-
tion is obtained in a single-pass decod-
ing strategy. The aim of this paper is
to assess the quality of the hypotheses
explored within different speech transla-
tion approaches. Evidence of the perfor-
mance is given through experimental re-
sults on a limited-domain task.
1 Introduction
Statistical speech translation (SST) was typ-
ically implemented as a pair of consecutive
steps in the so-called decoupled approach: with
an automatic speech recognition (ASR) system
placed before to a text-to-text translation sys-
tem. This approach involves two independent
decision processes: first, getting the most likely
string in the source language and next, get-
ting the expected translation into the target lan-
guage. Since the ASR system is not an ideal
device it might make mistakes. Hence, the text
translation system would have to manage with
the transcription errors. Being the translation
models (TMs) trained with positive samples of
well-formed source strings, they are very sensi-
tive to ill-formed strings in the source language.
Hence, it seems ambitious for TMs to aspire to
cope with both well and ill formed sentences in
the source language.
1.1 Related work
Regarding the coupling of acoustic and trans-
lation models, there are some contributions in
the literature that propose the use of semi-
decoupled approaches. On the one hand, in
(Zhang et al, 2004), SST is carried out by
99
an ASR placed before a TM with an addi-
tional stage that would re-score the obtained hy-
potheses within a log-linear framework gather-
ing features from both the ASR system (lexicon
and language model) and the TM (eg. distor-
tion, fertility) and also additional features (POS,
length etc.).
On the other hand, in (Quan et al, 2005), the
N-best hypotheses derived from an ASR sys-
tem were next translated by a TM, finally, a last
stage would re-score the hypotheses and make
a choice. Within the list of the N-best hypothe-
ses typically a number of them include some n-
grams that are identical, hence, the list results to
be an inefficient means of storing data. Alterna-
tively, in (Zhou et al, 2007) the search space
extracted from the ASR system, represented as
a word-graph (WG), was next explored by a TM
following a multilayer search algorithm.
Still, a further approach can be assumed
in order to make the graph-decoding com-
putationally cheaper, that is, confusion net-
works (Bertoldi et al, 2007). Confusion-
networks implement a linear approach of the
word-graphs, however, as a result, dummy hy-
potheses might be introduced and probabili-
ties mis-computed. Confusion networks traded
off between the accuracy and storage ability of
word-graphs for decoding time. Indeed, in (Ma-
tusov and Ney, 2011) an efficient means of do-
ing the decoding with confusion networks was
presented. Note that these approaches follow a
two-pass decoding strategy.
The aforementioned approaches imple-
mented phrase-based TMs within a log-linear
framework. In this context, in (Casacuberta
et al, 2008) a fully integrated approach was
examined. Under this approach, the translation
was carried out in a single-pass decoding,
involving a single decision process in which
acoustic and translations models cooperated.
This integration paradigm, was earlier proposed
in (Vidal, 1997), showing that a single-pass
decoding was enough to carry out SST.
Finally, in (Pe?rez et al, 2010) several SST de-
coding approaches including decoupled, N-best
lists and integrated were compared. Neverthe-
less, the paper focused on the potential scope of
the approaches, comparing the theoretical upper
threshold of their performance.
1.2 Contribution
All the models assessed in this work relay upon
exactly the same acoustic and translation mod-
els. It is the combination of them on which
we are focusing. In brief, the aim of this pa-
per is to compare different approaches to carry
out speech translation decoding. The compari-
son is carried out using exactly the same under-
lying acoustic and translation models in order
to allow to make a fair comparison of the abil-
ities inherent to the decoding strategy. Apart
from the decoupled and semi-decoupled strate-
gies we also focus on the fully-integrated ap-
proach. While the fully integrated approach al-
lows to provide the most-likely hypothesis, we
explored a variant: an integrated architecture
with a re-scoring LM that provided alternatives
derived from the integrated approach and used
re-scoring to make the final decision. Not only
an oracle-evaluation is provided as an upper-
threshold of the experiments but also an experi-
mental set-up to give empirical evidence.
The paper is arranged as follows: Section 2
introduces the formulation of statistical speech
translation (SST); Section 3 describes differ-
ent approaches to put into practice SST, plac-
ing emphasis on the assumptions behind each
of them. Section 4 is devoted to assess experi-
mentally the performance of each approach. Fi-
nally, in Section 5 the concussions drawn from
the experiments are summarized.
100
2 Statistical speech translation
The goal of speech translation, formulated un-
der the probabilistic framework, is to find the
most likely string in the target language (?t)
given the spoken utterance in the source lan-
guage. Speech signal in the source language
is characterized in terms of an array of acoustic
features in the source language, x. The decision
problem involved is formulated as follows:
t? = arg max
t
P (t|x) (1)
In this context, the text transcription in the
source language (denoted as s) is introduced as
a hidden variable and Bayes? rule applied:
t? = arg max
t
?
s
P (x|s, t)P (s, t) (2)
Assuming P (x|s, t) ? P (x|s), and using the
maximum term involved in the sum as an ap-
proach to the sum itself for the sake of compu-
tational affordability, we yield to:
t? ? arg max
t
max
s
P (x|s)P (s, t) (3)
As a result, the expected translation is built
relying upon both a translation model (P (s, t))
and an acoustic model in the source language
(P (x|s)). This approach requires the joint co-
operation of both models to implement the de-
cision problem since the maximum over s con-
cerns both of them.
2.1 Involved models
Being the goal of this paper to compare differ-
ent techniques to combine acoustic and trans-
lation models, it is important to keep constant
the underlying models while varying the strate-
gies to combine them. Before to delve into the
composition strategies and due to the fact that
some combination strategies are based on the
finite-state topology of the models, a summary
of the relevant features of the underlying mod-
els is given in this section.
2.1.1 Translation model
The translation model used in this work
to tackle all the approaches consists of a
stochastic finite-state transducer (SFST) en-
compassing phrases in the source and tar-
get languages together with a probability of
joint occurrence. The SFST (T ) is a tuple
T = ??,?, Q, q0, R, F, P ?, where:
? is a finite set of input symbols;
? is a finite set of output symbols;
Q is a finite set of states;
q0 ? Q is the initial state;
R ? Q ? ?+ ? ?? ? Q is a set of transi-
tions. (q, s?, t?, q?) ? R, represents a tran-
sition from the state q ? Q to the state
q? ? Q, with the source phrase s? ? ?+ and
producing the substring t? ? ??, where t?
might consist of zero or more target words
(|t?| ? 0);
F : Q? [0, 1] is a final state probability;
P : R? [0, 1] is a transition probability;
Subject to the stochastic constraint:
?q ? Q F (q) +
?
s?,t?,q?
P (q, s?, t?, q?) = 1 (4)
For further reading on formulation and prop-
erties of these machines turn to (Vidal et al,
2005).
The SFST can be understood as a statistical
bi-language implemented by means of finite-
state regular grammar (Casacuberta and Vidal,
2004) (in the same way as a stochastic finite-
state automaton can be used to model a sin-
gle language): A = ??, Q, q0, R, F, P ?, being
? ? ?+ ??? a finite-set of bilingual-phrases.
Likewise, bilingual n-gram models can be in-
ferred in practice (Marin?o et al, 2006).
101
2.1.2 Acoustic models
The acoustic model consists of a mapping of
text-transcriptions of lexical units in the source
language and their acoustic representation. That
comprises the composition of: 1) a lexical
model consisting of a mapping between the tex-
tual representation with their phone-like repre-
sentation in terms of a left-to-right sequence;
and 2) an inventory of phone-like units con-
sists of a typical three-state hidden Markov
model (Rabiner, 1989). Thus, acoustic model
lays on the composition of two finite-state mod-
els (depicted in Figure 1).
/T/ /j/ /e/ /l/ /o/
cielo
(a) Phonetic representation of a text lexical unit
/T/
/j/
/e/
(b) HMM phone-like units
Figure 1: Acoustic model requires composing
phone-like units within phonetic representation
of lexical units.
3 Decoding strategies
In the previous section the formulation of SST
was summarized. Let us now turn into prac-
tice and show the different strategies explored
to combine acoustic and translation models to
tackle SST. The approaches accounted are: de-
coupled, semi-decoupled and integrated archi-
tectures. While the former two are imple-
mentable by virtue of alternative TMs, the latter
is achieved thanks to the integration allowed by
finite-state framework. Thus, in order to com-
pare the combination rather than the TMs them-
selves, all of the combinations shall be put in
practice using the same SFST as TM.
3.1 Decoupled approach
Possibly the most widely used approach to
tackle speech translation is the so-called serial,
cascade or decoupled approach. It consists of
a text-to-text translation system placed after an
ASR system. This process is formally stated as:
t? ? arg max
t
max
s
P (x|s)P (s)P (t|s) (5)
In practice, previous expression is imple-
mented in two independent stages as follows:
1st stage: an ASR system would find the
most likely transcription (?s):
s? ? arg max
s
P (x|s)P (s) (6)
2nd stage next, given the expected string in
the source language (?s), a TM would find the
most likely translation:
t? ? arg max
t
P (t|?s) = arg max
t
P (?s, t) (7)
The TM involved in eq.(7) can be based on
either posterior or joint-probability as the dif-
ference between both of them is a normaliza-
tion term that does not intervene in the maxi-
mization process. The second stage has to cope
with expected transcription of speech (?s) which
does not necessarily convey the exact reference
source string (s). That is, the ASR might intro-
duce errors in the source string to be translated
in the next stage. However, the TMs are typ-
ically trained with correct source-target pairs.
Thus, transcription errors are seldom foreseen
even in models including smoothing (Martin et
al., 1999). In addition, TMs are extremely sen-
sitive to the errors in the input, in particular to
substitutions (Vilar et al, 2006).
This architecture represents a suboptimal
means of contending with SST as referred in
eq. (3). This approach barely takes advantage of
the involved knowledge sources, namely, acous-
tic and translation models.
102
3.2 Semi-Decoupled approach
Occasionally, the most probable translation
does not result to be the most accurate one with
respect to a given reference. That is, it might
happen that hypotheses with a slightly lower
probability than that of the expected hypothesis
turn to be more similar to the reference than the
expected hypothesis. This happens due to sev-
eral factors, amongst others, due to the sparsity
of the data with which the model was trained.
In brief, some sort of disparity between the
probability of the hypotheses and their quality
might arise in practice. The semi-decoupled ap-
proach arose to address this issue. Hence, rather
than translating a single transcription hypothe-
sis, a number of them are provided by the ASR
to the TM, and it is the latter that makes the de-
cision giving as a result the most likely transla-
tion. The decoupled approach is implemented
in two steps, and so is it the semi-decoupled ap-
proach. Details on the process are as follows:
1st stage: for a given utterance in the source
language, an ASR system, laying on source
acoustic model and source language model
(LM), would provide a search sub-space. This
sub-space is traced in the search process for the
most likely transcription of speech but without
getting rid of other highly probable hypotheses.
For what us concern, this sub-space is rep-
resented in terms of a graph of words in the
source language (S). The word-graph gath-
ers the hypotheses with a probability within a
threshold with respect to the optimal hypothesis
at each time-frame as it was formulated in (Ney
et al, 1997). The obtained graph is an acyclic
directed graph where the nodes are associated
with word-prefixes of a variable length, and the
edges join the word sequences allowed in the
recognition process with an associated recogni-
tion probability. The edges consist of the acous-
tic and language model probabilities as the ASR
system handles throughout the trellis.
2nd stage: translating the hypotheses within
S (the graph derived in the 1st stage) allows to
take into account alternative translations for the
given spoken utterance. The searching space
being explored is limited by the source strings
conveyed by S . The combination of the recog-
nition probability with the translation probabil-
ity results in a score that accounts both recogni-
tion and translation likelihood:
t? ? arg max
t
max
s?S
P (s)P (s, t) (8)
Thus, acoustic and translation models would
one re-score the other.
All in all, this semi-decoupled approach re-
sults in an extension of the decoupled one.
It accounts alternative transcriptions of speech
in an attempt to get good quality transcrip-
tions (rather than the most probable transcrip-
tion as in the case of the decoupled approach).
Amongst all the transcriptions, those with high
quality are expected to provide the best quality
in the target language. That is, by avoiding er-
rors derived from the transcription process, the
TM should perform better, and thus get transla-
tions of higher quality. Note that finally, a single
translation hypothesis is selected. To do so, the
highest combined probability is accounted.
3.3 Fully-integrated approach
Finite-state framework (by contrast to other
frameworks) makes a tight composition of mod-
els possible. In our case, of acoustic and trans-
lation finite-state models. The fully-integrated
approach, proposed in (Vidal, 1997), encfom-
passed acoustic and translation models within a
single model. To develop the fully-integrated
approach a finite-state acoustic model on the
source language (A) providing the text tran-
scription of a given acoustic utterance (A :
103
X ? S) can be composed with a text transla-
tion model (T ) that provides the translation of a
given text in the source language (T : S ? T )
and give as a result a transducer (Z = A ? T )
that would render acoustic utterances in the
source language to strings in the target lan-
guage. For the sake of efficiency in terms of
spatial cost, the models are integrated on-the-fly
in the same manner as it is done in ASR (Ca-
seiro and Trancoso, 2006).
The way in which integrated architecture ap-
proaches eq. (3) is looking for the most-likely
source-target translation pair as follows:
?(s, t) = arg max
(s,t)
P (s, t)P (x|s) (9)
That is, the search is driven by bilingual phrases
made up of acoustic elements in the source
language integrated within bilingual phrases of
words together with target phrases.
Then, the expected translation would simply
be approached as the target projection of ?(s, t),
the expected source-target string (also known
as the lower projection); and likewise, the ex-
pected transcription is obtained as a side-result
by the source projection (aka upper projection).
It is well-worth mentioning that this approach
implements fairly the eq. (3) without further
assumptions rather than those made in the de-
coding stage such as Viterbi-like decoding with
beam-search. All in all, acoustic and translation
models cooperate to find the expected transla-
tion. Moreover, it is carried out in a single-pass
decoding strategy by contrast to either decou-
pled or semi-decoupled approaches.
3.4 Integrated WG and re-scoring LM
The fully-integrated approach looks for the
single-best hypothesis within the integrated
acoustic-and-translation network. Following
the reasoning of Section 3.2, the most likely
path together with other locally close paths in
the integrated searching space can be extracted
and arranged in terms of a word graph. While
the WG derived in Section 3.2 was in source
language, this one would be bilingual.
Given a bilingual WG, the lower-side net
(WG.l) can be extracted keeping the topol-
ogy and the associated probability distributions
while getting rid of the input string of each tran-
sition, this gives as a result the projection of
the WG in the target language. Next, a target
language model (LM) would help to make the
choice for the most likely hypothesis amongst
those in the WG.l.
t? ? arg max
t
PWG.l(t)PLM (t) (10)
In other words, while in Section 3.2 the trans-
lation model was used to re-score alternative
transcriptions of speech whereas in this ap-
proach a target language models re-scores al-
ternative translations provided by the bilingual
WG. Note that this approach, as well as the
semi-decoupled one, entail a two-pass decoding
strategy. Both rely upon two models: the for-
mer focused on the source language WG, this
one focuses on the target language WG.
4 Experiments
The aim of this section is to assess empir-
ically the performance each of the four ap-
proaches previously introduced: decoupled,
semi-decoupled, fully-integrated and integrated
WG with re-scoring LM. The four approaches
differ on the decoding strategy implemented to
sort out the decision problem, but all of them
rely on the very same knowledge sources (that
is, the same acoustic and translation model).
The main features of the corpus used to carry
out the experimental layout are summarized in
Table 1. The training set was used to infer the
104
TM consisting of an SFST and the test set to as-
sess the SST decoding approaches. The test set
consisted of 500 training-independent pairs dif-
ferent each other, each of them was uttered by
at least 3 speakers.
Spanish Basque
Tr
ain Sentences 15,000Running words 191,000 187,000
Vocabulary 702 1,135
Te
st Sentences 1,800
Hours of speech 3.0 3.5
Table 1: Main features of the Meteus corpus.
The performance of each experiment is as-
sessed through well-known evaluation met-
rics, namely: bilingual evaluation under-study
(BLEU) (Papineni et al, 2002), word error-rate
(WER), translation edit rate (TER).
4.1 Results
The obtained results are given in Table 2. The
performance of the most-likely or single-best
translation derived by either decoupled or fully-
integrated architectures is shown in the first row
of Tables 2a and 2b respectively. The per-
formance of the semi-decoupled and integrated
WG with re-scoring LM is shown in the sec-
ond row. The highest performance achievable
by both the semi decoupled approach and the
integrated WG with re-scoring LM is given in
the third row. To do so, an oracle evaluation of
the alternatives was carried out and the score as-
sociated to the best choice achievable was given
as in (Pe?rez et al, 2010). Since the oracle evalu-
ation provides an upper threshold of the quality
achievable, the scope of each decoupled or in-
tegrated approaches can be assessed regardless
of the underlying decoding algorithms and ap-
proaches. The highest performance achievable
is reflected in the last row of Tables 2a and 2b.
4.2 Discussion
While the results with two-pass decoding strate-
gies (either decoupled or semi-decoupled ap-
proach) require an ASR engine, integrated ap-
proaches have the ability to get both the source
string together with its translation. This is why
we have make a distinction between ASR-WER
in the former and source-WER in the latter.
Nevertheless, our aim focuses on translation
rather than on recognition.
The results show that semi-decoupled ap-
proach outperforms the decoupled one. Simi-
larly, the approach based on the integrated WG
with the re-scoring target LM outperforms the
integrated approach. As a result, exploring dif-
ferent hypotheses and making the selection with
a second model allows to make refined deci-
sions. On the other hand, comparing the first
row of the Table 2a with the first row of the Ta-
ble 2b (or equally the second row of the former
with the second row of the latter), we conclude
that slightly better performance can be obtained
with the integrated approach.
Finally, comparing the third row of both Ta-
ble 2a and Table 2b, the conclusion is that the
eventual quality of the hypotheses within the in-
tegrated approach are significantly better than
those in the semi-decoupled approaches. That
is, what we can learn is that the integrated de-
coding strategy keeps much better hypotheses
than the semi-decoupled one throughout the de-
coding process. Still, while good quality hy-
potheses exist within the integrated approach,
the re-scoring with a target LM used to select
a single hypothesis from the entire network has
not resulted in getting the best possible hypoth-
esis. Oracle evaluation shows that the integrated
approach offers a leeway to achieve improve-
ments in the quality, yet, alternative strategies
have to be explored.
105
ASR target
WER BLEU WER TER
D 1-best 7.9 40.8 50.3 47.7
SD 7.9 42.2 47.6 44.7
SD tgt-oracle 7.5 57.6 36.2 32.8
(a) Decoupled and semi-decoupled
source target
WER BLEU WER TER
I 1-best 9.6 40.9 49.6 46.8
I WG + LM 9.3 42.6 46.7 43.9
I tgt-oracle 6.6 64.0 32.2 28.5
(b) Integrated and integrated WG with LM
Table 2: Assessment of SST approaches decoupled (2a) and integrated (2b) respectively.
5 Conclusions
Different approaches to cope with the SST de-
coding methodology were explored, namely,
decoupled approach, semi-decoupled approach,
fully-integrated approach and integrated ap-
proach with a re-scoring LM. The first two fol-
low a two-pass decoding strategy and focus on
exploring alternatives in the source language;
while the integrated one follows a single-pass
decoding and present tight cooperation between
acoustic and translation models.
All the experimental layouts used exactly the
same translation and acoustic models differing
only on the methodology used to overcome the
decision problem. In this way, we can assert
that the differences lay on the decoding strate-
gies rather than on the models themselves. Note
that implementing all the models in terms of
finite-state models allows to build both decou-
pled and integrated approaches.
Both decoupled and integrated decoding ap-
proaches aim at finding the most-likely transla-
tion under different assumptions. Occasionally,
the most probable translation does not result to
be the most accurate one with respect to a given
reference. On account of this, we turned to ana-
lyzing alternatives and making use of re-scoring
techniques on both approaches in an attempt
to make the most accurate hypothesis emerge.
This resulted in semi-decoupled and integrated-
WG with re-scoring target LM approaches.
What we can learn from the experiments
is that integrating the models allow to keep
good quality hypotheses in the decoding pro-
cess. Nevertheless, the re-scoring model has
not resulted in being able to make the most of
the integrated approach. In other words, there
are better quality hypotheses within the word-
graph rather than that selected by the re-scoring
target LM. Hence, further work should be fo-
cused on other means of selecting hypotheses
from the integrated word-graph.
However, undoubtedly significantly better
performance can be reached from the inte-
grated decoding strategy than from the semi-
decoupled one. It seems as though knowledge
sources modeling the syntactic differences be-
tween source and target languages should be
tackled in order to improve the performance,
particularly in our case, a strategy for further
work could go on the line of the recently tack-
led approach (Durrani et al, 2011).
Acknowledgments
This work was partially funded by the
Spanish Ministry of Science and Innovation:
through the T??mpano (TIN2011-28169-C05-04)
and iTrans2 (TIN2009-14511) projects; also
through MIPRCV (CSD2007-00018) project
within the Consolider-Ingenio 2010 program;
by the Basque Government to PR&ST research
group (GIC10/158, IT375-10), and by the Gen-
eralitat Valenciana under grants ALMPR (Prom-
eteo/2009/01) and GV/2010/067.
106
References
[Bertoldi et al2007] N. Bertoldi, R. Zens, and M.
Federico. 2008. Efficient speech translation by
confusion network decoding. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pg. 1696?1705
[Casacuberta and Vidal2004] F. Casacuberta and E.
Vidal. 2004. Machine translation with in-
ferred stochastic finite-state transducers. Compu-
tational Linguistics, 30(2): pg. 205?225.
[Casacuberta et al2008] F. Casacuberta, M. Fed-
erico, H. Ney, and E. Vidal. 2008. Recent efforts
in spoken language translation. IEEE Signal Pro-
cessing Magazine, 25(3): pg. 80?88.
[Caseiro and Trancoso2006] D. Caseiro and I. Tran-
coso. 2006. A specialized on-the-fly algo-
rithm for lexicon and language model composi-
tion. IEEE Transactions on Audio, Speech &
Language Processing, 14(4): pg. 1281?1291.
[Durrani et al2011] N. Durrani, H. Schmid, and A.
Fraser. 2011. A joint sequence translation model
with integrated reordering. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pg. 1045?
1054
[Marin?o et al2006] J. B. Marin?o, R. E. Banchs, J. M.
Crego, A. de Gispert, P. Lambert, J. A. R. Fonol-
losa, and M. R. Costa-jussa`. 2006. N-gram-
based machine translation. Computational Lin-
guistics, 32(4): pg. 527?549
[Martin et al1999] S. C. Martin, H. Ney, and
J. Zaplo. 1999. Smoothing methods in maxi-
mum entropy language modeling. IEEE Interna-
tional Conference on Acoustics, Speech, and Sig-
nal Processing , vol. 1, pg. 545?548
[Matusov and Ney2011] E. Matusov and H. Ney.
2011. Lattice-based ASR-MT interface for
speech translation. IEEE Transactions on Audio,
Speech, and Language Processing, 19(4): pg. 721
?732
[Ney et al1997] H. Ney, S. Ortmanns, and I. Lin-
dam. 1997. Extensions to the word graph method
for large vocabulary continuous speech recogni-
tion. IEEE International Conference on Acous-
tics, Speech, and Signal Processing, vol. 3, pg.
1791 ?1794
[Papineni et al2002] K. Papineni, S. Roukos, T.
Ward, and W.-J. Zhu. 2002. Bleu: a method for
automatic evaluation of machine translation. An-
nual Meeting on Association for Computational
Linguistics, pg. 311?318
[Pe?rez et al2010] A. Pe?rez, M. I. Torres, and F.
Casacuberta. 2010. Potential scope of a fully-
integrated architecture for speech translation. An-
nual Conference of the European Association for
Machine Translation, pg. 1?8
[Quan et al2005] V. H. Quan, M. Federico, and M.
Cettolo. 2005. Integrated n-best re-ranking for
spoken language translation. European Conver-
ence on Speech Communication and Technology,
Interspeech, pg. 3181?3184.
[Rabiner1989] L.R. Rabiner. 1989. A tutorial on
hidden markov models and selected applications
in speech recognition. Proceedings of the IEEE,
77(2): pg. 257?286
[Vidal et al2005] E. Vidal, F. Thollard, C. de la
Higuera, F. Casacuberta, and R. C. Carrasco.
2005. Probabilistic finite-state machines - part II.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(7): pg. 1026?1039
[Vidal1997] E. Vidal. 1997. Finite-state speech-to-
speech translation. International Conference on
Acoustic, Speech and Signal Processing, vol. 1,
pg. 111?114
[Vilar et al2006] David Vilar, Jia Xu, Luis Fernando
D?Haro, and H. Ney. 2006. Error Analysis of
Machine Translation Output. International Con-
ference on Language Resources and Evaluation,
pg. 697?702
[Zhang et al2004] R. Zhang, G. Kikui, H. Ya-
mamoto, T. Watanabe, F. Soong, and W. K. Lo.
2004. A unified approach in speech-to-speech
translation: integrating features of speech recog-
nition and machine translation. International
Conference on Computational Linguistics, pg.
1168-1174
[Zhou et al2007] B. Zhou, L. Besacier, and Y. Gao.
2007. On efficient coupling of ASR and SMT for
speech translation. IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
vol. 4, pg. 101?104
107
