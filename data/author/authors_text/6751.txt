Proceedings of the ACL Student Research Workshop, pages 43?48,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Using Emoticons to reduce Dependency in
Machine Learning Techniques for Sentiment Classification
Jonathon Read
Department of Informatics
University of Sussex
United Kingdom
j.l.read@sussex.ac.uk
Abstract
Sentiment Classification seeks to identify
a piece of text according to its author?s
general feeling toward their subject, be it
positive or negative. Traditional machine
learning techniques have been applied to
this problem with reasonable success, but
they have been shown to work well only
when there is a good match between the
training and test data with respect to topic.
This paper demonstrates that match with
respect to domain and time is also impor-
tant, and presents preliminary experiments
with training data labeled with emoticons,
which has the potential of being indepen-
dent of domain, topic and time.
1 Introduction
Recent years have seen an increasing amount of re-
search effort expended in the area of understanding
sentiment in textual resources. A sub-topic of this
research is that of Sentiment Classification. That
is, given a problem text, can computational meth-
ods determine if the text is generally positive or gen-
erally negative? Several diverse applications exist
for this potential technology, ranging from the auto-
matic filtering of abusive messages (Spertus, 1997)
to an in-depth analysis of market trends and con-
sumer opinions (Dave et al, 2003). This is a com-
plex and challenging task for a computer to achieve
? consider the difficulties involved in instructing a
computer to recognise sarcasm, for example.
Previous work has shown that traditional text clas-
sification approaches can be quite effective when
applied to the sentiment analysis problem. Models
such as Na??ve Bayes (NB), Maximum Entropy (ME)
and Support Vector Machines (SVM) can determine
the sentiment of texts. Pang et al (2002) used a bag-
of-features framework (based on unigrams and bi-
grams) to train these models from a corpus of movie
reviews labelled as positive or negative. The best ac-
curacy achieved was 82.9%, using an SVM trained
on unigram features. A later study (Pang and Lee,
2004) found that performance increased to 87.2%
when considering only those portions of the text
deemed to be subjective.
However, Engstro?m (2004) showed that the bag-
of-features approach is topic-dependent. A clas-
sifier trained on movie reviews is unlikely to per-
form as well on (for example) reviews of automo-
biles. Turney (2002) noted that the unigram unpre-
dictable might have a positive sentiment in a movie
review (e.g. unpredictable plot), but could be neg-
ative in the review of an automobile (e.g. unpre-
dictable steering). In this paper, we demonstrate
how the models are also domain-dependent ? how
a classifier trained on product reviews is not effective
when evaluating the sentiment of newswire articles,
for example. Furthermore, we show how the models
are temporally-dependent ? how classifiers are bi-
ased by the trends of sentiment apparent during the
time-period represented by the training data.
We propose a novel source of training data based
on the language used in conjunction with emoticons
in Usenet newsgroups. Training a classifier using
this data provides a breadth of features that, while it
43
Testing
FIN M&A MIX
Training
NB FIN 80.3 75.5 74.0
M&A 77.5 75.3 75.8
MIX 70.7 62.9 84.6
SVM FIN 78.8 72.7 68.9
M&A 74.5 75.5 75.5
MIX 72.0 68.9 81.1
Figure 1: Topic dependency in sentiment classification. Ac-
curacies, in percent. Best performance on a test set for each
model is highlighted in bold.
does not perform to the state-of-the-art, could func-
tion independent of domain, topic and time.
2 Dependencies in Sentiment Classification
2.1 Experimental Setup
In this section, we describe experiments we have
carried out to determine the influence of domain,
topic and time on machine learning based sentiment
classification. The experiments use our own imple-
mentation of a Na??ve Bayes classifier and Joachim?s
(1999) SVM light implementation of a Support Vec-
tor Machine classifier. The models were trained us-
ing unigram features, accounting for the presence
of feature types in a document, rather than the fre-
quency, as Pang et al (2002) found that this is the
most effective strategy for sentiment classification.
When training and testing on the same set, the
mean accuracy is determined using three-fold cross-
validation. In each case, we use a paired-sample
t-test over the set of test documents to determine
whether the results produced by one classifier are
statistically significantly better than those from an-
other, at a confidence interval of at least 95%.
2.2 Topic Dependency
Engstro?m (2004) demonstrated how machine-
learning techniques for sentiment classification can
be topic dependent. However, that study focused
on a three-way classification (positive, negative and
neutral). In this paper, for uniformity across differ-
ent data sets, we focus on only positive and negative
sentiment. This experiment also provides an oppor-
tunity to evaluate the Na??ve Bayes classifier as the
previous work used SVMs.
We use subsets of a Newswire dataset (kindly pro-
Testing
Newswire Polarity 1.0
Training
NB Newswire 78.2 57.6
Polarity 1.0 53.2 78.9
SVM Newswire 78.2 63.2
Polarity 1.0 63.6 81.5
Figure 2: Domain dependency in sentiment classification.
Accuracies, in percent. Best performance on a test set for each
model is highlighted in bold.
vided by Roy Lipski of Infonic Ltd.) that relate to
the topics of Finance (FIN), Mergers and Aquisi-
tions (M&A) and a mixture of both topics (MIX).
Each subset contains further subsets of articles of
positive and negative sentiment (selected by inde-
pendent trained annotators), each containing 100
stories. We trained a model on a dataset relating to
one topic and tested that model using the other top-
ics. Figure 1 shows the results of this experiment.
The tendency seems to be that performance in a
given topic is best if the training data is from the
same topic. For example, the Finance-trained SVM
classifier achieved an accuracy of 78.8% against ar-
ticles from Finance, but only 72.7% when predicting
the sentiment of articles from M&A. However, sta-
tistical testing showed that the results are not signifi-
cantly different when training on one topic and test-
ing on another. It is interesting to note, though, that
providing a dataset of mixed topics (the sub-corpus
MIX) does not necessarily reduce topic dependency.
Indeed, the performance of the classifiers suffers a
great deal when training on mixed data (confidence
interval 95%).
2.3 Domain Dependency
We conducted an experiment to compare the ac-
curacy when training a classifier on one domain
(newswire articles or movie reviews from the Polar-
ity 1.0 dataset used by Pang et al (2002)) and testing
on the other domain. In Figure 2, we see a clear in-
dication that models trained on one domain do not
perform as well on another domain. All differences
are significant at a confidence interval of 99.9%.
2.4 Temporal Dependency
To investigate the effect of time on sentiment clas-
sification, we constructed a new set of movie re-
44
Testing
Polarity 1.0 Polarity 2004
Training
NB Polarity 1.0 78.9 71.8
Polarity 2004 63.2 76.5
SVM Polarity 1.0 81.5 77.5
Polarity 2004 76.5 80.8
Figure 3: Temporal dependency in sentiment classification.
Accuracies, in percent. Best performance on a test set for each
model is highlighted in bold.
views, following the same approach used by Pang
et al (2002) when they created the Polarity 1.0
dataset. The data source was the Internet Movie Re-
view Database archive1 of movie reviews. The re-
views were categorised as positive or negative using
automatically extracted ratings. A review was ig-
nored if it was not written in 2003 or 2004 (ensuring
that the review was written after any in the Polar-
ity 1.0 dataset). This procedure yielded a corpus of
716 negative and 2,669 positive reviews. To create
the Polarity 20042 dataset we randomly selected 700
negative reviews and 700 positive reviews, matching
the size and distribution of the Polarity 1.0 dataset.
The next experiment evaluated the performance
of the models first against movie reviews from the
same time-period as the training set and then against
reviews from the other time-period. Figure 3 shows
the resulting accuracies.
These results show that while the models perform
well on reviews from the same time-period as the
training set, they are not so effective on reviews from
other time-periods (confidence interval 95%). It is
also apparent that the Polarity 2004 dataset performs
worse than the Polarity 1.0 dataset (confidence inter-
val 99.9%). A possible reason for this is that Polarity
2004 data is from a much smaller time-period than
that represented by Polarity 1.0.
3 Sentiment Classification using
Emoticons
One way of overcoming the domain, topic and time
problems we have demonstrated above would be to
find a source of much larger and diverse amounts
of general text, annotated for sentiment. Users of
1http://reviews.imdb.com/Reviews/
2The new datasets described in this paper are available at
http://www.sussex.ac.uk/Users/jlr24/data
Glyph Meaning Frequency
:-) smile 3.8739
;-) wink 2.4350
:-( frown 0.4961
:-D wide grin 0.1838
:-P tongue sticking out 0.1357
:-O surprise 0.0171
:-| disappointed 0.0146
:?( crying 0.0093
:-S confused 0.0075
:-@ angry 0.0038
:-$ embarrassed 0.0007
Figure 4: Examples of emoticons and the frequency of usage
observed in Usenet articles, in percent. For example, 2.435% of
downloaded Usenet articles contained a wink emoticon.
electronic methods of communication have devel-
oped visual cues that are associated with emotional
states in an attempt to state the emotion that their text
represents. These have become known as smileys
or emoticons and are glyphs constructed using the
characters available on a standard keyboard, repre-
senting a facial expression of emotion ? see Figure
4 for some examples. When the author of an elec-
tronic communication uses an emoticon, they are ef-
fectively marking up their own text with an emo-
tional state. This marked-up text can be used to train
a sentiment classifier if we assume that a smile in-
dicates generally positive text and a frown indicates
generally negative text.
3.1 Emoticon Corpus Construction
We collected a corpus of text marked-up with emoti-
cons by downloading Usenet newsgroups and saving
an article if it contained an emoticon listed in Figure
4. This process resulted in 766,730 articles being
stored, from 10,682,455 messages in 49,759 news-
groups inspected. Figure 4 also lists the percentage
of documents containing each emoticon type, as ob-
served in the Usenet newsgroups.
We automatically extracted the paragraph(s) con-
taining the emoticon of interest (a smile or a frown)
from each message and removed any superfluous
formatting characters (such as those used to indi-
cate article quotations in message threads). In order
to prevent quoted text from being considered more
than once, any paragraph that began with exactly the
same thirty characters as a previously observed para-
graph was disregarded. Finally, we used the classi-
fier developed by Cavnar and Trenkle (1994) to filter
45
Finance M&A Mixed
NB 46.0 ? 2.1 55.8 ? 3.8 49.0 ? 1.6
SVM 50.3 ? 1.7 57.8 ? 6.5 55.5 ? 2.7
Figure 5: Performance of Emoticon-trained classifier across
topics. Mean accuracies with standard deviation, in percent.
Newswire Polarity 1.0
NB 50.3 ? 2.2 56.8 ? 1.8
SVM 54.4 ? 2.8 54.0 ? 0.8
Figure 6: Performance of Emoticon-trained classifiers across
domains. Mean accuracies with standard deviation, in percent.
out any paragraphs of non-English text. This pro-
cess yielded a corpus of 13,000 article extracts con-
taining frown emoticons. As investigating skew be-
tween positive and negative distributions is outside
the scope of this work, we also extracted 13,000 arti-
cle extracts containing smile emoticons. The dataset
is referred to throughout this paper as Emoticons and
contains 748,685 words.
3.2 Emoticon-trained Sentiment Classification
This section describes how the Emoticons corpus3
was optimised for use as sentiment classification
training data. 2,000 articles containing smiles and
2,000 articles containing frowns were held-out as
optimising test data. We took increasing amounts
of articles from the remaining dataset (from 2,000
to 22,000 in increments of 1,000, an equal number
being taken from the positive and negative sets) as
optimising training data. For each set of training
data we extracted a context of an increasing num-
ber of tokens (from 10 to 1,000 in increments of 10)
both before and in a window4 around the smile or
frown emoticon. The models were trained using this
extracted context and tested on the held-out dataset.
The optimisation process revealed that the best-
performing settings for the Na??ve Bayes classifier
was a window context of 130 tokens taken from the
largest training set of 22,000 articles. Similarly, the
best performance for the SVM classifier was found
using a window context of 150 tokens taken from
3Note that in these experiments the emoticons are used as
anchors from which context is extracted, but are removed from
texts before they are used as training or test data.
4Context taken after an emoticon was also investigated, but
was found to be inferior. This is because approximately two-
thirds of article extracts end in an emoticon so when using after-
context few features are extracted.
Polarity 1.0 Polarity 2004
NB 56.8 ? 1.8 56.7 ? 2.2
SVM 54.0 ? 0.8 57.8 ? 1.8
Figure 7: Performance of Emoticon-trained classifier across
time-periods. Mean accuracies with standard deviation, in per-
cent.
20,000 articles.
The classifiers? performance in predicting the
smiles and frowns of article extracts was verified us-
ing these optimised parameters and ten-fold cross-
validation. The mean accuracy of the Na??ve Bayes
classifier was 61.5%, while the SVM classifier was
70.1%.
Using these same classifiers to predict the senti-
ment of movie reviews in Polarity 1.0 resulted in ac-
curacies of 59.1% (Na??ve Bayes) and 52.1% (SVM).
We repeated the optimisation process using a
held-out set of 100 positive and 100 negative re-
views from the Polarity 1.0 dataset, as it is possi-
ble that this test needs different parameter settings.
This revealed an optimum context of a window of
50 tokens taken from a training set of 21,000 arti-
cles for the Na??ve Bayes classifier. Interestingly, the
optimum context for the SVM classifier appeared to
be a window of only 20 tokens taken from a mere
2,000 training examples. This is clearly an anomaly,
as these parameters resulted in an accuracy of 48.9%
when testing against the reserved reviews of Polarity
1.0. We attribute this to the presence of noise, both
in the training set and in the held-out set, and dis-
cuss this below (Section 4.2). The second-best pa-
rameters according to the optimisation process were
a context of 510 tokens taken before an emoticon,
from a training set of 20,000 examples.
We used these optimised parameters to evaluate
the sentiments of texts in the test sets used to eval-
uate dependency in Section 2. Figures 5, 6 and 7
show the final, optimised results across topics, do-
mains and time-periods respectively. These tables
report the average accuracies over three folds, with
the standard deviation as a measure of error.
4 Discussion
The emoticon-trained classifiers perform well (up to
70% accuracy) when predicting the sentiment of ar-
ticle extracts from the Emoticons dataset, which is
encouraging when one considers the high level of
46
Training Testing Coverage
Polarity 1.0 Polarity 1.0 69.8
(three-fold cross-validation)
Emoticons FIN 54.9
M&A 58.1
MIX 60.2
Newswire 46.1
Polarity 1.0 41.1
Polarity 2004 42.6
Figure 8: Coverage of classifiers, in percent.
noise that is likely to be present in the dataset.
However, they perform only a little better than one
would expect by chance when classifying movie re-
views, and are not effective in predicting the senti-
ment of newswire articles. This is perhaps due to the
nature of the datasets ? one would expect language
to be informal in movie reviews, and even more so
in Usenet articles. In contrast, language in newswire
articles is far more formal. We might therefore infer
a further type of dependence in sentiment classifica-
tion, that of language-style dependency.
Also, note that neither machine-learning model
consistently out-performs the other. We speculate
that this, and the generally mediocre performance of
the classifiers, is due (at least) to two factors; poor
coverage of the features found in the test domains
and a high level of noise found in Usenet article ex-
tracts. We investigate these factors below.
4.1 Coverage
Figure 8 shows the coverage of the Emoticon-trained
classifiers on the various test sets. In these exper-
iments, we are interested in the coverage in terms
of unique token types rather than the frequency of
features, as this more closely reflects the training of
the models (see Section 2.1). The mean coverage
of the Polarity 1.0 dataset during three-fold cross-
validation is also listed as an example of the cov-
erage one would expect from a better-performing
sentiment classifier. The Emoticon-trained classifier
has much worse coverage in the test sets.
We analysed the change in coverage of the
Emoticon-trained classifiers on the Polarity 1.0
dataset. We found that the coverage continued to im-
prove as more training data was provided; the cov-
erage of unique token types was improving by about
0.6% per 1,000 training examples when the Emoti-
 48
 50
 52
 54
 56
 58
 60
 3000
 6000
 9000
 12000
 15000
 18000Training Size  100 200
 300 400
 500 600
 700 800
 900 1000
Context Size
 48
 50
 52
 54
 56
 58
 60
Accuracy (%)
Figure 9: Change in Performance of the SVM Classifier on
held-out reviews from Polarity 1.0, varying training set size and
window context size. The datapoints represent 2,200 experi-
ments in total.
cons dataset was exhausted.
It appears possible that more training data will im-
prove the performance of the Emoticon-trained clas-
sifiers by increasing the coverage. Potential sources
for this include online bulletin boards, chat forums,
and further newsgroup data from Usenet and Google
Groups5. Future work will utilise these sources to
collect more examples of emoticon use and analyse
any improvement in coverage and accuracy.
4.2 Noise in Usenet Article Extracts
The article extracts collected in the Emoticons
dataset may be noisy with respect to sentiment. The
SVM classifier seems particularly affected by this
noise. Figure 9 depicts the change in performance
of the SVM classifier when varying the training set
size and size of context extracted. There are signif-
icant spikes apparent for the training sizes of 2,000,
3,000 and 6,000 article extracts (as noted in Section
3.2), where the accuracy suddenly increases for the
training set size, then quickly decreases for the next
set size. This implies that the classifier is discover-
ing features that are useful in classifying the held-
out set, but the addition of more, noisy, texts soon
makes the information redundant.
Some examples of noise taken from the Emoti-
cons dataset are: mixed sentiment, e.g.
5http://groups.google.com
47
?Sorry about venting my frustration here but I
just lost it. :-( Happy thanks giving everybody
:-)?,
sarcasm, e.g.
?Thank you so much, that?s really encouraging
:-(?,
and spelling mistakes, e.g.
?The movies where for me a major desapoint-
ment :-(?.
In future work we will investigate ways to remove
noisy data from the Emoticons dataset.
5 Conclusions and Future Work
This paper has demonstrated that dependency in sen-
timent classification can take the form of domain,
topic, temporal and language style. One might sup-
pose that dependency is occurring because classi-
fiers are learning the semantic sentiment of texts
rather than the general sentiment of language used.
That is, the classifiers could be learning authors?
sentiment towards named entities (e.g. actors, direc-
tors, companies, etc.). However, this does not seem
to be the case. In a small experiment, we part-of-
speech tagged the Polarity 2004 dataset and auto-
matically replaced proper nouns with placeholders.
Retraining on this modified text did not significantly
affect performance.
But it may be that something more subtle is hap-
pening. Possibly, the classifiers are learning the
words associated with the semantic sentiment of en-
tities. For example, suppose that there has been a
well-received movie about mountaineering. During
this movie, there is a particularly stirring scene in-
volving an ice-axe and most of the reviewers men-
tion this scene. During training, the word ?ice-axe?
would become associated with a positive sentiment,
whereas one would suppose that this word does not
in general express any kind of sentiment.
In future work we will perform further tests to de-
termine the nature of dependency in machine learn-
ing techniques for sentiment classification. One way
of evaluating the ?ice-axe? effect could be to build a
?pseudo-ontology? of the movie reviews ? a map
of the sentiment-bearing relations that would enable
the analysis of the dependencies created by the train-
ing process. Other extensions of this work are to
collect more text marked-up with emoticons, and to
experiment with techniques to automatically remove
noisy examples from the training data.
Acknowledgements
This research was funded by a UK EPSRC stu-
dentship. I am very grateful to Thorsten Joachims,
Roy Lipski, Bo Pang and John Trenkle for kindly
making their data or software available, and to the
anonymous reviewers for their constructive com-
ments. Thanks also to Nick Jacobi for his discus-
sion of the ?ice-axe? effect. Special thanks to my su-
pervisor, John Carroll, for his continued advice and
encouragement.
References
W. B. Cavnar and J. M. Trenkle. 1994. N-Gram-Based
Text Categorization. In Proceedings of the Third An-
nual Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175, Las Vegas, Nevada.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
In Proceedings of the International World Wide Web
Conference, Budapest, Hungary.
Charlotta Engstro?m. 2004. Topic Dependence in Sen-
timent Classification. Master?s thesis, University of
Cambridge, July.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, Barcelona, Spain.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, University of Pennsylvania.
Ellen Spertus. 1997. Smokey: Automatic Recognition
of Hostile Messages. In Proceedings of the Innovative
Applications of Artificial Intelligence.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), pages 417?424, Philadelphia, Pennsyl-
vania.
48
Proceedings of the Linguistic Annotation Workshop, pages 93?100,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Expressions of Appraisal in English
Jonathon Read, David Hope and John Carroll
Department of Informatics
University of Sussex
United Kingdom
{j.l.read,drh21,j.a.carroll}@sussex.ac.uk
Abstract
The Appraisal framework is a theory of the
language of evaluation, developed within the
tradition of systemic functional linguistics.
The framework describes a taxonomy of the
types of language used to convey evaluation
and position oneself with respect to the eval-
uations of other people. Accurate automatic
recognition of these types of language can
inform an analysis of document sentiment.
This paper describes the preparation of test
data for algorithms for automatic Appraisal
analysis. The difficulty of the task is as-
sessed by way of an inter-annotator agree-
ment study, based on measures analogous to
those used in the MUC-7 evaluation.
1 Introduction
The Appraisal framework (Martin and White, 2005)
describes a taxonomy of the language employed in
communicating evaluation, explaining how users of
English convey attitude (emotion, judgement of peo-
ple and appreciation of objects), engagement (as-
sessment of the evaluations of other people) and
how writers may modify the strength of their atti-
tude/engagement. Accurate automatic analysis of
these aspects of language will augment existing re-
search in the fields of sentiment (Pang et al, 2002)
and subjectivity analysis (Wiebe et al, 2004), but as-
sessing the usefulness of analysis algorithms lever-
aging the Appraisal framework will require test data.
At present there are no machine-readable
Appraisal-annotated texts publicly available. Real-
world instances of Appraisal in use are limited
to example extracts that demonstrate the theory,
coming from a wide variety of genres as disparate
as news reporting (White, 2002; Martin, 2004) and
poetry (Martin and White, 2005). These examples,
while useful in demonstrating the various aspects
of Appraisal, can only be employed in a qualitative
analysis and would bring about inconsistencies
if analysed collectively ? one can expect the
writing style to depend upon the genre, resulting in
significantly different syntactic constructions and
lexical choices.
We therefore need to examine Appraisal across
documents in the same genre and investigate pat-
terns within that particular register. This paper dis-
cusses the methodology of an Appraisal annotation
study and an analysis of the inter-annotator agree-
ment exhibited by two human judges. The output
of this study has the additional benefit of bringing
a set of machine-readable annotations of Appraisal
into the public domain for further research.
This paper is structured as follows. The next sec-
tion offers an overview of the Appraisal framework.
Section 3 discusses the methodology adopted for
the annotation study. Section 4 discusses the mea-
sures employed to assess inter-annotator agreement
and reports the results of these measures. Section
5 offers an analysis of cases of systematic disagree-
ment. Other computational work utilising the Ap-
praisal framework is reviewed in Section 6. Section
7 summarises the paper and outlines future work.
2 The linguistic framework of Appraisal
The Appraisal framework (Martin and White, 2005)
is a development of work in Systemic Functional
93
appraisal
attitude
engagement
graduation
affect
judgement
appreciation
inclination
happiness
security
satisfaction
esteem
sanction
normality
capacity
tenacity
veracity
propriety
reaction
composition
valuation
impact
quality
balance
complexity
contract
expand
disclaim
proclaim
deny
counter
pronounce
endorse
concur
affirm
concede
entertain
attribute acknowledge
distance
force
focus
quantification
intensification
number
mass
extent
proximity (space)
proximity (time)
distribution (space)
distribution (time)
degree
vigour
Figure 1: The Appraisal framework.
Linguistics (Halliday, 1994) and is concerned with
interpersonal meaning in text?the negotiation of
social relationships by communicating emotion,
judgement and appreciation. The taxonomy de-
scribed by the Appraisal framework is depicted in
Figure 1.
Appraisal consists of three subsystems that oper-
ate in parallel: attitude looks at how one expresses
private state (Quirk et al, 1985) (one?s emotion and
opinions); engagement considers the positioning of
oneself with respect to the opinions of others and
graduation investigates how the use of language
functions to amplify or diminish the attitude and en-
gagement conveyed by a text.
2.1 Attitude: emotion, ethics and aesthetics
The Attitude sub-system describes three areas of pri-
vate state: emotion, ethics and aesthetics. An atti-
tude is further qualified by its polarity (positive or
negative). Affect identifies feelings?author?s emo-
tions as represented by their text. Judgement deals
with authors? attitude towards the behaviour of peo-
ple; how authors applaud or reproach the actions
of others. Appreciation considers the evaluation of
things?both man-made and natural phenomena.
2.2 Engagement: appraisals of appraisals
Through engagement, Martin and White (2005) deal
with the linguistic constructions by which authors
construe their point of view and the resources used
to adopt stances towards the opinions of other peo-
ple. The theory of engagement follows Stubbs
(1996) in that it assumes that all utterances convey
point of view and Bakhtin (1981) in supposing that
all utterances occur in a miscellany of other utter-
ances on the same motif, and that they carry both
implicit and explicit responses to one another. In
other words, all text is inherently dialogistic as it en-
codes authors? reactions to their experiences (includ-
ing previous interaction with other writers). Engage-
ment can be both retrospective (that is, an author will
acknowledge and agree or disagree with the stances
of others who have previously appraised a subject),
and prospective (one may anticipate the responses of
an intended audience and include counter-responses
in the original text).
2.3 Graduation: strength of evaluations
Martin and White (2005) consider the resources by
which writers alter the strength of their evaluation
as a system of graduation. Graduation is a general
property of both attitude and engagement. In atti-
tude it enables authors to convey greater or lesser
degrees of positivity or negativity, while graduation
of engagements scales authors? conviction in their
utterance.
Graduation is divided into two subsystems. Force
alters appraisal propositions in terms of its inten-
94
sity, quantity or temporality, or by means of spatial
metaphor. Focus considers the resolution of seman-
tic categories, for example:
They play real jazz.
They play jazz, sort of.
In real terms a musician either plays jazz or they
do not, but these examples demonstrate how authors
blur the lines of semantic sets and how binary rela-
tionships can be turned into scalar ones.
3 Annotation methodology
The corpus used in this study consists of unedited
book reviews. Book reviews are good candidates for
this study as, while they are likely to contain similar
language by virtue of being from the same genre of
writing, we can also expect examples of Appraisal?s
many classes (for example, the emotion attributed
to the characters in reviews of novels, judgements
of authors? competence and character, appreciation
of the qualities of books and engagement with the
propositions put forth by the authors under review).
The articles were taken from the web sites of
four British newspapers (The Guardian, The Inde-
pendent, The Telegraph and The Times) on two dif-
ferent dates?31 July 2006 and 11 September 2006.
Each review is attributed to a unique author. The
corpus is comprised of 38 documents, containing a
total of 36,997 tokens in 1,245 sentences.
Two human annotators, d and j, participated in
this study, assigning tags independently. The anno-
tators were well-versed in the Appraisal framework,
having studied the latest literature. The judges were
asked to annotate appraisal-bearing terms with the
appraisal type presumed to be intended by the au-
thor of the text. They were asked to highlight each
example of appraisal and specify the type of atti-
tude, engagement or graduation present. They also
assigned a polarity (positive or negative) to attitudi-
nal items and a scaling (up or down) to graduating
items, employing a custom-developed software tool
to annotate the documents.
Four alternative annotation strategies were con-
sidered. One approach is to allow only a single token
per annotation. However, this is too simplistic for
an Appraisal annotation study?a unit of Appraisal
is frequently larger than a single token. Consider the
following examples:
(1)
The design was deceptively?VERACITY simple?
COMPLEXITY. (?)
(2)
The design was deceptively simple?COMPLEXITY.
Example 1 demonstrates that a single-token ap-
proach is inappropriate as it ascribes a judgement
of someone?s honesty, whereas Example 2 indicates
the correct analysis?the sentence is an apprecia-
tion of the simplicity of the ?design?. This example
shows how it is necessary to annotate larger units of
appraisal-bearing language.
Including more tokens, however, increases the
complexity of the annotation task, and reduces the
likelihood of agreement between the judges, as the
annotated tokens of one judge may be a subset of,
or overlap with, those of another. We therefore ex-
perimented with tagging entire sentences in order to
constrain the annotators? range of choices. This re-
sulted in its own problems as there is often more than
one appraisal in a sentence, for example:
(3)
The design was deceptively simple?COMPLEXITY
and belied his ingenuity?CAPACITY.
An alternative approach is to permit annotators
to tag an arbitrary number of contiguous tokens.
Arbitrary-length tagging is disadvantageous as the
judges will frequently tag units of differing length,
but this can be compensated for by relaxing the rules
for agreement?for example, by allowing intersect-
ing annotations to match successfully (Wiebe et al,
2005). Bruce and Wiebe (1999) employ another
approach, creating units from every non-compound
sentence and each conjunct of every compound sen-
tence. This side-steps the problem of ambiguity in
appraisal unit length, but will still fail to capture both
appraisals demonstrated in the second conjunct of
Example 4.
(4)
The design was deceptively simple?COMPLEXITY
and belied his remarkable?NORMALITY
ingenuity?CAPACITY.
Ultimately in this study, we permitted judges to
annotate any number of tokens in order to allow
for multiple Appraisal units of differing sizes within
sentences. Annotation was carried out over two
rounds, punctuated by an intermediary analysis of
95
d j d j d j
Inclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59
Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63
Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63
Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14
Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55
Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39
Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56
Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72
Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45
Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29
Quality 2.55 3.40 Acknowledge 2.42 3.33
Table 1: The distribution of the Appraisal types selected by each annotator (%).
d j
Documents 115.74 77.21
Sentences 3.65 2.43
Words 0.12 0.08
Table 2: The density of annotations relative to the
number of documents, sentences and words.
agreement and disagreement between the two anno-
tators. The judges discussed examples of the most
common types of disagreement in an attempt to ac-
quire a common understanding for the second round,
but annotations from the first round were left unal-
tered.
Following the methodology described above, d
made 3,176 annotations whilst j made 2,886 anno-
tations. The distribution of the Appraisal types as-
cribed is shown in Table 1, while Table 2 details the
density of annotations in documents, sentences and
words.
4 Measuring inter-annotator agreement
The study of inter-annotator agreement begins by
considering the level of agreement exhibited by the
annotators in deciding which tokens are representa-
tive of Appraisal, irrespective of the type. As dis-
cussed, this is problematic as judges are liable to
choose different length token spans when marking
up what is essentially the same appraisal, as demon-
strated by Example 5.
(5)
[d] It is tempting to point to the bombs in Lon-
don and elsewhere, to the hideous mess?QUALITY
in Iraq, to recent victories of the Islamists, to
the violent and polarised rhetoric?PROPRIETY and
answer yes.
[j] It is tempting to point to the bombs in
London and elsewhere, to the hideous?QUALITY
mess?BALANCE in Iraq, to recent victories of Is-
lamists, to the violent?PROPRIETY and polarised?
PROPRIETY rhetoric and answer yes.
Wiebe et al (2005), who faced this problem when
annotating expressions of opinion under their own
framework, accept that it is necessary to consider the
validity of all judges? interpretations and therefore
consider intersecting annotations (such as ?hideous?
and ?hideous mess?) to be matches. The same relax-
ation of constraints is employed in this study.
Tasks with a known number of annotative units
can be analysed with measures of agreement such as
Cohen?s ? Coefficient (1960), but the judges? free-
dom in this task prohibits meaningful application of
this measure. For example, consider howword sense
annotators are obliged to choose from a limited fixed
set of senses for each token, whereas judges anno-
tating Appraisal are free to select one of thirty-two
classes for any contiguous substring of any length
within each document; there are 16
(
n2 ? n
)
pos-
sible choices in a document of n tokens (approxi-
mately 6.5 ? 108 possibilities in this corpus).
A wide range of evaluation metrics have been em-
ployed by the Message Understanding Conferences
(MUCs). The MUC-7 tasks included extraction of
named entities, equivalence classes, attributes, facts
and events (Chinchor, 1998). The participating sys-
tems were evaluated using a variety of related mea-
sures, defined in Table 3. These tasks are similar to
Appraisal annotation in that the units are formed of
an arbitrary number of contiguous tokens.
In this study the agreement exhibited by an an-
notator a is evaluated as a pair-wise comparison
against the other annotator b. Annotator b provides
96
COR Number correct
INC Number incorrect
MIS Number missing
SPU Number spurious
POS Number possible = COR + INC + MIS
ACT Number actual = COR + INC + SPU
FSC F-score = (2 ? REC ? PRE)
/ (REC + PRE)
REC Precision = COR/POS
PRE Recall = COR/ACT
SUB Substitution = INC/ (COR + INC)
ERR Error per response = (INC + SPU + MIS)
/ (COR + INC + SPU + MIS)
UND Under-generation = MIS/POS
OVG Over-generation = SPU/ACT
Table 3: MUC-7 score definitions (Chinchor 1998).
FSC REC PRE ERR UND OVG
d 0.682 0.706 0.660 0.482 0.294 0.340
j 0.715 0.667 0.770 0.444 0.333 0.230
x? 0.698 0.686 0.711 0.462 0.312 0.274
Table 4: MUC-7 test scores, evaluating the agree-
ment in text anchors selected by the annotators. x?
denotes the average value, calculated using the har-
monic mean.
a presumed gold standard for the purposes of evalu-
ating agreement. Note, however, that in this case it
does not necessarily follow that REC (a w.r.t. b) =
PRE (b w.r.t. a). Consider that a may tend to make
one-word annotations whilst b prefers to annotate
phrases; the set of a?s annotations will contain mul-
tiple matches for some of the phrases annotated by b
(refer to Example 5, for instance). The ?number cor-
rect? will differ for each annotator in the pair under
evaluation.
Table 4 lists the values for the MUC-7 measures
applied to the text spans selected by the annota-
tors. Annotator d is inclined to identify text as Ap-
praisal more frequently than annotator j. This re-
sults in higher recall for d, but with lower preci-
sion. Naturally, the opposite observation can be
made about annotator j. Both annotators exhibit a
high error rate at 48.2% and 44.4% for d and j re-
spectively. The substitution rate is not listed as there
are no classes to substitute when considering only
text anchor agreement. The second round of anno-
tation achieved slightly higher agreement (the mean
F-score increased by 0.033).
FSC REC PRE SUB ERR
0 0.698 0.686 0.711 0.000 0.462
1 0.635 0.624 0.647 0.090 0.511
2 0.528 0.518 0.538 0.244 0.594
3 0.448 0.441 0.457 0.357 0.655
4 0.396 0.388 0.403 0.433 0.696
5 0.395 0.388 0.403 0.433 0.696
Table 5: Harmonic means of the MUC-7 test scores
evaluating the agreement in text anchors and Ap-
praisal classes selected by the annotators, at each
level of hierarchical abstraction.
Having considered the annotators? agreement
with respect to text anchors, we go on to analyse
the agreement exhibited by the annotators with re-
spect to the types of Appraisal assigned to the text
anchors. The Appraisal framework is a hierarchi-
cal system?a tree with leaves corresponding to the
annotation types chosen by the judges. When in-
vestigating agreement in Appraisal type, the follow-
ing measures include not just the leaf nodes but also
their parent types, collapsing the nodes into increas-
ingly abstract representations. For example happi-
ness is a kind of affect, which is a kind of attitude,
which is a kind of appraisal. These relationships are
depicted in full in Figure 2. Note that in the follow-
ing measurements of inter-annotator agreement leaf
nodes are included in subsequent levels (for exam-
ple, focus is a leaf node at level 2, but is also consid-
ered to be a member of levels 3, 4 and 5).
Table 5 shows the harmonic means of the MUC-
7 measures of the annotators? agreement at each of
the levels depicted in Figure 2. As one might ex-
pect, the agreement steadily drops as the classes be-
come more concrete?classes become more specific
and more numerous so the complexity of the task
increases.
Table 5 also lists the average rate of substitutions
as the annotation task?s complexity increases, show-
ing that the annotators were able to fairly easily
distinguish between instances of the three subsys-
tems of Appraisal (Attitude, Engagement and Grad-
uation) as the substitution rate at level 1 is low (only
9%). As the number of possible classes increases an-
notators are more likely to confuse appraisal types,
with disagreement occurring on approximately 44%
of annotations at level 5. The second round of an-
notations resulted in slightly improved agreement at
97
Level 0: .698
Level 1: .635
Level 2: .528
Level 3: .448
Level 4: .396
Level 5: .395
appraisal
attitude: .701
engagement: .507
graduation: .479
affect: .519
judgement: .586
appreciation: .567
contract: .502
expand: .445
force: .420
focus: .287
inclination: .249
happiness: .448
security: .335
satisfaction: .374
esteem: .489
sanction: .575
reaction: .510
composition: .432
valuation: .299
disclaim: .555
proclaim: .336
entertain: .459
attribute: .427
quantification: .233
intensification: .513
normality: .289
capacity: .431
tenacity: .395
veracity: .519
propriety: .540
impact: .462
quality: .336
balance: .300
complexity: .314
deny: .451
counter: .603
pronounce: .195
endorse: .331
concur: .297
acknowledge: .390
distance: .415
number: .191
mass: .104
extent: .242
degree: .510
vigour: .117
affirm: .325
concede: .000
proximity (space): .000
proximity (time): .000
distribution (space): .110
distribution (time): .352
Figure 2: The Appraisal framework with hierarchical levels highlighted. Appraisal classes and levels are
accompanied by the harmonic mean of the F-scores of the annotators for that class/level.
each level of abstraction (the mean F-score increased
by 0.051 at the most abstract level).
Of course, some Appraisal classes are easier to
identify than others. Figure 2 summarises the agree-
ment for each node in the Appraisal hierarchy with
the harmonic mean of the F-scores of the annotators
for each class. Typically, the attitude annotations are
easiest to identify, whereas the other subsystems of
engagement and graduation tend to be more difficult.
The Proximity children of Extent exhibited no
agreement whatsoever. This seems to have arisen
from the differences in the judges? interpretations of
proximity. In the case of Proximity (Space), for ex-
ample, one judge annotated words that function to
modify the spatial distance of other concepts (e.g.
near), whereas the other selected words placing con-
cepts at a specific location (e.g. homegrown, local).
This confusion between modifying words and spe-
98
cific locations also accounts for the low agreement
in the Distribution (Space) type.
The measures show that it is also difficult to
achieve a consensus on what qualifies as engage-
ments of the Pronounce type. Both annotators select
expressions that assert the irrefutability of a propo-
sition (e.g. certainly or in fact or it has to be said).
Judge d, however, tends to perceive pronouncement
as occurring wherever the author makes an assertion
(e.g. this is or there will be). Judge j seems to re-
quire that the assertion carry a degree of emphasis to
include a term in the Pronounce class.
The low agreement of the Mass graduations can
also be explained in this way, as both d and j se-
lect strong expressions relating to size (e.g. massive
or scant). Annotator j found additional but weaker
terms like largely or slightly.
The Pronounce and Mass classes provide typical
examples of the disagreement exhibited by the an-
notators. It is not that the judges have wildly differ-
ent understandings of the system, but rather they dis-
agree in the bounds of a class?one annotator may
require a greater degree of strength of a term to war-
rant its inclusion in a class.
Contingency tables (not depicted due to space
constraints) reveal some interesting tendencies for
confusion between the two annotators. Approxi-
mately 33% of d?s annotations of Proximity (Space)
were ascribed as Capacity by j. The high percent-
age is due to the rarity of annotations of Proxim-
ity (Space), but the confusion comes from differing
units of Appraisal, as shown in Example 6.
(6)
[d] But at key points in this story, one gets
the feeling that the essential factors are op-
erating just outside?PROXIMITY (SPACE)
James?s field of vision?CAPACITY.
[j] But at key points in this story, one gets the
feeling that the essential factors are operating just
outside James?s field of vision?CAPACITY.
Another interesting case of frequent confusion is
the pair of Satisfaction and Propriety. Though not
closely related in the Attitude subsystem, j chooses
Propriety for 21% of d?s annotations of Satisfaction.
The confusion is typified by Example 7, where it is
apparent that there is disagreement in terms of who
is being appraised.
(7)
[d] Like him, Vermeer ? or so he chose to be-
lieve ? was an artist neglected?SATISFACTION and
wronged?SATISFACTION by critics and who had
died an almost unknown.
[j] Like him, Vermeer ? or so he chose to believe
? was an artist neglected and wronged?PROPRIETY
by critics and who had died an almost unknown.
Annotator d believes that the author is communi-
cating the artist?s dissatisfaction with the way he is
treated by critics, whereas j believes that the critics
are being reproached for their treatment of the artist.
This highlights a problem with the coding scheme,
which simplifies the task by assuming only one type
of Appraisal is conveyed by each unit.
5 Related work
Taboada and Grieve (2004) initiated computational
experimentation with the Appraisal framework, as-
signing adjectives into one of the three broad atti-
tude classes. The authors apply SO-PMI-IR (Turney,
2002) to extract and determine the polarity of adjec-
tives. They then use a variant of SO-PMI-IR to de-
termine a ?potential? value for affect, judgement and
appreciation, calculating the mutual information be-
tween the adjective and three pronoun-copular pairs:
I was (affect); he was (judgement) and it was (ap-
preciation). While the pairs seem compelling mark-
ers of the respective attitude types, they incorrectly
assume that appraisals of affect are limited to the
first person whilst judgements are made only of the
third person. We can expect a high degree of overlap
between the sets of documents retrieved by queries
formed using these pairs (e.g. I was a happy ?X?;
he was a happy ?X?; It was a happy ?X?).
Whitelaw et al (2005) use the Appraisal frame-
work to specify frames of sentiment. These ?Ap-
praisal Groups? are derived from aspects of Attitude
and Graduation:
Attitude: affect | judgement | appreciation
Orientation positive | negative
Force: low | neutral | high
Focus: low | neutral | high
Polarity: marked | unmarked
Their process begins with a semi-automatically con-
structed lexicon of these Appraisal groups, built us-
ing example terms from Martin and White (2005) as
seeds into WordNet synsets. The frames supplement
bag of words-based machine learning techniques for
99
sentiment analysis and they achieve minor improve-
ments over unigram features.
6 Summary
This paper has discussed the methodology of an ex-
ercise annotating book reviews according to the Ap-
praisal framework, a functional linguistic theory of
evaluation in English. The agreement exhibited by
two human judges was measured by analogy with
the evaluation employed for the MUC-7 shared tasks
(Chinchor, 1998).
The agreement varied greatly depending on the
level of abstraction in the Appraisal hierarchy
(a mean F-score of 0.698 at the most abstract
level through to 0.395 at the most concrete level).
The agreement also depended on the type being
annotated?there was more agreement evident for
types of attitude compared to types of engagement
or graduation.
The exercise is the first step in an ongoing study
of approaches for the automatic analysis of expres-
sions of Appraisal. The primary output of this work
is a corpus of book reviews independently annotated
with Appraisal types by two coders. Agreement was
in general low, but if one assumes that the intersec-
tion of both sets of annotations contains reliable ex-
amples, this leaves 2,223 usable annotations.
Future work will employ these annotations to
evaluate algorithms for the analysis of Appraisal,
and investigate the usefulness of the Appraisal
framework when in the computational analysis of
document sentiment and subjectivity.
Acknowledgments
We would like to thank Bill Keller for advice when
designing the annotation methodology. The work of
the first author is supported by a UK EPSRC stu-
dentship.
References
M. M. Bakhtin. 1981. The Dialogic Imagination. Uni-
versity of Texas Press, Austin. Translated by C. Emer-
son & M. Holquist.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: a case study in manual tagging. Natural
Language Engineering, 5(1):1?16.
N. Chinchor. 1998. MUC-7 test scores introduction.
In Proceedings of the Seventh Message Understanding
Conference.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measures,
20:37?46.
M. A. K. Halliday. 1994. An Introduction to Functional
Grammar. Edward Arnold, London.
J. R. Martin and P. R. R. White. 2005. Language of Eval-
uation: Appraisal in English. Palgrave Macmillan.
J. R. Martin. 2004. Mourning: how we get algned. Dis-
course & Society, 15(2-3):321?344.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, Philadelphia, PA, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman.
M. Stubbs. 1996. Towards a modal grammar of English:
a matter of prolonged fieldwork. In Text and Corpus
Analysis. Blackwell, Oxford.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal automatically. In Spring Symposium on Ex-
ploring Attitude and Affect in Text. American Associa-
tion for Artificial Intelligence, Stanford. AAAI Tech-
nical Report SS-04-07.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA, USA.
P. R. R.White. 2002. Appraisal? the language of evalu-
ation and stance. In Jef Verschueren, Jan-Ola O?stman,
Jan Blommaert, and Chris Bulcaen, editors, Handbook
of Pragmatics, pages 1?27. John Benjamins, Amster-
dam.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the 14th ACM international confer-
ence on Information and knowledge management.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational linguistics, 30(3):277?308.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
100
Speculation and Negation: Rules, Rankers,
and the Role of Syntax
Erik Velldal?
University of Oslo
Lilja ?vrelid?
University of Oslo
Jonathon Read?
University of Oslo
Stephan Oepen?
University of Oslo
This article explores a combination of deep and shallow approaches to the problem of resolving
the scope of speculation and negation within a sentence, specifically in the domain of biomedical
research literature. The first part of the article focuses on speculation. After first showing how
speculation cues can be accurately identified using a very simple classifier informed only by
local lexical context, we go on to explore two different syntactic approaches to resolving the
in-sentence scopes of these cues. Whereas one uses manually crafted rules operating over depen-
dency structures, the other automatically learns a discriminative ranking function over nodes
in constituent trees. We provide an in-depth error analysis and discussion of various linguistic
properties characterizing the problem, and show that although both approaches perform well
in isolation, even better results can be obtained by combining them, yielding the best published
results to date on the CoNLL-2010 Shared Task data. The last part of the article describes how our
speculation system is ported to also resolve the scope of negation. With only modest modifications
to the initial design, the system obtains state-of-the-art results on this task also.
1. Introduction
The task of providing a principled treatment of speculation and negation is a problem
that has received increased interest within the NLP community during recent years.
This is witnessed not only by this Special Issue, but also by the themes of several recent
shared tasks and dedicated workshops. The Shared Task at the 2010 Conference on Nat-
ural Language Learning (CoNLL) has been of central importance in this respect, where
the topic was speculation detection for the domain of biomedical research literature
? University of Oslo, Department of Informatics, PB 1080 Blindern, 0316 Oslo, Norway.
E-mail: {erikve,liljao,jread,oe}@ifi.uio.no.
Submission received: 5 April 2011; revised submission received: 30 September 2011; accepted for publication:
2 December 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
(Farkas et al 2010). This particular area has been the focus of much current research,
triggered by the release of the BioScope corpus (Vincze et al 2008)?a collection of
scientific abstracts, full papers, and clinical reports with manual annotations of words
that signal speculation or negation (so-called cues), as well as of the scopes of these cues
within the sentences. The following examples from BioScope illustrate how sentences
are annotated with respect to speculation. Cues are here shown using angle brackets,
with braces corresponding to their annotated scopes:
(1) {The specific role of the chromodomain is ?unknown?} but chromodomain
swapping experiments in Drosophila {?suggest? that they {?might? be
protein interaction modules}} [18].
(2) These data {?indicate that? IL-10 and IL-4 inhibit cytokine production by
different mechanisms}.
Negation is annotated in the same way, as shown in the following examples:
(3) Thus, positive autoregulation is {?neither? a consequence ?nor? the sole
cause of growth arrest}.
(4) Samples of the protein pair space were taken {?instead of? considering the
whole space} as this was more computationally tractable.
In this article we develop several linguistically informed approaches to automati-
cally identify cues and resolve their scope within sentences, as in the example annota-
tions. Our starting point is the system developed by Velldal, ?vrelid, and Oepen (2010)
for the CoNLL-2010 Shared Task challenge. This system implements a two-stage hybrid
approach for resolving speculation: First, a binary classifier is applied for identifying
cues, and then their in-sentence scope is resolved using a small set of manually defined
rules operating on dependency structures.
In the current article we present several important extensions to the initial system
design of Velldal, ?vrelid, and Oepen (2010): First, in Section 5, we present a simpli-
fied approach to cue classification, greatly reducing the model size and complexity
of our Support Vector Machine (SVM) classifier while at the same time giving better
accuracy. Then, after reviewing the manually defined dependency-based scope rules
(Section 6.1), we show how the scope resolution task can be handled using an alternative
approach based on learning a discriminative ranking function over subtrees of HPSG-
derived constituent trees (Section 6.2). Moreover, by combining this empirical ranking
approach with the manually defined rules (Section 6.3), we are able to obtain the best
published results so far (to the best of our knowledge) on the CoNLL-2010 Shared
Task evaluation data. Finally, in Section 7, we show how our speculation system can be
ported to also resolve the scope of negation. Only requiring modest modifications, the
system also obtains state-of-the-art results on this task. Rather than merely presenting
the implementation details of the new approaches we develop, we also provide in-depth
error analyses and discussion on the linguistic properties of the phenomena of both
speculation and negation.
Before turning to the details of our approach, however, we start by presenting the
relevant data sets and the resources used for pre-processing in Section 2, followed by
a presentation of the various evaluation measures we will use in Section 3. We also
provide a brief review of relevant previous work in Section 4.
370
Velldal et al Rules, Rankers, and the Role of Syntax
2. Data Sets and Preprocessing
Our experiments center on the biomedical abstracts, full papers, and clinical reports of
the BioScope corpus (Vincze et al 2008). This comprises 20,924 sentences (or other root-
level utterances), annotated with respect to both negation and speculation. Some basic
descriptive statistics for the data sets are provided in Table 1. We see that roughly 18% of
the sentences are annotated as uncertain, and 13% contain negations. Note that, for our
speculation experiments, we will be using only the abstracts and the papers for training,
corresponding to the official CoNLL-2010 Shared Task training data. Moreover, we will
be using the Shared Task version of this data, in which certain annotation errors had
been corrected. The Shared Task task organizers also provided a set of newly annotated
biomedical articles for evaluation purposes, constituting an additional 5,003 utterances.
This latter data set (also detailed in Table 1) will be used for held-out testing of our
speculation models. We will be using the following abbreviations when referring to the
various parts of the data: BSA (BioScope abstracts), BSP (full papers), BSE (the held-
out evaluation data), and BSR (clinical reports). Note that, when we get to the negation
task we will be using the original version of the BioScope data. Furthermore, as BSE
does not annotate negation, we instead follow the experimental set-up of Morante and
Daelemans (2009b) for the negation task, reporting 10-fold cross validation on BSA and
held-out testing on BSP and BSR.
2.1 Tokenization
The BioScope data (and other data sets in the CoNLL-2010 Shared Task), are provided
sentence-segmented only, and otherwise non-tokenized. Unsurprisingly, the GENIA
tagger (Tsuruoka et al 2005) has a central role in our pre-processing set-up. We found
that its tokenization rules are not always optimally adapted for the type of text in Bio-
Scope, however. For example, GENIA unconditionally introduces token boundaries for
some punctuation marks that can also occur token-internally, thus incorrectly splitting
tokens like 390,926, methlycobamide:CoM, or Ca(2+). Conversely, GENIA fails to isolate
some kinds of opening single quotes, because the quoting conventions assumed in
BioScope differ from those used in the GENIA Corpus, and it mis-tokenizes LATEX-
style n- and m-dashes. On average, one in five sentences in the CoNLL training data
Table 1
The top three rows summarize the components of the BioScope corpus?abstracts (BSA), full
papers (BSP), and clinical reports (BSR)?annotated for speculation and negation. The bottom
row details the held-out evaluation data (BSE) provided for the CoNLL-2010 Shared Task.
Columns indicate the total number of sentences and their average length, the number of
hedged/negated sentences, the number of cues, and the number of multiword cues. (Note that
BSE is not annotated for negation, and we do not provide speculation statistics for BSR as this
data set will only be used for the negation experiments.
Speculation Negation
Sentences Length Sentences Cues MWCs Sentences Cues MWCs
BSA 11,871 26.1 2,101 2,659 364 1,597 1,719 86
BSP 2,670 25.7 519 668 84 339 376 23
BSR 6,383 7.7 ? ? ? 865 870 8
BSE 5,003 27.6 790 1,033 87 ? ? ?
371
Computational Linguistics Volume 38, Number 2
exhibited GENIA tokenization problems. Our pre-processing approach thus deploys a
cascaded finite-state tokenizer (borrowed and adapted from the open-source English
Resource Grammar: Flickinger [2002]), which aims to implement the tokenization deci-
sions made in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)?much
like GENIA, in principle?but more appropriately treating corner cases like the ones
noted here.
2.2 PoS Tagging and Lemmatization
For part-of-speech (PoS) tagging and lemmatization, we combine GENIA (with its
built-in, occasionally deviant tokenizer) and TnT (Brants 2000), which operates on
pre-tokenized inputs but in its default model is trained on financial news from the
Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy
provided by GENIA in the biomedical domain, while using our improved tokenization
and producing inputs to the parsers that as much as possible resemble the conventions
used in the original training data for the (dependency) parser (the Penn Treebank, once
again).
To this effect, for the vast majority of tokens we can align the GENIA tokeniza-
tion with our own, and in these cases we typically use GENIA PoS tags and lemmas
(i.e., base-forms). For better normalization, we downcase all lemmas except for proper
nouns. GENIA does not make a PoS distinction between proper vs. common nouns
(as assumed in the Penn Treebank), however, and hence we give precedence to TnT
outputs for tokens tagged as nominal by both taggers. Finally, for the small number of
cases where we cannot establish a one-to-one correspondence between GENIA tokens
and our own tokenization, we rely on TnT annotation only.
2.3 A Methodological Caveat
Unsurprisingly, the majority of previous work on BioScope seems to incorporate infor-
mation from the GENIA tagger in one way or another, whether it regards tokenization,
lemmatization, PoS information, or named entity chunking. Using the GENIA tagger for
pre-processing introduces certain dependencies to be aware of, however, as the abstracts
in BioScope are in fact also part of the GENIA corpus (Collier et al 1999) on which the
GENIA tagger is trained. This means that the accuracy of the information provided by
the tagger on this subset of BioScope cannot be expected to be representative of the
accuracy on other texts. Moreover, this effect might of course also carry over to any
downstream components using this information.
For the experiments described in this article, GENIA supplies lemmas for the
n-gram features used by the cue classifiers, as well as PoS tags used in the input to
both the dependency parser and the Head-driven Phrase Structure Grammar (HPSG)
parser (which in turn provide the inputs to our various scope resolution components).
For the HPSG parser, a subset of the GENIA corpus was also used as part of the
training data for estimating an underlying statistical parse selection model, producing
n-best lists of ranked candidate parses (MacKinlay et al 2011). When reporting final
test results on the full papers (BSP or BSE) or the clinical reports (BSR), no such
dependencies between information sources exists. It does mean, however, that we can
reasonably expect to see some extra drop in performancewhen going fromdevelopment
results on data that includes the BioScope abstracts to the test results on these other
data sets.
372
Velldal et al Rules, Rankers, and the Role of Syntax
3. Evaluation Measures
In this section we seek to clarify the type of measures we will be using for evaluating
both the cue detection components (Section 3.1) and the scope resolution components
(Section 3.2). Essentially, we here follow the evaluation scheme established by the
CoNLL-2010 Shared Task on speculation detection, also applying this when evaluating
results for the negation task.
3.1 Evaluation Measures for Cue Identification
For the approaches presented for cue detection in this article (for both speculation and
negation), we will be reporting precision, recall, and F1 for three different levels of
evaluation; the sentence-level, the token-level, and the cue-level. The sentence-level scores
correspond to Task 1 in the CoNLL-2010 Shared Task, that is, correctly identifying
whether a sentence contains uncertainty or not. The scores at the token-level measure
the number of individual tokens within the span of a cue annotation that the classifier
has correctly labeled as a cue. Finally, the stricter cue-level scores measure how well a
classifier succeeds in identifying entire cues (which will in turn provide the input for
the downstream components that later try to resolve the scope of the speculation or
negation within the sentence). A true positive at the cue-level requires that the predicted
cue exactly matches the annotation in its entirety (full multiword cues included).
For assessing the statistical significance of any observed differences in performance,
we will be using a two-tailed sign-test applied to the token-level predictions. This
is a standard non-parametric test for paired samples, which in our setting considers
how often the predictions of two given classifiers differ. Note that we will only be
performing significance testing for the token-level evaluation (unless otherwise stated),
as this is the level that most directly corresponds to the classifier decisions. We will be
assuming a significance level of ? = 0.05, but also reporting actual p-values in cases
where differences are not found to be significant.
3.2 Evaluation Measures for Scope Resolution
When evaluating scope resolution we will be following the methodology of the CoNLL-
2010 Shared Task, also using the scoring software made available by the task organiz-
ers.1 We have modified the software trivially so that it can also be used to evaluate
negation labeling. As pointed out by Farkas et al (2010), this way of evaluating scope is
rather strict: A true positive (TP) requires an exact match for both the entire cue and the
entire scope. On the other hand, a false positive (FP) can be incurred by three different
events; (1) incorrect cue labeling with correct scope boundaries, (2) correct cue labeling
with incorrect scope boundaries, or (3) incorrectly labeled cue and scope. Moreover,
conditions (1) and (2) will give a double penalty, in the sense that they also count as false
negatives (FN) given that the gold-standard cue or scope is missed (Farkas et al 2010).
Finally, false negatives are of course also incurred by cases where the gold-standard
annotations specify a scope but the system makes no such prediction.
Of course, the evaluation scheme outlined here corresponds to an end-to-end eval-
uation of the overall system, where the cue detection performance carries over to the
1 The Java code for computing the scores can be downloaded from the CoNLL-2010 Shared Task Web site:
http://www.inf.u-szeged.hu/rgai/conll2010st/.
373
Computational Linguistics Volume 38, Number 2
scope-level performance. In order to better assess the performance of a scope resolution
component in isolation, we will also report scope results against gold-standard cues. Note
that, when using gold-standard cues, the number of false negatives and false positives
will always be identical, meaning that the scope-level figures for recall, precision, and F1
will all be identical as well, and we will therefore only be reporting the latter in this set-
up. (The reason for this is that, when assuming gold-standard cues, only error condition
(2) can occur, which will in turn always count both a false positive and a false negative,
making the two figures identical.)
Exactly how to define the paired samples that form the basis of the statistical
significance testing is less straightforward for the end-to-end scope-level predictions
than for the cue identification. It is also worth noting that the CoNLL-2010 Shared Task
organizers themselves refrained from including any significance testing when report-
ing the official results. In this article we follow a recall-centered approach: For each
cue/scope pair in the gold standard, we simply note whether it is correctly identified
or not by a given system. The sequence of boolean values that results (FP = 0, TP = 1)
can be directly paired with the corresponding sequence for a different system so that
the sign-test can be applied as above.
Note that our modified scorer for negation is available from our Web page of sup-
plemental materials,2 together with the system output (in XML following the BioScope
DTD) for all end-to-end runs with our final model configurations.
4. Related Work on Speculation Labeling
Although there exists a body of earlier work on identifying uncertainty on the sentence
level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the
task of resolving the in-sentence scope of speculation cues was first pioneered byMorante
and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al 2010)
entered largely uncharted territory and contributed to an increased interest for this task.
Virtually all systems for resolving speculation scope implement a two-stage archi-
tecture: First there is a component that identifies the speculation cues and then there is a
component for resolving the in-sentence scopes of these cues. In this section we provide a
brief review of previous work on this problem, putting emphasis of the best performers
from the two corresponding subtasks of the CoNLL-2010 Shared Task, cue detection
(Task 1) and scope resolution (Task 2).
4.1 Related Work on Identifying Speculation Cues
The top-ranked system for Task 1 in the official CoNLL-2010 Shared Task evaluation
approached cue identification as a sequence labeling problem (Tang et al 2010). Similarly to
the decision-tree approach of Morante and Daelemans (2009a), Tang et al (2010) set out
to label tokens according to a BIO-scheme; indicating whether they are at the Beginning,
Inside, or Outside of a speculation cue. In the ?cascaded? system architecture of Tang et
al. (2010), the predictions of both a Conditional Random Field (CRF) sequence classifier
and an SVM-based Hidden Markov Model (HMM) are both combined in a second CRF.
In terms of the overall approach, namely, viewing the problem as a sequence la-
beling task, Tang et al (2010) are actually representative of the majority of the Shared
Task participants for Task 1 (Farkas et al 2010), including the top three performers on
2 Supplemental materials; http://www.velldal.net/erik/modneg/.
374
Velldal et al Rules, Rankers, and the Role of Syntax
the official held-out data. Many participants instead approached the task as a word-by-
word token classification problem, however. Examples of this approach are the systems of
Velldal, ?vrelid, and Oepen (2010) and Vlachos and Craven (2010), sharing the fourth
rank position (out of 24 submitted systems) for Task 1.
In both the sequence- and token-classification approaches, sentences are labeled
as uncertain if they are found to contain a cue. In contrast to this, a third group of
systems instead label sentences directly, typically using bag-of-words features. Such
sentence classifiers tended to achieve a somewhat lower relative rank in the official Task 1
evaluation (Farkas et al 2010).
4.2 Related Work on Resolving Speculation Scope
As mentioned earlier, the task of resolving the scope of speculation was first introduced
inMorante andDaelemans (2009a), where a system initially designed for negation scope
resolution (Morante, Liekens, and Daelemans 2008) was ported to speculation. Their
general approach treats the scope resolution task in much the same way as the cue
identification task: as a sequence labeling task and using only token-level, lexical infor-
mation. Morante, van Asch, and Daelemans (2010) then extended on this system by also
adding syntactic features, resulting in the top performing system of the CoNLL-2010
Shared Task at the scope-level (corresponding to the second subtask). It is interesting
to note that all the top performers use various types of syntactic information in their
scope resolution systems: The output from a dependency parser (MaltParser) (Morante,
van Asch, and Daelemans 2010; Velldal, ?vrelid, and Oepen 2010), a tag sequence
grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination
with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010).
The majority of systems perform classification at the token level, using some variant
of machine learning with a BIO classification scheme and a post-processing step to
assemble the full scope (Farkas et al 2010), although several of the top performers
employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, ?vrelid, and
Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010).
5. Identifying Speculation Cues
We now turn to look at the details of our own system, starting in this section with
describing a simple yet effective approach to identifying speculation cues. A cue is here
taken to mean the words or phrases that signal the attitude of uncertainty or specula-
tion. As noted by Farkas et al (2010), most hedge cues typically fall in the following cate-
gories; adjectives or adverbs (probable, likely, possible, unsure, etc.), auxiliaries (may,might,
could, etc.), conjunctions (either. . . or, etc.), or verbs of speculation (suggest, suspect, sup-
pose, seem, etc.). Judging by the examples in the Introduction, it might at first seem that
the speculation cues can be identified merely by consulting a pre-compiled list. Most, if
not all, words that can function as cues can also occur as non-cues, however. More than
85% of the cue lemmas observed in the BioScope corpus also have non-cue occurrences.
To give just one example, a hedge detection system needs to correctly discriminate
between the use of appear as a cue in Example (5), and as a non-cue in Example (6):
(5) In 5 patients the granulocytes {?appeared? polyclonal} [. . . ]
(6) The effect appeared within 30 min and returned to basal levels after 2 h.
375
Computational Linguistics Volume 38, Number 2
In the approach of Velldal, ?vrelid, and Oepen (2010), a binary token classifier was
applied in a way that labeled each and every word as cue or non-cue. We will refer to this
mode of classification as word-by-word classification (WbW). The follow-up experi-
ments described by Velldal (2011) showed that comparable results could be achieved
using a filtering approach that ignores words not occurring as cues in the training data.
This greatly reduces both the number of relevant training examples and the number
of features in the model, and in the current article we simplify this ?disambiguation
approach? even further. In terms of modeling framework, we implement our models
as linear SVM classifiers, estimated using the SVMlight toolkit (Joachims 1999). We also
include results for a very simple baseline model, however?to wit, a WbW approach
classifying each word simply based on its observed majority usage as a cue or non-cue
in the training data. Then, as for all our models, if a given sentence is found to contain
a cue, the entire sentence is subsequently labeled uncertain. Before turning to the indi-
vidual models, however, we first describe how we deal with the issue ofmultiword cues.
5.1 Multiword Cues
In the BioScope annotations, it is possible for a speculation cue to span multiple tokens
(e.g., raise an intriguing hypothesis). As seen from Table 1, about 13.5% of the cues in the
training data are such multiword cues (MWCs). The distribution of these cues is very
skewed, however. For instance, although the majority of MWCs are very infrequent
(most of them occurring only once), the pattern indicate that accounts for more than 70%
of the cases alone. Exactly which cases are treated as MWCs often seems somewhat
arbitrary and we have come across several inconsistencies in the annotations. We there-
fore choose to not let the classifiers we develop in this article be sensitive to the notion
of multiword cues. A given word token is considered a cue as long as it falls within
the span of a cue annotation. Multiword cues are instead treated in a separate post-
processing step, applying a small set of heuristic rules that aim to capture only the most
frequently occurring patterns observed in the training data. For example, if we find that
indicate is classified as a cue and it is followed by that, a rule will fire that ensures we
treat these tokens as a single cue. (Note that the rules are only applied to sentences that
have already been labeled uncertain by the classifier.) Table 2 lists the lemma patterns
currently covered by our rules.
5.2 Reformulating the Classification Problem: A Filtered Model
Before detailing our approach, we start with some general observations about the data
and the task. An error analysis of the initial WbW classifier developed by Velldal,
Table 2
Patterns covered by our rules for multiword speculation cues.
cannot {be}? exclude
either .+ or
indicate that
may,? or may not
no {evidence | proof | guarantee}
not {known | clear | evident | understood | exclude}
raise the .* {possibility | question | issue | hypothesis}
whether or not
376
Velldal et al Rules, Rankers, and the Role of Syntax
?vrelid, and Oepen (2010) revealed it was not able to generalize to new speculation
cues beyond those observed during training. On the other hand, only a rather small
fragment of the test cues are actually unseen: Using a 10-fold split for the development
data, the average ratio of test cues that also occur as cues in training is more than 90%.
Another important observation we can take into account is that although it seems
reasonable to assume that anyword occurring as a cue can also occur as a non-cue (recall
that more than 85% of the observed cues also have non-cue occurrences in the training
data), the converse is less likely. Whereas the training data contains a total of approxi-
mately 17,600 unique base forms, only 143 of these ever occur as speculation cues.
As a consequence of these observations, Velldal (2011) proposed that one might
reasonably treat the set of cue words as a near-closed class, at least for the biomedical
data considered in this study. This means reformulating the problem as follows. Instead
of approaching the task as a classification problem defined for all words, we only
consider words that have a base form observed as a speculation cue in the training
material. By restricting the classifier to only this subset of words, we can simplify the
classification problem tremendously. As we shall see, it also has the effect of leveling
out the initial imbalance between negative and positive examples in the data, acting as
a (selective rather than random) downsampling technique.
One reasonable fear here, perhaps, might be that this simplification comes at the
expense of recall, as we are giving up on generalizing our predictions to any previously
unseen cues. As noted earlier, however, the initial WbW model of Velldal, ?vrelid, and
Oepen (2010) already failed to make any such generalizations, and, as we shall see, this
reformulation comes without any loss in performance and actually leads to an increase
in recall compared to a full WbWmodel using the same feature set.
Note that although we will approach the task as a ?disambiguation problem,? it is
not feasible to train separate classifiers for each individual base form. The frequency
distribution of the cue words in the training material is rather skewed with most cues
being very rare?many occurring as a cue only once (? 40%, constituting less than
1.5% of the total number of cue word instances). (Most of these words also have many
additional occurrences in the training data as non-cues, however.) For the majority of
the cue words, then, it seems we cannot hope to gather enough reliable information to
train individual classifiers. Instead, we want to be able to draw on information from
the more frequently occurring cues also when classifying or disambiguating the less
frequent ones. Consequently, we will still train a single global classifier.
Extending on the approach of Velldal (2011), we include a final simple step to reduce
the set of relevant training examples even further. As pointed out in Section 5.1, any
token occurring within a cue annotation is initially regarded as a cue word. Many
multiword cues also include function words, punctuation, and so forth, however. In
order to filter out such spurious but high-frequency ?cues,? we compiled a small stop-
list on the basis of the MWCs in training data (containing just a dozen tokens, namely,
a, an, as, be, for, of, that, the, to, with, ?,?, and ?-?).
5.2.1 Features. After experimenting with a wide range of different features, ?vrelid,
Velldal, and Oepen (2010) concluded that syntactic features appeared unnecessary for
the cue classification task, and that simple sequence-oriented n-gram features recording
immediate lexical context based on lemmas and surface forms is what gave the best
performance.
Initially, the n-gram feature templates we use in the current article record neighbors
for up to three positions left/right of the focus word. For increased generality, we also
include non-lexicalized variants, that is, recording only the neighbors while excluding
377
Computational Linguistics Volume 38, Number 2
the focus word itself. After a grid search across the various configurations of these
features, the best performance was found for a model recording n-grams of lemmas up
to three positions left and right of the focus word, and n-grams of surface forms up to
two positions to the right.
Table 3 shows the performance of the filteringmodel when using this feature config-
uration and testing by 10-fold cross-validation on the training data (BSA and BSP), also
contrasting performance with the majority usage baseline. Achieving a sentence-level
F1 of 92.04 (compared to 89.07 for the baseline), a token-level score of 89.57 (baseline =
86.42), and a cue-level score of 89.11 (baseline = 85.57), it performs significantly better
than the baseline. Applying the sign-test as described in Section 3.1, the token-level
differences were found to be significant for p < 0.05. It is also clear, however, that the
simple baseline appears to be fairly strong.
As discussed previously, part of the motivation for introducing the filtering scheme
is to create a model that is as simple as possible without sacrificing performance. In
addition to the evaluation scores, therefore, it is also worth noting some statistics related
to the classifier and the training data itself. Before looking into the properties of the fil-
tering set-up though, let us start, for the sake of comparison, by considering some prop-
erties of a learning set-up based on full WbW classification like the model of Velldal,
?vrelid, and Oepen (2010), assuming an identical feature configuration as used for the
given filtering model. The row titled WbW in Table 3 lists the development results for
this model, and we see that they are slightly lower than for the filtering model (with the
differences being significant for ? = 0.05). Although precision is slightly higher, recall is
substantially lower. Assuming a 10-fold cross-validation scheme like this, the number of
training examples presented to the WbW learner in each fold averages roughly 340,000,
corresponding to the total number of word tokens. Among these training examples,
the ratio of positive to negative examples (cues vs. non-cues) is roughly 1:100. In other
words, the data is initially very skewed when it comes to class balance. In terms of the
size of the feature set, the average number of distinct feature types per fold, assuming
the given feature configuration, would be roughly 2,600,000 under a WbW set-up.
Turning now to the filtering model, the average number of training examples
presented to the learner in each fold is reduced from roughly 340,000 to just 10,000.
Correspondingly, the average number of distinct feature types is reduced from well
above 2,600,000 to roughly 100,000. The class balance among the tokens given to the
learner is alsomuch less skewed, with positive examples now averaging 30%, compared
to 1% for the WbW set-up. Finally, we observe that the complexity of the model in
terms of how many training examples end up as support vectors (SVs) defining the
separating hyperplane is also considerably reduced: Although the average number of
SVs in each fold corresponds to roughly 14,000 examples for the WbW model, this is
down to roughly 5,000 for the final filtered model. Note that for the SVM regularization
Table 3
Development results for detecting speculationCUES:Averaged 10-fold cross-validation results for
the cue classifiers on both the abstracts and full papers in the BioScope training data (BSA andBSP).
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 91.07 87.21 89.07 91.61 81.85 86.42 90.49 81.16 85.57
WbW 95.01 88.03 91.37 95.29 82.78 88.58 94.65 82.26 88.02
Filtering 94.52 89.72 92.04 94.88 84.86 89.57 94.13 84.60 89.11
378
Velldal et al Rules, Rankers, and the Role of Syntax
parameter C, governing the trade-off between training error and margin size, we will
always be using the default value set by SVMlight. This value is analytically determined
from the training data, and further empirical tuning has in general not led to improve-
ments on our data sets.
5.2.2 The Effect of Data Size. Given how the filtered classifier treats the set of cues as a
closed class, a reasonable concern is its sensitivity to the size of the training set. In order
to further assess this effect, we computed learning curves showing how classifier per-
formance on the development data changes as we incrementally include more training
examples (see Figure 1). For reference we also include learning curves for the word-by-
word classifier using the identical feature configuration, as well as the majority usage
baseline.
As expected, we see that classifier performance steadily improves as more training
data is included. Although additional data would no doubt be beneficial, we reassur-
ingly observe that the curve seems to start gradually flattening out somewhat. If we
instead look at the performance curve for the WbW classifier we find that, while having
roughly the same shape as that of the filtered classifier, although consistently lower, it
nonetheless appears to be more sensitive to the size of the training set. Interestingly,
we see that the baseline model seems to be the one that is least affected by data size. It
actually outperforms the standard WbWmodel for the first three increments, but at the
same time it seems unable to benefit much at all from additional data.
5.2.3 Error Analysis.When looking at the distribution of errors at the cue-level (totaling
just below 700 across the 10-fold run), we find that roughly 74% are false negatives.
Rather than being caused by legitimate cue words being filtered out during training,
however, the FNs mostly pertain to a handful of high-frequency words that are also
highly ambiguous. When sorted according to error frequency, the top four candidates
alone constitute almost half the total number of FNs: or (24% of the FNs), can (10%),
could (7%), and either (6%). Looking more closely at the distribution of these words in
Figure 1
Learning curves showing the effect on token-level F1 for speculation cues when withdrawing
some portion of the training partitions across the 10-fold cycles. The size of the training set is
shown on a logarithmic scale to better see whether improvements are constant for n-fold
increases of data.
379
Computational Linguistics Volume 38, Number 2
the training data, it is easy to see how they pose a challenge for the learner. For example,
whereas or has a total of 1,215 occurrences, only 153 of these are annotated as a cue.
Distinguishing the different usages from each other can sometimes be difficult even for
a human eye, as testified also by the many inconsistencies we observed in the gold-
standard annotation of these cases.
Turning our attention to the other end of the tail, we find that just over 40 (8%) of the
FNs involve tokens for which there is only a single occurrence as a cue in the training
data. In other words, these would first appear to be exactly the tokens that we could
never get right, given our filtering scheme. We find, however, that most of these cases
regard tokens whose one and only appearance as a cue is as part of a multiword cue,
although they typically have a high number of other non-cue occurrences as well. For
example, although number occurs a total of 320 times, its one and only occurrence as
a cue is in the multiword cue address a number of questions. Given that this and several
other equally rare patterns are not currently covered by our MWC rules in the first
place, we would not have been able to get them right even if all the individual tokens
had been classified as cues (recall that a true positive at the cue-level requires an exact
match of the entire span). In total we find that 16% of the cue-level FNs corresponds to
multiword cues.
When looking at the frequency of multiword cues among the false positives, we
find that they only make up roughly 5% of the errors. Furthermore, a manual inspection
reveals that they can all be argued to be instances of annotation errors, in that we believe
these should actually be counted as true positives. Most of them involve indicate that and
not known, as in the following examples (where the cues assigned by our system are not
annotated as cues in BioScope):
(7) In contrast, levels of the transcriptional factor AP-1, which is ?not known?
to be important in B cell Ig production, were reduced by TGF-beta.
(8) Analysis of the nuclear extracts [. . . ] ?indicated that? the composition of
NF-kappa B was similar in neonatal and adult cells.
All in all, the errors in the FP category make up 26% of the total number of errors.
Just as for the FNs, the frequency distribution of the cues involved is quite skewed,
with a handful of highly frequent and highly ambiguous cue words accounting for the
bulk of the errors: The modal could (20%), and the adjectives putative (11%), possible
(6%), potential (6%), and unknown (5%). After manually inspecting the full set of FPs,
however, we find that at least 60% of them should really be counted as true positives.
The following are just a few examples where cues predicted by our classifier are not
annotated as such in BioScope and therefore counted as FPs.
(9) IEF-1, a pancreatic beta-cell type-specific complex ?believed? to regulate
insulin expression, is demonstrated to consist of at least two distinct
species, [. . . ]
(10) We ?hypothesize? that a mutation of the hGR glucocorticoid-binding
domain is the cause [. . . ]
(11) Antioxidants have been ?proposed? to be anti-atherosclerotic agents; [. . . ]
(12) Finally, matDCC might be further stabilized by the addition of roX1 RNA,
which could interact with several of the MSLs and ?perhaps? roX2 RNA
as well.
380
Velldal et al Rules, Rankers, and the Role of Syntax
One interesting source of real FPs concerns ?anti-hedges,? which in the training data
appear with a negation and as part of a multiword cue, for example no proof. During
testing, the classifier will sometimes wrongly predict a word like proof to be a specu-
lation cue, even when it is not negated. Because we already have MWC rules for cases
like this (see Section 5.1) it would be easy to also include a check for ?negative context,?
making sure that such tokens are not classified as cues if the requiredmultiword context
is missing.
Before rounding off this section, a brief look at the BioScope inter-annotator agree-
ment rates may offer some further perspective on the results discussed here. Note that
when creating the BioScope data, the decisions of two independent annotators were
merged by a third expert linguist who resolved any differences. The F1 of each set of
annotations toward the final gold-standard cues are reported by Vincze et al (2008) to
be 83.92 / 92.05 for the abstracts and 81.49 / 90.81 for the full papers. (Recall from Table 3
that our cue-level F1 for the cross-validation runs on the abstracts and papers is 89.11.)
When instead comparing the decisions of the two annotators directly, the F1 is reported
to be 79.12 for the abstracts and 77.60 for the papers.
5.3 Held-Out Results for Identifying Speculation Cues
Table 4 presents the final evaluation of the various cue classifiers developed in this
section, as applied to the held-out BSE test data. In addition to the evaluation results for
our own classifiers, Table 4 also includes the official test results for the system described
by Tang et al (2010). The sequence classifier developed by Tang et al (2010)?combining
a CRF classifier and a large-margin HMM model?obtained the best results for the
official Shared Task evaluation for Task 1 (i.e., sentence-level uncertainty detection), as
well as the highest cue-level scores.
As seen from Table 4, although themodel of Tang et al (2010) still achieves a slightly
higher F1 (81.34) than our filtered disambiguation model for the cue-level, our model
achieves a slightly higher F1 (86.58) for the sentence-level (yielding the best-published
result for this task so far, to the best of our knowledge). The differences are not deemed
statistically significant by a two-tailed sign-test, however (p = 0.37). It is interesting to
note, however, that the two approaches appear to have somewhat different strengths
and weaknesses: Whereas our filtering classifier consistently shows stronger precision
(and theWbWmodel even more so), the model of Tang et al (2010) is stronger on recall.
The sentence-level recall of our filtered classifier is still better than any of the remaining
23 systems submitted for the Shared Task evaluation, however, and, more interestingly,
it improves substantially on the recall of the full WbW classifier.
Table 4
Held-out results for identifying speculation cues: Applying the cue classifiers to the 5,003
sentences in BSE? the biomedical papers provided for the CoNLL-2010 Shared Task evaluation.
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 77.59 81.52 79.51 77.16 72.39 74.70 75.15 72.49 73.80
WbW 89.28 83.29 86.18 87.62 73.95 80.21 86.33 74.21 79.82
Filtering 87.87 85.32 86.58 86.46 76.74 81.31 84.79 77.17 80.80
Tang et al 2010 85.03 87.72 86.36 n/a n/a n/a 81.70 80.99 81.34
381
Computational Linguistics Volume 38, Number 2
We find that, just as for the development data, the reformulation of the cue clas-
sification task as a simple disambiguation problem improves F1 across all evaluation
levels, consistently outperforming the WbW classifiers. When computing a two-tailed
signed-test for the token-level decisions (where the WbW and filtering model achieves
an F1 of 80.21 and 81.31, respectively) the differences are not found to be significant (p =
0.12). As discussed in Section 5.2, however, it is important to bear in mind that the size
and complexity of the filtered ?disambiguation? model is greatly reduced compared
to the WbW model, using a much smaller number of features and relevant training
examples.
While on the topic of model complexity, it is also worth noting that many of the
systems participating in the CoNLL-2010 Shared Task challenge used fairly complex
and resource-heavy feature types, being sensitive to properties of document structure,
grammatical relations, deep syntactic structure, and so forth (Farkas et al 2010). The fact
that comparable or better results can be obtained using a relatively simplistic approach
as developed in this section, with surface-oriented features that are only sensitive to the
immediate lexical context, is an interesting result in its own right. In fact, even the simple
majority usage baseline classifier proves to be surprisingly competitive: Comparing its
sentence-level F1 to those of the official Shared Task evaluation, it actually outranks 7 of
the 24 submitted systems.
A final point that deserves some discussion is the drop in F1 that we observe when
going from the development results to the held-out results. There are several reasons for
this drop. Section 2.3 discussed how certain overfitting effects might be expected from
the GENIA-based pre-processing. In addition to this, it is likely that there are MWC
patterns in the held-out data that were not observed in the training data, and that are
therefore not covered by our MWC rules. Another factor that may have slightly inflated
the development results is the fact that we used a sentence-level rather than a document-
level partitioning of the data for cross-validation.
6. Resolving the Scope of Speculation Cues
Once the speculation cue has been determined using the cue detection system described
here, we go on to determine the scope of the speculation within the sentence. This task
corresponds to Task 2 of the CoNLL-2010 Shared Task. Example (13), which will be
used as a running example throughout this section, shows a scope-resolved BioScope
sentence where speculation is signaled by the modal verb may.
(13) {The unknown amino acid ?may? be used by these species}.
The exact scope will vary quite a lot depending on linguistic properties of the cue
in question, and in our approaches to scope resolution we rely heavily on syntactic
information. We experiment with two different approaches to syntactic analysis; data-
driven dependency parsing and grammar-driven phrase structure parsing. Because
scope determination in BioScope makes reference to subtle and fine-grained linguistic
distinctions (e.g., passivization or subject raising), in both cases we choose parsing
systems that make available comparatively ?deep? syntactic analyses. In the following
we present three different systems; a rule-based approach using dependency structures
(Section 6.1), a data-driven approach using an SVM ranker for selecting appropriate
subtrees in constituent structures (Section 6.2), and finally a hybrid approach combining
the rules and the ranker (Section 6.3).
382
Velldal et al Rules, Rankers, and the Role of Syntax
6.1 A Rule-Based Approach Using Dependency Structures
?vrelid, Velldal, and Oepen (2010) applied a small set of heuristic rules oper-
ating over syntactic dependency structures to define the scope for each cue. In the
following we will provide a detailed description of these rules and the syntactic gen-
eralizations they provide for the scope of speculation (Section 6.1.2). We will evalu-
ate their performance using both gold-standard cues and cues predicted by our cue
classifier (Section 6.1.3), in addition to providing an in-depth manual error analysis
(Section 6.1.5). We start out, however, by presenting some specifics about the processing
of the data; introducing the stacked dependency parser that produces the input to our
rules (Section 6.1.1) and quantifying the effect of using a domain-adapted PoS tagger
(Section 6.1.4).
6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source Malt-
Parser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing.
For improved accuracy and portability across domains and genres, we make our parser
incorporate the predictions of a large-scale, general-purpose Lexical-Functional Gram-
mar parser. A technique dubbed parser stacking enables the data-driven parser to learn
from the output of another parser, in addition to gold-standard treebank annotations
(Martins et al 2008; Nivre and McDonald 2008). This technique has been shown to
provide significant improvements in accuracy for both English and German (?vrelid,
Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown
to increase domain independence in data-driven dependency parsing (Zhang andWang
2009). The stacked parser used here is identical to the parser described in ?vrelid,
Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and
PoS tagging, which is performed as detailed in Sections 2.1?2.2. The parser combines
two quite different approaches?data-driven dependency parsing and ?deep? parsing
with a hand-crafted grammar?and thus provides us with a broad range of different
types of linguistic information to draw upon for the speculation resolution task.
MaltParser is based on a deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parse transitions. It supports a rich feature
representation of the parse history in order to guide parsing andmay easily be extended
to take into account additional features. The procedure to enable the data-driven parser
to learn from the grammar-driven parser is quite simple. We parse a treebank with
the XLE platform (Crouch et al 2008) and the English grammar developed within the
ParGram project (Butt et al 2002). We then convert the LFG output to dependency
structures, so that we have two parallel versions of the treebank?one gold-standard
and one with LFG annotation. We extend the gold-standard treebank with additional
information from the corresponding LFG analysis and train MaltParser on the
enhanced data set. For a description of the parse model features and the dependency
substructures proposed by XLE for each word token, see Nivre and McDonald (2008).
For further background on the conversion and training procedures, see ?vrelid, Kuhn,
and Spreyer (2009).
Table 5 shows the enhanced dependency representation for the sentence in Ex-
ample (13). For each token, the parsed data contains information on the word form,
lemma, and PoS, as well as the head and dependency relation (last two columns). The
added XLE information resides in the Features column and in the XLE-specific head and
dependency columns (XHead and XDep). Parser outputs, which in turn form the basis
for our scope resolution rules, also take this same form. The parser used in this work is
trained on the Wall Street Journal Sections 2?24 of the Penn Treebank (PTB), converted
383
Computational Linguistics Volume 38, Number 2
Table 5
Stacked dependency representation of the sentence in Example (13), lemmatized and annotated
with GENIA PoS tags, Malt parses (Head,DepRel), and XLE parses (XHead, XDep), as well as
other morphological and lexical semantic features extracted from the XLE analysis (Features).
Id Form PoS Features XHead XDep Head DepRel
1 The DT _ 4 SPECDET 4 NMOD
2 unknown JJ degree:attributive 4 ADJUNCT 4 NMOD
3 amino JJ degree:attributive 4 ADJUNCT 4 NMOD
4 acid NN pers:3|case:nom|num:sg|ntype:common 3 SUBJ 5 SBJ
5 may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0 ROOT 0 ROOT
6 be VB _ 7 PHI 5 VC
7 used VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 5 XCOMP 6 VC
8 by IN _ 9 PHI 7 LGS
9 these DT deixis:proximal 10 SPECDET 10 NMOD
10 species NNS num:pl|pers:3|case:obl|common:count|ntype:common 7 OBL-AG 8 PMOD
11 . . _ 0 PUNC 5 P
to dependency format (Johansson andNugues 2007) and extendedwith XLE features, as
described previously. Parsing uses the arc-eager mode of MaltParser and an SVMwith a
polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the
parser achieves a labeled accuracy score of 89.8, which is lower than the current state-
of-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and
Nivre 2011, although not directly comparable given that they test exclusively on WSJ
Section 23), but with the advantage of providing us with the deep linguistic information
from the XLE.
6.1.2 Rule Overview. Our scope resolution rules take as input a parsed sentence that has
been further tagged with speculation cues. We assume the default scope to start at the
cue word and span to the end of the sentence (modulo punctuation), and this scope also
provides the baseline when evaluating our rules.
In developing the rules, we made use of the information provided by the guidelines
for scope annotation in the BioScope corpus (Vincze et al 2008), combined with manual
inspection of the training data in order to further generalize over the phenomena
discussed by Vincze et al (2008) and work out interactions of constructions for various
types of cues. In the following, we discuss broad classes of rules, organized by categories
of speculation cues. An overview is also provided in Table 6, detailing the source of the
syntactic information used by the rule; MaltParser (M) or XLE (X). Note that, as there is
no explicit representation of phrase or clause boundaries in our dependency universe,
we assume a set of functions over dependency graphs, for example, finding the left- or
rightmost (direct) dependent of a given node, or recursively selecting left- or rightmost
descendants.
Coordination. The dependency analysis of coordination provided by our parser makes
the first conjunct the head of the coordination. For cues that are coordinating conjunc-
tions (PoS tag CC), such as or, we define the scope as spanning the whole coordinate
structure, that is, start scope is set to the leftmost dependent of the head of the coordina-
tion, and end scope is set to its rightmost dependent (conjunct). This analysis provides
us with coordinations at various syntactic levels, such as NP and N, AP and AdvP, or
VP as in Example (14):
(14) [...] the binding interfaces are more often {kept ?or? even reused} rather
than lost in the course of evolution.
384
Velldal et al Rules, Rankers, and the Role of Syntax
Table 6
Overview of dependency-based scope rules with information source (MaltParser or XLE),
organized by the triggering PoS of the cue.
PoS Description Source
cc Coordinations scope over their conjuncts M
in Prepositions scope over their argument with its descendants M
jjattr Attributive adjectives scope over their nominal head and its descendants M
jjpred Predicative adjectives scope over referential subjects and clausal arguments, M, X
if present
md Modals inherit subject-scope from their lexical verb and scope over their M, X
descendants
rb Adverbs scope over their heads with its descendants M
vbpass Passive verbs scope over referential subjects and the verbal descendants M, X
vbrais Raising verbs scope over referential subjects and the verbal descendants M, X
* For multiword cues, the head determines scope for all elements
* Back off from final punctuation and parentheses
Adjectives.We distinguish between adjectives (JJ) in attributive (nmod) function and adjec-
tives in predicative (prd) function. Attributive adjectives take scope over their (nominal)
head, with all its dependents, as in Example (15):
(15) The {?possible? selenocysteine residues} are shown in red, [...]
For adjectives in a predicative function the scope includes the subject argument of the
head verb (the copula), as well as a (possible) clausal argument, as in Example (16). The
scope does not, however, include expletive subjects, as in Example (17).
(16) Therefore, {the unknown amino acid, if it is encoded by a stop codon, is
?unlikely? to exist in the current databases of microbial genomes}.
(17) [...] it is quite {?likely? that there exists an extremely long sequence that is
entirely unique to U}.
Verbs. The scope of verbal cues is a bit more complex and depends on several factors.
In our rules, we distinguish passive usages from active usages, raising verbs from non-
raising verbs, and the presence or absence of a subject-control embedding context. The
scopes of both passive and raising verbs include the subject argument of their head
verb, as in Example (18), unless it is an expletive pronoun, as in Example (19).
(18) {Genomes of plants and vertebrates ?seem? to be free of any recognizable
Transib transposons} (Figure 1).
(19) It has been {?suggested? that unstructured regions of proteins are often
involved in binding interactions, particularly in the case of transient
interactions} 77.
In the case of subject control involving a speculation cue, specifically modals, sub-
ject arguments are included in scopes where the controller heads a passive construction
or a raising verb, as in our running Example (13).
385
Computational Linguistics Volume 38, Number 2
In general, the end scope of verbs should extend over the minimal clause that
contains the verb in question. In terms of dependency structures, we define the clause
boundary as comprising the chain of descendants of a verb which is not intervened by
a token with a higher attachment in the graph than the verb in question.
Prepositions and Adverbs. Cues that are tagged as prepositions (including some com-
plementizers) take scope over their argument, with all its descendants, Example (20).
Adverbs take scope over their head with all its (non-subject) syntactic descendants
Example (21).
(20) {?Whether? the codon aligned to the inframe stop codon is a nonsense
codon or not} was neglected [...]
(21) These effects are {?probably? mediated through the 1,25(OH)2D3
receptor}.
Multiword Cues. In the case of multiword cues, such as indicate that or either. . . or, we set
the scope of the unit as a whole to the maximal scope encompassing the scopes of both
units.
As an illustration of processing by the rules, consider our running Example (13),
with its syntactic analysis as shown in Table 5 and the dependency graph depicted
in Figure 2. This example invokes a variety of syntactic properties, including parts of
speech, argumenthood, voice, and so on. Initially, the scope of the speculation cue is
set to default scope. Then the subject control rule is applied, it checks the properties of
the verbal argument used, going through a chain of verbal dependents (VC) from the
modal verb may (indicated in red in Figure 2). Because it is marked as passive in
the LFG analysis (+pass), the start scope is set to include the subject of the cue word
(the leftmost descendant [NMOD] of its SBJ dependent, indicated in green in Figure 2).
6.1.3 Evaluating the Rules. Table 7 summarizes scope resolution performance (viewed as
a subtask in isolation) against both the CoNLL-2010 shared task training data (BSA and
BSP) and held-out evaluation data (BSE), using gold-standard cues. First of all, we note
that the default scope baseline, that is, unconditionally extending the scope of a cue to
the end of the sentence, yields much better results for the abstracts than the full papers.
The main reason is simply that the abstracts contain almost no cases of sentence final
bracketed expressions (e.g., citations and in-text references). Our scope rules improve
on the baseline by only 3.8 percentage points on the BSA data (F1 up from 69.84 to
Figure 2
Dependency representation for Example (13), indicating rule processing of the cue word may.
386
Velldal et al Rules, Rankers, and the Role of Syntax
Table 7
Resolving the scope of gold-standard speculation cues in the development and held-out data
using the dependency rules. For Default, the scope for each cue is always taken to span
rightwards to the end of the sentence.
Data Configuration F1
B
S
A Default 69.84
Dependency Rules 73.67
B
S
P Default 45.21
Dependency Rules 72.31
B
S
E Default 46.95
Dependency Rules 66.60
73.67). For BSP, however, we find that the rules improve on the baseline by as much as
27 points (up from 45.21 to 72.31). Similarly for the papers in the held-out BSE data, the
rules improve the F1 by 19.7 points (F1 up from 46.95 to 66.60).
Comparing to the result on the training data, we observe a substantial drop in
performance on the held-out data. There are several possible explanations for this effect.
First of all, there may well be some degree of overfitting of our rules to the training
data. The held-out data may contain speculation constructions that are not covered
by our current set of scope rules, or annotation of parallel constructions may in some
cases differ in subtle ways (see Section 6.1.5). The overfitting effects caused by the data
dependencies introduced by the various GENIA-based domain adaptation steps, as
described in Section 2.3, must also be taken into account.
6.1.4 PoS Tagging and Domain Variation. As mentioned in Section 6.1.1, an advantage of
stacking with a general-purpose LFG parser is that it can be expected to aid domain
portability. Nonetheless, substantial differences in domain and genre are bound to
negatively affect syntactic analysis (Gildea 2001), and our parser is trained on financial
news. MaltParser presupposes that inputs have been PoS tagged, however, leaving
room for variation in preprocessing. In this article we have aimed, on the one hand,
to make parser inputs conform as much as possible to the conventions established in its
PTB training data, while on the other hand taking advantage of specialized resources
for the biomedical domain.
To assess the impact of improved, domain-adapted inputs on our scope resolution
rules, we contrast two configurations: Running the parser in the exact same manner
as ?vrelid, Kuhn, and Spreyer (2009)?the first configuration uses TreeTagger (Schmid
1994) and its standard model for English (trained on the PTB) for preprocessing. In
the second configuration the parser input is provided by the refined GENIA-based
preprocessing described in Section 2.2. Evaluating the two modes of preprocessing on
the BSP subset of BioScope using gold-standard speculation cues, our scope resolution
rules achieve an F1 of 66.31 when using TreeTagger parser inputs, and 72.31 (see Table 7)
using our GENIA-based tagging and tokenization combination. These results underline
the importance of domain adaptation for accurate syntactic analysis.
6.1.5 Error Analysis. In Section 5.2.3 we discussed BioScope inter-annotator agreement
rates for the cue-level. Focusing only on the cases where the annotators agree with the
final gold-standard cues (as resolved by the chief annotator), Vincze et al (2008) report
387
Computational Linguistics Volume 38, Number 2
the scope-level F1 of the two annotators toward the gold standard to be 66.72 / 89.67 for
BSP. Comparing the decisions of the two annotators directly (i.e., treating one of the
annotations as gold-standard) yields an F1 of 62.50.
Using gold-standard cues, our scope resolution rules fail to exactly replicate the
target annotation in 185 (of 668) cases in the papers portion of the training material
(BSP), corresponding to an F1 of 72.31 as seen in Table 7. Two of the authors, who
are both trained linguists, performed a manual error analysis of these 185 cases. They
classify 156 (84%) as genuine system errors, 22 (12%) as likely3 annotation errors,
and the remaining 7 cases as involving controversial or seemingly arbitrary decisions
(?vrelid, Velldal, and Oepen 2010). Out of the 156 system errors, 85 (55%) were deemed
as resulting from missing or defective rules, and 71 system errors (45%) resulted from
parse errors. The latter were annotated as parse errors even in cases where there was
also a rule error.
The two most frequent classes of system errors pertain to (a) the recognition of
phrase and clause boundaries and (b) not dealing successfully with relatively superficial
properties of the text. Examples (22) and (23) illustrate the first class of errors, where
in addition to the gold-standard annotation we use vertical bars (?|?) to indicate scope
predictions of our system.
(22) [. . . ] {the reverse complement |mR of m will be ?considered? to . . . ]|}
(23) This |{?might? affect the results} if there is a systematic bias on the
composition of a protein interaction set|.
In our syntax-driven approach to scope resolution, system errors will almost always
correspond to a failure in determining constituent boundaries, in a very general sense.
In Example (22), for instance, the parser has failed to correctly locate the head of the
subject. Example (23), however, is specifically indicative of a key challenge in this task,
where adverbials of condition, reason, or contrast frequently attach within the depen-
dency domain of a speculation cue, yet are rarely included in the scope annotation.
For these system errors, the syntactic analysis may well be correct, although additional
information is required to resolve the scope.
Example (24) demonstrates our second frequent class of system errors. One in six
items in the BSP training data contains a sentence-final parenthesized element or trailing
number (e.g., Examples [18] or [19]); most of these are bibliographic or other in-text
references, which are never included in scope annotation. Hence, our system includes a
rule to ?back out? from trailing parentheticals; in cases such as Example (24), however,
syntax does not make explicit the contrast between an in-text reference versus another
type of parenthetical.
(24) More specifically, {|the bristle and leg phenotypes are ?likely? to result
from reduced signaling by Dl| (and not by Ser)}.
3 In some cases, there is no doubt that annotation is erroneous, that is, in violation of the available
annotation guidelines (Vincze et al 2008) or in conflict with otherwise unambiguous patterns. In other
cases, however, judgments are necessarily based on our own generalizations (e.g., assumptions about
syntactic analyses implicit in the BioScope annotations). Furthermore, selecting items for manual analysis
that do not align with the predictions made by our scope resolution rules is likely to bias our sample, such
that our estimated proportion of 12% annotation errors cannot be used to project an overall error rate.
388
Velldal et al Rules, Rankers, and the Role of Syntax
Moving on to apparent annotation errors, the rules for inclusion (or not) of the
subject in the scope of verbal speculation cues and decisions on boundaries (or internal
structure) of nominals seem problematic?as illustrated in Examples (25) and (26).4
(25) [. . . ] and |this is also {?thought? to be true for the full protein interaction
networks we are modeling}|.
(26) [. . . ] |redefinition of {one of them is ?feasible?}|.
Finally, the difficult corner cases invoke non-constituent coordination, ellipsis,
or NP-initial focus adverbs?and of course interactions of the phenomena discussed
herein. Without making the syntactic structures assumed explicit, it is often very diffi-
cult to judge such items.
6.2 A Data-Driven Approach Using an SVM Constituent Ranker
The error analysis indicated that it is often difficult to use dependency paths to define
phenomena that actually correspond to syntactic constituents. Furthermore, we felt that
the factors governing scope resolution would be better expressed in terms of soft con-
straints instead of absolute rules, thus enabling the scope resolver to consider a range
of relevant (potentially competing) contextual properties. In this section we describe
experiments with a novel approach to determining the in-sentence scope of speculation
that, rather than usingmanually defined heuristics operating on dependency structures,
instead uses a data-driven approach, ranking candidate scopes on the basis of constituent
trees. More precisely, our parse trees are licensed by the LinGO English Resource Gram-
mar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in
the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two
main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to
a syntactic constituent and secondly, that we can automatically learn a ranking function
that selects the correct constituent.
Our ranking approach to scope resolution is abstractly related to statistical parse
selection, and in particular work on discriminative parse selection for unification based
grammars, such as those by Johnson et al (1999), Riezler et al (2002), Malouf and
van Noord (2004), and Toutanova et al (2005). The overall goal is to learn a function
for ranking syntactic structures, based on training data that annotates which tree(s) are
correct and incorrect for each sentence. In our case, however, rather than discriminating
between complete analyses for a given sentence, we want to learn a ranking function
over candidate subtrees (i.e., constituents) within a parse (or possibly evenwithin several
parses). Figure 3 presents an example derivation tree that represents a complete HPSG
analysis. Starting from the cue and working through the tree bottom?up, there are three
candidate constituents to determine scope (marked in bold), each projecting onto a
substring of the full utterance, and each including at least the cue. Note that in the case
of multiword cues the intersection of each word?s candidates is selected, ensuring that
all cues appear within the scope projected by the candidate constituents.
The training data is then defined as follows. Given a parsed BioScope sentence,
the subtree that corresponds to the annotated scope for a given speculation cue will
4 As in the presentation of system errors, we include scope predictions of our own rules here too, which
we believe to be correct in these cases. Also in this class of errors, we find the occasional ?uninteresting?
mismatch, for example related to punctuation marks and inconsistencies around parentheses.
389
Computational Linguistics Volume 38, Number 2
Figure 3
An example derivation tree. Internal nodes are labeled with ERG rule identifiers; common HPSG
constructions near the top (e.g., subject-head, head-complement, adjunct-head), and lexical rules
(e.g., passivization of verbs or plural formation of nouns) closer to the leaves. The preterminals
are so-called LE types, corresponding to fine-grained parts of speech and reflecting close to a
thousand lexical distinctions.
be labeled as correct. Any other remaining constituents that also span the cue are
labeled as incorrect. We then attempt to learn a linear SVM-based scoring function that
reflects these preferences, using the implementation of ordinal ranking in the SVMlight
toolkit (Joachims 2002). Our definition of the training data, however, glosses over two
important details.
Firstly, the grammar will usually license not just one, but thousands or even hun-
dreds of thousands of different parses for a given sentence which are ranked by an
underlying parse selection model. Some parses may not necessarily contain a subtree
that aligns with the annotated scope.We therefore experiment with defining the training
data relative to n-best lists of available parses. Secondly, the rate of alignment between
annotated scopes and constituents of parsing results indicates the upper-bound per-
formance: For inputs where no constituents align with the correct scope substring,
a correct prediction will not be possible. Searching the n-best parses for alignments
enables additional instances of scope to be presented to the learner, however.
In the following, Section 6.2.1 summarizes the general parsing setup for the ERG, as
well as our rationale for the use of HPSG. Section 6.2.2 provides an empirical assessment
of the degree to which ERG analyses can be aligned with speculation scopes in BioScope
and reviews some frequent sources of alignment failures. After describing our feature
types for representing candidate constituents in Section 6.2.3, Section 6.2.4 details the
tuning of feature configurations and other ranker parameters. Finally, Section 6.2.5
provides an empirical assessment of stand-alone ranker performance, before we discuss
the integration of the dependency rules with the ranking approach in Section 6.3.
6.2.1 Basic Set-up: Parsing Biomedical Text Using the ERG. At some level of abstraction,
the approach to grammatical analysis embodied in the ERG is quite similar to the LFG
parser that was ?stacked? with our data-driven dependency parser in Section 6.1.1?
both are commonly considered comparatively ?deep? (and thus costly) approaches to
syntactic analysis. Judging from the BioScope annotation guidelines, subtle grammat-
ical distinctions are at play when determining scopes, for example, different types of
control verbs, expletives, or passivization (Vincze et al 2008). In contrast to the LFG
framework (with its distinction between so-called constituent and functional struc-
tures), the analyses provided by the ERG offer the convenience of a single syntactic
390
Velldal et al Rules, Rankers, and the Role of Syntax
representation?HPSG derivation trees, as depicted in Figure 3?where all contextual
information that we expect to be relevant for scope resolution is readily accessible.
For parsing biomedical text using the ERG, we build on the same preprocessing
pipeline as described in Section 2. A lattice of tokens annotated with parts of speech
and named entity hypotheses contributed by the GENIA tagger is input to the PET
HPSG parser (Callmeier 2002), a unification-based chart parser that first constructs a
packed forest of candidate analyses and then applies a discriminative parse ranking
model to selectively enumerate an n-best list of top-ranked candidates (Zhang, Oepen,
and Carroll 2007). To improve parse selection for this kind of data, we re-trained the
discriminative model following the approach of MacKinlay et al (2011), combining
gold-standard out-of-domain data from existing ERG treebanks with a fully automated
procedure seeking to take advantage of syntactic annotations in the GENIA Treebank.
Although we have yet to pursue domain adaptation in earnest and have not systemat-
ically optimized the parse selection component for biomedical text, model re-training
contributed about a one-point F1 improvement in stand-alone ranker performance over
the parsed subset of BSP (compare to Table 8).
As the ERG has not previously been adapted to the biomedical domain, unknown
word handling in the parser plays an important role. Here we build on a set of
somewhat underspecified ?generic? lexical entries for common open-class categories
provided by the ERG (thus complementing the 35,000-entry lexicon that comes with the
grammar), which are activated on the basis of PoS and NE annotation from preprocess-
ing. Other than these, there are no robustness measures in the parser, such that syntactic
analysis will fail in a number of cases, to wit, when the ERG is unable to derive a
complete, well-formed syntactic structure for the full input string. In this configuration,
the parser returns at least one derivation for 91.2% of all utterances in BSA, and 85.6%
and 81.4% for BSP and BSE, respectively.
6.2.2 Alignment of Constituents and Scopes. The constituent ranking approach makes ex-
plicit an assumption that is also present at the core of our dependency-based heuristics
(viz., the expectation that scope boundaries align with the boundaries of syntactically
meaningful units). This assumption is motivated by general BioScope annotation prin-
ciples, as Vincze et al (2008) suggest that the ?scope of a keyword can be determined
on the basis of syntax.? To determine the degree to which ERG analyses conform to this
expectation, we computed the ratio of alignment between scopes and constituents (over
parsed sentences) in BioScope, considering various sizes of n-best lists of parses. To
improve alignment we also apply a small number of slackening heuristics. These
rules allow (a) minor adjustments of scope boundaries around punctuation marks
Table 8
Ranker optimization on BSP: Showing ranker performance for various feature type
combinations compared with a random-choice baseline, only considering instances
where the gold-standard scope aligns to a constituent within the 1-best parse.
Features F1
Baseline 26.76
Path 78.10
Path+Surface 79.93
Path+Linguistic 83.72
Path+Surface+Linguistic 85.30
391
Computational Linguistics Volume 38, Number 2
(specifically, utterance-final punctuation is never included in BioScope annotations, yet
the ERG analyzes most punctuation marks as pseudo-affixes on lexical tokens; see Fig-
ure 3). Furthermore, the slackening rules (b) reduce the scope of a constituent to the right
when it includes a citation (see the discussion of parentheticals in Section 6.1.5); (c) re-
duce the scope to the left when the left-most terminal is an adverb and is not the cue; and
(d) ensure that the scope starts with the cue when the cue is a noun. Collectively, these
rules improve alignment (over parsed sentences) in BSP from 74.10% to 80.54%, when
only considering the syntactic analysis ranked most probable by the parse selection
model. Figure 4 further depicts the degree of alignment between speculation scopes and
constituents in the n-best derivations produced by the parser, again after application of
the slackening rules. Alignment when inspecting only the top-ranked parse is 84.37%
for BSA and 80.54% for BSP. Including the top 50-best derivations improves alignment
to 92.21% and 88.93%, respectively. Taken together with an observed parser coverage of
85.6% for BSP, these results mean that for only about 76% of all utterances in BSP can
the ranker potentially identify a constituent matching the gold-standard scope.
To shed some light on the cases where we fail to find an alignment, we manually
inspected all utterances in the BSP segment for which there were (a) syntactic analyses
available from the ERG and (b) no candidate constituents in any of the top-fifty parses
that mapped onto the gold-standard scope (after the application of the slackening rules).
The most interesting cases from this non-alignment analysis are ones judged as ?non-
syntactic? (25% of the total mismatches), which we interpret as violating the assumption
of the annotation guidelines under any possible interpretation of syntactic structure.
Following are select examples in this category:
(27) This allows us to {?address a number of questions?: what proportion of
each organism?s protein interaction network [. . . ] can be attributed to a
known domain-domain interaction}?
(28) As {?suggested? in 18, by making more such data sets available, it will be
possible to [. . . ] determine the most likely human interactions}.
Figure 4
The effect of incrementally including additional derivations from the n-best list when searching
for an alignment between a speculation scope and a constituent. Plots are shown for the BSA and
BSP subsets of the training data.
392
Velldal et al Rules, Rankers, and the Role of Syntax
(29) The {lack of specificity ?might? be attributed to a number of reasons, such
as the absence of other MSL components, the presence of other RNAs
interacting with MOF}, or worse [. . . ].
(30) [. . . ] thereby making {the use of this objective function ? and exploration
of other complex objective functions ? ?possible?}.
Example (27) is representative of a handful of similar cases, where complete sen-
tences are (implicitly or overtly) conjoined, yet the scope annotation encompasses only
part of one of the sentences. Example (28) is in a similar spirit, only in this case a
topicalized prepositional phrase (and hence an integral constituent) is only partially
included in the gold-standard scope. Although our slackening heuristics address a
number of cases of partial noun phrases (with a left scope boundary right before the
head noun or a pre-head attributive adjective), another handful of non-syntactic scopes
are of the type exemplified by Example (29), a class observed earlier already in the error
analysis of our dependency-based scope resolution rules (see Section 6.1.5). Finally,
Example (30) demonstrates one of many linguistically subtle corner cases: The causative
make in standard analyses of the resultative construction takes two arguments, namely,
an NP (the use of this objective function. . . ) and a predicative phrase (possible).
Alongside cases like these, our analysis considered 16% of mismatches owed to
divergent syntactic theories (i.e., structures that in principle can be analyzed in amanner
compatible with the BioScope gold-standard annotations, yet do not form matching
constituents in the ERG analyses). The by far largest class of mismatches was attributed
to parse ranking deficiencies: In close to 40% of cases, the ERG is capable of deriving
a constituent structure compatible with the scope annotations, but no such analysis
was available within the top 50 parses. Somewhat reassuringly, less than 6% of all
mismatches were classified as BioScope annotation errors, whereas a majority of re-
maining mismatches are owed to the recurring issue of parentheticals and bibliographic
references (see examples in Section 6.1.5).
6.2.3 Features of Candidate Scopes. We use three families of features to describe candi-
date constituents. Given our working hypothesis that scopes are aligned with syn-
tactic constituents, the most natural features to use are the location of constituents
within trees. We define these in terms of the paths from speculation cues to can-
didate constituents. For example, the correct candidate in Figure 3 has the feature
v_vp_mdl-p_le\hd-cmp_u_c\sb-hd_mc_c. We include both lexicalized and unlexical-
ized versions of this feature. As traversal from the cue to the candidate can involve
many nodes, we also include a more general version recording only the cue and the
root of the candidate constituent (rather than the full path including all intermediate
nodes). In a similar spirit we also generate bigram features for each path node and its
parent.
In addition to the given path features, we also exploit features describing the
surface properties of scope candidates. These include the enumeration of bigrams of
the preterminal lexical types, the cue position within the candidate (in tertile bins
relative to the candidate length), and the candidate size (in quartile bins relative to the
sentence length). Because punctuation may also be informative for scope resolution, we
also record whether punctuation was present at the end of the terminal preceding the
candidate or at the end of its right-most terminal.
The third family of features is concerned with specific linguistic phenomena de-
scribed in the BioScope annotation guidelines (Vincze et al 2008) or observed when
393
Computational Linguistics Volume 38, Number 2
developing the rules in Section 6.1. These include detection of passivization, subject
control verbs occurring with passivized verbs, subject raising verbs, and predicative
adjectives. Furthermore, these features are only activated when the subject of the con-
struction is not an expletive pronoun, and they are represented by appending the type
of phenomenon observed to the path features described here.
6.2.4 Ranker Optimization. We conducted several experiments designed to find an
optimal configuration of features. Table 8 lists the results of combinations of the fea-
ture families on the BSP data set when using gold-standard cues, reporting 10-fold
cross-validated F1 scores with respect to only the instances where the gold-standard
speculation scope aligns with constituents (i.e., the ?ideal circumstances? for the
ranker). The table also lists results for a random-choice baseline, calculated as the
mean ambiguity of each instance (i.e., the averaged reciprocal of the number of can-
didates). The feature optimization results indicate that each feature family is infor-
mative, and that the best result can be obtained by using all three in conjunction.
The comparatively largest improvement in ranker performance is obtained from the
?rule-like? linguistic feature family, which is noteworthy in two respects: First, our
current system includes only four such features, and second, these features parallel
some of the dependency-based rules of Section 6.1.2?suggesting that subtle syntactic
configurations are an important component also in our data-driven approach to scope
resolution.
As discussed in Section 6.2.2 and depicted in Figure 4, searching the best-ranked
parses can greatly increase the number of aligned constituents and thus improve the
upper-bound potential of the ranker. We therefore experimented with training using
the first aligned constituent in n-best derivations. At the same time we varied the
m-best derivations used during testing, using features from all m derivations. We found
that performance did not vary greatly, but that the best result was achieved when
n = 1 and m = 3 (note, however, that such optimization over n-best lists of ERG parses
will play a much greater role in the hybrid approach to scope resolution developed
in Section 6.3). As explained in Section 5.2.1, all experiments use the SVMlight de-
fault value for the regularization parameter, determined analytically from the training
data.
A cursory error analysis conducted over aligned items in BSP indicated similar
errors to those discussed in connection with the dependency rules (see Section 6.1.5).
There are a number of instances where the predicted scope is correct according to the
BioScope annotation guidelines, but the annotated scope is incorrect. We also note some
instances where the rule-like linguistic features are activated on the correct constituent,
but the ranker nevertheless selects a different candidate. In a strictly rule-based system,
these features would act as hard constraints and yield superior results in these cases.
Therefore, these instances seem a prime source of inspiration for further improvements
to the ranker in future work.
6.2.5 Evaluating the Ranker. Table 9 summarizes the performance of the constituent
ranker (coupledwith the default scope baseline in the case of unparsed items) compared
with the dependency rules, resolving the scope of both gold-standard and predicted
speculation cues. We note that the constituent ranker performs slightly superior to the
dependency rules on BSA but inferior (though well above the default scope baseline)
on BSP and BSE. Applying the sign-test (in the manner described in Section 3.2) to the
scope-level performance of the ranker and the rules on the held-out BSE data (using
gold-standard cues), the differences are found to be statistically significant.
394
Velldal et al Rules, Rankers, and the Role of Syntax
Table 9
Resolving the scope of speculation cues using the dependency rules, the constituent ranker,
and their combination. Whereas table (a) shows results for gold-standard cues, table (b) shows
end-to-end results for the cues predicted by the classifier of Section 5.2. Results are shown both
for the BioScope development data (for which both the scope ranker and the cue classifier is
applied using 10-fold cross-validation) and the CoNLL-2010 Shared Task evaluation data.
Data System F1
B
S
A
Rules 73.67
Ranker 75.48
Combined 79.56
B
S
P
Rules 72.31
Ranker 66.17
Combined 75.15
B
S
A
P Rules 73.40
Ranker 73.61
Combined 78.69
B
S
E
Rules 66.60
Ranker 58.37
Combined 69.60
(a) Resolving Gold-Standard Cues
Data System Prec Rec F1
B
S
A
Rules 72.47 66.42 69.31
Ranker 74.27 68.07 71.04
Combined 77.80 71.31 74.41
B
S
P
Rules 69.87 62.13 65.77
Ranker 62.63 55.69 58.95
Combined 72.05 64.07 67.83
B
S
A
P Rules 71.97 65.56 68.61
Ranker 71.99 65.59 68.64
Combined 76.67 69.85 73.11
B
S
E
Rules 58.95 54.21 56.48
Ranker 51.68 47.53 49.52
Combined 62.00 57.02 59.41
(b) Resolving Predicted Cues
Again we also observe a drop in performance for the results on the held-out data
comparedwith the development data.We attribute this drop partly to overfitting caused
by using GENIA abstracts to adapt the parse ranker to the biomedical domain (see
Section 2.3), but primarily to reduced parser coverage and constituent alignment in the
latter data sets. Improving these aspects should result in substantive gains in ranker
performance. Finally, note that the performance of the default baseline (which is much
better for the abstracts than the full papers of BSP and BSE) also carries over to ranker
performance for the cases where we do not have a parse.
6.3 Combining the Constituent Ranker and the Dependency Rules
Although both the constituent ranker and dependency rules perform well in isolation,
they do not necessarily perform well on the same test items. Consequently, we inves-
tigated the effects of combining their predictions. When ERG parses are available for
a given sentence, the dependency rules may be combined with the information used
by the constituent ranker. We implement this coupling by adding features that record
whether the (slackened) span of a candidate constituent matches the span of the scope
predicted by the rules (either exactly or just at one of its boundaries). When an ERG
parse is not available we simply revert to the prediction of the dependency rules.
Adding the rule prediction features may influence the effectiveness of considering
multiple parses, by compensating for the extra ambiguity. We therefore repeated our
examination of the effects of using the best-ranked parses for training and testing the
ranker. Figure 5 plots the effect on F1 for parsed sentences in BSP when including
constituents from the n-best derivations in training, and from the m-best derivations in
testing.We see that, when activating the dependency prediction features, the constituent
ranker performs best for n = 5 and m = 20.
Looking at the performance summaries of Table 9, we see that the combined ap-
proach consistently outperforms both the dependency rules and the constituent ranker
395
Computational Linguistics Volume 38, Number 2
Figure 5
Cross-validated F1 scores of the ranker combined with the dependency rules over gold cues
for parsed sentences from BSP, varying the maximum number of parse results employed for
training and testing.
in isolation, and the improvements are deemed significant with respect to both of them
(comparing results for BSE using gold-standard cues).
Comparing the combined approach to the plain ranker runs, there are two sources
for the improvements: the addition of the rule prediction features and the fact that we
fall back on using the rule predictions directly (rather than the default scope) when we
do not have an available constituent tree. To isolate the contribution of these factors,
we applied the ranker without the rule-prediction features (as in the initial ranker
set-up), but still using the rules as our fall-back strategy (as in the combined set-up).
Testing on BSP using gold-standard cues this gives an F1 of 69.61, meaning that the
8.98-percentage-point improvement of the combined model over the plain ranker owes
3.44 points to the rule-based fall-back strategy and 5.54 to the new rule-based features.
As discussed in Section 6.2.2, an important premise of the success of our ranking
approach is that scopes align with constituents. Indeed, we find that the performance
of both the ranker in isolation and the combined approach is superior on BSA, which is
the data set that exhibits the greatest proportion of aligned instances. We can therefore
expect that any improvements in our alignment procedure, as well as in the domain-
adapted ERG parse selection model, will also carry through to improve the overall
performance of our subtree ranking.
As a final evaluation of speculation resolution, Table 10 compares the end-to-end
performance of our combined approach with the best end-to-end performer in the
CoNLL-2010 Shared Task. In terms of both precision and recall, our cue classifier using
the combination of constituent ranking and dependency rules for scope resolution
achieves superior performance on BSE comparedwith the system ofMorante, van Asch,
and Daelemans (2010), improving on the overall F1 by more than 2 percentage points.
Whereas the token-level differences for cue classification are found to be significant, the
end-to-end scope-level differences are not (p = 0.39).
7. Porting the Speculation System to Negation
Dealing with negation in natural language has been a long-standing topic and there has
been work attempting to resolve the scope of negation in particular within the area of
sentiment analysis (Moilanen and Pulman 2007), where treatment of negation clearly
constitutes an important subtask and has been shown to provide improved sentiment
396
Velldal et al Rules, Rankers, and the Role of Syntax
Table 10
Final end-to-end results for scope resolution: Held-out testing on BSE, using the cue classifier
described in Section 5.2 while combining the dependency rules and the constituent ranker for
scope resolution. The results are compared to the system with the best end-to-end performance
in the CoNLL-2010 Shared Task (Morante, van Asch, and Daelemans 2010).
Cue Level Scope Level
System Configuration Prec Rec F1 Prec Rec F1
Cue classifier + Scope Rules & Ranking 84.79 77.17 80.80 62.00 57.02 59.41
Morante et al 2010 78.75 74.69 76.67 59.62 55.18 57.32
analysis (Councill, McDonald, and Velikovich 2010). The BioScope corpus (Vincze et al
2008), being annotated with negation as well as speculation, has triggered work on
negation detection in the biomedical domain as well. In this setting, there are a few
previous studies where the same system architecture has been successfully applied for
both speculation and negation. For example, whereas Morante and Daelemans (2009a)
try to resolve the scope of speculation using a system initially developed for negation
(Morante, Liekens, and Daelemans 2008), Zhu et al (2010) develop a system targeting
both tasks. In this section we investigate to what degree our speculation system can be
ported to also deal with negation, hoping that the good results obtained for speculation
will carry over to the negation task at a minimal cost in terms of adaption and modifica-
tion. We start by describing our experiments with porting the cue classifier to negation
in Section 7.1, and then present our modified set of dependency rules for resolving the
scope of the negation cues in Section 7.2. Section 7.3 presents the adaptation of the
constituent ranker, as well as the final end-to-end results when combining the ranker
and the rules, paralleling what we did for speculation. The relation to other relevant
work is discussed as we go along.
Some summarizing statistics for the negation annotations in BioScope were given
in Table 1. Note, however, that the additional evaluation data that we used for held-out
testing of the speculation system, does not contain negation annotations. For this reason,
and in order to be able to compare our results to those obtained in previous studies, we
here follow the partitioning established by Morante and Daelemans (2009b), reporting
10-fold cross-validation (for the cue classifier and the subtree ranker) on the abstracts
(BSA) and using the full papers (BSP) for held-out and cross text-type testing. Note
that for the development results using cross-validation, we partition the data on the
sentence-level, just as in Morante and Daelemans (2009b).
7.1 Identifying Negation Cues
Several previous approaches to detecting negation cues have been based on pre-
compiled lexicons, either alone (Councill, McDonald, and Velikovich 2010) or in combi-
nation with a learner (Morante and Daelemans 2009b). For the purpose of the current
article we wanted to investigate whether the ?filtered classification? approach that we
applied for detecting speculation cues would directly carry over to negation. Drawing
heavily on much of the discussion previously given for the speculation cue classifiers
in Section 5, the small modifications made to implement a classifier for negation cues
are described in Section 7.1.1. We then provide some discussion of the results in Sec-
tion 7.1.2, including comparison to previous work on negation cue detection byMorante
and Daelemans (2009b) and Zhu et al (2010) in Section 7.1.3.
397
Computational Linguistics Volume 38, Number 2
7.1.1 Classifier Description. Apart from re-tuning the feature configuration, the only
modifications that wemade with respect to the speculation classifier regard the rules for
multiword cues (as described for speculation in Section 5.1) and the corresponding stop-
list (Section 5.2). The overall approach, however, is the same:We train and apply a linear
SVM classifier that only considers words whose lemma has been observed as a negation
cue in the training data. Note that roughly 82% of the negation tokens are ambiguous in
the training data, in the sense that they have both cue and non-cue occurrences. Based
on the most frequently occurring MWC patterns observed in the abstracts we defined
post-processing rules to cover the cases shown in Table 11. Furthermore, and again
based on the MWCs, we compiled a small stop-list so that the classifier ignores certain
?spurious? tokens (namely, can, could, notable, of, than, the, with, and ?(?). Although this
of course means that the classifier will never label any such word as a cue, they will
typically be captured by the MWC rules instead.
When re-tuning the feature configuration based on the n-gram templates previously
described in Section 5.2.1, we find that the best performer for negation is the combina-
tion that records lemmas two positions to the left and the right of the target word, and
surface forms one position to the right.
7.1.2 Development Results. The performance of this model, evaluated by 10-fold cross-
validation on the BioScope abstracts, is shown in Table 12. Just as for speculation, we
also contrast the performance with a simple WbW majority usage baseline, classifying
each and every word according to its most frequent usage (cue vs. non-cue) in the
training data. Although this baseline proved to be surprisingly strong for speculation, it
is even stronger for negation: Evaluated at the token-level (though after the application
of the MWC rules) the baseline achieves an F1 of 93.60. Applying the filtering model
further improves this score to 96.00. The differences are found to be statistically signifi-
cant (according to the testing scheme described in Section 3.1), and the filtering classifier
also improves greatly with respect to the sentence-, and cue-level evaluations as well,
in particular with respect to the precision.
Recall that, when looking at the distribution of error types for the token-level
mistakes made by the speculation classifier (see Section 5.2.3), we found that almost 75%
were false negatives. The distribution of error types for the negation cue classifier is very
different: Almost 85% of the errors are false positives. After inspecting the actual cues
involved, we find the same situation as reported by Morante and Daelemans (2009b),
namely, that a very high number of the errors concern cases where not is labeled as a cue
by the classifier but not in the annotations. The same is true for the cue word absence,
and many of these cases appear to be annotation errors.
The class balance among tokens in the BioScope data is extremely skewed, with the
positive examples of negation constituting only 0.5% of the total number of examples.
Table 11
Patterns covered by our post-processing rules for multiword negation cues.
rather than
{can|could} not
no longer
instead of
with the * exception of
neither * nor
{no(t?)|neither} * nor
398
Velldal et al Rules, Rankers, and the Role of Syntax
Table 12
Results for negation cue detection, including the systems of Morante et al (2009b) and Zhu et al
(2010). Whereas the scores for BSA are obtained by 10-fold cross validation, the scores on BSP
and BSR represent held-out testing using a model trained on all the abstracts. The latter scores
thereby serves as a test of generalization performance across different text types within the
same domain.
Sentence Level Token Level Cue Level
Data Model Prec Rec F1 Prec Rec F1 Prec Rec F1
B
S
A
(1
0
-F
o
ld
) Baseline 90.34 98.81 94.37 89.28 98.40 93.60 88.92 97.78 93.14
Filtering 94.19 98.87 96.45 93.46 98.73 96.00 93.19 98.12 95.59
Morante n/a n/a n/a 84.72 98.75 91.20 94.15 90.67 92.38
Zhu n/a n/a n/a 94.35 94.99 94.67 n/a n/a n/a
B
S
P
(H
e
ld
-o
u
t) Baseline 79.48 99.41 88.34 75.96 99.00 85.96 74.55 98.41 84.84
Filtering 86.75 98.53 92.27 85.22 98.25 91.27 84.06 97.62 90.33
Morante n/a n/a n/a 87.18 95.72 91.25 85.55 78.31 81.77
Zhu n/a n/a n/a 87.47 90.48 88.95 n/a n/a n/a
B
S
R
(H
e
ld
-o
u
t) Baseline 96.64 96.42 96.53 96.12 96.01 96.06 95.87 95.98 95.93
Filtering 96.97 96.30 96.64 96.44 95.90 96.17 96.20 95.87 96.03
Morante n/a n/a n/a 97.33 98.09 97.71 96.38 91.62 93.94
Zhu n/a n/a n/a 88.54 86.81 87.67 n/a n/a n/a
In terms of the tokens actually considered by our filtering model, however, the numbers
look much healthier, with the negative examples actually being slightly outweighed
by the positives (just above 50%). Moreover, the average number of distinct n-gram
features instantiated across the 10-folds is approximately 17,500. The small size of the
feature set is of course due to the small number of training examples considered by the
learner: Whereas a WbW approach (like the majority usage baseline) would consider
every token in training data (just below 300,000 in each fold), this number is reduced
by almost 99% for the filtered disambiguation model. In effect, we can conclude that the
proposed approachmanages to combine very good results with very low computational
cost.
Figure 6 shows learning curves for both the word-by-word baseline and the fil-
tering model, plotting token-level F1 against percentages of data included in training.
Compared to the learning curves previously shown for speculation detection (Figure 1),
the curve for the filtering model seems to be somewhat flatter for negation. Looking at
the curve for the WbW unigram baseline, it again seems unable to benefit much from
any additional data after the first few increments.
7.1.3 Comparison to Related Work. To the best of our knowledge, the systems currently
achieving state-of-the-art results for detecting negation cues are those described by
Morante and Daelemans (2009b), Zhu et al (2010), and Councill, McDonald, and
Velikovich (2010). Although the latter work does not offer separate evaluation of the
cue detection scheme in isolation, Morante and Daelemans (2009b) and Zhu et al (2010)
provide cue evaluation for the data splits listed in Table 12; 10-fold cross-validation
experiments (with sentence-level partitioning) on the BioScope abstracts, and held-out
testing on the full papers and the clinical reports (with a model trained on the abstracts).
399
Computational Linguistics Volume 38, Number 2
Figure 6
Learning curves for both baseline and the filtered ?disambiguation? model showing the effect on
token-level negation cue F1 when including larger percentages (shown on a logarithmic scale)
of the training data across the 10-fold cycles on BSA.
The results5 reported by Morante and Daelemans (2009b) and Zhu et al (2010) are
token-level precision, recall, and F1. Having obtained the system output of Morante
and Daelemans (2009b), however, we also computed cue-level scores for their system.
Morante and Daelemans (2009b) identify cues using a small list of unambiguous
cue words compiled from the abstracts in combination with applying a decision tree
classifier to the remaining words. Their features record information about neighboring
word forms, PoS, and chunk information from GENIA. Zhu et al (2010) train an SVM
to classify tokens according to a BIO-scheme using surface-oriented n-gram features in
addition to various syntactic features extracted using the Berkley parser (Petrov and
Klein 2007) trained on the GENIA treebank. Looking at the results in Table 12, we see
that the performance of our cue classifier compares favorably with the systems of both
Morante and Daelemans (2009b) and Zhu et al (2010), achieving a higher cue-level F1
across all data sets (with differences in classifier decisions with respect to Morante and
Daelemans [2009b] being statistically significant for all of them).
For the 10-fold run, the biggest difference concerns token-level precision, where
both the system of Zhu et al (2010) and our own achieves a substantially higher score
than that of Morante and Daelemans (2009b). Turning to the cross-text experiment,
however, the precision of our system and that of Zhu et al (2010) suffers a large
drop, whereas the system of Morante and Daelemans (2009b) actually obtains a higher
precision than for the 10-fold run. These effects are reversed for recall, however, where
our system still maintains the higher score, also resulting in a higher F1. Looking at
the cue-level scores, we find that the precision of our system and that of Morante and
Daelemans (2009b) drops by an equal amount for the BSP cross-text testing. In terms of
recall, however, the cue-level scores of Morante and Daelemans (2009b) suffers a much
larger drop than that of our filtered classifier.
5 As the results reported by Morante and Daelemans (2009b) were inaccurate, we instead refer to values
obtained from personal communication with the authors.
400
Velldal et al Rules, Rankers, and the Role of Syntax
The drop in performance when going from cross-validation to held-out testing
can largely be attributed to the same factors discussed in relation to speculation
cues in Section 5.3 (e.g., GENIA-based pre-processing, sentence-level partitioning in
cross-validation, and unobserved MWCs). In addition, looking at the BioScope inter-
annotator agreement rates for negation cues it is not surprising that we should ob-
serve a drop in results going from BSA to BSP: Measured as the F1 of one of the
annotators with respect to the other, it is reported as 91.46 for BSA, compared with
79.42 for BSP (Vincze et al 2008). Turning to the F1-scores of each annotator with
respect to the final gold standard, the numbers are 91.71/98.05 for BSA and 86.77/91.71
for BSP.
The agreement rates for the clinical reports, on the other hand, are much closer to
those of the abstracts (Vincze et al 2008), and the held-out scores we observe on this data
set are generally also much better, not the least for the simple majority usage baseline.
In general the baseline again proves to be surprisingly competitive, most notably with
respect to recall where it actually outperforms all the other systems for both the cross-
text experiments. (Recall that the baseline scores also reflect the application of the MWC
rules, though.)
7.2 Adapting the Dependency Rules for Resolving Negation Scope
There have been several previous studies on resolving the scope of negation based on
the BioScope corpus. For example, Morante and Daelemans (2009b) present a meta-
learning approach that combines the output from three learners?a memory-based
model, an SVM classifier, and a CRF classifier?using lexical features, such as PoS and
chunk tags. Councill, McDonald, and Velikovich (2010) use a CRF learner with features
based on dependency parsing (e.g., detailing the PoS of the head and the dependency
path to the negation cue).
The annotation of speculation and negation in BioScope was performed using a
common set of principles. It therefore seems reasonable to assume our dependency-
based scope resolution rules for speculation should be general enough to allow porting
to negation with fairly limited efforts. On the other hand, negation is expressed linguis-
tically using quite different syntactic structures from speculation, so it is clear that some
modifications will be necessary as well.
As we recall, the dependency rules for speculation scope are triggered by the PoS
of the cue. Several of the same parts-of-speech (verbs, adverbs) also express negation.
As an initial experiment, therefore, we simply applied the speculation rules to negation
unmodified. As before, taking default scope to start at the cue word and spanning to
the end of the sentence provides us with a baseline system. We find that applying
our speculation scope rules directly to the task of negation scope resolution offers a
fair improvement over the baseline. For BSA and BSP, the default scope achieves F1
scores of 52.24 and 31.12, respectively, and the speculation rules applied directly without
modifications achieve 48.67 and 56.25.
In order to further improve on these results, we introduce a few new rules to
account specifically for negation. The general rule machinery is identical to the specu-
lation scope rules described in Section 6.1: The rules are triggered by the part of speech
of the cue and operate over the dependency representations output by the stacked
dependency parser described in Section 6.1.1. In developing the rules we consulted the
BioScope guidelines (Vincze et al 2008), as well as a descriptive study of negation in the
BioScope corpus (Morante 2010).
401
Computational Linguistics Volume 38, Number 2
Table 13
Additional dependency-based scope rules for negation, with information source (MaltParser or
XLE), organized by PoS of the cue.
PoS Description Source
DT Determiners scope over their head node and its descendants M
NN Nouns scope over their descendants M
NNnone none take scope over entire sentence if subject and otherwise over its descendants M
VB Verbs scope over their descendants M
RBvb Adverbs with verbal head scope over the descendants of the lexical verb M, X
RBother Adverbs scope over the descendants of the head M, X
7.2.1 Rule Overview. The added rules are presented in Table 13 and are described in more
detail subsequently, organized by the triggering PoS of the negation cue.
Determiners. Determiner cue words in BioScope are largely realized by the negative
determiner no. These take scope over their nominal head and its descendants, as seen in
Example (31):
(31) The finding that dexamethasone has {?no? effect on TPA-induced
activation of PKC} suggests [. . . ]
Nouns. Nominal cues take scope over their descendants (i.e., the members of the noun
phrase), as shown in Example (32).
(32) This unresponsiveness occurs because of a {?lack? of expression of the
beta-chain (accessory factor) of the IFN-gamma receptor}, while at the
same time [. . . ]
The negative pronoun none is tagged as a noun by our system, but deviates from regular
nouns in their negation scope: If the pronoun is a subject, it scopes over the remaining
sentence, as in Example (33), whereas in object function it simply scopes over the noun
phrase (Morante 2010). These are therefore treated specifically by our system.
(33) Similarly, {?none? of SCOPE?s component algorithms outperformed the
other ten programs on this data set by a statistically significant margin}.
Adverbs. Adverbs constitute the majority of negation cues and are largely realized by
the lexical item not. Syntactically, however, adverbs are a heterogeneous category. They
may modify a number of different head words and their scope will thus depend largely
on properties of the head. For instance, when an adverb is a nominal modifier, as in
Example (34), it has a narrow scope which includes only the head noun (34) and its
possible conjuncts.
(34) This report directly demonstrates that OTF-2 but {?not? OTF-1} regulates
the DRA gene
Verbal adverbs scope over the clause headed by the verbal head. As shown by Figure 2,
the parser?s analysis of verbal chains has the consequence that preverbal arguments and
modifiers, such as subjects and adverbs, are attached to the finite verb and postverbal
402
Velldal et al Rules, Rankers, and the Role of Syntax
arguments and modifiers are attached to the lexical verb, in cases where there is an
auxiliary. This rule thus locates the lexical verb (e.g., affect in Example [35]), in the
dependency path from the auxiliary head verb and defines scope over the descendants
of this verb. In cases where the lexical verb is passive, the subject is included in the scope
of the adverb, as in Example (36).
(35) IL-1 did {?not? affect the stability of the c-fos and c-jun transcripts}.
(36) {Levels of RNA coding for the receptor were ?not? modulated by exposure
to high levels of ligand}.
7.2.2 Evaluating the Negation Rules. The result of resolving the scope of gold-standard
negation cues using the new set of dependency rules (i.e., the speculation rules extended
with the negation specific rules of Table 13), are presented in Table 14, along with the
performance of the default scope baseline. First of all, we note that the baseline scores
provided by assigning default scope to all cues differ dramatically between the data sets,
ranging from an F1 of 52.24 for BSA, 31.12 for BSP, and 91.43 for BSR. In comparison,
the performance of the rules is fairly stable across BSA and BSP, and for both data sets
they improve substantially on the baseline (up by roughly 18.5 and 34.5 percentage
points on BSA and BSP, respectively). On BSR, however, the default scope baseline is
substantially stronger than for the other data sets, and even performs slightly better
than the rules. Recall from Table 1 that the average sentence length in the clinical reports
is substantially lower (7.7) than for the other data sets (average of 26), a property which
will make the default scope much more likely to succeed.
In order to shed more light on the performance of the rules on BSR, a manual
error analysis was performed, once again by two trained linguists working together. We
found that out of the total of 74 errors, 30 (40.5%) were parse errors, 29 (39.2%) were rule
errors, 8 (10.8%) were annotation errors, and 4 (5.4%) were undecided. Although it is
usually the case that short sentences are easier to parse, the reports contain a substantial
proportion of ungrammatical structures, such as missing subjects, dropped auxiliaries,
and bare noun phrases, as in Example (37), which clearly lead to lower parse quality,
resulting in 40% parse errors. There are also constructions, such as so-called run-on
constructions, as in Example (38), for which there is simply no correct analysis available
Table 14
Scope resolution for gold-standard negation cues across the BioScope sub-corpora.
Data Configuration F1
B
S
A
Default 52.24
Dependency Rules 70.91
Constituent Ranker 68.35
Combined 74.35
B
S
P
Default 31.12
Dependency Rules 65.69
Constituent Ranker 60.90
Combined 70.21
B
S
R
Default 91.43
Dependency Rules 90.86
Constituent Ranker 89.59
Combined 90.74
403
Computational Linguistics Volume 38, Number 2
within the dependency framework (which, for instance, requires that graphs should be
connected). In addition, the annotations of the reports data contain some idiosyncrasies
which the rules fail to reproduce. Twenty-four percent of the errors are found with the
same cue, namely, the adjective negative. The rules make attributive adjectives scope
over their nominal heads, whereas the BSR annotations define the scope to only cover
the cue word itself; see Example (37). The annotation errors were very similar to the
ones observed in the earlier error analysis of Section 6.1.5.
(37) |{?Negative?} chest radiograph|.
(38) |{?No? focal pneumonia}, normal chest radiograph|.
7.3 Adapting the Constituent Ranker for Negation
Adapting the SVM-based discriminative constituent ranker of Section 6.2 to also predict
the scope of negation is a straightforward procedure, requiring only minor modifi-
cations: Firstly, we developed a further slackening heuristic to ensure that predicted
scope does not begin with an auxiliary. Secondly, we augmented the family of linguistic
features to also record the presence of adverb cues with verbal heads (as specified by the
dependency-based scope rules in Table 13). Finally, we repeated the parameter tuning
for training with n-best and testing with m-best parses (as described in Section 6.2.4).
Performing 10-fold cross-validation on BSA using gold-standard negation cues, we
found that the optimal values for the ranker in isolation were n = 10 and m = 1.
When paralleling the combined approach developed in Section 6.3 (adding the rule-
predictions as a feature in the ranker while falling back on rule-predicted scope for
cases where we do not have an ERG parse) the optimal values were found to be n = 15
andm = 5. Examining the coverage of the parser and the alignment of constituents with
negation scope (considering the 50-best parses), we found that the upper-bound of the
constituent ranker (disregarding any fall-back strategy) on the BSA development set is
79.4% (compared to 83.6% for speculation).
Table 14 lists the performance of both the constituent ranker in isolation and the
combined approachwhen resolving the scope of gold-standard negation cues (reporting
10-fold cross-validation results for BSA, while using BSP and BSR for held-out testing).
We see that the dependency rules perform consistently better than the constituent
ranker, although the differences are not found to be statistically significant (the p-values
for BSA, BSP, and BSR are 0.06, 0.11, and 0.25, respectively). The combined approach
again outperforms the dependency rules on both BSA and BSP (and by a much larger
margin than we observed for speculation), however, with the improvements on both
data sets being significant. Just as we observed for the dependency rules in Section 7.2.2,
neither the constituent ranker nor the combined approach are effective in BSR.
7.4 End-to-End Evaluation with Comparison to Related Work
We now turn to evaluating our end-to-end negation system with SVM-based cue
classification and scope resolution using the combination of constituent ranking and
dependency-based rules. To put the evaluation in perspective we also compare our
results against the results of other state-of-the-art approaches to negation detection.
Comparison to previous work is complicated slightly by the fact that different data
splits and evaluation measures have been used across various studies. A commonly
reported measure in the literature on resolving negation scope is the percentage of
404
Velldal et al Rules, Rankers, and the Role of Syntax
correct scopes (PCS) as used by Morante and Daelemans (2009b), and Councill,
McDonald, and Velikovich (2010), among others. Councill, McDonald, and Velikovich
(2010) define PCS as the number of correct spans divided by the number of true spans. It
therefore corresponds roughly to the scope-level recall as reported in the current article.
The PCS notion of a correct scope, however, is less strict than in our set-up (Section 3.2):
Whereas we require an exact match of both the cue and the scope, Councill, McDonald,
and Velikovich (2010) do not include the cue identification in their evaluation.
Moreover, whereas the work of both Morante and Daelemans (2009b) and Councill,
McDonald, and Velikovich (2010) is based on the BioScope corpus, only Morante and
Daelemans (2009b) follow the same set-up assumed in the current article. Councill,
McDonald, and Velikovich (2010), on the other hand, evaluate by 5-fold cross-validation
on the papers alone, reporting a PCS score of 53.7%. When running our negation cue
classifier and constituent ranker (in the hybrid mode using the dependency features)
by 5-fold cross-validation on the papers we achieve a scope-level recall of 68.62 (and an
F1 of 64.50).
Table 15 shows a comparison of our negation scope resolution system with that
of Morante and Daelemans (2009b). Rather than using the PCS measure reported by
Morante and Daelemans (2009b), we have re-scored the output of their system accord-
ing to the CoNLL-2010 shared task scoring scheme, and it should therefore be kept in
mind that the system of Morante and Daelemans (2009b) originally was optimized with
respect to a slightly different metric.
For the cross-validated BSA experiments we find the results of the two systems
to be fairly similar, although the F1 achieved by our system is higher by more than
5 percentage points, mostly due to higher recall. For the cross-text experiments, the
differences are much more pronounced, with the F1 of our system being more than
22 points higher on BSP and more than 17 points higher on BSR. Again, the largest
differences are to be found for recall?even though this is the score that most closely
corresponds to the PCS metric used by Morante and Daelemans (2009b)?but as seen in
Table 15 there are substantial differences in precision as well. The scope-level differences
between the two systems are found to be statistically significant across all the three
BioScope sub-corpora.
Table 15
End-to-end results for our negation system, using the SVM cue classifier and the combination
of subtree ranking and dependency-based rules for scope resolution, comparing with Morante
et al (2009b).
Scope Level
Data Configuration Prec Rec F1
B
S
A
1
0
-F
o
ld Morante et al (2009b) 66.31 65.27 65.79
Cue classifier & Scope Rules + Ranking 69.30 72.89 71.05
B
S
P
H
e
ld
-o
u
t
Morante et al (2009b) 42.49 39.10 40.72
Cue classifier & Scope Rules + Ranking 58.58 68.09 62.98
B
S
R
H
e
ld
-o
u
t
Morante et al (2009b) 74.03 70.54 72.25
Cue classifier & Scope Rules + Ranking 89.62 89.41 89.52
405
Computational Linguistics Volume 38, Number 2
To some degree, some of the differences are to be expected, perhaps, at least with
respect to BSP. For example, the BSP evaluation represents a held-out setting for both
the cue and scope component in themachine learned system ofMorante andDaelemans
(2009b). While also true for our cue classifier and subtree ranker, it is not strictly
speaking the case for the dependency rules, and so the potential effect of any overfitting
during learningmight be less visible. The small set of manually defined rules are general
in nature, targeting the general syntactic constructions expressing negation, as shown
in Table 13. In addition to being based on the BioScope annotation guidelines, however,
both the abstracts and the full papers were consulted for patterns, and the fact that rule
development has included intermediate testing on BSP (although mostly during the
development of the initial set of speculation rules from which the negation rules are
derived) has likely made our system more tailored to the peculiarities of this data set.
When comparing the errors made by our system to those of Morante and Daelemans
(2009b), the most striking example of this is the inclusion of post-processing rules in our
system for ?backing off? from bracketed expressions (as discussed in Section 6.1). Al-
thoughmaking little difference on the abstracts, this has a huge impact when evaluating
the full papers, where bracketed expressions (citations, references to figures and tables,
etc.) are muchmore common, and the system output ofMorante and Daelemans (2009b)
seems to suffer from the lack of such robustness measures. In relation to the clinical
reports, one should bear in mind that, although our combined system outperforms that
of Morante and Daelemans (2009b) by a large margin, this result would still be rivaled
by simply using our default scope baseline, as is clear from Table 14.
The scope results of Zhu et al (2010) are unfortunately not currently directly com-
parable to ours, due to differences in evaluation methodologies. Whereas we perform
an exact match evaluation at the scope-level, as described in Section 3, Zhu et al (2010)
use a much less strict token-level evaluation even for their scopes in their end-to-end
evaluation. Nevertheless, our results appear to be highly competitive, because even
with the strict exact match criterion underlying our scope-level evaluation, our scores
are actually still higher for both the papers and the reports. (Zhu et al [2010] report an
F1 of 78.50 for the 10-fold runs on the abstracts, and 57.22 and 81.41 for held-out testing
on the papers and reports, respectively.)
8. Conclusion
This article has explored several linguistically informed approaches to the problem
of resolving the scope of speculation and negation within sentences. Our point of
departure was the system developed by Velldal, ?vrelid, and Oepen (2010) for the
CoNLL-2010 Shared Task challenge on resolving speculation in biomedical texts, where
a binary maximum entropy cue classifier was used in combination with a small set of
manually crafted scope resolution rules operating over dependency structures. In the
current article we have introduced several major extensions and improvements to this
initial system design.
First we presented a greatly simplified approach to cue identification using a linear
SVM classifier. The classifier only considers features of the immediate lexical context
of a target word, and it only aims to ?disambiguate? words that have already been
observed as speculation cues in the training data. The filtering imposed by this latter
?closed class? assumption greatly reduces the size and complexity of the model while
increasing classifier accuracy, yielding state-of-the-art performance on the CoNLL-2010
Shared Task evaluation data.
406
Velldal et al Rules, Rankers, and the Role of Syntax
We then presented a novel approach to the problem of resolving the scopes of
cues within a sentence. As an alternative to using the manually defined dependency
rules of our initial system, we showed how an SVM-based discriminative ranking
function can be learned for choosing subtrees from HPSG-based constituent structures.
An underlying assumption of the ranking approach is that annotated scopes actually
align with constituents, and we provided in-depth discussion and analysis of this issue.
Furthermore, while both the dependency rules and the constituent ranker achieve
good performance on their own, we showed how even better results can be achieved by
combining the two, as the errors they make are not always overlapping. The combined
approach uses the dependency rules for all cases where we do not have an available
HPSG parse, and for the cases where we do, the scope predicted by the rules is included
as a feature in the constituent ranker model. Together with the reformulation of our cue
classifier, this combined model for scope resolution obtains the best published results
so far on the CoNLL-2010 Shared Task evaluation data (to the best of our knowledge).
Finally, we have showed how all components of our speculation system are easily
ported to also handle the problem of resolving the scope of negation. With only modest
modifications, the system obtains state-of-the-art results also on the negation task. The
system outputs corresponding to the end-to-end experiments with our final model con-
figurations, for both speculation and negation, aremade available online (see footnote 2)
together with the relevant evaluation software.
Acknowledgments
We are grateful to the organizers of the
2010 CoNLL Shared Task and creators of
the BioScope resource; first, for engaging in
these kinds of community service, and
second for many in-depth discussions of
annotation and task details. We also want
to thank Buzhou Tang (HIT Shenzhen
Graduate School) and Roser Morante
(University of Antwerp), together with
their colleagues, for providing us with the
raw XML output of their negation and
speculation systems in order to enable
system comparisons. Andrew MacKinlay
(Melbourne University) and Dan Flickinger
(Stanford University) were of invaluable
help in adapting ERG parse selection to
the biomedical domain. We thank our
colleagues at the University of Oslo for
their comments and support during our
original participation in the 2010 CoNLL
Shared Task, as well as more recently in
preparing this manuscript. Large-scale
experimentation and engineering was
made possible though access to the TITAN
high-performance computing facilities at the
University of Oslo, and we are grateful to
the Scientific Computation staff at UiO, as
well as to the Norwegian Metacenter for
Computational Science. Last but not least,
we are indebted to the anonymous reviewers
for their careful reading and insightful
comments.
References
Brants, Thorsten. 2000. TnT. A statistical
Part-of-Speech tagger. In Proceedings of
the Sixth Conference on Applied Natural
Language Processing, pages 224?231,
Seattle, WA.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of the COLING
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei.
Callmeier, Ulrich. 2002. Preprocessing and
encoding techniques in PET. In Stephan
Oepen, Daniel Flickinger, Jun?ichi Tsujii,
and Hans Uszkoreit, editors, Collaborative
Language Engineering. A Case Study in
Efficient Grammar-based Processing. CSLI
Publications, Stanford, CA, pages 127?143.
Collier, Nigel, Hyun S. Park, Norihiro Ogata,
Yuka Tateishi, Chikashi Nobata, Tomoko
Ohta, Tateshi Sekimizu, Hisao Imai,
Katsutoshi Ibushi, and Jun I. Tsujii. 1999.
The GENIA project: Corpus-based
knowledge acquisition and information
extraction from genome research papers.
In Proceedings of the 9th Conference of the
European Chapter of the ACL, pages 271?272,
Bergen.
Councill, Isaac G., Ryan McDonald, and
Leonid Velikovich. 2010. What?s great
and what?s not: Learning to classify the
scope of negation for improved sentiment
407
Computational Linguistics Volume 38, Number 2
analysis. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 51?59, Uppsala.
Crouch, Dick, Mary Dalrymple, Ron Kaplan,
Tracy King, John Maxwell, and Paula
Newman. 2008. XLE documentation. Palo
Alto Research Center, Palo Alto, CA.
Farkas, Richard, Veronika Vincze, Gyorgy
Mora, Janos Csirik, and Gy?rgy Szarvas.
2010. The CoNLL 2010 Shared Task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the 14th Conference on Natural Language
Learning, pages 1?12, Uppsala.
Flickinger, Dan. 2002. On building a more
efficient grammar by exploiting types.
In Stephan Oepen, Dan Flickinger,
Jun?ichi Tsujii, and Hans Uszkoreit,
editors, Collaborative Language Engineering:
A Case Study in Efficient Grammar-based
Processing. CSLI Publications, Stanford,
CA, pages 1?17.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods
in Natural Language Processing,
pages 167?202, Pittsburgh, PA.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In Bernhard
Sch?lkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in
Kernel Methods: Support Vector Learning.
MIT Press, Cambridge, MA, pages 41?56.
Joachims, Thorsten. 2002. Optimizing
search engines using clickthrough data.
In Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 133?142,
Alberta.
Johansson, Richard and Pierre Nugues. 2007.
Extended constituent-to-dependency
conversion for English. In Proceedings of
16th Nordic Conference of Computational
Linguistics, pages 105?112, Tartu.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
unification-based grammars. In Proceedings
of the 37th Meeting of the Association for
Computational Linguistics, pages 535?541,
College Park, MD.
Kilicoglu, Halil and Sabine Bergler. 2010.
A high-precision approach to detecting
hedges and their scopes. In Proceedings of
the 14th Conference on Natural Language
Learning, pages 70?77, Uppsala.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings
of the HLT-NAACL 2004 Workshop:
Biolink 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24,
Boston, MA.
MacKinlay, Andrew, Rebecca Dridan,
Dan Flickinger, Stephan Oepen, and
Timothy Baldwin. 2011. Treeblazing:
Using external treebanks to filter parse
forests for parse selection and treebanking.
In Proceedings of the 5th International Joint
Conference on Natural Language Processing,
pages 246?254, Chiang Mai.
Malouf, Robert and Gertjan van Noord. 2004.
Wide coverage parsing with stochastic
attribute value grammars. In Proceedings
of the IJCNLP Workshop Beyond Shallow
Analysis, Hainan.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English. The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martins, Andre F. T., Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking
dependency parsers. In Proceedings
of the 2008 Conference on Empirical
Methods in Natural Language Processing,
pages 157?166, Waikiki, HI.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature.
In Proceedings of the 45th Meeting of the
Association for Computational Linguistics,
pages 992?999, Prague.
Moilanen, Karo and Stephen Pulman. 2007.
Sentiment composition. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing,
pages 378?382, Borovets.
Morante, Roser. 2010. Descriptive analysis
of negation cues in biomedical texts.
In Proceedings of the 7th International
Conference on Language Resources and
Evaluation, pages 1429?1436, Valletta.
Morante, Roser and Walter Daelemans.
2009a. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
BioNLP 2009 Workshop, pages 28?36,
Boulder, CO.
Morante, Roser and Walter Daelemans.
2009b. A metalearning approach to
processing the scope of negation.
In Proceedings of the 13th Conference on
Natural Language Learning, pages 21?29,
Boulder, CO.
Morante, Roser, Anthony Liekens, and
Walter Daelemans. 2008. Learning the
scope of negation in biomedical texts.
408
Velldal et al Rules, Rankers, and the Role of Syntax
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing, pages 715?724, Waikiki, HI.
Morante, Roser, Vincent van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scope of hedge
cues. In Proceedings of the 14th Conference
on Natural Language Learning, pages 40?47,
Uppsala.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2006. MaltParser: A data-driven
parser-generator for dependency parsing.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation, pages 2216?2219, Genoa.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency
parsers. In Proceedings of the 46th
Meeting of the Association for
Computational Linguistics,
pages 950?958, Columbus, OH.
?vrelid, Lilja, Jonas Kuhn, and Kathrin
Spreyer. 2009. Cross-framework
parser stacking for data-driven
dependency parsing. TAL special
issue on Machine Learning for NLP,
50(3):109?138.
?vrelid, Lilja, Erik Velldal, and Stephan
Oepen. 2010. Syntactic scope resolution
in uncertainty analysis. In Proceedings
of the 23rd International Conference on
Computational Linguistics, pages 1379?1387,
Beijing.
Petrov, Slav and Dan Klein. 2007.
Improved inference for unlexicalized
parsing. In Proceedings of Human Language
Technologies: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 404?411,
Rochester, NY.
Pollard, Carl and Ivan A. Sag. 1987.
Information-Based Syntax and Semantics.
Vol. 1: Fundamentals. CSLI Lecture
Notes # 13. CSLI Press, Stanford, CA.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven Phrase Structure Grammar.
The University of Chicago Press and
CSLI Publications, Chicago, IL.
Rei, Marek and Ted Briscoe. 2010.
Combining manual rules and supervised
learning for hedge cue and scope
detection. In Proceedings of the 14th
Conference on Natural Language Learning,
pages 56?63, Uppsala.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional
grammar and discriminative estimation
techniques. In Proceedings of the
40th Meeting of the Association for
Computational Linguistics, pages 271?278,
Philadelphia, PA.
Schmid, Helmut. 1994. Probabilistic
part-of-speech tagging using decision
trees. In International Conference on
New Methods in Language Processing,
pages 44?49, Manchester.
Szarvas, Gy?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords. In
Proceedings of the 46th Meeting of the
Association for Computational Linguistics,
pages 281?289, Columbus, OH.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the 14th Conference on
Natural Language Learning, pages 13?17,
Uppsala.
Toutanova, Kristina, Christopher D.
Manning, Dan Flickinger, and Stephan
Oepen. 2005. Stochastic HPSG parse
disambiguation using the Redwoods
corpus. Research on Language and
Computation, 3(1):83?105.
Tsuruoka, Yoshimasa, Yuka Tateishi,
Jin-Dong Kim, Tomoko Ohta, John
McNaught, Sophia Ananiadou, and
Jun?ichi Tsujii. 2005. Developing a robust
Part-of-Speech tagger for biomedical text.
In P. Bozanis and E. Houstis, editors,
Advances in Informatics. Springer, Berlin,
pages 382?392.
Velldal, Erik. 2011. Predicting speculation:
A simple disambiguation approach
to hedge detection in biomedical
literature. Journal of Biomedical Semantics,
2(Suppl 5):S7.
Velldal, Erik, Lilja ?vrelid, and Stephan
Oepen. 2010. Resolving speculation:
MaxEnt cue classification and
dependency-based scope rules.
In Proceedings of the 14th Conference on
Natural Language Learning, pages 48?55,
Uppsala.
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd
Farkas, Gy?rgy M?ra, and J?nos Csirik.
2008. The BioScope corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9
(Suppl. 11).
Vlachos, Andreas and Mark Craven. 2010.
Detecting speculative language using
syntactic dependencies and logistic
regression. In Proceedings of the 14th
409
Computational Linguistics Volume 38, Number 2
Conference on Natural Language Learning,
pages 18?25, Uppsala.
Zhang, Yi, Stephan Oepen, and John
Carroll. 2007. Efficiency in
unification-based n-best parsing. In
Proceedings of the 10th International
Conference on Parsing Technologies,
pages 48?59, Prague.
Zhang, Yi and Rui Wang. 2009.
Cross-domain dependency parsing
using a deep linguistic grammar.
In Proceedings of the 47th Meeting of the
Association for Computational Linguistics,
pages 378?386, Singapore.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based dependency parsing
with rich non-local features. In Proceedings
of the 49th Meeting of the Association for
Computational Linguistics, pages 188?193,
Portland, OR.
Zhu, Qiaoming, Junhui Li, Hongling
Wang, and Guodong Zhou. 2010.
A unified framework for scope
learning via simplified shallow
semantic parsing. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 714?724, Cambridge, MA.
410
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Simple Negation Scope Resolution through Deep Parsing:
A Semantic Solution to a Semantic Problem
Woodley Packard
?
, Emily M. Bender
?
, Jonathon Read
?
, Stephan Oepen
??
, and Rebecca Dridan
?
?
University of Washington, Department of Linguistics
?
Teesside University, School of Computing
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
ebender@uw.edu, sweaglesw@sweaglesw.org, j.read@tees.ac.uk, {oe |rdridan}@ifi.uio.no
Abstract
In this work, we revisit Shared Task 1
from the 2012
*
SEM Conference: the au-
tomated analysis of negation. Unlike the
vast majority of participating systems in
2012, our approach works over explicit
and formal representations of proposi-
tional semantics, i.e. derives the notion of
negation scope assumed in this task from
the structure of logical-form meaning rep-
resentations. We relate the task-specific
interpretation of (negation) scope to the
concept of (quantifier and operator) scope
in mainstream underspecified semantics.
With reference to an explicit encoding
of semantic predicate-argument structure,
we can operationalize the annotation deci-
sions made for the 2012
*
SEM task, and
demonstrate how a comparatively simple
system for negation scope resolution can
be built from an off-the-shelf deep parsing
system. In a system combination setting,
our approach improves over the best pub-
lished results on this task to date.
1 Introduction
Recently, there has been increased community in-
terest in the theoretical and practical analysis of
what Morante and Sporleder (2012) call modality
and negation, i.e. linguistic expressions that mod-
ulate the certainty or factuality of propositions.
Automated analysis of such aspects of meaning
is important for natural language processing tasks
which need to consider the truth value of state-
ments, such as for example text mining (Vincze
et al, 2008) or sentiment analysis (Lapponi et al,
2012). Owing to its immediate utility in the cura-
tion of scholarly results, the analysis of negation
and so-called hedges in bio-medical research liter-
ature has been the focus of several workshops, as
well as the Shared Task at the 2011 Conference on
Computational Language Learning (CoNLL).
Task 1 at the First Joint Conference on Lex-
ical and Computational Semantics (
*
SEM 2012;
Morante and Blanco, 2012) provided a fresh, prin-
cipled annotation of negation and called for sys-
tems to analyze negation?detecting cues (affixes,
words, or phrases that express negation), resolv-
ing their scopes (which parts of a sentence are ac-
tually negated), and identifying the negated event
or property. The task organizers designed and
documented an annotation scheme (Morante and
Daelemans, 2012) and applied it to a little more
than 100,000 tokens of running text by the nov-
elist Sir Arthur Conan Doyle. While the task and
annotations were framed from a semantic perspec-
tive, only one participating system actually em-
ployed explicit compositional semantics (Basile et
al., 2012), with results ranking in the middle of
the 12 participating systems. Conversely, the best-
performing systems approached the task through
machine learning or heuristic processing over syn-
tactic and linguistically relatively coarse-grained
representations; see ? 2 below.
Example (1), where ?? marks the cue and {}
the in-scope elements, illustrates the annotations,
including how negation inside a noun phrase can
scope over discontinuous parts of the sentence.
1
(1) {The German} was sent for but professed to
{know} ?nothing? {of the matter}.
In this work, we return to the 2012
*
SEM
task from a deliberately semantics-centered point
of view, focusing on the hardest of the three
sub-problems: scope resolution.
2
Where Morante
and Daelemans (2012) characterize negation as an
?extra-propositional aspect of meaning? (p. 1563),
1
Our running example is a truncated variant of an item
from the Shared Task training data. The remainder of the
original sentence does not form part of the scope of this cue.
2
Resolving negation scope is a more difficult sub-problem
at least in part because (unlike cue and event identification) it
is concerned with much larger, non-local and often discontin-
uous parts of each utterance. This intuition is confirmed by
Read et al (2012), who report results for each sub-problem
using gold-standard inputs; in this setup, scope resolution
showed by far the lowest performance levels.
69
we in fact see it as a core piece of composi-
tionally constructed logical-form representations.
Though the task-specific concept of scope of
negation is not the same as the notion of quan-
tifier and operator scope in mainstream under-
specified semantics, we nonetheless find that re-
viewing the 2012
*
SEM Shared Task annotations
with reference to an explicit encoding of seman-
tic predicate-argument structure suggests a sim-
ple and straightforward operationalization of their
concept of negation scope. Our system imple-
ments these findings through a notion of functor-
argument ?crawling?, using as our starting point
the underspecified logical-form meaning represen-
tations provided by a general-purpose deep parser.
Our contributions are three-fold: Theoretically,
we correlate the structures at play in the Morante
and Daelemans (2012) view on negation with
formal semantic analyses; methodologically, we
demonstrate how to approach the task in terms of
underspecified, logical-form semantics; and prac-
tically, our combined system retroactively ?wins?
the 2012
*
SEM Shared Task. In the following
sections, we review related work (? 2), detail our
own setup (? 3), and present and discuss our ex-
perimental results (? 4 and ? 5, respectively).
2 Related Work
Read et al (2012) describe the best-performing
submission to Task 1 of the 2012
*
SEM Confer-
ence. They investigated two approaches for scope
resolution, both of which were based on syntac-
tic constituents. Firstly, they created a set of 11
heuristics that describe the path from the preter-
minal of a cue to the constituent whose projec-
tion is predicted to match the scope. Secondly
they trained an SVM ranker over candidate con-
stituents, generated by following the path from a
cue to the root of the tree and describing each
candidate in terms of syntactic properties along
the path and various surface features. Both ap-
proaches attempted to handle discontinuous in-
stances by applying two heuristics to the predicted
scope: (a) removing preceding conjuncts from the
scope when the cue is in a conjoined phrase and
(b) removing sentential adverbs from the scope.
The ranking approach showed a modest advan-
tage over the heuristics (with F
1
equal to 77.9
and 76.7, respectively, when resolving the scope
of gold-standard cues in evaluation data). Read et
al. (2012) noted however that the annotated scopes
did not align with the Shared Task?provided con-
stituents for 14% of the instances in the training
data, giving an F
1
upper-bound of around 86.0 for
systems that depend on those constituents.
Basile et al (2012) present the only submission
to Task 1 of the 2012
*
SEM Conference which
employed compositional semantics. Their scope
resolution pipeline consisted primarily of the C&C
parser and Boxer (Curran et al, 2007), which pro-
duce Discourse Representation Structures (DRSs).
The DRSs represent negation explicitly, including
representing other predications as being within the
scope of negation. Basile et al (2012) describe
some amount of tailoring of the Boxer lexicon to
include more of the Shared Task scope cues among
those that produce the negation operator in the
DRSs, but otherwise the system appears to directly
take the notion of scope of negation from the DRS
and project it out to the string, with one caveat: As
with the logical-forms representations we use, the
DRS logical forms do not include function words
as predicates in the semantics. Since the Shared
Task gold standard annotations included such ar-
guably semantically vacuous (see Bender, 2013,
p. 107) words in the scope, further heuristics are
needed to repair the string-based annotations com-
ing from the DRS-based system. Basile et al re-
sort to counting any words between in-scope to-
kens which are not themselves cues as in-scope.
This simple heuristic raises their F
1
for full scopes
from 20.1 to 53.3 on system-predicted cues.
3 System Description
The new system described here is what we call
the MRS Crawler. This system operates over
the normalized semantic representations provided
by the LinGO English Resource Grammar (ERG;
Flickinger, 2000).
3
The ERG maps surface strings
to meaning representations in the format of Mini-
mal Recursion Semantics (MRS; Copestake et al,
2005). MRS makes explicit predicate-argument
relations, as well as partial information about
scope (see below). We used the grammar together
with one of its pre-packaged conditional Maxi-
mum Entropy models for parse ranking, trained
on a combination of encyclopedia articles and
tourism brochures. Thus, the deep parsing front-
end system to our MRS Crawler has not been
3
In our experiments, we use the 1212 release of the ERG,
in combination with the ACE parser (http://sweaglesw
.org/linguistics/ace/). The ERG and ACE are DELPH-
IN resources; see http://www.delph-in.net.
70
? h
1
,
h
4
:_the_q?0:3?(ARG0 x
6
, RSTR h
7
, BODY h
5
), h
8
:_german_n_1?4:10?(ARG0 x
6
),
h
9
:_send_v_for?15:19?(ARG0 e
10
, ARG1 , ARG2 x
6
), h
2
:_but_c?24:27?(ARG0 e
3
, L-HNDL h
9
, R-HNDL h
14
),
h
14
:_profess_v_to?28:37?(ARG0 e
13
, ARG1 x
6
, ARG2 h
15
), h
16
:_know_v_1?41:45?(ARG0 e
17
, ARG1 x
6
, ARG2 x
18
),
h
20
:_no_q?46:53?(ARG0 x
18
, RSTR h
21
, BODY h
22
), h
19
:thing?46:53?(ARG0 x
18
),
h
19
:_of_p?54:56?(ARG0 e
23
, ARG1 x
18
, ARG2 x
24
),
h
25
:_the_q?57:60?(ARG0 x
24
, RSTR h
27
, BODY h
26
), h
28
:_matter_n_of?61:68?(ARG0 x
24
, ARG1 )
{ h
27
=
q
h
28
, h
21
=
q
h
19
, h
15
=
q
h
16
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS analysis of our running example (1).
adapted to the task or its text type; it is applied
in an ?off the shelf? setting. We combine our
system with the outputs from the best-performing
2012 submission, the system of Read et al (2012),
firstly by relying on the latter for system negation
cue detection,
4
and secondly as a fall-back in sys-
tem combination as described in ? 3.4 below.
Scopal information in MRS analyses delivered
by the ERG fixes the scope of operators?such as
negation, modals, scopal adverbs (including sub-
ordinating conjunctions like while), and clause-
embedding verbs (e.g. believe)?based on their
position in the constituent structure, while leaving
the scope of quantifiers (e.g. a or every, but also
other determiners) free. From these underspec-
ified representations of possible scopal configu-
rations, a scope resolution component can spell
out the full range of fully-connected logical forms
(Koller and Thater, 2005), but it turns out that such
enumeration is not relevant here: the notion of
scope encoded in the Shared Task annotations is
not concerned with the relative scope of quantifiers
and negation, such as the two possible readings of
(2) represented informally below:
5
(2) Everyone didn?t leave.
a. ?(x)?leave(x) ? Everyone stayed.
b. ??(x)leave(x) ? At least some stayed.
However, as shown below, the information about
fixed scopal elements in an underspecified MRS is
sufficient to model the Shared Task annotations.
3.1 MRS Crawling
Fig. 1 shows the ERG semantic analysis for our
running example. The heart of the MRS is a mul-
tiset of elementary predications (EPs). Each ele-
4
Read et al (2012) predicted cues using a closed vocabu-
lary assumption with a supervised classifier to disambiguate
instances of cues.
5
In other words, a possible semantic interpretation of the
(string-based) Shared Task annotation guidelines and data is
in terms of a quantifier-free approach to meaning representa-
tion, or in terms of one where quantifier scope need not be
made explicit (as once suggested by, among others, Alshawi,
1992). From this interpretation, it follows that the notion of
scope assumed in the Shared Task does not encompass inter-
actions of negation operators and quantifiers.
mentary prediction includes a predicate symbol,
a label (or ?handle?, prefixed to predicates with
a colon in Fig. 1), and one or more argument
positions, whose values are semantic variables.
Eventualities (e
i
) in MRS denote states or activ-
ities, while instance variables (x
j
) typically corre-
spond to (referential or abstract) entities. All EPs
have the argument position ARG0, called the dis-
tinguished variable (Oepen and L?nning, 2006),
and no variable is the ARG0 of more than one non-
quantifier EP.
The arguments of one EP are linked to the argu-
ments of others either directly (sharing the same
variable as their value), or indirectly (through so-
called ?handle constraints?, where =
q
in Fig. 1 de-
notes equality modulo quantifier insertion). Thus
a well-formed MRS forms a connected graph. In
addition, the grammar links the EPs to the ele-
ments of the surface string that give rise to them,
via character offsets recorded in each EP (shown
in angle brackets in Fig. 1). For the purposes of
the present task, we take a negation cue as our en-
try point into the MRS graph (as our initial active
EP), and then move through the graph according
to the following simple operations to add EPs to
the active set:
Argument Crawling Add to the scope all EPs
whose distinguished variable or label is an argu-
ment of the active EP; for arguments of type h
k
,
treat any =
q
constraints as label equality.
Label Crawling Add all EPs whose label is iden-
tical to that of the active EP.
Functor Crawling Add all EPs that take the dis-
tinguished variable or label of the active EP as an
argument (directly or via =
q
constraints).
Our MRS crawling algorithm is sketched in
Fig. 2. To illustrate how the rules work, we will
trace their operation in the analysis of example (1),
i.e. traverse the EP graph in Fig. 1.
The negation cue is nothing, from character po-
sition 46 to 53. This leads us to _no_q as our en-
try point into the graph. Our algorithm states that
for this type of cue (a quantifier) the first step is
71
1: Activate the cue EP
2: if the cue EP is a quantifier then
3: Activate EPs reached by functor crawling from the distinguished variable (ARG0) of the cue EP
4: end if
5: repeat
6: for each active EP X do
7: Activate EPs reached by argument crawling or label crawling unless they are co-modifiers of the negation cue.
a
8: Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions
reached by ARG1: whether, when, because, to, with, although, unless, until, or as.
9: end for
10: until a fixpoint is reached (no additional EPs were activated)
11: Deactivate zero-pronoun EPs (from imperative constructions)
12: Apply semantically empty word handling rules (iterate until a fixpoint is reached)
13: Apply punctuation heuristics
Figure 2: Algorithm for scope detection by MRS crawling
a
Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is =
q
equated with the
label of the negation cue, it cannot be in-scope unless its ARG0 is an argument of the negation cue, or the ARG0 of the negation
cue is one of its own arguments. See ? 3.3 for elaboration.
functor crawling (see ? 3.3 below), which brings
_know_v_1 into the scope. We proceed with ar-
gument crawling and label crawling, which pick
up _the_q?0:3? and _german_n_1 as the ARG1.
Further, as the ARG2 of _know_v_1, we reach
thing and through recursive invocation we acti-
vate _of_p and, in yet another level of recursion,
_the_q?57:60? and _matter_n_of. At this point,
crawling has no more links to follow. Thus, the
MRS crawling operations ?paint? a subset of the
MRS graph as in-scope for a given negation cue.
3.2 Semantically Empty Word Handling
Our crawling rules operate on semantic represen-
tations, but the annotations are with reference to
the surface string. Accordingly, we need projec-
tion rules to map from the ?painted? MRS to the
string. We can use the character offsets recorded
in each EP to project the scope to the string. How-
ever, the string-based annotations also include
words which the ERG treats as semantically vacu-
ous. Thus in order to match the gold annotations,
we define a set of heuristics for when to count vac-
uous words as in scope. In (1), there are no se-
mantically empty words in-scope, so we illustrate
these heuristics with another example:
(3) ?I trust that {there is} ?nothing? {of consequence
which I have overlooked}??
The MRS crawling operations discussed above
paint the EPs corresponding to is, thing, of, conse-
quence, I, and overlooked as in-scope (underlined
in (3)). Conversely, the ERG treats the words that,
there, which, and have as semantically empty. Of
these, we need to add all except that to the scope.
Our vacuous word handling rules use the syntac-
tic structure provided by the ERG as scaffolding to
help link the scope information gleaned from con-
tentful words to vacuous words. Each node in the
syntax tree is initially colored either in-scope or
out-of-scope in agreement with the decision made
by the crawler about the lexical head of the corre-
sponding subtree. A semantically empty word is
determined to be in-scope if there is an in-scope
syntax tree node in the right position relative to it,
as governed by a short list of templates organized
by the type of the semantically empty word (par-
ticles, complementizers, non-referential pronouns,
relative pronouns, and auxiliary verbs).
As an example, the rule for auxiliary verbs like
have in our example (3) is that they are in scope
when their verb phrase complement is in scope.
Since overlooked is marked as in-scope by the
crawler, the semantically empty have becomes in-
scope as well. Sometimes the rules need to be
iterated. For example, the main rule for relative
pronouns is that they are in-scope when they fill
a gap in an in-scope constituent; which fills a gap
in the constituent have overlooked, but since have
is the (syntactic) lexical head of that constituent,
the verb phrase is not considered in-scope the first
time the rules are tried.
Similar rules deal with that (complementizers
are in-scope when the complement phrase is an ar-
gument of an in-scope verb, which is not the case
here) and there (non-referential pronouns are in-
scope when they are the subject of an in-scope VP,
which is true here).
72
3.3 Re-Reading the Annotation Guidelines
Our MRS crawling algorithm was defined by look-
ing at the annotated data rather than the annota-
tion guidelines for the Shared Task (Morante et al,
2011). Nonetheless, our algorithm can be seen as
a first pass formalization of the guidelines. In this
section, we briefly sketch how our algorithm cor-
responds to different aspects of the guidelines.
For negated verbs, the guidelines state that ?If
the negated verb is the main verb in the sen-
tence, the entire sentence is in scope.? (Morante
et al, 2011, 17). In terms of our operations de-
fined over semantic representations, this is ren-
dered as follows: all arguments of the negated
verb are selected by argument crawling, all in-
tersective modifiers by label crawling, and func-
tor crawling (Fig. 2, line 8) captures modal auxil-
iaries and non-intersective modifiers. The guide-
lines treat predicative adjectives under a separate
heading from verbs, but describe the same desired
annotations (scope over the whole clause; ibid.,
p. 20). Since these structures are analogous in the
semantic representations, the same operations that
handle negated verbs also handle negated predica-
tive adjectives correctly.
For negated subjects and objects, the guidelines
state that the negation scopes over ?all the clause?
and ?the clause headed by the verb? (Morante et
al., 2011, 19), respectively. The examples given in
the annotation guidelines suggest that these are in
fact meant to refer to the same thing. The negation
cue for a negated nominal argument will appear
as a quantifier EP in the MRS, triggering line 3 of
our algorithm. This functor crawling step will get
to the verb?s EP, and from there, the process is the
same as the last two cases.
In contrast to subjects and objects, negation of
a clausal argument is not treated as negation of the
verb (ibid., p. 18). Since in this case, the negation
cue will not be a quantifier in the MRS, there will
be no functor crawling to the verb?s EP.
For negated modifiers, the situation is somewhat
more complex, and this is a case where our crawl-
ing algorithm, developed on the basis of the anno-
tated data, does not align directly with the guide-
lines as given. The guidelines state that negated at-
tributive adjectives have scope over the entire NP
(including the determiner) (ibid., p. 20) and anal-
ogously negated adverbs have scope over the en-
tire clause (ibid., p. 21). However, the annotations
are not consistent, especially with respect to the
treatment of negated adjectives: while the head
noun and determiner (if present) are typically an-
notated as in scope, other co-modifiers, especially
long, post-nominal modifiers (including relative
clauses) are not necessarily included:
(4) ?A dabbler in science, Mr. Holmes, a picker up
of shells on the shores of {the} great ?un?{known
ocean}.
(5) Our client looked down with a rueful face at {his}
own ?un?{conventional appearance}.
(6) Here was {this} ?ir?{reproachable Englishman}
ready to swear in any court of law that the accused
was in the house all the time.
(7) {There is}, on the face of it, {something}
?un?{natural about this strange and sudden friend-
ship between the young Spaniard and Scott Eccles}.
Furthermore, the guidelines treat relative clauses
as subordinate clauses and thus negation inside a
relative clause is treated as bound to that clause
only, and includes neither the head noun of the
relative clause nor any of its other dependents in
its scope. However, from the perspective of MRS,
a negated relative clause is indistinguishable from
any other negated modifier of a noun. This treat-
ment of relative clauses (as well as the inconsis-
tencies in other forms of co-modification) is the
reason for the exception noted at line 7 of Fig. 2.
By disallowing the addition of EPs to the scope if
they share the label of the negation cue but are not
one of its arguments, we block the head noun?s EP
(and any EPs only reachable from it) in cases of
relative clauses where the head verb inside the rel-
ative clause is negated. It also blocks co-modifiers
like great, own, and the phrases headed by ready
and about in (4)?(7). As illustrated in these exam-
ples, this is correct some but not all of the time.
Having been unable to find a generalization cap-
turing when comodifiers are annotated as in scope,
we stuck with this approximation.
For negation within clausal modifiers of verbs,
the annotation guidelines have further informa-
tion, but again, our existing algorithm has the cor-
rect behavior: The guidelines state that a negation
cue inside of the complement of a subordinating
conjunction (e.g. if ) has scope only over the sub-
ordinate clause (ibid., p. 18 and p. 26). The ERG
treats all subordinating conjunctions as two-place
predicates taking two scopal arguments. Thus,
as with clausal complements of clause-embedding
verbs, the embedding subordinating conjunction
and any other arguments it might have are inac-
cessible, since functor crawling is restricted to a
handful of specific configurations.
73
As is usually the case with exercises in for-
malization, our crawling algorithm generalizes be-
yond what is given explicitly in the annotation
guidelines. For example, all arguments that are
treated as semantically nominal (including PP ar-
guments where the preposition is semantically
null) are treated in the same way as subjects and
objects; similarly, all arguments which are seman-
tically clausal (including certain PP arguments)
are handled the same way as clausal complements.
This is possible because we take advantage of the
high degree of normalization that the ERG accom-
plishes in mapping to the MRS representation.
There are also cases where we are more spe-
cific. The guidelines do not handle coordination in
detail, except to state that in coordinated clauses
negation is restricted to the clause it appears in
(ibid., p. 17?18) and to include a few examples of
coordination under the heading ?ellipsis?. In the
case of VP coordination, our existing algorithm
does not need any further elaboration to pick up
the subject of the coordinated VP but not the non-
negated conjunct, as shown in discussion of (1) in
? 3.1 above. In the case of coordination of negated
NPs, recall that to reach the main portion of the
negated scope we must first apply functor crawl-
ing. The functor crawling procedure has a general
mechanism to transparently continue crawling up
through coordinated structures while blocking fu-
ture crawling from traversing them again.
6
On the other hand, there are some cases in the
annotation guidelines which our algorithm does
not yet handle. We have not yet provided any anal-
ysis of the special cases for save and expect dis-
cussed in Morante et al, 2011, pp. 22?23, and also
do not have a means of picking out the overt verb
in gapping constructions (p. 24).
Finally, we note that even carefully worked out
annotation guidelines such as these are never fol-
lowed perfectly consistently by the human annota-
tors who apply them. Because our crawling algo-
rithm so closely models the guidelines, this puts
our system in an interesting position to provide
feedback to the Shared Task organizers.
3.4 Fall-Back Configurations
The close match between our crawling algorithm
and the annotation guidelines supported by the
mapping to MRS provides for very high precision
6
This allows ate to be reached in We ate bread but no fish.,
while preventing but and bread from being reached, which
they otherwise would via argument crawling from ate.
and recall when the analysis engine produces the
desired MRS.
7
However, the analysis engine does
not always provide the desired analysis, largely
because of idiosyncrasies of the genre (e.g. voca-
tives appearing mid-sentence) that are either not
handled by the grammar or not well modeled in the
parse selection component. In addition, as noted
above, there are a handful of negation cues we do
not yet handle. Thus, we also tested fall-back con-
figurations which use scope predictions based on
MRS in some cases, and scope predictions from
the system of Read et al (2012) in others.
Our first fall-back configuration (Crawler
N
in
Table 1) uses MRS-based predictions whenever
there is a parse available and the cue is one that
our system handles. Sometimes, the analysis
picked by the ERG?s statistical model is not the
correct analysis for the given context. To com-
bat such suboptimal parse selection performance,
we investigated using the probability of the top
ranked analysis (as determined by the parse selec-
tion model and conditioned on the sentence) as a
confidence metric. Our second fall-back configu-
ration (Crawler
P
in Table 1) uses MRS-based pre-
dictions when there is a parse available whose con-
ditional probability is at least 0.5.
8
4 Experiments
We evaluated the performance of our system using
the Shared Task development and evaluation data
(respectively CDD and CDE in Table 1). Since we
do not attempt to perform cue detection, we report
performance using gold cues and also using the
system cues predicted by Read et al (2012). We
used the official Shared Task evaluation script to
compute all scores.
4.1 Data Sets
The Shared Task data consists of chapters from
the Adventures of Sherlock Holmes mystery nov-
els and short stories. As such, the text is carefully
edited turn-of-the-20th-century British English,
9
7
And in fact, the task is somewhat noise-tolerant: some
parse selection decisions are independent of each other, and
a mistake in a part of the analysis far enough away from the
negation cue does not harm performance.
8
This threshold was determined empirically on the devel-
opment data. We also experimented with other confidence
metrics?the probability ratio of the top-ranked and second
parse or the entropy over the probability distribution of the
top 10 parses?but found no substantive differences.
9
In contrast, the ERG was engineered for the analysis of
contemporary American English, and an anecdotal analysis
of parse failures and imperfect top-ranked parses suggests
74
Gold Cues System Cues
Scopes Tokens Scopes Tokens
Set Method Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
C
D
D
Ranker 100.0 68.5 81.3 84.8 86.8 85.8 91.7 66.1 76.8 79.5 84.9 82.1
Crawler 100.0 53.0 69.3 89.3 67.0 76.6 90.8 53.0 66.9 84.7 65.9 74.1
Crawler
N
100.0 64.9 78.7 89.0 83.5 86.1 90.8 64.3 75.3 82.6 82.1 82.3
Crawler
P
100.0 70.2 82.5 86.4 86.8 86.6 91.2 67.9 77.8 80.0 84.9 82.4
Oracle 100.0 76.8 86.9 91.5 89.1 90.3
C
D
E
Ranker 98.8 64.3 77.9 85.3 90.7 87.9 87.4 61.5 72.2 82.0 88.8 85.3
Crawler 100.0 44.2 61.3 85.8 68.4 76.1 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
N
98.6 56.6 71.9 83.8 88.4 86.1 86.0 54.2 66.5 78.4 85.7 81.9
Crawler
P
98.8 65.5 78.7 86.1 90.4 88.2 87.6 62.7 73.1 82.6 88.5 85.4
Oracle 100.0 70.3 82.6 89.5 93.1 91.3
Table 1: Scope resolution performance of various configurations over each subset of the Shared Task
data. Ranker refers to the system of Read et al (2012); Crawler refers to our current system in isolation,
or falling back to the Ranker prediction either when the sentence is not covered by the parser (Crawler
N
),
or when the parse probability is predicted to be less than 0.5 (Crawler
P
); finally, Oracle simulates best
possible selection among the Ranker and Crawler predictions (and would be ill-defined on system cues).
annotated with token-level information about the
cues and scopes in every negated sentence. The
training set contains 848 negated sentences, the
development set 144, and the evaluation set 235.
As there can be multiple usages of negation in one
sentence, this corresponds to 984, 173, and 264
instances, respectively.
Being rule-based, our system does not require
any training data per se. However, the majority of
our rule development and error analysis were per-
formed against the designated training data. We
used the designated development data for a single
final round of error analysis and corrections. The
system was declared frozen before running with
the formal evaluation data. All numbers reported
here reflect this frozen system.
10
4.2 Results
Table 1 presents the results of our various config-
urations in terms of both (a) whole scopes (i.e. a
true positive is only generated when the predicted
scope matches the gold scope exactly) and (b) in-
scope tokens (i.e. a true positive for every token
the system correctly predicts to be in scope). The
table also details the performance upper-bound for
system combination, in which an oracle selects the
system prediction which scores the greater token-
wise F
1
for each gold cue.
The low recall levels for Crawler can be mostly
that the archaic style in the 2012
*
SEM Shared Task texts
has a strong adverse effect on the parser.
10
The code and data are available from http://www
.delph-in.net/crawler/, for replicability (Fokkens et al,
2013).
attributed to imperfect parser coverage. Crawler
N
,
which falls back just for parse failure brings the
recall back up, and results in F
1
levels closer to
the system of Read et al (2012), albeit still not
quite advancing the state of the art (except over
the development set). Our best results are from
Crawler
P
, which outperforms all other configura-
tions on the development and evaluation sets.
The Oracle results are interesting because they
show that there is much more to be gained in com-
bining our semantics-based system with the Read
et al (2012) syntactically-focused system. Further
analysis of these results to draw out the patterns of
complementary errors and strengths is a promising
avenue for future work.
4.3 Error Analysis
To shed more light on specific strengths and weak-
nesses of our approach, we performed a manual er-
ror analysis of scope predictions by Crawler, start-
ing from gold cues so as to focus in-depth analy-
sis on properties specific to scope resolution over
MRSs. This analysis was performed on CDD, in
order to not bar future work on this task. Of the
173 negation cue instances in CDD, Crawler by it-
self makes 94 scope predictions that exactly match
the gold standard. In comparison, the system of
Read et al (2012) accomplishes 119 exact scope
matches, of which 80 are shared with Crawler; in
other words, there are 14 cue instances (or 8%
of all cues) in which our approach can improve
over the best-performing syntax-based submission
to the original Shared Task.
75
We reviewed the 79 negation instances where
Crawler made a wrong prediction in terms of ex-
act scope match, categorizing the source of failure
into five broad error types:
(1) Annotation Error In 11% of all instances, we
consider the annotations erroneous or inconsistent.
These judgments were made by two of the authors,
who both were familiar with the annotation guide-
lines and conventions observable in the data. For
example, Morante et al (2011) unambiguously
state that subordinating conjunctions shall not be
in-scope (8), whereas relative pronouns should be
(9), and a negated predicative argument to the cop-
ula must scope over the full clause (10):
(8) It was after nine this morning {when we} reached
his house and {found} ?neither? {you} ?nor?
{anyone else inside it}.
(9) ?We can imagine that in the confusion of flight
something precious, something which {he could}
?not? {bear to part with}, had been left behind.
(10) He said little about the case, but from that little we
gathered that he also was not ?dis?{satisfied} at the
course of events.
(2) Parser Failure Close to 30% of Crawler fail-
ures reflect lacking coverage in the ERG parser,
i.e. inputs for which the parser does not make
available an analysis (within certain bounds on
time and memory usage).
11
In this work, we have
treated the ERG as an off-the-shelf system, but
coverage could certainly be straightforwardly im-
proved by adding analyses for phenomena partic-
ular to turn-of-the-20th-century British English.
(3) MRS Inadequacy Another 33% of our false
scope predictions are Crawler-external, viz. owing
to erroneous input MRSs due to imperfect disam-
biguation by the parser or other inadequacies in
the parser output. Again, these judgments (assign-
ing blame outside our own work) were double-
checked by two authors, and we only counted
MRS imperfections that actually involve the cue
or in-scope elements. Here, we could anticipate
improvements by training the parse ranker on in-
domain data or otherwise adapting it to this task.
(4) Cue Selection In close to 9% of all cases,
there is a valid MRS, but Crawler fails to pick out
an initial EP that corresponds to the negation cue.
This first type of genuine crawling failure often re-
lates to cues expressed as affixation (11), as well
11
Overall parsing coverage on this data is about 86%, but
of course all parser failures on sentences containing negation
surface in our error analysis of Crawler in isolation.
Scopes Tokens
Method Prec Rec F
1
Prec Rec F
1
C
D
E
Boxer 76.1 41.0 53.3 69.2 82.3 75.2
Crawler 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
P
87.6 62.7 73.1 82.6 88.5 85.4
Table 2: Comparison to Basile et al (2012).
as to rare usages of cue expressions that predomi-
nantly occur with different categories, e.g. neither
as a generalized quantifier (12):
(11) Please arrange your thoughts and let me know, in
their due sequence, exactly what those events are
{which have sent you out} ?un?{brushed} and un-
kempt, with dress boots and waistcoat buttoned
awry, in search of advice and assistance.
(12) You saw yourself {how} ?neither? {of the inspec-
tors dreamed of questioning his statement}, extraor-
dinary as it was.
(5) Crawler Deficiency Finally, a little more
than 16% of incorrect predictions we attribute to
our crawling rules proper, where we see many
instances of under-coverage of MRS elements
(13, 14) and a few cases of extending the scope too
wide (15). In the examples below, erroneous scope
predictions by Crawler are indicated through un-
derlining. Hardly any of the errors in this category,
however, involve semantically vacuous tokens.
(13) He in turn had friends among the indoor
servants who unite in {their} fear and
?dis?{like of their master}.
(14) He said little about the case, but from that
little we gathered that {he also was} ?not?
{dissatisfied at the course of events}.
(15) I tell you, sir, {I could}n?t move a finger, ?nor?
{get my breath}, till it whisked away and was gone.
5 Discussion and Comparison
The example in (1) nicely illustrates the strengths
of the MRS Crawler and of the abstraction pro-
vided by the deep linguistic analysis made pos-
sible by the ERG. The negated verb in that sen-
tence is know, and its first semantic argument is
The German. This semantic dependency is di-
rectly and explicitly represented in the MRS, but
the phrase expressing the dependent is not adja-
cent to the head in the string. Furthermore, even
a system using syntactic structure to model scope
would be faced with a more complicated task than
our crawling rules: At the level of syntax the de-
pendency is mediated by both verb phrase coordi-
nation and the control verb profess, as well as by
the semantically empty infinitival marker to.
76
The system we propose is very similar in spirit
to that of Basile et al (2012). Both systems map
from logical forms with explicit representations of
scope of negation out to string-based annotations
in the format provided by the Shared Task gold
standard. The main points of difference are in the
robustness of the system and in the degree of tai-
loring of both the rules for determining scope on
the logical form level and the rules for handling se-
mantically vacuous elements. The system descrip-
tion in Basile et al (2012) suggests relatively little
tailoring at either level: aside from adjustments to
the Boxer lexicon to make more negation cues take
the form of the negation operator in the DRS, the
notion of scope is directly that given in the DRS.
Similarly, their heuristic for picking up semanti-
cally vacuous words is string-based and straight-
forward. Our system, on the other hand, models
the annotation guidelines more closely in the def-
inition of the MRS crawling rules, and has more
elaborated rules for handling semantically empty
words. The Crawler alone is less robust than the
Boxer-based system, returning no output for 29%
of the cues in CDE. These factors all point to
higher precision and lower recall for the Crawler
compared to the Boxer-based system. At the to-
ken level, that is what we see. Since full-scope re-
call depends on token-level precision, the Crawler
does better across the board at the full-scope level.
A comparison of the results is shown in Table 2.
A final key difference between our results and
those of Basile et al (2012) is the cascading with
a fall-back system. Presumably a similar system
combination strategy could be pursued with the
Boxer-based system in place of the Crawler.
6 Conclusion and Outlook
Our motivation in this work was to take the design
of the 2012
*
SEM Shared Task on negation analy-
sis at face value?as an overtly semantic problem
that takes a central role in our long-term pursuit of
language understanding. Through both theoreti-
cal and practical reflection on the nature of repre-
sentations at play in this task, we believe we have
demonstrated that explicit semantic structure will
be a key driver of further progress in the analy-
sis of negation. We were able to closely align
two independently developed semantic analyses?
the negation-specific annotations of Morante et al
(2011), on the one hand, and the broad-coverage,
MRS meaning representations of the ERG, on the
other hand. In our view, the conceptual correla-
tion between these two semantic views on nega-
tion analysis reinforces their credibility.
Unlike the rather complex top-performing sys-
tems from the original 2012 competition, our MRS
Crawler is defined by a small set of general rules
that operate over general-purpose, explicit mean-
ing representations. Thus, our approach scores
high on transparency, adaptability, and replicabil-
ity. In isolation, the Crawler provides premium
precision but comparatively low recall. Its limi-
tations, we conjecture, reflect primarily on ERG
parsing challenges and inconsistencies in the tar-
get data. In a sense, our approach pushes a
larger proportion of the task into the parser, mean-
ing (a) there should be good opportunities for
parser adaptation to this somewhat idiosyncratic
text type; (b) our results can serve to offer feed-
back on ERG semantic analyses and parse rank-
ing; and (c) there is a much smaller proportion
of very task-specific engineering. When embed-
ded in a confidence-thresholded cascading archi-
tecture, our system advances the state of the art
on this task, and oracle combination scores sug-
gest there is much remaining room to better ex-
ploit the complementarity of approaches in our
study. In future work, we will seek to better un-
derstand the division of labor between the systems
involved through contrastive error analysis and
possibly another oracle experiment, constructing
gold-standard MRSs for part of the data. It would
also be interesting to try a task-specific adaptation
of the ERG parse ranking model, for example re-
training on the pre-existing treebanks but giving
preference to analyses that lead to correct Crawler
results downstream.
Acknowledgments
We are grateful to Dan Flickinger, the main devel-
oper of the ERG, for many enlightening discus-
sions and continuous assistance in working with
the analyses available from the grammar. This
work grew out of a discussion with colleagues of
the Language Technology Group at the University
of Oslo, notably Elisabeth Lien and Jan Tore L?n-
ning, to whom we are indebted for stimulating co-
operation. Furthermore, we have benefited from
comments by participants of the 2013 DELPH-
IN Summit, in particular Joshua Crowgey, Guy
Emerson, Glenn Slayden, Sanghoun Song, and
Rui Wang.
77
References
Alshawi, H. (Ed.). 1992. The Core Language Engine.
Cambridge, MA, USA: MIT Press.
Basile, V., Bos, J., Evang, K., and Venhuizen, N.
2012. UGroningen. Negation detection with Dis-
course Representation Structures. In Proceedings of
the 1st Joint Conference on Lexical and Computa-
tional Semantics (p. 301 ? 309). Montr?al, Canada.
Bender, E. M. 2013. Linguistic fundamentals for nat-
ural language processing: 100 essentials from mor-
phology and syntax. San Rafael, CA, USA: Morgan
& Claypool Publishers.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. 2005. Minimal Recursion Semantics. An intro-
duction. Research on Language and Computation,
3(4), 281 ? 332.
Curran, J., Clark, S., and Bos, J. 2007. Linguistically
motivated large-scale NLP with C&C and Boxer.
In Proceedings of the 45th Meeting of the Associa-
tion for Computational Linguistics Demo and Poster
Sessions (p. 33 ? 36). Prague, Czech Republic.
Flickinger, D. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engi-
neering, 6 (1), 15 ? 28.
Fokkens, A., van Erp, M., Postma, M., Pedersen,
T., Vossen, P., and Freire, N. 2013. Offspring
from reproduction problems. What replication fail-
ure teaches us. In Proceedings of the 51th Meet-
ing of the Association for Computational Linguistics
(p. 1691 ? 1701). Sofia, Bulgaria.
Koller, A., and Thater, S. 2005. Efficient solving and
exploration of scope ambiguities. In Proceedings of
the 43rd Meeting of the Association for Computa-
tional Linguistics: Interactive Poster and Demon-
stration Sessions (p. 9 ? 12). Ann Arbor, MI, USA.
Lapponi, E., Read, J., and ?vrelid, L. 2012. Repre-
senting and resolving negation for sentiment analy-
sis. In Proceedings of the 2012 ICDM workshop on
sentiment elicitation from natural text for informa-
tion retrieval and extraction. Brussels, Belgium.
Morante, R., and Blanco, E. 2012. *SEM 2012 Shared
Task. Resolving the scope and focus of negation. In
Proceedings of the 1st Joint Conference on Lexical
and Computational Semantics (p. 265 ? 274). Mon-
tr?al, Canada.
Morante, R., and Daelemans, W. 2012. ConanDoyle-
neg. Annotation of negation in Conan Doyle stories.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation. Istanbul,
Turkey.
Morante, R., Schrauwen, S., and Daelemans, W. 2011.
Annotation of negation cues and their scope guide-
lines v1.0 (Tech. Rep. # CTRS-003). Antwerp, Bel-
gium: Computational Linguistics & Psycholinguis-
tics Research Center, Universiteit Antwerpen.
Morante, R., and Sporleder, C. 2012. Modality and
negation. An introduction to the special issue. Com-
putational Linguistics, 38(2), 223 ? 260.
Oepen, S., and L?nning, J. T. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Read, J., Velldal, E., ?vrelid, L., and Oepen, S. 2012.
UiO1. Constituent-based discriminative ranking for
negation resolution. In Proceedings of the 1st Joint
Conference on Lexical and Computational Seman-
tics (p. 310 ? 318). Montr?al, Canada.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., and
Csirik, J. 2008. The BioScope corpus. Biomedical
texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11).
78
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 310?318,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UiO1: Constituent-Based Discriminative Ranking for Negation Resolution
Jonathon Read Erik Velldal Lilja ?vrelid Stephan Oepen
University of Oslo, Department of Informatics
{jread,erikve,liljao,oe}@ifi.uio.no
Abstract
This paper describes the first of two systems
submitted from the University of Oslo (UiO)
to the 2012 *SEM Shared Task on resolving
negation. Our submission is an adaption of
the negation system of Velldal et al (2012),
which combines SVM cue classification with
SVM-based ranking of syntactic constituents
for scope resolution. The approach further ex-
tends our prior work in that we also identify
factual negated events. While submitted for
the closed track, the system was the top per-
former in the shared task overall.
1 Introduction
The First Joint Conference on Lexical and Compu-
tational Semantics (*SEM 2012) hosts a shared task
on resolving negation (Morante and Blanco, 2012).
This involves the subtasks of (i) identifying nega-
tion cues, (ii) identifying the in-sentence scope of
these cues, and (iii) identifying negated (and factual)
events. This paper describes a system submitted by
the Language Technology Group at the University of
Oslo (UiO). Our starting point is the negation system
developed by Velldal et al (2012) for the domain of
biomedical texts, an SVM-based system for classi-
fying cues and ranking syntactic constituents to re-
solve cue scopes. However, we extend and adapt
this system in several important respects, such as in
terms of the underlying linguistic formalisms that
are used, the textual domain, handling of morpho-
logical cues and discontinuous scopes, and in that
the current system also identifies negated events.
The data sets used for the shared task include
the following, all based on negation-annotated Co-
nan Doyle (CD) stories (Morante and Daelemans,
2012): a training set of 3644 sentences (hereafter
referred to as CDT), a development set of 787 sen-
tences (CDD), and a held-out evaluation set of 1089
sentences (CDE). We will refer to the combination
of CDT and CDD as CDTD. An example of an an-
notated sentence is shown in (1) below, where the
cue is marked in bold, the scope is underlined, and
the event marked in italics.
(1) There was no answer.
We describe two different system configurations,
both of which were submitted for the closed track
(hence we can only make use of the data provided
by the task organizers). The systems only differ
with respect to how they were optimized. In the
first configuration, (hereafter I), all components in
the pipeline had their parameters tuned by 10-fold
cross-validation across CDTD. The second config-
uration (II) is tuned against CDD using CDT for
training. The rationale for this strategy is to guard
against possible overfitting effects that could result
from either optimization scheme, given the limited
size of the data sets. For the held-out testing all mod-
els are estimated on the entire CDTD.
Unless otherwise noted, all reported scores are
generated using the evaluation script provided by the
organizers, which breaks down performance with re-
spect to cues, events, scope tokens, and two vari-
ants of scope-level exact match (one requiring exact
match of cues and the other only partial cue match).
The latter two scores are identical for our system
hence are not duplicated in this paper. Furthermore,
as we did not optimize for the scope tokens measure
this is only reported for the final evaluation.
Note also that the evaluation actually includes
two variants of the metrics mentioned above; a set
of primary measures with precision computed as
P = TP/(TP + FP ) and a set of so-called B mea-
sures that instead uses P = TP/S, where S is the
310
total number of predictions made by the system. The
reason why S is not identical with TP + FP is
that partial matches are only counted as FNs (and
not FPs) in order to avoid double penalties. We
do not report the B measures for development test-
ing as they were only introduced for the final eval-
uation and hence were not considered in our sys-
tem optimization. We note though, that the relative-
ranking of participating systems for the primary and
B measures is identical, and that the correlation be-
tween the paired lists of scores is nearly perfect
(r = 0.997).
The paper is structured according to the compo-
nents of our system. Section 2 details the process of
identifying instances of negation through the disam-
biguation of known cue words and affixes. Section 3
describes our hybrid approach to scope resolution,
which utilizes both heuristic and data-driven meth-
ods to select syntactic constituents. Section 4 dis-
cusses our event detection component, which first
applies a classifier to filter out non-factual events
and then uses a learned ranking function to select
events among in-scope tokens. End-to-end results
are presented in Section 5.
2 Cue Detection
Cue identification is based on the light-weight clas-
sification scheme presented by Velldal et al (2012).
By treating the set of cue words as a closed class,
Velldal et al (2012) showed that one could greatly
reduce the number of examples presented to the
learner, and correspondingly the number of fea-
tures, while at the same time improving perfor-
mance. This means that the classifier only attempts
to ?disambiguate? known cue words, while ignoring
any words not observed as cues in the training data.
The classifier applied in the current submission
is extended to also handle morphological or affixal
negation cues, such as the prefix cue in impatience,
the infix in carelessness, and the suffix of colourless.
The negation affixes observed in CDTD are; the pre-
fixes un, dis, ir, im, and in; the infix less (we inter-
nally treat this as the suffixes lessly and lessness);
and the suffix less. Of the total set of 1157 cues in
the training and development data, 192 are affixal.
There are, however, a total of 1127 tokens matching
one of the affix patterns above, and while we main-
tain the closed class assumption also for the affixes,
the classifier will need to consider their status as a
cue or non-cue when attaching to any such token, as
in image, recklessness, and bless.
2.1 Features
In the initial formulation of Velldal (2011), an SVM
classifier was applied using simple n-gram features
over words, both full forms and lemmas, to the
left and right of the candidate cues. In addition to
these token-level features, the classifier we apply
here includes features specifically targeting affixal
cues. The first such feature records character n-
grams from both the beginning and end of the base
that an affix attaches to (up to five positions). For
a context like impossible we would record n-grams
such as {possi, poss, . . .} and {sible, ible, . . .}, and
combine this with information about the affix itself
(im) and the token part-of-speech (?JJ?).
For the second type of affix-specific features, we
try to emulate the effect of a lexicon look-up of the
remaining substring that an affix attaches to, check-
ing its status as an independent base form and its
part-of-speech. In order to take advantage of such
information while staying within the confines of the
closed track, we automatically generate a lexicon
from the training data, counting the instances of each
PoS tagged lemma in addition to n-grams of word-
initial characters (again recording up to five posi-
tions). For a given match of an affix pattern, a fea-
ture will then record these counts for the substring it
attaches to. The rationale for this feature is that the
occurrence of a substring such as un in a token such
as underlying should be less likely as a cue given
that the first part of the remaining string (e.g., derly)
would be an unlikely way to begin a word.
It is also possible for a negation cue to span multi-
ple tokens, such as the (discontinuous) pair neither /
nor or fixed expressions like on the contrary. There
are, however, only 16 instances of such multiword
cues (MWCs) in the entire CDTD. Rather than let-
ting the classifier be sensitive to these corner cases,
we cover such MWC patterns using a small set of
simple post-processing heuristics. A small stop-list
is used for filtering out the relevant words from the
examples presented to the classifier (on, the, etc.).
Note that, in terms of training the final classifiers,
CDTD provides us with a total of 1162 positive and
311
Data set Model Prec Rec F1
CDTD
Baseline 92.25 88.50 90.34
ClassifierI 94.99 95.07 95.03
CDD
Baseline 90.68 84.39 87.42
ClassifierII 93.75 95.38 94.56
CDE
Baseline 87.10 92.05 89.51
ClassifierI 91.42 92.80 92.10
ClassifierII 89.17 93.56 91.31
Table 1: Detecting negation cues using the two clas-
sifiers and the majority-usage baseline.
1100 negative training examples, given our closed-
class treatment of cues.
Before we turn to the results, note that the dif-
ference between the two submitted versions of the
classifier (I and II) only concerns the orders of the
n-grams used for the token-level features.1
2.2 Results
Table 1 presents the results for our cue classifier. As
an informed baseline, we also tried classifying each
word based on its most frequent use as a cue or non-
cue in the training data. (Affixal cue occurrences are
counted by looking at both the affix-pattern and the
base it attaches to, basically treating the entire token
as a cue. Tokens that end up being classified as cues
are then matched against the affix patterns observed
during training in order to correctly delimit the an-
notation of the cue.) This simple majority-usage
approach actually provides a fairly strong baseline,
yielding an F1 of 90.34 on CDTD. Compare this to
the F1 of 95.03 obtained by the classifier on the same
data set. However, when applying the models to the
held-out set, with models estimated over the entire
CDTD, the classifier suffers a slight drop in perfor-
mance, leaving the baseline even more competitive:
While our best performing final cue classifier (I)
achieves F1=92.10, the baseline achieves F1=89.51,
and even outperforms four of the ten cue detection
systems submitted for the shared task (three of the
12 shared task submissions use the same classifier).
1Classifier I records the lemma and full form of the target
token, and lemmas two positions left/right. Classifier II records
the lemma, form, and PoS of the target, full forms three posi-
tions to the left and one to the right, PoS one position right/left,
and lemmas three positions to the right. The affixal-specific fea-
tures are the same for both configurations as described above.
S
NP
EX
There
VP
VBD
was
NP
DT
no
NN
answer
.
.
Figure 1: Example parse tree provided in the data,
highlighting our candidate scope constituents.
Inspecting the predictions of the classifier on
CDD, which comprises a total of 173 gold anno-
tated cues, we find that Classifier I mislabels 11
false positives (FPs) and seven false negatives (FNs).
Of the FPs, we find five so-called false negation
cues (Morante et al, 2011), including three in-
stances of the fixed expression none the less. The
others are affixal cues, of which two are clearly
wrong (underworked, universal) while others might
arguably be due to annotation errors (insuperable,
unhappily, endless, listlessly). Among the FNs, two
are due to MWCs not covered by our heuristics (e.g.,
no more), with the remainder concerning affixes.
3 Constituent-Based Scope Resolution
During the development of our scope resolution sys-
tem we have pursued both a rule-based and data-
driven approach. Both are rooted in the assumption
that the scope of negations corresponds to a syntac-
tically meaningful unit. Our starting point here will
be the syntactic analyses provided by the task or-
ganizers (see Figure 1), generated using the rerank-
ing parser of Charniak and Johnson (2005). How-
ever, as alignment between scope annotations and
syntactic units is not straightforward for all cases,
we apply several exception rules that ?slacken? the
requirements for alignment, as discussed in Sec-
tion 3.1. In Sections 3.2 and 3.3 we detail our
rule-based and data-driven approaches, respectively.
Note that the predictions of the rule-based compo-
nent will be incorporated as features in the learned
model, similarly to the set-up described by Read et
al. (2011). Section 3.4 details the post-processing
we apply to handle cases of discontinuous scope, be-
312
fore Section 3.5 finally presents development results
together with a brief error analysis.
3.1 Constituent Alignment and Slackening
In order to test our initial assumption that syntactic
units correspond to scope annotations, we quantify
the alignment of scopes with constituents in CDT,
excluding 97 negations that do not have a scope.
We find that the initial alignment is rather low at
52.42%. We therefore formulate a set of slacken-
ing heuristics, designed to improve on this alignment
by removing certain constituents at the beginning or
end of a scope. First of all, removing constituent-
initial and -final punctuation improves alignment to
72.83%. We then apply the following slackening
rules, with examples indicating the resulting scope
following slackening (not showing events):
- Remove coordination (CC) and following con-
juncts if the coordination is a rightwards sibling
of an ancestor of the cue and it is not directly
dominated by an NP.
(2) Since we have been so unfortunate as to miss him
and have no notion [. . . ]
- Remove S* to the right of cue, if delimited by
punctuation.
(3) ?There is no other claimant, I presume ??
- Remove constituent-initial SBAR.
(4) If it concerned no one but myself I would not
try to keep it from you.?
- Remove punctuation-delimited NPs.
(5) ?But I can?t forget them, Miss Stapleton,? said I.
- Remove constituent-initial RB, CC, UH,
ADVP or INTJ.
(6) And yet it was not quite the last.
The slackening rules are based on a few obser-
vations. First, scope rarely crosses coordination
boundaries (with the exception of nominal coordi-
nation). Second, scope usually does not cross clause
boundaries (indicated by S/SBAR). Furthermore, ti-
tles and other nominals of address are not included
in the scope. Finally, sentence and discourse adver-
bials are often excluded from the scope. Since these
express semantic distinctions, we approximate this
RB//VP/SBAR if SBAR\WH*
RB//VP/S
RB//S
DT/NP if NP/PP
DT//SBAR if SBAR\WHADVP
DT//S
JJ//ADJPVP/S if S\VP\VB*[@lemma="be"]
JJ/NP/NP if NP\PP
JJ//NP
UH
IN/PP
NN/NP//S/SBAR if SBAR\WHNP
NN/NP//S
CC/SINV
Figure 2: Scope resolution heuristics.
notion syntactically using parts-of-speech and con-
stituent category labels expressing adverbials (RB),
coordinations (CC), various types of interjections
(UH, INTJ) and adverbial phrases (ADVP). We may
note here that syntactic categories are not always
sufficient to express semantic distinctions. Preposi-
tional phrases, for instance, are often used to express
the same type of discourse adverbials, but can also
express a range of other distinctions (e.g., tempo-
ral or locative adverbials), which are included in the
scope. So a slackening rule removing initial PPs was
tried but not found to improve overall alignment.
After applying the above slackening rules the
alignment rate for CDT improves to 86.13%. This
also represents an upper-bound on our performance,
as we will not be able to correctly predict a scope
that does not align with a (slackened) constituent.
3.2 Heuristics Operating over Constituents
The alignment of constituents and scopes reveal con-
sistent patterns and we therefore formulate a set of
heuristic rules over constituents. These are based
on frequencies of paths from the cue to the scope-
aligned constituent for the annotations in CDT, as
well as the annotation guidelines (Morante et al,
2011). The rules are formulated as paths over con-
stituent trees and are presented in Figure 2. The
path syntax is based on LPath (Lai and Bird, 2010).
The rules are listed in order of execution, showing
how more specific rules are consulted before more
general ones. We furthermore allow for some ad-
ditional functionality in the interpretation of rules
by enabling simple constraints that are applied to
the candidate constituent. For example, the rule
RB//VP/SBAR if SBAR\WH* will be activated when
the cue is an adverb having some ancestor VP which
has a parent SBAR, where the SBAR must contain a
WH-phrase among its children.
313
In cases where no rule is activated we use a de-
fault scope prediction, which expands the scope to
both the left and the right of the cue until either the
sentence boundary or a punctuation mark is reached.
The rules are evaluated individually in Section 3.5
below and the rule predictions are furthermore em-
ployed as features for the ranker described below.
3.3 Constituent Ranking
Our data-driven approach to scope resolution in-
volves learning a ranking function over candidate
syntactic constituents. The approach has similari-
ties to discriminative parse selection, except that we
here rank subtrees rather than full parses.
When defining the training data, we begin by se-
lecting negations for which the parse tree contains
a constituent that (after slackening) aligns with the
gold scope. We then select an initial candidate by
selecting the smallest constituent that spans all the
words in the cue, and then generate subsequent can-
didates by traversing the path to the root of the
tree (see Figure 1). This results in a mean ambi-
guity of 4.9 candidate constituents per negation (in
CDTD). Candidates whose projection corresponds
to the gold scope are labeled as correct; all others are
labeled as incorrect. Experimenting with a variety of
feature types (listed in Table 2), we use the imple-
mentation of ordinal ranking in the SVMlight toolkit
(Joachims, 2002) to learn a linear scoring function
for preferring correct candidate scopes.
The most informative feature type is the LPath
from cue, which in addition to recording the full
path from the cue to the candidate constituent
(e.g., the path to the correct candidate in Fig-
ure 1 is no/DT/NP/VP/S), also includes delexicalized
(./DT/NP/VP/S), generalized (no/DT//S), and gen-
eralized delexicalized versions (./DT//S).
Note that the rule prediction feature facilitates a
hybrid approach by recording whether the candidate
matches the boundaries of the scope predicted by the
rules of Section 3.2, as well as the degree of overlap.
3.4 Handling Discontinuous Scope
10.3% of the scopes in the training data are what
(Morante et al, 2011) refer to as discontinuous. This
means that the scope contains two or more parts
which are bridged by tokens other than the cue.
Feature types I II
LPath from cue ? ?
LPath from cue bigrams and trigrams ? ?
LPath from cue to left/right boundary ?
LPath to left/right boundary ?
LPath to root ?
Punctuation to left/right ? ?
Rule prediction ?
Sibling bigrams ?
Size in tokens, relative to sentence (%) ? ?
Surface bigrams ? ?
Tree distance from cue ? ?
Table 2: Features used to describe candidate con-
stituents for scope resolution, with indications of
presence in our two system configurations.
(7) I therefore spent the day at my club and did not
return to Baker Street until evening.
(8) There was certainly no physical injury of any kind.
The sentence in (7) exemplifies a common cause
of scopal discontinuity in the data, namely ellipsis
(Morante et al, 2011). Almost all of these are cases
of coordination, as in example (7) where the cue is
found in the final conjunct (did not return [. . . ]) and
the scope excludes the preceding conjunct(s) (there-
fore spent the day at my club). There are also some
cases of adverbs that are excluded from the scope,
causing discontinuity, as in (8), where the adverb
certainly is excluded from the scope.
In order to deal with discontinuous scopes we for-
mulate two simple post-processing heuristics, which
are applied after rules/ranking: (1) If the cue is in
a conjoined phrase, remove the previous conjuncts
from the scope, and (2) remove sentential adverbs
from the scope (where a list of sentential adverbs
was compiled from the training data).
3.5 Results
Our development procedure evaluated all permuta-
tions of feature combinations, searching for opti-
mal parameters using gold-standard cues. Table 2
indicates which features are included in our two
ranker configurations, i.e., tuning by 10-fold cross-
validation on CDTD (I) vs. a train/test-split for
CDT/CDD(II).
Table 3 lists the results of our scope resolution
approaches applied to gold cues. As a baseline, all
314
Data set Model Prec Rec F1
CDTD
Baseline 98.31 33.18 49.61
Rules 100.00 71.37 83.29
RankerI 100.00 73.55 84.76
CDD
Baseline 100.00 36.31 53.28
Rules 100.00 69.64 82.10
RankerII 100.00 70.24 82.52
CDE
Baseline 96.47 32.93 49.10
Rules 98.73 62.65 76.66
RankerI 98.77 64.26 77.86
RankerII 98.75 63.45 77.26
Table 3: Scope resolution for gold cues using the
two versions of the ranker, also listing the perfor-
mance of the rule-based approach in isolation.
cases are assigned the default scope prediction of the
rule-based approach. On CDTD this results in an F1
of 49.61 (P=98.31, R=33.18); compare to the ranker
in Configuration I on the same data set (F1=84.76,
P=100.00, R=73.55). We note that our different op-
timization procedures do not appear to have made
much difference to the learned ranking functions as
both perform similarly on the held-out data, though
suffering a slight drop in performance compared to
the development results. We also evaluate the rules
and observe that this approach achieves similar held-
out results. This is particularly note-worthy given
that there are only fourteen rules plus the default
scope baseline. Note that, as the rankers performed
better than the rules in isolation on both CDTD and
CDD during development, our final system submis-
sions are based on rankers I and II from Table 3.
We performed a manual error analysis of our
scope resolution system (RankerII) on the basis of
CDD (using gold cues). First, we may note that
parse errors are a common sources of scope res-
olution errors. It is well-known that coordina-
tion presents a difficult construction for syntactic
parsers, and we often find incorrectly parsed coordi-
nate structures among the system errors. Since coor-
dination is used both in the slackening rules and the
analysis of discontinuous scopes, these errors have
clear effects on system performance. We may fur-
ther note that discourse-level adverbials, such as in
the second place in example (9) below, are often in-
cluded in the scope assigned by our system, which
they should not be according to the gold annotation.
(9) But, in the second place, why did you not come at once?
There are also quite a few errors related to the scope
of affixal cues, which the ranker often erroneously
assigns a scope that is larger than simply the base
which the affix attaches to.
4 Event Detection
Our event detection component implements two
stages: First we apply a factuality classifier, and
then we identify negated events2 for those contexts
that have been labeled as factual. We detail the two
stages in order below.
4.1 Factuality Detection
The annotation guidelines of Morante et al (2011)
specify that events should only be annotated for
negations that have a scope and that occur in fac-
tual statements. This means that we can view the
*SEM data sets to implicitly annotate factuality and
non-factuality, and take advantage of this to train an
SVM factuality classifier. We take positive exam-
ples to correspond to negations annotated with both
a scope and an event, while negative examples corre-
spond to scope negations with no event. For CDTD,
this strategy gives 738 positive and 317 negative ex-
amples, spread over a total of 930 sentences. Note
that we do not have any explicit annotation of cue
words for these examples. All we have are instances
of negation that we know to be within a factual or
non-factual context, but the indication of factuality
may typically be well outside the annotated nega-
tion scope. For our experiments here, we therefore
use the negation cue itself as a place-holder for the
abstract notion of context that we are really classi-
fying. Given the limited amount of data, we only
optimize our factuality classifier by 10-fold cross-
validation on CDTD (i.e., the same configuration is
used for submissions I and II).
The feature types we use are all variations over
bag-of-words (BoW) features. We include left- and
right-oriented BoW features centered on the nega-
tion cue, recording forms, lemmas, and PoS, and us-
ing both unigrams and bigrams. The features are ex-
2Note that the annotation guidelines use the term event
rather broadly as referring to a process, action, state, or prop-
erty (Morante et al, 2011).
315
Data set Model Prec Rec F1 Acc
CDTD
Baseline 69.95 100.00 82.32 69.95
Classifier 84.51 96.07 89.92 83.98
CDE
Baseline 69.48 100.00 81.99 69.48
Classifier 77.73 95.91 85.86 78.31
Table 4: Results for factuality detection (using gold
negation cues and scopes). Due to the limited train-
ing data for factuality, the classifier is only opti-
mized by 10-fold cross-validation on CDTD.
tracted from the sentence as a whole, as well as from
a local window of six tokens to each side of the cue.
Table 4 provides results for factuality classifica-
tion using gold-standard cues and scopes.3 We also
include results for a baseline approach that simply
considers all cases to be factual, i.e., the majority
class. In this case precision is identical to accuracy
and recall is 100%. For precision and accuracy we
see that the classifier improves substantially over the
baseline on both data sets, although there is a bit of a
drop in performance when going from the 10-fold to
held-out results. There also seem to be some signs
of overfitting, given that roughly 70% of the training
examples end up as support vectors.
4.2 Ranking Events
Having filtered out non-factual contexts, events are
identified by applying a similar approach to that of
the scope-resolving ranker described in Section 3.3.
In this case, however, we rank tokens as candidates
for events. For simplicity in this first round of de-
velopment we make the assumption that all events
are single words. Thus, the system will be unable to
correctly predict the event in the 6.94% of instances
in CDTD that are multi-word.
We select candidate words from all those marked
as being in the scope (including substrings of to-
kens with affixal cues). This gives a mean ambigu-
ity of 7.8 candidate events per negation (in CDTD).
Then, discarding multi-word training examples, we
use SVMlight to learn a ranking function for identi-
fying events among the candidates.
Table 5 shows the features employed, with in-
3As this is not singled out as a separate subtask in the shared
task itself, these are the only scores in the paper not computed
using the script provided by the organizers.
Feature type I II
Contains affixal cue ?
Following lemma ?
Lemma ? ?
LPath to scope constituent ? ?
LPath to scope constituent bigrams ? ?
Part-of-speech ? ?
Position in scope ? ?
Preceding lemma ? ?
Preceding part-of-speech ? ?
Token distance from cue ? ?
Table 5: Features used to describe candidates for
event detection, with indications of presence in our
two system configurations.
Data set Model Prec Rec F1
CDTD RankerI 91.49 90.83 91.16
CDD RankerII 92.11 91.30 91.70
CDE
RankerI 83.73 83.73 83.73
RankerII 84.94 84.95 84.94
Table 6: Event detection for gold scopes and gold
factuality information.
dications as to their presence in our two configu-
rations (after an exhaustive search of feature com-
binations). The most important feature was LPath
to scope constituent. For example, in Figure 1
the scope constituent is the S root of the tree;
the path that describes the correct candidate is
answer/NN/NP/VP/S. As discussed in Section 3.3,
we also record generalized, delexicalized and gener-
alized delexicalized paths.
Table 6 lists the results of the event ranker applied
to gold-standard cues, scopes, and factuality. For a
comparative baseline, we implemented a keyword-
based approach that simply searches in-scope words
for instances of events previously observed in the
training set, sorted according to descending fre-
quency. This baseline achieves F1=29.44 on CDD.
For comparison, the ranker (II) achieves F1=91.70
on the same data set, as seen in Table 6. We also
see that Configuration II appears to generalize best,
with over 1.2 points improvement over the F1 of I.
An analysis of the event predictions for CDD in-
dicates that the most frequent errors (41.2%) are in-
stances where the ranker correctly predicts part of
the event but our single word assumption is invalid.
Another apparent error is that the system fails to
316
Submission I Submission II
Prec Rec F1 Prec Rec F1
Cues 91.42 92.80 92.10 89.17 93.56 91.31
Scopes 87.43 61.45 72.17 83.89 60.64 70.39
Scope Tokens 81.99 88.81 85.26 75.87 90.08 82.37
Events 60.50 72.89 66.12 60.58 75.00 67.02
Full negation 83.45 43.94 57.57 79.87 45.08 57.63
Cues B 89.09 92.80 90.91 86.97 93.56 90.14
Scopes B 59.30 61.45 60.36 56.55 60.64 58.52
Events B 57.62 72.89 64.36 58.60 75.00 65.79
Full negation B 42.18 43.94 43.04 41.90 45.08 43.43
Table 7: End-to-end results on the held-out data.
predict a main verb for the event, and instead pre-
dicts nouns (17.7% of all errors), modals (17.7%) or
prepositions (11.8%).
5 Held-Out Evaluation
Table 7 presents our final results for both system
configurations on the held-out evaluation data (also
including the B measures, as discussed in the intro-
duction). Comparing submission I and II, we find
that the latter has slightly better scores end-to-end.
However, as seen throughout the paper, the picture is
less clear-cut when considering the isolated perfor-
mance of each component. When ranked according
to the Full Negation measures, our submissions were
placed first and second (out of seven submissions in
the closed track, and twelve submissions total). It
is difficult to compare system performance on sub-
tasks, however, as each component will be affected
by the performance of the previous.
6 Conclusions
This paper has presented two closed-track submis-
sions for the *SEM 2012 shared task on negation
resolution. The systems were ranked first and sec-
ond overall in the shared task end-to-end evaluation,
and the submissions only differ with respect to the
data sets used for parameter tuning. There are four
components in the pipeline: (i) An SVM classifier
for identifying negation cue words and affixes, (ii)
an SVM-based ranker that combines empirical evi-
dence and manually-crafted rules to resolve the in-
sentence scope of negation, (iii) a classifier for de-
termining whether a negation is in a factual or non-
factual context, and (iv) a ranker that determines
(factual) negated events among in-scope tokens.
For future work we would like to try training sepa-
rate classifiers for affixal and token-level cues, given
that largely separate sets of features are effective for
the two cases. The system might also benefit from
sources of information that would place it in the
open track. These include drawing information from
other parsers and formalisms, generating cue fea-
tures from an external lexicon, and using additional
training data for factuality detection, e.g., FactBank
(Saur?? and Pustejovsky, 2009).
From observations on CDTD we note that approx-
imately 14% of scopes will be unresolvable as they
are not aligned with constituents (see Section 3.1).
This can perhaps be tackled by ranking tokens as
candidates for left and right scope boundaries (sim-
ilar to the event ranker in the current work). This
would improve the upper-bound to 100% at the ex-
pense of greatly increasing the number of candi-
dates. However, the strong discriminative power of
our current approach can still be incorporated using
constituent-based features.
Acknowledgments
We thank Roser Morante and Eduardo Blanco for
their work in organizing this shared task and com-
mitment to producing quality data. We also thank
the anonymous reviewers for their feedback. Large-
scale experimentation was carried out with the TI-
TAN HPC facilities at the University of Oslo.
317
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the Forty-Third Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, MI.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the Eighth
ACM International Conference on Knowledge Discov-
ery and Data Mining, Alberta.
Catherine Lai and Steven Bird. 2010. Querying linguis-
tic trees. Journal of Logic, Language and Information,
19:53?73.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics, Montreal.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja
?vrelid. 2011. Resolving speculation and negation
scope in biomedical articles using a syntactic con-
stituent ranker. In Proceedings of the Fourth Inter-
national Symposium on Languages in Biology and
Medicine, Singapore.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Erik Velldal. 2011. Predicting speculation: A simple dis-
ambiguation approach to hedge detection in biomedi-
cal literature. Journal of Biomedical Semantics, 2(5).
318
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 319?327,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UiO2: Sequence-Labeling Negation Using Dependency Features
Emanuele Lapponi Erik Velldal Lilja ?vrelid Jonathon Read
University of Oslo, Department of Informatics
{emanuel,erikve,liljao,jread}@ifi.uio.no
Abstract
This paper describes the second of two sys-
tems submitted from the University of Oslo
(UiO) to the 2012 *SEM Shared Task on re-
solving negation. The system combines SVM
cue classification with CRF sequence labeling
of events and scopes. Models for scopes and
events are created using lexical and syntactic
features, together with a fine-grained set of la-
bels that capture the scopal behavior of certain
tokens. Following labeling, negated tokens are
assigned to their respective cues using simple
post-processing heuristics. The system was
ranked first in the open track and third in the
closed track, and was one of the top perform-
ers in the scope resolution sub-task overall.
1 Introduction
Negation Resolution (NR) is the task of determin-
ing, for a given sentence, which tokens are affected
by a negation cue. The data set most prominently
used for the development of systems for automatic
NR is the BioScope Corpus (Vincze et al, 2008), a
collection of clinical reports and papers in the bio-
medical domain annotated with negation and specu-
lation cues and their scopes. The data sets released
in conjunction with the 2012 shared task on NR
hosted by The First Joint Conference on Lexical and
Computational Semantics (*SEM 2012) are com-
prised of the following negation annotated stories of
Conan Doyle (CD): a training set of 3644 sentences
drawn from The Hound of the Baskervilles (CDT), a
development set of 787 sentences taken from Wis-
teria Lodge (CDD; we will refer to the combina-
tion of CDT and CDD as CDTD), and a held-out
test set of 1089 sentences from The Cardboard Box
and The Red Circle (CDE). In these sets, the con-
cept of negation scope extends on the one adopted
in the BioScope corpus in several aspects: Nega-
tion cues are not part of the scope, morphological
(affixal) cues are annotated and scopes can be dis-
continuous. Moreover, in-scope states or events are
marked as negated if they are factual and presented
as events that did not happen (Morante and Daele-
mans, 2012). Examples (1) and (2) below are exam-
ples of affixal negation and discontinuous scope re-
spectively: The cues are bold, the tokens contained
within their scopes are underlined and the negated
event is italicized.
(1) Since we have been so unfortunate as to miss him [. . . ]
(2) If he was in the hospital and yet not on the staff he could
only have been a house-surgeon or a house-physician: lit-
tle more than a senior student.
Example (2) has no negated events because the sen-
tence is non-factual.
The *SEM shared task thus comprises three sub-
tasks: cue identification, scope resolution and event
detection. It is furthermore divided into two separate
tracks: one closed track, where only the data sup-
plied by the organizers (word form, lemma, PoS-tag
and syntactic constituent for each token) may be em-
ployed, and an open track, where participants may
employ any additional tools or resources.
Pragmatically speaking, a token can be either out
of scope or assigned to one or more of the three re-
maining classes: negation cue, in scope and negated
event. Additionally, in-scope tokens and negated
events are paired to the cues they are negated by.
319
Our system achieves this by remodeling the task as a
sequence labeling task. With annotations converted
to sequences of labels, we train a Conditional Ran-
dom Field (CRF) classifier with a range of different
feature types, including features defined over depen-
dency graphs. This article presents two submissions
for the *SEM shared task, differing only with re-
spect to how these dependency graphs were derived.
For our open track submission, the dependency rep-
resentations are produced by a state-of-the-art de-
pendency parser, whereas the closed track submis-
sion employs dependencies derived from the con-
stituent analyses supplied with the shared task data
sets through a process of constituent-to-dependency
conversion. In both systems, labeling of test data is
performed in two stages. First, cues are detected us-
ing a token classifier,1 and secondly, scope and event
resolution is achieved by post-processing the output
of the sequence labeler.
The two systems described in this paper have been
developed using CDT for training and CDD for test-
ing, and differ only with regard to the source of syn-
tactic information. All reported scores are generated
using an evaluation script provided by the task or-
ganizers. In addition to providing a full end-to-end
evaluation, the script breaks down results with re-
spect to identification of cues, events, scope tokens,
and two variants of scope-level exact match; one re-
quiring exact match also of cues and another only
partial cue match. For our system these two scope-
level scores are identical and so are not duplicated
in our reporting. Additionally we chose not to opti-
mize for the scope tokens measure, and hence this is
also not reported as a development result.
Note also that the official evaluation actually in-
cludes two different variants of the metrics men-
tioned above; a set of primary measures with pre-
cision computed as P=TP/(TP+FP) and a set of B
measures where precision is rather computed as
P=TP/SYS, where SYS is the total number of pre-
dictions made by the system. The reason why SYS is
not identical with TP+FP is that partial matches are
1Note that the cue classifier applied in the current paper is
the same as that used in the other shared task submission from
the University of Oslo (Read et al, 2012), and the two system
descriptions will therefore have much overlap on this particular
point. For all other components the architectures of the two
system are completely different, however.
only counted as FNs (and not FPs) in order to avoid
double penalties. We do not report the B measures
for development testing as they were introduced for
the final evaluation and hence were not considered
in our system optimization. We note though, that the
relative-ranking of participating systems for the pri-
mary and B measures is identical, and that the cor-
relation between the paired lists of scores is nearly
perfect (r=0.997).
The rest of the paper is structured as follows.
First, the cue classifier, its features and results are
described in Section 2. Section 3 presents the sys-
tem for scope and event resolution and details differ-
ent features, the model-internal representation used
for sequence-labeling, as well as the post-processing
component. Error analyses for the cue, scope and
event components are provided in the respective sec-
tions. Section 4 and 5 provide developmental and
held-out results, respectively. Finally, we provide
conclusions and some reflections regarding future
work in Section 6.
2 Cue detection
Identification of negation cues is based on the light-
weight classification scheme presented by Velldal et
al. (2012). By treating the set of cue words as a
closed class, Velldal et al (2012) showed that one
could greatly reduce the number of examples pre-
sented to the learner, and correspondingly the num-
ber of features, while at the same time improving
performance. This means that the classifier only at-
tempts to ?disambiguate? known cue words while
ignoring any words not observed as cues in the train-
ing data.
The classifier applied in the current submission
is extended to also handle affixal negation cues,
such as the prefix cue in impatience, the infix in
carelessness, and the suffix of colourless. The types
of negation affixes observed in CDTD are; the pre-
fixes un, dis, ir, im, and in; the infix less (we inter-
nally treat this as the suffixes lessly and lessness);
and the suffix less. Of the total number of 1157 cues
in the training and development set, 192 are affixal.
There are, however, a total of 1127 tokens matching
one of the affix patterns above, and while we main-
tain the closed class assumption also for the affixes,
the classifier will need to consider its status as a cue
320
or non-cue when attaching to any such token, like
for instance image, recklessness, and bless.
2.1 Features
In the initial formulation of Velldal (2011), an SVM
classifier was trained using simple n-gram features
over words, both full forms and lemmas, to the left
and right of the candidate cues. In addition to these
token-level features, the classifier we apply here in-
cludes some features specifically targeting morpho-
logical or affixal cues. The first such feature records
character n-grams from both the beginning and end
of the base that an affix attaches to (up to five po-
sitions). For a context like impossible we would
record n-grams such {possi, poss, pos, . . .} and
{sible, ible, ble, . . .}, and combine this with infor-
mation about the affix itself (im) and the token part-
of-speech (?JJ?).
For the second feature type targeting affix cues
we try to emulate the effect of a lexicon look-up
of the remaining substring that an affix attaches to,
checking its status as an independent base form and
its part-of-speech. In order to take advantage of
such information while staying within the confines
of the closed track, we automatically generate a lex-
icon from the training data, counting the instances
of each PoS tagged lemma in addition to n-grams
of word-initial characters (again recording up to five
positions). For a given match of an affix pattern, a
feature will then record the counts from this lexicon
for the substring it attaches to. The rationale for this
feature is that the occurrence of a substring such as
un in a token such as underlying should be consid-
ered more unlikely to be a cue given that the first
part of the remaining string (e.g., derly) would be an
unlikely way to begin a word.
Note that, it is also possible for a negation cue
to span multiple tokens, such as the (discontinuous)
pair neither / nor or fixed expressions like on the
contrary. There are, however, only 16 instances of
such multiword cues (MWCs) in the entire CDTD.
Rather than letting the classifier be sensitive to these
corner cases, we cover such MWC patterns using
a small set of simple post-processing heuristics. A
small stop-list is used for filtering out the relevant
words from the examples presented to the classifier
(on, the, etc.).
Data set Model Prec Prec F1
CDD
Baseline 90.68 84.39 87.42
Classifier 93.75 95.38 94.56
CDE
Baseline 87.10 92.05 89.51
Classifier 89.17 93.56 91.31
Table 1: Cue classification results for the final classifier
and the majority-usage baseline, showing test scores for
the development set (training on CDT) and the final held-
out set (training on CDTD).
2.2 Results
Table 1 presents results for the cue classifier. While
the classifier configuration was optimized against
CDD, the model used for the final held-out testing
is trained on the entire CDTD, which (given our
closed-class treatment of cues) provides a total of
1162 positive and 1100 negative training examples.
As an informed baseline, we also tried classifying
each word based on its most frequent use as cue
or non-cue in the training data. (Affixal cue oc-
currences are counted by looking at both the affix-
pattern and the base it attaches to, basically treating
the entire token as a cue. Tokens that end up be-
ing classified as cues are then matched against the
affix patterns observed during training in order to
correctly delimit the annotation of the cue.) This
simple majority-usage approach actually provides a
fairly strong baseline, yielding an F1 of 87.42 on
CDD (P=90.68, R=84.39). Compare this to the F1 of
94.56 obtained by the classifier on the same data set
(P=93.75, R=95.38). However, when applying the
models to the held-out set, with models estimated
over the entire CDTD, the baseline seems to able
to make good use of the additional data and proves
to be even more competitive: While our final cue
classifier achieves F1=91.31, the baseline achieves
F1=89.51, almost two percentage points higher than
its score on the development data, and even outper-
forms four of the ten cue detection systems submit-
ted for the shared task (three of the 12 shared task
submissions use the same classifier).
When inspecting the predictions of our final cue
classifier on CDD, comprising a total of 173 gold
annotated cues, we find that our system mislabels
11 false positives (FPs) and 7 false negatives (FNs).
321
Of the FPs, we find five so-called false negation cues
(Morante et al, 2011), including three instances of
none in the fixed expression none the less. The
others are affixal cues, of which two are clearly
wrong (underworked, universal) while others might
arguably be due to annotation errors (insuperable,
unhappily, endless, listlessly). Among the FNs, two
are due to MWCs not covered by our heuristics (e.g.,
no more), while the remaining errors concern af-
fixes, including one in an interesting context of dou-
ble negation; not dissatisfied.
3 Scope and event resolution
In this work, we model negation scope resolution
as a special instance of the classical IOB (Inside,
Outside, Begin) sequence labeling problem, where
negation cues are labeled to be sequence starters and
scopes and events as two different kinds of chunks.
CRFs allow the computation of p(X|Y), whereX is
a sequence of labels andY is a sequence of observa-
tions, and have already been shown to be efficient in
similar, albeit less involved, tasks of negation scope
resolution (Morante and Daelemans, 2009; Councill
et al, 2010). We employ the CRF implementation in
the Wapiti toolkit, using default settings (Lavergne
et al, 2010). A number of features were used to
create the models. In addition to the information
provided for each token in the CD corpus (lemma,
part of speech and constituent), we extracted both
left and right token distance to the closest negation
cue. Features were expanded to include forward and
backward bigrams and trigrams on both token and
PoS level, as well as lexicalized PoS unigrams and
bigrams2. Table 2 presents a complete list of fea-
tures. The more intricate, dependency-based fea-
tures are presented in Section 3.1, while the labeling
of both scopes and events is detailed in Section 3.2.
3.1 Dependency-based features
For the system submitted to the closed track, the syn-
tactic representations were converted to dependency
representations using the Stanford dependency con-
verter, which comes with the Stanford parser (de
Marneffe et al, 2006).3 These dependency represen-
2By lexicalized PoS we mean an instance of a PoS-Tag in
conjunction with the sentence token.
3Note that the converter was applied directly to the phrase-
structure trees supplied with the negation data sets, and the
General features
Token
Lemma
PoS unigram
Forward token bigram and trigram
Backward token bigram and trigram
Forward PoS trigram
Backward PoS trigram
Lexicalized PoS
Forward Lexicalized PoS bigram
Backward Lexicalized PoS bigram
Constituent
Dependency relation
First order head PoS
Second order head PoS
Lexicalized dependency relation
PoS-disambiguated dependency relation
Cue-dependent features
Token distance
Directed dependency distance
Bidirectional dependency distance
Dependency path
Lexicalized dependency path
Table 2: List of features used to train the CRF models.
tations result from a conversion of Penn Treebank-
style phrase structure trees, combining ?classic? head
finding rules with rules that target specific linguistic
constructions, such as passives or attributive adjec-
tives. The so-called basic format provides a depen-
dency graph which is a directed tree, see Figure 1
for an example.
For the open track submission we used Maltparser
(Nivre et al, 2006) with its pre-trained parse model
for English.4 The parse model has been trained on a
conversion of sections 2-21 of the Wall Street Jour-
nal section of the Penn Treebank to Stanford depen-
dencies, augmented with data from Question Bank.
The parser was applied to the negation data, using
the word tokens and supplied parts-of-speech as in-
put to the parser.
The features extracted via the dependency graphs
aim at modeling the syntactic relationship between
each token and the closest negation cue. Token dis-
tance was therefore complemented with two variants
of dependency distance from each token to the lexi-
Stanford parser was not used to parse the data.
4The pre-trained model is available from maltparser.org
322
we   have  never  gone  out  without  keeping  a  sharp  watch  ,  and  no  one  could  have  escaped  our  notice  .  "
nsubj
aux
neg
conj
cc
punct
prep
part
pcomp
dobj
det
amod
dep
nsubj
aux
aux
punct
punct
dobj
poss
root
ann. 1:
ann. 2:
ann. 3:
cue
cue
cue
labels: CUE CUE CUEN N E E
N N
N N E N N N NS O S O
N
Figure 1: A sentence from the CD corpus showing a dependency graph and the annotation-to-label conversion.
cally closest cue, Directed Distance (DD) and Bidi-
rectional Distance (BD). DD is extracted by follow-
ing the reversed, directed edges from token X to the
cue. If there is no such path, the value of the feature
is -1. BD uses the Dijkstra shortest path algorithm
on an undirected representation of the graph. The
latter feature proved to be more effective than the
former when not used together; using them in con-
junction seemed to confuse the model, thus the fi-
nal model utilizes only BD. We furthermore use the
Dependency Graph Path (DGP) as a feature. This
feature was inspired by the Parse Tree Path feature
presented in Gildea and Jurafsky (2002) in the con-
text of Semantic Role Labeling. It represents the
path traversed from each token to the cue, encod-
ing both the dependency relations and the direction
of the arc that is traversed: for instance, the rela-
tion between our and no in Figure 1 is described as
 poss  dobj  nsubj  det. Like Councill et
al. (2010), we also encode the PoS of the first and
second order syntactic head of each token. For the
token no in Figure 1, for instance, we record the PoS
of one and escaped, respectively.
3.2 Model-internal representation
The token-wise annotations in the CD corpus con-
tain multiple layers of information. Tokens may or
may not be negation cues and they can be either in
or out of scope; in-scope tokens may or may not
be negated events, and are associated with each of
the cues they are negated by. Moreover, scopes may
be (partially) overlapping, as in Figure 1, where the
PoS # S PoS # MCUE PoS # CUE
punctuation 1492 JJ 268 RB 1026
CC 52 RB 28 DT 296
IN + TO 46 NN 16 NN 146
RB 38 NN 4 UH 118
PRP 32 IN 2 IN 64
rest 118 rest ? rest 38
Table 3: Frequency distribution of parts of speech over
the S, MCUE and CUE labels in CDTD.
scope of without is contained within the scope of
never. We convert this representation internally by
assigning one of six labels to each token: O, CUE,
MCUE, N, E and S, for out-of-scope, cue, mor-
phological (affixal) cue, in-scope, event and nega-
tion stop respectively. The CUE, O, N and E la-
bels parallel the IOB chunking paradigm and are
eventually translated in the final annotations by our
post-processing component. MCUE and S extend
the label set to account for the specific behavior of
the tokens they are associated with. The rationale
behind the separation of cues in two classes is the
pronounced differences between the PoS frequency
distributions of standard versus morphological cues.
Table 3 presents the frequency distribution of PoS-
tags over the different cue types in CDTD and shows
that, unsurprisingly, the majority class for morpho-
logical cues is adjectives, which typically generate
different scope patterns compared to the majority
class for standard cues. The S label, a special in-
stance of an out-of-scope token, is defined as the
323
first non-cue, out-of-scope token to the right of one
labeled with N, and targets mostly punctuation.
After some experimentation with joint labeling of
scopes and events, we opted for separation of the
two models, hence training separate models for the
two tasks of scope resolution and event detection.
In the model for scopes, all E labels are switched
to N; conversely, Ns become Os in the event model.
Given the nature of the annotations, the predictions
provided by the model for events serve a double pur-
pose: finding the negated token in a sentence and
deciding whether a sentence is factual or not. The
outputs of the two classifiers are merged during post-
processing.
3.3 Post-processing
A simple, heuristics-based algorithm was applied
to the output of the labelers in order to pair each
in-scope token to its negation cue(s) and determine
overlaps. Our algorithm works by first determining
the overlaps among negation cues. Cue A negates
cue B if the following conditions are met:
? B is to the right of A.
? There are no tokens labeled with S between A
and B.
? Token distance between A and B does not ex-
ceed 10.
In the example in Figure 1, the overlapping condi-
tion holds for never and without but not for without
and no, because of the punctuation between them.
The token distance threshold of 10 was determined
empirically on CDT. In order to assign in-scope to-
kens to their respective cue, tokens labeled with N
are treated as follows:
? Assign each token T to the closest negation cue
A with no S-labeled tokens or punctuation sep-
arating it from T.
? If A was found to be negated by cue B, assign
T to B as well.
? If T is labeled with E by the event classifier,
mark it as an event.
F1
Configuration Closed Open
(A) O, N, CUE, MCUE, E, S 64.85 66.41Dependency Features
(B) O, N, CUE, MCUE, E, S 59.35 59.35No Dependency Features
(C) O, N, CUE, E 62.69 63.24Dependency Features
(D) O, N, CUE, E 56.44 56.44No Dependency Features
Table 4: Full negation results on CDD with gold cues.
This algorithm yields the correct annotations for
the example in Figure 1; when applied to label se-
quences originating from the gold scopes in CDD,
the reported F1 is 95%. We note that this loss of in-
formation could have been avoided by presenting the
CRF with a version of a sentence for each negation
cue. Then, when labeling new sentences, the model
could be applied repeatedly (based on the number of
cues provided by the cue detection system). How-
ever, training with multiple instances of the same
sentence could result in a dilution of the evidence
needed for scope labeling; this remains to be inves-
tigated in future work.
4 Development results
To investigate the effects of the augmented set of la-
bels and that of dependency features comparatively,
we present four different configurations of our sys-
tem in Table 4, using F1 for the stricter score that
counts perfect-match negation resolution for each
negation cue. Comparing (B) and (D), we observe
that explicitly encoding significant tokens with extra
labels does improve the performance of the classi-
fier. Comparing (A) to (B) and (C) to (B) shows the
effect of the dependency features with and without
the augmented set of labels. With (A) being our top
performing system and (D) a kind of internal base-
line, we observe that the combined effects of the la-
bels and dependency features is beneficial, with a
margin of about 8 and 10 percentage points for our
closed and open track systems respectively.
Table 5 presents the results for scope resolution on
CDD with gold cues. Interestingly, the constituent
324
Closed Open
Prec Rec F1 Prec Rec F1
Scopes 100.00 70.24 82.52 100.00 66.67 80.00
Scope Tokens 94.69 82.16 87.98 90.64 81.36 85.75
Negated 82.47 72.07 76.92 83.65 77.68 80.55
Full negation 100.00 47.98 64.85 100.00 49.71 66.41
Table 5: Results for scope resolution on CDD with gold cues.
trees converted to Stanford dependencies used in the
closed track outperform the open system employing
Maltparser on scopes, while for negated events the
latter is over 5 percentage points better than the for-
mer, as shown in Table 5.
4.1 Error analysis
We performed a manual error analysis of the scope
resolution on the development data using gold cue
information. Since our system does not deal specifi-
cally with discontinuous scopes, and seeing that we
are employing a sequence classifier with a fairly lo-
cal window, we are not surprised to find that a sub-
stantial portion of the errors are caused by discon-
tinuous scopes. In fact, in our closed track system,
these errors amount to 34% of the total number of
errors. Discontinuous scopes, as in (3) below, ac-
count for 9.3% of all scopes in CDD and the closed
task system does not analyze any of these correctly,
whereas the open system correctly analyzes one dis-
continuous scope.
(3) I therefore spent the day at my club and did not
return to Baker Street until evening.
A similar analysis with respect to event detection
on gold scope information indicated that errors are
mostly due to either predicting an event for a non-
factual context (false positive) or not predicting an
event for a factual context (false negative), i.e., there
are relatively few instances of predicting the wrong
token for a factual context (which result in both a
false negative and a false positive). This suggests
that the CRF has learned what tokens should be la-
beled as an event for a negation, but has not learned
so well how to determine whether the negation is
factual or non-factual. In this respect it may be that
incorporating information from a separate and dedi-
cated component for factuality detection ? as in the
system of Read et al (2012) ? could yield improve-
ments for the CRF event model.
5 Held-out evaluation
Final results on held-out data for both closed and
open track submissions are reported in Table 6. For
the final run, we trained our systems on CDTD. We
observe a similar relative performance to our devel-
opment results, with the open track system outper-
forming the closed track one, albeit by a smaller
margin than what we saw in development. We are
also surprised to see that despite not addressing dis-
continuous scopes directly, our system obtained the
best score on scope resolution (according to the met-
ric dubbed ?Scopes (cue match)?).
6 Conclusions and future work
This paper has provided an overview of our system
submissions for the *SEM 2012 shared task on re-
solving negation. This involves the subtasks of iden-
tifying negations cues, identifying the in-sentence
scope of these cues, as well as identifying negated
(and factual) events. While a simple SVM token
classifier is applied for the cue detection task, we ap-
ply CRF sequence classifiers for predicting scopes
and events. For the CRF models we experimented
with a fine-grained set of labels and a wide range of
feature types, drawing heavily on information from
dependency structures. We have detailed two dif-
ferent system configurations ? one submitted for
the open track and another for the closed track ?
and the two configurations only differ with respect
to the source used for the dependency parses: For
the closed track submission we simply converted
the constituent structures provided in the shared task
data to Stanford dependencies, while for the open
track we apply the Maltparser. For the end-to-end
evaluation, our submission was ranked first in the
open track and third in the closed track. The system
also had the best performance for each individual
sub-task in the open track, as well as being among
325
Closed Open
Prec Rec F1 Prec Rec F1
Cues 89.17 93.56 91.31 89.17 93.56 91.31
Scopes 85.71 62.65 72.39 85.71 62.65 72.39
Scope Tokens 86.03 81.55 83.73 82.25 82.16 82.20
Negated 68.18 52.63 59.40 66.90 57.40 61.79
Full negation 78.26 40.91 53.73 78.72 42.05 54.82
Cues B 86.97 93.56 90.14 86.97 93.56 90.14
Scopes B 59.32 62.65 60.94 59.54 62.65 61.06
Negated B 67.16 52.63 59.01 63.82 57.40 60.44
Full negation B 38.03 40.91 39.42 39.08 42.05 40.51
Table 6: End-to-end results on the held-out data.
the top-performers on the scope resolution sub-task
across both tracks.
Due to time constraints we were not able to di-
rectly address discontinuous scopes in our system.
For future work we plan on looking for ways to
tackle this problem by taking advantage of syntac-
tic information, both in the classification and in the
post-processing steps. We are also interested in de-
veloping the CRF-internal label-set to include more
informative labels. We also want to test the sys-
tem design developed for this task on other corpora
annotated for negation (or other related phenom-
ena such as speculation), as well as perform extrin-
sic evaluation of our system as a sub-component to
other NLP tasks such as sentiment analysis or opin-
ion mining. Lastly, we would like to try training
separate classifiers for affixal and token-level cues,
given that largely separate sets of features are effec-
tive for the two cases.
Acknowledgements
We thank colleagues at the University of Oslo, and
in particular Johan Benum Evensberget and Arne
Skj?rholt for fruitful discussions and suggestions.
We also thank the anonymous reviewers for their
helpful feedback.
References
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: Learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop On
Negation and Speculation in Natural Language Pro-
cessing, pages 51?59.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):245?288.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In CoNLL ?09: Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning, pages 21?29. Association for Com-
putational Linguistics.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 2216?2219.
Jonathon Read, Erik Velldal, Lilja ?vrelid, and Stephan
Oepen. 2012. UiO1: Constituent-based discrimina-
tive ranking for negation resolution. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal. Submission under review.
326
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Erik Velldal. 2011. Predicting speculation: A simple dis-
ambiguation approach to hedge detection in biomedi-
cal literature. Journal of Biomedical Semantics, 2(5).
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: Biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 (Suppl. 11).
327
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 88?97,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards an ACL Anthology Corpus with Logical Document Structure
An Overview of the ACL 2012 Contributed Task
Ulrich Sch?fer
DFKI Language Technology Lab
Campus D 3 1
D-66123 Saarbr?cken, Germany
ulrich.schaefer@dfki.de
Jonathon Read, Stephan Oepen
Department of Informatics
Universitetet i Oslo
0316 Oslo, Norway
{jread |oe}@ifi.uio.no
Abstract
The ACL 2012 Contributed Task is a com-
munity effort aiming to provide the full ACL
Anthology as a high-quality corpus with rich
markup, following the TEI P5 guidelines?
a new resource dubbed the ACL Anthology
Corpus (AAC). The goal of the task is three-
fold: (a) to provide a shared resource for ex-
perimentation on scientific text; (b) to serve
as a basis for advanced search over the ACL
Anthology, based on textual content and cita-
tions; and, by combining the aforementioned
goals, (c) to present a showcase of the benefits
of natural language processing to a broader au-
dience. The Contributed Task extends the cur-
rent Anthology Reference Corpus (ARC) both
in size, quality, and by aiming to provide tools
that allow the corpus to be automatically ex-
tended with new content?be they scanned or
born-digital.
1 Introduction?Motivation
The collection of the Association for Computational
Linguistics (ACL) Anthology began in 2002, with
3,100 scanned and born-digital1 PDF papers. Since
then, the ACL Anthology has become the open ac-
cess collection2 of scientific papers in the area of
Computational Linguistics and Language Technol-
ogy. It contains conference and workshop proceed-
ings and the journal Computational Linguistics (for-
merly the American Journal of Computational Lin-
guistics). As of Spring 2012, the ACL Anthol-
1The term born-digital means natively digital, i.e. prepared
electronically using typesetting systems like LATEX, OpenOffice,
and the like?as opposed to digitized (or scanned) documents.
2
http://aclweb.org/anthology
ogy comprises approximately 23,000 papers from 46
years.
Bird et al (2008) started collecting not only the
PDF documents, but also providing the textual con-
tent of the Anthology as a corpus, the ACL Anthol-
ogy Reference Corpus3 (ACL-ARC). This text ver-
sion was generated fully automatically and in differ-
ent formats (see Section 2.2 below), using off-the-
shelf tools and yielding somewhat variable quality.
The main goal was to provide a reference cor-
pus with fixed releases that researchers could use
and refer to for comparison. In addition, the vision
was formulated that manually corrected ground-
truth subsets could be compiled. This is accom-
plished so far for citation links from paper to paper
inside the Anthology for a controlled subset. The
focus thus was laid on bibliographic and bibliomet-
ric research and resulted in the ACL Anthology Net-
work (Radev et al, 2009) as a public, manually cor-
rected citation database.
What is currently missing is an easy-to-process
XML variant that contains high-quality running text
and logical markup from the layout, such as section
headings, captions, footnotes, italics etc. In prin-
ciple this could be derived from LATEX source files,
but unfortunately, these are not available, and fur-
thermore a considerable amount of papers have been
typeset with various other word processing software.
Here is where the ACL 2012 Contributed Task
starts: The idea is to combine OCR and PDFBox-
like born-digital text extraction methods and re-
assign font and logical structure information as part
of a rich XML format. The method would rely on
OCR exclusively only in cases where no born-digital
3
http://acl-arc.comp.nus.edu.sg
88
PDFs are available?in case of the ACL Anthology
mostly papers published before the year 2000. Cur-
rent results and status updates will always be acces-
sible through the following address:




	
http://www.delph-in.net/aac/
We note that manually annotating the ACL An-
thology is not viable. In a feasibility study we took
a set of five eight-page papers. After extracting
the text using PDFBox4 we manually corrected the
output and annotated it with basic document struc-
ture and cross-references; this took 16 person-hours,
which would suggest a rough estimate of some 25
person-years to manually correct and annotate the
current ACL Anthology. Furthermore, the ACL An-
thology grows substantially every year, requiring a
sustained effort.
2 State of Affairs to Date
In the following, we briefly review the current status
of the ACL Anthology and some of its derivatives.
2.1 ACL Anthology
Papers in the current Anthology are in PDF format,
either as scanned bitmaps or digitally typeset with
LATEX or word processing software. Older scanned
papers were often created using type writers, and
sometimes even contained hand-drawn graphics.
2.2 Anthology Reference Corpus (ACL-ARC)
In addition to the PDF documents, the ACL-ARC
also contains (per page and per paper)
? bitmap files (in the PNG file format)
? plain text in ?normal? reading order
? formatted text (in two columns for most of the
papers)
? XML raw layout format containing position in-
formation for each word, grouped in lines, with
font information, but no running text variant.
The latter three have been generated using OCR
software (OmniPage) operating on the bitmap files.
4
http://pdfbox.apache.org
However, OCR methods tend to introduce charac-
ter and layout recognition errors, from both scanned
and born-digital documents.
The born-digital subset of the ACL-ARC (mostly
papers that appeared in 2000 or later) also contains
PDFBox plain text output. However, this is not
available for approximately 4% of the born-digital
PDFs due to unusual font encodings. Note though,
that extracting text from PDFs in normal reading
order is not a trivial task (Berg et al, 2012), and
many errors exist. Furthermore, the plain text is
not dehyphenated, necessitating a language model
or lexicon-based lookup for post-processing.
2.3 ACL Anthology Network
The ACL Anthology Network (Radev et al, 2009)
is based on the ACL-ARC text outputs. It addition-
ally contains manually-corrected citation graphs, au-
thor and affiliation data for most of the Anthology
(papers until 2009).
2.4 Publications with the ACL Anthology as a
Corpus
We did a little survey in the ACL Anthology of pa-
pers reporting on having used the ACL Anthology as
corpus/dataset. The aim here is to get an overview
and distribution of the different NLP research tasks
that have been pursued using the ACL Anthology as
dataset. There are probably other papers outside the
Anthology itself, but these have not been looked at.
The pioneers working with the Anthology as cor-
pus are Ritchie et al (2006a, 2006b). They did work
related to citations which also forms the largest topic
cluster of papers applying or using Anthology data.
Later papers on citation analysis, summarization,
classification, etc. are Qazvinian et al (2010), Abu-
Jbara & Radev (2011), Qazvinian & Radev (2010),
Qazvinian & Radev (2008), Mohammad et al
(2009), Athar (2011), Sch?fer & Kasterka (2010),
and Dong & Sch?fer (2011).
Text summarization research is performed in
Qazvinian & Radev (2011) and Agarwal et al
(2011a, 2011b).
The HOO (?Help our own?) text correction shared
task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro-
zovskaya et al, 2011; Dahlmeier et al, 2011) aims
at developing automated tools and techniques that
89
assist authors, e.g. non-native speakers of English,
in writing (better) scientific publications.
Classification/Clustering related publications are
Muthukrishnan et al (2011) and Mao et al (2010).
Keyword extraction and topic models based on
Anthology data are addressed in Johri et al (2011),
Johri et al (2010), Gupta & Manning (2011), Hall
et al (2008), Tu et al (2010) and Daudaravic?ius
(2012). Reiplinger et al (2012) use the ACL An-
thology to acquire and refine extraction patterns for
the identification of glossary sentences.
In this workshop several authors have used the
ACL Anthology to analyze the history of compu-
tational linguistics. Radev & Abu-Jbara (2012) ex-
amine research trends through the citing sentences
in the ACL Anthology Network. Anderson et al
(2012) use the ACL Anthology to perform a people-
centered analysis of the history of computational
linguistics, tracking authors over topical subfields,
identifying epochs and analyzing the evolution of
subfields. Sim et al (2012) use a citation analysis to
identify the changing factions within the field. Vo-
gel & Jurafsky (2012) use topic models to explore
the research topics of men and women in the ACL
Anthology Network. Gupta & Rosso (2012) look
for evidence of text reuse in the ACL Anthology.
Most of these and related works would benefit
from section (heading) information, and partly the
approaches already used ad hoc solutions to gather
this information from the existing plain text ver-
sions. Rich text markup (e.g. italics, tables) could
also be used for linguistic, multilingual example ex-
traction in the spirit of the ODIN project (Xia &
Lewis, 2008; Xia et al, 2009).
3 Target Text Encoding
To select encoding elements we adopt the TEI P5
Guidelines (TEI Consortium, 2012). The TEI en-
coding scheme was developed with the intention of
being applicable to all types of natural language, and
facilitating the exchange of textual data among re-
searchers across discipline. The guidelines are im-
plemented in XML; we currently use inline markup,
but stand-off annotations have also been applied
(Ban?ski & Przepi?rkowski, 2009).
We use a subset of the TEI P5 Guidelines as
not all elements were deemed necessary. This pro-
cess was made easier through Roma5, an online
tool that assists in the development of TEI valida-
tors. We note that, while we initially use a simpli-
fied version, the schemas are readily extensible. For
instance, Przepi?rkowski (2009) demonstrates how
constituent and dependency information can be en-
coded following the guidelines, in a manner which
is similar to other prominent standards.
A TEI corpus is typically encoded as a sin-
gle XML document, with several text elements,
which in turn contain front (for abstracts), body
and back elements (for acknowledgements and bib-
liographies). Then, sections are encoded using div
elements (with xml:ids), which contain a heading
(head) and are divided into paragraphs (p). We
aim for accountability when translating between for-
mats; for example, the del element records deletions
(such as dehyphenation at line breaks).
An example of a TEI version of an ACL Anthol-
ogy paper is depicted in Figure 1 on the next page.
4 An Overview of the Contributed Task
The goal of the ACL 2012 Contributed Task is to
provide a high-quality version of the textual content
of the ACL Anthology as a corpus. Its rich text
XML markup will contain information on logical
document structure such as section headings, foot-
notes, table and figure captions, bibliographic ref-
erences, italics/emphasized text portions, non-latin
scripts, etc.
The initial source are the PDF documents of the
Anthology, processed with different text extraction
methods and tools that output XML/HTML. The in-
put to the task itself then consists of two XML for-
mats:
? PaperXML from the ACL Anthology Search-
bench6 (Sch?fer et al, 2011) provided
by DFKI Saarbr?cken, of all approximately
22,500 papers currently in the Anthology (ex-
cept ROCLING which are mostly in Chi-
nese). These were obtained by running a com-
mercial OCR program and applying logical
markup postprocessing and conversion to XML
(Sch?fer & Weitz, 2012).
5
http://www.tei-c.org/Roma/
6
http://aclasb.dfki.de
90
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title>
<author>
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii??*
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
</author>
</titleStmt>
<publicationStmt>
<publisher>Association for Computational Linguistics</publisher>
<pubPlace> Columbus, Ohio, USA</pubPlace>
<date>June 2008</date>
</publicationStmt>
<sourceDesc> [. . . ] </sourceDesc>
</fileDesc>
<encodingDesc> [. . . ] </encodingDesc>
</teiHeader>
<text>
<front>
<div type="abs">
<head>Abstract</head>
<p> [. . . ] </p>
</div>
</front>
<body>
<div xml:id="SE1">
<head>Introduction</head>
<p>
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame<del type="lb">-</del>
works (<ref target="#BI6">Charniak, 2000</ref>;
[. . . ]
</p>
</div>
</body>
<back>
<div type="ack">
<head>Acknowledgements</head>
<p> [. . . ] </p>
</div>
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI1">
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
<hi rend="italic">Computational Linguistics</hi>, 30(4):479?511.
</bibl>
[. . . ]
</listBibl>
<pb n="54"/>
</div>
</back>
</text>
</TEI>
Figure 1: An example of a TEI-compliant version of an ACL Anthology document P08-1006. Some elements are
truncated ([. . . ]) for brevity.
91
? TEI P5 XML generated by PDFExtract. For pa-
pers from after 1999, an additional high-quality
extraction step took place, applying state-of
the art word boundary and layout recognition
methods directly to the native, logical PDF
structure (Berg et al, 2012). As no charac-
ter recognition errors occur, this will form the
master format for textual content if available.
Because both versions are not perfect, a large, ini-
tial part of the Contributed Task requires automat-
ically adding missing or correcting markup, using
information from OCR where necessary (e.g. for ta-
bles). Hence, for most papers from after 1999 (cur-
rently approx. 70% of the papers), the Contributed
Task can make use of both representations simulta-
neously.
The role of paperXML in the Contributed Task is
to serve as fall-back source (1) for older, scanned
papers (mostly published before the year 2000), for
which born-digital PDF sources are not available,
or (2) for born-digital PDF papers on which the
PDFExtract method failed, or (3) for document parts
where PDFExtract does not output useful markup
such as currently for tables, cf. Section 4.2 below.
A big advantage of PDFExtract is its ability to ex-
tract the full Unicode character range without char-
acter recognition errors, while the OCR-based ex-
traction methods in our setup are basically limited
to Latin1 characters to avoid higher recognition er-
ror rates.
We proposed the following eight areas as possible
subtasks towards our goal.
4.1 Subtask 1: Footnotes
The first task addresses identification of footnotes,
assigning footnote numbers and text, and generating
markup for them in TEI P5 style. For example:
We first determine lexical heads of nonterminal
nodes by using Bikel's implementation of
Collins' head detection algorithm
<note place="foot" n="9">
<hi rend="monospace">http://www.cis.upenn.edu/
~dbikel/software.html</hi>
</note>
(<ref target="#BI1">Bikel, 2004</ref>;
<ref target="#BI11">Collins, 1997</ref>).
Footnotes are handled to some extent in PDFEx-
tract and paperXML, but the results require refine-
ment.
4.2 Subtask 2: Tables
Task 2 identifies figure/table references in running
text and links them to their captions. The latter
will also have to be distinguished from running text.
Furthermore, tables will have to be identified and
transformed into HTML style table markup. This
is currently not generated by PDFExtract, but the
OCR tool used for paperXML generation quite re-
liably recognizes tables and transforms tables into
HTML. Thus, a preliminary solution would be to in-
sert missing table content in PDFExtract output from
the OCR results. In the long run, implementing table
handling in PDFExtract would be desirable.
<ref target="#TA3">Table 3</ref> shows the
time for parsing the entire AImed corpus,...
<figure xml:id="TA3">
<head>Table 3: Parsing time (sec.)</head>
<!-- TEI table content markup here -->
</figure>
4.3 Subtask 3: Bibliographic Markup
The purpose of this task is to identify citations in
text and link them to the bibliographic references
listed at the end of each paper. In TEI markup, bibli-
ographies are contained in listBibl elements. The
contents of listBibl can range from formatted text
to moderately-structured entries (biblStruct) and
fully-structured entries (biblFull). For example:
We follow the PPI extraction method of
<ref target="#BI39">S?tre et al (2007)</ref>,
which is based on SVMs ...
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI39">
R. S?tre, K. Sagae, and J. Tsujii. 2007.
Syntactic features for protein-protein
interaction extraction. In
<hi rend="italic">LBM 2007 short papers</hi>.
</bibl>
</listBibl>
</div>
A citation extraction and linking tool that is
known to deliver good results on ACL Anthology
papers (and even comes with CRF models trained
on this corpus) is ParsCit (Councill et al, 2008). In
this volume, Nhat & Bysani (2012) provide an im-
plementation for this task using ParsCit and discuss
possible further improvements.
92
4.4 Subtask 4: De-hyphenation
Both paperXML and PDFExtract output contain soft
hyphenation indicators at places where the original
paper contained a line break with hyphenation. In
paperXML, they are represented by the Unicode soft
hyphen character (in contrast to normal dashes that
also occur). PDFExtract marks hyphenation from
the original text using a special element. How-
ever, both tools make errors: In some cases, the hy-
phens are in fact hard hyphens. The idea of this
task is to combine both sources and possibly ad-
ditional information, as in general the OCR pro-
gram used for paperXML more aggressively pro-
poses de-hyphenation than PDFExtract. Hyphen-
ation in names often persists in paperXML and
therefore remains a problem that will have to be ad-
dressed as well. For example:
In this paper, we present a comparative
eval<del type="lb">-</del>uation of syntactic
parsers and their output
represen<del type="lb">-</del>tations based on
different frameworks:
4.5 Subtask 5: Remove Garbage such as
Leftovers from Figures
In both paperXML and PDFExtract output, text
remains from figures, illustrations and diagrams.
This occurs more frequently in paperXML than in
PDFExtract output because text in bitmap figures
undergoes OCR as well. The goal of this subtask
is to recognize and remove such text.
Bitmaps in born-digital PDFs are embedded ob-
jects for PDFExtract and thus can be detected and
encoded within TEI P5 markup and ignored in the
text extraction process:
<figure xml:id="FI3">
<graphic url="P08-1006/FI3.png" />
<head>
Figure 3: Predicate argument structure
</head>
</figure>
4.6 Subtask 6: Generate TEI P5 Markup for
Scanned Papers from paperXML
Due to the nature of the extraction process, PDFEx-
tract output is not available for older, scanned pa-
pers. These are mostly papers from before 2000, but
also e.g. EACL 2003 papers. On the other hand, pa-
perXML versions exist for almost all papers of the
ACL Anthology, generated from OCR output. They
still need to be transformed to TEI P5, e.g. using
XSLT. The paperXML format and transformation to
TEI P5 is discussed in Sch?fer & Weitz (2012) in
this volume.
4.7 Subtask 7: Add Sentence Splitting Markup
Having a standard for sentence splitting with unique
sentence IDs per paper to which everyone can refer
to later could be important. The aim of this task is to
add sentence segmentation to the target markup. It
should be based on an open source tokenizer such as
JTok, a customizable open source tool7 that was also
used for the ACL Anthology Searchbench semantic
index pre-processing, or the Stanford Tokenizer8.
<p><s>PPI extraction is an NLP task to identify
protein pairs that are mentioned as interacting
in biomedical papers.</s> <s>Because the number
of biomedical papers is growing rapidly, it is
impossible for biomedical researchers to read
all papers relevant to their research; thus,
there is an emerging need for reliable IE
technologies, such as PPI identification.
</s></p>
4.8 Subtask 8: Math Formulae
Many papers in the Computational Linguistics area,
especially those dealing with statistical natural lan-
guage processing, contain mathematical formulae.
Neither paperXML nor PDFExtract currently pro-
vide a means to deal with these.
A math formula recognition is a complex task, in-
serting MathML9 formula markup from an external
tool (formula OCR, e.g. from InftyReader10) could
be a viable solution.
For example, the following could become the tar-
get format of MathML embedded in TEI P5, for
?? > 0 3 f (x) < 1:
<mrow>
<mo> there exists </mo>
<mrow>
<mrow>
<mi> &#916; <!--GREEK SMALL DELTA--></mi>
<mo> &gt; </mo>
<mn> 0 </mn>
7
http://heartofgold.opendfki.de/repos/trunk/
jtok; LPGL license
8
http://nlp.stanford.edu/software/tokenizer.
shtml; GPL V2 license
9
http://www.w3.org/TR/MathML/
10
http://sciaccess.net/en/InftyReader/
93
</mrow>
<mo> such that </mo>
<mrow>
<mrow>
<mi> f </mi>
<mo> &#2061; <!--FUNCTION APPL.--></mo>
<mrow>
<mo> ( </mo>
<mi> x </mi>
<mo> ) </mo>
</mrow>
</mrow>
<mo> &lt; </mo>
<mn> 1 </mn>
</mrow>
</mrow>
</mrow>
An alternative way would be to implement math
formula recognition directly in PDFExtract using
methods known from math OCR, similar to the page
layout recognition approach.
5 Discussion?Outlook
Through the ACL 2012 Contributed Task, we have
taken a (small, some might say) step further towards
the goal of a high-quality, rich-text version of the
ACL Anthology as a corpus?making available both
the original text and logical document structure.
Although many of the subtasks sketched above
did not find volunteers in this round, the Contributed
Task, in our view, is an on-going, long-term com-
munity endeavor. Results to date, if nothing else,
confirm the general suitability of (a) using TEI P5
markup as a shared target representation and (b) ex-
ploiting the complementarity of OCR-based tech-
niques (Sch?fer & Weitz, 2012), on the one hand,
and direct interpretation of born-digital PDF files
(Berg et al, 2012), on the other hand. Combin-
ing these approaches has the potential to solve the
venerable challenges that stem from inhomogeneous
sources in the ACL Anthology?e.g. scanned, older
papers and digital newer papers, generated from a
broad variety of typesetting tools.
However, as of mid-2012 there still is no ready-to-
use, high-quality corpus that could serve as a shared
starting point for the range of Anthology-based NLP
activities sketched in Section 1 above. In fact, we
remain slightly ambivalent about our recommenda-
tions for utilizing the current state of affairs and ex-
pected next steps?as we would like to avoid much
work getting underway with a version of the corpus
that we know is unsatisfactory. Further, obviously,
versioning and well-defined release cycles will be a
prerequisite to making the corpus useful for compa-
rable research, as discussed by Bird et al (2008).
In a nutshell, we see two possible avenues for-
ward. For the ACL 2012 Contributed Task, we col-
lected various views on the corpus data (as well as
some of the source code used in its production) in a
unified SVN repository. Following the open-source,
crowd-sourcing philosophy, one option would be to
make this repository openly available to all inter-
ested parties for future development, possibly aug-
menting it with support infrastructure like, for ex-
ample, a mailing list and shared wiki.
At the same time, our experience from the past
months suggests that it is hard to reach sufficient
momentum and critical mass to make substantial
progress towards our long-term goals, while con-
tributions are limited to loosely organized volun-
teer work. A possibility we believe might overcome
these limitations would be an attempt at formaliz-
ing work in this spirit further, for example through a
funded project (with endorsement and maybe finan-
cial support from organizations like the ACL, ICCL,
AFNLP, ELRA, or LDC).
A potential, but not seriously contemplated ?busi-
ness model? for the ACL Anthology Corpus could be
that only groups providing also improved versions
of the corpus would get access to it. This would
contradict the community spirit and other demands,
viz. that all code should be made publicly available
(as open source) that is used to produce the rich-text
XML for new papers added to the Anthology. To de-
cide on the way forward, we will solicit comments
and expressions of interest during ACL 2012, in-
cluding of course from the R50 workshop audience
and participants in the Contributed Task. Current
results and status updates will always be accessible
through the following address:




	
http://www.delph-in.net/aac/
The ACL publication process for conferences and
workshops already today supports automated collec-
tion of metadata and uniform layout/branding. For
future high-quality collections of papers in the area
of Computational Linguistics, the ACL could think
94
about providing extended macro packages for con-
ferences and journals that generate rich text and doc-
ument structure preserving (TEI P5) XML versions
as a side effect, in addition to PDF generation. Tech-
nically, it should be possible in both LATEX and (for
sure) in word processors such as OpenOffice or MS
Word. It would help reducing errors induced by
the tedious PDF-to-XML extraction this Contributed
Task dealt with.
Finally, we do think that it will well be possible to
apply the Contributed Task ideas and machinery to
scientific publications in other areas, including the
envisaged NLP research and existing NLP applica-
tions for search, terminology extraction, summariza-
tion, citation analysis, and more.
6 Acknowledgments
The authors would like to thank the ACL, the work-
shop organizer Rafael Banchs, the task contributors
for their pioneering work, and the NUS group for
their support. We are indebted to Rebecca Dridan
for helpful feedback on this work.
The work of the first author has been funded
by the German Federal Ministry of Education and
Research, projects TAKE (FKZ 01IW08003) and
Deependance (FKZ 01IW11003). The second and
third authors are supported by the Norwegian Re-
search Council through the VerdIKT programme.
References
Abu-Jbara, A., & Radev, D. (2011). Coherent
citation-based summarization of scientific papers.
In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human
language techologies (pp. 500?509). Portland,
OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011a). Scisumm: A multi-document summa-
rization system for scientific articles. In Proceed-
ings of the ACL-HLT 2011 system demonstrations
(pp. 115?120). Portland, OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011b). Towards multi-document summarization
of scientific articles: Making interesting compar-
isons with SciSumm. In Proceedings of the work-
shop on automatic summarization for different
genres, media, and languages (pp. 8?15). Port-
land, OR.
Anderson, A., McFarland, D., & Jurafsky, D.
(2012). Towards a computational history of the
ACL:1980?2008. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
Athar, A. (2011). Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of the ACL 2011 student session (pp. 81?87).
Portland, OR.
Ban?ski, P., & Przepi?rkowski, A. (2009). Stand-off
TEI annotation: the case of the National Corpus
of Polish. In Proceedings of the third linguistic
annotation workshop (pp. 64?67). Suntec, Singa-
pore.
Berg, ?. R., Oepen, S., & Read, J. (2012). To-
wards high-quality text stream extraction from
PDF. Technical background to the ACL 2012
Contributed Task. In Proceedings of the ACL-
2012 main conference workshop on Rediscover-
ing 50 Years of Discoveries. Jeju, Republic of
Korea.
Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,
Kan, M.-Y., Lee, D., Powley, B., Radev, D., &
Tan, Y. F. (2008). The ACL Anthology Reference
Corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proceed-
ings of the sixth international conference on lan-
guage resources and evaluation (LREC-08). Mar-
rakech, Morocco.
Councill, I. G., Giles, C. L., & Kan, M.-Y. (2008).
ParsCit: An open-source CRF reference string
parsing package. In Proceedings of LREC-2008
(pp. 661?667). Marrakesh, Morocco.
Dahlmeier, D., Ng, H. T., & Tran, T. P. (2011). NUS
at the HOO 2011 pilot shared task. In Proceedings
of the generation challenges session at the 13th
european workshop on natural language genera-
tion (pp. 257?259). Nancy, France.
Dale, R., & Kilgarriff, A. (2010). Helping Our Own:
Text massaging for computational linguistics as a
new shared task. In Proceedings of the 6th inter-
national natural language generation conference.
Trim, Co. Meath, Ireland.
95
Daudaravic?ius, V. (2012). Applying collocation seg-
mentation to the ACL Anthology Reference Cor-
pus. In Proceedings of the ACL-2012 main con-
ference workshop: Rediscovering 50 years of dis-
coveries. Jeju, Republic of Korea.
Dong, C., & Sch?fer, U. (2011). Ensemble-style
self-training on citation classification. In Pro-
ceedings of 5th international joint conference on
natural language processing (pp. 623?631). Chi-
ang Mai, Thailand.
Gupta, P., & Rosso, P. (2012). Text reuse with
ACL: (upward) trends. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.
Gupta, S., & Manning, C. (2011). Analyzing the
dynamics of research by extracting key aspects of
scientific papers. In Proceedings of 5th interna-
tional joint conference on natural language pro-
cessing (pp. 1?9). Chiang Mai, Thailand.
Hall, D., Jurafsky, D., & Manning, C. D. (2008).
Studying the history of ideas using topic models.
In Proceedings of the 2008 conference on empir-
ical methods in natural language processing (pp.
363?371). Honolulu, Hawaii.
Johri, N., Ramage, D., McFarland, D., & Jurafsky,
D. (2011). A study of academic collaborations
in computational linguistics using a latent mix-
ture of authors model. In Proceedings of the 5th
ACL-HLT workshop on language technology for
cultural heritage, social sciences, and humanities
(pp. 124?132). Portland, OR.
Johri, N., Roth, D., & Tu, Y. (2010). Experts?
retrieval with multiword-enhanced author topic
model. In Proceedings of the NAACL HLT 2010
workshop on semantic search (pp. 10?18). Los
Angeles, California.
Mao, Y., Balasubramanian, K., & Lebanon, G.
(2010). Dimensionality reduction for text using
domain knowledge. In COLING 2010: Posters
(pp. 801?809). Beijing, China.
Mohammad, S., Dorr, B., Egan, M., Hassan, A.,
Muthukrishan, P., Qazvinian, V., Radev, D., & Za-
jic, D. (2009). Using citations to generate surveys
of scientific paradigms. In Proceedings of human
language technologies: The 2009 annual confer-
ence of the north american chapter of the associa-
tion for computational linguistics (pp. 584?592).
Boulder, Colorado.
Muthukrishnan, P., Radev, D., & Mei, Q. (2011). Si-
multaneous similarity learning and feature-weight
learning for document clustering. In Proceedings
of textgraphs-6: Graph-based methods for natu-
ral language processing (pp. 42?50). Portland,
OR.
Nhat, H. D. H., & Bysani, P. (2012). Linking ci-
tations to their bibliographic references. In Pro-
ceedings of the ACL-2012 main conference work-
shop: Rediscovering 50 years of discoveries. Jeju,
Republic of Korea.
Przepi?rkowski, A. (2009). TEI P5 as an XML stan-
dard for treebank encoding. In Proceedings of the
eighth international workshop on treebanks and
linguistic theories (pp. 149?160). Milano, Italy.
Qazvinian, V., & Radev, D. R. (2008). Scientific
paper summarization using citation summary net-
works. In Proceedings of the 22nd international
conference on computational linguistics (COL-
ING 2008) (pp. 689?696). Manchester, UK.
Qazvinian, V., & Radev, D. R. (2010). Identi-
fying non-explicit citing sentences for citation-
based summarization. In Proceedings of the 48th
annual meeting of the association for computa-
tional linguistics (pp. 555?564). Uppsala, Swe-
den.
Qazvinian, V., & Radev, D. R. (2011). Learning
from collective human behavior to introduce di-
versity in lexical choice. In Proceedings of the
49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies (pp. 1098?1108). Portland, OR.
Qazvinian, V., Radev, D. R., & Ozgur, A. (2010).
Citation summarization through keyphrase ex-
traction. In Proceedings of the 23rd international
conference on computational linguistics (COL-
ING 2010) (pp. 895?903). Beijing, China.
Radev, D., & Abu-Jbara, A. (2012). Rediscovering
ACL discoveries through the lens of ACL Anthol-
ogy Network citing sentences. In Proceedings of
96
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.
Radev, D., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network corpus. In
Proceedings of the 2009 workshop on text and
citation analysis for scholarly digital libraries.
Morristown, NJ, USA.
Radev, D. R., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network. In Pro-
ceedings of the 2009 workshop on text and cita-
tion analysis for scholarly digital libraries (pp.
54?61). Suntec City, Singapore.
Reiplinger, M., Sch?fer, U., & Wolska, M. (2012).
Extracting glossary sentences from scholarly ar-
ticles: A comparative evaluation of pattern boot-
strapping and deep analysis. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.
Ritchie, A., Teufel, S., & Robertson, S. (2006a).
Creating a test collection for citation-based IR ex-
periments. In Proceedings of the human language
technology conference of the NAACL, main con-
ference (pp. 391?398). New York City.
Ritchie, A., Teufel, S., & Robertson, S. (2006b).
How to find better index terms through cita-
tions. In Proceedings of the workshop on how can
computational linguistics improve information re-
trieval? (pp. 25?32). Sydney, Australia.
Rozovskaya, A., Sammons, M., Gioja, J., & Roth,
D. (2011). University of illinois system in HOO
text correction shared task. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 263?266). Nancy, France.
Sch?fer, U., & Kasterka, U. (2010). Scientific au-
thoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL HLT
2010 workshop on computational linguistics and
writing: Writing processes and authoring aids
(pp. 7?14). Los Angeles, CA.
Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-
bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7?13). Portland, OR.
Sch?fer, U., & Weitz, B. (2012). Combining OCR
outputs for logical document structure markup.
Technical background to the ACL 2012 Con-
tributed Task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
Sim, Y., Smith, N. A., & Smith, D. A. (2012).
Discovering factions in the computational linguis-
tics community. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
TEI Consortium. (2012, February). TEI P5: Guide-
lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)
Tu, Y., Johri, N., Roth, D., & Hockenmaier, J.
(2010). Citation author topic model in expert
search. In COLING 2010: Posters (pp. 1265?
1273). Beijing, China.
Vogel, A., & Jurafsky, D. (2012). He said, she said:
Gender in the ACL anthology. In Proceedings of
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.
Xia, F., Lewis, W., & Poon, H. (2009). Language
ID in the context of harvesting language data off
the web. In Proceedings of the 12th conference
of the european chapter of the ACL (EACL 2009)
(pp. 870?878). Athens, Greece.
Xia, F., & Lewis, W. D. (2008). Repurposing the-
oretical linguistic data for tool development and
search. In Proceedings of the third international
joint conference on natural language processing:
Volume-i (pp. 529?536). Hyderabad, India.
Zesch, T. (2011). Helping Our Own 2011: UKP
lab system description. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 260?262). Nancy, France.
97
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 98?103,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards High-Quality Text Stream Extraction from PDF
Technical Background to the ACL 2012 Contributed Task
?yvind Raddum Berg, Stephan Oepen, and Jonathon Read
Department of Informatics, Universitetet i Oslo
{oyvinrb |oe |jread}@ifi.uio.no
Abstract
Extracting textual content and document
structure from PDF presents a surprisingly
(depressingly, to some, in fact) difficult chal-
lenge, owing to the purely display-oriented de-
sign of the PDF document standard. While a
variety of lower-level PDF extraction toolk-
its exist, none fully support the recovery of
original text (in reading order) and relevant
structural elements, even for so-called born-
digital PDFs, i.e. those prepared electronically
using typesetting systems like LATEX, OpenOf-
fice, and the like. This short paper summarizes
a new tool for high-quality extraction of text
and structure from PDFs, combining state-of-
the-art PDF parsing, font interpretation, layout
analysis, and TEI-compliant output of text and
logical document markup.?
1 Introduction?Motivation
To view a collection of scholarly articles like the
ACL Anthology as a structured knowledge base sub-
stantially transcends a na?ve notion of a corpus as
a mere collection of running text. Research litera-
ture is the result of careful editing and typesetting
and, thus, is organized around its complex internal
structure. Relevant structural elements can comprise
both geometric (e.g. pages, columns, blocks, or ta-
bles) and logical units (e.g. titles, abstracts, head-
ings, paragraphs, or citations)?where (ideally) ge-
ometric and logical document structure play hand
in hand to a degree that can make it hard to draw
clear dividing lines in some cases (e.g. in itemized
or numbered lists).
To date, the dominant standard for electronic doc-
ument archival is Portable Document Format (PDF),
?We are indebted to Rebecca Dridan, Ulrich Sch?fer, and the
ACL workshop reviewers for helpful feedback on this work.
originally created as a proprietary format by Adobe
Systems Incorporated in the early 1990s and sub-
sequently made an open ISO standard (which was
officially adopted in 2008 and embraced by Adobe
through a public license that grants royalty-free us-
age). PDF is something of a composite standard,
unifying at least three basic technologies:
1. A subset of the PostScript page ?programming?
language, dropping constructs like loops and
branches, but including all graphical operations
to draw layout elements, text, and images.
2. A font embedding system which allows a doc-
ument to ?carry along? a broad variety of fonts
(in various formats), as may be needed to en-
sure display just as the document was designed.
3. A structured storage system, which organizes
various data objects?for example images and
fonts?inside a PDF document.
All data objects in a PDF file are represented in
a visually-oriented way, as a sequence of operators
which?when interpreted by a PDF renderer?will
draw the document on a page canvas. This is a nat-
ural approach considering the design roots of PDF
as a PostScript successor and its original central role
in desktop publishing applications; but the implica-
tions of such visually-centered design are unfortu-
nate for the task of recovering textual content and
logical document structure.
Interpretation of PDF operators will provide one
with all the individual characters, as well as their
formatting and position on the page. However, they
generally do not convey information about higher
level text units such as tokens, lines, or columns?
information about boundaries between such units is
only available implicitly through whitespace, i.e. the
98
mere absence of textual or graphical objects. Fur-
thermore, data fragments comprising content text on
a page may consist of individual characters, parts of
a word, whole lines, or any combination thereof?
as dictated by font properties and kerning require-
ments. Complicating text extraction from PDF fur-
ther, there are no rules governing the order in which
content is encoded in the document. For example, to
produce a page with a two-column layout, the page
could be drawn by first drawing the first lines of the
left and right columns, then the second lines, etc.
Obtaining text in logical reading order, however, ob-
viously requires that the text in the left column be
processed before the one on the right, so a na?ve ap-
proach to text extraction based on the sequencing of
objects in the PDF file might produce undesirable
results.
Since the standard is now open and free for any-
one to use, we are fortunate to have several ma-
ture, open-source libraries to handle low-level pars-
ing and manipulation of objects in PDF documents.
For this project, we build on Apache PDFBox1, for
its maturity, relatively active support, and interface
flexibility. Originally as an MSc project in Com-
puter Science (Berg, 2011), we have developed a pa-
rameterizable toolkit for high-quality text and struc-
ture extraction from born-digital PDFs, which we
dub PDFExtract.2 In this application, we seek to ap-
proximate this structure by using all the visual clues
and information we have available.
The data presented in a PDF file consists of
streams of objects; by placing hardly any signifi-
cance on the order of elements within these streams,
and more on the visual result obtained by (virtu-
ally) ?rendering? PDF operations, the task of text and
structure extraction is shifted slightly?from what
traditionally amounts to stream-processing, and to-
wards a point of view related to computer vision.
This view, in fact, essentially corresponds to the
same problem tackled by OCR software, though
without the need to perform actual character recog-
nition. Some of the key elements of PDFExtract,
thus, build on related OCR techniques and adapt
and extend these to the PDF processing task. The
process of ?understanding? a PDF document in this
1See http://pdfbox.apache.org/ for details.
2See http://github.com/elacin/PDFExtract/.
context is called document layout analysis, a task
which is commonly treated as two sequential sub-
processes. First, a page image is subjected to geo-
metric layout analysis; the result of this first stage
then serves as input for a subsequent step of logi-
cal layout analysis and content extraction. The fol-
lowing sections briefly review core aspects of the
design and implementation of PDFExtract, ranging
from low-level whitespace detection (?2), over ge-
ometric and logical layout analysis (?3 and ?5, re-
spectively), to aspects of font handling (?4).
2 Whitespace Detection
As a prerequisite to all subsequent analysis, seg-
ment boundaries between tokens, lines, columns,
and other blocks of content need to be made ex-
plicit. Such boundaries are predominantly repre-
sented through whitespace, which is not overtly rep-
resented among the data objects in PDF files. The
approach to whitespace detection and page segmen-
tation in PDFExtract is an extension of the frame-
work proposed by Breuel (2002) (originally in the
context of OCR).
The first step here is to find a cover of the back-
ground whitespace of a document in terms of maxi-
mal empty rectangles. This is accomplished in a top-
down procedure, using a whole page as its starting
point, and working in a way abstractly analogous to
quicksort or branch and bound algorithms. Whites-
pace rectangles are identified in order of decreasing
?quality? (as determined by size, shape, position, and
relations to actual page content), which means that
the result will in general be globally optimal?in the
sense that no other (equal-sized) sequence of cover-
ing rectangles would yield a larger total quality sum.
Figure 1 illustrates the main idea of the algorithm,
which starts from a bound (initially the page at large)
and a set of non-empty rectangles, called obstacles.
If the set is empty, it means that the bound is a max-
imal rectangle with respect to other obstacles (sur-
rounding the bound). If, as in Figure 1, there are
obstacles, the bound needs to be further subdivided.
To this end, we choose one obstacle as a pivot, which
ideally is centered somewhere around the middle of
the bound. As no maximal rectangle can contain ob-
stacles, in particular not the pivot, there are four pos-
sibilities for the solution of the maximal whitespace
99
(a) (b) (c) (d)
Figure 1: Schematic example of one iteration of the whitespace covering algorithm. In (a) we see some obstacles (in
blue) contained within a bounding rectangle; in (b) one of them is chosen as as pivot (in red); and (c) and (d) show
how the original bound is divided into four smaller rectangles (in grey) around the pivot.
rectangle problem?one for each side of the pivot.
The areas of these four sub-bounds are computed, a
list of intersecting obstacles is computed for each of
them, and they are processed in turn.
As originally proposed by Breuel (2002), the
basic procedure proved applicable to born-digital
PDFs, though leaving room for improvements both
in terms of the quality of results and run-time perfor-
mance. Some deficiencies that were observed in pro-
cessing documents from the ACL Anthology (and
other samples of scholarly literature) are exempli-
fied in Figure 2, relating to smallish, ?stray? whites-
pace rectangles in the middle of otherwise contigu-
ous segments (top row in Figure 2), challenges re-
lated to relative differences in line spacing (middle),
and spurious vertical boundaries introduced by so-
called rivers, i.e. accidental alignment of horizon-
tal spacing across lines (bottom). Besides adjust-
ments to the rectangle ?quality? function, the prob-
lems were addressed by (a) allowing a small degree
Figure 2: Select challenges to whitespace covering ap-
proach: stray whitespace inbetween groups of text (top);
inter- vs. intra-paragraph spacing (middle); and ?rivers?
leading to spurious vertical boundaries (bottom).
of overlap between whitespace rectangles and obsta-
cles, (b) a strong preference for contiguous areas of
whitespace (thus making the procedure work from
the page borders inwards), (c) variable lower bounds
on the height and width of whitespace rectangles,
computed dynamically from font properties of sur-
rounding text, and (d) a small number of special-
ized heuristic rules, to block unwanted whitespace
rectangles in select configurations. Berg (2011) pro-
vides full details for these adaptations, as well as for
algorithmic optimizations and parameterization that
enable run-time throughputs of tens of pages per cpu
second.
3 Determining Page Layout
The high-level goal in analyzing page layout is to
produce a hierarchical representation of a page in
terms of blocks of homogenous content, thus mak-
ing explicit relevant spatial relationship between
them. In the realm of OCR, this task is often re-
ferred to as geometric layout analysis (see, for ex-
ample, (Cattoni et al, 1998)), whereas the term
(de)boxing has at times been used in the context of
text stream extraction from PDFs. In the following
paragraphs, we will focus on column boundary de-
tection, but PDFExtract essentially applies the same
general techniques to the identification of other rel-
evant inter-segment boundaries.
While whitespace rectangles are essential to col-
umn boundary identification, there is of course no
guarantee for the existence of one rectangle which
were equivalent to a whole column boundary. First,
as a natural consequence of the whitespace detection
procedure, horizontal rectangles can ?interrupt? can-
didate colum boundaries. Second, there may well be
typographic imperfections causing gaps in the iden-
tified whitespace (as exemplified in the top of Fig-
100
Figure 3: Select challenges to column identification: text
elements protruding into the margin (top) and gaps in
whitespace rectangle coverage (often owed to processing
bounds imposed for premium performance).
ure 3), or it can be the case that geometric constraints
or computational limits imposed on the whitespace
cover algorithm result in ?missing? whitespace rect-
angles (in the bottom of Figure 3). Whereas the orig-
inal design of Breuel (2002) makes no provisions for
these cases, PDFExtract adapts a revised, three-step
approach to column detection, viz. (a) extracting an
initial set of candidate boundaries; (b) heuristically
expanding column boundary candidates vertically;
and (c) combining logically equivalent boundaries
and filtering unwarranted ones. Here, both steps (a)
and (b) assume geometric constraints on the aspect
ratio of candidate column boundaries, as well as on
the existence and relative proportions of surround-
ing non-whitespace content. Again, please see Berg
(2011) for further background on these steps.
With column boundaries in place, PDFExtract
proceeds to the identification of blocks of content
(which may correspond to, for example, logical
paragraphs, headings, displayed equations, tables, or
graphical elements). This step, essentially, is real-
ized through a recursive ?flooding? function, form-
ing connected blocks from adjacent, non-whitespace
PDF data objects where there are no intervening
whitespace rectangles. Regions that (by content
or font properties) can be identified as (parts of)
mathematical equations receive special attention at
this stage, allowing limited amounts of horizon-
tally separating whitespace to be ignored for block
formation. In a similar spirit, line segmentation
(i.e. grouping of vertically aligned data objects) is
performed block-wise?sorting content within each
block by Y-coordinates and determining baselines
and inter-line spacing in a single downwards pass.
The final key component in geometric layout
analysis is the recovery of reading order (recalling
that PDFs do not provide reliable sequencing infor-
mation for data objects). PDFExtract adapts one
of the two techniques suggested by Breuel (2003),
viz. topological sorting of lines (which can include
single-line blocks, where no block-internal line seg-
mentation was detected) based on (a) relations of hi-
erarchical nesting and (b) relative geometric posi-
tions. PDFExtract was tested against a set of some
100 diverse PDF documents (from different sources
of scholarly literature, a range of distinct PDF gener-
ators, quite variable layout, and multiple languages),
and its topological content sorting (detailed further
in Berg, 2011) was found to give very satisfactory
results in terms of reading order recovery.
4 Font Handling and Word Segmentation
Many of the steps of geometric layout analysis out-
lined above depend on accurate coordinate informa-
tion for glyphs, which turned out an unforeseen low-
level challenge in our approach of building PDFEx-
tract on top of Apache PDFBox. Figure 4 (on the
left) shows a problematic example of ?raw? glyph
placement information. Several factors contribute to
incorrect glyph positioning, including the sheer va-
riety of font types supported in PDFs, missing in-
formation about non-standard, embedded fonts, and
design limitations and bugs in PDFBox. To work
around common issues, PDFExtract includes a cou-
ple of patches to PDFBox internals as well as spe-
cialized code for different types of font embedding
in PDF to perform boundary box computation, po-
sition offsetting, and and mapping to Unicode code
points. The (much improved though not quite per-
fect) result of these adjustments, when applied to
our running example, is depicted in the middle of
Figure 4.
With the ultimate goal of creating a high-quality
101
(a) (b) (c)
Figure 4: Examples of font-related challenges (before and after correction) and word segmentation.
(structured) text corpus from ACL Anthology doc-
uments, word segmentation naturally is a mission-
critical component of PDFExtract. Seeing that inter-
word whitespace is more often than not omitted
from PDF data objects, word segmentation?much
like other sub-tasks in geometric layout analysis?
operates in terms of display positions. Deter-
mining whether the distance between two adjacent
glyphs represents a word-separating whitespace or
not, might sound simple?but in practice it proved
difficult to devise a generic solution that performs
well across differences in fonts and sizes (and corre-
sponding variation in kerning, i.e. intra-word spac-
ing), deals with both high-quality and poor typog-
raphy, and is somewhat robust to remaining inac-
curacies in glyph positions . PDFExtract arrived at
a novel algorithm that approximates character text
spacing (as could be set by the PDF Tc operator) by
averaging a selection of the smaller character dis-
tances within a line. The resulting average charac-
ter spacing is subsequently used to normalize hori-
zontal distances, i.e. subtract line-specific character
spacing from every distance on that line?to ideally
center character distances around zero, while leav-
ing word distances larger (they will also be relatively
much larger than before in comparison). The iden-
tification of word boundaries itself, accordingly, be-
comes straightforward, comparing normalized dis-
tances to a percentage of the local font size. The
results of this process are shown for our example in
the right of Figure 4.
5 (Preliminary) Logical Layout Analysis
In our view, thorough geometric layout analysis is
an important prequisite of logical layout analysis.
Hence, the emphasis of Berg (2011) was with re-
spect to the geometric analysis. However, what fol-
lows is an overview of the preliminary procedure in
PDFExtract to determine logical document structure
from geometric layout and typographic information.
The process begins by collating a set of text styles
(i.e. unique combinations of font type and size).
Then, various heuristics govern the assignment of
styles to logical roles:
Body text Choose whichever style occurs most fre-
quently (in terms of the number of characters).
Title Choose the header-like block on the first page
that has the largest font size.
Abstract If one of the first pages has a single-line
block with a style which is bigger or bolder
than body text, and contains the word abstract,
it is chosen as an abstract header. All body text
until the next heading is the abstract text.
Footnote Search for blocks on the lower part of
the page that are smaller than body text; check
that they start with a number or other footnote-
indicating symbol.
Sections Identify section header styles by compil-
ing a list of styles that are either larger than or
have some emphasis on the body text style, and
have instances with evidence of section num-
bering (e.g. 1.1, (1a)). Infer the nesting level
of each section header style from its order of
occurrence in the document; a section head-
ing will always appear earlier than a subsection
heading, for instance.
Having identified the different components in the
document, these are used to create a logical hierar-
chical representation following the TEI P5 Guide-
lines (TEI Consortium, 2012) as introduced by
Sch?fer et al (2012). Title, abstract, floaters, and
figures are separated from the main text. The body
of the document is then collated into a tree of section
elements, with headers and body text. Body text is
collected by combining consecutive text blocks that
102
have identical styles, before inferring paragraphs on
the basis of indented initial lines. Dehyphenation is
tackled using a combination of a lexicon and a set of
orthographic rules.
6 Discussion?Outlook
PDFExtract provides a fresh and open-source take
on the problem of high-quality content and struc-
ture extraction from born-digital PDFs. Unlike ex-
isting initiatives (e.g. the basic TextExtraction
class of PDFBox or the pdftotext command line util-
ity from the Poppler library3), PDFExtract discards
sequencing information available in the so-called
PDF text stream, but instead applies and adapts tech-
niques from OCR?notably a whitespace covering
algorithm, column, block, and line detection, recov-
ery of reading order based on line-oriented topolog-
ical sort, and improved word segmentation taking
advantage of specialized PDF font interpretation.
While very comprehensive in terms of its geometric
layout analysis, PDFExtract to date only make avail-
able a limited range of logical layout analysis func-
tionality (and output into TEI-compliant markup),
albeit also in this respect more so than pre-existing
PDF text stream extraction approaches.
For the ACL 2012 Contributed Task on Redis-
covering 50 Years of Discoveries (Sch?fer et al,
2012), PDFExtract outputs for the born-digital sub-
set of the ACL Anthology are a component of the
?starter package? offered to participants, in the hope
that content and structure derived from OCR tech-
niques (Sch?fer & Weitz, 2012) and those extracted
directly from embedded content in the PDFs will
complement each other. As discussed in more detail
by Sch?fer et al (2012), the two approaches have
in part non-overlapping strengths and weaknesses,
such that aligning content elements that correspond
to each other across the two universes could yield a
multi-dimensional, ideally both more complete and
more accurate perspective. PDFExtract is a recent
development and remains subject to refinement and
extension. Beyond a limited quantitative and qual-
itative evaluation review by Berg (2011), the exact
quality levels of text and document structure that it
makes available (as well as relevant factors of varia-
tion, across different types of documents in the ACL
3See http://poppler.freedesktop.org/.
Anthology) remains to be determined empirically.
We make available the full package, accompanied
by some technical documentation (Berg, 2011), as
well as a sample of gold-standard TEI-compliant
target outputs) in the hope that it may serve as the
basis for future work towards the ACL Anthology
Corpus?both at our own sites (i.e. the University
of Oslo and DFKI Saarbr?cken) and collaborating
partners. We would enthusiastically welcome addi-
tional collaborators in this enterprise and will seek
to provide any reasonable assistance required for the
deployment and extension of PDFExtract.
References
Berg, ?. R. (2011). High precision text extraction
from PDF documents. MSc Thesis, University of
Oslo, Department of Informatics, Oslo, Norway.
Breuel, T. (2002). Two geometric algorithms for
layout analysis. In Proceedings of the 5th work-
shop on Document Analysis Systems (pp. 687?
692). Princeton, USA.
Breuel, T. (2003). Layout analysis based on text line
segment hypotheses. In Third international work-
shop on Document Layout Interpretation and its
Applications. Edinburgh, Scotland.
Cattoni, R., Coianiz, T., & Messelodi, S. (1998). Ge-
ometric layout analysis techniques for document
image understanding. A review (ITC-irst Techni-
cal Report TR#9703-09). Trento, Italy.
Sch?fer, U., Read, J., & Oepen, S. (2012). Towards
an ACL Anthology corpus with logical document
structure. An overview of the ACL 2012 con-
tributed task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
Sch?fer, U., & Weitz, B. (2012). Combining OCR
outputs for logical document structure markup.
Technical background to the ACL 2012 Con-
tributed Task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
TEI Consortium. (2012, February). TEI P5: Guide-
lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)
103
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 28?36, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Factuality Detection on the Cheap:
Inferring Factuality for Increased Precision in Detecting Negated Events
Erik Velldal
University of Oslo
Department of Informatics
erikve@ifi.uio.no
Jonathon Read
University of Oslo
Department of Informatics
jread@ifi.uio.no
Abstract
This paper describes a system for discriminat-
ing between factual and non-factual contexts,
trained on weakly labeled data by taking ad-
vantage of information implicit in annotations
of negated events. In addition to evaluating
factuality detection in isolation, we also evalu-
ate its impact on a system for event detection.
The two components for factuality detection
and event detection form part of a system for
identifying negative factual events, or coun-
terfacts, with top-ranked results in the *SEM
2012 shared task.
1 Introduction
The First Joint Conference on Lexical and Compu-
tational Semantics (*SEM 2012) is hosting a shared
task1 (Morante and Blanco, 2012) on identifying
various elements of negation, and one of the sub-
tasks is to identify negated events. However, only
events occurring in factual statements should be la-
beled. This paper describes pilot experiments on
how to train a factuality classifier by taking advan-
tage of implicit information on factuality in anno-
tations of negation. In addition to evaluating factu-
ality detection in isolation, we also assess its impact
when embedded in a system for event detection. The
system was ranked first for the *SEM 2012 subtask
of identifying negated events, and also formed part
of the top-ranked system in the shared task overall
(Read et al, 2012). The experiments presented in
this paper further improves on these initial results.
1The web site of the 2012 *SEM Shared Task:
http://www.clips.ua.ac.be/sem2012-st-neg/
Note that the system was designed for submission to
the closed track of the shared task, which means de-
velopment is constrained to using the data provided
by the task organizers.
The rest of the paper is structured as follows. We
start in Section 2 by giving a brief overview of re-
lated work and resources. In Section 3 we then
present the problem statement in more detail, along
with the relevant data sets. This section also dis-
cusses the notion of (non-)factuality assumed in the
current paper. We then go on to present and evaluate
the factuality classifier in Section 4. In Section 5
we move on to describe the event detection task,
which is handled by learning a discriminative rank-
ing function over candidate tokens within the nega-
tion scope, using features from paths in constituent
trees. Both the event ranking function and the fac-
tuality classifier are implemented using the Support
Vector Machine (SVM) framework. After evaluat-
ing the impact of factuality detection on event de-
tection, we finally provide some concluding remarks
and discussion of future directions in Section 6.
2 Related Work
Note that the *SEM 2012 shared task singled out
three separate subtasks for the problem of recogniz-
ing negation, namely the identification of negation
cues, their in-sentence scopes and the negated fac-
tual events. Most of the systems submitted for the
shared task correspondingly implemented a pipeline
consisting of three components, one for each sub-
task. One thing that set the system of Read et al
(2012) apart from other shared task submissions is
that it included a fourth component; a dedicated
28
classifier for identifying the factuality of a given
context. It is this latter problem which is the main
focus of the current paper, along with its interactions
with the task of identifying events.
The field has witnessed a growing body of work
dealing with uncertainty and speculative language
over the recent years, and in particular so within the
domain of biomedical literature. These efforts have
been propelled not least by the several shared tasks
that have targeted such phenomena. The shared task
at the 2010 Conference on Natural Language Learn-
ing (CoNLL) focused on speculation detection for
the domain of biomedical research literature (Farkas
et al, 2010), with data sets based on the BioScope
corpus (Vincze et al, 2008) which annotates so-
called speculation cues along with their scopes. The
BioNLP shared tasks of 2009 and 2011 mainly con-
cerned recognizing bio-molecular events in text, but
optional subtasks involved detecting whether these
events were affected by speculation or negation. The
data set used for this task is the Genia event corpus
(Kim et al, 2008) which annotates the uncertainty
of events according to the three labels certain, prob-
able and doubtful (but without explicitly annotating
cue words or scope as in BioScope).
The best performer in the BioNLP 2011 support-
ing task of detecting speculation modification of
events, the system of Kilicoglu and Bergler (2011),
achieved an end-to-end F1 of 27.25 using a manu-
ally compiled dictionary of trigger expressions to-
gether with a set of rules operating on syntactic de-
pendencies for identifying events and event modifi-
cation. Turning to the task of identifying specula-
tion cues in the BioScope data, current state-of-the-
art systems, implementing simple supervised classi-
fication approaches on the token- or sequence-level,
achieves F1-scores of well above 80 (Tang et al,
2010; Velldal et al, 2012). For the task of resolv-
ing the scopes of these cues, the current best systems
obtain end-to-end F1-scores close to 60 in held-out
testing (Morante et al, 2010; Velldal et al, 2012).
Note that the latter reference is from a forthcom-
ing issue of Computational Linguistics specifically
on modality and negation (Morante and Sporleder,
2012). In that same issue, Saur?? and Pustejovsky
(2012) present a linguistically motivated system for
factuality profiling with manually crafted rules op-
erating on dependency graphs. Conceptually treat-
ing factuality as a perspective that a particular source
(speaker) holds toward an event, the system aims to
make this attribution explicit. It is developed on the
basis of the FactBank corpus (Saur?? and Pustejovsky,
2009), containing manual annotations of pairs of
events and sources along the dimensions of polarity
(positive, negative, or underspecified) and certainty
(certain, probable, possible, or underspecified.
Prabhakaran et al (2010) report experiments with
belief tagging, which in many ways is similar to
factuality detection. Their starting point is a cor-
pus of 10.000 words comprising a variety of genres
(newswire text, emails, instructions, etc.) annotated
for speaker belief of stated propositions (Diab et al,
2009): Propositional heads are tagged as committed
belief (CB), non-committed belief (NCB), or not ap-
plicable (NA), meaning no belief is expressed by the
speaker. To some degree, CB and NCB can be seen
as similar to our categories of factuality and non-
factuality, respectively. Applying a one-versus-all
SVM classifier by 4-fold cross validation, and using
wide range of both lexical and syntactical features,
Prabhakaran et al (2010) report F1-scores of 69.6
for CB, 34.1 for NCB, and 64.5 for NA.
3 Data Sets and the Notion of Factuality
The data we will be using in the current study is
taken from a recently released corpus of Conan
Doyle (CD) stories annotated for negation (Morante
and Daelemans, 2012). The data is annotated with
negation cues, the in-sentence scope of those cues,
as well as the negated event, if any. The cue is the
word(s) or affix indicating a negation, The scope
then indicates the maximal extent of that negation,
while the event indicates the most basic negated el-
ement. In the annotation guidelines, Morante et al
(2011, p. 4) use the term event in a rather general
sense; ?[i]t can be a process, an action, or a state.?
The guidelines occasionally also refer to the no-
tion of negated elements as encompassing ?the main
event or property actually negated by the negation
cue? (Morante et al, 2011, p. 27). In the remainder
of this paper we will simply take event to conflate all
these senses.
Some examples of annotated sentences are shown
below. Throughout the paper we will use angle
brackets for marking negation cues, curly brackets
29
for scopes, and underlines for events.
(1) {There was} ?no? {answer}.
(2) {I do} ?n?t? {think that I am a coward} , Watson , but that
sound seemed to freeze my very blood .
In the terminology of Saur?? and Pustejovsky (2012),
the negation cues are negative polarity particles, and
all annotated events in the Conan Doyle data will
have a negative polarity and thereby represent coun-
terfacts, i.e., events with negative factuality. This
should not be confused with non-factuality; a coun-
terfactual statement is not uncertain.
Importantly, however, the Conan Doyle negation
corpus does not explicitly contain any annotation of
factuality. The annotation guidelines specify that
?we focus only on annotating information relative
to the negative polarity of an event? (Morante et al,
2011, p. 4). However, the guidelines also specify
that events should only be annotated for negations
that (i) have a scope and that (ii) occur in factual
statements (Morante et al, 2011, p. 27). (As we only
have annotations for the sentence-level it is possible
to have a cue without a scope in cases where the
cue negates a proposition in a preceding sentence.)
The notion of (non-)factuality assumed in the cur-
rent work will reflect the way it is defined in the
Conan Doyle annotation guidelines. Morante et al
(2011) lists the following types of constructions as
not expressing factual statements (we here show ex-
amples from CDDEV for each case):
- Imperatives:
(3) {Do} ?n?t? {move} , I beg you , Watson .
- Non-factual interrogatives:
(4) {You do} ?n?t? {believe it} , do you , Watson ?
- Conditional constructions:
(5) If {the law can do} ?nothing? we must take the risk our-
selves .
- Modal constructions:
(6) {The fault from what I hear may} ?not? {have been en-
tirely on one side} .
- Wishes or desires:
(7) ? I hope , ? said Dr. Mortimer , ? that {you do} ?not?
{look with suspicious eyes upon everyone [. . . ]}
- Suppositions or presumptions:
(8) I think , Watson , {a brandy and soda would do him} ?no?
{harm} .
- Future tense:
(9) {The shadow} has departed and {will} ?not? {return} .
Our goal then, will be to correctly identify these
cases in order to separate between factual and non-
factual contexts before identifying events. Note that,
while an event, if present, must always be embedded
in the scope, the indicators of factuality are typically
found well outside of this scope. The examples also
show that non-factuality here encompasses a wider
range of phenomena than what is traditionally cov-
ered in work on identifying hedging or speculation.
The examples above illustrate how we can take
the data to implicitly annotate factuality and non-
factuality, and we here show how to take advantage
of this to train a factuality classifier. For the exper-
iments in this paper we will let positive examples
correspond to negations that are annotated with both
a scope and an event, while negative examples cor-
respond to scoped negations with no event. For our
training and development data (CDDEV; more de-
tails below), this strategy gives 738 positive exam-
ples and 317 negatives, spread over 930 sentences.
Our weakly labeled data as defined above comes
with several limitations of course. The implicit la-
beling of factuality will be limited to sentences that
are negated. We will also not have access to an event
in the cases of non-factuality. Neither, do we have
any explicit annotation of factuality cue words for
these examples. All we have are instances of nega-
tion that we know to be within some non-delimited
factual or non-factual context. For our experiments
here will therefore use the negation cue itself as a
place-holder for the abstract notion of context that
we are really trying to make predictions about.
Table 1 presents some basic statistics for the rele-
vant data sets. For training and development we will
use the negation annotated version of The Hound of
the Baskerville?s (CDH) and Wisteria Lodge (CDW)
(Morante and Daelemans, 2012). We refer to the
combination of these two data sets as CDDEV. For
held-out testing we will use the evaluation data
sets prepared for the *SEM 2012 shared task; The
Cardboard Box (CDC) and The Red Circle (CDR)
(Morante and Blanco, 2012). We will use CDEVAL
to refer to the combination of CDC and CDR. Note
30
Scoped Negations
Data set Sentences Negations Factual Non-factual
CDH 3644 984 616 271
CDW 787 173 122 46
CDDEV 4431 1157 738 317
CDC 496 133 87 41
CDR 593 131 86 35
CDEVAL 1089 264 173 76
Table 1: Summary of the Conan Doyle negation data.
Note that the total number of negations (column 3) can
be smaller than the number of scoped negations (columns
4+5). The reason is that it is possible to have a cue with-
out a scope in cases where the cue negates a proposition
in a preceding sentence (which would not be reflected
in these sentence-level annotations). The numbers in the
column ?Factual? correspond to scoped negations that in-
clude an annotated event.
that the column Factual correspond to negations
with both a scope and event (i.e., positive examples,
in terms of factuality classification), while the Non-
factual column correspond to negations with scope
only and no event (negative examples).
4 Factuality Detection
Having described how we abstractly define our train-
ing data above, we can now move on to describe
our experiments with training a factuality classifier.
It is implemented as a linear binary SVM classi-
fier, estimated using the SVMlight toolkit (Joachims,
1999). We start by describing the feature types in
Section 4.1 and then present results in Section 4.2.
4.1 Features
The feature types we use are mostly variations over
bag-of-words (BoW) features. We include left/right
oriented BoW features centered on the negation cue,
recording forms, lemmas, and PoS, and using both
unigrams and bigrams. These features are extracted
both from the sentence as a whole, and from a local
window of six tokens to each side of the cue. The
optimal window size and the order of n-grams was
determined empirically.
The reason for including both local and sentence-
level BoW features is that we would like to be able
to assign different factuality labels to different in-
stances of negation within the same sentence, but
at the same time experiments showed sentence-level
features to be very important.
Note that, ideally our features should be centered
on the negated event, but since this information is
only available for factual contexts, we instead take
the negation cue as our starting point. In practice,
this seems to provide a good approximation, how-
ever, given that the negated event is typically found
in close vicinity of the negation cue.
In addition to the BoW type features we have fea-
tures explicitly recording the first full-stop punctua-
tion following the negation cue (i.e., ?.?, ?!?, or ???) as
well as whether there is an if to the left. Note that,
although this information is already implicit in the
BoW features, the model appeared to benefit from
having them explicitly coupled with the cue itself.
We also experimented with several other features
that were not included in the final configuration.
These included distance to co-occurring verbs, and
modal verbs in particular. We also recorded the pres-
ence of speculative verbs based on various word lists
manually extracted from the training data. None of
these features appeared to contribute information not
already present in the simple BoW features.
4.2 Results
Table 2 provides results for our factuality classifier
using gold cues and gold scopes. In addition, we
also include results for a baseline approach that sim-
ply considers all cases to be factual, i.e., the majority
class. Note that, in this case the precision (of fac-
tuality labeling) is identical to the accuracy, which
is close to 70% on both the development and held-
out set. The recall for the majority-class baseline is
of course at 100%, and the corresponding F1 is ap-
proximately 82 on both data sets. In comparison,
our classifier achieves an F1 of 89.92 for the 10-
fold cross-validation runs on the development data
and 87.10 on the held-out test data. The accuracy
is 83.98 and 80.72, respectively. Across both data
sets it is clear that the classifier offers substantial im-
provements over the baseline. We do however, ob-
serve a drop in performance particularly with respect
to precision when moving to the held-out set.When
inspecting the scores for the two individual sections
of the held-out set, CDC and CDR, we find that
31
Data set Model Prec Rec F1 Acc
CDDEV Baseline 69.95 100.00 82.32 69.95Classifier 84.51 96.07 89.92 83.98
CDEVAL Baseline 69.48 100.00 81.99 69.48Classifier 80.60 94.74 87.10 80.72
Table 2: Results for factuality detection (using gold nega-
tion cues and scopes), reporting 10-fold cross-validation
on CDDEV and held-out testing on CDEVAL.
the classifier seems to have more difficulties with
the former. Although recall is roughly the same
across the two sections (94.25 and 95.24, respec-
tively, which is again fairly close to the 10-fold re-
call of 96.07), precision suffers a much larger drop
on CDC than CDR (78.85 versus 82.47). On the
other hand, it is difficult to reliably assess perfor-
mance on the individual test sets, given the limited
amount of data: There are only 128 relevant test
cases in CDC and 121 in CDR. However, there also
seems to be signs of overfitting, in that an unhealthy
number of the training examples end up as support
vectors in the final model (close to 70%).
Note that the F1-scores cited above targets fac-
tuality as the positive class label. However, given
that this is in fact the majority class it might also
be instructive to look at F1-scores targeting non-
factuality. (In other words, we will use exactly the
same classifier predictions, but compute our scores
by letting true positives correspond to former true
negatives, false positives to former false negatives,
and so on, thereby treating non-factuality as the pos-
itive class we are trying to predict.) Of course,
while all accuracy scores will remain unchanged, the
majority-class baseline yields an F1 of 0 in this case,
as there will be no true positives. Table 3 lists the
non-factuality scores for the classifier.
Given that we are not aware of any other studies
on (non-)factuality detection on this data we are not
yet able to directly compare our results against those
of other approaches. Nonetheless, we believe the
state-of-the-art results cited in Section 2 for related
tasks such as belief tagging and identifying specu-
lation cues give reasons for being optimistic about
the results obtained with the simple classifier used
in these initial pilot experiments.
Data set Prec Rec F1
CDDEV 77.21 66.25 71.31
CDEVAL 81.25 50.00 61.91
Table 3: Results for non-factuality detection (using gold
negation cues and scopes). The scores are based on the
same classifier predictions as in Table 2, but treats non-
factuality as the positive class.
4.3 Error Analysis and Sample Size Effects
In order to gauge the effect that the size of the train-
ing set has on performance we also experimented
with leaving out portions of the training examples
in our 10-fold cross-validation runs. Figure 1 plots a
learning curve showing how classifier performance
on CDDEV changes as we incrementally include
more training examples. In order to more clearly
bring out the contrasts in performance we here plot
results against non-factuality scores. We also show
the size of the training set on a logarithmic scale to
better see whether improvements are constant for n-
fold increases of data. As can be seen, the learning
curve appears to be growing linearly with the incre-
ments in larger training samples and it seems safe to
assume that the classifier would greatly benefit from
 0
 10
 20
 30
 40
 50
 60
 70
 80
 10  100
F1
% of training data
Non-factuality predictions
Figure 1: Learning curve showing the effect on F1 for
non-factuality labeling when withdrawing portions of the
training partitions (shown on a logarithmic scale) across
the 10-fold cross-validation cycles.
32
additional training data.
This impression is strengthened by a manual in-
spection of the misclassifications for CDDEV. Quite
a number of errors seem related to a combination of
scarcity and noise in the data. As a fairly typical
example, consider the following negation which the
system incorrectly classifies as factual:
(10) ? I presume , sir , ? said he at last , ? that {it was} ?not?
{merely for the purpose of examining my skull that you
have done me the honour to call here last night and again
today} ? ?
One could have hoped that the BoW features record-
ing the presence of presume would have tipped this
prediction toward non-factual. However, while there
are ten occurrences of presume in CDDEV, only three
of these are in contexts that we can actually use as
part of our factuality training data. Apart from the
one in Example (10), these are shown in (11) and
(12) below, both of which indicate factual contexts
(given the labeling of an event). We would at least
consider Example (11) to reveal an error in the gold
annotation here, however.
(11) ? {There is} ?no? {other claimant} , I presume ? ?
(12) ? {I presume} ?nothing? .
We also get a few errors for incorrectly labeling
a context as factual in cases where there are no ob-
vious indicators of non-factuality but the annotation
does not mark an event, as in:
(13) ? ?Nothing? {of much importance} , Mr. Holmes .
For some of the other errors we observed it would
seem that introducing additional features that are
sensitive to the syntactic structure could be bene-
ficial. For example, consider sentence (14) below
where we incorrectly classify the first negation as
non-factual;
(14) [. . . ] {I had brought it} only to defend myself if attacked
and ?not? {to shoot {an} ?un?{armed man} who was}
running {away} .
The error is most likely due to overgeneralizing from
the presence of if. By letting the lexical features be
extracted from a context constrained by the syntax
tree rather than a simple sliding window, such errors
might be avoided.
For some more optimistic examples, note that the
previously listed examples of non-factuality in (3)
S
NP
EX
{There
VP
VBD
was
NP
DT
?no?
NN
answer}
.
.
Figure 2: Example of parse tree in the negation data set.
through (9) were all selected among cases that were
correctly predicted by our classifier.
In the next section we move on to describe a sys-
tem for identifying negated events and assess the im-
pact of the factuality classifier on this task (recall
from Section 3 that only negations occurring in fac-
tual statements should be assigned an event).
5 Event Detection
To identify events in factual instances of negation2
we employ an automatically-learned discriminative
ranking function. As training data we select all nega-
tion scopes that have a single-token3 event, and gen-
erate candidates from each token in the scope. The
candidate that matches the event is labeled as cor-
rect; all others are labeled as incorrect. For the ex-
ample sentence in Figure 2 there are three words
in the scope and thus three candidates for events:
There, was and answer.
5.1 Features
Candidates are primarily described in terms of paths
in constituent trees.4 In particular, we record the
full path from a candidate token to the constituent
whose projection matches the negation scope (i.e.,
the most-specific constituent that subsumes all can-
2Note that, although one could of course argue that negated
events should also be identified for non-factual contexts, that is
not how the task is construed in *SEM 2012 shared task or in
the Conan Doyle data sets.
3To simplify the system we assume that all events are single
tokens. It should be noted, however, that 9.85% of events in
CDDEV are actually composed of multiple tokens.
4Constituent trees from Charniak and Johnson?s Max-Ent
reranking parser (2005) were provided by the task organizers.
33
didates). In Figure 2 this is the S root of the
tree; the path that describes the correct candidate is
answer/NN/NP/VP/S. We also record delexical-
ized paths (e.g., ./NN/NP/VP/S) and generalized
paths (e.g., ./NN//S), as well as bigrams formed of
nodes on the path. Furthermore, we record some sur-
face properties of candidates, namely; lemma, part-
of-speech, direction and distance from cue, and po-
sition in scope. Finally, we record the lemma and
part-of-speech of the token immediately preceding
the candidate (development testing showed that in-
formation about the token following the candidate
was not beneficial).
Based on the features above we learn an SVM-
based scoring function using the implementation of
ordinal ranking in SVMlight (Joachims, 2002). We
use a linear kernel and empirically tune the regu-
larization parameter C (governing the trade-off be-
tween margin size and errors).
5.2 Results
Similarly to the learning curve shown above for
factuality detection, Figure 3 plots the F1 of event
detection on CDDEV when providing increasing
amounts of training data and using gold standard in-
formation on factuality. (Note that, except for end-
to-end results below, all scores reported in this paper
assumes gold negation cues and gold scopes, given
that we want to isolate the performance of the event
ranker and/or factuality classifier.) We see that the
performance is remarkably strong even at 10% of
the total data, and increases steadily until around
60%, at which point it appears to be leveling off.
It is unclear as to whether or not the ranker would
benefit from additional data. We also note differ-
ences with respect to the factuality learning curve
in Figure 1, both in terms of ?entry performance?
and overall trend. To some degree, there are gen-
eral reasons as to why one could expect to see dif-
ferences in learning curves for a discriminative rank-
ing/regression set-up and a classifier set-up (assum-
ing that the class distribution for the latter is unbal-
anced, as is typically the case). For a ranker, ev-
ery item provides useful training data, in the sense
that each item provides both positive and negative
examples (in our case selected from the candidate
tokens within a negation scope). For a classifier, the
few items providing examples of the minority class
 75
 80
 85
 90
 95
 10  100
F1
% of training data
Event predictions
Figure 3: Learning curve showing the effect on F1 for
event detection when using gold factuality and withdraw-
ing portions of the training partitions (shown on a loga-
rithmic scale) across the 10-fold cross-validation cycles.
will typically be the most valuable and it will there-
fore easily be more sensitive to having the training
sample restrained. Even so, it seems clear that the
factuality detection component and event detection
component belong to different ends of the spectrum
in terms of sensitivity to sample size.
Table 4 details the results of using the final rank-
ing model to predict negated events. For a compar-
ative baseline, we implemented a basic ranker that
uses only the candidate lemma as a single feature.
This baseline achieves an F1 of 73.90 (P=74.01,
R=73.80) on CDDEV when using factuality informa-
tion inferred from the gold-standard (and testing by
10-fold cross-validation). For comparison, the full
ranking model achieves an F1 of 90.42 (P=90.75,
R=90.10) on the same data set, as seen in Table 4.
Of course, the results for event detection us-
ing gold-standard factuality also provides the up-
per bound for what we can achieve using system
predicted factuality, i.e., applying the classifier de-
scribed in Section 4. In order to assess the im-
pact of the factuality classifier we also include re-
sults for event detection using the majority-class
baseline, which means simply assuming that all in-
stances of negations are factual. Table 4 lists re-
sults for event detection using system predicted fac-
tuality, compared to results using baseline and gold-
standard factuality. We find that the factuality clas-
sifier greatly improves precision of the event de-
34
Data set Factuality Prec Rec F1
CDDEV
Baseline 62.24 90.10 73.62
Classifier (10-fold) 78.48 82.98 80.67
Gold 90.75 90.10 90.42
CDEVAL
Baseline 58.26 84.94 69.11
Classifier (Held-out) 68.72 80.24 74.03
Gold 84.94 84.94 84.94
Table 4: Results for event detection using various meth-
ods for factuality detection.
tection. As can be expected, however, this comes
with a cost in terms of recall. In both 10-fold
cross-validation on CDDEV and held-out testing on
CDEVAL we find large improvements in F1, corre-
sponding to error reductions of 26.73% and 15.93%
respectively. As expected given the results discussed
in Section 4, the improvement is slightly less pro-
nounced for the held-out test results than the 10-fold
cross-validated development results. Although the
factuality classifier improves substantially over the
baseline, it is also clear that a large gap remains
toward the ?upper bound? results of using gold-
standard factuality. We take the results of the pilot
experiments described in this paper as a proof-of-
concept for using the CD data for training a factual-
ity classifier, and at the same time have high expec-
tations that future experimentation with additional
(syntactically oriented) feature types should be able
to further advance performance considerably.
Building on the system presented in Velldal et al
(2012), the initial *SEM 2012 shared task submis-
sion of Read et al (2012) also included an SVM
negation cue classifier (including support for mor-
phological cues) along with an SVM-based rank-
ing model over syntactic constituents for scope res-
olution. Coupled with the components for factual-
ity and event detection described above, the end-to-
end result for this system on CDEVAL for identify-
ing negated events is F1=67.02 (P=60.58, R=75.00),
making it the top-ranked submission in the shared
task.
6 Conclusions and Future Directions
This paper has demonstrated that a classifier for
discriminating between factuality and non-factuality
can be trained by taking advantage of implicit in-
formation on factuality found in the negation an-
notations of the Conan Doyle corpus (Morante and
Daelemans, 2012). Even though the pilot experi-
ments described in this paper use just simple lex-
ical features, the factuality classifier provides sub-
stantial improvements over the majority-class base-
line. We also present a system for detecting negated
events by learning an SVM-based discriminative
ranking function over candidate tokens within the
negation scope. We show that the factuality classi-
fier proves very useful for improving the precision
of event detection. In order to isolate the perfor-
mance of the event ranker and factuality classifier we
have focused on results for gold negation cues and
scopes in this paper, although end-to-end results for
the full system presented by Read et al (2012) are
also included. The system obtained the best results
for identifying negative factual events in the 2012
*SEM shared task.
It is worth noting that there is nothing inherently
negation specific about our factuality detection ap-
proach per se, save for how the training data happens
to be extracted in the current study. One reason for
using the implicit factuality information in the Co-
nan Doyle negation corpus is the advantage of get-
ting in-domain data, and this also allowed us to stay
within the confines of the closed track for the *SEM
shared task. For future experiments, however, we
would also like to test cross-domain portability by
both training and testing the factuality classifier us-
ing other annotated data sets such as FactBank, and
also add features that incorporate predictions from
speculation cue classifiers trained on BioScope.
Acknowledgments
We want to thank Roser Morante and Eduardo
Blanco for their effort in organizing the *SEM 2012
shared task and providing the annotations. We also
want to thank our colleagues at the University of
Oslo (UiO), in particular Lilja ?vrelid and Stephan
Oepen who contributed to the shared task submis-
sion. Large-scale experimentation was facilitated by
the TITAN HPC cluster at UiO. We also thank the
anonymous reviewers for their valuable comments
and suggestions.
35
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the Forty-Third Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, MI.
Mona T. Diab, Lori S. Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop (LAW 2009), pages 68?73, Singapore.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Natural Language Learning,
pages 1?12, Uppsala.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing, pages 41?56. MIT Press, Cambridge, MA.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the Eighth
ACM International Conference on Knowledge Discov-
ery and Data Mining, Alberta.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biologi-
cal event extraction. In Proceedings of the BioNLP
Shared Task 2011, pages 173?182, Portland, OR.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics, Montreal.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational Linguistics, 38(2):1?38.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based resolution of in-sentence
scope of hedge cues. In Proceedings of the 14th Con-
ference on Natural Language Learning, pages 40?47,
Uppsala.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Vinodkumar Prabhakaran, Owen Rambow, and Mona T.
Diab. 2010. Automatic committed belief tagging. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1014?1022, Beijing.
Jonathon Read, Erik Velldal, Lilja ?vrelid, and Stephan
Oepen. 2012. UiO1: Constituent-based discrimina-
tive ranking for negation resolution. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal. Submission under review.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Saur?? and James Pustejovsky. 2012. Are you sure
that this happened? assessing the factuality degree of
events in text. Computational Linguistics, 38(2).
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text. In
Proceedings of the 14th Conference on Natural Lan-
guage Learning, pages 13?17, Uppsala.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: Biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 (Suppl. 11).
36
