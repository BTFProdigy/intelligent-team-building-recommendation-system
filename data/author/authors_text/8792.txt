Named Entity Extraction from Noisy Input: Speech and OCR 
David Miller, Scan Boisen, Richard Schwartz, Rebecca Stone, Ralph Weischedel 
BBN Technologies 
70 Fawcett Street 
Cambridge, MA 02138 
dmiller@bbn.com, boisen@bbn.com, schwartz@bbn.com, rwstone@bbn.com, weischedel@bbn.com 
Abstract 
In this paper, we analyze the performance 
of name finding in the context of a variety 
of automatic speech recognition (ASR) 
systems and in the context of one optical 
character recognition (OCR) system. We 
explore the effects of word error rate from 
ASR and OCR, performance as a function 
of the amount of training data, and for 
speech, the effect of out-of-vocabulary 
errors and the loss of punctuation and mixed 
case 
I Introduction 
Information extraction systems have 
traditionally been evaluated on online text with 
relatively few errors in the input. For example, 
this description of the Nominator system 
(Wacholder et al 1997) would apply to several 
other systems: "We chose The Wall Street 
Journal corpus because it follows standard 
stylistic conventions, especially capitalization, 
which is essential for Nominator to work." The 
real-world challenge, however, is pointed out in 
Palmer and Day (1997): "It is also unknown 
how the existing high-scoring systems would 
perform on less well-behaved texts, such as 
single-case texts, non-newswire texts, or text 
obtained via optical character recognition 
(OCR)." 
In this paper we explore how performance 
degrades on noisy input, in particular on 
broadcast news (speech) and on newspaper 
(printed matter). Error rates of automatic 
speech recognizers (ASR) on broadcast news 
are still very high, e.g., 14-28% word error. 
Though character error can be very low for laser 
printer output, word error rates of 20% are 
possible for OCR systems applied to newsprint 
or low-quality printed matter. 
In this paper, we evaluate a learning algorithm, 
a hidden Markov model (HM) ,  for named 
entity extraction applied to human transcripts of 
news, to transcripts without case or punctuation 
(perfect speech output), to errorful ASR output 
and to OCR output. Extracting information from 
noisy sources poses the following challenges, 
which are addressed in the paper. 
? Since speech recognizers do not generate 
mixed case nor punctuation, how much do 
case and punctuation contribute to 
recognizing names in English? (Section 3.) 
Note that these challenges also arise in 
languages without case to signal proper 
nouns (e.g., Chinese, German, Japanese), in 
mono-case English or informal English 
(e.g., emails). 
? How much will performance degrade with 
increasing error in the input? (Section 4.) 
? How does closed vocabulary recognition 
affect information extraction performance? 
(Section 5) 
? For the learning algorithm employed, how 
much training and effort are required? 
(Section 6) 
? How much do lists of names contribute to 
performance? (Section 7) 
316 
2 Algorithms and Data 
2.1 Task Definition and Data 
The named entity (NE) task used for this 
evaluation requires the system to identify all 
named locations, named persons, named 
organizations, dates, times, monetary amounts, 
and percentages. The task definition is given in 
Chinchor, et al (1998). 
For speech recognition, roughly 175 hours of 
news broadcasts (roughly 1.2m words of audio) 
were available from the National Institute for 
Science and Technology (NIST) for training. 
All of that data includes both the audio and a 
manual transcription. The test set consisted of 3 
hours of news (roughly 25k words). 
For the combined OCR/NE system, the OCR 
component was trained on the University of 
Washington English Image Database, which is 
comprised primarily of technical journal 
articles. The NE system was trained separately 
on 690K words of 1993 Wall Street Journal 
(WSJ) data (roughly 1250 articles), including 
development data from the Sixth Message 
Understanding Conference (MUC-6) Named 
Entity evaluation. The test set was 
approximately 20K words of separate WSJ data 
(roughly 45 articles), also taken from the MUC- 
6 data set. Both test and training texts were 
original text (no OCR errors) in mixed case with 
normal punctuation. Printing the on-line text, 
rather than using the original newsprint, 
produced the images for OCR, which were all 
scanned at 600 DPI. 
2.2 Algorithms 
The information extraction system tested is 
IdentiFinder(TM), which has previously been 
detailed in Bikel et al (1997, 1999). In that 
system, an HMM labels each word either with 
one of the desired classes (e.g., person, 
organization, etc.) or with the label NOT-A- 
NAME (to represent "none of the desired 
classes"). The states of the HMM fall into 
regions, one region for each desired class plus 
one for NOT-A-NAME. (See Figure 2-1.) The 
HMM thus has a model of each desired class 
and of the other text. Note that the 
implementation is not confined to the seven 
name classes used in the NE task; the particular 
classes to be recognized can be easily changed 
via a parameter. 
Within each of the regions, we use a statistical 
bigram language model, and emit exactly one 
word upon entering each state. Therefore, the 
number of states in each of the name-class 
regions is equal to the vocabulary size. 
Additionally, there are two special states, the 
START-OF-SENTENCE and END-OF-SENTENCE 
states. In addition to generating the word, states 
may also generate f atures of that word. 
START-OF-SENTENCE END-OF SENTENCE 
Figure 2-1: Pictorial representation of conceptual model 
317 
3 Effect of Textual Clues 
The output of each of the speech recognizers i  
in SNOR (speech normalized orthographic 
representation) format, a format which is largely 
unpunctuated and in all capital letters 
(apostrophes and periods after spoken letters are 
preserved). When a typical NE extraction 
system runs on ordinary English text, it uses 
punctuation and capitalization as features that 
contribute to its decisions. In order to learn how 
much degradation i performance is caused by 
the absence of these features from SNOR 
format, we performed the following experiment. 
We took a corpus that had full punctuation and 
mixed case and preprocessed it to make three 
new versions: one with all upper case letters but 
punctuation preserved, one with original case 
but punctuation marks removed, and one with 
both case and punctuation removed. We then 
partitioned all four versions of the corpus into a 
training set and a held-out est set, using the 
same partition in all four versions, and 
measured I entiFinder's performance. 
The corpus we used for this experiment was the 
transcriptions of the second 100 hours of the 
Broadcast News acoustic modelling data, 
comprising 114 episodes. We partitioned this 
data to form a training set of 98 episodes 
(640,000 words) and a test set of 16 episodes 
(130,000 words). Because the test transcriptions 
were created by humans, they have a 0% word 
error rate. The results are shown in Table 3-1. 
The removal of case information has the greater 
effect, reducing performance by 2.3 points, 
while the loss of punctuation reduces 
performance by 1.4 points. The loss from 
removing both features is 3.4 points, less than 
the sum of the individual degradations. This 
suggests that there are some events where both 
mixed case and punctuation are required to lead 
IdentiFinder to the correct answer. 
Mixed Upper 
Case Case 
With punctuation 92.4 90.1 
Without punctuation 91.0 89.0 
Table 3-1: Effect of case and punctuation on 
performance(F-measure) on Broadcast News 
data 
It should be noted that because the data are 
transcriptions of speech, no version of the 
corpus contains all the textual clues that would 
appear in newspaper text like the MUC-7 New 
York Times data. In particular, numbers are 
written out in words as they would be spoken, 
not represented using digits, and abbreviations 
such as "Dr.", "Jr." or "Sept." are expanded out 
to their full spoken word. We conclude that the 
degradation in performance going from 
newspaper text to SNOR recognizer output is at 
least 3.4 points in the 0% WER case, and 
probably more due to these other missing text 
clues. 
4 Effect of Word Errors 
4.1 Optical Character 
(OCR) 
Recognition 
The OCR experiments were performed using the 
system described in Makhoul et al (1998). 
Recognition was performed at the character 
level, rather than the word level, so the 
vocabulary is not closed (unlike the ASR results 
discussed in subsequent sections). Figure 4-1 
shows IdentiFinder's performance under 4 
conditions of varying word error ate (WER): 
1. Original text (no OCR, 0% WER) 
2. OCR from high-quality (laser-printed) text 
images (2.7% WER) 
3. OCR on degraded images (13.7% WER). 
4." OCR on degraded images, processed with a 
weak character language model (19.1% 
WER) 
For the second and third conditions, 1.3M 
characters of Wall Street Journal were used for 
318 
OCR language model training: the fourth 
condition used a much weaker character 
language model, which accounts for the poorer 
performance. 
The interpolated line has been fit to the 
performance of the OCR-based systems, with a 
slope indicating 0.6 points of F-measure lost for 
each percentage point increase in word error. 
The line has been extrapolated to 0% WER: the 
actual 0% WER condition is 95.4, which only 
slightly exceeds the projected value. 
100 
95 ~ 
75 
7O 
5 10 15 20 25 30 
Word Error Rate 
Figure 4-1: IdentiFinder Named Entity 
performance as a function of OCR word 
error rate 
4.2 Automatic Speech Recognition 
(ASR) 
Figure 5-1 shows IdentiFinder's performance on 
all speech systems in the 1998 Hub-4 
evaluations (Przybocki, et al, 1999). These 
experiments were run in co-operation with 
NIST. The interpolated line has been fit to the 
errorful transcripts, and then extrapolated out to 
0% WER speech. As can be seen, the line fits 
the data extremely well, and has a slope of 0.7 
points of F-measure lost for each additional 1% 
of word error rate. The fact that the extrapolated 
' These figures do not reflect the best possible 
performance of the OCR system: for example, when 
testing on degraded ata, it would be usual to include 
representative data in training. This was not a 
concern for this experiment, however, which focussed 
on name finding performance. 
line slightly overestimates the actual 
performance at 0% WER (given by a A) 
indicates that the degradation may be sub-linear 
in the range 0-15% WER. 
leO 
95 
9o 
i= ,E 
~- 80 
75 
B 
E i i 
0 5 10 15 20 25 
Wo~ error r~e (Hub4 E~ 9e) 
Figure 4-2: IdentiFinder named-entity 
performance as a function of word error rate 
(in cooperation with NIST) 
5 Out of Vocabulary Rates for Names 
It is generally agreed that out-of-vocabulary 
(OOV) words do not have a major impact on the 
word error rate achieved by large vocabulary 
speech recognizers doing transcription. The 
reason is that speech lexicons are designed to 
include the most frequent words, thus ensuring 
that OOV words will represent only a small 
fraction of the words in any test set. However, 
we have seen that the ,OOV rate for words that 
are part of named-entities can be as much as a 
factor of ten greater than the baseline OOV for 
non-name words. This could make OOV a 
major problem for NE extraction from speech. 
To explore this, we measured the percentage of
names in the Broadcast News data that contain 
at least one OOV word as a function of lexicon 
size. For this purpose, we built lexicons simply 
by ordering the words of the 1998 Hub-4 
Language Modeling data according to 
30 
319 
Name Category Lexicon Size 
5K 10K 20K 40K 60K 80K 100K 120K 
PERSON 
ORGANIZATION 
LOCATION 
TIME 
MONEY 
DATE 
PERCENT 
34.7 52.7 69.9 85.1 89.4 91.1 91.9 93.9 
73.2 90.2 94.2 97.5 98.2 98.5 98.7 98.8 
76.6 87.1 92.2 96.2 97.5 98.0 98.8 99.1 
97.0 97.0 99.0 100 100 100 100 100 
94.4 98.2 98.8 100 100 100 100 100 
96.1 99.3 99.8 100 100 100 100 100 
98.9 99.3 I00 100 100 100 100 100 
Table 5-1: Percentage of in-vocabulary events as a function of lexicon size. 
frequency, and truncating the list at various 
lengths. The percentage of in-vocabulary events 
of each type as a function of lexicon size is 
shown in Table 5-1. 
Most modem speech recognizers employ a 
vocabulary of roughly 60,000 words; using a 
larger lexicon introduces more errors from 
acoustic perplexity than it fixes through 
enlarged vocabulary. It is clear from the table 
that the only name category that might suffer a 
significant OOV problem with a 60K 
vocabulary is PERSONs. One might imagine 
that a more carefully constructed lexicon could 
reduce the OOV rate for PERSONs while still 
staying within the 60,000 word limit. However, 
even if a cleverly designed 60K lexicon 
succeeded in having the name coverage of the 
frequency-ordered 120K word lexicon (which 
contains roughly 40,000 more proper names 
than the 60K lexicon), it would reduce the 
PERSON OOV rate by only 4% absolute. 
6 Effect of training set size 
6.1 Automatic Speech Recognition 
We have measured NE performance in the 
context of speech as a function of training set 
size and found that the performance increases 
logarithmically with the amount of training data 
for 15% WER test data as well as for 0% WER 
input. However the growth rate is slower for 
15% WER test data. We constructed small 
training sets of various size by randomly 
selecting sets of 6, 12, 25, and 49 episodes from 
the second 100 hours of annotated Broadcast 
News training data. We also defined a training 
set of 98 episodes from the second 100 hours, as 
well as sets containing the full 98 episodes plus 
some or all of the first 100 hours of Broadcast 
News training. Our largest training set contained 
1.2 million words, and our smallest a mere 
30,000 words. All training data were converted 
to SNOR format. 
Given that PERSONs account for roughly 50% 
of the named-entities in broadcast news, the 
maximum gain in F measure available for 
doubling the lexicon size is 2 points. Moreover, 
this gain would require that every PERSON 
name added to the vocabulary be recognized 
properly -- an unlikely prospect, since most of 
these words will not appear in the acoustic 
training for the recognizer. For these reasons, 
we conclude that the OOV problem is not a 
major factor in determining NE performance 
from speech. 
For each training set, we trained a separate 
IdentiFinder model and evaluated it on two 
versions of the 1998 Hub4-IE data -- the 0% 
WER transcription created by a human, and an 
ASR transcript with 15%. The results are 
plotted in Figure 6-1. The slopes of the 
interpolated lines predict that IdentiFinder's 
performance on 15% WER speech will increase 
by 1.5 points for each additional doubling of the 
training data, while performance goes up 1.8 
points per doubling of the training for perfect 
speech input. 
320 
100 
95 
0 L .  
$ 
E 
90 
85 
80 
75 
70 
v . - -  " 
? e. 
? 15% WER 
? 0% WER 
i \[ 
10000 100000 1000000 10000000 
Number of training words (log scale) 
Figure 6-1: Performance as a 
Possibly, the difference in slope of the two lines 
is that the real value of increasing the training 
set lies in increasing the number of distinct rare 
names that appear. Once an example is in the 
training, IdentiFinder is able to extract it and use 
it in test. However, when the test data is 
recognizer output, the rare names are less likely 
to appear in the test, either because they don't 
appear in the speech lexicon or they are poorly 
trained in the speech model and misrecognized. 
If they don't appear in the test, IdentiFinder can't 
make full use of the additional training, and thus 
performance on errorful input increases more 
slowly than it does on error-free input text. 
6.2 Optical Character Recognition 
A similar relationship between training size and 
performance is seen for the OCR test condition. 
function of training data for speech. 
The training was partitioned by documents into 
equal sized sets: 
Partition size Training Size 
Eighth 77.5 K words 
Quarter 155 K words 
Half 310 K words 
Whole 620 K words 
Using the same test set, each partition was used 
to train a separate model, which was then 
evaluated on the different word error conditions: 
performance was then averaged across each 
partition size to produce the data points below. 
Input Word Error Rate (WER) Eighth Quarter Half Whole 
0% WER (Original text) 92.4 93.7 94.3 95.3 
2.7% WER 90.0 90.8 91.6 92.5 
13.7% WER 84.3 85.2 86.0 86.6 
19.1% WER 79.6 80.4 80.8 82.5 
321 
100 
95 
9O 
8O 
75 
70 
10000 
Performance as a function of training size 
n - - ' - ' '~ -~ ' '~  .....m ..--B--'-- 
A0% WER 
\[\] 2.7% WER 
X 13.7% WER 
O19.1% WER 
100000 1000000 
Number of training words (log scala) 
Figure 6-2: Performance as a function of training data for OCR. 
While this graph of this data in Figure 6-2 
shows a logarithmic improvement, as with the 
ASR experiments, the rate of improvement is
substantially less, roughly 0.9 increase in F- 
measure for doubling the training data. This 
may be explained by the difference in difficulty 
between the two tests: even with only 77.5k 
words of training, the 0% WER performance 
exceeds the ASR system trained on 1.2M words. 
full point, while on recognizer produced output, 
performance goes u~p by only 0.3 points. 
/ 0% WER 15% WER 
Without lists 89.5 81.9 
With lists 90.5 82.2 
Table 7-1: Effect of lists in the presence of 
speech errors. 
8 Related Work  and Future Work 
7 Effect of Lists 
Like most NE extraction systems, IdentiFinder 
can use lists of strings of known to be names to 
estimate the probability that a word will be a 
name, given that it appears on a particular list. 
We trained two models on 1.2 million words of 
SNOR data, one with lists and one without. We 
tested on the human transcription (0% WER) 
and the ASR (15% WER) versions of the 1998 
evaluation transcripts. Table 7-1 shows the 
results. We see that on human constructed 
transcripts, lists improve the performance by a 
To our knowledge, no other information 
extraction technology has been applied to OCR 
material. 
For audio materials, three related efforts were 
benchmarked on NE extraction from broadcast 
news. Palmer, et al (1999) employs an HMM 
very similar to that reported for IdentifFinder 
(Bikel et al, 1997,1999). Renals et al (1999) 
reports on a rule-based system and an HMM 
integrated with a speech recognizer. Appelt and 
Martin (1999) report on the TEXTPRO system, 
which recognises names using manually written 
finite state sales. 
322 
Of these, the Palmer system and TEXTPRO 
report results on five different word error rates. 
Both degrade linearly, about .7F, with each 1% 
increase in WER from ASR. None report the 
effect of training set size, capitalization, 
punctuation, or out-of-vocabulary items. 
Of the four systems, IdentiFinder epresents 
state-of-the-art performance. Of all the systems 
evaluated, those with the simple architecture of 
ASR followed by information extraction 
performed markedly better than the system 
where extraction was more integrated with 
ASR. 
In general, these results compare favorably with 
results reported in the Message Understanding 
Conference (Chinchor, et al, 1998). The 
highest NE score in MUC-7 was 93.39; for 0% 
WER, our best score was 90.5 without case and 
punctuation which costs about 3.4 points. 
9 Conclusions 
First and foremost, the hidden Markov model is 
quite robust in the face of errorful input. 
Performance on both speech and OCR input 
degrades linearly as a function of word error. 
Even, without case information or punctuation 
in the input, the performance on the broadcast 
news task is above 90%, with only a 3.4 point 
degradation in performance due to missing 
textual clues. Performance even with 15% word 
error degrades by only about 8 points of F for 
both OCR and ASR systems. 
Second, because annotation can be performed 
quickly and inexpensively by non-experts, 
training-based systems like IdentiFinder offer a 
powerful advantage in moving to new languages 
and new domains. In our experience, annotation 
of English typically proceeds at 5k words per 
hour or more. This means interesting 
performance an be achieved with as little as 20 
hours of student annotation (i.e., at least 100k 
words). Increasing training continually 
improves performance, generally as the 
logarithm of the training set size. On 
transcribed speech, performance is already good 
(89.3 on 0% WER) with only 100 hours or 
643K words of training data. 
Third, though errors due to words out of the 
vocabulary of the speech recognizer are a 
problem, they represent only about 15% of the 
errors made by the combined speech recognition 
and named entity system. 
Fourth, we used exactly the same training data, 
modeling, and search algorithm for errorful 
input as we do for error-free input. For OCR, 
we trained on correct newswire once only for 
both correct text input 0% (WER) and for a 
variety of errorful text input conditions. For 
speech, we simply transformed text training data 
into SNOR format and retrained. Using this 
approach, the only cost of handling errorful 
input from OCR or ASR was a small amount of 
computing time. There were no rules to rewrite, 
no lists to change, and no vocabulary 
adjustments. Even so, the degradation in 
performance on errorful input is no worse than 
the word error rate of the OCR/ASR system. 
Acknowledgments 
The work reported here was supported in part by 
the Defense Advanced Research Projects 
Agency. Technical agent for part of this work 
was NRaD under contract number N66001-97- 
D-8501. The views and conclusions contained 
in this document are those of the authors and 
should not be interpreted as necessarily 
representing the official policies, either 
expressed or implied, of the Defense Advanced 
Research Projects Agency or the United States 
Government. 
References 
D. E. Appelt, D. Martin, "Named Entity Extraction 
from Speech: Approach and Results Using the 
Text.Pro System," Proceedings Of The DARPA 
Broadcast News Workshop, February 28-March 3, 
Morgan Kaufmann Publishers, pp 51-54 (1999). 
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel, 
'Nymble: a High-Performance L arning Name- 
finder". In Fifth Conference on Applied Natural 
Language Processing, (published by ACL) pp 194- 
201 (1997). 
323 
D. Bikel, R. Schwartz, and R. Weischedel, "An 
Algorithm that Learns What's in a Name," 
Machine Learning 34, pp 211-231, (1999). 
N. Chinchor, "MUC-7 Named Entity Task Definition 
Version 3.5". Available by ftp from 
ftp.muc.saic.com/pub/MUC/MUC7-guidelines. 
(1997). 
N. Chincor, P. Robinson, E. Brown, "HUB-4 Named 
Entity Task Definition Version 4.8". Available by 
ftp from www.nist.gov/speech/hub4_98. (1998). 
J. Makhoul, R. Schwartz, C. Lapre, and I. Bazzi, "A 
Script-Independent Methodology for Optical 
Character Recognition,", Pattern Recognition, pp 
1285-1294 (1998). 
Z. Lu, R. Schwartz, P. Natarajan, I. Bazzi, J. 
Makhoul, "Advances in the BBN BYBLOS OCR 
System," Proceedings of the International 
Conference on Document Analysis and 
Recognition, (1999). 
D. D. Palmer, J. D. Burger, M. Ostendorf, 
"Information Extraction from Broadcast News 
Speech Data," Proceedings Of The DARPA 
Broadcast News Workshop, February 28-March 3, 
Morgan Kaufmann Publishers, pp 41-46 (1999). 
M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, D. S. 
Pallett, "1998 Hub-4 Information Extraction 
Evaluation," Proceedings Of The DARPA 
Broadcast News Workshop, February 28-March 3, 
Morgan Kaufmann Publishers, pp 13-18 (1999). 
S. Renals, Y. Gotoh, R. Gaizauskas, M. Stevenson, 
"Baseline IE-NE Experiments Using the 
SPRACH/LASIE System," Proceedings Of The 
DARPA Broadcast News Workshop, February 28- 
March 3, Morgan Kaufmann Publishers, pp 47-50 
(1999). 
324 
A Novel Use of Statistical Parsing to Extract Information from 
Text 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel 
BBN Technologies 
70 Fawcett Street, Cambridge, MA 02138 
szmiller@bbn.com 
Abstract 
Since 1995, a few statistical parsing 
algorithms have demonstrated a 
breakthrough in parsing accuracy, as 
measured against the UPenn TREEBANK 
as a gold standard. In this paper we report 
adapting a lexicalized, probabilistic 
context-free parser to information 
extraction and evaluate this new technique 
on MUC-7 template elements and template 
relations. 
1 Introduction 
Since 1995, a few statistical parsing 
algorithms (Magerman, 1995; Collins, 1996 
and 1997; Charniak, 1997; Rathnaparki, 1997) 
demonstrated a breakthrough in parsing 
accuracy, as measured against he University 
of Pennsylvania TREEBANK as a gold 
standard. Yet, relatively few have embedded 
one of these algorithms in a task. Chiba, 
(1999) was able to use such a parsing 
algorithm to reduce perplexity with the long 
term goal of improved speech recognition. 
In this paper, we report adapting a lexicalized, 
probabilistic context-free parser with head 
rules (LPCFG-HR) to information extraction. 
The technique was benchmarked in the 
Seventh Message Understanding Conference 
(MUC-7) in 1998. 
Several technical challenges confronted us and 
were solved: 
? How could the limited semantic 
interpretation required in information 
extraction be integrated into the statistical 
learning algorithm? We were able to integrate 
both syntactic and semantic information into 
the parsing process, thus avoiding potential 
errors of syntax first followed by semantics. 
? Would TREEBANKing of the variety of 
news sources in MUC-7 be required? Or 
could the University of Pennsylvania's 
TREEBANK on Wall Street Journal 
adequately train the algorithm for New York 
Times newswire, which includes dozens of 
newspapers? Manually creating source- 
specific training data for syntax was not 
required. Instead, our parsing algorithm, 
trained on the UPenn TREEBANK, was run 
on the New York Times source to create 
unsupervised syntactic training which was 
constrained to be consistent with semantic 
annotation. 
* Would semantic annotation require 
computational linguists? We were able to 
specify relatively simple guidelines that 
students with no training in computational 
linguistics could annotate. 
2 Information Extraction Tasks 
We evaluated the new approach to information 
extraction on two of the tasks of the Seventh 
Message Understanding Conference (MUC-7) 
and reported in (Marsh, 1998). The Template 
Element (TE) task identifies organizations, 
persons, locations, and some artifacts (rocket 
and airplane-related artifacts). For each 
organization i an article, one must identify all 
of its names as used in the article, its type 
(corporation, government, or other), and any 
significant description of it. For each person, 
one must find all of the person's names within 
the document, his/her type (civilian or 
military), and any significant descriptions 
(e.g., titles). For each location, one must also 
give its type (city, province, county, body of 
water, etc.). For the following example, the 
226 
template element in  Figure I was to be 
generated: "...according to the report by 
Edwin Dorn, under secretary of defense for 
personnel and readiness . . . .  Dorn's conclusion 
that Washington..." 
<ENTITY-9601020516-13> := 
ENT_NAME: "Edwin Dorn" 
"Dorn" 
ENT_TYPE: PERSON 
ENT_DESCRIPTOR: "under secretary of 
defense for personnel and readiness" 
ENT_CATEGORY: PER_CIV 
Figure 1: An example of the information to be 
extracted for TE. 
The Template Relations (TR) task involves 
identifying instances of three relations in the 
text: 
? the products made by each company 
? the employees ofeach organization, 
? the (headquarters) location of each 
organization. 
TR builds on TE in that TR reports binary 
relations between elements of TE. For the 
following example, the template relation in 
Figure 2 was to be generated: "Donald M. 
Goldstein, a historian at the University of 
Pittsburgh who helped write..." 
<EMPLOYEE_OF-9601020516-5> := 
PERSON: <ENTITY-9601020516-18> 
ORGANIZATION: <ENTITY- 
9601020516-9> 
<ENTITY-9601020516-9> := 
ENT_NAME: "University of Pittsburgh" 
ENT_TYPE: ORGANIZATION 
ENT_CATEGORY: ORG_CO 
<ENTITY-9601020516-18> := 
ENT_NAME: "Donald M. Goldstein" 
ENT_TYPE: PERSON 
ENT_DESCRIPTOR: "a historian at the 
University of Pittsburgh" 
Figure 2: An example of information to be 
extracted for TR 
3 Integrated Sentential Processing 
Almost all approaches to information 
extraction - even at the sentence level - are 
based on the divide-and-conquer st ategy of 
reducing acomplex problem to a set of simpler 
ones. Currently, the prevailing architecture for 
dividing sentential processing is a four-stage 
pipeline consisting of: 
1. part-of-speech tagging 
2. name finding 
3. syntactic analysis, often limited to noun 
and verb group chunking 
4. semantic interpretation, usually based on 
pattern matching 
Since we were interested in exploiting recent 
advances in parsing, replacing the syntactic 
analysis tage of the standard pipeline with a 
modem statistical parser was an obvious 
possibility. However, pipelined architectures 
suffer from a serious disadvantage: rrors 
accumulate as they propagate through the 
pipeline. For example, an error made during 
part-of-speech-tagging may cause a future 
error in syntactic analysis, which may in turn 
cause a semantic interpretation failure. There 
is no opportunity for a later stage, such as 
parsing, to influence or correct an earlier stage 
such as part-of-speech tagging. 
An integrated model can limit the propagation 
of errors by making all decisions jointly. For 
this reason, we focused on designing an 
integrated model in which tagging, name- 
finding, parsing, and semantic interpretation 
decisions all have the opportunity to mutually 
influence ach other. 
A second consideration influenced our 
decision toward an integrated model. We were 
already using a generative statistical model for 
part-of-speech tagging (Weischedel et al 
1993), and more recently, had begun using a 
generative statistical model for name finding 
(Bikel et al 1997). Finally, our newly 
constructed parser, like that of (Collins 1997), 
was based on a generative statistical model. 
Thus, each component of what would be the 
first three stages of our pipeline was based on 
227 
the same general class of statistical model. 
Although each model differed in its detailed 
probability structure, we believed that the 
essential elements of all three models could be 
generalized in a single probability model. 
If the single generalized model could then be 
extended to semantic anal);sis, all necessary 
sentence level processing would be contained 
in that model. Because generative statistical 
models had already proven successful for each 
of the first three stages, we were optimistic 
that some of their properties - especially their 
ability to learn from large amounts of data, and 
their robustness when presented with 
unexpected inputs - would also benefit 
semantic analysis. 
4 Representing Syntax and Semantics 
Jointly 
Our integrated model represents yntax and 
semantics jointly using augmented parse trees. 
In these trees, the standard TREEBANK 
structures are augmented to convey semantic 
information, that is, entities and relations. An 
example of an augmented parse tree is shown 
in Figure 3. The five key facts in this example 
are: 
? "Nance" is the name of a person. 
? "A paid consultant to ABC News" 
describes a person. 
t "ABC News" is the name of an 
organization. 
? The person described as "a paid consultant 
to ABC News" is employed by ABC News. 
? The person named "Nance" and the person 
described as "a paid consultant to ABC News" 
are the same person. 
Here, each "reportable" name or description is 
identified by a "-r" suffix attached to its 
semantic label. For example, "per-r" identifies 
"Nance" as a named person, and "per-desc-r" 
identifies "a paid consultant to ABC News" as 
a person description. Other labels indicate 
relations among entities. For example, the co- 
reference relation between "Nance" and "a 
paid consultant to ABC News" is indicated by 
"per-desc-of." In this case, because the 
argument does not connect directly to the 
relation, the intervening nodes are labeled with 
semantics "-ptr" to indicate the connection. 
Further details are discussed in the section 
Tree Augmentation. 
5 Creating the Training Data 
To train our integrated model, we required a 
large corpus of augmented parse trees. Since it 
was known that the MUC-7 evaluation data 
would be drawn from a variety of newswire 
sources, and that the articles would focus on 
rocket launches, it was important that our 
training corpus be drawn from similar sources 
and that it cover similar events. Thus, we did 
not consider simply adding semantic labels to 
the existing Penn TREEBANK, which is 
drawn from a single source - the Wall Street 
Journal - and is impoverished in articles about 
rocket launches. 
Instead, we applied an information retrieval 
system to select a large number of articles 
from the desired sources, yielding a corpus 
rich in the desired types of events. The 
retrieved articles would then be annotated with 
augmented tree structures to serve as a training 
corpus. 
Initially, we tried to annotate the training 
corpus by hand marking, for each sentence, the 
entire augmented tree. It soon became 
painfully obvious that this task could not be 
performed in the available time. Our 
annotation staff found syntactic analysis 
particularly complex and slow going. By 
necessity, we adopted the strategy of hand 
marking only the semantics. 
Figure 4 shows an example of the semantic 
annotation, which was the only type of manual 
annotation we performed. 
To produce a corpus of augmented parse trees, 
we used the following multi-step training 
procedure which exploited the Penn 
TREEBANK 
228 
S 
per/np vp 
per-r/np 
I 
per/nnp 
I 
Nance , who is also a paid consultant to 
/ ~~-~1~p \ /// \ 
/ / I / o rg~ \ 
, wp vbz rb det vbn per-desc/nn to org'/nnporg/nnp , vbd 
I I I I I I I I I I I I 
ABe News , said ... 
Figure 3: An example of an augmented parse tree. 
1. The model (see Section 7) was first trained 
on purely syntactic parse trees from the 
TREEBANK, producing a model capable 
of broad-coverage syntactic parsing. 
parses that were consistent with the 
semantic annotation. A parse was 
considered consistent if no syntactic 
constituents crossed an annotated entity or 
description boundary. 
2. Next, for each sentence in the semantically 
annotated corpus: 
a. The model was applied to parse the 
sentence, constrained to produce only 
b. The resulting parse tree was then 
augmented to reflect semantic structure in 
addition to syntactic structure. 
/ 
F.?rso?l 
Nance 
coreference ~ employee  .ation 
person-descriptor -. 
Iorganization 1
, who is also a paid consultant to ABC News said ... 
Figure 4: An example of semantic annotation. 
229 
Applying this procedure yielded a new version 
of the semantically annotated corpus, now 
annotated with complete augmented trees like 
that in Figure 3. 
6 Tree Augmentation 
In this section, we describe the algorithm that 
was used to automatically produce augmented 
trees, starting with a) human-generated 
semantic annotations and b) machine- 
generated syntactic parse trees. For each 
sentence, combining these two sources 
involved five steps. These steps are given 
below: 
Tree Augmentation Algorithm 
. Nodes are inserted into the parse tree to 
distinguish names and descriptors that are 
not bracketed in the parse. For example, 
the parser produces a single noun phrase 
with no internal structure for "Lt. Cmdr. 
David Edwin Lewis". Additional nodes 
must be inserted to distinguish the 
description, "Lt. Cmdr.," and the name, 
"David Edwin Lewis." 
. Semantic labels are attached to all nodes 
that correspond to names or descriptors. 
These labels reflect he entity type, such as 
person, organization, or location, as well 
as whether the node is a proper name or a 
descriptor. 
. For relations between entities, where one 
entity is not a syntactic modifier of the 
other, the lowermost parse node that spans 
both entities is identified. A semantic tag 
is then added to that node denoting the 
relationship. For example, in the sentence 
"Mary Fackler Schiavo is the inspector 
general of the U.S. Department of 
Transportation," a co-reference semantic 
label is added to the S node spanning the 
name, "Mary Fackler Schiavo," and the 
descriptor, "the inspector general of the 
U.S. Department of Transportation." 
. Nodes are inserted into the parse tree to 
distinguish the arguments to each relation. 
In cases where there is a relation between 
two entities, and one of the entities is a 
syntactic modifier of the other, the inserted 
node serves to indicate the relation as well 
as the argument. For example, in the 
phrase "Lt. Cmdr. David Edwin Lewis," a 
node is inserted to indicate that "Lt. 
Cmdr." is a descriptor for "David Edwin 
Lewis." 
. Whenever a relation involves an entity that 
is not a direct descendant of that relation 
in the parse tree, semantic pointer labels 
are attached to all of the intermediate 
nodes. These labels serve to form a 
continuous chain between the relation and 
its argument. 
7 Model Structure 
In our statistical model, trees are generated 
according to a process imilar to that described 
in (Collins 1996, 1997). The detailed 
probability structure differs, however, in that it 
was designed to jointly perform part-of-speech 
tagging, name finding, syntactic parsing, and 
relation finding in a single process. 
For each constituent, the head is generated 
first, followed by the modifiers, which are 
generated from the head outward. Head 
words, along with their part-of-speech tags and 
features, are generated for each modifier as 
soon as the modifier is created. Word features 
are introduced primarily to help with unknown 
words, as in (Weischedel et al 1993). 
We illustrate the generation process by 
walking through a few of the steps of the parse 
shown in Figure 3. At each step in the 
process, a choice is made from a statistical 
distribution, with the probability of each 
possible selection dependent on particular 
features of previously generated elements. We 
pick up the derivation just after the topmost S 
and its head word, said, have been produced. 
The next steps are to generate in order: 
1. A head constituent for the S, in this case a 
VP. 
2. Pre-modifier constituents for the S. In this 
case, there is only one: a PER/NP. 
3. A head part-of-speech tag for the PER/NP, 
in this case PER/NNP. 
230 
4. A head word for the PER/NP, in this case 
nance. 
5. Word features for the head word of the 
PER/NP, in this case capitalized. 
6. A head constituent for the PER/NP, in this 
case a PER-R/NP. 
7. Pre-modifier constituents for the PER/NP. 
In this case, there are none. 
. Post-modifier constituents for the 
PER/NP. First a comma, then an SBAR 
structure, and then a second comma are 
each generated in turn. 
This generation process is continued until the 
entire tree has been produced. 
We now briefly summarize the probability 
structure of the model. The categories for 
head constituents, ch, are predicted based 
solely on the category of the parent node, cp: 
e(c h Icp), e.g. P(vpls )
Modifier constituent categories, Cm, are 
predicted based on their parent node, cp, the 
head constituent of their parent node, Chp, the 
previously generated modifier, Cm-1, and the 
head word of their parent, wp. Separate 
probabilities are maintained for left (pre) and 
right (post) modifiers: 
PL (Cm I Cp,Chp,Cm_l,Wp), e.g. 
PL ( per I np I s, vp, null, said) 
PR(c~ I Ce,Ch~,Cm-l, Wp), e.g. 
PR(null \[ s, vp, null, said) 
Part-of-speech tags, tin, for modifiers are 
predicted based on the modifier, Cm, the part- 
of-speech tag of the head word, th, and the 
head word itself, wh: 
P(t m ICm,th,wh), e.g. 
P(per / nnp \[ per /np, vbd, said) 
Head words, win, for modifiers are predicted 
based on the modifier, cm, the part-of-speech 
tag of the modifier word , t,,, the part-of- 
speech tag of the head word, th, and the head 
word itself, Wh: 
P(W m ICm,tmth,Wh),  e.g. 
P(nance I per / np, per / nnp, vbd, said) 
Finally, word features, fro, for modifiers are 
predicted based on the modifier, cm, the part- 
of-speech tag of the modifier word , tin, the 
part-of-speech tag of the head word , th, the 
head word itself, Wh, and whether or not the 
modifier head word, w,,, is known or unknown. 
P(fm \[Cm,tm,th,Wh,known(Wm)), e.g. 
P( cap I per I np, per / nnp, vbd, said, true) 
The probability of a complete tree is the 
product of the probabilities of generating each 
element in the tree. If we generalize the tree 
components (constituent labels, words, tags, 
etc.) and treat them all as simply elements, e, 
and treat all the conditioning factors as the 
history, h, we can write: 
P(tree) = H e(e I h) 
e~tree 
8 Training the Model 
Maximum likelihood estimates for the model 
probabilities can be obtained by observing 
frequencies in the training corpus. However, 
because these estimates are too sparse to be 
relied upon, we use interpolated estimates 
consisting of mixtures of successively lower- 
order estimates (as in Placeway et al 1993). 
For modifier constituents, 
components are: 
P'(cm I cp, chp, Cm_ l , w p) = 
21 P(c,, ICp,Chp,C,,_I,W,) 
+22 P(cm I%,chp,Cm_,) 
the mixture 
For part-of-speech tags, 
components are: 
P'(t m ICm,th,Wh)=21 P(t m Icm,wh) 
+'~2 e(tm I cm, th) 
+~3 P(t,, I C~,) 
the mixture 
For head words, the mixture components are: 
P'(w m I Cm,tm,th, wh) = JL 1 P(w m I Cm,tm, Wh) 
+22 P(wm Icm,tm,th) 
+23 P(w m I Cm,t,,) 
+~4 P(w, It,,) 
Finally, for word features, the mixture 
components are: 
231 
P'(f,, \[c,,,t~,t h, w h, known(w,,)) = 
21 P(f,, )c,,,t,,,wh,known(w,,)) 
+)\[2 e(f., \[c~,t,,,th,kn?wn(w,,)) 
+A3 e(L, \[c,,,t ,,known(w,,)) 
+As P(fm \[t,,,known(w,,)) 
9 Searching the Model 
Given a sentence to be analyzed, the search 
program must find the most likely semantic 
and syntactic interpretation. More precisely, it
must find the most likely augmented parse 
tree. Although mathematically the model 
predicts tree elements in a top-down fashion, 
we search the space bottom-up using a chart- 
based search. The search is kept tractable 
through a combination of CKY-style dynamic 
programming and pruning of low probability 
elements. 
9.1 Dynamic Programming 
Whenever two or more constituents are 
equivalent relative to all possible later parsing 
decisions, we apply dynamic programming, 
keeping only the most likely constituent in the 
chart. Two constituents are considered 
equivalent if: 
1. They have identical category labels. 
2. Their head constituents have identical 
labels. 
3. They have the same head word. 
4. Their leftmost modifiers have identical 
labels. 
. Their rightmost modifiers have identical 
labels. 
9.2 Pruning 
Given multiple constituents that cover 
identical spans in the chart, only those 
constituents with probabilities within a 
threshold of the highest scoring constituent are 
maintained; all others are pruned. For 
purposes of pruning, and only for purposes of 
pruning, the prior probability of each 
constituent category is multiplied by the 
generative probability of that constituent 
(Goodman, 1997). We can think of this prior 
probability as an estimate of the probability of 
generating a subtree with the constituent 
category, starting at the topmost node. Thus, 
the scores used in pruning can be considered 
as the product of: 
. The probability of generating a constituent 
of the specified category, starting at the 
topmost node. 
. The probability of generating the structure 
beneath that constituent, having already 
generated a constituent ofthat category. 
Given a new sentence, the outcome of this 
search process is a tree structure that encodes 
both the syntactic and semantic structure of the 
sentence. The semantics - that is, the entities 
and relations - can then be directly extracted 
from these sentential trees. 
10 Experimental Results 
Our system for MUC-7 consisted of the 
sentential model described in this paper, 
coupled with a simple probability model for 
cross-sentence merging. The evaluation 
results are summarized in Table 1. 
In both Template Entity (TE) and Template 
Relation (TR), our system finished in second 
place among all entrants. Nearly all of the 
work was done by the sentential model; 
disabling the cross-sentence model entirely 
reduced our overall F-Score by only 2 points. 
Task Recall Precision 
Entities (TE) 83% 84% 
Relations (TR) 64% 81% 
Table 1:MUC-7 scores. 
F-Score 
83.49% 
71.23% 
232 
Task Score 
Part-of-Speech Tagging 95.99 (% correct) 
Parsing (sentences <40 words) 85.06 (F-Score) 
Name Finding 92.28 (F-Score) 
Table 2: Component task performance. 
While our focus throughout the project was on 
TE and TR, we became curious about how 
well the model did at part-of-speech tagging, 
syntactic parsing, and at name finding. We 
evaluated part-of-speech tagging and parsing 
accuracy on the Wall Street Journal using a 
now standard procedure (see Collins 97), and 
evaluated name finding accuracy on the MUC- 
7 named entity test. The results are 
summarized in Table 2. 
While performance did not quite match the 
best previously reported results for any of 
these three tasks, we were pleased to observe 
that the scores were at or near state-of-the-art 
levels for all cases. 
11 Conclusions 
We have demonstrated, at least for one 
problem, that a lexicalized, probabilistic 
context-free parser with head rules (LPCFG- 
HR) can be used effectively for information 
extraction. A single model proved capable of 
performing all necessary sentential processing, 
both syntactic and semantic. We were able to 
use the Penn TREEBANK to estimate the 
syntactic parameters; no additional syntactic 
training was required. The semantic training 
corpus was produced by students according to 
a simple set of guidelines. This simple 
semantic annotation was the only source of 
task knowledge used to configure the model. 
Acknowledgements 
The work reported here was supported in part 
by the Defense Advanced Research Projects 
Agency. Technical agents for part of this work 
were Fort Huachucha and AFRL under 
contract numbers DABT63-94-C-0062, 
F30602-97-C-0096, and 4132-BBN-001. The 
views and conclusions contained in this 
document are those of the authors and should 
not be interpreted as necessarily representing 
the official policies, either expressed or 
implied, of the Defense Advanced Research 
Projects Agency or the United States 
Government. 
We thank Michael Collins of the University of 
Pennsylvania for his valuable suggestions. 
References 
Bikel, Dan; S. Miller; R. Schwartz; and R. 
Weischedel. (1997) "NYMBLE: A High- 
Performance Learning Name-finder." In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing, Association for 
Computational Linguistics, pp. 194-201. 
Collins, Michael. (1996) "A New Statistical Parser 
Based on Bigram Lexical Dependencies." In
Proceedings of the 34th Annual Meeting of the 
Association for Computational Linguistics, pp. 
184-191. 
Collins, Michael. (1997) "Three Generative, 
Lexicalised Models for Statistical Parsing." In 
Proceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics, pp. 
16-23. 
Marcus, M.; B. Santorini; and M. Marcinkiewicz. 
(1993) "Building a Large Annotated Corpus of 
English: the Penn Treebank." Computational 
Linguistics, 19(2):313-330. 
Goodman, Joshua. (1997) "Global Thresholding 
and Multiple-Pass Parsing." In Proceedings of 
the Second Conference on Empirical Methods in 
Natural Language Processing, Association for 
Computational Linguistics, pp. 11-25. 
Placeway, P., R. Schwartz, et al (1993). "The 
Estimation of Powerful Language Models from 
Small and Large Corpora." IEEE ICASSP 
Weischedel, Ralph; Marie Meteer; Richard 
Schwartz; Lance Ramshaw; and Jeff Palmucci. 
(1993) "Coping with Ambiguity and Unknown 
Words through Probabilistic Models." 
Computational Linguistics, 19(2):359-382. 
233 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72?80,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Effective Use of Linguistic and Contextual Information
for Statistical Machine Translation
Libin Shen and Jinxi Xu and Bing Zhang and
Spyros Matsoukas and Ralph Weischedel
BBN Technologies
Cambridge, MA 02138, USA
{lshen,jxu,bzhang,smatsouk,weisched}@bbn.com
Abstract
Current methods of using lexical features
in machine translation have difficulty in
scaling up to realistic MT tasks due to
a prohibitively large number of parame-
ters involved. In this paper, we propose
methods of using new linguistic and con-
textual features that do not suffer from
this problem and apply them in a state-of-
the-art hierarchical MT system. The fea-
tures used in this work are non-terminal
labels, non-terminal length distribution,
source string context and source depen-
dency LM scores. The effectiveness of
our techniques is demonstrated by signif-
icant improvements over a strong base-
line. On Arabic-to-English translation,
improvements in lower-cased BLEU are
2.0 on NIST MT06 and 1.7 on MT08
newswire data on decoding output. On
Chinese-to-English translation, the im-
provements are 1.0 on MT06 and 0.8 on
MT08 newswire data.
1 Introduction
Linguistic and context features, especially sparse
lexical features, have been widely used in re-
cent machine translation (MT) research. Unfor-
tunately, existing methods of using such features
are not ideal for large-scale, practical translation
tasks.
In this paper, we will propose several prob-
abilistic models to effectively exploit linguistic
and contextual information for MT decoding, and
these new features do not suffer from the scalabil-
ity problem. Our new models are tested on NIST
MT06 and MT08 data, and they provide signifi-
cant improvement over a strong baseline system.
1.1 Previous Work
The ideas of using labels, length preference and
source side context in MT decoding were explored
previously. Broadly speaking, two approaches
were commonly used in existing work.
One is to use a stochastic gradient descent
(SGD) or Perceptron like online learning algo-
rithm to optimize the weights of these features
directly for MT (Shen et al, 2004; Liang et al,
2006; Tillmann and Zhang, 2006). This method is
very attractive, since it opens the door to rich lex-
ical features. However, in order to robustly opti-
mize the feature weights, one has to use a substan-
tially large development set, which results in sig-
nificantly slower tuning. Alternatively, one needs
to carefully select a development set that simulates
the test set to reduce the risk of over-fitting, which
however is not always realistic for practical use.
A remedy is to aggressively limit the feature
space, e.g. to syntactic labels or a small fraction
of the bi-lingual features available, as in (Chiang
et al, 2008; Chiang et al, 2009), but that reduces
the benefit of lexical features. A possible generic
solution is to cluster the lexical features in some
way. However, how to make it work on such a
large space of bi-lingual features is still an open
question.
The other approach is to estimate a single score
or likelihood of a translation with rich features,
for example, with the maximum entropy (Max-
Ent) method as in (Carpuat and Wu, 2007; Itty-
cheriah and Roukos, 2007; He et al, 2008). This
method avoids the over-fitting problem, at the ex-
pense of losing the benefit of discriminative train-
ing of rich features directly for MT. However, the
feature space problem still exists in these pub-
lished models.
He et al (2008) extended the WSD-like ap-
proached proposed in (Carpuat and Wu, 2007) to
hierarchical decoders. In (He et al, 2008), lexical
72
features were limited on each single side due to the
feature space problem. In order to further reduce
the complexity of MaxEnt training, they ?trained
a MaxEnt model for each ambiguous hierarchical
LHS? (left-hand side or source side) of translation
rules. Different target sides were treated as possi-
ble labels. Therefore, the sample sets of each indi-
vidual MaxEnt model were very small, while the
number of features could easily exceed the number
of samples. Furthermore, optimizing individual
MaxEnt models in this way does not lead to global
maximum. In addition, MaxEnt models trained on
small sets are unstable.
The MaxEnt model in (Ittycheriah and Roukos,
2007) was optimized globally, so that it could bet-
ter employ the distribution of the training data.
However, one has to filter the training data ac-
cording to the test data to get competitive perfor-
mance with this model 1. In addition, the filtering
method causes some practical issues. First, such
methods are not suitable for real MT tasks, espe-
cially for applications with streamed input, since
the model has to be retrained with each new input
sentence or document and training is slow. Fur-
thermore, the model is ill-posed. The translation
of a source sentence depends on other source sen-
tences in the same batch with which the MaxEnt
model is trained. If we add one more sentence to
the batch, translations of other sentences may be-
come different due to the change of the MaxEnt
model.
To sum up, the existing models of employing
rich bi-lingual lexical information in MT are im-
perfect. Many of them are not ideal for practical
translation tasks.
1.2 Our Approach
As for our approach, we mainly use simple proba-
bilistic models, i.e. Gaussian and n-gram models,
which are more robust and suitable for large-scale
training of real data, as manifested in state-of-the-
art systems of speech recognition. The unique
contribution of our work is to design effective and
efficient statistical models to capture useful lin-
guistic and context information for MT decoding.
Feature functions defined in this way are robust
and ideal for practical translation tasks.
1According to footnote 2 of (Ittycheriah and Roukos,
2007), test set adaptation by test set sampling of the train-
ing corpus showed an advantage of more than 2 BLEU points
over a general system trained on all data.
1.2.1 Features
In this paper, we will introduce four new linguistic
and contextual feature functions. Here, we first
provide a high-level description of these features.
Details of the features are discussed in Section 2.
The first feature is based on non-terminal labels,
i.e. POS tags of the head words of target non-
terminals in transfer rules. This feature reduces
the ambiguity of translation rules. The other bene-
fit is that POS tags help to weed out bad target side
tree structures, as an enhancement to the target de-
pendency language model.
The second feature is based on the length dis-
tribution of non-terminals. In English as well as
in other languages, the same deep structure can
be represented in different syntactic structures de-
pending on the complexity of its constituents. We
model such preferences by associating each non-
terminal of a transfer rule with a probability distri-
bution over its length. Similar ideas were explored
in (He et al, 2008). However their length features
only provided insignificant improvement of 0.1
BLEU point. A crucial difference of our approach
is how the length preference is modeled. We ap-
proximate the length distribution of non-terminals
with a smoothed Gaussian, which is more robust
and gives rise to much larger improvement consis-
tently.
The third feature utilizes source side context in-
formation, i.e. the neighboring words of an input
span, to influence the selection of the target trans-
lation for a span. While the use of context infor-
mation has been explored in MT, e.g. (Carpuat
and Wu, 2007) and (He et al, 2008), the specific
technique we used by means of a context language
model is rather different. Our model is trained on
the whole training data, and it is not limited by the
constraint of MaxEnt training.
The fourth feature exploits structural informa-
tion on the source side. Specifically, the decoder
simultaneously generates both the source and tar-
get side dependency trees, and employs two de-
pendency LMs, one for the source and the other
for the target, for scoring translation hypotheses.
Our intuition is that the likelihood of source struc-
tures provides another piece of evidence about the
plausibility of a translation hypothesis and as such
would help weed out bad ones.
73
1.2.2 Baseline System and Experimental
Setup
We take BBN?s HierDec, a string-to-dependency
decoder as described in (Shen et al, 2008), as our
baseline for the following two reasons:
? It provides a strong baseline, which ensures
the validity of the improvement we would ob-
tain. The baseline model used in this paper
showed state-of-the-art performance at NIST
2008 MT evaluation.
? The baseline algorithm can be easily ex-
tended to incorporate the features proposed
in this paper. The use of source dependency
structures is a natural extension of the string-
to-tree model to a tree-to-tree model.
To ensure the generality of our results, we tested
the features on two rather different language pairs,
Arabic-to-English and Chinese-to-English, using
two metrics, IBM BLEU (Papineni et al, 2001)
and TER (Snover et al, 2006). Our experiments
show that each of the first three features: non-
terminal labels, length distribution and source side
context, improves MT performance. Surprisingly,
the source dependency feature does not produce
an improvement.
2 Linguistic and Context Features
2.1 Non-terminal Labels
In the original string-to-dependency model (Shen
et al, 2008), a translation rule is composed of a
string of words and non-terminals on the source
side and a well-formed dependency structure on
the target side. A well-formed dependency struc-
ture could be either a single-rooted dependency
tree or a set of sibling trees. As in the Hiero system
(Chiang, 2007), there is only one non-terminal X
in the string-to-dependency model. Any sub de-
pendency structure can be used to replace a non-
terminal in a rule.
For example, we have a source sentence in Chi-
nese as follows.
? jiantao zhuyao baohan liang fangmian
The literal translation for individual words is
? ?review? ?mainly? ?to consist of? ?two? ?part?
The reference translation is
? the review mainly consists of two parts
A single source word can be translated into
many English words. For example, jiantao can
be translated into a review, the review, reviews,
the reviews, reviewing, reviewed, etc. Suppose
we have source-string-to-target-dependency trans-
lation rules as shown in Figure 1. Since there is
no constraint on substitution, any translation for
jiantao could replace the X-1 slot.
One way to alleviate this problem is to limit the
search space by using a label system. We could
assign a label to each non-terminal on the target
side of the rules. Furthermore, we could assign a
label to the whole target dependency structure, as
shown in Figure 2. In decoding, each target de-
pendency sub-structure would be associated with
a label. Whenever substitution happens, we would
check whether the label of the sub-structure and
the label of the slot are the same. Substitutions
with unmatched labels would be prohibited.
In practice, we use a soft constraint by penaliz-
ing substitutions with unmatched labels. We intro-
duce a new feature: the number of times substitu-
tions with unmatched labels appear in the deriva-
tion of a translation hypothesis.
Obviously, to implement this feature we need to
associate a label with each non-terminal in the tar-
get side of a translation rule. The labels are gen-
erated during rule extraction. When we create a
rule from a training example, we replace a sub-
tree or dependency structure with a non-terminal
and associate it with the POS tag of the head word
if the non-terminal corresponds to a single-rooted
tree on the target side. Otherwise, it is assigned
the generic label X. (In decoding, all substitutions
of X are considered unmatched ones and incur a
penalty.)
2.2 Length Distribution
In English, the length of a phrase may determine
the syntactic structure of a sentence. For example,
possessive relations can be represented either as
?A?s B? or ?B of A?. The former is preferred if A
is a short phrase (e.g. ?the boy?s mother?) while
the latter is preferred if A is a complex structure
(e.g. ?the mother of the boy who is sick?).
Our solution is to build a model of length prefer-
ence for each non-terminal in each translation rule.
To address data sparseness, we assume the length
distribution of each non-terminal in a transfer rule
is a Gaussian, whose mean and variance can be
estimated from the training data. In rule extrac-
74
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X X?1 baohan
X
X?1
consists
X?2
X?2
ofmainly
zhuyao
Figure 1: Translation rules with one label X
the
X jiantao
reviews
the
X jiantao
review
X X?1 baohan
consists
NNS VBZNN
NN?1 NNS?2
X?2
ofmainly
zhuyao
Figure 2: Translation rules with multiple labels
tion, each time a translation rule is generated from
a training example, we can record the length of the
source span corresponding to a non-terminal. In
the end, we have a frequency histogram for each
non-terminal in each translation rule. From the
histogram, a Gaussian distribution can be easily
computed.
In practice, we do not need to collect the fre-
quency histogram. Since all we need to know are
the mean and the variance, it is sufficient to col-
lect the sum of the length and the sum of squared
length.
Let r be a translation rule that occurs N
r
times
in training. Let x be a specific non-terminal in that
rule. Let l(r, x, i) denote the length of the source
span corresponding to non-terminal x in the i-th
occurrence of rule r in training. Then, we can
compute the following quantities.
m
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i) (1)
s
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i)
2
, (2)
which can be subsequently used to estimate the
mean ?
r,x
and variance ?2
r,x
of x?s length distri-
bution in rule r as follows.
?
r,x
= m
r,x
(3)
?
2
r,x
= s
r,x
?m
2
r,x
(4)
Since many of the translation rules have few oc-
currences in training, smoothing of the above esti-
mates is necessary. A common smoothing method
is based on maximum a posteriori (MAP) estima-
tion as in (Gauvain and Lee, 1994).
m?
r,x
=
N
r
N
r
+ ?
m
r,x
+
?
N
r
+ ?
m?
r,x
s?
r,x
=
N
r
N
r
+ ?
s
r,x
+
?
N
r
+ ?
s?
r,x
,
where ? stands for an MAP distribution and ? rep-
resents a prior distribution. m?
r,x
and s?
r,x
can
be obtained from a prior Gaussian distribution
N (??
r,x
, ??
r,x
) via equations (3) and (4), and ? is
a weight of smoothing.
There are many ways to approximate the prior
distribution. For example, we can have one prior
for all the non-terminals or one for individual non-
terminal type. In practice, we assume ??
r,x
= ?
r,x
,
and approximate ??
r,x
as (?2
r,x
+ s
r,x
)
1
2 .
In this way, we do not change the mean, but
relax the variance with s
r,x
. We tried differ-
ent smoothing methods, but the performance did
not change much, therefore we kept this simplest
setup. We also tried the Poisson distribution, and
the performance is similar to Gaussian distribu-
tion, which is about 0.1 point lower in BLEU.
When a rule r is applied during decoding, we
compute a penalty for each non-terminal x in r
according to
P (l | r, x) =
1
?
r,x
?
2pi
e
?
(l??
r,x
)
2
2?
2
r,x
,
where l is length of source span corresponding to
x.
Our method to address the problem of length
bias in rule selection is very different from the
maximum entropy method used in existing stud-
ies, e.g. (He et al, 2008).
75
2.3 Context Language Model
In the baseline string-to-dependency system, the
probability a translation rule is selected in decod-
ing does not depend on the sentence context. In
reality, translation is highly context dependent. To
address this defect, we introduce a new feature,
called context language model. The motivation of
this feature is to exploit surrounding words to in-
fluence the selection of the desired transfer rule for
a given input span.
To illustrate the problem, we use the same ex-
ample mentioned in Section 2.1. Suppose the
source span for rule selection is zhuyao baohan,
whose literal translation is mainly and to consist
of. There are many candidate translations for this
phrase, for example, mainly consist of, mainly
consists of, mainly including, mainly includes, etc.
The surrounding words can help to decide which
translation is more appropriate for zhuyao bao-
han. We compare the following two context-based
probabilities:
? P ( jiantao | mainly consist )
? P ( jiantao | mainly consists )
Here, jiantao is the source word preceding the
source span zhuyao baohan.
In the training data, jiantao is usually trans-
lated into the review, third-person singular, then
the probability P ( jiantao | mainly consists ) will
be higher than P ( jiantao | mainly consist ), since
we have seen more context events like the former
in the training data.
Now we introduce context LM formally. Let the
source words be f
1
f
2
..f
i
..f
j
..f
n
. Suppose source
sub-string f
i
..f
j
is translated into e
p
..e
q
. We can
define tri-gram probabilities on the left and right
sides of the source span:
? left : P
L
(f
i?1
|e
p
, e
p+1
)
? right : P
R
(f
j+1
|e
q
, e
q?1
)
In our implementation, the left and right context
LMs are estimated from the training data as part
of the rule extraction procedure. When we exact a
rule, we collect two 3-gram events, one for the left
side and the other for the right side.
In decoding, whenever a partial hypothesis is
generated, we calculate the context LM scores
based on the leftmost two words and the rightmost
two words of the hypothesis as well as the source
context. The product of the left and right context
LM scores is used as a new feature in the scoring
function.
Please note that our approach is very different
from other approaches to context dependent rule
selection such as (Ittycheriah and Roukos, 2007)
and (He et al, 2008). Instead of using a large num-
ber of fine grained features with weights optimized
using the maximum entropy method, we treat con-
text dependency as an ngram LM problem, and it
is smoothed with Witten-Bell discounting. The es-
timation of the context LMs is very efficient and
robust.
The benefit is two fold. The estimation of the
context LMs is very efficient. It adds only one new
weight to the scoring function.
2.4 Source Dependency Language Model
The context LM proposed in the previous sec-
tion only employs source words immediately be-
fore and after the current source span in decod-
ing. To exploit more source context, we use a
source side dependency language model as an-
other feature. The motivation is to take advantage
of the long distance dependency relations between
source words in scoring a translation theory.
We extended string-to-dependency rules in
the baseline system to dependency-to-dependency
rules. In each dependency-to-dependency rule, we
keep record of the source string as well as the
source dependency structure. Figure 3 shows ex-
amples of dependency-to-dependency rules.
We extended the string-to-dependency decod-
ing algorithm in the baseline to accommodate
dependency-to-dependency theories. In decoding,
we build both the source and the target depen-
dency structures simultaneously in chart parsing
over the source string. Thus, we can compute the
source dependency LM score in the same way we
compute the target side score, using a procedure
described in (Shen et al, 2008).
We introduce two new features for the source
side dependency LM as follows, in a way similar
to the target side.
? Source dependency LM score
? Discount on ill-formed source dependency
structures
The source dependency LM is trained on the
source side of the bi-lingual training data with
Witten-Bell smoothing. The source dependency
LM score represents the likelihood of the source
76
X?1 X?2zhuyao
baohan
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X
X X?1
consists
X?2ofmainly
Figure 3: Dependency-to-dependency translation rules
dependency tree generated by the decoder. The
source dependency tree with the highest score is
the one that is most likely to be generated by the
dependency model that created the source side of
the training data.
Source dependency trees are composed of frag-
ments embedded in the translation rules. There-
fore, a source dependency LM score can be
viewed as a measure whether the translation rules
are put together in a way similar to the training
data. Therefore, a source dependency LM score
serves as a feature to represent structural con-
text information that is capable of modeling long-
distance relations.
However, unlike source context LMs, the struc-
tural context information is used only when two
partial dependency structures are combined, while
source context LMs work as a look-ahead feature.
3 Experiments
We designed our experiments to show the impact
of each feature separately as well as their cumula-
tive impact:
? BASE: baseline string-to-dependency system
? SLM: baseline + source dependency LM
? CLM: baseline + context LM
? LEN: baseline + length distribution
? LBL: baseline + syntactic labels
? LBL+LEN: baseline + syntactic labels +
length distribution
? LBL+LEN+CLM: baseline + syntactic labels
+ length distribution + context LM
All the models were optimized on lower-cased
IBM BLEU with Powell?s method (Powell, 1964;
Brent, 1973) on n-best translations (Ostendorf et
al., 1991), but evaluated on both IBM BLEU and
TER. The motivation is to detect if an improve-
ment is artificial, i.e., specific to the tuning met-
ric. For both Arabic-to-English and Chinese-to-
English MT, we tuned on NIST MT02-05 and
tested on MT06 and MT08 newswire sets.
The training data are different from what was
usd at MT06 or MT08. Our Arabic-to-English
data contain 29M Arabic words and 38M En-
glish words from 11 corpora: LDC2004T17,
LDC2004T18, LDC2005E46, LDC2006E25,
LDC2006G05, LDC2005E85, LDC2006E36,
LDC2006E82, LDC2006E95, Sakhr-A2E and
Sakhr-E2A. The Chinese-to-English data contain
107M Chinese words and 132M English words
from eight corpora: LDC2002E18, LDC2005T06,
LDC2005T10, LDC2006E26, LDC2006G05,
LDC2002L27, LDC2005T34 and LDC2003E07.
They are available under the DARPA GALE
program. Traditional 3-gram and 5-gram string
LMs were trained on the English side of the
parallel data plus the English Gigaword corpus
V3.0 in a way described in (Bulyko et al, 2007).
The target dependency LMs were trained on the
English side of the parallel training data. For that
purpose, we parsed the English side of the parallel
data. Two separate models were trained: one for
Arabic from the Arabic training data and the other
for Chinese from the Chinese training data.
To compute the source dependency LM for
Chinese-to-English MT, we parsed the Chinese
side of the Chinese-to-English parallel data. Due
to the lack of a good Arabic parser compatible
with the Sakhr tokenization that we used on the
source side, we did not test the source dependency
LM for Arabic-to-English MT.
When extracting rules with source dependency
structures, we applied the same well-formedness
constraint on the source side as we did on the tar-
get side, using a procedure described by (Shen
et al, 2008). Some candidate rules were thrown
away due to the source side constraint. On the
77
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08
CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92
LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45
LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57
LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16
LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80
Rescoring (5-gram LM)
BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15
CLM 51.57 49.54 41.74 43.88 51.44 49.37 41.63 43.74
LEN 52.05 50.01 41.50 43.72 51.88 49.89 41.51 43.47
LBL 51.80 49.69 41.54 43.76 51.93 49.86 41.27 43.33
LBL+LEN 51.90 49.76 41.41 43.70 52.42 50.29 40.93 43.00
LBL+LEN+CLM 52.61 50.51 40.77 43.03 52.60 50.56 40.69 42.81
Table 1: BLEU and TER percentage scores on MT06 and MT08 Arabic-to-English newswire sets.
other hand, one string-to-dependency rule may
split into several dependency-to-dependency rules
due to different source dependency structures. The
size of the dependency-to-dependency rule set is
slightly smaller than the size of the string-to-
dependency rule set.
Tables 1 and 2 show the BLEU and TER per-
centage scores on MT06 and MT08 for Arabic-
to-English and Chinese-to-English translation re-
spectively. The context LM feature, the length
feature and the syntax label feature all produce
a small improvement for most of the conditions.
When we combined the three features, we ob-
served significant improvements over the baseline.
For Arabic-to-English MT, the LBL+LEN+CLM
system improved lower-cased BLEU by 2.0 on
MT06 and 1.7 on MT08 on decoding output.
For Chinese-to-English MT, the improvements in
lower-cased BLEU were 1.0 on MT06 and 0.8 on
MT08. After re-scoring, the improvements be-
came smaller, but still noticeable, ranging from 0.7
to 1.4. TER scores were also improved noticeably
for all conditions, suggesting there was no metric
specific over-tuning.
Surprisingly, source dependency LM did not
provide any improvement over the baseline. There
are two possible reasons for this. One is that
the source and target parse trees were generated
by two stand-alone parsers, which may cause in-
compatible structures on the source and target
sides. By applying the well-formed constraints
on both sides, a lot of useful transfer rules are
discarded. A bi-lingual parser, trained on paral-
lel treebanks recently made available to the NLP
community, may overcome this problem. The
other is that the search space of dependency-to-
dependency decoding is much larger, since we
need to add source dependency information into
the chart parsing states. We will explore tech-
niques to address these problems in the future.
4 Discussion
Linguistic information has been widely used in
SMT. For example, in (Wang et al, 2007), syntac-
tic structures were employed to reorder the source
language as a pre-processing step for phrase-based
decoding. In (Koehn and Hoang, 2007), shallow
syntactic analysis such as POS tagging and mor-
phological analysis were incorporated in a phrasal
decoder.
In ISI?s syntax-based system (Galley et al,
2006) and CMU?s Hiero extension (Venugopal et
al., 2007), non-terminals in translation rules have
labels, which must be respected by substitutions
during decoding. In (Post and Gildea, 2008; Shen
et al, 2008), target trees were employed to im-
prove the scoring of translation theories. Mar-
ton and Resnik (2008) introduced features defined
on constituent labels to improve the Hiero system
(Chiang, 2005). However, due to the limitation of
MER training, only part of the feature space could
used in the system. This problem was fixed by
78
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69
SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46
CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77
LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41
LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49
LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65
LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51
Rescoring (5-gram LM)
BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60
SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21
CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28
LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51
LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48
LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46
LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98
Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets.
Chiang et al (2008), which used an online learn-
ing method (Crammer and Singer, 2003) to handle
a large set of features.
Most SMT systems assume that translation
rules can be applied without paying attention to
the sentence context. A few studies (Carpuat and
Wu, 2007; Ittycheriah and Roukos, 2007; He et
al., 2008; Hasan et al, 2008) addressed this de-
fect by selecting the appropriate translation rules
for an input span based on its context in the in-
put sentence. The direct translation model in (It-
tycheriah and Roukos, 2007) employed syntactic
(POS tags) and context information (neighboring
words) within a maximum entropy model to pre-
dict the correct transfer rules. A similar technique
was applied by He et al (2008) to improve the Hi-
ero system.
Our model differs from previous work on the
way in which linguistic and contextual informa-
tion is used.
5 Conclusions and Future Work
In this paper, we proposed four new linguistic
and contextual features for hierarchical decoding.
The use of non-terminal labels, length distribution
and context LM features gave rise to significant
improvement on Arabic-to-English and Chinese-
to-English translation on NIST MT06 and MT08
newswire data over a state-of-the-art string-to-
dependency baseline. Unlike previous work, we
employed robust probabilistic models to capture
useful linguistic and contextual information. Our
methods are more suitable for practical translation
tasks.
In future, we will continue this work in two
directions. We will employ a Gaussian model
to unify various linguistic and contextual fea-
tures. We will also improve the dependency-to-
dependency method with a better bi-lingual parser.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
R. P. Brent. 1973. Algorithms for Minimization With-
out Derivatives. Prentice-Hall.
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
M. Carpuat and D. Wu. 2007. Context-dependent
phrasal translation lexicons for statistical machine
translation. In Proceedings of Machine Translation
Summit XI.
79
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proceedings of the 2008
Conference of Empirical Methods in Natural Lan-
guage Processing.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of the 43th Annual Meeting of the Association for
Computational Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic models.
In COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
J.-L. Gauvain and Chin-Hui Lee. 1994. Maximum a
posteriori estimation for multivariate gaussian mix-
tureobservations of markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2).
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proceedings of the 2008 Conference
of Empirical Methods in Natural Language Process-
ing.
Z. He, Q. Liu, and S. Lin. 2008. Improving statistical
machine translation using lexicalized rule selection.
In Proceedings of COLING ?08: The 22nd Int. Conf.
on Computational Linguistics.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proceedings of the 2007 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language Process-
ing.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In COLING-ACL ?06: Proceed-
ings of 44th Annual Meeting of the Association for
Computational Linguistics and 21st Int. Conf. on
Computational Linguistics.
Y. Marton and P. Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language.
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
M. Post and D. Gildea. 2008. Parsers as language
models for statistical machine translation. In The
Eighth Conference of the Association for Machine
Translation in the Americas.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
7(2).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proceedings of
the 2004 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A New
String-to-Dependency Machine Translation Algo-
rithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical mt. In
COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-cfg
driven statistical mt. In Proceedings of the 2007 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
80
 	
	
-
 	



ffProceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 307?314, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Combining Deep Linguistics Analysis and Surface Pattern Learning:
A Hybrid Approach to Chinese Definitional Question Answering
Fuchun Peng, Ralph Weischedel, Ana Licuanan, Jinxi Xu
BBN Technologies
50 Moulton Street, Cambridge, MA, 02138
 
fpeng, rweisched, alicuan, jxu  @bbn.com
Abstract
We explore a hybrid approach for Chinese
definitional question answering by com-
bining deep linguistic analysis with sur-
face pattern learning. We answer four
questions in this study: 1) How helpful are
linguistic analysis and pattern learning? 2)
What kind of questions can be answered
by pattern matching? 3) How much an-
notation is required for a pattern-based
system to achieve good performance? 4)
What linguistic features are most useful?
Extensive experiments are conducted on
biographical questions and other defini-
tional questions. Major findings include:
1) linguistic analysis and pattern learning
are complementary; both are required to
make a good definitional QA system; 2)
pattern matching is very effective in an-
swering biographical questions while less
effective for other definitional questions;
3) only a small amount of annotation is
required for a pattern learning system to
achieve good performance on biographi-
cal questions; 4) the most useful linguistic
features are copulas and appositives; re-
lations also play an important role; only
some propositions convey vital facts.
1 Introduction
Due to the ever increasing large amounts of online
textual data, learning from textual data is becom-
ing more and more important. Traditional document
retrieval systems return a set of relevant documents
and leave the users to locate the specific information
they are interested in. Question answering, which
combines traditional document retrieval and infor-
mation extraction, solves this problem directly by
returning users the specific answers. Research in
textual question answering has made substantial ad-
vances in the past few years (Voorhees, 2004).
Most question answering research has been focus-
ing on factoid questions where the goal is to return
a list of facts about a concept. Definitional ques-
tions, however, remain largely unexplored. Defini-
tional questions differ from factoid questions in that
the goal is to return the relevant ?answer nuggets?
of information about a query. Identifying such an-
swer nuggets requires more advanced language pro-
cessing techniques. Definitional QA systems are
not only interesting as a research challenge. They
also have the potential to be a valuable comple-
ment to static knowledge sources like encyclopedias.
This is because they create definitions dynamically,
and thus answer definitional questions about terms
which are new or emerging (Blair-Goldensoha et
al., 2004).
One success in factoid question answering
is pattern based systems, either manually con-
structed (Soubbotin and Soubbotin, 2002) or ma-
chine learned (Cui et al, 2004). However, it is
unknown whether such pure pattern based systems
work well on definitional questions where answers
are more diverse.
Deep linguistic analysis has been found useful in
factoid question answering (Moldovan et al, 2002)
and has been used for definitional questions (Xu et
al., 2004; Harabagiu et al, 2003). Linguistic analy-
307
sis is useful because full parsing captures long dis-
tance dependencies between the answers and the
query terms, and provides more information for in-
ference. However, merely linguistic analysis may
not be enough. First, current state of the art lin-
guistic analysis such as parsing, co-reference, and
relation extraction is still far below human perfor-
mance. Errors made in this stage will propagate and
lower system accuracy. Second, answers to some
types of definitional questions may have strong local
dependencies that can be better captured by surface
patterns. Thus we believe that combining linguistic
analysis and pattern learning would be complemen-
tary and be beneficial to the whole system.
Work in combining linguistic analysis with pat-
terns include Weischedel et al (2004) and Jijkoun et
al. (2004) where manually constructed patterns are
used to augment linguistic features. However, man-
ual pattern construction critically depends on the do-
main knowledge of the pattern designer and often
has low coverage (Jijkoun et al, 2004). Automatic
pattern derivation is more appealing (Ravichandran
and Hovy, 2002).
In this work, we explore a hybrid approach to
combining deep linguistic analysis with automatic
pattern learning. We are interested in answering
the following four questions for Chinese definitional
question answering:
  How helpful are linguistic analysis and pattern
learning in definitional question answering?
  If pattern learning is useful, what kind of ques-
tion can pattern matching answer?
  How much human annotation is required for a
pattern based system to achieve reasonable per-
formance?
  If linguistic analysis is helpful, what linguistic
features are most useful?
To our knowledge, this is the first formal study of
these questions in Chinese definitional QA. To an-
swer these questions, we perform extensive experi-
ments on Chinese TDT4 data (Linguistic Data Con-
sortium, 2002-2003). We separate definitional ques-
tions into biographical (Who-is) questions and other
definitional (What-is) questions. We annotate some
question-answer snippets for pattern learning and
we perform deep linguistic analysis including pars-
ing, tagging, name entity recognition, co-reference,
and relation detection.
2 A Hybrid Approach to Definitional Ques-
tion Answering
The architecture of our QA system is shown in Fig-
ure 1. Given a question, we first use simple rules to
classify it as a ?Who-is? or ?What-is? question and
detect key words. Then we use a HMM-based IR
system (Miller et al, 1999) for document retrieval
by treating the question keywords as a query. To
speed up processing, we only use the top 1000 rel-
evant documents. We then select relevant sentences
among the returned relevant documents. A sentence
is considered relevant if it contains the query key-
word or contains a word that is co-referent to the
query term. Coreference is determined using an in-
formation extraction engine, SERIF (Ramshaw et
al., 2001). We then conduct deep linguistic anal-
ysis and pattern matching to extract candidate an-
swers. We rank all candidate answers by predeter-
mined feature ordering. At the same time, we per-
form redundancy detection based on  -gram over-
lap.
2.1 Deep Linguistic Analysis
We use SERIF (Ramshaw et al, 2001), a linguistic
analysis engine, to perform full parsing, name entity
detection, relation detection, and co-reference reso-
lution. We extract the following linguistic features:
1. Copula: a copula is a linking verb such as ?is?
or ?become?. An example of a copula feature
is ?Bill Gates is the CEO of Microsoft?. In this
case, ?CEO of Microsoft? will be extracted as
an answer to ?Who is Bill Gates??. To extract
copulas, SERIF traverses the parse trees of the
sentences and extracts copulas based on rules.
In Chinese, the rule for identifying a copula is
the POS tag ?VC?, standing for ?Verb Copula?.
The only copula verb in Chinese is ?

?.
2. Apposition: appositions are a pair of noun
phrases in which one modifies the other. For
example, In ?Tony Blair, the British Prime Min-
ister, ...?, the phrase ?the British Prime Min-
ister? is in apposition to ?Blair?. Extraction
of appositive features is similar to that of cop-
ula. SERIF traverses the parse tree and iden-
tifies appositives based on rules. A detailed
description of the algorithm is documented
308
Question Classification
Document Retrieval
Linguistic Analysis
Semantic Processing
Phrase Ranking
Redundancy Remove
Lists of Response
Answer Annotation
Name Tagging
Parsing
Preposition finding
Co?reference
Relation Extraction Training data
TreeBank
Name Annotation
Linguistic motivated
Pattern motivated
Question
Pattern MatchingPattern Learning
Figure 1: Question answering system structure
in (Ramshaw et al, 2001).
3. Proposition: propositions represent predicate-
argument structures and take the form:
predicate(    : 	 
  , ...,    :  
  ). The
most common roles include logical subject,
logical object, and object of a prepositional
phrase that modifies the predicate. For ex-
ample, ?Smith went to Spain? is represented
as a proposition, went(logical subject: Smith,
PP-to: Spain).
4. Relations: The SERIF linguistic analysis en-
gine also extracts relations between two ob-
jects. SERIF can extract 24 binary relations
defined in the ACE guidelines (Linguistic Data
Consortium, 2002), such as spouse-of, staff-of,
parent-of, management-of and so forth. Based
on question types, we use different relations, as
listed in Table 1.
Relations used for Who-Is questions
ROLE/MANAGEMENT, ROLE/GENERAL-STAFF,
ROLE/CITIZEN-OF, ROLE/FOUNDER,
ROLE/OWNER, AT/RESIDENCE,
SOC/SPOUSE, SOC/PARENT,
ROLE/MEMBER, SOC/OTHER-PROFESSIONAL
Relation used for What-Is questions
AT/BASED-IN, AT/LOCATED, PART/PART-OF
Table 1: Relations used in our system
Many relevant sentences do not contain the query
key words. Instead, they contain words that are co-
referent to the query. For example, in ?Yesterday UN
Secretary General Anan Requested Every Side...,
He said ... ?. The pronoun ?He? in the second sen-
tence refers to ?Anan? in the first sentence. To select
such sentences, we conduct co-reference resolution
using SERIF.
In addition, SERIF also provides name tagging,
identifying 29 types of entity names or descriptions,
such as locations, persons, organizations, and dis-
eases.
We also select complete sentences mentioning the
term being defined as backup answers if no other
features are identified.
The component performance of our linguistic
analysis is shown in Table 2.
Pre. Recall F
Parsing 0.813 0.828 0.820
Co-reference 0.920 0.897 0.908
Name-entity detection 0.765 0.753 0.759
Table 2: Linguistic analysis component performance
for Chinese
2.2 Surface Pattern Learning
We use two kinds of patterns: manually constructed
patterns and automatically derived patterns. A man-
ual pattern is a commonly used linguistic expression
that specifies aliases, super/subclass and member-
ship relations of a term (Xu et al, 2004). For exam-
ple, the expression ?tsunamis, also known as tidal
waves? gives an alternative term for tsunamis. We
309
use 23 manual patterns for Who-is questions and 14
manual patterns for What-is questions.
We also classify some special propositions as
manual patterns since they are specified by compu-
tational linguists. After a proposition is extracted,
it is matched against a list of predefined predicates.
If it is on the list, it is considered special and will
be ranked higher. In total, we designed 22 spe-
cial propositions for Who-is questions, such as  
 (become),   (elected as), and  (resign),
14 for What-is questions, such as 
	 (located at),

	 (created at), and   (also known as).
However, it is hard to manually construct such
patterns since it largely depends on the knowledge
of the pattern designer. Thus, we prefer patterns
that can be automatically derived from training data.
Some annotators labeled question-answer snippets.
Given a query question, the annotators were asked
to highlight the strings that can answer the question.
Though such a process still requires annotators to
have knowledge of what can be answers, it does not
require a computational linguist. Our pattern learn-
ing procedure is illustrated in Figure 2.
Generate Answer Snippet
Pattern Generalization
Pattern Selection
POS Tagging
Merging POS Tagging
and Answer Tagging
Answer Annotation
Figure 2: Surface Pattern Learning
Here we give an example to illustrate how pat-
tern learning works. The first step is annotation. An
example of Chinese answer annotation with English
translation is shown in Figure 3. Question words are
assigned the tag QTERM, answer words are tagged
ANSWER, and all other words are assigned BKGD,
standing for background words (not shown in the ex-
ample to make the annotation more readable).
To obtain patterns, we conduct full parsing to ob-
tain the full parse tree for a sentence. In our current
Chinese annotation:  ? Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 652?659, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Methodology for Extrinsically Evaluating Information Extraction  
Performance 
 
Michael Crystal, Alex Baron, Katherine Godfrey, Linnea Micciulla, Yvette Tenney, and 
Ralph Weischedel  
BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138-1119 
mcrystal@bbn.com 
 
Abstract 
This paper reports a preliminary study 
addressing two challenges in measuring 
the effectiveness of information extrac-
tion (IE) technology: 
? Developing a methodology for ex-
trinsic evaluation of IE; and, 
? Estimating the impact of improving 
IE technology on the ability to per-
form an application task. 
The methodology described can be em-
ployed for further controlled experi-
ments regarding information extraction. 
1 Introduction 
Intrinsic evaluations of information extraction 
(IE) have a history dating back to the Third Mes-
sage Understanding Conference1 (MUC-3) and 
continuing today in the Automatic Content Ex-
traction (ACE) evaluations.2  Extrinsic evalua-
tions of IE, measuring the utility of IE in a task, 
are lacking and needed (Jones, 2005).   
In this paper, we investigate an extrinsic 
evaluation of IE where the task is question an-
swering (QA) given extracted information.  In 
addition, we propose a novel method for explor-
ing hypothetical performance questions, e.g., if 
IE accuracy were x% closer to human accuracy, 
how would speed and accuracy in a task, e.g., 
QA, improve? 
                                                          
1 For more information on the MUC conferences, see 
http://www.itl.nist.gov/iad/894.02/related_projects/muc/.   
2 For an overview of ACE evaluations see 
http://www.itl.nist.gov/iad/894.01/tests/ace/.  
We plot QA accuracy and time-to-complete 
given eight extracted data accuracy levels rang-
ing from the output of SERIF, BBN?s state-of-
the-art IE system, to manually extracted data. 
2 Methodology 
Figure 1 gives an overview of the methodol-
ogy. The left portion of the figure shows source 
documents provided both to a system and a hu-
man to produce two extraction databases, one 
corresponding to SERIF?s automated perform-
ance and one corresponding to double-
annotated, human accuracy.  By merging por-
tions of those two sources in varying degrees 
(?blends?), one can derive several extracted da-
tabases ranging from machine quality, through 
varying percentages of improved performance, 
up to human accuracy. This method of blending 
databases provides a means of answering hypo-
thetical questions, i.e., what if the state-of-the-
art were x% closer to human accuracy, with a 
single set of answer keys. 
A person using a given extraction database 
performs a task, in our case, QA.  The measures 
of effectiveness in our study were time to com-
plete the task and percent of questions answered 
correctly.  An extrinsic measure of the value of 
improved IE technology performance is realized 
by rotating users through different extraction 
databases and questions sets.   
In our preliminary study, databases of fully 
automated IE and manual annotation (the gold 
standard) were populated with entities, relation-
ships, and co-reference links from 946 docu-
ments. The two initial databases representing 
machine extraction and human extraction re-
spectively were then blended to produce a con-
tinuum of database qualities from machine to 
652
human performance. ACE Value Scores3 were 
measured for each database. Pilot studies were 
conducted to develop questions for a QA task. 
Each participant answered four sets of questions, 
each with a different extraction database repre-
senting a different level of IE accuracy. An an-
swer capture tool recorded the time to answer 
each question and additional data to confirm that 
the participant followed the study protocol. The 
answers were then evaluated for accuracy and 
the relationship between QA performance and 
IE quality was established.  
Each experiment used four databases. The first ex-
periment used databases spanning the range from 
solely machine extraction to solely human extraction. 
Based on the results of this experiment, two further 
experiments focused on smaller ranges in database 
quality to study the relationship between IE and QA 
performance.  
2.1 Source Document Selection, Annota-
tion, and Extraction 
Source documents were selected based on the 
availability of manual annotation.  We identified 
946 broadcast news and newswire articles from 
recent ACE efforts, all annotated by the LDC 
according to the ACE guidelines for the relevant 
year (2002, 2003, 2004). Entities, relations, and 
within-document co-reference were marked.  
Inter-document co-reference annotation was 
added by BBN.  The 946 news articles com-
prised 363 articles (187,720 words) from news-
wire and 583 (122,216 words) from broadcast 
news. With some corrections to deal with errors 
and changes in guidelines, the annotations were 
loaded as the human (DB-quality 100) database. 
                                                          
3 The 2004 ACE evaluation plan, available at 
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-
v7.pdf, contains a full description of the scoring metric used in the 
evaluation.  Entity type weights were 1 and the level weights were 
NAM=1.0, NOM=0.5, and PRO=0.1. 
SERIF, BBN?s automatic IE system based on its 
predecessor, SIFT (Miller, 2000), was run on the 
946 ACE documents to create the machine (DB-
quality 0) database. SERIF is a statistically 
trained software system that automatically per-
forms entity, co-reference, and relationship in-
formation extraction. 
Intermediate IE performance was simulated 
by blending the human and automatically gener-
ated databases in various degrees using an inter-
polation algorithm developed specifically for 
this study. To create a blended database, DB-
quality n, all of the entities, relationships, and 
co-reference links common to the human and 
automatically generated databases are copied 
into a new one. Then, n% of the entity mentions 
in the human database (100), but not in the 
automatic IE system output (0), are copied; and, 
(100 ? n)% of the entity mentions in the auto-
matically generated database, but not in the hu-
man database, are copied. Next, the relationships 
for which both of the constituent entity mentions 
have been copied are also copied to the blended 
database. Finally, co-reference links and entities 
for the already copied entity mentions are copied 
into the blended database. 
For the first experiment, two intermediate ex-
traction databases were created: DB-qualities 33 
and 67. For the second experiment, two addi-
tional databases were created: 16.5 and 50. The 
first intermediate databases were both created 
using the 0 and 100 databases as seeds. The 16.5 
database was created by mixing the 0 and the 33 
databases in a 50% blend. The 50 database was 
created by doing the same with the 33 and 67 
databases.  For Experiment 3, 41 and 58 data-
bases were created by mixing the 33 and 50, and 
50 and 67 databases respectively.  
100
0
100
67
33
0<<IE Tool>>
Annotators
Source
docs
++
+
0
-
Blended
Extraction
Accuracy
Measure QA
Accuracy &
Speed
QA Task
Study establishes 
curve
++
+
0
-
10067330
Imputed Accuracy 
Requirement
A Priori Utility 
Threshold
Q
A
 P
er
fo
rm
an
ce
IE Accuracy (% human annotation)
Figure 1: Study Overview 
653
 
  DB Blend 
  
0 
(Machine) 16.5 33 41 50 58 67 
100 
(Human) 
  Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel 
Recall 64 33 70 40 74 45 76 48 79 54 82 58 86 65 100 100 
Pre. 74 50 77 62 79 67 80 70 83 75 85 78 89 82 100 100 
Value 60 29 67 37 71 42 73 45 77 51 80 56 84 63 100 100 
Table 1: Precision, Recall and Value Scores for Entities and Relations for each DB Blend 
 
  0 
(Machine) 16.5 33 41 50 58 67 
100 
(Human)
Entities 17,117 18,269 18,942 19,398 19,594 19,589 19,440 18,687 
Relations 6,684 6,675 6,905 7,091 7,435 7,808 8,406 11,032 
Descriptions 18,666 18,817 19,135 19,350 19,475 19,639 19,752 20,376 
Table 2: Entity, Relation and Description Counts for each DB Blend 
 
To validate the interpolation algorithm and 
blending procedure, we applied NIST?s 2004 
ACE Scorer to the eight extraction databases. 
Polynomial approximations were fitted against 
both the entity and relation extraction curves. 
Entity performance was found to vary linearly 
with DB blend (R2 = .9853) and relation per-
formance was found to vary with the square of 
DB blend (R2 = .9961). Table 1 shows the scores 
for each blend, and Table 2 shows the counts of 
entities, relationships, and descriptions. 
2.2 Question Answering Task 
Extraction effectiveness was measured by how 
well a person could answer questions given a 
database of facts, entities, and documents. Par-
ticipants answered four sets of questions using 
four databases. They accessed the database using 
BBN?s FactBrowser (Miller, 2001) and recorded 
their answers and source citations in a separate 
tool developed for this study, AnswerPad. 
Each database represented a different data-
base quality. In some databases, facts were miss-
ing, or incorrect facts were recorded. 
Consequently, answers were more accessible in 
some databases than in others, and participants 
had to vary their question answering strategy 
depending on the database. 
Participants were given five minutes to an-
swer each question. To ensure that they had ac-
tually located the answer rather than relied on 
world knowledge, they were required to provide 
source citations for every answer. The instruc-
tions emphasized that the investigation was a 
test of the system, and not of their world knowl-
edge or web search skills. Compliance with 
these instructions was high. Users resorted to 
knowledge-based proper noun searches only one 
percent of the time. In addition, keyword search 
was disabled to force participants to rely on the 
database features. 
2.3 Participants 
Study participants were recruited through local 
web lists and at local colleges and universities.  
Participants were restricted to college students 
and recent graduates with PC (not Mac) experi-
ence, without reading disabilities, for whom 
English was their native language. No other 
screening was necessary because the design 
called for each participant to serve as his or her 
own control, and because opportunities to use 
world knowledge in answering the questions 
were minimized through the interface and pro-
cedures. 
During the first two months of the study 23 
participants were used to help develop questions, 
participant criteria, and the overall test proce-
dure. Then, experiments were conducted com-
paring the 0, 33, 67, and 100 database blends 
(Experiment 1, 20 subjects); the 0, 16.5, 33, and 
50 database blends (Experiment 2, 20 subjects), 
and the 33, 41, 50, and 58 database blends (Ex-
periment 3, 24 subjects). 
654
2.4 Question Selection and Validation 
Questions were developed over two months of 
pilot studies. The goal was to find a set of ques-
tions that would be differentially supported by 
the 0, 33, 67, and 100 databases. We explored 
both ?random? and ?engineered? approaches. 
The random approach called for creating ques-
tions using only the documents, without refer-
ence to the kind of information extracted. Using 
a list of keywords, one person generated 86 
questions involving relationships and entities 
pertaining to politics and the military by scan-
ning the 946 ACE documents to find references 
to each keyword and devising questions based 
on the information she found.  
The alternative, engineered approach involved 
eliminating questions that were not supported by 
the types of information extracted by SERIF, 
and generating additional questions to fit the 
desired pattern of increasing support with in-
creased human annotation. This approach en-
sured that the question sets reflected the 
structural differences that are assumed to exist in 
the database, and produced psychophysical data 
that link degree of QA support to human per-
formance parameters. The IE results from four 
of the databases (0, 33, 67 and 100) were used to 
develop questions that received differential sup-
port from the different quality databases. For 
example, such a question could be answered us-
ing the automatically extracted results, but might 
be more straightforwardly answered given hu-
man annotation. 
Sixty-four questions, plus an additional ten 
practice questions, were created using the engi-
neering approach. Additional criteria that were 
followed in creating the question sets were: 1) 
Questions had to contain at least one reasonable 
entry hook into all four databases, e.g., the terms 
U.S. and America were considered too broad to 
be reasonable; and, 2) For ease of scoring, list-
type questions had to specify the number of an-
swers required. Alternative criteria were consid-
ered but rejected because they correlated with 
the aforementioned set.  The following are ex-
amples of engineered questions. 
? Identify eight current or former U.S. State 
Department workers. 
? In what two West Bank towns does Fatah 
have an office? 
? Name two countries where Osama bin 
Laden has been. 
? Were Lebanese women allowed to vote in 
municipal elections between two Shiite 
groups in the year 1998? 
Two question lists, one with 86 questions 
generated by the random procedure and one with 
64 questions generated by the engineered proce-
dure, were analyzed with respect to the degree of 
support afforded by each of the four databases as 
viewed through FactBrowser. Four a priori cri-
teria were established to assess degree of support 
? or its opposite, the degree of expected diffi-
culty ? for each question in each of the four da-
tabases. Ranked from easiest to hardest, they are 
listed in Table 3. 
The question can be answered? 
1. Directly with fact or description (answer 
is highlighted in FactBrowser citation) 
2. Indirectly with fact or description (an-
swer is not highlighted) 
3. With name mentioned in question (long 
list of mentions without context) 
4. Via database crawling 
Table 3: A Priori Question Difficulty Character-
istics, listed from easiest to hardest 
Table 4 shows the question difficulty levels 
for both question types, for each of four data-
bases. Analysis of the engineered set was done 
on all 64 questions.  Analysis for randomly gen-
erated questions was done on a random sample 
of 44 of the 86 questions.  Fifteen questions did 
not meet the question criteria, leaving 29.  
The randomly generated questions showed a 
statistically significant, but small, variation in 
expected difficulty, in part due to the number of 
unanswerable questions. While the questions 
were made up with respect to information found 
in the documents, the process did not consider 
the types of extracted entities and relations. This 
problem might have been mitigated by limiting 
the search to questions involving entities and 
relations that were part of the extraction task. 
By contrast, the engineered question set 
showed a highly significant decrease in expected 
difficulty as the percentage of human annotation 
in the database increased (P < 0.0001 for chi-
square analysis). This result is not surprising, 
given that the questions were constructed with 
reference to the list of entities in the four data-
655
bases. The analysis confirms that the experimen-
tal manipulation of different degrees of support 
provided by the four databases was achieved for 
this question set. 
Random Question Generation 
Difficulty 
Level        
(easiest to 
hardest) 
0% 
Human 
33% 
Human 
67% 
Human 
100% 
Human 
1 Fact-
Highlight 
7 10 13 15
2 Fact-
Indirect 
14 10 8 10
3 Mention 3 5 2 1
4 Web Crawl 5 4 6 3
Total 29 29 29 29
     
Engineered Question Generation 
Difficulty 
Level               
(from easiest 
to hardest) 
0% 
Human 
33% 
Human 
67 
Human 
100% 
Human 
1 Fact-
Highlight 
16 25 35 49
2 Fact-
Indirect 
23 20 18 14
3 Mention 7 14 11 1
4 Web Crawl 18 5 0 0
Total 64 64 64 64
Table 4: Anticipated Difficulty of Questions as a 
Function of Database Quality 
Preliminary human testing with both question 
sets suggested that the a priori difficulty indica-
tors predict human question answering perform-
ance. Experiments with the randomly generated 
questions, therefore, were unlikely to reveal 
much about the databases or about human ques-
tion answering performance. On the other hand, 
an examination of how different levels of data-
base quality affect human performance, in a psy-
chophysical experiment where structure is varied 
systematically, promised to address the question 
of how much support is needed for good per-
formance. 
Based on the question difficulties, and pilot 
study timing and performance results, the 64 
questions were grouped into four, 16-question 
balanced sets. 
2.5 Procedure 
Participants were tested individually at our site, 
in sessions lasting roughly four hours. Training 
prior to the test lasted for approximately a half 
hour. Training consisted of a walk-through of 
the interface features followed by guided prac-
tice with sample questions. The test consisted of 
four question sets, each with a different data-
base.  Participants were informed that they 
would be using a different database for each 
question set and that some might be easier to use 
than others. 
Questions were automatically presented and 
responses were captured in AnswerPad, a soft-
ware tool designed for the study. AnswerPad is 
shown in Figure 2.  
Key features of the tool include: 
? Limiting view to current question set ? 
disallowing participants to view previous 
question sets 
? Automatically connecting to correct db 
? Logging time spent on each question 
? Enforcing five-minute limit per question 
? Enforcing requirement that all answers in-
clude a citation 
 
Figure 2: AnswerPad Question Presentation and 
Answer Capture Interface 
Participants were given written documenta-
tion as part of their training. The participants 
were instructed to cut-and-paste question an-
swers and document citations from source 
documents into AnswerPad. 
Extracted facts and entities, and source docu-
ments were accessed through FactBrowser. 
FactBrowser, shown in Figure 3, is web-browser 
based and is invoked via a button in AnswerPad. 
FactBrowser allows one to enter a string, which 
656
is matched against the database of entity men-
tions. The list of entities that have at least one 
mention partially matching the string are re-
turned (e.g., ?Laura Bush?) along with an icon 
indicating the type of the entity and the number 
of documents in which the entity appears.  
Clicking on the entity in the left panel causes the 
top right panel to display all of the descriptions, 
facts, and mentions for the entity. Selecting one 
of these displays citations in which the descrip-
tion, fact, or mention occurs. Clicking on the 
citation opens up a document view in the lower 
right corner of the screen and highlights the ex-
tracted information in the text. When a docu-
ment is displayed, all of the entities detected in 
the document are listed down the left side of the 
document viewer.  
 
 
Figure 3: Browsing Tool Interface 
The browsing tool was instrumented to record 
command invocations so that the path a partici-
pant took to answer a question could be recre-
ated, and the participant?s adherence to protocol 
could be verified. Furthermore, the find function 
(Ctrl-F) was disabled to prevent users from per-
forming ad hoc searches of the documents in-
stead of using the extracted data. 
The order of question sets and the order of da-
tabase conditions were counterbalanced across 
participants, so that, for every four participants, 
every question set and database appeared once in 
every ordinal position, and every question set 
was paired once with every database. This 
avoided carryover effects from question order. 
2.6 Data Collected 
Based on the initial results from Experiment 1, a 
70% target effectiveness threshold was identi-
fied to occur between the 33 and 67 database 
blends. To refine and verify this finding, Ex-
periment 2 examined the 0, 16.5, 33, and 50 da-
tabase blends. Experiment 3 examined the 33, 
41, 50, and 58 database blends. 
AnswerPad collected participant-provided an-
swers to questions and the corresponding cita-
tions. In addition, AnswerPad recorded the time 
spent answering the questions. A limit of five 
minutes was imposed based on pilot study re-
sults. The browsing tool logged commands in-
voked while the user searched the fact-base for 
question answers. Questions were manually 
scored based on the answers in the provided 
corpus. No partial credit was given. The maxi-
mum score, for each database condition, was 16, 
for a total maximum score of 64. 
3 Results 
Figure 4 shows the question answer scores 
and times for each of the three individual ex-
periments, and for Experiments 1 and 2 com-
bined. Database quality affects both task speed 
(downward-sloping line) and task accuracy (up-
ward-sloping line) in the expected direction. A 
logistic fit, as for a binary-response curve, was 
used to fit the relationship between blend per-
centage and accuracy in each experiment. The 
logistic fit Goodman-Theil quasi-R2 was .9973 
for Experiment 1, .9594 for Experiment 2, .8936 
for Experiment 3, and .9959 for Experiments 1 
and 2 combined. 
For the target accuracy of 70%, the 95% con-
fidence interval for the required blend is (35,56) 
around a predicted 46% blend for Experiment 1, 
and (41,56) around a predicted 49% for Experi-
ments 1 and 2 combined. 
 
657
Experiment 1 Performance and Time vs DB Blend
56
68
75
82
202
174
152
140
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
225
Ti
m
e 
(s
ec
on
ds
)
Experiment 2 Performance and Time vs DB Blend
52
61
64
70
210
187
183
166
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
o
rr
ec
t)
100
125
150
175
200
225
T
im
e 
(s
ec
o
n
ds
)
Experiment 3 Performance and Time vs DB Blend
61
63
68 67
173
171
164
178
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
T
im
e 
(s
ec
o
n
d
s)
Experiments  1 & 2 Performance and Time vs DB Blend
54
61
66
70
75
82
206
187
179
166
152
140
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
225
Ti
m
e 
(s
ec
on
ds
)
% Correct Blend Lower Bound Logistic Fit for Blend
Blend Upper Bound Time  
Figure 4 QA Performance (upward-sloping) and QA Time (downward-sloping) vs. Extraction Blend 
Error Bars are Plus/Minus Standard Error of Mean (SEM) Within Each Blend 
Upper and Lower Bounds Are Approximate 95% Confidence Intervals Based on the Logistic Fit 
For the Blend (X) to Produce a Given Performance (Y) 
(Read these bounds horizontally, as bounds on X, with the upper bound to the right of the lower bound.) 
 
The downward-sloping line in each graph 
displays the average time to answer a question 
as a function of the extraction blend. For this 
analysis we used strict time, the time it took the 
participant to answer the question if he or she 
answered correctly, or the full 5 minutes allowed 
for any incorrectly answered question. This ad-
dresses the situation where a person quickly an-
swers all of the questions incorrectly.  The 
average question-answer time drops 32% as one 
moves from a machine generated extraction da-
tabase to a human generated database. A 
straight-line fit to the Experiment 1 and 2 com-
bined data predicts a drop of 6.5 seconds as the 
human proportion of the database increases by 
10 percentage points. 
A one-way repeated measures analysis of 
variance (ANOVA) was performed for Experi-
ment 1 (0-33-67-100), Experiment 2 (0-16.5-33-
50), and Experiment 3 (33-41-50-58). Table 5 
summarizes the results. In Experiments 1 and 2 
the impact of database quality on QA perform-
ance and on QA time were highly significant (P 
< 0.0001), but not for the narrower range of da-
tabases in Experiment 3. Other ANOVAs 
showed that the impact of trial order and ques-
tion set on QA performance were both non-
significant (P > 0.05). 
 
658
Experiment QA 
Performance 
Strict Time 
1 F(3,57) = 30.98, 
P < .0001  
F(3, 57) = 28.36 
P < .0001 
2 F(3,57)= 19.32, 
P < .0001  
F(3, 57) =  15.37,
P < .0001 
3 F(3,69)= 2.023, 
P = .1187 
F(3,69)= 1.053, 
P = .3747 
Table 5: ANOVA Analyses for QA Performance 
Expt. 1 used db blends of 0, 33, 67, and 100% 
Expt. 2 used db blends of 0, 16.5, 33, and 50% 
Expt. 3 used db blends of 33, 41, 50, and 58% 
In Experiment 1, Newman-Keuls contrasts 
indicate that the 0, 33, 67, and 100 databases 
differ significantly (P < .05) on their impact on 
QA quality. For Experiment 2, however, the 
16.5 and 33 database qualities were not shown to 
be different, nor were any of the database blends 
in Experiment 3. The data suggest that nearly 
half the improvement in QA quality from 0 to 
100 occurs by the 33 database blend, and more 
than half the improvement in QA quality from 0 
to 50 occurs by the 16.5 blend: a little ?human? 
goes a long way. Experiment 3 suggests that 
small differences in data blends make no practi-
cal difference in the results.  Alternatively, there 
might be real differences that are small enough 
such that a larger number of participants would 
be required to detect them. Experiment 3 also 
had two participants with atypical patterns of 
QA against blend, which might account for the 
failure to detect a difference between the 33 and 
50 or 58 blends as suggested by the results from 
Experiment 2. Furthermore, larger experiments 
could reveal whether the atypical participants 
were representatives of a subpopulation, or sim-
ply outliers. Bearing the possibility of outliers in 
mind, we used the combination of Experiments 
1 and 2 for the combined logistic analysis. 
4 Conclusions 
We presented a methodology for assessing in-
formation extraction effectiveness using an ex-
trinsic study. In addition, we demonstrated how 
a novel database blending (merging) strategy 
allows interpolating extraction quality from 
automated performance up through human accu-
racy, thereby decreasing the resources required 
to conduct effectiveness evaluations. 
Experiments showed QA accuracy and speed 
increased with higher IE performance, and that 
the database blend percentage was a good proxy 
for ACE value scores.  We emphasize that the 
study was not to show that IE supports QA bet-
ter than other technologies, rather to isolate util-
ity gains due to IE performance improvements. 
QA performance was plotted against human-
machine IE blend and, for example, 70% QA 
performance was achieved with a database blend 
between 41% and 46% machine extraction.  This 
corresponded to entity and relationship value 
scores of roughly 74 and 47 respectively. 
The logistic dose-response model provided a 
good fit and allowed for computation of confi-
dence bounds for the IE associated with a par-
ticular level of performance. The constraints 
imposed by AnswerPad and FactBrowser en-
sured that world knowledge was neutralized, and 
the repeated-measures design (using participants 
as their own controls across multiple levels of 
database quality) excluded inter-participant vari-
ability from experimental error, increasing the 
ability to detect differences with relatively small 
sample sizes. 
Acknowledgement 
This material is based upon work supported in 
part by the Department of the Interior under 
Contract No. NBCHC030014.  Any opinions, 
findings and conclusions or recommendations 
expressed in this material are those of the au-
thors and do not necessarily reflect the views of 
the Department of the Interior. 
References  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel, 
"A Novel Use of Statistical Parsing to Extract In-
formation from Text", in Proceedings of 1st Meet-
ing of the North American Chapter of the ACL, 
Seattle, WA., pp.226-233, 2000.  
S. Miller, S. Bratus, L. Ramshaw, R. Weischedel, and 
A. Zamanian. "FactBrowser Demonstration", Hu-
man Language Technology Conference, San 
Diego, 2001. 
D. Jones and E. Walton, ?Measuring the Utility of 
Human Language Technology for Intelligence 
Analysis,? 2005 International Conference on Intel-
ligence Applications, McLean, VA May, 2005. 
659
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57?60,
New York, June 2006. c?2006 Association for Computational Linguistics
OntoNotes: The 90% Solution 
 
 
Eduard Hovy Mitchell Marcus Martha Palmer Lance Ramshaw Ralph Weischedel 
USC/ICI Comp & Info Science ICS and Linguistics BBN Technologies BBN Technologies 
4676 Admiralty U. of Pennsylvania U. of Colorado 10 Moulton St. 10 Moulton St. 
Marina d. R., CA Philadelphia, PA Boulder, CO Cambridge, MA Cambridge, MA 
hovy 
@isi.edu 
mitch  
@cis.upenn.edu 
martha.palmer 
@colorado.edu 
lance.ramshaw 
@bbn.com 
weischedel 
@bbn.com 
 
 
 
 
Abstract* 
We describe the OntoNotes methodology and its 
result, a large multilingual richly-annotated corpus 
constructed at 90% interannotator agreement. An 
initial portion (300K words of English newswire 
and 250K words of Chinese newswire) will be 
made available to the community during 2007. 
1 Introduction 
Many natural language processing applications 
could benefit from a richer model of text meaning 
than the bag-of-words and n-gram models that cur-
rently predominate. Until now, however, no such 
model has been identified that can be annotated 
dependably and rapidly. We have developed a 
methodology for producing such a corpus at 90% 
inter-annotator agreement, and will release com-
pleted segments beginning in early 2007. 
The OntoNotes project focuses on a domain in-
dependent representation of literal meaning that 
includes predicate structure, word sense, ontology 
linking, and coreference. Pilot studies have shown 
that these can all be annotated rapidly and with 
better than 90% consistency. Once a substantial 
and accurate training corpus is available, trained 
algorithms can be developed to predict these struc-
tures in new documents. 
                                                        
*
 This work was supported under the GALE program of the 
Defense Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. 
This process begins with parse (TreeBank) and 
propositional (PropBank) structures, which provide 
normalization over predicates and their arguments.  
Word sense ambiguities are then resolved, with 
each word sense also linked to the appropriate 
node in the Omega ontology. Coreference is also 
annotated, allowing the entity mentions that are 
propositional arguments to be resolved in context. 
Annotation will cover multiple languages (Eng-
lish, Chinese, and Arabic) and multiple genres 
(newswire, broadcast news, news groups, weblogs, 
etc.), to create a resource that is broadly applicable. 
2 Treebanking 
The Penn Treebank (Marcus et al, 1993) is anno-
tated with information to make predicate-argument 
structure easy to decode, including function tags 
and markers of ?empty? categories that represent 
displaced constituents.  To expedite later stages of 
annotation, we have developed a parsing system 
(Gabbard et al, 2006) that recovers both of these 
latter annotations, the first we know of.  A first-
stage parser matches the Collins (2003) parser on 
which it is based on the Parseval metric, while si-
multaneously achieving near state-of-the-art per-
formance on recovering function tags (F-measure 
89.0). A second stage, a seven stage pipeline of 
maximum entropy learners and voted perceptrons, 
achieves state-of-the-art performance (F-measure 
74.7) on the recovery of empty categories by com-
bining a linguistically-informed architecture and a 
rich feature set with the power of modern machine 
learning methods. 
57
3 PropBanking  
The Penn Proposition Bank, funded by ACE 
(DOD), focuses on the argument structure of verbs, 
and provides a corpus annotated with semantic 
roles, including participants traditionally viewed as 
arguments and adjuncts.  The 1M word Penn Tree-
bank II Wall Street Journal corpus has been suc-
cessfully annotated with semantic argument 
structures for verbs and is now available via the 
Penn Linguistic Data Consortium as PropBank I 
(Palmer et al, 2005).   Links from the argument 
labels in the Frames Files to FrameNet frame ele-
ments and VerbNet thematic roles are being added.  
This style of annotation has also been successfully 
applied to other genres and languages. 
4 Word Sense  
Word sense ambiguity is a continuing major ob-
stacle to accurate information extraction, summari-
zation and machine translation.  The subtle fine-
grained sense distinctions in WordNet have not 
lent themselves to high agreement between human 
annotators or high automatic tagging performance. 
Building on results in grouping fine-grained 
WordNet senses into more coarse-grained senses 
that led to improved inter-annotator agreement 
(ITA) and system performance (Palmer et al, 
2004; Palmer et al, 2006), we have  developed a 
process for rapid sense inventory creation and an-
notation that includes critical links between the 
grouped word senses and the Omega ontology 
(Philpot et al, 2005; see Section 5 below). 
This process is based on recognizing that sense 
distinctions can be represented by linguists in an 
hierarchical structure, similar to a decision tree, 
that is rooted in very coarse-grained distinctions 
which become increasingly fine-grained until 
reaching WordNet senses at the leaves.  Sets of 
senses under specific nodes of the tree are grouped 
together into single entries, along with the syntac-
tic and semantic criteria for their groupings, to be 
presented to the annotators.   
As shown in Figure 1, a 50-sentence sample of 
instances is annotated and immediately checked for 
inter-annotator agreement.  ITA scores below 90% 
lead to a revision and clarification of the groupings 
by the linguist. It is only after the groupings have 
passed the ITA hurdle that each individual group is 
linked to a conceptual node in the ontology. In ad-
dition to higher accuracy, we find at least a three-
fold increase in annotator productivity. 
 
Figure 1. Annotation Procedure 
As part of OntoNotes we are annotating the 
most frequent noun and verb senses in a 300K 
subset of the PropBank, and will have this data 
available for release in early 2007.  
4.1 Verbs 
Our initial goal is to annotate the 700 most fre-
quently occurring verbs in our data, which are 
typically also the most polysemous; so far 300 
verbs have been grouped and 150 double anno-
tated. Subcategorization frames and semantic 
classes of arguments play major roles in determin-
ing the groupings, as illustrated by the grouping for 
the 22 WN 2.1 senses for drive in Figure 2.  In ad-
word
Check against ontology (1 person)
not OK
Annotate test (2 people)
Results: agreement 
and confusion matrix
Sense partitioning, creating definitions, 
commentary, etc. (2 or 3 people)
Adjudication (1 person)
OK 
not OK
Sa
ve
 
fo
r 
fu
ll
a
n
n
o
ta
tio
n
GI: operating or traveling via a vehi-
cle 
NP (Agent) drive NP, NP drive PP 
WN1: ?Can you drive a truck??, WN2: ?drive to school,?, WN3: ?drive her to 
school,?, WN12: ?this truck drives well,? WN13: ?he drives a taxi,?,WN14: ?The car 
drove around the corner,?, WN:16: ?drive the turnpike to work,?  
G2: force to a position or stance 
NP drive NP/PP/infinitival 
WN4: ?He drives me mad.,? WN6: ?drive back the invaders,? WN7: ?She finally 
drove him to change jobs,? WN8: ?drive a nail,? WN15: ?drive the herd,? WN22: 
?drive the game.? 
G3:  to exert energy on behalf of 
something NP drive NP/infinitival 
WN5: ?Her passion drives her,? WN10: ?He is driving away at his thesis.? 
G4: cause object to move rapidly by 
striking it NP drive NP 
WN9: ?drive the ball into the outfield ,? WN17 ?drive a golf ball,? WN18 ?drive a 
ball? 
Figure 2. A Portion of the Grouping of WordNet Senses for "drive? 
58
dition to improved annotator productivity and ac-
curacy, we predict a corresponding improvement 
in word sense disambiguation performance.  Train-
ing on this new data, Chen and Palmer (2005) re-
port 86.3% accuracy for verbs using a smoothed 
maximum entropy model and rich linguistic fea-
tures, which is 10% higher than their earlier, state-
of-the art performance on ungrouped, fine-grained 
senses. 
4.2 Nouns 
We follow a similar procedure for the annotation 
of nouns.  The same individual who groups Word-
Net verb senses also creates noun senses, starting 
with WordNet and other dictionaries.  We aim to 
double-annotate the 1100 most frequent polyse-
mous nouns in the initial corpus by the end of 
2006, while maximizing overlap with the sentences 
containing annotated verbs.   
Certain nouns carry predicate structure; these 
include nominalizations (whose structure obvi-
ously is derived from their verbal form) and vari-
ous types of relational nouns (like father, 
President, and believer, that express relations be-
tween entities, often stated using of).  We have 
identified a limited set of these whose structural 
relations can be semi-automatically annotated with 
high accuracy.   
5 Ontology  
In standard dictionaries, the senses for each word 
are simply listed.   In order to allow access to addi-
tional useful information, such as subsumption, 
property inheritance, predicate frames from other 
sources, links to instances, and so on, our goal is to 
link the senses to an ontology.  This requires de-
composing the hierarchical structure into subtrees 
which can then be inserted at the appropriate con-
ceptual node in the ontology. 
The OntoNotes terms are represented in the 
110,000-node Omega ontology (Philpot et al, 
2005), under continued construction and extension 
at ISI.  Omega, which has been used for MT, 
summarization, and database alignment, has been 
assembled semi-automatically by merging a vari-
ety of sources, including Princeton?s WordNet, 
New Mexico State University?s Mikrokosmos, and 
a variety of Upper Models, including DOLCE 
(Gangemi et al, 2002), SUMO (Niles and Pease, 
2001), and ISI?s Upper Model, which are in the 
process of being reconciled.  The verb frames from 
PropBank, FrameNet, WordNet, and Lexical Con-
ceptual Structures (Dorr and Habash, 2001) have 
all been included and cross-linked.   
In work planned for later this year, verb and 
noun sense groupings will be manually inserted 
into Omega, replacing the current (primarily 
WordNet-derived) contents. For example, of the 
verb groups for drive in the table above, G1 and 
G4 will be placed into the area of ?controlled mo-
tion?, while G2 will then sort with ?attitudes?.   
6 Coreference  
The coreference annotation in OntoNotes connects 
coreferring instances of specific referring expres-
sions, meaning primarily NPs that introduce or 
access a discourse entity. For example, ?Elco In-
dustries, Inc.?, ?the Rockford, Ill. Maker of fasten-
ers?, and ?it? could all corefer. (Non-specific 
references like ?officials? in ?Later, officials re-
ported?? are not included, since coreference for 
them is frequently unclear.) In addition, proper 
premodifiers and verb phrases can be marked when 
coreferent with an NP, such as linking, ?when the 
company withdrew from the bidding? to ?the with-
drawal of New England Electric?.  
Unlike the coreference task as defined in the 
ACE program, attributives are not generally 
marked. For example, the ?veterinarian? NP would 
not be marked in ?Baxter Black is a large animal 
veterinarian?. Adjectival modifiers like ?Ameri-
can? in ?the American embassy? are also not sub-
ject to coreference. 
Appositives are annotated as a special kind of 
coreference, so that later processing will be able to 
supply and interpret the implicit copula link. 
All of the coreference annotation is being dou-
bly annotated and adjudicated. In our initial Eng-
lish batch, the average agreement scores between 
each annotator and the adjudicated results were 
91.8% for normal coreference and 94.2% for ap-
positives. 
7 Related and Future Work  
PropBank I (Palmer et al, 2005), developed at 
UPenn, captures predicate argument structure for 
verbs; NomBank provides predicate argument 
structure for nominalizations and other noun predi-
cates (Meyers et al, 2004).  PropBank II annota-
59
tion (eventuality ID?s, coarse-grained sense tags, 
nominal coreference and selected discourse con-
nectives) is being applied to a small (100K) paral-
lel Chinese/English corpus (Babko-Malaya et al, 
2004).  The OntoNotes representation extends 
these annotations, and allows eventual inclusion of 
additional shallow semantic representations for 
other phenomena, including temporal and spatial 
relations, numerical expressions, deixis, etc. One 
of the principal aims of OntoNotes is to enable 
automated semantic analysis.  The best current al-
gorithm for semantic role labeling for PropBank 
style annotation (Pradhan et al, 2005) achieves an 
F-measure of 81.0 using an SVM. OntoNotes will 
provide a large amount of new training data for 
similar efforts.   
Existing work in the same realm falls into two 
classes: the development of resources for specific 
phenomena or the annotation of corpora. An ex-
ample of the former is Berkeley?s FrameNet pro-
ject (Baker et al, 1998), which produces rich 
semantic frames, annotating a set of examples for 
each predicator (including verbs, nouns and adjec-
tives), and describing the network of relations 
among the semantic frames.  An example of the 
latter type is the Salsa project (Burchardt et al, 
2004), which produced a German lexicon based on 
the FrameNet semantic frames and annotated a 
large German newswire corpus.  A second exam-
ple, the Prague Dependency Treebank (Hajic et al, 
2001), has annotated a large Czech corpus with 
several levels of (tectogrammatical) representation, 
including parts of speech, syntax, and topic/focus 
information structure. Finally, the IL-Annotation 
project (Reeder et al, 2004) focused on the repre-
sentations required to support a series of increas-
ingly semantic phenomena across seven languages 
(Arabic, Hindi, English, Spanish, Korean, Japanese  
and French). In intent and in many details, 
OntoNotes is compatible with all these efforts, 
which may one day all participate in a larger multi-
lingual corpus integration effort.   
References  
O. Babko-Malaya, M. Palmer, N. Xue, A. Joshi, and S. Ku-
lick. 2004. Proposition Bank II: Delving Deeper, Frontiers 
in Corpus Annotation, Workshop, HLT/NAACL  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berke-
ley FrameNet Project. In Proceedings of COLING/ACL, 
pages 86-90. 
J. Chen and M. Palmer.  2005.  Towards Robust High Per-
formance Word Sense Disambiguation of English Verbs 
Using Rich Linguistic Features. In Proceedings of 
IJCNLP2005, pp. 933-944. 
B. Dorr and N. Habash.  2001.  Lexical Conceptual Structure 
Lexicons. In Calzolari et al ISLE-IST-1999-10647-WP2-
WP3, Survey of Major Approaches Towards Bilin-
gual/Multilingual Lexicons.  
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and M. 
Pinkal. 2006. Consistency and Coverage: Challenges for 
exhaustive semantic annotation. In Proceedings of DGfS-
06. 
C. Fellbaum (ed.). 1998. WordNet: An On-line Lexical Data-
base and Some of its Applications. MIT Press. 
R. Gabbard, M. Marcus, and S. Kulick. Fully Parsing the Penn 
Treebank. In Proceedings of HLT/NAACL 2006.  
A. Gangemi, N. Guarino, C. Masolo, A. Oltramari, and L. 
Schneider. 2002. Sweetening Ontologies with DOLCE. In 
Proceedings of EKAW  pp. 166-181. 
J. Hajic, B. Vidov?-Hladk?, and P. Pajas.  2001: The Prague 
Dependency Treebank: Annotation Structure and Support. 
Proceeding of the IRCS Workshop on Linguistic Data-
bases, pp. 105?114. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. 
Building a Large Annotated Corpus of English: The Penn 
Treebank. Computational Linguistics 19: 313-330. 
A. Meyers, R. Reeves, C Macleod, R. Szekely, V. Zielinska, 
B. Young, and R. Grishman. 2004. The NomBank Project: 
An Interim Report. Frontiers in Corpus Annotation, Work-
shop in conjunction with HLT/NAACL. 
I. Niles and A. Pease.  2001.  Towards a Standard Upper On-
tology.  Proceedings of the International Conference on 
Formal Ontology in Information Systems (FOIS-2001). 
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004. Differ-
ent Sense Granularities for Different Applications, 2nd 
Workshop on Scalable Natural Language Understanding 
Systems, at HLT/NAACL-04,  
M. Palmer, H. Dang and C. Fellbaum. 2006. Making Fine-
grained and Coarse-grained Sense Distinctions, Both 
Manually and Automatically, Journal of Natural Language 
Engineering, to appear. 
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: A Corpus Annotated with Semantic Roles, 
Computational Linguistics, 31(1). 
A. Philpot, E.. Hovy, and P. Pantel. 2005. The Omega Ontol-
ogy. Proceedings of the ONTOLEX Workshop at IJCNLP 
 S. Pradhan, W. Ward, K. Hacioglu, J. Martin, D. Jurafsky.  
2005.  Semantic Role Labeling Using Different Syntactic 
Views.  Proceedings of the ACL.  
F. Reeder, B. Dorr, D. Farwell, N. Habash, S. Helmreich, E.H. 
Hovy, L. Levin, T. Mitamura, K. Miller, O. Rambow, A. 
Siddharthan. 2004.  Interlingual Annotation for MT Devel-
opment. Proceedings of AMTA.  
60
Proceedings of ACL-08: HLT, pages 577?585,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A New String-to-Dependency Machine Translation Algorithm
with a Target Dependency Language Model
Libin Shen
BBN Technologies
Cambridge, MA 02138, USA
lshen@bbn.com
Jinxi Xu
BBN Technologies
Cambridge, MA 02138, USA
jxu@bbn.com
Ralph Weischedel
BBN Technologies
Cambridge, MA 02138, USA
weisched@bbn.com
Abstract
In this paper, we propose a novel string-to-
dependency algorithm for statistical machine
translation. With this new framework, we em-
ploy a target dependency language model dur-
ing decoding to exploit long distance word
relations, which are unavailable with a tra-
ditional n-gram language model. Our ex-
periments show that the string-to-dependency
decoder achieves 1.48 point improvement in
BLEU and 2.53 point improvement in TER
compared to a standard hierarchical string-to-
string system on the NIST 04 Chinese-English
evaluation set.
1 Introduction
In recent years, hierarchical methods have been suc-
cessfully applied to Statistical Machine Translation
(Graehl and Knight, 2004; Chiang, 2005; Ding and
Palmer, 2005; Quirk et al, 2005). In some language
pairs, i.e. Chinese-to-English translation, state-of-
the-art hierarchical systems show significant advan-
tage over phrasal systems in MT accuracy. For ex-
ample, Chiang (2007) showed that the Hiero system
achieved about 1 to 3 point improvement in BLEU
on the NIST 03/04/05 Chinese-English evaluation
sets compared to a start-of-the-art phrasal system.
Our work extends the hierarchical MT approach.
We propose a string-to-dependency model for MT,
which employs rules that represent the source side
as strings and the target side as dependency struc-
tures. We restrict the target side to the so called well-
formed dependency structures, in order to cover a
large set of non-constituent transfer rules (Marcu et
al., 2006), and enable efficient decoding through dy-
namic programming. We incorporate a dependency
language model during decoding, in order to exploit
long-distance word relations which are unavailable
with a traditional n-gram language model on target
strings.
For comparison purposes, we replicated the Hiero
decoder (Chiang, 2005) as our baseline. Our string-
to-dependency decoder shows 1.48 point improve-
ment in BLEU and 2.53 point improvement in TER
on the NIST 04 Chinese-English MT evaluation set.
In the rest of this section, we will briefly dis-
cuss previous work on hierarchical MT and de-
pendency representations, which motivated our re-
search. In section 2, we introduce the model of
string-to-dependency decoding. Section 3 illustrates
of the use of dependency language models. In sec-
tion 4, we describe the implementation details of our
MT system. We discuss experimental results in sec-
tion 5, compare to related work in section 6, and
draw conclusions in section 7.
1.1 Hierarchical Machine Translation
Graehl and Knight (2004) proposed the use of target-
tree-to-source-string transducers (xRS) to model
translation. In xRS rules, the right-hand-side(rhs)
of the target side is a tree with non-terminals(NTs),
while the rhs of the source side is a string with
NTs. Galley et al (2006) extended this string-to-tree
model by using Context-Free parse trees to represent
the target side. A tree could represent multi-level
transfer rules.
The Hiero decoder (Chiang, 2007) does not re-
quire explicit syntactic representation on either side
of the rules. Both source and target are strings with
NTs. Decoding is solved as chart parsing. Hiero can
be viewed as a hierarchical string-to-string model.
Ding and Palmer (2005) and Quirk et al (2005)
577
itwill
find
boy
the
interesting
Figure 1: The dependency tree for sentence the boy will
find it interesting
followed the tree-to-tree approach (Shieber and Sch-
abes, 1990) for translation. In their models, depen-
dency treelets are used to represent both the source
and the target sides. Decoding is implemented as
tree transduction preceded by source side depen-
dency parsing. While tree-to-tree models can rep-
resent richer structural information, existing tree-to-
tree models did not show advantage over string-to-
tree models on translation accuracy due to a much
larger search space.
One of the motivations of our work is to achieve
desirable trade-off between model capability and
search space through the use of the so called well-
formed dependency structures in rule representation.
1.2 Dependency Trees
Dependency trees reveal long-distance relations be-
tween words. For a given sentence, each word has a
parent word which it depends on, except for the root
word.
Figure 1 shows an example of a dependency tree.
Arrows point from the child to the parent. In this
example, the word find is the root.
Dependency trees are simpler in form than CFG
trees since there are no constituent labels. However,
dependency relations directly model semantic struc-
ture of a sentence. As such, dependency trees are a
desirable prior model of the target sentence.
1.3 Motivations for Well-Formed Dependency
Structures
We restrict ourselves to the so-called well-formed
target dependency structures based on the following
considerations.
Dynamic Programming
In (Ding and Palmer, 2005; Quirk et al, 2005),
there is no restriction on dependency treelets used in
transfer rules except for the size limit. This may re-
sult in a high dimensionality in hypothesis represen-
tation and make it hard to employ shared structures
for efficient dynamic programming.
In (Galley et al, 2004), rules contain NT slots and
combination is only allowed at those slots. There-
fore, the search space becomes much smaller. Fur-
thermore, shared structures can be easily defined
based on the labels of the slots.
In order to take advantage of dynamic program-
ming, we fixed the positions onto which another an-
other tree could be attached by specifying NTs in
dependency trees.
Rule Coverage
Marcu et al (2006) showed that many useful
phrasal rules cannot be represented as hierarchical
rules with the existing representation methods, even
with composed transfer rules (Galley et al, 2006).
For example, the following rule
? <(hong)Chinese, (DT(the) JJ(red))English>
is not a valid string-to-tree transfer rule since the red
is a partial constituent.
A number of techniques have been proposed to
improve rule coverage. (Marcu et al, 2006) and
(Galley et al, 2006) introduced artificial constituent
nodes dominating the phrase of interest. The bi-
narization method used by Wang et al (2007) can
cover many non-constituent rules also, but not all of
them. For example, it cannot handle the above ex-
ample. DeNeefe et al (2007) showed that the best
results were obtained by combing these methods.
In this paper, we use well-formed dependency
structures to handle the coverage of non-constituent
rules. The use of dependency structures is due to the
flexibility of dependency trees as a representation
method which does not rely on constituents (Fox,
2002; Ding and Palmer, 2005; Quirk et al, 2005).
The well-formedness of the dependency structures
enables efficient decoding through dynamic pro-
gramming.
2 String-to-Dependency Translation
2.1 Transfer Rules with Well-Formed
Dependency Structures
A string-to-dependency grammar G is a 4-tuple
G =< R, X, Tf , Te >, where R is a set of transfer
rules. X is the only non-terminal, which is similar
to the Hiero system (Chiang, 2007). Tf is a set of
578
terminals in the source language, and Te is a set of
terminals in the target language1 .
A string-to-dependency transfer rule R ? R is a
4-tuple R =< Sf , Se, D,A >, where Sf ? (Tf ?
{X})+ is a source string, Se ? (Te ? {X})+ is a
target string, D represents the dependency structure
for Se, and A is the alignment between Sf and Se.
Non-terminal alignments in A must be one-to-one.
In order to exclude undesirable structures, we
only allow Se whose dependency structure D is
well-formed, which we will define below. In addi-
tion, the same well-formedness requirement will be
applied to partial decoding results. Thus, we will be
able to employ shared structures to merge multiple
partial results.
Based on the results in previous work (DeNeefe
et al, 2007), we want to keep two kinds of depen-
dency structures. In one kind, we keep dependency
trees with a sub-root, where all the children of the
sub-root are complete. We call them fixed depen-
dency structures because the head is known or fixed.
In the other, we keep dependency structures of sib-
ling nodes of a common head, but the head itself is
unspecified or floating. Each of the siblings must be
a complete constituent. We call them floating de-
pendency structures. Floating structures can repre-
sent many linguistically meaningful non-constituent
structures: for example, like the red, a modifier of
a noun. Only those two kinds of dependency struc-
tures are well-formed structures in our system.
Furthermore, we operate over well-formed struc-
tures in a bottom-up style in decoding. However,
the description given above does not provide a clear
definition on how to combine those two types of
structures. In the rest of this section, we will pro-
vide formal definitions of well-formed structures and
combinatory operations over them, so that we can
easily manipulate well-formed structures in decod-
ing. Formal definitions also allow us to easily ex-
tend the framework to incorporate a dependency lan-
guage model in decoding. Examples will be pro-
vided along with the formal definitions.
Consider a sentence S = w1w2...wn. Let
d1d2...dn represent the parent word IDs for each
word. For example, d4 = 2 means that w4 depends
1We ignore the left hand side here because there is only one
non-terminal X . Of course, this formalism can be extended to
have multiple NTs.
itwill
find
boy
the
find
boy
(a) (b) (c)
Figure 2: Fixed dependency structures
boy will
the
interestingit
(a) (b)
Figure 3: Floating dependency structures
on w2. If wi is a root, we define di = 0.
Definition 1 A dependency structure di..j is fixed
on head h, where h ? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
In addition, we say the category of di..j is
(?, h,?), where ? means this field is undefined.
Definition 2 A dependency structure di...dj is float-
ing with children C , for a non-empty set C ?
{i, ..., j}, or floating for short, if and only if it meets
the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
We say the category of di..j is (C,?,?) if j < h,
or (?,?, C) otherwise. A category is composed of
the three fields (A, h,B), where h is used to repre-
sent the head, and A and B are designed to model
left and right dependents of the head respectively.
A dependency structure is well-formed if and
only if it is either fixed or floating.
Examples
We can represent dependency structures with
graphs. Figure 2 shows examples of fixed structures,
Figure 3 shows examples of floating structures, and
Figure 4 shows ill-formed dependency structures.
It is easy to verify that the structures in Figures
2 and 3 are well-formed. 4(a) is ill-formed because
579
interestingwill
findfind
boy
(a) (b)
Figure 4: Ill-formed dependency structures
boy does not have its child word the in the tree. 4(b)
is ill-formed because it is not a continuous segment.
As for the example the red mentioned above, it is
a well-formed floating dependency structure.
2.2 Operations on Well-Formed Dependency
Structures and Categories
One of the purposes of introducing floating depen-
dency structures is that siblings having a common
parent will become a well-defined entity, although
they are not considered a constituent. We always
build well-formed partial structures on the target
side in decoding. Furthermore, we combine partial
dependency structures in a way such that we can ob-
tain all possible well-formed but no ill-formed de-
pendency structures during bottom-up decoding.
The solution is to employ categories introduced
above. Each well-formed dependency structure has
a category. We can apply four combinatory oper-
ations over the categories. If we can combine two
categories with a certain category operation, we can
use a corresponding tree operation to combine two
dependency structures. The category of the com-
bined dependency structure is the result of the com-
binatory category operations.
We first introduce three meta category operations.
Two of them are unary operations, left raising (LR)
and right raising (RR), and one is the binary opera-
tion unification (UF).
First, the raising operations are used to turn a
completed fixed structure into a floating structure.
It is easy to verify the following theorem according
to the definitions.
Theorem 1 A fixed structure with category
(?, h,?) for span [i, j] is also a floating structure
with children {h} if there are no outside words
depending on word h.
?k /? [i, j], dk 6= h. (1)
Therefore we can always raise a fixed structure if we
assume it is complete, i.e. (1) holds.
itwill
find
boy
the
interesting
LA
LA LA RA RA
LC RC
Figure 5: A dependency tree with flexible combination
Definition 3 Meta Category Operations
? LR((?, h,?)) = ({h},?,?)
? RR((?, h,?)) = (?,?, {h})
? UF((A1, h1, B1), (A2, h2, B2)) = NORM((A1 t
A2, h1 t h2, B1 t B2))
Unification is well-defined if and only if we can
unify all three elements and the result is a valid fixed
or floating category. For example, we can unify a
fixed structure with a floating structure or two float-
ing structures in the same direction, but we cannot
unify two fixed structures.
h1 t h2 =
?
?
?
h1 if h2 = ?
h2 if h1 = ?
undefined otherwise
A1 t A2 =
?
?
?
A1 if A2 = ?
A2 if A1 = ?
A1 ?A2 otherwise
NORM((A, h, B)) =
?
?
?
?
?
?
?
(?, h,?) if h 6= ?
(A,?,?) if h = ?, B = ?
(?,?, B) if h = ?, A = ?
undefined otherwise
Next we introduce the four tree operations on de-
pendency structures. Instead of providing the formal
definition, we use figures to illustrate these opera-
tions to make it easy to understand. Figure 1 shows
a traditional dependency tree. Figure 5 shows the
four operations to combine partial dependency struc-
tures, which are left adjoining (LA), right adjoining
(RA), left concatenation (LC) and right concatena-
tion (RC).
Child and parent subtrees can be combined with
adjoining which is similar to the traditional depen-
dency formalism. We can either adjoin a fixed struc-
ture or a floating structure to the head of a fixed
structure.
Complete siblings can be combined via concate-
nation. We can concatenate two fixed structures, one
fixed structure with one floating structure, or two
floating structures in the same direction. The flex-
ibility of the order of operation allows us to take ad-
580
will
find
boy
the
LA
LA
LA
will
find
boy
the
LA
LA
LC
2
3 2
1
1
3
(b)(a)
Figure 6: Operations over well-formed structures
vantage of various translation fragments encoded in
transfer rules.
Figure 6 shows alternative ways of applying op-
erations on well-formed structures to build larger
structures in a bottom-up style. Numbers represent
the order of operation.
We use the same names for the operations on cat-
egories for the sake of convenience. We can easily
use the meta category operations to define the four
combinatory operations. The definition of the oper-
ations in the left direction is as follows. Those in the
right direction are similar.
Definition 4 Combinatory category operations
LA((A1,?,?), (?, h2,?))
= UF((A1,?,?), (?, h2,?))
LA((?, h1,?), (?, h2,?))
= UF(LR((?, h1,?)), (?, h2,?))
LC((A1,?,?), (A2,?,?))
= UF((A1,?,?), (A2,?,?))
LC((A1,?,?), (?, h2,?))
= UF((A1,?,?), LR((?, h2,?)))
LC((?, h1,?), (A2,?,?))
= UF(LR((?, h1,?)), (A2,?,?))
LC((?, h1,?), (?, h2,?))
= UF(LR((?, h1,?)), LR((?, h2,?)))
It is easy to verify the soundness and complete-
ness of category operations based on one-to-one
mapping of the conditions in the definitions of cor-
responding operations on dependency structures and
on categories.
Theorem 2 (soundness and completeness)
Suppose X and Y are well-formed dependency
structures. OP(cat(X), cat(Y )) is well-defined for
a given operation OP if and only if OP(X,Y ) is
well-defined. Furthermore,
cat(OP(X, Y )) = OP(cat(X), cat(Y ))
Suppose we have a dependency tree for a red apple,
where both a and red depend on apple. There are
two ways to compute the category of this string from
the bottom up.
cat(Da red apple)
= LA(cat(Da), LA(cat(Dred), cat(Dapple)))
= LA(LC(cat(Da), cat(Dred)), cat(Dapple))
Based on Theorem 2, it follows that combinatory
operation of categories has the confluence property,
since the result dependency structure is determined.
Corollary 1 (confluence) The category of a well-
formed dependency tree does not depend on the or-
der of category calculation.
With categories, we can easily track the types of
dependency structures and constrain operations in
decoding. For example, we have a rule with depen-
dency structure find ? X , where X right adjoins
to find. Suppose we have two floating structures2 ,
cat(X1) = ({he, will},?,?)
cat(X2) = (?,?, {it, interesting})
We can replace X by X2, but not by X1 based on
the definition of category operations.
2.3 Rule Extraction
Now we explain how we get the string-to-
dependency rules from training data. The procedure
is similar to (Chiang, 2007) except that we maintain
tree structures on the target side, instead of strings.
Given sentence-aligned bi-lingual training data,
we first use GIZA++ (Och and Ney, 2003) to gen-
erate word level alignment. We use a statistical CFG
parser to parse the English side of the training data,
and extract dependency trees with Magerman?s rules
(1995). Then we use heuristic rules to extract trans-
fer rules recursively based on the GIZA alignment
and the target dependency trees. The rule extraction
procedure is as follows.
1. Initialization:
All the 4-tuples (P i,jf , P m,ne , D,A) are valid
phrase alignments, where source phrase P i,jf is
2Here we use words instead of word indexes in categories to
make the example easy to understand.
581
it
find
interesting(D1)
(D2)
it
X
find
interesting
(D?)
Figure 7: Replacing it with X in D1
aligned to target phrase P m,ne under alignment3
A, and D, the dependency structure for P m,ne ,
is well-formed. All valid phrase templates are
valid rules templates.
2. Inference:
Let (P i,jf , P m,ne , D1, A) be a valid rule tem-
plate, and (P p,qf , P s,te , D2, A) a valid phrase
alignment, where [p, q] ? [i, j], [s, t] ? [m,n],
D2 is a sub-structure of D1, and at least one
word in P i,jf but not in P
p,q
f is aligned.
We create a new valid rule template
(P ?f , P ?e, D?, A), where we obtain P ?f by
replacing P p,qf with label X in P
i,j
f , and obtain
P ?e by replacing P s,te with X in P m,ne . Further-
more, We obtain D? by replacing sub-structure
D2 with X in D14. An example is shown in
Figure 7.
Among all valid rule templates, we collect those
that contain at most two NTs and at most seven ele-
ments in the source as transfer rules in our system.
2.4 Decoding
Following previous work on hierarchical MT (Chi-
ang, 2005; Galley et al, 2006), we solve decoding
as chart parsing. We view target dependency as the
hidden structure of source fragments.
The parser scans all source cells in a bottom-up
style, and checks matched transfer rules according to
the source side. Once there is a completed rule, we
build a larger dependency structure by substituting
component dependency structures for corresponding
NTs in the target dependency structure of rules.
Hypothesis dependency structures are organized
in a shared forest, or AND-OR structures. An AND-
3By P i,jf aligned to P
m,n
e , we mean all words in P i,jf are
either aligned to words in P m,ne or unaligned, and vice versa.
Furthermore, at least one word in P i,jf is aligned to a word in
P m,ne .
4If D2 is a floating structure, we need to merge several
dependency links into one.
structure represents an application of a rule over
component OR-structures, and an OR-structure rep-
resents a set of alternative AND-structures with the
same state. A state means a n-tuple that character-
izes the information that will be inquired by up-level
AND-structures.
Supposing we use a traditional tri-gram language
model in decoding, we need to specify the leftmost
two words and the rightmost two words in a state.
Since we only have a single NT X in the formalism
described above, we do not need to add the NT la-
bel in states. However, we need to specify one of
the three types of the dependency structure: fixed,
floating on the left side, or floating on the right side.
This information is encoded in the category of the
dependency structure.
In the next section, we will explain how to ex-
tend categories and states to exploit a dependency
language model during decoding.
3 Dependency Language Model
For the dependency tree in Figure 1, we calculate the
probability of the tree as follows
Prob = PT (find)
?PL(will|find-as-head)
?PL(boy|will, find-as-head)
?PL(the|boy-as-head)
?PR(it|find-as-head)
?PR(interesting|it, find-as-head)
Here PT (x) is the probability that word x is the
root of a dependency tree. PL and PR are left and
right side generative probabilities respectively. Let
wh be the head, and wL1wL2 ...wLn be the children
on the left side from the nearest to the farthest. Sup-
pose we use a tri-gram dependency LM,
PL(wL1wL2 ...wLn |wh-as-head)
= PL(wL1 |wh-as-head)
?PL(wL2 |wL1 , wh-as-head)
?...? PL(wLn |wLn?1 , wLn?2) (2)
wh-as-head represents wh used as the head, and
it is different from wh in the dependency language
model. The right side probability is similar.
In order to calculate the dependency language
model score, or depLM score for short, on the fly for
582
partial hypotheses in a bottom-up decoding, we need
to save more information in categories and states.
We use a 5-tuple (LF,LN, h,RN,RF ) to repre-
sent the category of a dependency structure. h rep-
resents the head. LF and RF represent the farthest
two children on the left and right sides respectively.
Similarly, LN and RN represent the nearest two
children on the left and right sides respectively. The
three types of categories are as follows.
? fixed: (LF,?, h,?, RF )
? floating left: (LF,LN,?,?,?)
? floating right: (?,?,?, RN,RF )
Similar operations as described in Section 2.2 are
used to keep track of the head and boundary child
nodes which are then used to compute depLM scores
in decoding. Due to the limit of space, we skip the
details here.
4 Implementation Details
Features
1. Probability of the source side given the target
side of a rule
2. Probability of the target side given the source
side of a rule
3. Word alignment probability
4. Number of target words
5. Number of concatenation rules used
6. Language model score
7. Dependency language model score
8. Discount on ill-formed dependency structures
We have eight features in our system. The values of
the first four features are accumulated on the rules
used in a translation. Following (Chiang, 2005),
we also use concatenation rules like X ? XX for
backup. The 5th feature counts the number of con-
catenation rules used in a translation. In our sys-
tem, we allow substitutions of dependency struc-
tures with unmatched categories, but there is a dis-
count for such substitutions.
Weight Optimization
We tune the weights with several rounds of
decoding-optimization. Following (Och, 2003), the
k-best results are accumulated as the input of the op-
timizer. Powell?s method is used for optimization
with 20 random starting points around the weight
vector of the last iteration.
Rescoring
We rescore 1000-best translations (Huang and
Chiang, 2005) by replacing the 3-gram LM score
with the 5-gram LM score computed offline.
5 Experiments
We carried out experiments on three models.
? baseline: replication of the Hiero system.
? filtered: a string-to-string MT system as in
baseline. However, we only keep the transfer
rules whose target side can be generated by a
well-formed dependency structure.
? str-dep: a string-to-dependency system with a
dependency LM.
We take the replicated Hiero system as our
baseline because it is the closest to our string-to-
dependency model. They have similar rule extrac-
tion and decoding algorithms. Both systems use
only one non-terminal label in rules. The major dif-
ference is in the representation of target structures.
We use dependency structures instead of strings;
thus, the comparison will show the contribution of
using dependency information in decoding.
All models are tuned on BLEU (Papineni et al,
2001), and evaluated on both BLEU and Translation
Error Rate (TER) (Snover et al, 2006) so that we
could detect over-tuning on one metric.
We used part of the NIST 2006 Chinese-
English large track data as well as some LDC
corpora collected for the DARPA GALE program
(LDC2005E83, LDC2006E34 and LDC2006G05)
as our bilingual training data. It contains about
178M/191M words in source/target. Hierarchical
rules were extracted from a subset which has about
35M/41M words5, and the rest of the training data
were used to extract phrasal rules as in (Och, 2003;
Chiang, 2005). The English side of this subset was
also used to train a 3-gram dependency LM. Tra-
ditional 3-gram and 5-gram LMs were trained on a
corpus of 6G words composed of the LDC Gigaword
corpus and text downloaded from Web (Bulyko et
al., 2007). We tuned the weights on NIST MT05
and tested on MT04.
5It includes eight corpora: LDC2002E18, LDC2003E07,
LDC2004T08 HK News, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E34, and LDC2006G05
583
Model #Rules
baseline 140M
filtered 26M
str-dep 27M
Table 1: Number of transfer rules
Model BLEU% TER%lower mixed lower mixed
Decoding (3-gram LM)
baseline 38.18 35.77 58.91 56.60
filtered 37.92 35.48 57.80 55.43
str-dep 39.52 37.25 56.27 54.07
Rescoring (5-gram LM)
baseline 40.53 38.26 56.35 54.15
filtered 40.49 38.26 55.57 53.47
str-dep 41.60 39.47 55.06 52.96
Table 2: BLEU and TER scores on the test set.
Table 1 shows the number of transfer rules ex-
tracted from the training data for the tuning and
test sets. The constraint of well-formed dependency
structures greatly reduced the size of the rule set. Al-
though the rule size increased a little bit after incor-
porating dependency structures in rules, the size of
string-to-dependency rule set is less than 20% of the
baseline rule set size.
Table 2 shows the BLEU and TER scores
on MT04. On decoding output, the string-to-
dependency system achieved 1.48 point improve-
ment in BLEU and 2.53 point improvement in
TER compared to the baseline hierarchical string-
to-string system. After 5-gram rescoring, it achieved
1.21 point improvement in BLEU and 1.19 improve-
ment in TER. The filtered model does not show im-
provement on BLEU. The filtered string-to-string
rules can be viewed the string projection of string-
to-dependency rules. It means that just using depen-
dency structure does not provide an improvement on
performance. However, dependency structures al-
low the use of a dependency LM which gives rise to
significant improvement.
6 Discussion
The well-formed dependency structures defined here
are similar to the data structures in previous work on
mono-lingual parsing (Eisner and Satta, 1999; Mc-
Donald et al, 2005). However, here we have fixed
structures growing on both sides to exploit various
translation fragments learned in the training data,
while the operations in mono-lingual parsing were
designed to avoid artificial ambiguity of derivation.
Charniak et al (2003) described a two-step string-
to-CFG-tree translation model which employed a
syntax-based language model to select the best
translation from a target parse forest built in the first
step. Only translation probability P (F |E) was em-
ployed in the construction of the target forest due to
the complexity of the syntax-based LM. Since our
dependency LM models structures over target words
directly based on dependency trees, we can build a
single-step system. This dependency LM can also
be used in hierarchical MT systems using lexical-
ized CFG trees.
The use of a dependency LM in MT is similar to
the use of a structured LM in ASR (Xu et al, 2002),
which was also designed to exploit long-distance re-
lations. The depLM is used in a bottom-up style,
while SLM is employed in a left-to-right style.
7 Conclusions and Future Work
In this paper, we propose a novel string-to-
dependency algorithm for statistical machine trans-
lation. For comparison purposes, we replicated
the Hiero system as described in (Chiang, 2005).
Our string-to-dependency system generates 80%
fewer rules, and achieves 1.48 point improvement in
BLEU and 2.53 point improvement in TER on the
decoding output on the NIST 04 Chinese-English
evaluation set.
Dependency structures provide a desirable plat-
form to employ linguistic knowledge in MT. In the
future, we will continue our research in this direction
to carry out translation with deeper features, for ex-
ample, propositional structures (Palmer et al, 2005).
We believe that the fixed and floating structures pro-
posed in this paper can be extended to model predi-
cates and arguments.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
We are grateful to Roger Bock, Ivan Bulyko, Mike
Kayser, John Makhoul, Spyros Matsoukas, Antti-
Veikko Rosti, Rich Schwartz and Bing Zhang for
their help in running the experiments and construc-
tive comments to improve this paper.
584
References
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. In Proceedings of MT Summit IX.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based mt learn from phrase-based
mt? In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proceedings of the 43th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 541?548, Ann Arbor, Michigan, June.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL).
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the 2002 Conference of
Empirical Methods in Natural Language Processing.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proceedings of the
2004 Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefea,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic models. In COLING-
ACL ?06: Proceedings of 44th Annual Meeting of the
Association for Computational Linguistics and 21st
Int. Conf. on Computational Linguistics.
J. Graehl and K. Knight. 2004. Training tree transducers.
In Proceedings of the 2004 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
In Proceedings of the 9th International Workshop on
Parsing Technologies.
D. Magerman. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phraases. In Proceedings of the
2006 Conference of Empirical Methods in Natural
Language Processing.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the 43th Annual Meeting of the Association
for Computational Linguistics (ACL).
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Erhard W. Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 271?279, Ann Arbor, Michigan,
June.
S. Shieber and Y. Schabes. 1990. Synchronous tree ad-
joining grammars. In Proceedings of COLING ?90:
The 13th Int. Conf. on Computational Linguistics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas.
W. Wang, K. Knight, and D. Marcu. 2007. Binarizing
syntax trees to improve syntax-based machine transla-
tion accuracy. In Proceedings of the 2007 Conference
of Empirical Methods in Natural Language Process-
ing.
P. Xu, C. Chelba, and F. Jelinek. 2002. A study on richer
syntactic dependencies for structured language model-
ing. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL).
585
Cross-lingual Information Retrieval using Hidden Markov Models 
Jinxi Xu 
BBN Technologies 
70 Fawcett St. 
Cambridge, MA, USA 02138 
jxu@bbn.com 
Ralph Weischedel 
BBN Technologies 
70 Fawcett St. 
Cambridge, MA, USA 02138 
weischedel @bbn.com 
Abstract 
This paper presents empirical results in 
cross-lingual information retrieval using 
English queries to access Chinese 
documents (TREC-5 and TREC-6) and 
Spanish documents (TREC-4). Since our 
interest is in languages where resources 
may be minimal, we use an integrated 
probabilistic model that requires only a 
bilingual dictionary as a resource. We 
explore how a combined probability 
model of term translation and retrieval can 
reduce the effect of translation ambiguity. 
In addition, we estimate an upper bound 
on performance, if translation ambiguity 
were a solved problem. We also measure 
performance as a function of bilingual 
dictionary size. 
1 Introduction 
Cross-language information retrieval (CLIR) can 
serve both those users with a smattering of 
knowledge of other languages and also those 
fluent in them. For those with limited 
knowledge of the other language(s), CLIR offers 
a wide pool of documents, even though the user 
does not have the skill to prepare ahigh quality 
query in the other language(s). Once documents 
are retrieved, machine translation or human 
translation, if desired, can make the documents 
usable. For the user who is fluent in two or 
more languages, even though e/she may be able 
to formulate good queries in each of the source 
languages, CLIR relieves the user from having 
to do so. 
Most CLIR studies have been based on a variant 
of tf-idf; our experiments instead use a hidden 
Markov model (HMM) to estimate the 
probability that a document is relevant given the 
query. We integrated two simple estimates of 
term translation probability into the mono- 
lingual HMM model, giving an estimate of the 
probability that a document is relevant given a 
query in another language. 
In this paper we address the following questions: 
? How can a combined probability model of 
term translation and retrieval minimize the 
effect of translation ambiguity? (Sections 3, 
5, 6, 7, and 10) 
? What is the upper bound performance using 
bilingual dictionary lookup for term 
translation? (Section 8) 
? How much does performance d grade due to 
omissions from the bilingual dictionary and 
how does performance vary with size of 
such a dictionary? (Sections 8-9) 
All experiments were performed using a 
common baseline, an HMM-based (mono- 
lingual) indexing and retrieval engine. In order 
to design controlled experiments for the 
questions above, the IR system was run without 
sophisticated query expansion techniques. 
Our experiments are based on the Chinese 
materials of TREC-5 and TREC-6 and the 
Spanish materials of TREC-4. 
2 HMM for Mono-Lingual Retrieval 
Following Miller et al, 1999, the IR system 
ranks documents according to the probability 
that a document D is relevant given the query Q, 
P(D is R IQ). Using Bayes Rule, and the fact 
that P(Q) is constant for a given query, and our 
initial assumption of a uniform a priori 
95 
probability that a document is relevant, ranking 
documents according to P(Q\[D is R) is the same 
as ranking them according to P(D is RIQ). The 
approach therefore estimates the probability that 
a query Q is generated, given the document D is 
relevant. (A glossary of symbols used appears 
below.) 
We use x to represent the language (e.g. 
English) for which retrieval is carried out. 
According to that model of monolingual 
retrieval, it can be shown that 
p(Q \[ D is R) = I I  (aP(W \[ Gx) + (1- a)e(w ID)), 
W inQ 
where W's are query words in Q. Miller et al 
estimated probabilities as follows: 
* The transition probability a is 0.7 using the 
EM algorithm (Rabiner, 1989) on the TREC4 
ad-hoc query set. 
number of occurrences of W in C x 
? e0e  IGx)= length of Cx 
which is the general language probability for 
word W in language x.
number of occurrences of W in D 
? e (WlD)  = 
length of D 
In principle, any large corpus Cx that is 
representative of language x can be used in 
computing the general language probabilities. 
In practice, the collection to be searched is
used for that purpose. The length of a 
Q a query 
English query 
a document 
a document in foreign language y 
document is relevant 
a word 
an English corpus 
a corpus in language x 
QX 
D 
Dr 
D isR  
W 
Gx 
Cx 
Wx 
BL 
an English word 
foreign language y 
Wy a word in 
a bilingual dictionary 
A Glossary of Notation used in Formulas 
collection is the sum of the document 
lengths. 
3 HMM for Cross-lingual IR 
For CLIR we extend the query generation 
process o that a document Dy written in 
language y can generate a query Qx in language 
x. We use Wx to denote aword in x and Wy to 
denote aword in y. As before, to model general 
query words from language x, we estimate P(Wx 
\]Gx) by using a large corpus Cx in language x.
Also as before, we estimate P(WyIDy) to be the 
sample distribution of Wy in Dy. 
We use P(Wx\[Wy) to denote the probability that 
Wy is translated as Wx. Though terms often 
should not be translated independent of their 
context, we make that simplifying assumption 
here. We assume that the possible translations 
are specified by a bilingual lexicon BL. Since 
the event spaces for Wy's in P(WyIDy) are 
mutually exclusive, we can compute the output 
probability P(WxIDy): 
P(WxIDy)= ~P(WylDy)P(WxIWy)  
W inBL y 
We compute P(Q~IDy is R) as below: 
P(Qx IDr /sR) = I~I(aetwx IG,)+O-a)P(W~ IDy)) 
w.~,o. 
The above model generates queries from 
documents, that is, it attempts o determine how 
likely a particular query is given a relevant 
document. The retrieval system, however, can 
use either query translation or document 
translation. We chose query translation over 
document translation for its flexibility, since it 
allowed us to experiment with a new method of 
estimating the translation probabilities without 
changing the index structure. 
4 Experimental Set-up 
For retrieval using English queries to search 
Chinese documents, we used the TREC5 and 
TREC6 Chinese data which consists of 164,789 
documents from the Xinhua News Agency and 
People's Daily, averaging 450 Chinese 
characters/document. Each of the TREC topics 
has three Chinese fields: title, description and 
96 
narrative, plus manually translated, English 
versions of each. We corrected some of the 
English queries that contained errors, such as 
"Dali Lama" instead of the correct "Dalai Lama" 
and "Medina" instead of "Medellin." Stop 
words and stop phrases were removed. We 
created three versions of Chinese queries and 
three versions of English queries: short (title 
only), medium (title and description), and long 
(all three fields). 
For retrieval using English queries to search 
Spanish documents, we used the TREC4 
Spanish data, which has 57,868 documents. It 
has 25 queries in Spanish with manual 
translations toEnglish. We will denote the 
Chinese data sets as Trec5C and Trec6C and the 
Spanish data set as Trec4S. 
We used a Chinese-English lexicon from the 
Linguistic Data Consortium (LDC). We pre- 
processed the dictionary as follows: 
1. Stem Chinese words via a simple algorithm 
to remove common suffixes and prefixes. 
2. Use the Porter stemmer on English words. 
3. Split English phrases into words. If an 
English phrase is a translation for a Chinese 
word, each word in the phrase is taken as a 
separate translation for the Chinese word. ~ 
4. Estimate the translation probabilities. (We 
first report results assuming a uniform 
distribution on a word's translations. If a 
Chinese word c has n translations el, e2, ...en. 
each of them will be assigned equal probability, 
i.e., P(ei lc)=l/n.  Section 10 supplements this 
with a corpus-based distribution.) 
5. Invert he lexicon to make it an English- 
Chinese lexicon. That is, for each English word 
e, we associate it with a list of Chinese words cl, 
c2, ... Cm together with non-zero translation 
probabilities P( elc~). 
The resulting English-Chinese l xicon has 
80,000 English words. On average, each 
English word has 2.3 Chinese translations. 
For Spanish, we downloaded a bilingual 
English-Spanish lexicon from the Internet 
(http://www.activa.arrakis.es) containing around 
22,000 English words (16,000 English stems) 
and processed it similarly. Each English word 
has around 1.5 translations on average. A co- 
occurrence based stemmer (Xu and Croft, 1998) 
was used to stem Spanish words. One 
difference from the treatment of Chinese is to 
include the English word as one of its own 
translations in addition to its Spanish 
translations in the lexicon. This is useful for 
translating proper nouns, which often have 
identical spellings in English and Spanish but 
are routinely excluded from a lexicon. 
One problem is the segmentation f Chinese 
text, since Chinese has no spaces between 
words. In these initial experiments, we relied on 
a simple sub-string matching algorithm to 
extract words from Chinese text. To extract 
words from a string of Chinese characters, the 
algorithm examines any sub-string of length 2 or 
greater and recognizes it as a Chinese word if it 
is in a predefined dictionary (the LDC lexicon in 
our case). In addition, any single character 
which is not part of any recognized Chinese 
words in the first step is taken as a Chinese 
word. Note that this algorithm can extract a
compound Chinese word as well as its 
components. For example, the Chinese word for 
"particle physics" as well as the Chinese words 
for "particle" and "physics" will be extracted. 
This seems desirable because it ensures the 
retrieval algorithm will match both the 
compound words as well as their components. 
The above algorithm was used in processing 
Chinese documents and Chinese queries. 
English data from the 2 GB of TREC disks l&2 
was used to estimate P(WlG,..ngti~h), the general 
language probabilities for English words. The 
evaluation metric used in this study is the 
average precision using the trec_eval program 
(Voorhees and Harman, 1997). Mono-lingual 
retrieval results (using the Chinese and Spanish 
queries) provided our baseline, with the HMM 
retrieval system (Miller et al 1999). 
1 Clearly, this is not correct; however, it 
simplified implementation. 
97 
5 Retrieval Results 
Table 2 reports average precision for mono- 
lingual retrieval, average precision for cross- 
lingual, and the relative performance ratio of 
cross-lingual retrieval to mono-lingual. 
Relative performance of cross-lingual IR varies 
between 67% and 84% of mono-lingual IR. 
Trec6 Chinese queries have a somewhat higher 
relative performance than Trec5 Chinese 
queries. Longer queries have higher elative 
performance than short queries in general. 
Overall, cross-lingual performance using our 
HMM retrieval model is around 76% of mono- 
lingual retrieval. A comparison of our mono- 
lingual results with Trec5 Chinese and Trec6 
Chinese results published in the TREC 
proceedings (Voorhees and Harman, 1997, 
1998) shows that our mono-lingual results are 
close to the top performers in the TREC 
conferences. Our Spanish mono-lingual 
performance is also comparable tothe top 
automatic runs of the TREC4 Spanish task 
(Harrnan, 1996). Since these mono-lingual 
results were obtained without using 
sophisticated query processing techniques such 
as query expansion, we believe the mono-lingual 
results form a valid baseline. 
Query sets Mono- Cross- % of 
lingual lingual Mono- 
lingual 
Trec5C-short 0.2830 0.1889 67% 
Trec5C-medium 0.3427 0.2449 72% 
Trec5C-long 0.3750 0.2735 73% 
Trec6C-short 0.3423 0.2617 77% 
Trec6C-medium 0.4606 0.3872 84% 
Trec6C-long 0.5104 0.4206 82% 
Trec4S 0.2252 0.1729 77% 
Table 2: Comparing mono-lingual and cross- 
lingual retrieval performance. The scores on 
the monolingual and cross-lingual columns are 
average precision. 
6 Comparison with other Methods 
In this section we compare our approach with 
two other approaches. One approach is "simple 
substitution", i.e., replacing a query term with 
all its translations and treating the translated 
query as a bag of words in mono-lingual 
retrieval. Suppose we have a simple query 
Q=(a, b), the translations for a are al, a2, a3, and 
the translations for b are bl, b2. The translated 
query would be (at, a2, a3, b~, b2). Since all terms 
are treated as equal in the translated query, this 
gives terms with more translations (potentially 
the more common terms) more credit in 
retrieval, even though such terms hould 
potentially be given less credit if they are more 
common. Also, a document matching different 
translations ofone term in the original query 
may be ranked higher than a document that 
matches translations ofdifferent terms in the 
original query. That is, a document that 
contains terms at, a2 and a3 may be ranked 
higher than a document which contains terms at 
and bl. However, the second ocument is more 
likely to be relevant since correct translations of
the query terms are more likely to co-occur 
(Ballesteros and Croft, 1998). 
A second method is to structure the translated 
query, separating the translations for one term 
from translations for other terms. This approach 
limits how much credit he retrieval algorithm 
can give to a single term in the original query 
and prevents the translations ofone or a few 
terms from swamping the whole query. There 
are several variations of such a method 
(Ballesteros and Croft, 1998; Pirkola, 1998; Hull 
1997). One such method is to treat different 
translations ofthe same term as synonyms. 
Ballesteros, for example, used the INQUERY 
(Callan et al 1995) synonym operator to group 
translations ofdifferent query terms. However, 
if a term has two translations inthe target 
language, it will treat hem as equal even though 
one of them is more likely to be the correct 
translation than the other. By contrast, our 
HMM approach supports translation 
probabilities. The synonym approach is
equivalent to changing all non-zero translation 
probabilities P(W~\[ Wy)'s to 1 in our retrieyal 
function. Even estimating uniform translation 
probabilities gives higher weights to 
unambiguous translations and lower weights to 
highly ambiguous translations. 
98 
These intuitions are supported empirically by the 
results in Table 3. We can see that the HMM 
performs best for every query set. Simple 
substitution performs worst. The synonym 
approach is significantly better than substitution, 
but is consistently worse than the HMM 
translations were kept in disambiguation, the 
improvement would be 4% for Trec6C-medium. 
The results of this manual disambiguation 
suggest that there are limits to automatic 
disambiguation. 
Substi- Synonym HMM 
tution 
Trec5C-long 0.0391 0.2306 0.2735 
Trec6C-long 0.0941 0.3842 0.4206 
Trec4S 0.0935 0.1594 0.1729 
Table 3: Comparing different methods of 
query translation. All numbers are average 
precision. 
7 Impact of Translation Ambiguity 
To get an upper bound on performance ofany 
disambiguation technique, we manually 
disambiguated the Trec5C-medium, Trec6C- 
medium and Trec4S queries. That is, for each 
English query term, a native Chinese or Spanish 
speaker scanned the list of translations in the 
bilingual exicon and kept one translation 
deemed to be the best for the English term and 
discarded the rest. If none of the translations 
was correct, the first one was chosen. 
The results in Table 4 show that manual 
disambiguation improves performance by 17% 
on Trec5C, 4% on Trec4S, but not at all on 
Trec6C. Furthermore, the improvement on 
Trec5C appears to be caused by big 
improvements for a small number of queries. 
The one-sided t-test (Hull, 1993) at significance 
level 0.05 indicated that the improvement on 
Trec5C is not statistically significant. 
It seems urprising that disambiguation does not 
help at all for Trec6C. We found that many 
terms have more than one valid translation. For 
example, the word "flood" (as in "flood 
control") has 4 valid Chinese translations. Using 
all of them achieves the desirable ffect of query 
expansion. It appears that for Trec6C, the benefit 
of disambiguation is cancelled by choosing only 
one of several alternatives, discarding those 
other good translations. If multiple correct 
Query sets 
Trec5C-medium 
Trec6C-medium 
Trec4S 
(+4%) 
Degree of Disambiguation 
None Manual % of 
Mono- 
lingual 
0.2449 0.2873 84% 
(+17%) 
0.3872 0.3830 83% 
(-1%) 
0.1729 0.1799 80% 
Table 4: The effect of disambiguation on 
retrieval performance. The scores reported 
are average precision. 
8 Impact of Missing Translations 
Results in the previous ection showed that 
manual disambiguation can bring performance 
of cross-lingual IR to around 82% of mono- 
lingual IR. The remaining performance gap 
between mono-lingual nd cross-lingual IR is 
likely to be caused by the incompleteness of the 
bilingual exicon used for query translation, i.e., 
missing translations for some query terms. This 
may be a more serious problem for cross-lingual 
IR than ambiguity. To test the conjecture, for 
each English query term, a native speaker in 
Chinese or Spanish manually checked whether 
the bilingual exicon contains acorrect 
translation for the term in the context of the 
query. If it does not, a correct ranslation for the 
term was added to the lexicon. For the query 
sets Trec5C-medium and Trec6C-medium, there 
are 100 query terms for which the lexicon does 
not have a correct ranslation. This represents 
19% of the 520 query terms (a term is counted 
only once in one query). For the query set 
Trec4S, the percentage is 12%. 
The results in Table 5 show that with augmented 
lexicons, performance of cross-lingual IR is 
91%, 99% and 95% of mono-lingual IR on 
Trec5C-mediurn, Trec6C-medium and Trec4S. 
99 
The improvement over using the original exicon 
is 28%, 18% and 23% respectively. The results 
demonstrate the importance cff a complete 
lexicon. Compared with the results in section 7, 
the results here suggest that missing translations 
have a much larger impact on cross-lingual IR 
than translation ambiguity does. 
Query sets Original Augmented % o f  
lexicon lexicon Mono- 
lingual 
Trec5C- 0.2449 0.3131 91% 
medium (+28%) 
Trec6C- 0.3872 0.4589 99% 
medium (+18%) 
Trec4S 0.1729 0.2128 95% 
(+23%) 
Table 5: The impact of missing the right 
translations on retrieval performance. All  
scores are average precision. 
9 Impact of  Lexicon Size 
In this section we measure CLIR performance as
a function of lexicon size. We sorted the 
English words from TREC disks l&2 in order of 
decreasing frequency. For a lexicon of size n, 
we keep only the n most frequent English words. 
The upper graph in Figure 1 shows the curve of 
cross-lingual IR performance asa function of the 
size of the lexicon based on the Chinese short 
and medium-length queries. Retrieval 
performance was averaged over Trec5C and 
Trec6C. Initially retrieval performance increases 
sharply with lexicon size. After the dictionary 
exceeds 20,000, performance l vels off. An 
examination of the translated queries hows that 
words not appearing in the 20,000-word lexicon 
usually do not appear in the larger lexicons 
either. Thus, increases in the general lexicon 
beyond 20,000 words did not result in a 
substantial increase in the coverage of the query 
terms. 
The lower graph in Figure 1 plots the retrieval 
performance asa function of the percent of the 
full lexicon. The figure shows that short queries 
are more susceptible toincompleteness of the 
lexicon than longer queries. Using a 7,000-word 
lexicon, the short queries only achieve 75% of 
their performance with the full lexicon. In 
comparison, the medium-length queries achieve 
87% of their performance. 
\[--*- Short Query 4-- Medium Query J 
0.35 
0.3 
o.25 
== o.2 
0.15 
~. 0.1 
O.O5 
0 
0 10000 20000 30000 40000 50000 60000 
Lexicon Size 
\[ -*-- Short + Medium \] 
_-- 120 
o lO0I 
~g 00 
0 o o_  60 
,f. o 
0 
O,, 
10000 20000 30000 40000 5(X)O0 60000 
Lexicon Size 
Figure 1 Impact of lexicon size on cross-lingual IR 
performance 
We categorized the missing terms and found that 
most of them are proper nouns (especially 
locations and person ames), highly technical 
terms, or numbers. Such words understandably 
do not normally appear in traditional lexicons. 
Translation of numbers can be solved using 
simple rules. Transliteration, a technique that 
guesses the likely translations of a word based 
on pronunciation, can be readily used in 
translating proper nouns. 
Another technique is automatic discovery of 
translations from parallel or non-parallel corpora 
(Fung and Mckeown, 1997). Since traditional 
lexicons are more or less static repositories of 
knowledge, techniques that discover translation 
from newly published materials can supplement 
them with corpus-specific vocabularies. 
100 
10 Using a Parallel Corpus 
In this section we estimate translation 
probabilities from a parallel corpus rather than 
assuming uniform likelihood as in section 4. A 
Hong Kong News corpus obtained from the 
Linguistic Data Consortium has 9,769 news 
stories in Chinese with English translations. It
has 3.4 million English words. Since the 
documents are not exact ranslations of each 
other, occasionally having extra or missing 
sentences, we used document-level co- 
occurrence toestimate translation probabilities. 
The Chinese documents were "segmented" using 
the technique discussed in section 4. Let co(e,c) 
be the number of parallel documents where an 
English word e and a Chinese word c co-occur, 
and df(c) be the document frequency of c. If a 
Chinese word c has n possible translations el to 
en in the bilingual exicon, we estimate the 
corpus translation probability as: 
co(e i , c) 
P_  corpus(ell c) = 
i=n 
MAX(df (c ) ,  ~ co(e i, c)) 
i=1 
Since several translations for c may co-occur in 
a document, ~co(e~ c) can be greater than df(c). 
Using the maximum of the two ensures that 
E P_corpus(eilc)_<l. 
Instead of relying solely on corpus-based 
estimates from a small parallel corpus, we 
employ a mixture model as follows: 
P( e I c) = ~ P _ corpus( eI c) + (1- #)P_ lexicon( e\[ c) 
The retrieval results in Table 6 show that 
combining the probability estimates from the 
lexicon and the parallel corpus does improve 
retrieval performance. The best results are 
obtained when 13=0.7; this is better than using 
uniform probabilities by 9% on Trec5C-medium 
and 4% on Trec6C-medium. Using the corpus 
probability estimates alone results in a 
significant drop in performance, the parallel 
corpus is not large enough nor diverse nough 
for reliable stimation of the translation 
probabilities. In fact, many words do not appear 
in the corpus at all. With a larger and better 
parallel corpus, more weight should be given to 
the probability estimates from the corpus. 
Trec5 - Trec6- 
medium medium 
P_lexicon 0.2449 0.3872 
13=0.3 0.2557 0.3980 
13=0.5 0.2605 0.4021 
13=0.7 0.2658 0.4035 
P_corpus 0.2293 0.2971 
Table 6: Performance with different values 
of 13. All scores are average precision. 
11 Related Work 
Other studies which view IR as a query 
generation process include Maron and Kuhns, 
1960; Hiemstra nd Kraaij, 1999; Ponte and 
Croft, 1998; Miller et al 1999. Our work has 
focused on cross-lingual retrieval. 
Many approaches tocross-lingual IR have been 
published. One common approach is using 
Machine Translation (MT) to translate the 
queries to the language of the documents or 
translate documents othe language of the 
queries (Gey et al 1999; Oard, 1998). For most 
languages, there are no MT systems at all. Our 
focus is on languages where no MT exists, but a 
bilingual dictionary may exist or may be 
derived. 
Another common approach is term translation, 
e.g., via a bilingual exicon. (Davis and Ogden, 
1997; Ballesteros and Croft, 1997; Hull and 
Grefenstette, 1996). While word sense 
disambiguation has been a central topic in 
previous tudies for cross-lingual IR, our study 
suggests that using multiple weighted 
translations and compensating for the 
incompleteness of the lexicon may be more 
valuable. Other studies on the value of 
disambiguation for cross-lingual IR include 
Hiernstra nd de Jong, 1999; Hull, 1997. 
Sanderson, 1994 studied the issue of 
disarnbiguation for mono-lingual IR. 
101 
The third approach to cross-lingual retrieval is to 
map queries and documents o some 
intermediate r presentation, e.g latent semantic 
indexing (LSI) (Littman et al 1998), or the 
General Vector space model (GVSM), 
(Carbonell et al 1997). We believe our 
approach is computationally ess costly than 
(LSI and GVSM) and assumes less resources 
(WordNet in Diekema et al, 1999). 
12 Conclusions and Future Work 
We proposed an approach to cross-lingual IR 
based on hidden Markov models, where the 
system estimates the probability that a query in 
one language could be generated from a 
document in another language. Experiments 
using the TREC5 and TREC6 Chinese test sets 
and the TREC4 Spanish test set show the 
following: 
? Our retrieval model can reduce the 
performance d gradation due to translation 
ambiguity This had been a major limiting 
factor for other query-translation 
approaches. 
? Some earlier studies uggested that query 
translation is not an effective approach to 
cross-lingual IR (Carbonell et al 1997). 
However, our results uggest that query 
translation can be effective particularly if a 
bilingual dictionary is the primary bilingual 
resource available. 
? Manual selection from the translations in the 
bilingual dictionary improves performance 
little over the HMM. 
? We believe an algorithm cannot rule out a 
possible translation with absolute 
confidence; it is more effective to rely on 
probability estimation/re-estimation to 
differentiate likely translations and unlikely 
translations. 
? Rather than translation ambiguity, a more 
serious limitation to effective cross-lingual 
IR is incompleteness of the bilingual exicon 
used for query translation. 
? Cross-lingual IR performance is typically 
75% that of mono-lingual for our HMM on 
the Chinese and Spanish collections. 
Future improvements in cross-lingual IR will 
come by attacking the incompleteness of 
bilingual dictionaries and by improved query 
expansion and context-dependent translation. 
Our current model assumes that query terms are 
generated one at time. We would like to extend 
the model to allow phrase generation i the 
query generation process. We also wish to 
explore techniques to extend bilingual exicons. 
References 
L. Ballesteros and W.B. Croft 1997. "Phrasal 
translation and query expansion techniques for 
cross-language information retrieval." Proceedings 
of the 20th ACM SIGIR International Conference 
on Research and Development in Information 
Retrieval 1997, pp. 84-91. 
L. Ballesteros and W.B. Croft, 1998. "Resolving 
ambiguity for cross-language retrieval." 
Proceedings of the 21st ACM SIGIR Conference on 
Research and Development in Information 
Retrieval, 1998, pp. 64-71. 
J.P. Callan, W.B. Croft and J. Broglio. 1995. "TREC 
and TIPSTER Experiments with INQUERY". 
Information Processing and Management, pages 
327-343, 1995. 
J. Carbonell, Y. Yang, R. Frederking, R. Brown, Y. 
Geng and D. Lee, 1997. "Translingual information 
retrieval: a comparative evaluation." In 
Proceedings of the 15th International Joint 
Conference on Artificial Intelligence, 1997. 
M. Davis and W. Ogden, 1997. "QUILT: 
Implementing a Large Scale Cross-language Text 
Retrieval System." Proceedings of ACM SIGIR 
Conference, 1997. 
A. Diekema, F. Oroumchain, P. Sheridan and E. 
Liddy, 1999. "TREC-7 Evaluation of Conceptual 
Interlingual Document Retrieval (CINDOR) in 
English and French." TREC7 Proceedings, NIST 
special publication. 
P. Fung and K. Mckeown. "Finding Terminology 
Translations from Non-parallel Corpora." The 5 'h 
Annual Workshop on Very Large Corpora, Hong 
Kong: August 1997, 192n202 
F. Gey, J. He and A. Chen, 1999. "Manual queries 
and Machine Translation in cross-language 
retrieval at TREC-7". In TREC7 Proceedings, 
NIST Special Publication, 1999. 
102 
Harman, 1996. The TREC-4 Proceedings. NIST 
Special publication, 1996. 
D. Hiemstra nd F. de Jong, 1999. "Disambiguafion 
strategies for Cross-language Information 
Retrieval." Proceedings of the third European 
Conference on Research and Advanced Technology 
for Digital Libraries, pp. 274-293, 1999. 
D. Hiemstra and W. Kraaij, 1999. "Twenty-One at 
TREC-7: ad-hoc and cross-language track." In 
TREC-7 Proceedings, NIST Special Publication, 
1999. 
D. Hull, 1993. "Using Statistical Testing in the 
Evaluation of Retrieval Experiments." Proceedings 
of the 16th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, pages 329-338, 1993. 
D. A. Hull and G. Grefenstette, 1996. "A dictionary- 
based approach to multilingual information 
retrieval". Proceedings of ACM SIGIR Conference, 
1996. 
D. A. Hull, 1997. "Using structured queries for 
disambiguation in cross-language information 
retrieval." In AAAI Symposium on Cross-Language 
Text and Speech Retrieval. AAAI, 1997. 
M. E. Maron and K. L. Kuhns, 1960. "On 
Relevance, Probabilistic Indexing and Information 
Retrieval." Journal of the Association for 
": Computing Machinery, 1960, pp 216-244. 
D. Miller, T. Leek and R. Schwartz, 1999. "A 
Hidden Markov Model Information Retrieval 
System." Proceedings of the 22nd Annual 
International ACM S1GIR Conference on Research 
and Development in Information Retrieval, pages 
214-221, 1999. 
D.W. Oard, 1998. "A comparative study of query and 
document translation for cross-language 
information retrieval." In Proceedings of the Third 
Conference of the Association for Machine 
Translation in America (AMTA ), 1998. 
Ari Pirkola, 1998. "The effects of query structure 
and dictionary setups in dictionary-based cross- 
language information retrieval." Proceedings of 
ACM SIGIR Conference, 1998, pp 55-63. 
J. Ponte and W.B. Croft, 1998. "A Language 
Modeling Approach to Information Retrieval." 
Proceedings of the 21st Annual International ACM 
S1GIR Conference on Research and Development 
in Information Retrieval, pages 275-281, 1998. 
L. Rabiner, 1989. "A tutorial on hidden Markov 
models and selected applications in speech 
recognition." Proc. IEEE 77, pp. 257-286, 1989. 
M. Sanderson. "Word sense disambiguation and 
information retrieval." Proceedings of ACM SIGIR 
Conference, 1994, pp 142-15 I. 
Voorhees and Harman, 1997. TREC-5 Proceedings. 
E. Voorhees and D. Harman, Editors. NIST 
special publication. 
Voorhees and Harman, 1998. TREC-6 Proceedings. 
E. Voorhees and D. Harrnan, Editors. NIST 
special publication. 
J. Xu and W.B. Croft, 1998. "Corpus-based 
stemming using co-occurrence of word variants". 
ACM Transactions on Information Systems, 
January 1998, vol 16, no. 1. 
103 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616?625,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Statistical Machine Translation with a Factorized Grammar
Libin Shen and Bing Zhang and Spyros Matsoukas and
Jinxi Xu and Ralph Weischedel
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{lshen,bzhang,smatsouk,jxu,weisched}@bbn.com
Abstract
In modern machine translation practice, a sta-
tistical phrasal or hierarchical translation sys-
tem usually relies on a huge set of trans-
lation rules extracted from bi-lingual train-
ing data. This approach not only results in
space and efficiency issues, but also suffers
from the sparse data problem. In this paper,
we propose to use factorized grammars, an
idea widely accepted in the field of linguis-
tic grammar construction, to generalize trans-
lation rules, so as to solve these two prob-
lems. We designed a method to take advantage
of the XTAG English Grammar to facilitate
the extraction of factorized rules. We experi-
mented on various setups of low-resource lan-
guage translation, and showed consistent sig-
nificant improvement in BLEU over state-of-
the-art string-to-dependency baseline systems
with 200K words of bi-lingual training data.
1 Introduction
A statistical phrasal (Koehn et al, 2003; Och and
Ney, 2004) or hierarchical (Chiang, 2005; Marcu
et al, 2006) machine translation system usually re-
lies on a very large set of translation rules extracted
from bi-lingual training data with heuristic methods
on word alignment results. According to our own
experience, we obtain about 200GB of rules from
training data of about 50M words on each side. This
immediately becomes an engineering challenge on
space and search efficiency.
A common practice to circumvent this problem
is to filter the rules based on development sets in the
step of rule extraction or before the decoding phrase,
instead of building a real distributed system. How-
ever, this strategy only works for research systems,
for which the segments for translation are always
fixed.
However, do we really need such a large rule set
to represent information from the training data of
much smaller size? Linguists in the grammar con-
struction field already showed us a perfect solution
to a similar problem. The answer is to use a fac-
torized grammar. Linguists decompose lexicalized
linguistic structures into two parts, (unlexicalized)
templates and lexical items. Templates are further
organized into families. Each family is associated
with a set of lexical items which can be used to lex-
icalize all the templates in this family. For example,
the XTAG English Grammar (XTAG-Group, 2001),
a hand-crafted grammar based on the Tree Adjoin-
ing Grammar (TAG) (Joshi and Schabes, 1997) for-
malism, is a grammar of this kind, which employs
factorization with LTAG e-tree templates and lexical
items.
Factorized grammars not only relieve the burden
on space and search, but also alleviate the sparse
data problem, especially for low-resource language
translation with few training data. With a factored
model, we do not need to observe exact ?template
? lexical item? occurrences in training. New rules
can be generated from template families and lexical
items either offline or on the fly, explicitly or im-
plicitly. In fact, the factorization approach has been
successfully applied on the morphological level in
previous study on MT (Koehn and Hoang, 2007). In
this work, we will go further to investigate factoriza-
tion of rule structures by exploiting the rich XTAG
English Grammar.
We evaluate the effect of using factorized trans-
lation grammars on various setups of low-resource
language translation, since low-resource MT suffers
greatly on poor generalization capability of trans-
616
lation rules. With the help of high-level linguis-
tic knowledge for generalization, factorized gram-
mars provide consistent significant improvement
in BLEU (Papineni et al, 2001) over string-to-
dependency baseline systems with 200K words of
bi-lingual training data.
This work also closes the gap between compact
hand-crafted translation rules and large-scale unor-
ganized automatic rules. This may lead to a more ef-
fective and efficient statistical translation model that
could better leverage generic linguistic knowledge
in MT.
In the rest of this paper, we will first provide a
short description of our baseline system in Section 2.
Then, we will introduce factorized translation gram-
mars in Section 3. We will illustrate the use of the
XTAG English Grammar to facilitate the extraction
of factorized rules in Section 4. Implementation de-
tails are provided in Section 5. Experimental results
are reported in Section 6.
2 A Baseline String-to-Tree Model
As the baseline of our new algorithm, we use a
string-to-dependency system as described in (Shen
et al, 2008). There are several reasons why we take
this model as our baseline. First, it uses syntactic
tree structures on the target side, which makes it easy
to exploit linguistic information. Second, depen-
dency structures are relatively easier to implement,
as compared to phrase structure grammars. Third,
a string-to-dependency system provides state-of-the-
art performance on translation accuracy, so that im-
provement over such a system will be more convinc-
ing.
Here, we provide a brief description of the base-
line string-to-dependency system, for the sake of
completeness. Readers can refer to (Shen et al,
2008; Shen et al, 2009) for related information.
In the baseline string-to-dependency model, each
translation rule is composed of two parts, source and
target. The source sides is a string rewriting rule,
and the target side is a tree rewriting rule. Both
sides can contain non-terminals, and source and tar-
get non-terminals are one-to-one aligned. Thus, in
the decoding phase, non-terminal replacement for
both sides are synchronized.
Decoding is solved with a generic chart parsing
algorithm. The source side of a translation rule is
used to detect when this rule can be applied. The tar-
get side of the rule provides a hypothesis tree struc-
ture for the matched span. Mono-lingual parsing can
be viewed as a special case of this generic algorithm,
for which the source string is a projection of the tar-
get tree structure.
Figure 1 shows three examples of string-to-
dependency translation rules. For the sake of con-
venience, we use English for both source and target.
Upper-cased words represent source, while lower-
cased words represent target. X is used for non-
terminals for both sides, and non-terminal alignment
is represented with subscripts.
In Figure 1, the top boxes mean the source side,
and the bottom boxes mean the target side. As for
the third rule, FUN Q stands for a function word in
the source language that represents a question.
3 Translation with a Factorized Grammar
We continue with the example rules in Figure 1.
Suppose, we have ?... HATE ... FUN Q? in a given
test segment. There is no rule having both HATE
and FUN Q on its source side. Therefore, we have
to translate these two source words separately. For
example, we may use the second rule in Figure 1.
Thus, HATE will be translated into hates, which is
wrong.
Intuitively, we would like to have translation rule
that tell us how to translate X1 HATE X2 FUN Q
as in Figure 2. It is not available directly from the
training data. However, if we obtain the three rules
in Figure 1, we are able to predict this missing rule.
Furthermore, if we know like and hate are in the
same syntactic/semantic class in the source or target
language, we will be very confident on the validity
of this hypothesis rule.
Now, we propose a factorized grammar to solve
this generalization problem. In addition, translation
rules represented with the new formalism will be
more compact.
3.1 Factorized Rules
We decompose a translation rule into two parts,
a pair of lexical items and an unlexicalized tem-
plate. It is similar to the solution in the XTAG En-
glish Grammar (XTAG-Group, 2001), while here we
617
X1  LIKE  X2
likes
X1 X2
X1  HATE  X2
hates
X1 X2
X1  LIKE X2  FUN_Q
like
does X1 X2
Figure 1: Three examples of string-to-dependency translation rules.
X1  V  X2
VBZ
X1 X2
X1  V  X2
VBZ
X1 X2
X1  V  X2  FUN_Q
VB
does X1 X2
Figure 3: Templates for rules in Figure 1.
X1  HATE  X2  FUN_Q
hate
does X1 X2
Figure 2: An example of a missing rule.
work on two languages at the same time.
For each rule, we first detect a pair of aligned head
words. Then, we extract the stems of this word pair
as lexical items, and replace them with their POS
tags in the rule. Thus, the original rule becomes an
unlexicalized rule template.
As for the three example rules in Figure 1, we will
extract lexical items (LIKE, like), (HATE, hate) and
(LIKE, like) respectively. We obtain the same lexical
items from the first and the third rules.
The resultant templates are shown in Figure 3.
Here, V represents a verb on the source side, VB
stands for a verb in the base form, and VBZ means
a verb in the third person singular present form as
in the Penn Treebank representation (Marcus et al,
1994).
In the XTAG English Grammar, tree templates for
transitive verbs are grouped into a family. All transi-
tive verbs are associated with this family. Here, we
assume that the rule templates representing struc-
tural variations of the same word class can also be
organized into a template family. For example, as
shown in Figure 4, templates and lexical items are
associated with families. It should be noted that
a template or a lexical item can be associated with
more than one family.
Another level of indirection like this provides
more generalization capability. As for the missing
618
X1  V  X2
VBZ
X1 X2
Family Transitive_3
X1  V  X2  FUN_Q
VB
does X1 X2
X1  V  FUN_Past
VBD
X1
Family Intransitive_2
( LIKE, like ) ( HATE, hate ) ( OPEN, open ) ( HAPPEN, happen )
Figure 4: Templates and lexical items are associated with families.
rule in Figure 2, we can now generate it by replac-
ing the POS tags in the second template of Figure
4 with lexical items (HATE, hate) with their correct
inflections. Both the template and the lexical items
here are associated with the family Transitive 3..
3.2 Statistical Models
Another level of indirection also leads to a desirable
back-off model. We decompose a rule R into to two
parts, its template PR and its lexical items LR. As-
suming they are independent, then we can compute
Pr(R) as
Pr(R) = Pr(PR)Pr(LR), or
Pr(R) =
?
F Pr(PR|F )Pr(LR|F )Pr(F ), (1)
if they are conditionally independent for each fam-
ily F . In this way, we can have a good estimate for
rules that do not appear in the training data. The
second generative model will also be useful for un-
supervised learning of families and related probabil-
ities.
In this paper, we approximate families by using
target (English) side linguistic knowledge as what
we will explain in Section 4, so this changes the def-
inition of the task. In short, we will be given a list of
families. We will also be given an association table
B(L,F ) for lexical items L and families F , such
that B(L,F ) = true if and only L is associated
with F , but we do not know the distributions.
Let S be the source side of a rule or a rule tem-
plate, T the target side of a rule of a rule template.
We define Prb, the back-off conditional model of
templates, as follows.
Prb(PS |PT , L) =
?
F :B(L,F ) #(PS , PT , F )
?
F :B(L,F ) #(PT , F )
, (2)
where # stands for the count of events.
Let P and L be the template and lexical items of
R respectively. Let Prt be the MLE model obtained
from the training data. The smoothed probability is
then defined as follows.
Pr(RS |RT ) = (1 ? ?)Prt(RS |RT )
+?Prb(PS |PT , L), (3)
where ? is a parameter. We fix it to 0.1 in later ex-
periments. Conditional probability Pr(RT |RS) is
defined in a similar way.
3.3 Discussion
The factorized models discussed in the previous sec-
tion can greatly alleviate the sparse data problem,
especially for low-resource translation tasks. How-
ever, when the training data is small, it is not easy to
619
learn families. Therefore, to use unsupervised learn-
ing with a model like (1) somehow reduces a hard
translation problem to another one of the same diffi-
culty, when the training data is small.
However, in many cases, we do have extra infor-
mation that we can take advantage of. For example,
if the target language has rich resources, although
the source language is a low-density one, we can ex-
ploit the linguistic knowledge on the target side, and
carry it over to bi-lingual structures of the translation
model. The setup of X-to-English translation tasks
is just like this. This will be the topic of the next
section. We leave unsupervised learning of factor-
ized translation grammars for future research.
4 Using A Mono-Lingual Grammar
In this section, we will focus on X-to-English trans-
lation, and explain how to use English resources to
build a factorized translation grammar. Although we
use English as an example, this approach can be ap-
plied to any language pairs that have certain linguis-
tic resources on one side.
As shown in Figure 4, intuitively, the families
are intersection of the word families of the two lan-
guages involved, which means that they are refine-
ment of the English word families. For example,
a sub-set of the English transitive families may be
translated in the same way, so they share the same
set of templates. This is why we named the two fam-
ilies Transitive 3 and Intransitive 2 in Figure 4.
Therefore, we approximate bi-lingual families
with English families first. In future, we can use
them as the initial values for unsupervised learning.
In order to learn English families, we need to take
away the source side information in Figure 4, and
we end up with a template?family?word graph as
shown in Figure 5. We can learn this model on large
mono-lingual data if necessary.
What is very interesting is that there already exists
a hand-crafted solution for this model. This is the
XTAG English Grammar (XTAG-Group, 2001).
The XTAG English Grammar is a large-scale En-
glish grammar based on the TAG formalism ex-
tended with lexicalization and unification-based fea-
ture structures. It consists of morphological, syn-
tactic, and tree databases. The syntactic database
contains the information that we have represented
in Figure 5 and many other useful linguistic annota-
tions, e.g. features.
The XTAG English grammar contains 1,004 tem-
plates, organized in 53 families, and 221 individual
templates. About 30,000 lexical items are associ-
ated with these families and individual templates 1.
In addition, it also has the richest English morpho-
logical lexicon with 317,000 inflected items derived
from 90,000 stems. We use this resource to predict
POS tags and inflections of lexical items.
In our applications, we select all the verb fami-
lies plus one each for nouns, adjectives and adverbs.
We use the families of the English word as the fam-
ilies of bi-lingual lexical items. Therefore, we have
a list of about 20 families and an association table
as described in Section 3.2. Of course, one can use
other linguistic resources if similar family informa-
tion is provided, e.g. VerbNet (Kipper et al, 2006)
or WordNet (Fellbaum, 1998).
5 Implementation
Nowadays, machine translation systems become
more and more complicated. It takes time to write
a decoder from scratch and hook it with various
modules, so it is not the best solution for research
purpose. A common practice is to reduce a new
translation model to an old one, so that we can use
an existing system, and see the effect of the new
model quickly. For example, the tree-based model
proposed in (Carreras and Collins, 2009) used a
phrasal decoder for sub-clause translation, and re-
cently, DeNeefe and Knight (2009) reduced a TAG-
based translation model to a CFG-based model by
applying all possible adjunction operations offline
and stored the results as rules, which were then used
by an existing syntax-based decoder.
Here, we use a similar method. Instead of build-
ing a new decoder that uses factorized grammars,
we reduce factorized rules to baseline string-to-
dependency rules by performing combination of
templates and lexical items in an offline mode. This
is similar to the rule generation method in (DeNeefe
and Knight, 2009). The procedure is as follows.
In the rule extraction phase, we first extract all the
string-to-dependency rules with the baseline system.
1More information about XTAG is available online at
http://www.cis.upenn.edu/?xtag .
620
VBZ
X1 X2
Family Transitive
VB
does X1 X2
VBD
X1
Family Intransitive
like hate open happen
Figure 5: Templates, families, and words in the XTAG English Grammar.
For each extracted rule, we try to split it into various
?template?lexical item? pairs by choosing different
aligned words for delexicalization, which turns rules
in Figure 1 into lexical items and templates in Fig-
ure 3. Events of templates and lexical items are
counted according to the family of the target En-
glish word. If an English word is associated with
more than one family, the count is distributed uni-
formly among these families. In this way, we collect
sufficient statistics for the back-off model in (2).
For each family, we keep the top 200 most fre-
quent templates. Then, we apply them to all the
lexical items in this families, and save the gener-
ated rules. We merge the new rules with the original
one. The conditional probabilities for the rules in the
combined set is smoothed according to (2) and (3).
Obviously, using only the 200 most frequent tem-
plates for each family is just a rough approxima-
tion. An exact implementation of a new decoder for
factorized grammars can make better use of all the
templates. However, the experiments will show that
even an approximation like this can already provide
significant improvement on small training data sets,
i.e. with no more than 2M words.
Since we implement template application in an of-
fline mode, we can use exactly the same decoding
and optimization algorithms as the baseline. The de-
coder is a generic chart parsing algorithm that gen-
erates target dependency trees from source string in-
put. The optimizer is an L-BFGS algorithm that
maximizes expected BLEU scores on n-best hy-
potheses (Devlin, 2009).
6 Experiments on Low-Resource Setups
We tested the performance of using factorized gram-
mars on low-resource MT setups. As what we noted
above, the sparse data problem is a major issue when
there is not enough training data. This is one of the
cases that a factorized grammar would help.
We did not tested on real low-resource languages.
Instead, we mimic the low-resource setup with two
of the most frequently used language pairs, Arabic-
to-English and Chinese-to-English, on newswire
and web genres. Experiments on these setups will
be reported in Section 6.1. Working on a language
which actually has more resources allows us to study
the effect of training data size. This will be reported
in Section 6.2. In Section 6.3, we will show exam-
ples of templates learned from the Arabic-to-English
training data.
6.1 Languages and Genres
The Arabic-to-English training data contains about
200K (target) words randomly selected from an
LDC corpus, LDC2006G05 A2E set, plus an
Arabic-English dictionary with about 89K items.
We build our development sets from GALE P4 sets.
There are one tune set and two test sets for the MT
systems 2. TEST-1 has about 5000 segments and
TEST-2 has about 3000 segments.
2One of the two test sets will later be used to tune an MT
combination system.
621
MODEL
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English newswire
baseline 21.07 12.41 43.77 19.96 11.42 42.79 21.09 11.03 43.74
factorized 21.70 13.17 44.85 20.52 11.70 43.83 21.36 11.77 44.72
Arabic-to-English web
baseline 10.26 5.02 32.78 9.40 4.87 31.26 14.11 7.34 35.93
factorized 10.67 5.34 33.83 9.74 5.20 32.52 14.66 7.69 37.11
Chinese-to-English newswire
baseline 13.17 8.04 44.70 19.62 9.32 48.60 14.53 6.82 45.34
factorized 13.91 8.09 45.03 20.48 9.70 48.61 15.16 7.37 45.31
Chinese-to-English web
baseline 11.52 5.96 42.18 11.44 6.07 41.90 9.83 4.66 39.71
factorized 11.98 6.31 42.84 11.72 5.88 42.55 10.25 5.34 40.34
Table 1: Experimental results on Arabic-to-English / Chinese-to-English newswire and web data. %BL stands for
BLEU scores for documents whose BLEU scores are in the bottom 75% to 90% range of all documents. MET stands
for METEOR scores.
The Chinese-to-English training data contains
about 200K (target) words randomly selected from
LDC2006G05 C2E set, plus a Chinese-English dic-
tionary (LDC2002L27) with about 68K items. The
development data setup is similar to that of Arabic-
to-English experiments.
Chinese-to-English translation is from a morphol-
ogy poor language to a morphology rich language,
while Arabic-to-English translation is in the oppo-
site direction. It will be interesting to see if factor-
ized grammars help on both cases. Furthermore, we
also test on two genres, newswire and web, for both
languages.
Table 1 lists the experimental results of all the four
conditions. The tuning metric is expected BLEU.
We are also interested in the BLEU scores for doc-
uments whose BLEU scores are in the bottom 75%
to 90% range of all documents. We mark it as %BL
in the table. This metric represents how a system
performances on difficult documents. It is important
to certain percentile evaluations. We also measure
METEOR (Banerjee and Lavie, 2005) scores for all
systems.
The system using factorized grammars shows
BLEU improvement in all conditions. We measure
the significance of BLEU improvement with paired
bootstrap resampling as described by (Koehn, 2004).
All the BLEU improvements are over 95% confi-
dence level. The new system also improves %BL
and METEOR in most of the cases.
6.2 Training Data Size
The experiments to be presented in this section
are designed to measure the effect of training data
size. We select Arabic web for this set of experi-
ments. Since the original Arabic-to-English train-
ing data LDC2006G05 is a small one, we switch to
LDC2006E25, which has about 3.5M target words
in total. We randomly select 125K, 250K, 500K, 1M
and 2M sub-sets from the whole data set. A larger
one always includes a smaller one. We still tune on
expected BLEU, and test on BLEU, %BL and ME-
TEOR.
The average BLEU improvement on test sets is
about 0.6 on the 125K set, but it gradually dimin-
ishes. For better observation, we draw the curves of
BLEU improvement along with significance test re-
sults for each training set. As shown in Figure 6 and
7, more improvement is observed with fewer train-
ing data. This fits well with fact that the baseline MT
model suffers more on the sparse data problem with
smaller training data. The reason why the improve-
ment diminishes on the full data set could be that the
rough approximation with 200 most frequent tem-
plates cannot fully take advantage of this paradigm,
which will be discussed in the next section.
622
MODEL SIZE
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English web
baseline
125K
8.54 2.96 28.87 7.41 2.82 26.95 11.29 5.06 31.37
factorized 8.99 3.44 30.40 7.92 3.57 28.63 12.04 6.06 32.87
baseline
250K
10.18 4.70 32.21 8.94 4.35 30.31 13.71 6.93 35.14
factorized 10.57 4.96 33.22 9.34 4.78 31.51 14.02 7.28 36.25
baseline
500K
12.18 5.84 35.59 10.82 5.77 33.62 16.48 8.30 38.73
factorized 12.40 6.01 36.15 11.14 5.96 34.38 16.76 8.53 39.27
baseline
1M
13.95 7.17 38.49 12.48 7.12 36.56 18.86 10.00 42.18
factorized 14.14 7.41 38.99 12.66 7.34 37.14 19.11 10.29 42.56
baseline
2M
15.74 8.38 41.15 14.18 8.17 39.26 20.96 11.95 45.18
factorized 15.92 8.81 41.51 14.34 8.25 39.68 21.42 12.05 45.51
baseline
3.5M
16.95 9.76 43.03 15.47 9.08 41.28 22.83 13.24 47.05
factorized 17.07 9.99 43.18 15.49 8.77 41.41 22.72 13.10 47.23
Table 2: Experimental results on Arabic web. %BL stands for BLEU scores for documents whose BLEU scores are
in the bottom 75% to 90% range of all documents. MET stands for METEOR scores.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-1
Figure 6: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-1.
6.3 Example Templates
Figure 8 lists seven Arabic-to-English templates
randomly selected from the transitive verb family.
TMPL 151 is an interesting one. It helps to alleviate
the pronoun dropping problem in Arabic. However,
we notice that most of the templates in the 200 lists
are rather simple. More sophisticated solutions are
needed to go deep into the list to find out better tem-
plates in future.
It will be interesting to find an automatic or
semi-automatic way to discover source counterparts
of target treelets in the XTAG English Grammar.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-2
Figure 7: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-2.
Generic rules like this will be very close to hand-
craft translate rules that people have accumulated for
rule-based MT systems.
7 Conclusions and Future Work
In this paper, we proposed a novel statistical ma-
chine translation model using a factorized structure-
based translation grammar. This model not only al-
leviates the sparse data problem but only relieves the
burden on space and search, both of which are im-
minent issues for the popular phrasal and/or hierar-
chical MT systems.
623
VVB
TMPL_1
X1  V
VBD
X1
TMPL_121
TMPL_31
V  X1
for
VBG
X1
TMPL_151
TMPL_61
V  X1
VBN
by
X1
TMPL_181
TMPL_91
X1  V
VBD
X1
the
V  X1
VBD
he X1
X1  V  X2
VBZ
X1 X2
Figure 8: Randomly selected Arabic-to-English templates from the transitive verb family.
We took low-resource language translation, espe-
cially X-to-English translation tasks, for case study.
We designed a method to exploit family informa-
tion in the XTAG English Grammar to facilitate the
extraction of factorized rules. We tested the new
model on low-resource translation, and the use of
factorized models showed significant improvement
in BLEU on systems with 200K words of bi-lingual
training data of various language pairs and genres.
The factorized translation grammar proposed here
shows an interesting way of using richer syntactic
resources, with high potential for future research.
In future, we will explore various learning meth-
ods for better estimation of families, templates and
lexical items. The target linguistic knowledge that
we used in this paper will provide a nice starting
point for unsupervised learning algorithms.
We will also try to further exploit the factorized
representation with discriminative learning. Fea-
tures defined on templates and families will have
good generalization capability.
Acknowledgments
This work was supported by DARPA/IPTO Contract
HR0011-06-C-0022 under the GALE program3. We
thank Aravind Joshi, Scott Miller, Richard Schwartz
and anonymous reviewers for valuable comments.
3Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited). The views, opinions, and/or find-
ings contained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as representing the
official views or policies, either expressed or implied, of the De-
fense Advanced Research Projects Agency or the Department of
Defense.
624
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 101?104, Ann Arbor,
MI.
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of the 2009 Conference of Empirical
Methods in Natural Language Processing, pages 200?
209, Singapore.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference of Empirical Methods in Natural
Language Processing, pages 727?736, Singapore.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, Univ. of Maryland.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. The MIT Press.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer-Verlag.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extensive classifications of en-
glish verbs. In Proceedings of the 12th EURALEX In-
ternational Congress.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proceedings of the 2007 Conference of Empiri-
cal Methods in Natural Language Processing.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 48?54, Edmonton,
Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference of Empirical Methods in Natu-
ral Language Processing, pages 388?395, Barcelona,
Spain.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference of Empirical
Methods in Natural Language Processing, pages 44?
52, Sydney, Australia.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Kishore Papineni, Salim Roukos, and Todd Ward. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report, RC22176.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical Ma-
chine Translation. In Proceedings of the 2009 Confer-
ence of Empirical Methods in Natural Language Pro-
cessing, pages 72?80, Singapore.
XTAG-Group. 2001. A lexicalized tree adjoining gram-
mar for english. Technical Report 01-03, IRCS, Univ.
of Pennsylvania.
625
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1437?1446,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Extreme Extraction -- Machine Reading in a Week 
 
Marjorie Freedman, Lance Ramshaw, Elizabeth Boschee, Ryan Gabbard,  
Gary Kratkiewicz, Nicolas Ward, Ralph Weischedel 
Raytheon BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138 
mfreedma,lramshaw,eboschee,rgabbard,kratkiewicz, 
nward,weischedel@bbn.com 
 
The views expressed are those of the author and do not reflect the official policy or position of the Depart-
ment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.   
  
 
Abstract 
We report on empirical results in extreme 
extraction. It is extreme in that (1) from re-
ceipt of the ontology specifying the target 
concepts and relations, development is li-
mited to one week and that (2) relatively 
little training data is assumed. We are able 
to surpass human recall and achieve an F1 
of 0.51 on a question-answering task with 
less than 50 hours of effort using a hybrid 
approach that mixes active learning, boot-
strapping, and limited (5 hours) manual 
rule writing. We compare the performance 
of three systems: extraction with handwrit-
ten rules, bootstrapped extraction, and a 
combination. We show that while the recall 
of the handwritten rules surpasses that of 
the learned system, the learned system is 
able to improve the overall recall and F1.      
1 Introduction 
Throughout the Automatic Content Extraction 1 
(ACE) evaluations and the Message Understanding 
Conferences2 (MUC), teams typically had a year or 
more from release of the target to submitting sys-
tem results. One exception was MUC-6 (Grishman 
& Sundheim, 1996), in which scenario templates 
for changing positions were extracted given only 
one month. Our goal was to confine development 
to a calendar week, in fact, <50 person hours. This 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://www-nlpir.nist.gov/related_projects/muc/ 
is significant in two ways: the less effort it takes to 
bring up a new domain, (1) the more broadly ap-
plicable the technology is and (2) the less effort 
required to run a diagnostic research experiment. 
Our second goal concerned minimizing training 
data. Rather than approximately 250k words of 
entity and relation annotation as in ACE, only ~20 
example pairs per relation-type were provided as 
training. Reducing the training requirements has 
the same two desirable outcomes: demonstrating 
that the technology can be broadly applicable and 
reducing the overhead for running experiments. 
The system achieved recall of 0.49 and precision 
of 0.53 (for an F1 of 0.51) on a blind test set of 60 
queries of the form Ri(arg1, arg2), where Ri is one 
of the 5 new relations and exactly one of arg1 or 
arg2 is a free variable for each query. 
Key to this achievement was a hybrid of:   
? a variant of (Miller, et al, 2004) to learn two 
new classes of entities via automatically induced 
word classes and active learning (6 hours) 
? bootstrap relation learning (Freedman et al 
2010) to learn 5 new relation classes (2.5 hours),  
? handwritten patterns over predicate-argument 
structure (5 hours), and 
? coreference (20 hours) 
Our bootstrap learner is initialized with relation 
tuples (not annotated text) and uses LDC?s Giga-
word and Wikipedia as a background corpus to 
learn patterns for relation detection that are based 
on normalized predicate argument structure as well 
as surface strings.  
These early empirical results suggest the follow-
ing: (1) It is possible to specify a domain, adapt 
our system, and complete manual scoring, includ-
1437
ing human performance, within a month. Experi-
ments in machine reading (and in extraction) can 
be performed much more quickly and cheaply than 
ever before. (2) Through machine learning and 
limited human pattern writing (6 hours), we 
adapted a machine reading system within a week 
(using less than 50 person hours), achieving ques-
tion answering performance with an F1 of 0.5 and 
with recall 11% higher (relative) to a human read-
er. (3) Unfortunately, machine learning, though 
achieving 80% precision,3 significantly lags behind 
a gifted human pattern writer in recall. Thus, boot-
strap learning with much higher recall at minimal 
sacrifice in precision is highly desirable. 
2 Related Work 
This effort is evaluated extrinsically via formal 
questions expressed as a binary relation with one 
free variable. This contrasts with TREC Question 
Answering, 4  where the questions are in natural 
language, and not restricted to a single binary rela-
tion. Like the ?list? queries of TREC QA, the re-
quirement is to find all answers, not just one. 
Though question interpretation is not required in 
our work, interpretation of the text corpus is. 
The goal of rapid adaptation has been tested in 
other contexts. In 2003, a series of experiments in 
adapting to a new language in less than month 
tested system performance on Cebuano and Hindi. 
The primary goal was to adapt to a new language, 
rather than a new domain. The extraction partici-
pants focused on named-entity recognition, not 
relation extraction (May, et al 2003; Sekine & 
Grishman, 2003; Li & McCallum, 2003; Maynard 
et al 2003). The scenario templates of MUC-6 
(Grishman & Sundheim, 1996) are more similar to 
our relation extraction task, although the domain is 
quite different. Our experiment allowed for 1 week 
of development time, while MUC-6 allowed a 
month. The core entities in the MUC-6 task 
(people and organizations) had been worked on 
previously. In contrast all of our relations included 
at least one novel class. While MUC-6 systems 
tended to use finite-state patterns, they did not in-
corporate bootstrapping or patterns based on the 
output of a statistical parser.   
                                                          
3 Handwritten patterns achieved 52% precision. 
4 http://trec.nist.gov/data/qamain.html 
For learning entity classes, we follow Miller, et 
al., (2004), using word clustering and active learn-
ing to train a perceptron model, but unlike that 
work we apply the technique not just to names but 
also to descriptions. An alternative approach to 
learning classes, applying structural patterns to 
bootstrap description recognition without active 
learning, is seen in Riloff (1996) and Kozareva et 
al., (2008)   
Much research (e.g. Ramshaw 2001) has fo-
cused on learning relation extractors using large 
amounts of supervised training, as in ACE. The 
obvious weakness of such approaches is the result-
ing reliance on manually annotated examples, 
which are expensive and time-consuming to create.  
Others have explored bootstrap relation learn-
ing from seed examples. Agichtein & Gravano 
(2000) and Ravichandran & Hovy (2002) reported 
results for generating surface patterns for relation 
identification; others have explored similar ap-
proaches (e.g. Pantel & Pennacchiotti, 2006). Mit-
chell et al (2009) showed that for macro-reading, 
precision and recall can be improved by learning a 
large set of interconnected relations and concepts 
simultaneously. None use coreference to find train-
ing examples; all use surface (word) patterns. 
Freedman et. al (2010) report improved perfor-
mance from using predicate structure for boot-
strappped relation learning.  
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran 
and Hovy (2002) report results in TREC QA, 
where extracting one instance of a relation can be 
sufficient, rather than detecting all instances. Mit-
chell et al (2009), while demonstrating high preci-
sion, do not measure recall. 
By contrast, our work emphasizes recall, not 
just precision. Our question answering task asks 
list-like questions that require multiple answers.  
We also include the results of a secondary, extrac-
tion evaluation which requires that the system 
identify every mention of the relations in a small 
set of documents. This evaluation is loosely based 
on the relation mention detection task in ACE.  
3 Task Set-Up and Evaluation 
Our effort was divided into four phases. During the 
first phase, a third party produced an ontology and 
the resources, which included: brief (~1 paragraph) 
guidelines for each relation and class in the ontolo-
1438
gy; ~20 examples for each relation in the ontology; 
2K documents that are rich in domain relations. 
Table 1 lists the 5 new relations and number of ex-
amples provided for each. Arguments in italics 
were known by the system prior to the evaluation.  
Relation Ex. 
possibleTreatment(Substance, Condition) 23 
expectedDateOnMarket(Substance, Date) 11 
responsibleForTreatment(Substance, Agent) 19 
studiesDisease(Agent, Condition) 16 
hasSideEffect(Substance, Condition) 27 
Table 1: New Relations and Number of Examples 
In phase two, we spent one week extending our 
extraction system for the new ontology. During the 
third phase, we ran our system over 10K docu-
ments to extract all instances of domain relations 
from those documents. In the fourth phase, our 
question answering system used the extracted in-
formation to answer queries.  
4 Approach to Domain Specialization 
Our approach to extracting domain relations inte-
grated novel relation and class detectors into an 
existing extraction system, designed primarily 
around the ACE tasks. The existing system uses a 
discriminatively trained classifier to detect the enti-
ty and value types of ACE. It also produces a syn-
tactic parse for each sentence; normalizes these 
parses to find logical predicate argument structure; 
and detects and coreferences pronominal, nominal, 
and name mentions for each of the 7 ACE entity 
types (Person, Organization, Geopolitical Entity, 
Location, Facility, Weapon, and Vehicle).5  
The extraction system has three components that 
allow for rapid adaptation to a new domain:  
? Class detectors trained using word classes de-
rived from unsupervised clustering and sentence-
selected training data. 
? A bootstrap relation learner which given a few 
seed examples learns patterns that indicate the 
presence of relations.  
? An expressive pattern language which allows a 
developer to express rules for relation extraction 
in a simple, but fast manner.  
 
 
Component Approach Effort 
Class Recognizer Active Learning 6 hrs 
                                                          
5 The extraction system detects relations and events in the 
ACE ontology, but these were not used in the current work.  
 
Class Recognizer Web-Mined List 1 hrs 
Relation Recognizer 
Semi-supervised 
Bootstrapping 
8.5 hrs 
Relation Recognizer Manual Patterns 5 hrs 
Coreference Heuristics 20 hrs 
Table 2: Effort and Approach for New Domain 
4.1 Class Extraction  
Each of the relations in the new domain included at 
least one argument that was new. While question 
answering requires the system to identify the 
classes only when they appear in a relation, know-
ledge of when a class is present provides important 
information for relation extraction. For example in 
our ontology, Y is a treatment for X only if Y is a 
substance. Thus, ?Group counseling sessions are 
effective treatments for depression? does not con-
tain an instance of possibleTreatment(), while 
?SSRIs are effective treatments for depression? 
does. The bootstrap learner allows constraints 
based on argument type. To use this capability, we 
trained the recognizer at the beginning of the week 
of domain adaptation and used the predicted 
classes during learning.  
We annotated 1064 sentences (~31K words) us-
ing active learning combined with unsupervised 
word clusters (Miller,et al, 2004) for the following 
classes: Substance-Name, Substance-Description, 
Condition-Name, and Condition-Description. Ge-
neric noun-phrases like new drugs, the illness, etc 
were labeled as descriptors. Because of the time 
frame, we did not develop extensive guidelines nor 
measure inter-annotator agreement. Annotation 
took 6 hours. We supplemented our annotation 
with lists of substances and treatments from the 
web, which took 1 hour.  
4.2 Coreference 
Providing a name reference is generally preferable 
to a non-specific string (e.g. the drugs), but not 
always feasible; for instance, reports of new re-
search may appear without a name for the drug. 
Our existing system?s coreference algorithms op-
erate only on mentions of ACE entity types (per-
sons, organizations, GPEs, other locations, 
facilities, vehicles, and weapons). During the week 
of domain adaption we developed new heuristics 
for coreference over non-ACE types.  Most of our 
heuristics are domain independent (e.g. linking the 
parts of an appositive). Our decision to annotate 
names and descriptions separately was driven par-
1439
tially by the need to select the best reference (i.e. 
name) for co-referent clusters. Adding coreference 
heuristics for the two new entity types was the sin-
gle most time-consuming activity, taking 20 of the 
total 43 hours. 
4.3 Relation Extraction  
For relation extraction, we used both pattern 
learning and handwritten patterns. We initialized 
our bootstrap relation learner with the example 
instances provided with the domain ontology; Ta-
ble 3 includes examples of the instances provided 
to the system as training. Our bootstrap relation 
learner finds instances of the relation argument 
pairs in text and then proposes both predicate-
argument structure and word-based connections 
between the arguments as possible new patterns for 
the relation. The learner automatically prunes po-
tential patterns using information about the number 
of known-to-be true and novel instances matched 
by a proposed pattern. By running the pattern ex-
tractor over a large corpus, the proposed patterns 
generate new seeds which are in turn are used to 
propose new patterns. For this experiment, we in-
corporated a small amount of supervision during 
the bootstrapping process (roughly 1 hour total per 
relation); we also performed ~30 minutes total in 
pruning domain patterns at the end of learning.  
 Relation Arg-1 Arg-2 
possTreatmnt AZT AIDS 
studyDisease Dr Henri Joyeux cancer 
studyDisease Samir Khleif cancer 
Table 3: Sample Instances for Initializing Learner 
We also used a small amount of human effort 
creating rules for detecting the relations. The pat-
tern writer was given the guidelines, the examples, 
and a 2K document background corpus and spent 1 
hour per relation writing rules.  
The learned patterns use a subset of the full pat-
tern language used by the pattern-writer. The lan-
guage operates over surface-strings as well as 
predicate-argument structure. Figure 1 illustrates 
learned and handwritten patterns for the possible-
TreatmentRelation(). The patterns in rectangles 
match surface-string patterns; the tree-like patterns 
match normalized predicate argument structure.  
The ?WORD- token indicates a wild card of 1-3 
words.  The blue rectangles at the root of the trees 
in the handwritten patterns are sets of predicates 
that can be matched by the pattern. 
5 Evaluation  
Our question answering evaluation was inspired 
by the evaluation in DARPA?s machine reading 
program, which requires systems to map the in-
formation in text into a formal ontology and an-
swer questions based on that ontology. Unlike 
ACE, this allows evaluators to measure perfor-
mance without exhaustively annotating documents, 
allows for balance between rare and common rela-
tions, and implicitly measures coreference without 
requiring explicit annotation of answer keys for 
coreference. However because the evaluation only 
measures performance on the set of queries, many 
relation instances will be unscored. Furthermore, 
the system is not rewarded for finding the same 
relation multiple times; finding 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat) is 
the same as finding 1 (or 10) instances.  
 
Figure 1: Sample Patterns for possibleTreatment() 
 
The evaluation included only queries of the type 
Find all instances for which the relation P(X, Z) is 
true where one of X or Z is constant. For example, 
Find possible treatments for diabetes; or What is 
expected date to market for Abilify? There were 60 
queries in the evaluation set to be answered from a 
10K document corpus. To produce a preliminary 
answer key, annotators were given the queries and 
corpus indexed by Google Desktop. Annotators 
were given 1 hour to find potential answers to each 
query. If no answers were found after 1 hour, the 
annotators were given a second hour to look for 
answers. For two queries, both of the form Find 
treatments with an expected date to market of MM-
YYYY, even after two hours of searching the anno-
tators were unable to find any answers.6  
Annotator answers served as the initial gold-
standard. Given this initial answer key, annotators 
reviewed system answers and aligned them with 
gold-standard answers. System output not aligned 
with the initial gold standard was assessed as cor-
rect or incorrect. We assume that the final gold-
standard constitutes a complete answer key, and 
                                                          
6 Evaluators wanted some queries with no answers. 
1440
are thus able to calculate recall for our system and 
for humans7. Because we had only one annotator 
for each query and because we assumed that any 
answer found by an annotator was correct, we 
could not estimate human precision on this task.  
Answers can be specific named concepts (e.g. 
Penicillin) or generic descriptions (e.g. drug, ill-
ness). Given the sentence, ACME produces a wide 
range of drugs including treatments for malaria 
and athletes foot,? our reading system would ex-
tract the relations responsibleForTreatment(drugs, 
ACME), possibleTreatment(drugs, malaria), pos-
sibleTreatment(drugs, athletes foot). When a name 
was available in the document, annotators marked 
the answer as correct, but underspecified. We cal-
culated precision and recall treating underspecified 
answers as incorrect and separately calculated pre-
cision and recall counting underspecified answers 
as correct. When treated as correct, there was less 
than a 0.05 absolute increase in both precision and 
recall. Unless otherwise specified, all scores re-
ported here use the stricter condition which treats 
underspecified answers as incorrect.  
We also evaluated extracting all information in a 
small document collection (here human search of 
the 10k documents does not play a role in finding 
answers). Individuals were asked to annotate every 
instance of the 5 relations in a set of 102 docu-
ments. Recall, Precision, and F were calculated by 
aligning system responses to the answer key. Sys-
tem answers that aligned are correct; those that did 
not are incorrect; and answers in the key that were 
not found by the system are misses. Unlike the 
question answering evaluation, this evaluation 
measures the ability to find every instance of a 
fact. If the gold standard includes 100 instances of 
isPossibleTreatment(Penicillin, Strep Throat), re-
call will decrease for each instance missed. The 
?extraction? evaluation does not penalize systems 
for missing coreference.  
6 Results 
6.1 Class Detection 
                                                          
7 The answer key may contain some answers that were found 
neither by the annotator nor by the systems described here, 
since the answer key includes answers pooled from other sys-
tems not reported in this paper. The system reported here was 
the highest performing of all those participating in the experi-
ment. Furthermore, if a system answer is marked as correct, 
but underspecified, the specific  answer is put in the key. 
The recall, precision, and F1 for class detection 
using 10-fold cross validation of the ~1K anno-
tated sentences appear in the 3-5th columns of Table 
4. Given the amount of training, our results are 
lower than in Miller et al(2004) (an F1 of 90 with 
less than 25K words of training). Several factors 
could explain this: Finding boundaries and types 
for descriptions is more complex than for names in 
English. 8  Our classes, pharmaceutical substances 
and physiological conditions, may have been more 
difficult to learn. Our classes are less common in 
news reporting; as such, both word-class clusters 
and active learning may have been less effective. 
Finally, our evaluation was done on a 10-fold split 
of the active-learning selected data; bias in select-
ing the data could explain at least a part of our 
lower performance.  
Type 
# in 
GS 
Without Lists With Lists 
R P F R P F 
Subst-D 789 77 85 80.8 78 85 81.3 
Subst-N 410 70 82 75.5 77 81 78.9 
Cond-D 427 72 78 74.9 72 77 74.4 
Cond-N 963 80 87 83.4 84 83 83.5 
Table 4: Cross Validation:  Condition & Substance 
We noticed that the system frequently reported 
country names to be substance-names. Surprising-
ly, we found that our well-trained name finder 
made the opposite mistake, occasionally reporting 
drugs as geo-political entities.  
We incorporated lists of known substances and 
conditions to improve recall. Performance on the 
same cross-validation split is shown in the final 
three columns of Table 4. Incorporating the lists led 
to recall gains for both substance-name and condi-
tion-name. Because a false-alarm in class recogni-
tion only leads to an incorrect relation extraction if 
it appears in a context indicating a domain relation, 
false alarms of classes may be less important in the 
question answering and extraction evaluations.   
6.2 Question Answering and Extraction 
Figure 2 and Table 6 show system performance 
using only handwritten rules (HW), only learned 
patterns (L), and combining both (C). Figure 2  
includes scores calculated with all of the systems? 
answers (in the dotted boxes), and with just those 
answers that were deemed useful (discussed be-
                                                          
8 English names are capitalized; person names have a typical 
form and are frequently signaled by titles; organization names 
frequently have clear signal words, such as Corp. 
1441
low). We include annotator recall. Handwritten 
patterns outperform learned patterns consistently 
with much higher recall. Encouragingly, however, 
1. The combined system?s recall and F-Score 
are noticeably higher for 3 of the relations.  
2. The learned patterns generate answers not 
found by handwritten patterns.  
3. The learned patterns have high precision.9 
There is variation across the different relations. 
The two best performing relations possibleTreat-
ment() and studiesDisease() have F1 more than 
twice as high as the two worst performing rela-
tions, expectedDateToMarket() and hasSideEf-
fect(). This is primarily due to differences in recall.  
 
Figure 2: Overall Q/A Performance: All answers in  
dotted boxes; 'Useful Answers' unboxed 
The combined system?s recall (0.49), while low, 
is higher than that of the annotators (0.44). While 
hardly surprising that a machine can process in-
formation much more quickly than a person, it is 
encouraging that higher recall is achieved even 
with only one week?s effort. In the context of our 
pooled answer-key, the relatively low recall of 
both the system and the annotator suggests that 
there was little overlap between the answers found 
by the annotator and those found by the system.  
As already described, the system answers can 
include both specific references (e.g. Prozac) and 
more generic references (the drug). When a more 
specific answer is present in the document, generic 
references have been treated as incorrect. Howev-
er, sometimes there is not a more specific refer-
ence; for example an article written before a drug 
has been released may never name the drug. Scores 
reported thus far treat such answers as correct. 
These answers would be useful when answering 
more complex queries. For example, given the sen-
                                                          
9 The learned patterns' high precision is to be expected for two 
reasons. First, a few bad patterns were manually removed for 
each relation. More importantly, the learning algorithm strong-
ly favors high precision patterns because it needs to maintain a 
seed set with low noise in order to learn effectively.  
tence ?ACME spent 5 years developing a pill to 
treat the flu which it will release next week,? ex-
tracting relations involving ?the pill?  would allow 
a system to answer questions that use multiple rela-
tions in the ontology to for example ask about  or-
ganizations developing treatments for the flu, or 
the expected date of release for ACME?s drugs. 
However, in our simple question answering 
framework such generic answers never convey 
novel information and thus were probably ignored 
by human annotators.  
 To measure the impact of treating these generic 
references as correct,10 we did additional annota-
tion on the correct answers, marking answers as 
?useful? (specific) and ?not-useful? (generic). The 
unboxed bars in Figure 2 show performance when 
?not-useful? answers are removed from the answer-
key and the responses. For the four relations where 
there was a change Table 5 provides the relative 
change performance when only ?useful? answers 
are considered. The annotator?s recall increases 
noticeably while the combined system?s drops. 
This results in the overall recall of annotators sur-
passing that of the combined system.   
Relation 
Recall Precision 
A C H L C H L 
possTreat 12 10 10 14 -10 -11 -3 
respTreat 9 0 -5 8 -4 -4 -1 
studyDis 12 -6 -9 13 -11 -13 0 
hasSidEff 3 4 4 4 0 0 0 
Total 11 -2 -4 6 -9 -10 -2 
Table 5: Relative Change in Recall and Precision When 
Non-Useful Answers are Removed 
Table 7 shows the total number of answers pro-
duced by annotators and by each system, as well as 
the percentage of queries with at least one correct 
answer for each system. For one relation expec-
tedDateOnMarket(), the learned system did not 
find any answers. This relation had far fewer an-
swers found by annotators and occurred far more 
rarely in the fully annotated extraction set (see Ta-
ble 8). Anecdotally, extracting this relation fre-
quently required co-referencing ?it? (e.g. ?It will be 
released in March 2011?). Our heuristics for core-
ference of the new classes did not account for pro-
nouns. Learning from such examples would 
require coreference during bootstrapping. Most 
likely, the learned system was unable to generate 
enough novel instances to continue bootstrapping 
                                                          
10 Generic answers were treated as correct only if a more spe-
cific reference was not available in the document.  
1442
and was thus unable to learn the relation.  
Relation Type 
(# Queries; # Correct Ans.) 
Recall Precision F 
A C HW L C HW L C HW L 
possTreatment (10;247) 0.27 0.63 0.50 0.34 0.51 0.47 0.83 0.56 0.48 0.48 
respForTreat (15;134) 0.73 0.33 0.24 0.22 0.66 0.78 0.73 0.44 0.37 0.33 
expectDateMarkt (11;60) 0.90 0.17 0.17 0.00 0.77 0.83 0.00 0.27 0.28 0 
studiesDisease (13;292) 0.23 0.67 0.59 0.09 0.51 0.50 0.79 0.58 0.54 0.16 
hasSideEffect (11;104) 0.80 0.10 0.13 0.02 0.83 0.70 1.00 0.17 0.23 0.04 
Total (60;837) 0.44 0.49 0.42 0.17 0.53 0.52 0.80 0.51 0.46 0.28 
Table 6: Question Answering Results by Relation Type 
Relation Type 
 
Total Number of Answers % Queries with At Least 1 Corr. Ans 
A C HW L A C HW L 
possTreatment  66 303 261 100 100.0% 90.0% 90.0% 90.0% 
respForTreat  98 67 41 40 100.0% 66.7% 60.0% 60.0% 
expectDateMarkt  54 13 12 0 72.7% 45.5% 45.5% 0.0% 
studiesDisease  68 379 347 33 100.0% 61.5% 46.2% 46.2% 
hasSideEffect  83 12 20 2 72.7% 36.4% 45.5% 18.2% 
Total  369 774 681 175 90.0% 60.0% 56.7% 43.3% 
Table 7: Number of Answers and Number of Queries Answered 
Overall, the system did better on relations hav-
ing more correct answers. Bootstrap learning has 
an easier time discovering new instances and new 
patterns when there are more examples to work 
with. Even a human pattern writer will have more 
examples to generalize from for common relations.  
While possibleTreatment() and hasSideEffect() 
have similar F-scores, their performance is very 
different at the query level. The system was able to 
find at least one correct answer to every possible-
Treatment() query; however only 72.7% of the stu-
diesDisease() queries were answered.  
Table 8 presents results from the extraction 
evaluation where a set of ~100 documents were 
annotated for all mentions of the 5 relations. Be-
cause every mention in the document set must be 
found, the system cannot rely on finding the easiest 
answers for common relations. The results in Table 
8 are significantly lower than for the question ans-
wering tasks; yet some of the same trends are 
present. Handwritten rules outperform learned pat-
terns. For at least some relations, the combination 
of the two improves performance. The three rela-
tions for which the learned system has the lowest 
performance on the question-answering task have 
the fewest instances annotated in the document set. 
Fewer instance in the large corpus make bootstrap-
ping more difficult?the learner is less able to gen-
erate novel instances to expand its pattern set.  
7 Discussion 
7.1 Sources of Error 
The most common source of error is pattern cover-
age. In the following figure, the system identified 
responsibleForTreatment(Janssen Pharmaceutical, 
Sporanox), but missed the corresponding relation 
between Novartis and Lamisil.  
 
 
 
 
 
Relation Type # Relations Found Recall Precision F 
GS C HW L C HW L C HW L C HW L 
possibleTreatment 518 225 187 68 0.15 0.10 0.09 0.34 0.28 0.66 0.21 0.15 0.15 
respForTreatment 387 101 77 36 0.10 0.08 0.05 0.41 0.40 0.50 0.17 0.13 0.08 
expDateOnMarket 66 13 13 0 0.06 0.06 0.00 0.31 0.31 0.00 0.10 0.10 0.00 
studiesDisease 136 95 91 4 0.08 0.09 0.00 0.12 0.13 0.00 0.10 0.11 0.00 
hasSideEffect 256 26 25 2 0.04 0.04 0.00 0.39 0.40 0.50 0.07 0.07 0.01 
Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations 
Sporanox is made by Janssen Pharmaceutica Inc., 
of Titusville, N.J. Lamisil is a product of Novartis 
Pharmaceuticals of East Hanover, N.J. 
 
 
1443
Missed class instances contribute to errors, some-
times originating in errors in tokenization (e.g. not 
removing the ?_? in each drug name in a bulleted 
list of the form ?_Trovan, an antibiotic...; etc.) 
However, many drug-names are simply missed: 
 
The system correctly identifies Rebif and Aricept 
as drugs, but misses Pregabalin and Serono. In 
both misses, the immediately preceding and fol-
lowing words provide little evidence that the word 
refers to a drug rather than some other product. 
Substance detection might be better served with a 
web-scale, list-learning approach like the doubly 
anchored patterns described in (Kozareva et al, 
2008). Alternatively, our approach may need to be 
extended to include a larger context window. 
7.2 Learned Patterns  
One of the ways in which learned patterns supple-
ment handwritten ones is learning highly specific 
surface-string patterns that are insensitive to errors 
in parsing. Figure 3 illustrates two examples of 
what appear to be easy cases of possibleTreat-
ment(). Because the handwritten patterns are not 
exhaustive and make extensive use of syntactic 
structure, parse errors prevented the system based 
on handwritten rules from firing. Learned surface-
string patterns were able to find these relations.  
Even when the syntactic structure is correct, 
learned patterns capture expressions not common 
enough to have been noticed by the rule writer. For 
example, while the handwritten patterns included 
?withdrew? as a predicate indicating a company 
was responsible for a drug, they did not include 
?pulled.? By including ?pulled?, learned patterns 
extracted responsibleForTreatment() from ?Ameri-
can Home Products pulled Duract, a painkiller.? 
Similarly, the learned patterns include an explicit 
pattern ?CONDITION drug called SUBSTANCE?, 
and thus extracted a possibleTreatment() relation 
from ?newly approved narcolepsy drug called 
modafinil? without relying on the coreference 
component to link drug to modafinil.  
Handwritten Patterns 
Despite the examples above of successfully learned 
patterns, handwritten patterns perform significantly 
better. In the active-learning context used for these 
experiments, the handwritten rules also required 
less manual effort. This comparison is not entirely 
fair-- while learned patterns required more hours, 
supervising the bootstrapping algorithm requires 
no training. The handwritten patterns, in contrast, 
require a trained expert.  
 
Figure 3: Extractions Missed by Handwritten Rules & 
the Erroneous Parses that Hid them 
While handwritten rules and learned patterns use 
the same language, they make use of it differently. 
The handwritten patterns group similar concepts 
together. A human pattern writer adds relevant 
synonyms, as well as words that are not synonym-
ous but in the pattern context can be used inter-
changeably. In Figure 4, the handwritten patterns 
include three word-sets: (patient*, people, partici-
pant*); (given, taken, took, using); and (report*, 
experience*, develop*, suffer*). The ?*? serves as a 
wild-card to further generalize a pattern. The word-
sets in Figure 4 illustrate challenges for a learned 
system: the words are not synonyms, but rather are 
words that can be used to imply the relation.  
A human pattern writer frequently generates 
new classes not in the domain ontology. In Figure 
4, the circled patterns form a class of ?people tak-
ing a substance.? The handwritten patterns for stu-
diesDisease() include classes targeting scientists 
and researchers. These classes are not necessarily 
triggered by nouns. Such classes allow the pattern 
writer to include complex patterns as in Figure 4 
and to write relatively precise, but open-ended pat-
terns such as: if there is a single named-drug and a 
named, non-side-effect disease in the same sen-
tence, the drug is a treatment for the disease.  
Pfizer also hopes to introduce Pregabalin next 
year for treatment of neuropathic pain, epilepsy 
and anxiety?Other deals include co-promoting 
Rebif for multiple sclerosis with its discoverer, 
Serono, and marketing Aricept for Alzheimer's 
disease with its developer, Eisai Co. 
1444
 
Figure 4: Learned and Handwritten Patterns for  
hasSideEffect() 
A final difference between handwritten and 
learned patterns is the level of predicate-argument 
complexity used. In general, handwritten patterns 
account for larger spans of predicate argument 
structure while learned patterns tend to limit them-
selves to the connections between the arguments of 
the relation with minor extensions.  
8 Conclusions and Lessons Learned 
First, it is encouraging that the synthesis of learn-
ing algorithms and handwritten algorithms can 
achieve an F1 of 0.51 in a new domain in a week 
(<50 hours of effort). Second, it is exciting that so 
little training data is required: ~20 relation pairs 
out of context (~2.5 hours of effort) and ~6 hours 
of active learning for the new classes.  
Third, the effectiveness of learning algorithms is 
still not competitive with handwritten patterns 
based on predicate-argument structure (~5 hours of 
effort on top of active learning for entities). 
Though the learned patterns have high precision 
(0.80 on average), recall is low (0.17) and varied 
greatly across the relations. Though the dominant 
factor in missing relations is pattern coverage, 
missing instances of classes contributed to low re-
call. Comparing learned patterns to manually writ-
ten patterns, (1) synonyms or other lexical 
alternatives that a human pattern writer would in-
clude, (2) the creation of subclasses for argument 
types, and (3) the scope of patterns11 are each ma-
jor sources of the disparity in coverage. Research 
on learning approaches to raise recall without sig-
nificant sacrifice in precision seems essential.  
Fourth, despite the disparity in performance of 
learned versus manual patterns, and despite the low 
                                                          
11 Learned patterns tend to focus on the structure that appears 
between the two arguments, rather than structure surrounding 
the left and right arguments. 
recall of learned patterns, the combined system?s 
recall and F-Score are higher for three of the rela-
tions because the learned patterns generated an-
swers not found by handwritten patterns. We found 
examples where highly specific, learned, surface-
level patterns (lexical patterns) occasionally found 
information missed by handwritten patterns due to 
parsing errors or general low coverage. 
Fifth, the effort for coreference was the most 
time-consuming, given that every new relation 
contained at least one of the new argument types. 
While we included this in our estimate of domain 
adaptation, the infrastructure we built is domain 
generic. Improving generic coreference will reduce 
domain specific effort in future.  
Perhaps most significant of all, running a com-
plete experiment from definition of the domain 
through creation of training data and measurement 
of end-to-end performance of the system can be 
completed in a month. The ability to rapidly, 
cheaply, and empirically measure the impact of 
extraction research could prove a significant spur 
to research across the board. 
These experiments suggest three possible direc-
tions for improving the ability to quickly develop 
information extraction technology for a new set of 
relations: (1) reducing the amount of supervision 
provided to the bootstrap-learner; (2) improving 
the bootstrapping approach to reach the level of 
recall achieved by the human pattern writer elimi-
nating the need for a trained expert during domain 
adaptation; and (3) focusing improvements to the 
bootstrapping approach on techniques that allow it 
to find more of the instances missed by the pattern 
writer, thus improving the accuracy of the hybrid 
system.   
Acknowledgments 
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. Distribu-
tion Statement ?A? (Approved for Public Release, 
Distribution Unlimited) Thank you to the review-
ers for your insightful comments and to Michelle 
Franchini for coordinating the assessment effort. 
References 
E. Agichtein and L. Gravano. Snowball: extracting rela-
tions from large plain-text collections. In Proceed-
ings of the ACM Conference on Digital Libraries, pp. 
85-94, 2000.  
1445
A. Blum and T. Mitchell. Combining Labeled and Un-
labeled Data with Co-Training. In Proceedings of the 
1998 Conference on Computational Learning 
Theory, July 1998.  
E. Boschee, V. Punyakanok, R. Weischedel. An Explo-
ratory Study Towards ?Machines that Learn to Read?. 
Proceedings of AAAI BICA Fall Symposium, No-
vember 2008. 
J. Chen, D. Ji, C. Tan and Z. Niu. (2006). Relation ex-
traction using label propagation based semi-
supervised learning. COLING-ACL 2006: 129-136. 
July 2006. 
M. Freedman, E. Loper, E. Boschee, and R. Weischedel. 
Empirical Studies in Learning to Read. Proceedings 
of NAACL 2010 Workshop on Formalisms and Me-
thodology for Learning by Reading, pp. 61-69, June 
2010. 
W. Li and A. McCallum.  Rapid development of Hindi 
named entity recognition using conditional random 
fields and feature induction. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
R Grishman and B. Sundheim. Message Understanding 
Conference-6 : A Brief History", in COLING-96, 
Proc . of the Int'l Conj. on Computational Linguis-
tics, 1996.  
Z. Kozareva and E. Hovy. Not All Seeds Are Equal: 
Measuring the Quality of Text Mining Seeds. Human 
Language Technologies: The 2010 Annual Confe-
rence of the North American Chapter of the Associa-
tion for Computational Linguistics, June, 2010, pp. 
618-626. 
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic 
class learning from the web with hyponym pattern 
linkage graphs. In Proceedings of ACL-08: HLT, 
pages 1048?1056.  
J. May, A. Brunstein, P. Natarajan,  and R. Weischedel.  
Surprise! What's in a Cebuano or Hindi Name? 
Transactions on Asian Language Information 
Processing (TALIP), Volume 2 Issue 3  September, 
2003. 
D. Maynard, V. Tablan, K. Bontcheva, and H. Cun-
ningham. Rapid customization of an information ex-
traction system for a surprise language. Transactions 
on Asian Language Information Processing (TALIP), 
Volume 2 Issue 3  September, 2003. 
S. Miller, J. Guinness, and A. Zamanian, ?Name Tag-
ging with Word Cluster and Discriminative Train-
ing?, Proceedings of HLT/NAACL 2004, pp. 337-
342, 2004 
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 
R. Wang. ?Populating the Semantic Web by Macro-
Reading Internet Text. Invited paper, Proceedings of 
the 8th International Semantic Web Conference 
(ISWC 2009).  
NIST, ACE 2007: 
http://www.itl.nist.gov/iad/mig/tests/ace/2007/softwa
re.html 
P. Pantel and M. Pennacchiotti. Espresso: Leveraging 
Generic Patterns for Automatically Harvesting Se-
mantic Relations. In Proceedings of Conference on 
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 
Sydney, Australia, 2006.  
L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R. 
Stone, R. Weischedel, A. Zamanian, ?Experiments in 
multi-modal automatic content extraction?, Proceed-
ings of Human Technology Conference, March 2001.  
D. Ravichandran and E. Hovy. Learning surface text 
patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), 
pages 41?47, Philadelphia, PA, 2002.  
E. Riloff. Automatically generating extraction patterns 
from untagged text. In Proceedings of the Thirteenth 
National Conference on Artificial Intelligence, pages 
1044-1049, 1996.  
S. Sekine and R. Grishman.  Hindi-English cross-lingual 
question-answering  system. Transactions on Asian 
Language Information Processing (TALIP), Volume 
2 Issue 3  September, 2003. 
G. Zhou, J. Li, L. Qian, Q. Zhu. Semi-Supervised 
Learning for Relation Extraction. Proceedings of the 
Third International Joint Conference on Natural 
Language Processing: Volume-I. 2008. 
 
1446
String-to-Dependency Statistical
Machine Translation
Libin Shen?
Raytheon BBN Technologies
Jinxi Xu??
Raytheon BBN Technologies
Ralph Weischedel?
Raytheon BBN Technologies
We propose a novel string-to-dependency algorithm for statistical machine translation. This
algorithm employs a target dependency language model during decoding to exploit long distance
word relations, which cannot be modeled with a traditional n-gram language model. Experiments
show that the algorithm achieves significant improvement in MT performance over a state-of-
the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.
1. Introduction
n-gram Language Models (LMs) have been widely used in current Statistical Machine
Translation (SMT) systems. Because they treat a sentence as a flat string of tokens, a
drawback of traditional n-gram LMs is that they cannot model long range word rela-
tions, such as predicate?argument attachments, that are critical to translation quality.
We propose a hierarchical string-to-dependency translation model that exploits
a dependency LM while decoding (as opposed to during reranking n-best output) to
score alternative translations based on their structural soundness. In order to generate
the structured output (dependency trees) required for dependency LM scoring, transla-
tion rules in our system represent the target side as dependency structures. We restrict
the target side of the rules to well-formed dependency structures to weed out bad
translation rules and enable efficient decoding through dynamic programming. Due to
the flexibility of well-formed dependency structures, such structures can cover a large
set of non-constituent transfer rules (Marcu et al 2006) that have been shown useful
for MT.
For comparison purposes, as our baseline, we replicated the Hiero decoder (Chiang
2005), a state-of-the-art hierarchical string-to-string model. Our experiments show that
the string-to-dependency decoder significantly improves MT performance. Overall, the
? 10 Moulton Street, Cambridge, MA 02138. E-mail: libinshen@gmail.com.
?? 10 Moulton Street, Cambridge, MA 02138. E-mail: jxu@bbn.com.
? 10 Moulton Street, Cambridge, MA 02138. E-mail: weisched@bbn.com.
Submission received: 6 March 2009; revised submission received: 1 December 2009; accepted for publication:
18 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
improvement in BLEU score is around 2 BLEU points on NIST Arabic-to-English and
Chinese-to-English newswire test sets.
Section 2 briefly discusses previous approaches to SMT in order to motivate our
work. Section 3 provides an overview of our string-to-dependency translation system.
Section 4 provides a complete description of our system, including formal definitions
of well-formed dependency structures and their operations, as well as proofs about
their key properties. Section 5 describes the implementation details, which include
rule extraction, decoding, using dependency LM scores, and using labels in translation
rules. We discuss experimental results in Section 6, compare our work with related
work in Section 7, and draw conclusions in Section 8.
2. Previous Approaches to SMT
Phrase-based systems (Koehn, Och, and Marcu 2003; Och 2003) had dominated SMT
until recently. Such systems typically treat the input as a sequence of phrases (word
n-grams), reorder them, and produce a translation for the reordered sentence based on
translation options of each source phrase. A prominent feature of such systems is the
use of an n-gram LM to measure the quality of translation hypotheses. A drawback of
such systems is that the lack of structural information in the output makes it impossible
to score translation hypotheses based on their structural soundness.
The Hiero system (Chiang 2007) was a major breakthrough in SMT. Translation
rules in Hiero contain non-terminals (NTs), as well as words, which allow the input
to be translated in a hierarchical manner. Because both the source and target sides of
its translation rules are strings with NTs, Hiero can be viewed as a hierarchical string-
to-string model. Despite the hierarchical nature of its decoder, Hiero lacks the ability to
measure translation quality based on structural relations such as predicate?argument
agreement.
Yamada and Knight (2001) proposed a syntax-based translation model that transfers
a source parse tree into a target string. This method depends on the quality of source
side parsing, and ignores target information during source side analysis. Mi, Huang,
and Liu (2008) later proposed a translation model that takes the source parse forest as
MT input to reduce translation errors due to imperfect source side analysis.
Galley et al (2004) proposed an MT model which produces target parse trees for
string inputs in order to exploit the syntactic structure of the target language. Galley
et al (2006) formalized this approach with tree transducers (Graehl and Knight 2004)
by using context-free parse trees to represent the target side. However, it was later
shown by Marcu et al (2006) and Wang, Knight, and Marcu (2007) that coverage could
be a big issue for the constituent based rules, even though the translation rule set was
already very large.
Carreras and Collins (2009) introduced a string-to-tree MT model based on spinal
Tree Adjoining Grammar (TAG) (Joshi and Schabes 1997; Shen, Champollion, and Joshi
2008). In this model, a translation rule is composed of a source string and a target
elementary tree. Target hypothesis trees are combined with the adjoining operation,
and there are no NT slots for substitution as in LTAG-spinal parsing (Shen and Joshi
2005, 2008; Carreras, Collins, and Koo 2008). Without the constraint of NT slots, the
adjoining operation allows very flexible composition, so that the search space becomes
much larger. One has to carefully prune the search space.
DeNeefe and Knight (2009) proposed another TAG-based MT model. In their
implementation, a TAG grammar was transformed to an equivalent Tree Insertion
650
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Grammar (TIG). In this way, they do not have an explicit adjoining operation in their
system, and as such reduce the search space in decoding. Sub-trees are combined with
NT substitution.
Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to
take advantage of structural knowledge on both sides?for example, as in the papers
by Hajic? et al (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and
Cherry (2005). Although tree-to-tree models can represent rich structural information
of the input and the output, they have not significantly improved MT performance,
possibly due to a much larger grammar and search space. On the other hand, Smith and
Eisner (2006) showed the necessity of allowing loose transformations between the trees,
which made tree-to-tree models even more complicated.
3. Overview of String-to-Dependency Translation
Our system is designed to address problems with existing SMT approaches. It is novel
in two respects. First, it uses a dependency LM to model long-distance relations. Sec-
ond, it uses well-formed dependency structures to represent translation hypotheses to
achieve an effective trade-off between model coverage and decoding complexity.
3.1 Dependency-Based Translation and Language Models
Our system generates target dependency trees as output and exploits a dependency LM
in scoring translation hypotheses. As described before, the goal of using a dependency
LM is to exploit long-distance word dependencies and as such model the quality of the
output more accurately.
Figure 1 shows an example dependency tree. Each arrow points from a child to its
parent. In this example, the word find is the root.
For the purpose of comparison, we first show how a simplified SMT system uses an
n-gram LM to score translation hypotheses:
S?e = argmax
Se
P(Se|Sf )
w1P(Sf |Se)
w2P(Se)
w3 (1)
where w1,w2, and w3 are feature weights. Sf is the input and Se?s are outputs. P(Se|Sf )
is the probability of the target string given the source, and P(Sf |Ss) is the probability of
the source given the target. P(Se) is the prior probability of the target string Se using an
n-gram LM.
Figure 1
The dependency tree for sentence the boy will find it interesting.
651
Computational Linguistics Volume 36, Number 4
In comparison, the scoring function in our system is:
D? = argmax
D
P(D|Sf )
w1P(Sf |D)
w2P(D)w3 (2)
where P(D) is the dependency LM score of target dependency tree D. We will show how
to compute P(D) in Section 5.4.
We can rewrite Equation (2) with a linear model:
D? = argmax
D
n
?
i=1
wiFi(Sf ,D) (3)
where n = 3,F1 = log P(D|Sf ),F2 = log P(Sf |D), and F3 = log P(D). In practice, we use
both a dependency LM and a traditional n-gram LM (also known as a string LM), as
well as several other features, in our decoder. Section 5.6 lists all the features used in
our decoder.
3.2 Well-Formed Dependency Structures
A central question in our system design is: What kinds of dependency structures are
allowed in translation rules? One extreme would be to allow any arbitrary multiple
level treelets, as in Ding and Palmer (2005) and Quirk, Menezes, and Cherry (2005).
One can define translation rules on any fragment of a parse/dependency tree. It offers
maximum coverage of translation patterns, but suffers from data sparseness and a large
search space.
The other extreme would be to allow only complete (CFG) constituents. This offers
a more robust model and a small search space, but excludes many useful transfer rules.
In our system, the target side hypotheses are restricted to well-formed dependency
structures (see Section 4 for formal definitions) for a trade-off between rule coverage,
model robustness, and decoding complexity. In short, a well-formed dependency struc-
ture is either (1) a single rooted tree, with each child being a complete sub-tree, or (2) a
sequence of siblings, each being a complete sub-tree.
Well-formed dependency structures are very flexible and can represent a variety of
non-constituent rules in addition to rules that are complete constituents. For example,
the following translation
hong
Chinese-to-English
?????????????? the red
is obviously useful for Chinese-to-English MT, but cannot be represented in some tree-
based translation systems since the red is a partial constituent. However, it is a valid
dependency structure in our system.
4. Formalism
We first formally define the well-formed dependency structures, which are used to
represent target hypotheses. Then, we define the operations to build well-formed de-
pendency structures from the bottom up in decoding.
652
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
4.1 Well-Formed Dependency Structures and Categories
In order to exclude undesirable structures and reduce the search space, we only allow
Se whose dependency structure D is well formed, which we will define subsequently.
The well-formedness requirement will be applied to partial decoding results.
Based on the results of previous work (DeNeefe et al 2007), we keep two kinds of
dependency structures, fixed and floating. Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. We call them fixed dependency
structures because the head is known or fixed. Floating structures consist of a number
of consecutive sibling nodes of a common head, but the head itself is unspecified. Each
of the siblings must be a complete constituent. Floating structures can represent many
linguistically meaningful non-constituent structures: for example, like the red, a modifier
of a noun. Only those two kinds of dependency structures are well-formed structures in
our system.
In the rest of this section, we will provide formal definitions of well-formed
structures and combinatory operations over them, so that we can easily manipulate
them in decoding. Examples will be provided along with the formal definitions to aid
understanding.
Consider a sentence S = w1w2...wn. Let d1d2...dn represent the parent word IDs for
each word. For example, d4 = 2 means that w4 depends on w2. If wi is a root, we define
di = 0.
Definition 1
A dependency structure didi+1...dj, or di..j for short, is fixed on head h, where h ? [i, j],
or fixed for short, if and only if it meets the following conditions
1. dh /? [i, j]
2. ?k ? [i, j] and k = h, dk ? [i, j]
3. ?k /? [i, j], dk = h or dk /? [i, j]
We say the category of di..j is (?, h,?), where ? means this field is undefined.
Definition 2
A dependency structure di...dj is floating with children C, for a non-empty set C ?
{i, ..., j}, or floating for short, if and only if it meets the following conditions
1. ?h /? [i, j], s.t.?k ? C, dk = h
2. ?k ? [i, j] and k /? C, dk ? [i, j]
3. ?k /? [i, j], dk /? [i, j]
We say the category of di..j is (C,?,?) if j < h, which means that children are on the left
side of the head, or (?,?,C) otherwise.
A category is composed of the three fields (A, h,B), where h is used to represent the
head, and fields A and B represent left and right dependents of the head, respectively.
A dependency structure is well-formed if and only if it is either fixed or floating.
653
Computational Linguistics Volume 36, Number 4
Examples
We represent dependency structures with graphs. Figure 2 shows examples of fixed
structures, Figure 3 shows examples of floating structures, and Figure 4 shows ill-
formed dependency structures.
The structures in Figures 2 and 3 are well-formed. Figure 4(a) is ill-formed because
boy does not have its child word the in the tree. Figure 4(b) is ill-formed because it is not
a continuous segment due to the missing it.
As for the example the red mentioned earlier, it is a well-formed floating dependency
structure.
It is easy to see that a floating structure whose child set C has only one element is
also a fixed structure. Actually, this is a desirable property on which we will introduce
meta category operations later. However, for the sake of convenience, we would like to
assign a single category to each well-formed structure.
Figure 2
Fixed dependency structures.
Figure 3
Floating dependency structures.
Figure 4
Ill-formed dependency structures.
654
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Definition 3
Let structure T be well formed. Category cat of T is defined as follows
cat(T) =
?
?
?
(?, h,?) if T is fixed on h
(C,?,?) if T is floating with children C on the left side AND |C| > 1
(?,?,C) if T is floating with children C on the right side AND |C| > 1
cat is well-defined according to Definitions 1 and 2.
4.2 Operations
One of the purposes of introducing floating dependency structures is that siblings hav-
ing a common parent will become a well-defined entity, although they are not con-
sidered a constituent. We always build well-formed partial structures on the target side
in decoding. Furthermore, we combine partial dependency structures in a way such
that we can obtain all possible well-formed dependency structures (but no ill-formed
ones) during bottom?up decoding.
The solution is to employ categories (introduced earlier). Each well-formed depen-
dency structure has a category. We can apply four combinatory operations over the
categories. If we can combine two categories with a certain category operation, we can
use a corresponding tree operation to combine two dependency structures. The cate-
gory of the combined dependency structure is the result of the combinatory category
operations.
Operations on Well-Formed Dependency Structures
There are four types of operations on well-formed dependency structures. Instead of
providing formal definitions, we use figures to illustrate these operations to make them
easy to understand. Figure 1 shows a traditional dependency tree. Figure 5 shows the
four operations for combining partial dependency structures, which are left adjoining
(LA), right adjoining (RA), left concatenation (LC), and right concatenation (RC). We
always combine two well-formed structures in one of the four ways, and obtain a larger
well-formed structure.
Two structures can be combined by adjoining, which is similar to the traditional
dependency formalism. We can adjoin either a fixed structure or a floating structure to
the head of a fixed structure.
Completed siblings can be combined via concatenation. We can concatenate two
fixed structures, one fixed structure with one floating structure, or two floating struc-
tures in the same direction.
The flexibility of the order of operation allows us to take advantage of various
translation fragments encoded in transfer rules. Figure 6 shows alternative ways of
applying operations on well-formed structures to build larger structures in a bottom?
up style. Numbers represent the order of operation. The fact that the same dependency
structure can have multiple derivations means that we can utilize various rules learned
from different training samples. Such flexibility is important for MT.
655
Computational Linguistics Volume 36, Number 4
Figure 5
Operations over well-formed structures.
Figure 6
Two alternative derivations of an example dependency tree.
Meta Operations on Categories
We first introduce three meta category operations, which will later be used to define
category operations. Two of the meta operations are unary operations, left raising (LR)
and right raising (RR), and one is the binary operation unification (UF).
Definition 4
Meta Category Operations
 LR((?, h,?)) = ({h},?,?)
 RR((?, h,?)) = (?,?, {h})
 UF((A1, h1,B1), (A2, h2,B2)) = NORM((A1 unionsq A2, h1 unionsq h2,B1 unionsq B2))
First, the raising operations are used to turn a completed fixed structure into a
floating structure, according to Theorem 1.
Theorem 1
A fixed structure with category (?, h,?) for span [i, j] is also a floating structure with
children {h} if there are no outside words depending on word h, which means that
?k /? [i, j], dk = h (4)
Proof
It suffices to show that all the three conditions of floating structures hold. Conditions 1
and 2 immediately follow from conditions 1 and 2 of the fixed structure, respectively.
Condition 3 is met according to Equation (4) and condition 3 of the fixed structure. 
656
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Therefore, we can always raise a fixed structure if we assume it is complete, that is,
Equation (4) holds.
Unification is well-defined if and only if we can unify all three elements and the
result is a valid fixed or floating category. For example, we can unify a fixed structure
with a floating structure or two floating structures in the same direction, but we cannot
unify two fixed structures.
h1 unionsq h2 =
?
?
?
h1 if h2 = ?
h2 if h1 = ?
undefined otherwise
A1 unionsq A2 =
?
?
?
A1 if A2 = ?
A2 if A1 = ?
A1 ? A2 otherwise
NORM((A, h,B)) =
?
?
?
?
?
?
?
(?, h,?) if h = ?
(A,?,?) if h = ?,B = ?
(?,?,B) if h = ?,A = ?
undefined otherwise
Operations on Categories
Now we define category operations. For the sake of convenience, we use the same
names for category operations and dependency structure operations. We can easily use
the meta category operations to define the four combinatory category operations. The
definition of the operations is as follows.
Definition 5
Combinatory category operations
LA((A1,?,?), (?, h2,?)) = UF((A1,?,?), (?, h2,?))
LA((?, h1,?), (?, h2,?)) = UF(LR((?, h1,?)), (?, h2,?))
LC((A1,?,?), (A2,?,?)) = UF((A1,?,?), (A2,?,?))
LC((A1,?,?), (?, h2,?)) = UF((A1,?,?), LR((?, h2,?)))
LC((?, h1,?), (A2,?,?)) = UF(LR((?, h1,?)), (A2,?,?))
LC((?, h1,?), (?, h2,?)) = UF(LR((?, h1,?)), LR((?, h2,?)))
RA((?, h1,?), (?,?,B2)) = UF((?, h1,?), (?,?,B2))
RA((?, h1,?), (?, h2,?)) = UF((?, h1,?), RR((?, h2,?)))
RC((?,?,B1), (?,?,B2)) = UF((?,?,B1), (?,?,B2))
RC((?, h1,?), (?,?,B2)) = UF(RR((?, h1,?)), (?,?,B2))
RC((?,?,B1), (?, h2,?)) = UF((?,?,B1), RR((?, h2,?)))
RC((?, h1,?), (?, h2,?)) = UF(RR((?, h1,?)), RR((?, h2,?)))
657
Computational Linguistics Volume 36, Number 4
Based on the definitions of dependency structure operations and category op-
erations, one can verify the one-to-one correspondence. This correspondence can be
formally stated in the following theorem.
Theorem 2
Suppose X and Y are well-formed dependency structures and OP(cat(X), cat(Y)) is well-
defined. We have
cat(OP(X,Y)) = OP(cat(X), cat(Y)) (5)
Proof
The proof of the theorem is rather routine, so we just give a sketch here. One can show
it by induction on the number of nodes in dependency structures. It suffices to show
that Equation (5) holds for all the operations. Actually, the category operations are
designed to meet this requirement; the three fields of a category represent the head
and the children on both sides. 
With category operations, we can easily track the types of dependency structures
and constrain operations in decoding.
Soundness and Completeness
Now we show the soundness and completeness of the operations on dependency
structures. If we follow the operations defined herein, we will build all the well-formed
structures and only the well-formed structures.
Theorem 3 (Soundness)
Let X and Y be two well-defined dependency structures, and OP an operation over X
and Y. It can be shown that OP(X,Y) is also a well-defined dependency structure.
Proof
Theorem 3 immediately follows Theorem 2. 
Theorem 4 (Completeness)
Let Z be a well-defined dependency structure with at least two nodes. It can be
shown that there exist well-formed structures X,Y and an operation OP, such that
Z = OP(X,Y).
Proof
If Z is fixed on h, without losing generality, we assume g is the leftmost child (or
rightmost if there is no left child) of h. We detach g from h, and obtain two sub-trees
X and Y which are rooted on g and h respectively. It can be verified that X and Y are
well-formed, and Z = LA(X,Y).
If Z is floating with children {c1, c2, ..., cn}, where n > 1, we can split it into two
floating structures with children {c1} and {c2, ..., cn}, respectively. It is easy to verify
that they are the sub-structures X and Y that we are looking for. 
658
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
5. Implementation
5.1 Translation Rules
Translation rules are central to an MT system. In our system, each rule translates a
source sub-string into a target dependency structure. The target side of the translation
rules constitutes a tree grammar.
One way to define a tree grammar is in the way we described earlier. Two well-
formed structures can be combined into a larger one with adjoining or concatenation,
and there is no non-terminal slot for substitution. This is similar to tree grammars
without substitution, such as the original TAG (Joshi, Levy, and Takahashi 1975) and
LTAG-spinal (Shen, Champollion, and Joshi 2008). A corresponding MT model was
proposed in Carreras and Collins (2009). Search space is a major problem for such an
approach, as we described earlier.
In our system, we introduced NT substitution to combat the search problem. The
NT slots for substitution come from what we have observed in training data. Combina-
tion of well-formed dependency structures can only happen on NT slots. By replacing
NT slots with well-formed structures, we implicitly adjoin or concatenate sub-structures
based on the dependency information stored in rules. We extract the translation rules
from the training data containing word-to-word alignment and target parse trees, which
we will explain in the next section. A similar strategy was employed by DeNeefe and
Knight (2009). They turned a TAG into an equivalent TIG.
In addition to these extracted rules, we also have special rules to adjoin or concate-
nate two neighboring hypotheses. Each of the special rules has two NT slots, but they
vary on target dependency structures. They are comparable to the glue rules in Chiang
(2005).
To formalize translation rules and grammars, a string-to-dependency grammar G is
a 4-tuple G = ?R,X,Tf ,Te? where R is a set of transfer rules. X is the only non-terminal
type.1 Tf is a set of terminals (words) in the source language, and Te is a set of terminals
in the target language.
A string-to-dependency transfer rule R ? R is a 4-tuple R = ?Sf ,Se,D,A? where
Sf ? (Tf ? {X})+ is a source string, Se ? (Te ? {X})+ is a target string, D represents the
dependency structure for Se, and A is the alignment between Sf and Se. Non-terminal
alignments in A must be one-to-one. We ignore the left hand side for both source and
target, since there is only one NT type.
5.2 Rule Extraction
Now we explain how we extract string-to-dependency rules from parallel training data.
The procedure is similar to Chiang (2007) except that we maintain tree structures on the
target side, instead of strings.
Given sentence-aligned bilingual training data, we first use GIZA++ (Och and
Ney 2003) to generate word level alignment. We use a statistical CFG parser to parse
1 Later in the article, we will introduce label information for NTs. However, labels are treated as soft
features, and there is still a single NT type. In fact, other useful information can also be treated as soft
features, for example, length distribution for each NT observed in the training data. Details are provided
in Shen et al (2009).
659
Computational Linguistics Volume 36, Number 4
Figure 7
An example to show the rule extraction procedure. In this example, the word it is replaced with
a non-terminal X, which generates a hierarchical translation rule.
the English side of the training data, and extract dependency trees with Magerman?s
rules (1995). Then we use heuristic rules to extract transfer rules recursively based on
word alignments and the target dependency trees. The rule extraction procedure is as
follows.
1. Initialization:
All the 4-tuples ?Pi,j
f
,Pm,ne ,D,A? are valid span templates, where source
phrase P
i,j
f
is aligned to target phrase Pm,ne under alignment
2 A. D is a
well-formed dependency structure for Pm,ne . All valid span templates are
valid rule templates.
2. Inference:
Let ?Pi,j
f
,Pm,ne ,D1,A? be a valid rule template, and ?P
p,q
f
,Ps,te ,D2,A? a
valid span template, where range [p, q] ? [i, j], [s, t] ? [m,n], D2 is a
sub-structure of D1, and at least one word in P
i,j
f
but not in P
p,q
f
is aligned.
We create a new valid rule template ?P?f ,P
?
e,D
?,A?, where we obtain P?f
by replacing P
p,q
f
with label X in P
i,j
f
, and obtain P?e by replacing P
s,t
e with
X in Pm,ne . Furthermore, we obtain D
? by replacing sub-structure D2
with X in D1.
3 An example is shown in Figure 7.
By applying the inference rule recursively, we can generate rules with arbitrary
aligned NT slots if there are enough words and alignments. In order to make the size
of the grammar manageable, we keep only rules with at most two NT slots and at most
seven source elements.
Following previous work (Och and Ney 2003; Chiang 2007), we have three features
for each rule, which are P(source|target), P(target|source), and the lexical translation
probability given by GIZA. The two conditional probabilities are simply estimated by
counting in all the extracted rules.
2 P
i,j
f
represents the ith to the jth words on the source side, and Pm,ne represents the m
th to the nth words
on the target side. By P
i,j
f
aligned to Pm,ne , we mean all words in P
i,j
f
are either aligned to words in
Pm,ne or unaligned, and vice versa. Furthermore, at least one word in P
i,j
f
is aligned to a word in Pm,ne .
3 If D2 is a floating structure, we need to merge several dependency links into one.
660
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
5.3 Decoding
Following previous work on hierarchical MT (Chiang 2005; Galley et al 2006), we solve
the decoding problem with chart parsing. We view the target dependency trees as
hidden structures in the input. The task of decoding is then to find the best hidden
structure for the input given the transfer grammar and the language models (a string
n-gram LM and a dependency LM).
The parser scans all source cells in a bottom?up style, and checks matched transfer
rules according to the source side. Once there is a completed rule, we build a larger de-
pendency structure by substituting component dependency structures for correspond-
ing NTs in the target dependency structure of rules.
Hypotheses, that is, candidate dependency structures, are organized in a shared
forest, or AND?OR structures. An AND-structure represents an application of a rule
over component OR-structures, and an OR-structure represents a set of alternative
AND-structures with the same state. A state keeps the necessary information about
hypotheses under it, which is needed for computing scores for higher level hypotheses
for dynamic programming. For example, with an n-gram string LM in decoding, a state
keeps the leftmost n ? 1 words and the rightmost n ? 1 words shared by hypotheses in
that state. Because of the use of a dependency LM in decoding, the state information
also includes boundary information about dependency structures for the purpose of
computing dependency LM scores for larger structures.
In the next section, we will explain how to extend categories and states to exploit a
dependency language model during decoding.
5.4 Using Dependency LM Scores
For the dependency tree in Figure 1, we calculate the probability of the tree as follows
P = PT(find)
?PL(will | find-as-head)
?PL(boy | will, find-as-head)
?PL(the | boy-as-head)
?PR(it | find-as-head)
?PR(interesting | it, find-as-head)
Here PT(x) is the probability that word x is the root of a dependency tree. PL and PR
are left and right side generative probabilities respectively. Let wh be the head, and
wL1wL2 ...wLn be all the children on the left side, from the nearest to the farthest. We
use a tri-gram dependency LM,
PL(wL1wL2 ...wLn |wh-as-head) = PL(wL1 |wh-as-head) ? PL(wL2 |wL1 ,wh-as-head) ? ...
?PL(wLn |wLn?1 ,wLn?2 ) ? PL(STOP|wLn ,wLn?1 ) (6)
In this formula, wh-as-head represents the event that w is used as the head, and wLi
represents the event that wLi is a sibling word. The computation of STOP probabilities
greatly complicates the implementation of inside dependency LM probabilities, so we
ignored it in practice. Right side probability PR is defined in a similar way.
661
Computational Linguistics Volume 36, Number 4
We should note that other orders of dependency LMs (e.g., bi-gram or 4-gram) can
be used by changing the independence assumptions in the above formulas. The choice
of using a tri-gram model in our experiments is a trade-off between model robustness
and sharpness given the training data available.
In order to calculate the dependency language model score, or depLM score for
short, on the fly for partial hypotheses in a bottom?up decoding, we need to save more
information in categories and states.
We use a 5-tuple ?LF,LN, h,RN,RF? to represent the category of a dependency
structure. h represents the head. Relative to the head, LF is the farthest children on
the left side and RF the farthest children on the right side. Similarly, LN is the nearest
children on the left side and RN the nearest children on the right. The three types of
categories are as follows.
 fixed: ?LF,?, h,?,RF?
 floating left: ?LF,LN,?,?,??
 floating right: ??,?,?,RN,RF?
Furthermore, operations similar to those described in Section 4.2 are used to keep track
of the head and boundary child nodes, which are then used to compute depLM scores
in decoding.
5.5 Using Labels in Transfer Rules
In the formalism introduced in the previous section, there is only a single non-terminal
type X. This may result in loss of information in the training data. For example, there is
a rule whose target dependency structure is X1 ? says ? X2, where X1 and X2 depend
on says. In the training data, X1 comes from a tree rooted on a noun and X2 comes from a
tree rooted on a verb. Without this information in the rule, any structure could be placed
in either of these two slots in the decoding phase.
We alleviate this problem by associating a label with each non-terminal in the rules.
Specifically, each non-terminal has a label, and the whole target structure side also has
a label. When we replace an NT with a sub-structure, we check if the label of the sub-
structure is the same as the NT label. If they do not match, we assign a penalty to this
replacement.
An obvious choice of the label is the POS tag of the head word, if it is a fixed tree. In
the previous example, the target structure would generate X1(NN) ? says ? X2(VBP),
where NN means noun (singular or mass) and VBP means verb (non-3rd person sin-
gular present), and the whole target structure has a label of VBZ, which means verb
(3rd person singular present). If we replace NN with a sub-tree rooted at, for example,
a preposition, there will be a penalty.
In our system, we use the POS tag of the head word as the label of a fixed structure.
We always use the generic label for floating structures. Any NT substitution with this
label involved is regarded as a mismatch. In other words, there is a penalty for inserting
any floating structure during decoding.
This extension does not affect the basic formalism of dependency structures de-
scribed in the previous section. Instead, we modify the representation of translation
rules and states in the decoder. For each rule, if its dependency structure is of a fixed
type, the whole structure has a label which is the POS tag of the head word. Otherwise,
the label is X. Similarly, each NT slot has a label which is defined in the same way,
662
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
based on the dependency structure from which the rule is extracted. In decoding,
each state has an extra field representing the label for the dependency structure of the
hypothesis.
5.6 Other Details
We have nine features in our system.
1. Log probability of the source side given the target side of a rule
2. Log probability of the target side given the source side of a rule
3. Log probability of word alignment
4. Number of target words
5. Number of special rules (see Section 5.1) used
6. Log probability of string LM
7. Log probability of dependency LM
8. Discount on ill-formed dependency structures
9. Discount on unmatched labels
The values of the first four features are accumulated on the rules used in a transla-
tion. The fifth feature counts the number of times the adjoining and concatenation rules
are used in a translation. The string LM score and dependency LM score are the next
two features.
In practice, we also allow hypotheses that do not have well-formed structures in
derivation, but they are penalized. For this purpose, we introduce the null dependency
structure e. For any operation OP and dependency structure X,Y, we have
OP(X,Y) = e if this operation is not defined in Definition 5
OP(X, e) = X
OP(e,X) = X
Because part of a hypothesis may have a null dependency structure, we cannot calculate
dependency LM scores on some of the related words. Therefore, we give a discount for
each of these words. This is the eighth feature.
There are two sources for null dependency structures. One is the use of an unde-
fined operation, for example, left-adjoining a right floating structure to a fixed structure.
The other source is a lack of target structure information in translation rules. The parser
that we used may fail to generate parse trees for short segments?for example, dictio-
nary items. In these cases, we extracted the so-called phrasal rules with null dependency
structures. We limited phrasal rules to at most three lexical items for each side.
The last feature counts the number of substitutions with unmatched labels.
In decoding, partial hypotheses are mapped into states. The states maintain suf-
ficient statistics for feature calculation. For example, each state should memorize the
leftmost two words and rightmost two words for LM score calculation. Similar exten-
sions are required for dependency LM score and NT labels. Therefore, we use beam
search with cube pruning as in Chiang (2005) for speedup. Like chart parsing, the
663
Computational Linguistics Volume 36, Number 4
computational complexity of decoding time is O(n3 ? B ? |G|), where n is the length
of the source sentence, B is beam width, and |G| is the maximal number of transfer rules
applicable to a span with translation grammar G. This number agrees with the empirical
results.
We tune the weights with several rounds of decoding and optimization. Following
Och (2003), the k-best results are accumulated as the input to the optimizer. Powell?s
method is used for optimization with 20 random starting points around the weight
vector of the last iteration. For improved results, we rescore 1,000-best translations,
generated using the technique described by Huang and Chiang (2005), by replacing
tri-gram string LM scores in the output with 5-gram string LM scores. The algorithm to
tune the rescoring weights is similar to the one to tune the decoder weights.
6. Experiments
We experimented with four models:
 baseline: hierarchical string to string translation, using our own replication
of the Hiero system (Chiang 2007)
 filtered: like the baseline, it uses string to string rules, except that rules
whose target side does not correspond to a well-formed structure in rule
extraction are excluded. No dependency LM is used in decoding
 str-dep: string-to-dependency system. It uses rules with target dependency
structures and a dependency LM in decoding
 labeled: an enhanced str-dep model with POS tags as labels
We use the Hiero model as our baseline because it is the closest to our string-to-
dependency model. They use similar rule extraction and decoding algorithms. The
major difference is in the representation of target structures. We use dependency
structures instead of strings; thus, the comparison will show the contribution of using
dependency information in decoding.
All models were tuned on BLEU (Papineni, Roukos, and Ward 2001), and evaluated
on BLEU, TER (Snover et al 2006), and METEOR (Banerjee and Lavie 2005). It is well
known that all automatic scores are crude approximations of translation quality. It is not
uncommon for a technique to improve the metric that is used for tuning but hurt other
metrics. The use of multiple metrics helps us avoid drawing false conclusions based on
metric-specific improvements. For both Arabic-to-English and Chinese-to-English MT,
we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
The training data for Arabic-to-English MT contains around 1.9 million pairs of
bi-lingual sentences from ten corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC-
2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, and
SSUSAC27 (Sakhr Arabic-English Parallel Corpus). The training data for Chinese-to-
English MT contains around 1.0 million pairs of bi-lingual sentences from eight corpora:
LDC2002E18, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006G05, LDC2002L27,
LDC2005T34, and LDC2003E07.
The dependency LMs were trained on the same parallel training data. For that pur-
pose, we parsed the English side of the parallel data. Two separate models were
trained: one for Arabic from the Arabic training data and the other for Chinese from
the Chinese training data. Traditional tri-gram and 5-gram string LMs were trained on
664
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Table 1
Number of transfer rules.
Model Arabic-to-English Chinese-to-English
baseline 337,542,137 193,922,173
filtered 32,057,337 39,005,696
str-dep 35,801,341 41,013,346
labeled 41,201,100 43,705,510
the English side of the parallel data as well as the English Gigaword corpus V3.0 in a
way described by Bulyko et al (2007).
Table 1 shows the number of transfer rules extracted from the training data for
the tuning and test sets. The constraint of well-formed dependency structures greatly
reduced the size of the rule set. Although the rule size increased a little bit after incorpo-
rating dependency structures and labels in rules, the size of string-to-dependency rule
set is about 10% to 20% of the baseline.
Tables 2 and 3 show the BLEU, TER, and METEOR scores on MT06 and MT08 for
Arabic-to-English MT. Tables 4 and 5 show the scores for Chinese-to-English MT.
For system comparison, we primarily rely on the lower-cased BLEU score of the
decoding output because it is the metric on which all systems were tuned. We measured
the significance of BLEU, TER, and METEOR with paired bootstrap resampling as
proposed by Koehn (2004). In Tables 2 through 5, (+/-) represent being better/worse
than the baseline at 95% confidence level, respectively, and (*) represents insignificant
difference from the baseline.
For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 on
MT06 and 1.2 on MT08 before 5-gram rescoring. For Chinese-to-English MT, the im-
provements in BLEU were 1.0 on MT06 and 1.4 on MT08. After rescoring, the improve-
ments became smaller, ranging from 0.8 to 1.3. All the BLEU improvements on 5-gram
scores are statistically significant.
The use of POS labels in transfer rules further improves the BLEU score by about
0.7 points on average. The overall BLEU improvement on lower-cased decoding output
Table 2
BLEU, TER, and METEOR percentage scores on MT06 Arabic-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 47.50 45.48 44.79 46.97 66.17
filtered 46.64 (-) 44.47 (-) 45.38 (*) 47.96 (-) 66.64 (*)
str-dep 48.75 (+) 46.74 (+) 43.43 (+) 45.79 (+) 67.18 (+)
labeled 49.33 (+) 47.07 (+) 43.09 (+) 45.53 (+) 67.04 (+)
Rescoring (5-gram LM)
baseline 50.38 48.33 42.64 44.87 67.25
filtered 49.60 (-) 47.51 (-) 43.50 (-) 45.81 (-) 67.44 (*)
str-dep 51.24 (+) 49.23 (+) 42.08 (*) 44.42 (*) 67.89 (+)
labeled 51.80 (+) 49.69 (+) 41.54 (+) 43.76 (+) 67.97 (+)
665
Computational Linguistics Volume 36, Number 4
Table 3
BLEU, TER, and METEOR percentage scores on MT08 Arabic-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 48.41 46.13 43.83 46.18 67.45
filtered 47.37 (-) 45.24 (-) 44.39 (-) 46.83 (-) 67.17 (*)
str-dep 49.58 (+) 47.46 (+) 42.80 (+) 45.08 (+) 68.08 (+)
labeled 50.46 (+) 48.19 (+) 42.27 (+) 44.57 (+) 67.78 (+)
Rescoring (5-gram LM)
baseline 50.50 48.35 42.78 44.92 67.98
filtered 49.56 (-) 47.49 (-) 43.20 (*) 45.44 (*) 67.79 (*)
str-dep 51.23 (+) 49.11 (+) 42.01 (+) 44.15 (+) 68.65 (+)
labeled 51.93 (+) 49.86 (+) 41.27 (+) 43.33 (+) 68.40 (+)
is 1.8 points on MT06 and 2.1 points on MT08 for Arabic-to-English translation, and
2.0 points on MT06 and 1.6 points on MT08 for Chinese-to-English translation.
METEOR scores became significantly better for all conditions. TER improved sig-
nificantly for Arabic-to-English but marginally on Chinese-to-English tasks. The results
on METEOR and TER suggested that the new model did improve translation accuracy.
The filtered string-to-string rules can be viewed as the string projection of string-
to-dependency rules. It shows the performance of using dependency structure for rule
filtering only. The results are very interesting. On Arabic-to-English, the filtered model
was significantly worse, which means that many useful rules were lost due to the
structural constraints. On Chinese-to-English, the tri-gram scores of the filtered model
were a little bit worse. However, after 5-gram rescoring, the BLEU scores became higher
than the baseline, and METEOR scores were even significantly better. We suspect that
the different performance that we observed is due to the difference in source languages
and their tokenization methods.
Table 4
BLEU, TER, and METEOR percentage scores on MT06 Chinese-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 36.40 34.79 54.98 56.53 57.25
filtered 36.02 (*) 34.23 (*) 55.29 (*) 57.03 (*) 57.60 (+)
str-dep 37.44 (+) 35.62 (+) 54.64 (*) 56.47 (*) 57.42 (+)
labeled 38.37 (+) 36.53 (+) 54.14 (+) 55.99 (*) 58.42 (+)
Rescoring (5-gram LM)
baseline 37.88 36.18 53.80 55.45 57.44
filtered 38.52 (*) 36.74 (*) 54.09 (*) 55.69 (*) 58.16 (+)
str-dep 38.91 (+) 37.04 (+) 53.65 (*) 55.45 (*) 57.99 (+)
labeled 39.11 (+) 37.30 (+) 53.61 (*) 55.29 (*) 58.69 (+)
666
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Table 5
BLEU, TER, and METEOR percentage scores on MT08 Chinese-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 31.64 29.56 57.35 59.37 54.93
filtered 31.26 (*) 29.42 (*) 57.46 (*) 59.28 (*) 55.16(+)
str-dep 33.05 (+) 31.26 (+) 56.79 (*) 58.69 (+) 55.18(+)
labeled 33.25 (+) 31.34 (+) 56.60 (+) 58.49 (+) 56.01(+)
Rescoring (5-gram LM)
baseline 33.06 31.21 55.84 57.71 55.18
filtered 33.25 (*) 31.22 (*) 56.53 (*) 58.39 (-) 56.08 (+)
str-dep 34.34 (+) 32.32 (+) 55.60 (*) 57.60 (*) 55.91 (+)
labeled 35.02 (+) 33.00 (+) 55.39 (*) 57.48 (*) 56.46 (+)
In any case, the purpose of the filtered model is not to propose the use of structural
constraints for rule filtering, although it greatly reduced the rule size and allowed
the use of more useful training data potentially. The use of structural constraints is
compulsory for the introduction of dependency LMs and non-terminal labels, which
compensated for the loss of rule filtering, and led to significant overall improvement.
7. Comparison to Related Work
Fox (2002), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005) showed
that, for the purpose of representing word relations, dependency structures are ad-
vantageous over CFG structures because they do not require complete constituents. A
number of techniques have been proposed to improve rule coverage. Marcu et al (2006)
and Galley et al (2006) introduced artificial constituent nodes dominating the phrase of
interest. The binarization method used by Wang, Knight, and Marcu (2007) can cover
many non-constituent rules also, but not all of them. DeNeefe et al (2007) showed that
the best results were obtained by combining these methods.
Charniak, Knight, and Yamada (2003) described a two-step string-to-CFG-tree
translation model which employed a syntax-based language model to select the best
translation from a target parse forest built in the first step. A crucial difference from our
work is that they only used the tree-based LM in rescoring, possibly due to the com-
plexity of the syntax-based LM. In contrast, our system uses a dependency LM directly
in decoding and as such can prune out unpromising hypotheses as soon as possible.
The use of a dependency LM in MT is similar to the use of a structured LM in
ASR (Chelba and Jelinek 2000; Xu, Chelba, and Jelinek 2002), with the same motivation
of exploiting long-distance relations. A difference is that the dependency LM is used
bottom?up in our MT system, whereas the structured LM is used left-to-right in ASR.
Another difference is that long-distance relations are more important in MT due to
word re-orderings.
The well-formed dependency structures defined here are similar to the data struc-
tures in previous work on monolingual parsing (Eisner and Satta 1999; McDonald,
Crammer, and Pereira 2005), which allowed floating structures as well defined states
in derivation, too. However, as for monolingual parsing, one usually wants exactly
667
Computational Linguistics Volume 36, Number 4
one derivation for each parse tree, so as to avoid spurious ambiguity of derivations
for the same parse. The derivation model proposed by Eisner and Satta (1999) satisfied
this prerequisite, and had O(n3) complexity with a bi-lexical probability model, which
was O(n4) in many other derivation models. In our MT model, the motivation is to
exploit various translation fragments learned from the training data, and the opera-
tions in monolingual parsing were designed to avoid artificial ambiguity of derivation.
Another difference is that we have fixed structures growing on both sides, whereas
fixed structures in (Eisner and Satta 1999) can only grow in one direction.
The formalism for well-formed structures and the operations over them were
inspired by the well-known approach of Combinatory Categorial Grammar (CCG)
(Steedman 2000). In fact, the names of left raising and right raising stem from the
raising operation in CCG.
The string-to-dependency formalism can be viewed as a special case of Synchro-
nous Tree Adjoining Grammar (STAG) (Shieber and Schabes 1990). Trees on the source
side are weakened to strings, and multi-rooted structures are employed on the tar-
get side. The adjoining operation in our model is similar to attachment in LTAG-
spinal (Shen, Champollion, and Joshi 2008) and sister adjunction in variants (Rambow,
Shanker, and Weir 1995; Chiang 2000; Carreras, Collins, and Koo 2008) of TAG (Joshi and
Schabes 1997). Translation rules can be viewed as constraints on the tree operations.
8. Conclusions and Future Work
In this article, we propose a novel string-to-dependency algorithm for statistical ma-
chine translation. It employs a target dependency language model to exploit long dis-
tance word relations in decoding, which cannot be captured with a traditional n-gram
language model.
Compared with a state-of-the-art hierarchical string-to-string system, our string-to-
dependency system generates about 80% fewer rules. The overall gain in BLEU score
on lower-cased decoding output is about two points.
Dependency structures provide a desirable platform for employing linguistic
knowledge in MT. We will extend our approach with deeper linguistic features such as
propositional structures (Palmer, Gildea, and Kingsbury 2005). The fixed and floating
structures proposed in this article can be extended to model predicates and arguments.
Acknowledgments
This work4 was supported by DARPA/
IPTO Contract No. HR0011-06-C-0022
under the GALE program. The views,
opinions, and/or findings contained in
this article/presentation are those of the
author/presenter and should not be
interpreted as representing the official views
or policies, either expressed or implied, of
the Defense Advanced Research Projects
Agency or the Department of Defense.
We are grateful to our colleagues Roger Bock,
Ivan Bulyko, Mike Kayser, Jeff Ma, John
4 Distribution Statement ?A? (Approved for
Public Release, Distribution Unlimited).
Makhoul, Spyros Matsoukas, Antti-Veikko
Rosti, Rich Schwartz, Bing Zhang, and
Rabih Zbib for their help in running the
experiments and constructive comments
to improve this article. Mike Kayser helped
to proofread the manuscript. We also thank
Zhifei Li and the anonymous reviewers for
their suggestions to improve this article.
References
Banerjee, Satanjeev and Alon Lavie. 2005.
Meteor: An automatic metric for MT
evaluation with improved correlation
with human judgments. In Proceedings
of the 43th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 101?104, Ann Arbor, MI.
668
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Bulyko, Ivan, Spyros Matsoukas, Richard
Schwartz, Long Nguyen, and John
Makhoul. 2007. Language model
adaptation in machine translation from
speech. In Proceedings of the 32nd IEEE
International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),
pages 117?120, Honolulu, HI.
Carreras, Xavier and Michael Collins. 2009.
Non-projective parsing for statistical
machine translation. In Proceedings of
the 2009 Conference of Empirical Methods
in Natural Language Processing,
pages 200?209, Singapore.
Carreras, Xavier, Michael Collins, and
Terry Koo. 2008. TAG, dynamic
programming, and the perceptron for
efficient, feature-rich parsing. In
Proceedings of the 12th Conference on
Computational Natural Language Learning,
pages 9?16, Manchester.
Charniak, Eugene, Kevin Knight, and Knight
Yamada. 2003. Syntax-based language
models for statistical machine translation.
In Proceedings of MT Summit IX,
pages 40?46, New Orleans, LA.
Chelba, Ciprian and Frederick Jelinek. 2000.
Structured language modeling. Computer
Speech and Language, 14(4):283?332.
Chiang, David. 2000. Statistical parsing
with an automatically extracted tree
adjoining grammar. In Proceedings of
the 38th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 456?463, Hong Kong.
Chiang, David. 2005. A hierarchical phrase-
based model for statistical machine
translation. In Proceedings of the 43th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
DeNeefe, Steve and Kevin Knight. 2009.
Synchronous tree adjoining machine
translation. In Proceedings of the 2009
Conference of Empirical Methods in Natural
Language Processing, pages 727?736,
Singapore.
DeNeefe, Steve, Kevin Knight, Wei Wang,
and Daniel Marcu. 2007. What can
syntax-based MT learn from phrase-based
MT? In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language
Processing, pages 755?763, Prague.
Ding, Yuan and Martha Palmer. 2005.
Machine translation using probabilistic
synchronous dependency insertion
grammars. In Proceedings of the 43th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 541?548, Ann
Arbor, MI.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 205?208, Sapporo.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton
grammars. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 457?464, College Park, MD.
Fox, Heidi. 2002. Phrasal cohesion and
statistical machine translation.
In Proceedings of the 2002 Conference
of Empirical Methods in Natural
Language Processing, pages 304?311,
Philadelphia, PA.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic models. In
COLING-ACL ?06: Proceedings of 44th
Annual Meeting of the Association for
Computational Linguistics and 21st
International Conference on Computational
Linguistics, pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the 2004
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 273?280, Boston, MA.
Graehl, Jonathan and Kevin Knight. 2004.
Training tree transducers. In Proceedings of
the 2004 Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 105?112, Boston, MA.
Hajic?, Jan, Martin C?mejrek, Jason Eisner,
Gerald Penn, Owen Rambow, Dragomir
Radev, Yuan Ding, Terry Koo, and Kristen
Parton. 2002. Natural language generation
in the context of machine translation.
Final report of JHU Summer Workshop
project, Johns Hopkins University,
Baltimore, MD.
Huang, Liang and David Chiang. 2005.
Better k-best parsing. In Proceedings of the
9th International Workshop on Parsing
Technologies, pages 53?64, Vancouver.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
669
Computational Linguistics Volume 36, Number 4
Journal of Computer and System Sciences,
10(1):136?163.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of Formal
Languages, volume 3. Springer-Verlag,
Berlin, pages 69?124.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the 2004 Conference
of Empirical Methods in Natural Language
Processing, pages 388?395, Barcelona.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase based
translation. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 48?54,
Edmonton.
Magerman, David. 1995. Statistical decision-
tree models for parsing. In Proceedings of
the 33rd Annual Meeting of the Association
for Computational Linguistics,
pages 276?283, Cambridge, MA.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases.
In Proceedings of the 2006 Conference of
Empirical Methods in Natural Language
Processing, pages 44?52, Sydney.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online large-
margin training of dependency parsers.
In Proceedings of the 43th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 91?98, Ann Arbor, MI.
Mi, Haitao, Liang Huang, and Qun Liu. 2008.
Forest-based translation. In Proceedings of
the 46th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 192?199, Columbus, OH.
Och, Franz J. 2003. Minimum error rate
training for statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo.
Och, Franz J. and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?52.
Palmer, Martha, Daniel Gildea, and
Paul Kingsbury. 2005. The proposition
bank: An annotated corpus of semantic
roles. Computational Linguistics,
31(1):71?106.
Papineni, Kishore, Salim Roukos, and
Todd Ward. 2001. BLEU: A method for
automatic evaluation of machine
translation. IBM Research Report No.
RC22176, Armonk, NY.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed phrasal
SMT. In Proceedings of the 43th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 271?279,
Ann Arbor, MI.
Rambow, Owen, Vijay K. Shanker, and
David Weir. 1995. D-tree grammars.
In Proceedings of the 33rd Annual Meeting
of the Association for Computational
Linguistics, pages 151?158,
Cambridge, MA.
Shen, Libin, Lucas Champollion, and
Aravind K. Joshi. 2008. LTAG-spinal
and the Treebank: A new resource for
incremental, dependency and semantic
parsing. Language Resources and Evaluation,
42(1):1?19.
Shen, Libin and Aravind K. Joshi. 2005.
Incremental LTAG Parsing. In Proceedings
of Human Language Technology Conference
and Conference on Empirical Methods in
Natural Language Processing, pages 811?818,
Vancouver.
Shen, Libin and Aravind K. Joshi. 2008.
LTAG dependency parsing with
bidirectional incremental construction.
In Proceedings of the 2008 Conference
of Empirical Methods in Natural Language
Processing, pages 495?504, Honolulu, HI.
Shen, Libin, Jinxi Xu, Bing Zhang, Spyros
Matsoukas, and Ralph Weischedel.
2009. Effective use of linguistic and
contextual information for statistical
machine translation. In Proceedings of the
2009 Conference of Empirical Methods in
Natural Language Processing, pages 72?80,
Singapore.
Shieber, Stuart and Yves Schabes. 1990.
Synchronous tree adjoining grammars.
In Proceedings of COLING ?90: The 13th
International Conference on Computational
Linguistics, pages 253?258, Helsinki.
Smith, David A. and Jason Eisner. 2006.
Quasi-synchronous grammars:
Alignment by soft projection of syntactic
dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine
Translation, pages 23?30, New York, NY.
Snover, Matthew, Bonnie Dorr, Richard
Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation
edit rate with targeted human annotation.
In Proceedings of Association for Machine
Translation in the Americas, pages 223?231,
Cambridge, MA.
670
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Steedman, Mark. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA.
Wang, Wei, Kevin Knight, and Daniel Marcu.
2007. Binarizing syntax trees to improve
syntax-based machine translation accuracy.
In Proceedings of the 2007 Conference
of Empirical Methods in Natural Language
Processing, pages 746?754, Prague.
Xu, Peng, Ciprian Chelba, and Frederick
Jelinek. 2002. A study on richer syntactic
dependencies for structured language
modeling. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 191?198,
Philadelphia, PA.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523?530, Toulouse.
671

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288?293,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 Coreference for Learning to Extract Relations:  
Yes, Virginia, Coreference Matters 
 
Ryan Gabbard 
rgabbard@bbn.com 
 
Marjorie Freedman 
mfreedma@bbn.com 
 
Ralph Weischedel 
weischedel@bbn.com 
 
Raytheon BBN Technologies, 10 Moulton St., Cambridge, MA 02138 
 
The views expressed are those of the author and do not reflect the official policy or position of the De-
partment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.  
 
 
Abstract 
As an alternative to requiring substantial su-
pervised relation training data, many have ex-
plored bootstrapping relation extraction from 
a few seed examples. Most techniques assume 
that the examples are based on easily spotted 
anchors, e.g., names or dates. Sentences in a 
corpus which contain the anchors are then 
used to induce alternative ways of expressing 
the relation. We explore whether coreference 
can improve the learning process. That is, if 
the algorithm considered examples such as his 
sister, would accuracy be improved? With co-
reference, we see on average a 2-fold increase 
in F-Score. Despite using potentially errorful 
machine coreference, we see significant in-
crease in recall on all relations. Precision in-
creases in four cases and decreases in six.  
1 Introduction 
As an alternative to requiring substantial super-
vised relation training data (e.g. the ~300k words 
of detailed, exhaustive annotation in Automatic 
Content Extraction (ACE) evaluations1) many have 
explored bootstrapping relation extraction from a 
few (~20) seed instances of a relation. Key to such 
approaches is a large body of unannotated text that 
can be iteratively processed as follows:  
1. Find sentences containing the seed instances. 
2. Induce patterns of context from the sentences. 
3. From those patterns, find more instances. 
4. Go to 2 until some condition is reached. 
Most techniques assume that relation instanc-
es, like hasBirthDate(Wolfgang Amadeus Mozart, 
                                                          
1
 http://www.nist.gov/speech/tests/ace/ 
1756), are realized in the corpus as relation texts2 
with easily spotted anchors like Wolfgang 
Amadeus Mozart was born in 1756.  
In this paper we explore whether using corefer-
ence can improve the learning process. That is, if 
the algorithm considered texts like his birth in 
1756 for the above relation, would performance of 
the learned patterns be better? 
2 Related Research 
There has been much work in relation extraction 
both in traditional supervised settings and, more 
recently, in bootstrapped, semi-supervised settings. 
To set the stage for discussing related work, we 
highlight some aspects of our system. Our work 
initializes learning with about 20 seed relation in-
stances and uses about 9 million documents of un-
annotated text3 as a background bootstrapping 
corpus. We use both normalized syntactic structure 
and surface strings as features. 
Much has been published on learning relation 
extractors using lots of supervised training, as in 
ACE, which evaluates system performance in de-
tecting a fixed set of concepts and relations in text. 
Researchers have typically used this data to incor-
porate a great deal of structural syntactic infor-
mation in their models (e.g. Ramshaw, 2001), but 
the obvious weakness of these approaches is the 
resulting reliance on manually annotated examples, 
which are expensive and time-consuming to create. 
                                                          
2
 Throughout we will use relation instance to refer to a fact 
(e.g. ORGHasEmployee(Apple, Steve Jobs)), while we will use 
relation text to refer a particular sentence entailing a relation 
instance (e.g. Steve Jobs is Apple?s CEO).  
3
 Wikipedia and the LDC?s Gigaword newswire corpus. 
288
Others have explored automatic pattern genera-
tion from seed examples. Agichtein & Gravano 
(2000) and Ravichandran & Hovy (2002) reported 
results for generating surface patterns for relation 
identification; others have explored similar ap-
proaches (e.g. Pantel & Pennacchiotti, 2006). 
Mitchell et al (2009) showed that for macro-
reading, precision and recall can be improved by 
learning a large set of interconnected relations and 
concepts simultaneously. In all cases, the ap-
proaches used surface (word) patterns without co-
reference. In contrast, we use the structural 
features of predicate-argument structure and em-
ploy coreference. Section 3 describes our particular 
approach to pattern and relation instance scoring 
and selection.  
Another research strand (Chen et al, 2006 & 
Zhou et al, 2008) explores semi-supervised rela-
tion learning using the ACE corpus and assuming 
manual mention markup. They measure the accu-
racy of relation extraction alone, without including 
the added challenge of resolving non-specific rela-
tion arguments to name references. They limit their 
studies to the small ACE corpora where mention 
markup is manually encoded.  
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran 
and Hovy (2002) report results in the Text Retriev-
al Conference (TREC) Question Answering track, 
where extracting one text of a relation instance can 
be sufficient, rather than detecting all texts. Mitch-
ell et al (2009), while demonstrating high preci-
sion, do not measure recall. 
In contrast, our study has emphasized recall. A 
primary focus on precision allows one to ignore 
many relation texts that require coreference or 
long-distance dependencies; one primary goal of 
our work is to measure system performance in ex-
actly those areas. There are at least two reasons to 
not lose sight of recall. For the majority of entities 
there will be only a few mentions of that entity in 
even a large corpus. Furthermore, for many infor-
mation-extraction problems the number documents 
at runtime will be far less than web-scale.  
3 Approach 
Figure 1 depicts our approach for learning patterns 
to detect relations. At each iteration, the steps are:   
(1) Given the current relation instances, find possi-
ble texts that entail the relation by finding sentenc-
es in the corpus containing all arguments of an in-
stance.  
(2)  As in Freedman et al (2010) and Boschee et 
al. (2008), induce possible patterns using the con-
text in which the arguments appear. Patterns in-
clude both surface strings and normalized syntactic 
structures.4 Each proposed pattern is applied to the 
corpus to find a set of hypothesized texts.
 
For each 
pattern, a confidence score is assigned using esti-
mated precision5 and recall.  The highest confi-
dence patterns are added to the pattern set.6 
(3) The patterns are applied to the corpus to find 
additional possible relation instances. For each 
proposed instance, we estimate a score using a Na-
ive Bayes model with the patterns as the features.  
When using coreference, this score is penalized if 
an instance?s supporting evidence involves low-
confidence coreference links. The highest scoring 
instances are added to the instance set. 
 (4) After the desired number of iterations (in these 
experiments, 20) is complete, a human reviews the 
resulting pattern set and removes those patterns 
which are clearly incorrect (e.g. ?X visited Y? for 
hasBirthPlace).7   
 
Figure 1: Approach to learning relations 
We ran this system in two versions: ?Coref has 
no access to coreference information, while +Coref 
(the original system) does. The systems are other-
wise identical. Coreference information is provided 
by BBN?s state-of-the-art information extraction 
                                                          
4
 Surface text patterns with wild cards are not proposed until 
the third iteration.  
5
 Estimated recall is the weighted fraction of known instances 
found. Estimated precision is the weighted average of the 
scores of matched instances; scores for unseen instances are 0.   
6
 As more patterns are accepted in a given iteration, we raise 
the confidence threshold.  Usually, ~10 patterns are accepted 
per iteration.  
7
 This takes about ten minutes per relation, which is less than 
the time to choose the initial seed instances. 
pattern 
database
proposed
instances
proposed 
patterns
proposed
pairs
retrieve from 
corpus
retrieve from corpus
induce
prune 
and add
granted
patent
obj
INVENTOR
INVENTION
iobj
for
Thomas Edison ? light bulb
Alexander G. Bell ... telephone
Ben Franklin ? lightning rod
Edison invented the light bulb
Bell built the first telephone
Edison was granted a U.S. patent 
for the light bulb
Franklin invented the lightning rod
example pairs
instances
289
system (Ramshaw, et al, 2011; NIST, 2007) in a 
mode which sacrifices some accuracy for speed 
(most notably by reducing the parser?s search 
space). The IE system processes over 50MB/hour 
with an  average EDR Value score when evaluated 
on an 8-fold cross-validation of the ACE 2007.  
+Coref can propose relation instances from text 
in which the arguments are expressed as either 
name or non-name mentions. When the text of an 
argument of a proposed instance is a non-name, the 
system uses coreference to resolve the non-name to 
a name. -Coref can only propose instances based 
on texts where both arguments are names.8 
This has several implications: If a text that en-
tails a relation instance expresses one of the argu-
ments as a non-name mention (e.g. ?Sue?s husband 
is here.?), -Coref will be unable to learn an in-
stance from that text. Even when all arguments are 
expressed as names, -Coref may need to use more 
specific, complex patterns to learn the instance 
(e.g. ?Sue asked her son, Bob, to set the table?). 
We expect the ability to run using a ?denser,? more 
local space of patterns to be a significant advantage 
of +Coref. Certain types of patterns (e.g. patterns 
involving possessives) may also be less likely to be 
learned by -Coref. Finally, +Coref has access to 
much more training data at the outset because it 
can find more matching seed instances,9 potentially 
leading to better and more stable training. 
4 Evaluation Framework 
Estimating recall for bootstrapped relation learning 
is a challenge except for corpora small enough for 
complete annotation to be feasible, e.g., the ACE 
corpora. ACE typically had a test set of ~30,000 
words and ~300k for training. Yet, with a small 
corpus, rare relations will be inadequately repre-
sented.10 Macro-reading evaluations (e.g. Mitchell, 
2009) have not estimated recall, but have measured 
precision by sampling system output and determin-
ing whether the extracted fact is true in the world. 
                                                          
8
 An instance like hasChild(his father, he) would be useful 
neither during training nor (without coreference) at runtime. 
9
 An average of 12,583 matches versus 2,256 matches. If mul-
tiple mentions expressing an argument occur in one sentence, 
each match is counted, inflating the difference. 
10
 Despite being selected to be rich in the 18 ACE relation 
subtypes, the 10 most frequent subtypes account for over 90% 
of the relations with the 4 most frequent accounting for 62%; 
the 5 least frequent relation subtypes occur less than 50 times. 
Here we extend this idea to both precision and re-
call in a micro-reading context.  
Precision is measured by running the system 
over the background corpus and randomly sam-
pleing 100 texts that the system believes entail 
each relation. From the mentions matching the ar-
gument slots of the patterns, we build a relation 
instance.  If these mentions are not names (only 
possible for +Coref), they are resolved to names 
using system coreference.  For example, given the 
passage in Figure 2 and the pattern ?(Y, poss:X)?, 
the system would match the mentions X=her and 
Y=son, and build the relation instance 
hasChild(Ethel Kennedy, Robert F. Kennedy Jr.). 
During assessment, the annotator is asked 
whether, in the context of the whole document, a 
given sentence entails the relation instance. We 
thus treat both incorrect relation extraction and 
incorrect reference resolution as mistakes.  
To measure recall, we select 20 test relation in-
stances and search the corpus for sentences con-
taining all arguments of a test instance (explicitly 
or via coreference). We randomly sampled from 
this set, choosing at most 10 sentences for each test 
instance, to form a collection of at most 200 sen-
tences likely to be texts expressing the desired rela-
tion. These sentences were then manually 
annotated in the same manner as the precision an-
notation. Sentences that did not correctly convey 
the relation instance were removed, and the re-
maining set of sentences formed a recall set.  We 
consider a recall set instance to be found by a sys-
tem if the system finds a relation of the correct 
type in the sentence. We intentionally chose to 
sample 10 sentences from each test example, rather 
than sampling from the set of all sentences found. 
This prevents one or two very commonly ex-
pressed instances from dominating the recall set. 
As a result, the recall test set is biased away from 
?true? recall, because it places a higher weight on 
the ?long tail? of instances. However, this gives a 
more accurate indication of the system?s ability to 
find novel instances of a relation.  
Ethel Kennedy says that when the family gathered 
for Thanksgiving she wanted the children to know 
what a real turkey looked like. So she sent her son, 
Robert F. Kennedy Jr., to a farm to buy two birds. 
Figure 2: Passage entailing hasChild relation 
290
5 Empirical Results 
Table 1 gives results for precision, recall, and F 
for +Coref (+) and ?Coref (-). In all cases remov-
ing coreference causes a drop in recall, ranging 
from only 33%(hasBirthPlace) to over 90% 
(GPEEmploys). The median drop is 68%. 
 
5.1 Recall 
There are two potential sources of ?Coref?s 
lower recall. For some relation instances, the text 
will contain only non-named instances, and as a 
result -Coref will be unable to find the instance.     
-Coref is also at a disadvantage while learning, 
since it has access to fewer texts during bootstrap-
ping.  Figure 311 presents the fraction of instances 
in the recall test set for which both argument 
names appear in the sentence.  Even with perfect 
patterns, -Coref has no opportunity to find roughly 
25% of the relation texts because at least one ar-
gument is not expressed as a name.   
To further understand -Coref?s lower perfor-
mance, we created a third system, *Coref, which 
used coreference at runtime but not during train-
ing.12 In a few cases, such as hasBirthPlace, 
*Coref is able to almost match the recall of the 
system that used coreference during learning 
(+Coref), but on average the lack of coreference at 
runtime accounts for only about 25% of the differ-
ence, with the rest accounted for by differences in 
the pattern sets learned. 
Figure 4 shows the distribution of argument 
mention types for +Coref on the recall set.  Com-
paring this to Figure 3, we see that +Coref uses 
name-name pairs far less often than it could (less 
                                                          
11
 Figures 3 & 4 do not include hasBirthDate: There is only 1 
potential named argument for this relation, the other is a date.  
12
 *Coref was added after reading paper reviews, so there was 
not time to do annotation for a precision evaluation for it. 
than 50% of the time overall).  Instead, even when 
two names are present in a sentence that entails the 
relation, +Coref chooses to find the relation in 
name-descriptor and name-pronoun contexts which 
are often more locally related in the sentences. 
 
Figure 4: Distribution of argument mention types for 
+Coref matches on the recall set 
For the two cases with the largest drops in re-
call, ORGEmploys and GPEEmploys, +Coref and ?
Coref have very different trajectories during train-
ing.  For example, in the first iteration, ?Coref 
learns patterns involving director, president, and 
head for ORGEmploys, while +Coref learns pat-
terns involving joined and hired.  We speculate 
that ?Coref may become stuck because the most 
frequent name-name constructions, e.g. ORG/GPE 
title PERSON (e.g. Brazilian President Lula da 
Silva), are typically used to introduce top officials. 
For such cases, even without co-reference, system 
specific effort and tuning could potentially have 
improved ?Coref?s ability to learn the relations.  
5.2 Precision 
Results on precision are mixed. While for 4 of 
the relations +Coref is higher, for the 6 others the 
addition of coreference reduces precision. The av-
erage precisions for +Coref and ?Coref are 82.2 
and 87.8, and the F-score of +Coref exceeded that 
 
0 %
20 %
40 %
60 %
80 %
100 %
1 2 3 4 5 6 7 8 9
Other
Combi nati ons
Both Desc
Name & Pron
Name & Desc
Both Na me
 P+ P- R+ R- R* F+ F- 
attendSchool (1) 83 97 49 16 27 62 27 
GPEEmploy(2) 91 96 29 3 3 44 5 
GPELeader (3) 87 99 48 28 30 62 43 
hasBirthPlace (4) 87 97 57 37 53 69 53 
hasChild (5) 70 60 37 17 11 48 27 
hasSibling (6) 73 69 67 17 17 70 28 
hasSpouse (7) 61 96 72 22 31 68 36 
ORGEmploys(8) 92 82 22 4 7 35 7 
ORGLeader (9) 88 97 73 32 42 80 48 
hasBirthDate (10) 90 85 45 13 32 60 23 
Table 1: Precision, Recall, and F scores 
 
Figure 3: Fraction of recall instances with name 
mentions present in the sentence for both arguments. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1 2 3 4 5 6 7 8 9
%
 R
ec
al
l I
ns
ta
nc
es
291
of ?Coref for all relations. Thus while +Coref pays 
a price in precision for its improved recall, in many 
applications it may be a worthwhile tradeoff. 
Though one might expect that errors in coref-
erence would reduce precision of +Coref, such er-
rors may be balanced by the need to use longer 
patterns in ?Coref. These patterns often include 
error-prone wildcards which lead to a drop in pre-
cision. Patterns with multiple wildcards were also 
more likely to be removed as unreliable in manual 
pattern pruning, which may have harmed the recall 
of ?Coref, while improving its precision. 
5.3 Further Analysis 
Our analysis thus far has focused on micro-
reading which requires a system find all mentions 
of an instance relation ? i,e, in our evaluation Or-
gLeader(Apple, Steve Jobs) might occur in as 
many as 20 different contexts.  While ?Coref per-
forms poorly at micro-reading, it could still be ef-
fective for macro-reading, i.e. finding at least one 
instance of the relation OrgLeader(Apple, Steve 
Jobs). As a rough measure of this, we also evaluat-
ed recall by counting the number of test instances 
for which at least one answer was found by the two 
systems. With this method, +Coref?s recall is still 
higher for all but one relation type, although the 
gap between the systems narrows somewhat. 
 
In addition to our recall evaluation, we meas-
ured the number of sentences containing relation 
instances found by each of the systems when ap-
plied to 5,000 documents (see Table 3).  For al-
most all relations, +Coref matches many more 
sentences, including finding more sentences for 
those relations for which it has higher precision. 
6 Conclusion 
Our experiments suggest that in contexts where 
recall is important incorporating coreference into a 
relation extraction system may provide significant 
gains. Despite being noisy, coreference infor-
mation improved F-scores for all relations in our 
test, more than doubling the F-score for 5 of the 
10.  
Why is the high error rate of coreference not 
very harmful to +Coref?  We speculate that there 
are two reasons. First, during training, not all co-
reference is treated equally.  If the only evidence 
we have for a proposed instance depends on low 
confidence coreference links, it is very unlikely to 
be added to our instance set for use in future itera-
tions.  Second, for both training and runtime, many 
of the coreference links relevant for extracting the 
relation set examined here are fairly reliable, such 
as wh-words in relative clauses. 
There is room for more investigation of the 
question, however. It is also unclear if the same 
result would hold for a very different set of rela-
tions, especially those which are more event-like 
than relation-like. 
Acknowledgments 
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. The views 
expressed are those of the authors and do not re-
flect the official policy or position of the Depart-
ment of Defense or the U.S. Government. We 
would like to thank our reviewers for their helpful 
comments and Martha Friedman, Michael Heller, 
Elizabeth Roman, and Lorna Sigourney for doing 
our evaluation annotation.  
  
 +Coref -Coref #Test 
Instances 
ORGEmploys 8 2 20 
GPEEmploys 12 3 19 
hasSibling 11 4 19 
hasBirthDate 12 5 17 
hasSpouse 15 9 20 
ORGLeader 14 9 19 
attendedSchool 17 12 20 
hasBirthPlace 19 15 20 
GPELeader 15 13 19 
hasChild 6 6 19 
Table 2: Number of test seeds where at least one 
instance is found in the evaluation. 
Prec Number of Sentences 
Relation P+ P- +Cnt -Cnt *Cnt 
attendedSchool 83 97 541 212 544 
hasChild 91 96 661 68 106 
hasSpouse 87 99 1262 157 282 
hasSibling 87 97 313 72 272 
GPEEmploys 70 60 1208 308 313 
GPELeader 73 69 1018 629 644 
ORGEmploys 61 96 1698 142 209 
ORGLeader 92 82 1095 207 286 
hasBirthDate 88 97 231 131 182 
hasBirthPlace 90 85 836 388 558 
Table 3: Number of sentences in which each system 
found relation instances   
292
References 
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In 
Proceedings of the ACM Conference on Digital Li-
braries, pp. 85-94. 
M. Banko, M. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proceedings of the International 
Joint Conference on Artificial Intelligence. 
A. Baron and M. Freedman. 2008. Who is Who and 
What is What: Experiments in Cross Document Co-
Reference. In Empirical Methods in Natural Lan-
guage Processing.  
A. Blum and T. Mitchell. 1998. Combining Labeled and 
Unlabeled Data with Co-Training. In Proceedings of 
the 1998 Conference on Computational Learning 
Theory. 
E. Boschee, V. Punyakanok, R. Weischedel. 2008. An 
Exploratory Study Towards ?Machines that Learn to 
Read?. Proceedings of AAAI BICA Fall Symposium. 
J. Chen, D. Ji, C. Tan and Z. Niu. 2006. Relation extrac-
tion using label propagation based semi-supervised 
learning. COLING-ACL 2006: 129-136. 
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 
R. Wang. 2009. Populating the Semantic Web by 
Macro-Reading Internet Text. Invited paper, Pro-
ceedings of the 8th International Semantic Web Con-
ference (ISWC 2009).  
National Institute of Standards and Technology.  2007. 
NIST 2007 Automatic Content Extraction Evaluation 
Official Results. http://www.itl.nist.gov/iad/mig/ 
tests/ace/2007/doc/ace07_eval_official_results 
_20070402.html 
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging Generic Patterns for Automatically Harvesting 
Semantic Relations. In Proceedings of Conference on 
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 
Sydney, Australia.  
L. Ramshaw, E. Boschee, S. Bratus, S. Miller, R. Stone, 
R. Weischedel, A. Zamanian. 2001. Experiments in 
multi-modal automatic content extraction, In Pro-
ceedings of Human Language Technology Confer-
ence.  
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride, 
R. Weischedel, A. Zamanian. 2011. SERIF Language 
Processing ? Efficient Trainable Language Under-
standing. In Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global 
Autonomous Language Exploitation. Springer. 
D. Ravichandran and E. Hovy. 2002. Learning surface 
text patterns for a question answering system. In 
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002), 
pages 41?47, Philadelphia, PA.  
E. Riloff. 1996. Automatically generating extraction 
patterns from untagged text. In Proceedings of the 
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044-1049.  
G. Zhou, J. Li, L. Qian, Q. Zhu. 2008. Semi-Supervised 
Learning for Relation Extraction. Proceedings of the 
Third International Joint Conference on Natural 
Language Processing: Volume-I.  
Z. Kozareva and E. Hovy. Not All Seeds Are Equal: 
Measuring the Quality of Text Mining Seeds. 2010. 
Human Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the As-
sociation for Computational Linguistics pp. 618-626. 
 
 
293
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 341?345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language Use: What can it Tell us? 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
 
 
Abstract 
For 20 years, information extraction has fo-
cused on facts expressed in text. In contrast, 
this paper is a snapshot of research in progress 
on inferring properties and relationships 
among participants in dialogs, even though 
these properties/relationships need not be ex-
pressed as facts. For instance, can a machine 
detect that someone is attempting to persuade 
another to action or to change beliefs or is as-
serting their credibility? We report results on 
both English and Arabic discussion forums. 
1 Introduction 
Extracting explicitly stated information has been 
tested in MUC1 and ACE2 evaluations. For exam-
ple, for the text Mushaima'a, head of the opposi-
tion Haq movement, an ACE system extracts the 
relation LeaderOf(Mushaima'a, HaqMovement). In 
TREC QA3 systems answered questions, e.g.  
?When was Mozart born??, for which the answer is 
contained in one or a few extracted text phrases.  
Sentiment analysis uses implicit meaning of 
text, but has focused primarily on text known to be 
rich in opinions (product reviews, editorials) and 
delves into only one aspect of implicit meaning.  
Our long-term goal is to predict social roles in 
informal group discussion from language uses 
(LU), even if those roles are not explicitly stated; 
for example, using the communication during a 
meeting, identify the leader of a group. This paper 
provides a snapshot of preliminary, ongoing re-
search in predicting two classes of language use: 
                                                          
1
 http://www-nlpir.nist.gov/related_projects/muc/ 
2
 http://www.nist.gov/speech/tests/ace/ 
3
 http://trec.nist.gov/data/qa.html 
Establish-Credibility and Attempt-To-Persuade. 
Technical challenges include dealing with the facts 
that those LUs are rare and subjective and that hu-
man judgments have low agreement.  
Our hybrid statistical & rule-based approach 
detects those two LUs in English and Arabic. Our 
results are that (1) annotation at the message (turn) 
level provides training data useful for predicting 
rare phenomena at the discussion level while re-
ducing the requirement for turn-level predictions to 
be accurate; (2)weighing subjective judgments 
overcomes the need for high annotator consistency. 
Because the phenomena are rare, always predicting 
the absence of a LU is a very high baseline. For 
English, the system beats those baselines. For Ara-
bic, more work is required, since only 10-20% of 
the amount of training data exists so far.  
2 Language Uses (LUs) 
A language use refers to an aspect of the social 
intention of how a communicator uses language.  
The information that supports a decision about an 
implicit social action or role is likely to be distrib-
uted over more than one turn in a dialog; therefore, 
a language use is defined, annotated, and predicted 
across a thread in the dialog. Because our current 
work uses discussion forums, threads provide a 
natural, explicit unit of analysis. Our current work 
studies two language uses.  
An Attempt-to-Persuade occurs when a poster 
tries to convince other participants to change their 
beliefs or actions over the course of a thread. Typi-
cally, there is at least some resistance on the part of 
the posters being persuaded. To distinguish be-
tween actual persuasion and discussions that in-
volve differing opinions, a poster needs to engage 
341
in multiple persuasion posts (turns) to be consid-
ered exhibiting the LU.  
Establish-Credibility occurs when a poster at-
tempts to increase their standing within the group. 
This can be evidenced with any of several moves, 
e.g., explicit statements of authority, demonstration 
expertise through knowledge, providing verifiable 
information (e.g., from a trusted source or citing 
confirmable facts), or providing a justified opinion 
(e.g., a logical argument or personal experience).  
3 Challenges 
There were two significant challenges: (a) sparsity 
of the LUs, and (b) inter-annotator agreement. To 
address the sparsity of data, we tried to automati-
cally select data that was likely to contain content 
of interest. Data selection focused on the number 
of messages and posters in a thread, as well as the 
frequency of known indicators like quotations. 
(withheld). Despite these efforts, the LUs of inter-
est were rare, especially in Arabic.  
Annotation was developed using cycles of 
guideline development, annotation, evaluation of 
agreement, and revision of guidelines. Elsewhere, 
similar, iterative annotation processes have yielded 
significant improvements in agreement for word 
sense and coreference (Hovy et al, 2006). While 
LUs were annotated for a poster over the full 
thread, annotators also marked specific messages 
in the thread for presence of evidence of the lan-
guage use. Table 1 includes annotator consistency 
at both the evidence (message) and LU level.   
 English Arabic 
 Msg LU Msg LU 
 Agr # Agr # Agr # Agr # 
Per. 0.68 4722 0.75 2151 0.57 652 0.49 360 
Cred. 0.66 3594 0.68 1609 0.35 652 0.45 360 
Table 1: Number of Annotated Data Units and Annota-
tor Agreement (measured as F) 
The consistency numbers for this task were sig-
nificantly lower than we have seen in other lan-
guage processing tasks. Discussions suggested that 
disagreement did not come from a misunderstand-
ing of the task but was the result of differing intui-
tions about difficult-to-define labels. In the 
following two sections, we describe how the eval-
uation framework and system development pro-
ceeded despite low levels of consistency.  
4 Evaluation Framework 
Task. The task is to predict for every participant in 
a given thread, whether the participant exhibits 
Attempt-to-Persuade and/or Establish-Credibility. 
If there is insufficient evidence of an LU for a par-
ticipant, then the LU value for that poster is nega-
tive. The external evaluation measured LU 
predictions. Internally we measured predictions of 
message-level evidence as well. 
Corpora. For English, 139 threads from 
Google Groups and LiveJournal have been anno-
tated for Attempt-to-Persuade, and 103 threads for 
Attempt-to-Establish-Credibility. For Arabic, 
threads were collected from al-handasa.net.4 31 
threads were annotated for both tasks. Counts of 
annotated messages appear in Table 1. 
Measures. Due to low annotator agreement, at-
tempting to resolve annotation disagreement by the 
standard adjudication process was too time-
consuming. Instead, the evaluation scheme, similar 
to the pyramid scheme used for summarization 
evaluation, assigns scores to each example based 
on its level of agreement among the annotators. 
Specifically, each example is assigned positive and 
negative scores, p = n+/N and n = n-/N, where n+ is 
the number of annotators that annotate the example 
as positive, and n- for the negative. N is the total 
number of annotators. A system that outputs posi-
tive on the example results in p correct and n incor-
rect. The system gets p incorrect and n correct for 
predicting negative. Partial accuracy and F-
measure can then be computed. 
Formally, let X = {xi} be a set of examples. 
Each example xi is associated with positive and 
negative scores, pi and ni. Let ri = 1 if the system 
outputs positive for example xi and 0 for negative. 
The partial accuracy, recall, precision, and F-
measure can be computed by: 
pA = 100??i(ripi+(1-ri)ni) / ?i(pi+ni) 
pR = 100??iripi / ?ipi 
pP = 100? ?iripi / ?iri 
pF = 2 pR pP/(pR+pP) 
The maximum pA and pF may be less than 100 
when there is disagreement between annotators. To 
achieve accuracy and F scores on a scale of 100, 
pA and pF are normalized using the maximum 
achievable scores with respect to the data. 
npA = 100?pA/max(pA) 
npF = 100?pF/max(pF) 
                                                          
4
 URLs and judgments are available by email. 
342
5 System and Empirical Results 
Our architecture is shown in Figure 1. We process 
a thread in three stages: (1) linguistic analysis of 
each message (post) to yield features, (2) Predic-
tion of message-level properties using an SVM on 
the extracted features, and (3) Simple rules that 
predict language uses over the thread.  
 
Figure 1: Message and LU Prediction 
Phase 1: The SERIF Information Extraction 
Engine extracts features which are designed to cap-
ture different aspects of the posts. The features in-
clude simple features that can be extracted from 
the surface text of the posts and the structure of the 
posts within the threads. These may correlate di-
rectly or indirectly correlate to the language uses. 
In addition, more syntactic and semantic-driven 
features are also used. These can indicate the spe-
cific purpose of the sentences; specifically target-
ing directives, imperatives, or shows authority. The 
following is a partial list of features which are used 
both in isolation and in combination with each oth-
er. 
Surface and structural features: average sen-
tence length; number of names, pronouns, and dis-
tinct entities; number of sentences, URLs (links), 
paragraphs and out-of-vocabulary words; special 
styles (bold, italics, stereotypical punctuation e.g. 
!!!! ), depth in thread, and presence of a quotation. 
Syntactic and semantic features: predicate-
argument structure including the main verb, sub-
ject, object, indirect object, adverbial modifier, 
modal modifier, and negation, imperative verbs, 
injection words, subjective words, and mentions of 
attack events. 
Phase 2: Given training data from the message 
level (Section 3), an SVM predicts if the post con-
tains evidence for an LU. The motivation for this 
level is (1) Posts provide a compact unit with reli-
ably extractable, specific, explicit features. (2) 
There is more training data at the post level. (3) 
Pointing to posts offers a more clear justification 
for the predictions. (4) In our experiments, errors 
here do not seem to percolate to the thread level. In 
fact, accuracy at the message level is not directly 
predictive of accuracy at the thread level. 
Phase 3: Given the infrequency of the Attempt-
to-Persuade and Establish-Credibility LUs, we 
wrote a few rules to predict LUs over threads, giv-
en the predictions at the message level. For in-
stance, if the number of messages with evidence 
for persuasion is greater than 2 from a given partic-
ipant, then the system predicts AttemptToPer-
suade. Phase 3 is by design somewhat robust to 
errors in Phase 2. To predict that a poster is exhib-
iting the Attempt-to-Persuade LU, the system need 
not find every piece of evidence that the LU is pre-
sent, but rather just needs to find sufficient evi-
dence for identifying the LU.  
Our message level classifiers were trained with 
an SVM that optimizes F-measure (Joachims, 
2005). Because annotation disagreement is a major 
challenge, we experimented with various ways to 
account for (and make use of) noisy, dual annotat-
ed text. Initially, we resolved the disagreement au-
tomatically, i.e. removing examples with 
disagreement; treating an example as negative if 
any annotator marked the example negative; and 
treating an example as positive if any annotator 
marked the example as positive. An alternative 
(and more principled) approach is to incorporate 
positive and negative scores for each example into 
the optimization procedure. Because each example 
was annotated by the same number of annotators (2 
in this case), we are able to treat each annotator?s 
decision as an independent example without aug-
menting the SVM optimization process.  
The results below use the training procedure 
that performed best on the leave-one-thread-out 
cross validation results (Table 23 and Table 34). 
Counts of threads appear in Section 4. We compare 
our system?s performance (S) with two simple 
baselines. Baseline-A (A) always predicts absent 
for the LU/evidence. Baseline-P (P) predicts posi-
tive (present) for all messages/LUs. Table 4Table 3 
shows results for predicting message level evi-
dence of an LU (Phase 2). Table 5Table 4 shows 
performance on the task of predicting an LU for 
each poster. 
The results show significantly worse perfor-
mance in Arabic than English-- not surprising con-
sidering 5-10-fold difference in training examples. 
Additionally, Arabic messages are much shorter, 
and the phenomena is even more rare (as illustrated 
by the high npA, accuracy, of the A baseline).  
343
  Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 72.5 83.2 0.0 0.0 77.6 95.0 0.0 0.0 
P 40.4 29.7 61.1 50.7 33.9 14.4 54.5 30.9 
S 86.5 81.3 79.2 61.9 86.7 95.5 73.9 54.0 
Table 43: Performance on Message Level Evidence 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 90.9 86.7 0.0 0.0 87.7 90.2 0.0 0.0 
P 12.1 27.0 23.8 48.2 18.0 21.5 33.7 41.1 
S 94.6 88.3 76.8 38.8 95.1 92.4 80.0 36.0 
Table 54: Cross Validation Performance on Poster LUs  
Table 6Table 5 shows LU prediction results 
from an external evaluation on held out data. Un-
like our dataset, each example in the external eval-
uation dataset was annotated by 3 annotators. The 
results are similar to our internal experiment. 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 96.2 98.4 0.0 0.0 93.6 94.0 93.6 0.0 
P 13.1 4.2 27.6 11.7 11.1 10.1 11.1 22.2 
S 96.5 94.6 75.1 59.1 97.7 92.5 97.7 24.7 
Table 65: External, Held-Out Results on Poster LUs  
6 Related Research 
Research in authorship profiling (Chung & Penne-
baker, 2007; Argamon et al in press; and Abbasi 
and Chen, 2005) has identified traits, such as sta-
tus, sex, age, gender, and native language. Models 
and predictions in this field have primarily used 
simple word-based features, e.g. occurrence and 
frequency of function words. 
Social science researchers have studied how so-
cial roles develop in online communities (Fisher, et 
al., 2006), and have attempted to categorize these 
roles in multiple ways (Golder and Donath 2004; 
Turner et al, 2005). Welser et al (2007) have in-
vestigated the feasibility of detecting such roles 
automatically using posting frequency (but not the 
content of the messages). 
Sentiment analysis requires understanding the 
implicit nature of the text. Work on perspective 
and sentiment analysis frequently uses a corpus 
known to be rich in sentiment such as reviews or 
editorials (e.g. (Hardisty, 2010), (Somasundaran& 
Weibe, 2009). The MPQA corpus (Weibe, 2005) 
annotates polarity for sentences in newswire, but 
the focus of this corpus is at the sentence level. 
Both the MPQA corpus and the various corpora of 
editorials and reviews have tended towards more 
formal, edited, non-conversational text. Our work 
in contrast, specifically targets interactive discus-
sions in an informal setting. Work outside of com-
putational linguistics that has looked at persuasion 
has tended to examine language in a persuasive 
context (e.g. sales, advertising, or negotiations).  
Like the current work, Strzalkowski, et al 
(2010) investigates language uses over informal 
dialogue. Their work focuses on chat transcripts in 
an experimental setting designed to be rich in the 
phenomena of interest. Like our work, their predic-
tions operate over the conversation, and not a sin-
gle utterance. The specific language uses in their 
work (topic/task control, involvement, and disa-
greement) are different than those discussed here. 
Our work also differs in the data type of interest. 
We work with threaded online discussions in 
which the phenomena in question are rare. Our 
annotators and system must distinguish between 
the language use and text that is opinionated with-
out an intention to persuade or establish credibility.   
7 Conclusions and Future Work 
In this work in progress, we presented a hybrid 
statistical & rule-based approach to detecting prop-
erties not explicitly stated, but evident from lan-
guage use. Annotation at the message (turn) level 
provided training data useful for predicting rare 
phenomena at the discussion level while reducing 
the need for turn-level predictions to be accurate. 
Weighing subjective judgments overcame the need 
for high annotator consistency. For English, the 
system beats both baselines with respect to accura-
cy and F, despite the fact that because the phenom-
ena are rare, always predicting the absence of a 
language use is a high baseline. For Arabic, more 
work is required, particularly since only 10-20% of 
the amount of training data exists so far. 
This work has explored LUs, the implicit, social 
purpose behind the words of a message. Future 
work will explore incorporating LU predictions to 
predict the social roles played by the participants in 
a thread, for example using persuasion and credi-
bility to establish which participants in a discus-
sion are serving as informal leaders.  
344
Acknowledgement 
This research was funded by the Office of the Director 
of National Intelligence (ODNI), Intelligence Advanced 
Research Projects Activity (IARPA), through the _____.  
All statements of fact, opinion or conclusions contained 
herein are those of the authors and should not be con-
strued as representing the official views or policies of 
IARPA, the ODNI or the U.S. Government. 
References 
Argamon, S., Koppel, M., Pennebaker, J.W., and Schler, 
J. (2009). ?Automatically profiling the author of 
an anonymous text?. Communications of the Asso-
ciation for Computing Machinery (CACM). Vol-
ume 52 Issue 2. 
Abbasi A., and Chen H. (2005). ?Applying authorship 
analysis to extremist-group web forum messages?. 
In IEEE Intelligent Systems, 20(5), pp. 67?75. 
Boyd, D, Golder, S, and Lotan, G. (2010). ?Tweet, 
Tweet, Retweet: Conversational Aspects of Re-
tweeting on Twitter.? HICSS-43. IEEE: Kauai, HI. 
Chung, C.K., and Pennebaker, J.W. (2007). ?The psy-
chological functions of function words?. In K. 
Fiedler (Ed.), Social communication, pp. 343-359. 
New York: Psychology Press. 
Golder S., and Donath J. (2004) "Social Roles in Elec-
tronic Communities," presented at the Association 
of Internet Researchers (AoIR). Brighton, England 
Hovy E., Marcus M., Palmer M., Ramshaw L., and 
Weischedel R. (2006). ?Ontonotes: The 90% solu-
tion?. In Proceedings of the Human Language 
Technology Conference of the NAACL, Compan-
ion Volume: Short Papers, pp. 57?60. Association 
for Computational Linguistics, New York City, 
USA. 
Joachims, T. (2005), ?A Support Vector Method for 
Multivariate Performance Measures?, Proceedings 
of the International Conference on Machine 
Learning (ICML). 
Kelly, J., Fisher, D., Smith, D., (2006) ?Friends, foes, 
and fringe: norms and structure in political discus-
sion networks?, Proceedings of the 2006 interna-
tional conference on Digital government research.  
NIST Speech Group. (2008). ?The ACE 2008 evalua-
tion plan: Assessment of Detection and Recogni-
tion of Entities and Relations Within and Across 
Documents?. 
http://www.nist.gov/speech/tests/ace/2008/doc/ace
08 -evalplan.v1.2d.pdf 
Ranganath, R., Jurafsky, D., and McFarland, D. (2009) 
?It?s Not You, it?s Me: Detecting Flirting and its 
Misperception in Speed-Dates? Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pages 334?342. 
Somasundaran, S & Wiebe, J
 
(2009). Recognizing 
Stances in Online Debates. ACL-IJCNLP 2009. 
Strzalkowski, T, Broadwell, G, Stromer-Galley, J, 
Shaikh, S, Taylor, S and Webb, N. (2010) ?Model-
ing Socio-Cultural Phenomena in Discourse?. 
Proceedings of the 23rd International Conference 
on Computational Linguistics (Coling 2010), pag-
es 1038?1046, Beijing, August 2010 
Turner T. C., Smith M. A., Fisher D., and Welser H. T. 
(2005) ?Picturing Usenet: Mapping computer-
mediated collective action?. In Journal of Com-
puter-Mediated Communication, 10(4). 
Voorhees, E. & Tice, D. (2000)."Building a Question 
Answering Test Collection", Proceedings of 
SIGIR, pp. 200-207. 
Welser H. T., Gleave E., Fisher D., and Smith M., 
(2007). "Visualizing the signatures of social roles in 
online discussion groups," In The Journal of Social 
Structure, vol. 8, no. 2. 
Wiebe, J, Wilson, T and Cardie, C (2005). Annotating 
expressions of opinions and emotions in language. 
Language Resources and Evaluation, volume 39, is-
sue 2-3, pp. 165-210. 
 
 
345
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 61?69,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Empirical Studies in Learning to Read 
Marjorie Freedman, Edward Loper, Elizabeth Boschee, Ralph Weischedel 
BBN Raytheon Technologies 
10 Moulton St 
Cambridge, MA 02139 
{mfreedma, eloper, eboschee, weischedel}@bbn.com 
Abstract 
In this paper, we present empirical results on 
the challenge of learning to read. That is, giv-
en a handful of examples of the concepts and 
relations in an ontology and a large corpus, 
the system should learn to map from text to 
the concepts/relations of the ontology. In this 
paper, we report contrastive experiments on 
the recall, precision, and F-measure (F) of the 
mapping in the following conditions: (1) em-
ploying word-based patterns, employing se-
mantic structure, and combining the two; and 
(2) fully automatic learning versus allowing 
minimal questions of a human informant. 
1 Introduction 
This paper reports empirical results with an algo-
rithm that ?learns to read? text and map that text 
into concepts and relations in an ontology specified 
by the user. Our approach uses unsupervised and 
semi-supervised algorithms to harness the diversity 
and redundancy of the ways concepts and relations 
are expressed in document collections. Diversity 
can be used to automatically generate patterns and 
paraphrases for new concepts and relations to 
boost recall. Redundancy can be exploited to au-
tomatically check and improve the accuracy of 
those patterns, allowing for system learning with-
out human supervision.  
For example, the system learns how to recog-
nize a new relation (e.g. invent), starting from 5-20 
instances (e.g. Thomas Edison + the light bulb). 
The system iteratively searches a collection of 
documents to find sentences where those instances 
are expressed (e.g. ?Thomas Edison?s patent for 
the light bulb?), induces patterns over textual fea-
tures found in those instances (e.g. pa-
tent(possessive:A, for:B)), and repeats the cycle by 
applying the generated patterns to find additional 
instances followed by inducing more patterns from 
those instances. Unsupervised measures of redun-
dancy and coverage are used to estimate the relia-
bility of the induced patterns and learned instances; 
only the most reliable are added, which minimizes 
the amount of noise introduced at each step.  
There have been two approaches to evaluation 
of mapping text to concepts and relations: Auto-
matic Content Extraction (ACE)1 and Knowledge 
Base Population (KBP)2. In ACE, complete ma-
nual annotation for a small corpus (~25k words) 
was possible; thus, both recall and precision could 
be measured across every instance in the test set. 
This evaluation can be termed micro reading in 
that it evaluates every concept/relation mention in 
the corpus. In ACE, learning algorithms had 
roughly 300k words of training data.  
By contrast, in KBP, the corpus of documents 
in the test set was too large for a complete answer 
key. Rather than a complete answer key, relations 
were extracted for a list of entities; system output 
was pooled and judged manually. This type of 
reading has been termed macro reading3, since 
finding any instance of the relation in the 1.3M 
document corpus is measured success, rather than 
finding every instance. Only 118 queries were pro-
vided, though several hundred were created and 
distributed by participants.  
In the study in this paper, recall, precision, and 
F are measured for 11 relations under the following 
contrastive conditions 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://apl.jhu.edu/~paulmac/kbp.html  
3 See http://rtw.ml.cmu.edu/papers/mitchell-iswc09.pdf   
61
1. Patterns based on words vs. predicate-
argument structure vs. combining both. 
2. Fully automatic vs. a few periodic res-
ponses by humans to specific queries. 
Though many prior studies have focused on 
precision, e.g., to find any text justification to an-
swer a question, we focus equally on recall and 
report recall performance as well as precision. This 
addresses the challenge of finding information on 
rarely mentioned entities (no matter how challeng-
ing the expression). We believe the effect will be 
improved technology overall. We evaluate our sys-
tem in a micro-reading context on 11 relations. In a 
fully automatic configuration, the system achieves 
an F of .48 (Recall=.37, Precision=.68). With li-
mited human intervention, F rises to .58 (Re-
call=.49, Precision=.70). We see that patterns 
based on predicate-argument structure (text 
graphs) outperform patterns based on surface 
strings with respect to both precision and recall. 
Section 2 describes our approach; section 3, 
some challenges; section 4, the implementation; 
section 5, evaluation; section 6, empirical results 
on extraction type; section 7, the effect of periodic, 
limited human feedback; section 8, related work; 
and section 9, lessons learned and conclusions. 
2 Approach 
Our approach for learning patterns that can be used 
to detect relations is depicted in Figure 1. Initially, 
a few instances of the relation tuples are provided, 
along with a massive corpus, e.g., the web or the 
gigaword corpus from the Linguistic Data Consor-
tium (LDC). The diagram shows three inventor-
invention pairs, beginning with Thomas Edi-
son?light bulb. From these, we find candidate 
sentences in the massive corpus, e.g., Thomas Edi-
son invented the light bulb. Features extracted from 
the sentences retrieved, for example features of the 
text-graph (the predicate-argument structure con-
necting the two arguments), provide a training in-
stance for pattern induction. The induced patterns 
are added to the collection (database) of patterns.
Running the extended pattern collection over the 
corpus finds new, previously unseen relation 
tuples. From these new tuples, additional sentences 
which express those tuples can be retrieved, and 
the cycle of learning can continue. 
There is an analogous cycle of learning con-
cepts from instances and the large corpus; the ex-
periments in this paper do not report on that paral-
lel learning cycle. 
Figure 1: Approach to Learning Relations 
At the ith iteration, the steps are 
1. Given the set of hypothesized instances of the 
relation (triples HTi), find instances of such 
triples in the corpus. (On the first iteration, 
?hypothesized? triples are manually-generated 
seed examples.) 
2. Induce possible patterns. For each proposed 
pattern P: 
a. Apply pattern P to the corpus to generate a 
set of triples TP
b. Estimate precision as the confidence-
weighted average of the scores of the 
triples in TP. Reduce precision score by the 
percentage of triples in TP that violate us-
er-specified relation constraints (e.g. arity 
constraints described in 4.3)  
c. Estimate recall as the confidence-weighted 
percentage of triples in HTi found by the 
pattern 
3. Identify a set of high-confidence patterns HPi
using cutoffs automatically derived from rank-
based curves for precision, recall, and F-
measure (?=0.7) 
4. Apply high-confidence patterns to a Web-scale 
corpus to hypothesize new triples. For each 
proposed triple T 
a. Estimate score(T) as the expected proba-
bility that T is correct, calculated by com-
bining the respective precision and recall 
scores of all of the patterns that did or did 
not return it (using the Na?ve Bayes as-
sumption that all patterns are independent) 
b. Estimate confidence(T) as the percentage 
of patterns in HPi by which T was found 
5. Identify a set of high-confidence triples HTi+1

	














	


	 







	



	

	
	
 

	 
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1?27,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
CoNLL-2011 Shared Task:
Modeling Unrestricted Coreference in OntoNotes
Sameer Pradhan
BBN Technologies,
Cambridge, MA 02138
pradhan@bbn.com
Lance Ramshaw
BBN Technologies,
Cambridge, MA 02138
lramshaw@bbn.com
Mitchell Marcus
University of Pennsylvania,
Philadelphia, 19104
mitch@linc.cis.upenn.edu
Martha Palmer
University of Colorado,
Boulder, CO 80309
martha.palmer@colorado.edu
Ralph Weischedel
BBN Technologies,
Cambridge, MA 02138
weischedel@bbn.com
Nianwen Xue
Brandeis University,
Waltham, MA 02453
xuen@cs.brandeis.edu
Abstract
The CoNLL-2011 shared task involved pre-
dicting coreference using OntoNotes data. Re-
sources in this field have tended to be lim-
ited to noun phrase coreference, often on a
restricted set of entities, such as ACE enti-
ties. OntoNotes provides a large-scale corpus
of general anaphoric coreference not restricted
to noun phrases or to a specified set of en-
tity types. OntoNotes also provides additional
layers of integrated annotation, capturing ad-
ditional shallow semantic structure. This pa-
per briefly describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task
including the format, pre-processing informa-
tion, and evaluation criteria, and presents and
discusses the results achieved by the partic-
ipating systems. Having a standard test set
and evaluation parameters, all based on a new
resource that provides multiple integrated an-
notation layers (parses, semantic roles, word
senses, named entities and coreference) that
could support joint models, should help to en-
ergize ongoing research in the task of entity
and event coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Automatic identification of coreferring entities and
events in text has been an uphill battle for several
decades, partly because it can require world knowl-
edge which is not well-defined and partly owing to
the lack of substantial annotated data. Early work
on corpus-based coreference resolution dates back
to the mid-90s by McCarthy and Lenhert (1995)
where they experimented with using decision trees
and hand-written rules. A systematic study was
then conducted using decision trees by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have been developed
to push the state of the art in coreference resolu-
tion forward (Morton, 2000; Harabagiu et al, 2001;
McCallum and Wellner, 2004; Culotta et al, 2007;
Denis and Baldridge, 2007; Rahman and Ng, 2009;
Haghighi and Klein, 2010). Various different knowl-
edge sources from shallow semantics to encyclo-
pedic knowledge are being exploited (Ponzetto and
Strube, 2005; Ponzetto and Strube, 2006; Versley,
2007; Ng, 2007). Researchers continued finding
novel ways of exploiting ontologies such as Word-
Net. Given that WordNet is a static ontology and
as such has limitation on coverage, more recently,
there have been successful attempts to utilize in-
formation from much larger, collaboratively built
resources such as Wikipedia (Ponzetto and Strube,
2006). In spite of all the progress, current techniques
still rely primarily on surface level features such as
string match, proximity, and edit distance; syntac-
tic features such as apposition; and shallow seman-
tic features such as number, gender, named entities,
semantic class, Hobbs? distance, etc. A better idea
of the progress in the field can be obtained by read-
ing recent survey articles (Ng, 2010) and tutorials
(Ponzetto and Poesio, 2009) dedicated to this sub-
ject.
Corpora to support supervised learning of this
task date back to the Message Understanding Con-
ferences (MUC). These corpora were tagged with
coreferring entities identified by noun phrases in the
text. The de facto standard datasets for current coref-
erence studies are the MUC (Hirschman and Chin-
1
chor, 1997; Chinchor, 2001; Chinchor and Sund-
heim, 2003) and the ACE1 (G. Doddington et al,
2000) corpora. The MUC corpora cover all noun
phrases in text, but represent small training and test
sets. The ACE corpora, on the other hand, have much
more annotation, but are restricted to a small subset
of entities. They are also less consistent, in terms of
inter-annotator agreement (ITA) (Hirschman et al,
1998). This lessens the reliability of statistical ev-
idence in the form of lexical coverage and seman-
tic relatedness that could be derived from the data
and used by a classifier to generate better predic-
tive models. The importance of a well-defined tag-
ging scheme and consistent ITA has been well rec-
ognized and studied in the past (Poesio, 2004; Poe-
sio and Artstein, 2005; Passonneau, 2004). There
is a growing consensus that in order for these to be
most useful for language understanding applications
such as question answering or distillation ? both of
which seek to take information access technology
to the next level ? we need more consistent anno-
tation of larger amounts of broad coverage data for
training better automatic techniques for entity and
event identification. Identification and encoding of
richer knowledge ? possibly linked to knowledge
sources ? and development of learning algorithms
that would effectively incorporate them is a neces-
sary next step towards improving the current state
of the art. The computational learning community,
in general, is also witnessing a move towards eval-
uations based on joint inference, with the two pre-
vious CoNLL tasks (Surdeanu et al, 2008; Hajic? et
al., 2009) devoted to joint learning of syntactic and
semantic dependencies. A principle ingredient for
joint learning is the presence of multiple layers of
semantic information.
One fundamental question still remains, and that
is ? what would it take to improve the state of the art
in coreference resolution that has not been attempted
so far? Many different algorithms have been tried in
the past 15 years, but one thing that is still lacking
is a corpus comprehensively tagged on a large scale
with consistent, multiple layers of semantic infor-
mation. One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
is to explore whether it can fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
1http://projects.ldc.upenn.edu/ace/data/
2http://www.bbn.com/nlp/ontonotes
ers entities and events not limited to noun phrases
or a limited set of entity types. A small portion of
this corpus from the newswire and broadcast news
genres (?120k) was recently used for a SEMEVAL
task (Recasens et al, 2010). As mentioned earlier,
the coreference layer in OntoNotes constitutes just
one part of a multi-layered, integrated annotation of
shallow semantic structure in text with high inter-
annotator agreement, which also provides a unique
opportunity for performing joint inference over a
substantial body of data.
The remainder of this paper is organized as
follows. Section 2 presents an overview of the
OntoNotes corpus. Section 3 describes the coref-
erence annotation in OntoNotes. Section 4 then de-
scribes the shared task, including the data provided
and the evaluation criteria. Sections 5 and 6 then de-
scribe the participating system results and analyze
the approaches, and Section 7 concludes.
2 The OntoNotes Corpus
The OntoNotes project has created a corpus of large-
scale, accurate, and integrated annotation of multi-
ple levels of the shallow semantic structure in text.
The idea is that this rich, integrated annotation cov-
ering many layers will allow for richer, cross-layer
models enabling significantly better automatic se-
mantic analysis. In addition to coreference, this
data is also tagged with syntactic trees, high cov-
erage verb and some noun propositions, partial verb
and noun word senses, and 18 named entity types.
However, such multi-layer annotations, with com-
plex, cross-layer dependencies, demands a robust,
efficient, scalable mechanism for storing them while
providing efficient, convenient, integrated access to
the the underlying structure. To this effect, it uses a
relational database representation that captures both
the inter- and intra-layer dependencies and also pro-
vides an object-oriented API for efficient, multi-
tiered access to this data (Pradhan et al, 2007a).
This should facilitate the creation of cross-layer fea-
tures in integrated predictive models that will make
use of these annotations.
Although OntoNotes is a multi-lingual resource
with all layers of annotation covering three lan-
guages: English, Chinese and Arabic, for the scope
of this paper, we will just look at the English por-
tion. Over the years of the development of this cor-
pus, there were various priorities that came into play,
and therefore not all the data in the English portion is
annotated with all the different layers of annotation.
There is a core portion, however, which is roughly
2
1.3M words which has been annotated with all the
layers. It comprises ?450k words from newswire,
?150k from magazine articles, ?200k from broad-
cast news, ?200k from broadcast conversations and
?200k web data.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A syntactic layer representing a re-
vised Penn Treebank (Marcus et al, 1993;
Babko-Malaya et al, 2006).
? Propositions ? The proposition structure of
verbs in the form of a revised PropBank(Palmer
et al, 2005; Babko-Malaya et al, 2006).
? Word Sense ? Coarse grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize cov-
erage. The word sense granularity is tailored
to achieve 90% inter-annotator agreement as
demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files
and each individual sense has been connected
to multiple WordNet senses. This provides a
direct access to the WordNet semantic struc-
ture for users to make use of. There is also a
mapping from the word senses to the PropBank
frames and to VerbNet (Kipper et al, 2000) and
FrameNet (Fillmore et al, 2003).
? Named Entities ? The corpus was tagged with
a set of 18 proper named entity types that
were well-defined and well-tested for inter-
annotator agreement by Weischedel and Burn-
stein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a limited
set of entity types (Pradhan et al, 2007b). We
will take a look at this in detail in the next sec-
tion.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich set
of entities and events ? not restricted to a few types,
as has been characteristic of most coreference data
available until now ? has been tagged with a high
degree of consistency. Attributive coreference is
tagged separately from the more common identity
coreference.
Two different types of coreference are distin-
guished in the OntoNotes data: Identical (IDENT),
and Appositive (APPOS). Appositives are treated
separately because they function as attributions, as
described further below. The IDENT type is used
for anaphoric coreference, meaning links between
pronominal, nominal, and named mentions of spe-
cific referents. It does not include mentions of
generic, underspecified, or abstract entities.
Coreference is annotated for all specific entities
and events. There is no limit on the semantic types
of NP entities that can be considered for coreference,
and in particular, coreference is not limited to ACE
types.
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by auto-
matically extracting all of the NP mentions from the
Penn Treebank, though the annotators can also add
additional mentions when appropriate. In the fol-
lowing two examples (and later ones), the phrases
notated in bold form the links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
3.1 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with an-
other verb. The intent is to annotate the VP, but we
mark the single-word head for convenience. This in-
cludes morphologically related nominalizations (3)
and noun phrases that refer to the same event, even
if they are lexically distinct from the verb (4). In the
following two examples, only the chains related to
the growth event are shown.
(3) Sales of passenger cars grew 22%. The strong
growth followed year-to-year increases.
(4) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3
3.2 Pronouns
All pronouns and demonstratives are linked to any-
thing that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged as coreferent.)
(5) Senate majority leader Bill Frist likes to tell a
story from his days as a pioneering heart sur-
geon back in Tennessee. A lot of times, Frist re-
calls, *you?d have a critical patient lying there
waiting for a new heart, and *you?d want to
cut, but *you couldn?t start unless *you knew
that the replacement heart would make *it to
the operating room.
3.3 Generic mentions
Generic nominal mentions can be linked with refer-
ring pronouns and other definite mentions, but are
not linked to other generic nominal mentions. This
would allow linking of the bracketed mentions in (6)
and (7), but not (8).
(6) Officials said they are tired of making the same
statements.
(7) Meetings are most productive when they are
held in the morning. Those meetings, however,
generally have the worst attendance.
(8) Allergan Inc. said it received approval to
sell the PhacoFlex intraocular lens, the first
foldable silicone lens available for *cataract
surgery. The lens? foldability enables it to be
inserted in smaller incisions than are now pos-
sible for *cataract surgery.
Bare plurals, as in (6) and (7), are always consid-
ered generic. In example (9) below, there are two
generic instances of parents. These are marked as
distinct IDENT chains (with separate chains distin-
guished by subscripts X, Y and Z), each containing
a generic and the related referring pronouns.
(9) ParentsX should be involved with theirX chil-
dren?s education at home, not in school. TheyX
should see to it that theirX kids don?t play tru-
ant; theyX should make certain that the children
spend enough time doing homework; theyX
should scrutinize the report card. ParentsY are
too likely to blame schools for the educational
limitations of theirY children. If parentsZ are
dissatisfied with a school, theyZ should have
the option of switching to another.
In (10) below, the verb ?halve? cannot be linked
to ?a reduction of 50%?, since ?a reduction? is in-
definite.
(10) Argentina said it will ask creditor banks to
*halve its foreign debt of $64 billion ? the
third-highest in the developing world . Ar-
gentina aspires to reach *a reduction of 50%
in the value of its external debt.
3.4 Pre-modifiers
Proper pre-modifiers can be coreferenced, but
proper nouns that are in a morphologically adjecti-
val form are treated as adjectives, and not corefer-
enced. For example, adjectival forms of GPEs such
as Chinese in ?the Chinese leader?, would not be
linked. Thus we could coreference United States in
?the United States policy? with another referent, but
not American ?the American policy.? GPEs and Na-
tionality acronyms (e.g. U.S.S.R. or U.S.). are also
considered adjectival. Pre-modifier acronyms can be
coreferenced unless they refer to a nationality. Thus
in the examples below, FBI can be coreferenced to
other mentions, but U.S. cannot.
(11) FBI spokesman
(12) *U.S. spokesman
Dates and monetary amounts can be considered
part of a coreference chain even when they occur as
pre-modifiers.
(13) The current account deficit on France?s balance
of payments narrowed to 1.48 billion French
francs ($236.8 million) in August from a re-
vised 2.1 billion francs in July, the Finance
Ministry said. Previously, the July figure was
estimated at a deficit of 613 million francs.
(14) The company?s $150 offer was unexpected.
The firm balked at the price.
3.5 Copular verbs
Attributes signaled by copular structures are not
marked; these are attributes of the referent they mod-
ify, and their relationship to that referent will be
captured through word sense and propositional ar-
gument tagging.
4
(15) JohnX is a linguist. PeopleY are nervous
around JohnX, because heX always corrects
theirY grammar.
Copular (or ?linking?) verbs are those verbs that
function as a copula and are followed by a sub-
ject complement. Some common copular verbs are:
be, appear, feel, look, seem, remain, stay, become,
end up, get. Subject complements following such
verbs are considered attributes, and not linked. Since
Called is copular, neither IDENT nor APPOS corefer-
ence is marked in the following case.
(16) Called Otto?s Original Oat Bran Beer, the brew
costs about $12.75 a case.
3.6 Small clauses
Like copulas, small clause constructions are not
marked. The following example is treated as if the
copula were present (?John considers Fred to be an
idiot?):
(17) John considers *Fred *an idiot.
3.7 Temporal expressions
Temporal expressions such as the following are
linked:
(18) John spent three years in jail. In that time...
Deictic expressions such as now, then, today, to-
morrow, yesterday, etc. can be linked, as well as
other temporal expressions that are relative to the
time of the writing of the article, and which may
therefore require knowledge of the time of the writ-
ing to resolve the coreference. Annotators were al-
lowed to use knowledge from outside the text in re-
solving these cases. In the following example, the
end of this period and that time can be coreferenced,
as can this period and from three years to seven
years.
(19) The limit could range from three years to
seven yearsX, depending on the composition
of the management team and the nature of its
strategic plan. At (the end of (this period)X)Y,
the poison pill would be eliminated automati-
cally, unless a new poison pill were approved
by the then-current shareholders, who would
have an opportunity to evaluate the corpora-
tion?s strategy and management team at that
timeY.
In multi-date temporal expressions, embedded
dates are not separately connected to to other men-
tions of that date. For example in Nov. 2, 1999, Nov.
would not be linked to another instance of November
later in the text.
3.8 Appositives
Because they logically represent attributions, appos-
itives are tagged separately from Identity corefer-
ence. They consist of a head, or referent (a noun
phrase that points to a specific object/concept in the
world), and one or more attributes of that referent.
An appositive construction contains a noun phrase
that modifies an immediately-adjacent noun phrase
(separated only by a comma, colon, dash, or paren-
thesis). It often serves to rename or further define
the first mention. Marking appositive constructions
allows us to capture the attributed property even
though there is no explicit copula.
(20) Johnhead, a linguistattribute
The head of each appositive construction is distin-
guished from the attribute according to the following
heuristic specificity scale, in a decreasing order from
top to bottom:
Type Example
Proper noun John
Pronoun He
Definite NP the man
Indefinite specific NP a man I know
Non-specific NP man
This leads to the following cases:
(21) Johnhead, a linguistattribute
(22) A famous linguistattribute, hehead studied at ...
(23) a principal of the firmattribute, J. Smithhead
In cases where the two members of the appositive
are equivalent in specificity, the left-most member of
the appositive is marked as the head/referent. Defi-
nite NPs include NPs with a definite marker (the) as
well as NPs with a possessive adjective (his). Thus
the first element is the head in all of the following
cases:
(24) The chairman, the man who never gives up
(25) The sheriff, his friend
(26) His friend, the sheriff
In the specificity scale, specific names of diseases
and technologies are classified as proper names,
whether they are capitalized or not.
(27) A dangerous bacteria, bacillium, is found
5
Type Description
Annotator Error An annotator error. This is a catch-all category for cases of errors that do not fit in the other
categories.
Genuine Ambiguity This is just genuinely ambiguous. Often the case with pronouns that have no clear an-
tecedent (especially this & that)
Generics One person thought this was a generic mention, and the other person didn?t
Guidelines The guidelines need to be clear about this example
Callisto Layout Something to do with the usage/design of Callisto
Referents Each annotator thought this was referring to two completely different things
Possessives One person did not mark this possessive
Verb One person did not mark this verb
Pre Modifiers One person did not mark this Pre Modifier
Appositive One person did not mark this appositive
Extent Both people marked the same entity, but one person?s mention was longer
Copula Disagreement arose because this mention is part of a copular structure
a) Either each annotator marked a different half of the copula
b) Or one annotator unnecessarily marked both
Figure 1: Description of various disagreement types
Figure 1: The distribution of disagreements across the various types in Table 2
Sheet1
Page 1
Copulae 2%Appositives 3%Pre Modifiers 3%Verbs 3%Possessives 4%Refer nts 7%Callisto Layout 8%Guidelines 8%Generics 11%Genuine Ambiguity 25%Annotator Error 26%
Copulae
Appositives
Pre Modifiers
Verbs
Possessives
Referents
Callisto Layout
Guidelines
Generics
Genuine Ambiguity
Annotator Error
0% 5% 10% 15% 20% 25% 30%
Figure 2: The distribution of disagreements across the various types in Table 1
When the entity to which an appositive refers is
also mentioned elsewhere, only the single span con-
taining the entire appositive construction is included
in the larger IDENT chain. None of the nested NP
spans are linked. In the example below, the en-
tire span can be linked to later mentions to Richard
Godown. The sub-spans are not included separately
in the IDENT chain.
(28) Richard Godown, president of the Indus-
trial Biotechnology Association
Ages are tagged as attributes (as if they were el-
lipses of, for example, a 42-year-old):
(29) Mr.Smithhead, 42attribute,
3.9 Special Issues
In addition to the ones above, there are some special
cases such as:
? No coreference is marked between an organi-
zation and its members.
Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJ
Newswire 80.9 85.2 88.3
Broadcast News 78.6 83.5 89.4
Broadcast Conversation 86.7 91.6 93.7
Magazine 78.4 83.2 88.8
Web 85.9 92.2 91.2
Table 1: Inter Annotator and Adjudicator agreement for
the Coreference Layer in OntoNotes measured in terms
of the MUC score.
? GPEs are linked to references to their govern-
ments, even when the references are nested
NPs, or the modifier and head of a single NP.
3.10 Annotator Agreement and Analysis
Table 1 shows the inter-annotator and annotator-
adjudicator agreement on all the genres of
OntoNotes. We also analyzed about 15K dis-
agreements in various parts of the data, and grouped
them into one of the categories shown in Figure 1.
Figure 2 shows the distribution of these different
types that were found in that sample. It can be
6
seen that genuine ambiguity and annotator error
are the biggest contributors ? the latter of which is
usually captured during adjudication, thus showing
the increased agreement between the adjudicated
version and the individual annotator version.
4 CoNLL-2011 Coreference Task
This section describes the CoNLL-2011 Corefer-
ence task, including its closed and open track ver-
sions, and characterizes the data used for the task
and how it was prepared.
4.1 Why a Coreference Task?
Despite close to a two-decade history of evaluations
on coreference tasks, variation in the evaluation cri-
teria and in the training data used have made it dif-
ficult for researchers to be clear about the state of
the art or to determine which particular areas require
further attention. There are many different parame-
ters involved in defining a coreference task. Looking
at various numbers reported in literature can greatly
affect the perceived difficulty of the task. It can seem
to be a very hard problem (Soon et al, 2001) or one
that is somewhat easier (Culotta et al, 2007). Given
the space constraints, we refer the reader to Stoy-
anov et al (2009) for a detailed treatment of the
issue.
Limitations in the size and scope of the available
datasets have also constrained research progress.
The MUC and ACE corpora are the two that have
been used most for reporting comparative results,
but they differ in the types of entities and corefer-
ence annotated. The ACE corpus is also one that
evolved over a period of almost five years, with dif-
ferent incarnations of the task definition and dif-
ferent corpus cross-sections on which performance
numbers have been reported, making it hard to un-
tangle and interpret the results.
The availability of the OntoNotes data offered an
opportunity to define a coreference task based on a
larger, more broad-coverage corpus. We have tried
to design the task so that it not only can support the
current evaluation, but also can provide an ongoing
resource for comparing different coreference algo-
rithms and approaches.
4.2 Task Description
The CoNLL-2011 shared task was based on the En-
glish portion of the OntoNotes 4.0 data. The task
was to automatically identify mentions of entities
and events in text and to link the coreferring men-
tions together to form entity/event chains. The target
coreference decisions could be made using automat-
ically predicted information on the other structural
layers including the parses, semantic roles, word
senses, and named entities.
As is customary for CoNLL tasks, there were two
tracks, closed and open. For the closed track, sys-
tems were limited to using the distributed resources,
in order to allow a fair comparison of algorithm per-
formance, while the open track allowed for almost
unrestricted use of external resources in addition to
the provided data.
4.2.1 Closed Track
In the closed track, systems were limited to the pro-
vided data, plus the use of two pre-specified external
resources: i) WordNet and ii) a pre-computed num-
ber and gender table by Bergsma and Lin (2006).
For the training and test data, in addition to the
underlying text, predicted versions of all the supple-
mentary layers of annotation were provided, where
those predictions were derived using off-the-shelf
tools (parsers, semantic role labelers, named entity
taggers, etc.) as described in Section 4.4.2. For the
training data, however, in addition to predicted val-
ues for the other layers, we also provided manual
gold-standard annotations for all the layers. Partici-
pants were allowed to use either the gold-standard or
predicted annotation for training their systems. They
were also free to use the gold-standard data to train
their own models for the various layers of annota-
tion, if they judged that those would either provide
more accurate predictions or alternative predictions
for use as multiple views, or wished to use a lattice
of predictions.
More so than previous CoNLL tasks, corefer-
ence predictions depend on world knowledge, and
many state-of-the-art systems use information from
external resources such as WordNet, which can
add a layer that helps the system to recognize se-
mantic connections between the various lexical-
ized mentions in the text. Therefore, the use of
WordNet was allowed, even for the closed track.
Since word senses in OntoNotes are predominantly3
coarse-grained groupings of WordNet senses, sys-
tems could also map from the predicted or gold-
standard word senses provided to the sets of under-
lying WordNet senses. Another significant piece of
knowledge that is particularly useful for coreference
but that is not available in the layers of OntoNotes is
that of number and gender. There are many different
3There are a few instances of novel senses introduced in
OntoNotes which were not present in WordNet, and so lack a
mapping back to the WordNet senses
7
ways of predicting these values, with differing accu-
racies, so in order to ensure that participants in the
closed track were working from the same data, thus
allowing clearer algorithmic comparisons, we spec-
ified a particular table of number and gender predic-
tions generated by Bergsma and Lin (2006), for use
during both training and testing.
Following the recent CoNLL tradition, partici-
pants were allowed to use both the training and the
development data for training the final model.
4.2.2 Open Track
In addition to resources available in the closed track,
the open track, systems were allowed to use external
resources such as Wikipedia, gazetteers etc. This
track is mainly to get an idea of a performance ceil-
ing on the task at the cost of not getting a compar-
ison across all systems. Another advantage of the
open track is that it might reduce the barriers to par-
ticipation by allowing participants to field existing
research systems that already depend on external re-
sources ? especially if there were hard dependen-
cies on these resources. They can participate in the
task with minimal or no modification to their exist-
ing system.
4.3 Coreference Task Data
Since there are no previously reported numbers on
the full version of OntoNotes, we had to create
a train/development/test partition. The only por-
tion of OntoNotes that has a previously determined,
widely used, standard split is the WSJ portion of the
newswire data. For that subcorpus, we maintained
the same partition. For all the other portions we cre-
ated stratified training, development and test parti-
tions over all the sources in OntoNotes using the pro-
cedure shown in Algorithm 1. The list of training,
development and test document IDs can be found on
the task webpage.4
4.4 Data Preparation
This section gives details of the different annota-
tion layers including the automatic models that were
used to predict them, and describes the formats in
which the data were provided to the participants.
4.4.1 Manual Annotation Gold Layers
We will take a look at the manually annotated, or
gold layers of information that were made available
for the training data.
4http://conll.bbn.com/download/conll-train.id
http://conll.bbn.com/download/conll-dev.id
http://conll.bbn.com/download/conll-test.id
Algorithm 1 Procedure used to create OntoNotes
training, development and test partitions.
Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,
DEV, TEST
1: TRAIN? ?
2: DEV? ?
3: TEST? ?
4: for all SOURCE ? ONTONOTES do
5: if SOURCE = WALL STREET JOURNAL then
6: TRAIN? TRAIN ? SECTIONS 02 ? 21
7: DEV? DEV ? SECTIONS 00, 01, 22, 24
8: TEST? TEST ? SECTION 23
9: else
10: if Number of files in SOURCE ? 10 then
11: TRAIN? TRAIN ? FILE IDS ending in 1 ? 8
12: DEV? DEV ? FILE IDS ending in 0
13: TEST? TEST ? FILE IDS ending in 9
14: else
15: DEV? DEV ? FILE IDS ending in 0
16: TEST? TEST ? FILE ID ending in the highest number
17: TRAIN? TRAIN ? Remaining FILE IDS for the
SOURCE
18: end if
19: end if
20: end for
21: return TRAIN, DEV, TEST
Coreference The manual coreference annotation
is stored as chains of linked mentions connecting
multiple mentions of the same entity. Coreference is
the only document-level phenomenon in OntoNotes,
and the complexity of annotation increases non-
linearly with the length of a document. Unfortu-
nately, some of the documents ? especially ones in
the broadcast conversation, weblogs, and telephone
conversation genre ? are very long which prohib-
ited us from efficiently annotating them in entirety.
These had to be split into smaller parts. We con-
ducted a few passes to join some adjacent parts, but
since some documents had as many as 17 parts, there
are still multi-part documents in the corpus. Since
the coreference chains are coherent only within each
of these document parts, for this task, each such part
is treated as a separate document. Another thing
to note is that there were some cases of sub-token
annotation in the corpus owing to the fact that to-
kens were not split at hyphens. Cases such as pro-
WalMart had the sub-span WalMart linked with another
instance of the same. The recent Treebank revision
which split tokens at most hyphens, made a majority
of these sub-token annotations go away. There were
still some residual sub-token annotations. Since
subtoken annotations cannot be represented in the
CoNLL format, and they were a very small quantity
? much less than even half a percent ? we decided to
ignore them.
For various reasons, not all the documents in
OntoNotes have been annotated with all the differ-
8
Corpora Words Documents
Total Train Dev Test Total Train Dev Test
MUC-6 25K 12K 13K 60 30 30
MUC-7 40K 19K 21K 67 30 37
ACE (2000-2004) 1M 775K 235K - - -
OntoNotes5 1.3M 1M 136K 142K 2,083(2,999) 1,674(2,374) 202(303) 207(322)
Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The
numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.
Each part was considered a separate document for evaluation purposes.
Syntactic category Train Development Test
Count % Count % Count %
NP 60,345 59.71 8,463 59.31 8,629 53.09
PRP 25,472 25.21 3,535 24.78 5,012 30.84
PRP$ 8,889 8.80 1,208 8.47 1,466 9.02
NNP 2,643 2.62 468 3.28 475 2.92
NML 900 0.89 151 1.06 118 0.73
Vx 1,915 1.89 317 2.22 314 1.93
Other 893 0.88 126 0.88 239 1.47
Overall 101,057 100.00 14,268 100.00 16,253 100.00
Table 3: Distribution of mentions in the data by their syn-
tactic category.
Train Development Test
Entities/Chains 26,612 3,752 3,926
Links 74,652 10,539 12,365
Mentions 101,264 14,291 16,291
Table 4: Number of entities, links and mentions in the
OntoNotes 4.0 data.
ent layers of annotation, with full coverage.6 There
is a core portion, however, which is roughly 1.3M
words which has been annotated with all the layers.
This is the portion that we used for the shared task.
The number of documents in the corpus for this
task, for each of the different genres, are shown in
Table 2. Tables 3 and 4 shows the distribution of
mentions by the syntactic categories, and the counts
of entities, links and mentions in the corpus respec-
tively. All of this data has been Treebanked and
PropBanked either as part of the OntoNotes effort
or some preceding effort.
For comparison purposes, Table 2 also lists the
number of documents in the MUC-6, MUC-7, and
ACE (2000-2004) corpora. The MUC-6 data was
taken from the Wall Street Journal, whereas the
MUC-7 data was from the New York Times. The
ACE data spanned many different genres similar to
6Given the nature of word sense annotation, and changes in
project priorities, we could not annotate all the low frequency
verbs and nouns in the corpus. Furthermore, PropBank annota-
tion currently only covers verb predicates.
the ones in OntoNotes.
Parse Trees This represents the syntactic layer
that is a revised version of the Penn Treebank. For
purposes of this task, traces were removed from the
syntactic trees, since the CoNLL-style data format,
being indexed by tokens, does not provide any good
means of conveying that information. Function tags
were also removed, since the parsers that we used
for the predicted syntax layer did not provide them.
One thing that needs to be dealt with in conversa-
tional data is the presence of disfluencies (restarts,
etc.). In the original OntoNotes parses, these are
marked using a special EDITED7 phrase tag ? as was
the case for the Switchboard Treebank. Given the
frequency of disfluencies and the performance with
which one can identify them automatically,8 a prob-
able processing pipeline would filter them out be-
fore parsing. Since we did not have a readily avail-
able tagger for tagging disfluencies, we decided to
remove them using oracle information available in
the Treebank.
Propositions The propositions in OntoNotes con-
stitute PropBank semantic roles. Most of the verb
predicates in the corpus have been annotated with
their arguments. Recent enhancements to the Prop-
Bank to make it synchronize better with the Tree-
bank (Babko-Malaya et al, 2006) have enhanced
the information in the proposition by the addition of
two types of LINKs that represent pragmatic corefer-
ence (LINK-PCR) and selectional preferences (LINK-
SLC). More details can be found in the addendum to
the PropBank guidelines9 in the OntoNotes 4.0 re-
7There is another phrase type ? EMBED in the telephone con-
versation genre which is similar to the EDITED phrase type, and
sometimes identifies insertions, but sometimes contains logical
continuation of phrases, so we decided not to remove that from
the data.
8A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 Precision
and 67 recall.
9doc/propbank/english-propbank.pdf
9
lease. Since the community is not used to this rep-
resentation which relies heavily on the trace struc-
ture in the Treebank which we are excluding, we de-
cided to unfold the LINKs back to their original rep-
resentation as in the Release 1.0 of the Proposition
Bank. This functionality is part of the OntoNotes
DB Tool.10
Word Sense Gold word sense annotation was
supplied using sense numbers as specified in
the OntoNotes list of senses for each lemma.11
The sense inventories that were provided in the
OntoNotes 4.0 release were not all mapped to the lat-
est version 3.0 of WordNet, so we provided a revised
version of the sense inventories, containing mapping
to WordNet 3.0, on the task page for the participants.
Named Entities Named Entities in OntoNotes
data are specified using a catalog of 18 Name types.
Other Layers Discourse plays a vital role in
coreference resolution. In the case of broadcast con-
versation, or telephone conversation data, it partially
manifests in the form of speakers of a given utter-
ance, whereas in weblogs or newsgroups it does so
as the writer, or commenter of a particular article
or thread. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be au-
tomatically deduced, but since it would add addi-
tional complexity to the already complex task, we
decided to provide oracle information of this meta-
data both during training and testing. In other words,
speaker and author identification was not treated
as an annotation layer that needed to be predicted.
This information was provided in the form of an-
other column in the .conll table. There were some
cases of interruptions and interjections that ideally
would associate parts of a sentence to two different
speakers, but since the frequency of this was quite
small, we decided to make an assumption of one
speaker/writer per sentence.
4.4.2 Predicted Annotation Layers
The predicted annotation layers were derived using
automatic models trained using cross-validation on
other portions of OntoNotes data. As mentioned ear-
lier, there are some portions of the OntoNotes corpus
that have not been annotated for coreference but that
have been annotated for other layers. For training
10http://cemantix.org/ontonotes.html
11It should be noted that word sense annotation in OntoNotes
is note complete, so only some of the verbs and nouns have
word sense tags specified.
Senses Lemmas
1 1,506
2 1,046
> 2 1,016
Table 6: Word sense polysemy over verb and noun lem-
mas in OntoNotes
models for each of the layers, where feasible, we
used all the data that we could for that layer from
the training portion of the entire OntoNotes release.
Parse Trees Predicted parse trees were produced
using the Charniak parser (Charniak and Johnson,
2005).12 Some additional tag types used in the
OntoNotes trees were added to the parser?s tagset,
including the NML tag that has recently been added
to capture internal NP structure, and the rules used to
determine head words were appropriately extended.
The parser was then re-trained on the training por-
tion of the release 4.0 data using 10-fold cross-
validation. Table 5 shows the performance of the
re-trained Charniak parser on the CoNLL-2011 test
set. We did not get a chance to re-train the re-ranker,
and since the stock re-ranker crashes when run on n-
best parses containing NMLs, because it has not seen
that tag in training, we could not make use of it.
Word Sense We trained a word sense tagger us-
ing a SVM classifier and contextual word and part
of speech features on all the training portion of the
OntoNotes data. The OntoNotes 4.0 corpus com-
prises a total of 14,662 sense definitions across 4877
verb and noun lemmas13. The distribution of senses
per lemma is as shown in Table 6. Table 7 shows
the performance of this classifier over both the verbs
and nouns in the CoNLL-2011 test set. Again this
performance is not directly comparable to any re-
ported in the literature before, and it seems lower
then performances reported on previous versions
of OntoNotes because this is over all the genres
of OntoNotes, and aggregated over both verbs and
nouns in the CoNLL-2011 test set.
Propositions To predict propositional structure,
ASSERT14 (Pradhan et al, 2005) was used, re-
trained also on all the training portion of the release
12http://bllip.cs.brown.edu/download/reranking-
parserAug06.tar.gz
13The number of lemmas in Table 6 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
7933.
14http://cemantix.org/assert.html
10
All Sentences Sentence len < 40
N POS R P F N R P F
Broadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90
Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98
Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87
Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11
Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43
Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11
Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94
Table 5: Parser performance on the CoNLL-2011 test set
Frameset Total Total % Perfect Argument ID + Class
Accuracy Sentences Propositions Propositions P R F
Broadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63
Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04
Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64
Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49
Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37
Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09
Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.
Accuracy
Broadcast Conversation (BC) 0.70
Broadcast News (BN) 0.68
Magazine (MZ) 0.60
Newswire (NW) 0.62
Weblogs and Newsgroups (WB) 0.63
Overall 0.65
Table 7: Word sense performance over both verbs and
nouns in the CoNLL-2011 test set
4.0 data. Given time constraints, we had to per-
form two modifications: i) Instead of a single model
that predicts all arguments including NULL argu-
ments, we had to use the two-stage mode where the
NULL arguments are first filtered out and the remain-
ing NON-NULL arguments are classified into one of
the argument types, and ii) The argument identifi-
cation module used an ensemble of ten classifiers
? each trained on a tenth of the training data and
performed an unweighted voting among them. This
should still give a close to state of the art perfor-
mance given that the argument identification perfor-
mance tends to start to be asymptotic around 10k
training instances. At first glance, the performance
on the newswire genre is much lower than what has
been reported for WSJ Section 23. This could be
attributed to two factors: i) the fact that we had to
compromise on the training method, but more im-
portantly because ii) the newswire in OntoNotes not
only contains WSJ data, but also Xinhua news. One
could try to verify using just the WSJ portion of the
data, but it would be hard as it is not only a sub-
set of the documents that the performance has been
reported on previously, but also the annotation has
been significantly revised; it includes propositions
for be verbs missing from the original PropBank,
and the training data is a subset of the original data
as well. Table 8 shows the detailed performance
numbers.
In addition to automatically predicting the argu-
ments, we also trained a classifier to tag PropBank
frameset IDs in the data using the same word sense
module as mentioned earlier. OntoNotes 4.0 con-
tains a total of 7337 framesets across 5433 verb
lemmas.15 An overwhelming number of them are
monosemous, but the more frequent verbs tend to be
polysemous. Table 9 gives the distribution of num-
ber of framesets per lemma in the PropBank layer of
the OntoNotes 4.0 data.
During automatic processing of the data, we
tagged all the tokens that were tagged with a part
of speech VBx. This means that there would be cases
where the wrong token would be tagged with propo-
sitions. The CoNLL-2005 scorer was used to gener-
ate the scores.
Named Entities BBN?s IdentiFinderTMsystem
was used to predict the named entities. Given the
15The number of lemmas in Table 9 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
4229.
11
Framesets Lemmas
1 2,722
2 321
> 2 181
Table 9: Frameset polysemy across lemmas
Overall BC BN MZ NW TC WB
F F F F F F F
ALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2
Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7
Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0
Event 27.6 00.0 34.8 30.8 47.6 - 13.3
Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9
GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7
Language 41.2 - 50.0 50.0 00.0 20.0 75.0
Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0
Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5
Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5
NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0
Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1
Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6
Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8
Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0
Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2
Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7
Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6
Table 10: Named Entity performance on the CoNLL-
2011 test set
time constraints, we could not re-train it on the
OntoNotes data and so an existing, pre-trained
model was used, therefore the results are not a
good indicator of the model?s best performance.
The pre-trained model had also used a somewhat
different catalog of name types, which did not
include the OntoNotes NORP type (for nationalities,
organizations, religions, and political parties),
so that category was never predicted. Table 10
shows the overall performance of the tagger on the
CoNLL-2011 test set, as well as the performance
broken down by individual name types. IdentiFinder
performance has been reported to be in the low 90?s
on WSJ test set.
Other Layers As noted above, systems were al-
lowed to make use of gender and number predic-
tions for NPs using the table from Bergsma and Lin
(Bergsma and Lin, 2006).
4.4.3 Data Format
In order to organize the multiple, rich layers of anno-
tation, the OntoNotes project has created a database
representation for the raw annotation layers along
with a Python API to manipulate them (Pradhan et
al., 2007a). In the OntoNotes distribution the data is
organized as one file per layer, per document. The
API requires a certain hierarchical structure with
documents at the leaves inside a hierarchy of lan-
guage, genre, source and section. It comes with var-
ious ways of cleanly querying and manipulating the
data and allows convenient access to the sense in-
ventory and propbank frame files instead of having
to interpret the raw .xml versions. However, main-
taining format consistency with earlier CoNLL tasks
was deemed convenient for sites that already had
tools configured to deal with that format. Therefore,
in order to distribute the data so that one could make
the best of both worlds, we created a new file type
called .conll which logically served as another layer
in addition to the .parse, .prop, .name and .coref
layers. Each .conll file contained a merged repre-
sentation of all the OntoNotes layers in the CoNLL-
style tabular format with one line per token, and with
multiple columns for each token specifying the input
annotation layers relevant to that token, with the fi-
nal column specifying the target coreference layer.
Because OntoNotes is not authorized to distribute
the underlying text, and many of the layers contain
inline annotation, we had to provide a skeletal form
(.skel of the .conll file which was essentially the
.conll file, but with the word column replaced with
a dummy string. We provided an assembly script
that participants could use to create a .conll file tak-
ing as input the .skel file and the top-level directory
of the OntoNotes distribution that they had sepa-
rately downloaded from the LDC16 Once the .conll
file is created, it can be used to create the individual
layers such as .parse, .name, .coref etc. using an-
other set of scripts. Since the propositions and word
sense layers are inherently standoff annotation, they
were provided as is, and did not require that extra
merging step. One thing thing that made this data
creation process a bit tricky was the fact that we had
dissected some of the trees for the conversation data
to remove the EDITED phrases. Table 11 describes
the data provided in each of the column of the .conll
format. Figure 3 shows a sample from a .conll file.
4.5 Evaluation
This section describes the evaluation criteria used.
Unlike for propositions, word sense and named en-
tities, where it is simply a matter of counting the
correct answers, or for parsing, where there are sev-
eral established metrics, evaluating the accuracy of
coreference continues to be contentious. Various al-
16OntoNotes is deeply grateful to the Linguistic Data Con-
sortium for making the source data freely available to the task
participants.
12
Column Type Description
1 Document ID This is a variation on the document filename
2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3 Word number This is the word index in the sentence
4 Word The word itself
5 Part of Speech Part of Speech of the word
6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the
word/part-of-speech leaf replaced with a *. The full parse can be created by substituting
the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the
rows of that column.
7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-
tion. All other rows are marked with a -
8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.
9 Word sense This is the word sense of the word in Column 3.
10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and
Web Log data.
11 Named Entities These columns identifies the spans representing various named entities.
12:N Predicate Arguments There is one column each of predicate argument structure information for the predicate
mentioned in Column 7.
N Coreference Coreference chain information encoded in a parenthesis structure.
Table 11: Format of the .conll file used on the shared task
#begin document (nw/wsj/07/wsj_0771); part 000
...
...
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S(S* - - - - * * (ARG1* * * -
nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)
nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)
nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -
nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -
nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)
nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 9 ?? ?? *) - - - - * * *) * * -
nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15
nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)
nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -
nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -
nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -
nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -
nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -
nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23
nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -
nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -
nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -
nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -
nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -
nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -
nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -
nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -
nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)
nw/wsj/07/wsj_0771 0 30 . . *)) - - - - * * * * * -
nw/wsj/07/wsj_0771 0 0 ?? ?? (TOP(S* - - - - * * * -
nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)
nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -
nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6
nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)
nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -
nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)
nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -
nw/wsj/07/wsj_0771 0 8 . . *)) - - - - * * * -
#end document
Figure 3: Sample portion of the .conll file.
13
ternative metrics have been proposed, as mentioned
below, which weight different features of a proposed
coreference pattern differently. The choice is not
clear in part because the value of a particular set of
coreference predictions is integrally tied to the con-
suming application.
A further issue in defining a coreference metric
concerns the granularity of the mentions, and how
closely the predicted mentions are required to match
those in the gold standard for a coreference predic-
tion to be counted as correct.
Our evaluation criterion was in part driven by the
OntoNotes data structures. OntoNotes coreference
distinguishes between identity coreference and ap-
positive coreference, treating the latter separately
because it is already captured explicitly by other lay-
ers of the OntoNotes annotation. Thus we evaluated
systems only on the identity coreference task, which
links all categories of entities and events together
into equivalent classes.
The situation with mentions for OntoNotes is also
different than it was for MUC or ACE. OntoNotes
data does not explicitly identify the minimum ex-
tents of an entity mention, but it does include hand-
tagged syntactic parses. Thus for the official evalua-
tion, we decided to use the exact spans of mentions
for determining correctness. The NP boundaries
for the test data were pre-extracted from the hand-
tagged Treebank for annotation, and events trig-
gered by verb phrases were tagged using the verbs
themselves. This choice means that scores for the
CoNLL-2011 coreference task are likely to be lower
than for coref evaluations based on MUC, where the
mention spans are specified in the input,17 or those
based on ACE data, where an approximate match is
often allowed based on the specified head of the NP
mention.
4.5.1 Metrics
As noted above, the choice of an evaluation met-
ric for coreference has been a tricky issue and there
does not appear to be any silver bullet approach that
addresses all the concerns. Three metrics have been
proposed for evaluating coreference performance
over an unrestricted set of entity types: i) The link
based MUC metric (Vilain et al, 1995), ii) The men-
tion based B-CUBED metric (Bagga and Baldwin,
1998) and iii) The entity based CEAF (Constrained
Entity Aligned F-measure) metric (Luo, 2005). Very
recently BLANC (BiLateral Assessment of Noun-
Phrase Coreference) measure (Recasens and Hovy,
17as is the case in this evaluation with Gold Mentions
2011) has been proposed as well. Each of the met-
ric tries to address the shortcomings or biases of the
earlier metrics. Given a set of key entities K, and
a set of response entities R, with each entity com-
prising one or more mentions, each metric generates
its variation of a precision and recall measure. The
MUC measure if the oldest and most widely used. It
focuses on the links (or, pairs of mentions) in the
data.18 The number of common links between en-
tities in K and R divided by the number of links
in K represents the recall, whereas, precision is the
number of common links between entities in K and
R divided by the number of links in R. This met-
ric prefers systems that have more mentions per en-
tity ? a system that creates a single entity of all
the mentions will get a 100% recall without signifi-
cant degradation in its precision. And, it ignores re-
call for singleton entities, or entities with only one
mention. The B-CUBED metric tries to addresses
MUCS?s shortcomings, by focusing on the mentions
and computes recall and precision scores for each
mention. If K is the key entity containing mention M,
and R is the response entity containing mention M,
then recall for the mention M is computed as |K?R||K|
and precision for the same is is computed as |K?R||R| .
Overall recall and precision are the average of the
individual mention scores. CEAF aligns every re-
sponse entity with at most one key entity by finding
the best one-to-one mapping between the entities us-
ing an entity similarity metric. This is a maximum
bipartite matching problem and can be solved by
the Kuhn-Munkres algorithm. This is thus a entity
based measure. Depending on the similarity, there
are two variations ? entity based CEAF ? CEAFe and
a mention based CEAF ? CEAFe. Recall is the total
similarity divided by the number of mentions in K,
and precision is the total similarity divided by the
number of mentions in R. Finally, BLANC uses a
variation on the Rand index (Rand, 1971) suitable
for evaluating coreference. There are a few other
measures ? one being the ACE value, but since this
is specific to a restricted set of entities (ACE types),
we did not consider it.
4.5.2 Official Evaluation Metric
In order to determine the best performing system
in the shared task, we needed to associate a single
number with each system. This could have been
one of the metrics above, or some combination of
more than one of them. The choice was not sim-
ple, and while we consulted various researchers in
18The MUC corpora did not tag single mention entities.
14
the field, hoping for a strong consensus, their con-
clusion seemed to be that each metric had its pros
and cons. We settled on the MELA metric by Denis
and Baldridge (2009), which takes a weighted av-
erage of three metrics: MUC, B-CUBED, and CEAF.
The rationale for the combination is that each of the
three metrics represents a different important dimen-
sion, the MUC measure being based on links, the
B-CUBED based on mentions, and the CEAF based
on entities. For a given task, a weighted average
of the three might be optimal, but since we don?t
have an end task in mind, we decided to use the un-
weighted mean of the three metrics as the score on
which the winning system was judged. We decided
to use CEAFe instead of CEAFm.
4.5.3 Scoring Metrics Implementation
We used the same core scorer implementation19 that
was used for the SEMEVAL-2010 task, and which
implemented all the different metrics. There were a
couple of modifications done to this scorer after it
was used for the SEMEVAL-2010 task.
1. Only exact matches were considered cor-
rect. Previously, for SEMEVAL-2010 non-exact
matches were judged partially correct with a
0.5 score if the heads were the same and the
mention extent did not exceed the gold men-
tion.
2. The modifications suggested by Cai and Strube
(2010) were incorporated in the scorer.
Since there are differences in the version used for
CoNLL and the one available on the download site,
and it is possible that the latter would be revised in
the future, we have archived the version of the scorer
on the CoNLL-2011 task webpage.20
5 Systems and Results
About 65 different groups demonstrated interest in
the shared task by registering on the task webpage.
Of these, 23 groups submitted system outputs on the
test set during the evaluation week. 18 groups sub-
mitted only closed track results, 3 groups only open
track results, and 2 groups submitted both closed and
open track results. 2 participants in the closed track,
did not write system papers, so we don?t use their re-
sults in the discussion. Their results will be reported
on the task webpage.
19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3
20http://conll.bbn.com/download/scorer.v4.tar.gz
The official results for the 18 systems that submit-
ted closed track outputs are shown in Table 12, with
those for the 5 systems that submitted open track
results in Table 13. The official ranking score, the
arithmetic mean of the F-scores of MUC, B-CUBED
and CEAFe, is shown in the rightmost column. For
convenience, systems will be referred to here using
the first portion of the full name, which is unique
within each table.
For completeness, the tables include the raw pre-
cision and recall scores from which the F-scores
were derived. The tables also include two additional
scores (BLANC and CEAFm) that did not factor into
the official ranking score. Useful further analysis
may be possible based on these results beyond the
preliminary results presented here.
As discussed previously in the task description,
we will consider three different test input conditions:
i) Predicted only (Official), ii) Predicted plus gold
mention boundaries, and iii) Predicted plus gold
mentions
5.1 Predicted only (Official)
For the official test, beyond the raw source text,
coreference systems were provided only with the
predictions from automatic engines as to the other
annotation layers (parses, semantic roles, word
senses, and named entities).
In this evaluation it is important to note that the
mention detection score cannot be considered in iso-
lation of the coreference task as has usually been the
case. This is mainly owing to the fact that there are
no singleton entities in the OntoNotes data. Most
systems removed singletons from the response as a
post-processing step, so not only will they not get
credit for the singleton entities that they correctly re-
moved from the data, but they will be penalized for
the ones that they accidentally linked with another
mention. What this number does indicate is the ceil-
ing on recall that a system would have got in absence
of being penalized for making mistakes in corefer-
ence resolution. A close look at the Table 12 indi-
cates a possible outlier in case of the sapena system.
The recall for this system is very high, and precision
way lower than any other system. Further investi-
gations uncovered that the reason for this aberrant
behavior was that fact that this system opted to keep
singletons in the response. By design, the scorer re-
moves singletons that might be still present in the
system, but it does so after the mention detection
accuracy is computed.
The official scores top out in the high 50?s. While
this is lower than the figures cited in previous coref-
15
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
75
.07
66
.81
70
.7
0
61
.76
57
.53
59
.57
68
.40
68
.23
68
.31
56
.37
56
.37
56
.3
7
43
.41
47
.75
45
.4
8
70
.63
76
.21
73
.02
57
.7
9
sap
en
a
92
.39
28
.19
43
.20
56
.32
63
.16
59
.55
62
.75
72
.08
67
.09
53
.51
53
.51
53
.51
44
.75
38
.38
41
.32
69
.50
73
.07
71
.10
55
.99
ch
an
g
68
.08
61
.96
64
.88
57
.15
57
.15
57
.15
67
.14
70
.53
68
.7
9
54
.40
54
.40
54
.40
41
.94
41
.94
41
.94
71
.19
77
.09
73
.7
1
55
.96
nu
gu
es
69
.87
68
.08
68
.96
60
.20
57
.10
58
.61
66
.74
64
.23
65
.46
51
.45
51
.45
51
.45
38
.09
41
.06
39
.52
71
.99
70
.31
71
.11
54
.53
san
tos
67
.80
63
.25
65
.45
59
.21
54
.30
56
.65
68
.79
62
.81
65
.66
49
.54
49
.54
49
.54
35
.86
40
.21
37
.91
73
.37
66
.91
69
.46
53
.41
son
g
57
.81
80
.41
67
.26
53
.73
67
.79
59
.9
5
60
.65
66
.05
63
.23
46
.29
46
.29
46
.29
43
.37
30
.71
35
.96
69
.49
59
.71
61
.47
53
.05
sto
ya
no
v
70
.84
64
.98
67
.78
63
.61
54
.04
58
.43
72
.58
53
.27
61
.44
46
.08
46
.08
46
.08
32
.00
40
.82
35
.88
73
.21
58
.93
60
.88
51
.92
sob
ha
67
.82
62
.09
64
.83
51
.08
49
.88
50
.48
62
.63
65
.43
64
.00
49
.48
49
.48
49
.48
40
.65
41
.82
41
.23
61
.40
68
.35
63
.88
51
.90
ko
bd
an
i
62
.06
60
.04
61
.03
55
.64
51
.50
53
.49
69
.66
62
.43
65
.85
42
.70
42
.70
42
.70
32
.33
35
.40
33
.79
61
.86
63
.51
62
.61
51
.04
zh
ou
61
.08
63
.59
62
.31
45
.65
52
.79
48
.96
57
.14
72
.91
64
.07
47
.53
47
.53
47
.53
43
.19
36
.79
39
.74
61
.10
73
.94
64
.72
50
.92
ch
art
on
65
.90
62
.77
64
.30
55
.09
50
.05
52
.45
66
.26
58
.44
62
.10
46
.82
46
.82
46
.82
34
.33
39
.05
36
.54
69
.94
62
.23
64
.80
50
.36
ya
ng
71
.92
57
.53
63
.93
59
.91
46
.43
52
.31
71
.64
55
.14
62
.32
46
.55
46
.55
46
.55
30
.28
42
.39
35
.33
71
.11
61
.75
64
.63
49
.99
ha
o
64
.50
64
.11
64
.30
57
.89
51
.42
54
.47
67
.83
55
.43
61
.01
45
.07
45
.07
45
.07
30
.08
35
.76
32
.67
72
.61
62
.37
65
.35
49
.38
xin
xin
65
.49
58
.71
61
.92
48
.54
44
.85
46
.62
61
.59
62
.28
61
.93
44
.75
44
.75
44
.75
35
.19
38
.62
36
.83
63
.04
65
.83
64
.27
48
.46
zh
an
g
55
.35
68
.25
61
.13
42
.03
55
.62
47
.88
52
.57
73
.05
61
.14
44
.46
44
.46
44
.46
42
.00
30
.28
35
.19
62
.84
69
.22
65
.21
48
.07
ku
mm
erf
eld
69
.77
56
.97
62
.72
46
.39
39
.56
42
.70
63
.60
57
.30
60
.29
45
.35
45
.35
45
.35
35
.05
42
.26
38
.32
58
.74
61
.58
59
.91
47
.10
zh
ek
ov
a
67
.49
37
.60
48
.29
28
.87
20
.66
24
.08
67
.14
56
.67
61
.46
40
.43
40
.43
40
.43
31
.57
41
.21
35
.75
52
.77
57
.05
53
.77
40
.43
irw
in
17
.06
61
.09
26
.67
12
.45
50
.60
19
.98
35
.07
89
.90
50
.46
31
.68
31
.68
31
.68
45
.84
17
.38
25
.21
51
.48
56
.83
51
.12
31
.88
Ta
ble
12
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
74
.31
67
.87
70
.9
4
62
.83
59
.34
61
.0
3
68
.85
69
.01
68
.9
3
56
.70
56
.70
56
.7
0
43
.29
46
.80
44
.9
8
71
.90
76
.55
73
.96
58
.3
1
cai
67
.15
67
.64
67
.40
56
.73
58
.90
57
.80
64
.60
71
.03
67
.66
53
.37
53
.37
53
.37
42
.71
40
.68
41
.67
69
.77
73
.96
71
.62
55
.71
ury
up
ina
70
.60
66
.31
68
.39
59
.70
55
.70
57
.63
66
.29
64
.12
65
.18
51
.42
51
.42
51
.42
38
.34
42
.17
40
.16
69
.23
68
.54
68
.88
54
.32
kle
nn
er
64
.41
60
.28
62
.28
49
.04
50
.71
49
.86
61
.70
68
.61
64
.97
50
.03
50
.03
50
.03
41
.28
39
.70
40
.48
66
.05
73
.90
69
.05
51
.77
irw
in
24
.60
62
.27
35
.27
18
.56
51
.01
27
.21
38
.97
85
.57
53
.55
33
.86
33
.86
33
.86
43
.33
19
.36
26
.76
51
.62
52
.91
51
.76
35
.84
Ta
ble
13
:P
erf
orm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
79
.52
71
.25
75
.1
6
65
.87
62
.05
63
.9
0
69
.52
70
.55
70
.0
3
59
.26
59
.26
59
.2
6
46
.29
50
.48
48
.3
0
72
.00
78
.55
74
.7
7
60
.7
4
nu
gu
es
74
.18
70
.74
72
.42
64
.33
60
.05
62
.12
68
.26
65
.17
66
.68
53
.84
53
.84
53
.84
39
.86
44
.23
41
.93
72
.53
71
.04
71
.75
56
.91
ch
an
g
63
.37
73
.18
67
.92
55
.00
65
.50
59
.79
62
.16
76
.65
68
.65
54
.95
54
.95
54
.95
46
.77
37
.17
41
.42
70
.97
79
.30
74
.29
56
.62
san
tos
65
.82
69
.90
67
.80
57
.76
61
.39
59
.52
64
.49
70
.27
67
.26
51
.87
51
.87
51
.87
41
.42
38
.16
39
.72
72
.72
71
.97
72
.34
55
.50
ko
bd
an
i
67
.11
65
.09
66
.08
62
.63
56
.80
59
.57
73
.20
62
.22
67
.27
44
.49
44
.49
44
.49
32
.87
37
.25
34
.92
64
.07
64
.13
64
.10
53
.92
sto
ya
no
v
76
.90
64
.73
70
.29
69
.81
55
.01
61
.54
77
.07
52
.54
62
.48
48
.08
48
.08
48
.08
30
.97
44
.84
36
.64
76
.57
60
.33
62
.96
53
.55
zh
an
g
59
.62
71
.19
64
.89
46
.06
58
.75
51
.64
53
.89
73
.41
62
.16
46
.62
46
.62
46
.62
43
.49
32
.11
36
.95
64
.11
70
.47
66
.54
50
.25
son
g
58
.43
77
.64
66
.68
46
.66
68
.40
55
.48
54
.40
70
.19
61
.29
43
.62
43
.62
43
.62
43
.77
25
.88
32
.53
66
.29
58
.76
60
.22
49
.77
zh
ek
ov
a
69
.19
57
.27
62
.67
33
.48
37
.15
35
.22
55
.47
68
.23
61
.20
41
.31
41
.31
41
.31
38
.29
34
.65
36
.38
53
.45
63
.33
54
.79
44
.27
Ta
ble
14
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
cl
os
ed
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
78
.71
72
.33
75
.39
66
.93
63
.91
65
.39
70
.09
71
.49
70
.78
59
.78
59
.78
59
.78
46
.34
49
.62
47
.92
73
.38
79
.00
75
.83
61
.36
Ta
ble
15
:P
erf
orm
an
ce
of
sys
tem
sin
the
sup
ple
me
nta
ry
op
en
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
bo
un
da
ri
es
16
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
ch
an
g
10
0
10
0
10
0
80
.46
84
.75
82
.55
72
.84
74
.57
73
.70
69
.71
69
.71
69
.71
70
.45
60
.75
65
.24
78
.01
76
.57
77
.26
73
.83
Ta
ble
16
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,c
lo
se
d
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
83
.37
10
0
90
.93
74
.79
89
.68
81
.56
67
.46
86
.88
75
.95
70
.73
70
.73
70
.73
77
.75
51
.05
61
.64
76
.65
85
.85
80
.35
73
.05
Ta
ble
17
:P
erf
orm
an
ce
of
sys
tem
sin
the
su
pp
le
m
en
ta
ry
,o
pe
n
tra
ck
usi
ng
pre
dic
ted
inf
orm
ati
on
plu
sg
ol
d
m
en
ti
on
s
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.79
68
.34
72
.3
2
63
.29
58
.96
61
.05
68
.84
68
.72
68
.78
57
.28
57
.28
57
.2
8
44
.19
48
.75
46
.3
6
70
.93
76
.58
73
.36
58
.7
3
sap
en
a
95
.27
29
.07
44
.55
56
.99
63
.91
60
.25
62
.89
72
.31
67
.27
53
.90
53
.90
53
.90
45
.22
38
.70
41
.71
69
.71
73
.32
71
.32
56
.41
ch
an
g
69
.88
63
.61
66
.60
58
.48
58
.48
58
.48
67
.42
70
.91
69
.1
2
55
.21
55
.21
55
.21
42
.66
42
.66
42
.66
71
.42
77
.36
73
.9
6
56
.75
nu
gu
es
72
.96
71
.08
72
.01
62
.68
59
.46
61
.03
67
.24
64
.89
66
.04
52
.82
52
.82
52
.82
39
.25
42
.50
40
.81
72
.57
70
.86
71
.68
55
.96
san
tos
70
.39
65
.67
67
.95
61
.28
56
.20
58
.63
69
.25
63
.16
66
.07
50
.47
50
.47
50
.47
36
.51
41
.15
38
.69
73
.92
67
.32
69
.93
54
.46
son
g
59
.24
82
.39
68
.92
54
.92
69
.29
61
.27
60
.89
66
.27
63
.46
46
.97
46
.97
46
.97
44
.49
31
.15
36
.65
69
.73
59
.87
61
.61
53
.79
sto
ya
no
v
74
.43
68
.28
71
.22
67
.18
57
.08
61
.7
2
74
.06
53
.45
62
.09
47
.40
47
.40
47
.40
32
.78
42
.52
37
.02
74
.10
59
.34
61
.31
53
.61
sob
ha
71
.06
65
.06
67
.93
53
.91
52
.64
53
.27
63
.17
66
.14
64
.62
50
.80
50
.80
50
.80
41
.77
43
.03
42
.39
61
.91
69
.15
64
.49
53
.43
ko
bd
an
i
65
.98
63
.83
64
.89
59
.22
54
.81
56
.93
70
.49
63
.12
66
.60
44
.17
44
.14
44
.15
33
.19
36
.50
34
.77
62
.52
64
.25
63
.32
52
.77
zh
ou
64
.11
66
.74
65
.40
48
.00
55
.51
51
.48
57
.18
73
.71
64
.40
48
.40
48
.40
48
.40
44
.18
37
.35
40
.48
61
.54
74
.86
65
.30
52
.12
ch
art
on
71
.01
67
.64
69
.28
59
.24
53
.82
56
.40
67
.10
59
.02
62
.80
48
.91
48
.91
48
.91
35
.96
41
.39
38
.48
70
.65
62
.71
65
.34
52
.56
ya
ng
73
.73
58
.97
65
.53
61
.23
47
.45
53
.47
71
.88
55
.13
62
.40
47
.05
47
.05
47
.05
30
.54
43
.16
35
.77
71
.39
61
.92
64
.83
50
.55
ha
o
66
.79
66
.38
66
.59
59
.55
52
.89
56
.02
68
.27
55
.46
61
.20
45
.95
45
.95
45
.95
30
.76
36
.81
33
.51
73
.22
62
.73
65
.78
50
.24
xin
xin
69
.05
61
.91
65
.28
50
.99
47
.11
48
.97
61
.59
62
.70
62
.14
45
.64
45
.64
45
.64
35
.86
39
.57
37
.62
63
.42
66
.29
64
.68
49
.58
zh
an
g
57
.41
70
.78
63
.40
43
.48
57
.53
49
.53
52
.44
73
.60
61
.24
44
.97
44
.97
44
.97
42
.71
30
.44
35
.55
63
.12
69
.63
65
.53
48
.77
ku
mm
erf
eld
71
.05
58
.01
63
.87
47
.42
40
.44
43
.65
63
.73
57
.39
60
.39
45
.76
45
.76
45
.76
35
.30
42
.72
38
.66
58
.89
61
.77
60
.07
47
.57
zh
ek
ov
a
72
.65
40
.48
51
.99
31
.73
22
.70
26
.46
66
.92
56
.68
61
.37
41
.04
41
.04
41
.04
31
.93
42
.17
36
.34
53
.09
57
.86
54
.22
41
.39
irw
in
17
.58
62
.96
27
.49
12
.69
51
.59
20
.37
34
.88
89
.98
50
.27
31
.71
31
.71
31
.71
46
.13
17
.33
25
.20
51
.51
56
.93
51
.14
31
.95
Ta
ble
18
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
cl
os
ed
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
Sy
ste
m
Me
nti
on
De
tec
tio
n
M
UC
B-
CU
BE
D
CE
AF
m
CE
AF
e
BL
AN
C
Of
fic
ial
R
P
F
R
P
F1
R
P
F2
R
P
F
R
P
F3
R
P
F
F
1
+
F
2
+
F
3
3
lee
76
.01
69
.43
72
.5
7
64
.40
60
.83
62
.5
7
69
.34
69
.57
69
.4
5
57
.68
57
.68
57
.6
8
44
.15
47
.85
45
.9
2
72
.23
76
.94
74
.3
2
59
.3
1
cai
69
.32
69
.82
69
.57
58
.39
60
.63
59
.49
64
.88
71
.53
68
.04
54
.36
54
.36
54
.36
43
.74
41
.58
42
.64
70
.13
74
.39
72
.01
56
.72
ury
up
ina
72
.10
67
.72
69
.84
60
.74
56
.68
58
.64
66
.43
64
.25
65
.32
52
.00
52
.00
52
.00
38
.87
42
.85
40
.76
69
.43
68
.73
69
.07
54
.91
kle
nn
er
71
.73
67
.14
69
.36
55
.17
57
.04
56
.09
62
.67
70
.69
66
.44
53
.25
53
.25
53
.25
44
.27
42
.39
43
.31
67
.45
75
.92
70
.68
55
.28
irw
in
25
.24
63
.87
36
.18
18
.90
51
.94
27
.71
38
.79
85
.64
53
.40
33
.89
33
.89
33
.89
43
.59
19
.31
26
.76
51
.66
52
.98
51
.80
35
.96
Ta
ble
19
:H
ea
d
w
or
d
ba
se
d
pe
rfo
rm
an
ce
of
sys
tem
sin
the
of
fic
ia
l,
op
en
tra
ck
usi
ng
all
pre
dic
ted
inf
orm
ati
on
17
erence evaluations, that is as expected, given that the
task here includes predicting the underlying men-
tions and mention boundaries, the insistence on ex-
act match, and given that the relatively easier appos-
itive coreference cases are not included in this mea-
sure. The top-performing system (lee) had a score
of 57.79 which is about 1.8 points higher than that
of the second (sapena) and third (chang) ranking
systems, which scored 55.99 and 55.96 respectively.
Another 1.5 points separates them from the fourth
best score of 54.53 (nugues). Thus the performance
differences between the better-scoring systems were
not large, with only about three points separating the
top four systems.
This becomes even clearer if we merge in the re-
sults of systems that participated only in the open
track but that made relatively limited use of outside
resources.21 Comparing that way, the cai system
scores in the same ball park as the second rank sys-
tems (sapena and chang). The uryupina system sim-
ilarly scores very close to nugues?s 54.53
Given that our choice of the official metric was
somewhat arbitrary, if is also useful to look at
the individual metrics, including the mention-based
CEAFm and BLANC metrics that were not part of
the official metric. The lee system which scored
the best using the official metric does slightly worse
than song on the MUC metric, and also does slightly
worse than chang on the B-CUBED and BLANC met-
rics. However, it does much better than every other
group on the entity-based CEAFe, and this is the pri-
mary reason for its 1.8 point advantage in the offi-
cial score. If the CEAFe measure does indicate the
accuracy of entities in the response, this suggests
that the lee system is doing better on getting coher-
ent entities than any other system. This could be
partly due to the fact that that system is primarily
a precision-based system that would tend to create
purer entities. The CEAFe measure also seems to pe-
nalize other systems more harshly than do the other
measures.
We cannot compare these results to the ones ob-
tained in the SEMEVAL-2010 coreference task using
a small portion of OntoNotes data because it was
only using nominal entities, and had heuristically
added singleton mentions to the OntoNotes data22
21The cai system specifically mentions that, and the only re-
source that the uryupina system used outside of the closed track
setting was the Stanford named entity tagger.
22The documentation that comes with the SEMEVAL data
package from LDC (LDC2011T01) states: ?Only nominal
mentions and identical (IDENT) types were taken from the
OntoNotes coreference annotation, thus excluding coreference
5.2 Predicted plus gold mention boundaries
We also explored performance when the systems
were provided with the gold mention boundaries,
that is, with the exact spans (expressed in terms of
token offsets) for all of the NP constituents in the
human-annotated parse trees for the test data. Sys-
tems could use this additional data to ensure that the
output mention spans in their entity chains would not
clash with those in the answer set. Since this was
a secondary evaluation, it was an optional element,
and not all participants ran their systems on this task
variation. The results for those systems that did par-
ticipate in this optional task are shown in Tables 14
(closed track) and 15 (open track).
Most of the better scoring systems did supply
these results. While all systems did slightly better
here in terms of raw scores, the performance was
not much different from the official task, indicating
that mention boundary errors resulting from prob-
lems in parsing do not contribute significantly to the
final output.23
One side benefit of performing this supplemental
evaluation was that it revealed a subtle bug in the
automatic scoring routine that we were using that
could double-count duplicate correct mentions in a
given entity chain. These can occur, for example, if
the system considers a unit-production NP-PRP com-
bination as two mentions that identify the exact same
token in the text, and reports them as separate men-
tions. Most systems had a filter in their processing
that selected only one of these duplicate mentions,
but the kobdani system considered both as potential
mentions, and its developers tuned their algorithm
using that flawed version of the scorer.
When we fixed the scorer and re-evaluated all of
the systems, the kobdani system was the only one
whose score was affected significantly, dropping by
about 8 points, which lowered that system?s rank
from second to ninth. It is not clear how much of
this was owing to the fact that the system?s param-
relations with verbs and appositives. Since OntoNotes is only
annotated with multi-mention entities, singleton referential ele-
ments were identified heuristically: all NPs and possessive de-
terminers were annotated as singletons excluding those func-
tioning as appositives or as pre-modifiers but for NPs in the
possessive case. In coordinated NPs, single constituents as well
as the entire NPs were considered to be mentions. There is no
reliable heuristic to automatically detect English expletive pro-
nouns, thus they were (although inaccurately) also annotated as
singletons.?
23It would be interesting to measure the overlap between the
entity clusters for these two cases, to see whether there was
any substantial difference in the mention chains, besides the ex-
pected differences in boundaries for individual mentions.
18
eters had been tuned using the scorer with the bug,
which double-credited duplicate mentions. To find
out for sure, one would have to re-tune the system
using the modified scorer.
One difficulty with this supplementary evaluation
using gold mention boundaries is that those bound-
aries alone provide only very partial information.
For the roughly 10% of mentions that the automatic
parser did not correctly identify, while the systems
knew the correct boundaries, they had no hierarchi-
cal parser or semantic role label information, and
they also had to further approximate the already
heuristic head word identification. This incomplete
data complicated the systems? task and also compli-
cates interpretation of the results.
5.3 Predicted plus gold mentions
The final supplementary condition that we explored
was if the systems were supplied with the manually-
annotated spans for exactly those mentions that did
participate in the gold standard coreference chains.
This supplies significantly more information than
the previous case, where exact spans were supplied
for all NPs, since the gold mentions list here will also
include verb headwords that are linked to event NPs,
but will not include singleton mentions, which do
not end up as part of any chain. The latter constraint
makes this test seem somewhat artificial, since it di-
rectly reveals part of what the systems are designed
to determine, but it still has some value in quanti-
fying the impact that mention detection has on the
overall task and what the results are if the mention
detection is perfect.
Since this was a logical extension of the task and
since the data was available to the participants for
the development set, a few of the sites did run ex-
periments of this type. Therefore we decided to pro-
vide the gold mentions data to a few sites who had
reported these scores, so that we could compute the
performance on the test set. The results of these ex-
periments are shown in Tables 16 and 17. The results
show that performance does go up significantly, in-
dicating that it is markedly easier for the systems
to generate better entities given gold mentions. Al-
though, ideally, one would expect a perfect mention
detection score, it is the case that one of the two sys-
tems ? lee ? did not get a 100% Recall. This could
possibly be owing to unlinked singletons that were
removed in post-processing.
The lee system developers also ran a further ex-
periment where both gold mentions for the elements
of the coreference chains and also gold annota-
tions for all the other layers were available to the
system. Surprisingly, the improvement in corefer-
ence performance from having gold annotation of
the other layers was almost negligible. This sug-
gests that either: i) the automatic models are pre-
dicting those layers well enough that switching to
gold doesn?t make much difference; ii) information
from the other layers does not provide much lever-
age for coreference resolution; or iii) current coref-
erence models are not capable of utilizing the infor-
mation from these other layers effectively. Given
the performance numbers on the individual layers
cited earlier, (i) seems unlikely, and we hope that
further research in how best to leverage these lay-
ers will result in models that can benefit from them
more definitively.
5.4 Head word based scoring
In order to check how stringent the official, exact
match scoring is, we also performed a relaxed scor-
ing. Unlike ACE and MUC, the OntoNotes data does
not have manually annotated minimum spans that
a mention must contain to be considered correct.
However, OntoNotes does have manual syntactic
analysis in the form of the Treebank. Therefore, we
decided to approximate the minimum spans by using
the head words of the mentions using the gold stan-
dard syntax tree. If the response mention contained
the head word and did not exceed the true mention
boundary, then it was considered correct ? both from
the point of view of mention detection, and corefer-
ence resolution. The scores using this relaxed strat-
egy for the open and closed track submissions using
predicted data are shown in Tables 18 and 19. It
can be observed that the relaxed, head word based,
scoring does not improve performance very much.
The only exception was the klenner system whose
performance increased from 51.77 to 55.28. Over-
all, the ranking remained quite stable, though it did
change for some adjacent systems which had very
close exact match scores.
5.5 Genre variation
In order to check how the systems did on various
genres, we scored their performance per genre as
well. Tables 20 and 21 summarize genre based per-
formance for the closed and open track participants
respectively. System performance does not seem
to vary as much across the different genres as is
normally the case with language processing tasks,
which could suggest that coreference is relatively
genre insensitive, or it is possible that scores are
two low for the difference to be apparent. Compar-
isons are difficult, however, because the spoken gen-
19
MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC O
F F F F F F F F F F F F F F
lee GENRE zhou GENRE
BC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1
BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5
MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0
NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2
TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5
WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5
sapena charton
BC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1
BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9
MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3
NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9
TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0
WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0
chang yang
BC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3
BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3
MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5
NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6
TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4
WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9
nugues hao
BC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8
BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9
MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5
NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0
TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8
WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2
santos xinxin
BC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9
BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9
MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9
NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0
TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1
WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0
song zhang
BC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1
BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9
MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5
NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9
TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7
WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1
stoyanov kummerfield
BC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4
BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1
MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7
NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6
TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7
WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6
sobha zhekova
BC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8
BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0
MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1
NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3
TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1
WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4
kobdani irwin
BC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6
BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3
MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1
NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1
TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4
WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8
Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance. MD
represents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and O
represents the OFFICIAL score.
20
res were treated here with perfect speech recognition
accuracy and perfect speaker turn information. Un-
der more realistic application conditions, the spread
in performance between genres might be greater.
MD MUC BCUB Cm Ce BLANC O
F F F F F F F
lee GENRE
BC 72.7 61.7 67.0 54.5 43.6 72.7 57.4
BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3
MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2
NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9
TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9
WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9
cai
BC 69.7 59.1 66.0 50.5 39.9 69.2 55.0
BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9
MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4
NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0
TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2
WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2
uryupina
BC 70.2 58.3 62.7 48.7 38.0 68.7 53.0
BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8
MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8
NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9
TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2
WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6
klenner
BC 63.2 50.3 63.4 48.2 38.9 66.8 50.8
BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1
MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0
NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7
TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3
WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9
irwin
BC 36.6 27.6 50.9 32.0 25.5 50.2 34.7
BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0
MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6
NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0
TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6
WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2
Table 21: Detailed look at the performance per genre for
the official, open track using predicted information. MD
represents MENTION DETECTION; BCUB represents B-
CUBED; Cm represents CEAFm; Ce represents CEAFe and
O represents the OFFICIAL score.
6 Approaches
Tables 22 and 23 summarize the approaches of the
participating systems along with some of the impor-
tant dimensions.
Most of the systems broke the problem into two
phases, first identifying the potential mentions in the
text and then linking the mentions to form corefer-
ence chains. Most participants also used rule-based
approaches for mention detection, though two did
use trained models. While trained morels seem able
to better balance precision and recall, and thus to
achieve a higher F-score on the mention task itself,
their recall tends to be quite a bit lower than that
achievable by rule-based systems designed to fa-
vor recall. This impacts coreference scores because
the full coreference system has no way to recover
if the mention detection stage misses a potentially
anaphoric mention.
Only one of the participating systems cai at-
tempted to do joint mention detection and corefer-
ence resolution. While it did not happen to be among
the top-performing systems, the difference in perfor-
mance could be due to the richer features used by
other systems rather than to the use of a joint model.
Most systems represented the markable mentions
internally in terms of the parse tree NP constituent
span, but some systems used shared attribute mod-
els, where the attributes of the merged entity are
determined collectively by heuristically merging the
attribute types and values of the different constituent
mentions.
Various types of trained models were used for pre-
dicting coreference. It is interesting to note that
some of the systems, including the best-performing
one, used a completely rule-based approach even for
this component.
Most participants appear not to have focused
much on eventive coreference, those coreference
chains that build off verbs in the data. This usu-
ally meant that mentions that should have linked to
the eventive verb were instead linked in with some
other entity. Participants may have chosen not to fo-
cus on events because they pose unique challenges
while making up only a small portion of the data.
Roughly 91% of mentions in the data are NPs and
pronouns.
In the systems that used trained models, many
systems used the approach described in Soon et al
(2001) for selecting the positive and negative train-
ing examples, while others used some of the al-
ternative approaches that have been introduced in
the research literature more recently. Many of the
trained systems also were able to improve their per-
formance by using feature selection, though things
varied some depending on the example selection
strategy and the classifier used. Almost half of the
trained systems used the feature selection strategy
from Soon et al (2001) and found it beneficial. It is
not clear whether the other systems did not explore
this path, or whether it just did not prove as useful in
their case.
7 Conclusions
In this paper we described the anaphoric coreference
information and other layers of annotation in the
21
Ta
sk
Sy
nta
x
Le
arn
ing
Fra
me
wo
rk
Ma
rka
ble
Ide
nti
fic
ati
on
Ma
rka
ble
Ve
rb
Fe
atu
re
Se
lec
tio
n
#F
eat
ure
s
Tra
ini
ng
lee
C+
O
P
Ru
le-
ba
sed
Ru
les
to
ex
clu
de
Co
pu
lar
co
nst
ruc
tio
n,
Ap
po
sit
ive
s,P
leo
na
sti
ci
t,e
tc.
Fe
atu
re
de
pe
nd
en
t
wi
th
sha
red
att
rib
ute
s
?
?
?
sap
en
a
C
P
De
cis
ion
Tre
e+
Re
lax
ati
on
La
be
lin
g
NP
(m
ax
im
al
spa
n)
+P
RP
+N
E
+C
ap
ita
liz
ed
no
un
he
uri
sti
c
Fu
llp
hra
se
?
?
Tra
in
+D
ev
ch
an
g
C
P
Le
arn
ing
Ba
sed
Jav
a
NP
,N
E,
PR
P,
PR
P$
Fu
llp
hra
se
?
?
Tra
in
+D
ev
cai
O
P
Co
mp
ute
hy
pe
red
ge
we
igh
tso
n
30
%
of
tra
ini
ng
da
ta
NP
,P
RP
,P
RP
$,
Ba
se
ph
ras
ec
hu
nk
s,
Ple
on
ast
ic
it
filt
er
Fu
llp
hra
se
?
?
?
nu
gu
es
C
D
Lo
gis
tic
Re
gre
ssi
on
(LI
BL
IN
EA
R)
NP
,P
RP
$a
nd
seq
ue
nc
eo
fN
NP
(s)
in
po
st
pro
ces
sin
gu
sin
gA
LI
AS
an
dS
TR
IN
GM
AT
CH
He
ad
wo
rd
?
Fo
rw
ard
+B
ack
wa
rd
sta
rtin
gf
rom
So
on
fea
tur
es
et
24
Tra
in
+D
ev
ury
up
ina
O
P
De
cis
ion
Tre
e.
Di
ffe
ren
t
cla
ssi
fie
rs
for
Pro
no
mi
na
la
nd
no
n-P
ron
om
ina
lm
en
tio
ns
NP
,N
E,
PR
P,
PR
P$
,a
nd
rul
es
to
ex
clu
de
som
es
pe
cifi
cc
ase
s
Fu
llp
hra
se
?
Mu
lti-
Ob
jec
tiv
e
Op
tim
iza
tio
no
nt
hre
e
spl
its
.N
SG
A-
II
46
Tra
in
+D
ev
san
tos
C
P
ET
L
(E
ntr
op
yg
uid
ed
Tra
nsf
orm
ati
on
al
Le
arn
ing
)
co
mm
itte
ea
nd
Ra
nd
om
Fo
res
t
(W
EK
A)
Al
lN
Pa
nd
all
pro
no
un
sa
nd
PE
R,
OR
G,
GP
E
in
NP
Fu
llp
hra
se
?
Inh
ere
nt
to
the
cla
ssi
fie
rs
Tra
in
+D
ev
son
g
C
P
Ma
xE
nt
(O
pe
nN
LP
)
Me
nti
on
de
tec
tio
nc
las
sifi
er
Fu
llp
hra
se
?
Sa
me
fea
tur
es
et,
bu
t
pe
rc
las
sifi
er
40
Tra
in
sto
ya
no
v
C
P
Av
era
ge
dp
erc
ep
tro
n
NE
an
dp
oss
ess
ive
sin
ad
dit
ion
to
AC
E
ba
sed
sys
tem
Fu
llp
hra
se
?
?
76
?
sob
ha
C
P
CR
Ff
or
no
n-p
ron
om
ina
la
nd
sal
ien
ce
fac
tor
for
pro
no
mi
na
l
res
olu
tio
n
Ma
ch
ine
lea
rne
dp
leo
na
sti
ci
t,p
lus
NP
,P
RP
,
PR
P$
an
dN
E
Mi
nim
al
(C
hu
nk
/N
E)
an
dM
ax
im
um
spa
n
?
?
Tra
in
kle
nn
er
O
D
Ru
le-
ba
sed
.S
ali
en
ce
me
asu
re
usi
ng
de
pe
nd
en
cie
sg
en
era
ted
fro
m
tra
ini
ng
da
ta
NP
,N
E,
PR
P,
PR
P$
Sh
are
d
att
rib
ute
d/t
ran
sit
ivi
ty
by
usi
ng
av
irtu
al
pro
tot
yp
e
?
?
?
ko
bd
an
i
C
P
De
cis
ion
Tre
e
NP
(no
me
nti
on
of
PR
P$
)
Sta
rtw
ord
,E
nd
wo
rd
an
dH
ead
of
NP
?
Inf
orm
ati
on
ga
in
rat
io
Tra
in
zh
ou
C
P
SV
M
tre
ek
ern
el
usi
ng
BC
po
rtio
n
of
the
da
ta
Ru
le-
ba
sed
;F
ive
rul
es:
PR
P$
,P
RP
,N
E,
sm
all
est
NP
sub
sum
ing
NE
an
dD
ET
+N
P
Fu
llp
hra
se
?
?
17
Tra
in
+D
ev
ch
art
on
C
P
Mu
lti-
lay
er
pe
rce
ptr
on
Ru
les
ba
sed
on
PO
S,
NE
an
dfi
lte
ro
ut
ple
on
ast
ic
it
usi
ng
rul
e-b
ase
dfi
lte
r
Fu
llp
hra
se
?
?
22
Tra
in
ya
ng
C
P
Ma
xE
nt
(M
AL
LE
T)
NP
,P
RP
,P
RP
$,
pre
-m
od
ifie
rs
an
dv
erb
s
Fu
llp
hra
se
?
?
40
Tra
in
+D
ev
ha
o
C
P
Ma
xE
nt
NP
,P
RP
,P
RP
$,
VB
D
ful
lp
hra
se
?
?
Tra
in
+D
ev
xin
xin
C
P
IL
P/I
nfo
rm
ati
on
ga
in
NP
,P
RP
,P
RP
$
Fu
llp
hra
se
?
Inf
orm
ati
on
ga
in
rat
io
65
?
zh
an
g
C
P
SV
M
IO
B
cla
ssi
fic
ati
on
Fu
llp
hra
se
?
?
?
ku
mm
erfi
eld
C
P
Un
sup
erv
ise
dg
en
era
tiv
em
od
el
NP
,P
RP
,P
RP
$w
ith
ma
xim
al
spa
n
Fu
llp
hra
se
?
?
?
zh
ek
ov
a
C
P
TI
M
BL
me
mo
ry
ba
sed
lea
rne
r
NP
,P
rop
er
no
un
s,P
RP
,P
RP
$,
plu
sv
erb
wi
th
pre
dic
ate
lem
ma
He
ad
wo
rd
?
?
Tra
in
+D
ev
irw
in
C+
O
P
Cl
ass
ific
ati
on
-ba
sed
ran
ke
r
NP
,P
RP
,P
RP
$
Sh
are
da
ttri
bu
tes
?
?
?
Ta
ble
22
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rt
I.I
nt
he
Ta
sk
co
lum
n,
C/
O
rep
res
en
ts
wh
eth
er
the
sys
tem
pa
rtic
ipa
ted
in
the
cl
os
ed
,o
pe
n
or
bo
th
tra
ck
s.
In
the
Sy
nta
xc
olu
mn
,a
Pr
ep
res
en
ts
tha
tth
es
yst
em
su
sed
ap
hra
se
str
uc
tur
eg
ram
ma
rr
ep
res
en
tat
ion
of
syn
tax
,w
he
rea
sa
D
rep
res
en
ts
tha
tth
ey
use
da
de
pe
nd
en
cy
rep
res
en
tat
ion
.
22
Po
sit
ive
Tra
ini
ng
Ex
am
ple
s
Ne
ga
tiv
eT
rai
nin
gE
xa
mp
les
De
co
din
g
Pa
rse
Co
nfi
gu
rat
ion
lee
?
?
Mu
lti-
pa
ss
Sie
ve
s
sap
en
a
Al
lm
en
tio
np
air
sa
nd
lon
ge
ro
fn
est
ed
me
nti
on
sw
ith
co
mm
on
he
ad
ke
pt
Me
nti
on
pa
irs
wi
th
les
sth
an
thr
esh
old
(5)
nu
mb
er
of
dif
fer
en
ta
ttri
bu
te
val
ue
sa
re
co
nsi
de
red
(22
%
ou
to
f9
9%
ori
gin
al
are
dis
car
de
d)
Ite
rat
ive
1-b
est
ch
an
g
Cl
ose
sta
nte
ced
en
t
Al
lp
rec
ed
ing
me
nti
on
sin
au
nio
no
fo
fg
ol
d
an
dp
re
di
ct
ed
me
nti
on
s.
Me
nti
on
sw
he
re
the
firs
tis
pro
no
un
an
do
the
rn
ot
are
no
t
co
nsi
de
red
Be
stl
ink
an
dA
lll
ink
ss
tra
teg
y;
wi
th
an
d
wi
tho
ut
co
nst
rai
nts
?B
est
lin
kw
ith
ou
t
co
nst
rai
nts
wa
ss
ele
cte
df
or
the
offi
cia
lru
n
cai
We
igh
tsa
re
tra
ine
do
np
art
of
the
tra
ini
ng
da
ta
Re
cu
rsi
ve
2-w
ay
Sp
ect
ral
clu
ste
rin
g
(A
ga
rw
al,
20
05
)
nu
gu
es
Cl
ose
stA
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Cl
ose
st-
firs
tc
lus
ter
ing
for
pro
no
un
sa
nd
Be
st-
firs
tc
lus
ter
ing
for
no
n-p
ron
ou
ns
1-b
est
ury
up
ina
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
me
nti
on
pa
irm
od
el
wi
tho
ut
ran
kin
ga
sin
So
on
20
01
san
tos
Ex
ten
de
dv
ers
ion
of
So
on
(20
01
)w
he
re
in
ad
dit
ion
to
the
irs
tra
teg
y,
po
sit
ive
an
dn
ega
tiv
e
ex
am
ple
sf
rom
me
nti
on
sin
the
sen
ten
ce
of
the
clo
ses
tp
rec
ed
ing
an
tec
ed
en
ta
re
co
nsi
de
red
Lim
ite
dn
um
be
ro
fp
rec
ed
ing
me
nti
on
s6
0
for
au
tom
ati
ca
nd
40
giv
en
go
ld
bo
un
da
rie
s;
Ag
gre
ssi
ve
-m
erg
ec
lus
ter
ing
(M
cca
rth
ya
nd
Le
nh
ert
,1
99
5)
son
g
Pre
-cl
ust
er
pa
irm
od
els
sep
ara
te
for
eac
hp
air
NP
-N
P,
NP
-PR
Pa
nd
PR
P-P
RP
Pre
-cl
ust
ers
,w
ith
sin
gle
ton
pro
no
un
pre
-cl
ust
ers
,a
nd
use
clo
ses
t-fi
rst
clu
ste
rin
g.
Di
ffe
ren
tli
nk
mo
de
lsb
ase
do
nt
he
typ
eo
f
lin
kin
gm
en
tio
ns
?N
P-P
RP
,P
RP
-PR
Pa
nd
NP
-N
P
sto
ya
no
v
Sm
art
Pa
irG
en
era
tio
n(
Sm
art
PG
)w
he
re
the
typ
eo
fa
nte
ced
en
tis
de
ter
mi
ne
db
yt
he
typ
eo
f
an
ap
ho
ru
sin
ga
set
of
rul
es
Sin
gle
-lin
kc
lus
ter
ing
by
co
mp
uti
ng
tra
nsi
tiv
ec
los
ure
be
tw
een
pa
irw
ise
po
sit
ive
s.
sob
ha
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Pro
no
mi
na
l:a
llp
rec
ed
ing
NP
sin
the
sen
ten
ce
an
dp
rec
ed
ing
4s
en
ten
ces
kle
nn
er
?
?
Inc
rem
en
tal
en
tity
cre
ati
on
ko
bd
an
i
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
.T
hre
sho
ld
of
10
0w
ord
s
use
df
or
lon
gd
oc
um
en
ts
1-b
est
zh
ou
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
?
ch
art
on
Fro
m
the
en
do
fth
ed
oc
um
en
t,u
nti
la
n
an
tec
ed
en
tis
fou
nd
,o
r1
0m
en
tio
ns
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t
ML
Pw
ith
sco
re
of
0.5
use
df
or
lin
kin
ga
nd
10
me
nti
on
s
ya
ng
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Ma
xim
um
23
sen
ten
ces
to
the
lef
t;
Co
nst
rai
ne
dc
lus
ter
ing
ha
o
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
am
sea
rch
(L
uo
,2
00
4)
Pa
ck
ed
for
est
xin
xin
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Be
st-
firs
tc
lus
ter
ing
fol
low
ed
by
IL
P
op
tim
iza
tio
n
zh
an
g
Cl
ose
sta
nte
ced
en
t(S
oo
n,
20
01
)
Ne
ga
tiv
ee
xa
mp
les
in
be
tw
een
an
ap
ho
ra
nd
clo
ses
ta
nte
ced
en
t(S
oo
n,
20
01
)
Wi
nd
ow
of
10
0m
ark
ab
les
ku
mm
erfi
eld
?
?
Pre
-a
nd
po
st-
res
olu
tio
nfi
lte
rs
Gi
ve
n+
Be
rke
ley
pa
rse
rp
ars
es;
pa
rse
s
wi
tho
ut
NM
Ls
im
pro
ve
dp
erf
orm
an
ce
sli
gh
tly
;re
-tr
ain
ed
Be
rke
ley
pa
rse
r
zh
ek
ov
a
Ex
am
ple
sin
the
pa
stt
hre
es
en
ten
ces
Fro
m
las
tp
oss
ibl
em
en
tio
ni
nd
oc
um
en
t
irw
in
Cl
ust
er
qu
ery
wi
th
NU
LL
clu
ste
rfo
rd
isc
ou
rse
new
me
nti
on
s
Cl
ust
er-
ran
kin
ga
pp
roa
ch
(ra
hm
an
,2
00
9)
Ta
ble
23
:P
art
ici
pa
tin
gs
yst
em
pro
file
s?
Pa
rtI
I.T
his
foc
use
so
nt
he
wa
yp
osi
tiv
ea
nd
ne
ga
tiv
ee
xa
mp
les
we
re
ge
ne
rat
ed
an
dt
he
de
co
din
gs
tra
teg
yu
sed
.
23
OntoNotes corpus, and presented the results from an
evaluation on learning such unrestricted entities and
events in text. The following represent our conclu-
sions on reviewing the results:
? Perhaps the most surprising finding was that the
best-performing system (lee) was completely
rule-based, rather than trained. This suggests
that their rule-based approach was able to do
a more effective job of combining the multiple
sources of evidence than the trained systems.
The features for coreference prediction are cer-
tainly more complex than for many other lan-
guage processing tasks, which makes it more
challenging to generate effective feature com-
binations. The rule-based approach used by
the best-performing system seemed to benefit
from a heuristic that captured the most con-
fident links before considering less confident
ones, and also made use of the information in
the guidelines in a slightly more refined man-
ner than other systems. They also included ap-
positives and copular constructions in their cal-
culations. Although OntoNotes does not count
those as instances of IDENT coreference, using
that information may have helped their system
discover additional useful links.
? It is interesting to note that the developers of
the lee system also did the experiment of run-
ning their system using gold standard informa-
tion on the individual layers, rather than auto-
matic model predictions. The somewhat sur-
prising result was that using perfect informa-
tion for the other layers did not end up improv-
ing coreference performance much, if at all. It
is not clear whether this means that: i) Auto-
matic predictors for the individual layers are
accurate enough already; ii) Information cap-
tured by those supplementary layers actually
does not provide much leverage for resolving
coreference; or iii) researchers have yet have
found an effective way of capturing and utiliz-
ing the extra information provided by these lay-
ers.
? It does seem that collecting information about
an entity by merging information across the
various attributes of the mentions that comprise
it can be useful, though not all systems that at-
tempted this achieved a benefit.
? System performance did not seem to vary as
much across the different genres as is nor-
mally the case with language processing tasks,
which could suggest that coreference is rela-
tively genre insensitive, or it is possible that
scores are two low for the difference to be ap-
parent. Comparisons are difficult, however, be-
cause the spoken genres were treated here with
perfect speech recognition accuracy and perfect
speaker turn information. Under more realis-
tic application conditions, the spread in perfor-
mance between genres might be greater.
? It is noteworthy that systems did not seem to
attempt the kind of joint inference that could
make use of the full potential of various layers
available in OntoNotes, but this could well have
been owing to the limited time available for the
shared task.
? We had expected to see more attention paid to
event coreference, which is a novel feature in
this data, but again, given the time constraints
and given that events represent only a small
portion of the total, it is not surprising that most
systems chose not to focus on it.
? Scoring coreference seems to remain a signif-
icant challenge. There does not seem to be an
objective way to establish one metric in prefer-
ence to another in the absence of a specific ap-
plication. On the other hand, the system rank-
ings do not seem terribly sensitive to the par-
ticular metric chosen. It is interesting that both
versions of the CEAF metric ? which tries to
capture the goodness of the entities in the out-
put ? seem much lower than the other metric,
though it is not clear whether that means that
our systems are doing a poor job of creating
coherent entities or whether that metric is just
especially harsh.
Finally, it is interesting to note that the problem of
coreference does not seem to be following the same
kind of learning curve that we are used to with other
problems of this sort. While performance has im-
proved somewhat, it is not clear how far we will be
able to go given the strategies at hand, or whether
new techniques will be needed to capture additional
information from the texts or from world knowl-
edge. We hope that this corpus and task will provide
a useful resource for continued experimentation to
help resolve this issue.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
24
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
We would like to thank all the participants. Without
their hard work, patience and perseverance this eval-
uation would not have been a success. We would
also like to thank the Linguistic Data Consortium
for making the OntoNotes 4.0 corpus freely and
timely available to the participants. Emili Sapena,
who graciously allowed the use of his scorer
implementation, and made available enhancements
and immediately fixed issues that were uncovered
during the evaluation. Finally, we offer our special
thanks to Llu??s Ma`rquez and Joakim Nivre for their
wonderful support and guidance without which this
task would not have been successful.
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in synchronizing the English
treebank and propbank. In Workshop on Frontiers in
Linguistically Annotated Corpora 2006, July.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40, Sydney,
Australia, July.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the 11th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?10, pages 28?36.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the Second Meeting of North American Chapter
of the Association of Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, MI, June.
Nancy Chinchor and Beth Sundheim. 2003. Message
understanding conference (MUC) 6. In LDC2003T13.
Nancy Chinchor. 2001. Message understanding confer-
ence (MUC) 7. In LDC2001T02.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models for
coreference resolution. In HLT/NAACL, pages 81?88.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
HLT/NAACL.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87?96.
Charles Fillmore, Christopher Johnson, and Miriam R. L.
Petruck. 2003. Background to framenet. Interna-
tional Journal of Lexicography, 16(3).
G. G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassell, and R. Weischedel.
2000. The automatic content extraction (ACE)
program-tasks, data, and evaluation. In Proceedings
of LREC.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1?18, Boulder, Colorado, June.
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL.
L. Hirschman and N. Chinchor. 1997. Coreference task
definition (v3.0, 13 jul 97). In Proceedings of the Sev-
enth Message Understanding Conference.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT/NAACL,
pages 57?60, New York City, USA, June. Association
for Computational Linguistics.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2000. A large-scale classification of
english verbs. Language Resources and Evaluation,
42(1):21 ? 40.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
25
Methods in Natural Language Processing, pages 25?
32, Vancouver, British Columbia, Canada, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems (NIPS).
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Thomas S. Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, Octo-
ber.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the IJCAI.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
R. Passonneau. 2004. Computing reliability for corefer-
ence annotation. In Proceedings of LREC.
Massimo Poesio and Ron Artstein. 2005. The reliability
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. In Proceedings of the Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Massimo Poesio. 2004. The mate/gnome scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art nlp approaches to coreference resolu-
tion: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-
pore, August.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th
Meeting of the European Chapter of the Associa-
tion for Computational Linguistics, pages 143?146,
Trento, Italy, April.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the HLT/NAACL, pages 192?199, New York City,
N.Y., June.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal, 60(1):11?39.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. OntoNotes: A Unified Relational Semantic
Representation. International Journal of Semantic
Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b.
Unrestricted Coreference: Indentifying Entities and
Events in OntoNotes. In in Proceedings of the
IEEE International Conference on Semantic Comput-
ing (ICSC), September 17-19.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore, Au-
gust. Association for Computational Linguistics.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336).
Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden, July.
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun phrase.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
26
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model theoretic coreference
scoring scheme. In Proceedings of the Sixth Message
Undersatnding Conference (MUC-6), pages 45?52.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog
no.: LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
27
Proceedings of the First Workshop on Metaphor in NLP, pages 58?66,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Automatic Extraction of Linguistic Metaphor with LDA Topic Modeling 
Ilana Heintz*, Ryan Gabbard*, Mahesh Srinivasan+, *, David Barner+, Donald S. Black*, 
 Marjorie Freedman*, Ralph Weischedel*  
 
* Raytheon BBN Technologies 
10 Moulton St,  
Cambridge MA 02139 
 
{iheintz, rgabbard,  
mfreedman, dblack, 
rweischedel}@bbn.com 
+University of California, San Diego 
5336 McGill Hall,  
9500 Gilman Drive 
La Jolla, CA 92093-0109 
 
 barner@ucsd.edu,  
mahesh.srinivasan@gmail.com  
Abstract 
We aim to investigate cross-cultural patterns 
of thought through cross-linguistic investiga-
tion of the use of metaphor.  As a first step, 
we produce a system for locating instances of 
metaphor in English and Spanish text.  In con-
trast to previous work which relies on re-
sources like syntactic parsing and WordNet, 
our system is based on LDA topic modeling, 
enabling its application even to low-resource 
languages, and requires no labeled data.  We 
achieve an F-score of 59% for English. 
1 Introduction 
Patterns in the use of metaphors can provide a 
great deal of insight into a culture. Cultural differ-
ences expressed linguistically as metaphor can play 
a role in matters as complex and important as dip-
lomatic relations.  For instance, Thornborrow 
(1993) discusses the different metaphors that are 
used in the context of security in French and Brit-
ish coverage of two major post-cold-war summit 
meetings.  Example metaphors such as ?the corner-
stone of the new security structure,? ?structures for 
defence and security cooperation,? and ?the emerg-
ing shape of Europe,? exemplify the English use of 
the source concept structure in describing the tar-
get concept of security.  In contrast, the metaphors 
?des r?gles de s?curit? nouvelles (new rules of se-
curity)?, ?une r?vision fondamentale des disposi-
tions de s?curit? (a fundamental revision of 
security provisions)?, and ?un syst?me de s?curit? 
europ?en (a system of European security)? exem-
plify the French use of the more abstract source 
concept system to describe the same target concept.  
As Thornborrow notes, the implied British concep-
tion of security as ?concrete, fixed, and immobile? 
contrasts deeply with the French conception of se-
curity as ?a system as a series of processes.? 
Our ultimate goal is to use metaphor to further 
our knowledge of how different cultures under-
stand complex topics.  Our immediate goal in this 
paper is to create an automated system to find in-
stances of metaphor in English and Spanish text. 
Most existing work on metaphor identification 
(Fass, 1991; Martin, 1994; Peters and Peters, 2000; 
Mason, 2004; Birke and Sarkar, 2006; Gegigan et 
al., 2006; Krishnakumaran and Zhu, 2007; Shutova 
et  al., 2010; Shutova et al, 2012)1 has relied 
on some or all of handwritten rules, syntactic pars-
ing, and semantic databases like WordNet (Fell-
baum, 1998) and FrameNet (Baker et al, 1998).  
This limits the approaches to languages with rich 
linguistic resources.  As our ultimate goal is broad, 
cross-linguistic application of our system, we can-
not rely on resources which would be unavailable 
in resource-poor languages.  Instead, we apply 
LDA topic modeling (Blei et al, 2003b) which 
requires only an adequate amount of raw text in the 
target language.  This work is similar to Bethard et 
al. (2009), in which an SVM model is trained with 
LDA-based features to recognize metaphorical 
text. There the work is framed as a classification 
task, and supervised methods are used to label 
metaphorical and literal text.  Here, the task is one 
of recognition, and we use heuristic-based, unsu-
                                                          
1 See Shutova (2010) for a survey of existing approaches 
58
pervised methods to identify the presence of meta-
phor in unlabeled text. We hope to eliminate the 
need for labeled data which, as discussed in 
Bethard et al (2009) and elsewhere, is very diffi-
cult to produce for metaphor recognition. 
2 Terminology 
We will refer to a particular instance of metaphori-
cal language in text as a linguistic metaphor.  
Each such metaphor talks about a target concept 
in terms of a source concept.  For example, in 
?Dems, like rats, will attack when cornered? the 
source concept is animals and the target concept is 
politicians2, or at a higher level, governance.  The 
abstract mapping between a source concept and a 
target concept will be referred to as a conceptual 
metaphor which is grounded by a collection of 
linguistic metaphors. 
In this work, we restrict our attention to a single 
target concept, governance.  Our definition of gov-
ernance is broad, including views of the governed 
and those who govern, institutions of government, 
laws, and political discourse.  We used a large col-
lection (see Table 1) of potential source concepts.  
Beginning with the source concepts of primary 
metaphors, which are hypothesized to be univer-
sal (Grady, 1998), we expanded our set to include 
source concepts commonly found in the scientific 
literature about metaphor, as well as those found 
by human annotators manually collecting instances 
of governance-related metaphors. 
 
Animals Fishing Plants 
Baseball Flight Race 
Body Football Religion 
Botany Gambling Sick 
Boundary Grasp Size 
Chess Health Sound 
Color Height Sports 
Combustion Light Taste 
Cooking Liquid Temperature 
Courtship Machine Texture 
Cut Maritime Theater 
Directional force Money Time of day 
Dogs Motion Toxicity 
Drug use Mythology Vehicle 
Electricity Natural disasters War 
Energy source Nuclear Weaponry 
Entry Odor Weather 
                                                          
2 ?Dems?' refers to the Democratic Party, an American politi-
cal party 
Family Pathways Weight 
Farming Physical structure Wild west 
Fight Planning  
Table 1: English Source Concepts 
3 High-level system overview 
 
Figure 1: System Overview 
 
Our main hypothesis is that metaphors are likely to 
be found in sentences that exhibit evidence of both 
a source and a target concept.  The core idea of our 
system is to use LDA topics as proxies for seman-
tic concepts which may serve as the source or tar-
get for a metaphor.  For a given language, we build 
an LDA model from Wikipedia and then align its 
topics to potential source and target concepts, 
which are defined by small human-created lists of 
seed words. 
At runtime, the system first does LDA infer-
ence on our input corpus to get topic probabilities 
for each document and sentence.  The system then 
selects those sentences linked by LDA to both a 
source-aligned topic and a target-aligned topic.3 
For example, a sentence containing ??virtud so-
                                                          
3 This is a distant, automatic relative of the ?directed-search? 
technique of Martin (1994). 
59
cial para construir la democracia??4 will be se-
lected because LDA strongly associates it with 
both the topic [elecciones, ministro, sucesor, ?]5, 
aligned to the target concept governance, and the 
topic [edificio, arquitectura, torre,?] 6, aligned to 
the source concept physical structure.  
Next, the system identifies the words in each 
selected sentence that are strongly associated with 
each concept. In the sentence above, it marks vir-
tud and democracia as target-associated and con-
struir as source-associated. 
Next it applies two filters. First, we exclude any 
sentence with too few words that are not LDA 
stopwords, because the model's predictions may be 
very inaccurate in these cases.  Second, if the topic 
associated with the source model for a sentence is 
also a top-ranked topic for the document as a 
whole, the sentence is excluded.  The reason for 
this is that if the source concept is present through-
out the document, it is probably being used literal-
ly (see Figure 2). 
Finally, it uses previously-computed infor-
mation to determine a final score.  All linguistic 
metaphors scoring above a certain threshold are 
returned.  By varying this threshold, the user can 
vary the precision-recall tradeoff as needed. A dia-
gram of the system can be found in Figure 1. 
 
Figure 2: Even though the last sentence is relevant to the 
source concept pathways and the target concept govern-
ance, it will be correctly rejected because pathways-
aligned topics are present throughout the document. 
4 Implementation Details: Training 
Our runtime system requires as input an LDA 
model, a list of seed words for each concept, and 
an alignment between concepts and LDA topics. 
4.1 LDA Topic Model 
The topics defined by LDA topic modeling serve 
as stand-ins for the more abstractly-defined source 
and target concepts underlying the metaphors.  The 
input to training our LDA model is the full text of 
                                                          
4 social virtue to build democracy 
5 elections, minister, successor 
6 building, architecture, tower 
Wikipedia articles in the target language.  Wikipe-
dia is available in numerous languages and serves 
as a corpus of general knowledge, providing us 
with topics corresponding to a broad range of con-
cepts.  Our LDA model is trained using MALLET 
(McCallum, 2002) for 1000 iterations with 100 
topics, optimizing hyperparameters every 10 itera-
tions after a 100 iteration burn-in period. The 500 
most common tokens in the training corpus were 
used as stopwords. The result of LDA is 100 top-
ics, where each topic is a probability distribution 
over the training corpus vocabulary.  Representa-
tive words for example English topics are shown in 
Figure 3. 
 
Figure 3: Sample LDA topics with representative terms 
4.2 Concept Seed Word Lists 
For each concept  , we have a label and a small set 
of seed words representing that concept, referred to 
as     .  These lists were created by hand in Eng-
lish and then translated into Spanish by native 
speakers. The translation was not intended to be 
exact; we instructed the annotators to create the 
lists in a way that was appropriate for their lan-
guage and culture.  For instance, the football topic 
for English describes American football, but in 
Spanish, the same topic describes soccer. 
4.3 Concept-Topic Alignment 
The final input to our system is an alignment be-
tween concepts and topics, with every topic being 
mapped to at most one concept.  In addition to the 
seed lists and LDA model, this alignment process 
takes a score threshold        and a maximum 
number of alignments per source and target con-
cept   and  .  
The alignment algorithm is as follows. We 
align each topic   to the concept   with the maxi-
mum score       , which measures the concept 
terms? summed probability in the LDA topic: 
                      .  We remove all align-
ments where                . Finally, for each 
concept, only the   highest scoring alignments 
are kept, where   may be different for source and 
Our county has many roads in bad shape.  
Thousands of our bridges are structurally 
deficient.  Congress needs to pass a new 
highway bill. 
theater stage musical miss actreess 
theory philosophy pp study scientific 
knowledge 
nfl bowl yards coach players card yard 
governor republican senate election congress 
60
target. We refer to the aligned topics for a concept 
  as     . 
Label Seed List 
Words 
Aligned Topics 
Vehicle vehicle, 
wheels, gas, 
bus 
0.035: engine, car, 
model 
0.29: railway, 
trains, train 
0.022: energy, 
gas, linear 
Animals animal, beast, 
cattle 
0.066: animals, 
animal, species 
Courtship courtship, ro-
mance, court 
None 
Governance aristocrat, bi-
partisan, citi-
zen, duke 
0.25: Election, 
elected, parliament 
0.22: Governor, 
republican, Senate 
0.14: sir, lord,  
henry 
0.13: kingdom, 
emperor, empire 
0.12: rights, legal, 
laws 
Table 2: Sample concepts, manually-created seed lists, 
and aligned topics 
A last condition on the topic-concept alignment 
is the assignment of topics to trump concepts. Our 
only trump concept in this study is war. If an LDA 
topic is aligned with both the war concept and the 
governance concept, it is removed from alignment 
with the governance concept. We do this because 
war is so tightly associated with governments that 
the alignment algorithm invariably aligns it to the 
governance topic.  However, war is also a very 
important source concept for governance meta-
phors; our choice is to suffer on recall by missing 
some governance-relevant sentences, but increase 
recall on metaphors for which the source concept is 
war. Sample topic-concept alignments are shown 
inTable 2. By inspecting the resulting alignments 
by hand, we chose the following parameter values 
for both languages:       =0.01,  =3,  =5.   
The process of defining concepts is simple and 
fast and the alignment method is inexpensive.  
Therefore, while we have not captured all possible 
source concepts in our initial list, expanding this 
list is not difficult.  We can define new source con-
cepts iteratively as we analyze metaphors that our 
extraction system misses, and we can add target 
concepts as our interests broaden. 
5 Implementation Details: Runtime 
The system receives as input a corpus of docu-
ments, their LDA decodings, the LDA decodings 
of each sentence treated as a separate document, 
and the topic-concept alignments. Each four-tuple 
          is processed independently, where   is 
the language,   is the source concept,   is the tar-
get concept, and   is the sentence. 
 
Determining Concept Relevance: Recall our 
basic intuition that a sentence relevant both to an 
LDA topic in      (termed source-relevant) and 
one in      (termed target-relevant) is potentially 
metaphorical.  The system judges a sentence   to 
be  -relevant if the probability of  -aligned topics 
in that sentence is above a threshold:       
                      , where        is an ad-
justable parameter tuned by hand.         is 0.06 in 
English and 0.05 in Spanish.        is 0.1 in both 
languages. On the source side, the system removes 
all topics in      from        and renormalizes 
before determining relevance in order to avoid pe-
nalizing sentences for having very strong evidence 
of relevance to governance in addition to providing 
evidence of relevance to a source concept.  For 
reference below, let                    (a 
measure of how strongly the sentence is associated 
with its topics) and let 
                            (the most proba-
ble  -aligned topic in the sentence). 
If   is not both source- and target-relevant, the 
system stops and the sentence is not selected. 
 
Finding Concept-Associated Words: The system 
next creates sets    of the words in   associated 
with the concept  .  Let                   .  
Then let   
  {                   , where 
      is a hand tuned parameter set to 0.1 for both 
languages. That is, any word whose probability in 
the topic is higher than a theshold is included as a 
concept-associated word in that sentence.  Let 
               and vice-versa. Note that words 
which could potentially be associated with either 
concept are associated with neither.  For reference 
below, let                      (the most 
strongly concept-associated words in the sentence) 
61
and                    (the combined 
strength of those associations).  
If   lacks words strongly associated with the 
source and target concepts (that is,    or    is 
empty), the system stops and the sentence is not 
selected. 
Filters: The system applies two filters. First,   
must have at least four words which are not LDA 
stopwords; otherwise, the LDA predictions which 
drive the system's concept-relevance judgements 
tend to be unreliable.  Second, the most likely 
source topic       must not be one of the top 10 
topics for the document as a whole, for reasons 
described above.  If either of these requirements 
fail, the system stops and the sentence is not se-
lected. 
Final Scoring: Finally, the system determines 
if  
  (  (     )  (     )            )         
where        is a hand-tuned threshold set to -10.0 
for English and -13.0 for Spanish.  This takes into 
account the strength of association between topics 
and the sentence, between the annotated words and 
the topics, and between the topics and their aligned 
concepts.  Any sentence passing this threshold is 
selected as a linguistic metaphor. 
6 Example Output 
We provide examples of both true and false posi-
tives extracted by our system.  The annotations of 
source and target-associated words in each sen-
tence are those defined as    and    above.  The 
source concept animals is used for all examples. 
1. ModeratesT we all hear are an endangeredS 
speciesS, Sen. Richard 
2. DemsT like ratsS sometimes attack when cor-
nered 
3. ObamaT 's world historical political ambitions 
crossbredS with his 
4. At least DemocraticT representativesT are 
snakeheadS fish 
5. Another whopperS from Cleveland, GOPT 
lawyer backs him up 
6. Previous post: Illinois GOPT lawmakerT ar-
rested in animalS feed bag related incident 
7. Next post: National Enquirer catfighting 
Michelle ObamaT has clawsS out for that nice 
Ann Romney 
8. Sen. Lisa MurkowskiT R AK independent 
from Alaska - thank you silly Repubs, teaS 
party her out ha  
Examples 1 through 4 are correct metaphors ex-
tracted by our system.  In each, some words related 
to the target concept governance are described us-
ing terms related to the source concept animals.  
Example 1 best represents the desired output of our 
system, such that it contains a governance- and 
animals-relevant metaphor and the terms associat-
ed with the metaphor are properly annotated. Some 
issues do arise in these true positive examples. Ex-
ample 2, while often termed a simile, is counted as 
a metaphor for our purposes.  In example 3, the 
source term is correctly annotated, but the target 
terms should be political ambitions rather than  
Obama.  It is unclear why the term snakehead but 
not the term fish in example 4 is associated with 
the source concept.  
Examples 5 through 8 represent system errors.  
In example 5, the fact that the word whopper oc-
curs frequently to describe a large animal (espe-
cially a fish) causes the sentence to be mistakenly 
identified as relevant to the source concept animal.  
The source term animal in example 6 is clearly 
relevant to the source concept, but it is being used 
literally.  The document-level source concept fil-
tering does not entirely eliminate this error class.  
While example 7 contains a metaphor and has 
some relationship to American politics, it would be 
counted as an error in our evaluations because the 
metaphor itself is not related to governance. In ex-
ample 8, we have two errors. First, tea is strongly 
present in the topic aligned to the animal concept, 
causing the sentence to be incorrectly marked as 
source-relevant. Second, because our topic model 
operates at the level of individual words, it was 
unable to recognize that tea here is part of the 
fixed, governance-related phrase tea party. 7 
7 Evaluation 
7.1 Collecting Evaluation Data 
We collected a domain-specific corpus in each 
language.  We curated a set of news websites and 
governance-relevant blogs in English and Spanish 
and then collected data from these websites over 
the course of several months. For each language, 
we ran our system over this corpus (all steps in 
                                                          
7 an American political movement 
62
Section 5), produced a set of linguistic metaphors 
for each topic-aligned source concept (the target 
concept was always governance), and ranked them 
by the final score (Section 4.4). Below, we will 
refer to the set of all linguistic metaphors sharing 
the same source and target concept as a conceptual 
metaphor. 
7.2 Simple Evaluation 
For this evaluation, we selected the top five exam-
ples for each conceptual metaphor.  If the same 
sentence was selected by multiple conceptual met-
aphors, it was kept for only the highest scoring 
one.  We then added enough of the highest-ranked 
unselected metaphors to create a full set of 300. 
We then added random sentences from the corpus 
that were not selected as metaphorical by the sys-
tem to bring the total to 600.  Our Spanish annota-
tors were unavailable at the time this evaluation 
took place, so we are only able to report results for 
English in this case. 
For each of these instances, two annotators 
were asked the question, ?Is there a metaphor 
about governance in this example?? These annota-
tors had previous experience in identifying meta-
phors for this study, both by searching manually in 
online texts and evaluating previous versions of 
our system.  Over time we have given them feed-
back on what does and does not constitute a meta-
phor.  In this case, the annotators were given 
neither the system's concept-word association an-
notations nor the source concept associated with 
the instance.  In one way, the evaluation was gen-
erous, because any metaphor in the extracted sen-
tence would benefit precision even if it was not the 
metaphor found by our system. On the other hand, 
the same is true for the random sentences; while 
the system will only extract metaphors with source 
concepts in our list, the annotators had no such 
restriction. This causes the recall score to suffer.  
The annotation task was difficult, with a  -score of 
0.48.  The resulting scores are given in Table 3.   
The examples given in Section 5 illustrate the error 
classes found among the false positives identified 
by the human annotators. There are many cases 
where the source-concept associated terms are used 
literally rather than metaphorically, and many cas-
es where the system-found metaphor is not about 
governance.  Some text processing issues, such as 
a bug in our sentence breaking script, as well as the 
noisy nature of blog and blog comment input, 
caused some of the examples to be difficult to in-
terpret or evaluate.  
 
Annotator Precision ?Recall? F Kappa 
1 
2 
65 
43 
67 
60 
66 
50 
0.48 
Mean 54 64 59  
Table 3: Simple English Evaluation 
7.3 Stricter Evaluation 
Common Experimental Setup 
We did a second evaluation of both English and 
Spanish using a different paradigm.  For each lan-
guage, we selected the 250 highest-ranked linguis-
tic metaphor instances in the corpus.  Subjects on 
Amazon Mechanical Turk were shown instances 
with the system-predicted concept-associated 
words highlighted and asked if the highlighted 
words were being used metaphorically (options 
were yes and no).  Each subject was randomly 
asked about roughly a quarter of the data. 
 
We paid the subjects $10 per hour.  We added 
catch trial sentences which asked the subject to 
simply answer yes or no as a way of excluding 
those not actually reading the sentences.  Subjects 
answering these questions incorrectly were exclud-
ed (17 in English, 25 in Spanish).  
We defined the metaphoricity of an instance to 
be the fraction of subjects who answered yes for 
that instance. We define the metaphoricity of a 
conceptual metaphor as the average metaphoricity 
of its groundings among the instances in this eval-
uation set.  
 
  
63
English Results 
We restricted our subjects to those claiming to 
be native English speakers who had IP addresses 
within the U.S. and had 115 participants.  The ex-
amples were grouped into 66 conceptual meta-
phors. The mean metaphoricity of instances was 
0.41 (standard deviation=0.33).  The mean meta-
phoricity of the conceptual metaphors (Figure 4), 
was 0.39 (SD=0.26).  Although there was wide 
variance in metaphoricity across conceptual meta-
phors, it appears likely that most of the conceptual 
metaphors discovered by the system are correct: 
65% of the conceptual metaphors had metaphorici-
ty greater than 0.25, and 73% greater than 0.2. 
Given that many metaphors are conventional and 
difficult to detect in natural language (Lakoff and 
Johnson, 1980), it is possible that even in cases in 
which only a minority of subjects detected a meta-
phor, a metaphor nonetheless exists 
Spanish Results 
We restricted our subjects to those claiming to be 
native speakers of Mexican Spanish with IP ad-
dresses in the US (57) or Mexico (29).  The in-
stances were grouped into 52 conceptual meta-
phors.  The mean metaphoricity of instances was 
0.33 (SD=0.23) and for conceptual metaphors 
(Figure 4), 0.31 (SD=0.16). 60% of conceptual 
metaphors had metaphoricity greater than 0.25, and 
73% greater than 0.2.  That performance was only 
slightly lower than English is a positive indication 
of our method?s cross-linguistic potential. 
8 Discussion and Future Work 
We observed a number of problems with our ap-
proach which provide avenues for future research. 
8.1 Topics as Proxies of Primary Metaphor 
Concepts 
Many of the metaphors missed by our system were 
instances of primary metaphor, especially those 
involving movement and spatial position.  Our 
LDA approach is poorly suited to these because the 
source concepts are not well-characterized by word 
co-occurrence: words describing movement and 
spatial position do not have a strong tendency to 
co-occur with other such words, at least in Wik-
ipedia.  Augmenting our system with a separate 
 
 
 
 
Figure 4: Metaphoricity of Conceptual Metaphors for English (top) and Spanish (bottom) 
64
approach to primary metaphor would boost its per-
formance significantly. 
8.2 Topics as Proxies of Non-Primary Meta-
phor Concepts 
We found that most of our potential source con-
cepts did not correspond to any LDA topic. How-
ever, many of these, such as wild west, have fairly 
strong word co-occurrence patterns, so they plau-
sibly could be found by a different topic modeling 
algorithm.  There are two promising approaches 
here which could potentially be combined.  The 
first is to use a hierarchical LDA algorithm (Blei et 
al, 2003b) to allow concepts to align to topics with 
varying degrees of granularity, from the very gen-
eral (e.g. war) to the very specific (e.g. wild west).  
The second is to use constrained LDA approaches 
(Andrzejewski and Zhu, 2009; Hu et al, 2010) to 
attempt to force at least one topic to correspond to 
each of our seed concept lists.   
A different approach would leave behind seed 
lists entirely.  In our current approach, only about 
one third of the topics modeled by LDA are suc-
cessfully aligned with a source concept from our 
hand-made list.  However, some non-aligned LDA 
topics have properties similar to those that were 
chosen to represent source concepts.  For instance, 
the topic whose highest ranked terms are [institute, 
professor, engineering, degree] is comprised of a 
set of semantically coherent and concrete terms, 
and could be assigned a reasonably accurate label 
such as higher education.  If we were to choose 
LDA topics based on the terms? coherence and 
concreteness (and perhaps other relevant, measura-
ble properties), then assign a label using a method 
such as that in Mei et al (2007), we would be able 
to leverage more of the concepts in the LDA mod-
el. This would increase the recall of our system, 
and also reduce some of the confusion associated 
with incorrect labeling of concepts in linguistic and 
conceptual metaphors.  Applying Labeled LDA, as 
in Ramage et al (2009), would be a similar ap-
proach. 
8.3 Confusion of Literal and Metaphorical 
Usage of Source Concepts 
Another major problem was the confusion between 
literal and metaphorical usage of source terms.  
This is partly addressed by our document topics 
filter, but more sophisticated use of document con-
text for this purpose would be helpful.  A similar 
filter based on contexts across the test corpus 
might be useful. 
8.4 Fixed Expressions 
Some of our errors were due to frequent fixed 
phrases which included a word strongly associated 
with a source topic, like Tea Party.  Minimum de-
scription length (MDL) phrase-finding or similar 
techniques could be used to filter these out.  Initial 
experiments performed after the evaluations dis-
cussed above show promise in this regard. Using 
the MDL algorithm (Rissanen, 1978), we devel-
oped a list of likely multi-word expressions in the 
Wikipedia corpus.  We then concatenated these 
phrases in the Wikipedia corpus before LDA mod-
eling and in the test corpus before metaphor pre-
diction.  Though we did not have time to formally 
evaluate the results, a subjective analysis showed 
fewer of these fixed phrases appearing as indica-
tors of metaphor (as words in    or   ). 
8.5 Difficulty of Annotation 
A different method of presentation of metaphors to 
the subjects, for instance with annotations marking 
where in the sentence we believed metaphor to 
exist or with a suggestion of the source concept, 
may have improved agreement and perhaps the 
system?s evaluation score. 
8.6 Summary 
We have presented a technique for linguistic and 
conceptual metaphor discovery that is cross-
linguistically applicable and requires minimal lin-
guistic resources.  Our approach of looking for 
overlapping semantic concepts allows us to find 
metaphors of any syntactic structure.  The frame-
work of our metaphor discovery technique is flexi-
ble in its ability to incorporate a wide variety of 
source and target concepts. The only linguistic re-
sources the system requires are a corpus of gen-
eral-knowledge text adequate for topic modeling 
and a small set of seed word lists. We could im-
prove our system by applying new research in au-
tomatic topic modeling, by creating new filters and 
scoring mechanisms to discriminate between literal 
and figurative word usages, and by creating train-
ing data to allow us to automatically set certain 
system parameters.   
 
65
Acknowledgements 
Supported by the Intelligence Advanced Research Pro-
jects Activity (IARPA) via Department of Defense US 
Army Research Laboratory contract number W911NF-
12-C0-0023. The U.S. Government is authorized to 
reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation there-
on.  Disclaimer: The views and conclusions contained 
herein are those of the authors and should not be inter-
preted as necessarily representing the official policies or 
endorsements, either expressed or implied, of IARPA, 
DoD/ARL, or the U.S. Government.? 
References 
David Andrzejewski and Xiaojin Zhu. 2009. Latent Di-
richlet Allocation with Topic-in-Set Knowledge. In 
Proceedings of NAACL Workshop on Semi-
Supervised Learning for NLP. 
Collin Baker, Charles Fillmore, and John Lowe. 1998. 
The Berkeley FrameNet project. In Proceedings of 
COLING-ACL. 
Stephen Bethard, Vicky Tzuyin Lai and James H. Mar-
tin.  2009. Topic Model Analysis of Metaphor Fre-
quency for Psycholinguistic Stimuli.  .  In Proc. Of 
NAACL-HLT Workshop on Computational Ap-
proaches to Linguistic Creativity. 
Julia Birke and Anoop Sarkar. 2006. A Clustering Ap-
proach for the Nearly Unsupervised Recognition of 
Nonliteral Language. In Proceedings of EACL. 
David Blei, Thomas Griffiths, Michael Jordan, and 
Joshua Tenenbaum. 2003a. Hierarchical topic models 
and the nested Chinese restaurant process. In Pro-
ceedings of NIPS. 
David Blei, Andrew Ng, and Michael Jordan. 2003b. 
Latent Dirichlet Allocation. Journal of Machine 
Learning Research, 2003(3):993?1022. 
Dan Fass. 1991. met*: A Method for Discriminating 
Metonymy and Metaphor by Computer. Computa-
tional Linguistics, 17(1):49?90. 
Christine Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA. 
Matt Gegigan, John Bryant, Srini Narayanan, and 
Branimir Ciric. 2006. Catching Metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural 
Language Understanding. 
Joseph E. Grady. 1998. Foundations of meaning: Prima-
ry metaphors and primary scenes. UMI. 
Yuenin Hu, Jordan Boyd-Graber, and Brianna Satinoff. 
2010. Interactive Topic Modeling. In Proceedings of 
ACL. 
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. 
Hunting Elusive Metaphors Using Lexical Re-
sources. In Proceedings of the Workshop on Compu-
tational Approaches to Figurative Language. 
George Lakoff and Mark Johnson. 1980. Metaphors We 
Live By. University of Chicago. 
James H. Martin. 1994. MetaBank: A knowledge-base 
of metaphoric language convention. Computational 
Intelligence, 10(2):134?149. 
Zachary Mason. 2004. CorMet: A Computational, Cor-
pus-Based Conventional Metaphor Extraction Sys-
tem. Computational Linguistics, 30(1):23?44. 
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit. 
http://mallet.cs.umass.edu. 
Qiaozhu Mei, Xuehua Shen, and Chengxiang Zhai.  
Automatic Labeling of Multinomial Topic Models.  
In Proceedings of KDD ?07.  2007. 
Wim Peters and Ivonne Peters. 2000. Lexicalised Sys-
tematic Polysemy in WordNet. In Proceedings of 
LREC. 
Daniel Ramage, David Hall, Ramesh Nallapati and 
Christopher D. Manning. 2009. Labeled LDA: A su-
pervised topic model for credit attribution in multi-
labeled corpora.  In Proceedings of EMNLP. 
Jorma Rissanen.  Modeling by shortest data description. 
Automatica 14:465-471. 
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010. 
Metaphor Identification Using Noun and Verb Clus-
tering. In Proceedings of COLING. 
Ekaterina Shutova, Simone Teufel, and Anna Korhonen. 
2012. Statistical Metaphor Processing. Computation-
al Linguistics. Uncorrected proof. 
Ekaterina Shutova. 2010. Models of metaphor in NLP. 
In Proceedings of ACL. 
Joanna Thornborrow. 1993. Metaphors of security: a 
comparison of representation in defence discourse in 
post-cold-war France and Britain. Discource & Soci-
ety, 4(1):99?119 
 
 
66
