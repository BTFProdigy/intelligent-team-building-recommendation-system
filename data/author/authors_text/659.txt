Assigning Function Tags to Parsed Text* 
Don B laheta  and  Eugene Charn iak  
{dpb, ec}@cs, brown, edu 
Department of Computer Science 
Box 1910 / 115 Waterman St . - -4th  floor 
Brown University 
Providence, RI 02912 
Abst rac t  
It is generally recognized that the common on- 
terminal abels for syntactic constituents (NP, 
VP, etc.) do not exhaust he syntactic and se- 
mantic information one would like about parts 
of a syntactic tree. For example, the Penn Tree- 
bank gives each constituent zero or more 'func- 
tion tags' indicating semantic roles and other 
related information ot easily encapsulated in 
the simple constituent labels. We present a sta- 
tistical algorithm for assigning these function 
tags that, on text already parsed to a simple- 
label level, achieves an F-measure of 87%, which 
rises to 99% when considering 'no tag' as a valid 
choice. 
1 In t roduct ion  
Parsing sentences using statistical information 
gathered from a treebank was first examined a 
decade ago in (Chitrao and Grishman, 1990) 
and is by now a fairly well-studied problem 
((Charniak, 1997), (Collins, 1997), (Ratna- 
parkhi, 1997)). But to date, the end product of 
the parsing process has for the most part been 
a bracketing with simple constituent labels like 
NP, VP, or SBAR. The Penn treebank contains a 
great deal of additional syntactic and seman- 
tic information from which to gather statistics; 
reproducing more of this information automat- 
ically is a goal which has so far been mostly 
ignored. This paper details a process by which 
some of this information--the function tags-- 
may be recovered automatically. 
In the Penn treebank, there are 20 tags (fig- 
ure 1) that can be appended to constituent la- 
bels in order to indicate additional information 
about the syntactic or semantic role of the con- 
* This research was funded in part by NSF grants LIS- 
SBR-9720368 and IGERT-9870676. 
stituent. We have divided them into four cate- 
gories (given in figure 2) based on those in the 
bracketing uidelines (Bies et al, 1995). A con- 
stituent can be tagged with multiple tags, but 
never with two tags from the same category. 1 
In actuality, the case where a constituent has 
tags from all four categories never happens, but 
constituents with three tags do occur (rarely). 
At a high level, we can simply say that hav- 
ing the function tag information for a given text 
is useful just because any further information 
would help. But specifically, there are distinct 
advantages for each of the various categories. 
Grammatical tags are useful for any application 
trying to follow the thread of the text--they find 
the 'who does what' of each clause, which can 
be useful to gain information about the situa- 
tion or to learn more about the behaviour of 
the words in the sentence. The form/function 
tags help to find those constituents behaving in 
ways not conforming to their labelled type, as 
well as further clarifying the behaviour of ad- 
verbial phrases. Information retrieval applica- 
tions specialising in describing events, as with a 
number of the MUC applications, could greatly 
benefit from some of these in determining the 
where-when-why of things. Noting a topicalised 
constituent could also prove useful to these ap- 
plications, and it might also help in discourse 
analysis, or pronoun resolution. Finally, the 
'miscellaneous' tags are convenient at various 
times; particularly the CLI~ 'closely related' tag, 
which among other things marks phrasal verbs 
and prepositional ditransitives. 
To our knowledge, there has been no attempt 
so far to recover the function tags in pars- 
ing treebank text. In fact, we know of only 
1There is a single exception i the corpus: one con- 
stituent is tagged with -LOC-I~R. This appears to be an 
error. 
234 
ADV Non-specific adverbial 
BNF Benefemtive 
CLF It-cleft 
CLR 'Closely related' 
DIR Direction 
DTV Dative 
EXT Extent 
HLN Headline 
LGS Logical subject 
L0C Location 
MNI~ Manner 
N0M Nominal 
PRD Predicate 
PRP Purpose 
PUT Locative complement of 'put' 
SBJ Subject 
TMP Temporal 
TPC Topic 
TTL Title 
V0C Vocative 
Grammatical 
DTV 0.48% 
LGS 3.0% 
PRD 18.% 
PUT 0.26% 
SBJ 78.% 
v0c 0.025% 
Figure 1: Penn treebank function tags 
53.% Form/Function 37.% Topicalisation 2.2% 
0.25% NOM 6.8% 2.5% TPC 100% 2.2% 
1.5% ADV 11.% 4.2% 
9.3% BN'F 0.072% 0.026% 
0.13% DIR 8.3% 3.0% 
41.% EXT 3.2% 1.2% 
0.013% LOC 25.% 9.2% 
MNR 6.2% 2.3% 
PI~ 5.2% 1.9% 
33.% 12.% 
Miscellaneous 9.5% 
CLR 94.% 8.8% 
CLF 0 .34% 0.03% 
HLN 2.6% 0.25% 
TTL 3.1% 0.29% 
Figure 2: Categories of function tags and their relative frequencies 
one project that used them at all: (Collins, 
1997) defines certain constituents as comple- 
ments based on a combination of label and func- 
tion tag information. This boolean condition is 
then used to train an improved parser. 
2 Features  
We have found it useful to define our statisti- 
cal model in terms of features. A 'feature', in 
this context, is a boolean-valued function, gen- 
erally over parse tree nodes and either node la- 
bels or lexical items. Features can be fairly sim- 
ple and easily read off the tree (e.g. 'this node's 
label is X', 'this node's parent's label is Y'), or 
slightly more complex ('this node's head's part- 
of-speech is Z'). This is concordant with the us- 
age in the maximum entropy literature (Berger 
et al, 1996). 
When using a number of known features to 
guess an unknown one, the usual procedure is
to calculate the value of each feature, and then 
essentially look up the empirically most proba- 
ble value for the feature to be guessed based on 
those known values. Due to sparse data, some 
of the features later in the list may need to be 
ignored; thus the probability of an unknown fea- 
ture value would be estimated as 
P(flYl, ? ?, Y,) 
P ( f l f l ,  f2 , . . . , f j ) ,  j < n ,  (1) 
where/3 refers to an empirically observed prob- 
ability. Of course, if features 1 through i only 
co-occur a few times in the training, this value 
may not be reliable, so the empirical probability 
is usually smoothed: 
P(flf l ,  Ii) 
AiP(flfl, fa , . . . ,  fi) 
+ (2) 
The values for )~i can then be determined ac- 
cording to the number of occurrences of features 
1 through i together in the training. 
One way to think about equation 1 (and 
specifically, the notion that j will depend on 
the values of f l . . .  fn) is as follows: We begin 
with the prior probability of f .  If we have data 
indicating P(flfl), we multiply in that likeli- 
hood, while dividing out the original prior. If 
we have data for /3(f l f l ,  f2), we multiply that 
in while dividing out the P(flfl) term. This is 
repeated for each piece of feature data we have; 
at each point, we are adjusting the probability 
235 
P(flfl,f2,... ,fn) p(/) P(SlA) P(SlSl, S:) 
P(f) P(f lf l)  
P(flfl,..., Yi-1, A) 
-,_-o " p- ff, 
P(flft, $2,..., f~) 
P(flA, A,... ,f?-x) 
j<n  
(3) 
we already have estimated. If knowledge about 
feature fi makes S more likely than with just 
f l . . .  fi-1, the term where fi is added will be 
greater than one and the running probability 
will be adjusted upward. This gives us the new 
probability shown in equation 3, which is ex- 
actly equivalent to equation 1 since everything 
except the last numerator cancels out of the 
equation. The value of j is chosen such that 
features f l . . - f j  are sufficiently represented in 
the training data; sometimes all n features are 
used, but often that would cause sparse data 
problems. Smoothing isperformed on this equa- 
tion exactly as before: each term is interpolated 
between the empirical value and the prior esti- 
mated probability, according to a value of Ai 
that estimates confidence. But aside from per- 
haps providing a new way to think about the 
problem, equation 3 is not particularly useful 
as it is--it is exactly the same as what we had 
before. Its real usefulness comes, as shown in 
(Charniak, 1999), when we move from the no- 
tion of a feature chain to a feature tree. 
These feature chains don't capture verything 
we'd like them to. If there are two independent 
features that are each relatively sparse but occa- 
sionally carry a lot of information, then putting 
one before the other in a chain will effectively 
block the second from having any effect, since 
its information is (uselessly) conditioned on the 
first one, whose sparseness will completely di- 
lute any gain. What we'd really like is to be able 
to have a feature tree, whereby we can condition 
those two sparse features independently on one 
common predecessor feature. As we said be- 
fore, equation 3 represents, for each feature fi, 
the probability of f based on fi and all its pre- 
decessors, divided by the probability of f based 
only on the predecessors. In the chain case, this 
means that the denominator is conditioned on 
every feature from 1 to i - 1, but if we use a 
feature tree, it is conditioned only on those fea- 
tures along the path to the root of the tree. 
A notable issue with feature trees as opposed 
to feature chains is that the terms do not all 
cancel out. Every leaf on the tree will be repre- 
target ~ 
feature 
Figure 3: A small example feature tree 
sented in the numerator, and every fork in the 
tree (from which multiple nodes depend) will 
be represented at least once in the denomina- 
tor. For example: in figure 3 we have a small 
feature tree that has one target feature and four 
conditioning features. Features b and d are in- 
dependent ofeach other, but each depends on a; 
c depends directly only on b. The unsmoothed 
version of the corresponding equation would be 
P(fla, b, c, d) ,~ 
p ,~ P(fla) ~)(f\]a, b) P(f\[a, b, c) P(fla, d) 
which, after cancelling of terms and smoothing, 
results in 
P(fla, b, c, d) P(fla, b, c)P(fla, d) 
P(fla) (4) 
Note that strictly speaking the result is not a 
probability distribution. It could be made into 
one with an appropriate normalisation--the 
so-called partition function in the maximum- 
entropy literature. However, if the indepen- 
dence assumptions made in the derivation of 
equation 4 are good ones, the partition func- 
tion will be close to 1.0. We assume this to be 
the case for our feature trees. 
Now we return the discussion to function tag- 
ging. There are a number of features that seem 
236 
function 
tag label 
succeeding preceding 
, , . / -d~e l  laf)el 
pare_p~ 
gra-'n~arent's parent's 
label head's POS 
grandparent's 
h ~ P O S  
headS~ parent's 
P ~ e a d  
head 
alt-head's 
POs alt-~ead 
Figure 4: The feature tree used to guess function tags 
to condition strongly for one function tag or an- 
other; we have assembled them into the feature 
tree shown in figure 4. 2 This figure should be 
relatively self-explanatory, except for the notion 
of an 'alternate head'; currently, an alternate 
head is only defined for prepositional phrases, 
and is the head of the object of the preposi- 
tional phrase. This data is very important in 
distinguishing, for example, 'by John' (where 
John might be a logical subject) from 'by next 
year' (a temporal modifier) and 'by selling it' 
(an adverbial indicating manner). 
3 Exper iment  
In the training phase of our experiment, we 
gathered statistics on the occurrence of func- 
tion tags in sections 2-21 of the Penn treebank. 
Specifically, for every constituent in the tree- 
bank, we recorded the presence of its function 
tags (or lack thereof) along with its condition- 
ing information. From this we calculated the 
empirical probabilities of each function tag ref- 
erenced in section 2 of this paper. Values of )~ 
were determined using EM on the development 
corpus (treebank section 24). 
To test, then, we simply took the output of 
our parser on the test corpus (treebank section 
23), and applied a postprocessing step to add 
function tags. For each constituent in the tree, 
we calculated the likelihood of each function tag 
according to the feature tree in figure 4, and 
for each category (see figure 2) we assigned the 
most likely function tag (which might be the 
null tag). 
2The reader will note that  the ' features'  l isted in the 
tree are in fact not  boolean-valued; each node in the 
given tree can be assumed to s tand for a chain of boolean 
features, one per potent ia l  value at  that  node, exact ly 
one of which will be true. 
4 Eva luat ion  
To evaluate our results, we first need to deter- 
mine what is 'correct'. The definition we chose 
is to call a constituent correct if there exists in 
the correct parse a constituent with the same 
start and end points, label, and function tag 
(or lack thereof). Since we treated each of the 
four function tag categories as a separate fea- 
ture for the purpose of tagging, evaluation was 
also done on a per-category basis. 
The denominator of the accuracy measure 
should be the maximum possible number we 
could get correct. In this case, that means 
excluding those constituents hat were already 
wrong in the parser output; the parser we used 
attains 89% labelled precision-recall, so roughly 
11% of the constituents are excluded from the 
function tag accuracy evaluation. (For refer- 
ence, we have also included the performance of
our function tagger directly on treebank parses; 
the slight gain that resulted is discussed below.) 
Another consideration is whether to count 
non-tagged constituents in our evaluation. On 
the one hand, we could count as correct any 
constituent with the correct tag as well as any 
correctly non-tagged constituent, and use as 
our denominator the number of all correctly- 
labelled constituents. (We will henceforth refer 
to this as the 'with-null' measure.) On the other 
hand, we could just count constituents with the 
correct tag, and use as our denominators the 
total number of tagged, correctly-labelled con- 
stituents. We believe the latter number ('no- 
null') to be a better performance metric, as it 
is not overwhelmed by the large number of un- 
tagged constituents. Both are reported below. 
237 
Category 
Grammatical 
Form/Function 
Topicalisation 
Miscellaneous 
Overall 
Table 1: Baseline performance 
Baseline 1 
(never tag) Tag Precision 
86.935% SBJ 10.534% 
91.786% THP 3.105% 
99.406% TPC 0.594% 
98.436% CLR 1.317% 
94.141% - -  3.887% 
Baseline 2 (always choose most likely tag) 
Recall F-measure 
80.626% 18.633% 
37.795% 5.738% 
100.00% 1.181% 
84.211% 2.594% 
66.345% 7.344% 
Table 2: Performance within each category 
With-null - - -No-nu l l - -  
Category Accuracy Precision Recall F-measure 
Grammatical 98.909% 95.472% 95.837% 95.654% 
Form/Function 97.104% 80.415% 77.595% 78.980% 
Topicalisation 99.915% 92.195% 93.564% 92.875% 
Miscellaneous 98.645% 55.644% 65.789% 60.293% 
5 Resu l ts  
5.1 Base l ines  
There are, it seems, two reasonable baselines 
for this and future work. First of all, most con- 
stituents in the corpus have no tags at all, so 
obviously one baseline is to simply guess no tag 
for any constituent. Even for the most com- 
mon type of function tag (grammatical), this 
method performs with 87% accuracy. Thus the 
with-null accuracy of a function tagger needs to 
be very high to be significant here. 
The second baseline might be useful in ex- 
amining the no-null accuracy values (particu- 
larly the recall): always guess the most common 
tag in a category. This means that every con- 
stituent gets labelled with '-SBJ-THP-TPC-CLR' 
(meaning that it is a topicalised temporal sub- 
ject that is 'closely related' to its verb). This 
combination of tags is in fact entirely illegal 
by the treebank guidelines, but performs ad- 
equately for a baseline. The precision is, of 
course, abysmal, for the same reasons the first 
baseline did so well; but the recall is (as one 
might expect) substantial. The performances 
of the two baseline measures are given in Table 
1. 
5.2 Per fo rmance  in ind iv idua l  
categor ies 
In table 2, we give the results for each category. 
The first column is the with-null accuracy, and 
the precision and recall values given are the no- 
null accuracy, as noted in section 4. 
Grammatical tagging performs the best of the 
four categories. Even using the more difficult 
no-null accuracy measure, it has a 96% accu- 
racy. This seems to reflect the fact that gram- 
matical relations can often be guessed based on 
constituent labels, parts of speech, and high- 
frequency lexical items, largely avoiding sparse- 
data problems. Topicalisation can similarly be 
guessed largely on high-frequency information, 
and performed almost as well (93%). 
On the other hand, we have the 
form/function tags and the 'miscellaneous' 
tags. These are characterised by much more 
semantic information, and the relationships 
between lexical items are very important, 
making sparse data a real problem. All the 
same, it should be noted that the performance 
is still far better than the baselines. 
5.3 Per fo rmance  w i th  o ther  feature  
trees 
The feature tree given in figure 4 is by no means 
the only feature tree we could have used. In- 
238 
Table 3: Overall performance on different inputs 
With-null - -No-nu l l -  
Category Accuracy Precision Recall F-measure 
Parsed 98.643% 87.173% 87.381% 87.277% 
Treebank 98.805% 88.450% 88.493% 88.472% 
deed, we tried a number of different rees on the 
development corpus; this tree gave among the 
best overall results, with no category perform- 
ing too badly. However, there is no reason to 
use only one feature tree for all four categories; 
the best results can be got by using a separate 
tree for each one. One can thus achieve slight 
(one to three point) gains in each category. 
5.4 Overal l  per fo rmance  
The overall performance, given in table 3, ap- 
pears promising. With a tagging accuracy of 
about 87%, various information retrieval and 
knowledge base applications can reasonably ex- 
pect to extract useful information. 
The performance given in the first row is (like 
all previously given performance values) the 
function-tagger's performance on the correctly- 
labelled constituents output by our parser. For 
comparison, we also give its performance when 
run directly on the original treebank parse; since 
the parser's accuracy is about 89%, working di- 
rectly with the treebank means our statistics 
are over roughly 12% more constituents. This 
second version does slightly better. 
The main reason that tagging does worse on 
the parsed version is that although the con- 
stituent itself may be correctly bracketed and la- 
belled, its exterior conditioning information can 
still be incorrect. An example of this that ac- 
tually occurred in the development corpus (sec- 
tion 24 of the treebank) is the 'that' clause in 
the phrase 'can swallow the premise that the re- 
wards for such ineptitude are six-figure salaries', 
correctly diagrammed in figure 5. The function 
tagger gave this SBAR an ADV tag, indicating an 
unspecified adverbial function. This seems ex- 
tremely odd, given that its conditioning infor- 
mation (nodes circled in the figure) clearly show 
that it is part of an NP, and hence probably mod- 
ifies the preceding NN. Indeed, the statistics give 
the probability of an ADV tag in this condition- 
ing environment as vanishingly small. 
vP 
the ( premise ) ~  
Figure 5: SBAR and conditioning info 
the premise ~ ... 
Figure 6: SBAR and conditioning info, as parsed 
However, this was not the conditioning infor- 
mation that the tagger received. The parser 
had instead decided on the (incorrect) parse in 
figure 6. As such, the tagger's decision makes 
much more sense, since an SBAR under two VPs 
whose heads are VB and MD is rather likely to be 
an ADV. (For instance, the 'although' clause of 
the sentence 'he can help, although he doesn't 
want to.' has exactly the conditioning environ- 
ment given in figure 6, except that its prede- 
cessor is a comma; and this SBAR would be cor- 
rectly tagged ADV.) The SBAR itself is correctly 
bracketed and labelled, so it still gets counted 
in the statistics. Happily, this sort of case seems 
to be relatively rare. 
239 
Another thing that lowers the overall perfor- 
mance somewhat is the existence of error and in- 
consistency in the treebank tagging. Some tags 
seem to have been relatively easy for the human 
treebank taggers, and have few errors. Other 
tags have explicit caveats that, however well- 
justified, proved difficult to remember for the 
taggers--for instance, there are 37 instances of 
a PP being tagged with LGS (logical subject) in 
spite of the guidelines specifically saying, '\[LGS\] 
attaches to the NP object of by and not to the 
PP node itself.' (Bies et al, 1995) Each mistag- 
ging in the test corpus can cause up to two spu- 
rious errors, one in precision and one in recall. 
Still another source of difficulty comes when the 
guidelines are vague or silent on a specific issue. 
To return to logical subjects, it is clear that 'the 
loss' is a logical subject in 'The company was 
hurt by the loss', but what about in 'The com- 
pany was unperturbed by the loss' ? In addition, 
a number of the function tags are authorised for 
'metaphorical use', but what exactly constitutes 
such a use is somewhat inconsistently marked. 
It is as yet unclear just to what degree these 
tagging errors in the corpus are affecting our 
results. 
6 Conc lus ion  
This work presents a method for assigning func- 
tion tags to text that has been parsed to the 
simple label level. Because of the lack of prior 
research on this task, we are unable to com- 
pare our results to those of other researchers; 
but the results do seem promising. However, a 
great deal of future work immediately suggests 
itself: 
? Although we tested twenty or so feature 
trees besides the one given in figure 4, the 
space of possible trees is still rather un- 
explored. A more systematic investiga- 
tion into the advantages ofdifferent feature 
trees would be useful. 
? We could add to the feature tree the val- 
ues of other categories of function tag, or 
the function tags of various tree-relatives 
(parent, sibling). 
? One of the weaknesses of the lexical fea- 
tures is sparse data; whereas the part of 
speech is too coarse to distinguish 'by John' 
(LGS) from 'by Monday' (TMP), the lexi- 
cal information may be too sparse. This 
could be assisted by clustering the lexical 
items into useful categories (names, dates, 
etc.), and adding those categories as an ad- 
ditional feature type. 
? There is no reason to think that this work 
could not be integrated irectly into the 
parsing process, particularly if one's parser 
is already geared partially or entirely to- 
wards feature-based statistics; the func- 
tion tag information could prove quite use- 
ful within the parse itself, to rank several 
parses to find the most plausible. 
Re ferences  
Adam L. Berger, Stephen A. Della Pietra, 
and Vincent J. Della Pietra. 1996. A 
maximum entropy approach to natural lan- 
guage processing. Computational Linguistics, 
22(1):39-71. 
Ann Bies, Mark Ferguson, Karen Katz, and 
Robert MacIntyre, 1995. Bracketing Guide- 
lines for Treebank H Style Penn Treebank 
Project, January. 
Eugene Charniak. 1997. Statistical pars- 
ing with a context-free grammar and word 
statistics. In Proceedings of the Fourteenth 
National Conference on Artificial Intelli- 
gence, pages 598-603, Menlo Park. AAAI 
Press/MIT Press. 
Eugene Charniak. 1999. A maximum-entropy- 
inspired parser. Technical Report CS-99-12, 
Brown University, August. 
Mahesh V. Chitrao and Ralph Grishman. 1990. 
Statistical parsing of messages. In DARPA 
Speech and Language Workshop, pages 263- 
266. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
ceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics, 
pages 16-23. 
Adwait Ratnaparkhi. 1997. A linear observed 
time statistical parser based on maximum en- 
tropy models. In Proceedings of the Second 
Annual Conference on Empirical Methods in 
Natural Language Processing, pages 1-10. 
240 
Handling noisy training and testing data
Don Blaheta
Department of Computer Science
Brown University
dpb@cs.brown.edu
Abstract
In the eld of empirical natural language
processing, researchers constantly deal with
large amounts of marked-up data; whether
the markup is done by the researcher or
someone else, human nature dictates that it
will have errors in it. This paper will more
fully characterise the problem and discuss
whether and when (and how) to correct the
errors. The discussion is illustrated with
specic examples involving function tagging
in the Penn treebank.
1 Introduction: Errors
Nobody?s perfect. A cliche, but in the eld of empir-
ical natural language processing, we know it to be
true: on a daily basis, we work with large corpora
created by, and often marked up by, humans. Falli-
ble as ever, these humans have made errors. For the
errors in content, be they spelling, syntax, or some-
thing else, we can hope to build more robust systems
that will be able to handle them. But what of the
errors in markup?
In this paper, we propose a system for cataloguing
corpus errors, and discuss some strategies for dealing
with them as a research community. Finally, we will
present an example (function tagging) that demon-
strates the appropriateness of our methods.
2 An error taxonomy
2.1 Type A: Detectable errors
The easiest errors, which we have dubbed \Type A",
are those that can be automatically detected and
xed. These typically come up when there would
be multiple reasonable ways of tagging a certain in-
teresting situation: the markup guidelines arbitrarily
choose one, and the human annotator unthinkingly
uses the other.


h
h
h




X
X


X
X

9
the dealersby
NP
...should go here
This LGS tag...
VBN PP-LGS
INstarted
VP
Figure 1: A function tag error of Type A
The canonical example of this sort of thing is the
treebank?s LGS tag, representing the \logical sub-
ject" of a passive construction. It makes a great
deal of sense to put this tag on the NP object of
the ?by? construction; it makes almost as much sense
to tag the PP itself, especially since (given a choice)
most other function tags are put there. The tree-
bank guidelines specically choose the former: \It
attaches to the NP object of by and not to the PP
node itself." (Bies et al, 1995) Nevertheless, in sev-
eral cases the annotators put the tag on the PP, as
shown in Figure 1. We can automatically correct this
error by algorithmically removing the LGS tag from
any such PP and adding it to the object thereof.
The unifying feature of all Type A errors is that
the annotator?s intent is still clear. In the LGS case,
the annotator managed to clearly indicate the pres-
ence of a passive construction and its logical subject.
Since the transformation from what was marked to
what ought to have been marked is straightforward
and algorithmic, we can easily apply this correction
to all data.
2.2 Type B: Fixable errors
Next, we come to the Type B errors, those which
are xable but require human intervention at some
point in the process. In theory, this category could
include errors that could be found automatically but
require a human to x; this doesn?t happen in prac-
tice, because if an error is suciently systematic that
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 111-116.
                         Proceedings of the Conference on Empirical Methods in Natural


?
?
?
?
(
(
(h
h
h




 P
P
VP
ADVP
hard
NP
company
NP
Mistag?should be VBN
VBD
hit
PP
by ...
Figure 2: A part-of-speech error of Type B
1
an algorithm can detect it and be certain that it is
in fact an error, it can usually be corrected with cer-
tainty as well. In practice, the instances of this class
of error are all cases where the computer can?t detect
the error for certain. However, for all Type B errors,
once detected, the correction that needs to be made
is clear, at least to a human observer with access to
the annotation guidelines.
Certain Type B errors are moderately easy to
nd. When annotators misunderstand a complicated
markup guideline, they mismark in a somewhat pre-
dictable way. While not being totally systematically
detectable, an algorithm can leverage these patterns
to extract a list of tags or parses that might be incor-
rect, which a human can then examine. Some errors
of this type (henceforth \Type B
1
") include:
 VBD / VBN. Often the past tense form of a verb
(VBD) and its past participle (VBN) have the
same form, and thus annotators sometimes mis-
take one for the other, as in Figure 2. Some such
cases are not detectable, which is why this is not
Type A.1
 IN / RB / RP. There are specic tests and guide-
lines for telling these three things apart, but fre-
quently a preposition (IN) is marked when an
adverb (RB) or particle (PRT) would be more
appropriate. If an IN is occurring somewhere
other than under a PP, it is likely to be a mistag.
Occasionally, an extracted list of maybe-errors will
be \perfect", containing only instances that are ac-
tually corpus errors. This happens when the pat-
tern is a very good heuristic, though not necessarily
valid (which is why the errors are Type B
1
, and not
Type A). When ling corrections for these, it is still
best to annotate them individually, as the correc-
tions may later be applied to an expanded or modi-
1There is a subclass of this error which is Type A:
when we find a VBD whose grandparent is a VP headed
by a form of ?have?, we can deterministically retag it as
VBN.
ed data set, for which the heuristic would no longer
be perfect.
Other xable errors are pretty much isolated.
Within section 24 of the treebank, for instance, we
have:
 the word ?long? tagged as an adjective (JJ) when
clearly used as a verb (VB)
 the word ?that? parsed into a noun phrase instead
of heading a subordinate clause, as in Figure 3
 a phrase headed by ?about?, as in ?think about?,
tagged as a location (LOC)
These isolated errors (resulting, presumably, from
a typo or a moment of inattention on the part of
the annotator) are not in any way predictable, and
can be found essentially only by examining the out-
put of one?s algorithm, analysing the \errors", and
noticing that the treebank was incorrect, rather than
(or in addition to) the algorithm. We will call these
Type B
2
.
2.3 Type C: Systematic inconsistency
Sometimes, there is a construction that the markup
guidelines writers didn?t think about, didn?t write
up, or weren?t clear about. In these cases, annota-
tors are left to rely on their own separate intuitions.
This leaves us with markup that is inconsistent and
therefore clearly partially in error, but with no obvi-
ous correction. There is really very little to be done
about these, aside from noting them and perhaps
controlling for them in the evaluation.
Some Type C errors in the treebank include:
 ?ago?. English?s sole postposition seems to have
given annotators some diculty. Lacking a
postposition tag, many tagged such occurrences
of ?ago? as a preposition (IN); others used the
adverb tag (RB) exclusively.2 Since some occur-
rences really are adverbs, this just makes a big
mess.
 ADVP-MNR. The MNR tag is meant to be ap-
plied to constituents denoting manner or instru-
ment. Some annotators (but not all) seemed
to decide that any adverbial phrase (ADVP)
headed by an ?-ly? word must get a MNR tag, ap-
plying it to words like ?suddenly?, ?significantly?,
and ?clearly?.
2In particular, the annotators of sections 05, 09, 12,
17, 20, and 24 used IN sometimes, while the others tagged
all occurrences of ?ago? as adverbs.
  
 ?
?
?


h
h
h

P
P

 P
P
*NONE*
0
S
NP VP
were ...
SBAR
DT NNS
that subsidies
should be
 
 
 ?
?
?


h
h
h

 P
P
IN S
NP VP
were ...
SBAR
that
NNS
subsidies
Figure 3: A parse error of Type B
2
The hallmark of a Type C error is that even what
ought to be correct isn?t always clear, and as a result,
any plan to correct a group of Type C errors will
have to rst include discussion on what the correct
markup guideline should be.
3 tsed
In order to eect these changes in some communi-
cable way, we have implemented a program called
tsed, by analogy with and inspired by the already
prevalent tgrep search program.3 It takes a search
pattern and a replacement pattern, and after nd-
ing the constituent(s) that match the search pattern,
modies them and prints the result. For those al-
ready familiar with tgrep search syntax, this should
be moderately intuitive.
To the basic pattern-matching syntax of tgrep, we
have added a few extra restriction patterns (for spec-
ifying sentence number and head word), as well as a
way of marking nodes for later reference in the re-
placement pattern (by simply wrapping a constituent
in square brackets instead of parentheses).
The replacement syntax is somewhat more com-
plicated, because wherever possible we want to be
able to construct the new trees by reference to the
old tree, in order to preserve modiers and structure
we may not know about when we write the pattern.
For full details of the program?s abilities, consult the
program documentation, but here are the main ones:
 Relabelling. Constituents can be relabelled with
no change to any of their modiers or children.
 Tagging. A tag can be added to or removed from
a constituent, without changing any modiers or
children.
 Reference. Constituents in the search pattern
can be included by reference in the replacement
pattern.
3tgrep was written by Richard Pito of the University
of Pennsylvania, and comes with the treebank.
 Construction. New structure can be built by
specifying it in the usual S-expression format,
e.g. (NP (NN snork)). Usually used in combi-
nation with Reference patterns.
Along with tsed itself, we distribute a Perl pro-
gram wsjsed to process treebank change scripts like
the following:
{2429#0-b}<<EOF
NP $ [ADJP] > (VP / keep) (S \0 \1)
NP <<, markets - SBJ
EOF
This script would make a batch modication to the
zeroth sentence of the 29th le in section 24. The
batch includes two corrections: the rst matches
a noun phrase (NP) whose sister is an ADJP and
whose parent is a VP headed by the word ?keep?. The
matched NP node is replaced by a (created) S node
whose children will be that very NP and its sister
ADJP. The second correction then nds an NP that
ends in the word ?markets? and marks it with the SBJ
function tag.
Distributing changes in this form is important for
two reasons. First of all, by giving changes in their
minimal, most general forms, they are small and easy
to transmit, and easy to merge. Perhaps more im-
portantly, since corpora are usually copyrighted and
can only be used by paying a fee to the controlling
body (usually LDC or ELDA), we need a way to dis-
tribute only the changes, in a form that is useless
without having bought the original corpus. Scripts
for tsed, or for wsjsed, serve this purpose.
These programs are available from our website.4
4 When to correct
Now that we have analysed the dierent types of
errors that can occur and how to correct them, we
can discuss when and whether to do so.
4http://www.cs.brown.edu/~dpb/tsed/
4.1 Training
In virtually all empirical NLP work, the training set
is going to encompass the vast majority of the data.
As such, it is usually impractical for a human (or
even a whole lab of humans) to sit down and revise
the training. Type A errors can be corrected easily
enough, as can some Type B
1
errors whose heuristics
have a high yield. Purely on grounds of practicality,
though, it would be dicult to eect signicant cor-
rection on a training set of any signicant size (such
as for the treebank).
Practicality aside, correcting the training set is a
bad idea anyway. After expending an enormous ef-
fort to perfect one training set, the net result is just
one correct training set. While it might make certain
things easier and probably will improve the results
of most algorithms, those improved results will not
be valid for those same algorithms trained on other,
non-perfect data; the vast majority of corpora will
still be noisy. If a user of an algorithm, e.g. an ap-
plication developer, chooses to perfect a training set
to improve the results, that would be helpful, but it
is important that researchers report results that are
likely to be applicable more generally, to more than
one training set. Furthermore, robustness to errors
in the training, via smoothing or some other mech-
anism, will also make an algorithm robust to sparse
data, the ever-present spectre that haunts nearly ev-
ery problem in the eld; thus eliminating all errors
in the training ought not to have as much of an eect
on a strong algorithm.
4.2 Testing
Testing data is another story, however. In terms of
practicality, it is more feasible, as the test set is usu-
ally at least an order of magnitude smaller than the
training. More important, though, is the issue of
fairness. We need to continue using noisy training
data in order to better model real-world use, but it
is unfair and unreasonable to have noise in the gold
standard5, which causes an algorithm to be penalised
where it is more correct than the human annotation.
As performance on various tasks improves, it be-
comes ever more important to be able to correct the
testing data. A ?mere? 1% improvement on a result of
75% is not impressive, as it represents just a 4% re-
duction in apparent error, but the same 1% improve-
ment on a result of 95% represents a 20% reduction
in apparent error! In the end, a noisy gold standard
sets an upper bound of less than 100% on perfor-
mance, which is if nothing else counterintuitive.
5Sometimes more of a pyrite standard, really.
4.3 Ethical considerations
Of course, we cannot simply go about changing the
corpus willy-nilly. We refer the reader to chapter 7
of David Magerman?s thesis (1994) for a cogent dis-
cussion of why changing either the training or the
testing data is a bad idea. However, we believe that
there are now some changed circumstances that war-
rant a modication of this ethical dictum.
First, we are not allowed to look at testing data.
How to correct it, then? An initial reaction might
be to \promise" to forget everything seen while cor-
recting the test corpus; this is not reasonable.
Another solution exists, however, which is nearly
as good and doesn?t raise any ethical questions.
Many research groups already use yet another sec-
tion, separate from both the training and testing,
as a sort of development corpus.6 When developing
an algorithm, we must look at some output for de-
bugging, preliminary evaluation, and parameter esti-
mation; so this development section is used for test-
ing until a piece of work is ready for publication, at
which point the \true" test set is used. Since we are
all reading this development output already anyway,
there is no harm in reading it to perform corrections
thereon. In publication, then, one can publish the
results of an algorithm on both the unaltered and
corrected versions of the development section, in ad-
dition to the results on the unaltered test section.
We can then presume that a corrected version of the
test corpus would result in a perceived error reduc-
tion comparable to that on the development corpus.
Another problem mentioned in that chapter is of a
researcher quietly correcting a test corpus, and pub-
lishing results on the modied data (without even
noting that it was modied). The solution to this
is simple: any results on modied data will need to
acknowledge that the data is modied (to be hon-
est), and those modications need to be made public
(to facilitate comparisons by later researchers). For
Type A errors xed by a simple rule, it may be rea-
sonable to publish them directly in the paper that
gives the results.7 For Type B errors, it would be
more reasonable to simply publish them on a web-
site, since there are bound to be a large number of
them.8
6In the treebank, this is usually section 24.
7The rule we used to fix the LGS problem noted in
section 2.1 is as follows:
{24*-bg}<<EOF
NP !- LGS > (PP - LGS) - LGS
PP - LGS ! LGS
EOF
8The 235 corrections made to section 24 are available
at http://www.cs.brown.edu/~dpb/tbfix/.
Finally, we would like to note that one of the rea-
sons Magerman was ready to dismiss error in the
testing was that the test data had \a consistency
rate much higher than the accuracy rate of state-of-
the-art parsers". This is no longer true.
4.4 Practical considerations
As multiple researchers each begin to impose their
own corrections, there are several new issues that
will come up. First of all, even should everyone pub-
lish their own corrections, and post comparisons to
previous researchers? corrected results, there is some
danger that a variety of dierent correction sets will
exist concurrently. To some extent this can be mit-
igated if each researcher posted both their own cor-
rections by themselves, and a full list of all correc-
tions they used (including their own). Even so, from
time to time these varied correction sets will need to
be collected and merged for the whole community to
use.
More dicult to deal with is the fact that, in-
evitably, there will be disputes as to what is correct.
Sometimes these will be between the treebank ver-
sion and a proposed correction; there will probably
also be cases where multiple competing corrections
are suggested. There really is no good systematic
policy for dealing with this. Disputes will have to
be handled on a case-by-case basis, and researchers
should probably note any disputes to their correc-
tions that they know of when publishing results, but
beyond that it will have to be up to each researcher?s
personal sense of ethics.
In all cases, a search-and-replace pattern should
be made as general as possible (without being too
general, of course), so that it interacts well with
other modications. Various researchers are already
working with (deterministically) dierent versions of
corpora|with new tags added, or empty nodes re-
moved, or some tags collapsed, for instance, not to
mention other corrections already performed|and it
would be a bad idea to distribute corrections that are
specic to one version of these. When in doubt, one
should favour the original form of the corpus, natu-
rally.
The nal issue is not a practical problem, but an
Algorithm error 44%
Parse error 20%
Treebank error 18%
Type C error 13%
Dubious 6%
Table 1: Analysis of reported errors
observation: once a researcher publishes a correction
set, any further corrections by other researchers are
likely to decrease the results of the rst researcher?s
algorithm, at least somewhat. This is due to the fact
that that researcher is usually not going to notice
corpus errors when the algorithm errs in the same
way. This unfortunate consequence is inevitable, and
hopefully will prove minor.
5 Experimental results
As a sort of case study in the meta-algorithms pre-
sented in the previous sections, we will look at
the problem of function tagging in the treebank.
Blaheta and Charniak (2000) describe an algorithm
for marking sentence constituents with function tags
such as SBJ (for sentence subjects) and TMP (for
temporal phrases). We trained this algorithm on sec-
tions 02{21 of the treebank and ran it on section 24
(the development corpus), then analysed the output.
First, we printed out every constituent with a func-
tion tag error. We then examined the sentence in
which each occurred, and determined whether the
error was in the algorithm or in the treebank, or
elsewhere, as reported in Table 1. Of the errors we
examined, less than half were due solely to an algo-
rithmic failure in the function tagger itself. The next
largest category was parse error: this function tag-
ging algorithm requires parsed input, and in these
cases, that input was incorrect and led the function
tagger astray; had the tagger received the treebank
parse, it would have given correct output. In just
under a fth of the reported \errors", the algorithm
was correct and the treebank was denitely wrong.
The remainder of cases we have identied either as
Type C errors|wherein the tagger agreed with many
training examples, but the \correct" tag agreed with
many others|or at least \dubious", in the cases that
weren?t common enough to be systematic inconsis-
tencies but where the guidelines did not clearly pre-
fer the treebank tag over the tagger output, or vice
versa.
Next, we compiled all the noted treebank errors
and their corrections. The most common correc-
tion involved simply adding, removing, or changing
a function tag to what the algorithm output (with a
net eect of improving our score). However, it should
be noted that when classifying reported errors, we
examined their contexts, and in so doing discovered
other sorts of treebank error. Mistags and misparses
did not directly aect us; some function tag correc-
tions actually decreased our score. All corrections
were applied anyway, in the hope of cleaner evalua-
tions for future researchers. In total, we made 235
corrections, including about 130 simple retags.
Grammatical tags Form/function tags Topicalisation tags
P R F P R F P R F
Treebank 96.37% 95.04% 95.70% 81.61% 76.44% 78.94% 96.74% 94.68% 95.70%
Fixed 97.08% 95.27% 96.16% 85.75% 77.51% 81.42% 97.85% 95.79% 96.81%
False error 19.56% 4.64% 10.70% 22.51% 4.54% 11.78% 34.05% 20.86% 25.81%
Table 2: Function tagging results, adjusted for treebank error
Finally, we re-evaluated the algorithm?s output on
the corrected development corpus. Table 2 shows
the resulting improvements.9 Precision, recall, and
F-measure are calculated as in (Blaheta and Char-
niak, 2000). The false error rate is simply the percent
by which the error is reduced; in terms of the per-
formance on the treebank version (t) and the xed
version (f),
False error =
f ? t
1.0? t
 100%
This is the percentage of the reported errors that are
due to treebank error.
The topicalisation result is nice, but since the TPC
tag is fairly rare (121 occurrences in section 24), these
numbers may not be robust. It is interesting, though,
that the false error rate on the two major tag groups
is so similar|roughly 20% in precision and 5% in
recall for each, leading to 10% in F-measure. First
of all, this parallelism strengthens our assertion that
the false error rate, though calculated on a devel-
opment corpus, can be presumed to apply equally
to the test corpus, since it indicates that the hu-
man missed tag and mistag rates may be roughly
constant. Second, the much higher improvement on
precision indicates that the majority of treebank er-
ror (at least in the realm of function tagging) is due
to human annotators forgetting a tag.
6 Conclusion
In this paper, we have given a new characterisation
of the sorts of noise one nds in empirical NLP, and
a roadmap for dealing with it in the future. For
many of the problems in the eld, the state of the
art is now suciently advanced that evaluation error
is becoming a signicant factor in reported results;
we show that it is correctable within the constraints
of practicality and ethics.
Although our examples all came from the Penn
treebank, the taxonomy presented is applicable to
9We did not run corrections on, nor do we show re-
sults for, Blaheta and Charniak?s ?misc? grouping, both
because there were very many of them in the reported
error list and because they are very frequently wrong in
the treebank.
any corpus annotation project. As long as there are
typographical errors, there will be Type B errors;
and unclear or counterintuitive guidelines will forever
engender Type A and Type C errors. Furthermore,
we expect that the experimental improvement shown
in Section 5 will be reflected in projects on other an-
notated corpora|perhaps to a lesser or greater de-
gree, depending on the diculty of the annotation
task and the prior performance of the computer sys-
tem.
An eect of the continuing improvement of the
state of the art is that researchers will begin (or
have begun) concentrating on specic subproblems,
and will naturally report results on those subprob-
lems. These subproblems are likely to involve the
complicated cases, which are presumably also more
subject to annotator error, and are certain to involve
smaller test sets, thus increasing the performance ef-
fect of each individual misannotation. As the sizes
of the subproblems decrease and their complexity in-
creases, the ability to correct the evaluation corpus
will become increasingly important.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert
MacIntyre, 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project, January.
Don Blaheta and Eugene Charniak. 2000. Assign-
ing function tags to parsed text. In Proceedings
of the 1st Annual Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 234{240.
David M. Magerman. 1994. Natural language pars-
ing as statistical pattern recognition. Ph.D. thesis,
Stanford University, February.
