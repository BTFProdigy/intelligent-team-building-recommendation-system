Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880?889,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polylingual Topic Models
David Mimno Hanna M. Wallach Jason Naradowsky
University of Massachusetts, Amherst
Amherst, MA 01003
{mimno, wallach, narad, dasmith, mccallum}cs.umass.edu
David A. Smith Andrew McCallum
Abstract
Topic models are a useful tool for analyz-
ing large text collections, but have previ-
ously been applied in only monolingual,
or at most bilingual, contexts. Mean-
while, massive collections of interlinked
documents in dozens of languages, such
as Wikipedia, are now widely available,
calling for tools that can characterize con-
tent in many languages. We introduce a
polylingual topic model that discovers top-
ics aligned across multiple languages. We
explore the model?s characteristics using
two large corpora, each with over ten dif-
ferent languages, and demonstrate its use-
fulness in supporting machine translation
and tracking topic trends across languages.
1 Introduction
Statistical topic models have emerged as an in-
creasingly useful analysis tool for large text col-
lections. Topic models have been used for analyz-
ing topic trends in research literature (Mann et al,
2006; Hall et al, 2008), inferring captions for im-
ages (Blei and Jordan, 2003), social network anal-
ysis in email (McCallum et al, 2005), and expand-
ing queries with topically related words in infor-
mation retrieval (Wei and Croft, 2006). Much of
this work, however, has occurred in monolingual
contexts. In an increasingly connected world, the
ability to access documents in many languages has
become both a strategic asset and a personally en-
riching experience. In this paper, we present the
polylingual topic model (PLTM). We demonstrate
its utility and explore its characteristics using two
polylingual corpora: proceedings of the European
parliament (in eleven languages) and a collection
of Wikipedia articles (in twelve languages).
There are many potential applications for
polylingual topic models. Although research liter-
ature is typically written in English, bibliographic
databases often contain substantial quantities of
work in other languages. To perform topic-based
bibliometric analysis on these collections, it is
necessary to have topic models that are aligned
across languages. Such analysis could be sig-
nificant in tracking international research trends,
where language barriers slow the transfer of ideas.
Previous work on bilingual topic modeling
has focused on machine translation applications,
which rely on sentence-aligned parallel transla-
tions. However, the growth of the internet, and
in particular Wikipedia, has made vast corpora
of topically comparable texts?documents that are
topically similar but are not direct translations of
one another?considerably more abundant than
ever before. We argue that topic modeling is
both a useful and appropriate tool for leveraging
correspondences between semantically compara-
ble documents in multiple different languages.
In this paper, we use two polylingual corpora
to answer various critical questions related to
polylingual topic models. We employ a set of di-
rect translations, the EuroParl corpus, to evaluate
whether PLTM can accurately infer topics when
documents genuinely contain the same content.
We also explore how the characteristics of dif-
ferent languages affect topic model performance.
The second corpus, Wikipedia articles in twelve
languages, contains sets of documents that are not
translations of one another, but are very likely to
be about similar concepts. We use this corpus
to explore the ability of the model both to infer
similarities between vocabularies in different lan-
guages, and to detect differences in topic emphasis
between languages. The internet makes it possible
for people all over the world to access documents
from different cultures, but readers will not be flu-
ent in this wide variety of languages. By linking
topics across languages, polylingual topic mod-
els can increase cross-cultural understanding by
providing readers with the ability to characterize
880
the contents of collections in unfamiliar languages
and identify trends in topic prevalence.
2 Related Work
Bilingual topic models for parallel texts with
word-to-word alignments have been studied pre-
viously using the HM-bitam model (Zhao and
Xing, 2007). Tam, Lane and Schultz (Tam et
al., 2007) also show improvements in machine
translation using bilingual topic models. Both
of these translation-focused topic models infer
word-to-word alignments as part of their inference
procedures, which would become exponentially
more complex if additional languages were added.
We take a simpler approach that is more suit-
able for topically similar document tuples (where
documents are not direct translations of one an-
other) in more than two languages. A recent ex-
tended abstract, developed concurrently by Ni et
al. (Ni et al, 2009), discusses a multilingual topic
model similar to the one presented here. How-
ever, they evaluate their model on only two lan-
guages (English and Chinese), and do not use the
model to detect differences between languages.
They also provide little analysis of the differ-
ences between polylingual and single-language
topic models. Outside of the field of topic mod-
eling, Kawaba et al (Kawaba et al, 2008) use
a Wikipedia-based model to perform sentiment
analysis of blog posts. They find, for example,
that English blog posts about the Nintendo Wii of-
ten relate to a hack, which cannot be mentioned in
Japanese posts due to Japanese intellectual prop-
erty law. Similarly, posts about whaling often
use (positive) nationalist language in Japanese and
(negative) environmentalist language in English.
3 Polylingual Topic Model
The polylingual topic model (PLTM) is an exten-
sion of latent Dirichlet alocation (LDA) (Blei et
al., 2003) for modeling polylingual document tu-
ples. Each tuple is a set of documents that are
loosely equivalent to each other, but written in dif-
ferent languages, e.g., corresponding Wikipedia
articles in French, English and German. PLTM as-
sumes that the documents in a tuple share the same
tuple-specific distribution over topics. This is un-
like LDA, in which each document is assumed to
have its own document-specific distribution over
topics. Additionally, PLTM assumes that each
?topic? consists of a set of discrete distributions
D
N
1
T
N
L
...
w
? ?
wz
z
...
?
1
?
L
?
1
?
L
Figure 1: Graphical model for PLTM.
over words?one for each language l = 1, . . . , L.
In other words, rather than using a single set of
topics ? = {?
1
, . . . ,?
T
}, as in LDA, there are L
sets of language-specific topics, ?
1
, . . . ,?
L
, each
of which is drawn from a language-specific sym-
metric Dirichlet with concentration parameter ?
l
.
3.1 Generative Process
A new document tuplew = (w
1
, . . . ,w
L
) is gen-
erated by first drawing a tuple-specific topic dis-
tribution from an asymmetric Dirichlet prior with
concentration parameter ? and base measurem:
? ? Dir (?, ?m). (1)
Then, for each language l, a latent topic assign-
ment is drawn for each token in that language:
z
l
? P (z
l
|?) =
?
n
?
z
l
n
. (2)
Finally, the observed tokens are themselves drawn
using the language-specific topic parameters:
w
l
? P (w
l
| z
l
,?
l
) =
?
n
?
l
w
l
n
|z
l
n
. (3)
The graphical model is shown in figure 1.
3.2 Inference
Given a corpus of training and test document
tuples?W and W
?
, respectively?two possible
inference tasks of interest are: computing the
probability of the test tuples given the training
tuples and inferring latent topic assignments for
test documents. These tasks can either be accom-
plished by averaging over samples of ?
1
, . . . ,?
L
and ?m from P (?
1
, . . . ,?
L
, ?m |W
?
, ?) or by
evaluating a point estimate. We take the lat-
ter approach, and use the MAP estimate for ?m
and the predictive distributions over words for
?
1
, . . . ,?
L
. The probability of held-out docu-
ment tuples W
?
given training tuples W is then
approximated by P (W
?
|?
1
, . . . ,?
L
, ?m).
Topic assignments for a test document tuple
w = (w
1
, . . . ,w
L
) can be inferred using Gibbs
881
sampling. Gibbs sampling involves sequentially
resampling each z
l
n
from its conditional posterior:
P (z
l
n
= t |w, z
\l,n
,?
1
, . . . ,?
L
, ?m)
? ?
l
w
l
n
|t
(N
t
)
\l,n
+ ?m
t
?
t
N
t
? 1 + ?
, (4)
where z
\l,n
is the current set of topic assignments
for all other tokens in the tuple, while (N
t
)
\l,n
is
the number of occurrences of topic t in the tuple,
excluding z
l
n
, the variable being resampled.
4 Results on Parallel Text
Our first set of experiments focuses on document
tuples that are known to consist of direct transla-
tions. In this case, we can be confident that the
topic distribution is genuinely shared across all
languages. Although direct translations in multi-
ple languages are relatively rare (in contrast with
comparable documents), we use direct translations
to explore the characteristics of the model.
4.1 Data Set
The EuroParl corpus consists of parallel texts in
eleven western European languages: Danish, Ger-
man, Greek, English, Spanish, Finnish, French,
Italian, Dutch, Portuguese and Swedish. These
texts consist of roughly a decade of proceedings
of the European parliament. For our purposes we
use alignments at the speech level rather than the
sentence level, as in many translation tasks using
this corpus. We also remove the twenty-five most
frequent word types for efficiency reasons. The
remaining collection consists of over 121 million
words. Details by language are shown in Table 1.
Table 1: Average document length, # documents, and
unique word types per 10,000 tokens in the EuroParl corpus.
Lang. Avg. leng. # docs types/10k
DA 160.153 65245 121.4
DE 178.689 66497 124.5
EL 171.289 46317 124.2
EN 176.450 69522 43.1
ES 170.536 65929 59.5
FI 161.293 60822 336.2
FR 186.742 67430 54.8
IT 187.451 66035 69.5
NL 176.114 66952 80.8
PT 183.410 65718 68.2
SV 154.605 58011 136.1
Models are trained using 1000 iterations of
Gibbs sampling. Each language-specific topic?
word concentration parameter ?
l
is set to 0.01.
centralbank europ?iske ecb s l?n centralbanks 
zentralbank ezb bank europ?ischen investitionsbank darlehen 
??????? ???????? ???????? ??? ????????? ???????? 
bank central ecb banks european monetary 
banco central europeo bce bancos centrales 
keskuspankin ekp n euroopan keskuspankki eip 
banque centrale bce europ?enne banques mon?taire 
banca centrale bce europea banche prestiti 
bank centrale ecb europese banken leningen 
banco central europeu bce bancos empr?stimos 
centralbanken europeiska ecb centralbankens s l?n 
b?rn familie udnyttelse b?rns b?rnene seksuel 
kinder kindern familie ausbeutung familien eltern 
?????? ??????? ?????????? ??????????? ?????? ???????? 
children family child sexual families exploitation 
ni?os familia hijos sexual infantil menores 
lasten lapsia lapset perheen lapsen lapsiin 
enfants famille enfant parents exploitation familles 
bambini famiglia figli minori sessuale sfruttamento 
kinderen kind gezin seksuele ouders familie 
crian?as fam?lia filhos sexual crian?a infantil 
barn barnen familjen sexuellt familj utnyttjande 
m?l n? m?ls?tninger m?let m?ls?tning opn? 
ziel ziele erreichen zielen erreicht zielsetzungen 
??????? ????? ?????? ?????? ?????? ???????? 
objective objectives achieve aim ambitious set 
objetivo objetivos alcanzar conseguir lograr estos 
tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen 
objectif objectifs atteindre but cet ambitieux 
obiettivo obiettivi raggiungere degli scopo quello 
doelstellingen doel doelstelling bereiken bereikt doelen 
objectivo objectivos alcan?ar atingir ambicioso conseguir 
m?l m?let uppn? m?len m?ls?ttningar m?ls?ttning 
andre anden side ene andet ?vrige 
anderen andere einen wie andererseits anderer 
????? ???? ???? ????? ?????? ???? 
other one hand others another there 
otros otras otro otra parte dem?s 
muiden toisaalta muita muut muihin muun 
autres autre part c?t? ailleurs m?me 
altri altre altro altra dall parte 
andere anderzijds anderen ander als kant 
outros outras outro lado outra noutros 
andra sidan ? annat ena annan 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
Figure 2: EuroParl topics (T=400)
The concentration parameter ? for the prior over
document-specific topic distributions is initialized
to 0.01T , while the base measure m is initialized
to the uniform distribution. Hyperparameters ?m
are re-estimated every 10 Gibbs iterations.
4.2 Analysis of Trained Models
Figure 2 shows the most probable words in all lan-
guages for four example topics, from PLTM with
400 topics. The first topic contains words relating
to the European Central Bank. This topic provides
an illustration of the variation in technical ter-
minology captured by PLTM, including the wide
array of acronyms used by different languages.
The second topic, concerning children, demon-
strates the variability of everyday terminology: al-
though the four Romance languages are closely
882
related, they use etymologically unrelated words
for children. (Interestingly, all languages except
Greek and Finnish use closely related words for
?youth? or ?young? in a separate topic.) The third
topic demonstrates differences in inflectional vari-
ation. English and the Romance languages use
only singular and plural versions of ?objective.?
The other Germanic languages include compound
words, while Greek and Finnish are dominated by
inflected variants of the same lexical item. The fi-
nal topic demonstrates that PLTM effectively clus-
ters ?syntactic? words, as well as more semanti-
cally specific nouns, adjectives and verbs.
Although the topics in figure 2 seem highly fo-
cused, it is interesting to ask whether the model
is genuinely learning mixtures of topics or simply
assigning entire document tuples to single topics.
To answer this question, we compute the posterior
probability of each topic in each tuple under the
trained model. If the model assigns all tokens in
a tuple to a single topic, the maximum posterior
topic probability for that tuple will be near to 1.0.
If the model assigns topics uniformly, the maxi-
mum topic probability will be near 1/T . We com-
pute histograms of these maximum topic prob-
abilities for T ? {50, 100, 200, 400, 800}. For
clarity, rather than overlaying five histograms, fig-
ure 3 shows the histograms converted into smooth
curves using a kernel density estimator.
1
Although
there is a small bump around 1.0 (for extremely
short documents, e.g., ?Applause?), values are
generally closer to, but greater than, 1/T .
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
10
12
Smoothed histograms of max(P(t))
Maximum topic probability in document
Den
sity 800 topics400 topics200 topics100 topics50 topics
Figure 3: Smoothed histograms of the probability of the
most probable topic in a document tuple.
Although the posterior distribution over topics
for each tuple is not concentrated on one topic,
it is worth checking that this is not simply be-
cause the model is assigning a single topic to the
1
We use the R density function.
tokens in each of the languages. Although the
model does not distinguish between topic assign-
ment variables within a given document tuple (so
it is technically incorrect to speak of different pos-
terior distributions over topics for different docu-
ments in a given tuple), we can nevertheless divide
topic assignment variables between languages and
use them to estimate a Dirichlet-multinomial pos-
terior distribution for each language in each tuple.
For each tuple we can then calculate the Jensen-
Shannon divergence (the average of the KL di-
vergences between each distribution and a mean
distribution) between these distributions. Figure 4
shows the density of these divergences for differ-
ent numbers of topics. As with the previous fig-
ure, there are a small number of documents that
contain only one topic in all languages, and thus
have zero divergence. These tend to be very short,
formulaic parliamentary responses, however. The
vast majority of divergences are relatively low (1.0
indicates no overlap in topics between languages
in a given document tuple) indicating that, for each
tuple, the model is not simply assigning all tokens
in a particular language to a single topic. As the
number of topics increases, greater variability in
topic distributions causes divergence to increase.
0.0 0.1 0.2 0.3 0.4 0.5
0
5
10
15
20
Smoothed histograms of inter?language JS divergence
Jensen?Shannon Divergence
Den
sity
800 topics400 topics200 topics100 topics50 topics
Figure 4: Smoothed histograms of the Jensen-Shannon
divergences between the posterior probability of topics be-
tween languages.
4.3 Language Model Evaluation
A topic model specifies a probability distribution
over documents, or in the case of PLTM, docu-
ment tuples. Given a set of training document tu-
ples, PLTM can be used to obtain posterior esti-
mates of ?
1
, . . . ,?
L
and ?m. The probability of
previously unseen held-out document tuples given
these estimates can then be computed. The higher
the probability of the held-out document tuples,
the better the generalization ability of the model.
883
Analytically calculating the probability of a set
of held-out document tuples given ?
1
, . . . ,?
L
and
?m is intractable, due to the summation over an
exponential number of topic assignments for these
held-out documents. However, recently developed
methods provide efficient, accurate estimates of
this probability. We use the ?left-to-right? method
of (Wallach et al, 2009). We perform five esti-
mation runs for each document and then calculate
standard errors using a bootstrap method.
Table 2 shows the log probability of held-out
data in nats per word for PLTM and LDA, both
trained with 200 topics. There is substantial varia-
tion between languages. Additionally, the predic-
tive ability of PLTM is consistently slightly worse
than that of (monolingual) LDA. It is important to
note, however, that these results do not imply that
LDA should be preferred over PLTM?that choice
depends upon the needs of the modeler. Rather,
these results are intended as a quantitative analy-
sis of the difference between the two models.
Table 2: Held-out log probability in nats/word. (Smaller
magnitude implies better language modeling performance.)
PLTM does slightly worse than monolingual LDA models,
but the variation between languages is much larger.
Lang PLTM sd LDA sd
DA -8.11 0.00067 -8.02 0.00066
DE -8.17 0.00057 -8.08 0.00072
EL -8.44 0.00079 -8.36 0.00087
EN -7.51 0.00064 -7.42 0.00069
ES -7.98 0.00073 -7.87 0.00070
FI -9.25 0.00089 -9.21 0.00065
FR -8.26 0.00072 -8.19 0.00058
IT -8.11 0.00071 -8.02 0.00058
NL -7.84 0.00067 -7.75 0.00099
PT -7.87 0.00085 -7.80 0.00060
SV -8.25 0.00091 -8.16 0.00086
As the number of topics is increased, the word
counts per topic become very sparse in mono-
lingual LDA models, proportional to the size of
the vocabulary. Figure 5 shows the proportion
of all tokens in English and Finnish assigned to
each topic under LDA and PLTM with 800 topics.
More than 350 topics in the Finnish LDA model
have zero tokens assigned to them, and almost all
tokens are assigned to the largest 200 topics. En-
glish has a larger tail, with non-zero counts in all
but 16 topics. In contrast, PLTM assigns a sig-
nificant number of tokens to almost all 800 top-
ics, in very similar proportions in both languages.
PLTM topics therefore have a higher granularity ?
i.e., they are more specific. This result is impor-
tant: informally, we have found that increasing the
granularity of topics correlates strongly with user
perceptions of the utility of a topic model.
0 200 400 600 800
0.00
0.01
0.02
0.03
0.04
Sorted topic rank
Perc
enta
ge o
f tok
ens
Figure 5: Topics sorted by number of words assigned.
Finnish is in black, English is in red; LDA is solid, PLTM is
dashed. LDA in Finnish essentially learns a 200 topic model
when given 800 topics, while PLTM uses all 800 topics.
4.4 Partly Comparable Corpora
An important application for polylingual topic
modeling is to use small numbers of comparable
document tuples to link topics in larger collections
of distinct, non-comparable documents in multiple
languages. For example, a journal might publish
papers in English, French, German and Italian. No
paper is exactly comparable to any other paper, but
they are all roughly topically similar. If we wish
to perform topic-based bibliometric analysis, it is
vital to be able to track the same topics across all
languages. One simple way to achieve this topic
alignment is to add a small set of comparable doc-
ument tuples that provide sufficient ?glue? to bind
the topics together. Continuing with the exam-
ple above, one might extract a set of connected
Wikipedia articles related to the focus of the jour-
nal and then train PLTM on a joint corpus consist-
ing of journal papers and Wikipedia articles.
In order to simulate this scenario we create a
set of variations of the EuroParl corpus by treat-
ing some documents as if they have no paral-
lel/comparable texts ? i.e., we put each of these
documents in a single-document tuple. To do this,
we divide the corpusW into two sets of document
tuples: a ?glue? set G and a ?separate? set S such
that |G| / |W| = p. In other words, the proportion
of tuples in the corpus that are treated as ?glue?
(i.e., placed in G) is p. For every tuple in S, we
assign each document in that tuple to a new single-
document tuple. By doing this, every document in
S has its own distribution over topics, independent
of any other documents. Ideally, the ?glue? doc-
884
uments in G will be sufficient to align the topics
across languages, and will cause comparable doc-
uments in S to have similar distributions over top-
ics even though they are modeled independently.
Table 3: The effect of the proportion p of ?glue? tuples on
mean Jensen-Shannon divergence in estimated topic distribu-
tions for pairs of documents in S that were originally part of
a document tuple. Lower divergence means the topic distri-
butions distributions are more similar to each other.
p Mean JS # of pairs Std. Err.
0.01 0.83755 487670 0.00018
0.05 0.79144 467288 0.00021
0.1 0.70228 443753 0.00026
0.25 0.38480 369608 0.00029
0.5 0.29712 246380 0.00030
Table 4: Topics are meaningful within languages but di-
verge between languages when only 1% of tuples are treated
as ?glue? tuples. With 25% ?glue? tuples, topics are aligned.
lang Topics at p = 0.01
DE ru?land russland russischen tschetschenien sicherheit
EN china rights human country s burma
FR russie tch?etch?enie union avec russe r?egion
IT ho presidente mi perch?e relazione votato
lang Topics at p = 0.25
DE ru?land russland russischen tschetschenien ukraine
EN russia russian chechnya cooperation region belarus
FR russie tch?etch?enie avec russe russes situation
IT russia unione cooperazione cecenia regione russa
We train PLTM with 100 topics on corpora with
p ? {0.01, 0.05, 0.1, 0.25, 0.5}. We use 1000 it-
erations of Gibbs sampling with ? = 0.01. Hy-
perparameters ?m are re-estimated every 10 it-
erations. We calculate the Jensen-Shannon diver-
gence between the topic distributions for each pair
of individual documents in S that were originally
part of the same tuple prior to separation. The
lower the divergence, the more similar the distri-
butions are to each other. From the results in fig-
ure 4, we know that leaving all document tuples
intact should result in a mean JS divergence of
less than 0.1. Table 3 shows mean JS divergences
for each value of p. As expected, JS divergence is
greater than that obtained when all tuples are left
intact. Divergence drops significantly when the
proportion of ?glue? tuples increases from 0.01 to
0.25. Example topics for p = 0.01 and p = 0.25
are shown in table 4. At p = 0.01 (1% ?glue? doc-
uments), German and French both include words
relating to Russia, while the English and Italian
word distributions appear locally consistent but
unrelated to Russia. At p = 0.25, the top words
for all four languages are related to Russia.
These results demonstrate that PLTM is appro-
priate for aligning topics in corpora that have only
a small subset of comparable documents. One area
for future work is to explore whether initializa-
tion techniques or better representations of topic
co-occurrence might result in alignment of topics
with a smaller proportion of comparable texts.
4.5 Machine Translation
Although the PLTM is clearly not a substitute for
a machine translation system?it has no way to
represent syntax or even multi-word phrases?it is
clear from the examples in figure 2 that the sets of
high probability words in different languages for a
given topic are likely to include translations. We
therefore evaluate the ability of the PLTM to gen-
erate bilingual lexica, similar to other work in un-
supervised translation modeling (Haghighi et al,
2008). In the early statistical translation model
work at IBM, these representations were called
?cepts,? short for concepts (Brown et al, 1993).
We evaluate sets of high-probability words in
each topic and multilingual ?synsets? by compar-
ing them to entries in human-constructed bilingual
dictionaries, as done by Haghighi et al (2008).
Unlike previous work (Koehn and Knight, 2002),
we evaluate all words, not just nouns. We col-
lected bilingual lexica mapping English words to
German, Greek, Spanish, French, Italian, Dutch
and Swedish. Each lexicon is a set of pairs con-
sisting of an English word and a translated word,
{w
e
, w
`
}. We do not consider multi-word terms.
We expect that simple analysis of topic assign-
ments for sequential words would yield such col-
locations, but we leave this for future work.
For every topic t we select a small number K
of the most probable words in English (e) and
in each ?translation? language (`): W
te
and W
t`
,
respectively. We then add the Cartesian product
of these sets for every topic to a set of candidate
translations C. We report the number of elements
of C that appear in the reference lexica. Results
for K = 1, that is, considering only the single
most probable word for each language, are shown
in figure 6. Precision at this level is relatively
high, above 50% for Spanish, French and Italian
with T = 400 and 800. Many of the candidate
pairs that were not in the bilingual lexica were
valid translations (e.g. EN ?comitology? and IT
885
?comitalogia?) that simply were not in the lexica.
We also do not count morphological variants: the
model finds EN ?rules? and DE ?vorschriften,? but
the lexicon contains only ?rule? and ?vorschrift.?
Results remain strong as we increase K. With
K = 3, T = 800, 1349 of the 7200 candidate
pairs for Spanish appeared in the lexicon.
l l
l
l
l
200 400 600 800
0
100
200
300
400
500
Translation pairs at K=1
Topics
Corr
ect t
rans
lation
s
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ESFRITDESVEL
Figure 6: Are the single most probable words for a given
topic in different languages translations of each other? The
number of such pairs that appear in bilingual lexica is shown
on the y-axis. For T = 800, the top English and Spanish
words in 448 topics were exact translations of one another.
4.6 Finding Translations
In addition to enhancing lexicons by aligning
topic-specific vocabulary, PLTM may also be use-
ful for adapting machine translation systems to
new domains by finding translations or near trans-
lations in an unstructured corpus. These aligned
document pairs could then be fed into standard
machine translation systems as training data. To
evaluate this scenario, we train PLTM on a set of
document tuples from EuroParl, infer topic distri-
butions for a set of held-out documents, and then
measure our ability to align documents in one lan-
guage with their translations in another language.
It is not necessarily clear that PLTM will be ef-
fective at identifying translations. In finding a low-
dimensional semantic representation, topic mod-
els deliberately smooth over much of the varia-
tion present in language. We are therefore inter-
ested in determining whether the information in
the document-specific topic distributions is suffi-
cient to identify semantically identical documents.
We begin by dividing the data into a training
set of 69,550 document tuples and a test set of
17,435 document tuples. In order to make the task
more difficult, we train a relatively coarse-grained
PLTM with 50 topics on the training set. We then
use this model to infer topic distributions for each
40
50
60
70
80
90
100
Min query doc length
% o
f tra
nsl 
at r
ank
0 50 100 200
Rank 1
Rank 5
Rank 10
Rank 20
Figure 7: Percent of query language documents for which
the target language translation is ranked at or above 1, 5, 10
or 20 by JS divergence, averaged over all language pairs.
of the 11 documents in each of the held-out doc-
ument tuples using a method similar to that used
to calculate held-out probabilities (Wallach et al,
2009). Finally, for each pair of languages (?query?
and ?target?) we calculate the difference between
the topic distribution for each held-out document
in the query language and the topic distribution for
each held-out document in the target language. We
use both Jensen-Shannon divergence and cosine
distance. For each document in the query language
we rank all documents in the target language and
record the rank of the actual translation.
Results averaged over all query/target language
pairs are shown in figure 7 for Jensen-Shannon
divergence. Cosine-based rankings are signifi-
cantly worse. It is important to note that the
length of documents matters. As noted before,
many of the documents in the EuroParl collection
consist of short, formulaic sentences. Restrict-
ing the query/target pairs to only those with query
and target documents that are both longer than 50
words results in significant improvement and re-
duced variance: the average proportion of query
documents for which the true translation is ranked
highest goes from 53.9% to 72.7%. Performance
continues to improve with longer documents, most
likely due to better topic inference. Results vary
by language. Table 5 shows results for all tar-
get languages with English as a query language.
Again, English generally performs better with Ro-
mance languages than Germanic languages.
5 Results on Comparable Texts
Directly parallel translations are rare in many lan-
guages and can be extremely expensive to pro-
duce. However, the growth of the web, and in par-
ticular Wikipedia, has made comparable text cor-
886
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,
centers around Nordic counties. The center topic, actor role television actress, is relatively uniform. The right topic, ottoman
empire khan byzantine, is popular in all languages but especially in regions near Istanbul.
Table 5: Percent of English query documents for which the
translation was in the top n ? {1, 5, 10, 20} documents by JS
divergence between topic distributions. To reduce the effect
of short documents we consider only document pairs where
the query and target documents are longer than 100 words.
Lang 1 5 10 20
DA 78.0 90.7 93.8 95.8
DE 76.6 90.0 93.4 95.5
EL 77.1 90.4 93.3 95.2
ES 81.2 92.3 94.8 96.7
FI 76.7 91.0 94.0 96.3
FR 80.1 91.7 94.3 96.2
IT 79.1 91.2 94.1 96.2
NL 76.6 90.1 93.4 95.5
PT 80.8 92.0 94.7 96.5
SV 80.4 92.1 94.9 96.5
pora ? documents that are topically similar but are
not direct translations of one another ? consider-
ably more abundant than true parallel corpora.
In this section, we explore two questions re-
lating to comparable text corpora and polylingual
topic modeling. First, we explore whether com-
parable document tuples support the alignment of
fine-grained topics, as demonstrated earlier using
parallel documents. This property is useful for
building machine translation systems as well as
for human readers who are either learning new
languages or analyzing texts in languages they do
not know. Second, because comparable texts may
not use exactly the same topics, it becomes cru-
cially important to be able to characterize differ-
ences in topic prevalence at the document level (do
different languages have different perspectives on
the same article?) and at the language-wide level
(which topics do particular languages focus on?).
5.1 Data Set
We downloaded XML copies of all Wikipedia ar-
ticles in twelve different languages: Welsh, Ger-
man, Greek, English, Farsi, Finnish, French, He-
brew, Italian, Polish, Russian and Turkish. These
versions of Wikipedia were selected to provide a
diverse range of language families, geographic ar-
eas, and quantities of text. We preprocessed the
data by removing tables, references, images and
info-boxes. We dropped all articles in non-English
languages that did not link to an English article. In
the English version of Wikipedia we dropped all
articles that were not linked to by any other lan-
guage in our set. For efficiency, we truncated each
article to the nearest word after 1000 characters
and dropped the 50 most common word types in
each language. Even with these restrictions, the
size of the corpus is 148.5 million words.
We present results for a PLTM with 400 topics.
1000 Gibbs sampling iterations took roughly four
days on one CPU with current hardware.
5.2 Which Languages Have High Topic
Divergence?
As with EuroParl, we can calculate the Jensen-
Shannon divergence between pairs of documents
within a comparable document tuple. We can then
average over all such document-document diver-
gences for each pair of languages to get an over-
all ?disagreement? score between languages. In-
terestingly, we find that almost all languages in
our corpus, including several pairs that have his-
torically been in conflict, show average JS diver-
gences of between approximately 0.08 and 0.12
for T = 400, consistent with our findings for
EuroParl translations. Subtle differences of sen-
timent may be below the granularity of the model.
887
sadwrn blaned gallair at lloeren mytholeg 
space nasa sojus flug mission 
?????????? sts nasa ???? small 
space mission launch satellite nasa spacecraft 
??????? ??????? ???? ???? ??????? ?????  
sojuz nasa apollo ensimm?inen space lento 
spatiale mission orbite mars satellite spatial 
?????? ? ???? ??? ???? ????  
spaziale missione programma space sojuz stazione 
misja kosmicznej stacji misji space nasa 
??????????? ???? ???????????? ??????? ???????
uzay soyuz ay uzaya salyut sovyetler 
sbaen madrid el la jos? sbaeneg 
de spanischer spanischen spanien madrid la 
???????? ??????? de ??????? ??? ??????? 
de spanish spain la madrid y 
?????? ???? ????????? ???????  de ????  
espanja de espanjan madrid la real 
espagnol espagne madrid espagnole juan y 
???? ??????? ????? ?? ?????? ????  
de spagna spagnolo spagnola madrid el 
de hiszpa?ski hiszpanii la juan y 
?? ?????? ??????? ??????? ????????? de 
ispanya ispanyol madrid la k?ba real 
bardd gerddi iaith beirdd fardd gymraeg 
dichter schriftsteller literatur gedichte gedicht werk 
??????? ?????? ?????? ???? ??????? ???????? 
poet poetry literature literary poems poem 
???? ???? ????? ?????? ??? ????  
runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi 
po?te ?crivain litt?rature po?sie litt?raire ses 
?????? ????? ???? ???? ????? ?????
poeta letteratura poesia opere versi poema 
poeta literatury poezji pisarz in jego 
???? ??? ???????? ?????????? ?????? ????????? 
?air edebiyat ?iir yazar edebiyat? adl? 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 9: Wikipedia topics (T=400).
Overall, these scores indicate that although indi-
vidual pages may show disagreement, Wikipedia
is on average consistent between languages.
5.3 Are Topics Emphasized Differently
Between Languages?
Although we find that if Wikipedia contains an ar-
ticle on a particular subject in some language, the
article will tend to be topically similar to the arti-
cles about that subject in other languages, we also
find that across the whole collection different lan-
guages emphasize topics to different extents. To
demonstrate the wide variation in topics, we cal-
culated the proportion of tokens in each language
assigned to each topic. Figure 8 represents the es-
timated probabilities of topics given a specific lan-
guage. Competitive cross-country skiing (left) ac-
counts for a significant proportion of the text in
Finnish, but barely exists in Welsh and the lan-
guages in the Southeastern region. Meanwhile,
interest in actors and actresses (center) is consis-
tent across all languages. Finally, historical topics,
such as the Byzantine and Ottoman empires (right)
are strong in all languages, but show geographical
variation: interest centers around the empires.
6 Conclusions
We introduced a polylingual topic model (PLTM)
that discovers topics aligned across multiple lan-
guages. We analyzed the characteristics of PLTM
in comparison to monolingual LDA, and demon-
strated that it is possible to discover aligned top-
ics. We also demonstrated that relatively small
numbers of topically comparable document tu-
ples are sufficient to align topics between lan-
guages in non-comparable corpora. Additionally,
PLTM can support the creation of bilingual lexica
for low resource language pairs, providing candi-
date translations for more computationally intense
alignment processes without the sentence-aligned
translations typically used in such tasks. When
applied to comparable document collections such
as Wikipedia, PLTM supports data-driven analysis
of differences and similarities across all languages
for readers who understand any one language.
7 Acknowledgments
The authors thank Limin Yao, who was involved
in early stages of this project. This work was
supported in part by the Center for Intelligent In-
formation Retrieval, in part by The Central In-
telligence Agency, the National Security Agency
and National Science Foundation under NSF grant
number IIS-0326249, and in part by Army prime
contract number W911NF-07-1-0216 and Uni-
versity of Pennsylvania subaward number 103-
548106, and in part by National Science Founda-
tion under NSF grant #CNS-0619337. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are the authors?
and do not necessarily reflect those of the sponsor.
References
David Blei and Michael Jordan. 2003. Modeling an-
notated data. In SIGIR.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. CL, 19(2):263?311.
888
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In EMNLP.
Mariko Kawaba, Hiroyuki Nakasaki, Takehito Utsuro,
and Tomohiro Fukuhara. 2008. Cross-lingual blog
analysis based on multilingual blog distillation from
multilingual Wikipedia entries. In ICWSM.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Gideon Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leverag-
ing topic analysis. In JCDL.
Andrew McCallum, Andr?es Corrada-Emmanuel, and
Xuerui Wang. 2005. Topic and role discovery in
social networks. In IJCAI.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In WWW.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 28:187?
207.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In SIGIR.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In NIPS.
889
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 810?820, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improving NLP through Marginalization of Hidden Syntactic Structure
Jason Naradowsky, Sebastian Riedel, and David A. Smith
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, U.S.A.
{narad, riedel, dasmith}@cs.umass.edu
Abstract
Many NLP tasks make predictions that are in-
herently coupled to syntactic relations, but for
many languages the resources required to pro-
vide such syntactic annotations are unavail-
able. For others it is unclear exactly how
much of the syntactic annotations can be ef-
fectively leveraged with current models, and
what structures in the syntactic trees are most
relevant to the current task.
We propose a novel method which avoids
the need for any syntactically annotated data
when predicting a related NLP task. Our
method couples latent syntactic representa-
tions, constrained to form valid dependency
graphs or constituency parses, with the predic-
tion task via specialized factors in a Markov
random field. At both training and test time we
marginalize over this hidden structure, learn-
ing the optimal latent representations for the
problem. Results show that this approach pro-
vides significant gains over a syntactically un-
informed baseline, outperforming models that
observe syntax on an English relation extrac-
tion task, and performing comparably to them
in semantic role labeling.
1 Introduction
Many NLP tasks are inherently tied to syntax, and
state-of-the-art solutions to these tasks often rely on
syntactic annotations as either a source for useful
features (Zhang et al2006, path features in relation
extraction) or as a scaffolding upon which a more
narrow, specialized classification can occur (as of-
ten done in semantic role labeling). This decou-
pling of the end task from its intermediate repre-
sentation is sometimes known as the two-stage ap-
proach (Chang et al2010) and comes with several
drawbacks. Most notably this decomposition pro-
hibits the learning method from utilizing the labels
from the end task when predicting the intermediate
representation, a structure which must have some
correlation to the end task to provide any benefit.
Relying on intermediate representations that are
specifically syntactic in nature introduces its own
unique set of problems. Large amounts of syntac-
tically annotated data is difficult to obtain, costly
to produce, and often tied to a particular domain
that may vary greatly from that of the desired end
task. Additionally, current systems often utilize only
a small amount of the annotation for any particular
task. For instance, performing named entity recogni-
tion (NER) jointly with constituent parsing has been
shown to improve performance on both tasks, but
the only aspect of the syntax which is leveraged by
the NER component is the location of noun phrases
(Finkel and Manning, 2009). By instead discover-
ing a latent representation jointly with the end task
we address all of these concerns, alleviating the need
for any syntactic annotations, while simultaneously
attempting to learn a latent syntax relevant to both
the particular domain and structure of the end task.
We phrase the joint model as factor graph and
marginalize over the hidden structure of the inter-
mediate representation at both training and test time,
to optimize performance on the end task. Infer-
ence is done via loopy belief propagation, making
this framework trivially extensible to most graph
structures. Computation over latent syntactic rep-
810
resentations is made tractable with the use of special
combinatorial factors which implement unlabeled
variants of common dynamic-programming parsing
algorithms, constraining the hidden representation
to realize valid dependency graphs or constituency
trees.
We apply this strategy to two common NLP tasks,
coupling a model for the end task prediction with
latent and general syntactic representations via spe-
cialized logical factors which learn associations be-
tween latent and observed structure. In comparisons
with identical models which observe ?gold? syntac-
tic annotations, derived from off-the-shelf parsers or
provided with the corpora, we find that our hidden
marginalization method is comparable in both tasks
and almost every language tested, sometimes signifi-
cantly outperforming models which observe the true
syntax.
The following sections serves as a preliminary,
introducing an inventory of factors and variables
for constructing factor graph representations of
syntactically-coupled NLP tasks. Section 3 explores
the benefits of this method on relation extraction
(RE), where we compare the use dependency and
constituency structure as latent representations. We
then turn to a more established semantic role label-
ing (SRL) task (?4) where we evaluate across a wide
range of languages.
2 Latent Pseudo-Syntactic Structure
The models presented in this paper are phrased in
terms of variables in an undirected graphical model,
Markov random field. More specifically, we imple-
ment the model as a factor graph, a bipartite graph
composed of factors and variables in which we can
efficiently compute the marginal beliefs of any vari-
able set with the sum-product algorithm for cyclic
graphs, loopy belief propagation,. We now intro-
duce the basic variable and factor components used
throughout the paper.
2.1 Latent Dependency Structure
Dependency grammar is a lexically-oriented syn-
tactic formalism in which syntactic relationships
are expressed as dependencies between individual
words. Each non-root word specifies another as
its head, provided that the resulting structure forms
a valid directed graph, ie. there are no cycles in
the graph. Due to the flexibility of this representa-
tion it is often used to describe free-word-order lan-
guages, and increasingly preferred in NLP for more
language-in-use scenarios. A dependency graph can
be modeled with the following nodes, as first pro-
posed by Smith and Eisner (2008):
? Let {Link(i, j) : 0 ? i ? j ? n, n 6= j}
be O(n2) boolean variables corresponding to
the possible links in a dependency parse. Li,j
= true implies that there is a dependency from
parent i to child j.
? Let {LINK(i, j) : 0 ? i ? j ? n, n 6= j}
be O(n2) unary factors, each paired with a cor-
responding Link(i, j) variable and expressing
the independent belief that Link(i, j) = true.
2.2 Latent Constituency Structure
Alternatively we can describe the more structured
constituency formalism by setting up a representa-
tion over span variables:
? Let {Span(i, j) : 0 ? i < j ? n} be O(n2)
boolean variables such that Span(i, j) = true
iff there is a bracket spanning i to j 1.
? Let {SPAN(i, j) : 0 ? i < j ? n} be O(n2)
unary factors, each attached to the correspond-
ing Span(i, j) variable. These factors score the
independent suitability of each span to appear
in an unlabeled constituency tree.
All boolean variables presented in this paper will
be paired to unary factors in this manner, which
we will omit in future descriptions. This encom-
passes the necessary representational structure for
both syntactic formalisms, but nothing introduced
up to this point guarantees that either of these rep-
resentations will form a valid tree or DAG.
2.3 Combinatorial Factors
Naively constraining these latent representations
through the introduction of many interconnected
ternary factors is possible, but would likely be com-
putationally intractable. However, as observed in
1In practice, we do not need to include variables for spans
of width 1 or n, since they will always be true.
811
Smith and Eisner (2008), we can encapsulating
common dynamic programming algorithms within
special-purpose factors to efficiently globally con-
strain variable configurations . Since the outgoing
messages from such factors to a variable can be com-
puted from the factor?s posterior beliefs about that
variable, there is no difficulty in exchanging beliefs
between these special-purpose factors and the rest
of the graph, and inference can proceed using the
standard sum-product or max-product belief prop-
agation. Here we present two combinatorial factors
that provide efficient ways of constraining the model
to fit common syntactic frameworks.
? Let CKYTREE be a global combinatorial fac-
tor, as used in previous work in efficient pars-
ing (Naradowsky and Smith, 2012), attached to
all the Span(i, j) variables. This factor con-
tributes a factor of 1 to the model?s score iff the
span variables collectively form a legal, binary
bracketing and a factor of 0 otherwise. It en-
forces, therefore, a hard constraint on the vari-
ables, computing beliefs via an unlabeled vari-
ant of the inside-outside algorithm.
? Let DEP-TREE be a global combinatorial fac-
tor, as presented in Smith and Eisner (2008),
which attaches to all Link(i, j) variables and
similarly contributes a factor of 1 iff the config-
uration of Link variables forms a valid projec-
tive dependency graph. A graph is projective if
its edges do not cross.
2.4 Marginal MAP Inference
It is straightforward to train these latent variable
models to maximize the marginal probability of their
outputs, conditioning on their inputs, and marginal-
izing out the latent syntactic variables. To compute
feature expectations, we can use marginal inference
techniques such as sampling and sum-product belief
propagation to compute marginal probabilities.
A knottier problem arises when we want to find
the best assignment to the variables of interest
while marginalizing out ?nuisance? latent variables.
This is the problem of marginal MAP inference?
sometimes known as consensus decoding?which
has been shown to be NP-hard and without a poly-
nomial time approximation scheme (Sima?an, 1996;
Casacuberta and Higuera, 2000). In the NLP com-
munity, these inference problems often arise when
dealing with spurious ambiguity where multiple
derivations can lead to the same derived structure. In
tree substitution grammars, for instance, there may
be many ways of combining elementary trees to pro-
duce the same output tree; in machine translation,
many different elementary phrases or elementary
tree pairs might produce the same output string. For
syntactic parsing, Goodman (1996) proposed a vari-
ational method for summing out spurious ambiguity
that was equivalent to minimum Bayes risk decoding
(Goel and Byrne, 2000; Kumar and Byrne, 2004)
with a constituent-recall loss function. For MT,
May and Knight (2006) proposed methods for de-
terminizing tree automata to reduce ambiguity, and
Li et al2009) proposed a variational method based
on n-gram loss functions. More recently, Liu and Ih-
ler (2011) analyzed message-passing algorithms for
marginal MAP.
In this paper, we adopt a simple minimum Bayes
risk decoding scheme. First, we perform sum-
product belief propagation on the full factor graph.
Then, we maximize the expected accuracy of the
variables of interest, subject to any hard constraints
on them (such as mutual exclusion among labels). In
some cases with complex combinatorial constraints,
this simple MBR scheme has proved more effec-
tive than exact decoding over all variables (Auli and
Lopez, 2011).
3 Relation Extraction
Performing a syntax-based NLP task in most real-
world scenarios requires that the incoming data first
be parsed using a pre-trained parsing model. For
some tasks, like relation extraction, many data sets
lack syntactic annotation and these circumstances
persist even into the training phase. In this sec-
tion we explore such scenarios and contrast the use
of parser-provided syntactic annotation to marginal-
izing over latent representations of constituency or
dependency syntax. We show the hidden syntactic
models are not just competitive with these ?oracle?
models, but in some configurations can actually out-
perform them.
Relation extraction is the task of identifying se-
mantic relations between sets of entities in text (as
812
illustrated in Fig. 1b), and a good proving ground
for latent syntactic methods for two reasons. First,
because entities share a semantic relationship, un-
der most linguistic analyses these entities will also
share some syntactic relation. Indeed, syntactic fea-
tures have long been an extremely useful source
of information for relation extraction systems (Cu-
lotta and Sorensen, 2004; Mintz et al2009). Sec-
ondly, relation extraction has been a common task
for pioneering efforts in processing data mined from
the internet, and otherwise noisy or out-of-domain
data. In particular, large noisily-annotated data sets
have been generated by leveraging freely available
knowledge bases such as Freebase (Bollacker et al
2008; Mintz et al2009). Such data sets have been
utilized successfully for relation extraction from the
web (Bunescu and Mooney, 2007).
3.1 Model
We present a simple model for representing rela-
tional structure, with the only variables present be-
ing a set of boolean-valued variables representing an
undirected dependency between two entities, and an
additional set of boolean label variables representing
the type label of the relation.
? Let {Rel(i, j : 0 ? i < j ? n} be O(n2)
boolean variables such that Rel(i, j) = true iff
there is a relation spanning i to j.
? Let {Rel-Label(i, j, ?) : ? ? L, and 0 ? i <
j ? n} be O(|L|n2) boolean variables such
that Rel-Label(i, j, ?) = true iff there is a re-
lation spanning i to j with relation type ?.
? Let {ATMOST1(i, j) : 0 ? i < j ? n} be
O(n2) factors, each coordinating the set L of
possible nonterminal variables to the Rel vari-
able at each i, j tuple, allowing a Rel-Label
variable to be true iff all other label variables
are false and Rel(i, j) = true.
Here the Rel(i, j) and Rel-Label(i, j) variables
simply express the representation of the problem,
while the ATMOST1 factors are logical constraints
ensuring that only one label will apply to a particu-
lar relation.
3.2 Coordination Factors
An important contribution of this work is the intro-
duction of a flexible, general framework for connect-
ing the latent and observable partitions of the model.
We accomplish this through the use of two addi-
tional factors, each expressing the same basic logic,
which learn when to coordinate and when to ignore
correlations between the latent syntax and the end
task. While here we specify binary and ternary ver-
sions of these factors, they also generalize to higher
dimensions.
? Let {D-CONNECT(i, j, k) : 0 ? i < j ?
n; 0 ? k ? n} be O(n3) factors coordinating
any number of dependency syntax Link(i, j)
variables with representational variables on the
end task, multiplying in 1 to the model score
unless all variables are on, in which case it mul-
tiplies a connective potential ? derived from
its features. Thus it functions logically as a
soft NAND factor. In this ternary formulation k
represents a hidden dependency head or pivot
which is shared between two syntactic depen-
dencies anchored at the indices of the entities
in the relation (as illustrated in Fig. 1).
? Let {C-CONNECT(i, j) : 0 ? i < j ?
n} be O(n2) factors coordinating syntactic
Span(i, j) and relation arc Rel(i, j), identi-
cally to D-CONNECT but with a 1-to-1 map-
ping. Intuitively the joint model might learn
? > 1, i.e., constituency spans and task predic-
tion relations are more likely to be coterminous.
The difficulty in working with latent dependency
syntax is that we posit that the RE variables do not
share a 1-to-1 mapping with variables in the hid-
den representation. We expect instead, according
to linguistic intuition, that a relation between enti-
ties at position i and j in the sentence should have
corresponding syntactic dependencies but that they
are likely to realize this by sharing the same head
word (as depicted in Fig.1), a word whose identity
should help label the relation. Therefore we intro-
duce a special coordination factor, D-CONNECT as
a ternary factor to capture the relationship between
pairs of latent syntactic variables and a single rela-
tion variable, pivoting on the same unknown head
word.
813
Figure 1: Latent Dependency coupling for the RE task.
The D-CONNECT factor expresses ternary connection re-
lations because the shared head word of the proposed re-
lation is unknown. As is convention, variables are repre-
sented by circles, factors by rectangles.
We introduce six model scenarios.
? Baseline, simply the arc-factored model con-
sisting only of Rel and corresponding Label
variables for each entity. Features on the re-
lation factors, which are common to all model
configurations, are combinations of lexical in-
formation (i.e., the words that form the entity,
the pos-tags of the entities, etc.) as well as the
distance between the relation. This is a light-
weight model and generally does not attempt
to exhaustively leverage all possible proven
sources of useful features (Zhou et al2005)
towards a higher absolute score, but rather to
serve as a point of comparison to the models
which rely on syntactic information.
? Baseline-Ent, a variant of Baseline with addi-
tional features which include combinations of
mention type, entity type, and entity sub-type.
? Oracle D-Parse, in which we also instantiate a
full set of latent dependency syntax variables,
and connect them to the baseline model us-
ing D-CONNECT factors. Syntax variables are
clamped to their true values.
? Oracle C-Parse, the constituency syntax ana-
logue of Oracle D-Parse.
? Hidden D-Parse, which is an extension of Or-
acle D-Parse in which we connect all syntax
variables to a DEP-TREE factor, syntax vari-
ables are unobserved, and are learned jointly
with the end task. The features for latent syntax
are a subset of those used in dependency pars-
ing (McDonald et al2005).
? Hidden C-Parse, the constituency syntax ana-
logue of Hidden D-Parse. The feature set is
similar but bigrams are taken over the words
defining the constituent span, rather than the
words defining the head/modifier relation.
Coordination factor features for the syntactically-
informed models are particularly important. This
became evident in initial experiments where the
baseline was often able to outperform the hidden
syntactic model. However, inclusion of entity and
mention label features into the connection factors
provides the model with greater reasoning over
when to coordinate or ignore the relation predictions
with the underlying syntax. These are a proper sub-
set of the Baseline-Ent features.
3.3 Data
We evaluate these models using the 2005 Auto-
matic Content Extraction (ACE) data set (Walker,
2006), using the English (dual-annotated) and Chi-
nese (solely annotator #1 data set) sections. Each
corpus is annotated with entity mentions?tagged as
PER, ORG, LOC, or MISC?and, where applica-
ble, what type of relation exists between them (e.g.,
coarse: PHYS; fine: Located). But like most cor-
pora available for the task, the burden of acquiring
corresponding syntactic annotation is left to the re-
searcher. In this situation it is common to turn to
existing pre-trained parsing models.
We generate our data by first splitting the raw
text paragraphs into sentences. Chinese sentences
814
ACE Results
English Chinese
Unlabeled Labeled Unlabeled Labeled
Model P R F1 P R F1 P R F1 P R F1
Baseline 85.4 57.0 68.4 83.0 55.3 66.4 42.9 26.8 33.0 42.6 21.3 28.4
Baseline-Ent 87.2 65.4 74.8 85.8 64.4 73.6 55.2 31.1 39.8 51.2 29.4 37.4
Oracle D-Parse 89.3 67.4 76.8 89.3 66.2 75.4 60.0 32.6 42.2 58.1 31.3 40.7
Hidden D-Parse 87.8 69.8 77.7 85.3 67.8 75.6 48.0 32.0 38.4 47.2 30.0 36.7
Oracle C-Parse 89.1 68.7 77.6 87.5 67.5 76.2 66.8 37.8 48.3 63.8 37.0 46.8
Hidden C-Parse 90.5 69.9 78.9 88.8 68.6 77.4 56.3 32.3 41.0 53.4 31.6 39.7
Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the
syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on
the smaller Chinese data set.
are also tokenized according to Penn Chinese Tree-
bank standards (Xue et al2005). The sentences are
then tagged and parsed using the Stanford CoreNLP
tools, using the standard pre-trained models for tag-
ging (Toutanvoa and Manning, 2000), and the fac-
tored parsing model of Klein and Manning (2002).
The distributed grammar is trained on a variety of
sources, including the standard Wallstreet Journal
corpus, but also biomedical, translation, and ques-
tions. We then apply entity and relation annota-
tions noisily to the data, collapsing multi-word en-
tities into one term. We filter out sentences with
fewer than two entities (and are thus incapable of
containing relations) and sentences with more than
40 words (to keep the parses more reliable). This
yields 6966 sentences for English data, but unfortu-
nately only 747 sentences for the Chinese. Nine of
every ten sentences comprise the training set, with
every tenth sentence reserved for test.
3.4 Results
We train all models using 20 iterations of stochastic
gradient descent, each with a maximum of 10 BP it-
erations (though in practice we find convergence to
often occur much earlier). The results are presented
in Table 1, showing precision, recall, and F-measure
for both labeled and unlabeled prediction. For En-
glish, not only is the hidden marginalization method
a suitable replacement for the syntactic trees pro-
vided by pre-trained, state-of-the-art models, but in
both configurations we find that inducing an optimal
hidden structure is preferable to the parser-produced
annotations. On Chinese, where the data set is atyp-
ically small, we still observe improved performance
over the baseline in the constituency-based model
though it is not able to match the observed syntax
model.
Despite the intuition that both entities occupy
roles as modifiers of the same verb, we find that
the Hidden D-Parse model often fails to recover the
correct latent structure, and that even when success-
ful dependency parses are observed, the head word
is often not uniquely indicative of the relation type
(as known is not strongly correlated with the relation
type EMPLOYS in the phrase: Shigeru Miyamoto,
best known for his work at the video game company
Nintendo). Hence when it comes to relation extrac-
tion, at least on our relatively small data sets, we find
the simplest approach to latent syntactic structure is
the best.
We now turn to the task of semantic role label-
ing to evaluate this method on a more established
hand-annotated data set, and a more varied set of
languages.
4 Semantic Role Labeling
The task of semantic role labeling (SRL) aims to
detect and label the semantic relationships between
particular words, most commonly verbs (referred to
in the domain as predicates), and their arguments
(Meza-Ruiz and Riedel, 2009).
In a manner similar to RE, there is a strong corre-
lation between the presence of an SRL relation and
there existing an underlying syntactic dependency,
though this is not always expressed as directly as a
1-to-1 correspondence. This has historically moti-
vated a reliance on syntactic annotation, and some
of the most successful methods have simply applied
815
Pred
5
At Most 1
sense.
01
sense.
02
sense. 
|S|
.
d.) Sense Prediction
Arg
5, 1
Arg
5, 2
role
A0
role
A1
.  .  .
c.) Argument Prediction
b.) Syntactic Layer
Link
5, 1
D-Connect
5, 1
a.) Syntactic Combinatorial Constraint
DEP-Tree
Link
5, 2
D-Connect
5, 2
Link
5, 3
D-Connect
5, 3
At Most 1
Arg
5, 3
Link
5, n
role
A2
.   .   .
D-Connect
5, n
role
A-TM
.
.
Figure 2: A tiered graphic representing the three different SRL model configurations. The baseline system is described
in the bottom (c & d), the separate panels highlighting the independent predictions of this model: sense labels are
assigned in an entirely separate process from argument prediction. Pruning in the model takes place primarily in
this tier, since we observe true predicates we only instantiate over these indices. The middle tier (b.) illustrates the
syntactic representation layer, and the connective factors between syntax and SRL. In the observed syntax model
the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid
tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic
variables. In this scenario all labels in (b.) are hidden at both training and test time.
feature-rich classifiers to the parsed trees. Related
work has recognized the large annotation burden the
task demands, but aimed to keep the syntactic anno-
tations and induce semantic roles (Lang and Lapata,
2010). In this section we will take the opposite ap-
proach, disregarding the syntactic annotations which
we argue are more costly to acquire, as they require
more formal linguistic training to produce.
4.1 Model
We present a simple, flexible model for SRL in
which sense predictions are made independently of
the rest of the model, and argument predictions are
made independently of each other. The model struc-
ture is composed as depicted in Fig. 2.
? Let {Arg(i, j) : 0 ? i < j ? n} be O(n2)
boolean variables such that Arg(i, j) = true
iff predicate i takes token j as an argument.
? Let {Role(i, j, ?) : ? ? L, and 0 ? i <
j ? n} be O(|L|n2) boolean variables such
that Role(i, j, ?) = true iff Arg(i, j) is true
and takes the role label ?.
? Let {Sense(i, ?) : ? ? S, and 0 ? i ?
n} be O(|S|n) boolean variables such that
Sense(i, ?) = true iff predicate i has sense
?.
4.1.1 Features
At the coarsest level both the SRL and RE models
are specifying binary predictions between a pair of
indices in the sentence, and a set of labels for each
dependency that happens to be true. Similarly we
use almost identical features in SRL as we did in
816
Figure 3: Examining the learned hidden representation for SRL. In this example the syntactic dependency arcs derived
from gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shown
in the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correct
predictions. In contrast, the hidden model structure (right) learns a representation that closely parallels the desired end
task predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correct
prediction, with true labels GA, KARA, etc.), and providing some evidence towards the fourth. The dependency tree
corresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs> 0.5
are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity).
RE, with the sole exception that we incorporate the
observable lemma and morphological features into
bigrams on predicate/argument pairs. For sense pre-
diction we rely only on unigram features taken in a
close (w = 2) window of the target predicate.
For the coordinating factors we use subsets of
combinations of word, part-of-speech, and capital-
ization features taken between head and argument,
and concatenate these with the distance and direc-
tion between the predicate and argument. We do not
find the performance of the system to be as sensi-
tive to which features are present in the coordinating
factors as we did in the RE task.
4.2 Data
We evaluate our SRL model using the data set devel-
oped for the CoNLL 2009 shared task competition
(Hajic? et al2009), which features seven languages
and provides an ideal opportunity to measure the
ability of the hidden structure to generalize across
languages of disparate origin and varied character-
istics. It also provides the opportunity to observe
a variety of different annotation styles and biases,
some of which our model was able to uncover as ill-
suited to common models for the task. The data it-
self provides word, lemma, part-of-speech, and mor-
phological feature information, along with gold de-
pendency parses. Words which denote predicates are
identified, and their (train time) arguments are pro-
vided. They are also annotated with a sense label
for each predicate, which is scored as an additional
SRL dependency. Thus the task involves predicting
for each predicate a set of argument dependencies
and the sense label associated with that predicate.
817
Unlabeled Labeled CoNLL 2009 F1
Data Model P R F1 P R F1 MAX. MEAN MED.
Catalan
Baseline 92.20 62.43 74.48 73.80 58.76 65.43
Oracle Syn. 98.48 96.17 97.31 70.42 68.78 69.59 80.3 71.0 74.1
Hidden Syn. 95.21 92.84 94.01 68.86 67.15 67.99
Chinese
Baseline 72.48 64.82 68.44 65.97 59.00 62.29
Oracle Syn. 98.57 78.98 87.69 87.64 70.22 77.97 78.6 72.2 70.4
Hidden Syn. 90.79 79.09 84.53 81.97 71.40 76.32
Czech
Baseline 97.73 56.50 71.61 84.80 48.80 61.84
Oracle Syn. 98.62 81.25 89.09 92.94 68.25 74.84 85.4 72.4 71.7
Hidden Syn. 92.39 89.35 90.85 74.41 71.96 73.16
English
Baseline 92.46 71.56 80.68 84.56 65.45 73.78
Oracle Syn. 96.75 82.25 88.91 85.48 72.67 78.55 85.6 75.6 72.1
Hidden Syn. 95.06 79.06 86.32 83.82 69.72 76.12
German
Baseline 93.49 44.24 60.06 75.00 35.49 48.18
Oracle Syn. 95.18 79.11 86.41 73.24 60.87 66.49 79.7 68.1 67.8
Hidden Syn. 91.92 86.26 89.00 69.47 65.19 67.26
Japanese
Baseline 91.64 43.36 58.87 80.41 38.05 51.66
Oracle Syn. 93.84 48.15 63.64 90.06 46.21 61.08 78.2 62.7 72.0
Hidden Syn. 90.88 73.47 81.25 73.42 59.36 65.65
Spanish
Baseline 82.90 39.47 53.48 67.64 32.21 43.64
Oracle Syn. 98.96 94.19 96.52 70.68 67.27 68.93 80.5 70.4 73.4
Hidden Syn. 96.15 90.53 93.25 68.81 64.79 66.74
Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained
using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where
poor sense prediction hindered absolute performance.
4.3 Results
We evaluate across a set of model configurations
analogous to before. All experiments used 30 itera-
tions of SGD with a Gaussian prior, and a max 10 it-
erations of BP to compute the marginals for each ex-
ample. In comparison to the CoNLL competition en-
tries (Table 2, rightmost columns) our syntactically-
informed models generally fall in the middle of the
rankings. This is not surprising given the indepen-
dent predictions of the model and the very general,
language universal assumptions we have made in the
model structure and feature sets. However, in terms
of gauging the usefulness of the hidden syntactic
marginalization method the results are extremely
compelling, with only marginal differences between
the performance of the observed-syntax model, es-
pecially relative to the baseline.
And despite the simplicity of the model, we still
manage to perform at state-of-the-art levels in a
few instances, sometimes outperforming most of the
competition entries without observing any syntax.
The performance on Chinese is an example of this,
with our system outperforming all but the best sys-
tem, and the hidden syntactic model only slightly
behind.
Abstracting away from the performance compar-
isons against other systems, the unlabeled results are
the more revealing evidence for the use of hidden
syntactic structure. Here the average hidden model
score (88.89) almost outperforms the observed syn-
tax model (90.22, and vs. 66.80 baseline), mostly
due to the large margins on the unlabeled Japanese
scores. The strong independence between sense
prediction and argument prediction hinders perfor-
mance on the labeled task, but on all languages we
find an extremely significant improvement exploit-
ing hidden syntactic structure in comparison to the
baseline system?the hidden model recovers more
than 92% of the gap between the baseline and the
observed syntax model. It is also interesting to note
that in the shared task competition the two languages
which systems lost the most performance between
their parsing F1 and their SRL F1 were Japanese
and German. As illustrated in Fig. 3, the corre-
818
spondence between syntax and SRL are extremely,
and systematically, poor. In this example our hid-
den structure model was able to assign strong beliefs
to the latent syntactic variables which correspond to
the correct predicate/argument pairs, allowing it to
correctly identify three of the four SRL arguments
when the joint model failed to recover one.
5 Related Work
This work is perhaps mostly closely related to
the Learning over Constrained Latent Representa-
tions (LCLR) framework of Chang et al2010).
Their abstract problem formulation is identical: both
paradigms seek to couple the end task to an interme-
diate representation which is not accessible to the
learning algorithm. However much of the intent,
scale, and methodology is different. LCLR aims
to provide a flexible latent structure for increasing
the representational power of the model in a use-
ful way, and is demonstrated on tasks and domains
where data availability is not a key concern. In con-
trast, while our hidden structure models may outper-
form their observed syntax counterparts, our focus
is as much on alleviating the burden of procuring
large amounts of syntactic annotation as it is about
increasing the expressiveness of the model. To that
end we constrain a more sophisticated latent repre-
sentation and couple it to highly structured output
predictions, opposed to binary classification prob-
lems. In methodology, we perform the more com-
putationally intensive marginalization operation in-
stead of maximizing.
Marginalization of hidden structure is also funda-
mental to other work, and featured most prominently
in generative Bayesian latent variable models (Teh
et al2006). Our approach is trained discrimina-
tively, affording the use of very rich feature sets and
the prediction of partial structures without needing
to specify a full derivation. Similar approaches have
been used in more linear latent variable CRF-based
models (McCallum et al2005), but these must only
marginalize only over hidden states of a much more
compact representation. Naively extending this to
tree-based constraints would often be computation-
ally inefficient, and we avoid intractability through
the encapsulation of much of the dynamic program-
ming machinery into specialized factors. Moreover,
using loopy belief propagation means that the in-
ference method is not closely coupled to the task
structure, and need not change when applying this
method to other types of graphs.
6 Conclusion
We have presented a novel method of coupling
syntactically-oriented NLP tasks to combinatorially-
constrained hidden syntactic representations, and
have shown that marginalizing over this latent rep-
resentation not only provides significant improve-
ments over syntactically-uninformed baselines, but
occasionally improves performance when compared
to systems which observe syntax. On the task of
relation extraction we find that a constituency rep-
resentation provides the most improvement over the
baseline, while in the SRL domain our model is ex-
tremely competitive with the best reported results on
Chinese, and outperforms the model using the pro-
vided parses on German and Japanese.
We believe this method delivers very promising
results in our presented tasks, opening the door to
new lines of research examining what types of con-
straints and what configurations of hidden struc-
ture are most beneficial for particular tasks and lan-
guages. Moreover, we present one type of coordinat-
ing factor, as both D-CONNECT and C-CONNECT
logically express a soft NAND function, but more
sophisticated coupling schemes are another natural
direction to pursue. Finally, we use sum-product
variant of belief propagation inference, but more
specialized inference schemes may show additional
benefits.
Acknowledgements
We would like to thank Andrea Gesmundo for help in
procuring sections of the CoNLL 2009 shared task data.
This work was supported in part by the Center for Intel-
ligent Information Retrieval and in part by Army prime
contract number W911NF-07-1-0216 and University of
Pennsylvania subaward number 103-548106. The Uni-
versity of Massachusetts also gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
view of the DARPA, AFRL, or the US government.
819
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In ACL,
pages 470?480.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In SIGMOD, pages 1247?1250, New
York, NY, USA. ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In ACL.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In ICGI, pages 15?24.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained latent
representations. In NAACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL, Barcelona,
Spain.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In
NAACL, pages 326?334.
Vaibbhava Goel and William J. Byrne. 2000. Minimum
Bayes risk automatic speech recognition. Computer
Speech and Language, 14(2):115?135.
Joshua T. Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177?183.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL: Shared Task,
pages 1?18.
Dan Klein and Chris Manning. 2002. Fast exact infer-
ence with a factored model for natural language pro-
cessing. In NIPS.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, pages 169?176.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In HLT-NAACL, pages 939?
947.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Qiang Liu and Alexander Ihler. 2011. Variational algo-
rithms for marginal MAP. In UAI, pages 453?462.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. In HLT-NAACL, pages 351?358.
Andrew McCallum, Kedar Bellare, and Fernando C. N.
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In UAI, pages 388?395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT-EMNLP,
pages 523?530.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL, pages 1003?1011.
Jason Naradowsky and David A. Smith. 2012. Combina-
torial constraints for constituency parsing in graphical
novels. Technical report, University of Massachusetts
Amherst.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175?1180.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Kristina Toutanvoa and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP, pages 63?
70.
Christopher Walker. 2006. Ace 2005 multilingual train-
ing corpus. number ldc2006t06. In Linguistic Data
Consortium.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, pages 207?238.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In NAACL, pages 288?295.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In ACL, pages 427?434.
820
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 885?894,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Discriminative Model for Joint Morphological Disambiguation and
Dependency Parsing
John Lee
Department of Chinese,
Translation and Linguistics
City University of Hong Kong
jsylee@cityu.edu.hk
Jason Naradowsky, David A. Smith
Department of Computer Science
University of Massachusetts, Amherst
{narad,dasmith}@cs.umass.edu
Abstract
Most previous studies of morphological dis-
ambiguation and dependency parsing have
been pursued independently. Morphological
taggers operate on n-grams and do not take
into account syntactic relations; parsers use
the ?pipeline? approach, assuming that mor-
phological information has been separately
obtained.
However, in morphologically-rich languages,
there is often considerable interaction between
morphology and syntax, such that neither can
be disambiguated without the other. In this pa-
per, we propose a discriminative model that
jointly infers morphological properties and
syntactic structures. In evaluations on various
highly-inflected languages, this joint model
outperforms both a baseline tagger in morpho-
logical disambiguation, and a pipeline parser
in head selection.
1 Introduction
To date, studies of morphological analysis and
dependency parsing have been pursued more or
less independently. Morphological taggers dis-
ambiguate morphological attributes such as part-
of-speech (POS) or case, without taking syntax
into account (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001); dependency parsers commonly assume
the ?pipeline? approach, relying on morphologi-
cal information as part of the input (Buchholz and
Marsi, 2006; Nivre et al, 2007). This approach
serves many languages well, especially those with
less morphological ambiguity. In English, for ex-
ample, accuracy of POS tagging has risen above
97% (Toutanova et al, 2003), and that of depen-
dency parsing has reached the low nineties (Nivre
et al, 2007). For these languages, there may be little
to be gained to justify the computational cost of in-
corporating syntactic inference during the morpho-
logical tagging task; conversely, it is doubtful that
errorful morphological information is a main cause
of errors in English dependency parsing.
However, the pipeline approach seems more prob-
lematic for morphologically-rich languages with
substantial interactions between morphology and
syntax (Tsarfaty, 2006). Consider the Latin sen-
tence, Una dies omnis potuit praecurrere amantis,
?One day was able to make up for all the lovers?1. As
shown in Table 1, the adjective omnis (?all?) is am-
biguous in number, gender, and case; there are seven
valid analyses. From the perspective of a finite-
state morphological tagger, the most attractive anal-
ysis is arguably the singular nominative, since omnis
is immediately followed by the singular verb potuit
(?could?). Indeed, the baseline tagger used in this
study did make this decision. Given its nominative
case, the pipeline parser assigned the verb potuit to
be its head; the two words form the typical subject-
verb relation, agreeing in number.
Unfortunately, as shown in Figure 1, the word om-
nis in fact modifies the noun amantis, at the end of
the sentence. As a result, despite the distance be-
tween them, they must agree in number, gender and
case, i.e., both must be plural masculine (or femi-
nine) accusative. The pipeline parser, acting on the
input that omnis is nominative, naturally did not see
1Taken from poem 1.13 by Sextus Propertius, English trans-
lation by Katz (2004).
885
Latin Una dies omnis potuit praecurrere amantis
English one day all could to surpass lovers
Number sg pl sg pl sg sg pl sg - sg pl
Gender f n m/f m/f m/f m/f/n m/f - - m/f/n m/f
Case nom/ab nom/acc nom nom/acc nom gen acc - - gen acc
Table 1: The Latin sentence ?Una dies omnis potuit praecurrere amantis?, meaning ?One day was able to make up
for all the lovers?, shown with glosses and possible morphological analyses. The correct analyses are shown in bold.
The word omnis has 7 possible combinations of number, gender and case, while amantis has 5. Disambiguation partly
depends on establishing amantis as the head of omnis, and so the two must agree in all three attributes.
this agreement, and therefore did not consider this
syntactic relation likely.
Such a dilemma is not uncommon in languages
with relatively free word order. On the one hand,
it appears difficult to improve morphological tag-
ging accuracy on words like omnis without syntactic
knowledge; on the other hand, a parser cannot reli-
ably disambiguate syntax unless it has accurate mor-
phological information, in this example the agree-
ment in number, gender, and case.
In this paper we propose to attack this chicken-
and-egg problem with a discriminative model that
jointly infers morphological and syntactic properties
of a sentence, given its words as input. In eval-
uations on various highly-inflected languages, the
model outperforms both a baseline tagger in mor-
phological disambiguation, and a pipeline parser in
head selection.
After a description of previous work (?2), the
joint model (?3) will be contrasted with the base-
line pipeline model (?4). Experimental results (?5-
6) will then be presented, followed by conclusions
and future directions.
2 Previous Work
Since space does not allow a full review of the vast
literature on morphological analysis and parsing, we
focus only on past research involving joint morpho-
logical and syntactic inference (?2.1); we then dis-
cuss Latin (?2.2), a language representative of the
challenges that motivated our approach.
2.1 Joint Morphological and Syntactic
Inference
Most previous work in morphological disambigua-
tion, even when applied on morphologically com-
plex languages with relatively free word order,
potuit
could
dies
day
una
one
praecurrere
to surpass
amantis
lovers
omnis
all
Figure 1: Dependency tree for the sentence ?Una dies
omnis potuit praecurrere amantis?. The word omnis is
an adjective modifying the noun amantis. This informa-
tion is key to the morphological disambiguation of both
words, as shown in Table 1.
such as Turkish (Hakkani-Tu?r et al, 2000) and
Czech (Hajic? et al, 2001), did not consider syn-
tactic relationships between words. In the litera-
ture on data-driven parsing, two recent studies at-
tempted joint inference on morphology and syntax,
and both considered phrase-structure trees for Mod-
ern Hebrew (Cohen and Smith, 2007; Goldberg and
Tsarfaty, 2008).
The primary focus of morphological processing in
Modern Hebrew is splitting orthographic words into
morphemes: clitics such as prepositions, pronouns,
and the definite article must be separated from the
core word. Each of the resulting morphemes is then
tagged with an atomic ?part-of-speech? to indicate
word class and some morphological features. Sim-
ilarly, the English POS tags in the Penn Treebank
combine word class information with morphologi-
886
cal attributes such as ?plural? or ?past tense?.
Cohen and Smith (2007) separately train a dis-
criminative conditional random field (CRF) for seg-
mentation and tagging, and a generative probabilis-
tic context-free grammar (PCFG) for parsing. At de-
coding time, the two models are combined as a prod-
uct of experts. Goldberg and Tsarfaty (2008) pro-
pose a generative joint model. This paper is the first
to use a fully discriminative model for joint morpho-
logical and syntactic inference on dependency trees.
2.2 Latin
Unlike Modern Hebrew, Latin does not require ex-
tensive morpheme segmentation2. However, it does
have a relatively free word order, and is also highly
inflected, with each word having up to nine morpho-
logical attributes, listed in Table 2. In addition to its
absolute numbers of cases, moods, and tenses, Latin
morphology is fusional. For instance, the suffix
?is in omnis cannot be segmented into morphemes
that separately indicate gender, number, and case.
According to the Latin morphological database en-
coded in MORPHEUS (Crane, 1991), 30% of Latin
nouns can be parsed as another part-of-speech, and
on average each has 3.8 possible morphological in-
terpretations.
We know of only one previous attempt in data-
driven dependency parsing for Latin (Bamman and
Crane, 2008), with the goal of constructing a dy-
namic lexicon for a digital library. Parsing is per-
formed using the usual pipeline approach, first with
the TreeTagger analyzer (Schmid, 1994) and then
with a state-of-the-art dependency parser (McDon-
ald et al, 2005). Head selection accuracy was
61.49%, and rose to 64.99% with oracle morpho-
logical tags. Of the nine morphological attributes,
gender and especially case had the lowest accu-
racy. This observation echoes the findings for
Czech (Smith et al, 2005), where case was also the
most difficult to disambiguate.
3 Joint Model
This section describes a model that jointly infers
morphological and syntactic properties of a sen-
tence. It will be presented as a graphical model,
2Except for enclitics such as -que, -ve, and -ne, but their
segmentation is rather straightforward compared to Modern He-
brew or other Semitic languages.
Attribute Values
Part-of- noun, verb, participle, adjective,
speech adverb, conjunction, preposition,
(POS) pronoun, numeral, interjection,
exclamation, punctuation
Person first, second, third
Number singular, plural
Tense present, imperfect, perfect,
pluperfect, future perfect, future
Mood indicative, subjunctive, infinitive,
imperative, participle, gerund,
gerundive, supine
Voice active, passive
Gender masculine, feminine, neuter
Case nominative, genitive, dative,
accusative, ablative, vocative,
locative
Degree comparative, superlative
Table 2: Morphological attributes and values for Latin.
Ancient Greek has the same attributes; Czech and Hun-
garian lack some of them. In all categories except POS,
a value of null (?-?) may also be assigned. For example, a
noun has ?-? for the tense attribute.
starting with the variables and then the factors,
which represents constraints on the variables. Let
n be the number of words and m be the number of
possible values for a morphological attribute. The
variables are:
? WORD: the n words w1,...,wn of the input sen-
tence, all observed.
? TAG: O(nm) boolean variables3 Ta,i,v, corre-
sponding to each value of the morphological at-
tributes listed in Table 2. Ta,i,v = true when
the word wi has value v as its morphological
attribute a. In Figure 2, CASE3,acc is the short-
hand representing the variable Tcase,3,acc. It is
set to true since the wordw3 has the accusative
case.
? LINK: O(n2) boolean variables Li,j corre-
sponding to a possible link between each pair
3The TAG variables were actually implemented as multino-
mials, but are presented here as booleans for ease of understand-
ing.
887
UNIGRAMCASE?
UNIGRAMCASE?
CASE?
LINK
CASE?
LINK
CASE?
LINK
CASE?
LINK
CASE    6,gen
CASE    3,gen
CASE    3,nom
 3,accCASE
UNIGRAMCASE?
UNIGRAMCASE?
UNIGRAMCASE?
CASE    2,...
CASE
LINK
CASE
 6,acc
CASE?
BIGRAMCASE?BIGRAM
TREE WORD?LINKWORDLINK
CASE    5,...
L L3,6 4,6
Figure 2: The joint model (?3) depicted as a graphical model. The variables, all boolean, are represented by circles and
are bolded if their correct values are true. Factors are represented by rectangles and are bolded if they fire. For clarity,
this graph shows only those variables and factors associated with one pair of words (i.e., w3=omnis and w6=amantis)
and with one morphological attribute (i.e., case). The variables L3,6, CASE3,acc and CASE6,acc are bolded, indicating
that w3 and w6 are linked and both have the accusative case. The ternary factor CASE-LINK, that connects to these
three variable, therefore fires.
of words4. Li,j = true when there is a depen-
dency link from the word wi to the word wj . In
Figure 2, the variable L3,6 is set to true since
there is a dependency link between the words
w3 and w6.
We define a probability distribution over all joint as-
signments A to the above variables,
p(A) =
1
Z
?
k
Fk(A) (1)
where Z is a normalizing constant. The assign-
ment A is subject to a hard constraint, represented
in Figure 2 as TREE, requiring that the values of
the LINK variables must yield a tree, which may
be non-projective. The factors Fk(A) represent soft
constraints evaluating various aspects of the ?good-
ness? of the tree structure implied by A. We say a
factor ?fires? when all its neighboring variables are
4Variables for link labels can be integrated in a straightfor-
ward manner, if desired.
true and it evaluates to a non-negative real num-
ber; otherwise, it evaluates to 1 and has no effect
on the product in equation (1). Soft constraints in
the model are divided into local and link factors, to
which we now turn.
3.1 Local Factors
The local factors consult either one word or two
neighboring words, and their morphological at-
tributes. These factors express the desirability of the
assignments of morphological attributes based on lo-
cal context. There are three types:
? TAG-UNIGRAM: There are O(nm) such unary
factors, each instance of which is connected to
a TAG variable. The factor fires when Ta,i,v
is true. The features consist of the value v
of the morphological attribute concerned, com-
bined with the word identity of wi, with back-
off using all suffixes of the word. The CASE-
UNIGRAM factors shown in Figure 2 are ex-
amples of this family of factors.
888
? TAG-BIGRAM: There are O(nm2) of such bi-
nary factors, each connected to the TAG vari-
ables of a pair of neighboring words. The factor
fires when Ta,i,v1 and Ta,i+1,v2 are both true.
The CASE-BIGRAM factors shown in Figure 2
are examples of this family of factors.
? TAG-CONSISTENCY: For each word, the TAG
variables representing the possible POS val-
ues are connected to those representing the val-
ues of other morphological attributes, yield-
ing O(nm2) binary factors. They fire when
Tpos,i,v1 and Ta,i,v2 are both true. These fac-
tors are intended to discourage inconsistent as-
signments, such as a non-null tense for a noun.
It is clear that so far, none of these factors are aware
of the morphological agreement between omnis and
amantis, crucial for inferring their syntactic relation.
We now turn our attention to link factors, which
serve this purpose.
3.2 Link Factors
The link factors consult all pairs of words, possibly
separated by a long distance, that may have a de-
pendency link. These factors model the likelihood
of such a link based on the word identities and their
morphological attributes:
? WORD-LINK: There areO(n2) such unary fac-
tors, each connected to a LINK variable, as
shown in Figure 2. The factor fires when Li,j
is true. Features include various combina-
tions of the word identities of the parent wi and
child wj , and 5-letter prefixes of these words,
replicating the so-called ?basic features? used
by McDonald et al (2005).
? POS-LINK: There are O(n2m2) such ternary
factors, each connected to the variables Li,j ,
Ti,pos,vi and Tj,pos,vj . It fires when all three are
true or, in other words, when the parent word
wi has POS vi, and the child wj has POS vj .
Features replicate all the so-called ?basic fea-
tures? used by McDonald et al (2005) that in-
volve POS. These factors are not shown in Fig-
ure 2, but would have exactly the same struc-
ture as the CASE-LINK factors.
Beyond these basic features, McDonald et al
(2005) also utilize POS trigrams and POS 4-
grams. Both include the POS of two linked
words, wi and wj . The third component in the
trigrams is the POS of each word wk located
between wi and wj , i < k < j. The two ad-
ditional components that make up the 4-grams
are subsets of the POS of words located to the
immediate left and right of wi and wj .
If fully implemented in our joint model, these
features would necessitate two separate fami-
lies of link factors: O(n3m3) factors for the
POS trigrams, and O(n2m4) factors for the
POS 4-grams. To avoid this substantial in-
crease in model complexity, these features are
instead approximated: the POS of all words
involved in the trigrams and 4-grams, except
those of wi and wj , are regarded as fixed, their
values being taken from the output of a mor-
phological tagger (?4.1), rather than connected
to the appropriate TAG variables. This approxi-
mation allows these features to be incorporated
in the POS-LINK factors.
? MORPH-LINK: There are O(n2m2) such
ternary factors, each connected to the variables
Li,j , Ti,a,vi and Tj,a,vj , for every attribute a
other than POS. The factor fires when all three
variables are true, and both vi and vj are non-
null; i.e., it fires when the parent word wi has
vi as its morphological attribute a, and the child
wj has vj . Features include the combination of
vi and vj themselves, and agreement between
them. The CASE-LINK factors in Figure 2 are
an example of this family of factors.
4 Baselines
To ensure a meaningful comparison with the joint
model, our two baselines are both implemented in
the same graphical model framework, and trained
with the same machine-learning algorithm. Roughly
speaking, they divide up the variables and factors of
the joint model and train them separately. For mor-
phological disambiguation, we use the baseline tag-
ger described in ?4.1. For dependency parsing, our
baseline is a ?pipeline? parser (?4.2) that infers syn-
tax upon the output of the baseline tagger.
889
4.1 Baseline Morphological Tagger
The tagger is a graphical model with the WORD
and TAG variables, connected by the local fac-
tors TAG-UNIGRAM, TAG-BIGRAM, and TAG-
CONSISTENCY, all used in the joint model (?3).
4.2 Baseline Dependency Parser
The parser has no local factors, but has the same
variables as the joint model and the same features
from all three families of link factors (?3). However,
since it takes as input the morphological attributes
predicted by the tagger, the TAG variables are now
observed. This leads to a change in the structure
of the link factors ? all features from the POS-
LINK factors now belong to the WORD-LINK fac-
tors, since the POS of all words are observed. In
short, the features of the parser are a replication of
(McDonald et al, 2005), but also extended beyond
POS to the other morphological attributes, with the
features in the MORPH-LINK factors incorporated
into WORD-LINK for similar reasons.
5 Experimental Set-up
5.1 Data
Our evaluation focused on the Latin Dependency
Treebank (Bamman and Crane, 2006), created at
the Perseus Digital Library by tailoring the Prague
Dependency Treebank guidelines for the Latin lan-
guage. It consists of excerpts from works by eight
Latin authors. We randomly divided the 53K-word
treebank into 10 folds of roughly equal sizes, with an
average of 5314 words (347 sentences) per fold. We
used one fold as the development set and performed
cross-validation on the other nine.
To measure how well our model generalizes
to other highly-inflected, relatively free-word-order
languages, we considered Ancient Greek, Hungar-
ian, and Czech. Their respective datasets consist of
8000 sentences from the Ancient Greek Dependency
Treebank (Bamman et al, 2009), 5800 from the
Hungarian Szeged Dependency Treebank (Vincze et
al., 2010), and a subset of 3100 from the Prague De-
pendency Treebank (Bo?hmova? et al, 2003).
5.2 Training
We define each factor in (1) as a log-linear function:
Fk(A) = exp
?
h
?hfh(A,W, k) (2)
Given an assignment A and words W , fh is an
indicator function describing the presence or ab-
sence of the feature, and ?h is the corresponding set
of weights learned using stochastic gradient ascent,
with the gradients inferred by loopy belief propaga-
tion (Smith and Eisner, 2008). The variance of the
Gaussian prior is set to 1. The other two parameters
in the training process, the number of belief propa-
gation iterations and the number of training rounds,
were tuned on the development set.
5.3 Decoding
The output of the joint model is the assignment to
the TAG and LINK variables. Loopy belief propaga-
tion (BP) was used to calculate the posterior proba-
bilities of these variables. For TAG, we emit the tag
with the highest posterior probability as computed
by sum-product BP. We produced head attachments
by first calculating the posteriors of the LINK vari-
ables with BP and then passing them to an edge-
factored tree decoder. This is equivalent to mini-
mum Bayes risk decoding (Goodman, 1996), which
is used by Cohen and Smith (2007) and Smith and
Eisner (2008). This MBR decoding procedure en-
forces the hard constraint that the output be a tree
but sums over possible morphological assignments.5
5.4 Reducing Model Complexity
In principle, the joint model should consider every
possible combination of morphological attributes for
every word. In practice, to reduce the complexity
of the model, we used a pre-existing morphological
database, MORPHEUS (Crane, 1991), to constrain
the range of possible values of the attributes listed
in Table 2; more precisely, we add a hard constraint,
requiring that assignments to the TAG variables be
compatible with MORPHEUS. This constraint signif-
icantly reduces the value of m in the big-O notation
5This approach to nuisance variables has also been used
effectively for parsing with tree-substitution grammars, where
several derived trees may correspond to each derivation tree,
and parsing with PCFGs with latent annotations.
890
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 94.4 94.5 94.4 94.5
Person 99.4 99.5 97.1 97.6
Number 95.3 95.9 93.7 94.5
Tense 98.0 98.2 93.2 93.9
Mood 98.1 98.3 93.8 94.4
Voice 98.5 98.6 95.3 95.7
Gender 93.1 93.9 87.7 89.1
Case 89.3 90.0 79.9 81.2
Degree 99.9 99.9 86.4 90.8
UAS 61.0 61.9 ? ?
Table 3: Latin morphological disambiguation and pars-
ing. For some attributes, such as degree, a substan-
tial portion of words have the null value. The non-null
columns provides a sharper picture by excluding these
?easy? cases. Note that POS is never null.
for the number of variables and factors described in
?3. To illustrate the effect, the graphical model of
the sentence in Table 1, whose six words are all cov-
ered by the database, has 1,866 factors; without the
benefit of the database, the full model would have
31,901 factors.
The MORPHEUS database was automatically gen-
erated from a list of stems, inflections, irregular
forms and morphological rules. It covers about 99%
of the distinct words in the Latin Dependency Tree-
bank. At decoding time, for each fold, the database
is further augmented with tags seen in training data.
After this augmentation, an average of 44 words are
?unseen? in each fold.
Similarly, we constructed morphological dictio-
naries for Czech, Ancient Greek, and Hungarian
from words that occurred at least five times in the
training data; words that occurred fewer times were
unrestricted in the morphological attributes they
could take on.
6 Experimental Results
We compare the performance of the pipeline model
(?4) and the joint model (?3) on morphological dis-
ambiguation and unlabeled dependency parsing.
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 95.5 95.7 95.5 95.7
Person 98.4 98.8 93.5 95.6
Number 91.2 92.3 87.0 88.4
Tense 98.4 98.8 92.7 96.1
Voice 98.5 98.7 93.2 95.8
Gender 86.6 87.9 75.6 78.0
Case 84.1 85.6 74.3 76.5
Degree 97.9 98.0 90.1 90.1
UAS 67.4 68.7 ? ?
Table 4: Czech morphological disambiguation and pars-
ing. As with Latin, the model is least accurate with
noun/adjective categories of gender number, and case,
particularly when considering only words whose true
value is non-null for those attributes. Joint inference with
syntactic features improves accuracy across the board.
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 94.9 95.7 94.9 95.7
Person 98.7 99.0 92.2 94.6
Number 97.4 97.9 96.5 97.1
Tense 96.8 97.2 84.1 86.8
Mood 97.9 98.3 91.4 93.2
Voice 97.8 98.0 91.3 92.4
Gender 95.4 96.1 90.7 91.9
Case 95.9 96.3 92.0 92.6
Degree 99.8 99.9 33.3 55.6
UAS 68.0 70.5 ? ?
Table 5: Ancient Greek morphological disambiguation
and parsing. Noun/adjective morphology is more accu-
rate, but verbal morphology is more problematic.
Model Tagger Joint Tagger Joint
Attr. ? all all non-null non-null
POS 95.8 95.8 95.8 95.8
Person 98.5 98.6 94.9 94.1
Number 97.4 97.5 96.8 96.6
Tense 98.9 99.3 97.2 97.3
Mood 98.7 99.2 95.8 97.3
Case 96.7 97.0 94.5 94.9
Degree 97.9 98.1 87.5 88.6
UAS 78.2 78.8 ? ?
Table 6: Hungarian morphological disambiguation and
parsing. The agglutinative morphological system makes
local cues more effective, but syntactic information helps
in almost all categories.
891
6.1 Morphological Disambiguation
As seen in Table 3, the joint model outperforms6
the baseline tagger in all attributes in Latin morpho-
logical disambiguation. Among words not covered
by the morphological database, accuracy in POS is
slightly better, but lower for case, gender and num-
ber.
The joint model made the most gains on adjec-
tives and participles. Both parts-of-speech are par-
ticularly ambiguous: according to MORPHEUS, 43%
of the adjectives can be interpreted as another POS,
most frequently nouns; while participles have an av-
erage of 5.5 morphological interpretations. Both
also often have identical forms for different genders,
numbers and cases. In these situations, syntactic
considerations help nudge the joint model to the cor-
rect interpretations.
Experiments on the other three languages bear out
similar results: the joint model improves morpho-
logical disambiguation. The performance of Czech
(Table 4) exhibits the closest analogue to Latin: gen-
der, number, and case are much less accurately pre-
dicted than are the other morphological attributes.
Like Latin, Czech lacks definite and indefinite arti-
cles to provide high-confidence cues for noun phrase
boundaries.
The Ancient Greek treebank comprises both ar-
chaic texts, before the development of a definite ar-
ticle, and later classic Greek, which has a definite
article; Hungarian has both a definite and an indefi-
nite article. In both languages (Tables 5 and 6), noun
and adjective gender, number, and case are more
accurately predicted than in Czech and Latin. The
verbal system of ancient Greek, in contrast, is more
complex than that of the other languages, so mood,
voice, and tense accuracy are lower.
6.2 Dependency Parsing
In addition to morphological disambiguation, we
also measured the performance of the joint model
on dependency parsing of Latin and the other lan-
guages. The baseline pipeline parser (?4.2) yielded
61.00% head selection accuracy (i.e., unlabeled at-
tachment score, UAS), outperformed7 by the joint
6The differences are statistically significant in all (p < 0.01
by McNemar?s Test) but POS (p = 0.5).
7Significant at p < e?11 by McNemar?s Test.
model at 61.88%. The joint model showed simi-
lar improvements in Ancient Greek, Hungarian, and
Czech.
Wrong decisions made by the baseline tagger of-
ten misled the pipeline parser. For adjectives, the ex-
ample shown in Table 1 and Figure 1 is a typical sce-
nario, where an accusative adjective was tagged as
nominative, and was then misanalyzed by the parser
as modifying a verb (as a subject) rather than mod-
ifying an accusative noun. For participles modify-
ing a noun, the wrong noun was often chosen based
on inaccurate morphological information. In these
cases, the joint model, entertaining all morpholog-
ical possibilities, was able to find the combination
of links and morphological analyses that are collec-
tively more likely.
The accuracy figures of our baselines are compa-
rable, but not identical, to their counterparts reported
in (Bamman and Crane, 2008). The differences may
partially be attributed to the different morphologi-
cal tagger used, and the different learning algorithm,
namely Margin Infused Relaxed Algorithm (MIRA)
in (McDonald et al, 2005) rather than maximum
likelihood. More importantly, the Latin Dependency
Treebank has grown from about 30K at the time of
the previous work to 53K at present, resulting in sig-
nificantly different training and testing material.
Gold Pipeline Parser When given perfect mor-
phological information, the Latin parser performs at
65.28% accuracy in head selection. Despite the or-
acle morphology, the head selection accuracy is still
below other languages. This is hardly surprising,
given the relatively small training set, and that the
?the most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection?, as observed at the recent dependency pars-
ing shared task (Nivre et al, 2007); both of these are
characteristics of Latin.
A particularly troublesome structure is coordina-
tion; the most frequent link errors all involve either a
parent or a child as a conjunction. In a list of words,
all words and coordinators depend on the final coor-
dinator. Since the factors in our model consult only
one link at a time, they do not sufficiently capture
this kind of structures. Higher-order features, partic-
ularly those concerned with links with grandparents
and siblings, have been shown to benefit dependency
892
parsing (Smith and Eisner, 2008) and may be able to
address this issue.
7 Conclusions and Future Work
We have proposed a discriminative model that
jointly infers morphological properties and syntactic
structures. In evaluations on various highly-inflected
languages, this joint model outperforms both a base-
line tagger in morphological disambiguation, and a
pipeline parser in head selection.
This model may be refined by incorporating richer
features and improved decoding. In particular, we
would like to experiment with higher-order features
(?6), and with maximum a posteriori decoding, via
max-product BP or (relaxed) integer linear program-
ming. Further evaluation on other morphological
systems would also be desirable.
Acknowledgments
We thank David Bamman and Gregory Crane for
their feedback and support. Part of this research
was performed by the first author while visiting
Perseus Digital Library at Tufts University, un-
der the grants A Reading Environment for Ara-
bic and Islamic Culture, Department of Education
(P017A060068-08) and The Dynamic Lexicon: Cy-
berinfrastructure and the Automatic Analysis of His-
torical Languages, National Endowment for the Hu-
manities (PR-50013-08). The latter two authors
were supported by Army prime contract #W911NF-
07-1-0216 and University of Pennsylvania subaward
#103-548106; by SRI International subcontract #27-
001338 and ARFL prime contract #FA8750-09-C-
0181; and by the Center for Intelligent Information
Retrieval. Any opinions, findings, and conclusions
or recommendations expressed in this material are
the authors? and do not necessarily reflect those of
the sponsors.
References
David Bamman and Gregory Crane. 2006. The Design
and Use of a Latin Dependency Treebank. Proc. Work-
shop on Treebanks and Linguistic Theories (TLT).
Prague, Czech Republic.
David Bamman and Gregory Crane. 2008. Building a
Dynamic Lexicon from a Digital Library. Proc. 8th
ACM/IEEE-CS Joint Conference on Digital Libraries
(JCDL 2008). Pittsburgh, PA.
David Bamman, Francesco Mambrini, and Gregory
Crane. 2009. An Ownership Model of Anno-
tation: The Ancient Greek Dependency Treebank.
Proc. Workshop on Treebanks and Linguistic Theories
(TLT).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The PDT: a 3-level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora, A.
Abeille? (ed). Kluwer.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
Proc. CoNLL. New York, NY.
Shay B. Cohen and Noah A. Smith. 2007. Joint Morpho-
logical and Syntactic Disambiguation. Proc. EMNLP-
CoNLL. Prague, Czech Republic.
Gregory Crane. 1991. Generating and Parsing Classical
Greek. Literary and Linguistic Computing 6(4):243?
245.
Yoav Goldberg and Reut Tsarfaty. 2008. A Single Gen-
erative Model for Joint Morphological Segmentation
and Syntactic Parsing. Proc. ACL. Columbus, OH.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. Proc. ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?.
2001. Serial Combination of Rules and Statistics: A
Case Study in Czech Tagging. Proc. ACL.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statis-
tical Morphological Disambiguation for Agglutinative
Languages. Proc. COLING.
Vincent Katz. 2004. The Complete Elegies of Sextus
Propertius. Princeton University Press, Princeton, NJ.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jana Hajic?. 2005. Non-projective Dependency
Parsing using Spanning Tree Algorithms. Proc.
HLT/EMNLP.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. Proc. ACL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. Proc. CoNLL Shared Task Session
of EMNLP-CoNLL. Prague, Czech Republic.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging using Decision Trees. Proc. International
Conference on New Methods in Language Processing.
Manchester, UK.
Noah A. Smith, David A. Smith and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields. Proc. HLT/EMNLP. Vancouver,
Canada.
893
David Smith and Jason Eisner. 2008. Dependency Pars-
ing by Belief Propagation. Proc. EMNLP. Honolulu,
Hawaii.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
Proc. HLT-NAACL. Edmonton, Canada.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. Proc.
COLING-ACL Student Research Workshop.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian Dependency Treebank. Proc. LREC.
894
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 895?904,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Bilingual Morpheme Segmentation and Alignment with
Context-rich Hidden Semi-Markov Models
Jason Naradowsky?
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003
narad@cs.umass.edu
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Abstract
This paper describes an unsupervised dynamic
graphical model for morphological segmen-
tation and bilingual morpheme alignment for
statistical machine translation. The model ex-
tends Hidden Semi-Markov chain models by
using factored output nodes and special struc-
tures for its conditional probability distribu-
tions. It relies on morpho-syntactic and lex-
ical source-side information (part-of-speech,
morphological segmentation) while learning a
morpheme segmentation over the target lan-
guage. Our model outperforms a competi-
tive word alignment system in alignment qual-
ity. Used in a monolingual morphological seg-
mentation setting it substantially improves ac-
curacy over previous state-of-the-art models
on three Arabic and Hebrew datasets.
1 Introduction
An enduring problem in statistical machine trans-
lation is sparsity. The word alignment models of
modern MT systems attempt to capture p(ei|fj),
the probability that token ei is a translation of fj .
Underlying these models is the assumption that the
word-based tokenization of each sentence is, if not
optimal, at least appropriate for specifying a concep-
tual mapping between the two languages.
However, when translating between unrelated lan-
guages ? a common task ? disparate morphological
systems can place an asymmetric conceptual bur-
den on words, making the lexicon of one language
much more coarse. This intensifies the problem of
sparsity as the large number of word forms created
?This research was conducted during the author?s internship
at Microsoft Research
through morphologically productive processes hin-
ders attempts to find concise mappings between con-
cepts.
For instance, Bulgarian adjectives may contain
markings for gender, number, and definiteness. The
following tree illustrates nine realized forms of the
Bulgarian word for red, with each leaf listing the
definite and indefinite markings.
Feminine Neuter
Singular Plural
Root
Masculine
cherven(iq)(iqt) cherveni(te)chervena(ta) cherveno(to)
Table 1: Bulgarian forms of red
Contrast this with English, in which this informa-
tion is marked either on the modified word or by sep-
arate function words.
In comparison to a language which isn?t mor-
phologically productive on adjectives, the alignment
model must observe nine times as much data (as-
suming uniform distribution of the inflected forms)
to yield a comparable statistic. In an area of research
where the amount of data available plays a large role
in a system?s overall performance, this sparsity can
be extremely problematic. Further complications are
created when lexical sparsity is compounded with
the desire to build up alignments over increasingly
larger contiguous phrases.
To address this issue we propose an alternative
to word alignment: morpheme alignment, an align-
ment that operates over the smallest meaningful sub-
sequences of words. By striving to keep a direct 1-
to-1 mapping between corresponding semantic units
across languages, we hope to find better estimates
895
??
the red flower
cherven tsvet
DET ADJ NN
iai
s
they want to
h nA^
PRN VB INF
dyrsr~d y
teach him
VB PRN
nw y
?????? ???? ?? ??
? ?? ?????
?
? ? ?
te
Figure 1: A depiction of morpheme-level alignment. Here dark lines indicate the more stem-focused alignment
strategy of a traditional word or phrasal alignment model, while thin lines indicate a more fine-grained alignment
across morphemes. In the alignment between English and Bulgarian (a) the morpheme-specific alignment reduces
sparsity in the adjective and noun (red flowers) by isolating the stems from their inflected forms. Despite Arabic
exhibiting templatic morphology, there are still phenomena which can be accounted for with a simpler segmentational
approach. The Arabic alignment (b) demonstrates how the plural marker on English they would normally create
sparsity by being marked in three additional places, two of them inflections in larger wordforms.
for the alignment statistics. Our results show that
this improves alignment quality.
In the following sections we describe an un-
supervised dynamic graphical model approach to
monolingual morphological segmentation and bilin-
gual morpheme alignment using a linguistically mo-
tivated statistical model. In a bilingual setting,
the model relies on morpho-syntactic and lexical
source-side information (part-of-speech, morpho-
logical segmentation, dependency analysis) while
learning a morpheme segmentation over the tar-
get language. In a monolingual setting we intro-
duce effective use of context by feature-rich mod-
eling of the probabilities of morphemes, morpheme-
transitions, and word boundaries. These additional
sources of information provide powerful bias for un-
supervised learning, without increasing the asymp-
totic running time of the inference algorithm.
Used as a monolingual model, our system sig-
nificantly improves the state-of-the-art segmenta-
tion performance on three Arabic and Hebrew data-
sets. Used as a bilingual model, our system out-
performs the state-of-the-art WDHMM (He, 2007)
word alignment model as measured by alignment er-
ror rate (AER).
In agreement with some previous work on to-
kenization/morpheme segmentation for alignment
(Chung and Gildea, 2009; Habash and Sadat, 2006),
we find that the best segmentation for alignment
does not coincide with the gold-standard segmenta-
tion and our bilingual model does not outperform
our monolingual model in segmentation F-Measure.
2 Model
Our model defines the probability of a target lan-
guage sequence of words (each consisting of a se-
quence of morphemes), and alignment from target
to source morphemes, given a source language se-
quence of words (each consisting of a sequence of
morphemes).
An example morpheme segmentation and align-
ment of phrases in English-Arabic and English-
Bulgarian is shown in Figure 1. In our task setting,
the words of the source and target language as well
as the morpheme segmentation of the source (En-
glish) language are given. The morpheme segmen-
tation of the target language and the alignments be-
tween source and target morphemes are hidden.
The source-side input, which we assume to be
English, is processed with a gold morphological
segmentation, part-of-speech, and dependency tree
analysis. While these tools are unavailable in
resource-poor languages, they are often available for
at least one of the modeled languages in common
translation tasks. This additional information then
provides a source of features and conditioning infor-
mation for the translation model.
Our model is derived from the hidden-markov
model for word alignment (Vogel et al, 1996; Och
and Ney, 2000). Based on it, we define a dynamic
896
cherven.i.te 
flowerthe red
= 'cherven'
?1
= 2
a1
= OFF
b1
= OFF
b2
= ON
b3
 = 'i'
?2
 = 'te'
?3
= 4 = 1
a2 a3
s
= stem
t1
= suffix = suffix
t2 t3
Figure 2: A graphical depiction of the model generating
the transliteration of the first Bulgarian word from Figure
1. Trigram dependencies and some incoming/outgoing
arcs have been omitted for clarity.
graphical model which lets us encode more lin-
guistic intuition about morpheme segmentation and
alignment: (i) we extend it to a hidden semi-markov
model to account for hidden target morpheme seg-
mentation; (ii) we introduce an additional observa-
tion layer to model observed word boundaries and
thus truly represent target sentences as words com-
posed of morphemes, instead of just a sequence
of tokens; (iii) we employ hierarchically smoothed
models and log-linear models to capture broader
context and to better represent the morpho-syntactic
mapping between source and target languages. (iv)
we enrich the hidden state space of the model to en-
code morpheme types {prefix,suffix,stem}, in ad-
dition to morpheme alignment and segmentation in-
formation.
Before defining our model formally, we introduce
some notation. Each possible morphological seg-
mentation and alignment for a given sentence pair
can be described by the following random variables:
Let ?1?2 . . . ?I denote I morphemes in the seg-
mentation of the target sentence. For the Example
in Figure 1 (a) I=5 and ?1=cherven, ?2=i . . . , and
?5=ia. Let b1, b2, . . . bI denote Bernoulli variables
indicating whether there is a word boundary after
morpheme ?i. For our example, b3 = 1, b5 = 1,
and the other bi are 0. Let c1, c2, . . . , cT denote
the non-space characters in the target string, and
wb1, . . . , wbT denote Bernoulli variables indicating
whether there is a word boundary after the corre-
sponding target character. For our example, T = 14
(for the Cyrillic version) and the only wb variables
that are on are wb9 and wb14. The c and wb vari-
ables are observed. Let s1s2 . . . sT denote Bernoulli
segmentation variables indicating whether there is a
morpheme boundary after the corresponding char-
acter. The values of the hidden segmentation vari-
ables s together with the values of the observed c
and wb variables uniquely define the values of the
morpheme variables ?i and the word boundary vari-
ables bi. Naturally we enforce the constraint that
a given word boundary wbt = 1 entails a segmen-
tation boundary st = 1. If we use bold letters
to indicate a vector of corresponding variables, we
have that c,wb, s=?,b. We will define the assumed
parametric form of the learned distribution using the
?,b but the inference algorithms are implemented
in terms of the s and wb variables.
We denote the observed source language mor-
phemes by e1 . . . eJ . Our model makes use of ad-
ditional information from the source which we will
mention when necessary.
The last part of the hidden model state repre-
sents the alignment between target and source mor-
phemes and the type of target morphemes. Let
tai = [ai, ti], i = 1 . . . I indicate a factored state
where ai represents one of the J source words (or
NULL) and ti represents one of the three morpheme
types {prefix,suffix,stem}. ai is the source mor-
pheme aligned to ?i and ti is the type of ?i.
We are finally ready to define the desired proba-
bility of target morphemes, morpheme types, align-
ments, and word boundaries given source:
P (?, ta,b|e) =
I?
i=1
PT (?i|tai, bi?1, bi?2, ?i?1, e)
? PB(bi|?i, ?i?1, tai, bi?1, bi?2, e)
? PD(tai|tai?1, bi?1, e) ? LP (|?i|)
We now describe each of the factors used by our
model in more detail. The formulation makes ex-
plicit the full extent of dependencies we have ex-
plored in this work. By simplifying the factors
897
we can recover several previously used models for
monolingual segmentation and bilingual joint seg-
mentation and alignment. We discuss the relation-
ship of this model to prior work and study the impact
of the novel components in our experiments.
When the source sentence is assumed to be empty
(and thus contains no morphemes to align to) our
model turns into a monolingual morpheme segmen-
tation model, which we show exceeds the perfor-
mance of previous state-of-the-art models. When we
remove the word boundary component, reduce the
order of the alignment transition, omit the morpho-
logical type component of the state space, and retain
only minimal dependencies in the morpheme trans-
lation model, we recover the joint tokenization and
alignment model based on IBM Model-1 proposed
by (Chung and Gildea, 2009).
2.1 Morpheme Translation Model
In the model equation, PT denotes the morpheme
translation probability. The standard dependence on
the aligned source morpheme is represented as a de-
pendence on the state tai and the whole annotated
source sentence e. We experimented with multiple
options for the amount of conditioning context to be
included. When most context is used, there is a bi-
gram dependency of target language morphemes as
well as dependence on two previous boundary vari-
ables and dependence on the aligned source mor-
pheme eai as well as its POS tag.
When multiple conditioning variables are used we
assume a special linearly interpolated backoff form
of the model, similar to models routinely used in lan-
guage modeling.
As an example, suppose we estimate the mor-
pheme translation probability as PT (?i|eai , ti). We
estimate this in the M-step, given expected joint
counts c(?i, eai , ti) and marginal counts derived
from these as follows:
PT (?i|eai , ti) =
c(?i,eai ,ti)+?2P2(?i|ti)
c(eai ,ti)+?2
The lower order distributions are estimated recur-
sively in a similar way:
P2(?i|ti) =
c(?i,ti)+?1P1(?i)
c(ti)+?1
P1(?i) =
c(?i)+?0P0(?i)
c(.)+?0
For P0 we used a unigram character language
model. This hierarchical smoothing can be seen
as an approximation to hierarchical Dirichlet priors
with maximum aposteriori estimation.
Note how our explicit treatment of word bound-
ary variables bi allows us to use a higher order de-
pendence on these variables. If word boundaries are
treated as morphemes on their own, we would need
to have a four-gram model on target morphemes to
represent this dependency which we are now repre-
senting using only a bigram model on hidden mor-
phemes.
2.2 Word Boundary Generation Model
The PB distribution denotes the probability of gen-
erating word boundaries. As a sequence model of
sentences the basic hidden semi-markov model com-
pletely ignores word boundaries. However, they can
be powerful predictors of morpheme segments (by
for example, indicating that common prefixes fol-
low word boundaries, or that common suffixes pre-
cede them). The log-linear model of (Poon et al,
2009) uses word boundaries as observed left and
right context features, and Morfessor (Creutz and
Lagus, 2007) includes boundaries as special bound-
ary symbols which can inform about the morpheme
state of a morpheme (but not its identity).
Our model includes a special generative process
for boundaries which is conditioned not only on the
previous morpheme state but also the previous two
morphemes and other boundaries. Due to the fact
that boundaries are observed their inclusion in the
model does not increase the complexity of inference.
The inclusion of this distribution lets us estimate
the likelihood of a word consisting of one,two,three,
or more morphemes. It also allows the estimation of
likelihood that particular morphemes are in the be-
ginning/middle/end of words. Through the included
factored state variable tai word boundaries can also
inform about the likelihood of a morpheme aligned
to a source word of a particular pos tag to end a
word. We discuss the particular conditioning con-
text for this distribution we found most helpful in
our experiments.
Similarly to the PT distribution, we make use of
multiple context vectors by hierarchical smoothing
of distributions of different granularities.
898
2.3 Distortion Model
PD indicates the distortion modeling distribution
we use. 1 Traditional distortion models represent
P (aj |aj?1, e), the probability of an alignment given
the previous alignment, to bias the model away from
placing large distances between the aligned tokens
of consecutively sequenced tokens. In addition to
modeling a larger state space to also predict mor-
pheme types, we extend this model by using a spe-
cial log-linear model form which allows the integra-
tion of rich morpho-syntactic context. Log-linear
models have been previously used in unsupervised
learning for local multinomial distributions like this
one in e.g. (Berg-Kirkpatrick et al, 2010), and for
global distributions in (Poon et al, 2009).
The special log-linear form allows the inclusion
of features targeted at learning the transitions among
morpheme types and the transitions between corre-
sponding source morphemes. The set of features
with example values for this model is depicted in
Table 3. The example is focussed on the features
firing for the transition from the Bulgarian suffix
te aligned to the first English morpheme ?i?1 =
te, ti?1=suffix, ai?1=1, to the Bulgarian root tsvet
aligned to the third English morpheme ?i = tsvet,
ti=root, ai=3. The first feature is the absolute dif-
ference between ai and ai?1 + 1 and is similar to
information used in other HMM word alignment
models (Och and Ney, 2000) as well as phrase-
translation models (Koehn, 2004). The alignment
positions ai are defined as indices of the aligned
source morphemes. We additionally compute distor-
tion in terms of distance in number of source words
that are skipped. This distance corresponds to the
feature name WORD DISTANCE. Looking at both
kinds of distance is useful to capture the intuition
that consecutive morphemes in the same target word
should prefer to have a higher proximity of their
aligned source words, as compared to consecutive
morphemes which are not part of the same target
word. The binned distances look at the sign of the
distortion and bin the jumps into 5 bins, pooling the
distances greater than 2 together. The feature SAME
TARGET WORD indicates whether the two consecu-
1To reduce complexity of exposition we have omitted the
final transition to a special state beyond the source sentence end
after the last target morpheme.
Feature Value
MORPH DISTANCE 1
WORD DISTANCE 1
BINNED MORPH DISTANCE fore1
BINNED WORD DISTANCE fore1
MORPH STATE TRANSITION suffix-root
SAME TARGET WORD False
POS TAG TRANSITION DET-NN
DEP RELATION DET?NN
NULL ALIGNMENT False
conjunctions ...
Figure 3: Features in log-linear distortion model firing
for the transition from te:suffix:1 to tsvet:root:3 in the
example sentence pair in Figure 1a.
tive morphemes are part of the same word. In this
case, they are not. This feature is not useful on its
own because it does not distinguish between differ-
ent alignment possibilities for tai, but is useful in
conjunction with other features to differentiate the
transition behaviors within and across target words.
The DEP RELATION feature indicates the direct de-
pendency relation between the source words con-
taining the aligned source morphemes, if such rela-
tionship exists. We also represent alignments to null
and have one null for each source word, similarly to
(Och and Ney, 2000) and have a feature to indicate
null. Additionally, we make use of several feature
conjunctions involving the null, same target word,
and distance features.
2.4 Length Penalty
Following (Chung and Gildea, 2009) and (Liang and
Klein, 2009) we use an exponential length penalty
on morpheme lengths to bias the model away from
the maximum likelihood under-segmentation solu-
tion. The form of the penalty is:
LP (|?i|) = 1
e|?i|
lp
Here lp is a hyper-parameter indicating the power
that the morpheme length is raised to. We fit this pa-
rameter using an annotated development set, to op-
timize morpheme-segmentation F1. The model is
extremely sensitive to this value and performs quite
poorly if such penalty is not used.
2.5 Inference
We perform inference by EM training on the aligned
sentence pairs. In the E-step we compute expected
899
counts of all hidden variable configurations that are
relevant for our model. In the M-step we re-estimate
the model parameters (using LBFGS in the M-step
for the distortion model and using count interpola-
tion for the translation and word-boundary models).
The computation of expectations in the E-step
is of the same order as an order two semi-markov
chain model using hidden state labels of cardinality
(J ? 3 = number of source morphemes times num-
ber of target morpheme types). The running time
of the forward and backward dynamic programming
passes is T ? l2 ? (3J)2, where T is the length of
the target sentence in characters, J is the number
of source morphemes, and l is the maximum mor-
pheme length. Space does not permit the complete
listing of the dynamic programming solution but it
is not hard to derive by starting from the dynamic
program for the IBM-1 like tokenization model of
(Chung and Gildea, 2009) and extending it to ac-
count for the higher order on morphemes and the
factored alignment state space.
Even though the inference algorithm is low poly-
nomial it is still much more expensive than the infer-
ence for an HMM model for word-alignment with-
out segmentation. To reduce the running time of the
model we limit the space of considered morpheme
boundaries as follows:
Given the target side of the corpus, we derive a
list of K most frequent prefixes and suffixes using a
simple trie-based method proposed by (Schone and
Jurafsky, 2000).2 After we determine a list of al-
lowed prefixes and suffixes we restrict our model to
allow only segmentations of the form : ((p*)r(s*))+
where p and s belong to the allowed prefixes and
suffixes and r can match any substring.
We determine the number of prefixes and suffixes
to consider using the maximum recall achievable by
limiting the segmentation points in this way. Re-
stricting the allowable segmentations in this way not
only improves the speed of inference but also leads
to improvements in segmentation accuracy.
2Words are inserted into a trie with each complete branch
naturally identifying a potential suffix, inclusive of its sub-
branches. The list comprises of the K most frequent of these
complete branches. Inserting the reversed words will then yield
potential prefixes.
3 Evaluation
For a majority of our testing we borrow the paral-
lel phrases corpus used in previous work (Snyder
and Barzilay, 2008), which we refer to as S&B.
The corpus consists of 6,139 short phrases drawn
from English, Hebrew, and Arabic translations of
the Bible. We use an unmodified version of this
corpus for the purpose of comparing morphological
segmentation accuracy. For evaluating morpheme
alignment accuracy, we have also augmented the En-
glish/Arabic subset of the corpus with a gold stan-
dard alignment between morphemes. Here mor-
phological segmentations were obtained using the
previously-annotated gold standard Arabic morpho-
logical segmentation, while the English was prepro-
cessed with a morphological analyzer and then fur-
ther hand annotated with corrections by two native
speakers. Morphological alignments were manually
annotated. Additionally, we evaluate monolingual
segmentation models on the full Arabic Treebank
(ATB), also used for unsupervised morpheme seg-
mentation in (Poon et al, 2009).
4 Results
4.1 Morpheme Segmentation
We begin by evaluating a series of models which are
simplifications of our complete model, to assess the
impact of individual modeling decisions. We focus
first on a monolingual setting, where the source sen-
tence aligned to each target sentence is empty.
Unigram Model with Length Penalty
The first model we study is the unigram mono-
lingual segmentation model using an exponential
length penalty as proposed by (Liang and Klein,
2009; Chung and Gildea, 2009), which has been
shown to be quite accurate. We refer to this model as
Model-UP (for unigram with penalty). It defines the
probability of a target morpheme sequence as fol-
lows: (?1 . . . ?I) = (1? ?)
?I
i=1 ?PT (?i)LP (|?i|)
This model can be (almost) recovered as a spe-
cial case of our full model, if we drop the transition
and word boundary probabilities, do not model mor-
pheme types, and use no conditioning for the mor-
pheme translation model. The only parameter not
present in our model is the probability ? of gener-
ating a morpheme as opposed to stopping to gener-
900
ate morphemes (with probability 1 ? ?). We exper-
imented with this additional parameter, but found it
had no significant impact on performance, and so we
do not report results including it.
We select the value of the length penalty power
by a gird search in the range 1.1 to 2.0, using .1 in-
crements and choosing the values resulting in best
performance on a development set containing 500
phrase pairs for each language. We also select the
optimal number of prefixes/suffixes to consider by
measuring performance on the development set. 3
Morpheme Type Models
The next model we consider is similar to the un-
igram model with penalty, but introduces the use
of the hidden ta states which indicate only mor-
pheme types in the monolingual setting. We use
the ta states and test different configurations to de-
rive the best set of features that can be used in the
distortion model utilizing these states, and the mor-
pheme translation model. We consider two vari-
ants: (1) Model-HMMP-basic (for HMM model
with length penalty), which includes the hidden
states but uses them with a simple uniform transition
matrix P (tai|tai?1, bi?1) (uniform over allowable
transitions but forbidding the prefixes from transi-
tioning directly to suffixes, and preventing suffixes
from immediately following a word boundary), and
(2) a richer model Model-HMMP which is allowed
to learn a log-linear distortion model and a feature
rich translation model as detailed in the model defi-
nition. This model is allowed to use word boundary
information for conditioning (because word bound-
aries are observed), but does not include the PB pre-
dictive word boundary distribution.
Full Model with Word Boundaries
Finally we consider our full monolingual model
which also includes the distribution predicting word
boundary variables bi. We term this model Model-
FullMono. We detail the best context features for
the conditional PD distribution for each language.
We initialize this model with the morpheme trans-
3For the S&B Arabic dataset, we selected to use seven pre-
fixes and seven suffixes, which correspond to maximum achiev-
able recall of 95.3. For the S&B Hebrew dataset, we used six
prefixes and six suffixes, for a maximum recall of 94.3. The
Arabic treebank data required a larger number of affixes: we
used seven prefixes and 20 suffixes, for a maximum recall of
98.3.
lation unigram distribution of ModelHMMP-basic,
trained for 5 iterations.
Table 4 details the test set results of the different
model configurations, as well as previously reported
results on these datasets. For our main results we use
the automatically derived list of prefixes and suffixes
to limit segmentation points. The names of models
that use such limited lists are prefixed by Dict in the
Table. For comparison, we also report the results
achieved by models that do not limit the segmenta-
tion points in this way.
As we can see the unigram model with penalty,
Dict-Model-UP, is already very strong, especially
on the S&B Arabic dataset. When the segmenta-
tion points are not limited, its performance is much
worse. The introduction of hidden morpheme states
in Dict-HMMP-basic gives substantial improvement
on Arabic and does not change results much on the
other datasets. A small improvement is observed
for the unconstrained models.4 When our model in-
cludes all components except word boundary pre-
diction, Dict-Model-HMMP, the results are substan-
tially improved on all languages. Model-HMMP is
also the first unconstrained model in our sequence
to approach or surpass previous state-of-the-art seg-
mentation performance.
Finally, when the full model Dict-MonoFull is
used, we achieve a substantial improvement over
the previous state-of-the-art results on all three cor-
pora, a 6.5 point improvement on Arabic, 6.2 point
improvement on Hebrew, and a 9.3 point improve-
ment on ATB. The best configuration of this model
uses the same distortion model for all languages: us-
ing the morph state transition and boundary features.
The translation models used only ti for Hebrew and
ATB and ti and ?i?1 for Arabic. Word bound-
ary was predicted using ti in Arabic and Hebrew,
and additionally using bi?1 and bi?2 for ATB. The
unconstrained models without affix dictionaries are
also very strong, outperforming previous state-of-
the-art models. For ATB, the unconstrained model
slightly outperforms the constrained one.
The segmentation errors made by this system shed
light on how it might be improved. We find the dis-
4Note that the inclusion of states in HMMP-basic only
serves to provide a different distribution over the number of
morphemes in a word, so it is interesting it can have a positive
impact.
901
Arabic Hebrew ATB
P R F1 P R F1 P R F1
UP 88.1 55.1 67.8 43.2 87.6 57.9 79.0 54.6 64.6
Dict-UP 85.8 73.1 78.9 57.0 79.4 66.3 61.6 91.0 73.5
HMMP-basic 83.3 58.0 68.4 43.5 87.8 58.2 79.0 54.9 64.8
Dict-HMMP-basic 84.8 76.3 80.3 56.9 78.8 66.1 69.3 76.2 72.6
HMMP 73.6 76.9 75.2 70.2 73.0 71.6 94.0 76.1 84.1
Dict-HMMP 82.4 81.3 81.8 62.7 77.6 69.4 85.2 85.8 85.5
MonoFull 80.5 87.3 83.8 72.2 71.7 72.0 86.2 88.5 87.4
Dict-MonoFull 86.1 83.2 84.6 73.7 72.5 73.1 92.9 81.8 87.0
Poon et. al 76.0 80.2 78.1 67.6 66.1 66.9 88.5 69.2 77.7
S&B-Best 67.8 77.3 72.2 64.9 62.9 63.9 ? ? ?
Morfessor 71.1 60.5 65.4 65.4 57.7 61.3 77.4 72.6 74.9
Figure 4: Results on morphological segmentation achieved by monolingual variants of our model (top) with results
from prior work are included for comparison (bottom). Results from models with a small, automatically-derived list
of possible prefixes and suffixes are labeled as ?Dict-? followed by the model name.
tributions over the frequencies of particular errors
follow a Zipfian skew across both S&B datasets,
with the Arabic being more pronounced (the most
frequent error being made 27 times, with 627 er-
rors being made just once) in comparison with the
Hebrew (with the most frequent error being made
19 times, and with 856 isolated errors). However,
in both the Arabic and Hebrew S&B tasks we find
that a tendency to over-segment certain characters
off of their correct morphemes and on to other fre-
quently occurring, yet incorrect, particles is actually
the cause of many of these isolated errors. In Ara-
bic the system tends to over segment the character
aleph (totally about 300 errors combined). In He-
brew the source of error is not as overwhelmingly
directed at a single character, but yod and he, the
latter functioning quite similarly to the problematic
Arabic character and frequently turn up in the corre-
sponding places of cognate words in Biblical texts.
We should note that our models select a large
number of hyper-parameters on an annotated devel-
opment set, including length penalty, hierarchical
smoothing parameters ?, and the subset of variables
to use in each of three component sub-models. This
might in part explain their advantage over previous-
state-of-the-art models, which might use fewer (e.g.
(Poon et al, 2009) and (Snyder and Barzilay, 2008))
or no specifically tuned for these datasets hyper-
parameters (Morfessor (Creutz and Lagus, 2007)).
4.2 Alignment
Next we evaluate our full bilingual model and a sim-
pler variant on the task of word alignment. We use
the morpheme-level annotation of the S&B English-
Arabic dataset and project the morpheme alignments
to word alignments. We can thus compare align-
ment performance of the results of different segmen-
tations. Additionally, we evaluate against a state-
of-the-art word alignment system WDHMM (He,
2007), which performs comparably or better than
IBM-Model4. The table in Figure 5 presents the re-
sults. In addition to reporting alignment error rate
for different segmentation models, we report their
morphological segmentation F1.
The word-alignment WDHMM model performs
best when aligning English words to Arabic words
(using Arabic as source). In this direction it is
able to capture the many-to-one correspondence be-
tween English words and arabic morphemes. When
we combine alignments in both directions using the
standard grow-diag-final method, the error goes up.
We compare the (Chung and Gildea, 2009) model
(termed Model-1) to our full bilingual model. We
can recover Model-1 similarly to Model-UP, except
now every morpheme is conditioned on an aligned
source morpheme. Our full bilingual model outper-
forms Model-1 in both AER and segmentation F1.
The specific form of the full model was selected as
in the previous experiments, by choosing the model
with best segmentations of the development set.
For Arabic, the best model conditions target mor-
902
Arabic Hebrew
Align P Align R AER P R F1 P R F1
Model-1 (C&G 09) 91.6 81.2 13.9 72.4 76.2 74.3 61.0 71.8 65.9
Bilingual full 91.0 88.3 10.3 90.0 72.0 80.0 63.3 71.2 67.0
WDHMM E-to-A 82.4 96.7 11.1
WDHMM GDF 82.1 94.6 12.1
Figure 5: Alignment Error Rate (AER) and morphological segmentation F1 achieved by bilingual variants of our
model. AER performance of WDHMM is also reported. Gold standard alignments are not available for the Hebrew
data set.
phemes on source morphemes only, uses the bound-
ary model with conditioning on number of mor-
phemes in the word, aligned source part-of-speech,
and type of target morpheme. The distortion model
uses both morpheme and word-based absolute dis-
tortion, binned distortion, morpheme types of states,
and aligned source-part-of-speech tags. Our best
model for Arabic outperforms WDHMM in word
alignment error rate. For Hebrew, the best model
uses a similar boundary model configuration but a
simpler uniform transition distortion distribution.
Note that the bilingual models perform worse than
the monolingual ones in segmentation F1. This
finding is in line with previous work showing that
the best segmentation for MT does not necessarily
agree with a particular linguistic convention about
what morphemes should contain (Chung and Gildea,
2009; Habash and Sadat, 2006), but contradicts
other results (Snyder and Barzilay, 2008). Further
experimentation is required to make a general claim.
We should note that the Arabic dataset used
for word-alignment evaluation is unconventionally
small and noisy (the sentences are very short
phrases, automatically extracted using GIZA++).
Thus the phrases might not be really translations,
and the sentence length is much smaller than in stan-
dard parallel corpora. This warrants further model
evaluation in a large-scale alignment setting.
5 Related Work
This work is most closely related to the unsupervised
tokenization and alignment models of Chung and
Gildea (2009), Xu et al (2008), Snyder and Barzilay
(2008), and Nguyen et al (2010).
Chung & Gildea (2009) introduce a unigram
model of tokenization based on IBM Model-1,which
is a special case of our model. Snyder and Barzi-
lay (2008) proposes a hierarchical Bayesian model
that combines the learning of monolingual segmen-
tations and a cross-lingual alignment; their model is
very different from ours.
Incorporating morphological information into
MT has received reasonable attention. For exam-
ple, Goldwater & McClosky (2005) show improve-
ments when preprocessing Czech input to reflect
a morphological decomposition using combinations
of lemmatization, pseudowords, and morphemes.
Yeniterzi and Oflazer (2010) bridge the morpholog-
ical disparity between languages in a unique way
by effectively aligning English syntactic elements
(function words connected by dependency relations)
to Turkish morphemes, using rule-based postpro-
cessing of standard word alignment. Our work is
partly inspired by that work and attempts to auto-
mate both the morpho-syntactic alignment and mor-
phological analysis tasks.
6 Conclusion
We have described an unsupervised model for mor-
pheme segmentation and alignment based on Hid-
den Semi-Markov Models. Our model makes use
of linguistic information to improve alignment qual-
ity. On the task of monolingual morphological seg-
mentation it produces a new state-of-the-art level on
three datasets. The model shows quantitative im-
provements in both word segmentation and word
alignment, but its true potential lies in its finer-
grained interpretation of word alignment, which will
hopefully yield improvements in translation quality.
Acknowledgements
We thank the ACL reviewers for their valuable
comments on earlier versions of this paper, and
Michael J. Burling for his contributions as a corpus
annotator and to the Arabic aspects of this paper.
903
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. 2010. Unsupervised
learning with features. In Proceedings of the North
American chapter of the Association for Computa-
tional Linguistics (NAACL).
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
North American Chapter of the Association for Com-
putational Linguistics.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL 2nd Statistical MT work-
shop, pages 80?87.
Philip Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
P. Liang and D. Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proceedings of the International Con-
ference on Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In North American Chap-
ter of the Association for Computation Linguistics
- Human Language Technologies 2009 conference
(NAACL/HLT-09).
Patrick Schone and Daniel Jurafsky. 2000. Knowlege-
free induction of morphology using latent semantic
analysis. In Proceedings of the Conference on Compu-
tational Natural Language Learning (CoNLL-2000).
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In In COLING 96: The 16th Int. Conf. on Com-
putational Linguistics.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised chinese word
segmentation for statistical machine translation. In
COLING.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from english to turkish. In
Proceedings of Association of Computational Linguis-
tics.
904
