Evangelising Language Technology:
A Practically-Focussed Undergraduate Program
Robert Dale, Diego Moll? Aliod and Rolf Schwitter
Centre for Language Technology
Division of Information and Communication Sciences
Macquarie University, Sydney, Australia
{rdale|diego|rolfs}@ics.mq.edu.au
Abstract
This paper describes an
undergraduate program in Language
Technology that we have developed
at Macquarie University. We
question the industrial relevance of
much that is taught in NLP courses,
and emphasize the need for a
practical orientation as a means to
growing the size of the field. We
argue that a more evangelical
approach, both with regard to
students and industry, is required.
The paper provides an overview of
the material we cover, and makes
some observations for the future on
the basis of our experiences so far.
1 Introduction
This paper describes our experiences in setting
up an undergraduate program in language
technology, with a particular emphasis on the
philosophy that lies behind the decisions we
have made in designing this program.
In Section 2, we sketch the background to
the program, and outline the perspective we
take on teaching in this area. Against this
backdrop, in Section 3, we describe the
orientation and content of the program in some
detail. In Section 4 we discuss the evaluation of
the program, identify some lessons we have
learned regarding what works and what
doesn?t, and point to where we intend to go in
the future.
2 Background
2.1 How The Program Came About
Our program is hosted by the Department of
Computing at Macquarie, which offers a typical
range of computer science courses. At this
university, standard undergraduate degree
programs are three years in length. Students
may elect to stay on for a fourth year in order to
obtain an honours degree, although in a
marketable area like computing, relatively few
students stay on beyond third year. The
teaching year is split into two thirteen week
semesters, with the first semester running from
March through June and the second semester
from August to November.
In 2000, we obtained government funding
to set up an undergraduate program in
language technology.1 To obtain this funding,
we argued that skills in the language
technologies were critical to the development of
the next generations of computer interfaces,
echoing statements made by many both in
industry and academia. Central to our proposal
was the identification of the twin streams of (a)
spoken language interaction and (b) smart text
processing, particularly with regard to the Web;
we took the view that these two major areas
would define the future of commercial NLP
activities over the next five years. Our proposal
emphasised heavily a practical orientation,
whereby we set our goal to be the training of
knowledge workers who will design and
develop practical applications in these areas.
Our proposal was supported by a number of
industry partners, including the Australian
branches of Motorola, Sun Microsystems,
Philips Speech Systems, and the government
research agency CSIRO.
1 We will refrain from entering into an argument
here as to the appropriate semantic distinctions
between the terms ?language technology?, ?natural
language processing? and ?computational linguistics?.
For current purposes, we?ll simply assume that all
three terms effectively cover approximately the same
territory.
                     July 2002, pp. 27-32.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
2.2 Our Philosophical Orientation
Our perception was that, in many institutions,
natural language processing and computational
linguistics courses tended to share two
particular characteristics.
First, relatively few institutions have more
than one course at undergraduate level that
provides material in this area. In many cases,
material in NLP or CL appears only as part of a
more general course on Artificial Intelligence.
This is of course determined by a range of local
factors, including inevitably the interests and
knowledge of available staff. However, an
important factor in many institutions that do not
have a long-established and strong research
group in the area is the widely-held sentiment
that NLP is a somewhat peripheral topic, or a
subject of purely theoretical interest. This makes
it hard for those staff who are interested in
teaching in this area to argue for a significant
presence in the curriculum.
A second observation is that the material
taught in introductory courses often tends to
focus on what we might call computational
syntax: writing grammars and building parsers.
Again, there are good reasons for this: some
would argue that you can?t do much else until
this material is covered, and this is clearly the
corner of NLP that is most well-established with
consolidated results, as reflected by the balance
of coverage found in texts such as [Allen 1995]
and [Jurafsky and Martin 2000], and, perhaps
less so than in the past, the topic coverage at
conferences such as ACL and Coling.2
With regard to the first of these
observations, we take a strong position. If, as a
community, we believe our own rhetoric about
NLP being critical for machine interfaces and
information processing technologies of the
future, then NLP needs to become a much more
central part of computing curricula: every
student should be exposed to this area. Our
desire, presumably shared by most who work in
the area, is to see the field of NLP grow, with
many more knowledgeable practitioners,
particularly in industry.
2 One of the authors recently completed a book
project that had as its goal the production of a
resource that would meet this concern by providing a
more balanced coverage of different aspects of NLP:
see Dale et al[2000]. Unfortunately, this book is too
large and expensive in its current form for use in our
courses.
With regard to our second observation,
however, we take the view that the focus
adopted in much undergraduate teaching in this
area does not support this goal as well as it
might. Teaching students about grammars and
parsers may serve as a suitable introduction to
further study in the area, but the bulk of
students who undertake undergraduate degrees
will go on to work in industry; only a minority
are likely to work in research laboratories or
undertake doctoral studies. Consequently,
those graduates who find themselves in a
position where they might have the opportunity
to use language processing techniques for the
development of sophisticated applications are
unlikely to have the full range of tools they need
at their disposal. The relatively narrow focus of
much undergraduate NLP teaching may also be
in part responsible for the fairly widespread
view amongst the uninitiated that NLP is
basically about parsing and not much else. This
perception results in occasional postings to
bulletin boards where senders from outside the
NLP research community request a ?parser?,
with their queries expressed in terms that make
it clear that they believe this one component will
solve all their NLP problems.
2.3 The Importance of the Job Market
We believe that if NLP is really to grow into a
field of substantial visibility and worth in the
wider industry community, there is a need to
raise the status of study in NLP beyond that of a
niche interest. The key to making this happen is
to emphasize the practical utility of work in the
field.
There is a real chicken-and-egg situation
here. We will only see an explosion in the
number of real NLP applications if there are
more well-rounded NLP practitioners working
in industry exploring and developing those
applications; but students are very savvy about
the job market, and, faced with a choice, are
unlikely to choose an NLP course over, say, a
networking course, when faced with the relative
proportions of job ads they see in the press and
on the web.
There are two related consequences of this.
First, evangelism is critical: we need to get more
trained students out there, offering NLP
solutions to problems. At the same time, we
need to give students concepts and techniques
that enable them to provide those solutions. We
need to provide material that students can see is
relevant, and that can be used in many contexts.
In our analysis, the job market for skills in
language processing, to the extent that it is
identifiable, consists of two major segments.
First, and most obviously, there are
companies that develop voice applications: there
are a great many companies now working in
this area, and voice recognition is a recognized
industry sector.
Second, there are companies that might use
NLP techniques in developing applications that
process, maintain and reuse documents,
whether on the desktop or on the Web. While
the first of these segments is quite clearly
identifiable, it is much more difficult to identify
a sector that focuses on using NLP techniques
on text. With some notable exceptions (and
these are largely small startups), we do not tend
to find companies whose focus is NLP. This is
not really surprising; NLP is just one tool
amongst many that might be used in document
processing, and document processing is
something that crops up in many contexts.
We therefore have a particular challenge
here: we need to communicate to students that
NLP is something they may be able to use in
their future careers, but we can?t point to many
job ads that specifically request NLP skills. The
intuition of those working in the field is that this
stuff ought to be something that can make a
difference in the processing of documents, but
there is not a lot of visible evidence that it is
being used in those situations. Anecdotal
personal experience suggests that many
companies would benefit from the application
of NLP skills but are not aware of this. One
suspects that organizations may often be
making use of techniques that we might want to
think of as NLP, but that these techniques are
not recognized as such.
3 The Program
Given the above, our goal was to construct a
range of courses that covered a broad range of
material that students might be able to use in
their subsequent careers. To emphasise the
practical orientation of what we wanted to do,
we deliberately pitched the program as being
concerned with Language Technology, rather
than as a program in either Natural Language
Processing or Computational Linguistics.
There is clearly something of an evangelical
element to this: we wanted to make students
aware of a broad range of techniques that we
would label Language Technology, with the
goal that, over time and as these students enter
the work force, an awareness would start to
spread that these techniques are widely usable.
This is not a short-term strategy: it takes several
years for the results of these efforts to permeate
through the system to a stage where they can be
evaluated, but it is essential to get started.
In this section, we present a summary of the
material we deliver in the courses that make up
our program. More detail on each of these
courses, and the program as a whole, can be
found at http://www.clt.mq.edu.au/Teaching.
The program consists of four courses that focus
principally on Language Technology, and an
additional course that looks more broadly at
technologies for working with the web. Figure 1
shows the prerequisite structure that currently
holds between these courses.
3.1 Comp248: An Introduction to
Natural Language Processing
Taught in the second half of second year, this is
the course in our program that most closely
matches the typical undergraduate NLP course.
The design of this course was driven by a desire
to show students that they could build a useful,
functioning application using NLP techniques;
to this end, we felt it was important not to teach
only computational syntax, but also something
about semantics. Our position here is that
syntactic processing is only a means to an end,
348:
Intelligent Text
Processing
248:
Introduction to
NLP
249:
Web Technology
349:
Interactive NL
Systems
448:
Advanced
Topics in NLP
Figure 1 : The Prerequisite Structure
and we felt it important to quickly get students
to the stage where they could actually see some
practical import of what they were doing. To
this end, in the first half of the course we take a
fairly standard approach to teaching Prolog,
whereby the students do some rudimentary
morphological processing, build some Definite
Clause Grammars, and learn about parsing
techniques. In the second half of the course, we
add semantics to the mix: although we teach an
introduction to lambda calculus at this stage, for
the practical work we focus on a much
shallower approach to semantics (effectively
semantic grammars), and the students build a
NL database query system that allows them to
ask questions of a database of flights. Along the
way they learn about unification-based
grammar, case frames, lexical resources,
WordNet, and semantic networks. The guiding
principle throughout is relevance to building a
practical application.
3.2 Comp249: Web Technology
Although this course is part of our Language
Technology program, it does not contain a
significant language technology element (at
least as the term is currently construed). It turns
out that the background material taught here
has proven to be very useful in other courses we
teach, so we are considering binding this course
more tightly to the others. The course covers:
Perl programming, web design, client-server
computing, search engines, XML and related
technologies, database integration, privacy and
security, VoiceXML, and content management;
inevitably, with such broad coverage, most
topics are treated relatively briefly.
Our goal for this course is to target a
student body who have little awareness of what
NLP is and to get them to see LT in a wider
perspective. The success of this course, which is
by far the most popular of the units in the
program, has led us to explore better ways of
leveraging this interest.
3.3 Comp348: Intelligent Text
Processing
At the third year level, we offer two courses that
take the second year material as a base. We
noted earlier that we viewed the job market as
consisting of two relatively distinct sectors, one
concerned with voice processing and one
concerned with document processing. This
perception is very deliberately reflected in the
individual biases of the third year offerings;
Comp348 addresses the needs of document
processing, whereas Comp349, discussed later,
leans more towards voice processing.
The course on intelligent text processing
covers basics of text processing using Perl;
tokenisation and sentence segmentation, text
summarisation; information retrieval; corpus-
based approaches, part of speech tagging, word
sense disambiguation, information extraction;
and machine translation. Again, this is a lot of
material to cover, and inevitably we only skim
the surface of many topics. However, in the first
offering of the course, students did significant
assignments in both text summarisation (using
sentence extraction) and information extraction.
The latter assignment was run roughly along the
lines of the Message Understanding
Conferences: using conference announcements
as a data set, the students were provided with a
training set on the basis of which they built an
information extraction system; this was then
tested against unseen data, and scores were
automatically derived. Now in its second
offering, our intention is to use anaphor
resolution as the focus of an assignment.
Our goal in this course is to provide
students with a toolset for text processing from
a language technology perspective. We focus on
relatively shallow methods, since these are the
methods students are most likely to find
themselves using in their subsequent careers.
Our driving aim here is for our alumni to
recognize that LT provides solutions.
3.4 Comp349: Interactive Natural
Language Systems
As already indicated, this course aims to
provide knowledge that students need in order
to be effective in the voice processing industry
sector.
The focus here is on, effectively, text- and
speech-based dialog systems. In the first half of
the course, we cover a significant amount of
relatively theoretical material, covering question
answering systems, database interfaces, and
answer extraction. Students build a quite
sophisticated text-based natural language query
system.
In the second half of the course, we attempt
to apply the theoretical ideas in the very
practical context of building spoken language
dialog systems. We begin by using the CSLU
Toolkit3, which the students use to build a voice
banking application. We then introduce
VoiceXML in some detail; using a PC-based
development environment, students build a
simple flight reservations system.4
We place a heavy emphasis here on aspects
of voice user inferface (VUI) design; in the
practical half of the course, the materials we use
take a similar approach to that taken in vendor
courses that aim to train dialog designers and
grammar writers. At the same time, we have as
an important aim a clear exposition of the
relationship between the ideas explored in
research systems and commercially deployed
systems; in practice it can be very hard to see a
path from the former to the latter. We make
clear to students that our goal is to teach them
how to build practical dialog applications now,
but to get them to think about what the next
generations of such applications might be in the
light of the results that come out of research
laboratories.
3.5 Comp448: Advanced Topics in
Natural Language Processing
For those students who stay on for a fourth year,
we run a course that is more driven by a
selection of specific research topics. At the time
of writing, the first offering of this course is
being delivered. We are using the course to
cover in more depth core topics that are only
really touched upon in earlier courses, with
more detailed exploration of word sense
disambiguation, anaphora resolution, discourse
structure and natural language generation. The
course is seminar-based, with a high proportion
3 This toolkit provides an excellent environment
for teaching students to think about issues such as
dialog flow, as well as introducing them to many
other aspects of spoken language dialog systems. See
http:// cslu.cse.ogi.edu/toolkit/.
4 We have experimented with a number of
different VoiceXML development environments
which are freely available over the web; each has its
advantages and disadvantages. Currently we?ve had
most success with Motorola?s MADK : see
http://developers.motorola.com/developers/. At the
time of writing, however, this does not support the
new VoiceXML 2.0 standard, so we are considering
other alternatives.
of student presentations, and an assignment in
anaphor resolution.
The level of interest amongst students at
this level is such that we expect to offer
additional honours level courses later in the
current academic year.
4 Outcomes and Issues
The program has been operating since the
second half of 2000. Since that time, we have
taught Comp248 twice and Comp349 once;
Comp249 and Comp348 are currently being
taught for the second time; and Comp448 is
being taught for the first time.
It is too early to establish to what extent the
material we have taught is impacting on
graduates? work practices: the first students to
complete degrees that incorporate our courses
are only now graduating. However, we have
made use of a number of feedback and review
mechanisms over the last 18 months, and these
have already provided us with new ideas for
how to improve what we are trying to do.
4.1 Evaluating Course Content
We make use of the typical infrastructure made
available for evaluation purposes: student-staff
liaison committees, formal questionnaires, and
also a significant amount of informal feedback
through discussions with students. We also
have a management advisory board with
representation from industry; this meets twice a
year to review the development of the program
and to comment on its industrial relevance.
Generally, the courses have been very well
received by the students who take them. Our
advisory board is very comfortable with the
material we teach, but we suffer here from the
problem that the voice recognition industry is
better represented here than the hard-to-define
document processing industry alluded to
earlier. So, we have strong evidence that
students find the material interesting,
challenging and informative; our industry
partners think we are going in the right
direction; but we have yet to demonstrate that
the wider industry community will see a benefit
from students who have grasped this material.
4.2 Course Materials
We have faced a not insignificant problem in
finding appropriate course materials for these
courses, with the consequence that we have had
to develop most things from scratch. For the
first offering of Comp248, the introductory NLP
course, we used Allen [1995]; in the second
offering, we found Covington [1994] to be more
useful. Although this is technically out of print,
Prentice Hall has a technology for producing
short print runs on demand.
The materials problem was more severe in
our third year courses, since there are no even
vaguely adequate textbooks for the material we
wanted to cover. We provide students with a
comprehensive reading packet, but it is not easy
to find appropriate survey or introductory
readings in the various topic areas we cover. As
a consequence of this we are exploring the
possibility of writing a textbook that covers the
material in each of these courses.
5 Lesssons Learned and Future
Directions
Eighteen months from the start of the program,
we are reasonably assured that we are going in
the right direction; some things, inevitably,
require fine tuning. We note here some key
consequences of our experiences so far.
5.1 Voice Captures the Imagination
Perhaps not surprisingly, it is the study of voice
recognition that has really captured students?
imaginations. The level of enthusiasm
generated in a laboratory full of students
wearing headsets talking to their machines is
wonderful to watch (although the working
environment doesn?t do a lot for speech
recognizer accuracy). With this in mind, we are
reworking our second year course, Comp248, so
that it will contain some of the voice material
currently used in third year. We are also
considering an emphasis here on technology
that students might meet outside of the
curriculum, such as chatterbots. Our strategy
here is to entice students into the area with
appealing content, and draw them into the more
theoretically challenging material in later
courses.
5.2 Document Processing as a Theme
It has become obvious that our Web Technology
course could play a more coherent role in our
program. One obvious direction we are
pursuing is to cement the two strands identified
earlier even further, by seeing the Web
Technology course specifically as a precursor for
the Intelligent Text Processing course. At the
same time, we are considering broadening the
third year course to cover Document Processing
more generally, as a way of making its relevance
more apparent; a shift of this kind might also
permit the inclusion of more material on
information retrieval and related technologies,
which are of some significance from an industry
perspective.
5.3 Linguistic Background
We have met the common, and not unexpected,
problem that some students do not have a
sufficient grasp of linguistic matters to perform
satisfactorily in this area. To this end, we have
initiated the introduction of a first year course
that covers basic aspects of linguistics, logic and
computation, taught by ourselves in conjunction
with the University?s Departments of
Philosophy and Linguistics.
5.4 Conclusions
So far, our program has been seen as very
successful from an academic perspective, and
has generated significant interest amongst
students. Our next challenge is to persuade the
wider industry to see students with this training
as very valuable assets. We have instituted an
alumni program that will attempt to track these
students, with the expectation of some
preliminary feedback being available by the end
of the calendar year.
References
James Allen [1995] Natural Language
Understanding. Benjamin Cummings, Menlo
Park, CA.
Michael Covington [1994] Natural Language
Processing for Prolog Programmers. Prentice Hall,
NJ.
Robert Dale, Hermann Moisl and Harold
Somers [2000] Handbook of Natural Language
Processing. Marcel Dekker, NY.
Daniel Jurafsky and James Martin [2000] Speech
and Language Processing: An Introduction to
Natural Language Processing, Computational
Linguistics and Speech Recognition. Prentice Hall,
NJ.
Exploiting Paraphrases in a Question Answering System
Fabio Rinaldi, James Dowdall,
Kaarel Kaljurand, Michael Hess
Institute of Computational Linguistics,
University of Zu?rich
Winterthurerstrasse 190
CH-8057 Zu?rich, Switzerland
{rinaldi,dowdall,kalju,hess}
@ifi.unizh.ch
Diego Molla?
Centre for Language Technology,
Macquarie University,
Sydney NSW 2109, Australia
{diego}@ics.mq.edu.au
Abstract
We present a Question Answering system
for technical domains which makes an in-
telligent use of paraphrases to increase the
likelihood of finding the answer to the user?s
question. The system implements a simple
and efficient logic representation of ques-
tions and answers that maps paraphrases
to the same underlying semantic represen-
tation. Further, paraphrases of technical
terminology are dealt with by a separate
process that detects surface variants.
1 Introduction
The problem of paraphrases conceals a number of
different linguistic problems, which in our opinion
need to be treated in separate ways. In fact, para-
phrases can happen at various levels in language. Us-
ing the examples provided in the call for papers for
this workshop, we would like to attempt a simple
classification, without any pretense of being exhaus-
tive:
1. Lexical synonymy.
Example: article, paper, publication
2. Morpho-syntactic variants.
a) Oswald killed Kennedy. / Kennedy was killed
by Oswald.
b) Edison invented the light bulb. / Edison?s
invention of the light bulb.
while (a) is purely syntactical (active vs pas-
sive), (b) involves a nominalisation.
3. PP-attachment.
a plant in Alabama / the Alabama plant
4. Comparatives vs superlatives.
be better than anybody else / be the best
5. Subordinate clauses vs separate sentences linked
by anaphoric pronouns.
The tree healed its wounds by growing new bark.
/ The tree healed its wounds. It grew new bark.
6. Inference.
The stapler costs $10. / The price of the stapler
is $10.
Where is Thimphu located? / Thimphu is capi-
tal of what country?
Of course combinations of the different types are
possible, e.g. Oswald killed Kennedy / Kennedy
was assassinated by Oswald is a combination of (1)
and (2).
Different types of knowledge and different linguis-
tic resources are needed to deal with each of the
above types. While type (1) can be dealt with us-
ing a resource such as WordNet (Fellbaum, 1998),
type (2) needs effective parsing and mapping of syn-
tactic structures into a common deeper structure,
possibly using a repository of nominalisations like
NOMLEX (Meyers et al, 1998). More complex
approaches are needed for the other types, up to
type (6) where generic world knowledge is required,
for instance to know that being a capital of a country
implies being located in that country. 1 Such world
knowledge could be expressed in the form of axioms,
like the following:
(X costs Y) iff (the price of X is Y)
In this paper we focus on the role of paraphrases
in a Question Answering (QA) system targeted at
1Note that the reverse is not true, and therefore this
is not a perfect paraphrase.
technical manuals. Technical documentation is char-
acterised by vast amounts of domain-specific termi-
nology, which needs to be exploited for providing in-
telligent access to the information contained in the
manuals (Rinaldi et al, 2002b). The approach taken
by QA systems is to allow a user to ask a query (for-
mulated in natural language) and have the system
search a background collection of documents in order
to locate an answer. The field of Question Answer-
ing has flourished in recent years2, in part, due to
the QA track of the TREC competitions (Voorhees
and Harman, 2001). These competitions evaluate
systems over a common data set alowing develop-
ers to benchmark performance in relation to other
competitors.
It is a common assumption that technical termi-
nology is subject to strict controls and cannot vary
within a given editing process. However this assump-
tion proves all too often to be incorrect. Unless edi-
tors are making use of a terminology control system
that forces them to use a specific version of a term,
they will naturally tend to use various paraphrases
to refer to the intended domain concept. Besides in
a query a user could use an arbitrary paraphrases of
the target term, which might happen to be one of
those used in the manual itself or might happen to
be a novel one.
We describe some potential solutions to this prob-
lem, taking our Question Answering system as an ex-
ample. We show which benefits our approach based
on paraphrases bring to the system. So far two dif-
ferent domains have been targeted by the system.
An initial application aims at answering questions
about the Unix man pages (Molla? et al, 2000a; Molla?
et al, 2000b). A more complex application targets
the Aircraft Maintenance Manual (AMM) of the Air-
bus A320 (Rinaldi et al, 2002b). Recently we have
started new work, using the Linux HOWTOs as a
new target domain.
In dealing with these domains we have identified
two major obstacles for a QA system, which we can
summarise as follows:
? The Parsing Problem
? The Paraphrase Problem
The Parsing Problem consists in the increased
difficulty of parsing text in a technical domain due to
domain-specific sublanguage. Various types of multi
word expressions characterise these domains, in par-
ticular referring to specific concepts like tools, parts
or procedures. These multi word expressions might
2Although early work in AI already touched upon the
topic, e.g. (Woods, 1977).
include lexical items which are either unknown to
a generic lexicon (e.g. coax cable) or have a spe-
cific meaning unique to this domain. Abbreviations
and acronyms are another common source of incon-
sistencies. In such cases the parser might either
fail to identify the compound as a phrase and con-
sequently fail to parse the sentence including such
items. Alternatively the parser might attempt to
?guess? their lexical category (in the set of open class
categories), leading to an exponential growth of the
number of possible syntactic parses. Not only the in-
ternal structure of the compound can be multi-way
ambiguous, even the boundaries of the compounds
might be difficult to detect and the parsers might
try odd combinations of the tokens belonging to the
compounds with neighbouring tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who can-
not be expected to be completely familiar with the
domain terminology. Even experienced users, who
know very well the domain, might not remember the
exact wording of a compound and use a paraphrase
to refer to the underlying domain concept. Besides
even in the manual itself, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identified as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphrases might be cre-
ated by the users each time they query the system.
In the rest of this paper we describe first our Ques-
tion Answering System (in Section 2) and briefly
show how we solved the first of the two problems
described above. Then, in Section 3 we show in de-
tail how the system is capable of coping with the
Paraphrase Problem. Finally in Section 4 we discuss
some related work.
2 A Question Answering System for
Technical Domains
Over the past few years our research group has devel-
oped an Answer Extraction system (ExtrAns) that
works by transforming documents and queries into a
semantic representation called Minimal Logical Form
(MLF) (Molla? et al, 2000a) and derives the answers
by logical proof from the documents. A full linguis-
tic (syntactic and semantic) analysis, complete with
lexical alternations (synonyms and hyponyms) is per-
formed. While documents are processed in an off-line
stage, the query is processed on-line.
Two real world applications have so far been im-
plemented with the same underlying technology. The
original ExtrAns system (Molla? et al, 2000b) is used
///// a.d electrical coax cable.n4 connects.v062 the.d external antenna.n1 to.o the.d ANT connection.n1 /////
-Wd
ff Dsu ff Ss
-
MVp
-Os
ff Ds
-Js
ff Ds
RW
Figure 1: An Example of LG Output
to extract answers to arbitrary user queries over the
Unix documentation files (?man pages?). A set of
500+ unedited man pages has been used for this ap-
plication. An on-line demo of ExtrAns can be found
at the project web page.3
 Knowledge 
Base
Document
Linguistic
Analysis
Term
processing
Figure 2: Off-line
Processing of Docu-
ments
More recently we tackled
a different domain, the Air-
plane Maintenance Manu-
als (AMM) of the Air-
bus A320 (Rinaldi et al,
2002b), which offered the
additional challenges of an
SGML-based format and a
much larger size (120MB).4
Despite being developed
initially for a specific do-
main, ExtrAns has demon-
strated a high level of do-
main independence.
As we work on relatively
small volumes of data we
can afford to process (in
an off-line stage) all the
documents in our collection
rather than just a few se-
lected paragraphs (see Fig-
ure 2). Clearly in some sit-
uations (e.g. processing in-
coming news) such an ap-
proach might not be fea-
sible and paragraph index-
ing techniques would need
to be used. Our current ap-
proach is particularly tar-
geted to small and medium sized collections.
In an initial phase all multi-word expressions
from the domain are collected and structured in
an external resource, which we will refer to as the
TermBase (Rinaldi et al, 2003; Dowdall et al, 2003).
The document sentences (and user queries) are syn-
tactically processed with the Link Grammar (LG)
parser (Sleator and Temperley, 1993) which uses a
3http://www.ifi.unizh.ch/cl/extrans/
4Still considerably smaller than the size of the docu-
ment collections used for TREC
grammar with a wide coverage of English and has
a robust treatment of ungrammatical sentences and
unknown words. The multi-word terms from the the-
saurus are identified and passed to the parser as sin-
gle tokens. This prevents (futile) analysis of the in-
ternal structure of terms (see Figure 1), simplifying
parsing by 46%. This solves the first of the problems
that we have identified in the introduction (?The
Parsing Problem?).
In later stages of processing, a corpus-based ap-
proach (Brill and Resnik, 1994) is used to deal with
ambiguities that cannot be solved with syntactic in-
formation only, in particular attachments of preposi-
tional phrases, gerunds and infinitive constructions.
ExtrAns adopts an anaphora resolution algorithm
(Molla? et al, 2003) that is based on Lappin and Le-
ass? approach (Lappin and Leass, 1994). The original
algorithm, which was applied to the syntactic struc-
tures generated by McCord?s Slot Grammar (Mc-
Cord et al, 1992), has been ported to the output of
Link Grammar. So far the resolution is restricted to
sentence-internal pronouns but the same algorithm
can be applied to sentence-external pronouns too.
A lexicon of nominalisations based on NOMLEX
(Meyers et al, 1998) is used for the most important
cases. The main problem here is that the semantic
relationship between the base words (mostly, but not
exclusively, verbs) and the derived words (mostly,
but not exclusively, nouns) is not sufficiently sys-
tematic to allow a derivation lexicon to be compiled
automatically. Only in relatively rare cases is the
relationship as simple as with to edit <a text> ?
editor of <a text> / <text> editor, as the effort
that went into building resources such as NOMLEX
also shows.
User queries are processed on-line and converted
into MLFs (possibly expanded by synonyms) and
proved by refutation over the document knowledge
base (see Figure 3). Pointers to the original text at-
tached to the retrieved logical forms allow the system
to identify and highlight those words in the retrieved
sentence that contribute most to that particular an-
swer. When the user clicks on one of the answers
provided, the corresponding document will be dis-
played with the relevant passages highlighted.
 Knowledge 
Base
ANSWERSQuery
Document
Linguistic
Analysis
Paraphrase
Identification
Figure 3: On-line Processing of Queries
The meaning of the documents and of the queries
produced by ExtrAns is expressed by means of Mini-
mal Logical Forms (MLFs). The MLFs are designed
so that they can be found for any sentence (using
robust approaches to treat very complex or ungram-
matical sentences), and they are optimized for NLP
tasks that involve the semantic comparison of sen-
tences, such as Answer Extraction.
The expressivity of the MLFs is minimal in the
sense that the main syntactic dependencies between
the words are used to express verb-argument rela-
tions, and modifier and adjunct relations. However,
complex quantification, tense and aspect, temporal
relations, plurality, and modality are not expressed.
One of the effects of this kind of underspecification
is that several natural language queries, although
slightly different in meaning, produce the same logi-
cal form.
The main feature of the MLFs is the use of reifi-
cation (the expression of abstract concepts as con-
crete objects) to achieve flat expressions (Molla? et
al., 2000b). The MLFs are expressed as conjunc-
tions of predicates with all the variables existentially
bound with wide scope. For example, the MLF of
the sentence ?cp will quickly copy the files? is:
(1) holds(e4), object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type cp and of type command,
there is an entity x6 (a file), there is an entity e4,
which represents a copying event where the first ar-
gument is x1 and the second argument is x6, there
is an entity p3 which states that e4 is done quickly,
and the event e4, that is, the copying, holds. The
entities o1, o2, o3, e4, and p3 are the result of reifi-
cation. The reification of the event, e4, has been used
to express that the event is done quickly. The other
entities are not used in this MLF, but other more
complex sentences may need to refer to the reifica-
tion of properties (adjective-modifying adverbs) or
object predicates (non-intersective adjectives such as
the alleged suspect).
ExtrAns finds the answers to the questions by
forming the MLFs of the questions and then run-
ning Prolog?s default resolution mechanism to find
those MLFs that can prove the question. When no
direct proof for the user query is found, the system
is capable of relaxing the proof criteria in a stepwise
manner. First, hyponyms of the query terms will be
added as disjunctions in the logical form of the ques-
tion, thus making it more general but still logically
correct. If that fails, the system will attempt approx-
imate matching, in which the sentence (or sentences)
with the highest overlap of predicates with the query
is retrieved. The (partially) matching sentences are
scored and the best fits are returned. In the case
that this method finds too many answers because
the overlap is too low, the system will attempt key-
word matching, in which syntactic criteria are aban-
doned and only information about word classes is
used. This last step corresponds approximately to a
traditional passage-retrieval methodology with con-
sideration of the POS tags.
3 Dealing with Paraphrases
The system is capable of dealing with paraphrases
at two different levels. On the phrase level, differ-
ent surface realizations (terms) which refer to the
same domain concept will be mapped into a com-
mon identifier (synset identifier). On the sentence
level, paraphrases which involve a (simple) syntactic
transformation will be dealt with by mapping them
into the same logical form. In this section we will
describe these two approaches and discuss ways to
cope with complex types of parapharases.
3.1 Identifying Terminological Paraphrases
During the construction of the MLFs, thesaurus
terms are replaced by their synset identifiers. This
results in an implicit ?terminological normalization?
for the domain. The benefit to the QA process is
an assurance that a query and answer need not in-
volve exactly the same surface realization of a term.
Utilizing the synsets in the semantic representation
means that when the query includes a term, ExtrAns
returns sentences that logically answer the query, in-
Fastr
Term
Extraction
Hyponymy
Thesaurus ExtrAns
Document
Figure 4: Term Processing
volving any known paraphrase of that term.
For example, the logical form of the query Where
are the stowage compartments installed? is trans-
lated internally into the Horn query (2).
(2) evt(install,A,[B,C]),
object(D,E,[B]),
object(s stowage compartment,G,[C])
This means that a term (belonging to the same
synset as stowage compartment) is involved in an in-
stall event with an anonymous object. If there is
an MLF from the document that can match exam-
ple (2), then it is selected as a candidate answer and
the sentence it originates from is shown to the user.
The process of terminological variation is well
investigated (Ibekwe-SanJuan and Dubois, 2002;
Daille et al, 1996; Ibekwe-Sanjuan, 1998). The
primary focus has been to use linguistically based
variation to expand existing term sets through cor-
pus investigation or to produce domain representa-
tions. A subset of such variations identifies terms
which are strictly synonymous. ExtrAns gathers
these morpho-syntactic variations into synsets. The
sets are augmented with terms exhibiting three
weaker synonymy relations described by Hamon &
Nazarenko (2001). These synsets are organized into
a hyponymy (isa) hierarchy, a small example of which
can be seen in Figure 5. Figure 4 shows a schematic
representation of this process.
The first stage is to normalize any terms that con-
tain punctuation by creating a punctuation free ver-
sion and recording the fact that that the two are
strictly synonymous. Further processing is involved
in terms containing brackets to determine if the
bracketed token is an acronym or simply optional. In
the former case an acronym-free term is created and
the acronym is stored as a synonym of the remain-
ing tokens which contain it as a regular expression.
So evac is synonymous with evacuation and ohsc is
synonymous with overhead stowage compartment. In
cases such as emergency (hard landings) the brack-
eted tokens can not be interpreted as acronyms and
so are not removed.
The synonymy relations are identified using the
terminology tool Fastr (Jacquemin, 2001). Every to-
ken of each term is associated with its part-of-speech,
its morphological root, and its synonyms. Phrasal
rules represent the manner in which tokens combine
to form multi-token terms, and feature-value pairs
carry the token specific information. Metarules li-
cense the relation between two terms by constrain-
ing their phrase structures in conjunction with the
morphological and semantic information on the indi-
vidual tokens.
The metarules can identify simple paraphrases
that result from morpho-syntactic variation (cargo
compartment door ?? doors of the cargo compart-
ment), terms with synonymous heads (electrical ca-
ble ?? electrical line), terms with synonymous mod-
ifiers (fastener strip ?? attachment strip) and both
(functional test ?? operational check). For a de-
scription of the frequency and range of types of vari-
ation present in the AMM see Rinaldi et al (2002a).
3.2 Identifying Syntactic Paraphrases
An important effect of using a simplified semantic-
based representation such as the Minimal Logical
Forms is that various types of syntactic variations
are automatically captured by a common representa-
tion. This ensures that many potential paraphrases
in a user query can map to the same answer into the
manual.
For example the question shown in Figure 6 can
be answered thanks to the combination of two fac-
tors. On the lexical level ExtrAns knows that APU
is an abbreviation of Auxiliary Power Unit, while on
the syntactic level the active and passive voices (sup-
plies vs supplied with) map into the same underlying
representation (the same MLF).
Another type of paraphrase which can be detected
at this level is the kind that was classified as type (3)
in the introduction. For example the question: Is
the sensor connected to the APU ECB?, can locate
the answer This sensor is connected to the Elec-
tronic Control Box (ECB) of the APU. This has been
achieved by introducing meaning postulates that op-
erate at the level of the MLFs (such as ?any predicate
that affects an object will also affect the of -modifiers
of that object?).
3.3 Weaker Types of Paraphrases
When the thesaurus definition of terminological syn-
onymy fails to locate an answer from the docu-
ment collection, ExtrAns explores weaker types of
paraphrases, where the equivalence between the two
terms might not be complete.
TERM
doors of the cargo compartment
cargo compartment door
cargo comparment doors
cargo-compartment door
emergency ( hard landings )
emergency hard landings
emergency hard landing
emergency evacuation (evac)
emergency evacuation
evacuation
evac
electrical cable
electrical line
fastner strip
attachment strip
functional test
operational check
door functional test
stowage compartment
overhead stowage compartment
OHSC
1
2
3
5
6
7
10
9
8
11
Figure 5: A Sample of the TermBase
Figure 6: Active vs Passive Voice
First, ExtrAns makes use of the hyponymy rela-
tions, which can be considered as sort of unidirec-
tional paraphrases. Instead of looking for synset
members, the query is reformulated to included hy-
ponyms and hyperonyms of the terms:
(3) (object(s stowage compartment,A,[B]);
object(s overhead stowage compartment,A,[B])),
evt(install,C,[D,B]),
object(E,F,[D|G])
Now the alternative objects are in a logical OR rela-
tion. This query finds the answer in Figure 7 (where
stowage compartment is a hyperonym of overhead
stowage compartment).
We have implemented a very simple ad-hoc algo-
rithm to determine lexical hyponymy between terms.
Term A is a hyponym of term B if (i) A has more to-
kens than B, (ii) all the tokens of B are present in A,
and (iii) both terms have the same head. There are
three provisions. First, ignore terms with dashes and
brackets as cargo compartment is not a hyponym of
cargo - compartment and this relation (synonymy) is
already known from the normalisation process. Sec-
ond, compare lemmatised versions of the terms to
capture that stowage compartment is a hyperonym
of overhead stowage compartments. Finally, the head
of a term is the rightmost non-symbol token (i.e. a
word) which can be determined from the part-of-
speech tags. This hyponymy relation is compara-
ble to the insertion variations defined by Daille et
al. (1996).
The expressivity of the MLF can further be ex-
panded through the use of meaning postulates of the
type: ?If x is installed in y, then x is in y?. This
ensures that the query Where are the equipment and
furnishings? extracts the answer The equipment and
furnishings are installed in the cockpit.
4 Related Work
The importance of detecting paraphrasing in Ques-
tion Answering has been shown dramatically in
TREC9 by the Falcon system (Harabagiu et al,
2001), which made use of an ad-hoc module capable
of caching answers and detecting question similar-
ity. As in that particular evaluation the organisers
deliberately used a set of paraphrases of the same
questions, such approach certainly helped in boost-
ing the performance of the system. In an environ-
ment where the same question (in different formula-
tions) is likely to be repeated a number of times, a
module capable of detecting paraphrases can signif-
icantly improve the performance of a Question An-
Figure 7: Overhead stowage compartment is a Hyponym of Stowage compartment
swering system.
Another example of application of paraphrases for
Question Answering is given in (Murata and Isahara,
2001), which further argues for the importance of
paraphrases for other applications such Summarisa-
tion, error correction and speech generation.
Our approach for the acquisition of terminological
paraphrases might have some points in common with
the approach described in (Terada and Tokunaga,
2001). The motivation that they bring forward for
the necessity of identifying abbreviations is related to
the problem that we have called ?the Parsing Prob-
lem?.
A very different approach to paraphrases is taken
in (Takahashi et al, 2001) where they formulate the
problem as a special case of Machine Translation,
where the source and target language are the same
but special rules, based on different parameters, li-
cense different types of surface realizations.
Hamon & Nazarenko (2001) explore the termino-
logical needs of consulting systems. This type of IR
guides the user in query/keyword expansion or pro-
poses various levels of access into the document base
on the original query. A method of generating three
types of synonymy relations is investigated using gen-
eral language and domain specific dictionaries.
5 Conclusion
Automatic recognition of paraphrases is an effec-
tive technique to ease the information access bur-
den in a technical domain. We have presented some
techniques that we have adopted in a Question An-
swering system for dealing with paraphrases. These
techniques range from the detection of lexical para-
phrases and terminology variants, to the use of a
simplified logical form that provides the same repre-
sentation for morpho-syntactic paraphrases, and the
use of meaning postulates for paraphrases that re-
quire inferences.
References
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
ambiguation. In Proc. COLING ?94, volume 2,
pages 998?1004, Kyoto, Japan.
Beatrice Daille, Benot Habert, Christian Jacquemin,
and Jean Royaute?. 1996. Empirical observation of
term variations and principles for their description.
Terminology, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
SanJuan, and Eric SanJuan. 2003. Complex
structuring of term variants for Question Answer-
ing. In Proc. ACL-2003 Workshop on Multiword
Expressions, Sapporo, Japan.
Christiane Fellbaum 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Thierry Hamon and Adeline Nazarenko. 2001. De-
tection of synonymy links between terms: Experi-
ment and results. In Didier Bourigault, Christian
Jacquemin, and Marie-Claude L?Homme, editors,
Recent Advances in Computational Terminology,
pages 185?208. John Benjamins Publishing Com-
pany.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana G??rju, Vasile Rus, and Paul Morarescu.
2001. Falcon: Boosting knowledge for answer
engines. In Voorhees and Harman (Voorhees and
Harman, 2001).
Fidelia Ibekwe-SanJuan and Cyrille Dubois. 2002.
Can Syntactic Variations Highlight Semantic
Links Between Domain Topics? In Proceedings
of the 6th International Conference on Terminol-
ogy and Knowledge Engineering (TKE02), pages
57?64, Nancy, August.
Fidelia Ibekwe-Sanjuan. 1998. Terminological Vari-
ation, a Means of Identifying Research Topics from
Texts. In Proceedings of COLING-ACL, pages
571?577, Quebec,Canada, August.
Christian Jacquemin. 2001. Spotting and Discover-
ing Terms through Natural Language Processing.
MIT Press.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535?561.
Michael McCord, Arendse Bernth, Shalom Lap-
pin, and Wlodek Zadrozny. 1992. Natural lan-
guage processing within a slot grammar frame-
work. International Journal on Artificial Intelli-
gence Tools, 1(2):229?277.
Adam Meyers, Catherine Macleod, Roman Yangar-
ber, Ralph Grishman, Leslie Barrett, and Ruth
Reeves. 1998. Using NOMLEX to produce
nominalization patterns for information extrac-
tion. In Proceedings: the Computational Treat-
ment of Nominals, Montreal, Canada, (Coling-
ACL98 workshop), August.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000a. Answer Extraction using
a Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000b. Extrans, an answer ex-
traction system. T.A.L. special issue on Informa-
tion Retrieval oriented Natural Language Process-
ing.
Diego Molla?, Rolf Schwitter, Fabio Rinaldi, James
Dowdall, and Michael Hess. 2003. Anaphora res-
olution in ExtrAns. In Proceedings of the Interna-
tional Symposium on Reference Resolution and Its
Applications to Question Answering and Summa-
rization, 23?25 June, Venice, Italy.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal model for paraphrasing - using transformation
based on a defined criteria. In Proceedings of the
NLPRS2001 Workshop on Automatic Paraphras-
ing: Theories and Applications.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002a. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Interna-
tional Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?
30 August.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002b. Towards An-
swer Extraction: an application to Technical Do-
mains. In ECAI2002, European Conference on Ar-
tificial Intelligence, Lyon, 21?26 July.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, and Magnus Karlsson. 2003. The Role
of Technical Terminology in Question Answering.
In Proceedings of TIA-2003, Terminologie et In-
telligence Artificielle, Strasbourg, April.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, and
Kentaro Inui. 2001. Kura: A revision-based
lexico-structural paraphrasing engine. In Proceed-
ings of the NLPRS2001 Workshop on Automatic
Paraphrasing: Theories and Applications.
Akira Terada and Takenobu Tokunaga. 2001. Au-
tomatic disabbreviation by using context informa-
tion. In Proceedings of the NLPRS2001 Workshop
on Automatic Paraphrasing: Theories and Appli-
cations.
Ellen M. Voorhees and Donna Harman, editors.
2001. Proceedings of the Ninth Text REtrieval
Conference (TREC-9), Gaithersburg, Maryland,
November 13-16, 2000.
W.A. Woods. 1977. Lunar rocks in natural English:
Explorations in Natural Language Question An-
swering. In A. Zampolli, editor, Linguistic Struc-
tures Processing, volume 5 of Fundamental Studies
in Computer Science, pages 521?569. North Hol-
land.
Workshop on TextGraphs, at HLT-NAACL 2006, pages 37?44,
New York City, June 2006. c?2006 Association for Computational Linguistics
Learning of Graph-based Question Answering Rules
Diego Molla?
Department of Computing
Macquarie University
Sydney 2109, Australia
diego@ics.mq.edu.au
Abstract
In this paper we present a graph-based
approach to question answering. The
method assumes a graph representation
of question sentences and text sentences.
Question answering rules are automati-
cally learnt from a training corpus of ques-
tions and answer sentences with the an-
swer annotated. The method is indepen-
dent from the graph representation formal-
ism chosen. A particular example is pre-
sented that uses a specific graph represen-
tation of the logical contents of sentences.
1 Introduction
Text-based question answering (QA) is the process
of automatically finding the answers to arbitrary
questions in plain English by searching collections
of text files. Recently there has been intensive re-
search in this area, fostered by evaluation-based
conferences such as the Text REtrieval Conference
(TREC) (Voorhees, 2001b), the Cross-Lingual Eval-
uation Forum (CLEF) (Vallin et al, 2005), and the
NII-NACSIS Test Collection for Information Re-
trieval Systems workshops (NTCIR) (Kando, 2005).
Current research focuses on factoid question an-
swering, whereby the answer is a short string that
indicates a fact, usually a named entity. An exam-
ple of a factoid question is Who won the 400m race
in the 2000 Summer Olympic games?, which has a
short answer: Cathy Freeman.
There are various approaches to question answer-
ing. The focus of this paper is on rule-based sys-
tems. A rule could be, say, ?if the question is of
the form Who is the <position> of <country>? and
a text sentence says <position> of <country> Y
and Y consists of two capitalised words, then Y is
the answer?). Such a rule was used by Soubbotin
(2001), who developed a system who obtained the
best accuracy in the 2001 Text REtrieval Conference
(Voorhees, 2001a). The system developed by Soub-
botin (2001) relied on the development of a large
set of patterns of potential answer expressions, and
the allocation of those patterns to types of questions.
The patterns were developed by hand by examining
the data.
Soubbotin (2001)?s work shows that a rule-based
QA system can produce good results if the rule set is
comprehensive enough. Unfortunately, if the system
is ported to a new domain the set of rules needs to
be ported as well. It has not been proven that rules
like the ones developed by Soubbotin (2001), which
were designed for the TREC QA task, can be ported
to other domains. Furthermore, the process of pro-
ducing the rules was presumably very labour inten-
sive. Consequently, the cost of manually producing
new rules for a specialised domain could become too
expensive for some domains.
In this paper we present a method for the auto-
matic learning of question answering rules by apply-
ing graph manipulation methods. The method relies
on the representation of questions and answer sen-
tences as graphs. Section 2 describes the general
format of the graph-based QA rules and section 3
describes the method to learn the rules. The meth-
ods described on the above two sections are indepen-
dent of the actual sentence representation formalism,
37
as long as the representation is a graph. Section 4
presents a specific application using logical graphs.
Finally, sections 5 and 6 focus on related research
and final conclusions, respectively.
2 Question Answering Rules
In one form or another, a question answering rule
must contain the following information:
1. a pattern that matches the question;
2. a pattern that matches the corresponding an-
swer sentence; and
3. a pointer to the answer in the answer sentence
The patterns in our rules are expressed as graphs
with vertices containing variables. A vertex with
a variable can unify with a subgraph. For exam-
ple, Figure 1 shows two graphs and a pattern that
matches both graphs.
Graph 1
1 2 3
4
5
Graph 2
1 2 7 8
9
Pattern
1 2 X
Y
Figure 1: Two graphs and a pattern (variables in up-
percase)
Such patterns are used to match the graph repre-
sentation of the question. If a pattern defined in a
rule matches a question sentence, then the rule ap-
plies to the sentence.
Our rules specify the pattern of the answer sen-
tence in an unusual way. Instead of keeping a pat-
tern to match the answer sentence, our rules define
an extension graph that will be added to the graph of
the question. The rationale for this is that we want to
reward answer sentences that have a high similarity
with the question. Therefore, the larger the num-
ber of vertices and edges that are shared between the
question and the answer, the better. The extension
graph contains information that simulates the dif-
ference between a question sentence and a sentence
containing an answer.
For example, lets us use graph representations
of syntactic dependency structures. We will base
our representation on the output of Connexor
(Tapanainen and Ja?rvinen, 1997), but the choice of
parser is arbitrary. The same method applies to the
output of any parser, as long as it can be represented
as a graph. In our choice, the dependency structure
is represented as a bipartite graph where the lexi-
cal entries are the vertices represented in boxes and
the dependency labels are the vertices represented in
ovals. Figure 2 shows the graphs of a question and
an answer sentence, and an extension of the question
graph. The answer is shown in thick lines, and the
extension is shown in dashed lines. This is what we
aim to reproduce with our graph rules. In particular,
the extension of the question graph is such that the
graph of the answer sentence becomes a subgraph of
the extended question graph.
The question and answer sentence of Figure 2
have an almost identical dependency graph and con-
sequently the extension required to the question
graph is very small. Sentence pairs with more dif-
ferences would induce a more substantial extension
graph.
Note that the extended graph still contains the rep-
resentation of information that does not appear in
the answer sentence, namely the question term what
book. There is no need to remove any element from
the question graph because, as we will see later, the
criteria to score the answer extracted are based on
the overlap between graphs.
In sum, a graph rule has the following compo-
nents:
Rp a question pattern;
Re an extension graph, which is a graph to be added
38
Q: What book did Rachel Carson write in 1962? A: In 1962 Rachel Carson wrote ?Silent Spring?
write
v ch obj loc
do book in
subj det pcomp
carson what 1962
attr
rachel
write
subj obj tmp
carson in
attr
1962
spring
attr
silent
Q extended
write
v ch obj loc
do book in
subj det pcomp
carson what 1962
attr
rachel
spring
attr2
silent
Figure 2: Graph of a question, an answer sentence, and an extension of the question graph
39
to the question graph; and
Ra a pointer to the answer in the extension graph
An example of a rule is shown in Figure 3. This
rule is derived from the pair of question and answer
sentence shown in Figure 2.
X
obj
Y
det
what
ANSWER
Figure 3: Example of a QA rule. Rp is in solid lines,
Re is in dashed lines, and Ra is in thick lines.
The rule can be used with a fresh pair of question
qi and answer sentence asi. Let us use the notation
Gr(s) to denote the graph that represents the string
s. Also, unless said explicitly, names starting with
uppercase denote graphs, and names starting with
lowercase denote strings. Informally, the process to
find the answer is:
1. If Gr(qi) matches Rp then the rule applies.
Otherwise try a new rule.
2. Extend Gr(qi) with re to produce a new graph
EReqi .
3. Compute the overlap between EReqi and
Gr(asi).
4. If a part of Ra is in the resulting overlap, then
expand its projection on Gr(asi).
The crucial point in the process is to determine
the projection of an overlap on the answer sentence,
and then to extend it. Once the overlap is found in
step 3, if this overlap includes part of the annotated
answer, that is if it includes Ra, then part of the an-
swer will be the string in the answer sentence that
corresponds to the overlap. The full answer can be
retrieved by expanding the answer found in the over-
lap by following the outgoing edges in the graph of
qi What book did Michael Ende write in 1984? ex-
tended with the extension graph (Re) of Figure 3
write
v ch obj loc
do book in
subj det pcomp
ende what 1984
attr
michael
ANSWER
asi In 1984 Michael Ende wrote the novel titled
?The Neverending Story?
write
v ch obj loc
do novel in
subj det mod pcomp
ende the title 1984
attr mod
michael story
det attr
the neverending
Figure 4: An extended graph of a question and a
graph of an answer sentence
40
write
v ch obj loc
do in
subj pcomp
ende 1984
attr
michael
novel
Figure 5: Overlap of the graphs of Figure 4
the answer. Part of the process is shown in Figures 4
and 5.
In Figure 5 the overlap between the extended
question graph and the answer sentence graph con-
tains the answer fragment novel. After expanding it
we obtain the full answer the novel titled ?The Never
Ending Story?.1
3 Learning of Graph Rules
To learn a QA rule we need to determine the in-
formation that is common between a question and
a sentence containing an answer. In terms of graphs,
this is a variant of the well-known problem of find-
ing the maximum common subgraph (MCS) of two
graphs (Bunke et al, 2002).
The problem of finding the MCS of two graphs is
known to be NP-complete, but there are implemen-
tations that are fast enough for practical uses, espe-
cially if the graphs are not particularly large (Bunke
et al, 2002). Given that our graphs are used to repre-
sent sentences, their size would usually stay within
a few tens of vertices. This size is acceptable.
There is an algorithm based on Conceptual
Graphs (Myaeng and Lo?pez-Lo?pez, 1992) which is
particularly efficient for our purposes.Their method
follows the traditional procedure of building the as-
sociation graph of the two input graphs. However, in
1Note that this answer is not an exact answer according to
the TREC definition since it contains the string the novel titled;
one further step would be needed to extract the exact answer;
this is work for further research.
contrast with the traditional approach, which finds
the cliques of the association graph (and this is the
part that is NP-complete), the method by Myaeng
and Lo?pez-Lo?pez (1992) first simplifies the associa-
tion graph by merging some of its vertices, and then
it proceeds to searching the cliques. By so doing the
algorithm is still exponential on the size of n, but
now n is smaller than with the traditional approach
for the same input graphs.
The method presented by Myaeng and Lo?pez-
Lo?pez (1992) finds connected graphs but we also
need to find overlaps that form unconnected graphs.
For example, Figure 6 shows two graphs and their
MCS. The resulting MCS is an unconnected graph,
though Myaeng and Lo?pez-Lo?pez (1992)?s algo-
rithm returns the two parts of the graph as indepen-
dent MCSs. It is easy to modify the original algo-
rithm to obtain the desired output, as we did.
Graph 1 Graph 2
1
2
3
4
5
1
2
3 4
5
MCS (overlap)
1
2
4
5
Figure 6: MCS of two graphs
Given two graphs G1 and G2, then their MCS
is MCS(G1, G2). To simplify the notation, we
will often refer to the MCS of two sentences
as MCS(s1, s2). This is to be understood to
be the MCS of the graphs of the two sentences
MCS(Gr(s1), Gr(s2)).
Let us now assume that the graph rule R is origi-
nated from a pair (q,as) in the training corpus, where
q is a question and as a sentence containing the an-
swer a. The rule components are built as follows:
Rp is the MCS of q and as, that is, MCS(q, as).
Re is the path between the projection of Rp in
Gr(as) and the actual answer Gr(a).
Ra is the graph representation of the exact answer.
41
Note that this process defines Rp as the MCS of
question and answer sentence. Consequently, Rp
is a subgraph of both the question and the answer
sentence. This constraint is stronger than that of a
typical QA rule, where the pattern needs to match
the question only. The resulting question pattern is
therefore more general than it could be had one man-
ually built the rule. Rp does not include question-
only elements in the question pattern because it is
difficult to determine what components of the ques-
tion are to be added to the pattern, and what compo-
nents are idiosyncratic to the specific question used
in the training set.
Rules learnt this way need to be generalised in or-
der to form generic patterns. We currently use a sim-
ple method of generalisation: convert a subset of the
vertices into variables. To decide whether a vertex
can be generalised a list of very common vertices is
used. This is the list of ?stop vertices?, in analogy to
the concept of stop words in methods to detect key-
words in a string. Thus, if a vertex is not in the list
of stop vertices, then the vertex can be generalised.
The list of stop vertices is fixed and depends on the
graph formalism used.
For the question answering process it is useful to
associate a weight to every rule learnt. The rule
weight is computed by testing the accuracy of the
rule in the training corpus. This way, rules that over-
generalise acquire a low weight. The weight W(r)
of a rule r is computed according to its precision on
the training set:
W(r) =
# correct answers found
# answers found
4 Application: QA with Logical Graphs
The above method has been applied to graphs rep-
resenting the logical contents of sentences. There
has been a long tradition on the use of graphs for
this kind of sentence representation, such as Sowa?s
Conceptual Graphs (Sowa, 1979), and Quillian?s Se-
mantic Nets (Quillian, 1968). In our particular ex-
periment we have used a graph representation that
can be built automatically and that can be used effi-
ciently for QA (Molla? and van Zaanen, 2006).
A Logical Graph (LG) is a directed, bipartite
graph with two types of vertices, concepts and re-
lations.
Concepts Examples of concepts are objects dog, ta-
ble, events and states run, love, and properties
red, quick.
Relations Relations act as links between concepts.
To facilitate the production of the LGs we have
decided to use relation labels that represent
verb argument positions. Thus, the relation 1
indicates the link to the first argument of a verb
(that is, what is usually a subject). The re-
lation 2 indicates the link to the second argu-
ment of a verb (usually the direct object), and
so forth. Furthermore, relations introduced by
prepositions are labelled with the prepositions
themselves. Our relations are therefore close to
the syntactic structure.
An example of a LG is shown in Figure 7, where
the concepts are pictured in boxes and the relations
are pictured in ovals.
The example in Figure 7 shows LG?s ability to
provide the graph representation of sentences with
embedded clauses. In contrast, other theories (such
as Sowa (1979)?s Conceptual Graphs) would rep-
resent the sentence as a graph containing vertices
that are themselves graphs. This departs from the
usual definition of a graph, and therefore standard
Graph Theory algorithms would need to be adapted
for Conceptual Graphs. An advantage of our LGs,
therefore, is that they can be manipulated with stan-
dard Graph Theory algorithms such as the ones de-
scribed in this paper.
Using the LG as the graph representation of
questions and answer sentences, we implemented a
proof-of-concept QA system. The implementation
and examples of graphs are described by Molla? and
van Zaanen (2005) and here we only describe the
method to generalise rules and the decisions taken
to choose the exact answer.
The process to generalise rules takes advantage
of the two kinds of vertices. Basically, relation ver-
tices represent names of relations and we considered
these to be important in the rule. Consequently rela-
tions edges were left unmodified in the generalised
rule. Concept vertices are generalised by replacing
them with generic variables, except for a specific set
of ?stop concepts? which were not generalised. The
list of stop concepts is very small:
42
tom 1 believe 2
want1mary 2
marry1 2
sailor
Tom believes that Mary wants to marry a sailor
Figure 7: Example of a Logical Graph
and, or, not, nor, if, otherwise, have, be,
become, do, make
Every question/answer pair in the training corpus
generates one rule (or more if we use a process of
increasingly generalising the rules). Since the rule is
based on deep linguistic information, it generalises
over syntactic paraphrases. Consequently, a small
training corpus suffices to produce a relatively large
number of rules.
The QA system was trained with an annotated
corpus of 560 pairs of TREC questions and answer
sentences where the answers were manually anno-
tated. We only tested the ability of the system to ex-
tract the exact answers. Thus, the system accepted
pairs of question and answer sentences (where the
sentence is guaranteed to contain an answer), and
returned the exact answer. Given a question and an-
swer sentence pair, the answer is found by applying
all matching rules. All strings found as answers are
ranked by multiplying the rule weights and the sizes
of the overlaps. If an answer is found by several
rules, its score is the sum of all scores of each indi-
vidual sentence. Finally, if an answer occurs in the
question it is ignored. The results of a five-fold cross
validation on the annotated corpus gave an accuracy
(percentage of questions where the correct answer
was found) of 21.44%. Given that the QA system
does not do any kind of question classification and it
does not use any NE recogniser, the results are sat-
isfactory.
5 Related Research
There have been other attempts to learn QA rules au-
tomatically. For example, Ravichandran and Hovy
(2002) learns rules based on simple surface patterns.
Given that surface patterns ignore much linguistic
information, it becomes necessary to gather a large
corpus of questions together with their answers and
sentences containing the answers. To obtain such
a corpus Ravichandran and Hovy (2002) mine the
Web to gather the relevant data.
Other methods learn patterns based on syntactic
information. For example, Shen et al (2005) de-
velop a method of extracting dependency paths con-
necting answers with words found in the question.
However we are not aware of any method that at-
tempts to learn patterns based on logical informa-
tion, other than our own.
There is recent interest on the use of graph
methods for Natural Language Processing, such
as document summarisation (Mihalcea, 2004) doc-
ument retrieval (Montes-y-Go?mez et al, 2000;
Mishne, 2004), and recognition of textual entailment
(Pazienza et al, 2005). The present very workshop
shows the current interest on the area. However,
we are not aware of any significant research about
the use of conceptual graphs (or any other form of
graph representation) for question answering other
than our own.
6 Conclusions
We have presented a method to learn question an-
swering rules by applying graph manipulation meth-
ods on the representations of questions and answer
sentences. The method is independent of the actual
graph representation formalism.
We are studying to combine WordNet with a
Named Entity Recogniser to produce generalised
rules. This way it becomes possible to replace ver-
tices with vertex types (e.g. ?PERSON?, ?DATE?,
etc). We are also exploring the use of machine learn-
ing techniques to learn classes of vertices. In par-
ticular, grammar induction techniques (van Zaanen,
2002) could be applied to learn types of regularities
in the strings.
43
Further research will also focus on developing
methods to extend the question pattern Rp with in-
formation found in the question only. A possibility
is to keep a database of question subgraphs that are
allowed to be added to Rp. This database could be
built by hand, but ideally it should be learnt auto-
matically.
Additional research efforts will be allocated to de-
termine degrees of word similarity or paraphrasing,
such as the connection between was born in and ?s
birthplace is. In particular, we will explore the use
of nominalisations. We will also study paraphrasing
methods to detect these connections.
Considering that text information as complex as
syntactic information or even logic and semantic
information can be expressed in graphs (Quillian,
1968; Schank, 1972; Sowa, 1979), we are convinced
that the time is ripe to explore the use of graphs for
question answering.
References
H. Bunke, P. Foggia, C. Guidobaldi, C. Sansone, and
M. Vento. 2002. A comparison of algorithms for
maximum common subgraph on randomly connected
graphs. In Lecture Notes on Computer Science, vol-
ume 2396, pages 123?132. Springer-Verlag, Heidel-
berg.
Noriko Kando. 2005. Overview of the fifth NTCIR
workshop. In Proceedings NTCIR 2005.
Rada Mihalcea. 2004. Graph-based ranking algorithms
for sentence extraction applied to text summarization.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, companion
volume (ACL 2004).
Gilad Mishne. 2004. Source code retrieval using concep-
tual graphs. Master?s thesis, University of Amsterdam.
Diego Molla? and Menno van Zaanen. 2005. Learning
of graph rules for question answering. In Proc. ALTW
2005, Sydney.
Diego Molla? and Menno van Zaanen. 2006. An-
swerfinder at TREC 2005. In Proceedings TREC
2005. NIST.
Manuel Montes-y-Go?mez, Aurelio Lo?pez-Lo?pez, and
Alexander Gelbukh. 2000. Information retrieval with
conceptual graph matching. In Proc. DEXA-2000,
number 1873 in Lecture Notes in Computer Science,
pages 312?321. Springer-Verlag.
Sung H. Myaeng and Aurelio Lo?pez-Lo?pez. 1992. Con-
ceptual graph matching: a flexible algorithm and ex-
periments. Journal of Experimentation and Theoreti-
cal Artificial Intelligence, 4:107?126.
Maria Teresa Pazienza, Marco Pennacchiotti, and
Fabio Massimo Zanzotto. 2005. Textual entailment
as syntactic graph distance: a rule based and a SVM
based approach. In Proceedings PASCAL RTE chal-
lenge 2005.
Ross Quillian. 1968. Semantic memory. In Semantic
Information Processing, pages 216?270. MIT Press.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proc. ACL2002.
Roger C. Schank. 1972. Conceptual dependency: A the-
ory of natural language understanding. Cognitive Psy-
chology, 3(4):532?631.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Robert Dale, Kam-Fai Wong, Jian
Su, and Oi Yee Kwong, editors, Natural Language
Processing IJCNLP 2005: Second International Joint
Conference, Jeju Island, Korea, October 11-13, 2005.
Proceedings. Springer-Verlag.
M. M. Soubbotin. 2001. Patterns of potential answer
expression as clues to the right answers. In Voorhees
and Harman (Voorhees and Harman, 2001).
John F. Sowa. 1979. Semantics of conceptual graphs. In
Proc. ACL 1979, pages 39?44.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Proc. ANLP-97.
ACL.
Alessandro Vallin, Bernardo Magnini, Danilo Giampic-
colo, Lili Aunimo, Christelle Ayache, Petya Osenova,
Anselmo Pe nas, Maarten de Rijke, Bogdan Sacaleanu,
Diana Santos, and Richard Sutcliffe. 2005. Overview
of the CLEF 2005 multilingual question answering
track. In Proceedings CLEF 2005. Working note.
Menno van Zaanen. 2002. Bootstrapping Structure into
Language: Alignment-Based Learning. Ph.D. thesis,
University of Leeds, Leeds, UK.
Ellen M. Voorhees and Donna K. Harman, editors. 2001.
The Tenth Text REtrieval Conference (TREC 2001),
number 500-250 in NIST Special Publication. NIST.
Ellen M. Voorhees. 2001a. Overview of the TREC 2001
question answering track. In Voorhees and Harman
(Voorhees and Harman, 2001).
Ellen M. Voorhees. 2001b. The TREC question answer-
ing track. Natural Language Engineering, 7(4):361?
378.
44
Answer Extraction 
Towards better Evaluations of NLP  Systems 
Ro l f  Schwi t te r  and D iego  Mo l l~  and Rache l  Fourn ie r  and Michae lHess  
Depar tment  of Informat ion Technology 
Computat iona l  Linguistics Group 
University of Zurich 
CH-8057 Zurich 
\[schwitter, molla, fournier, hess\] @ifi. unizh, ch 
Abst rac t  
We argue that reading comprehension tests are 
not particularly suited for the evaluation of 
NLP systems. Reading comprehension tests are 
specifically designed to evaluate human reading 
skills, and these require vast amounts of world 
knowledge and common-sense r asoning capa- 
bilities. Experience has shown that this kind of 
full-fledged question answering (QA) over texts 
from a wide range of domains is so difficult for 
machines as to be far beyond the present state 
of the art of NLP. To advance the field we pro- 
pose a much more modest evaluation set:up, viz. 
Answer Extraction (AE) over texts from highly 
restricted omains. AE aims at retrieving those 
sentences from documents that contain the ex- 
plicit answer to a user query. AE is less ambi- 
tious than full-fledged QA but has a number of 
important advantages over QA. It relies mainly 
on linguistic knowledge and needs only a very 
limited amount of world knowledge and few in- 
ference rules. However, it requires the solution 
of a number of key linguistic problems. This 
makes AE a suitable task to advance NLP tech- 
niques in a measurable way. Finally, there is a 
real demand for working AE systems in techni: 
cal domains. We outline how evaluation proce- 
dures for AE systems over real world domains 
might look like and discuss their feasibility. 
1 On  the  Des ign  o f  Eva luat ion  
Methods  for  NLP  Systems 
The idea that the systematic and principled 
evaluation of document processing systems is 
crucial for the development of the field as a 
whole has gained wide acceptance in the com- 
munity during the last decade. In a num- 
ber of large-scale projects (among them TREC 
(Voorhees and Harman, 1998) and MUC (MUC- 
7, 1998)), evaluation procedures for specific 
types of systems have been used extensively, and 
refined over the years. Three things were com- 
mon to these evaluations: First, the systems to 
be evaluated were each very closely tied to a par- 
ticular task (document retrieval and information 
extraction, respectively). Second, the evalua- 
tion was of the black box type, i.e. it considered 
only system input-output relations without re- 
gard to the specific mechanisms by which the 
outputs were obtained. Third, the amount of 
data to be processed was enormous (several gi- 
gabytes for TREC). 
There is general agreement that these com- 
petitive evaluations had a striking and bene- 
ficial effect on the performance of the various 
systems tested over the years. However, it is 
also recognized (albeit less generally) that these 
evaluation experiments also had the, less ben- 
eficial, effect that the participating systems fo- 
cussed increasingly more narrowly on those few 
parameters that were measured in the evalua- 
tion, to the detriment of more general prop- 
erties. In some cases this meant that power- 
ful and linguistically interesting but slow sys- 
tems were dropped in favour of shallow but fast 
systems with precious little linguistic content. 
Thus the system with which SRI participated 
in the MUC-3 evaluation in 1991, TACITUS 
(Hobbs et al, 1991), a true text-understanding 
system, was later replaced by FASTUS (Appelt 
et al, 1995; Hobbs et al, 1996), a much sim- 
pler, and vastly faster, information extraction 
system. The reason was that TACITUS was 
spending so much of its time attempting to make 
sense of portions of the text that were irrelevant 
to the task that recall was mediocre. We ar- 
gue that the set-up of these competitive valu- 
ations, and in particular the three parameters 
mentioned above, drove the development of the 
participating systems towards becoming impres- 
20 
sive feats of engineering, fine-tuned to one very 
specific task, but with limited relevance outside 
this task and with little linguistically relevant 
content. We argue that these evaluations there- 
fore did not drive progress in Computational 
Linguistics very much. 
We therefore think it a timely idea to con- 
ceive of evaluation methodologies which mea- 
sure the linguistically relevant functions of NLP 
systems and thus advance Computational Lin- 
guistics as a science rather than as an engineer- 
ing discipline. The suggestion made by the or- 
ganizers of this workshop on how this could be 
achieved has-four comPonents. First, they sug- 
gest to use full-fledged text-based question an- 
swering (QA) as task. Second, they suggest a 
relatively small amount off text (compared with 
the volumes of text used in TREC) as test data. 
Third they (seem to) suggest o .use texts from 
a wide range off domains. Finally they suggest 
to use pre-existing question/answer pairs, de- 
veloped for and tested on humans, as evaluation 
benchmark (Hirschman et al, 1999). 
However, our experience in the field leads us 
to believe that this evaluation set-up will not 
help Computational Linguistics as much as it 
would be needed, mainly because it is way too 
ambitious. We fear that this fact will force de- 
velopers, again, to design all kinds of ad-hoc so- 
lutions and efficiency hacks which will severely 
limit the scientific relevance of the resulting sys- 
tems. We argue that three of the four compo- 
nents of the suggested set-up must be reduced 
considerably in scope to make the test-bed help- 
ful. 
First, we think the task is too difficult. Full- 
fledged QA on the basis of natural language 
texts is far beyond the present state of the 
art. The example of the text-based QA sys- 
tem LILOG (Herzog and Rollinger, 1991) has 
shown that the analysis of texts to the depth 
required for real QA over their contents is so re- 
source intensive as to be unaffordable in any real 
world context.  After an investment of around 65 
person-years of work the LILOG system could 
answer questions over a few (reputedly merely 
three) texts of around one page length each from 
an extremely narrow domain (city guides and 
the like). We think it is fair to say that the situ- 
ation in our field has not changed enough in the 
meantime to invalidate this finding. 
Second, we agree that the volume off data to 
be used should be relatively small. We must 
avoid that the sheer pressure of the volumes of 
texts to be processed forces system developers 
to use shallow methods. 
Third, we think it is very important o restrict 
the domain of the task. We certainly do not ar- 
gue in favour of some toy domain but we get 
the impression that the reading comprehension 
texts under consideration cover a far too wide 
range of topics. We think that technical man- 
uals are a better choice. They cover a narrow 
domain (such as computer operating systems, 
or airplanes), and they also use a relatively re- 
stricted type of language with a reasonably clear 
semantic foundation. 
Fourth, we think that tests that are specif- 
ically designed to evaluate to what extent a 
human being understands a text are intrinsi- 
cally unsuitable for our present purposes. Al- 
though it would admittedly be very convenient 
to have "well written" texts, "good" questions 
about them and the "correct" answers all in one 
package, the texts are not "real world" language 
(in that they were written specifically for these 
tests), and the questions are:just far too difficult, 
primarily because they rely on exactly those 
components of language understanding where 
humans excel and computers are abominably 
poor (inferences over world knowledge). 
In Section 2 we outline what kinds of prob- 
lems would have to be solved by a QA sys- 
tem if it were to answer the test questions 
given in (WRC, 2000). Most of the prob- 
lems would require enormous amounts of world 
knowledge and vast numbers of lexical inference 
rules for a solution, on top of all the "classi- 
cal" linguistic problems our field has been strug- 
gling with (ambiguities, anaphoric references, 
synonymy/hyponymy).  We will then argue in 
Section 3 that a more restricted kind of task, 
Answer Extraction, is better suited as experi- 
mental set-up as it would focus our forces on 
these unsolved but reasonably well-understood 
problems, rather than divert them to the ill- 
understood and fathomless black' hole of world 
knowledge. In Section 4, we will finally outline 
how evaluation procedures in this context might 
look like. 
21 
...k 
2 Why Read ing  Comprehens ion  
Tests  v ia QA are  Too :Difficult 
Reading comprehension tests are designed to 
measure how well human readers understand 
what they read. Each story comes with a set 
of questions about information that is stated 
or implied in the text. The readers demon- 
strate their understanding of the story by an- 
swering the questions about it. Thus, read- 
ing comprehension tests assume a cognitive pro- 
cess of human beings. This process involves ex- 
panding the mental model of a text by using 
its implications and presuppositions, retrieving 
the stored information, performing inferences to 
make implicit information explicit, and generat- 
ing the surface strings that express this infor- 
mation. Many different forms of knowledge take 
part in this process: linguistic, procedural and 
world knowledge. All these forms coalesce in 
the memory of the reader and it is very difficult 
to clearly distinguish and reconstruct them in a 
QA system. At first sight the story published in 
(WRC, 2000) is easy to understand because the 
sentences are short and cohesive. But it turns 
out that a classic QA system would need vast 
amounts of knowledge and inference rules in or- 
der to understand the text and to give sensible 
answers. 
Let us investigate what kind of information 
a full-fledged QA system needs in order to an- 
swer the questions that come with the reading 
comprehension test (Figure 1) and discuss how 
difficult it is to provide this information. 
To answer the first question 
(1) Who collects maple sap? 
the system needs to know that the mass noun 
sap in the text sentence 
Farmers collect the sap. 
is indeed the maple sap mentioned in the 
question. The compound noun maple sap is a se- 
mantically narrower term than the noun sap and 
encodes an implicit relation between the first el- 
ement maple and the head noun sap. This rela- 
tion names the origin of the material. Since no 
explicit information about the relation between 
the two objects is available in the text an ideal 
QA system would have to assume such a relation 
by a form of abductive reasoning. 
How.Maple  Syrup is Made 
Maple syrup comes from sugar maple trees. At 
one time, maple syrup was used to make sugar. 
This is why the tree is called a "sugar" maple 
tree. 
Sugar maple trees make sap. Farmers collect he 
sap. The best time to collect sap is in February 
land March. The nights must be cold and the 
days warm. 
The framer drills a few small holes in each tree. 
He puts a spout in each hole. Then he hangs 
a bucket on the end of each spout. The bucket 
has a cover to keep rain and snow out. The sap 
drips into the bucket. About 10 gallons of sap 
come from each hole. 
1. Who collects maple sap? 
(Farmers) 
2. What does the farmer hang from a spout? 
(A bucket) 
3. When is sap collected? 
(February and March) 
4. Where does the maple sap come from? 
(Sugar maple trees) 
5. Why is the bucket covered? 
(to keep rain and snow out) 
Figure 1: Reading comprehension test 
To answer the second question 
(2) What does the farmer hang from a spout? 
successfully the system would need at least 
three different kinds of knowledge: 
First, it would need discourse knowledge to 
resolve the intersentential co-reference between 
the anaphor he and the antecedent the farmer 
in the following text sequence: 
The farmer drills- a few small holes in each 
tree. \[...\] Then he hangs a bucket ... 
Although locating antecedents has proved to 
be one of the hard problems of natural lan- 
guage processing, the anaphoric reference reso- 
lution can be done easily in this case because the 
antecedent is the most recent preceding noun 
phrase thgt agrees in gender, number and per- 
son. 
22 
Second, the system would require linguistic 
knowledge to deal with the synonymy relation 
between hang on and hang .from, and the at- 
tachment ambiguity of the prepositional phrase 
used in the text sentence and the query. 
Third, the system needs an inference rule that 
makes somehow clear that the noun phrase a 
spout expressed in the query is entailed in the 
more complex noun phrase the end of each spout 
in the text sentence. Additionally, to process 
this relation the system would require an infer- 
ence rule of the form: 
IF X does Y to EACH Z 
THEN X does Y to A Z. 
The third question 
(3) When is sap collected? 
asks for the time point when' ~ap is collected 
but the text gives only a rule-like recommenda- 
tion 
The best time to collect sap is in February 
and March. 
with an additional constraint 
The nights must be cold and the days warm. 
and does not say that the sap is in fact col- 
lected in February and March. The bridging 
inference that the system would need to model 
here is not founded on linguistic knowledge but 
on world knowledge. Solving this problem is 
very hard. It could be argued that default rules 
may solve such problems but it is not clear 
whether formal methods are able to handle the 
sort of default reasoning required for represent- 
ing common-sense reasoning. 
To give an answer for the fourth question 
(4) Where does the maple sap come .from? 
the system needs to know that maple sap 
comes from sugar maple trees. This informa- 
tion is not explicitly available in the text. In- 
stead of saying where maple sap comes from the 
text says where maple syrup comes from: 
Maple syrup comes .from sugar maple trees. 
23 
There exists a metonymy relation between 
these two compound nouns. The compound 
noun maple syrup (i.e. product) can only be 
substituted by maple sap (i.e. material), if the 
system is able to deal with metonymy. Together 
with the information in the sentence 
Sugar maple trees make sap. 
and an additional exical inference rule in 
form of a meaning postulate 
IF X makes Y THEN Y comes from X. 
the system could deduce (in theory) first sap 
and then by abductive reasoning assume that 
the sap found is maple sap. Meaning postulates 
are true by virtue of the meaning they link. Ob- 
servation cannot prove them false. 
To answer the fifth question 
(5) Why is the bucket covered? 
the system needs to know that the syntac- 
tically different expressions has a cover and is 
covered have the same propositional content. 
The system needs an explicit lexical inference 
rule in form of a conditional equivalence 
IF Conditions 
THEN X has a cover ~-> X is covered. 
that converts the verbal phrase with the nom- 
inal expression i to a the corresponding passive 
construction (and vice versa) taking the present 
context into consideration. 
As these concrete xamples how, the task of 
QA over this simple piece of text is frighten- 
ingly difficult. Finding the correct answers to 
the questions requires far more information that 
one would think at first. Apart from linguistic 
knowledge a vast amount of world knowledge 
and a number of bridging inferences are nec- 
essary to answer these seemingly simple ques- 
tions. For human beings bridging inferences 
are automatic and for the most part uncon- 
scious. The hard task consists in reconstructing 
all this information coming from different knowl- 
edge sources and modeling the suitable inference 
rules in a general way so that the system scales 
up. 
3 Answer  Ext ract ion  as an  
A l te rnat ive  Task  
An alternative to QA is answer extraction (AE). 
The general goal of AE is the same as that of 
QA, to find answers to user queries in textual 
documents. But the way to achieve this is differ- 
ent. Instead of generating the answer from the 
information given in the text (possibly in im- 
plicit form only), an AE system will retrieve the 
specific sentence(s) in the text that contain(s) 
the explicit answer to the query. In addition, 
those phrases in the sentence that represent the 
explicit a_nswer to the query may be highlighted. 
For example, let us assume that the following 
sentence is in the text (and we are going to use 
examples from a technical domain, that of the 
Unix user's manual): 
(1) cp copies the contents of filenamel onto 
filename2. 
If the user asks the query 
Which command copies files? 
a QA system will return: 
cp 
However, an AE system will return all the 
sentences in the text that directly answer the 
question, among them (1). 
Obviously, an AE system is far less power- 
ful than a real QA system. Information that 
is not explicit in a text will not be found, let 
alone information that must be derived from 
textual information together with world knowl- 
edge. But AE has a number of important ad- 
vantages over QA as a test paradigm. First, an 
obvious advantage of this approach is that the 
user receives first-hand information, right from 
the text, rather than system-generated replies. 
It is therefore much easier for the user to de- 
termine whether the result is reliable. Second, 
it is a realistic task (as the systems we are de- 
scribing below proves) as there is no need to 
generate natural language output, and there is 
less need to perform complex inferences because 
it merely looks up things in the texts which axe 
explicitly there. It need not use world knowl- 
edge. Third, it requires the solution of a num- 
ber of well-defined and truly important linguistic 
problems and is therefore well suited to measure, 
and advance, progress in these respects. We will 
come to this later. And finally, there is a real 
demand for working AE systems in technical do- 
mains since the standard IR approaches just do 
not work in a satisfactory manner in many appli- 
cations where the user is in pressure to quickly 
find a specific answer to a specific question, and 
not just (potentially long) lists of pointers to 
(potentially large) documents that may (or may 
not) be relevant o the query. Examples of ap- 
plications are on-line software help systems, in- 
terfaces to machine-readable technical manuals, 
help desk systems in large organizations, and 
public enquiry systems accessible over the Web. 
The basic procedure we use in our approach 
to AE is as follows: In an off-line stage, the 
documents are processed and the core mean- 
ing of each sentence is extracted and stored as 
so-called minimal logical forms. In an on-line 
stage, the user query is also processed to pro- 
duce a minimal ogical form. In order to retrieve 
answer sentences from the document collection, 
the minimal logical form of the query is proved, 
by a theorem prover, over the minimal logical 
forms of the entire document collection (Moll~t 
et al, 1998). Note that this method will not re- 
trieve patently wrong answer sentences like bkup 
files all copies on the hard disk in response to 
queries like Which command copies files? This 
is the kind of response we inevitably get if we 
use some variation of the bag-of-words approach 
adopted by IR based systems not performing 
any kind of content analysis. 
We are currently developing two AE sys- 
tems. The first, ExtrAns, uses deep linguis- 
tic analysis to perform AE over the Unix man- 
pages. The prototype of this system uses 500 
Unix manpages, and it can be tested over the 
Web \[http://www.ifi.unizh.ch/cl/extrans\]. In 
the second (new) project, WebExtrAns, we in- 
tend to perform AE-over the "Aircraft Main- 
tenance Manual" of the Airbus 320 (ADRES, 
1996). The larger volume of data (about 900 kg 
of printed paper!) will represent an opportunity 
to test the scalability of an AE system that uses 
deep linguistic analysis. 
There is a number of important areas of re- 
search that ExtrAns and WebExtrAns, and by 
extension any AE system, has to focus on. First 
of all, in order to generate the logical form of the 
24 
sentences, the following must be tackled: Find- 
ing the verb arguments, performing disambigua- 
tion, anaphora resolution, and coping with nom- 
inalizations, passives, ditransitives, compound 
nouns, synonymy, and hyponymy (Moll~t et al, 
1998; Mollh and Hess, 2000). Second, the very 
idea of producing the logical forms of real-world 
text requires the formalization of the logical 
form notation so that it is expressive nough but 
still remaining usable (Schwitter et al, 1999). 
Finally, the goal of producing a practical system 
for a real-world application eeds to address the 
issue of robustness and scalability (Moll~t and 
Hess, 1999).-- 
Note that the fact that AE and QA share the 
same goal makes it possible to start a project 
that initially performs AE, and gradually en- 
hance and extend it with inference and gener- 
ation modules, until we get a full-fledged QA 
system. This is the long-time g0al of our cur- 
rent series of projects on AE. 
4 Eva luat ing  the  Resu l ts  
Instead of using reading comprehension tests 
that are meant for humans, not machines, we 
should produce the specific tests that would 
evaluate the AE capability of machines. Here 
is our proposal. 
Concerning test queries, it is always better to 
use real world queries than queries that were ar- 
tificially constructed to match a portion of text. 
Experience has shown time and again that real 
people tend to come up with questions different 
from those the test designers could think of. By 
using, as we suggest, manuals of real world sys- 
tems, it is possible to tap the interaction of real 
users with this system as a source of real ques- 
tions (we do this by logging the questions ub- 
mitted to our system over the Web). Another 
way of finding queries is to consult he FAQ lists 
concerning a given system sometimes available 
on the Web. In both cases you will have to fil- 
ter out those queries that have no answers in the 
document collection or that are clearly beyond 
the scope of the system to evaluate (for exam- 
ple, if the inference needed to answer a query is 
too complex, even for a human judge). 
Concerning answers, the principal measures 
for the AE task must be recall and precision, 
applied to individual answer sentences. Recall 
is the number of correct answer sentences the 
system retrieved divided by the total number 
of correct answers in the entire document col- 
lection. Precis ion is the number of correct an- 
swer sentences the system retrieved ivided by 
the total number of answers it returned. As is 
known all too well, recall is nearly impossible to 
determine in an exact fashion for all but toy ap- 
plications ince the totality of correct answers in 
the entire document collection has to be found 
mainly by hand. Almost certainly one will have 
to resort to (hopefully) representative samples 
of documents to arrive at a reasonable approxi- 
mation to this value. Precision is easier to deter- 
mine although even this step can become very 
time consuming in real world applications. 
If, on the other hand, one only needs to do 
an approximate evaluation of the AE system, it 
would be possible to find a representative s t of 
correct answers by making a person write the 
ideal answers, and then automatically finding 
the sentences in the documents that are seman- 
tically close to these ideal answers. Semantic 
closeness between a sentence and the ideal an- 
swer can be computed by combining the suc- 
c inctness and correctness of the sentence with 
respect to the ideal answer. Succinctness and 
correctness are the counterparts ofprecision and 
recall, but on the sentence level. These mea- 
sures can be computed by checking the overlap 
of words between the sentence and the ideal an- 
swer (Hirschman et al, 1999), but we suggest a 
more content-based approach. 
Our proposal is to compare not words in a 
sentence, but their logical forms. Of course, this 
comparison can be done only if it is possible to 
agree on how logical forms should look like, to 
compute them, and to perform comparisons be- 
tween them. The second and third conditions 
can be fulfilled if the logical forms are simple 
lists of predicates that contain some minimal se- 
mantic information, as it is the case in ExtrAns 
(Schwitter et al, 1999). In this paper we will 
use a simplification of the minimal ogical forms 
used by ExtrAns. Below are two sentences with 
their logical forms: 
(1) rm removes one or more files. 
remove(x ,y ) ,  rm(x) ,  f i le(y) 
(2) csplit pr ints  the character counts .for each 
file created, and removes any files it creates 
i f  an error occurs. 
25 
print(x,y), csplit(x), character-count(y), 
remove(x ,z ) ,  fi le(z), create(x,z), oc- 
cur(e), error(e) 
As an example of how to compute succinct- 
ness and correctness, take the following ques- 
tion: 
Which command removes files? 
The ideal answer is a full sentence that con- 
tains the information given by the question and 
the information requested. Since rm is the com- 
mand used to remove files, the ideal answer is: 
rm removes  f i les.  
remove(x,y), rm(x), file(y) 
Instead of computing the overlap of words, 
succinctness and correctness ofa sentence can be 
determined by computing the overlap of predi- 
cates. The overlap of the predicates (overlap 
henceforth) of two sentences is the maximum 
set of predicates that can be used as part of the 
logical form in both sentences. The predicates 
in boldface in the two examples above indicate 
the overlap with the ideal answer: 3 for (1), and 
2 for (2). 
Succinctness of a sentence with respect o an 
ideal answer (precision on the sentence level) is 
the ratio between the overlap and the total num- 
ber of predicates in the sentence. Succinctness 
is, therefore, 3/3=1 for (1), and 2/8=0.25 for 
(2). 
Correctness of a sentence with respect o an 
ideal answer (recall on the sentence level) is the 
ratio between the overlap and the number of 
predicates in the ideal answer. In the exam- 
ples above, correctness i 3/3=1 for (1), and 
2/3=0.66 for (2). 
A combined measure of succinctness and cor- 
rectness could be used to determine the seman- 
tic closeness of the sentences to the ideal an- 
swer. By establishing a threshold to the seman- 
tic closeness, one can find the sentences in the 
documents that are answers to the user's query. 
The advantage of using overlap of predicates 
against overlap of words is that the relations be- 
tween the words also affect the measure for suc- 
cinctness and correctness. We can see this in 
the following artificial example. Let us suppose 
that the ideal answer to a query is: 
Madrid defeated Barcelona. 
defeat(x,y), madrid(x), barcelona(y) 
The following candidate sentence produces 
the same predicates: 
Barcelona defeated Madrid. 
defeat(x,y), madr id (y ) ,  barce lona(x)  
However, at most two predicates only can be 
chosen at the same time (in boldface), because 
of the restrictions of the arguments.  In the 
ideal answer, the first argument of "defeat" is 
Madrid and the second argument is Barcelona. 
In the candidate sentence, however, the argu- 
ments are reversed (the name of the variables 
have no effect on this). The overlap is, therefore, 
2. Succinctness and correctness are 2/3=0.66 
and 2/3=0.66, respectively. 
5 Conc lus ion  
We are convinced that reading comprehension 
tests are too difficult for the current state of 
art in natural language processing. Our anal- 
ysis of the Maple Syrup story shows how much 
world knowledge and inference rules are needed 
to actually answer the test questions correctly. 
Therefore, we think that a more restricted kind 
of task that focuses rather on tractable problems 
than on AI-hard problems of question-answering 
(QA) is better suited to take our field a step 
further. Answer Extraction (AE) is an alter- 
native to QA that relies mainly O n linguistic 
knowledge. AE aims at retrieving those exact 
passages of a document hat directly answer a 
given user query. AE is less ambitious than full- 
fledged QA since the answers are not generated 
from a knowledge base but looked up in the doc- 
uments. These documents come from a well- 
defined (technical) domain and consist of a rela- 
tively small volume of data. Our test queries are 
real world queries that express a concrete infor- 
mation need. To evaluate our AE systems, we 
propose besides precision and recall two addi- 
tional measures: succinctness and correctness. 
They measure the quality of answer sentences 
on the sentence level and are computed on the 
basis of the overlap of logical predicates. 
To round out the picture, we address the ques- 
tions in (WRC, 2000) in the view of what we said 
in this paper: 
26 
Q: Can such exams \[reading comprehension 
tests\] be used to evaluate computer-based lan- 
guage understanding effectively and e~ciently? 
A: We think that no language unders tand-  
ing system will currently be able to answer a sig- 
nificant proportion of such questions, which will 
make evaluation results difficult at best, mean- 
ingless at worst. 
Q: Would they provide an impetus and test 
bed for interesting and useful research? 
A: We think that the impetus they might pro- 
vide would drive development in the wrong di- 
rection, viz. towards the creation of (possibly 
impressive) engineering feats without much lin- 
guistically interestingcontent. 
Q: Are they too hard for current technology? 
A: Definitely, and by a long shot. 
Q: Or are they too easy, such that simple 
hacks can score high, although there is clearly 
no understanding involved? ., 
A: "Simple hacks" would almost certainly 
score higher than linguistically interesting meth- 
ods but not because the task is too simple but 
because it is far too difficult. 
References 
ADRES, 1996. A319/A320/A321 Aircraft 
Maintenance Manual. Airbus Industrie, 
Blagnac Cedex, France. Rev. May 1. 
Douglas E. Appelt, Jerry R. Hobbs, John Bear, 
David Israel, Megumi Kameyama, Andy 
Kehler, David Martin, Karen Myers, and 
Mabry Tyson. 1995. SRI International FAS- 
TUS system MUC-6 test results and analysis. 
In Proc. Sixth Message Understanding Con- 
\]erence (MUC-6), Columbia, Maryland. 
Otthein Herzog and Claus-Rainer Rollinger, ed- 
itors. 1991. Text Understanding in LILOG: 
Integrating Computational Linguistics and 
Artificial Intelligence - final report on the 
IBM Germany LILOG project, volume 546 of 
Lecture Notes in Computer Science. Springer- 
Verlag, Berlin. 
Lynette Hirschman, Marc Light, Eric Breck, and 
John D. Burger. 1999. Deep Read: A read- 
ing comprehension system. In Proc. A CL '99. 
University of Maryland. 
Jerry Hobbs, Douglas E. Appelt, John S. Bear, 
Mabry Tyson, and David Magerman. 1991. 
The TACITUS system: The MUC-3 experi- 
ence. Technical report, AI Center, SRI Inter- 
national, Menlo Park, CA. 
Jerry R. Hobbs, Douglas E. Appelt, John Bear, 
David Israel, Megumi Kameyama, Mark 
Stickel, and Mabry Tyson. 1996. FASTUS: 
A cascaded finite-state transducer for extract- 
ing information from natural-language t xt. 
In E. Roche and Y. Schabes, editors, Finite 
State Devices for Natural Language Process- 
ing. MIT Press, Cambridge, MA. 
Diego Moll~ and Michael Hess. 1999. On 
the scalability of the answer extraction sys- 
tem "ExtrAns". In Proc. Applications of 
Natural Language to Information Systems 
(NLDB'99), pages 219-224, Klagenfurt, Aus- 
tria. 
Diego Moll~ and Michael Hess. 2000. Deal- 
ing with ambiguities in an answer extrac- 
tion system. In Representation and Treatment 
of Syntactic Ambiguity in Natural Language 
Processing, Paris. ATALA. 
Diego Moll~, Jawad Berri, and Michael Hess. 
1998. A real world implementation f answer 
extraction. In Proc. of the 9th International 
Conference and Workshop on Database and 
Expert Systems. Workshop "Natural Language 
and Information Systems" (NLIS'98), pages 
143-148, Vienna, August. 
MUC-7. 1998. Proc. of the seventh mes- 
sage understanding conference (MUC-7). 
http://www.muc.saic.com. 
Rolf Schwitter, Diego Moll~, and Michael Hess. 
1999. Extrans - -  answer extraction from 
technical documents by minimal ogical forms 
and selective highlighting. In Proc. Third In- 
ternational Tbilisi Symposium on Language, 
Logic and Computation, Batumi, Georgia. 
http://www.ifi.unizh.ch/cl/. 
Ellen M. Voorhees and Donna Harman. 1998. 
Overview of the seventh Text REtrieval Con- 
ference (TREC-7). In Ellen M. Voorhees and 
Donna Harman, editors, The Seventh Text 
REtrieval Conference (TREC-7), number 
500-242 in NIST Special Publication, pages 1- 
24. NIST-DARPA, Government Printing Of- 
rice. 
WRC. 2000. Workshop on reading compre- 
hension texts as evaluation for computer- 
based language understanding systems. 
http://www.gte.com/AboutGTE/gto/anlp- 
naacl2000/comprehension.html. 
27 
Special Section on Restricted-Domain
Question Answering
Question Answering in Restricted Domains:
An Overview
Diego Molla??
Macquarie University, Australia
Jose? Luis Vicedo?
University of Alicante, Spain
Automated question answering has been a topic of research and development since the earliest AI
applications. Computing power has increased since the first such systems were developed, and
the general methodology has changed from the use of hand-encoded knowledge bases about simple
domains to the use of text collections as the main knowledge source over more complex domains.
Still, many research issues remain. The focus of this article is on the use of restricted domains
for automated question answering. The article contains a historical perspective on question
answering over restricted domains and an overview of the current methods and applications
used in restricted domains. A main characteristic of question answering in restricted domains is
the integration of domain-specific information that is either developed for question answering or
that has been developed for other purposes. We explore the main methods developed to leverage
this domain-specific information.
1. Introduction
There has been an interest in representing knowledge and automatically processing
it from the time of the first generation of computers. This interest has increased from
the end of the 1980s to become an urgent necessity. Decisive factors in this increase of
interest are an unprecedented growth in the amount of digital information available, an
explosion of growth in the use of computers for communications, and the increasing
number of users that have access to all this information.
These circumstances have fostered research into information systems that can facil-
itate the localization, retrieval, and manipulation of these enormous quantities of data.
Question Answering (QA) is one of these research fields.
In this article, QA is defined as the task whereby an automated machine (such as
a computer) answers arbitrary questions formulated in natural language. QA systems
are especially useful in situations in which a user needs to know a very specific piece of
information and does not have the time?or just does not want?to read all the available
documentation related to the search topic in order to solve the problem at hand.
? Division of Information and Communication Sciences, Macquarie University, New South Wales 2109,
Australia. E-mail: diego@ics.mq.edu.au.
? Departamento de Lenguajes y Sistemas Informa?ticos, Universidad de Alicante, Campus de San Vicente
del Raspeig, Apdo. 99. Alicante, Spain. E-mail: vicedo@dlsi.ua.es.
Submission received: 2 June 2006; revised submission received: 15 October 2006; accepted for publication:
23 October 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
Research in QA has been developed from two different scientific perspectives,
artificial intelligence (AI) and information retrieval (IR).
Work in QA since the early stages of AI has led to systems that respond to questions
using the knowledge encoded in databases as an information source. Obviously, these
systems can only provide answers concerning the information previously encoded in
the database. The benefit of this approach is that having a conceptual model of the
application domain represented in the database structure allows the use of advanced
techniques such as theorem proving and deep reasoning in order to address complex
information needs.
Currently we are witnessing a surge of activity in the area from the perspective of
IR, initiated by the Question Answering track of TREC1 in 1999 (Voorhees 2001). Since
then, increasingly powerful systems have participated in TREC and other evaluation
fora such as CLEF2 (Vallin et al 2005) and NTCIR3 (Kando 2005). From this perspective,
question answering focuses on finding text excerpts that contain the answer within
large collections of documents. The tasks set in these conferences have molded a specific
kind of question answering that is easy to evaluate and that focuses on the use of fast
and shallow methods that are generally independent of the application domain. In
other words, current research focuses on text-based, open-domain question answering.
Both trends have developed in parallel and represent the opposite ends of a spec-
trum connecting what we might label as structured knowledge-based and free text-
based question answering. Whereas structured knowledge-based QA systems are well
adapted to applications managing complex queries in a very structured information
environment, the kind of research developed in TREC, CLEF, and NTCIR is probably
better suited to broad-purpose generic applications dealing with simple factual ques-
tions such as World Wide Web?based question answering.
However, both approaches have serious disadvantages when they attempt to tackle
important real applications that handle complex questions by combining domain-
specific information typically expressed in different sources (structured, semistructured,
unstructured, etc.) using reasoning techniques. Examples of such applications are:
Interfaces to machine-readable technical manuals: Many software applications are
very complex and they are accompanied by extensive documentation. A QA sys-
tem that finds specific answers to a user?s question based on such documentation
would be very useful.
Front-ends to knowledge sources: Many disciplines and areas of human activity have
their own specific knowledge sources. An example is the medical domain, which,
as we shall see in this article, contains a wealth of technical information and
resources that can be used for a QA system targeting this kind of information.
Help desk systems in large organizations: Help desk staff in large organizations need
to quickly satisfy the customer?s need for information. Although many such
requests for information will be found in FAQs available to the help desk staff,
there will always be requests that are unique and that require staff to have access
to fast methods to find the relevant information. End systems tailored to such staff
(who can be trained) are different from QA systems designed for the end user, but
they still need to leverage the organization domain.
1 Text REtrieval Conference (http://trec.nist.gov/).
2 Cross Language Evaluation Forum (http://clef.iei.pi.cnr.it/).
3 NII-NACSIS Test Collection for IR Systems (http://research.nii.ac.jp/ntcir/).
42
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
It might be argued that focusing research on restricted domains is limiting because
the results are too specific and not open to generalization. This may have been the case
with early work in natural language processing (NLP), which focused on restricted
domains simply because of limitations in computing power and in theoretical cov-
erage. This is not the case nowadays. The availability of comprehensive and reliable
resources in complex domains enables interesting and fruitful research to be carried out
in restricted-domain natural language processing.
In short, research in restricted-domain question answering (RDQA) addresses
problems related to the incorporation of domain-specific information into current
state-of-the-art QA technology with the hope of achieving deep reasoning capabili-
ties and reliable accuracy performance in real world applications. In fact, as a not-
too-long-term vision, we are convinced that research in restricted domains will drive
the convergence between structured knowledge-based and free text-based question
answering.
In this article we survey past and current work on question answering in restricted
domains. In the process we will highlight the advantages of developing systems based
on restricted domains. Section 2 provides a historical note on question answering,
with an emphasis on restricted domains, and focusing mainly on early work. Sec-
tion 3 presents desirable characteristics of restricted domains for the development of
NLP research in general, and question answering in particular. Section 4 comments
on some of the main factors that distinguish question answering in an open domain
from question answering in a restricted domain. Section 5 focuses on the use of
domain-specific resources for question answering. Section 6 outlines current restricted-
domain question answering methods. Section 7 notes the main aspects to consider
when building a restricted-domain question answering system. Section 8 introduces
the articles in this special section of the journal, and finally Section 9 presents some
conclusions.
2. Early Work
Two examples of early question-answering systems are BASEBALL and LUNAR. BASE-
BALL answered questions about baseball games played in the American league over
one season (Green et al 1961), and LUNAR answered questions about the analysis
of rock samples from the Apollo moon missions (Woods 1997). Both systems were
very successful in their chosen domains. In particular, LUNAR was demonstrated at
a lunar science convention in 1971, where it was able to answer 90% of questions
posed by geologists without prior instructions with regard to the allowable phrasing
(Hirschman and Gaizauskas 2001). Both LUNAR and BASEBALL are examples of what
have been described as natural language interfaces to databases, that is, their source
of information was a database that contained the relevant information about the topic.
The user?s question was converted into a database query, and the database output was
given as the answer. The very specific nature of the domains enabled the construction of
appropriately comprehensive databases, and a domain-specific question analysis that
enabled a mapping from the meaning of the user?s question onto the corresponding
database query.
LUNAR and BASEBALL are only two of the most salient examples of early work on
question answering, but there were many other such systems. Simmons?s (1965) survey
described a variety of early QA systems. Most of these focused on restricted domains by
developing a database of knowledge and providing a natural language interface. Still,
43
Computational Linguistics Volume 33, Number 1
many of these early systems (including LUNAR and BASEBALL) were no more than
?toy systems? that focused on very limited domains. Those early systems that used a
corpus of text as the inherent information source typically processed small volumes of
text and would rely on a human to disambiguate the corpus sentences or convert them
to a simplified version of English.
During the 1970s and 1980s there was intensive research on the development of the-
oretical bases for computational linguistics. This research prompted the development of
QA systems on domains that were more complex than those of the earlier systems. The
main goal of this research was to use QA as an application framework within which
general NLP theories could be tested. This work culminated in large and ambitious
projects such as the Berkeley Unix Consultant (Wilensky et al 1994).
The Berkeley Unix Consultant project (UC) used the domain of the UNIX operat-
ing system to develop a help system that combined research in planning, reasoning,
natural language processing, and knowledge representation. The user?s question was
analyzed and a meaning representation corresponding to the question was encoded in
a knowledge representation formalism. Then, UC hypothesized the actual information
needs of the user by consulting the user model and applying goal analysis. The answer
was tailored to the user?s expertise and goals. The sample dialogues provided were
certainly impressive. However, no transcripts of real-world dialogues were provided
and therefore it cannot be determined whether the methods and theories developed in
UC were robust enough for practical use.
Most of the early work attempted to implement QA systems from the early per-
spective of AI or computational linguistics. As noted earlier, due to the limitations
of the time, question answering focused on restricted domains. A turning point was
reached in 1999, with the introduction of the QA track in the TREC (Voorhees 1999). The
popularity of the QA track in TREC has enabled research on QA from an IR perspective.
From the IR community?s point of view, the task of question answering is reduced
to the task of finding the text that contains the answer to the question and extracting
the answer. Text documents are viewed as a source of unstructured information that
is structured by indexing it. Indexing the documents makes it feasible to locate the
fragments that are closely related to the question terms by applying term-matching
techniques.
A consequence of this new perspective is the application of domain-independent
methods, allowing what has been called open-domain question-answering. This ap-
proach is largely used in current QA systems. It is beyond the scope of this article to
survey the techniques and systems used in open-domain QA; the interested reader is
referred to the proceedings of TREC, which are available on-line.4 Instead, in the sub-
sequent sections we will review current approaches to question answering in restricted
domains. But before that, let us analyze what a restricted domain is.
3. Characteristics of Restricted Domains
The nature of a particular restricted domain affects the kinds of questions asked and
answers that can be expected. Consequently, different restricted domains benefit from
different QA techniques. Some domains are particularly appropriate for the develop-
ment of question answering systems. Minock (2005) lists three desiderata for a restricted
4 http://trec.nist.gov.
44
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
domain within the context of World Wide Web?based question answering?that is,
question answering that relies on documents taken from the World Wide Web as the
main source for finding answers. According to Minock, a restricted domain must meet
the following desiderata:
1. It should be circumscribed.
2. It should be complex.
3. It should be practical.
The same desiderata apply, with some modifications, to restricted domains on question
answering that is not World Wide Web?based.
3.1 Circumscription
Minock?s original description of a circumscribed domain is motivated by the user?s need
to know what to expect of the World Wide Web?based QA system at hand and to know
what questions are appropriate to the domain at hand. An example of a domain that
would fare low in this desideratum is that of current events, because the user might
have difficulty in ascertaining what questions can be asked. An example of a good
domain according to this desideratum would be a science domain such as astronomy or
chemistry.
If the QA system is not World Wide Web?based and, furthermore, is intended for
use within a corporation, however, users do not face the problem of wondering what
questions are appropriate. Rather, a more important motivation for a circumscribed
domain is the need for clearly defined knowledge sources. The range of techniques used
in a restricted domain should not need to use extensive knowledge from outside the
chosen domain. Rather, a domain that has authoritative and comprehensive resources is
to be preferred. Examples of resources include actual databases containing the required
information.
It is natural to assume that the more restricted the domain is and the more circum-
scribed it is, the more possible it is to obtain such comprehensive databases. For more
complex domains, useful resources are terminology databases and domain ontologies.
An added value is the existence of well-accepted terminology and ontology standards.
3.2 Complexity
A domain should be complex enough to warrant the use of a QA system. This may
seem an obvious statement, but it is important to bear in mind that, in a desire to find
a domain that is fully circumscribed, one might attempt to develop a QA system in a
domain where a simple list of facts or a FAQ would be sufficient to satisfy the user?s
need for information. There is no need for a QA system in such domains.
Developing a system for a simple domain does not advance research in any signif-
icant area. Such domains are to be left to those who are more interested in capitalizing
on current research advances to develop practical applications, rather than extending
current research boundaries. In general, the more complex a domain is, the more inter-
esting it becomes for the researcher and the more useful it presumably is to the user.
There is a balance to be achieved between the need for a complex domain and
that of a circumscribed domain, because these two desiderata are in conflict. At some
point, if a domain is complex enough, it becomes difficult to manage and there is a
45
Computational Linguistics Volume 33, Number 1
higher probability of requiring resources belonging to other domains; in other words,
the domain becomes less circumscribed.
3.3 Practicality
Practicality is an important desideratum to consider when developing a QA system.
The domain should be of use to a relatively large group of people. Otherwise one risks
wasting effort on a system that nobody would use, such as for an artificially constructed
toy domain. The choice of domain affects the kind of users to target. Therefore, for each
domain it is important to determine the kinds of questions asked in the specific domain
(question style and terminology used are two important factors to consider), the sort of
information that is most commonly requested, and the level of detail expected in the
answers.
4. Open-Domain versus Restricted-Domain Question Answering
There are various factors that determine the best techniques to use in restricted-domain
question answering, and whether techniques used in open-domain question answering
would be effective in restricted-domain question answering. In this section we will
briefly introduce some of these factors.
4.1 The Size of the Data
A well-known method used in open-domain QA is derived from redundancy-based
techniques. These techniques were first discussed by Brill et al (2001), who observed
that, as the size of the text corpus increases, it becomes more likely that the answer
to a specific question can be found with data-intensive methods that do not require a
complex language model. Thus, if the question is Who killed Abraham Lincoln?, it is easier
to find the answer in John Wilkes Booth killed Abraham Lincoln than in John Wilkes Booth
is perhaps America?s most infamous assassin. He is best known for having fired the bullet that
ended Abraham Lincoln?s life. Redundancy-based techniques are likely to have a weaker
impact in restricted-domain QA, especially in the case of domains with relatively small
corpora.
Domains with relatively small corpora will naturally have relatively fewer sen-
tences that contain the answer. In those domains it becomes important to use so-
phisticated language processing techniques, including the resolution of inferences, if
necessary, to find the answer. The haystack of a restricted domain is relatively small,
but it also has fewer needles.
Note that, if the size of the corpus is relatively small, it becomes possible to apply
complex NLP techniques to the complete corpus off-line. Nowadays it is possible to
parse the entire corpus used in the QA track of TREC and to extract all its named entities.
It is therefore feasible to parse and extract the named entities of corpora of restricted
domains.
4.2 Domain Context
The actual domain provides a specific context to the question-answering process. Con-
sequently the set of senses available to words is typically a subset of all the available
senses. The impact of word-sense disambiguation is possibly reduced in RDQA, though
46
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
it should be noted that some words would still have several senses available and
therefore word-sense disambiguation still plays a role. We are not aware of any studies
on the impact of word-sense disambiguation on restricted domains and certainly this
area is worth exploring.
The kinds of questions asked in a restricted domain are naturally different from
those asked in an open domain. Users of a restricted domain, and especially users who
are experts in the domain, will use specific terminology and will pose rather technical
questions that require very specific answers. Questions asked by such users are much
more complex than those of casual users of open-domain QA systems. This is certainly
the case in the medical domain, as the articles included in this special section show
convincingly. The challenges related to solving those questions are certainly worth the
effort in pursuing research in RDQA.
4.3 Resources
An important difference between open-domain and restricted-domain QA is the ex-
istence of specific resources for restricted domains that can be used. In the following
sections we will comment on these resources.
5. Use of Domain-Specific Resources
Intuitively, a good method for answering questions in a restricted domain needs to
leverage any information available about the domain in order to be able to address
users? information needs with the specificity and depth required.
The type of information available for a particular domain is intrinsically related
to the complexity of the domain and the particular needs of the domain users. Hence,
domain knowledge representation can range from simple lists of specialized entities
and terms to high-level ontologies where all the domain knowledge is unambiguously
represented.
Within computer science, an ontology is usually defined as a formal explicit de-
scription of concepts in the domain of discourse, together with their attributes, roles,
restrictions, and other defining features (Noy and McGuinness 2001). The relations
between the concepts are also expressed formally. The two most common relations
shown in an ontology are subclass (?is a subtype of?) and instance (?is an instance of?),
but other relations can be included, such as meronymy (?part of?) and, in the case of
WordNet (Fellbaum 1998), entailment.
For the purposes of this article, we will refer to all the possible domain knowledge
representations as ontological resources.
Generally, domains that are complex, circumscribed, and practical are likely to
have available ontological resources that can be used to quick-start QA research and
development. These resources are typically developed for the domain users to help
them categorize the domain knowledge and agree on notational standards, and to help
them retrieve information using conventional information retrieval applications.
5.1 Open-Domain Ontologies
There are ontologies that are designed without a specific domain in mind. These are
referred here as open-domain ontologies. A widely used open-domain ontology is
WordNet (Fellbaum 1998). WordNet contains a large list of open-class words grouped
47
Computational Linguistics Volume 33, Number 1
into synonym sets (the ?synsets?). A range of synset relations is encoded, such as hyper-
nymy/hyponymy, meronymy, and entailment. WordNet alo includes word relations,
such as antonymy. A departure from ontologies like Cyc (Lenat and Guha 1990) is that
WordNet does not include formal definitions of the features of the objects. Still, for the
purposes of this article, WordNet is an ontology. This view is supported by its use in
many systems, including open-domain question answering systems (Moldovan and
Novischi 2002).
Open-domain ontologies like WordNet, however, are of limited use for QA in re-
stricted domains. This is so because the information is unlikely to be well balanced with
respect to the chosen domain. For example, parts of open-domain ontologies are too
coarse-grained for specific restricted domains, whereas other parts are too fine-grained.
And worse, open-domain ontologies may contain information that is not appropriate
for specific restricted domains.
Open-domain ontologies are too coarse-grained. Restricted domains, and especially tech-
nical domains, abound in terms that are specific to the domain and largely unknown
in other domains. Open-domain ontologies typically do not include these specific
terms. In some domains, however, these terms may be used widely. Consequently,
open-domain ontologies will need to be complemented with terminology lists or local
ontologies.
Open-domain ontologies are too fine-grained. Open-domain ontologies that map words to
concepts, as is the case with WordNet, face the problem of polysemous words, that is,
words with multiple meanings. However, those ambiguous words are usually unam-
biguous in restricted domains. Take the noun file. WordNet 1.7.1 lists four meanings,
shown in Table 1. Of the four meanings, only the first one (?a set of related records kept
together?) is relevant within domains related to software development. Open-domain
ontologies therefore risk overloading the system with concepts that are rarely, if ever,
used within the chosen restricted domain.
Open-domain ontologies may have information that is not appropriate for the domain. The
most damaging property of open-domain ontologies is that they may contain informa-
tion that is misleading in certain restricted domains. Restricted domains notoriously
overload some terms commonly used outside their domain. For example, the usual
meaning of the verb print is to render something into printed matter. However, within
the domain of computer programming, the verb print usually means to display on
the computer monitor. Consequently, a system that uses an open-domain ontology
would possibly misinterpret the meaning of print in the question Which C++ instruc-
Table 1
Four senses of the noun file in WordNet 1.7.1.
Sense Gloss
1. A set of related records (either written or electronic) kept together
2. A line of persons or things ranged one behind the other
3. A container for keeping papers in order
4. A steel hand tool with small sharp teeth on some or all of its surfaces; used for
smoothing wood or metal
48
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
tion prints words onto the screen? This sense of print is not available in WordNet5 and
therefore it is not possible to apply word-sense disambiguation techniques to find the
appropriate sense.
5.2 Uses of Ontological Resources
Ontological resources define a common vocabulary for accessing information in a do-
main and this makes it easier to manage domain information as regards the following
(Noy and McGuinness 2001):
 sharing common understanding of the structure of information among
people or software agents
 enabling reuse of domain knowledge
 making domain assumptions explicit
 separating domain knowledge from the operational knowledge
 making possible different analysis of the domain knowledge
Among theses concerns, enabling the separation of domain knowledge and operational
knowledge is probably the most valuable characteristic for QA purposes. This fact al-
lows the separation of the process of representing the concepts expressed in a document
from the use of the relations between concepts for deduction or reasoning processes.
On the other hand, formalisms, theories, and algorithms either designed for domain
document representation or reasoning may be made independent from the chosen
domain ontology and can also be applied to different domains, thus enhancing system
portability between domains.
Research on using ontologies for QA has benefited from the following:
 The increasing availability of ontologies encoding different kinds of
knowledge. We can find ontologies ranging from general world
knowledge resources, such as WordNet (Fellbaum 1998), EuroWordNet
(Vossen 1998), Cyc (Lenat and Guha 1990), and FrameNet (Johnson and
Fillmore 2000, to very specific domain knowledge, such as the medical
domain (Lindberg, Humphreys, and McCray 1993) or the chemistry
domain (Barker et al 2004).
 Steady achievements in knowledge representation and reasoning (KR&R)
techniques, which enable precise representation of both domain-related
information and domain-related reasoning and deduction mechanisms
(Barker et al 2004).
 Advances in the development of modular and robust natural language
processing systems (Abney 1996; Hobbs et al 1997; Basili and Zanzotto
2002) in the context of the use of ontological resources for both textual
interpretation and representation (Ait-Mokhtar and Chanod 1997) and
database access (Popescu, Etzioni, and Kautz 2003).
5 This was the case for the on-line version of WordNet 2.1 (http://wordnet.princeton.edu/) on
8 October 2006.
49
Computational Linguistics Volume 33, Number 1
 Increasing success in the development of ontology-based QA frameworks
where answers are derived from reasoning processes over questions and
document ontological representations (Zajac 2001).
Ontology-based question answering systems attack the answer-retrieval problem
by means of an internal unambiguous knowledge representation. Both questions and
knowledge are represented using specific knowledge models based on ontological en-
tities, concepts, and relations. The answering of questions is performed by applying
different reasoning and proof techniques that allow the detection of textual entail-
ment, which is useful in determining whether a given sentence answers a particular
question.
6. The State of the Art in RDQA
Current work on QA in restricted domains tends to exploit the characteristics of the
domain in order to improve the accuracy and practicability of the system. This is
done largely by determining the types of information needs in the chosen domain, by
studying the format of questions asked, and by leveraging the ontological information
available in the domain.
Some domains are complex domains that have a history of users attempting to
streamline the process to find specific information. An example of such a domain is that
of medicine. It is important for a doctor to quickly diagnose the illness of a patient, and
to determine if a patient is developing a new variation of an illness that has occurred
before. Given the importance of finding the correct diagnosis and treatment, the domain
of medicine has developed trusted resources that can be used for question answering in
this domain. Zweigenbaum (2003) provides examples of resources for terminology and
corpora of authoritative material.
Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat-
ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele-
ments from medical abstracts obtained from MEDLINE. PICO structures are the frames
used for evidence-based medicine. Sang, Bouma, and de Rijke (2005a) describe several
strategies for populating a database with medical information related to diseases, symp-
toms, and treatments, which is automatically extracted from medical texts. This struc-
tured information is used for answering medical-related questions. Niu and Hirst (2004)
describe a method for identifying semantic classes and the relations between them in
medical texts. This approach is able to build an ontology for the domain automatically.
Yu, Sable, and Zhu (2005) present an algorithm to classify medical questions based
on a well-known hierarchical evidence taxonomy (Ely et al 2002). Rinaldi, Dowdall,
and Schneider (2004) describe the difficulties in adapting an existing RDQA system
developed for assisting questions on UNIX technical manuals (Molla? et al 2000) to the
Genomics domain.
Benamara (2004) reports in detail on one of the currently most advanced RDQA
systems. WEBCOOP is a logic-based system that integrates knowledge representation
and advanced reasoning procedures to generate responses to natural queries. This
system has been developed for the tourism domain.
As for any knowledge intensive application, using ontologies for QA has as a
limitation the restrictions imposed by the underlying knowledge representation models.
Thus, in the following subsections we will focus on the efforts that are being employed
from both historical trends in QA research (structured knowledge?based and free-text?
50
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
based perspectives) to provide systems with deep reasoning capabilities supported by
ontological domain information. We introduce several works that aim at combining the
use of various ontologies and we also describe current attempts to separate the process-
ing of domain-dependent information from generic domain-independent information
with the goal of increasing portability across domains.
6.1 Ontologies and Structured Knowledge?Based QA
As noted earlier, the first QA systems focused on the development of natural language
interfaces to databases (NLIDBs). This is a natural approach to follow in circumscribed
domains that are not very complex. The idea is to produce a structured information
resource containing comprehensive information on the contents of the domain. This
information resource is produced before any question is asked and is queried over when
the user asks a question.
There is a wealth of research in the area of NLIDBs and it is not within the scope
of this article to survey this important area of research. Rather, we refer the reader to
Androutsopoulos, Ritchie, and Thanisch (1995). Work in NLIDBs assumes an existing
database that is queried over. If the database does not exist, it is created by using meth-
ods based on information-extraction technology. The aim is to extract all the information
that might be used as an answer. A clear candidate is the use of named entities, but the
creation of templates has also been tried in open domains (Srihari and Li 2000) and
restricted domains (Weischedel, Xu, and Licuanan 2004).
There are other systems that support this kind of knowledge-based question-
answering, including some dealing with questions unanticipated at the time of system
construction. These include the AP Chemistry question-answering system (Barker et al
2004), Cyc (Lenat and Guha 1990), the Botany Knowledge Base system (Porter et al
1988), the two systems developed for DARPA?s High Performance Knowledge Base
(HPKB) project (Cohen et al 1988), and the two systems developed for DARPA?s Rapid
Knowledge Formation (RKF) project (Schrag et al 2002).
6.2 Ontologies and Free-Text?Based QA
In this approach, users pose questions in natural language to knowledge bases made
up of documents also written in natural language. In this case ontologies are used to
define a language in which questions and documents can be represented and exploited
to obtain the required answers. The translation from natural language to the internal
representation is automatic; this presupposes fully unambiguous representations that
are currently beyond our capabilities.
The main characteristic of these approaches is the intensive use of an ontology
in the different parts of the question answering system. For instance, the ontology is
used in the representation of the question and the documents, in the refinement of
the initial query, in the reasoning processes carried out over the classes and subclasses
from the ontology, and in the similarity algorithms employed for answer retrieval and
extraction.
Zajac (2001) presents an ontology-based semantic framework for question answer-
ing where both questions and source texts are parsed into underspecified semantic
expressions where names of the semantic atoms and predicates are defined in an in-
terlingual ontology. Answer retrieval is done using subsumption and unification, and
queries are expanded using ontological rules.
51
Computational Linguistics Volume 33, Number 1
6.3 Integrating Heterogeneous Sources of Information
More interesting than using a single database is the combination of databases with
semistructured information (such as text with some XML markup) or even unstruc-
tured information (i.e., plain text). This has been proposed for World Wide Web?based
question answering (Lin 2002), given the availability of pockets of information stored
in databases on the World Wide Web. The idea is to analyze the question and find the
relevant database among a preselected list if this is possible. If there are no suitable data-
bases or it is not possible to determine the appropriate database query, then standard
question-answering techniques are applied using the World Wide Web as a resource.
The same strategy can be applied to question answering over restricted domains by
keeping a set of relevant databases and a corpus of documents to query over in case the
question is not covered in the databases.
There are two main issues that need to be handled by a QA system that relies on
heterogeneous sources:
Interface: The resources in each domain will have their own formats and interfaces,
which must be unified by the QA system.
Selection: The QA system needs to determine the actual resource within which to look
for the answer.
Given that the actual domain-specific resources range from simple word lists to struc-
tured databases, interfacing to them is by no means simple. Two approaches are envis-
aged (Lin 2002):
Slurp: Extract all the information from the multiple sources and create a database
containing all the information. By having all the information in a unified database,
the interfacing problem is easily solved and it is even possible to handle queries
that the original databases were unable to handle (such as queries that rely on
knowledge from various domains). This method is practical if the actual databases
are available locally and their format is known. However, some databases are
available as on-line resources only and any attempt to slurp all their information
through methodical queries may be frowned upon by the database owners.
Wrap: Provide an application program interface (API) to the individual databases. The
set of databases can be seen as a federated database system. The choice of pro-
viding an API has the obvious disadvantage that it may not be possible to devise
a unified API that makes the best of what is available in the domain resources.
The compromise would be a set of APIs that may or may not be able to query the
resource with the full power of the original resource interface.
A step beyond portable QA systems is to build a meta-domain QA system. A meta-
domain QA system specializes in several restricted domains by acting as a knowledge
broker to specialized domain modules. An example of such a system is START (Katz
1997), which currently is a World Wide Web?based QA system that uses a wide range
of structured data available on the Internet.
MOSES (Basili et al 2004) is an ontology-based QA system in which users pose
questions in natural language to knowledge bases of facts extracted from a federation
of Web sites and organized in topic map repositories. This approach uses an ontology-
based methodology to search, create, maintain, and adapt semantically structured
World Wide Web content according to the vision of the Semantic Web in a domain
related to university World Wide Web sites.
52
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
AQUA (Vargas-Vera, Motta, and Domingue 2003) combines knowledge encoded
in a database with domain-related documents through an ontology that describes aca-
demic life. AQUA tries to answer a question using its knowledge base. If a query cannot
be satisfied via the database, it tries to find an answer on domain-related World Wide
Web pages.
The L&C system (Ceusters, Smith, and Van Mol 2003) is one of the most ambitious
works in the medical domain; it tries to combine authoritative medical knowledge
with information about patients. The information needed by physicians is of two sorts.
First, there is information concerning patients, such as the changes in Mr. X?s blood
pressure over the past three days, or the substances to which Ms. Y is allergic. Second,
there is what can be defined as medical knowledge, that is, the information found in
textbooks, journal articles, clinical studies, and so on. The final objective of this work
is to combine these two types of information so that the QA system, when asked,
for example, whether it is safe to give the patient an additional shot of a hypoten-
sive agent in order to reduce bleeding, would respond with: Can you please wait for
45 seconds because the patient?s blood pressure has been dropping slightly already for the last
2 minutes?
6.4 Porting to Other Domains
Developing a system in a specific domain could be time-consuming. It is natural to think
of ways to reuse technologies (or even code) in QA systems from other domains or from
open-domain QA systems. A topic that is intimately related is that of portability to other
domains.
Some question-answering systems are designed with the goals of re-usability and
portability in mind. These are generic systems that can be localized to specific domains.
For example, JAVELIN (Nyberg et al 2005) is an open-domain QA system that can be
extended to focus on restricted domains. Special care was taken to leverage ontologies
specific to the chosen domain by developing a Java API. The specific ontological in-
formation extracted is the type hierarchy and sets of synonyms (AKA, or ?also known
as? extraction). Another example that demonstrates efforts in adapting an open-domain
QA system to a specialized geographical environment can be seen in the work by Ferre?s
and Rodr??guez (2006).
Another approach, developed by Frank et al (2005), is based on the use of struc-
tured knowledge sources. This approach applies deep linguistic analysis to the question
and transforms it into an internal representation based on conceptual and semantic
characteristics. This representation is domain-independent and provides a natural in-
terface to the underlying knowledge databases. This approach has been implemented
as a prototype for two application domains: the domain of Nobel prize winners and the
domain of language technology.
Another issue is that of localizing an open-domain QA system to a restricted do-
main. Nyberg et al (2005) provides a case study that describes the problems in adapting
an existing open-domain QA system to be able to deal with knowledge from existing
domain ontologies.
7. Building a Restricted-Domain QA System: Main Considerations
It is difficult to imagine a general methodology for the development of an RDQA
system. On the one hand, current systems are overly influenced by the specific
53
Computational Linguistics Volume 33, Number 1
characteristics and requirements of the domains, from the different types of questions
to be answered to the heterogeneity of the knowledge available for the domain. On the
other hand, the known methodological proposals (Minock 2005) are so general that they
could be used to design any kind of information system.
Rather than propose a design methodology, we want to emphasize the main points
to be taken into consideration when designing a QA system for a specific domain. These
points are related to the analysis and modeling of the domain information and the
selection of the appropriate technology required by the QA system. They can be listed
as follows:
 domain query system analysis
 domain knowledge selection
 domain knowledge acquisition and representation
 system interface design
 technological requirements selection
Domain query system analysis: Knowing in detail all the different ways users ask for
information is a prerequisite for being effective in a restricted-domain scenario.
Questions need to be analyzed, classified, and associated with the different types
of information the users request. The kinds of questions in a restricted domain
may vary from general open-domain factoid and definition questions to very
special kinds of questions that depend on the selected domain.
Domain knowledge selection: The amount and type of authoritative knowledge
available for computational treatment is especially variable across different
domains. For instance, there are plenty of resources for biomedical (Zweigenbaum
2003) or technical related domains, whereas, on the other hand, less popular
domains (such as the legal domain) have minimal elaborated knowledge but
have the advantage of enormous quantities of raw text. Domain information
can be represented in different formats: from unstructured plain text documents
to semi-structured (e.g., templates, SGML annotated text) or highly structured
knowledge encoded in large databases and authoritative ontologies. Selecting the
appropriate domain knowledge resources in each particular case is an important
aspect in the design of an RDQA.
Domain knowledge acquisition and representation: Using the domain knowledge
for QA purposes requires the definition of an internal representation model
that allows the integration or combination of the different information sources
available for the domain. The complexity of the representation model used will
be proportional to the complexity of the information sources needed for encoding
domain knowledge. The model selected for domain knowledge representation
will also determine the kind of operational processes and reasoning techniques
allowed in the domain.
System interface design: In order to obtain a natural mode of communication between
users and the system, the interaction needs to be tailored according to the
domain characteristics and the user requirements. Usually, natural language
(NL) interfaces are preferred because they allow fluent communication between
the users and the system. Nevertheless, as current natural language processing
54
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
technologies do not allow the automatic translation of natural language text into
a fully unambiguous content representation, NL interfaces may be sometimes
substituted by template-like interfaces or unambiguous formal outputs (only
useful for expert users) when exact knowledge understanding and representation
is required.
Technological requirements selection: The abilities we expect from an RDQA system
depend explicitly on the different aspects of the domain analysis that we have
presented before. Decisions on the specific technology and methods to use will be
taken according to the type of questions to be solved, the availability of specialized
resources, and the representational model used for encoding the domain knowl-
edge. As discussed in previous sections (see Sections 4 and 6), QA in restricted
domains usually requires techniques that differ substantially from the techniques
used in open-domain systems. Restricted domains enable the possibility of using
comprehensive ontological knowledge, thus making it possible to perform more
complex inferences than in open-domain QA and therefore leveraging the possi-
bility of answering more complex questions. From this perspective taking accurate
design decisions customized to the task requirements and the domain resources is
essential.
8. Introduction to the Articles in this Special Section
Demner-Fushman and Lin?s article (Answering clinical questions with knowledge-based
and statistical techniques) extends previous work by the authors (Demner-Fushman and
Lin 2005) on a QA system in the medical domain. The system is designed to satisfy
information needs within the framework of evidence-based medicine (EBM) whereby
a doctor needs to gather the current best evidence, namely, high-quality patient-
centered clinical research. The data source used by the system is the set of MEDLINE
abstracts, a large bibliographic database that is accessed on-line via PubMed. Input
questions in this domain are highly specific and complex. Following practice in the
domain, the input questions are formulated as PICO-based frames representing the
major elements of a query in EBM: Problem/Population, Intervention, Comparison,
and Outcome. A central task of the system is the automatic identification of PICO
elements in the MEDLINE abstracts and their matching with the input query frame.
In the process the system uses the Unified Medical Language System (UMLS), an
extensive ontology specialized on this domain. This system is a clear example of the
adaptation of the task of question answering to a specific and highly practical domain
using specialized resources in order to satisfy information needs formulated as complex,
structured questions.
Hallett, Scott, and Power?s article (Composing questions through conceptual authoring)
focuses on the stage of question formulation. Questions in a QA system over a spe-
cialized domain where the users are domain experts are typically complex in nature.
This results in a problem both for the user, who needs to provide all the specific
information in the question, and to the system that needs to analyze the question.
The solution proposed in this article is to facilitate question formulation by means of
Conceptual Authoring, whereby the user edits a formal representation of the query and
receives feedback from an automatically generated natural language representation of
that query. The article describes this method within the context of a QA system for a
database of electronic health records. An analysis of the question model in this domain
55
Computational Linguistics Volume 33, Number 1
is presented, together with an evaluation of the usability of the method. This article
presents a concept of complex query formulation that can potentially be ported to other
specialized domains.
9. Conclusions
In this article we have presented an overview of methods used in QA in restricted
domains and we have argued for developing research in this area. To conclude we
would like to comment on two reasons for developing question answering in restricted
domains:
Development of vertical systems: Restricted domains allow the development of sys-
tems that can provide the full range of processing levels and achieve a com-
plete, end-to-end application. It therefore becomes possible to develop complete
systems that can be used without the need for any time-consuming training on
the methods required to formulate questions or to interpret the system results.
Furthermore, restricted domains can provide a focus for the research and develop-
ment of generic theories on complex question answering in particular and natural
language processing in general. A clear example is the UC project developed in
the 1980s. By reducing the research space it becomes possible to focus on solving
complex problems that would not be attempted if the main drive was to produce
a system that works in an open-domain fashion.
Applicability to current needs: General and broad scope systems are not effective in
domains restricted to the interests of different kind of users: from employees in
institutions and companies trying to find information in manuals and procedures,
to professionals in specialized domains like law, medicine, biology, mechanics,
programming, and so on. Notice that professionals in each of these areas re-
quire different types of information in their daily activities (e.g., there is a con-
siderable difference between looking for general information on the Internet as
opposed to looking for the empty weight of a wing of the Airbus A319 in a
technical manual).
A major difference between open-domain question answering and restricted-domain
question answering is the existence of domain-dependent information that can be used
to improve the accuracy of the system. Much of the focus of this article has been on
forms of tapping information from these resources.
Some domains are more appropriate for developing question answering systems
than others. A domain must be circumscribed enough so that a comprehensive on-
tological resource can be built for the domain. A domain must be complex enough
so that it presents challenging research problems in the area of natural language
processing. Finally, a domain must be practical enough so that the end product is
useful to a significant segment of the population. Domains (such as, for example,
biomedicine) that meet al these properties are naturally more popular for researchers
and developers. Consequently they have some of the best ontological information and
large corpora of texts and questions that can be used for the development of such
QA systems.
Question answering on restricted domains requires the processing of complex ques-
tions and offers the opportunity to carry out complex analysis of the text sources and
the questions. Restricted domains also provide comprehensive ontologies and domain
56
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
resources that can help in the task of processing complex questions and finding the
answers. The challenges and opportunities are there for us to take.
References
Abney, S. 1996. Part-of-speech Tagging and
Partial Parsing. Corpus-Based Methods in
Language and Speech. Kluwer Academic
Publishers, Dordrecht.
Ait-Mokhtar, Salah and Jean-Pierre Chanod.
1997. Incremental finite-state parsing. In
Fifth Conference on Applied Natural Language
Processing (ANLP 97), pages 72?79,
Washington, DC.
Androutsopoulos, I., G. D. Ritchie, and
P. Thanisch. 1995. Natural language
interfaces to databases?an introduction.
Natural Language Engineering, 1(1):29?81.
Barker, Ken, Vinay K. Chaudhri, Shaw Yi
Chaw, Peter Clark, James Fan, David
Israel, Sunil Mishra, Bruce W. Porter,
Pedro Romero, Dan Tecuci, and Peter Z.
Yeh. 2004. A Question-answering system
for AP chemistry: Assessing KR&R
technologies. In Principles of Knowledge
Representation and Reasoning: Proceedings
of the Ninth International Conference
(KR2004), pages 488?497, Whistler,
Canada.
Basili, Roberto, Dorte H. Hansen, Patrizia
Paggio, Maria Teresa Pazienza, and
Fabio Massimo Zanzotto. 2004.
Ontological resources and question
answering. In Workshop on Pragmatics of
Question Answering, held jointly with
NAACL 2004, Boston, Massachusetts.
Basili, Roberto and Fabio Massimo Zanzotto.
2002. Parsing engineering and empirical
robustness. Natural Language Engineering,
8(2/3): 97?120.
Benamara, Farah. 2004. Cooperative question
answering in restricted domains: The
WEBCOOP Experiments. In Workshop on
Question Answering in Restricted Domains.
42nd Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 31?38, Barcelona, Spain.
Brill, Eric, Jimmy Lin, Michele Banko,
Susan Dumais, and Andrew Ng. 2001.
Data-intensive question answering. In
Proceedings TREC 2001, number 500?250
in NIST Special Publications. NIST,
pages 393?400, Gaithersberg, MD.
Ceusters, Werner, Barry Smith, and
Maarten Van Mol. 2003. Using
ontology in query answering systems:
Scenarios, requirements and challenges.
In 2nd CoLogNET-ElsNET Symposium.
Questions and Answers: Theoretical and
Applied Perspectives, Amsterdam.
Chung, Hoojung, Young-In Song,
Kyoung-Soo Han, Do-Sang Yoon,
Joo-Young Lee, and Hae-Chang Rim.
2004. A Practical QA System in Restricted
Domains. In Workshop on Question
Answering in Restricted Domains. 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 39?45, Barcelona, Spain.
Cohen, P., R. Schrag, E. Jones, A. Pease,
A. Lin, B. Starr, D. Easter, D. Gunning,
and M. Burke. 1988. The DARPA high
performance knowledge bases project.
AI Magazine, 19(4):25?49.
Demner-Fushman, Dina and Jimmy Lin.
2005. Knowledge extraction for clinical
question answering: Preliminary results.
In Workshop on Question Answering in
Restricted Domains. 20th National Conference
on Artificial Intelligence (AAAI-05),
pages 1?9, Pittsburgh, PA.
Diekema, Anne R., Ozgur Yilmazel, and
Elizabeth D. Liddy. 2004. Evaluation of
restricted domain question-answering
systems. In Workshop on Question
Answering in Restricted Domains. 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 2?7, Barcelona, Spain.
Doan-Nguyen, Hai and Leila Kosseim.
2004. The problem of precision in
restricted-domain question-answering.
Some proposed methods of improvement.
In Workshop on Question Answering in
Restricted Domains. 42nd Annual Meeting
of the Association for Computational
Linguistics (ACL-2004), pages 8?15,
Barcelona, Spain.
Ely, J., J. Osheroff, M. Ebell, M. Chambliss,
D. Vinson, J. Stevermer, and E. Pifer.
2002. Obstacles to answering doctors?
questions about patient care with
evidence: Qualitative study. British
Medical Journal, 324:710?713.
Fellbaum, Christiane. 1998. WordNet:
Introduction. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database, Language, Speech, and
Communication. MIT Press, Cambrige,
MA, pages 1?19.
Ferre?s, Daniel and Horacio Rodr??guez.
2006. Experiments adapting an
open-domain question answering
system to the geographical domain
using scope-based resources. In 11th
Conference of the European Chapter of the
57
Computational Linguistics Volume 33, Number 1
Association of Computational Linguistics.
Workshop on Multilingual Question
Answering - MLQA?06, Trento, Italy.
Frank, Anette, Hans-Ulrich Krieger, Feiyu
Xu, Hans Uszkoreit, Berthold Crysmann,
Brigitte Jo?rg, and Ulrich Scha?fer. 2006.
Question answering from structured
knowledge sources. Journal of Applied
Logic, Special Issue on Questions and
Answers: Theoretical and Applied
Perspectives, 1:29.
Frank, Anette, Hans-Ulrich Krieger, Feiyu
Xu, Hans Uszkoreit, Berthold Crysmann,
Brigitte Jo?rg, and Ulrich Scha?fer. 2005.
Querying structured knowledge sources.
In Workshop on Question Answering in
Restricted Domains. 20th National Conference
on Artificial Intelligence (AAAI-05),
pages 10?19, Pittsburgh, PA.
Gabbay, Igal. 2004. Retrieving Definitions from
Scientific Text in the Salmon Fish Domain by
Lexical Pattern Matching. Ph.D. thesis,
University of Limerick.
Galitsky, Boris. 2001a. A natural language
question answering system for human
genome domain. In Proceedings of the
2nd IEEE International Symposium on
Bioinformatics and Bioengineering,
Bethesda, MD.
Galitsky, Boris. 2001b. Semi-structured
knowledge representation for the
automated financial advisor. In Proceedings
of the Fourteenth International Conference on
Industrial and Engineering Applications of
Artificial Intelligence and Expert Systems,
pages 874?879, Budapest, Hungary.
Green, B. F., A. K. Wolf, C. Chomsky, and
K. Laughery. 1961. Baseball: An automatic
question answerer. In Proceedings Western
Computing Conference, volume 19,
pages 219?224.
Hejazi, Mahmoud R., Maryam S. Mirian,
Kourosh Neshatian, Bahador R. Ofoghi,
and Ehsan Darudi. 2004. An
ontology-based question answering
system with auto extraction and
categorization capabilities in the
domain of telecommunications.
The CSI Journal on Computer Science
and Engineering, 2(1).
Hirschman, Lynette and Rob Gaizauskas.
2001. Natural language question
answering: The view from here. Natural
Language Engineering, 7(4):275?300.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama,
Mark Stickel, and Mabry Tyson. 1997.
FASTUS: A Cascaded Finite-state
Transducer for Extracting Information from
Natural-Language Text. MIT Press,
Cambridge, MA.
Johnson, Christopher and Charles J.
Fillmore. 2000. The FrameNet tagset for
frame-semantic and syntactic coding of
predicate-argument structure. In Janyce
Wiebe, editor, Proceedings of the 1st Meeting
of the North American Chapter of the
Association for Computational Linguistics,
Seattle, WA.
Kacmarcik, Gary. 2005. Question answering
in role-playing games. In Workshop
on Question Answering in Restricted
Domains. 20th National Conference on
Artificial Intelligence (AAAI-05),
pages 51?55. Pittsburgh, PA.
Kando, Noriko. 2005. Overview of the
fifth NTCIR workshop. In Proceedings
of the Fifth NTCIR Workshop Meeting on
Evaluation of Information Access Technologies:
Information Retrieval, Question Answering
and Cross-Lingual Information Access,
Tokyo, Japan.
Katz, Boris. 1997. From sentence processing
to information access on the World Wide
Web. In AAAI Spring Symposium on Natural
Language Processing for the World Wide Web,
pages 77?94, Stanford, CA.
Katz, Boris, Sue Felshin, Deniz Yuret, Ali
Ibrahim, Jimmy Lin, Gregory Marton,
Alton Jerome McFarland, and Baris
Temelkuran. 2002. Omnibase: Uniform
access to heterogeneous data for question
answering. In Proceedings of the 6th
International Conference on Applications of
Natural Language to Information Systems,
pages 230?234, Stockholm, Sweden.
Katz, Boris, Jimmy J. Lin, and Sue Felshin.
2002. The START multimedia information
system: Current technology and future
directions. In Proceedings of the International
Workshop on Multimedia Information
Systems, Tempe, AZ.
Kim, Soo-Min and Eduard Hovy. 2005.
Identifying opinion holders for
question answering in opinion texts.
In Workshop on Question Answering
in Restricted Domains. 20th National
Conference on Artificial Intelligence
(AAAI-05), pages 20?26, Pittsburgh, PA.
Lenat, D. and R. V. Guha. 1990. Building Large
Knowledge-Based Systems: Representation and
Inference in the Cyc Project. Addison-Wesley.
Lin, Jimmy. 2002. The Web as a resource
for question answering: Perspectives
and challenges. In Proceedings of the Third
International Conference on Language
Resources and Evaluation, pages 2120?2127,
Las Palmas, Spain.
58
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
Lindberg, D. A., B. L. Humphreys, and
A. T. McCray. 1993. The unified medical
language system. Methods of Information in
Medicine, 32(4):281?291.
Minock, Michael. 2005. Where are the ?killer
applications? of restricted domain question
answering? In Proceedings of the IJCAI
Workshop on Knowledge Reasoning in
Question Answering, page 4, Edinburgh,
Scotland.
Moldovan, Dan and Adrian Novischi. 2002.
Lexical chains for question answering.
In Proceedings of the 19th International
Conference on Computational Linguistics,
Taipei, Taiwan.
Molla?, Diego, Rolf Schwitter, Michael Hess,
and Rachel Fournier. 2000. Extrans, an
answer extraction system. Traitement
Automatique des Langues, 41(2):495?522.
Molla?, Diego and Jose? Luis Vicedo, editors.
2004. Workshop on Question Answering in
Restricted Domains. 42th Annual Meeting of
the Association for Computational Linguistics
(ACL-2004), Barcelona, Spain.
Molla?, Diego and Jose? Luis Vicedo, editors.
2005. Workshop on Question Answering in
Restricted Domains. Twentieth National
Conference on Artificial Intelligence
(AAAI-05), Pittsburgh, Pennsylvania, USA.
Niu, Yun and Graeme Hirst. 2004. Analysis
of semantic classes in medical text
for question answering. In Workshop
on Question Answering in Restricted
Domains. 42nd Annual Meeting of
the Association for Computational
Linguistics (ACL-2004), pages 54?61,
Barcelona, Spain.
Noy, N. F. and D. L. McGuinness. 2001.
Ontology development 101: A guide to
creating your first ontology. Technical
Report KSL-01-05, Knowledge Systems
Laboratory.
Nyberg, Eric, Teruko Mitamura, Robert
Frederking, Vasco Pedro, Matthew W.
Bilotti, Andrew Schlaikjer, and Kerry
Hannan. 2005. Extending the JAVELIN
system with domain semantics.
In Question Answering in Restricted
Domains: Papers from the AAAI Workshop,
pages 36?40, Pittsburgh, PA.
Popescu, Ana-Maria, Oren Etzioni,
and Henry Kautz. 2003. Towards a
theory of natural language interfaces to
databases. In Proceedings of the 2003
International Conference on Intelligent
User Interfaces (IUI-03), pages 149?157,
New York.
Porter, B., J. Lester, K. Murray, K. Pittman,
A. Souther, L. Acker, and T. Jones.
1988. AI research in the context of a
multifunctional knowledge base:
The botany knowledge base project.
Technical Report, AI-88-88, Department
of Computer Sciences, University of
Texas at Austin.
Rinaldi, Fabio, James Dowdall, and Gerold
Schneider. 2004. Answering questions
in the genomics domain. In Proceedings of
the ACL04 Workshop on Question Answering
in Restricted Domains, pages 46?53,
Barcelona, Spain.
Rinaldi, Fabio, Michael Hess, James
Dowdall, Diego Molla?, and Rolf Schwitter.
2004. Question answering in
terminology-rich technical domains. In
Mark T. Maybury, editor, New Directions in
Question Answering. AAAI Press/MIT
Press, Cambridge, MA, pages 71?82.
Rotaru, Mihai and Diane J. Litman. 2005.
Improving question answering for
reading comprehension tests by
combining multiple systems. In Workshop
on Question Answering in Restricted
Domains. 20th National Conference
on Artificial Intelligence (AAAI-05),
pages 46?50, Pittsburgh, PA.
Sang, Erik Tjong Kim, Gosse Bouma, and
Maarten de Rijke. 2005a. Developing
offline strategies for answering medical
questions. In Workshop on Question
Answering in Restricted Domains.
20th National Conference on Artificial
Intelligence (AAAI-05), pages 41?45,
Pittsburgh, PA.
Schrag, R., M. Pool, V. Chaudhri, R. C.
Kahlert, J. Powers, P. Cohen,
J. Fitzgerald, and S. Mishra. 2002.
Experimental evaluation of subject
matter expert-oriented knowledge base
authoring tools. In Proceedings of the
2002 PerMIS Workshop: Measuring the
Performance and Intelligence of Systems,
pages 272?279, Gaithersburg, MD.
Simmons, R. F. 1965. Answering English
questions by computer: A survey.
Communications of the ACM, 8(1):53?70.
Srihari, Rohini and Wei Li. 2000. Information
extraction supported question answering.
In Proceedings of TREC 8 (1999),
pages 185?196, Gaithersburg, MD.
Tsur, Oren, Maarten de Rijke, and Khalil
Sima?an. 2004. BioGrapher: Biography
questions as a restricted domain question
answering task. In Workshop on Question
Answering in Restricted Domains. 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 23?30, Barcelona, Spain.
59
Computational Linguistics Volume 33, Number 1
Vallin, Alessandro, Bernardo Magnini,
Danilo Giampiccolo, Lili Aunimo,
Christelle Ayache, Petya Osenova,
Anselmo Pe nas, Maarten de Rijke,
Bogdan Sacaleanu, Diana Santos, and
Richard Sutcliffe. 2005. Overview of
the CLEF 2005 multilingual question
answering track. In Proceedings of
CLEF 2005, Vienna, Austria.
Vargas-Vera, Maria and Enrico Motta. 2004.
AQUA: A question answering system for
heterogeneous sources. Technical Report
KMI-04-20, KMI.
Vargas-Vera, Maria, Enrico Motta, and
John Domingue. 2003. AQUA: An
ontology-driven question answering
system. In Mark T. Maybury, editor, New
Directions in Question Answering, Papers
from 2003 AAAI Spring Symposium,
Stanford University, pages 53?57.
Stanford, CA.
Voorhees, Ellen M. 1999. The TREC-8
question answering track report. In
Proceedings of TREC-8, pages 77?82,
Gaithersburg, MD.
Voorhees, Ellen M. 2001. The TREC question
answering track. Natural Language
Engineering, 7(4):361?378.
Vossen, Piek, editor. 1998. Euro WordNet:
A Multilingual Database with Lexical
Semantic Networks. Kluwer Academic
Publishers, Dordrecht, Holland.
Weischedel, Ralph, Jinxi Xu, and
Ana Licuanan. 2004. A hybrid
approach to answering biographical
questions. In Mark T. Maybury,
editor, New Directions in Question
Answering. AAAI Press/MIT
Press, Cambridge, MA, chapter 5,
pages 59?69.
Wilensky, Robert, David N. Chin, Marc
Luria, James Martin, James Mayfield,
and Dekai Wu. 1994. The Berkeley
Unix Consultant project. Computational
Linguistics, 14(4):35?84.
Woods, William A. 1997. Conceptual
indexing: A better way to organize
knowledge. Technical Report SMLI
TR-97-61, Sun Microsystems, Inc.
Yu, Hong, Carl Sable, and Hai Ran Zhu.
2005. Classifying medical questions
based on an Evidence Taxonomy. In
Workshop on Question Answering in
Restricted Domains. 20th National
Conference on Artificial Intelligence
(AAAI-05), pages 27?35, Pittsburgh, PA.
Zajac, Re?mi. 2001. Towards ontological
question answering. In Proceedings of
ACL2001, Workshop on Open Domain
QA, Toulouse.
Zweigenbaum, Pierre. 2003. Question
answering in biomedicine. In Proceedings
of EACL2003, Workshop on NLP for
Question Answering, Budapest.
Appendix A: List of QA Systems in Restricted Domains
The following list is by no means exhaustive. Our purpose in presenting this list is to
show the breadth of current research and applications in RDQA. We welcome updates
and additions to the list, which will be maintained at http://www.ics.mq.edu.au/
?diego/answerfinder/rdqa/.
1. Generic systems
 JAVELIN (Nyberg et al 2005)
? http://www.cs.cmu.edu/?ehn/JAVELIN/
 QUETAL (Frank et al 2005, 2006)
? http://www.dfki.de/pas/f2w.cgi?ltp/quetal-e
 AQUA (Vargas-Vera and Motta 2004; Vargas-Vera, Motta, and
Domingue 2003)
? http://kmi.open.ac.uk/projects/akt/aqua/
? http://kmi.open.ac.uk/projects/akt/publications.cfm
 START (Katz 1997; Katz et al 2002; Katz, Lin, and Felshin 2002)
? http://start.csail.mit.edu/
2. Collaborative learning for engineering education
 KAAS (Diekema, Yilmazel, and Liddy 2004)
60
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
3. Services provided by a large company
 Concordia University system (Doan-Nguyen and Kosseim 2004)
4. Salmon fish biology
 SOK-I (Gabbay 2004)
5. Biography information
 BioGrapher (Tsur, de Rijke, and Sima?an 2004)
 BBN Technologies (Weischedel, Xu, and Licuanan 2004)
6. Tourism
 WEBCOOP (Benamara 2004)
7. Weather forecasts
 System by Korea University and Sangmyung University
(Chung et al 2004)
8. Technical domains
 ExtrAns (Rinaldi et al 2004)
? http://www.ifi.unizh.ch/cl/extrans/
 TeLQAS (Hejazi et al 2004)
? http://www.neshatian.org/projects/telqas/
9. Genomics
 ExtrAns (Rinaldi, Dowdall, and Schneider 2004)
 System by KnowledgeTrail (Galitsky 2001a)
10. Financial
 System by KnowledgeTrail (Galitsky 2001b)
11. Medical domain
 EpoCare (Niu and Hirst 2004)
 system by University of Maryland (Demner-Fushman and
Lin 2005)
 question classification by Columbia University and Cooper Union
(Yu, Sable, and Zhu 2005)
 IMIX
12. Geographic domain
 System by UPC (Ferra?s and Rodr??guez 2006)
13. Nobel prizes
 System by DFKI (Frank et al 2005)
14. Language technology
 System by DFKI (Frank et al 2005)
15. Opinion texts
 System by University of Southern California (Kim and Hovy 2005)
16. Reading comprehension texts
 RC QA (Rotaru and Litman 2005)
17. Role-playing games
 System by Microsoft Research (Kacmarcik 2005)
61

Intrinsic versus Extrinsic Evaluations of Parsing Systems
Diego Molla?
Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2109, Australia
diego@ics.mq.edu.au
Ben Hutchinson
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW, United Kingdom
B.Hutchinson@sms.ed.ac.uk
Abstract
A wide range of parser and/or grammar
evaluation methods have been reported
in the literature. However, in most cases
these evaluations take the parsers in-
dependently (intrinsic evaluations), and
only in a few cases has the effect
of different parsers in real applications
been measured (extrinsic evaluations).
This paper compares two evaluations
of the Link Grammar parser and the
Conexor Functional Dependency Gram-
mar parser. The parsing systems, de-
spite both being dependency-based, re-
turn different types of dependencies,
making a direct comparison impossi-
ble. In the intrinsic evaluation, the accu-
racy of the parsers is compared indepen-
dently by converting the dependencies
into grammatical relations and using the
methodology of Carroll et al (1998) for
parser comparison. In the extrinsic eval-
uation, the parsers? impact in a practi-
cal application is compared within the
context of answer extraction. The dif-
ferences in the results are significant.
1 Introduction
Parsing is a principal stage in many natural lan-
guage processing (NLP) systems. A good parser is
expected to return an accurate syntactic structure
of a sentence. This structure is typically forwarded
to other modules so that they can work with un-
ambiguous and well-defined structures represent-
ing the sentences. It is to be expected that the
performance of an NLP system quickly degrades
if the parsing system returns incorrect syntactic
structures, and therefore an evaluation of parsing
coverage and accuracy is important.
According to Galliers and Sparck Jones (1993),
there are two main criteria in performance evalua-
tion: ?Intrinsic criteria are those relating to a sys-
tem?s objective, extrinsic criteria those relating to
its function i.e. to its role in relation to its setup?s
purpose.? (Galliers and Sparck Jones, 1993, p22).
Thus, an intrinsic evaluation of a parser would
analyse the accuracy of the results returned by the
parser as a stand-alone system, whereas an ex-
trinsic evaluation would analyse the impact of the
parser within the context of a broader NLP appli-
cation.
There are currently several parsing
systems that attempt to achieve a wide
coverage of the English language (such
as those developed by Collins (1996),
Ja?rvinen and Tapanainen (1997), and
Sleator and Temperley (1993)). There is also
substantial literature on parsing evaluation (see,
for example, work by Sutcliffe et al (1996),
Black (1996), Carroll et al (1998), and
Bangalore et al (1998)). Recently there has
been a shift from constituency-based (e.g. count-
ing crossing brackets (Black et al, 1991)) to
dependency-based evaluation (Lin, 1995; Carroll
et al, 1998). Those evaluation methodologies
typically focus on comparisons of stand-alone
parsers (intrinsic evaluations). In this paper we
report on the comparison between an intrinsic
evaluation and an evaluation of the impact of
the parser in a real application (an extrinsic
evaluation).
We have chosen answer extraction as an exam-
ple of a practical application within which to test
the parsing systems. In particular, the extrinsic
evaluation uses ExtrAns, an answer extraction sys-
tem that operates over Unix manual pages (Molla?
et al, 2000). The two grammar systems to com-
pare are Link Grammar (Sleator and Temperley,
1993) and the Conexor Functional Dependency
Grammar parser (Tapanainen and Ja?rvinen, 1997)
(henceforth referred to as Conexor FDG). These
parsing systems were chosen because both include
a dependency-based parser and a comprehensive
grammar of English. However, the structures re-
turned are so different that a direct comparison be-
tween them is not straightforward. In Section 2 we
review the main differences between Link Gram-
mar and Conexor FDG. In Section 3 we present
the intrinsic comparison of parsers, and in Sec-
tion 4 we comment on the extrinsic comparison
within the context of answer extraction. The re-
sults of the evaluations are discussed in Section 5.
2 Link Grammar and Conexor FDG
Link Grammar (Sleator and Temperley, 1993) is
a grammar theory that is strongly dependency-
based. A freely available parsing system that im-
plements the Link Grammar theory has been de-
veloped at Carnegie Mellon University. The pars-
ing system includes an extensive grammar and lex-
icon and has a wide coverage of the English lan-
guage. Conexor FDG (Tapanainen and Ja?rvinen,
1997) is a commercial parser and grammar, based
on the theory of Functional Dependency Gram-
mar, and was originally developed at the Univer-
sity of Helsinki.
Despite both being dependency-based, there are
substantial differences between the structures re-
turned by the two parsers. Figure 1 shows Link
Grammar?s output for a sample sentence, and Fig-
ure 2 shows the dependency structure returned
by Conexor FDG for comparison. Table 1 ex-
plains the dependency types used in the depen-
dency structures of the figures.
The differences between the dependency struc-
tures returned by Link Grammar 2.1 and Conexor
FDG 3.6 can be summarised as follows.
Direction of dependency: Link Grammar?s
?links?, although similar to true dependencies, do
not state which participant is the head and which
is the dependent. However, Link Grammar uses
different link types for head-right links and head-
left links, so this information can be recovered.
Conexor FDG always indicates the direction of the
dependence.
Clausal heads: Link Grammar generally
chooses the front-most element to be the head
of a clause, rather than the main verb. This is
true of both matrix and subordinate clauses, as
exemplified by the Wd and R links in Figure 1.
Conexor FDG follows the orthodox convention of
choosing the main verb as the head of the clause.
Graph structures: Link Grammar?s links com-
bine dependencies at the surface-syntactic and
deep-syntactic levels (e.g., the link Bs, which
links a noun modified by a subject-type relative
clause to the relative clause?s head verb, in Fig-
ure 1 indicates a deep-syntactic dependency). The
resulting structures are graphs rather than trees.
An example is shown in Figure 1, where the noun
man modified by a relative clause is linked to both
the complementiser and the head verb of the rela-
tive clause.
Conjunctions: Our version of Link Grammar
analyses a coordinating conjunction as the head of
a coordinated phrase (Figure 1). This is a modifi-
cation of Link Grammar?s default behaviour which
returns a list of parses, one parse per conjunct.
However in Conexor FDG?s analyses the head will
be either the first or the last conjunct, depending
on whether the coordinated phrase?s head lies to
the left or to the right (Figure 2).
Dependency types: Link Grammar uses a set of
about 90 link types and many subtypes, which ad-
dress very specific syntactic constructions (e.g. the
link type EB connects adverbs to forms of be be-
fore a noun phrase or prepositional phrase: He
is APPARENTLY a good programmer). On the
other hand, Conexor FDG uses a set of 32 de-
///// the man.n that came.v ate.v bananas.n and apples.n with a fork.n1
Wd
Ds
Ss
Bs
R RS
MVp
O^ Js
Ds
Figure 1: Output of Link Grammar.
///// the man that came ate bananas and apples with a fork
 main <
>det
> subj 
 mod<
>subj
 ins <
obj< cc<
 cc <  pcomp<
>det
Figure 2: Dependency structure returned by Conexor FDG.
pendency relations, ranging from traditional gram-
matical functions (e.g. subject, object), to specific
types of modifiers (e.g. frequency, duration, loca-
tion).
Both Conexor FDG and Link Grammar also
return non-dependency information. For Link
Grammar, this consists of some word class in-
formation, shown as suffixes in Figure 1. For
Conexor FDG, the base form morphological in-
formation of each word is returned, along with a
?functional? tag or morpho-syntactic function and
a ?surface syntactic? tag for each word.1
3 Intrinsic Evaluations
Given that both parses are dependency-based, in-
trinsic evaluations that are based on constituency
structures (e.g. (Black et al, 1991)) are hard
to perform. Dependency-based evaluations are
not easy either: directly comparing dependency
graphs (as suggested by Lin (1995), for exam-
ple) becomes difficult given the differences be-
tween the structures returned by the Link Gram-
mar parser and Conexor FDG. We there-
fore need an approach that is independent from
the format of the parser output. Following
Carroll et al (1998) we use grammatical relations
to compare the accuracy of Link Grammar and
Conexor FDG. Carroll et al (1998) propose a set
of twenty parser-independent grammatical rela-
tions arranged in a hierarchy representing differ-
ent degrees of specificity. Four relations from the
hierarchy are shown in Table 2. The arguments to
1See (Ja?rvinen and Tapanainen, 1997) for more informa-
tion on the output from Conexor FDG.
each relation specify a head, a dependent, and pos-
sibly an initial grammatical relation (in the case
of SUBJ in passive sentences, for example) or the
?type?, which specifies the word introducing the
dependent (in the case of XCOMP).
For example, the grammatical relations of the
sentence the man that came ate bananas and ap-
ples with a fork without asking has the following
relations:
SUBJ(eat,man, ),
OBJ(eat,banana),
OBJ(eat,apple),
MOD(fork,eat,with),
SUBJ(come,man, ),
MOD(that,man,come),
XCOMP(without,eat,ask)
The terms ?head? and ?dependent? used
by Carroll et al (1998) to refer to the arguments
of grammatical relations should not be con-
fused with the similar terms in the theory of
dependency grammar. Grammatical relations
and dependency arcs represent different phe-
nomena. An example should suffice to illustrate
the difference; consider The man that came ate
bananas and apples with a fork. In dependency
grammar a unique head is assigned to each word,
for example the head of man is ate. However
man is the dependent of more than one gram-
matical relation, namely SUBJ(eat,man, )
and SUBJ(come,man, ). Furthermore, in
dependency grammar a word can have at most
one dependent of each argument type, and so ate
can have at most one object, for example. But
Link Grammar Conexor FDG
Name Description Name Description
Bs Singular external object of relative clause cc Coordination
Ds Singular determiner det Determiner
Js Singular object of a preposition ins <not documented>
MVp Verb-modifying preposition main Main element
O? Object mod General post-modifier
R Relative clause obj Object
RS Part of subject-type relative clause pcomp Prepositional complement
Ss Singular subject subj Subject
Wd Declarative sentence
Table 1: Some of the dependency types used by Link Grammar and Conexor FDG.
Relation Description
SUBJ(head, dependent, initial gr) Subject
OBJ(head, dependent) Object
XCOMP(type, head, dependent) Clausal complement without an overt subject
MOD(type, head, dependent) Modifier
Table 2: Grammatical relations used in the intrinsic evaluation.
the same is not true for grammatical relations,
and we get both OBJ(eat,banana) and
OBJ(eat,apple).
3.1 Accuracy
Our intrinsic evaluation began on the assumption
that grammatical relations could be deduced from
the dependency structures returned by the parsers.
In practise, however, this deduction process is not
always straightforward; for example complexity
arises when arguments are shared across clauses.
In addition, Link Grammar?s analysis of the front-
most elements as clausal heads complicates the
grammatical relation deduction when there are
modifying clauses.
An existing corpus of 500 sentences/10,000
words annotated with grammatical relations was
used for the evaluation (Carroll et al, 1999). We
restricted the evaluation to just the four relations
shown in Table 2. This decision had two motiva-
tions. Firstly, since the dependency parsers? out-
put did not recognise some distinctions made in
the hierarchy of relations, it did not make sense to
test these distinctions. Secondly, we wanted the
deduction of grammatical relations to be as simple
a process as possible, to minimise the chance of
introducing errors. This second consideration also
led us to purposefully ignore the sharing of argu-
ments induced by control verbs, as this could not
always be deduced reliably. Since this was done
for both parsers the comparison remains meaning-
ful.
Algorithms for producing grammatical relations
from Link Grammar and Conexor FDG output
were developed and implemented. The results of
parsing the corpus are shown in Table 3. Since
Conexor FDG returns one parse per sentence only
and Link Grammar returns all parses ranked, the
first (i.e. the best) parse returned by Link Gram-
mar was used in the intrinsic evaluation.
The table shows significantly lower values of
recall and precision for Link Grammar. This is
partly due to the fact that Link Grammar?s links
often do not connect the head of the clause, as we
have seen with the Wd link in Figure 1.
3.2 Speed
Link Grammar took 1,212 seconds to parse the
10,000 word corpus, while Conexor FDG took
20.5 seconds. This difference is due partly to the
fact that Link Grammar finds and returns multiple
(and often many) alternative parses. For example,
With Link
Grammar
With
Conexor
FDG
Precision SUBJ 50.3% 73.6%
OBJ 48.5% 84.8%
XCOMP 62.2% 76.2%
MOD 57.2% 63.7%
Average 54.6% 74.6%
Recall SUBJ 39.1% 64.5%
OBJ 50% 53.4%
XCOMP 32.1% 64.7%
MOD 53.7% 56.2%
Average 43.7% 59.7%
Table 3: Accuracy of identification of grammatical
relations.
Link Grammar found a total of 410,509 parses of
the 505 corpus sentences.
4 Extrinsic Evaluations
It is important to know not only the accuracy of
a parser but how possible parsing errors affect the
success of an NLP application. This is the goal of
an extrinsic evaluation, where the system is eval-
uated in relation to the embedding setup. Using
answer extraction as an example of an NLP appli-
cation, we compared the performance of the Link
Grammar system and Conexor FDG.
4.1 Answer Extraction and ExtrAns
The fundamental goal of Answer Extraction (AE)
is to locate those exact phrases of unedited text
documents that answer a query worded in nat-
ural language. AE has received much attention
recently, as the increasingly active Question An-
swering track in TREC demonstrates (Voorhees,
2001b; Voorhees, 2001a).
ExtrAns is an answer extraction system that
operates over UNIX manual pages (Molla? et al,
2000). A core process in ExtrAns is the produc-
tion of semantic information in the shape of logi-
cal forms for each sentence of each manual page,
as well as the user query. These logical forms are
designed so that they can be derived from any sen-
tence (using robust approaches to treat very com-
plex or ungrammatical sentences), and they are op-
timised for NLP tasks that involve the semantic
comparison of sentences, such as AE.
ExtrAns? logical forms are called minimal log-
ical forms (MLFs) because they encode the mini-
mum information required for effective answer ex-
traction. In particular, only the main dependencies
between the verb and arguments are expressed,
plus modifier and adjunct relations. Thus, com-
plex quantification, tense and aspect, temporal re-
lations, plurality, and modality are not expressed.
The MLFs use reification to achieve flat expres-
sions, very much in the line of Davidson (1967),
Hobbs (1985), and Copestake et al (1997). In the
current implementation only reification to objects,
eventualities (events or states), and properties is
applied. For example, the MLF of the sentence cp
will quickly copy files is:
holds(e4),
object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type command;2 there is an
entity x6 (a file); there is an entity e4, which rep-
resents a copying event where the first argument
is x1 and the second argument is x6; there is an
entity p3which states that e4 is done quickly, and
the event e4, that is, the copying, holds.
ExtrAns finds the answers to the questions by
converting the MLFs of the questions into Prolog
queries and then running Prolog?s default resolu-
tion mechanism to find those MLFs that can prove
the question.
This default search procedure is called the syn-
onym mode since ExtrAns uses a small WordNet-
style thesaurus (Fellbaum, 1998) to convert all the
synonyms into a synonym representative. Extr-
Ans also has an approximate mode which, be-
sides normalising all synonyms, scores all docu-
ment sentences on the basis of the maximum num-
ber of predicates that unify between the MLFs of
the query and the answer candidate (Molla? et al,
2000). If all query predicates can be matched then
2ExtrAns uses additional domain knowledge to infer that
cp is a command.
the approximate mode returns exactly the same an-
swers as the synonym mode.
4.2 The Comparison
Ideally, answer extraction systems should be eval-
uated according to how successful they are in help-
ing users to complete their tasks. The use of the
system will therefore depend on such factors as
how many potential answers the user is presented
with at a time, the way these potential answers are
ranked, how many potential answers the user is
prepared to read while searching for an actual an-
swer, and so on. These issues, though important,
are beyond the scope of the present evaluation. In
this evaluation we focus solely on the relevance of
the set of results returned by ExtrAns.
4.2.1 Method
Resources from a previous evaluation of Extr-
Ans (Molla? et al, 2000) were re-used for this eval-
uation. These resources were: a) a collection of
500 man pages, and b) a test set of 26 queries and
relevant answers found in the 500 manual pages.
The careful and labour-intensive construction of
the test set gives us confidence that practically all
relevant answers to each query are present in the
test set. The queries themselves were selected ac-
cording to the following criteria:
? There must be at least one answer in the man-
ual page collection.
? The query asks how to perform a particular
action, or how a particular command works.
? The query is simple, i.e. it asks only one
question.
The manual pages were parsed using Conexor
FDG and Link Grammar. The latter has a param-
eter for outputting either all parses found, or just
the best parse found, and both parameter settings
were used. The queries were then parsed by both
parsers and their logical forms were used to search
the respective databases. The experiment was re-
peated using both the synonym and approximate
search modes.
Parser Precision4 Recall F-score
Conexor FDG 55.8% 8.9% 0.074
LG?best 49.7% 11.4% 0.099
LG?all 50.9% 13.1% 0.120
Table 4: Averages per query in synonym mode.
Parser Precision4 Recall F-score
Conexor FDG 28.3% 21.9% 0.177
LG?best 31.8% 15.8% 0.150
LG?all 40.5% 20.5% 0.183
Table 5: Averages per query in approximate mode.
4.2.2 Results
Precision, Recall and the F-score (with Preci-
sion and Recall equally weighted) for each query
were calculated.3 When no results were returned
for a query the precision could not be calculated,
but the F-score is equal to zero. The results are
shown in Tables 4 and 5. The number of times the
results for a query contained no relevant answers
are shown in Table 6.
The tables show that the approximate mode
gives better results than the synonym mode. This
is to be expected, since the synonym mode returns
exact matches only and therefore some questions
may not produce any results. For those questions,
recall and F would be zero. In fact, the number of
questions without answers in the synonym mode
is so large that the comparison between Conexor
FDG and Link Grammar becomes unreliable in
this mode. In this discussion, therefore, we will
focus on the approximate mode.
The results returned by Link Grammar when all
parses are considered are significantly better than
when only the first (i.e. the best) parse is consid-
3F was calculated using the expression
F = 2 ? |returned and relevant||returned| + |relevant|
which is equivalent to the usual formulation (with ? = 1):
F = (?2 + 1) ? Precision ? Recall?2Precision + Recall
4Average over queries for which precision is defined, i.e.
when the number of returns is non-zero.
Parser Search mode No
results
returned
Nothing
relevant
returned
Con. FDG Synonym 20 20
Con. FDG Approximate 0 8
LG?best Synonym 16 18
LG?best Approximate 1 11
LG?all Synonym 15 18
LG?all Approximate 4 12
Table 6: Numbers of times no relevant answers
were found.
ered. This shows that, in the answer extraction
task, it is better to use the logical forms of all
possible sentence interpretations. Recall increases
and, remarkably, precision increases as well. This
means that the system is more likely to include
new relevant answers when all parses are consid-
ered.
In many applications it is more practical to con-
sider one parse only. Conexor FDG, for example,
returns one parse only, and the parsing speed com-
parison (Section 3.2) shows an important differ-
ence in parsing time. If we compare Conexor FDG
with Link Grammar set to return just the best parse
? since Conexor FDG returns one parse only, this
is the fairest comparison ? we can see that recall
of the system using Conexor FDG is higher than
that of the system using Link Grammar, while re-
taining similar precision.
5 Discussion
The fairest extrinsic comparison between Conexor
FDG and Link Grammar is the one that uses the
best parse returned by Link Grammar, and the an-
swer extraction method follows the approximate
mode. With these settings, Conexor FDG pro-
duces better results than Link Grammar. However,
the results of the extrinsic comparison are far less
dramatic than those of the intrinsic comparison,
specially in the precision figures.
One reason for the difference in the results is
that the intrinsic evaluation compares grammatical
relation accuracy, whereas the answer extraction
system used in the extrinsic evaluation uses logi-
cal forms. A preliminary inspection of the gram-
matical relations and logical forms of questions
and correct answers shows that high overlap of
grammatical relations does not translate into high
overlap of logical forms. A reason for this differ-
ence is that the semantic interpreters used in the
extrinsic evaluation explore exhaustively the de-
pendency structures returned by both parsing sys-
tems and they try to recover as much information
as possible. In contrast with this, the generators of
grammatical relations used in the intrinsic evalua-
tion provide the most direct mapping from depen-
dency structures to grammatical relations. For ex-
ample, typically a dependency structure would not
show a long dependency like the subject of come
in the sentence John wanted Mary to come:
John wanted.v Mary to.o come.v
Ss
TOo
Os I
As a result, the grammatical relations would not
show the subject of come. However, the subject
of come can be traced by following several de-
pendencies (I, TOo and Os above) and ExtrAns?
semantic interpreters do follow these dependen-
cies. In other words, the semantic interpreters
use more information than what is directly en-
coded in the dependency structures. Therefore,
the logical forms contain richer information than
the grammatical relations. We decided not to op-
timise the grammatical relations used in our eval-
uation because we wanted to test the expressivity
of the inherent grammars. It would be question-
able whether we should recover more information
than what is directly expressed. After all, provided
that the parse contains all the words in the origi-
nal order, we can theoretically ignore the sentence
structure and still recover all the information.
6 Summary and Further Work
We have performed intrinsic evaluations of
parsers and extrinsic evaluations within the
context of answer extraction. These evaluations
strengthen Galliers and Sparck Jones (1993)?s
claim that intrinsic evaluations are of very limited
value. In particular, our evaluations show that
intrinsic evaluations may provide results that
are distorted with respect to the most intuitive
purpose of a parsing system: to deliver syntactic
structures to subsequent modules of practical NLP
systems. There is a clear need for frameworks for
extrinsic evaluations of parsers for different NLP
applications.
Further research to confirm this conclusion will
be to try and minimise the occurrence of vari-
ables in the experiments by using the same corpus
for both the intrinsic and the extrinsic evaluations
and/or by using an answer extraction system that
operates on the level of grammatical relations in-
stead of MLFs. Additional further research will
be the use of other intrinsic evaluation methodolo-
gies and extrinsic evaluations within the context of
various other embedding setups.
Acknowledgement
This research is supported by the Macquarie Uni-
versity New Staff grant MUNS?9601/0069.
References
Srinivas Bangalore, Anoop Sarkar, Christine Doran,
and Beth Ann Hockey. 1998. Grammar & parser
evaluation in the XTAG project. In Proc. Workshop
on the Evaluation of Parsing Systems, LREC98.
Ezra Black, S.P. Abney, D. Flickinger, C. Gdaniec,
R. Grisham, P. Harrison, D. Hindle, R. Ingria,
F. Jelinek, J. Klavans, M. Liberman, M.P. Mar-
cus, S. Roukos, B. Santorini, and T. Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proc.
DARPA Speech and Natural Language Workshop,
pages 306?311, Pacific Grove, CA. Morgan Kauf-
mann.
Ezra Black. 1996. Evaluation of broad-coverage
natural-language parsers. In Ronald A. Cole, Joseph
Mariani, Hans Uszkoreit, Annie Zaenen, and Victor
Zue, editors, Survey of the State of the Art in Hu-
man Language Technology, pages 488?490. CSLU,
Oregon Graduate Institute.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proc. LREC98.
John Carroll, G. Minnen, and T. Briscoe. 1999. Corpus
annotation for parser evaluation.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc.
ACL. Santa Cruz.
Ann Copestake, Dan Flickinger, and Ivan A. Sag.
1997. Minimal recursion semantics: an introduc-
tion. Technical report, CSLI, Stanford University,
Stanford, CA.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81?120. Univ. of Pitts-
burgh Press.
Christiane Fellbaum. 1998. Wordnet: Introduction. In
Christiane Fellbaum, editor, WordNet: an electronic
lexical database, Language, Speech, and Communi-
cation, pages 1?19. MIT Press, Cambrige, MA.
Julia R. Galliers and Karen Sparck Jones. 1993. Evalu-
ating natural language processing systems. Techni-
cal Report TR-291, Computer Laboratory, Univer-
sity of Cambridge.
Jerry R. Hobbs. 1985. Ontological promiscuity. In
Proc. ACL?85, pages 61?69. University of Chicago,
Association for Computational Linguistics.
Timo Ja?rvinen and Pasi Tapanainen. 1997. A depen-
dency parser for english. Technical Report TR-1,
Department of Linguistics, University of Helsinki,
Helsinki.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proc. IJCAI-
95, pages 1420?1425, Montreal, Canada.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000. Extrans, an answer extrac-
tion system. T.A.L., 41(2):495?522.
Daniel D. Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Proc. Third Inter-
national Workshop on Parsing Technologies, pages
277?292.
Richard F. E. Sutcliffe, Heinz-Detlev Koch, and An-
nette McElligott, editors. 1996. Industrial Parsing
of Software Manuals. Rodopi, Amsterdam.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Procs. ANLP-97.
ACL.
Ellen M. Voorhees. 2001a. Overview of the TREC
2001 question answering track. In Ellen M.
Voorhees and Donna K. Harman, editors, Proc.
TREC-10, number 500-250 in NIST Special Publi-
cation. NIST.
Ellen M. Voorhees. 2001b. The TREC question
answering track. Natural Language Engineering,
7(4):361?378.
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74?81
Manchester, UK. August 2008
Indexing on Semantic Roles for Question Answering
Luiz Augusto Pizzato
Centre for Language Technology
Macquarie University
Sydney, Australia
pizzato@ics.mq.edu.au
Diego Molla?
Centre for Language Technology
Macquarie University
Sydney, Australia
diego@ics.mq.edu.au
Abstract
Semantic Role Labeling (SRL) has been
used successfully in several stages of auto-
mated Question Answering (QA) systems
but its inherent slow procedures make it
difficult to use at the indexing stage of the
document retrieval component. In this pa-
per we confirm the intuition that SRL at
indexing stage improves the performance
of QA and propose a simplified technique
named the Question Prediction Language
Model (QPLM), which provides similar in-
formation with a much lower cost. The
methods were tested on four different QA
systems and the results suggest that QPLM
can be used as a good compromise be-
tween speed and accuracy.
1 Introduction
Semantic Role Labeling (SRL) has been imple-
mented or suggested as a means to aid several Nat-
ural Language Processing (NLP) tasks such as in-
formation extraction (Kogan et al, 2005), multi-
document summarization (Barzilay et al, 1999)
and machine translation (Quantz and Schmitz,
1994). Question Answering (QA) is one task that
takes advantage of SRL, and in fact much of the
research about the application of SRL to NLP is
related to QA. Thus, Narayanan and Harabagiu
(2004) apply the argument-predicate relationship
from PropBank (Palmer et al, 2005) together with
the semantic frames from FrameNet (Baker et al,
1998) to create an inference mechanism to improve
QA. Kaisser and Webber (2007) apply semantic
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
relational information in order to transform ques-
tions into information retrieval queries and further
analyze the results to find the answers for natural
language questions. Sun et al (2005) use a shal-
low semantic parser to create semantic roles in or-
der to match questions and answers. Shen and La-
pata (2007) developed an answer extraction mod-
ule that incorporates FrameNet style semantic role
information. They deal with the semantic role as-
signment as a optimization problem in a bipartite
graph and the answer extraction as a graph match-
ing over the semantic relations.
Most of the studies that use SRL or similar tech-
niques to QA apply semantic relation tools on the
input or output of the Information Retrieval phase
of their system. Our paper investigates the use of
semantic information for indexing documents. Our
hypothesis is that allowing Semantic Role infor-
mation at the indexing stage the question analyzer
and subsequent stages of the QA system can obtain
higher accuracy by providing an implicit query an-
alyzer as well as more precise retrieval. Theoret-
ically, the inclusion of this information at index-
ing time can also speed up the overall QA process
since syntactic rephrasing or re-ranking of docu-
ments based on semantic roles would not be nec-
essary. However, SRL techniques are still highly
complex and they demand a computational power
that is not yet available to most research groups
when working with large corpora. In our experi-
ence the annotation of a 3GB corpus, such as the
AQUAINT (Graff, 2002), using a semantic role
labeler, for instance SwiRL from Surdeanu and
Turmo (2005) can take more than one year using
a standard PC configuration1 .
In order to efficiently process a corpus with se-
1Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM
74
mantic relations, we have developed an alterna-
tive annotation strategy based on word-to-word re-
lations instead of noun phrase-to-predicate rela-
tions. We define semantic triples based on syn-
tactic clues; this approach was also studied by
Litkowski (1999) but some major differences with
our work are that we use automatically learned
rules to generate the semantic relations, and that
we use different semantic labels than those de-
fined by Litkowski, some more specific and some
more general. Our annotation scheme is named the
Question Prediction Language Model (QPLM) and
represents relations between pairs of words using
labels such as Who and When, according to how
one word complements the other.
In the following section we provide an overview
of the proposed semantic annotation module. Then
in Section 3 we detail the information retrieval
framework used that allows the indexing and re-
trieval of semantic information. Section 4 de-
scribes the experimental setup and presents the re-
sults. Finally, Section 5 presents the concluding
remarks and some discussion of further work.
2 Question Prediction Language Model
QPLM, as described in Pizzato and Molla? (2007),
represents sentences by specifying the semantic re-
lationship among its components using question
words. In this way, we focus on dividing the prob-
lem of representing a large sentence into small
questions that could be asked about its compo-
nents. QPLM is expressed by triples ?(?) ? ?
where ? is a question word, ? is the word that con-
cerns the question word ? and ? is the word that
answers the relation ? about ?. For instance the
relation Who(eat) ? Jack tells us that the per-
son who eats is Jack. The representation of our se-
mantic relations as triples ?(?) ? ? is important
because it allows the representation of sentences as
directed graphs of semantic relations. This repre-
sentation has the capacity of generating questions
about the sentence being analyzed. Figure 1 shows
such a representation of the sentence: ?John asked
that a flag be placed in every school?.
Having the sentence of Figure 1 and remov-
ing a possible answer ? from any relation triple,
it is possible to formulate a complete question
about this sentence that would require ? as an
answer. For instance, we can observe that re-
moving the node John we obtain the question
?Who asked for a flag to be placed in every
J o h n a s k e d
p l a c e d s c h o o l
e v e r yf l ag
w h o
 w h a t
 w h a t  w h i c h
w h e r e     
Figure 1: Graph Representation
school?? where Who was extracted from the triple
Who(ask) ? John. The same is valid for other
relations, such as removing the word school to ob-
tain the question ?Where did John ask for a flag
to be placed??. The name Question Prediction
for this model is due to its capability of generat-
ing questions regarding the sentence that has been
modeled.
We have developed a process to automatically
annotate QPLM information, the process is rule
based where the rules are automatically learned
from a corpus obtained from mapping PropBank
into QPLM instances. The mapping between se-
mantic roles and QPLM is not one-to-one, which
reduces the accuracy of the training corpus. A
sample of 40 randomly selected documents was
manually evaluated showing that nearly 90% of the
QPLM triples obtained were correctly converted
from the PropBank mapping. PropBank does not
give us some relations that we wish to include such
as ownership (Whose(car) ? Maria) or quan-
tity (HowMany(country) ? twenty)), but it
does give us the benefits of a large training set cov-
ering a variety of different predicates.
Our QPLM annotation tool, like most SRL
tools, makes use of a syntactic parser and a named-
entity (NE) recognizer. We are currently using
Connexor2 for syntactic parsing and LingPipe3 for
named-entity recognition.
An evaluation of our QPLM annotation has
shown a reasonable precision (50%) with a low
recall (24%). Both precision and recall seem to
be connected with the choice of training corpus.
The high precision is influenced by the large train-
ing set and the different variety of predicates. The
low recall is due to the low amount of connections
that can be mapped from one sentence in Prop-
Bank to QPLM. As we will present in Section 4,
QPLM helps to improve results for QA even when
2http://www.connexor.com
3http://alias-i.com/lingpipe/
75
this training corpus is not optimal. This suggests
that if a more suitable corpus is used to create the
QPLM rules then we can improve the already pos-
itive results. An ideal training corpus would con-
tain all QPLM pairs; not only verbs and head of
noun phrases but also connections among all rele-
vant words in a sentence.
3 Indexing and Retrieving Semantic
Information
A document index that contains information about
semantic relations provides a way of finding docu-
ments on the basis of meaningful relations among
words instead of simply their co-occurrence or
proximity to each other. A semantic relation index
allows the retrieval of the same piece of informa-
tion when queried using syntactic variations of the
same query such as: ?Bill kicked the ball? or ?The
ball was kicked by Bill?.
Several strategies can be used to build the index-
ing structure that includes relational information.
The task of IR requires fast indexing and retrieval
of information regardless of the amount of data
stored and how it is going to be retrieved. From
our experience, the use of relational databases is
acceptable only if the amount of documents and
speed of indexing and retrieval is not a concern.
When database systems are used on large IR sys-
tems there is always a trade off between the speed
of indexing and the speed of retrieval as well speed
and storage efficiency.
The best approach for IR has always been a cus-
tom built inverted file structure. In the semantic
role/QPLM case it is important to develop an in-
dexing structure that can maintain the annotation
information. Because it is important to allow dif-
ferent types of information to be indexed, we im-
plemented a framework for information retrieval
that easily incorporates different linguistic infor-
mation. The framework allows fast indexing and
retrieval and the implementation of different rank-
ing strategies.
With the inclusion of relational information, the
framework provides a way to retrieve documents
according to a query of semantically connected
words. This feature is best used when queries are
formed as sentences in natural language. A sim-
plified representation of the framework index is
shown in Figure 2 for a QPLM annotated sentence.
Figure 2 shows that the relation of words are rep-
resented by a common relation identifier and a re-
QPLM representation for ?Bill kicked the ball?:
ID Relation
11 Who(kick) ? Bill
12 What(kick) ? ball
Inverted file representation:
Term Document Rel. ID Rel. Type Role
Bill 1 11 Who Arg
kick 1 11 Who Pred
12 What Pred
ball 1 12 What Arg
Figure 2: Simplified representation of the indexing
of QPLM relations
Query Returns documents that
?(kick) ? ? contain the word kick
Who(kick) ? ? inform that someone kicks
Who(?) ? Bill inform that Bill does an action
Who(kick) ? Bill inform that Bill kicks
Figure 3: QPLM Queries (asterisk symbol is used
to represent a wildcard)
lation type. The roles that each word plays in a
relation is also included within the same record.
The IF is optimized so that redundant information
is not represented, as illustrated by the record of
the word kick and the single document number.
The framework also provides a way to include
words that have not been explicitly related to other
words in the text just in the same way as a stan-
dard bag-of-words (BoW) approach. This feature
is important even when the text is fully semanti-
cally or syntactically parsed. Many words may not
be associated with the others in a sentence because
of different reasons such as errors in the parser.
Therefore, even if the query presented to the re-
trieval component is not a proper natural language
sentence or it fails to be analyzed, the system will
perform as a normal BoW system.
Once the retrieval query is analyzed, it is pos-
sible to perform queries that focus on retrieving
all documents where a certain relation occurs as
well as all documents where a certain word plays
a specific role. The example in Figure 3 demon-
strates some queries and what documents or sen-
tences they return.
A document containing the sentence ?Bill kicked
the ball? would be retrieved for all the queries in
Figure 3. The framework also allows the formula-
tion of more complex queries such as:
(Who(kick) ? ?) ? (What(kick) ? ball)
76
Each token is indexed by itself (i.e not together
with the related words) including the information
from the relations it is part of. This is done with no
overhead or redundant information being stored.
This approach makes it possible to keep the stan-
dard models for document ranking. A normal
calculation of Term Frequency (TF) and Inverted
Document Frequency (IDF) is performed when
taking the terms individually or as BoW, while
only a minimal modification of TF/IDF is required
when a more complex retrieval strategy is needed.
The ranking strategy is based on a vector space
model. Documents and queries are represented
as three different vectors: bag-of-words (BoW-V),
partial relation (PR-V) and full relation (FR-V).
The weights of the vector tokens are calculated us-
ing the weights of their individual tokens in the
context of the vector being analyzed. In BoW-V,
weights are calculated based on words; PR-V uses
individual words and their relation types; FR-V
uses the association of a specific word with another
word. Figure 4 illustrates the contents of these vec-
tors for the sentence ?John loves Mary, but Mary
likes Brad? when used as a query:
BoW-V: ?[John:1], [loves:1], [Mary:2],[likes:1], [Brad:1]?
PR-V:
?[John:ARG0:1], [loves:PRED:1],
[Mary:ARG1:1], [Mary:ARG0:1],
[likes:PRED:1], [Brad:ARG1:1]?
FR-V:
?[John:ARG0:loves:1],
[Mary:ARG1:loves:1],
[Mary:ARG0:likes:1],
[Brad:ARG1:likes:1]?
Figure 4: Vectors used for document ranking
The tokens of the above example would have
different weights if the same sentence appeared in
a document with additional sentences. Because of
their lower frequency, it is expected that the com-
ponents of FR-V and, in a lesser extent, of PR-V to
have a stronger impact on the calculation of simi-
larity than the components of BoW-V. With this
approach, for queries with relations that are not
indexed, the method is equivalent to a traditional
BoW approach.
4 Experiments and Evaluation
We have performed a series of experiments using
the techniques described on Section 3 in order to
verify the usefulness of QPLM in comparison to
SRL based on PropBank. We compared both se-
mantic annotations by using it with IR and under
QA evaluation methods.
4.1 Configuration of experiments
We performed experiments using data resources
from the QA track of the TREC conferences
(Voorhees and Dang, 2006) and the evaluation
scripts available at their TREC website of years
2004, 2005 and 2006. The retrieval experiments
were carried out using only a reduced set of docu-
ments from the AQUAINT corpus because the se-
mantic role labelers tested were not able to parse
the full set, unlike QPLM which parsed all docu-
ments successfully.
The SRL tool SwiRL (Surdeanu and Turmo,
2005) has a good precision and coverage, however
it is slow and quite unstable when parsing large
amounts of data. We have assembled a cluster
of computers in order to speed up the corpus an-
notation, but even when having around ten ded-
icated computers the estimated completion time
was larger than one year. The lack of semantic an-
notators that can quickly evaluate large amount of
data gave us the stimulus needed to use a simplified
and quicker technique. We used the QPLM anno-
tation tool which takes less than 3 weeks to fully
annotate the 3GB of data from the AQUAINT cor-
pus using a single machine.
Since we wanted to determinate how QPLM
compares to SRL, particularly on the basis of its
usage for IR and for QA, we performed some
tests using the available amount of data anno-
tated with semantic roles, and the same docu-
ments with QPLM. The part of the AQUAINT
corpus annotated includes the first 41,116 docu-
ments, in chronological order, from the New York
Times (NYT) newspaper. We used the 1,448 ques-
tions from the QA track of 2004, 2005 and 2006
from the TREC competition. Since these questions
are not always self contained and in some cases
(OTHER-type questions) not even a proper natu-
ral language sentence, we performed some ques-
tion modification so that the entire topic text could
be included. These modifications include substitu-
tion of key pronouns as well as the inclusion of the
whole topic text when shorter representations were
found. In some extreme cases when no substitution
was possible and the question did not mention the
topic, we added a phrase containing the topic at the
start of the question. Some examples are presented
77
Topic: Gordon Gekko
Question: What year was the movie released?
Modification: Regarding Gordon Gekko, what year
was the movie released?
Question: What was Gekko?s profession?
Modification: What was Gordon Gekko?s profession?
Question: Other
Modification: Tell me more about Gordon Gekko.
Figure 5: Modifications applied to TREC ques-
tions
in Figure 5.
Using these questions as queries for our IR
framework, we retrieved a set of 50 documents for
every question. We analyzed the impact of the se-
mantic annotation when used on document indices
by checking the presence of the answer string in
the documents returned. We also obtained a list
of 50 documents using solely the BoW approach
in order to compare what is the gain over standard
retrieval.
4.2 Evaluation of retrieval sets
Table 1 presents the results of the retrieval set using
TREC?s QA track from 2004, 2005 and 2006 us-
ing the BoW, the SRL and the QPLM approaches.
Because we performed the evaluation of these doc-
uments automatically, we consider a document rel-
evant on the only basis of the presence of the
required answer string. We adopted the evalua-
tion metrics for QA documents sets proposed by
Roberts and Gaizauskas (2004). We used the fol-
lowing metrics: p@n as the precision at n docu-
ments or percentage of documents containing an
answer when retrieving at most n documents; c@n
as the coverage at n documents or percentage of
questions that can be answered using up to n doc-
uments for each question; and r@n as the redun-
dancy at n document or the average number of an-
swers found in the first n documents per question.
As observed in Table 1, the SRL approach gives
the best results for all question sets on all evalu-
ation metrics, with the exception of c@50 on the
2006 question set. In most other retrieval sets
the baseline performs worse than both QPLM and
SRL, however for 2004 questions it performed bet-
ter than QPLM on p@50 and r@50. It is interesting
to observe that the QPLM results for the same year
on c@50 are better than the BoW approach indi-
cating that a larger amount of questions can poten-
tially be answered by QPLM.
2004 p@50 c@50 r@50
BoW 5.85% 33.33% 2.92
SRL 6.40% 35.33% 3.20
QPLM 5.58% 34.47% 2.79
2005 p@50 c@50 r@50
BoW 10.03% 41.13% 5.02
SRL 11.00% 43.77% 5.50
QPLM 10.58% 42.08% 5.29
2006 p@50 c@50 r@50
BoW 7.30% 34.57% 3.65
SRL 8.73% 36.33% 4.37
QPLM 8.31% 38.45% 4.16
Table 1: Experimental results of index approaches
on TREC questions
4.3 Experiments on QA systems
To better understand the relation between the re-
trieved document sets and question answering we
applied the retrieval sets to four question answer-
ing systems:
? Aranea: Developed by Lin (2007), the Aranea
system utilizes the redundancy from the
World Wide Web using different Web Search
Engines. The system relies on the text snip-
pets to generate candidate answers. It applies
filtering techniques based on intuitive rules,
as well as the expected answer classes with
named-entities recognition defined by regular
expressions and a fixed list for some special
cases.
? OpenEphyra: Developed by Schlaefer et al
(2007), the OpenEphyra framework attempts
to be a test bench for question answering tech-
niques. The system approaches QA in a fairly
standard way. Using a three-stage QA archi-
tecture (Question Analysis, Information Re-
trieval, Answer Extraction), it performed rea-
sonably well at the QA Track at TREC 2007
by using Web Search engines on its IR stage
and mapping the answers back into the TREC
corpus.
? MetaQA System: Similar to the Aranea QA
system, MetaQA (Pizzato and Molla, 2005)
makes heavy use of redundancy and the in-
formation provided by Web Search Engines.
However it goes a step further by combining
different classes of Web Search engines (in-
cluding Web Question Answering Systems)
and assigning different confidence scores to
each of the classes.
78
? AnswerFinder: Developed by Molla? and Van
Zaanen (2006), the AnswerFinder QA system
unique feature is the use of QA graph rules
learned automatically from a small training
corpus. These graph rules are based on
the maximum common subgraph between the
deep syntactic representation of a question
and a candidate answer sentence. The graphs
were derived from the output of the Connexor
dependency-based parser.
For most of these systems some modifications of
the standard system configuration were required.
All the systems used, with the exception of An-
swerFinder, make heavy use of web search en-
gines and the redundancy obtained to find their
answers. For our experiments we had to turn the
Web search off, causing a significant drop in per-
formance when compared to the reported results in
the literature. Because AnswerFinder?s IR compo-
nent is performed offline, the integration is seam-
less and only required providing the system with
a list of documents in the same format as TREC
distributes the ranked list of files per topic. The
OpenEphyra framework is well designed and im-
plemented, however the interaction between its
components still depended on the overall system
architecture, which makes the implementation of
new modules for the system quite difficult.
With the exception of AnswerFinder, all the QA
systems received a retrieval set as a collection of
snippets. This was based on the fact that these
systems are based on Web Retrieval and they ex-
pect to receive documents in this format. We ex-
tracted for every document the 255 character win-
dow where more question words (non-stopwords)
were found. The implementation of different rank-
ing strategies for passage retrieval such as those
described by Tellex et al (2003) could improve the
results for individual QA systems. However, a pre-
liminary evaluation of the passage retrieval have
shown us that the 255 character window with the
current snippet construction method was enough
to achieve near optimal performance on the docu-
ment set used.
The results obtained by the QA systems were
processed using the answer regular expressions
distributed by TREC. The numbers described in
this study show the factoid score for correct an-
swers. We have not used the exact answer be-
cause it required some cleaning of the answer log
files and some modification of some QA systems.
2004 2005 2006
BoW 5.00% 2.30% 2.10%
SRL 6.10% 3.50% 2.70%
QPLM 5.00% 2.50% 3.50%
Table 2: Factoid results for C@1 on the Aranea
system
2004 2005 2006
BoW 2.50% 5.10% 3.00%
SRL 3.30% 7.00% 4.40%
QPLM 2.80% 6.20% 4.20%
Table 3: Factoid results for C@1 on the OpenE-
phyra system
Therefore, the results shown on Tables 2, 3 and
5 are product of the same retrieval set and result
of the same evaluation procedure. Results of the
MetaQA system at Table 4 are presented as cover-
age at answer 10 (C@10) since this system has a
non standard approach for QA that is invalidated
by the methodology of this test. The results in the
other tables could be understood as either precision
or coverage at answer 1, we will refer to them as
C@1.
We observed that the results from the QA sys-
tem are consistent with the findings from the re-
sults of the retrieval system. The Aranea QA sys-
tem results on Table 2 show an average improve-
ment for the SRL approach. QPLM has similar
performance to BoW for 2004 and 2005 questions
but outperforms both techniques on 2006 ques-
tions.
The results shown by OpenEphyra in Table 3
also demonstrate that semantic annotation can help
question answering when used in the IR stages of a
QA system. The best results were observed when
SRL was applied. QPLM followed SRL and out-
performed BoW on three tests. It is important to
point out that results for the retrieval set alne in
Table 1 showed BoW outperforming QPLM for
2004 questions on both redundancy and precision
metrics. This might be an indication that OpenE-
phyra answer extraction modules are more precise
than the other QA systems and do not heavily rely
on redundancy as do the Aranea and the MetaQA
systems.
Because of the high dependency on Web
sources, the MetaQA system performed rather
poorly. As explained earlier, the results were mea-
sured using C@10 instead of C@1. The reason for
this is that the MetaQA system is meant to be an
aggregator of information sources and its ranking
79
2004 2005 2006
BoW 0.87% 3.31% 1.24%
SRL 2.61% 3.87% 1.99%
QPLM 0.43% 3.31% 1.24%
Table 4: Factoid results for C@10 on the MetaQA
system
2004 2005 2006
BoW 1.10% 2.50% 1.20%
SRL 1.80% 2.60% 2.20%
QPLM 1.80% 2.70% 2.00%
Table 5: Factoid results for C@1 on the An-
swerFinder system
mechanisms only work when sufficient evidence is
given for certain entities. Not only was the system
not designed for the single-source setup, but it was
not designed to provide a single answer. Neverthe-
less, even with the non-conformity of the system,
it appears to support that semantic markup can en-
hance the IR results for QA. Not surprisingly the
extra redundancy presented in the 2004 BoW re-
trieval contributed to better results in this redun-
dancy based QA system.
Results in Table 5 show that AnswerFinder cor-
rectly answered only a few questions for the given
question set. On the other hand, it provided some
consistent results such that the improvements were
due to additional correct answers and not to a
larger but different set of correct answers. The
AnswerFinder QA system showed a similar perfor-
mance for both semantic-based strategies and both
outperformed the BoW strategy.
In this section we have shown an evaluation of
different retrieval sets of documents using four dis-
tinct QA systems. We have observed that semantic
strategies not only assist the retrieval of better doc-
uments, but also help in finding answers for ques-
tions when used with QA systems.
5 Concluding Remarks
In this work we propose the use of semantic re-
lation in QA. We also present QPLM as an alter-
native to SRL. QPLM is a simpler approach to se-
mantic annotation based on relations between pairs
of words, which gives a large advantage in speed
performance over SRL. We show some compari-
son of retrieval sets using the questions from the
QA track of TREC and conclude that SRL and
QPLM improve the quality of the retrieval set over
a standard BoW approach. From these results we
also observe that QPLM performance does not fall
much behind SRL.
We performed an evaluation using four QA sys-
tems. These systems are conceptually different
which gives a broad perspective of the obtained re-
sults. The results once again show the effective-
ness of semantic annotation. Over QA, SRL has
performed better than the other techniques, but was
closely followed by QPLM. The results obtained
here suggest that QPLM is a cheaper and effective
method of semantic annotation that can help in tun-
ing the search component of a QA system to find
the correct answers for a question.
The results presented in this work for all QA
systems are much lower than those reported in
the literature. This is an undesirable but ex-
pected problem that occurred not only because of
the modifications carried on the QA systems but
mainly because of the reduced number of docu-
ments used for this evaluation. We are looking into
more efficient alternatives for performing the SRL
annotation of the AQUAINT corpus.
Only recently we have been able to test Koomen
et al (2005) SRL tool. This SRL tool is the top
ranking SRL tool at the CoNLL-2005 Shared Task
Evaluation and it seems to be much faster than
SwiRL. Preliminary tests suggest that it is able
to perform the annotation of AQUAINT in almost
one full year using a single computer; however,
this tool, like SwiRL, is not very stable, crashing
several times during the experiments. As further
work, we plan to employ several computers and
attempt to parse the whole AQUAINT corpus with
this tool.
It is important to point out that although the tool
of Koomen et al seems much faster than SwiRL,
QPLM still outperforms both of them on speed by
large. QPLM represents word relations that are
built using rules from syntactic and NE informa-
tion. This simpler representation, combined with
a smaller number of supporting NLP tools, allow
QPLM to be faster than current SRL tools. We
plan to carry out further work on the QPLM tool
to increase its performance on both speed and ac-
curacy. QPLM?s precision and recall figures are
going to be improved by using a hand annotated
corpus. QPLM?s speed suggest that it can be cur-
rently used on IR tools as a pre-processing engine.
It is understandable that any delay in the IR phases
is undesirable when dealing with large amount of
data, therefore optimizing the speed of QPLM is
one of our priorities.
80
Acknowledgement
This work was supported by an iMURS scholar-
ship from Macquarie University and the CSIRO.
References
Baker, Collin F., Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Com-
putational linguistics, pages 86?90, Morristown, NJ,
USA. Association for Computational Linguistics.
Barzilay, Regina, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of
the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics,
pages 550?557, Morristown, NJ, USA. Association
for Computational Linguistics.
Graff, David. 2002. The AQUAINT corpus of english
news text. CDROM. ISBN: 1-58563-240-6.
Kaisser, Michael and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings
of the ACL 2007 Workshop on Deep Linguistic Pro-
cessing,, page 4148, Prague, Czech Republic, June.
c2007 Association for Computational Linguistics.
Kogan, Y., N. Collier, S. Pakhomov, and M. Krautham-
mer. 2005. Towards semantic role labeling & ie in
the medical literature. In American Medical Infor-
matics Association Annual Symposium., Washing-
ton, DC.
Koomen, P., V. Punyakanok, D. Roth, and W. Yih.
2005. Generalized inference with multiple seman-
tic role labeling systems (shared task paper). In
Dagan, Ido and Dan Gildea, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 181?184.
Lin, Jimmy. 2007. An exploration of the principles un-
derlying redundancy-based factoid question answer-
ing. ACM Trans. Inf. Syst., 25(2):6.
Litkowski, K. 1999. Question-answering using seman-
tic relation triples. In In Proceedings of the 8th Text
Retrieval Conference (TREC-8, pages 349?356.
Molla, Diego and Menno van Zaanen. 2006. An-
swerfinder at TREC 2005. In The Fourteenth Text
REtrieval Conference (TREC 2005), Gaithersburg,
Maryland. National Institute of Standards and Tech-
nology.
Narayanan, Srini and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 693,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguist., 31(1):71?
106.
Pizzato, Luiz Augusto and Diego Molla. 2005. Ex-
tracting exact answers using a meta question answer-
ing system. In Proceedings of the Australasian Lan-
guage Technology Workshop 2005 (ALTA-2005).,
The University of Sydney, Australia, December.
Pizzato, Luiz Augusto and Diego Molla?. 2007. Ques-
tion prediction language model. In Proceedings
of the Australasian Language Technology Workshop
2007, pages 92?99, Melbourne, December.
Quantz, Joachim and Birte Schmitz. 1994.
Knowledge-based disambiguation for machine
translation. Minds and Machines, 4(1):39?57,
February.
Roberts, Ian and Robert J. Gaizauskas. 2004. Eval-
uating passage retrieval approaches for question an-
swering. In McDonald, Sharon and John Tait, edi-
tors, Advances in Information Retrieval, 26th Euro-
pean Conference on IR Research, ECIR 2004, Sun-
derland, UK, April 5-7, 2004, Proceedings, volume
2997 of Lecture Notes in Computer Science, pages
72?84. Springer.
Schlaefer, N., P. Gieselmann, and G. Sautter. 2007.
The ephyra qa system at trec 2006 the Ephyra QA
system at TREC 2006. In The Fifteenth Text RE-
trieval Conference (TREC 2006).
Shen, Dan and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 12?21, Prague,
June 2007. Association for Computational Linguis-
tics.
Sun, R. X., J. J. Jiang, Y. F. Tan, H. Cui, T. S. Chua, and
M. Y. Kan. 2005. Using syntactic and semantic rela-
tion analysis in question answering. In Proceedings
of the TREC.
Surdeanu, Mihai and Jordi Turmo. 2005. Semantic
role labeling using complete syntactic analysis. In
Proceedings of CoNLL 2005 Shared Task, June.
Tellex, Stefanie, Boris Katz, Jimmy Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR ?03: Proceedings of the
26th annual international ACM SIGIR conference on
Research and development in informaion retrieval,
pages 41?47, New York, NY, USA. ACM Press.
Voorhees, Ellen M. and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering
track. In Text REtrieval Conference.
81
