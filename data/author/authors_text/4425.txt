Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 533?542,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Word Games for Semantic Data Collection
David Vickrey Aaron Bronzan William Choi Aman Kumar
Jason Turner-Maier Arthur Wang Daphne Koller
Stanford University
Stanford, CA 94305-9010
{dvickrey,abronzan,aman,arthurex,koller}@cs.stanford.edu
{wchoi25,jasonptm}@stanford.edu
Abstract
Obtaining labeled data is a significant obstacle
for many NLP tasks. Recently, online games
have been proposed as a new way of obtain-
ing labeled data; games attract users by be-
ing fun to play. In this paper, we consider the
application of this idea to collecting seman-
tic relations between words, such as hyper-
nym/hyponym relationships. We built three
online games, inspired by the real-life games
of ScattergoriesTM and TabooTM. As of June
2008, players have entered nearly 800,000
data instances, in two categories. The first
type of data consists of category/answer pairs
(?Types of vehicle?,?car?), while the second
is essentially free association data (?subma-
rine?,?underwater?). We analyze both types
of data in detail and discuss potential uses of
the data. We show that we can extract from
our data set a significant number of new hy-
pernym/hyponym pairs not already found in
WordNet.
1 Introduction
One of the main difficulties in natural language pro-
cessing is the lack of labeled data. Typically, obtain-
ing labeled data requires hiring human annotators.
Recently, building online games has been suggested
an alternative to hiring annotators. For example, von
Ahn and Dabbish (2004) built the ESP Game1, an
online game in which players tag images with words
that describe them. It is well known that there are
large numbers of web users who will play online
games. If a game is fun, there is a good chance that
sufficiently many online users will play.
We have several objectives in this paper. The
first is to discuss design decisions in building word
games for collecting data, and the effects of these
decisions. The second is to describe the word games
1www.gwap.com/gwap/gamesPreview/espgame
that we implemented and the kinds of data they are
designed to collect. As of June 2008, our games
have been online for nearly a year, and have col-
lected nearly 800,000 data instances. The third goal
is to analyze the resulting data and demonstrate that
the data collected from our games is potentially use-
ful in linguistic applications. As an example appli-
cation, we show that the data we have collected can
be used to augment WordNet (Fellbaum, 1998) with
a significant number of new hypernyms.
2 General Design Guidelines
Our primary goal is to produce a large amount of
clean, useful data. Each of these three objectives
(?large?, ?clean?, and ?useful?) has important im-
plications for the design of our games.
First, in order to collect large amounts of data,
the game must be attractive to users. If the game
is not fun, people will not play it. This requirement
is perhaps the most significant factor to take into ac-
count when designing a game. For one thing, it tends
to discourage extremely complicated labeling tasks,
since these are more likely to be viewed as work. It
would certainly be a challenge (although not neces-
sarily impossible) to design a game that yields la-
beled parse data, for example.
In this paper, we assume that if people play a
game in real life, there is a good chance they will
play it online as well. To this end, we built on-
line versions of two popular ?real-world? games:
ScattergoriesTM and TabooTM. Not only are these
games fun, but there is also a preexisting demand
for online versions of these games, driving search
traffic to our site. We will go into more detail about
these games in the next section.
An important characteristic of these games is that
they involve more than one player. Interacting with
another player increases the sense of fun. Another
important feature these games share is that they are
533
timed. Timing has several advantages. First, tim-
ing helps make the games feel more ?game-like?, by
adding a sense of urgency. Without timing, it risks
feeling more like a labeling task than a game.
The next requirement is that the data be clean.
First, the players must be capable of producing high-
quality annotations. Second, the game should en-
courage users to enter relevant data. We award
points as a motivating factor, but this can lead play-
ers to enter irrelevant data, or collude with other
players, in order to get a higher score. In particu-
lar, collusion is more likely when players can freely
communicate. An excellent technique for producing
good data, used effectively in the ESP game, is to
require the players to match on their inputs. Requir-
ing players to match their partner?s hidden answers
discourages off-topic answers and makes it quite dif-
ficult to collude (requiring outside communication).
We use this technique in all of our games.
Finally, the data must be useful. Ideally, it would
be directly applicable to an NLP task. This require-
ment can come into conflict with the other goals.
There are certainly many kinds of data that would
be useful for NLP tasks (such as labeled parses), but
designing a game to collect this data that people will
play and that produces clean data is difficult.
In this paper, we focus on a particular kind of lin-
guistic data: semantic relationships between pairs of
words and/or phrases. We do this for several rea-
sons. First, this kind of data is relatively simple,
leading to fun games which produce relatively clean
data. Second, the real-world games we chose to
emulate naturally produce this kind of data. Third,
there are a number of recent works which focus on
extracting these kinds of relationships, e.g. (Snow
et al, 2006; Nakov & Hearst, 2008). Our work
presents an interesting new way of extracting this
type of data. Finally, at least one of these kinds of
relationships, the hypernym, or ?X is a Y? relation,
has proven to be useful for a variety of NLP tasks.
3 Description of Our Games
We now describe our three games in detail.
3.1 Categorilla
Categorilla, inspired by ScattergoriesTM, asks play-
ers to supply words or phrases which fit specific cat-
egories, such as ?Things that fly? or ?Types of fish?.
In addition, each game has a specific letter which all
answers must begin with. Thus, if the current game
has letter ?b?, reasonable answers would be ?bird?
and ?barracuda?, respectively. In each game, a ran-
domly matched pair of players are given the same
10 categories; they receive points when they match
with the other player for a particular category. Play-
ers are allowed to type as may answers for a given
category as they wish (until a match is made for that
category). After a match is made, the players get
to see what word they matched on for that category.
Each answer is supposed to fit into a specific cate-
gory, so the data is automatically structured.
Our system contains 8 types of categories, many
of which were designed to correspond to linguistic
resources used in NLP applications. Table 1 de-
scribes the category types.
The purpose of the first three types of categories is
to extract hypernym/hyponym pairs like those found
in WordNet (e.g., ?food? is a hypernym of ?pizza?).
In fact, the categories were automatically generated
from WordNet, as follows. First, we assigned counts
Cs to each synset s in WordNet using the Sem-
Cor2 labeled data set of word senses. Let desc(s)
be the set of descendants of s in the hypernym hi-
erarchy. Then for each pair of synsets s, d, where
d ? desc(s), we computed a conditional distribu-
tion P (d|s) = CdP
d??desc(s) Cd?
, the probability that
we choose node d from among the descendants of
s. Finally, we computed the entropy of each node s
in WordNet,
?
d?desc(s) P (d|s)logP (d|s). Synsets
with many different descendants occurring in Sem-
Cor will have higher entropies. Each node with a
sufficiently high entropy was chosen as a category.
We then turned each synset into a category by tak-
ing the first word in that synset and plugging it into
one of several set phrases. For nouns, we tried two
variants (?Types of food? and ?Foods?). Depend-
ing on the noun, either of these may be more natu-
ral (consider ?Cities? vs. ?Types of city?). ?Types
of food? tends to produce more adjectival answers
than ?Foods?. We tried only one variation for verbs
(?Methods of paying?). This phrasing is not per-
fect; in particular, it encourages non-verb answers
like ?credit card?.
The second group of categories tries to capture se-
lectional preferences of verbs ? for example, ?ba-
2Available at www.cs.unt.edu/ rada/downloads.html
534
Name # Description Example Good Answer
NHyp 269 Members of a class of nouns ?Vehicles? ?car?
NType 269 Members of a class of nouns ?Types of vehicle? ?car?
VHyp 70 Members of a class of verbs ?Methods of cutting? ?trimming?
VS 1380 Subjects of a verb ?Things that eat? ?cats?
VO 909 Direct objects of a verb ?Things that are abandoned? ?family?
VPP 77 Preposition arguments of a verb ?Things that are accused of? ?crime?
Adj 219 Things described by an adjective ?Things that are recycled? ?cans?
O 105 Other; mostly ?Things found at/in ...? ?Things found in a school? ?teachers?
Table 1: Summary of category types. # indicates the number of categories of that type.
nana? makes sense as the object of ?eat? but not as
the subject. Our goal with these categories was to
produce data useful for automatically labeling se-
mantic roles (Gildea & Jurafsky, 2002), where selec-
tional preferences play an important role. We tried
three different types of categories, corresponding to
subjects, objects, and prepositional objects. Exam-
ples are ?Things that eat?, ?Things that are eaten?,
and ?Things that are eaten with?, to which good
answers would be ?animals?, ?food?, and ?forks?.
These categories were automatically generated us-
ing the labeled parses in Penn Treebank (Marcus
et al, 1993) and the labeled semantic roles of Prop-
Bank (Kingsbury et al, 2002). To generate the
object categories, for example, for each verb we
then counted the number of times a core argument
(ARG0-ARG5) appeared as the direct object of that
verb (according to the gold-standard parses), and
used all verbs with count at least 5. This guaran-
teed that all generated categories were grammati-
cally correct and captured information about core
arguments for that verb. Most of the prepositional
object categories proved to be quite confusing (e.g.,
?Things that are acted as?), so we manually removed
all but the most clear. Not surprisingly, the use of
the Wall Street Journal had a noticeable effect on the
types of categories extracted; they have a definite fi-
nancial bias.
The third group of categories only has one
type, which consists of adjective categories such as
?Things that are large?. While we did not have any
specific task in mind for this category type, having a
database of attributes/noun pairs seems potentially
useful for various NLP tasks. To generate these
categories, we simply took the most common ad-
jectives in the SemCor data set. Again, the result-
ing set of adjectives reflect the corpus; for example,
?Things that are green? was not generated as a cate-
gory, while ?Things that are corporate? was.
The final group of categories were hand-written.
This group was added to make sure that a sufficient
number of ?fun? categories were included, since
some of the category types, particularly the verb
categories, are somewhat confusing and difficult.
Most of the hand-written categories are of the form
?Things found at/in X?, where X is a location, such
as ?Japan? or ?the ocean?.
The starting letter requirement also has important
consequences for data collection. It was designed
to increase the variety of obtained data; without this
restriction, players might produce a smaller set of
?obvious? answers. As we will see in the results,
this restriction did indeed lead to a great diversity of
answers, but at a severe cost to data quality.
3.2 Categodzilla
Categodzilla is a slightly modified version of Cat-
egorilla, with the starting letter constraint relaxed.
The combination of difficult categories and rare let-
ters often leads to bad answers in Categorilla. To in-
crease data quality, in Categodzilla for each category
there are three boxes. In the first box you can type
any word you want. Answers in the second box must
start with a given ?easy? letter such as ?c?. Answers
in the third box must start with a given ?hard? letter,
such as ?k?. The boxes much be matched in order;
guesses typed in the first box which match either of
the other two boxes are automatically propagated.
3.3 Free Association
Free Association, inspired by TabooTM, simply asks
players to type words related to a given ?seed? word.
Players are not allowed to type any of several words
on a ?taboo? list, specific to the current seed word.
535
As soon as a match is achieved, players move on to
a new seed word.
The seed words came from two sources. The first
was the most common words in SemCor. The sec-
ond was the Google unigram data, which lists the
most common words on the web. In both cases, we
filtered out stop words (including all prepositions).
Unlike Categorilla, we found that nearly all col-
lected Free Association data was of good quality,
due to the considerably easier nature of the task. Of
course, we do lose the structure present in Catego-
rilla. As the name suggests, the collected data is es-
sentially free word association pairs. We analyze the
data in depth to see what kinds of relations we got.
4 Existing Word Games
Two notable word games already exist for collecting
linguistic data. The first is the Open Mind Common
Sense system3 (Chklovski, 2003). The second is
Verbosity4 (von Ahn et al, 2006). Both these games
are designed to extract common sense facts, and thus
have a different focus than our games.
5 Bots
There may not always be enough players available
online to match a human player with another human
player. Therefore, one important part of designing
an online game is building a bot which can func-
tion in the place of a player. The bots for all of our
games are similar. Each has a simple random model
which determines how long to wait between guesses.
The bot?s guesses are drawn from past guesses made
by human players for that category/seed word (plus
starting letter in the case of Categorilla). Just as with
a human player, as soon as one of the bot?s guesses
matches one of the player?s, a match is made.
If there are no past guesses, the bot instead makes
?imaginary? guesses. For example, in Categorilla,
we make the (obviously false) assumption that for
every category and every starting letter there are ex-
actly 20 possible answers, and that both the player?s
guesses and the bot?s imaginary guesses are drawn
from those 20 answers. Then, given the number
of guesses made by the player and the number of
imaginary guesses made by the bot, the probabil-
ity of a match can be computed (assuming that all
3http://commons.media.mit.edu/en
4www.gwap.com/gwap/gamesPreview/verbosity
Grla Gdza Free
Game Length 3min 3min 2min
Games Played 19656 2999 15660
Human-Human Games 428 45 401
Categories 3298 3298 9488
Guesses Collected 391804 78653 307963
Guesses/Categories 119 24 32
Unique Guesses 340433 56142 221874
Guesses: All/Unique 1.15 1.40 1.39
Guesses/Games 19.9 26.2 19.7
Guesses per minute 6.6 8.7 9.9
Table 2: Statistics for Categorilla, Categodzilla, and Free
Association.
guesses are made independently). Once this proba-
bility passes a certain threshold, randomly generated
for each category at the start of each game, the bot
matches one of the player?s guesses, chosen at ran-
dom. The Free Association bot works similarly.
For Free Association, the bot rarely has to resort
to generating these imaginary guesses. In Catego-
rilla, due to the starting letter requirement, the bot
has to make imaginary guesses much more often.
Imaginary guessing can encourage poor behavior on
the part of players, since they see that matches can
occur for obviously bad answers. They may also re-
alize that they are playing against a bot.
An additional complication for Categorilla and
Categodzilla is that the bot has to decide which cat-
egories to make guesses for, and in what order. Our
current guessing model takes into account past diffi-
culty of the category and the current guessing of the
human player to determine where to guess next.
6 Users and Usage
Table 2 shows statistics of each of the games, as
of late June 2008. While we have collected nearly
800,000 data instances, nearly all of the games were
between a human and the bot. Over the course of
a year, our site received between 40 and 100 vis-
its a day; this was not enough to make it likely for
human-human games to occur. The fact that we still
collected this amount of data suggests that our bot is
a satisfactory substitue for a human teammate. We
have anecdotally found that most players do not re-
alize they are playing against a bot. While most of
the data comes from games between a human and a
bot, our data set consists only of input by the human
players.
536
110
100
1000
1 2 3-5 6-1
0
11-
20
21-
50
51-
100
101
-
200
201
-
500
501
-
100
0
100
1-2
000
Games Played
Nu
mb
er
 
of 
Us
er
s
Categorilla
Categodzilla
Free Association
Figure 1: Users are grouped by number of games played.
Note that this graph is on a double-log scale.
Our main tool for attracting traffic to our site was
Google. First, we obtained $1 a day in AdWords,
which pays for between 7 to 10 clicks on our ad
a day. Second, our site is in the top 10 results for
many relevant searches, such as ?free online scatter-
gories?.
Categorilla was the most popular of the games,
with about 25% more games played than Free As-
sociation. Taking the longer length of Categorilla
games into account (see Table 2), this corresponds
to almost 90% more play time. This is despite the
fact that Free Association is the first game listed on
our home page. We hypothesize that this is because
ScattergoriesTM is a more popular game in real life,
and so many people come to our site specifically
looking for an online ScattergoriesTM game. Cat-
egodzilla has been played signficantly less; it has
been available for less time and is listed third on the
site. Even for Categodzilla, the least played game,
we have collected on average 24 guesses per cate-
gory.
Several of our design decisions for the games
were based on trying to increase the diversity of an-
swers. Categorilla has the highest answer diversity.
For a given category, each answer occurred on aver-
age only 1.15 times. In general, this average should
increase with the amount of collected data. How-
ever, Categodzilla and Free Association have col-
lected significantly fewer answers per category than
Categorilla, but still have a higher average, around
1.4. The high answer diversity of Categorilla is a
direct result of the initial letter constraint. For all
three games, the majority of category/answer pairs
occurred only once.
Figure 1 shows the distribution over users of the
0
0.02
0.04
0.06
0.08
0.1
0.12
a b c d e f g h i j k l m n o p q r s t u v w x y z *
Fra
ctio
n o
f An
sw
ers
Categorilla
Categodzilla
Free Assocation
Figure 2: Fraction of answers with given initial letter. *
denotes everything nonalphabetical.
number of games played. Not surprisingly, it follows
the standard Zipfian curve; there are a large number
of users who have played only a few games, and a
few users who have played a lot of games. The mid-
dle of the curve is quite thick; for both Categorilla
and Free Association there are more than 100 play-
ers who have played between 21 and 50 games.
Figure 2 shows the distribution of initial letters
of collected answers for each game. Categorilla
is nearly flat over all letters besides ?q?, ?x?, and
?z? which are never chosen as the inital letter con-
straint. This means players make a similar number
of guesses even for difficult initial letters. In con-
trast, the distribution of initial letters for Free Asso-
ciation data reflects the relatively frequency of initial
letters in English. Even though Categodzilla does
have letter constraints in the 2nd and 3rd columns,
its statistics over initial letter are very similar to Free
Association.
7 Categorilla and Categodzilla Data
In our analyses, we take ALL guesses made at any
time, whether or not they actually produced a match.
This greatly increases the amount of usable data, but
also increases the amount of noise in the data.
The biggest question about the data collected
from Categorilla and Categodzilla is the quality of
the data. Many categories can be difficult or some-
what confusing, and the initial letter constraint fur-
ther increases the difficulty.
To evaluate the quality of the data, we asked
three volunteer labelers to label 1000 total cate-
gory/answer pairs. Each labeler labeled every pair
with one of three labels, ?y?, ?n?, or ?k?. ?y? means
that the answer fit the category. ?n? means that it
537
Annotator y k n
#1 72 13 115
#2 77 27 96
#3 88 42 70
Majority 76 29 95
Table 3: Comparison of annotators
Data Set y k n
Control 30 14 156
Categorilla 76 29 95
Categodzilla 144 23 33
Table 4: Overall answer accuracy
does not fit. ?k? means that it ?kind of? fits. This was
mostly left up to the labelers; the only suggestion
was that one use of ?k? could be if the category was
?Things that eat? and the answer was ?sandwich.?
Here, the answer is clearly related to the category,
but doesn?t actually fit.
The inter-annotator agreement was reasonable,
with a Fleiss? kappa score of .49. The main differ-
ence between annotators was how permissive they
were; the percentage of answers labeled ?n? ranged
from 58% for the first annotator to 35% for the third.
The labeled pairs were divided into 5 subgroups of
200 pairs each (described below); Table 3 shows the
number of each label for the Categorilla-Random
subset. We aggregated the different annotations by
taking a majority vote; if all three answers were dif-
ferent, the item was labeled ?k?. Table 3 also shows
the statistics of the majority vote on the same subset.
Overall Data Quality. We compared results
for three random subsets of answers, Control-
Random, Categorilla-Random, and Categodzilla-
Random. Categorilla-Random was built by select-
ing 200 random category/answer pairs from the Cat-
egorilla data. Note that category/answer pairs that
occurred more than once were more likely to be se-
lected. Categodzilla-Random was built similarly.
Control-Random was built by randomly selecting
two sets of 200 category/answer pairs each (includ-
ing data from both Categorilla and Categodzilla),
and then combining the categories from the first set
with the answers from the second to generate a set
of random category/answer pairs.
Table 4 shows results for these three subsets. The
chance for a control answer to be labeled ?y? was
15%. Categorilla produces data that is significantly
Category Results -- Categorilla
0
5
10
15
20
25
NHyp NType VHyp VS VO VPP Adj O
Category Type
n
k
y
Figure 3: Categorilla accuracy by category type
better than control, with 38% of answers labeled ?y?.
Categodzilla, which is more relaxed about initial let-
ter restrictions, is significantly better than Catego-
rilla, with 72% of answers labeled ?y?. This relax-
ation has an enormous impact on the quality of the
data. Note however that these statistics are not ad-
justed for accuracy of individual players; it may be
that only more accurate players play Categodzilla.
Effect of Category Type on Data Quality.
Within each type of category (see Table 1), cer-
tain categories appear much more often than oth-
ers due to the way categories are selected (at least
two ?easy? categories are guaranteed every game).
To adjust for this, we built a subset of 200 cat-
egory/answer pairs by selecting 25 different cate-
gories randomly from each type of category. We
then selected an answer at random from among the
answers submitted for that category. In addition, we
built a control set using the same 200 categories but
instead using answers selected at random from the
entire Categorilla data set. Results for Categorilla
data are shown in Figure 3; we omit the correspond-
ing graph for control for lack of space. For most
categories, the Categorilla data is significantly bet-
ter than the control. The hand-written category type,
O, has the best data quality, which is not surpris-
ing because these categories allow the most possible
answers, and thus are easiest of think of answers for.
These categories also have the highest number of ?y?
labels for the control. Next best are the hypernym
categories, NType. NType is much higher than the
other noun hypernym category NHyp because the
?Type of? phrasing is generally more natural and al-
lows for adjectival answers. The VPP category type,
which tries to extract prepositional objects, contains
538
Data Set Letters Size y k n
Control Easy 127 .14 .08 .78
Control Hard 72 .15 .06 .79
Categorilla Easy 106 .45 .14 .41
Categorilla Hard 94 .30 .15 .55
Table 5: Accuracy of easy letters vs. hard letters. Size is
the number of answers for that row.
the most number of ?k? annotations; this is because
players often put answers that are subjects or ob-
jects of the verb, such as ?pizza? for ?Things that
are eaten with?. The adjective category type, Adj,
has the lowest increase over the control; this is likely
due to the nature of the extracted adjectives.
Effect of Initial Letter on Data Quality. In
general, we would expect common initial letters to
yield better data since there are more possible an-
swers to choose from. We did not have enough la-
beled data to do letter by letter statistics. Instead, we
broke the letters into two groups, based on the em-
pirical difficulty of obtaining matches when given
that initial letter. The easy letters were ?abcfhlmn-
prst?, while the hard letters were ?degijkouvwy?. Ta-
ble 5 shows the results on Categorilla-Random and
Control-Random on these two subsets. First, note
that the results on Control-Random are the same for
hard letters and easy letters. This means that words
starting with common letters are not more likely to
fit in a category. For both hard letters and easy let-
ters, the accuracy is considerably better on the Cat-
egorilla data. However, the increase in the number
of ?y? labels for easy letters is twice that for hard
letters. The quality of data for hard letters is consid-
erably worse than that for easy letters.
8 Free Association Data
In contrast to Categorilla and even Categodzilla, we
found that the Free Association data was quite clean.
However, it is also not structured; we simply get
pairs of related words. Thus, the essential question
for this game is what kind of data we get.
To analyze the types of relationships between
words, the authors labeled 500 randomly extracted
unique pairs with a rich set of word-word relations,
described in Table 6. This set of relations was de-
signed to capture the observed relationships encoun-
tered in the Free Association data. Unlike our Cat-
egorilla labeled set, pairs that occurred more than
once were NOT more likely to be selected than pairs
that occurred once (i.e., the category/answer pairs
were aggregated prior to sampling). Sampling in this
way led to more diversity in the pairs extracted.
To label each pair, the authors found a sequence
of relationships which connected the two words. In
many cases, this was a single link. For example,
?dragon? and ?wing? are connected by a single link,
?wing? IS PART OF ?dragon?. In others, multiple
links were required. For the seed word ?dispute? and
answer ?arbitrator?, we can connect using two links:
?dispute? IS OBJECT OF ?resolve?, ?arbitrator? IS
SUBJECT OF ?resolve?. There were two other pos-
sible ways to label a pair. First, they might be totally
unrelated (i.e., a bad answer). Second, they might
be related, but not connectable using our set of basic
relations. For example, ?echo? is clearly related to
?valley?, but in a complicated way.
The quality of the data is considerably higher than
Categorilla and Categodzilla; under 10% of words
are unrelated. Slightly over 20% of the pairs are la-
beled Misc, i.e., the words are related but in a com-
plicated way. 3% of the pairs can be linked with a
chain of two simple relations. The remaining 67%
of all pairs were linked with a single simple relation.
The category Desc deserves some discussion.
This category included both simple adjective de-
scriptions, such as ?creek? and ?noisy?, and also
qualifiers, such as ?epidemic? and ?typhoid?, where
one word specifies what kind of thing the other is.
The distinction between Desc and Phrase was sim-
ply based on to what extent the combination of the
two words was a set phrase (such as ?east? and ?Ger-
many?).
Schulte im Walde et al (2008) address very sim-
ilar issues to those discussed in this section. They
built a free association data set containing about
200,000 German word pairs using a combination of
online and offline volunteers (but not a game). They
then analyze the resulting associations by comparing
the resulting pairs to a large-scale lexical resource,
GermaNet (the German counterpart of WordNet).
Our data analysis was by hand, making it compar-
atively small scale but more detailed. It would be
interesting to compare the data sets to see whether
the use of a game affects the resulting data.
9 Filtering Bad Data
In this section, we consider a simple heuristic for
filtering bad data: only retaining answers that were
539
Name # Description Example
Misc 103 Words related, but in a complicated way ?echo?, ?valley?
Desc 76 One of the words describes the other ?cards?, ?business?
None 47 Words are not related ?congress?,?store?
Syn 46 The words are synonyms ?downturn?, ?dip?
Obj 33 One word is the object of the other ?exhale?,?emission?
Hyp 30 One word is an example of the other ?cabinet?,?furniture?
?Syn 29 The words are ?approximate? synonyms ?maverick?,?outcast?
Cousin 21 The words share a common hypernym (is-a) relation ?meter?,?foot?
Has 18 One word ?has? the other ?supermarket?,?carrots?
2-Chain 15 Words are linked by a chain of two simple relations ?arbitrator?,?dispute?
Phrase 13 Words make a phrase; similar to Desc ?East?, ?Germany?
Part 11 One is a part of the other ?dragon?,?wings?
At 10 One is found at the other ?harbor?, ?lake?
Subj 8 One is the subject of the other ?actor?, ?pretend?
Form 7 One is a form of the other ?revere?,?reverence?
Def 7 One defines the other ?blind?,?unable to see?
Opp 7 The two are opposites ?positive?,?negative?
Sound 6 The two words sound similar ?boutique?,?antique?
Sub 5 One is a subword of the other ?outlet?, ?out?
Unit 2 One is a unit of the other ?reel?,?film?
Made 2 One is made of the other ?knee?,?bone?
Table 6: Relation types for 500 hand-labeled examples. # indicates the number of pairs with that label.
guessed some minimum number of times. Note that
in this section all answers were stemmed in order to
combine counts across plurals and verb tenses.
For the Categorilla data, filtering out cate-
gory/answer pairs that only occurred once from
Categorilla-Random left a total of 64 answers (from
an original 200), of which 36 were labeled ?y? and 8
were labeled ?k?. The fraction of ?y? labels in the
reduced set is 56%, up from 38% in the original
set. This gain in quality comes at the cost of losing
slightly over two-thirds of the data.
For Categodzilla-Random, a similar filter left 88
(out of 200), with 79 labeled ?y? and 7 labeled ?k?.
For the hand-labeled Free Association data, apply-
ing this filter yielded a total of 123 pairs (out of an
original 500), with only 2 having no relation5. In
these two games, this filter eliminates nearly all bad
data while keeping a reasonable fraction of the data.
Clearly, this filter is less effective for Catego-
rilla than the other two games. One of the main
reasons for this is that the letter constraints cause
5The higher fraction of lost pairs for Free Association is pri-
marily due to the method of sampling pairs for evaluation, as
discussed in Section 8.
people to try to fit words starting with that letter
into all categories that they even vaguely relate to,
rather than thinking of words that really fit that cat-
egory. Examples include {?Art supplies?,?jacket?},
{?Things found in Chicago?,?king?} and {?Things
that are African?,?yak?}. Of course, we can further
increase the quality of the data by making the fil-
ter more restrictive, at the cost of losing more data.
For example, removing answers occuring fewer than
5 times from Categorilla-Random leaves only 8 an-
swers (out of 200), 7 labeled ?y? and 1 labeled ?n?.
There are other ways we could filter the data. For
example, suppose we are given an outside database
of pairs of words which are known to be semanti-
cally related. We could apply the following heuris-
tic: if an answer to a particular category is similar to
many other answers for that category, then that an-
swer is likely to be a good one. Preliminary experi-
ments using distributional similarity of words as the
similarity metric suggest that this heuristic captures
complimentary information to the guess frequency
heuristic. We leave as future work a full integration
of the two heuristics into a single improved filter.
540
Classified Type # Example
Real hypernyms 96 ?equipment?,?racquet?
Compound hypernyms 32 ?arrangement?,?flower?
Adjectives 25 ?building?,?old?
Sort-of hypernyms 14 ?vegetable?,?salad?
Not hypernyms 33 ?profession?,?money?
Table 7: Breakdown of potential hypernym pairs
10 Using the Data
Categorilla and Categodzilla produce structured data
which is already in a usable or nearly usable form.
For example, the NHyp and NType categories pro-
duce lists of hypernyms, which could be used to aug-
ment WordNet. We looked at this particular applica-
tion in some detail.
First, in order to remove noisy data, we used
only Categodzilla data and removed answers which
occurred only once. We took all category/answer
pairs where the category was of type either NHyp or
NType, and where the answer was a noun. This re-
sulted in 1604 potential hypernym/hyponym pairs.
Of these, 733 (or 46%) were already in WordNet.
The remaining 871 were not found in WordNet. We
then hand-labeled a random subset of 200 of the 871
to determine how many of them were real hyper-
nym/hyponym pairs. The results are shown in Ta-
ble 7. Counting compound hyponyms, nearly two-
thirds of the pairs are real hypernym/hyponym pairs.
These new pairs could directly augment WordNet.
For example, for the word ?crime?, WordNet has
as hyponyms ?burglary? and ?fraud?. However,
it doesn?t have ?arson?, ?homicide?, or ?murder?,
which are among the 871 new pairs. WordNet lists
?wedding? as being an ?event?, but not ?birthday?.
The verb subject, object, and prepositional object
categories were designed to collect data about the
selectional preferences of verbs. These categories
turned out to be problematic for several reasons.
First, statistics about selectional preferences of verbs
are not too difficult to extract from the web (although
in some cases they might be somewhat noisy). Thus,
the motivation for extracting this data using a game
is not as apparent. Second, providing arguments of
verbs out of the context of a sentence may be too dif-
ficult. For example, for the category ?Things that are
accumulated?, there a couple of obvious answers,
such as ?wealth? or ?money?, but beyond these it
becomes more difficult. In the context of an actual
document, quite a lot of things can accumulate, but
outside of that context it is difficult to think of them.
One solution to this problem would be to provide
context. For example, the category ?Things that ac-
cumulate in your body? is both easier to think of
answers for and probably collects more useful data.
However, automatically creating categories with the
right level of specificity is not a trivial task; our ini-
tial experiments suggested that it is easy to gener-
ate too much context, creating an uninteresting cat-
egory.
The Free Association game produces a lot of very
clean data, but does not classify the relationships be-
tween the words. While a web of relationships might
be useful by itself, classifying the pairs by relation
type would clearly be valuable. Snow et al (2006)
and Nakov and Hearst (2008), among others, look at
using a large amount of unlabeled data to classify
relations between words. One issue with extract-
ing new relations from text, for example meronyms
(part-of relationships), is that they tend to occur
fairly rarely. Thus, it is very easy to get a large num-
ber of spurious pairs. Using our data as a set of can-
didate pairs for relation extraction could greatly re-
duce the resulting noise. We believe that application
of existing techniques to the data from the Free As-
sociation game could lead to a clean, classified set of
word-word relations, but leave this as future work.
11 Discussion and Future Work
One way to extend Categorilla and Categodzilla
would be to add additional types of categories. For
example, a meronym category type (e.g. ?Parts of a
car?) would work well. Further developing the verb
categories (e.g., ?Things that accumulate in your
body?) is another challenging but interesting direc-
tion; these categories would produce phrase-word
relationships rather than word-word relationships.
Probably the most interesting direction for future
work is trying to increase the complexity of the data
collected from a game. There are two significant dif-
ficulties: keeping the game fun, and making sure the
collected data is not too noisy. One interesting ques-
tion for future research is whether different game ar-
chitectures might be better suited to certain kinds
of data. For example, a ?telephone? style game,
where players relay a phrase or sentence through
some noisy channel, might be an interesting way to
obtain paraphrase data.
541
References
Chklovski, T. (2003). Using analogy to acquire com-
monsense knowledge from human contributors.
Thesis.
Fellbaum, C. (Ed.). (1998). Wordnet: An electronic
lexical database. MIT Press.
Gildea, D., & Jurafsky, D. (2002). Automatic label-
ing of semantic roles. Computational Linguistics.
Kingsbury, P., Palmer, M., & Marcus, M. (2002).
Adding semantic annotation to the penn treebank.
Proceedings of the Human Language Technology
Conference (HLT?02).
Marcus, M., Marcinkiewicz, M., & Santorini, B.
(1993). Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguis-
tics.
Nakov, P., & Hearst, M. (2008). Solving relational
similarity problems using the web as a corpus.
Proceedings of ACL.
Schulte imWalde, S., Melinger, A., Roth, M., &We-
ber, A. (2008). An empirical characterisation of
response types in german association norms. To
appear, Research on Language and Computation.
Snow, R., Jurafsky, D., & Ng, A. (2006). Semantic
taxonomy induction from heterogenous evidence.
Proceedings of COLING/ACL.
von Ahn, L., & Dabbish, L. (2004). Labeling images
with a computer game. ACM CHI.
von Ahn, L., Kedia, M., & Blum, M. (2006). Ver-
bosity: a game for collecting common-sense facts.
Proceedings of the SIGCHI conference on Human
Factors in computing systems.
542
Automatic Semantic Grouping in a Spoken Language User Interface 
Toolkit 
 
Hassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, Huy 
Nguyen, Fuad Rahman, Yuliya Tarnikova, Timotius Tjahjadi and Che Wilcox 
 
BCL Technologies Inc. 
Santa Clara, CA 95050 U.S.A. 
fuad@bcltechnologies.com 
 
 
Abstract 
With the rapid growth of real 
application domains for NLP systems, 
there is a genuine demand for a general 
toolkit from which programmers with no 
linguistic knowledge can build specific 
NLP systems. Such a toolkit should 
provide an interface to accept sample 
sentences and convert them into 
semantic representations so as to allow 
programmers to map them to domain 
actions. In order to reduce the workload 
of managing a large number of semantic 
forms individually, the toolkit will 
perform what we call semantic grouping 
to organize the forms into meaningful 
groups. In this paper, we present three 
semantic grouping methods: similarity-
based, verb-based and category-based 
grouping, and their implementation in 
the SLUI toolkit. We also discuss the 
pros and cons of each method and how 
they can be utilized according to the 
different domain needs. 
1 Introduction and Motivation 
With the improvement of natural language 
processing (NLP) and speech recognition 
techniques, spoken language will become the 
input of choice for software user interfaces, as 
it is the most natural way of communication. In 
the mean time, the domains for NLP systems, 
especially those handling speech input, have 
grown rapidly in recent years. However, most 
computer programmers do not have enough 
linguistic knowledge to develop an NLP 
system to handle speech input. There is a 
genuine demand for a general toolkit from 
which programmers with no linguistic 
knowledge can rapidly build speech based 
NLP systems to handle their domain specific 
problems more accurately (Alam, 2000). The 
toolkit will allow programmers to generate 
Spoken Language User Interface (SLUI) front 
ends for new and existing applications using, 
for example, a program-through-example 
method. In this methodology, the programmer 
will specify a set of sample input sentences or 
a domain corpus for each task. The toolkit will 
then organize the sentences by meaning and 
even generate a large set of syntactic variations 
for a given sentence. It will also generate the 
code that takes a user?s spoken request and 
executes a command on an application. This 
methodology is similar to using a GUI toolkit 
to develop a graphical user interface so that 
programmers can develop GUI without 
learning graphics programming. Currently this 
is an active research area, and the present work 
is funded by the Advanced Technology 
Program (ATP) of the National Institute of 
Standards and Technology (NIST). 
In the program-through-example approach, 
the toolkit should provide an interface for the 
programmers to input domain specific corpora 
and then process the sentences into semantic 
representations so as to capture the semantic 
meanings of the sentences. In a real world 
application, this process results in a large 
number of semantic forms. Since the 
programmers have to manually build the links 
between these forms and their specific domain 
actions, they are likely to be overwhelmed by 
the workload imposed by the large number of 
individual semantic forms. In order to 
significantly reduce this workload, we can 
organize these forms in such a way so that the 
programmers can manipulate them as groups 
rather than as individual items. This will speed 
up the generation process of the domain 
specific SLUI system. We call this process the 
semantic grouping process. 
One straightforward way to group is to 
organize different syntactic forms expressing 
the same meaning together. For example,  
 
(1.1) I want to buy this book online. 
(1.2) Can I order this book online? 
(1.3) How can I purchase this book online? 
(1.4) What do I need to do to buy this book 
online?  
 
The semantic forms of the above sentences 
may not be the same, but the action the 
programmer has in mind in an e-business 
domain is more or less the same: to actually 
buy the book online. In addition to the above 
sentences, there are many variations that an 
end-user might use. The embedded NLP 
system should be able to recognize the 
similarity among the variations so that the 
SLUI system can execute the same command 
upon receiving the different queries. This 
requires a group to contain only sentences with 
the same meaning. However in real 
applications, this might be difficult to achieve 
because user requests often have slight 
differences in meaning. 
This difficulty motivates a different style 
for semantic grouping: organizing the semantic 
forms into groups so that those in the same 
group can be mapped roughly to the same 
action. The action can be either a command, 
e.g., buy something, or concerning an object, 
e.g., different ways of gathering information 
about an object. For example, sentence (1.5) 
would be grouped together with the above 
example sentences because it poses the same 
request: buy books; and sentences (1.6) to (1.8) 
would be in one group because they are all 
about price information. 
 
(1.5) I want to buy the latest book about e-
business. 
 
(1.6) Please send me a price quote. 
(1.7) What is the reseller price? 
(1.8) Do you have any package pricing for 
purchasing multiple products at once? 
This type of grouping is the focus of this 
paper. We propose three grouping methods: 
similarity-based grouping, verb-based 
grouping and category-based grouping. The 
process of grouping semantic forms is domain 
dependent and it is difficult to come up with a 
generally applicable standard to judge whether 
a grouping is appropriate or not. Different 
grouping techniques can give programmers 
different views of their data in order to satisfy 
different goals.  
This paper is organized into 6 sections. In 
Section 2, we briefly describe the system for 
which the grouping algorithms are proposed 
and implemented. Section 3 presents the three 
grouping methods in detail. In Section 4, we 
describe how the algorithms are implemented 
in our system. We test the methods using a set 
a sentences from our corpus and discuss the 
pros and cons of each method in Section 5. 
Finally, in Section 6, we draw conclusions and 
propose some future work. 
2 SLUITK 
As mentioned in the previous section, the 
Spoken Language User Interface Toolkit 
(SLUITK) allows programmers with no 
linguistic knowledge to rapidly develop a 
spoken language user interface for their 
applications. The toolkit should incorporate 
the major components of an NLP front 
end, such as a spell checker, a parser and a 
semantic representation generator. Using 
the toolkit, a programmer will be able to create 
a system that incorporates complex NLP 
techniques such as syntactic parsing and 
semantic understanding.  
2.1 The Work Flow 
Using an Automatic Speech Recognition 
(ASR) system, the SLUITK connects user 
input to the application, allowing spoken 
language control of the application. The 
SLUITK generates semantic representations of 
each input sentence. We refer to each of these 
semantic representations as a frame, which is 
basically a predicate-argument representation 
of a sentence.  
The SLUITK is implemented using the 
following steps: 
1. SLUITK begins to create a SLUI by 
generating semantic representations of 
sample input sentences provided by the 
programmer. 
2. These representations are expanded using 
synonym sets and other linguistic devices, 
and stored in a Semantic Frame Table 
(SFT). The SFT becomes a 
comprehensive database of all the 
possible commands a user could request a 
system to do. It has the same function as 
the database of parallel translations in an 
Example-based machine translation 
system (Sumita and Iida, 1991). 
3. The toolkit then creates methods for 
attaching the SLUI to the back end 
applications. 
4. When the SLUI enabled system is 
released, a user may enter an NL 
sentence, which is translated into a 
semantic frame by the system. The SFT is 
then searched for an equivalent frame. If a 
match is found, the action or command 
linked to this frame is executed. 
 
In a real application, a large number of 
frames might be generated from a domain 
corpus. The semantic grouper takes the set of 
frames as the input and outputs the same 
frames organized in a logical manner.  
2.2 The Corpus 
We use a corpus of email messages from our 
customers for developing and testing the 
system. These email messages contain 
questions, comments and general inquiries 
regarding our document-conversion products.  
We modified the raw email programmatically 
to delete the attachments, HTML and other 
tags, headers and sender information. In 
addition, we manually deleted salutations, 
greetings and any information that was not 
directly related to customer support. The 
corpus contains around 34,640 lines and 
170,000 words. We constantly update it with 
new email from our customers.  
We randomly selected 150 sentential 
inquiries to motivate and test the semantic 
grouping methods discussed in this paper. 
3 Semantic Grouping 
We have mentioned in Section 1 that grouping 
semantic frames is domain dependent.  
Grouping depends on the nature of the 
application and also the needs of the domain 
programmer. Since this is a real world 
problem, we have to consider the efficiency of 
grouping. It is not acceptable to let the 
programmer wait for hours to group one set of 
semantic forms. The grouping should be fairly 
fast, even on thousands of frames. 
These different considerations motivate 
several grouping methods: similarity-based 
grouping, verb-based grouping and category-
based grouping. In this section, we describe 
each of these methods in detail. 
3.1 Similarity-based Grouping  
Similarity-based grouping gathers sentences 
with similar meanings together, e.g., sentences 
(1.1) to (1.4). There is a wide application for 
this method. For example, in open domain 
question-answering systems, questions need to 
be reformulated so that they will match 
previously posted questions and therefore use 
the cached answers to speed up the process 
(Harabagiu et al, 2000).   
The question reformulation algorithm of 
Harabagiu et al tries to capture the similarity 
of the meanings expressed by two sentences. 
For a given set of questions, the algorithm 
formulates a similarity matrix from which 
reformulation classes can be built. Each class 
represents a class of equivalent questions. 
The algorithm for measuring the similarity 
between two questions tries to find lexical 
relationships between every two questions that 
do not contain stop words. The algorithm 
makes use of the WordNet concept hierarchy 
(Fellbaum, 1998) to find synonym and 
hypernym relations between words. 
This algorithm does not infer information 
about the meanings of the questions, but rather 
uses some kind of similarity measurement in 
order to simulate the commonality in meaning. 
This is a simplified approach. Using different 
threshold, they can achieve different degrees of 
similarity, from almost identical to very 
different. 
This method can be used for similarity-
based grouping to capture the similarity in 
meanings expressed by different sentences. 
3.2 Verb-based Grouping 
Among the sentences normally used in the e-
business domain, imperative sentences often 
appear in sub-domains dominated by 
command-and-control requests. In such an 
application, the verb expresses the command 
that the user wants to execute and therefore 
plays the most important role in the sentence. 
Based on this observation, a grouping can be 
based on the verb or verb class only. For 
example, sentences with buy or purchase etc. 
as the main verbs are classified into one group 
whereas those with download as the main verb 
are classified into a different group, even when 
the arguments of the verbs are the same. 
This is similar to sorting frames by the 
verb, taking into account simple verb synonym 
information.  
3.3 Category-based Grouping 
Since SLUITK is a generic toolkit whereas the 
motivation for grouping is application 
dependent, we need to know how the 
programmer wants the groups to be organized. 
We randomly selected 100 sentences from our 
corpus and asked two software engineers to 
group them in a logical order. They came up 
with very different groups, but their thoughts 
behind the groups are more or less the same. 
This motivates the category-based grouping. 
This grouping method puts less emphasis 
on each individual sentence, but tries to 
capture the general characteristics of a given 
corpus.  For example, we want to group by the 
commands (e.g., buy) or objects (e.g., a 
software) the corpus is concerned with. If a 
keyword of a category appears in a given 
sentence, we infer that sentence belongs to the 
category. For example, sentences (1.6) to (1.8) 
will be grouped together because they all 
contain the keyword price. 
These sentences will not be grouped 
together by the similarity-based method 
because their similarity is not high enough, nor 
by the verb-based method because the verbs 
are all different. 
4 Grouping in SLUITK 
Because we cannot foresee the domain needs 
of the programmer, we implemented all three 
methods in SLUITK so that the programmer 
can view their data in several different ways.  
The programmer is able to choose which type 
of grouping scheme to implement. 
In the question reformulation algorithm of 
(Harabagiu, et al 2000), all words are treated 
identically in the question similarity 
measurement. However, our intuition from 
observing the corpus is that the verb and the 
object are more important than other 
components of the sentence and therefore 
should be given more weight when measuring 
similarity. In Section 4.1, we describe our 
experiment with the grouping parameters to 
test our intuition. 
4.1 Experimenting with Parameters 
We think that there are two main 
parameters affecting the grouping result: the 
weight of the syntactic components and the 
threshold for the similarity measurement in the 
similarity-based method. Using 100 sentences 
from our corpus, we tried four different types 
of weighting scheme and three thresholds with 
the category-based methods. Human judgment 
on the generated groups confirmed our 
intuition that the object plays the most 
important role in grouping and the verb is the 
second most important. The differences in 
threshold did not seem to have a significant 
effect on the similarity-based grouping.  This 
is probably due to the strict similarity 
measurement. 
This experiment gives us a relatively 
optimal weighting scheme and threshold for 
the similarity-based grouping. 
One relevant issue concerns the 
simplification of the semantic frames. For a 
sentence with multiple verbs, we can simplify 
the frame based on the verbs used in the 
sentence. The idea is that some verbs such as 
action verbs are more interesting in the e-
business domain than others, e.g., be and have. 
If we can identify such differences in the verb 
usage, we can simplify the semantic frames by 
only keeping the interesting verb frames. For 
example, in the following sentences, the verb 
buy is more interesting than be and want, and 
the generated semantic frames should contain 
only the frame for buy. 
 
(4.1) Is it possible to buy this software online? 
(4.2) I want to buy this software online.  
 
Figure 1: A screen shot of SLUITK 
We make use of a list of stop-words from 
(Frakes, 1992) in order to distinguish between 
interesting and uninteresting verbs. We look 
for frames headed by stop-words and follow 
some heuristics to remove the sub-frame of the 
stop-word. For example, if there is at least one 
verb that is not a stop-word, we remove all 
other stop-words from the frame. In the 
sentence [Is it possible to] buy the software in 
Germany?, be is a stop-word, so only the 
frame for buy is kept. This process removes the 
redundant part of a frame so that the grouping 
algorithm only considers the most important 
part of a frame. 
4.2 Implementation in SLUITK 
Figure 1 shows a screen shot of the interface of 
the SLUITK, which shows several grouped 
semantic frames. In this section, we give more 
detail about the implementation of the three 
grouping methods used in SLUITK.  
 
Similarity-based grouping 
 
Similar to (Harabagiu, et al 2001), our 
similarity-based grouping algorithm calculates 
the similarity between every two frames in the 
input collection. If the similarity is above a 
certain threshold, the two frames are 
considered similar and therefore should be 
grouped together. If two frames in two 
different groups are similar, then the two 
groups should be combined to a single group. 
The central issue here is how to measure the 
similarity between two frames. 
Since we have found that some syntactic 
components are more important to grouping 
than others, we use a weighted scheme to 
measure similarity. For each frame, all words 
(except for stop-words) are extracted and used 
for similarity calculation. We give different 
weights to different sentence components. 
Since in an e-business domain, the verb and 
the object of a sentence are usually more 
important than other components because they 
express the actions that the programmers want 
to execute, or the objects for which they want 
to get more information, the similarity of these 
components are emphasized through the 
weighting scheme. The similarity score of two 
frames is the summation of the weights of the 
matched words.  
There is a match between two words when 
we find a lexical relationship between them. 
We extend the method of (Harabagiu, et al 
2000) and define a lexical relationship between 
two words W1 and W2 as in the following: 
Table 1 : Comparison of grouping methods 
 
 
1. If W1 and W2 have a common 
morphological root. Various stemming 
packages can be used for this purpose, for 
example, Porter Stemmer (Porter, 1997). 
2. If W1 and W2 are synonyms, i.e., W2 is 
in the WordNet synset of W1. 
3. If the more abstract word is a WordNet 
hypernym of the other. 
4. If one word is the WordNet holonym of 
the other (signaling part of, member of 
and substance of relations); 
5. If W1 is the WordNet antonym of W2.  
 
Domain specific heuristics can also be used 
to connect words. For example, in the e-
business domain, you and I can be treated as 
antonyms in the following sentences: 
 
(4.3) Can I buy this software? 
(4.4) Do you sell this software? 
 
When none of the above is true, there is no 
lexical relation between two given words. 
Because the similarity-based grouping 
needs to consult WordNet frequently for 
lexical relations, it becomes very slow for even 
a few hundred frames. We have to change the 
algorithm to speed up the process, as it is too 
slow for real world applications.  
Instead of comparing every two frames, we 
put all the words from an existing group 
together. When a new frame is introduced, we 
compare the words in this new frame with the 
word collection of each group. The similarity 
scores are added up as before, but it needs to 
be normalized over the number of words in the 
collection. When the similarity is above a 
certain threshold, the new frame is classified as 
a member of the group. This significantly 
reduces the comparison needed for classifying 
a frame, and therefore reduces the number of 
times WordNet needs to be consulted. 
We compared this improved algorithm with 
the original one on 30 handcrafted examples; 
the generated groups are very similar. 
 
Verb-based grouping 
 
The verb-based grouping implementation is 
fairly straightforward and has been described 
in Section 3.2. 
 
Category-base grouping 
 
For the category-based method, we first count 
all the non stop-words in a given corpus and 
retrieve a set of most frequent words and their 
corresponding word classes from the corpus. 
This process also makes use of the WordNet 
synonym, hypernym, holonym and antonym 
information. These word classes form the 
categories of each group. We then check the 
verbs and objects of each sentence to see if 
they match these words. That is, if a category 
word or a lexically related word appears as the 
verb or the object of a sentence, the sentence is 
classified as a member of that group. For 
example, we can pick the most frequent 20 
words and divide the corpus into 21 groups, 
where the extra group contains all sentences 
that cannot be classified. The programmer can 
decide the number of groups they want. This 
gives the programmer more control over the 
grouping result. 
5 Discussion 
We tested the three methods on 100 sentences 
from our corpus. We had 5 people evaluate the 
generated groups. They all thought that 
grouping was a very useful feature of the 
toolkit. Based on their comments, we 
summarize the pros and cons of each method 
in Table 1. 
The similarity-based grouping produces a 
large number of groups, most of which contain 
only one sentence. This is because there are 
usually several unrelated words in each 
sentence, which decreases the similarity 
scores. In addition, using WordNet we 
sometimes miss the connections between 
lexical items. The verb-based grouping 
 Similarity-based Verb-based Category-based 
Group Size small small large 
Number of Groups large large variable 
Speed slow on large corpus fast slow on large corpus 
Application general command-and-control only general 
produces slightly larger groups, but also 
produces many single sentence groups. 
Another problem is that when sentences 
contain only stop-word verbs, e.g., be, the 
group will look rather arbitrary. For example, a 
group of sentences with be as the main verb 
can express completely different semantic 
meanings. The small group size is a 
disadvantage of both methods. The number of 
groups of the category-based grouping can 
change according to the user specification. In 
general it produces less groups than the other 
methods and the group size is much larger, but 
the size becomes smaller for less frequent 
category words. 
Both the similarity-based and category-
based grouping methods are slow because they 
frequently need to use WordNet to identify 
lexical relationships. The verb-based method is 
much faster, which is the primary advantage of 
this method. 
The verb-based method should be used in a 
command-and-control domain because it 
requires at least one non stop-word verb in the 
sentence. However, it will have a hard time in 
a domain that needs to handle questions. From 
the point of view of assigning a domain 
specific action to a group, this grouping is the 
best because each verb can be mapped to an 
action.  Therefore, the programmer can link an 
action to each group rather than to each 
individual frame. When the group size is 
relatively large, this can greatly reduce the 
workload of the programmer.  
The category-based method produces a 
better view of the data because the sentences in 
each group seem to be consistent with the 
keywords of the category. The disadvantage is 
that it is difficult to link a group to a single 
action, and the programmer might have to re-
organize the groups during action assignment.  
The similarity-based method did not 
perform well on the testing corpus, but it might 
work better on a corpus containing several 
different expressions of the same semantic 
information. 
In summary, each method has its 
advantages and disadvantages. The decision of 
which one to choose depends mainly on the 
needs of the domain programmer and the 
composition of the input corpus. 
 
6 Conclusions and Future Work 
In this paper we propose semantic grouping as 
a way to solve the problem of manipulating 
semantic frames in developing a general 
Spoken Language User Interface Toolkit 
(SLUITK). We introduced three methods for 
grouping semantic frames generated by the 
NLP components of the toolkit. We tested the 
methods and discussed the advantages and 
disadvantages of each method. Since the 
judgment of the grouping result is application 
dependent, the methods co-exist in our 
SLUITK to suit the requirement of different 
applications. 
Future work includes improving the 
efficiency and accuracy of the methods and 
testing them on a larger corpus. 
 
References 
Alam H. (2000) Spoken Language Generic 
User Interface (SLGUI). Technical Report 
AFRL-IF-RS-TR-2000-58, Air Force Research 
Laboratory, Rome.  
Fellbaum C. (1998) WordNet, An 
Electronic Lexical Database, The MIT Press, 
Cambridge, Massachusetts. 
Frakes W. and Baeza-Yates R. (1992) 
Information Retrieval, Data Structures and 
Algorithms, Prentice-Hall. 
HaraBagiu S. and Moldovan D. and Pasca 
M. and Mihalcea R. and Surdeanu M. and 
Bunescu R. and Girju R. and Rus V. and 
Morarescu P. (2000) FALCON: Boosting 
Knowledge for Answer Engines, TREC 9. 
Porter M. (1997) An algorithm for suffix 
stripping, in Readings in Information 
Retrieval, Karen Sparck Jones and Peter Willet 
(ed), San Francisco: Morgan Kaufmann. 
Sumita E. and Iida H. (1991) Experiments 
and Prospects of Example-Based Machine 
Translation. In Proceedings of the Annual 
Meeting of the Association for Computational 
Linguistics, pp. 185-192. 
 
Extending A Broad-Coverage Parser for a General NLP Toolkit 
 
Hassan Alam, Hua Cheng, Rachmat Hartono, Aman Kumar, Paul Llido, Crystal Nakatsu, Fuad 
Rahman, Yuliya Tarnikova, Timotius Tjahjadi and Che Wilcox  
 
BCL Technologies Inc. 
Santa Clara, CA 95050 U.S.A. 
fuad@bcltechnologies.com 
 
Abstract 
With the rapid growth of real world 
applications for NLP systems, there 
is a genuine demand for a general 
toolkit from which programmers 
with no linguistic knowledge can 
build specific NLP systems. Such a 
toolkit should have a parser that is 
general enough to be used across 
domains, and yet accurate enough for 
each specific application. In this 
paper, we describe a parser that 
extends a broad-coverage parser, 
Minipar (Lin, 2001), with an 
adaptable shallow parser so as to 
achieve both generality and accuracy 
in handling domain specific NL 
problems. We test this parser on our 
corpus and the results show that the 
accuracy is significantly higher than 
a system that uses Minipar alone. 
1 Introduction 
With the improvement of natural language 
processing (NLP) techniques, domains for 
NLP systems, especially those handling speech 
input, are rapidly growing. However, most 
computer programmers do not have enough 
linguistic knowledge to develop NLP systems. 
There is a genuine demand for a general toolkit 
from which programmers with no linguistic 
knowledge can rapidly build NLP systems that 
handle domain specific problems more 
accurately (Alam, 2000). The toolkit will allow 
programmers to generate natural language 
front ends for new and existing applications 
using, for example, a program-through-
example method. In this methodology, the 
programmer will specify a set of sample input 
sentences or a domain corpus for each task. 
The toolkit will then organize the sentences by 
similarity and generate a large set of syntactic 
variations of a given sentence. It will also 
generate the code that takes a user?s natural 
language request and executes a command on 
an application. Currently this is an active 
research area, and the Advanced Technology 
Program (ATP) of the National Institute of 
Standards and Technology (NIST) is funding 
part of the work.  
In order to handle natural language input, 
an NLP toolkit must have a parser that maps a 
sentence string to a syntactic structure. The 
parser must be both general and accurate. It 
has to be general because programmers from 
different domains will use the toolkit to 
generate their specific parsers. It has to be 
accurate because the toolkit targets commercial 
domains, which usually require high accuracy.  
The accuracy of the parser directly affects the 
accuracy of the generated NL interface. In the 
program-through-example approach, the 
toolkit should convert the example sentences 
into semantic representations so as to capture 
their meanings. In a real world application, this 
process will involve a large quantity of data. If 
the programmers have to check each syntactic 
or semantic form by hand in order to decide if 
the corresponding sentence is parsed correctly, 
they are likely to be overwhelmed by the 
workload imposed by the large number of 
sentences, not to mention that they do not have 
the necessary linguistic knowledge to do this. 
Therefore the toolkit should have a broad-
coverage parser that has the accuracy of a 
parser designed specifically for a domain. 
One solution is to use an existing parser 
with relatively high accuracy.  Using existing 
parsers such as (Charniak, 2000; Collins, 
1999) would eliminate the need to build a 
parser from scratch. However, there are two 
problems with such an approach. First, many 
parsers claim high precision in terms of the 
number of correctly parsed syntactic relations 
rather than sentences, whereas in commercial 
applications, the users are often concerned 
with the number of complete sentences that are 
parsed correctly. The precision might drop 
considerably using this standard. In addition, 
although many parsers are domain 
independent, they actually perform much 
better in the domains they are trained on or 
implemented in. Therefore, relying solely on a 
general parser would not satisfy the accuracy 
needs for a particular domain.  
Second, since each domain has its own 
problems, which cannot be foreseen in the 
design of the toolkit, customization of the 
parser might be needed. Unfortunately, using 
an existing parser does not normally allow this 
option. One solution is to build another parser 
on top of the general parser that can be 
customized to address domain specific parsing 
problems such as ungrammatical sentences. 
This domain specific parser can be built 
relatively fast because it only needs to handle a 
small set of natural language phenomena. In 
this way, the toolkit will have a parser that 
covers wider applications and in the mean time 
can be customized to handle domain specific 
phenomena with high accuracy. In this paper 
we adopt this methodology. 
The paper is organized into 6 sections. In 
Section 2, we briefly describe the NLP toolkit 
for which the parser is proposed and 
implemented. Section 3 introduces Minipar, 
the broad-coverage parser we choose for our 
toolkit, and the problems this parser has when 
parsing a corpus we collected in an IT domain. 
In Section 4, we present the design of the 
shallow parser and its disadvantages. We 
describe how we combine the strength of the 
two parsers and the testing result in Section 5. 
Finally, in Section 6, we draw conclusions and 
propose some future work. 
2 NLP Toolkit 
In the previous section, we mentioned a 
Natural Language Processing Toolkit 
(NLPTK) that allows programmers with no 
linguistic knowledge to rapidly develop natural 
language user interfaces for their applications. 
The toolkit should incorporate the major 
components of an NLP system, such as a spell 
checker, a parser and a semantic representation 
generator. Using the toolkit, a software 
engineer will be able to create a system that 
incorporates complex NLP techniques such as 
syntactic parsing and semantic understanding.  
In order to provide NL control to an 
application, the NLPTK needs to generate 
semantic representations for input sentences. 
We refer to each of these semantic forms as a 
frame, which is basically a predicate-argument 
representation of a sentence.  
The NLPTK is implemented using the 
following steps: 
 
1. NLPTK begins to create an NLP front end 
by generating semantic representations of 
sample input sentences provided by the 
programmer. 
2. These representations are expanded using 
synonym sets and stored in a Semantic 
Frame Table (SFT), which becomes a 
comprehensive database of all the 
possible commands a user could request 
the system to do.  
3. The toolkit then creates methods for 
attaching the NLP front end to the back 
end applications. 
4. When the NLP front end is released, a user 
may enter an NL sentence, which is 
translated into a semantic frame by the 
system. The SFT is then searched for an 
equivalent frame. If a match is found, the 
action or command linked to this frame is 
executed. 
In order to generate semantic 
representations in Step 1, the parser has to 
parse the input sentences into syntactic trees. 
During the process of building an NLP system, 
the programmer needs to customize the parser 
of the toolkit for their specific domain. For 
example, the toolkit provides an interface to 
highlight the domain specific words that are 
not in the lexicon.  The toolkit then asks the 
programmer for information that helps the 
system insert the correct lexical item into the 
lexicon. The NLPTK development team must 
handle complicated customizations for the 
programmer. For example, we might need to 
change the rules behind the domain specific 
parser to handle certain natural language input. 
In Step 4, when the programmer finishes 
building an NLP application, the system will 
implement a domain specific parser. The 
toolkit has been completely implemented and 
tested.  
We use a corpus of email messages from 
our customers for developing the system. 
These emails contain questions, comments and 
general inquiries regarding our document-
conversion products. We modified the raw 
email programmatically to delete the 
attachments, HTML tags, headers and sender 
information. In addition, we manually deleted 
salutations, greetings and any information not 
directly related to customer support. The 
corpus contains around 34,640 lines and 
170,000 words. We constantly update it with 
new emails from our customers.  
From this corpus, we created a test corpus 
of 1000 inquiries to test existing broad-
coverage parsers and the parser of the toolkit. 
3 Minipar in NLPTK 
We choose to use Minipar (Lin, 2001), a 
widely known parser in commercial domains, 
as the general parser of NLPTK. It is worth 
pointing out that our methodology does not 
depend on any individual parser, and we can 
use any other available parser.  
3.1 Introduction to Minipar 
Minipar is a principle-based, broad-coverage 
parser for English (Lin, 2001). It represents its 
grammar as a network of nodes and links, 
where the nodes represent grammatical 
categories and the links represent types of 
dependency relationships. The grammar is 
manually constructed, based on the Minimalist 
Program (Chomsky, 1995). 
Minipar constructs all possible parses of an 
input sentence. It makes use of the frequency 
counts of the grammatical dependency 
relationships extracted by a collocation 
extractor (Lin, 1998b) from a 1GB corpus 
parsed with Minipar to resolve syntactic 
ambiguities and rank candidate parse trees. 
The dependency tree with the highest ranking 
is returned as the parse of the sentence.  
The Minipar lexicon contains about 
130,000 entries, derived from WordNet 
(Fellbaum, 1998) with additional proper 
names. The lexicon entry of a word lists all 
possible parts of speech of the word and its 
subcategorization frames (if any).  
Minipar achieves about 88% precision and 
80% recall with respect to dependency 
relationships (Lin, 1998a), evaluated on the 
SUSANNE corpus (Sampson, 1995), a subset 
of the Brown Corpus of American English. 
3.2 Disadvantages of Minipar 
In order to see how well Minipar performs in 
our domain, we tested it on 584 sentences from 
our corpus. Instead of checking the parse trees, 
we checked the frames corresponding to the 
sentences, since the accuracy of the frames is 
what we are most concerned with. If any part 
of a frame was wrong, we treated it as an error 
of the module that contributed to the error. We 
counted all the errors caused by Minipar and 
its accuracy in terms of correctly parsed 
sentences is 77.6%. Note that the accuracy is 
actually lower because later processes fix some 
errors in order to generate correct frames. 
The majority of Minipar errors fall in the 
following categories: 
 
1. Tagging errors: some nouns are mis-
tagged as verbs. For example, in Can I get 
a copy of the batch product guide?, guide 
is tagged as a verb. 
2. Attachment errors: some prepositional 
phrases (PP) that should be attached to 
their immediate preceding nouns are 
attached to the verbs. For example, in Can 
Drake convert the PDF documents in 
Japanese?, in Japanese is attached to 
convert. 
3. Missing lexical entries: some domain 
specific words such as download and their 
usages are not in the Minipar lexicon. 
This introduces parsing errors because 
such words are tagged as nouns by 
default.  
4. Inability to handle ungrammatical 
sentences: in a real world application, it is 
unrealistic to expect the user to enter only 
grammatical sentences. Although Minipar 
still produces a syntactic tree for an 
ungrammatical sentence, the tree is ill 
formed and cannot be used to extract the 
semantic information being expressed. 
 
In addition, Minipar, like other broad-
coverage parsers, cannot be adapted to specific 
applications. Its accuracy does not satisfy the 
needs of our toolkit. We have to build another 
parser on top of Minipar to enable domain 
specific customizations to increase the parsing 
accuracy. 
4 The Shallow Parser 
Our NLPTK maps input sentences to action 
requests. In order to perform an accurate 
mapping the toolkit needs to get information 
such as the sentence type, the main predicate, 
the arguments of the predicate, and the 
modifications of the predicate and arguments 
from a sentence. In other words, it mostly 
needs local dependency relationships. 
Therefore we decided to build a shallow parser 
instead of a full parser. A parser that captures 
the most frequent verb argument structures in a 
domain can be built relatively fast. It takes less 
space, which can be an important issue for 
certain applications. For example, when 
building an NLP system for a handheld 
platform, a light parser is needed because the 
memory cannot accommodate a full parser. 
4.1 Introduction 
We built a KWIC (keyword in context) verb 
shallow parser. It captures only verb predicates 
with their arguments, verb argument modifiers 
and verb adjuncts in a sentence. The resulting 
trees contain local and subjacent dependencies 
between these elements. 
The shallow parser depends on three levels 
of information processing: the verb list, 
subcategorization (in short, subcat) and 
syntactic rules. The verb subcat system is 
derived from Levin?s taxonomy of verbs and 
their classes (Levin, 1993). We have 24 verb 
files containing 3200 verbs, which include all 
the Levin verbs and the most frequent verbs in 
our corpus. A verb is indexed to one or more 
subcat files and each file represents a particular 
alternation semantico-syntactic sense. We have 
272 syntactic subcat files derived from the 
Levin verb semantic classes. The syntactic 
rules are marked for argument types and 
constituency, using the Penn Treebank tagset 
(Marcus, 1993). They contain both generalized 
rules, e.g.,  .../NN, and specified rules, e.g., 
purchase/VBP. An example subcat rule for the 
verb purchase looks like this: .../DT .../JJ 
.../NN, .../DT .../NN from/RP .../NN for/RP 
.../NN. The first element says that purchase 
takes an NP argument, and the second says that 
it takes an NP argument and two PP adjuncts. 
We also encoded specific PP head class 
information based on the WordNet concepts in 
the rules for some attachment disambiguation. 
The shallow parser works like this: it first 
tags an incoming sentence with Brill tagger 
(Brill, 1995) and matches verbs in the tagged 
sentence with the verb list. If a match is found, 
the parser will open the subcat files indexed to 
that verb and gather all the syntactic rules in 
these specific subcat files. It then matches the 
verb arguments with these syntactic rules and 
outputs the results into a tree. The parser can 
control over-generation for any verb because 
the syntactic structures are limited to that 
particular verb's syntactic structure set from 
the Levin classes. 
4.2 Disadvantages of Shallow Parser 
The disadvantages of the shallow parser are 
mainly due to its simplified design, including: 
1. It cannot handle sentences whose main 
verb is be or phrasal sentences without a 
verb because the shallow parser mainly 
targets command-and-control verb 
argument structures.  
2. It cannot handle structures that appear 
before the verb. Subjects will not appear 
in the parse tree even though it might 
contain important information. 
3. It cannot detect sentence type, for 
example, whether a sentence is a question 
or a request. 
4. It cannot handle negative or passive 
sentences. 
We tested the shallow parser on 500 
sentences from our corpus and compared the 
results with the output of Minipar. We 
separated the sentences into five sets of 100 
sentences. After running the parser on each set, 
we fixed the problems that we could identify.  
This was our process of training the parser. 
Table 1 shows the data obtained from one such 
cycle. Since the shallow parser cannot handle 
sentences with the main verb be, these 
sentences are excluded from the statistics. So 
the test set actually contains 85 sentences. 
In Table 1, the first column and the first 
row show the statistics for the shallow parser 
and Minipar respectively. The upper half of the 
table is for the unseen data, where 55.3% of 
the sentences are parsed correctly and 11.8% 
incorrectly (judged by humans) by both 
parsers. 18.9% of the sentences are parsed 
correctly by Minipar, but incorrectly by the 
shallow parser, and 14.1% vise versa. The 
lower half of the table shows the result after 
fixing some shallow parser problems, for 
example, adding a new syntactic rule. The 
accuracy of the parser is significantly 
improved, from 69.4% to 81.2%. This shows 
the importance of adaptation to specific 
domain needs, and that in our domain, the 
shallow parser outperforms Minipar. 
 
SP/MP Correct 
(74.1%) 
Wrong 
(25.9%) 
Correct (69.4%) 47 (55.3%) 12 (14.1%) 
Wrong (30.6%) 16 (18.9%) 10 (11.8%) 
SP/MP Correct 
(74.1%) 
Wrong 
(25.9%) 
Correct (81.2%) 53 (62.4%) 16 (18.8%) 
Wrong (18.8%) 10 (11.8%) 6 (7.1%) 
Table 1: Comparison of the shallow 
parser with Minipar on 85 sentences 
The parsers do not perform equally well on 
all sets of sentences. For some sets, the 
accuracies of Minipar and the shallow parser 
drop to 60.9% and 67.8% respectively. 
5 Extending Minipar with the 
Shallow Parser 
Each parser has pros and cons. The advantage 
of Minipar is that it is a broad-coverage parser 
with relatively high accuracy, and the 
advantage of the shallow parser is that it is 
adaptable. For this reason, we intend to use 
Minipar as our primary parser and the shallow 
parser a backup. Table 1 shows only a small 
percentage of sentences parsed incorrectly by 
both parsers (about 7%). If we always choose 
the correct tree between the two outputs, we 
will have a parser with much higher accuracy. 
Therefore, combining the advantages of the 
two parsers will achieve better performance in 
both coverage and accuracy. Now the question 
is how to decide if a tree is correct or not.  
5.1 Detecting Parsing Errors 
In an ideal situation, each parser should 
provide a confidence level for a tree that is 
comparable to each other. We would choose 
the tree with higher confidence. However, this 
is not possible in our case because weightings 
of the Minipar trees are not publicly available, 
and the shallow parser is a rule-based system 
without confidence information.  
Instead, we use a few simple heuristics to 
decide if a tree is right or wrong, based on an 
analysis of the trees generated for our test 
sentences. For example, given a sentence, the 
Minipar tree is incorrect if it has more than one 
subtree connected by a top-level node whose 
syntactic category is U (unknown).  A shallow 
parser tree is wrong if there are unparsed 
words at the end of the sentence after the main 
verb (except for interjections). We have three 
heuristics identifying a wrong Minipar tree and 
two identifying a wrong shallow parser tree. If 
a tree passes these heuristics, we must label the 
tree as a good parse.  This may not be true, but 
we will compensate for this simplification 
later. The module implementing these 
heuristics is called the error detector. 
We tested the three heuristics for Minipar 
trees on a combination of 84 requestive, 
interrogative and declarative sentences. The 
results are given in the upper part of Table 2. 
The table shows that 45 correct Minipar trees 
(judged by humans) are identified as correct by 
the error detector and 18 wrong trees are 
identified as wrong, so the accuracy is 75%. 
Tagging errors and some attachment errors 
cannot be detected. 
 
MP/ED Correct 
(76.2%) 
Wrong 
(23.8%) 
Correct (56%) 45 (53.6%) 2 (2.4%) 
Wrong (44%) 19 (22.6%) 18 (21.4%) 
SP/ED Correct 
(73%) 
Wrong 
(26%) 
Correct (59%) 58 (58%) 1 (1%) 
Wrong (40%) 15 (15%) 25 (25%) 
Table 2: The performance of the parse 
tree error detector 
We tested the two heuristics for shallow 
parser trees on 100 sentences from our corpus 
and the result is given in the lower part of 
Table 2. The accuracy is about 83%. We did 
not use the same set of sentences to test the 
two sets of heuristics because the coverage of 
the two parsers is different. 
5.2 Choosing the Better Parse Trees 
We run the two parsers in parallel to generate 
two parse trees for an input sentence, but we 
cannot depend only on the error detector to 
decide which tree to choose because it is not 
accurate enough. Table 2 shows that the error 
detector mistakenly judges some wrong trees 
as correct, but not the other way round. In 
other words, when the detector says a tree is 
wrong, we have high confidence that it is 
indeed wrong, but when it says a tree is 
correct, there is some chance that the tree is 
actually wrong. This motivates us to 
distinguish three cases: 
1. When only one of the two parse trees is 
detected as wrong, we choose the correct 
tree, because no matter what the correct 
tree actually is, the other tree is definitely 
wrong so we cannot choose it. 
2. When both trees are detected as wrong, we 
choose the Minipar tree because it handles 
more syntactic structures. 
3. When both trees are detected as correct, 
we need more analysis because either 
might be wrong. 
We have mentioned in the previous sections 
the problems with both parsers. By comparing 
their pros and cons, we come up with 
heuristics for determining which tree is better 
for the third case above.  
The decision flow for selecting the better 
parse is given in Figure 1. Since the shallow 
parser cannot handle negative and passive 
sentences as well as sentences with the main 
verb be, we choose the Minipar trees for such 
sentences. The shallow parser outperforms 
Minipar on tagging and some PP attachment 
because it checks the WordNet concepts. So, 
when we detect differences concerning part-of-
speech tags and PP attachment in the parse 
trees, we choose the shallow parser tree as the 
output. In addition, we prefer the parse with 
bigger NP chunks.  
We tested these heuristics on 200 sentences 
and the result is shown in Table 3. The first 
row specifies whether a Minipar tree or a 
shallow parser tree is chosen as the final 
output. The first column gives whether the 
final tree is correct or incorrect according to 
human judgment. 88% of the time, Minipar 
trees are chosen and they are 82.5% accurate. 
The overall contribution of Minipar to the 
accuracy is 73.5%. The improvement from just 
using Minipar is about 7%, from about 75.5% 
to 82.5%. This is a significant improvement. 
The main computational expense of 
running two parsers in parallel is time. Since 
our shallow parser has not been optimized, the 
extended parser is about 2.5 times slower than 
Minipar alone. We hope that with some 
optimization, the speed of the system will 
increase considerably. Even in the current time 
frame, it takes less than 0.6 second to parse a 
15 word sentence. 
 
Final tree MP tree 
(88%) 
SP tree 
(11%) 
Correct (82.5%) 73.5% 9% 
Wrong (16.5%) 14.5% 2% 
Table 3: Results for the extended parser 
6 Conclusions and Future Work 
In this paper we described a parser that extends 
a broad-coverage parser, Minipar, with a 
domain adaptable shallow parser in order to 
achieve generality and higher accuracy at the 
same time. This parser is an important 
component of a general NLP Toolkit, which 
helps programmers quickly develop an NLP 
front end that handles natural language input 
from their end users. We tested the parser on 
200 sentences from our corpus and the result 
shows significant improvement over using 
Minipar alone. 
Future work includes improving the 
efficiency and accuracy of the shallow parser. 
Also, we will test the parser on a different 
domain to see how much work is required to 
switch to a new domain. 
 
References 
Alam H. (2000) Spoken Language Generic 
User Interface (SLGUI). Technical Report 
AFRL-IF-RS-TR-2000-58, Air Force Research 
Laboratory, Rome.  
Brill E. (1992) A Simple Rule-based Part of 
Speech Tagger. In Proceedings of the 3rd 
Conference on Applied Natural Language 
Processing. 
Charniak E. (2000) A Maximum-Entropy-
Inspired Parser. In Proceedings of the 1st 
Meeting of NAACL. Washington. 
Chomsky N. (1995) Minimalist Program. 
MIT Press. 
Collins M. (1999) Head-Driven Statistical 
Models for Natural Language Parsing. PhD 
Dissertation, University of Pennsylvania. 
Is the sentence
passive?
Is the SP tree empty?
Is the SP  tree correct?
Adding the sentence type
and subject of the M P tree to
the SP tree
No
No
Yes
Accept M P tree
M inipar (M P) tree Shallow Parser (SP) tree
Final parse tree
Yes
No
Is the sentence
negative?
No
Yes
Yes
Yes
No
No
Yes
No
Is the M P tree correct?
No
Is the size of the M P
tree bigger than or
equal to that of the
SP  tree?
Is the length of an
NP chunk in  the M P
tree longer?
Yes
Does the M P tree
have less verb  tags?
Yes
No
No
Yes
Yes
No
Yes Does SP finds a verb
when M P assigns a
sentence type as NP?
Are the verb  tags in
the two trees
inconsistent?
Are there unequal
number of verb tags
in the trees?
Figure 1: Decision flow for parse tree selection 
 
Levin B. (1993) English Verb Classes and 
Alternations: A Preliminary Investigation. 
University of Chicago Press, Chicago. 
Lin D. (1998a) Dependency-based 
Evaluation of Minipar. In Workshop on the 
Evaluation of Parsing Systems, Spain. 
Lin D. (1998b) Extracting Collocations 
from Text Corpora. In Workshop on 
Computational Terminology, Montreal, 
Canada, pp. 57-63. 
Lin D. (2001) Latat: Language and Text 
Analysis Tools. In Proceedings of Human 
Language Technology Conference, CA, USA. 
Marcus M., Santorini B. and Marcinkiewicz 
M. (1993) Building a Large Annotated Corpus 
of English: The Penn Treebank, Computational 
Linguistics, vol. 19, no. 2, pp. 313-330.  
Sampson G. (1995) English for the 
Computer. Oxford University Press. 
