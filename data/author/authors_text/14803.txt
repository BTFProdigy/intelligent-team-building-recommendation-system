Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 496?501,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Putting it Simply: a Context-Aware Approach to Lexical Simplification
Or Biran
Computer Science
Columbia University
New York, NY 10027
ob2008@columbia.edu
Samuel Brody
Communication & Information
Rutgers University
New Brunswick, NJ 08901
sdbrody@gmail.com
Noe?mie Elhadad
Biomedical Informatics
Columbia University
New York, NY 10032
noemie@dbmi.columbia.edu
Abstract
We present a method for lexical simplifica-
tion. Simplification rules are learned from a
comparable corpus, and the rules are applied
in a context-aware fashion to input sentences.
Our method is unsupervised. Furthermore, it
does not require any alignment or correspon-
dence among the complex and simple corpora.
We evaluate the simplification according to
three criteria: preservation of grammaticality,
preservation of meaning, and degree of sim-
plification. Results show that our method out-
performs an established simplification base-
line for both meaning preservation and sim-
plification, while maintaining a high level of
grammaticality.
1 Introduction
The task of simplification consists of editing an in-
put text into a version that is less complex linguisti-
cally or more readable. Automated sentence sim-
plification has been investigated mostly as a pre-
processing step with the goal of improving NLP
tasks, such as parsing (Chandrasekar et al, 1996;
Siddharthan, 2004; Jonnalagadda et al, 2009), se-
mantic role labeling (Vickrey and Koller, 2008) and
summarization (Blake et al, 2007). Automated sim-
plification can also be considered as a way to help
end users access relevant information, which would
be too complex to understand if left unedited. As
such, it was proposed as a tool for adults with
aphasia (Carroll et al, 1998; Devlin and Unthank,
2006), hearing-impaired people (Daelemans et al,
2004), readers with low-literacy skills (Williams and
Reiter, 2005), individuals with intellectual disabil-
ities (Huenerfauth et al, 2009), as well as health
INPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking magnate.
CANDIDATE RULES:
{magnate? king} {magnate? businessman}
OUTPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking businessman.
Figure 1: Input sentence, candidate simplification rules,
and output sentence.
consumers looking for medical information (El-
hadad and Sutaria, 2007; Dele?ger and Zweigen-
baum, 2009).
Simplification can take place at different levels of
a text ? its overall document structure, the syntax
of its sentences, and the individual phrases or words
in a sentence. In this paper, we present a sentence
simplification approach, which focuses on lexical
simplification.1 The key contributions of our work
are (i) an unsupervised method for learning pairs of
complex and simpler synonyms; and (ii) a context-
aware method for substituting one for the other.
Figure 1 shows an example input sentence. The
word magnate is determined as a candidate for sim-
plification. Two learned rules are available to the
simplification system (substitute magnate with king
or with businessman). In the context of this sen-
tence, the second rule is selected, resulting in the
simpler output sentence.
Our method contributes to research on lexical
simplification (both learning of rules and actual sen-
tence simplification), a topic little investigated thus
far. From a technical perspective, the task of lexi-
cal simplification bears similarity with that of para-
1Our resulting system is available for download at
http://www.cs.columbia.edu/ ob2008/
496
phrase identification (Androutsopoulos and Malaka-
siotis, 2010) and the SemEval-2007 English Lexi-
cal Substitution Task (McCarthy and Navigli, 2007).
However, these do not consider issues of readabil-
ity and linguistic complexity. Our methods lever-
age a large comparable collection of texts: En-
glish Wikipedia2 and Simple English Wikipedia3.
Napoles and Dredze (2010) examined Wikipedia
Simple articles looking for features that characterize
a simple text, with the hope of informing research
in automatic simplification methods. Yatskar et al
(2010) learn lexical simplification rules from the edit
histories of Wikipedia Simple articles. Our method
differs from theirs, as we rely on the two corpora as a
whole, and do not require any aligned or designated
simple/complex sentences when learning simplifica-
tion rules.4
2 Data
We rely on two collections ? English Wikipedia
(EW) and Simple English Wikipedia (SEW). SEW
is a Wikipedia project providing articles in Sim-
ple English, a version of English which uses fewer
words and easier grammar, and which aims to be
easier to read for children, people who are learning
English and people with learning difficulties. Due to
the labor involved in simplifying Wikipedia articles,
only about 2% of the EW articles have been simpli-
fied.
Our method does not assume any specific align-
ment or correspondance between individual EW and
SEW articles. Rather, we leverage SEW only as
an example of an in-domain simple corpus, in or-
der to extract word frequency estimates. Further-
more, we do not make use of any special properties
of Wikipedia (e.g., edit histories). In practice, this
means that our method is suitable for other cases
where there exists a simplified corpus in the same
domain.
The corpora are a snapshot as of April 23, 2010.
EW contains 3,266,245 articles, and SEW contains
60,100 articles. The articles were preprocessed as
follows: all comments, HTML tags, and Wiki links
were removed. Text contained in tables and figures
2http://en.wikipedia.org
3http://simple.wikipedia.org
4Aligning sentences in monolingual comparable corpora has
been investigated (Barzilay and Elhadad, 2003; Nelken and
Shieber, 2006), but is not a focus for this work.
was excluded, leaving only the main body text of
the article. Further preprocessing was carried out
with the Stanford NLP Package5 to tokenize the text,
transform all words to lower case, and identify sen-
tence boundaries.
3 Method
Our sentence simplification system consists of two
main stages: rule extraction and simplification. In
the first stage, simplification rules are extracted from
the corpora. Each rule consists of an ordered word
pair {original? simplified} along with a score indi-
cating the similarity between the words. In the sec-
ond stage, the system decides whether to apply a rule
(i.e., transform the original word into the simplified
one), based on the contextual information.
3.1 Stage 1: Learning Simplification Rules
3.1.1 Obtaining Word Pairs
All content words in the English Wikipedia Cor-
pus (excluding stop words, numbers, and punctua-
tion) were considered as candidates for simplifica-
tion. For each candidate word w, we constructed a
context vectorCVw, containing co-occurrence infor-
mation within a 10-token window. Each dimension
i in the vector corresponds to a single word wi in
the vocabulary, and a single dimension was added to
represent any number token. The value in each di-
mension CVw[i] of the vector was the number of oc-
currences of the corresponding wordwi within a ten-
token window surrounding an instance of the candi-
date word w. Values below a cutoff (2 in our exper-
iments) were discarded to reduce noise and increase
performance.
Next, we consider candidates for substitution.
From all possible word pairs (the Cartesian product
of all words in the corpus vocabulary), we first re-
move pairs of morphological variants. For this pur-
pose, we use MorphAdorner6 for lemmatization, re-
moving words which share a common lemma. We
also prune pairs where one word is a prefix of the
other and the suffix is in {s, es, ed, ly, er, ing}. This
handles some cases which are not covered by Mor-
phAdorner. We use WordNet (Fellbaum, 1998) as
a primary semantic filter. From all remaining word
pairs, we select those in which the second word, in
5http://nlp.stanford.edu/software/index.shtml
6http://morphadorner.northwestern.edu
497
its first sense (as listed in WordNet)7 is a synonym
or hypernym of the first.
Finally, we compute the cosine similarity scores
for the remaining pairs using their context vectors.
3.1.2 Ensuring Simplification
From among our remaining candidate word pairs,
we want to identify those that represent a complex
word which can be replaced by a simpler one. Our
definition of the complexity of a word is based on
two measures: the corpus complexity and the lexical
complexity. Specifically, we define the corpus com-
plexity of a word as
Cw =
fw,English
fw,Simple
where fw,c is the frequency of word w in corpus c,
and the lexical complexity as Lw = |w|, the length
of the word. The final complexity ?w for the word
is given by the product of the two.
?w = Cw ? Lw
After calculating the complexity of all words par-
ticipating in the word pairs, we discard the pairs for
which the first word?s complexity is lower than that
of the second. The remaining pairs constitute the
final list of substitution candidates.
3.1.3 Ensuring Grammaticality
To ensure that our simplification substitutions
maintain the grammaticality of the original sentence,
we generate grammatically consistent rules from
the substitution candidate list. For each candidate
pair (original, simplified), we generate all consis-
tent forms (fi(original), fi(substitute)) of the two
words using MorphAdorner. For verbs, we create
the forms for all possible combinations of tenses and
persons, and for nouns we create forms for both sin-
gular and plural.
For example, the word pair (stride, walk) will gen-
erate the form pairs (stride, walk), (striding, walk-
ing), (strode, walked) and (strides, walks). Signifi-
cantly, the word pair (stride, walked) will generate
7Senses in WordNet are listed in order of frequency. Rather
than attempting explicit disambiguation and adding complex-
ity to the model, we rely on the first sense heuristic, which is
know to be very strong, along with contextual information, as
described in Section 3.2.
exactly the same list of form pairs, eliminating the
original ungrammatical pair.
Finally, each pair (fi(original), fi(substitute)) be-
comes a rule {fi(original) ? fi(substitute)},
with weight Similarity(original, substitute).
3.2 Stage 2: Sentence Simplification
Given an input sentence and the set of rules learned
in the first stage, this stage determines which words
in the sentence should be simplified, and applies
the corresponding rules. The rules are not applied
blindly, however; the context of the input sentence
influences the simplification in two ways:
Word-Sentence Similarity First, we want to en-
sure that the more complex word, which we are at-
tempting to simplify, was not used precisely because
of its complexity - to emphasize a nuance or for its
specific shade of meaning. For example, suppose we
have a rule {Han? Chinese}. We would want to
apply it to a sentence such as ?In 1368 Han rebels
drove out the Mongols?, but to avoid applying it to
a sentence like ?The history of the Han ethnic group
is closely tied to that of China?. The existence of
related words like ethnic and China are clues that
the latter sentence is in a specific, rather than gen-
eral, context and therefore a more general and sim-
pler hypernym is unsuitable. To identify such cases,
we calculate the similarity between the target word
(the candidate for replacement) and the input sen-
tence as a whole. If this similarity is too high, it
might be better not to simplify the original word.
Context Similarity The second factor has to do
with ambiguity. We wish to detect and avoid cases
where a word appears in the sentence with a differ-
ent sense than the one originally considered when
creating the simplification rule. For this purpose, we
examine the similarity between the rule as a whole
(including both the original and the substitute words,
and their associated context vectors) and the context
of the input sentence. If the similarity is high, it is
likely the original word in the sentence and the rule
are about the same sense.
3.2.1 Simplification Procedure
Both factors described above require sufficient
context in the input sentence. Therefore, our sys-
tem does not attempt to simplify sentences with less
than seven content words.
498
Type Gram. Mean. Simp.
Baseline 70.23(+13.10)% 55.95% 46.43%
System 77.91(+8.14)% 62.79% 75.58%
Table 1: Average scores in three categories: grammatical-
ity (Gram.), meaning preservation (Mean.) and simplifi-
cation (Simp.). For grammaticality, we show percent of
examples judged as good, with ok percent in parentheses.
For all other sentences, each content word is ex-
amined in order, ignoring words inside quotation
marks or parentheses. For each word w, the set of
relevant simplification rules {w ? x} is retrieved.
For each rule {w ? x}, unless the replacement
word x already appears in the sentence, our system
does the following:
? Build the vector of sentence context SCVs,w in a
similar manner to that described in Section 3.1,
using the words in a 10-token window surround-
ing w in the input sentence.
? Calculate the cosine similarity of CVw and
SCVs,w. If this value is larger than a manually
specified threshold (0.1 in our experiments), do
not use this rule.
? Create a common context vector CCVw,x for the
rule {w ? x}. The vector contains all fea-
tures common to both words, with the feature
values that are the minimum between them. In
other words, CCVw,x[i] = min(CVw[i], CVx[i]).
We calculate the cosine similarity of the common
context vector and the sentence context vector:
ContextSim = cosine(CCVw,x, SCVs,w)
If the context similarity is larger than a threshold
(0.01), we use this rule to simplify.
If multiple rules apply for the same word, we use
the one with the highest context similarity.
4 Experimental Setup
Baseline We employ the method of Devlin and
Unthank (2006) which replaces a word with its most
frequent synonym (presumed to be the simplest) as
our baseline. To provide a fairer comparison to our
system, we add the restriction that the synonyms
should not share a prefix of four or more letters
(a baseline version of lemmatization) and use Mor-
phAdorner to produce a form that agrees with that
of the original word.
Type Freq. Gram. Mean. Simp.
Base High 63.33(+20)% 46.67% 50%
Sys. High 76.67(+6.66)% 63.33% 73.33%
Base Med 75(+7.14)% 67.86% 42.86%
Sys. Med 72.41(+17.25)% 75.86% 82.76%
Base Low 73.08(+11.54)% 53.85% 46.15%
Sys. Low 85.19(+0)% 48.15% 70.37%
Table 2: Average scores by frequency band
Evaluation Dataset We sampled simplification
examples for manual evaluation with the following
criteria. Among all sentences in English Wikipedia,
we first extracted those where our system chose to
simplify exactly one word, to provide a straightfor-
ward example for the human judges. Of these, we
chose the sentences where the baseline could also
be used to simplify the target word (i.e., the word
had a more frequent synonym), and the baseline re-
placement was different from the system choice. We
included only a single example (simplified sentence)
for each rule.
The evaluation dataset contained 65 sentences.
Each was simplified by our system and the baseline,
resulting in 130 simplification examples (consisting
of an original and a simplified sentence).
Frequency Bands Although we included only a
single example of each rule, some rules could be
applied much more frequently than others, as the
words and associated contexts were common in the
dataset. Since this factor strongly influences the
utility of the system, we examined the performance
along different frequency bands. We split the eval-
uation dataset into three frequency bands of roughly
equal size, resulting in 46 high, 44 med and 40 low.
Judgment Guidelines We divided the simplifica-
tion examples among three annotators 8 and ensured
that no annotator saw both the system and baseline
examples for the same sentence. Each simplification
example was rated on three scales: Grammaticality
- bad, ok, or good; Meaning - did the transforma-
tion preserve the original meaning of the sentence;
and Simplification - did the transformation result in
8The annotators were native English speakers and were not
the authors of this paper. A small portion of the sentence pairs
were duplicated among annotators to calculate pairwise inter-
annotator agreement. Agreement was moderate in all categories
(Cohen?s Kappa = .350? .455 for Simplicity, .475? .530 for
Meaning and .415? .425 for Grammaticality).
499
a simpler sentence.
5 Results and Discussion
Table 1 shows the overall results for the experiment.
Our method is quantitatively better than the base-
line at both grammaticality and meaning preserva-
tion, although the difference is not statistically sig-
nificant. For our main goal of simplification, our
method significantly (p < 0.001) outperforms the
baseline, which represents the established simplifi-
cation strategy of substituting a word with its most
frequent WordNet synonym. The results demon-
strate the value of correctly representing and ad-
dressing content when attempting automatic simpli-
fication.
Table 2 contains the results for each of the fre-
quency bands. Grammaticality is not strongly influ-
enced by frequency, and remains between 80-85%
for both the baseline and our system (considering
the ok judgment as positive). This is not surpris-
ing, since the method for ensuring grammaticality is
largely independent of context, and relies mostly on
a morphological engine. Simplification varies some-
what with frequency, with the best results for the
medium frequency band. In all bands, our system is
significantly better than the baseline. The most no-
ticeable effect is for preservation of meaning. Here,
the performance of the system (and the baseline) is
the best for the medium frequency group. However,
the performance drops significantly for the low fre-
quency band. This is most likely due to sparsity of
data. Since there are few examples from which to
learn, the system is unable to effectively distinguish
between different contexts and meanings of the word
being simplified, and applies the simplification rule
incorrectly.
These results indicate our system can be effec-
tively used for simplification of words that occur
frequently in the domain. In many scenarios, these
are precisely the cases where simplification is most
desirable. For rare words, it may be advisable to
maintain the more complex form, to ensure that the
meaning is preserved.
Future Work Because the method does not place
any restrictions on the complex and simple corpora,
we plan to validate it on different domains and ex-
pect it to be easily portable. We also plan to extend
our method to larger spans of texts, beyond individ-
ual words.
References
Androutsopoulos, Ion and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence
Research 38:135?187.
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proc. EMNLP. pages 25?32.
Blake, Catherine, Julia Kampov, Andreas Or-
phanides, David West, and Cory Lown. 2007.
Query expansion, lexical simplification, and sen-
tence selection strategies for multi-document
summarization. In Proc. DUC.
Carroll, John, Guido Minnen, Yvonne Canning,
Siobhan Devlin, and John Tait. 1998. Practical
simplication of english newspaper text to assist
aphasic readers. In Proc. AAAI Workshop on Inte-
grating Artificial Intelligence and Assistive Tech-
nology.
Chandrasekar, R., Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In Proc. COLING.
Daelemans, Walter, Anja Hthker, and Erik
Tjong Kim Sang. 2004. Automatic sentence
simplification for subtitling in Dutch and English.
In Proc. LREC. pages 1045?1048.
Dele?ger, Louise and Pierre Zweigenbaum. 2009.
Extracting lay paraphrases of specialized expres-
sions from monolingual comparable medical cor-
pora. In Proc. Workshop on Building and Using
Comparable Corpora. pages 2?10.
Devlin, Siobhan and Gary Unthank. 2006. Help-
ing aphasic people process online information. In
Proc. ASSETS. pages 225?226.
Elhadad, Noemie and Komal Sutaria. 2007. Mining
a lexicon of technical terms and lay equivalents.
In Proc. ACL BioNLP Workshop. pages 49?56.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Database. MIT Press, Cambridge,
MA.
Huenerfauth, Matt, Lijun Feng, and Noe?mie El-
hadad. 2009. Comparing evaluation techniques
500
for text readability software for adults with intel-
lectual disabilities. In Proc. ASSETS. pages 3?10.
Jonnalagadda, Siddhartha, Luis Tari, Jo?rg Haken-
berg, Chitta Baral, and Graciela Gonzalez. 2009.
Towards effective sentence simplification for au-
tomatic processing of biomedical text. In Proc.
NAACL-HLT . pages 177?180.
McCarthy, Diana and Roberto Navigli. 2007.
Semeval-2007 task 10: English lexical substitu-
tion task. In Proc. SemEval. pages 48?53.
Napoles, Courtney and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proc. of the NAACL-
HLT Workshop on Computational Linguistics and
Writing. pages 42?50.
Nelken, Rani and Stuart Shieber. 2006. Towards
robust context-sensitive sentence alignment for
monolingual corpora. In Proc. EACL. pages 161?
166.
Siddharthan, Advaith. 2004. Syntactic simplifica-
tion and text cohesion. Technical Report UCAM-
CL-TR-597, University of Cambridge, Computer
Laboratory.
Vickrey, David and Daphne Koller. 2008. Apply-
ing sentence simplification to the CoNLL-2008
shared task. In Proc. CoNLL. pages 268?272.
Williams, Sandra and Ehud Reiter. 2005. Generating
readable texts for readers with low basic skills. In
Proc. ENLG. pages 127?132.
Yatskar, Mark, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For the
sake of simplicity: Unsupervised extraction of
lexical simplifications from wikipedia. In Proc.
NAACL-HLT . pages 365?368.
501
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 69?73,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Aggregated Word Pair Features for Implicit Discourse Relation
Disambiguation
Or Biran
Columbia University
Department of Computer Science
orb@cs.columbia.edu
Kathleen McKeown
Columbia University
Department of Computer Science
kathy@cs.columbia.edu
Abstract
We present a reformulation of the word
pair features typically used for the task
of disambiguating implicit relations in the
Penn Discourse Treebank. Our word pair
features achieve significantly higher per-
formance than the previous formulation
when evaluated without additional fea-
tures. In addition, we present results
for a full system using additional features
which achieves close to state of the art per-
formance without resorting to gold syntac-
tic parses or to context outside the relation.
1 Introduction
Discourse relations such as contrast and causal-
ity are part of what makes a text coherent. Be-
ing able to automatically identify these relations
is important for many NLP tasks such as gener-
ation, question answering and textual entailment.
In some cases, discourse relations contain an ex-
plicit marker such as but or because which makes
it easy to identify the relation. Prior work (Pitler
and Nenkova, 2009) showed that where explicit
markers exist, the class of the relation can be dis-
ambiguated with f-scores higher than 90%.
Predicting the class of implicit discourse rela-
tions, however, is much more difficult. Without an
explicit marker to rely on, work on this task ini-
tially focused on using lexical cues in the form
of word pairs mined from large corpora where
they appear around an explicit marker (Marcu and
Echihabi, 2002). The intuition is that these pairs
will tend to represent semantic relationships which
are related to the discourse marker (for example,
word pairs often appearing around but may tend
to be antonyms). While this approach showed
some success and has been used extensively in
later work, it has been pointed out by multiple
authors that many of the most useful word pairs
are pairs of very common functional words, which
contradicts the original intuition, and it is hard to
explain why these are useful.
In this work we focus on the task of identi-
fying and disambiguating implicit discourse rela-
tions which have no explicit marker. In particular,
we present a reformulation of the word pair fea-
tures that have most often been used for this task
in the past, replacing the sparse lexical features
with dense aggregated score features. This is the
main contribution of our paper. We show that our
formulation outperforms the original one while re-
quiring less features, and that using a stop list of
functional words does not significantly affect per-
formance, suggesting that these features indeed
represent semantically related content word pairs.
In addition, we present a system which com-
bines these word pairs with additional features to
achieve near state of the art performance without
the use of syntactic parse features and of context
outside the arguments of the relation. Previous
work has attributed much of the achieved perfor-
mance to these features, which are easy to get in
the experimental setting but would be less reliable
or unavailable in other applications.1
2 Related Work
This line of research began with (Marcu and Echi-
habi, 2002), who used a small number of unam-
biguous explicit markers and patterns involving
them, such as [Arg1, but Arg2] to collect sets of
word pairs from a large corpus using the cross-
product of the words in Arg1 and Arg2. The au-
thors created a feature out of each pair and built a
naive bayes model directly from the unannotated
corpus, updating the priors and posteriors using
maximum likelihood. While they demonstrated
1Reliable syntactic parses are not always available in do-
mains other than newswire, and context (preceding relations,
especially explicit relations) is not always available in some
applications such as generation and question answering.
69
some success, their experiments were run on data
that is unnatural in two ways. First, it is balanced.
Second, it is constructed with the same unsuper-
vised method they use to extract the word pairs -
by assuming that the patterns correspond to a par-
ticular relation and collecting the arguments from
an unannotated corpus. Even if the assumption is
correct, these arguments are really taken from ex-
plicit relations with their markers removed, which
as others have pointed out (Blair-Goldensohn et
al., 2007; Pitler et al, 2009) may not look like true
implicit relations.
More recently, implicit relation prediction has
been evaluated on annotated implicit relations
from the Penn Discourse Treebank (Prasad et al,
2008). PDTB uses hierarchical relation types
which abstract over other theories of discourse
such as RST (Mann and Thompson, 1987) and
SDRT (Asher and Lascarides, 2003). It contains
40, 600 annotated relations from the WSJ corpus.
Each relation has two arguments, Arg1 and Arg2,
and the annotators decide whether it is explicit or
implicit.
The first to evaluate directly on PDTB in a re-
alistic setting were Pitler et al (2009). They used
word pairs as well as additional features to train
four binary classifiers, each corresponding to one
of the high-level PDTB relation classes. Although
other features proved to be useful, word pairs were
still the major contributor to most of these clas-
sifiers. In fact, their best system for comparison
included only the word pair features, and for all
other classes other than expansion the word pair
features alone achieved an f-score within 2 points
of the best system. Interestingly, they found that
training the word pair features on PDTB itself was
more useful than training them on an external cor-
pus like Marcu and Echihabi (2002), although in
some cases they resort to information gain in the
external corpus for filtering the word pairs.
Zhou et al (2010) used a similar method and
added features that explicitly try to predict the
implicit marker in the relation, increasing perfor-
mance. Most recently to the best of our knowl-
edge, Park and Cardie (2012) achieved the highest
performance by optimizing the feature set. An-
other work evaluating on PDTB is (Lin et al,
2009), who are unique in evaluating on the more
fine-grained second-level relation classes.
3 Word Pairs
3.1 The Problem: Sparsity
While Marcu and Echihabi (2002)?s approach of
training a classifier from an unannotated corpus
provides a relatively large amount of training data,
this data does not consist of true implicit relations.
However, the approach taken by Pitler et al (2009)
and repeated in more recent work (training directly
on PDTB) is problematic as well: when training a
model with so many sparse features on a dataset
the size of PDTB (there are 22, 141 non-explicit
relations overall), it is likely that many important
word pairs will not be seen in training.
In fact, even the larger corpus of Marcu and
Echihabi (2002) may not be quite large enough
to solve the sparsity issue, given that the num-
ber of word pairs is quadratic in the vocabulary.
Blair-Goldensohn et al (2007) report that using
even a very small stop list (25 words) significantly
reduces performance, which is counter-intuitive.
They attribute this finding to the sparsity of the
feature space. An analysis in (Pitler et al, 2009)
also shows that the top word pairs (ranked by
information gain) all contain common functional
words, and are not at all the semantically-related
content words that were imagined. In the case
of some reportedly useful word pairs (the-and; in-
the; the-of...) it is hard to explain how they might
affect performance except through overfitting.
3.2 The Solution: Aggregation
Representing each word pair as a single feature has
the advantage of allowing the weights for each pair
to be learned directly from the data. While pow-
erful, this approach requires large amounts of data
to be effective.
Another possible approach is to aggregate some
of the pairs together and learn weights from the
data only for the aggregated sets of words. For this
approach to be effective, the pairs we choose to
group together should have similar meaning with
regard to predicting the relation.
Biran and Rambow (2011) is to our knowledge
the only other work utilizing a similar approach.
They used aggregated word pair set features to
predict whether or not a sentence is argumentative.
Their method is to group together word pairs that
have been collected around the same explicit dis-
course marker: for every discourse marker such
as therefore or however, they have a single fea-
ture whose value depends only on the word pairs
70
collected around that marker. This is reasonable
given the intuition that the marker pattern is unam-
biguous and points at a particular relation. Using
one feature per marker can be seen as analogous
(yet complementary) to Zhou et al (2010)?s ap-
proach of trying to predict the implicit connective
by giving a score to each marker using a language
model.
This work uses binary features which only in-
dicate the appearance of one or more of the pairs.
The original frequencies of the word pairs are not
used anywhere. A more powerful approach is to
use an informed function to weight the word pairs
used inside each feature.
3.3 Our Approach
Our approach is similar in that we choose to ag-
gregate word pairs that were collected around the
same explicit marker. We first assembled a list of
all 102 discourse markers used in PDTB, in both
explicit and implicit relations.2
Next, we extract word pairs for each marker
from the Gigaword corpus by taking the cross
product of words that appear in a sentence around
that marker. This is a simpler approach than us-
ing patterns - for example, the marker because can
appear in two patterns: [Arg1 because Arg2] and
[because Arg1, Arg2], and we only use the first.
We leave the task of listing the possible patterns
for each of the 102 markers to future work because
of the significant manual effort required. Mean-
while, we rely on the fact that we use a very large
corpus and hope that the simple pattern [Arg1
marker Arg2] is enough to make our features use-
ful. There are, of course, markers for which this
pattern does not normally apply, such as by com-
parison or on one hand. We expect these features
to be down-weighted by the final classifier, as ex-
plained at the end of this section. When collect-
ing the pairs, we stem the words and discard pairs
which appear only once around the marker.
We can think of each discourse marker as hav-
ing a corresponding unordered ?document?, where
each word pair is a term with an associated fre-
quency. We want to create a feature for each
marker such that for each data instance (that is,
for each potential relation in the PDTB data) the
value for the feature is the relevance of the marker
document to the data instance.
2in implicit relations, there is no marker in the text but the
implicit marker is provided by the human annotators
Each data instance in PDTB consists of two ar-
guments, and can therefore also be represented
as a set of word pairs extracted from the cross-
product of the two arguments. To represent the rel-
evance of the instance to each marker, we set the
value of the marker feature to the cosine similarity
of the data instance and the marker?s ?document?,
where each word pair is a dimension.
While the terms (i.e. word pairs) of the
data instance are weighted by simple occurence
count, we weight the terms in each marker?s
document with tf-idf, where tf is defined in
one of two ways: normalized term frequency
( count(t)max{count(s,d):s?d}) and pointwise mutual infor-
mation (log count(t)count(w1)?count(w2)), where w1 and w2are the member words of the pair. Idf is calculated
normally given that the set of all documents is de-
fined as the 102 marker documents.
We then train a binary classifier (logistic regres-
sion) using these 102 features for each of the four
high-level relations in PDTB: comparison, con-
tingency, expansion and temporal. To make sure
our results are comparable to previous work, we
treat EntRel relations as instances of expansion
and use sections 2-20 for training and sections 21-
22 for testing. We use a ten fold stratified cross-
validation of the training set for development. Ex-
plicit relations are excluded from all data sets.
As mentioned earlier, there are markers that do
not fit the simple pattern we use. In particular,
some markers always or often appear as the first
term of a sentence. For these, we expect the list of
word pairs to be empty or almost empty, since in
most sentences there are no words on the left (and
recall that we discard pairs that appear only once).
Since the features created for these markers will
be uninformative, we expect them to be weighted
down by the classifier and have no significant ef-
fect on prediction.
4 Evaluation of Word Pairs
For our main evaluation, we evaluate the perfor-
mance of word pair features when used with no
additional features. Results are shown in Table 1.
Our word pair features outperform the previous
formulation (represented by the results reported by
(Pitler et al, 2009), but used by virtually all previ-
ous work on this task). For most relation classes,
tf is significantly better than pmi. 3
3Significance was verified for our own results in all exper-
iments shown in this paper with a standard t-test
71
Comparison Contingency Expansion Temporal
Pitler et al, 2009 21.96 (56.59) 45.6 (67.1) 63.84 (60.28) 16.21 (61.98)
tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09)
pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53)
tf-idf, with stop list 23.77 44.33 65.33 16.98
Table 1: Main evaluation. F-measure (accuracy) for various implementations of the word pairs features
Comparison Contingency Expansion Temporal
Best System 25.4 (63.36) 46.94 (68.09) 75.87 (62.84) 20.23 (68.35)
features used pmi+1,2,3,6 tf+ALL tf+8 tf+3,9
Pitler et al, 2009 21.96 (56.59) 47.13 (67.3) 76.42 (63.62) 16.76 (63.49)
Zhou et al, 2010 31.79 (58.22) 47.16 (48.96) 70.11 (54.54) 20.3 (55.48)
Park and Cardie, 2012 31.32 (74.66) 49.82 (72.09) 79.22 (69.14) 26.57 (79.32)
Table 2: Secondary evaluation. F-measure (accuracy) for the best systems. tf and pmi refer to the word
pair features used (by tf implementation), and the numbers refer to the indeces of Table 3
Comp. Cont. Exp. Temp.
1 WordNet 20.07 34.07 52.96 11.58
2 Verb Class 14.24 24.84 49.6 10.04
3 MPN 23.84 38.58 49.97 13.16
4 Modality 17.49 28.92 13.84 10.72
5 Polarity 16.46 26.36 65.15 11.58
6 Affect 18.62 31.59 59.8 13.37
7 Similarity 20.68 34.5 43.16 12.1
8 Negation 8.28 22.47 75.87 11.1
9 Length 20.75 31.28 65.72 10.19
Table 3: F-measure for each feature category
We also show results using a stop list of 50 com-
mon functional words. The stop list has only a
small effect on performance except in the tempo-
ral class. This may be because of functional words
like was and will which have a temporal effect.
5 Other Features
For our secondary evaluation, we include addi-
tional features to complement the word pairs. Pre-
vious work has relied on features based on the gold
parse trees of the Penn Treebank (which overlaps
with PDTB) and on contextual information from
relations preceding the one being disambiguated.
We intentionally limit ourselves to features that do
not require either so that our system can be readily
used on arbitrary argument pairs.
WordNet Features: We define four features
based on WordNet (Fellbaum, 1998) - Synonyms,
Antonyms, Hypernyms and Hyponyms. The values
are the counts of word pairs in the cross-product of
the words in the arguments that have the particular
relation (synonymy, antonymy etc) between them.
Verb Class: This is the count of pairs of verbs
from Arg1 and Arg2 that share the same class, de-
fined as the highest level Levin verb class (Levin,
1993) from the LCS database (Dorr, 2001).
Money, Percentages and Numbers (MPN): The
counts of currency symbols/abbreviations, per-
centage signs or cues (?percent?, ?BPS?...) and
numbers in each argument.
Modality: Presence or absence of each English
modal in each argument.
Polarity: Based on MPQA (Wilson et al, 2005).
We include the counts of positive and negative
words according to the MPQA subjectivity lexicon
for both arguments. Unlike Pitler et al (2009), we
do not use neutral polarity features. We also do not
explicitly group negation with polarity (although
we do have separate negation features).
Affect: Based on the Dictionary of Affect in Lan-
guage (Whissell, 1989). Each word in the DAL
gets a score for three dimensions - pleasantness
(pleasant - unpleasant), activation (passive - ac-
tive) and imagery (hard to imagine - easy to imag-
ine). We use the average score for each dimension
in each argument as a feature.
Content Similarity: We use the cosine similarity
and word overlap of the arguments as features.
Negation: Presence or absence of negation terms
in each of the arguments.
Length: The ratio between the lengths (counts of
words) of the arguments.
6 Evaluation of Additional Features
For our secondary evaluation, we present results
for each feature category on its own in Table 3 and
for our best system for each of the relation classes
in Table 2. We show results for the best systems
from (Pitler et al, 2009), (Zhou et al, 2010) and
72
(Park and Cardie, 2012) for comparison.
7 Conclusion
We presented an aggregated approach to word pair
features and showed that it outperforms the previ-
ous formulation for all relation types but contin-
gency. This is our main contribution. With this
approach, using a stop list does not have a major
effect on results for most relation classes, which
suggests most of the word pairs affecting perfor-
mance are content word pairs which may truly be
semantically related to the discourse structure.
In addition, we introduced the new and useful
WordNet, Affect, Length and Negation feature cat-
egories. Our final system outperformed the best
system from Pitler et al (2009), who used mostly
similar features, for comparison and temporal and
is competitive with the most recent state of the
art systems for contingency and expansion with-
out using any syntactic or context features.
Acknowledgments
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Studies in Natural Language Process-
ing Series. Cambridge University Press.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog by classifying text as argu-
mentative. International Journal of Semantic Com-
puting, 5(4):363?381, December.
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and refin-
ing rhetorical-semantic relation models. In HLT-
NAACL, pages 428?435. The Association for Com-
putational Linguistics.
Bonnie J. Dorr. 2001. LCS Verb Database, Online
Software Database of Lexical Conceptual Structures
and Documentation. University Of Maryland Col-
lege Park.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University Of
Chicago Press.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 343?351.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In ACL, pages 368?375. ACL.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108?112.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In ACL/IJCNLP (Short Papers), pages 13?16. The
Association for Computer Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In ACL/IJCNLP, pages 683?691. The
Association for Computer Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
Cynthia M. Whissell. 1989. The dictionary of affect in
language.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347?354.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
73
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 37?45,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Detecting Influencers in Written Online Conversations
Or Biran1* Sara Rosenthal1* Jacob Andreas1**
Kathleen McKeown1* Owen Rambow2?
1 Department of Computer Science, Columbia University, New York, NY 10027
2 Center for Computational Learning Systems, Columbia University, New York, NY 10027
* {orb, sara, kathy}@cs.columbia.edu
** jda2129@columbia.edu ?rambow@ccls.columbia.edu
Abstract
It has long been established that there is a cor-
relation between the dialog behavior of a par-
ticipant and how influential he or she is per-
ceived to be by other discourse participants.
In this paper we explore the characteristics of
communication that make someone an opinion
leader and develop a machine learning based
approach for the automatic identification of
discourse participants that are likely to be in-
fluencers in online communication. Our ap-
proach relies on identification of three types
of conversational behavior: persuasion, agree-
ment/disagreement, and dialog patterns.
1 Introduction
In any communicative setting where beliefs are ex-
pressed, some are more influential than others. An
influencer can alter the opinions of their audience,
resolve disagreements where no one else can, be rec-
ognized by others as one who makes important con-
tributions, and often continue to influence a group
even when not present. Other conversational par-
ticipants often adopt their ideas and even the words
they use to express their ideas. These forms of per-
sonal influence (Katz and Lazarsfeld, 1955) are part
of what makes someone an opinion leader. In this
paper, we explore the characteristics of communica-
tion that make someone an opinion leader and de-
velop a machine learning based approach for the au-
tomatic identification of discourse participants who
are likely to be influencers in online communication.
Detecting influential people in online conversa-
tional situations has relevance to online advertising
strategies which exploit the power of peer influence
on sites such as Facebook. It has relevance to analy-
sis of political postings, in order to determine which
candidate has more appeal or which campaign strat-
egy is most successful. It is also relevant for design-
ing automatic discourse participants for online dis-
cussions (?chatbots?) as it can provide insight into
effective communication. Despite potential applica-
tions, analysis of influence in online communication
is a new field of study in part because of the rela-
tively recent explosion of social media. Thus, there
is not an established body of theoretical literature in
this area, nor are there established implementations
on which to improve. Given this new direction for
research, our approach draws on theories that have
been developed for identifying influence in spoken
dialog and extends them for online, written dialog.
We hypothesize that an influencer, or an influencer?s
conversational partner, is likely to engage in the fol-
lowing conversational behaviors:
Persuasion: An influencer is more likely to express
personal opinions with follow-up (e.g., justification,
reiteration) in order to convince others.
Agreement/disagreement: A conversational partner
is more likely to agree with an influencer, thus im-
plicitly adopting his opinions.
Dialog Patterns: An influencer is more likely to par-
ticipate in certain patterns of dialog, for example
initiating new topics of conversation, contributing
more to dialog than others, and engendering longer
dialog threads on the same topic.
Our implementation of this approach comprises
a system component for each of these conversa-
tional behaviors. These components in turn provide
37
the features that are the basis of a machine learn-
ing approach for the detection of likely influencers.
We test this approach on two different datasets, one
drawn from Wikipedia discussion threads and the
other drawn from LiveJournal weblogs. Our results
show that the system performs better for detection
of influencer on LiveJournal and that there are in-
teresting differences across genres for detecting the
different forms of conversational behavior.
The paper is structured as follows. After review-
ing related work, we define influence, present our
data and methods. We present a short overview of
the black box components we use for persuasion and
detection of agreement/disagreement, but our focus
is on the development of the influencer system as a
whole and thus we spend most time exploring the
results of experimentation with the system on dif-
ferent data sets, analyzing which components have
most impact. We first review related work.
2 Related Work
It has long been established that there is a correlation
between the conversational behavior of a discourse
participant and how influential he or she is perceived
to be by the other discourse participants (Bales et al,
1951; Scherer, 1979; Brook and Ng, 1986; Ng et al,
1993; Ng et al, 1995). Specifically, factors such as
frequency of contribution, proportion of turns, and
number of successful interruptions have been identi-
fied as being important indicators of influence. Reid
and Ng (2000) explain this correlation by saying that
?conversational turns function as a resource for es-
tablishing influence?: discourse participants can ma-
nipulate the dialog structure in order to gain influ-
ence. This echoes a starker formulation by Bales
(1970): ?To take up time speaking in a small group
is to exercise power over the other members for at
least the duration of the time taken, regardless of the
content.? Simply claiming the conversational floor is
a feat of power. This previous work presents two is-
sues for a study aimed at detecting influence in writ-
ten online conversations.
First, we expect the basic insight ? conversation
as a resource for influence ? to carry over to written
dialog: we expect to be able to detect influence in
written dialog as well. However, some of the charac-
teristics of spoken dialog do not carry over straight-
forwardly to written dialog, most prominently the
important issue of interruptions: there is no interrup-
tion in written dialog. Our work draws on findings
for spoken dialog, but we identify characteristics of
written dialog which are relevant to influence.
Second, the insistence of Bales (1970) that power
is exercised through turn taking ?regardless of con-
tent? may be too strong. Reid and Ng (2000) discuss
experiments which address not just discourse struc-
ture features, but also a content feature which repre-
sents how closely a turn is aligned with the overall
discourse goal of one of two opposing groups (with
opposing opinions on a specific issue) participating
in the conversation. They show that interruptions are
more successful if aligned with the discourse goal.
They propose a model in which such utterances
?lead to participation which in turn predicts social
influence?, so that the correlation between discourse
structure and influence is really a secondary phe-
nomenon. However, transferring such results to
other types of interactions (for example, in which
there are not two well-defined groups) is challeng-
ing. In this study, we therefore examine two types of
features as they relate to influence: content-related
(persuasion and agreement/disagreement), and dis-
course structure-related.
So far, there has been little work in NLP related
to influencers. Quercia et al (2011) look at influ-
encers? language use in Twitter contrasted to other
users? groups and find some significant differences.
However, their analysis and definition relies quite
heavily on the particular nature of social activity
on Twitter. Rienks (2007) discusses detecting influ-
encers in a corpus of conversations. While he fo-
cuses entirely on non-linguistic behavior, he does
look at (verbal) interruptions and topic initiations
which can be seen as corresponding to some of our
Dialog Patterns Language Uses.
3 What is an Influencer?
Our definition of an influencer was collectively for-
mulated by a community of researchers involved in
the IARPA funded project on Socio Cultural Content
in Language (SCIL).
This group defines an influencer to be someone
who:
38
P1 by Arcadian <pc1>There seems to be a much better list at the National Cancer Institute than the one we?ve
got.</pc1><pa1>It ties much better to the actual publication (the same 11 sections, in the same order).</pa1>
I?d like to replace that section in this article. Any objections?
P2 by JFW <pc2><a1>Not a problem.</a1></pc2>Perhaps we can also insert the relative incidence as
published in this month?s wiki Blood journal
P3 by Arcadian I?ve made the update. I?ve included template links to a source that supports looking up
information by ICD-O code.
P4 by Emmanuelm Can Arcadian tell me why he/she included the leukemia classification to this lymphoma
page? It is not even listed in the Wikipedia leukemia page! <pc3>I vote for dividing the WHO classification
into 4 parts in 4 distinct pages: leukemia, lymphoma, histocytic and mastocytic neoplasms.</pc3><pa3>
Remember, Wikipedia is meant to be readable </pa3>by all. Let me know what you think before I delete
the non-lymphoma parts.
P5 by Arcadian Emmanuelm, aren?t you the person who added those other categories on 6 July 2005?
P6 by Emmanuelm <d1>Arcadian, I added only the lymphoma portion of the WHO classification.
You added the leukemias on Dec 29th.</d1>Would you mind moving the leukemia portion to the
leukemia page?
P7 by Emmanuelm <pc4>Oh, and please note that I would be very comfortable with a ?cross-coverage?
of lymphocytic leukemias in both pages.</pc4>My comment is really about myeloid, histiocytic and
mast cell neoplasms who share no real relationship with lymphomas.
P8 by Arcadian <pa5><a2>To simplify the discussion, I have restored that section to your version.
</a2></pa5>You may make any further edits, and <pc6>I will have no objection.</pc6>
P9 by JFW The full list should be on the hematological malignancy page, and the lymphoma part can be here.
<pc7>It would be defendable to list ALL and CLL here.</pc7><pa7>They fall under the lymphoproliferative
disorders.</pa7>
Table 1: Influence Example: A Wikipedia discussion thread displaying Emmanuelm as the influencer. Replies are
indicated by indentation (for example, P2 is a response to P1). All Language Uses are visible in this example: Attempt
to Persuade ({pci, pai}), Claims (pci), Argumentation (pai), Agreement (ai), Disagreement (di), and the five Dialog
Patterns Language Uses (eg. Arcadian has positive Initiative).
1. Has credibility in the group.
2. Persists in attempting to convince others, even if
some disagreement occurs
3. Introduces topics/ideas that others pick up on or
support.
By credibility, we mean someone whose ideas are
adopted by others or whose authority is explicitly
recognized. We hypothesize that this shows up
through agreement by other conversants. By per-
sists, we mean someone who is able to eventually
convince others and often takes the time to do so,
even if it is not quick. This aspect of our definition
corresponds to earlier work in spoken dialog which
shows that frequency of contributions and propor-
tion of turns is a method people use to gain influence
(Reid and Ng, 2000; Bales, 1970). By point 3, we
see that the influencer may be influential even in di-
recting where the conversation goes, discussing top-
ics that are of interest to others. This latter feature
can be measured through the discourse structure of
the interaction. The influencer must be a group par-
ticipant but need not be active in the discussion(s)
where others support/credit him.
The instructions that we provided to annotators
included this definition as well as examples of who
is not an influencer. We told annotators that if some-
one is in a hierarchical power relation (e.g., a boss),
then that person is not an influencer to sub-ordinates
(or, that is not the type of influencer we are look-
ing for). We also included someone with situational
power (e.g., authority to approve other?s actions) or
power in directing the communication (e.g., a mod-
erator) as negative examples.
We also gave positive examples of influencers. In-
fluencers include an active participant who argues
against a disorganized group and resolves a discus-
sion is an influencer, a person who provides an an-
swer to a posted question and the answer is accepted
after discussion, and a person who brings knowledge
to a discussion. We also provided positive and neg-
39
ative examples for some of these cases.
Table 1 shows an example of a dialog where there
is evidence of influence, drawn from a Wikipedia
Talk page. A participant (Arcadian) starts the thread
with a proposal and a request for support from other
participants. The influencer (Emmanuelm) later
joins the conversation arguing against Arcadian?s
proposal. There is a short discussion, and Arcadian
defers to Emmanuelm?s position. This is one piece
of dialog within this group where Emmanuelm may
demonstrate influence. The goal of our system is to
find evidence for situations like this, which suggests
that a person is more likely to be an influencer.
Since we attempt to find local influence (a per-
son who is influential in a particular thread, as op-
posed to influential in general), our notion of influ-
encer is consistent with diverse views on social in-
fluence. It is consistent with the definition of influ-
encer proposed by Gladwell (2001) and Katz (1957):
an exceptionally convincing and influential person,
set apart from everyone else by his or her ability to
spread opinions. While it superficially seems incon-
sistent with Duncan Watts? concept of ?accidental
influentials? (Watts, 2007), that view does not make
the assertion that a person cannot be influential in
a particular situation (in fact, it predicts that some-
one will) - only that one cannot in general identify
people who are always more likely to be influencers.
4 Data and Annotation
Our data set consists of documents from two differ-
ent online sources: weblogs from LiveJournal and
discussion forums from Wikipedia.
LiveJournal is a virtual community in which peo-
ple write about their personal experiences in a we-
blog. A LiveJournal entry is composed of a post
(the top-level content written by the author) and a
set of comments (written by other users and the au-
thor). Every comment structurally descends either
from the post or from another comment.
Each article on Wikipedia has a discussion forum
(called a Talk page) associated with it that is used
to discuss edits for the page. Each forum is com-
posed of a number of threads with explicit topics,
and each thread is composed of a set of posts made
by contributors. The posts in a Wikipedia discussion
thread may or may not structurally descend from
other posts: direct replies to a post typically descend
from it. Other posts can be seen as descending from
the topic of the thread.
For consistency of terms, from here on we refer to
each weblog or discussion forum thread as a thread
and to each post or comment as a post.
We have a total of 333 threads: 245 from Live-
Journal and 88 from Wikipedia. All were annotated
for influencers. The threads were annotated by two
undergraduate students of liberal arts. These stu-
dents had no prior training or linguistic background.
The annotators were given the full definition from
section 3 and asked to list the participants that they
thought were influencers. Each thread may in princi-
ple have any number of influencers, but one or zero
influencers per thread is the common case and the
maximal number of influencers found in our dataset
was two. The inter-annotator agreement on whether
or not a participant is an influencer (given by Co-
hen?s Kappa) is 0.72.
5 Method
Our approach is based on three conversational be-
haviors which are identified by separate system
components described in the following three sec-
tions. Figure 1 shows the pipeline of the Influencer
system and Table 1 displays a Wikipedia discussion
thread where there is evidence of an influencer and
in which we have indicated the conversational be-
haviors as they occur. Motivated by our definition,
each component is concerned with an aspect of the
likely influencer?s discourse behavior:
Persuasion examines the participant?s language to
identify attempts to persuade, such as {pc1, pa1} in
Table 1, which consist of claims (e.g. pc1) made
by the participant and supported by argumentations
(e.g. pa1). It also identifies claims and argumenta-
tions independently of one another (pc4 and pa5).
Agreement/Disagreement examines the other par-
ticipants? language to find how often they agree or
disagree with the participant?s statements. Examples
are a1 and d1 in Table 1.
Dialog Patterns examines how the participant inter-
acts in the discussion structurally, independently of
the content and the language used. An example of
this is Arcadian being the first poster and contribut-
ing the most posts in the thread in Table 1.
40
Figure 1: The influencer pipeline. Solid lines indicate
black-box components, which we only summarize in this
paper. Dashed lines indicate components described here.
Each component contributes a number of Lan-
guage Uses which fall into that category of conver-
sational behavior and these Language Uses are used
directly as features in a supervised machine learn-
ing model to predict whether or not a participant is
an influencer. For example, Dialog Patterns con-
tributes the Language Uses Initiative, Irrelevance,
Incitation, Investment and Interjection.
The Language Uses of the Persuasion and Agree-
ment/Disagreement components are not described in
detail in this paper, and instead are treated as black
boxes (indicated by solid boxes in Figure 1). We
have previously published work on some of these
(Biran and Rambow, 2011; Andreas et al, 2012).
The remainder of this section describes them briefly
and provides the results of evaluations of their per-
formance (in Table 2). The next section describes
the features of the Dialog Patterns component.
5.1 Persuasion
This component identifies three Language Uses: At-
tempt to Persuade, Claims and Argumentation.
We define an attempt to persuade as a set of con-
tributions made by a single participant which may
be made anywhere within the thread, and which are
all concerned with stating and supporting a single
claim. The subject of the claim does not matter:
an opinion may seem trivial, but the argument could
still have the structure of a persuasion.
Our entire data set was annotated for attempts to
persuade. The annotators labeled the text partici-
pating in each instance with either claim, the stated
opinion of which the author is trying to persuade
others or argumentation, an argument or evidence
that supports that claim. An attempt to persuade
must contain exactly one claim and at least one in-
stance of argumentation, like the {claim, argumen-
tation} pairs {pc1, pa1} and {pc3, pj3} in Table 1.
In addition to the complete attempt to persuade
Language Use, we also define the less strict Lan-
guage Uses claims and argumentation, which use
only the subcomponents as stand-alones.
Our work on argumentation, which builds on
Rhetorical Structure Theory (Mann and Thompson,
1988), is described in (Biran and Rambow, 2011).
5.2 Agreement/Disagreement
Agreement and disagreement are two Language
Uses that model others? acceptance of the partici-
pant?s statements. Annotation (Andreas et al, 2012)
is performed on pairs of phrases, {p1, p2}. A phrase
is a substring of a post or comment in a thread. The
annotations are directed since each post or comment
has a time stamp associated with it. This means that
p1 and p2 are not interchangeable. p1 is called the
?target phrase?, and p2 is called the ?subject phrase?.
A person cannot agree with him- or herself, so the
author of p1 and p2 cannot be the same. Each anno-
tation is also labeled with a type: either ?agreement?
or ?disagreement?.
6 Dialog Patterns
The Dialog Patterns component extracts features
based on the structure of the thread. Blogs and dis-
cussion threads have a tree structure, with a blog
post or a topic of discussion as the root and a set of
41
Component Wikipedia LiveJournal
P R F P R F
Attempt 79.1 69.6 74 57.5 48.2 52.4
to persuade
Claims 83.6 74.5 78.8 53.7 13.8 22
Argumentation 23.3 91.7 37.1 30.9 48.9 37.8
Agreement 12 31.9 17.4 20 50 28.6
Disagreement 8.7 9.5 9.1 6.3 14.3 8.7
Table 2: Performance of the black-box Language Uses in
terms of Precision (P), Recall (R), and F-measure(F).
Conversational
Behavior
Language Use
(Feature)
Users
Component A J E
Persuasion Claims 2/6 2/6 2/6
Argumentation Y Y Y
Attempt to Per-
suade
Y Y Y
Agreement/ Agreement 1/1 0/1 0/1
Disagreement Disagreement 1/1 0/1 0/1
Dialog Initiative Y N N
Patterns Irrelevance 2/4 1/2 1/3
Incitation 4 1 3
Interjection 1/9 2/9 4/9
Investment 4/9 2/9 3/9
Table 3: The feature values for each of the partici-
pants, Arcadian (A), JFW (J), and Emmanuelm (E), in
the Wikipedia discussion thread shown in Table 1.
comments or posts which are marked as a reply - ei-
ther to the root or to an earlier post. The hypothesis
behind Dialog Patterns is that influencers have typ-
ical ways in which they participate in a thread and
which are visible from the structure alone.
The Dialog Patterns component contains five sim-
ple Language Uses:
Initiative The participant is or is not the first poster
of the thread.
Irrelevance The percentage of the participant?s
posts that are not replied to by anyone.
Incitation The length of the longest branch of
posts which follows one of the participant?s posts.
Intuitively, the longest discussion started directly by
the participant.
Investment The participant?s percentage of all posts
in the thread.
Interjection The point in the thread, represented
as percentage of posts already posted, at which the
participant enters the discussion.
7 System and Evaluation
The task of the system is to decide for each partici-
pant in a thread whether or not he or she is an influ-
encer in that particular thread. It is realized with a
supervised learning model: we train an SVM with a
small number of features, namely the ten Language
Uses. One of our goals in this work is to evaluate
which Language Uses allow us to more accurately
classify someone as an influencer. Table 3 shows
the full feature set and feature values for the sample
discussion thread in Table 1. We experimented with
a number of different classification methods, includ-
ing bayesian and rule-based models, and found that
SVM produced the best results.
7.1 Evaluation
We evaluated on Wikipedia and LiveJournal sepa-
rately. The data set for each corpus consists of all
participants in all threads for which there was at least
one influencer. We exclude threads for which no in-
fluencer was found, narrowing our task to finding the
influencers where they exist. For each participant X
in each thread Y, the system answers the following
question: Is X an influencer in Y?
We used a stratified 10-fold cross validation of
each data set for evaluation, ensuring that the same
participant (from two different threads) never ap-
peared in both training and test at each fold, to elim-
inate potential bias from fitting to a particular partic-
ipant?s style. The system components were identical
when evaluating both data sets, except for the claims
system which was trained on sentiment-annotated
data from the corpus on which it was evaluated.
Table 4 shows the performance of the full system
and of systems using only one Language Use feature
compared against a baseline which always answers
positively (X is always an influencer in Y). It also
shows the performance for the best system, which
was found for each data set by looking at all possible
combinations of the features. The best system for
the Wikipedia data set is composed of four features:
Claims, Argumentation, Agreement and Investment.
The best LiveJournal system is composed of all five
Dialog Patterns features, Attempt to Persuade and
Argumentation. We found our results to be statis-
42
System Wikipedia LiveJournal
P R F P R F
Baseline: all-
yes
16.2 100 27.9 19.2 19.2 32.2
Full 40.5 80.5 53.9 61.7 82 70.4
Initiative 31.6 31.2 31.4 73.5 72.7 73.1
Irrelevance 21.7 77.9 34 19.2 100 32.2
Incitation 28.3 77.9 41.5 49.5 73.8 59.2
Investment 43 71.4 53.7 50.2 75.4 60.3
Interjection 24.7 88.3 38.6 36.9 91.3 52.5
Agreement 36 46.8 40.7 45.1 82.5 58.3
Disagreement 35.3 70.1 47 19.2 100 32.2
Claims 40 72.7 51.6 54.3 76 63.3
Argumentation 19 98.7 31.8 31.1 85.2 45.6
Attempt 23.7 79.2 36.5 37.4 48.1 42.1
to persuade
Best system 47 80.5 59.3 66.2 84.7 74.3
Table 4: Performance in terms of Precision (P), Recall
(R), and F-measure (F) using the baseline (everyone is an
influencer), all features (full), individual features one at a
time, and the best feature combination for each data set.
tically significant (with the Bonferroni adjustment)
in paired permutation tests between the best system,
the full system and the baseline of each data set.
When we first performed these experiments, we
used all threads in the data set. The performance on
this full set was lower, as shown in Table 5 due to
the presence of threads with no influencers. Threads
in which the annotators could not find a clear influ-
encer tend to be of a different nature: there is either
no clear topic of discussion, or no argument (every-
one is in agreement). We leave the task of distin-
guishing these threads from those which are likely
to have an influencer to future work.
7.2 Evaluating with Perfect Components
In a hierarchical system such as ours, errors can
be attributed to imperfect components or to a bad
choice of features, so it is important to look at the
potential contribution of the components. As an ex-
ample, Table 6 shows the difference between our
Attempt to Persuade system and a hypothetical per-
fect Attempt to Persuade component, simulated by
using the gold annotations, when predicting influ-
encer directly (i.e., a participant is an influencer iff
she makes an attempt to persuade).
Clearly, when predicting influencers, Attempt to
System Wikipedia LiveJournal
P R F P R F
Baseline 13.9 100 24.5 14.2 100 24.9
Full 36.7 79.2 50.2 46.3 79.8 58.6
Best 40.1 76.6 52.7 48.2 81.4 60.6
Table 5: Performance on the data set of all threads, in-
cluding those with no influencers. The ?Best System? is
the system that performed best on the filtered data set.
Data Set Our System Gold Answers
P R F P R F
Wikipedia 23.6 69.4 35.2 23.8 81.6 36.9
LiveJournal 37.5 48.1 42.1 40.7 61.8 49
Table 6: Performance of the Attempt to Persuade compo-
nent in directly predicting influencers. A comparison of
our system and the component?s gold annotation. These
experiments were run on the full data set, which is why
the system results are not exactly those of Table 4.
Persuade is a stronger indicator in LiveJournal than
it is in Wikipedia. However, as shown in Table 2,
our Attempt to Persuade system performs better on
Wikipedia. This situation is reflected in Table 6,
where the lower quality of the system component in
LiveJournal corresponds to a significantly lower per-
formance when applied to the influencer task. These
results demonstrate that Attempt to Persuade is a
good feature: a more precise feature value means
higher predictability of influencer. In the future we
will perform similar analyses for the other features.
8 Discussion
We evaluated our system on two corpora - Live-
Journal and Wikipedia discussions - which differ in
structure, context and discussion topics. As our re-
sults show, they also differ in the way influencers
behave and the way others respond to them. To
illustrate the differences, we contrast the sample
Wikipedia thread (Table 1) with an example from
LiveJournal (Table 7).
It is common in LiveJournal for the blogger to be
an influencer, as is the case in our example thread,
because the topic of the thread is set by the blog-
ger and comments are typically made by her friends.
This fact is reflected in our results: Initiative is a
very strong indicator in LiveJournal, but not so in
43
P1 by poconell <pc1>He really does make good on his promises! </pc1><pa1>Day three in office, and the
Global Gag Rule (A.K.A?The Mexico City Policy?) is gone!</pa1>I was holding my breath, hoping it
wouldn?t be left forgotte. He didn?t wait. <pc2>He can see the danger and risk in this policy, and the damage
it has caused to women and families.</pc2><pc3>I love that man!</pc3>
P2 by thalialunacy <a1>I literally shrieked ?HELL YES!? in my car when I heard. :D:D:D</a1>
P3 by poconell <a2>Yeah, me too</a2>
P4 by lunalovepotter <pc4><a3>He is SO AWESOME!</a3></pc4><pa4>Right down to business, no
ifs, ands, or buts! :D</pa4>
P5 by poconell <pc5>It?s amazing to see him so serious too!</pc5><pa5>This is one tough,
no-nonsense man!</pa5>
P6 by penny sieve My icon says it all :)
P7 by poconell <pc6>And I?m jealous of you with that President!</pc6><pa6>We tried to overthrow
our Prime Minister, but he went crying to the Governor General. </pa6>
Table 7: Influence Example: A LiveJournal discussion thread displaying poconell as the influencer. All the Language
Uses are visible in this example: agreement/disagreement (ai/di), persuasion ({pci, pai}, pci, pai), and dialog patterns
(eg. poconell has positive Initiative). This example is very different from the Wikipedia example in Table 1.
Wikipedia, where the discussion is between a group
of editors, all of whom are equally interested in the
topic. In general, the Dialog Patterns features are
stronger in LiveJournal. We believe this is due to the
fact that the tree structure in LiveJournal is strictly
enforced. In Wikipedia, people do not always reply
directly to the relevant post. Investment is the excep-
tion: it does not make use of the tree structure, and
is therefore an important indicator in Wikipedia.
Attempt to Persuade is useful in LiveJournal (the
influencer poconell makes three attempts to per-
suade in Table 7) but less so in Wikipedia. This is
explained by the precision of the gold system in Ta-
ble 6. Only 23.8% of those who attempt to persuade
in Wikipedia are influencers, compared with 40.7%
in LiveJournal. Attempts to Persuade are more com-
mon in Wikipedia (all participants attempt to per-
suade in Table 1), since people write there specifi-
cally to argue their opinion on how the article should
be edited. Conversely, agreement is a stronger pre-
dictor of influence in Wikipedia than in LiveJournal;
we believe that is because of a similar phenomenon,
that people in LiveJournal (who tend to know each
other) agree with each other more often. Disagree-
ment is not a strong indicator for either corpus which
may say something about influencers in general -
they can be disagreed with as often as anyone else.
9 Conclusion and Future Work
We have studied the relevance of content-related
conversational behavior (persuasion and agree-
ment/disagreement), and discourse structure-related
conversational behavior to detection of influence.
Identifying influencers is a hard task, but we are
able to show good results on the LiveJournal corpus
where we achieve an F-measure of 74.3%. Despite
a lower performance on Wikipedia, we are still able
to significantly outperform the baseline which yields
only 28.2%. Differences in performance between
the two seem to be attributable in part to the more
straightforward dialog structure in LiveJournal.
There are several areas for future work. In our
current work, we train and evaluate separately for
our two corpora. Alternatively, we could investigate
different training and testing combinations: train on
one corpus and evaluate on the other; a mixed cor-
pus for training and testing; genre-independent cri-
teria for developing different systems (e.g. length of
thread). We will also evaluate on new genres (such
as the Enron emails) in order to gain an appreciation
of how different genres of written dialog are.
Acknowledgment
This work has been supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0141. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon.
44
References
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagreement
in threaded discussion. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Computation (LREC), Istanbul, Turkey, May.
R. F. Bales, Strodtbeck, Mills F. L., T. M., and M. Rose-
borough. 1951. Channels of communication in small
groups. American Sociological Review, pages 16(4),
461?468.
R. F. Bales. 1970. Personality and interpersonal be-
haviour.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog. In Proceedings of the Fifth
IEEE International Conference on Semantic Comput-
ing.
M.E. Brook and S. H. Ng. 1986. Language and social
influence in small conversational groups. Journal of
Language and Social Psychology, pages 5(3), 201?
210.
Malcolm Gladwell. 2001. The tipping point: how little
things can make a big difference. Abacus.
Elihu Katz and Paul F. Lazarsfeld. 1955. Personal in-
fluence. Free Press, Glencoe, IL. by Elihu Katz and
Paul F. Lazarsfeld. With a foreword by Elmo Roper.
?A report of the Bureau of Applied Social Research,
Columbia University.? Bibliography: p. 381-393.
E. Katz. 1957. The Two-Step Flow of Communication:
An Up-To-Date Report on an Hypothesis. Bobbs-
Merrill Reprint Series in the Social Sciences, S137.
Ardent Media.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
S. H. Ng, D. Bell, and M. Brooke. 1993. Gaining turns
and achieving high in influence ranking in small con-
versational groups. British Journal of Social Psychol-
ogy, pages 32, 265?275.
S. H. Ng, M Brooke, and M. Dunne. 1995. Interruption
and in influence in discussion groups. Journal of Lan-
guage and Social Psychology, pages 14(4),369?381.
Daniele Quercia, Jonathan Ellis, Licia Capra, and Jon
Crowcroft. 2011. In the mood for being influential on
twitter. In SocialCom/PASSAT, pages 307?314. IEEE.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as a
resource for in influence: evidence for prototypical ar-
guments and social identification processes. European
Journal of Social Psychology, pages 30, 83?100.
Rutger Joeri Rienks. 2007. Meetings in smart environ-
ments : implications of progressing technology. Ph.D.
thesis, Enschede, the Netherlands, July.
K. R. Scherer. 1979. Voice and speech correlates of per-
ceived social influence in simulated juries. In H. Giles
and R. St Clair (Eds), Language and social psychol-
ogy, pages 88?120. Oxford: Blackwell.
Duncan Watts. 2007. The accidental influentials. Har-
vard Business Review.
45
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 85?92,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Technologies in IBM WatsonTM
Alfio Gliozzo
IBM Watson Research Center
Yorktown Heights, NY 10598
gliozzo@us.ibm.com
Or Biran
Columbia University
New York, NY 10027
orb@cs.columbia.edu
Siddharth Patwardhan
IBM Watson Research Center
Yorktown Heights, NY 10598
siddharth@us.ibm.com
Kathleen McKeown
Columbia University
New York, NY 10027
kathy@cs.columbia.edu
Abstract
This paper describes a seminar course de-
signed by IBM and Columbia University
on the topic of Semantic Technologies,
in particular as used in IBM WatsonTM
? a large scale Question Answering sys-
tem which famously won at Jeopardy! R?
against two human grand champions. It
was first offered at Columbia University
during the 2013 spring semester, and will
be offered at other institutions starting in
the fall semester. We describe the course?s
first successful run and its unique features:
a class centered around a specific indus-
trial technology; a large-scale class project
which student teams can choose to par-
ticipate in and which serves as the ba-
sis for an open source project that will
continue to grow each time the course is
offered; publishable papers, demos and
start-up ideas; evidence that the course can
be self-evaluating, which makes it poten-
tially appropriate for an online setting; and
a unique model where a large company
trains instructors and contributes to creat-
ing educational material at no charge to
qualifying institutions.
1 Introduction
In 2007, IBM Research took on the grand chal-
lenge of building a computer system that can per-
form well enough on open-domain question an-
swering to compete with champions at the game of
Jeopardy! In 2011, the open-domain question an-
swering system dubbed Watson beat the two high-
est ranked players in a two-game Jeopardy! match.
To be successful at Jeopardy!, players must re-
tain enormous amounts of information, must have
strong language skills, must be able to understand
precisely what is being asked, and must accurately
determine the likelihood they know the right an-
swer. Over a four year period, the team at IBM
developed the Watson system that competed on
Jeopardy! and the underlying DeepQA question
answering technology (Ferrucci et al, 2010). Wat-
son played many games of Jeopardy! against cel-
ebrated Jeopardy! champions and, in games tele-
vised in February 2011, won against the greatest
players of all time, Ken Jennings and Brad Rutter.
DeepQA has applications well beyond Jeop-
ardy!, however. DeepQA is a software architec-
ture for analyzing natural language content in both
questions and knowledge sources. DeepQA dis-
covers and evaluates potential answers and gathers
and scores evidence for those answers in both un-
structured sources, such as natural language doc-
uments, and structured sources such as relational
databases and knowledge bases. Figure 1 presents
a high-level view of the DeepQA architecture.
DeepQA utilizes a massively parallel, component-
based pipeline architecture (Ferrucci, 2012) which
uses an extensible set of structured and unstruc-
tured content sources as well as a broad range of
pluggable search and scoring components that al-
low integration of many different analytic tech-
niques. Machine Learning techniques are used to
learn the weights for each scoring component in
order to combine them into a single final score.
Watson components include a large variety of state
of the art solutions originating in the fields of Nat-
ural Language Processing (NLP), Machine Learn-
ing (ML), Information Retrieval (IR), Semantic
Web and Cloud Computing. IBM is now aggres-
sively investing in turning IBM Watson from a re-
search prototype to an industry level highly adapt-
able system to be applied in dozens of business ap-
85
Figure 1: Overview of the DeepQA architecture
plications ranging from healthcare to finance (Fer-
rucci et al, 2012).
Finding that particular combination of skills in
the entry-level job market is hard: in many cases
students have some notion of Machine Learning
but are not strong in Natural Language Processing;
in other cases they have background in Knowledge
Management and some of the basics of Semantic
Web, but lack an understanding of statistical mod-
els and Machine Learning. In most cases semantic
integration is not a topic of interest, and so un-
derstanding sophisticated platforms like Apache
UIMATM (Ferrucci and Lally, 2004) is a chal-
lenge. Learning how to develop the large scale in-
frastructure and technology needed for IBM Wat-
son prepares students for the real-world challenges
of large-scale natural language projects that are
common in industry settings and which students
have little experience with before graduation.
Of course, IBM is interested in hiring entry-
level students as a powerful way of scaling Wat-
son. Therefore, it has resolved to start an ed-
ucational program focused on these topics. Ini-
tially, tutorials were given at scientific conferences
(NAACL, ISWC and WWW, among others), uni-
versities and summer schools. The great number
of attendees (usually in the range of 50 to 150)
and strongly positive feedback received from the
students was a motivation to transform the didac-
tic material collected so far into a full graduate-
level course, which has been offered for the first
time at Columbia University. The course (which
is described in the rest of this paper) received very
positive evaluations from the students and will be
used as a template to be replicated by other part-
ner universities in the following year. Our ultimate
goal is to develop high quality didactic material
for an educational curriculum that can be used by
interested universities and professors all over the
world.
2 Syllabus and Didactic Material
The syllabus1 is divided equally between classes
specifically on the Watson system, its architec-
ture and technologies used within it, and classes
on more general topics that are relevant to these
technologies. In particular, background classes on
Natural Language Processing; Distributional Se-
mantics; the Semantic Web; Domain Adaptation
and the UIMA framework are essential for under-
standing the Watson system and producing suc-
cessful projects.
The course at Columbia included four lectures
by distinguished guest speakers from IBM, which
were advertised to the general Columbia commu-
nity as open talks. Instead of exams, the course
included two workshop-style presentation days:
one at the mid term and another at the end of the
1The syllabus is accessible on line http://www.
columbia.edu/?ag3366
86
course. During these workshops, all student teams
gave presentations on their various projects. At the
mid-term workshop, teams presented their project
idea and timeline, as well as related work and the
state-of-the-art of the field. At the final workshop,
they presented their completed projects, final re-
sults and demos. This workshop was also made
open to the Columbia community and in particu-
lar to faculty and affiliates interested in start-ups.
The workshops will be discussed in further detail
in the following sections. The syllabus is briefly
detailed here.
? Introduction: The Jeopardy! Challenge
The motivation behind Watson, the task and
its challenges (Prager et al, 2012; Tesauro et
al., 2012; Lewis, 2012).
? The DeepQA Architecture Chu-Carroll et
al. (2012b), Ferrucci (2012), Chu-Carroll et
al. (2012a), Lally et al (2012).
? Natural Language Processing Background
Pre-processing, tokenization, POS tagging,
named entity recognition, syntactic parsing,
semantic role labeling, word sense disam-
biguation, evaluation best practices and met-
rics.
? Natural Language Processing in Watson
Murdock et al (2012a), McCord et al (2012).
? Structured Knowledge in Watson Murdock
et al (2012b), Kalyanpur et al (2012), Fan et
al. (2012).
? Semantic Web OWL, RDF, Semantic Web
resources.
? Domain Adaptation Ferrucci et al (2012).
? UIMA The UIMA framework, Annotators,
Types, Descriptors, tools. Hands-on exercise
with the class project architecture (Epstein et
al., 2012).
? Midterm Workshop Presentations of each
team?s project idea and their research into re-
lated work and the state of the art.
? Distributional Semantics Miller et al
(2012), Gliozzo and Isabella (2005).
? Machine Learning and Strategy in Watson
? What Watson Tells Us About Cognitive
Computing
? Final Workshop Presentations of each
team?s final project implementation, evalua-
tion, demo and future plans.
3 Watson-like Architecture for Projects
The goal of the class projects was for the stu-
dents to learn to design and develop language tech-
nology components in an environment very sim-
ilar to IBM?s Watson architecture. We provided
the students with a plug-in framework for seman-
tic search, into which they could integrate their
project code. Student projects will be described
in the following section. This section details the
framework that was made available to the students
in order to develop their projects.
Like the Watson system, the project framework
for this class was built on top of Apache UIMA
(Ferrucci and Lally, 2004)2 ? an open-source
software architecture for building applications that
handle unstructured information.
The Watson system makes extensive use of
UIMA to enable interoperability and scale-out of a
large question answering system. The architecture
(viz., DeepQA) of Watson (Ferrucci, 2012) defines
several high-level ?stages? of analysis in the pro-
cessing pipeline, such as Question and Topic Anal-
ysis, Primary Search, Candidate Answer Genera-
tion, etc. Segmentation of the system into high-
level stages enabled a group of 25 researchers at
IBM to independently work on different aspects
of the system with little overhead for interoper-
ability and system integration. Each stage of the
pipeline clearly defined the inputs and outputs ex-
pected of components developed for that particu-
lar stage. The researchers needed only to adhere
to these input/output requirements for their indi-
vidual components to easily integrate them into
the system. Furthermore, the high-level stages in
Watson, enabled massive scale-out of the system
through the use of the asynchronous scaleout ca-
pability of UIMA-AS.
Using the Watson architecture for inspitration,
we developed a semantic search framework for the
class projects. As shown in Figure 2, the frame-
work consists of a UIMA pipeline that has several
high-level stages (similar to those of the Watson
system):
2http://uima.apache.org
87
Figure 2: Overview of the class project framework
1. Query Analysis
2. Primary Document Search
3. Structured Data Search
4. Query Expansion
5. Expanded Query Analysis
6. Secondary Document Search
The input to this system is provided by a Query
Collection Reader, which reads a list of search
queries from a text file. The Query Collec-
tion Reader is a UIMA ?collection reader? that
reads the text queries into memory data struc-
tures (UIMA CAS structures) ? one for each
text query. These UIMA CASes flow through the
pipeline and are processed by the various process-
ing stages. The processing stages are set up so
that new components designed to perform the task
of each processing stage can easily be added to the
pipeline (or existing components easily modified).
The expected inputs and outputs of components in
each processing stage are clearly defined, which
makes the task of the team building the component
simpler: they no longer have to deal with man-
aging data structures and are spared the overhead
of converting from and into formats of data ex-
changed between various components. All of the
overhead is handled by UIMA. Furthermore, some
of the processing stages generate new CAS struc-
tures and the flow of all the UIMA CAS structures
through this pipeline is controlled by a ?Flow Con-
troller? designed by us for this framework.
The framework was made available to each of
the student teams, and their task was to build
their project by extending this framework. Even
though we built the framework to perform seman-
tic search over a text corpus, many of the teams
in this course had projects that went far beyond
just semantic search. Our hope was that each team
would be able to able independently develop inter-
esting new components for the processing stages
of the pipeline, and at the end of the course we
would be able to merge the most interesting com-
ponents to create a single useful application. In the
following section, we describe the various projects
undertaken by the student teams in the class, while
Section 5 discusses the integration of components
from student projects and the demo application
that resulted from the integrated system.
4 Class Projects
Projects completed for this course fall into three
types: scientific projects, where the aim is to
produce a publishable paper; integrated projects,
where the aim is to create a component that will be
integrated into the class open-source project; and
independent demo projects, where the aim is to
produce an independent working demo/prototype.
The following section describes the integrated
projects briefly.
4.1 Selected Project Descriptions
As described in section 3, the integrated class
project is a system with an architecture which, al-
though greatly simplified, is reminiscent of Wat-
son?s. While originally intended to be simply a
semantic search tool, some of the student teams
created additional components which resulted in
a full question answering system. Those projects
88
as well as a few other related ones are described
below.
Question Categorization: Using the DBPedia
ontology (Bizer et al, 2009) as a semantic
type system, this project classifies questions
by their answer type. It can be seen as a sim-
plified version of the question categorization
system in Watson. The classification is based
on a simple bag-of-words approach with a
few additional features.
Answer Candidate Ranking: Given the answer
type as well as additional features derived by
the semantic search component, this project
uses regression to rank the candidate an-
swers which themselves come from semantic
search.
Twitter Semantic Search: Search in Twitter is
difficult due to the huge variations among
tweets in lexical terms, spelling and style, and
the limited length of the tweets. This project
employs LSA (Landauer and Dumais, 1997)
to cluster similar tweets and increase search
accuracy.
Fine-Grained NER in the Open Domain: This
project uses DBPedia?s ontology as a type
system for named entities of type Person.
Given results from a standard NER system,
it attempts to find the fine-grained classifica-
tion of each Person entity by finding the most
similar type. Similarity is computed using
traditional distributional methods, using the
context of the entity and the contexts of each
type, collected from Wikipedia.
News Frame Induction: Working with a large
corpus of news data collected by Columbia
Newsblaster, this team used the Machine
Linking API to tag entities with semantic
types. From there, they distributionally col-
lected ?frames? prevalent in the news do-
main such as ?[U.S President] meeting with
[British Prime Minister]?.
Other projects took on problems such as Sense
Induction, NER in the Biomedical domain, Se-
mantic Role Labeling, Semantic Video Search,
and a mobile app for Event Search.
5 System Integration and Demonstration
The UIMA-based architecture described in section
3 allows us to achieve a relatively easy integra-
tion of different class projects, independently de-
veloped by different teams, in a common archi-
tecture and expose their functionality with a com-
bined class project demo. The demo is a collab-
oratively developed semantic search engine which
is able to retrieve knowledge from structured data
and visualize it for the user in a very concise way.
The input is a query; it can be a natural language
question or simply a set of keywords. The output
is a set of entities and their relations, visualized
as an entity graph. Figure 3 shows the results of
the current status of our class project demo on the
following Jeopardy! question.
This nation of 200 million has fought
small independence movements like
those in Aceh and East Timor.
The output is a set of DBPedia entities related to
the question, grouped by Type (provided by the
DBPedia ontology). The correct answer, ?Indone-
sia?, is among the candidate entities of type Place.
Note that only answers of type Place and Agent
have been selected: this is due to the question cate-
gorization component, implemented by one of the
student teams, that allows us to restrict the gener-
ated answer set to those answers having the right
types.
The demo will be hosted for one year fol-
lowing the end of the course at http://
watsonclass.no-ip.biz. Our goal is to
incrementally improve this demo, leveraging any
new projects developed in future versions of the
course, and to build an open source software com-
munity involving students taking the course.
6 Evaluation
The course at Columbia drew a relatively large au-
dience. A typical size for a seminar course on a
special topic is estimated at 15-20 students, while
ours drew 35. The vast majority were Master?s stu-
dents; there were also three PhD students and five
undergraduates.
During the student workshops, students were
asked to provide grades for each team?s presen-
tation and project. After the instructor indepen-
dently gave his own grades, we looked at the cor-
relation between the average grades given by the
students and those give by the instructor. While
89
Figure 3: Screenshot of the project demo
90
Team 1 2 3 4 5 6 7 8 9 10 11
Instructor?s grade B+ B C+ A- B- A+ B B- B+ A B-
TA?s grade B+ B B A B- A B- B+ B+ A C+
Class? average grade B/B+ B+/A- B/B+ A- B/B+ A- B+ A-/A B+/A- A-/A B/B+
Table 1: Grades assigned to class projects
the students tended to be more ?generous? (their
average grade for each team was usually half a
grade above the instructor?s), the agreement was
quite high. Table 1 shows the grades given by the
instructor, the teaching assistant and the class av-
erage for the midterm workshop.
Feedback about the course from the students
was very good. Columbia provides electonic
course evaluations to the students which are com-
pletely optional. Participation in the evaluation for
this course was just under 50% in the midterm
evaluation and just over 50% in the final eval-
uation. The scores (all in the 0-5 range) given
by the students in relevant categories were quite
high: ?Overall Quality? got an average score of
4.23, ?Amount Learned? got 4, ?Appropriateness
of Workload? 4.33 and ?Fairness of Grading Pro-
cess? got 4.42.
The course resulted in multiple papers that are
or will soon be under submission, as well as a few
projects that may be developed into start-ups. Al-
most all student teams agreed to share their code
in an open source project that is currently being
set up, and which will include the current question
answering and semantic search system as well as
additional side projects.
7 Conclusion
We described a course on the topic of Semantic
Technologies and the IBM Watson system, which
features a diverse curriculum tied together by its
relevance to an exciting, demonstrably successful
real-world system. Through a combined architec-
ture inspired by Watson itself, the students get the
experience of developing an NLP-heavy compo-
nent with specifications mandated by the larger
architecture, which requires a combination of re-
search and software engineering skills that is com-
mon in the industry.
An exciting result of this course is that the
class project architecture and many of the student
projects are to be maintained as an open source
project which the students can, if they choose,
continue to be involved with. The repository and
community of this project can be expected to grow
each time the class is offered. Even after one class,
it already contains an impressive semantic search
system.
Feedback for this course from the students
was excellent, and many teams have achieved
their personal goals as stated at the beginning of
the semester, including paper submissions, opera-
tional web demos and mobile apps.
Our long term goal is to replicate this course in
multiple top universities around the world. While
IBM does not have enough resources to always
do this with its own researchers, it is instead go-
ing to provide the content material and the open
source code generated so far to other universities,
encouraging professors to teach the course them-
selves. Initially we will work on a pilot phase
involving only a restricted number of professors
and researchers that are already in collaboration
with IBM Research, and eventually (if the posi-
tive feedback we have seen so far is repeated in
the pilot phase) give access to the same content to
a larger group.
References
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia?
Crystallization Point for the Web of Data. Journal
of Web Semantics: Science, Services and Agents on
the World Wide Web, 7(3):154?165, September.
J. Chu-Carroll, J. Fan, B. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012a. Finding Nee-
dles in the Haystack: Search and Candidate Gener-
ation. IBM Journal of Research and Development,
56(3.4):6:1?6:12.
J. Chu-Carroll, J. Fan, N. Schlaefer, and W. Zadrozny.
2012b. Textual Resource Acquisition and Engineer-
ing. IBM Journal of Research and Development,
56(3.4):4:1?4:11.
E. Epstein, M. Schor, B. Iyer, A. Lally, E. Brown, and
J. Cwiklik. 2012. Making Watson Fast. IBM Jour-
nal of Research and Development, 56(3.4):15:1?
15:12.
J. Fan, A. Kalyanpur, D. Gondek, and D. Ferrucci.
2012. Automatic Knowledge Extraction from Doc-
uments. IBM Journal of Research and Development,
56(3.4):5:1?5:10.
91
D. Ferrucci and A. Lally. 2004. UIMA: an Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan,
D. Gondek, A. Kalyanpur, A. Lally, J. W. Murdock,
E. Nyberg, J. Prager, N. Schlaefer, and C. Welty.
2010. Building Watson: An Overview of the
DeepQA project. AI magazine, 31(3):59?79.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and
E. Mueller. 2012. Watson: Beyond Jeopardy. Arti-
ficial Intelligence (in press).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1:1?1:15.
A. Gliozzo and T. Isabella. 2005. Semantic Domains
in Computational Linguistics. Technical report.
A. Kalyanpur, B. Boguraev, S. Patwardhan, J. W.
Murdock, A. Lally, C. Welty, J. Prager, B. Cop-
pola, A. Fokoue-Nkoutche, L. Zhang, Y. Pan, and
Z. Qiu. 2012. Structured Data and Inference in
DeepQA. IBM Journal of Research and Develop-
ment, 56(3.4):10:1?10:14.
A. Lally, J. Prager, M. McCord, B. Boguraev, S. Pat-
wardhan, J. Fan, P. Fodor, and J. Chu-Carroll. 2012.
Question Analysis: How Watson Reads a Clue. IBM
Journal of Research and Development, 56(3.4):2:1?
2:14.
T. Landauer and S. Dumais. 1997. A Solution to
Plato?s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction and Representation
of Knowledge. Psychological Review, 104(2):211?
240.
B. Lewis. 2012. In the Game: The Interface between
Watson and Jeopardy! IBM Journal of Research and
Development, 56(3.4):17:1?17:6.
M. McCord, J. W. Murdock, and B. Boguraev. 2012.
Deep Parsing in Watson. IBM Journal of Research
and Development, 56(3.4):3:1?3:15.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych.
2012. Using Distributional Similarity for Lexical
Expansion in Knowledge-based Word Sense Disam-
biguation. In Proceedings of the International Con-
ference on Computational Linguistics, pages 1781?
1796, Mumbai, India, December.
J. W. Murdock, J. Fan, A. Lally, H. Shima, and
B. Boguraev. 2012a. Textual Evidence Gathering
and Analysis. IBM Journal of Research and Devel-
opment, 56(3.4):8:1?8:14.
J. W. Murdock, A. Kalyanpur, C. Welty, J. Fan, D. Fer-
rucci, D. Gondek, L. Zhang, and H. Kanayama.
2012b. Typing Candidate Answers Using Type Co-
ercion. IBM Journal of Research and Development,
56(3.4):7:1?7:13.
J. Prager, E. Brown, and J. Chu-Carroll. 2012. Spe-
cial Questions and Techniques. IBM Journal of Re-
search and Development, 56(3.4):11:1?11:13.
G. Tesauro, D. Gondek, J. Lenchner, J. Fan, and
J. Prager. 2012. Simulation, Learning, and Op-
timization Techniques in Watson?s Game Strate-
gies. IBM Journal of Research and Development,
56(3.4):16:1?16:11.
92
