Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1222?1232,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SPred: Large-scale Harvesting of Semantic Predicates
Tiziano Flati and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{flati,navigli}@di.uniroma1.it
Abstract
We present SPred, a novel method for the
creation of large repositories of semantic
predicates. We start from existing colloca-
tions to form lexical predicates (e.g., break
?) and learn the semantic classes that best
fit the ? argument. To do this, we extract
all the occurrences in Wikipedia which
match the predicate and abstract its argu-
ments to general semantic classes (e.g.,
break BODY PART, break AGREEMENT,
etc.). Our experiments show that we are
able to create a large collection of seman-
tic predicates from the Oxford Advanced
Learner?s Dictionary with high precision
and recall, and perform well against the
most similar approach.
1 Introduction
Acquiring semantic knowledge from text automat-
ically is a long-standing issue in Computational
Linguistics and Artificial Intelligence. Over the
last decade or so the enormous abundance of in-
formation and data that has become available has
made it possible to extract huge amounts of pat-
terns and named entities (Etzioni et al, 2005), se-
mantic lexicons for categories of interest (Thelen
and Riloff, 2002; Igo and Riloff, 2009), large do-
main glossaries (De Benedictis et al, 2013) and
lists of concepts (Katz et al, 2003). Recently,
the availability of Wikipedia and other collabora-
tive resources has considerably boosted research
on several aspects of knowledge acquisition (Hovy
et al, 2013), leading to the creation of several
large-scale knowledge resources, such as DBPe-
dia (Bizer et al, 2009), BabelNet (Navigli and
Ponzetto, 2012), YAGO (Hoffart et al, 2013),
MENTA (de Melo and Weikum, 2010), to name
but a few. This wealth of acquired knowledge
is known to have a positive impact on important
fields such as Information Retrieval (Chu-Carroll
and Prager, 2007), Information Extraction (Krause
et al, 2012), Question Answering (Ferrucci et al,
2010) and Textual Entailment (Berant et al, 2012;
Stern and Dagan, 2012).
Not only are these knowledge resources ob-
tained by acquiring concepts and named entities,
but they also provide semantic relations between
them. These relations are extracted from unstruc-
tured or semi-structured text using ontology learn-
ing from scratch (Velardi et al, 2013) and Open
Information Extraction techniques (Etzioni et al,
2005; Yates et al, 2007; Wu and Weld, 2010;
Fader et al, 2011; Moro and Navigli, 2013) which
mainly stem from seminal work on is-a relation
acquisition (Hearst, 1992) and subsequent devel-
opments (Girju et al, 2003; Pasca, 2004; Snow et
al., 2004, among others).
However, these knowledge resources still lack
semantic information about language units such
as phrases and collocations. For instance, which
semantic classes are expected as a direct object
of the verb break? What kinds of noun does the
adjective amazing collocate with? Recognition of
the need for systems that are aware of the selec-
tional restrictions of verbs and, more in general, of
textual expressions, dates back to several decades
(Wilks, 1975), but today it is more relevant than
ever, as is testified by the current interest in se-
mantic class learning (Kozareva et al, 2008) and
supertype acquisition (Kozareva and Hovy, 2010).
These approaches leverage lexico-syntactic pat-
terns and input seeds to recursively learn the se-
mantic classes of relation arguments. However,
they require the manual selection of one or more
seeds for each pattern of interest, and this selec-
tion influences the amount and kind of semantic
classes to be learned. Furthermore, the learned
classes are not directly linked to existing resources
such as WordNet (Fellbaum, 1998) or Wikipedia.
The goal of our research is to create a large-
scale repository of semantic predicates whose lex-
ical arguments are replaced by their semantic
classes. For example, given the textual expres-
sion break a toe we want to create the correspond-
1222
ing semantic predicate break a BODY PART, where
BODY PART is a class comprising several lexical
realizations, such as leg, arm, foot, etc.
This paper provides three main contributions:
? We propose SPred, a novel approach which
harvests predicates from Wikipedia and gen-
eralizes them by leveraging core concepts
from WordNet.
? We create a large-scale resource made up of
semantic predicates.
? We demonstrate the high quality of our se-
mantic predicates, as well as the generality
of our approach, also in comparison with our
closest competitor.
2 Preliminaries
We introduce two preliminary definitions which
we use in our approach.
Definition 1 (lexical predicate). A lexical pred-
icate w1 w2 . . . wi ? wi+1 . . . wn is a regular
expression, where wj are tokens (j = 1, . . . , n), ?
matches any sequence of one or more tokens, and
i ? {0, . . . , n}. We call the token sequence which
matches ? the filling argument of the predicate.
For example, a * of milk matches occurrences
such as a full bottle of milk, a glass of milk, a car-
ton of milk, etc. While in principle * could match
any sequence of words, since we aim at general-
izing nouns, in what follows we allow ? to match
only noun phrases (e.g., glass, hot cup, very big
bottle, etc.).
Definition 2 (semantic predicate). A semantic
predicate is a sequence w1 w2 . . . wi c wi+1
. . . wn, where wj are tokens (j = 1, . . . , n),
c ? C is a semantic class selected from a fixed
set C of classes, and i ? {0, . . . , n}.
As an example, consider the semantic predicate
cup of BEVERAGE,1 where BEVERAGE is a se-
mantic class representing beverages. This pred-
icate matches phrases like cup of coffee, cup of
tea, etc., but not cup of sky. Other examples in-
clude: MUSICAL INSTRUMENT is played by, a
CONTAINER of milk, break AGREEMENT, etc.
Semantic predicates mix the lexical information
of a given lexical predicate with the explicit se-
mantic modeling of its argument. Importantly, the
same lexical predicate can have different classes as
its argument, like cup of FOOD vs. cup of BEVER-
AGE. Note, however, that different classes might
convey different semantics for the same lexical
1In what follows we denote the SEMANTIC CLASS in
small capitals and the lexical predicate in italics.
predicate, such as cup of COUNTRY, referring to
cup as a prize instead of cup as a container.
3 Large-Scale Harvesting of Semantic
Predicates
The goal of this paper is to provide a fully auto-
matic approach for the creation of a large repos-
itory of semantic predicates in three phases. For
each lexical predicate of interest (e.g., break ?):
1. We extract all its possible filling arguments
from Wikipedia, e.g., lease, contract, leg,
arm, etc. (Section 3.1).
2. We disambiguate as many filling arguments
as possible using Wikipedia, obtaining a
set of corresponding Wikipedia pages, e.g.,
Lease, Contract, etc. (Section 3.2).
3. We create the semantic predicates by general-
izing the Wikipedia pages to their most suit-
able semantic classes, e.g., break AGREE-
MENT, break LIMB, etc. (Section 3.3).
We can then exploit the learned semantic predi-
cates to assign the most suitable semantic class to
new filling arguments for the given lexical predi-
cate (Section 3.4).
3.1 Extraction of Filling Arguments
Let pi be an input lexical predicate (e.g., break ?).
We search the English Wikipedia for all the to-
ken sequences which match pi, resulting in a list
of noun phrases filling the ? argument. We show
an excerpt of the output obtained when searching
Wikipedia for the arguments of the lexical predi-
cate a * of milk in Table 1. As can be seen, a wide
range of noun phrases are extracted, from quanti-
ties such as glass and cup to other aspects, such as
brand and constituent.
The output of this first step is a set Lpi of triples
(a, s, l) of filling arguments a matching the lexi-
cal predicate pi in a sentence s of the Wikipedia
corpus, with a potentially linked to a page l (e.g.,
see the top 3 rows in Table 1; l =  if no link is
provided, see bottom rows of the Table).2 Note
that Wikipedia is the only possible corpus that can
be used here for at least two reasons: first, in or-
der to extract relevant arguments, we need a large
corpus of a definitional nature; second, we need
wide-coverage semantic annotations of filling ar-
guments.
3.2 Disambiguation of Filling Arguments
The objective of the second step is to disambiguate
as many arguments in Lpi as possible for the lex-
2We will also refer to l as the sense of a in sentence s.
1223
a full [[bottle]] of milk
a nice hot [[cup]] of milk
a cold [[glass]] of milk
a very big bottle of milk
a brand of milk
a constituent of milk
Table 1: An excerpt of the token sequences
which match the lexical predicate a * of milk in
Wikipedia (filling argument shown in the second
column; following the Wikipedia convention we
provide links in double square brackets).
ical predicate pi. We denote Dpi = {(a, s, l) :
l 6= } ? Lpi as the set of those arguments origi-
nally linked to the corresponding Wikipedia page
(like the top three linked arguments in Table 1).
Therefore, in the rest of this section we will focus
only on the remaining triples (a, s, ) ? Upi, where
Upi = Lpi \Dpi, i.e., those triples whose arguments
are not semantically annotated. Our goal is to re-
place  with an appropriate sense, i.e., page, for a.
For each such triple (a, s, ) ? Upi, we apply the
following disambiguation heuristics:
? One sense per page: if another occurrence
of a in the same Wikipedia page (indepen-
dent of the lexical predicate) is linked to a
page l, then remove (a, s, ) from Upi and add
(a, s, l) to Dpi. In other words, we propa-
gate an existing annotation of a in the same
Wikipedia page and apply it to our ambigu-
ous item. For instance, cup of coffee appears
in the Wikipedia page Energy drink in the
sentence ?[. . . ] energy drinks contain more
caffeine than a strong cup of coffee?, but this
occurrence of coffee is not linked. How-
ever the second paragraph contains the sen-
tence ?[[Coffee]], tea and other naturally caf-
feinated beverages are usually not considered
energy drinks?, where coffee is linked to the
Coffee page. This heuristic naturally reflects
the broadly known assumption about lexi-
cal ambiguity presented in (Yarowsky, 1995),
namely the one-sense-per-discourse heuris-
tic.
? One sense per lexical predicate: if
?(a, s?, l) ? Dpi, then remove (a, s, ) from
Upi and add (a, s, l) to Dpi. If multiple senses
of a are available, choose the most frequent
one in Dpi. For example, in the page Singa-
porean cuisine the occurrence of coffee in the
sentence ?[. . . ] combined with a cup of cof-
fee and a half-boiled egg? is not linked, but
we have collected many other occurrences,
all linked to the Coffee page, so this link
gets propagated to our ambiguous item as
well. This heuristic mimes the one-sense-per-
collocation heuristic presented in (Yarowsky,
1995).
? Trust the inventory: if Wikipedia provides
only one sense for a, i.e., only one page title
whose lemma is a, link a to that page. Con-
sider the instance ?At that point, Smith threw
down a cup of Gatorade? in page Jimmy
Clausen; there is only one sense for Gatorade
in Wikipedia, so we link the unannotated oc-
currence to it.
As a result, the initial set of disambiguated ar-
guments in Dpi is augmented with all those triples
for which any of the above three heuristics apply.
Note that Dpi might contain the same argument
several times, occurring in different sentences and
linked many times to the same page or to differ-
ent pages. Notably, the discovery of new links is
made through one scan of Wikipedia per heuristic.
The three disambiguation strategies, applied in the
same order as presented above, contribute to pro-
moting the most relevant sense for a given word.
Finally, let A be the set of arguments in Dpi,
i.e., A := {a : ?(a, s, l) ? Dpi}. For each argu-
ment a ? Awe select the majority sense sense(a)
of a and collect the corresponding set of sen-
tences sent(a) marked with that sense. Formally,
sense(a) := argmaxl |{(x, y, z) ? Dpi : x =
a?z = l}| and sent(a) := {s : (a, s, sense(a)) ?
Dpi}.
3.3 Generalization to Semantic Classes
Our final objective is to generalize the annotated
arguments to semantic classes picked out from a
fixed set C of classes. As explained below, we as-
sume the set C to be made up of representative
synsets from WordNet. We perform this in two
substeps: we first link all our disambiguated argu-
ments to WordNet (Section 3.3.1) and then lever-
age the WordNet taxonomy to populate the seman-
tic classes in C (Section 3.3.2).
3.3.1 Linking to WordNet
So far the arguments in Dpi have been semanti-
cally annotated with the Wikipedia pages they re-
fer to. However, using Wikipedia as our sense in-
ventory is not desirable; in fact, contrarily to other
commonly used lexical-semantic networks such
as WordNet, Wikipedia is not formally organized
in a structured, taxonomic hierarchy. While it is
true that attached to each Wikipedia page there are
one or more categories, these categories just pro-
vide shallow information about the class the page
1224
belongs to. Indeed, categories are not ideal for
representing the semantic classes of a Wikipedia
page for at least three reasons: i) many cate-
gories do not express taxonomic information (e.g.,
the English page Albert Einstein provides cate-
gories such as DEATHS FROM ABDOMINAL AOR-
TIC ANEURYSM and INSTITUTE FOR ADVANCED
STUDY FACULTY); ii) categories are mostly struc-
tured in a directed acyclic graph with multiple
parents per category (even worse, cycles are pos-
sible in principle); iii) there is no clear way of
identifying core semantic classes from the large
set of available categories. Although efforts to-
wards the automatic taxonomization of Wikipedia
categories do exist in the literature (Ponzetto and
Strube, 2011; Nastase and Strube, 2013), the re-
sults are of a lower quality than a hand-built lexical
resource. Therefore, as was done in previous work
(Mihalcea and Moldovan, ; Ciaramita and Altun,
2006; Izquierdo et al, 2009; Erk and McCarthy,
2009; Huang and Riloff, 2010), we pick out our
semantic classes C from WordNet and leverage its
manually-curated taxonomy to associate our argu-
ments with the most suitable class. This way we
avoid building a new taxonomy and shift the prob-
lem to that of projecting the Wikipedia pages ?
associated with annotated filling arguments ? to
synsets in WordNet. We address this problem in
two steps:
Wikipedia-WordNet mapping. We exploit an
existing mapping implemented in BabelNet (Nav-
igli and Ponzetto, 2012), a wide-coverage
multilingual semantic network that integrates
Wikipedia and WordNet.3 Based on a disam-
biguation algorithm, BabelNet establishes a map-
ping ? : Wikipages ? Synsets which links
about 50,000 pages to their most suitable Word-
Net senses.4
Mapping extension. Nevertheless, BabelNet is
able to solve the problem only partially, because it
still leaves the vast majority of the 4 million En-
glish Wikipedia pages unmapped. This is mainly
due to the encyclopedic nature of most pages,
which do not have a counterpart in the WordNet
dictionary. To address this issue, for each un-
mapped Wikipedia page p we obtain its textual
definition as the first sentence of the page.5 Next,
3http://babelnet.org
4We follow (Navigli, 2009) and denote with wip the i-th
sense of w in WordNet with part of speech p.
5According to the Wikipedia guidelines, ?The article
should begin with a short declarative sentence, answer-
ing two questions for the nonspecialist reader: What (or
who) is the subject? and Why is this subject notable??,
extracted from http://en.wikipedia.org/wiki/
we extract the hypernym from the textual defini-
tion of p by applying Word-Class Lattices (Navigli
and Velardi, 2010, WCL6), a domain-independent
hypernym extraction system successfully applied
to taxonomy learning from scratch (Velardi et al,
2013) and freely available online (Faralli and Nav-
igli, 2013). If a hypernym h is successfully ex-
tracted and h is linked to a Wikipedia page p?
for which ?(p?) is defined, then we extend the
mapping by setting ?(p) := ?(p?). For instance,
the mapping provided by BabelNet does not pro-
vide any link for the page Peter Spence; thanks to
WCL, though, we are able to set the page Jour-
nalist as its hypernym, and link it to the WordNet
synset journalist1n.This way our mapping extension now covers
539,954 pages, i.e., more than an order of mag-
nitude greater than the number of pages originally
covered by the BabelNet mapping.
3.3.2 Populating the Semantic Classes
We now proceed to populating the semantic
classes in C with the annotated arguments ob-
tained for the lexical predicate pi.
Definition 3 (semantic class of a synset). The
semantic class for a WordNet synset S is the class
c among those in C which is the most specific hy-
pernym of S according to the WordNet taxonomy.
For instance, given the synset tap water1n, its se-mantic class is water1n (while the other more gen-eral subsumers in C are not considered, e.g., com-
pound2n, chemical1n, liquid3n, etc).For each argument a ? A for which a
Wikipedia-to-WordNet mapping ?(sense(a))
could be established as a result of the linking
procedure described above, we associate a with
the semantic class of ?(sense(a)). For example,
consider the case in which a is equal to tap water
and sense(a) is equal to the Wikipedia page Tap
water, in turn mapped to tap water1n via ?; wethus associate tap water with its semantic class
water1n. If more than one class can be found weadd a to each of them.7
Ultimately, for each class c ? C, we obtain
a set support(c) made up of all the arguments
a ? A associated with c. For instance, sup-
port(beverage1n) = { chinese tea, 3.2% beer, hotcocoa, cider, . . . , orange juice }. Note that, thanks
to our extended mapping (cf. Section 3.3.1), the
support of a class can also contain arguments not
covered in WordNet (e.g., hot cocoa and tejuino).
Wikipedia:Writing_better_articles.
6http://lcl.uniroma1.it/wcl
7This can rarely happen due to multiple hypernyms avail-
able in WordNet for the same synset.
1225
Pclass(c|pi) c support(c)
0.1896 wine1n wine, sack, white wine, red wine, wine in china, madeira wine, claret, kosher wine
0.1805 coffee1n turkish coffee, drip coffee, espresso, coffee, cappucino, caffe` latte, decaffeinated coffee, latte
0.1143 herb2n green tea, indian tea, black tea, orange pekoe tea, tea
0.1104 water1n water, seawater
0.0532 beverage1n chinese tea, 3.2% beer, orange soda, boiled water, hot chocolate, hot cocoa, tejuino, cider,
beverage, cocoa, coffee milk, lemonade, orange juice
0.0403 milk1n skim milk, milk, cultured buttermilk, whole milk
0.0351 beer1n 3.2% beer, beer
0.0273 alcohol1n mead, umeshu, kava, rice wine, ja?germeister, kvass, sake, gin, rum
0.0182 poison1n poison
Table 2: Highest-probability semantic classes for the lexical predicate pi = cup of *, according to our set
C of semantic classes.
Since not all classes are equally relevant to the
lexical predicate pi, we estimate the conditional
probability of each class c ? C given pi on the
basis of the number of sentences which contain an
argument in that class. Formally:
Pclass(c|pi) =
?
a?support(c) |sent(a)|
Z , (1)
where Z is a normalization factor calculated as
Z =
?
c??C
?
a?support(c?) |sent(a)|. As an ex-ample, in Table 2 we show the highest-probability
classes for the lexical predicate cup of ?.
As a result of the probabilistic association of
each semantic class c with a target lexical predi-
cate w1 w2 . . . wi ? wi+1 . . . wn, we obtain a
semantic predicate w1 w2 . . . wi c wi+1 . . . wn.
3.4 Classification of new arguments
Once the semantic predicates for the input lexical
predicate pi have been learned, we can classify a
new filling argument a of pi. However, the class
probabilities calculated with Formula 1 might not
provide reliable scores for several classes, includ-
ing unseen ones whose probability would be 0.
To enable wide coverage we estimate a second
conditional probability based on the distributional
semantic profile of each class. To do this, we per-
form three steps:
1. For each WordNet synset S we create a dis-
tributional vector ~S summing the noun occur-
rences within all the Wikipedia pages p such
that ?(p) = S. Next, we create a distribu-
tional vector for each class c ? C as follows:
~c =
?
S?desc(c) ~S,
where desc(c) is the set of all synsets
which are descendants of the semantic class
c in WordNet. As a result we obtain a
predicate-independent distributional descrip-
tion for each semantic class in C.
2. Now, given an argument a of a lexical predi-
cate pi, we create a distributional vector ~a by
summing the noun occurrences of all the sen-
tences s such that (a, s, l) ? Lpi (cf. Section
3.1).
3. Let Ca be the set of candidate semantic
classes for argument a, i.e., Ca contains the
semantic classes for the WordNet synsets of
a as well as the semantic classes associated
with ?(p) for all Wikipedia pages p whose
lemma is a. For each candidate class c ? Ca,
we determine the cosine similarity between
the distributional vectors ~c and ~a as follows:
sim(~c,~a) = ~c ? ~a||~c|| ||~a|| .
Then, we determine the most suitable seman-
tic class c ? Ca of argument a as the class
with the highest distributional probability, es-
timated as:
Pdistr(c|pi, a) =
sim(~c,~a)?
c??Ca sim(~c ?,~a)
. (2)
We can now choose the most suitable class c ?
Ca for argument a which maximizes the proba-
bility mixture of the distributional probability in
Formula 2 and the class probability in Formula 1:
P (c|pi, a) = ?Pdistr(c|pi, a)+(1??)Pclass(c|pi),
(3)
where ? ? [0, 1] is an interpolation factor.
We now illustrate the entire process of our al-
gorithm on a real example. Given a textual ex-
pression such as virus replicate, we: (i) extract
all the filling arguments of the lexical predicate
* replicate; (ii) link and disambiguate the ex-
tracted filling arguments; (iii) query our system for
the available virus semantic classes (i.e., {virus1n,virus3n}); (iv) build the distributional vectors for
1226
the candidate semantic classes and the given in-
put argument; (v) calculate the probability mix-
ture. As a result we obtain the following rank-
ing, virus1n:0.250, virus3n:0.000894, so that the firstsense of virus in WordNet 3.0 is preferred, being
an ?ultramicroscopic infectious agent that repli-
cates itself only within cells of living hosts?.
4 Experiment 1: Oxford Lexical
Predicates
We evaluate on the two forms of output produced
by SPred: (i) the top-ranking semantic classes of a
lexical predicate, as obtained with Formula 1, and
(ii) the classification of a lexical predicate?s argu-
ment with the most suitable semantic class, as pro-
duced using Formula 3. For both evaluations, we
use a lexical predicate dataset built from the Ox-
ford Advanced Learner?s Dictionary (Crowther,
1998).
4.1 Set of Semantic Classes
The selection of which semantic classes to include
in the set C is of great importance. In fact, hav-
ing too many classes will end up in an overly fine-
grained inventory of meanings, whereas an exces-
sively small number of classes will provide lit-
tle discriminatory power. As our set C of seman-
tic classes we selected the standard set of 3,299
core nominal synsets available in WordNet.8 How-
ever, our approach is flexible and can be used with
classes of an arbitrary level of granularity.
4.2 Datasets
The Oxford Advanced Learner?s Dictionary pro-
vides usage notes that contain typical predicates in
various semantic domains in English, e.g., Travel-
ing.9 Each predicate is made up of a fixed part
(e.g., a verb) and a generalizable part which con-
tains one or more nouns.
Examples include fix an election/the vote, bac-
teria/microbes/viruses spread, spend money/sav-
ings/a fortune. In the case that more than one
noun was provided, we split the textual expres-
sion into as many items as the number of nouns.
For instance, from spend money/savings/a fortune
we created three items in our dataset, i.e., spend
money, spend savings, spend a fortune. The split-
ting procedure generated 6,220 instantiated lexical
predicate items overall.
8http://wordnetcode.princeton.edu/
standoff-files/core-wordnet.txt
9http://oald8.oxfordlearnersdictionaries.
com/usage_notes/unbox_colloc/
k Prec@k Correct Total
1 0.94 46 49
2 0.87 85 98
3 0.86 124 145
4 0.83 160 192
5 0.82 194 237
6 0.81 228 282
7 0.80 261 326
8 0.78 288 370
9 0.77 318 414
10 0.76 349 458
11 0.75 379 502
12 0.75 411 546
13 0.75 445 590
14 0.76 479 634
15 0.75 510 678
16 0.75 544 721
17 0.76 577 763
18 0.76 612 806
19 0.76 643 849
20 0.75 671 892
Table 3: Precision@k for ranking the semantic
classes of lexical predicates.
4.3 Evaluating the Semantic Class Ranking
Dataset. Given the above dataset, we general-
ized each item by pairing its fixed verb part with *
(i.e., we keep ?verb predicates? only, since they
are more informative). For instance, the three
items bacteria/microbes/viruses spread were gen-
eralized into the lexical predicate * spread. The to-
tal number of different lexical predicates obtained
was 1,446, totaling 1,429 distinct verbs (note that
the dataset might contain the lexical predicate *
spread as well as spread *).10
Methodology. For each lexical predicate we cal-
culated the conditional probability of each seman-
tic class using Formula 1, resulting in a ranking
of semantic classes. To evaluate the top ranking
classes, we calculated precision@k, with k rang-
ing from 1 to 20, by counting all applicable classes
as correct, e.g., location1n is a valid semantic classfor travel to * while emotion1n is not.
Results. We show in Table 3 the precision@k
calculated over a random sample of 50 lexical
predicates.11 As can be seen, while the classes
quality is pretty high with low values of k, per-
formance gradually degrades as we let k increase.
This is mostly due to the highly polysemous nature
of the predicates selected (e.g., occupy *, leave *,
help *, attain *, live *, etc.). We note that high per-
formance, attaining above 80%, can be achieved
10The low number of items per predicate is due to the orig-
inal Oxford resource.
11One lexical predicate did not have any semantic class
ranking.
1227
by focusing up to the first 7 classes output by our
system, with a 94% precision@1.
4.4 Evaluating Classification Performance
Dataset. Starting from the lexical predicate
items obtained as described in Section 4.2, we se-
lected those items belonging to a random sample
of 20 usage notes among those provided by the
Oxford dictionary, totaling 3,245 items. We then
manually tagged each item?s argument (e.g., virus
in viruses spread) with the most suitable seman-
tic class (e.g., virus1n), obtaining a gold standarddataset for the evaluation of our argument classifi-
cation algorithm (cf. Section 3.4).
Methodology. In this second evaluation we
measure the accuracy of our method at assigning
the most suitable semantic class to the argument
of a lexical predicate item in our gold standard.
We use three customary measures to determine the
quality of the acquired semantic classes, i.e., pre-
cision, recall and F1. Precision is the number of
items which are assigned the correct class (as eval-
uated by a human) over the number of items which
are assigned a class by the system. Recall is the
number of items which are assigned the correct
class over the number of items to be classified. F1
is the harmonic mean of precision and recall.
Tuning. The only parameter to be tuned is the
factor ? that we use to mix the two probabilities
in Formula 3 (cf. Section 3.4). For tuning ? we
used a held-out set of 8 verbs, randomly sampled
from the lexical predicates not used in the dataset.
We created a tuning set using the annotated argu-
ments in Wikipedia for these verbs: we trained the
model on 80% of the annotated lexical predicate
arguments (i.e., the class probability estimates in
Formula 1) and then applied the probability mix-
ture (i.e., Formula 3) for classifying the remain-
ing 20% of arguments. Finally, we calculated the
performance in terms of precision, recall and F1
with 11 different values of ? ? {0, 0.1, . . . , 1.0},
achieving optimal performance with ? = 0.2.
Results. Table 4 shows the results on the seman-
tic class assignments. Our system shows very high
precision, above 85%, while at the same time at-
taining an adequate 68% recall. We also compared
against a random baseline that randomly selects
one out of all the candidate semantic classes for
each item, achieving only moderate results. A sub-
sequent error analysis revealed the common types
of error produced by our system: terms for which
we could not provide (1) any WordNet concept
Method Precision Recall F1
SPred 85.61 68.01 75.80
Random 40.96 40.96 40.96
Table 4: Performance on semantic class assign-
ment.
(e.g., political corruption) or (2) any candidate se-
mantic class (e.g., immune system).
4.5 Disambiguation heuristics impact
As a follow-up analysis, for each dataset we con-
sidered the impact of each disambiguation heuris-
tic described in Section 3.2 according to how many
times it was triggered. Starting from the entire set
of 1,446 lexical predicates from the Oxford dictio-
nary (see Section 4.3), we counted the number of
argument triples (a, s, l) already disambiguated in
Wikipedia (i.e., l 6= ) and those disambiguated
thanks to our disambiguation strategies. Table
5 shows the statistics. We note that, while the
amount of originally linked arguments is very low
(about 2.5% of total), our strategies are able to
considerably increase the size of the initial set of
linked instances. The most effective strategies ap-
pear to be the One sense per page and the Trust the
inventory, which contribute 26.16% and 31.33%
of the total links, respectively.
Even though most of the triples (i.e., 68 out of
almost 74 million) remain unlinked, the ratio of
distinct arguments which we linked to WordNet
is considerably higher, calculated as 3,723,979
linked arguments over 12,431,564 distinct argu-
ments, i.e., about 30%.
5 Experiment 2: Comparison with
Kozareva & Hovy (2010)
Due to the novelty of the task carried out by SPred,
the resulting output may be compared with only a
limited number of existing approaches. The most
similar approach is that of Kozareva and Hovy
(2010, K&H) who assign supertypes to the argu-
ments of arbitrary relations, a task which resem-
bles our semantic predicate ranking. We therefore
performed a comparison on the quality of the most
highly-ranked supertypes (i.e., semantic classes)
using their dataset of 24 relation patterns (i.e., lex-
ical predicates).
Dataset. The dataset contained 14 lexical pred-
icates (e.g., work for * or * fly to), 10 of which
were expanded in order to semantify their left- and
right-side arguments (e.g., * work for and work
for *); for the remaining 4 predicates just a single
1228
Total Linked in One sense One sense per Trust the Not
triples Wikipedia per page lexical predicate inventory linked
73,843,415 1,795,608 1,433,634 533,946 1,716,813 68,363,414
Table 5: Statistics on argument triple linking for all the lexical predicates in the Oxford dataset.
k Prec@k Correct Total
1 0.88 21 24
2 0.90 43 48
3 0.88 63 72
4 0.89 85 96
5 0.91 109 120
6 0.91 131 144
7 0.92 154 168
8 0.91 175 192
9 0.92 198 216
10 0.92 221 240
11 0.92 242 264
12 0.92 264 288
13 0.91 284 312
14 0.90 304 336
15 0.91 327 360
16 0.91 348 384
17 0.90 367 408
18 0.89 386 432
19 0.89 407 456
20 0.89 429 480
Table 6: Precision@k for the semantic classes of
the relations of Kozareva and Hovy (2010).
side was generalized (e.g., * dress). While most of
the relations apply to persons as a supertype, our
method could find arguments for each of them.
Methodology. We carried out the same evalua-
tion as in Section 4.3. We calculated precision@k
of the semantic classes obtained for each relation
in the dataset of K&H. Because the set of appli-
cable classes was potentially unbounded, we were
not able to report recall directly.
Results. K&H reported an overall accuracy of
the top-20 supertypes of 92%. As can be seen in
Table 6 we exhibit very good performance with in-
creasing values of k. A comparison of Table 3 with
Table 6 shows considerable differences in perfor-
mance between the two datasets. We attribute this
difference to the higher average WordNet poly-
semy of the verbal component of the Oxford pred-
icates (on average 2.64 senses for K&H against
6.52 for the Oxford dataset).
Although we cannot report recall, we list the
number of Wikipedia arguments and associated
classes in Table 7, which provides an estimate of
the extraction capability of SPred. The large num-
ber of classes found for the arguments demon-
strates the ability of our method to generalize to
a variety of semantic classes.
Predicate Number of args Number of classes
cause * 181,401 1,339
live in * 143,628 600
go to * 134,712 867
* cause 92,160 1,244
work in * 79,444 770
* go to 71,794 746
* live in 61,074 541
work on * 58,760 840
work for * 58,332 681
work at * 31,904 511
* work in 24,933 528
* celebrate 23,333 408
Table 7: Number of arguments and associated
classes for the 12 most frequent lexical predicates
of Kozareva and Hovy (2010) extracted by SPred
from Wikipedia.
6 Related work
The availability of Web-scale corpora has led to
the production of large resources of relations (Et-
zioni et al, 2005; Yates et al, 2007; Wu and Weld,
2010; Carlson et al, 2010; Fader et al, 2011).
However, these resources often operate purely at
the lexical level, providing no information on the
semantics of their arguments or relations. Several
studies have examined adding semantics through
grouping relations into sets (Yates and Etzioni,
2009), ontologizing the arguments (Chklovski and
Pantel, 2004), or ontologizing the relations them-
selves (Moro and Navigli, 2013). However, analy-
sis has largely been either limited to ontologizing
a small number of relation types with a fixed in-
ventory, which potentially limits coverage, or has
used implicit definitions of semantic categories
(e.g., clusters of arguments), which limits inter-
pretability. For example, Mohamed et al (2011)
use the semantic categories of the NELL system
(Carlson et al, 2010) to learn roughly 400 valid
ontologized relations from over 200M web pages,
whereas WiSeNet (Moro and Navigli, 2012) lever-
ages Wikipedia to acquire relation synsets for an
open set of relations. Despite these efforts, no
large-scale resource has existed to date that con-
tains ontologized lexical predicates. In contrast,
the present work provides a high-coverage method
for learning argument supertypes from a broad-
coverage ontology (WordNet), which can poten-
tially be leveraged in relation extraction to ontolo-
1229
gize relation arguments.
Our method for identifying the different seman-
tic classes of predicate arguments is closely related
to the task of identifying selectional preferences.
The most similar approaches to it are taxonomy-
based ones, which leverage the semantic types
of the relations arguments (Resnik, 1996; Li and
Abe, 1998; Clark and Weir, 2002; Pennacchiotti
and Pantel, 2006). Nevertheless, despite their high
quality sense-tagged data, these methods have of-
ten suffered from lack of coverage. As a result,
alternative approaches have been proposed that es-
chew taxonomies in favor of rating the quality of
potential relation arguments (Erk, 2007; Cham-
bers and Jurafsky, 2010) or generating probabil-
ity distributions over the arguments (Rooth et al,
1999; Pantel et al, 2007; Bergsma et al, 2008;
Ritter et al, 2010; Se?aghdha, 2010; Bouma, 2010;
Jang and Mostow, 2012) in order to obtain higher
coverage of preferences.
In contrast, we overcome the data sparsity of
class-based models by leveraging the large quan-
tity of collaboratively-annotated Wikipedia text in
order to connect predicate arguments with their
semantic class in WordNet using BabelNet (Nav-
igli and Ponzetto, 2012); because we map directly
to WordNet synsets, we provide a more readily-
interpretable collocation preference model than
most similarity-based or probabilistic models.
Verb frame extraction (Green et al, 2004) and
predicate-argument structure analysis (Surdeanu
et al, 2003; Yakushiji et al, 2006) are two areas
that are also related to our work. But their gener-
ality goes beyond our intentions, as we focus on
semantic predicates, which is much simpler and
free from syntactic parsing.
Another closely related work is that of Hanks
(2013) concerning the Theory of Norms and Ex-
ploitations, where norms (exploitations) represent
expected (unexpected) classes for a given lexical
predicate. Although our semantified predicates do,
indeed, provide explicit evidence of norms ob-
tained from collective intelligence and would pro-
vide support for this theory, exploitations present
a more difficult task, different from the one ad-
dressed here, due to its focus on identifying prop-
erty transfer between the semantic class and the
exploited instance.
The closest technical approach to ours is that
of Kozareva and Hovy (2010), who use recursive
patterns to induce semantic classes for the argu-
ments of relational patterns. Whereas their ap-
proach requires both a relation pattern and one
or more seeds, which bias the types of semantic
classes that are learned, our proposed method re-
quires only the pattern itself, and as a result is ca-
pable of learning an unbounded number of differ-
ent semantic classes.
7 Conclusions
In this paper we present SPred, a novel approach
to large-scale harvesting of semantic predicates.
In order to semantify lexical predicates we ex-
ploit the wide coverage of Wikipedia to extract
and disambiguate lexical predicate occurrences,
and leverage WordNet to populate the semantic
classes with suitable predicate arguments. As a re-
sult, we are able to ontologize lexical predicate in-
stances like those available in existing dictionaries
(e.g., break a toe) into semantic predicates (such
as break a BODY PART).
For each lexical predicate (such as break ?),
our method produces a probability distribution
over the set of semantic classes (thus covering the
different expected meanings for the filling argu-
ments) and is able to classify new instances with
the most suitable class. Our experiments show
generally high performance, also in comparison
with previous work on argument supertyping.
We hope that our semantic predicates will en-
able progress in different Natural Language Pro-
cessing tasks such as Word Sense Disambigua-
tion (Navigli, 2009), Semantic Role Labeling
(Fu?rstenau and Lapata, 2012) or even Textual En-
tailment (Stern and Dagan, 2012) ? each of which
is in urgent need of reliable semantics. While we
focused on semantifying lexical predicates, as fu-
ture work we will apply our method to the ontol-
ogization of large amounts of sequences of words,
such as phrases or textual relations (e.g., consid-
ering Google n-grams appearing in Wikipedia).
Notably, our method should, in principle, gener-
alize to any semantically-annotated corpus (e.g.,
Wikipedias in other languages), provided lexical
predicates can be extracted with associated seman-
tic classes.
In order to support future efforts we are releas-
ing our semantic predicates as a freely available
resource.12
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
Thanks go to David A. Jurgens, Silvia Necs?ulescu,
Stefano Faralli and Moreno De Vincenzi for their
help.
12http://lcl.uniroma1.it/spred
1230
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional prefer-
ence from unlabeled text. In Proc. of EMNLP, pages
59?68, Stroudsburg, PA, USA.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154?165.
Gerlof Bouma. 2010. Collocation Extraction beyond
the Independence Assumption. In Proc. of ACL,
Short Papers, pages 109?114, Uppsala, Sweden.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proc. of AAAI, pages 1306?1313,
Atlanta, Georgia.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selec-
tional preferences. In Proc. of ACL, pages 445?453,
Stroudsburg, PA, USA.
Tim Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb rela-
tions. In Proc. of EMNLP, pages 33?40, Barcelona,
Spain.
Jennifer Chu-Carroll and John Prager. 2007. An exper-
imental study of the impact of information extraction
accuracy on semantic search performance. In Proc.
of CIKM, pages 505?514, Lisbon, Portugal.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-Coverage Sense Disambiguation and Infor-
mation Extraction with a Supersense Sequence Tag-
ger. In Proc. of EMNLP, pages 594?602, Sydney,
Australia.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Jonathan Crowther, editor. 1998. Oxford Advanced
Learner?s Dictionary. Cornelsen & Oxford, 5th edi-
tion.
Flavio De Benedictis, Stefano Faralli, and Roberto
Navigli. 2013. GlossBoot: Bootstrapping multilin-
gual domain glossaries from the Web. In Proc. of
ACL, Sofia, Bulgaria.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proc. of CIKM, pages 1099?1108, New York, NY,
USA.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proc. of EMNLP, pages 440?
449, Stroudsburg, PA, USA.
Katrin Erk. 2007. A Simple, Similarity-based Model
for Selectional Preferences. In Proc. of ACL, pages
216?223, Prague, Czech Republic.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proc. of EMNLP, pages 1535?1545,
Edinburgh, UK.
Stefano Faralli and Roberto Navigli. 2013. A Java
framework for multilingual definition and hypernym
extraction. In Proc. of ACL, Comp. Volume, Sofia,
Bulgaria.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: an overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135?
171.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proc. of
HLT-NAACL, pages 1?8, Edmonton, Canada.
Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing Frame Semantic Verb Classes from
WordNet and LDOCE. In Proc. of ACL, pages 375?
382, Barcelona, Spain.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. University Press Group Limited.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of COL-
ING, pages 539?545, Nantes, France.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and artificial intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-Specific Semantic Class Taggers from (Al-
most) Nothing. In Proc. of ACL, pages 275?285,
Uppsala, Sweden.
Sean P. Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with Web-based corrobo-
ration. In Proc. of UMSLLS, pages 18?26, Strouds-
burg, PA, USA.
Rube?n Izquierdo, Armando Sua?rez, and German Rigau.
2009. An Empirical Study on Class-Based Word
Sense Disambiguation. In Proc. of EACL, pages
389?397, Athens, Greece.
Hyeju Jang and Jack Mostow. 2012. Inferring se-
lectional preferences from part-of-speech n-grams.
In Proc. of EACL, pages 377?386, Stroudsburg, PA,
USA.
1231
Boris Katz, Jimmy J. Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew W. Bilotti, Sue Felshin, Aaron
Fernandes, Gregory Marton, and Federico Mora.
2003. Integrating Web-based and Corpus-based
Techniques for Question Answering. In Proc. of
TREC, pages 426?435, Gaithersburg, Maryland.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
Arguments and Supertypes of Semantic Relations
Using Recursive Patterns. In Proc. of ACL, pages
1482?1491, Uppsala, Sweden.
Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Proc.
ACL/HLT, pages 1048?1056, Columbus, Ohio.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proc. of ISWC 2012, Part I, pages 263?278,
Boston, MA.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217?244.
Rada Mihalcea and Dan Moldovan. eXtended Word-
Net: Progress report. In Proceedings of the NAACL-
01 Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, Penn.
Thahir Mohamed, Estevam Hruschka, and Tom
Mitchell. 2011. Discovering Relations between
Noun Categories. In Proc. of EMNLP, pages 1447?
1455, Edinburgh, Scotland, UK.
Andrea Moro and Roberto Navigli. 2012. WiSeNet:
Building a Wikipedia-based semantic network with
ontologized relations. In Proc. of CIKM, pages
1672?1676, Maui, HI, USA.
Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open In-
formation Extraction Paradigm. In Proc. of IJCAI,
Beijing, China.
Vivi Nastase and Michael Strube. 2013. Transform-
ing wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62?85.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proc. of ACL, pages 1318?1327, Up-
psala, Sweden.
Roberto Navigli. 2009. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1?69.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: learning inferential selec-
tional preferences. In Proc. of NAACL, pages 564?
571, Rochester, NY.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of CIKM, pages
137?145, New York, NY, USA.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proc. of COLING,
pages 793?800, Sydney, Australia.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737?1756.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127?159.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In Proc. of ACL, pages 424?434, Uppsala,
Sweden. ACL.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a seman-
tically annotated lexicon via EM-based clustering.
In Proc. of ACL, pages 104?111, Stroudsburg, PA,
USA.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proc. of ACL, pages
435?444, Uppsala, Sweden.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In NIPS, pages 1297?1304, Cam-
bridge, Mass.
Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proc. of ACL 2012, System Demonstra-
tions, pages 73?78, Jeju Island, Korea.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proc. ACL,
pages 8?15, Stroudsburg, PA, USA.
M. Thelen and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. In Proc. of EMNLP, pages
214?221, Salt Lake City, UT, USA.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3).
Yorick Wilks. 1975. A preferential, pattern-seeking,
semantics for natural language inference. Artificial
Intelligence, 6(1):53?74.
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. In Proc. of ACL, pages
118?127, Uppsala, Sweden.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun?ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proc. of
EMNLP, pages 284?292, Stroudsburg, PA, USA.
David Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. of ACL, pages 189?196, Cambridge, MA,
USA.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: open informa-
tion extraction on the web. In Proc. of NAACL-
Demonstrations, pages 25?26, Stroudsburg, PA,
USA.
1232
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 945?955,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project
Tiziano Flati, Daniele Vannella, Tommaso Pasini and Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
{flati,vannella,navigli}@di.uniroma1.it
p.tommaso@gmail.com
Abstract
We present WiBi, an approach to the
automatic creation of a bitaxonomy for
Wikipedia, that is, an integrated taxon-
omy of Wikipage pages and categories.
We leverage the information available in
either one of the taxonomies to reinforce
the creation of the other taxonomy. Our
experiments show higher quality and cov-
erage than state-of-the-art resources like
DBpedia, YAGO, MENTA, WikiNet and
WikiTaxonomy. WiBi is available at
http://wibitaxonomy.org.
1 Introduction
Knowledge has unquestionably become a key
component of current intelligent systems in many
fields of Artificial Intelligence. The creation and
use of machine-readable knowledge has not only
entailed researchers (Mitchell, 2005; Mirkin et al,
2009; Poon et al, 2010) developing huge, broad-
coverage knowledge bases (Hovy et al, 2013;
Suchanek and Weikum, 2013), but it has also
hit big industry players such as Google (Singhal,
2012) and IBM (Ferrucci, 2012), which are mov-
ing fast towards large-scale knowledge-oriented
systems.
The creation of very large knowledge bases
has been made possible by the availability of
collaboratively-curated online resources such as
Wikipedia and Wiktionary. These resources are
increasingly becoming enriched with new con-
tent in many languages and, although they are
only partially structured, they provide a great deal
of valuable knowledge which can be harvested
and transformed into structured form (Medelyan
et al, 2009; Hovy et al, 2013). Prominent
examples include DBpedia (Bizer et al, 2009),
BabelNet (Navigli and Ponzetto, 2012), YAGO
(Hoffart et al, 2013) and WikiNet (Nastase and
Strube, 2013). The types of semantic relation
in these resources range from domain-specific, as
in Freebase (Bollacker et al, 2008), to unspec-
ified relations, as in BabelNet. However, un-
like the case with smaller manually-curated re-
sources such as WordNet (Fellbaum, 1998), in
many large automatically-created resources the
taxonomical information is either missing, mixed
across resources, e.g., linking Wikipedia cate-
gories to WordNet synsets as in YAGO, or coarse-
grained, as in DBpedia whose hypernyms link to a
small upper taxonomy.
Current approaches in the literature have mostly
focused on the extraction of taxonomies from the
network of Wikipedia categories. WikiTaxonomy
(Ponzetto and Strube, 2007), the first approach
of this kind, is based on the use of heuristics
to determine whether is-a relations hold between
a category and its subcategories. Subsequent ap-
proaches have also exploited heuristics, but have
extended them to any kind of semantic relation
expressed in the category names (Nastase and
Strube, 2013). But while the aforementioned at-
tempts provide structure for categories that sup-
ply meta-information for Wikipedia pages, sur-
prisingly little attention has been paid to the ac-
quisition of a full-fledged taxonomy for Wikipedia
pages themselves. For instance, Ruiz-Casado et
al. (2005) provide a general vector-based method
which, however, is incapable of linking pages
which do not have a WordNet counterpart. Higher
coverage is provided by de Melo and Weikum
(2010) thanks to the use of a set of effective heuris-
tics, however, the approach also draws on Word-
Net and sense frequency information.
In this paper we address the task of taxono-
mizing Wikipedia in a way that is fully indepen-
dent of other existing resources such as WordNet.
We present WiBi, a novel approach to the cre-
ation of a Wikipedia bitaxonomy, that is, a tax-
onomy of Wikipedia pages aligned to a taxonomy
of categories. At the core of our approach lies the
idea that the information at the page and category
945
level are mutually beneficial for inducing a wide-
coverage and fine-grained integrated taxonomy.
2 WiBi: A Wikipedia Bitaxonomy
We induce a Wikipedia bitaxonomy, i.e., a taxon-
omy of pages and categories, in 3 phases:
1. Creation of the initial page taxonomy: we
first create a taxonomy for the Wikipedia
pages by parsing textual definitions, ex-
tracting the hypernym(s) and disambiguating
them according to the page inventory.
2. Creation of the bitaxonomy: we leverage
the hypernyms in the page taxonomy, to-
gether with their links to the corresponding
categories, so as to induce a taxonomy over
Wikipedia categories in an iterative way. At
each iteration, the links in the page taxonomy
are used to identify category hypernyms and,
conversely, the new category hypernyms are
used to identify more page hypernyms.
3. Refinement of the category taxonomy: fi-
nally we employ structural heuristics to over-
come inherent problems affecting categories.
The output of our three-phase approach is a bitax-
onomy of millions of pages and hundreds of thou-
sands of categories for the English Wikipedia.
3 Phase 1: Inducing the Page Taxonomy
The goal of the first phase is to induce a taxonomy
of Wikipedia pages. Let P be the set of all the
pages and let T
P
= (P,E) be the page taxonomy
directed graph whose nodes are pages and whose
edge set E is initially empty (E := ?). For each
p ? P our aim is to identify the most suitable gen-
eralization p
h
? P so that we can create the edge
(p, p
h
) and add it to E. For instance, given the
page APPLE, which represents the fruit meaning
of apple, we want to determine that its hypernym
is FRUIT and add the hypernym edge connecting
the two pages (i.e., E := E?{(APPLE, FRUIT)}).
To do this, we perform a syntactic step, in which
the hypernyms are extracted from the page?s tex-
tual definition, and a semantic step, in which the
extracted hypernyms are disambiguated according
to the Wikipedia inventory.
3.1 Syntactic step: hypernym extraction
In the syntactic step, for each page p ? P , we
extract zero, one or more hypernym lemmas, that
is, we output potentially ambiguous hypernyms
for the page. The first assumption, which follows
Julia Fiona Roberts is
an
American actress
NNP NNP NNP VBZ DT JJ
NN
nn
nn
nsubj
cop
det
amod
Figure 1: A dependency tree example with copula.
the Wikipedia guidelines and is validated in the
literature (Navigli and Velardi, 2010; Navigli and
Ponzetto, 2012), is that the first sentence of each
Wikipedia page p provides a textual definition for
the concept represented by p. The second assump-
tion we build upon is the idea that a lexical tax-
onomy can be obtained by extracting hypernyms
from textual definitions. This idea dates back to
the early 1970s (Calzolari et al, 1973), with later
developments in the 1980s (Amsler, 1981; Calzo-
lari, 1982) and the 1990s (Ide and V?eronis, 1993).
To extract hypernym lemmas, we draw on the
notion of copula, that is, the relation between the
complement of a copular verb and the copular verb
itself. Therefore, we apply the Stanford parser
(Klein and Manning, 2003) to the definition of a
page in order to extract all the dependency rela-
tions of the sentence. For example, given the def-
inition of the page JULIA ROBERTS, i.e., ?Julia
Fiona Roberts is an American actress.?, the Stan-
ford parser outputs the set of dependencies shown
in Figure 1. The noun involved in the copula re-
lation is actress and thus it is taken as the page?s
hypernym lemma. However, the extracted hyper-
nym is sometimes overgeneral (one, kind, type,
etc.). For instance, given the definition of the
page APOLLO, ?Apollo is one of the most impor-
tant and complex of the Olympian deities in an-
cient Greek and Roman religion [...].?, the only
copula relation extracted is between is and one.
To cope with this problem we use a list of stop-
words.
1
When such a term is extracted as hyper-
nym, we replace it with the rightmost noun of the
first following noun sequence (e.g., deity in the
above example). If the resulting lemma is again a
stopword we repeat the procedure, until a valid hy-
pernym or no appropriate hypernym can be found.
Finally, to capture multiple hypernyms, we iter-
atively follow the conj and and conj or relations
starting from the initially extracted hypernym. For
example, consider the definition of ARISTOTLE:
?Aristotle was a Greek philosopher and polymath,
a student of Plato and teacher of Alexander the
Great.? Initially, the philosopher hypernym is
selected thanks to the copula relation, then, fol-
1
E.g., species, genus, one, etc. Full list available online.
946
lowing the conjunction relations, also polymath,
student and teacher are extracted as hypernyms.
While more sophisticated approaches like Word-
Class Lattices could be applied (Navigli and Ve-
lardi, 2010), we found that, in practice, our hy-
pernym extraction approach provides higher cov-
erage, which is critical in our case.
3.2 Semantic step: hypernym disambiguation
Since our aim is to connect pairs of pages via
hypernym relations, our second step consists of
disambiguating the obtained hypernym lemmas of
page p by associating the most suitable page with
each hypernym. Following previous work (Ruiz-
Casado et al, 2005; Navigli and Ponzetto, 2012),
as the inventory for a given lemma we consider the
set of pages whose main title is the lemma itself,
except for the sense specification in parenthesis.
For instance, given fruit as the hypernym for AP-
PLE we would like to link APPLE to FRUIT as op-
posed to, e.g., FRUIT (BAND) or FRUIT (ALBUM).
3.2.1 Hypernym linkers
To disambiguate hypernym lemmas, we exploit
the structural features of Wikipedia through a
pipeline of hypernym linkers L = {L
i
}, applied
in cascade order (cf. Section 3.3.1). We start with
the set of page-hypernym pairs H = {(p, h)} as
obtained from the syntactic step. The successful
application of a linker to a pair (p, h) ? H yields
a page p
h
as the most suitable sense of h, result-
ing in setting isa(p, h) = p
h
. At step i, the i-
th linker L
i
? L is applied to H and all the hy-
pernyms which the linker could disambiguate are
removed from H . This prevents lower-precision
linkers from overriding decisions taken by more
accurate ones.
We now describe the hypernym linkers. In what
follows we denote with p
h
? p
h
the fact that the
definition of a Wikipedia page p contains an oc-
currence of h linked to page p
h
. Note that p
h
is
not necessarily a sense of h.
Crowdsourced linker If p
h
? p
h
, i.e., the hyper-
nym h is found to have been manually linked to p
h
in p by Wikipedians, we assign isa(p, h) = p
h
.
For example, because capital was linked in the
BRUSSELS page definition to CAPITAL CITY, we
set isa(BRUSSELS, capital) = CAPITAL CITY.
Category linker Given the set W ? P of
Wikipedia pages which have at least one category
in common with p, we select the majority sense
of h, if there is one, as hyperlinked across all the
definitions of pages in W :
isa(p, h) = argmax
p
h
?
p
?
?W
1(p
?
h
? p
h
)
where 1(p
?
h
? p
h
) is the characteristic function
which equals 1 if h is linked to p
h
in page
p
?
, 0 otherwise. For example, the linker sets
isa(EGGPLANT, plant) = PLANT because most of
the pages associated with TROPICAL FRUIT, a cat-
egory of EGGPLANT, contain in their definitions
the term plant linked to the PLANT page.
Multiword linker If p
m
? p
h
and m is a
multiword expression containing the lemma h
as one of its words, set isa(p, h) = p
h
. For
example, we set isa(PROTEIN, compound) =
CHEMICAL COMPOUND, as chemical compound
is linked to CHEMICAL COMPOUND in the defini-
tion of PROTEIN.
Monosemous linker If h is monosemous in
Wikipedia (i.e., there is only a single page p
h
for
that lemma), link it to its only sense by setting
isa(p, h) = p
h
. For example, we extract the
hypernym businessperson from the definition of
MERCHANT and, as it is unambiguous, we link
it to BUSINESSPERSON.
Distributional linker Finally, we provide a dis-
tributional approach to hypernym disambiguation.
We represent the textual definition of page p as a
distributional vector ~v
p
whose components are all
the English lemmas in Wikipedia. The value of
each component is the occurrence count of the cor-
responding content word in the definition of p.
The goal of this approach is to find the best
link for hypernym h of p among the pages h is
linked to, across the whole set of definitions in
Wikipedia. Formally, for each p
h
such that h
is linked to p
h
in some definition, we define the
set of pages P (p
h
) whose definitions contain a
link to p
h
, i.e., P (p
h
) = {p
?
? P |p
?
h
? p
h
}.
We then build a distributional vector ~v
p
?
for each
p
?
? P (p
h
) as explained above and create an ag-
gregate vector ~v
p
h
=
?
p
?
~v
p
?
. Finally, we de-
termine the similarity of p to each p
h
by calcu-
lating the dot product between the two vectors
sim(p, p
h
) = ~v
p
? ~v
p
h
. If sim(p, p
h
) > 0 for any
p
h
we perform the following association:
isa(p, h) = argmax
p
h
sim(p, p
h
)
For example, thanks to this linker we set
isa(VACUUM CLEANER, device) = MACHINE.
947
Figure 2: Distribution of linked hypernyms.
3.3 Page Taxonomy Evaluation
Statistics We applied the above linkers to the
October 2012 English Wikipedia dump. Out of
the 3,829,058 total pages, 4,270,232 hypernym
lemmas were extracted in the syntactic step for
3,697,113 pages (covering more than 96% of the
total). Due to illformed definitions, though, it
was not always possible to extract the hypernym
lemma: for example, 6 APRIL 2010 BAGHDAD
BOMBINGS is defined as ?A series of bomb ex-
plosions destroyed several buildings in Baghdad?,
which only implicitly provides the hypernym.
The semantic step disambiguated 3,718,612 hy-
pernyms for 3,294,562 Wikipedia pages, i.e., cov-
ering more than 86% of the English pages with at
least one disambiguated hypernym. Figure 2 plots
the number and distribution of hypernyms disam-
biguated by our hypernym linkers.
Taxonomy quality To evaluate the quality of
our page taxonomy we randomly sampled 1,000
Wikipedia pages. For each page we provided: i)
a list of suitable hypernym lemmas for the page,
mainly selected from its definition; ii) for each
lemma the correct hypernym page(s). We calcu-
lated precision as the average ratio of correct hy-
pernym lemmas (senses) to the total number of
lemmas (senses) returned for all the pages in the
dataset, recall as the number of correct lemmas
(senses) over the total number of lemmas (senses)
in the dataset, and coverage as the fraction of
pages for which at least one lemma (sense) was
returned, independently of its correctness. Results,
both at lemma- and sense-level, are reported in Ta-
ble 1. Not only does our taxonomy show high pre-
cision and recall in extracting ambiguous hyper-
nyms, it also disambiguates more than 3/4 of the
hypernyms with high precision.
3.3.1 Hypernym linker order
The optimal order of application of the above
linkers is the same as that presented in Section
3.2.1. It was established by selecting the combina-
tion, among all possible permutations, which max-
imized precision on a tuning set of 100 randomly
sampled pages, disjoint from our page dataset.
Prec. Rec. Cov.
Lemma 94.83 90.20 98.50
Sense 82.77 75.10 89.20
Table 1: Page taxonomy performance.
4 Phase 2: Inducing the Bitaxonomy
The page taxonomy built in Section 3 will serve
as a stable, pivotal input to the second phase, the
aim of which is to build our bitaxonomy, that is, a
taxonomy of pages and categories. Our key idea
is that the generalization-specialization informa-
tion available in each of the two taxonomies is
mutually beneficial. We implement this idea by
exploiting one taxonomy to update the other, and
vice versa, in an iterative way, until a fixed point
is reached. The final output of this phase is, on the
one hand, a page taxonomy augmented with addi-
tional hypernymy relations and, on the other hand,
a category taxonomy which is built from scratch.
4.1 Initialization
Our bitaxonomy B = {T
P
, T
C
} is a pair consist-
ing of the page taxonomy T
P
= (P,E), as ob-
tained in Section 3, and the category taxonomy
T
C
= (C, ?), which initially contains all the cate-
gories as nodes but does not include any hypernym
edge between category nodes. In the following
we describe the core algorithm of our approach,
which iteratively and mutually populates and re-
fines the edge sets E(T
P
) and E(T
C
).
4.2 The Bitaxonomy Algorithm
Preliminaries Before proceeding, we define
some basic concepts that will turn out to be use-
ful for presenting our algorithm. We denote by
super
T
(t) the set of all ancestors of a node t in the
taxonomy T (be it T
P
or T
C
). We further define a
verification function t;
T
t
?
which, in the case of
T
C
, returns true if there is a path in the Wikipedia
category network between t and t
?
, false other-
wise, and, in the case of T
P
, returns true if t
?
is
a sense, i.e., a page, of a hypernym h of t (that
is, (t, h) ? H , cf. Section 3.2.1). For instance,
SPORTSMEN ;
T
C
MEN BY OCCUPATION holds
for categories because the former is a sub-category
of the latter in Wikipedia, and RADIOHEAD ;
T
P
BAND (MUSIC) for pages, because band is a hy-
pernym extracted from the textual definition of
RADIOHEAD and BAND (MUSIC) is a sense of
band in Wikipedia. Note that, while the super
function returns information that we have already
learned, i.e., it is in T
P
and T
C
, the ; operator
948
holds just for candidate is-a relations, as it uses
knowledge from Wikipedia itself which is poten-
tially incorrect. For instance, SPORTSMEN ;
T
C
MEN?S SPORTS in the Wikipedia category net-
work, and RADIOHEAD ;
T
P
BAND (RADIO) be-
tween the two Wikipedia pages, both hold accord-
ing to our definition of ;, while connecting the
wrong hypernym candidates. At the core of our
algorithm, explained below, is the mutual lever-
aging of the super function from one of the two
taxonomies (pages or categories) to decide about
which candidates (for which a ; relation holds)
in the other taxonomy are real hypernyms.
Finally, we define the projection operator pi,
such that pi(c), c ? C, is the set of pages
categorized with c, and pi(p), p ? P , is the
set of categories associated with page p in
Wikipedia. For instance, the pages which belong
to the category OLYMPIC SPORTS are given by
pi(OLYMPIC SPORTS) = {BASEBALL, BOXING,
. . . , TRIATHLON}. Vice versa, pi(TRIATHLON) =
{MULTISPORTS, OLYMPIC SPORTS, . . . , OPEN
WATER SWIMMING}. The projection operator pi
enables us to jump from one taxonomy to the other
and expresses the mutual membership relation be-
tween pages and categories.
Algorithm We now describe in detail the bitax-
onomy algorithm, whose pseudocode is given in
Algorithm 1. The algorithm takes as input the two
taxonomies, initialized as described in Section 4.1.
Starting from the category taxonomy (line 1), the
algorithm updates the two taxonomies in turn, un-
til convergence is reached, i.e., no more edges can
be added to any side of the bitaxonomy. Let T be
the current taxonomy considered at a given mo-
ment and T
?
its dual taxonomy. The algorithm
proceeds by selecting a node t ? V (T ) for which
no hypernym edge (t, t
h
) could be found up until
that moment (line 3), and then tries to infer such
a relation by drawing on the dual taxonomy T
?
(lines 5-12). This is the core of the bitaxonomy al-
gorithm, in which hypernymy knowledge is trans-
ferred from one taxonomy to the other. By apply-
ing the projection operator pi to t, the algorithm
considers those nodes t
?
aligned to t in the dual
taxonomy (line 5) and obtains their hypernyms t
?
h
using the super
T
?
function (line 6). The nodes
reached in T
?
act as a clue for discovering the suit-
able hypernyms for the starting node t ? V (T ).
To perform the discovery, the algorithm projects
each such hypernym node t
?
h
? S and increments
the count of each projection t
h
? pi(t
?
h
) (line
Algorithm 1 The Bitaxonomy Algorithm
Input: T
P
, T
C
1: T := T
C
, T
?
:= T
P
2: repeat
3: for all t ? V (T ) s.t. @(t, t
h
) ? E(T ) do
4: reset count
5: for all t
?
? pi(t) do
6: S := super
T
?
(t
?
)
7: for all t
?
h
? S do
8: for all t
h
? pi(t
?
h
) do count(t
h
)++ end for
9: end for
10: end for
11:
?
t
h
:= argmax
t
h
: t;
T
t
h
count(t
h
)
12: if count(
?
t
h
) > 0 thenE(T ) := E(T )?{(t,
?
t
h
)}
13: end for
14: swap T and T
?
15: until convergence
16: return {T, T
?
}
8). Finally, the node
?
t
h
? V (T ) with maximum
count, and such that t ;
T
?
t
h
holds, if one exists,
is promoted as hypernym of t and a new hypernym
edge (t,
?
t
h
) is added toE(T ) (line 12). Finally, the
role of the two taxonomies is swapped and the pro-
cess is repeated until no more change is possible.
Example Let us illustrate the algorithm by way
of an example. Assume we are in the first iteration
(T = T
C
) and consider the Wikipedia category
t = OLYMPICS (line 3) and its super-categories
{MULTI-SPORT EVENTS, SPORT AND POLITICS,
INTERNATIONAL SPORTS COMPETITIONS}. This
category has 27 pages associated with it (line
5), 23 of which provide a hypernym page in T
P
(line 6): e.g., PARALYMPIC GAMES, associated
with the category OLYMPICS, is a MULTI-SPORT
EVENT and is therefore contained in S. By con-
sidering and counting the categories of each page
in S (lines 7-8), we end up counting the category
MULTI-SPORT EVENTS four times and other
categories, such as AWARDS and SWIMSUITS,
once. As MULTI-SPORT EVENTS has the highest
count and is connected to OLYMPICS by a path
in the Wikipedia category network (line 11),
the hypernym edge (OLYMPICS, MULTI-SPORT
EVENTS) is added to T
C
(line 12).
5 Phase 3: Category taxonomy
refinement
As the final phase, we refine and enrich the cate-
gory taxonomy. The goal of this phase is to pro-
vide broader coverage to the category taxonomy
T
C
created as explained in Section 4. We apply
three enrichment heuristics which add hypernyms
to those categories c for which no hypernym could
be found in phase 2, i.e., @c
?
s.t. (c, c
?
) ? E(T
C
).
949
Single super-category As a first structural re-
finement, we automatically link an uncovered cat-
egory c to c
?
if c
?
is the only direct super-category
of c in Wikipedia.
Sub-categories We increase the hypernym cov-
erage by exploiting the sub-categories of each un-
covered category c (see Figure 3a). In detail,
for each uncovered category c we consider the
set sub(c) of all the Wikipedia subcategories of
c (nodes c
1
, c
2
, . . . , c
n
in Figure 3a) and then let
each category vote, according to its direct hyper-
nym categories in T
C
(the vote is as in Algo-
rithm 1). Then we proceed in decreasing order
of vote and select the highest-ranking category c
?
which is connected to c via a path in T
C
, i.e.,
c ;
T
C
c
?
. We then pick up the direct ancestor
c
??
of c which lies in the path from c to c
?
and
add the hypernym edge (c, c
??
) to E(T
C
). For ex-
ample, consider the category FRENCH TELEVI-
SION PEOPLE; since this category has no asso-
ciated pages, in phase 2 no hypernym could be
found. However, by applying the sub-categories
heuristic, we discover that TELEVISION PEOPLE
BY COUNTRY is the hypernym most voted by our
target category?s descendants, such as FRENCH
TELEVISION ACTORS and FRENCH TELEVISION
DIRECTORS. Since TELEVISION PEOPLE BY
COUNTRY is at distance 1 in the Wikipedia
category network from FRENCH TELEVISION
PEOPLE, we add (FRENCH TELEVISION PEOPLE,
TELEVISION PEOPLE BY COUNTRY) to E(T
C
).
Super-categories We then apply a similar
heuristic involving super-categories (see Figure
3b). Given an uncovered category c, we consider
its direct Wikipedia super-categories and let them
vote, according to their hypernym categories in
T
C
. Then we proceed in decreasing order of vote
and select the highest-ranking category c
?
which is
connected to c in T
C
, i.e., c;
T
C
c
?
. We then pick
up the direct ancestor c
??
of c which lies in the path
from c to c
?
and add the edge (c, c
??
) to E(T
C
).
5.1 Bitaxonomy Evaluation
Category taxonomy statistics We applied
phases 2 and 3 to the output of phase 1, which
was evaluated in Section 3.3. In Figure 4a we
show the increase in category coverage at each
iteration throughout the execution of the two
phases (1SUP, SUB and SUPER correspond to
the three above heuristics of phase 3). The final
outcome is a category taxonomy which includes
594,917 hypernymy links between categories,
c
?
d
e
c
??
c
c
1
c
2
. . .
c
n
(a) Sub categ. heuristic.
hypernym in T
C
Wikipedia super-category
c
?
c
???
c
1
c
??
c
m
. . .
c
(b) Super categ. heuristic.
Figure 3: Heuristic patterns for the coverage re-
finement of the category taxonomy.
covering more than 96% of the 618,641 categories
in the October 2012 English Wikipedia dump.
The graph shows the steepest slope in the first
iterations of phase 2, which converges around
400k categories at iteration 30, and a significant
boost due to phase 3 producing another 175k
hypernymy edges, with the super-category heuris-
tic contributing most. 78.90% of the nodes in
T
C
belong to the same connected component.
The average height of the biggest component of
T
C
is 23.26 edges and the maximum height is
49. We note that the average height of T
C
is
much greater than that of T
P
, which reflects the
category taxonomy distinguishing between very
subtle classes, such as ALBUMS BY ARTISTS,
ALBUMS BY RECORDING LOCATION, etc.
Category taxonomy quality To estimate the
quality of the category taxonomy, we ran-
domly sampled 1,000 categories and, for each of
them, we manually associated the super-categories
which were deemed to be appropriate hypernyms.
Figure 4b shows the performance trend as the al-
gorithm iteratively covers more and more cate-
gories. Phase 2 is particularly robust across it-
erations, as it leads to increased recall while re-
taining very high precision. As regards phase 3,
the super-categories heuristic leads to only a slight
precision decrease, while improving recall consid-
erably. Overall, the final taxonomy T
C
achieves
85.80% precision, 83.40% recall and 97.20% cov-
erage on our dataset.
Page taxonomy improvement As a result of
phase 2, 141,105 additional hypernymy links were
also added to the page taxonomy, resulting in
an overall 82.99% precision, 77.90% recall and
92.10% coverage, with a non-negligible 3% boost
from phase 1 to phase 2 in terms of recall and cov-
erage on our Wikipedia page dataset.
We also calculated some statistics for the result-
ing taxonomy obtained by aggregating the 3.8M
950
Figure 4: Category taxonomy evaluation.
hypernym links in a single directed graph. Over-
all, 99% of nodes belong to the same connected
component, with a maximum height of 29 and an
average height on the biggest component of 6.98.
6 Related Work
Although the extraction of taxonomies from
machine-readable dictionaries was already being
studied in the early 1970s (Calzolari et al, 1973),
pioneering work on large amounts of data only
appeared in the 1990s (Hearst, 1992; Ide and
V?eronis, 1993). Approaches based on hand-
crafted patterns and pattern matching techniques
have been developed to provide a supertype for
the extracted terms (Etzioni et al, 2004; Blohm,
2007; Kozareva and Hovy, 2010; Navigli and Ve-
lardi, 2010; Velardi et al, 2013, inter alia). How-
ever, these methods do not link terms to existing
knowledge resources such as WordNet, whereas
those that explicitly link do so by adding new
leaves to the existing taxonomy instead of acquir-
ing wide-coverage taxonomies from scratch (Pan-
tel and Ravichandran, 2004; Snow et al, 2006).
The recent upsurge of interest in collabo-
rative knowledge curation has enabled several
approaches to large-scale taxonomy acquisition
(Hovy et al, 2013). Most approaches initially
focused on the Wikipedia category network, an
entangled set of generalization-containment rela-
tions between Wikipedia categories, to extract the
hypernymy taxonomy as a subset of the network.
The first approach of this kind was WikiTaxonomy
(Ponzetto and Strube, 2007; Ponzetto and Strube,
2011), based on simple, yet effective lightweight
heuristics, totaling more than 100k is-a relations.
Other approaches, such as YAGO (Suchanek et
al., 2008; Hoffart et al, 2013), yield a taxonom-
ical backbone by linking Wikipedia categories to
WordNet. However, the categories are linked to
the first, i.e., most frequent, sense of the category
head in WordNet, involving only leaf categories in
the linking.
Interest in taxonomizing Wikipedia pages, in-
stead, developed with DBpedia (Auer et al, 2007),
which pioneered the current stream of work aimed
at extracting semi-structured information from
Wikipedia templates and infoboxes. In DBpedia,
entities are mapped to a coarse-grained ontology
which is collaboratively maintained and contains
only about 270 classes corresponding to popular
named entity types, in contrast to our goal of struc-
turing the full set of Wikipedia articles in a larger
and finer-grained taxonomy.
A few notable efforts to reconcile the two sides
of Wikipedia, i.e., pages and categories, have
been put forward very recently: WikiNet (Nas-
tase et al, 2010; Nastase and Strube, 2013) is a
project which heuristically exploits different as-
pects of Wikipedia to obtain a multilingual con-
cept network by deriving not only is-a relations,
but also other types of relations. A second project,
MENTA (de Melo and Weikum, 2010), creates
one of the largest multilingual lexical knowledge
bases by interconnecting more than 13M articles
in 271 languages. In contrast to our work, hy-
pernym extraction is supervised in that decisions
are made on the basis of labelled training exam-
ples and requires a reconciliation step owing to
the heterogeneous nature of the hypernyms, some-
thing that we only do for categories, due to their
noisy network. While WikiNet and MENTA bring
together the knowledge available both at the page
and category level, like we do, they either achieve
low precision and coverage of the taxonomical
structure or exhibit overly general hypernyms, as
we show in our experiments in the next section.
Our work differs from the others in at least three
respects: first, in marked contrast to most other re-
sources, but similarly to WikiNet and WikiTaxon-
omy, our resource is self-contained and does not
depend on other resources such as WordNet; sec-
ond, we address the taxonomization task on both
sides, i.e., pages and categories, by providing an
algorithm which mutually and iteratively transfers
knowledge from one side of the bitaxonomy to the
other; third, we provide a wide coverage bitaxon-
omy closer in structure and granularity to a manual
WordNet-like taxonomy, in contrast, for example,
to DBpedia?s flat entity-focused hierarchy.
2
2
Note that all the competitors on categories have average
height between 1 and 3.69 on their biggest component, while
we have 23.26, while on pages their height is between 1.9 and
4.22, while ours is 6.98. Since WordNet?s average height is
8.07 we deem WiBi to be the resource structurally closest to
WordNet.
951
Dataset System Prec. Rec. Cov.
Pages
WiBi 84.11 79.40 92.57
WikiNet 57.29
??
71.45
??
82.01
DBpedia 87.06 51.50
??
55.93
MENTA 81.52 72.49
?
88.92
Categories
WiBi 85.18 82.88 97.31
WikiTax 88.50 54.83
??
59.43
YAGO 94.13 53.41
??
56.74
MENTA 87.11 84.63 97.15
MENTA
?ENT
85.18 71.95
??
84.47
Table 2: Page and category taxonomy evaluation.
?
(
??
) denotes statistically significant difference,
using ?
2
test, p < 0.02 (p < 0.01) between WiBi
and the daggered resource.
7 Comparative Evaluation
7.1 Experimental Setup
We compared our resource (WiBi) against the
Wikipedia taxonomies of the major knowledge re-
sources in the literature providing hypernym links,
namely DBpedia, WikiNet, MENTA, WikiTax-
onomy and YAGO (see Section 6). As datasets,
we used our gold standards of 1,000 randomly-
sampled pages (see Section 3.3) and categories
(see Section 5.1). In order to ensure a level playing
field, we detected those pages (categories) which
do not exist in any of the above resources and re-
moved them to ensure full coverage of the dataset
across all resources. For each resource we cal-
culated precision, by manually marking each hy-
pernym returned for each page (category) as cor-
rect or not. As regards recall, we note that in
two cases (i.e., DBpedia returning page super-
types from its upper taxonomy, YAGO linking cat-
egories to WordNet synsets) the generalizations
are neither pages nor categories and that MENTA
returns heterogeneous hypernyms as mixed sets of
WordNet synsets, Wikipedia pages and categories.
Given this heterogeneity, standard recall across re-
sources could not be calculated. For this reason we
calculated recall as described in Section 3.3.
7.2 Results
Wikipedia pages We first report the results of
the knowledge resources which provide page hy-
pernyms, i.e., we compare against WikiNet, DB-
pedia and MENTA. We use the original outputs
from the three resources: the first two are based
on dumps which are from the same year as the one
used in WiBi (cf. Section 3.3), while MENTA is
based on a dump dating back to 2010 (consisting
of 3.25M pages and 565k categories). We decided
to include the latter for comparison purposes, as it
uses knowledge from 271 Wikipedias to build the
final taxonomy. However, we recognize its perfor-
mance might be relatively higher on a 2012 dump.
We show the results on our page hypernym
dataset in Table 2 (top). As can be seen, WikiNet
obtains the lowest precision, due to the high num-
ber of hypernyms provided, many of which are
incorrect, with a recall between that of DBpe-
dia and MENTA. WiBi outperforms all other re-
sources with 84.11% precision, 79.40% recall and
92.57% coverage. MENTA seems to be the clos-
est resource to ours, however, we remark that the
hypernyms output by MENTA are very heteroge-
neous: 48% of answers are represented by a Word-
Net synset, 37% by Wikipedia categories and 15%
are Wikipedia pages. In contrast to all other re-
sources, WiBi outputs page hypernyms only.
Wikipedia categories We then compared all the
knowledge resources which deal with categories,
i.e., WikiTaxonomy, YAGO and MENTA. For the
latter two, the above considerations about the 2012
dump hold, whereas we reimplemented WikiTax-
onomy, which was based on a 2009 dump, to run it
on the same dump as WiBi. We excluded WikiNet
from our comparison because it turned out to have
low coverage of categories (i.e., less than 1%).
We show the results on our category dataset
in Table 2 (bottom). Despite other systems ex-
hibiting higher precision, WiBi generally achieves
higher recall, thanks also to its higher category
coverage. YAGO obtains the lowest recall and
coverage, because only leaf categories are consid-
ered. MENTA is the closest system to ours, ob-
taining slightly higher precision and recall. No-
tably, however, MENTA outputs the first WordNet
sense of entity for 13.17% of all the given answers,
which, despite being correct and accounted in pre-
cision and recall, is uninformative. Since a system
which always outputs entity would maximise all
the three measures, we also calculated the perfor-
mance for MENTA when discarding entity as an
answer; as Table 2 shows (bottom, MENTA
?ENT
),
recall drops to 71.95%. Further analysis, pre-
sented below, shows that the specificity of its hy-
pernyms is considerably lower than that of WiBi.
7.3 Analysis of the results
To get further insight into our results we per-
formed two additional analyses of the data. First,
we estimated the level of specialization of the
hypernyms in the different resources on our two
datasets. The idea is that a hypernym should be
952
Dataset System (X) WiBi=X WiBi>X WiBi<X
Pages
WikiNet 33.38 34.94 31.68
DBpedia 31.68 56.71 11.60
MENTA 19.04 50.85 30.12
Categories
WikiTax 43.11 38.51 18.38
YAGO 12.36 81.14 6.50
MENTA 12.36 73.69 13.95
Table 3: Specificity comparison.
valid while at the same time being as specific as
possible (e.g., SINGER should be preferred over
PERSON). We therefore calculated a measure,
which we called specificity, that computes the per-
centage of times a system outputs a more specific
answer than another system. To do this, we anno-
tated each hypernym returned by a system as fol-
lows: ?1 if the answer was wrong, 0 if missing, >
0 if correct; more specific answers were assigned
higher scores. When comparing two systems, we
select the respective most specific answers a
1
, a
2
and say the first system is more specific than the
latter whenever score(a
1
) > score(a
2
). Table 3
shows the results for all the resources and for both
the page and category taxonomies: WiBi consis-
tently provides considerably more specific hyper-
nyms than any other resource (middle column).
A second important aspect that we analyzed was
the granularity of each taxonomy, determined by
drawing each resource on a bidimensional plane
with the number of distinct hypernyms on the
x axis and the total number of hypernyms (i.e.,
edges) in the taxonomy on the y axis. Figures 5a
and 5b show the position of each resource for the
page and the category taxonomies, respectively.
As can be seen, WiBi, as well as the page tax-
onomy of MENTA, is the resource with the best
granularity, as not only does it attain high cover-
age, but it also provides a larger variety of classes
as generalizations of pages and categories. Specif-
ically, WiBi provides over 3M hypernym pages
chosen from a range of 94k distinct hypernyms,
while others exhibit a considerably smaller range
of distinct hypernyms (e.g., DBpedia by design,
but also WikiNet, with around 11k distinct page
hypernyms). The large variety of classes provided
by MENTA, however, is due to including more
than 100k Wikipedia categories (among which,
categories about deaths and births alone repre-
sent about 2% of the distinct hypernyms). As re-
gards categories, while the number of distinct hy-
pernyms of WiBi and WikiTaxonomy is approxi-
mately the same (around 130k), the total number
of hypernyms (around 580k for both taxonomies)
is distributed over half of the categories in Wiki-
(a) Page taxonomies (b) Category taxonomies
Figure 5: Hypernym granularity for the resources.
Taxonomy compared to WiBi, resulting in a dou-
ble number of hypernyms per category, but lower
coverage (cf. Table 2).
8 Conclusions
In this paper we have presented WiBi, an auto-
matic 3-phase approach to the construction of a
bitaxonomy for the English Wikipedia, i.e., a full-
fledged, integrated page and category taxonomy:
first, using a set of high-precision linkers, the page
taxonomy is populated; next, a fixed point algo-
rithm populates the category taxonomy while en-
riching the page taxonomy iteratively; finally, the
category taxonomy undergoes structural refine-
ments. Coverage, quality and granularity of the
bitaxonomy are considerably higher than the tax-
onomy of state-of-the-art resources like DBpedia,
YAGO, MENTA, WikiNet and WikiTaxonomy.
Our contributions are three-fold: i) we propose
a unified, effective approach to the construction of
a Wikipedia bitaxonomy, a richer structure than
those produced in the literature; ii) our method for
building the bitaxonomy is self-contained, thanks
to its independence from external resources (like
WordNet) and the virtual absence of supervision,
making WiBi replicable on any new version of
Wikipedia; iii) the taxonomy provides nearly full
coverage of pages and categories, encompassing
the entire encyclopedic knowledge in Wikipedia.
We will apply our video games with a purpose
(Vannella et al, 2014) to validate WiBi. We also
plan to integrate WiBi into BabelNet (Navigli and
Ponzetto, 2012), so as to fully taxonomize it, and
exploit its high quality for improving semantic
predicates (Flati and Navigli, 2013).
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Luca Telesca for his implementation of
WikiTaxonomy and Jim McManus for his com-
ments on the manuscript.
953
References
Robert A. Amsler. 1981. A Taxonomy for English
Nouns and Verbs. In Proceedings of Association for
Computational Linguistics (ACL ?81), pages 133?
138, Stanford, California, USA.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735, Bu-
san, Korea.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154?165.
Sebastian Blohm. 2007. Using the web to reduce data
sparseness in pattern-based information extraction.
In Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases (PKDD), pages 18?29, Warsaw, Poland.
Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the International
Conference on Management of Data (SIGMOD ?08),
SIGMOD ?08, pages 1247?1250, New York, NY,
USA.
Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-
polli. 1973. Working on the Italian Machine Dictio-
nary: a Semantic Approach. In Proceedings of the
5th Conference on Computational Linguistics (COL-
ING ?73), pages 49?70, Pisa, Italy.
Nicoletta Calzolari. 1982. Towards the organization of
lexical definitions on a database structure. In Proc.
of the 9th Conference on Computational Linguistics
(COLING ?82), pages 61?64, Prague, Czechoslo-
vakia.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proceedings of Conference on Information and
Knowledge Management (CIKM ?10), pages 1099?
1108, New York, NY, USA.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
ItAll: (preliminary results). In Proceedings of the
13th International Conference on World Wide Web
(WWW ?04), pages 100?110, New York, NY, USA.
ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
David A. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3):1.
Tiziano Flati and Roberto Navigli. 2013. SPred:
Large-scale Harvesting of Semantic Predicates. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1222?1232, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the International Conference on Computational
Linguistics (COLING ?92), pages 539?545, Nantes,
France.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from Wikipedia. Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Nancy Ide and Jean V?eronis. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
the Workshop on Knowledge Bases and Knowledge
Structures, pages 257?266, Tokyo, Japan.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), pages 3?10,
Vancouver, British Columbia, Canada.
Zornitsa Kozareva and Eduard H. Hovy. 2010. A
Semi-Supervised Method to Learn and Construct
Taxonomies Using the Web. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?10), pages 1110?1118,
Seattle, WA, USA.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 558?566,
Athens, Greece.
Tom Mitchell. 2005. Reading the Web: A Break-
through Goal for AI. AI Magazine.
Vivi Nastase and Michael Strube. 2013. Transform-
ing Wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62?85.
954
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A Very Large Scale Multi-Lingual Concept Net-
work. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010), pages 1318?1327, Uppsala, Sweden,
July. Association for Computational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL HLT
2013), Boston, Massachusetts, 2?7 May 2004, pages
321?328.
Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from Wikipedia.
In Proceedings of the 22nd Conference on the Ad-
vancement of Artificial Intelligence (AAAI ?07), Van-
couver, B.C., Canada, 22?26 July 2007, pages
1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737?1756.
Hoifung Poon, Janara Christensen, Pedro Domingos,
Oren Etzioni, Raphael Hoffmann, Chloe Kiddon,
Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Ste-
fan Schoenmackers, Stephen Soderland, Dan Weld,
Fei Wu, and Congle Zhang. 2010. Machine Read-
ing at the University of Washington. In Proceedings
of the 1st International Workshop on Formalisms
and Methodology for Learning by Reading in con-
junction with NAACL-HLT 2010, pages 87?95, Los
Angeles, California, USA.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lec-
ture Notes in Computer Science, pages 380?386.
Springer Verlag.
Amit Singhal. 2012. Introducing the Knowledge
Graph: Things, Not Strings. Technical report, Of-
ficial Blog (of Google). Retrieved May 18, 2012.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 801?
808.
Fabian Suchanek and Gerhard Weikum. 2013. Knowl-
edge harvesting from text and Web sources. In IEEE
29th International Conference on Data Engineer-
ing (ICDE 2013), pages 1250?1253, Brisbane, Aus-
tralia. IEEE Computer Society.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Daniele Vannella, David Jurgens, Daniele Scarfini,
Domenico Toscani, and Roberto Navigli. 2014.
Validating and Extending Semantic Knowledge
Bases using Video Games with a Purpose. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2014),
Baltimore, USA.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3):665?707.
955
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 67?72,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
WoSIT: A Word Sense Induction Toolkit
for Search Result Clustering and Diversification
Daniele Vannella, Tiziano Flati and Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
{vannella,flati,navigli}@di.uniroma1.it
Abstract
In this demonstration we present WoSIT,
an API for Word Sense Induction (WSI)
algorithms. The toolkit provides imple-
mentations of existing graph-based WSI
algorithms, but can also be extended with
new algorithms. The main mission of
WoSIT is to provide a framework for the
extrinsic evaluation of WSI algorithms,
also within end-user applications such as
Web search result clustering and diversifi-
cation.
1 Introduction
The Web is by far the world?s largest information
archive, whose content ? made up of billions of
Web pages ? is growing exponentially. Unfortu-
nately the retrieval of any given piece of infor-
mation is an arduous task which challenges even
prominent search engines such as those developed
by Google, Yahoo! and Microsoft. Even today,
such systems still find themselves up against the
lexical ambiguity issue, that is, the linguistic prop-
erty due to which a single word may convey dif-
ferent meanings.
It has been estimated that around 4% of Web
queries and 16% of the most frequent queries are
ambiguous (Sanderson, 2008). A major issue as-
sociated with the lexical ambiguity phenomenon
on the Web is the low number of query words sub-
mitted by Web users to search engines. A pos-
sible solution to this issue is the diversification of
search results obtained by maximizing the dissimi-
larity of the top-ranking Web pages returned to the
user (Agrawal et al., 2009; Ashwin Swaminathan
and Kirovski, 2009). Another solution consists of
clustering Web search results by way of clustering
engines such as Carrot
1
and Yippy
2
and presenting
them to the user grouped by topic.
1
http://search.carrot2.org
2
http://yippy.com
Diversification and Web clustering algorithms,
however, do not perform any semantic analysis of
search results, clustering them solely on the basis
of their lexical similarity. Recently, it has been
shown that the automatic acquisition of the mean-
ings of a word of interest, a task referred to as
Word Sense Induction, can be successfully inte-
grated into search result clustering and diversifica-
tion (Navigli and Crisafulli, 2010; Di Marco and
Navigli, 2013) so as to outperform non-semantic
state-of-the-art Web clustering systems.
In this demonstration we describe a new toolkit
for Word Sense Induction, called WoSIT, which
i) provides ready implementations of existing
WSI algorithms; ii) can be extended with addi-
tional WSI algorithms; iii) enables the integration
of WSI algorithms into search result clustering
and diversification, thereby providing an extrinsic
evaluation tool. As a result the toolkit enables the
objective comparison of WSI algorithms within an
end-user application in terms of the degree of di-
versification of the search results of a given am-
biguous query.
2 WoSIT
In Figure 1 we show the workflow of the WoSIT
toolkit, composed of three main phases: WSI;
semantically-enhanced search result clustering
and diversification; evaluation. Given a target
query q whose meanings we want to automati-
cally acquire, the toolkit first builds a graph for q,
obtained either from a co-occurrence database, or
constructed programmatically by using any user-
provided input. The co-occurrence graph is then
input to a WSI algorithm, chosen from among
those available in the toolkit or implemented by
the user. As a result, a set of word clusters
is produced. This concludes the first phase of
the WoSIT workflow. Then, the word clusters
produced are used for assigning meanings to the
search results returned by a search engine for the
query q, i.e. search result disambiguation. The
67
  
+ MANUAL ANNOTATIONS
DBDB
Search resultdisambiguation
w1w3w2w4w5w6 s4s5s1 s3
s2
Eval ResultsWord ClustersWSIAlgorithm WSI EvaluatorAssignment of results to clustersCo-occurrence graph
Co-occurrence Information Web search engine
WSI Semantically EnhancedSearch Result Clustering
Dataset
Evaluation
Figure 1: The WoSIT workflow.
outcome is that we obtain a clustering of search
results. Finally, during the third phase, we apply
the evaluation module which performs an evalua-
tion of the search result clustering quality and the
diversification performance.
We now describe in detail the three main phases
of WoSIT.
2.1 Word Sense Induction
The first phase of WoSIT consists of the automatic
identification of the senses of a query of inter-
est, i.e. the task of Word Sense Induction. Al-
though WoSIT enables the integration of custom
implementations which can potentially work with
any WSI paradigm, the toolkit provides ready-to-
use implementations of several graph-based algo-
rithms that work with word co-occurrences. All
these algorithms carry out WSI in two steps: co-
occurrence graph construction (Section 2.1.1) and
discovery of word senses (Section 2.1.2).
2.1.1 Co-occurrence graph construction
Given a target query q, we build a co-occurrence
graph G
q
= (V,E) such that V is the set of
words co-occurring with q and E is the set of undi-
rected edges, each denoting a co-occurrence be-
tween pairs of words in V . In Figure 2 we show
an example of a co-occurrence graph for the target
word excalibur.
WoSIT enables the creation of the co-
occurrence graph either programmatically, by
adding edges and vertices according to any user-
specific algorithm, or starting from the statis-
tics for co-occurring words obtained from a co-
occurrence database (created, e.g., from a text cor-
pus, as was done by Di Marco and Navigli (2013)).
In either case, weights for edges have to be pro-
vided in terms of the correlation strength between
pairs of words (e.g. using Dice, Jaccard or other
co-occurrence measures).
The information about the co-occurrence
database, e.g. a MySQL database, is provided
programmatically or via parameters in the prop-
erties configuration file (db.properties).
The co-occurrence database has to follow a
given schema provided in the toolkit docu-
mentation. An additional configuration file
(wosit.properties) also allows the user
to specify additional constraints, e.g. the
minimum weight value of co-occurrence (the
wordGraph.minWeight parameter) to be
added as edges to the graph.
The graphs produced can also be saved to binary
(i.e. serialized) or text file:
g.saveToSer(fileName);
g = WordGraph.loadFromSer(fileName);
g.saveToTxt(fileName);
g = WordGraph.loadFromTxt(fileName);
We are now ready to provide our co-occurrence
graph, created with just a few lines of code, as in-
put to a WSI algorithm, as will be explained in the
next section.
2.1.2 Discovery of Word Senses
Once the co-occurrence graph for the query q is
built, it can be input to any WSI algorithm which
extends the GraphClusteringAlgorithm
class in the toolkit. WoSIT comes with a number
of ready-to-use such algorithms, among which:
68
Car
Limousine
King Arthur
Excalibur
Film
Fantasy
Book
0.02
0.015
0.025
0.005
0.04
0.006
0.007
0.01
0.013
0.012
0.02
Figure 2: Example of a co-occurrence graph for
the word excalibur.
? Balanced Maximum Spanning Tree (B-
MST) (Di Marco and Navigli, 2013), an ex-
tension of a WSI algorithm based on the
calculation of a Maximum Spanning Tree
(Di Marco and Navigli, 2011) aimed at bal-
ancing the number of co-occurrences in each
sense cluster.
? HyperLex (V?eronis, 2004), an algorithm
which identifies hubs in co-occurrence
graphs, thereby identifying basic meanings
for the input query.
? Chinese Whispers (Biemann, 2006), a ran-
domized algorithm which partitions nodes by
means of the iterative transfer of word sense
information across the co-occurrence graph
(Biemann, 2006).
? Squares, Triangles and Diamonds
(SquaT++) (Di Marco and Navigli, 2013),
an extension of the SquaT algorithm (Navigli
and Crisafulli, 2010) which exploits three
cyclic graph patterns to determine and
discard those vertices (or edges) with weak
degree of connectivity in the graph.
We also provide an implementation of a word
clustering algorithm, i.e. Lin98 (Lin, 1998),
which does not rely on co-occurrence graphs, but
just on the word co-occurrence information to it-
eratively refine word clusters on the basis of their
?semantic? relationships.
A programmatic example of use of the B-MST
WSI algorithm is as follows:
BMST mst = new BMST(g);
mst.makeClustering();
Clustering wordClusters =
mst.getClustering();
where g is a co-occurrence graph created as ex-
plained in Section 2.1.1, provided as input to
the constructor of the algorithm?s class. The
makeClustering method implements the in-
duction algorithm and creates the word clus-
ters, which can then be retrieved calling the
getClustering method. As a result an in-
stance of the Clustering class is provided.
As mentioned above, WoSIT also enables
the creation of custom WSI implementa-
tions. This can be done by extending the
GraphClusteringAlgorihm abstract class.
The new algorithm just has to implement two
methods:
public void makeClustering();
public Clustering getClustering();
As a result, the new algorithm is readily inte-
grated into the WoSIT toolkit.
2.2 Semantically-enhanced Search Result
Clustering and Diversification
We now move to the use of the induced senses of
our target query q within an application, i.e. search
result clustering and diversification.
Search result clustering. The next step (cf. Fig-
ure 1) is the association of the search results re-
turned by a search engine for query q with the most
suitable word cluster (i.e. meaning of q). This can
be done in two lines:
SnippetAssociator associator =
SnippetAssociator.getInstance();
SnippetClustering clustering =
associator.associateSnippet(
targetWord,
searchResults,
wordClusters,
AssociationMetric.DEGREE_OVERLAP);
The first line obtains an instance of the class
which performs the association between search re-
sult snippets and the word clusters obtained from
the WSI algorithm. The second line calls the asso-
ciation method associateSnippet which in-
puts the target word, the search results obtained
from the search engine, the word clusters and, fi-
nally, the kind of metric to use for the associa-
tion. Three different association metrics are im-
plemented in the toolkit:
? WORD OVERLAP performs the association by
maximizing the size of the intersection be-
tween the word sets in each snippet and the
word clusters;
? DEGREE OVERLAP performs the association
by calculating for each word cluster the sum
69
of the vertex degrees in the co-occurrence
graph of the words occurring in each snippet;
? TOKEN OVERLAP is similar in spirit to
WORD OVERLAP, but takes into account each
token occurrence in the snippet bag of words.
Search result diversification. The above two
lines of code return a set of snippet clusters and, as
a result, semantically-enhanced search result clus-
tering is performed. At the end, the resulting clus-
tering can be used to provide a diversified rerank-
ing of the results:
List<Snippet> snippets =
clustering.diversify(sorter);
The diversify method returns a flat list of
snippet results obtained according to the Sorter
object provided in input. The Sorter abstract
class is designed to rerank the snippet clusters ac-
cording to some predefined rule. For instance, the
CardinalitySorter class, included in the
toolkit, sorts the clusters according to the size of
each cluster. Once a sorting order has been es-
tablished, an element from each snippet cluster is
added to an initially-empty list; next, a second el-
ement from each cluster is added, and so on, until
all snippets are added to the list.
The sorting rules implemented in the toolkit are:
? CardinalitySorter: sorts the clusters
according to their size, i.e. the number of ver-
tices in the cluster;
? MeanSimilaritySorter: sorts the clus-
ters according to the average association
score between the snippets in the cluster and
the backing word cluster (defined by the se-
lected association metrics).
Notably, the end user can then implement his or
her own custom sorting procedure by simply ex-
tending the Sorter class.
2.2.1 Search Result Datasets
The framework comes with two search result
datasets of ambiguous queries: the AMBI-
ENT+MORESQUE dataset made available by
Bernardini et al. (2009) and Navigli and Crisa-
fulli (2010), respectively, and the SemEval-2013-
Task11 dataset.
3
New result datasets can be pro-
vided by users complying with the dataset format
described below.
3
For details visit http://lcl.uniroma1.it/
wosit/.
A search result dataset in WoSIT is made up of
at least two files:
? topics.txt, which contains the queries
(topics) of interest together with their nu-
meric ids. For instance:
id description
1 polaroid
2 kangaroo
3 shakira
... ...
? results.txt, which lists the search re-
sults for each given query, in terms of URL,
page title and page snippet:
ID url title snippet
1.1 http://www.polaroid.com/ Polaroid | Home ...
1.2 http://www.polaroid.com/products products...
1.3 http://en.wikipedia.org/wiki/Polaroid_Cor...
... ...
Therefore, the two files provide the queries and the
corresponding search results returned by a search
engine. In order to enable an automatic evaluation
of the search result clustering and diversification
output, two additional files have to be provided:
? subTopics.txt, which for each query
provides the list of meanings for that query,
e.g.:
ID description
1.1 Polaroid Corporation, a multinational con...
1.2 Instant film photographs are sometimes kn...
1.3 Instant camera (or Land camera), sometime...
... ...
? STRel.txt, which provides the manual as-
sociations between each search result and the
most suitable meaning as provided in the
subTopics.txt file. For instance:
subTopicID resultID
1.1 1.1
1.1 1.2
1.1 1.3
... ...
2.3 WSI Evaluator
As shown in Figure 1 the final component of our
workflow is the evaluation of WSI when integrated
into search result clustering and diversification (al-
ready used by Navigli and Vannella (2013)). This
component, called the WSI Evaluator, takes as
input the snippet clusters obtained for a given
query together with the fully annotated search re-
sult dataset, as described in the previous section.
Two kinds of evaluations are carried out, described
in what follows.
70
1 Dataset searchResults = Dataset.getInstance();
2 DBConfiguration db = DBConfiguration.getInstance();
3 for(String targetWord : dataset.getQueries())
4 {
5 WordGraph g = WordGraph.createWordGraph(targetWord, searchResults, db);
6 BMST mst = new BMST(g);
7 mst.makeClustering();
8 SnippetAssociator snippetAssociator = SnippetAssociator.getInstance();
9 SnippetClustering snippetClustering = snippetAssociator.associateSnippet(
10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP);
11 snippetClustering.export("output/outputMST.txt", true);
12 }
13 WSIEvaluator.evaluate(searchResults, "output/outputMST.txt");
Figure 3: An example of evaluation code for the B-MST clustering algorithm.
2.3.1 Evaluation of the clustering quality
The quality of the output produced by
semantically-enhanced search result cluster-
ing is evaluated in terms of Rand Index (Rand,
1971, RI), Adjusted Rand Index (Hubert and
Arabie, 1985, ARI), Jaccard Index (JI) and,
finally, precision and recall as done by Crabtree et
al. (2005), together with their F1 harmonic mean.
2.3.2 Evaluation of the clustering diversity
To evaluate the snippet clustering diversity the
measures of S-recall@K and S-precision@r (Zhai
et al., 2003) are calculated. These measures de-
termine how many different meanings of a query
are covered in the top-ranking results shown to the
user. We calculate these measures on the output of
the three different association metrics illustrated in
Section 2.2.
3 A Full Example
We now show a full example of usage of the
WoSIT API. The code shown in Figure 3 initially
obtains a search result dataset (line 1), selects a
database (line 2) and iterates over its queries (line
3). Next, a co-occurrence graph for the current
query is created from a co-occurrence database
(line 5) and an instance of the B-MST WSI algo-
rithm is created with the graph as input (line 6).
After executing the algorithm (line 7), the snippets
for the given query are clustered (lines 8-10). The
resulting snippet clustering is appended to an out-
put file (line 11). Finally, the WSI evaluator is run
on the resulting snippet clustering using the given
dataset (line 13).
3.1 Experiments
We applied the WoSIT API to the AMBI-
ENT+MORESQUE dataset using 4 induction al-
Algorithm
Assoc. Web1T
metr. ARI JI F1 # cl.
SquaT++
WO 69.65 75.69 59.19 2.1
DO 69.21 75.45 59.19 2.1
TO 69.67 75.69 59.19 2.1
B-MST
WO 60.76 71.51 64.56 5.0
DO 66.48 69.37 64.84 5.0
TO 63.17 71.21 64.04 5.0
HyperLex
WO 60.86 72.05 65.41 13.0
DO 66.27 68.00 71.91 13.0
TO 62.82 70.87 65.08 13.0
Chinese Whispers
WO 67.75 75.37 60.25 12.5
DO 65.95 69.49 70.33 12.5
TO 67.57 74.69 60.50 12.5
Table 1: Results of WSI algorithms with a Web1T
co-occurrence database and the three association
metrics (Word Overlap, Degree Overlap and To-
ken Overlap). The reported measures are Ad-
justed Rand Index (ARI), Jaccard Index (JI) and
F1. We also show the average number of clusters
per query produced by each algorithm.
gorithms among those available in the toolkit,
where co-occurrences were obtained from the
Google Web1T corpus (Brants and Franz, 2006).
In Table 1 we show the clustering quality results
output by the WoSIT evaluator, whereas in Fig-
ure 4 we show the diversification performance in
terms of S-recall@K.
3.2 Conclusions
In this demonstration we presented WoSIT, a full-
fledged toolkit for Word Sense Induction algo-
rithms and their integration into search result clus-
tering and diversification. The main contributions
are as follows: first, we release a Java API for
performing Word Sense Induction which includes
several ready-to-use implementations of existing
algorithms; second, the API enables the use of the
acquired senses for a given query for enhancing
71
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0 20.0
S-reca
ll-at-K
K
HyperLexBMSTChineseWSquaT++
Figure 4: S-recall@K performance.
search result clustering and diversification; third,
we provide an evaluation component which, given
an annotated dataset of search results, carries out
different kinds of evaluation of the snippet cluster-
ing quality and diversity.
WoSIT is the first available toolkit which pro-
vides an end-to-end approach to the integration of
WSI into a real-world application. The toolkit en-
ables an objective comparison of WSI algorithms
as well as an evaluation of the impact of apply-
ing WSI to clustering and diversifying search re-
sults. As shown by Di Marco and Navigli (2013),
this integration is beneficial and allows outperfor-
mance of non-semantic state-of-the-art Web clus-
tering systems.
The toolkit, licensed under a Creative Com-
mons Attribution-Non Commercial-Share Alike
3.0 License, is available at http://lcl.
uniroma1.it/wosit/.
References
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halver-
son, and Samuel Ieong. 2009. Diversifying search
results. In Proc. of the Second International Confer-
ence on Web Search and Web Data Mining (WSDM
2009), pages 5?14, Barcelona, Spain.
Cherian V. Mathew Ashwin Swaminathan and Darko
Kirovski. 2009. Essential Pages. In Proc. of the
2009 IEEE/WIC/ACM International Joint Confer-
ence on Web Intelligence and Intelligent Agent Tech-
nology, volume 1, pages 173?182.
Andrea Bernardini, Claudio Carpineto, and Massim-
iliano D?Amico. 2009. Full-Subtopic Retrieval
with Keyphrase-Based Search Results Clustering.
In Proc. of Web Intelligence 2009, volume 1, pages
206?213, Los Alamitos, CA, USA.
Chris Biemann. 2006. Chinese Whispers - an Effi-
cient Graph Clustering Algorithm and its Applica-
tion to Natural Language Processing Problems. In
Proc. of TextGraphs: the First Workshop on Graph
Based Methods for Natural Language Processing,
pages 73?80, New York City.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, ver. 1, LDC2006T13. In Linguistic Data Con-
sortium, Philadelphia, USA.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selec-
tion. In Proc. of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172?
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2011. Clus-
tering Web Search Results with Maximum Spanning
Trees. In Proc. of the XIIth International Confer-
ence of the Italian Association for Artificial Intelli-
gence (AI*IA), pages 201?212, Palermo, Italy.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and Diversifying Web Search Results with
Graph-Based Word Sense Induction. Computa-
tional Linguistics, 39(3):709?754.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing Partitions. Journal of Classification, 2(1):193?
218.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proc. of the 17
th
Inter-
national Conference on Computational linguistics
(COLING), pages 768?774, Montreal, Canada.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing Word Senses to Improve Web Search Result
Clustering. In Proc. of the 2010 Conference on Em-
pirical Methods in Natural Language Processing,
pages 116?126, Boston, USA.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 Task 11: Evaluating Word Sense In-
duction & Disambiguation within An End-User Ap-
plication. In Proc. of the 7
th
International Work-
shop on Semantic Evaluation (SemEval 2013), in
conjunction with the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 193?201, Atlanta, USA.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical association, 66(336):846?850.
Mark Sanderson. 2008. Ambiguous queries: test col-
lections need more sense. In Proc. of the 31st an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 499?506, Singapore.
Jean V?eronis. 2004. HyperLex: lexical cartography
for information retrieval. Computer, Speech and
Language, 18(3):223?252.
ChengXiang Zhai, William W. Cohen, and John Laf-
ferty. 2003. Beyond independent relevance: Meth-
ods and evaluation metrics for subtopic retrieval. In
Proc. of the 26th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 10?17, Toronto, Canada.
72
