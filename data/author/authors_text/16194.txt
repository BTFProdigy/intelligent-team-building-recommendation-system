Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 344?348,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Latent Information to Predict Diffusions of Novel Topics on 
Social Networks 
Tsung-Ting Kuo1*, San-Chuan Hung1, Wei-Shih Lin1, Nanyun Peng1, Shou-De Lin1, 
Wei-Fen Lin2 
1Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan 
2MobiApps Corporation, Taiwan 
*d97944007@csie.ntu.edu.tw 
 
Abstract 
This paper brings a marriage of two seemly 
unrelated topics, natural language 
processing (NLP) and social network 
analysis (SNA). We propose a new task in 
SNA which is to predict the diffusion of a 
new topic, and design a learning-based 
framework to solve this problem. We 
exploit the latent semantic information 
among users, topics, and social connections 
as features for prediction. Our framework is 
evaluated on real data collected from public 
domain. The experiments show 16% AUC 
improvement over baseline methods. The 
source code and dataset are available at 
http://www.csie.ntu.edu.tw/~d97944007/dif
fusion/ 
1 Background 
The diffusion of information on social networks 
has been studied for decades. Generally, the 
proposed strategies can be categorized into two 
categories, model-driven and data-driven. The 
model-driven strategies, such as independent 
cascade model (Kempe et al, 2003), rely on 
certain manually crafted, usually intuitive, models 
to fit the diffusion data without using diffusion 
history. The data-driven strategies usually utilize 
learning-based approaches to predict the future 
propagation given historical records of prediction 
(Fei et al, 2011; Galuba et al, 2010; Petrovic et al, 
2011).  Data-driven strategies usually perform 
better than model-driven approaches because the 
past diffusion behavior is used during learning 
(Galuba et al, 2010). 
Recently, researchers started to exploit content 
information in data-driven diffusion models (Fei et 
al., 2011; Petrovic et al, 2011; Zhu et al, 2011). 
However, most of the data-driven approaches 
assume that in order to train a model and predict 
the future diffusion of a topic, it is required to 
obtain historical records about how this topic has 
propagated in a social network (Petrovic et al, 
2011; Zhu et al, 2011). We argue that such 
assumption does not always hold in the real-world 
scenario, and being able to forecast the propagation 
of novel or unseen topics is more valuable in 
practice. For example, a company would like to 
know which users are more likely to be the source 
of ?viva voce? of a newly released product for 
advertising purpose. A political party might want 
to estimate the potential degree of responses of a 
half-baked policy before deciding to bring it up to 
public. To achieve such goal, it is required to 
predict the future propagation behavior of a topic 
even before any actual diffusion happens on this 
topic (i.e., no historical propagation data of this 
topic are available). Lin et al also propose an idea 
aiming at predicting the inference of implicit 
diffusions for novel topics (Lin et al, 2011). The 
main difference between their work and ours is that 
they focus on implicit diffusions, whose data are 
usually not available. Consequently, they need to 
rely on a model-driven approach instead of a data-
driven approach. On the other hand, our work 
focuses on the prediction of explicit diffusion 
behaviors. Despite the fact that no diffusion data of 
novel topics is available, we can still design a data-
driven approach taking advantage of some explicit 
diffusion data of known topics. Our experiments 
show that being able to utilize such information is 
critical for diffusion prediction. 
2 The Novel-Topic Diffusion Model 
We start by assuming an existing social network G 
= (V, E), where V is the set of nodes (or user) v, 
and E is the set of link e. The set of topics is 
344
denoted as T. Among them, some are considered as 
novel topics (denoted as N), while the rest (R) are 
used as the training records.  We are also given a 
set of diffusion records D = {d | d = (src, dest, t)}, 
where src is the source node (or diffusion source), 
dest is the destination node, and t is the topic of the 
diffusion that belongs to R but not N. We assume 
that diffusions cannot occur between nodes without 
direct social connection; any diffusion pair implies 
the existence of a link e = (src, dest ?)  E. Finally, 
we assume there are sets of keywords or tags that 
relevant to each topic (including existing and novel 
topics). Note that the set of keywords for novel 
topics should be seen in that of existing topics. 
From these sets of keywords, we construct a topic-
word matrix TW = (P(wordj | topici))i,j of which the 
elements stand for the conditional probabilities that 
a word appears in the text of a certain topic. 
Similarly, we also construct a user-word matrix 
UW= (P(wordj | useri))i,j from these sets of 
keywords. Given the above information, the goal is 
to predict whether a given link is active (i.e., 
belongs to a diffusion link) for topics in N. 
2.1 The Framework 
The main challenge of this problem lays in that the 
past diffusion behaviors of new topics are missing. 
To address this challenge, we propose a supervised 
diffusion discovery framework that exploits the 
latent semantic information among users, topics, 
and their explicit / implicit interactions. Intuitively, 
four kinds of information are useful for prediction: 
? Topic information: Intuitively, knowing the 
signatures of a topic (e.g., is it about politics?) 
is critical to the success of the prediction. 
? User information: The information of a user 
such as the personality (e.g., whether this user 
is aggressive or passive) is generally useful. 
? User-topic interaction: Understanding the users' 
preference on certain topics can improve the 
quality of prediction. 
? Global information: We include some global 
features (e.g., topology info) of social network. 
Below we will describe how these four kinds of 
information can be modeled in our framework. 
2.2 Topic Information 
We extract hidden topic category information to 
model topic signature. In particular, we exploit the 
Latent Dirichlet Allocation (LDA) method (Blei et 
al., 2003), which is a widely used topic modeling 
technique, to decompose the topic-word matrix TW 
into hidden topic categories:  
                        TW = TH * HW 
, where TH is a topic-hidden matrix, HW is hidden-
word matrix, and h is the manually-chosen 
parameter to determine the size of hidden topic 
categories. TH indicates the distribution of each 
topic to hidden topic categories, and HW indicates 
the distribution of each lexical term to hidden topic 
categories. Note that TW and TH include both 
existing and novel topics.  We utilize THt,*, the row 
vector of the topic-hidden matrix TH for a topic t, 
as a feature set. In brief, we apply LDA to extract 
the topic-hidden vector THt,* to model topic 
signature (TG) for both existing and novel topics. 
Topic information can be further exploited. To 
predict whether a novel topic will be propagated 
through a link, we can first enumerate the existing 
topics that have been propagated through this link. 
For each such topic, we can calculate its similarity 
with the new topic based on the hidden vectors 
generated above (e.g., using cosine similarity 
between feature vectors). Then, we sum up the 
similarity values as a new feature: topic similarity 
(TS). For example, a link has previously 
propagated two topics for a total of three times 
{ACL, KDD, ACL}, and we would like to know 
whether a new topic, EMNLP, will propagate 
through this link. We can use the topic-hidden 
vector to generate the similarity values between 
EMNLP and the other topics (e.g., {0.6, 0.4, 0.6}), 
and then sum them up (1.6) as the value of TS. 
2.3 User Information 
Similar to topic information, we extract latent 
personal information to model user signature (the 
users are anonymized already). We apply LDA on 
the user-word matrix UW: 
UW = UM * MW 
, where UM is the user-hidden matrix, MW is the 
hidden-word matrix, and m is the manually-chosen 
size of hidden user categories. UM indicates the 
distribution of each user to the hidden user 
categories (e.g., age). We then use UMu,*, the row 
vector of UM for the user u, as a feature set. In 
brief, we apply LDA to extract the user-hidden 
vector UMu,* for both source and destination nodes 
of a link to model user signature (UG). 
345
2.4 User-Topic Interaction 
Modeling user-topic interaction turns out to be 
non-trivial. It is not useful to exploit latent 
semantic analysis directly on the user-topic matrix 
UR = UQ * QR , where UR represents how many 
times each user is diffused for existing topic R (R 
?
 T), because UR does not contain information of 
novel topics, and neither do UQ and QR. Given no 
propagation record about novel topics, we propose 
a method that allows us to still extract implicit 
user-topic information. First, we extract from the 
matrix TH (described in Section 2.2) a subset RH 
that contains only information about existing topics. 
Next we apply left division to derive another user-
hidden matrix UH: 
UH = (RH \ URT)T = ((RHT RH
 
)-1 RHT URT)T 
Using left division, we generate the UH matrix 
using existing topic information. Finally, we 
exploit UHu,*, the row vector of the user-hidden 
matrix UH for the user u, as a feature set. 
Note that novel topics were included in the 
process of learning the hidden topic categories on 
RH; therefore the features learned here do 
implicitly utilize some latent information of novel 
topics, which is not the case for UM. Experiments 
confirm the superiority of our approach. 
Furthermore, our approach ensures that the hidden 
categories in topic-hidden and user-hidden 
matrices are identical. Intuitively, our method 
directly models the user?s preference to topics? 
signature (e.g., how capable is this user to 
propagate topics in politics category?). In contrast, 
the UM mentioned in Section 2.3 represents the 
users? signature (e.g., aggressiveness) and has 
nothing to do with their opinions on a topic. In 
short, we obtain the user-hidden probability vector 
UHu,* as a feature set, which models user 
preferences to latent categories (UPLC). 
2.5 Global Features 
Given a candidate link, we can extract global 
social features such as in-degree (ID) and out-
degree (OD). We tried other features such as 
PageRank values but found them not useful. 
Moreover, we extract the number of distinct topics 
(NDT) for a link as a feature. The intuition behind 
this is that the more distinct topics a user has 
diffused to another, the more likely the diffusion 
will happen for novel topics. 
2.6 Complexity Analysis 
The complexity to produce each feature is as below: 
(1) Topic information: O(I * |T| * h * Bt) for LDA 
using Gibbs sampling, where I is # of the 
iterations in sampling, |T| is # of topics, and Bt 
is the average # of tokens in a topic. 
(2) User information: O(I * |V| * m * Bu) , where |V| is # of users, and Bu is the average # of 
tokens for a user. 
(3) User-topic interaction: the time complexity is 
O(h3 + h2 * |T| + h * |T| * |V|). 
(4) Global features: O(|D|), where |D| is # of 
diffusions. 
3 Experiments 
For evaluation, we try to use the diffusion records 
of old topics to predict whether a diffusion link 
exists between two nodes given a new topic.  
3.1 Dataset and Evaluation Metric 
We first identify 100 most popular topic (e.g., 
earthquake) from the Plurk micro-blog site 
between 01/2011 and 05/2011. Plurk is a popular 
micro-blog service in Asia with more than 5 
million users (Kuo et al, 2011). We manually 
separate the 100 topics into 7 groups. We use 
topic-wise 4-fold cross validation to evaluate our 
method, because there are only 100 available 
topics. For each group, we select 3/4 of the topics 
as training and 1/4 as validation. 
The positive diffusion records are generated 
based on the post-response behavior. That is, if a 
person x posts a message containing one of the 
selected topic t, and later there is a person y 
responding to this message, we consider a 
diffusion of t has occurred from x to y (i.e., (x, y, t) 
is a positive instance). Our dataset contains a total 
of 1,642,894 positive instances out of 100 distinct 
topics; the largest and smallest topic contains 
303,424 and 2,166 diffusions, respectively. Also, 
the same amount of negative instances for each 
topic (totally 1,642,894) is sampled for binary 
classification (similar to the setup in KDD Cup 
2011 Track 2). The negative links of a topic t are 
sampled randomly based on the absence of 
responses for that given topic. 
The underlying social network is created using 
the post-response behavior as well. We assume 
there is an acquaintance link between x and y if and 
346
only if x has responded to y (or vice versa) on at 
least one topic. Eventually we generated a social 
network of 163,034 nodes and 382,878 links. 
Furthermore, the sets of keywords for each topic 
are required to create the TW and UW matrices for 
latent topic analysis; we simply extract the content 
of posts and responses for each topic to create both 
matrices. We set the hidden category number h = m 
= 7, which is equal to the number of topic groups. 
We use area under ROC curve (AUC) to 
evaluate our proposed framework (Davis and 
Goadrich, 2006); we rank the testing instances 
based on their likelihood of being positive, and 
compare it with the ground truth to compute AUC. 
3.2 Implementation and Baseline 
After trying many classifiers and obtaining similar 
results for all of them, we report only results from 
LIBLINEAR with c=0.0001 (Fan et al, 2008) due 
to space limitation. We remove stop-words, use 
SCWS (Hightman, 2012) for tokenization, and  
MALLET (McCallum, 2002) and GibbsLDA++ 
(Phan and Nguyen, 2007) for LDA. 
There are three baseline models we compare the 
result with. First, we simply use the total number 
of existing diffusions among all topics between 
two nodes as the single feature for prediction. 
Second, we exploit the independent cascading 
model (Kempe et al, 2003), and utilize the 
normalized total number of diffusions as the 
propagation probability of each link. Third, we try 
the heat diffusion model (Ma et al, 2008), set 
initial heat proportional to out-degree, and tune the 
diffusion time parameter until the best results are 
obtained. Note that we did not compare with any 
data-driven approaches, as we have not identified 
one that can predict diffusion of novel topics.  
3.3 Results 
The result of each model is shown in Table 1. All 
except two features outperform the baseline. The 
best single feature is TS. Note that UPLC performs 
better than UG, which verifies our hypothesis that 
maintaining the same hidden features across 
different LDA models is better. We further conduct 
experiments to evaluate different combinations of 
features (Table 2), and found that the best one (TS 
+ ID + NDT) results in about 16% improvement 
over the baseline, and outperforms the combination 
of all features. As stated in (Witten et al, 2011), 
adding useless features may cause the performance 
of classifiers to deteriorate. Intuitively, TS captures 
both latent topic and historical diffusion 
information, while ID and NDT provide 
complementary social characteristics of users. 
 
Table 1: Single-feature results. 
 
Table 2: Feature combination results. 
4 Conclusions 
The main contributions of this paper are as below: 
1. We propose a novel task of predicting the 
diffusion of unseen topics, which has wide 
applications in real-world.  
2. Compared to the traditional model-driven or 
content-independent data-driven works on 
diffusion analysis, our solution demonstrates 
how one can bring together ideas from two 
different but promising areas, NLP and SNA, 
to solve a challenging problem. 
3. Promising experiment result (74% in AUC) 
not only demonstrates the usefulness of the 
proposed models, but also indicates that 
predicting diffusion of unseen topics without 
historical diffusion data is feasible. 
Acknowledgments 
This work was also supported by National Science 
Council, National Taiwan University and Intel 
Corporation under Grants NSC 100-2911-I-002-001, 
and 101R7501. 
Method Feature AUC
Baseline
Existing Diffusion 58.25%
Independent Cascade 51.53%
Heat Diffusion 56.08%
Learning
Topic Signature (TG) 50.80%
Topic Similarity (TS) 69.93%
User Signature (UG) 56.59%
User Preferences to
Latent Categories (UPLC) 61.33%
In-degree (ID) 65.55%
Out-degree (OD) 59.73%
Number of Distinct Topics (NDT) 55.42%
Method Feature AUC
Baseline Existing Diffusion 58.25%
Learning
ALL 65.06%
TS + UPLC + ID + NDT 67.67%
TS + UPLC + ID 64.80%
TS + UPLC + NDT 66.01%
TS + ID + NDT 73.95%
UPLC + ID + NDT 67.24%
347
References  
David M. Blei, Andrew Y. Ng & Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3.993-1022. 
Jesse Davis & Mark Goadrich. 2006. The relationship 
between Precision-Recall and ROC curves. 
Proceedings of the 23rd international conference on 
Machine learning, Pittsburgh, Pennsylvania. 
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang & Chih-Jen Lin. 2008. LIBLINEAR: A 
Library for Large Linear Classification. J. Mach. 
Learn. Res., 9.1871-74. 
Hongliang Fei, Ruoyi Jiang, Yuhao Yang, Bo Luo & 
Jun Huan. 2011. Content based social behavior 
prediction: a multi-task learning approach. 
Proceedings of the 20th ACM international 
conference on Information and knowledge 
management, Glasgow, Scotland, UK. 
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, 
Zoran Despotovic & Wolfgang Kellerer. 2010. 
Outtweeting the twitterers - predicting information 
cascades in microblogs. Proceedings of the 3rd 
conference on Online social networks, Boston, MA. 
Hightman. 2012. Simple Chinese Words Segmentation 
(SCWS). 
David Kempe, Jon Kleinberg & Eva Tardos. 2003. 
Maximizing the spread of influence through a social 
network. Proceedings of the ninth ACM SIGKDD 
international conference on Knowledge discovery 
and data mining, Washington, D.C. 
Tsung-Ting Kuo, San-Chuan Hung, Wei-Shih Lin, 
Shou-De Lin, Ting-Chun Peng & Chia-Chun Shih. 
2011. Assessing the Quality of Diffusion Models 
Using Real-World Social Network Data. Conference 
on Technologies and Applications of Artificial 
Intelligence, 2011. 
C.X. Lin, Q.Z. Mei, Y.L. Jiang, J.W. Han & S.X. Qi. 
2011. Inferring the Diffusion and Evolution of 
Topics in Social Communities. Proceedings of the 
IEEE International Conference on Data Mining, 
2011. 
Hao Ma, Haixuan Yang, Michael R. Lyu & Irwin King. 
2008. Mining social networks using heat diffusion 
processes for marketing candidates selection. 
Proceeding of the 17th ACM conference on 
Information and knowledge management, Napa 
Valley, California, USA. 
Andrew Kachites McCallum. 2002. MALLET: A 
Machine Learning for Language Toolkit. 
Sasa Petrovic, Miles Osborne & Victor Lavrenko. 2011. 
RT to Win! Predicting Message Propagation in 
Twitter. International AAAI Conference on Weblogs 
and Social Media, 2011. 
Xuan-Hieu Phan & Cam-Tu Nguyen. 2007. 
GibbsLDA++: A C/C++ implementation of latent 
Dirichlet alocation (LDA). 
Ian H. Witten, Eibe Frank & Mark A. Hall. 2011. Data 
Mining: Practical machine learning tools and 
techniques. San Francisco: Morgan Kaufmann 
Publishers Inc. 
Jiang Zhu, Fei Xiong, Dongzhen Piao, Yun Liu & Ying 
Zhang. 2011. Statistically Modeling the 
Effectiveness of Disaster Information in Social 
Media. Proceedings of the 2011 IEEE Global 
Humanitarian Technology Conference. 
 
 
348
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145?150,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Online Plagiarism Detection Through Exploiting Lexical, Syntactic, and 
Semantic Information 
 
Wan-Yu Lin Nanyun Peng Chun-Chao Yen Shou-de Lin 
Graduate Institute of 
Networking and 
Multimedia, National 
Taiwan University 
Institute of 
Computational 
Linguistic, Peking 
University 
Graduate Institute of 
Networking and 
Multimedia, National 
Taiwan University 
Graduate Institute of 
Networking and 
Multimedia, National 
Taiwan University 
r99944016@csie
.ntu.edu.tw 
pengnanyun@pku
.edu.cn 
r96944016@csie
.ntu.edu.tw 
sdlin@csie.ntu
.edu.tw 
 
Abstract 
In this paper, we introduce a framework that 
identifies online plagiarism by exploiting lexical, 
syntactic and semantic features that includes 
duplication-gram, reordering and alignment of 
words, POS and phrase tags, and semantic 
similarity of sentences. We establish an ensemble 
framework to combine the predictions of each 
model. Results demonstrate that our system can 
not only find considerable amount of real-world 
online plagiarism cases but also outperforms 
several state-of-the-art algorithms and commercial 
software. 
Keywords 
Plagiarism Detection, Lexical, Syntactic, Semantic 
1. Introduction 
Online plagiarism, the action of trying to create a 
new piece of writing by copying, reorganizing or 
rewriting others? work identified through search 
engines, is one of the most commonly seen 
misusage of the highly matured web technologies. 
As implied by the experiment conducted by 
(Braumoeller and Gaines, 2001), a powerful 
plagiarism detection system can effectively 
discourage people from plagiarizing others? work. 
A common strategy people adopt for online-
plagiarism detection is as follows. First they 
identify several suspicious sentences from the 
write-up and feed them one by one as a query to a 
search engine to obtain a set of documents. Then 
human reviewers can manually examine whether 
these documents are truly the sources of the 
suspicious sentences. While it is quite 
straightforward and effective, the limitation of this 
strategy is obvious. First, since the length of search 
query is limited, suspicious sentences are usually 
queried and examined independently. Therefore, it 
is harder to identify document level plagiarism 
than sentence level plagiarism. Second, manually 
checking whether a query sentence plagiarizes 
certain websites requires specific domain and 
language knowledge as well as considerable 
amount of energy and time. To overcome the 
above shortcomings, we introduce an online 
plagiarism detection system using natural language 
processing techniques to simulate the above 
reverse-engineering approach. We develop an 
ensemble framework that integrates lexical, 
syntactic and semantic features to achieve this goal. 
Our system is language independent and we have 
implemented both Chinese and English versions 
for evaluation. 
2. Related Work 
Plagiarism detection has been widely discussed in 
the past decades (Zou et al, 2010). Table 1. 
summarizes some of them: 
Author 
Comparison 
Unit 
Similarity Function 
Brin et al, 
1995 
Word + 
Sentence 
Percentage of matching 
sentences. 
White and 
Joy, 2004 
Sentence 
Average overlap ratio of 
the sentence pairs using 2 
pre-defined thresholds. 
Niezgoda 
and Way, 
2006 
A human 
defined 
sliding 
window 
Sliding windows ranked 
by the average length per 
word. 
Cedeno and 
Rosso,  2009 
Sentence + 
n-gram 
Overlap percentage of n-
gram in the sentence pairs. 
145
Pera and Ng, 
2010 
Sentence 
A pre-defined 
resemblance function 
based on word correlation 
factor. 
Stamatatos, 
2011 
Passage 
Overlap percentage of 
stopword n-grams. 
Grman and 
Ravas, 2011 
Passage 
Matching percentage of 
words with given 
thresholds on both ratio 
and absolute number of 
words in passage. 
Table 1. Summary of related works 
Comparing to those systems, our system exploits 
more sophisticated syntactic and semantic 
information to simulate what plagiarists are trying 
to do. 
There are several online or charged/free 
downloadable plagiarism detection systems such as 
Turnitin, EVE2, Docol? c, and CATPPDS which 
detect mainly verbatim copy. Others such as 
Microsoft Plagiarism Detector (MPD), Safeassign, 
Copyscape and VeriGuide, claim to be capable of 
detecting obfuscations. Unfortunately those 
commercial systems do not reveal the detail 
strategies used, therefore it is hard to judge and 
reproduce their results for comparison. 
3. Methodology 
 
Figure 1. Detection Flow 
The data flow is shown above in Figure 1.  
3.1 Query a Search Engine 
We first break down each article into a series of 
queries to query a search engine. Several systems 
such as (Liu at al., 2007) have proposed a similar 
idea. The main difference between our method and 
theirs is that we send unquoted queries rather than 
quoted ones. We do not require the search results 
to completely match to the query sentence. This 
strategy allows us to not only identify the 
copy/paste type of plagiarism but also re-write/edit 
type of plagiarism.  
3.2 Sentence-based Plagiarism Detection 
Since not all outputs of a search engine contain an 
exact copy of the query, we need a model to 
quantify how likely each of them is the source of 
plagiarism. For better efficiency, our experiment 
exploits the snippet of a search output to represent 
the whole document. That is, we want to measure 
how likely a snippet is the plagiarized source of the 
query. We designed several models which utilized 
rich lexical, syntactic and semantic features to 
pursue this goal, and the details are discussed 
below.  
3.2.1 Ngram Matching (NM) 
One straightforward measure is to exploit the n-
gram similarity between source and target texts. 
We first enumerate all n-grams in source, and then 
calculate the overlap percentage with the n-grams 
in the target. The larger n is, the harder for this 
feature to detect plagiarism with insertion, 
replacement, and deletion. In the experiment, we 
choose n=2. 
3.2.2 Reordering of Words (RW) 
Plagiarism can come from the reordering of words. 
We argue that the permutation distance between S1 
and S2 is an important indicator for reordered 
plagiarism. The permutation distance is defined as 
the minimum number of pair-wise exchanging of 
matched words needed to transform a sentence, S2, 
to contain the same order of matched words as 
another sentence, S1. As mentioned in (S?rensena 
and Sevaux, 2005), the permutation distance can 
be calculated by the following expression  
? ?1, ?2 =   ???
?
?=?+1
??1
?=1   
where  
??? =  
1, ?? ?1 ? > ?1 ? ??? ?2 ? < ?2 ? 
          0, ?????????                                                   
  
S1(i) and S2(i) are indices of the i
th matched 
word in sentences S1 and S2 respectively and n is 
the number of matched words between  the 
sentences S1 and S2. Let ? =  
n2? n
2
 be the 
normalized term, which is the maximum possible 
distance between S1 and S2, then the reordering 
146
score of the two sentences, expressed as s(S1, S2), 
will be s S1 , S2  = 1 ?  
d S1 ,S2 
?
 
3.2.3 Alignment of Words (AW) 
Besides reordering, plagiarists often insert or 
delete words in a sentence. We try to model such 
behavior by finding the alignment of two word 
sequences. We perform the alignment using a 
dynamic programming method as mentioned in 
(Wagner and Fischer, 1975).  
However, such alignment score does not reflect 
the continuity of the matched words, which can be 
an important cue to identify plagiarism. To 
overcome such drawback, we modify the score as 
below. 
New Alignment Score =
 ??
|? |?1
?=1
|?|?1
 
where    ?? =
1
# ??  ?????  ???????  ?? ,??+1 +1
 
M is the list of matched words, and Mi is the i
th 
matched word in M. This implies we prefer fewer 
unmatched words in between two matched ones. 
3.2.4 POS and Phrase Tags of Words (PT, PP) 
Exploiting only lexical features can sometimes 
result in some false positive cases because two sets 
of matched words can play different roles in the 
sentences. See S1 and S2 in Table 2. as a possible 
false positive case. 
S1: The man likes the woman 
S2: The woman is like the man 
Word S1: Tag S2: Tag S1: Phrase S2: Phrase 
man NN NN NP PP 
like VBZ IN VP PP 
woman NN NN VP NP 
Table 2. An example of matched words with different 
tags and phrases 
Therefore, we further explore syntactic features 
for plagiarism detection. To achieve this goal, we 
utilize a parser to obtain POS and phrase tags of 
the words. Then we design an equation to measure 
the tag/phrase similarity. 
Sim = 
???  ???? ???  ?????  ??? ?  ?????????  ???  
???  ???? ???  ?????  
 
We paid special attention to the case that 
transforms a sentence from an active form to a 
passive-form or vice versa. A subject originally in 
a Noun Phrase can become a Preposition Phrase, 
i.e. ?by ??, in the passive form while the object in 
a Verb Phrase can become a new subject in a Noun 
Phrase. Here we utilize the Stanford Dependency 
provided by Stanford Parser to match the 
tag/phrase between active and passive sentences.  
3.2.5 Semantic Similarity (LDA) 
Plagiarists, sometimes, change words or phrases to 
those with similar meanings. While previous works 
(Y. Lin et al, 2006) often explore semantic 
similarity using lexical databases such as WordNet 
to find synonyms, we exploit a topic model, 
specifically latent Dirichlet alocation (LDA, D. M. 
Blei et al, 2003), to extract the semantic features 
of sentences. Given a set of documents represented 
by their word sequences, and a topic number n, 
LDA learns the word distribution for each topic 
and the topic distribution for each document which 
maximize the likelihood of the word co-occurrence 
in a document. The topic distribution is often taken 
as semantics of a document. We use LDA to obtain 
the topic distribution of a query and a candidate 
snippet, and compare the cosine similarity of them 
as a measure of their semantic similarity. 
3.3 Ensemble Similarity Scores 
Up to this point, for each snippet the system 
generates six similarity scores to measure the 
degree of plagiarism in different aspects. In this 
stage, we propose two strategies to linearly 
combine the scores to make better prediction. The 
first strategy utilizes each model?s predictability 
(e.g. accuracy) as the weight to linearly combine 
the scores. In other words, the models that perform 
better individually will obtain higher weights. In 
the second strategy we exploit a learning model (in 
the experiment section we use Liblinear) to learn 
the weights directly.  
3.4 Document Level Plagiarism Detection 
For each query from the input article, our system 
assigns a degree-of-plagiarism score to some 
plausible source URLs. Then, for each URL, the 
system sums up all the scores it obtains as the final 
score for document-level degree-of-plagiarism. We 
set up a cutoff threshold to obtain the most 
plausible URLs. At the end, our system highlights 
the suspicious areas of plagiarism for display. 
147
4. Evaluation 
We evaluate our system from two different angles. 
We first evalaute the sentence level plagirism 
detection using the PAN corpus in English. We 
then evaluate the capability of the full system to 
detect on-line plagiarism cases using annotated 
results in Chinese. 
4.1 Sentence-based Evaluations 
We want to compare our model with the state-of-
the-art methods, in particular the winning entries in 
plagiarism detection competition in PAN 1 . 
However, the competition in PAN is designed for 
off-line plagiarism detection; the entries did not 
exploit an IR system to search the Web like we do. 
Nevertheless, we can still compare the core 
component of our system, the sentence-based 
measuring model with that of other systems. To 
achieve such goal, we first randomly sampled 370 
documents from PAN-2011 external plagiarism 
corpus (M. Potthast et al, 2010) containing 2882 
labeled plagiarism cases.  
 To obtain high-quality negative examples for 
evaluation, we built a full-text index on the corpus 
using Lucene package. Then we use the suspicious 
passages as queries to search the whole dataset 
using Lucene. Since there is length limitation in 
Lucene (as well as in the real search engines), we 
further break the 2882 plagiarism cases into 6477 
queries. We then extract the top 30 snippets 
returned by the search engine as the potential 
negative candidates for each plagiarism case. Note 
that for each suspicious passage, there is only one 
target passage (given by the ground truth) that is 
considered as a positive plagiarism case in this data, 
and it can be either among these 30 cases or not. 
However, we union these 30 cases with the ground 
truth as a set, and use our (as well as the 
competitors?) models to rank the degree-of-
plagiarism for all the candidates. We then evaluate 
the rank by the area-under-PR-curve (AUC) score. 
We compared our system with the winning entry of 
PAN 2011 (Grman and Ravas, 2011) and the 
stopword ngram model that claims to perform 
better than this winning entry by Stamatatos (2011). 
The results of each individual model and ensemble 
using 5-fold cross validation are listed in Table 3. 
It shows that NM is the best individual model, and 
                                                                
1 The website of PAN-2011 is http://pan.webis.de/ 
an ensemble of three features outperforms the 
state-of-the-art by 26%. 
NM RW AW PT PP LDA 
0.876 0.596 0.537 0.551 0.521 0.596 
                                     (a) 
 Ours ensemble 
Pan-11 
Champion 
Stopword 
Ngram 
AUC 
0.882 
(NM+RW+PP) 
0.620 0.596 
                                     (b) 
Table 3. (a) AUC for each individual model (b) AUC of 
our ensemble and other state-of-the-art algorithms 
4.2 Evaluating the Full System 
To evaluate the overall system, we manually 
collect 60 real-world review articles from the 
Internet for books (20), movies (20), and music 
albums (20). Unfortunately for an online system 
like ours, there is no ground truth available for 
recall measure. We conduct two differement 
evalautions. First we use the 60 articles as inputs to 
our system, ask 5 human annotators to check 
whether the articles returned by our system can be 
considered as plagiarism. Among all 60 review 
articles, our system identifies a considerablely high 
number of copy/paste articles, 231 in total. 
However, identifying this type of plagiarism is 
trivial, and has been done by many similar tools. 
Instead we focus on the so-called smart-plagiarism 
which cannot be found through quoting a query in 
a search engine. Table 4. shows the precision of 
the smart-plagiarism articles returned by our 
system. The precision is very high and outperforms 
a commertial tool Microsoft Plagiarism Detector. 
 Book Movie Music 
Ours 
280/288 
(97%) 
88/110 
(80%) 
979/1033 
(95%) 
MPD 
44/53 
(83%) 
123/172 
(72%) 
120/161 
(75%) 
Table 4. Precision of Smart Plagiarism 
In the second evaluation, we first choose 30 
reviews randomly. Then we use each of them as 
queries into Google and retrieve a total of 5636 
pieces of snippet candidates. We then ask 63 
human beings to annotate whether those snippets 
represent plagiarism cases of the original review 
article. Eventually we have obtained an annotated 
148
dataset and found a total of 502 plagiarized 
candidates with 4966 innocent ones for evalaution. 
Table 5. shows the average AUC of 5-fold cross 
validation. The results show that our method 
outperforms the Pan-11 winner slightly, and much 
better than the Stopword Ngram.  
NM RW AW PT PP LDA 
0.904 0.778 0.874 0.734 0.622 0.581 
(a) 
 Ours ensemble 
Pan-11 
Champion 
Stopword 
Ngram 
AUC 
0.919 
(NM+RW+AW
+PT+PP+LDA) 
0.893 0.568 
(b) 
Table 5. (a) AUC for each individual model (b) AUC of 
our ensemble and other state-of-the-art algorithms 
4.3 Discussion 
There is some inconsistency of the performance of 
single features in these two experiments. The main 
reason we believe is that the plagiarism cases were 
created in very different manners. Plagiarism cases 
in PAN external source are created artificially 
through word insertions, deletions, reordering and 
synonym substitutions. As a result, features such as 
word alignment and reordering do not perform 
well because they did not consider the existence of 
synonym word replacement. On the other hand, 
real-world plagiarism cases returned by Google are 
those with matching-words, and we can find better 
performance for AW. 
The performances of syntactic and semantic 
features, namely PT, PP and LDA, are consistently 
inferior than other features. It is because they often 
introduce false-positives as there are some non-
plagiarism cases that might have highly overlapped 
syntactic or semantic tags. Nevertheless, 
experiments also show that these features can 
improve the overall accuracy in ensemble. 
We also found that the stopword Ngram model 
is not applicable universally. For one thing, it is 
less suitable for on-line plagiarism detection, as the 
length limitation for queries diminishes the 
usability of stopword n-grams. For another, 
Chinese seems to be a language that does not rely 
as much on stopwords as the latin languages do to 
maintain its syntax structure. 
Samples of our system?s finding can be found 
here, http://tinyurl.com/6pnhurz 
5. Online Demo System 
We developed an online demos system using 
JAVA (JDK 1.7). The system currently supports 
the detection of documents in both English and 
Chinese. Users can either upload the plain text file 
of a suspicious document, or copy/paste the 
content onto the text area, as shown below in 
Figure 2.  
 
Figure 2. Input Screen-Shot 
Then the system will output some URLs and 
snippets as the potential source of plagiarism. (see 
Figure 3.) 
Figure 3. Output Screen-Shot 
6. Conclusion 
Comparing with other online plagiarism 
detection systems, ours exploit more sophisticated 
features by modeling how human beings plagiarize 
online sources. We have exploited sentence-level 
plagiarism detection on lexical, syntactic and 
semantic levels. Another noticeable fact is that our 
approach is almost language independent. Given a 
parser and a POS tagger of a language, our 
framework can be extended to support plagiarism 
detection for that language.  
149
7. References 
Salha Alzahrani, Naomie Salim, and Ajith Abraham, 
?Understanding Plagiarism Linguistic Patterns, 
Textual Features and Detection Methods ? in IEEE 
Transactions on systems , man and cyberneticsPart C: 
Applications and reviews, 2011 
D. M.  Blei,  A.  Y. Ng, M. I. Jordan,  and J. Lafferty. 
Latent dirichlet alocation. Journal of Machine 
Learning Research, 3:2003, 2003. 
Bear F. Braumoeller and Brian J. Gaines. 2001. Actions 
Do Speak Louder Than Words: Deterring Plagiarism 
with the Use of Plagiarism-Detection Software. In 
Political Science & Politics, 34(4):835-839. 
Sergey Brin, James Davis, and Hector Garcia-molina. 
1995. Copy Detection Mechanisms for Digital 
Documents. In Proceedings of the ACM SIGMOD 
Annual Conference, 24(2):398-409.  
Alberto Barr?n Cede?o and Paolo Rosso. 2009. On 
Automatic Plagiarism Detection based on n-grams 
Comparison. In Proceedings of the 31th European 
Conference on IR Research on Advances in 
Information Retrieval, ECIR 2009, LNCS 5478:696-
700, Springer-Verlag, and Berlin Heidelberg,  
Jan Grman and Rudolf Ravas. 2011. Improved 
implementation for finding text similarities in large 
collections of data.In Proceedings of PAN 2011. 
NamOh Kang, Alexander Gelbukh, and SangYong Han. 
2006. PPChecker: Plagiarism Pattern Checker in 
Document Copy Detection. In Proceedings of TSD-
2006, LNCS, 4188:661-667. 
Yuhua Li, David McLean, Zuhair A. Bandar, James D. 
O? Shea, and Keeley Crockett. 2006. Sentence 
Similarity Based on Semantic Nets and Corpus 
Statistics. In Proceedings of the IEEE Transactions 
on Knowledge and Data Engineering, 18(8):1138-
1150. 
Yi-Ting Liu, Heng-Rui Zhang, Tai-Wei Chen, and Wei-
Guang Teng. 2007. Extending Web Search for 
Online Plagiarism Detection. In Proceedings of the 
IEEE International Conference on Information Reuse 
and Integration, IRI 2007. 
Caroline Lyon, Ruth Barrett, and James Malcolm. 2004. 
A Theoretical Basis to the Automated Detection of 
Copying Between Texts, and its Practical 
Implementation in the Ferret Plagiarism and 
Collusion Detector. In Proceedings of Plagiarism: 
Prevention, Practice and Policies 2004 Conference. 
Sebastian Niezgoda and Thomas P. Way. 2006. 
SNITCH: A Software Tool for Detecting Cut and 
Paste Plagiarism. In Proceedings of the 37th SIGCSE 
Technical Symposium on Computer Science 
Education, p.51-55. 
Maria Soledad Pera and Yiu-kai Ng. 2010. IOS Press 
SimPaD: A Word-Similarity Sentence-Based 
Plagiarism Detection Tool on Web Documents. In 
Journal on Web Intelligence and Agent Systems, 9(1). 
Xuan-Hieu Phan and Cam-Tu Nguyen. GibbsLDA++: 
A C/C++ implementation of latent Dirichlet 
allocation (LDA), 2007  
Martin Potthast, Benno Stein, Alberto Barr?n Cede?o, 
and Paolo Rosso. An Evaluation Framework for 
Plagiarism Detection. In 23rd International 
Conference on Computational Linguistics (COLING 
10), August 2010. Association for Computational 
Linguistics. 
Kenneth S?rensena and Marc Sevaux. 2005. 
Permutation Distance Measures for Memetic 
Algorithms with Population Management. In 
Proceedings of 6th Metaheuristics International 
Conference. 
Efstathios Stamatatos, "Plagiarism Detection Based on 
Structural Information" in Proceedings of the 20th 
ACM international conference on Information and 
knowledge management, CIKM'11 
Robert A. Wagner and Michael J. Fischer. 1975. The 
String-to-string correction problem. In Journal of the 
ACM, 21(1):168-173. 
Daniel R. White and Mike S. Joy. 2004. Sentence-Based 
Natural Language Plagiarism Detection. In Journal 
on Educational Resources in Computing JERIC 
Homepage archive, 4(4).  
Du Zou, Wei-jiang Long, and Zhang Ling. 2010. A 
Cluster-Based Plagiarism Detection Method. In Lab 
Report for PAN at CLEF 2010. 
 
 
 
 
 
 
 
 
 
 
150
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625?630,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Stochastic Contextual Edit Distance and Probabilistic FSTs
Ryan Cotterell and Nanyun Peng and Jason Eisner
Department of Computer Science, Johns Hopkins University
{ryan.cotterell,npeng1,jason}@cs.jhu.edu
Abstract
String similarity is most often measured
by weighted or unweighted edit distance
d(x, y). Ristad and Yianilos (1998) de-
fined stochastic edit distance?a probabil-
ity distribution p(y | x) whose parame-
ters can be trained from data. We general-
ize this so that the probability of choosing
each edit operation can depend on contex-
tual features. We show how to construct
and train a probabilistic finite-state trans-
ducer that computes our stochastic con-
textual edit distance. To illustrate the im-
provement from conditioning on context,
we model typos found in social media text.
1 Introduction
Many problems in natural language processing
can be viewed as stochastically mapping one
string to another: e.g., transliteration, pronuncia-
tion modeling, phonology, morphology, spelling
correction, and text normalization. Ristad and
Yianilos (1998) describe how to train the param-
eters of a stochastic editing process that moves
through the input string x from left to right, trans-
forming it into the output string y. In this paper we
generalize this process so that the edit probabilities
are conditioned on input and output context.
We further show how to model the conditional
distribution p(y | x) as a probabilistic finite-state
transducer (PFST), which can be easily combined
with other transducers or grammars for particu-
lar applications. We contrast our probabilistic
transducers with the more general framework of
weighted finite-state transducers (WFST), explain-
ing why our restriction provides computational ad-
vantages when reasoning about unknown strings.
Constructing the finite-state transducer is tricky,
so we give the explicit construction for use by oth-
ers. We describe how to train its parameters when
the contextual edit probabilities are given by a log-
linear model. We provide a library for training
both PFSTs and WFSTs that works with OpenFST
(Allauzen et al, 2007), and we illustrate its use
with simple experiments on typos, which demon-
strate the benefit of context.
2 Stochastic Contextual Edit Distance
Our goal is to define a family of probability distri-
butions p
?
(y | x), where x ? ?
?
x
and y ? ?
?
y
are
input and output strings over finite alphabets ?
x
and ?
y
, and ? is a parameter vector.
Let x
i
denote the i
th
character of x. If i < 1 or
i > |x|, then x
i
is the distinguished symbol BOS
or EOS (?beginning/end of string?). Let x
i:j
denote
the (j ? i)-character substring x
i+1
x
i+2
? ? ?x
j
.
Consider a stochastic edit process that reads in-
put string x while writing output string y. Having
read the prefix x
0:i
and written the prefix y
0:j
, the
process must stochastically choose one of the fol-
lowing 2|?
y
|+ 1 edit operations:
? DELETE: Read x
i+1
but write nothing.
? INSERT(t) for some t ? ?
y
: Write y
j+1
= t
without reading anything.
? SUBST(t) for some t ? ?
y
: Read x
i+1
and
write y
j+1
= t. Note that the traditional
COPY operation is obtained as SUBST(x
i+1
).
In the special case where x
i+1
= EOS, the choices
are instead INSERT(t) and HALT (where the latter
may be viewed as copying the EOS symbol).
The probability of each edit operation depends
on ? and is conditioned on the left input context
C
1
= x
(i?N
1
):i
, the right input context C
2
=
x
i:(i+N
2
)
, and the left output context C
3
=
y
(j?N
3
):j
, where the constants N
1
, N
2
, N
3
? 0
specify the model?s context window sizes.
1
Note
that the probability cannot be conditioned on right
output context because those characters have not
yet been chosen. Ordinary stochastic edit dis-
tance (Ristad and Yianilos, 1998) is simply the
case (N
1
, N
2
, N
3
) = (0, 1, 0), while Bouchard-
C?ot?e et al (2007) used roughly (1, 2, 0).
Now p
?
(y | x) is the probability that this pro-
cess will write y as it reads a given x. This is the
total probability (given x) of all latent edit oper-
ation sequences that write y. In general there are
exponentially many such sequences, each imply-
ing a different alignment of y to x.
1
IfN
2
= 0, so that we do not condition on x
i+1
, we must
still condition on whether x
i+1
= EOS (a single bit). We
gloss over special handling for N
2
= 0; but it is in our code.
625
This model is reminiscent of conditional mod-
els in MT that perform stepwise generation of one
string or structure from another?e.g., string align-
ment models with contextual features (Cherry and
Lin, 2003; Liu et al, 2005; Dyer et al, 2013), or
tree transducers (Knight and Graehl, 2005).
3 Probabilistic FSTs
We will construct a probabilistic finite-state
transducer (PFST) that compactly models p
?
(y |
x) for all (x, y) pairs.
2
Then various computa-
tions with this distribution can be reduced to stan-
dard finite-state computations that efficiently em-
ploy dynamic programming over the structure of
the PFST, and the PFST can be easily combined
with other finite-state distributions and functions
(Mohri, 1997; Eisner, 2001).
A PFST is a two-tape generalization of the well-
known nondeterministic finite-state acceptor. It
is a finite directed multigraph where each arc is
labeled with an input in ?
x
? {}, an output in
?
y
?{}, and a probability in [0, 1]. ( is the empty
string.) Each state (i.e., vertex) has a halt proba-
bility in [0, 1], and there is a single initial state q
I
.
Each path from q
I
to a final state q
F
has
? an input string x, given by the concatenation
of its arcs? input labels;
? an output string y, given similarly;
? a probability, given by the product of its arcs?
probabilities and the halt probability of q
F
.
We define p(y | x) as the total probability of all
paths having input x and output y. In our applica-
tion, a PFST path corresponds to an edit sequence
that reads x and writes y. The path?s probability is
the probability of that edit sequence given x.
We must take care to ensure that for any x ? ?
?
x
,
the total probability of all paths accepting x is 1,
so that p
?
(y | x) is truly a conditional probability
distribution. This is guaranteed by the following
sufficient conditions (we omit the proof for space),
which do not seem to appear in previous literature:
? For each state q and each symbol b ? ?
x
, the
arcs from q with input label b or  must have
total probability of 1. (These are the available
choices if the next input character is x.)
2
Several authors have given recipes for finite-state trans-
ducers that perform a single contextual edit operation (Kaplan
and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van
Noord, 1999). Such ?rewrite rules? can be individually more
expressive than our simple edit operations of section 2; but it
is unclear how to train a cascade of them to model p(y | x).
? For each state q, the halt action and the arcs
from q with input label  must have total
probability of 1. (These are the available
choices if there is no next input character.)
? Every state q must be co-accessible, i.e., there
must be a path of probability > 0 from q to
some q
F
. (Otherwise, the PFST could lose
some probability mass to infinite paths. The
canonical case of this involves an loop q ? q
with input label  and probability 1.)
We take the first two conditions to be part of the
definition of a PFST. The final condition requires
our PFST to be ?tight? in the same sense as a
PCFG (Chi and Geman, 1998), although the tight-
ness conditions for a PCFG are more complex.
In section 7, we discuss the costs and benefits of
PFSTs relative to other options.
4 The Contextual Edit PFST
We now define a PFST topology that concisely
captures the contextual edit process of section 2.
We are given the alphabets ?
x
,?
y
and the context
window sizes N
1
, N
2
, N
3
? 0.
For each possible context triple C =
(C
1
, C
2
, C
3
) as defined in section 2, we construct
an edit state q
C
whose outgoing arcs correspond
to the possible edit operations in that context.
One might expect that the SUBST(t) edit oper-
ation that reads s = x
i+1
and writes t = y
j+1
would correspond to an arc with s, t as its input
and output labels. However, we give a more effi-
cient design where in the course of reaching q
C
,
the PFST has already read s and indeed the en-
tire right input context C
2
= x
i:(i+N
2
)
. So our
PFST?s input and output actions are ?out of sync?:
its read head is N
2
characters ahead of its write
head. When the edit process of section 2 has read
x
0:i
and written y
0:j
, our PFST implementation
will actually have read x
0:(i+N
2
)
and written y
0:j
.
This design eliminates the need for nondeter-
ministic guessing (of the right context x
i:(i+N
2
)
) to
determine the edit probability. The PFST?s state is
fully determined by the characters that it has read
and written so far. This makes left-to-right com-
position in section 5 efficient.
A fragment of our construction is illustrated in
Figure 1. An edit state q
C
has the following out-
going edit arcs, each of which corresponds to an
edit operation that replaces some s ? ?
x
? {}
with some t ? ?
y
? {}:
626
b     c
y     _
a     bc
z       _
a      b
x      _
a     ba
x       _
?
:
z
 
/
?
:
?
 
/
 
?
:
y
 
/
?:x / 
i
n
s
e
r
t
 
z
insert x
s
u
b
s
t
i
t
u
t
e
 
y
 
f
o
r
 
b
d
e
l
e
t
e
 
b
b      c
x      _   
r
e
a
d
 
c
c
:
?
 
/
 
1
      p(INSERT(x) | (a,bc,x) )
 
 
 
 
 
 
 
p
(
I
N
S
E
R
T
(
z
)
 
|
 
(
a
,
b
c
,
x
)
 
)
p
(
S
U
B
S
T
(
y
)
 
|
 
(
a
,
b
c
,
x
)
 
)
 
 
 
 
 
p
(
D
E
L
E
T
E
(
b
)
 
|
 
(
a
,
b
c
,
x
)
 
)
      c
Figure 1: A fragment of a PFST withN
1
= 1, N
2
= 2, N
3
=
1. Edit states are shaded. A state q
C
is drawn with left and
right input contexts C
1
, C
2
in the left and right upper quad-
rants, and left output context C
3
in the left lower quadrant.
Each arc is labeled with input:output / probability.
? A single arc with probability p(DELETE | C)
(here s = (C
2
)
1
, t = )
? For each t ? ?
y
, an arc with probability
p(INSERT(t) | C) (here s = )
? For each t ? ?
y
, an arc with probability
p(SUBST(t) | C) (here s = (C
2
)
1
)
Each edit arc is labeled with input  (because s
has already been read) and output t. The arc leads
from q
C
to q
C
?
, a state that moves s and t into
the left contexts: C
?
1
= suffix(C
1
s,N
1
), C
?
2
=
suffix(C
2
, N
2
? |s|), C
?
3
= suffix(C
3
t,N
3
).
Section 2 mentions that the end of x requires
special handling. An edit state q
C
whose C
2
=
EOS
N
2
only has outgoing INSERT(t) arcs, and has
a halt probability of p(HALT | C). The halt proba-
bility at all other states is 0.
We must also build some non-edit states of the
form q
C
where |C
2
| < N
2
. Such a state does not
have the full N
2
characters of lookahead that are
needed to determine the conditional probability of
an edit. Its outgoing arcs deterministically read
a new character into the right input context. For
each s ? ?
x
, we have an arc of probability 1 from
q
C
to q
C
?
where C
?
= (C
1
, C
2
s, C
3
), labeled with
input s and output . Following such arcs from q
C
will reach an edit state after N
2
? |C
2
| steps.
The initial state q
I
with I = (BOS
N
1
, , BOS
N
3
)
is a non-edit state. Other non-edit states are con-
structed only when they are reachable from an-
other state. In particular, a DELETE or SUBST arc
always transitions to a non-edit state, since it con-
sumes one of the lookahead characters.
5 Computational Complexity
We summarize some useful facts without proof.
For fixed alphabets ?
x
and ?
y
, our final
PFST, T , has O(|?
x
|
N
1
+N
2
|?
y
|
N
3
) states and
O(|?
x
|
N
1
+N
2
|?
y
|
N
3
+1
) arcs. Composing this T
with deterministic FSAs takes time linear in the
size of the result, using a left-to-right, on-the-fly
implementation of the composition operator ?.
Given strings x and y, we can compute p
?
(y |
x) as the total probability of all paths in x ? T ? y.
This acyclic weighted FST has O(|x| ? |y|) states
and arcs. It takes onlyO(|x| ? |y|) time to construct
it and sum up its paths by dynamic programming,
just as in other edit distance algorithms.
Given only x, taking the output language of
x ? T yields the full distribution p
?
(y | x)
as a cyclic PFSA with O(|x| ? ?
N
3
y
) states and
O(|x| ? ?
N
3
+1
y
) arcs. Finding its most probable
path (i.e., most probable aligned y) takes time
O(|arcs| log |states|), while computing every arc?s
expected number of traversals under p(y | x) takes
time O(|arcs| ? |states|).
3
p
?
(y | x) may be used as a noisy channel
model. Given a language model p(x) repre-
sented as a PFSA X , X ? T gives p(x, y) for all
x, y. In the case of an n-gram language model
with n ? N
1
+ N
2
, this composition is effi-
cient: it merely reweights the arcs of T . We
use Bayes? Theorem to reconstruct x from ob-
served y: X ? T ? y gives p(x, y) (proportional
to p(x | y)) for each x. This weighted FSA has
O(?
N
1
+N
2
x
? |y|) states and arcs.
6 Parameterization and Training
While the parameters ? could be trained via var-
ious objective functions, it is particularly effi-
cient to compute the gradient of conditional log-
likelihood,
?
k
log p
?
(y
k
| x
k
), given a sample
of pairs (x
k
, y
k
). This is a non-convex objective
function because of the latent x-to-y alignments:
we do not observe which path transduced x
k
to y
k
.
Recall from section 5 that these possible paths are
represented by the small weighted FSA x
k
?T ?y
k
.
Now, a path?s probability is defined by multiply-
ing the contextual probabilities of edit operations
e. As suggested by Berg-Kirkpatrick et al (2010),
we model these steps using a conditional log-
linear model, p
?
(e | C)
def
=
1
Z
C
exp
(
? ?
~
f(C, e)
)
.
3
Speedups: In both runtimes, a factor of |x| can be elimi-
nated from |states| by first decomposing x ?T into its O(|x|)
strongly connected components. And the |states| factor in the
second runtime is unnecessary in practice, as just the first few
iterations of conjugate gradient are enough to achieve good
approximate convergence when solving the sparse linear sys-
tem that defines the forward probabilities in the cyclic PFSA.
627
To increase log p
?
(y
k
| x
k
), we must raise the
probability of the edits e that were used to trans-
duce x
k
to y
k
, relative to competing edits from the
same contexts C. This means raising ? ? f(C, e)
and/or lowering Z
C
. Thus, log p
?
(y
k
| x
k
) de-
pends only on the probabilities of edit arcs in T
that appear in x
k
? T ? y
k
, and the competing edit
arcs from the same edit states q
C
.
The gradient?
?
log p
?
(y
k
| x
k
) takes the form
?
C,e
c(C, e)
[
~
f(C, e)?
?
e
?
p
?
(e
?
| C)
~
f(C, e
?
)
]
where c(C, e) is the expected number of times that
e was chosen in context C given (x
k
, y
k
). (That
can be found by the forward-backward algorithm
on x
k
?T ? y
k
.) So the gradient adds up the differ-
ences between observed and expected feature vec-
tors at contexts C, where contexts are weighted by
how many times they were likely encountered.
In practice, it is efficient to hold the counts
c(C, e) constant over several gradient steps, since
this amortizes the work of computing them. This
can be viewed as a generalized EM algorithm that
imputes the hidden paths (giving c) at the ?E? step
and improves their probability at the ?M? step.
Algorithm 1 provides the training pseudocode.
Algorithm 1 Training a PFST T
?
by EM.
1: while not converged do
2: reset al counts to 0 . begin the ?E step?
3: for k ? 1 to K do . loop over training data
4: M = x
k
? T
?
? y
k
. small acyclic WFST
5: ~? = FORWARD-ALGORITHM(M )
6:
~
? = BACKWARD-ALGORITHM(M )
7: for arc A ?M , from state q ? q
?
do
8: if A was derived from an arc in T
?
representing edit e, from edit state q
C
, then
9: c(C, e) += ?
q
? prob(A) ? ?
q
?
/?
q
I
10: ? ? L-BFGS(?, EVAL, max iters=5) . the ?M step?
11: function EVAL(?) . objective function & its gradient
12: F ? 0;?F ? 0
13: for context C such that (?e)c(C, e) > 0 do
14: count? 0; expected? 0; Z
C
? 0
15: for possible edits e in context C do
16: F += c(C, e) ? (? ?
~
f(C, e))
17: ?F += c(C, e) ?
~
f(C, e)
18: count += c(C, e)
19: expected += exp(? ?
~
f(C, e)) ?
~
f(C, e)
20: Z
C
+= exp(? ?
~
f(C, e))
21: F -= count ? logZ
C
;?F -= count ?expected/Z
C
22: return (F,?F )
7 PFSTs versus WFSTs
Our PFST model of p(y | x) enforces a normal-
ized probability distribution at each state. Drop-
ping this requirement gives a weighted FST
(WFST), whose path weightsw(x, y) can be glob-
ally normalized (divided by a constant Z
x
) to ob-
tain probabilities p(y | x). WFST models of con-
textual edits were studied by Dreyer et al (2008).
PFSTs and WFSTs are respectively related to
MEMMs (McCallum et al, 2000) and CRFs (Laf-
ferty et al, 2001). They gain added power from
hidden states and  transitions (although to permit
a finite-state encoding, they condition on x in a
more restricted way than MEMMs and CRFs).
WFSTs are likely to beat PFSTs as linguistic
models,
4
just as CRFs beat MEMMs (Klein and
Manning, 2002). A WFST?s advantage is that the
probability of an edit can be indirectly affected by
the weights of other edits at a distance. Also, one
could construct WFSTs where an edit?s weight di-
rectly considers local right output context C
4
.
So why are we interested in PFSTs? Because
they do not require computing a separate normal-
izing contant Z
x
for every x. This makes it com-
putationally tractable to use them in settings where
x is uncertain because it is unobserved, partially
observed (e.g., lacks syllable boundaries), or nois-
ily observed. E.g., at the end of section 5, X rep-
resented an uncertain x. So unlike WFSTs, PFSTs
are usable as the conditional distributions in noisy
channel models, channel cascades, and Bayesian
networks. In future we plan to measure their mod-
eling disadvantage and attempt to mitigate it.
PFSTs are also more efficient to train under con-
ditional likelihood. It is faster to compute the gra-
dient (and fewer steps seem to be required in prac-
tice), since we only have to raise the probabilities
of arcs in x
k
? T ? y
k
relative to competing arcs
in x
k
? T . We visit at most |x
k
| ? |y
k
| ? |?
y
| arcs.
By contrast, training a WFST must raise the prob-
ability of the paths in x
k
? T ? y
k
relative to the
infinitely many competing paths in x
k
? T . This
requires summing around cycles in x
k
?T , and re-
quires visiting all of its |x
k
| ? |?
y
|
N
3
+1
arcs.
8 Experiments
To demonstrate the utility of contextual edit trans-
ducers, we examine spelling errors in social me-
dia data. Models of spelling errors are useful in
a variety of settings including spelling correction
itself and phylogenetic models of string variation
4
WFSTs can also use a simpler topology (Dreyer et al,
2008) while retaining determinism, since edits can be scored
?in retrospect? after they have passed into the left context.
628
-8
-7
-6
-5
-4
2000 4000 6000
# Training Examples
Me
an L
og-
Lik
elih
ood
Backoff
FALSE
TRUE
Topology
T010
T020
T110
T111
2
3
4
5
6
2000 4000 6000
# Training Examples
Me
an E
xpe
cted
 Ed
it D
ista
nce
Backoff
FALSE
TRUE
Topology
T010
T020
T110
T111
Figure 2: (a) Mean log p(y | x) for held-out test examples. (b) Mean expected edit distance (similarly).
(Mays et al, 1991; Church and Gale, 1991; Ku-
kich, 1992; Andrews et al, 2014).
To eliminate experimental confounds, we use
no dictionary or language model as one would in
practice, but directly evaluate our ability to model
p(correct | misspelled). Consider (x
k
, y
k
) =
(feeel, feel). Our model defines p(y | x
k
) for all y.
Our training objective (section 6) tries to make this
large for y = y
k
. A contextual edit model learns
here that e 7?  is more likely in the context of ee.
We report on test data how much probability
mass lands on the true y
k
. We also report how
much mass lands ?near? y
k
, by measuring the ex-
pected edit distance of the predicted y to the truth.
Expected edit distance is defined as
?
y
p
?
(y |
x
k
)d(y, y
k
) where d(y, y
k
) is the Levenshtein dis-
tance between two strings. It can be computed us-
ing standard finite-state algorithms (Mohri, 2003).
8.1 Data
We use an annotated corpus (Aramaki, 2010) of
50000 misspelled words x from tweets along with
their corrections y. All examples have d(x, y) = 1
though we do not exploit this fact. We randomly
selected 6000 training pairs and 100 test pairs. We
regularized the objective by adding ??||?||
2
2
, where
for each training condition, we chose ? by coarse
grid search to maximize the conditional likelihood
of 100 additional development pairs.
8.2 Context Windows and Edit Features
We considered four different settings for the con-
text window sizes (N
1
, N
2
, N
3
): (0,1,0)=stochas-
tic edit distance, (1,1,0), (0,2,0), and (1,1,1).
Our log-linear edit model (section 6) includes
a dedicated indicator feature for each contextual
edit (C, e), allowing us to fit any conditional dis-
tribution p(e | C). In our ?backoff? setting, each
(C, e) also has 13 binary backoff features that it
shares with other (C
?
, e
?
). So we have a total of 14
feature templates, which generate over a million
features in our largest model. The shared features
let us learn that certain properties of a contextual
edit tend to raise or lower its probability (and the
regularizer encourages such generalization).
Each contextual edit (C, e) can be character-
ized as a 5-tuple (s, t, C
1
, C
?
2
, C
3
): it replaces
s ? ?
x
? {} with t ? ?
y
? {} when s falls be-
tween C
1
and C
?
2
(so C
2
= sC
?
2
) and t is preceded
by C
3
. Then each of the 14 features of (C, e) in-
dicates that a particular subset of this 5-tuple has a
particular value. The subset alays includes s, t,
or both. It never includes C
1
or C
?
2
without s, and
never includes C
3
without t.
8.3 Results
Figures 2a and 2b show the learning curves. We
see that both metrics improve with more training
data; with more context; and with backoff. With
backoff, all of the contextual edit models substan-
tially beat ordinary stochastic edit distance, and
their advantage grows with training size.
9 Conclusion
We have presented a trainable, featurizable model
of contextual edit distance. Our main contribu-
tion is an efficient encoding of such a model as
a tight PFST?that is, a WFST that is guaranteed
to directly define conditional string probabilities
without need for further normalization. We are re-
leasing OpenFST-compatible code that can train
both PFSTs and WFSTs (Cotterell and Renduch-
intala, 2014). We formally defined PFSTs, de-
scribed their speed advantage at training time, and
noted that they are crucial in settings where the in-
put string is unknown. In future, we plan to deploy
our PFSTs in such settings.
629
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11?23. Springer.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2014. Robust entity clustering via phylogenetic in-
ference. In Proceedings of ACL.
Eiji Aramaki. 2010. Typo corpus. Available at http:
//luululu.com/tweet/#cr, January.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
HLT-NAACL, pages 582?590.
Alexandre Bouchard-C?ot?e, Percy Liang, Thomas L.
Griffiths, and Dan Klein. 2007. A probabilistic ap-
proach to language change. In NIPS.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings
of ACL, pages 88?95.
Zhiyi Chi and Stuart Geman. 1998. Estimation
of probabilistic context-free grammars. Computa-
tional Linguistics, 24(2):299?305.
Kenneth W. Church and William A. Gale. 1991. Prob-
ability scoring for spelling correction. Statistics and
Computing, 1(2):93?103.
Ryan Cotterell and Adithya Renduchintala. 2014.
brezel: A library for training FSTs. Technical re-
port, Johns Hopkins University.
Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
EMNLP, EMNLP ?08, pages 1080?1089.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proceedings of NAACL-
HLT, pages 644?648.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for learning finite-state transducers. In Proceed-
ings of the ESSLLI Workshop on Finite-State Meth-
ods in NLP.
Dale Gerdemann and Gertjan van Noord. 1999. Trans-
ducers from rewrite rules with backreferences. In
Proceedings of EACL.
Ronald M. Kaplan and Martin Kay. 1994. Regu-
lar models of phonological rule systems. Compu-
tational Linguistics, 20(3):331?378.
Dan Klein and Christopher D. Manning. 2002. Condi-
tional structure versus conditional estimation in NLP
models. In Proceedings of EMNLP, pages 9?16.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Proc. of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing).
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys
(CSUR), 24(4):377?439.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of ACL, pages 459?466.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517?522.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation. In
Proceedings of ICML.
Mehryar Mohri and Richard Sproat. 1996. An efficient
compiler for weighted rewrite rules. In Proceedings
of ACL, pages 231?238.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?311.
Mehryar Mohri. 2003. Edit-distance of weighted au-
tomata: General definitions and algorithms. Inter-
national Journal of Foundations of Computer Sci-
ence, 14(06):957?982.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
630
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674?679,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Polylingual Topic Models from
Code-Switched Social Media Documents
Nanyun Peng Yiming Wang Mark Dredze
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD USA
{npeng1,freewym,mdredze}@jhu.edu
Abstract
Code-switched documents are common
in social media, providing evidence for
polylingual topic models to infer aligned
topics across languages. We present
Code-Switched LDA (csLDA), which in-
fers language specific topic distributions
based on code-switched documents to fa-
cilitate multi-lingual corpus analysis. We
experiment on two code-switching cor-
pora (English-Spanish Twitter data and
English-Chinese Weibo data) and show
that csLDA improves perplexity over
LDA, and learns semantically coherent
aligned topics as judged by human anno-
tators.
1 Introduction
Topic models (Blei et al, 2003) have become stan-
dard tools for analyzing document collections, and
topic analyses are quite common for social media
(Paul and Dredze, 2011; Zhao et al, 2011; Hong
and Davison, 2010; Ramage et al, 2010; Eisen-
stein et al, 2010). Their popularity owes in part to
their data driven nature, allowing them to adapt to
new corpora and languages. In social media espe-
cially, there is a large diversity in terms of both the
topic and language, necessitating the modeling of
multiple languages simultaneously. A good candi-
date for multi-lingual topic analyses are polylin-
gual topic models (Mimno et al, 2009), which
learn topics for multiple languages, creating tuples
of language specific distributions over monolin-
gual vocabularies for each topic. Polylingual topic
models enable cross language analysis by group-
ing documents by topic regardless of language.
Training of polylingual topic models requires
parallel or comparable corpora: document tuples
from multiple languages that discuss the same
topic. While additional non-aligned documents
User 1: ?Don Samuel es un crack! #VamosM?exico #DaleTri
RT @User4: Arriba! Viva Mexico! Advanced to GOLD.
medal match in ?Football?!
User 2: @user1 rodo que tal el nuevo Mountain ?
User 3: @User1 @User4 wow this is something !! Ja ja ja
Football well said
Figure 1: Three users discuss Mexico?s football
team advancing to the Gold medal game in the
2012 Olympics in code-switched Spanish and En-
glish.
can be folded in during training, the ?glue? doc-
uments are required to aid in the alignment across
languages. However, the ever changing vocabu-
lary and topics of social media (Eisenstein, 2013)
make finding suitable comparable corpora diffi-
cult. Standard techniques ? such as relying on ma-
chine translation parallel corpora or comparable
documents extracted from Wikipedia in different
languages ? fail to capture the specific terminol-
ogy of social media. Alternate methods that rely
on bilingual lexicons (Jagarlamudi and Daum?e,
2010) similarly fail to adapt to shifting vocabular-
ies. The result: an inability to train polylingual
models on social media.
In this paper, we offer a solution: utilize code-
switched social media to discover correlations
across languages. Social media is filled with ex-
amples of code-switching, where users switch be-
tween two or more languages, both in a conversa-
tion and even a single message (Ling et al, 2013).
This mixture of languages in the same context sug-
gests alignments between words across languages
through the common topics discussed in the con-
text.
We learn from code-switched social media by
extending the polylingual topic model framework
to infer the language of each token and then auto-
matically processing the learned topics to identify
aligned topics. Our model improves both in terms
of perplexity and a human evaluation, and we pro-
vide some example analyses of social media that
rely on our learned topics.
674
2 Code-Switching
Code-switched documents has received consider-
able attention in the NLP community. Several
tasks have focused on identification and analysis,
including mining translations in code-switched
documents (Ling et al, 2013), predicting code-
switched points (Solorio and Liu, 2008a), identi-
fying code-switched tokens (Lignos and Marcus,
2013; Yu et al, 2012; Elfardy and Diab, 2012),
adding code-switched support to language mod-
els (Li and Fung, 2012), linguistic processing of
code switched data (Solorio and Liu, 2008b), cor-
pus creation (Li et al, 2012; Diab and Kamboj,
2011), and computational linguistic analyses and
theories of code-switching (Sankofl, 1998; Joshi,
1982).
Code-switching specifically in social media has
also received some recent attention. Lignos and
Marcus (2013) trained a supervised token level
language identification system for Spanish and
English code-switched social media to study code-
switching behaviors. Ling et al (2013) mined
translation spans for Chinese and English in code-
switched documents to improve a translation sys-
tem, relying on an existing translation model to aid
in the identification and extraction task. In contrast
to this work, we take an unsupervised approach,
relying only on readily available document level
language ID systems to utilize code-switched data.
Additionally, our focus is not on individual mes-
sages, rather we aim to train a model that can be
used to analyze entire corpora.
In this work we consider two types of code-
switched documents: single messages and conver-
sations, and two language pairs: Chinese-English
and Spanish-English. Figure 1 shows an exam-
ple of a code-switched Spanish-English conversa-
tion, in which three users discuss Mexico?s foot-
ball team advancing to the Gold medal game in
the 2012 Summer Olympics. In this conversation,
some tweets are code-switched and some are in a
single language. By collecting the entire conver-
sation into a single document we provide the topic
model with additional content. An example of a
Chinese-English code-switched messages is given
by Ling et al (2013):
watup Kenny Mayne!! - Kenny Mayne
??????!!
Here a user switches between languages in a single
message. We empirically evaluate our model on
both conversations and messages. In the model
presentation we will refer to both as ?documents.?
3 csLDA
To train a polylingual topic model on social me-
dia, we make two modifications to the model of
Mimno et al (2009): add a token specific language
variable, and a process for identifying aligned top-
ics.
First, polylingual topic models require paral-
lel or comparable corpora in which each docu-
ment has an assigned language. In the case of
code-switched social media data, we require a per-
token language variable. However, while docu-
ment level language identification (LID) systems
are common place, very few languages have per-
token LID systems (King and Abney, 2013; Lig-
nos and Marcus, 2013).
To address the lack of available LID systems,
we add a per-token latent language variable to the
polylingual topic model. For documents that are
not code-switched, we observe these variables to
be the output of a document level LID system. In
the case of code-switched documents, these vari-
ables are inferred during model inference.
Second, polylingual topic models assume the
aligned topics are from parallel or comparable cor-
pora, which implicitly assumes that a topics pop-
ularity is balanced across languages. Topics that
show up in one language necessarily show up in
another. However, in the case of social media,
we can make no such assumption. The topics
discussed are influenced by users, time, and lo-
cation, all factors intertwined with choice of lan-
guage. For example, English speakers will more
likely discuss Olympic basketball while Spanish
speakers football. There may be little or no docu-
ments on a given topic in one language, while they
are plentiful in another. In this case, a polylin-
gual topic model, which necessarily infers a topic-
specific word distribution for each topic in each
language, would learn two unrelated word dis-
tributions in two languages for a single topic.
Therefore, naively using the produced topics as
?aligned? across languages is ill-advised.
Our solution is to automatically identify aligned
polylingual topics after learning by examining
a topic?s distribution across code-switched docu-
ments. Our metric relies on distributional proper-
ties of an inferred topic across the entire collec-
tion.
675
To summarize, based on the model of Mimno et
al. (2009) we will learn:
? For each topic, a language specific word distri-
bution.
? For each (code-switched) token, a language.
? For each topic, an identification as to whether
the topic captures an alignment across lan-
guages.
The first two goals are achieved by incorporat-
ing new hidden variables in the traditional polylin-
gual topic model. The third goal requires an auto-
mated post-processing step. We call the resulting
model Code-Switched LDA (csLDA). The gener-
ative process is as follows:
? For each topic z ? T
? For each language l ? L
? Draw word distribution
?
l
z
?Dir(?
l
)
? For each document d ? D:
? Draw a topic distribution ?
d
? Dir(?)
? Draw a language distribution
?
d
?Dir(?)
? For each token i ? d:
? Draw a topic z
i
? ?
d
? Draw a language l
i
? ?
d
? Draw a word w
i
? ?
l
z
For monolingual documents, we fix l
i
to the LID
tag for all tokens. Additionally, we use a single
background distribution for each language to cap-
ture stopwords; a control variable pi, which fol-
lows a Dirichlet distribution with prior parameter-
ized by ?, is introduced to decide the choice be-
tween background words and topic words follow-
ing (Chemudugunta et al, 2006)
1
. We use asym-
metric Dirichlet priors (Wallach et al, 2009), and
let the optimization process learn the hyperparam-
eters. The graphical model is shown in Figure 2.
3.1 Inference
Inference for csLDA follows directly from LDA.
A Gibbs sampler learns the word distributions ?
l
z
for each language and topic. We use a block Gibbs
sampler to jointly sample topic and language vari-
ables for each token. As is customary, we collapse
out ?, ? and ?. The sampling posterior is:
P (z
i
, l
i
|w, z
?i
, l
?i
, ?, ?, ?) ?
(n
l,z
w
i
)
?i
+ ?
n
l,z
?i
+W?
?
m
z,d
?i
+ ?
m
d
?i
+ T ?
?
o
l,d
?i
+ ?
o
d
?i
+ L?
(1)
where (n
l,z
w
i
)
?i
is the number of times the type for
word w
i
assigned to topic z and language l (ex-
1
Omitted from the generative process but shown in Fig. 2.
?
?
l
i
?
d
?
d
?
l
z
?
l
b
?
B?
z
i
b
i
w
i
D
N
L
T
Figure 2: The graphical model for csLDA.
cluding current word w
i
), m
z,d
?i
is the number of
tokens assigned to topic z in document d (exclud-
ing current word w
i
), o
l,d
?i
is the number of tokens
assigned to language l in document d (excluding
current word w
i
), and these variables with super-
scripts or subscripts omitted are totals across all
values for the variable. W is the number of words
in the corpus. All counts omit words assigned
to the background. During sampling, words are
first assigned to the background/topic distribution
and then topic and language are sampled for non-
background words.
We optimize the hyperparameters ?, ?, ? and ?
by interleaving sampling iterations with a Newton-
Raphson update to obtain the MLE estimate for
the hyperparameters. Taking ? as an example, one
step of the Newton-Raphson update is:
?
new
= ?
old
?H
?1
?L
??
(2)
where H is the Hessian matrix and
?L
??
is the gra-
dient of the likelihood function with respect to
the optimizing hyperparameter. We interleave 200
sampling iterations with one Newton-Raphson up-
date.
3.2 Selecting Aligned Topics
We next identify learned topics (a set of related
word-distributions) that truly represent an aligned
topic across languages, as opposed to an unrelated
set of distributions for which there is no support-
ing alignment evidence in the corpus. We begin by
measuring how often each topic occurs in code-
switched documents. If a topic never occurs in
a code-switched document, then there can be no
evidence to support alignment across languages.
For the topics that appear at least once in a code-
switched document, we estimate their probability
676
in the code-switched documents by a MAP esti-
mate of ?. Topics appearing in at least one code-
switched document with probability greater than
a threshold p are selected as candidates for true
cross-language topics.
4 Data
We used two datasets: a Sina Weibo Chinese-
English corpus (Ling et al, 2013) and a Spanish-
English Twitter corpus.
Weibo Ling et al (2013) extracted over 1m
Chinese-English parallel segments from Sina
Weibo, which are code-switched messages. We
randomly sampled 29,705 code-switched mes-
sages along with 42,116 Chinese and 42,116 En-
glish messages from the the same time frame. We
used these data for training. We then sampled
an additional 2475 code-switched messages, 4221
English and 4211 Chinese messages as test data.
Olympics We collected tweets from July 27,
2012 to August 12, 2012, and identified 302,775
tweets about the Olympics based on related hash-
tags and keywords (e.g. olympics, #london2012,
etc.) We identified code-switched tweets using
the Chromium Language Detector
2
. This system
provides the top three possible languages for a
given document with confidence scores; we iden-
tify a tweet as code-switched if two predicted lan-
guages each have confidence greater than 33%.
We then used the tagger of Lignos and Marcus
(2013) to obtain token level LID tags, and only
tweets with tokens in both Spanish and English are
used as code-switched tweets. In total we iden-
tified 822 Spanish-English code-switched tweets.
We further expanded the mined tweets to full con-
versations, yielding 1055 Spanish-English code-
switched documents (including both tweets and
conversations), along with 4007 English and 4421
Spanish tweets composes our data set. We reserve
10% of the data for testing.
5 Experiments
We evaluated csLDA on the two datasets and eval-
uated each model using perplexity on held out data
and human judgements. While our goal is to learn
polylingual topics, we cannot compare to previous
polylingual models since they require comparable
data, which we lack. Instead, we constructed a
baseline from LDA run on the entire dataset (no
2
https://code.google.com/p/chromium-compact-language-detector/
language information.) For each model, we mea-
sured the document completion perplexity (Rosen-
Zvi et al, 2004) on the held out data. We ex-
perimented with different numbers of topics (T ).
Since csLDA duplicates topic distributions (T ?L)
we used twice as many topics for LDA.
Figure 3 shows test perplexity for varying T and
perplexity for the best setting of csLDA (T =60)
and LDA (T =120). The table lists both mono-
lingual and code-switched test data; csLDA im-
proves over LDA in almost every case, and across
all values of T . The background distribution (-bg)
has mixed results for LDA, whereas for csLDA
it shows consistent improvement. Table 4 shows
some csLDA topics. While there are some mis-
takes, overall the topics are coherent and aligned.
We use the available per-token LID system
(Lignos and Marcus, 2013) for Spanish/English
to justify csLDA?s ability to infer the hidden lan-
guage variables. We ran csLDA-bg with l
i
set to
the value provided by the LID system for code-
switched documents (csLDA-bg with LID), which
gives csLDA high quality LID labels. While we
see gains for the code-switched data, overall the
results for csLDA-bg and csLDA-bg with LID are
similar, suggesting that the model can operate ef-
fectively even without a supervised per-token LID
system.
5.1 Human Evaluation
We evaluate topic alignment quality through a hu-
man judgements (Chang et al, 2009). For each
aligned topic, we show an annotator the 20 most
frequent words from the foreign language topic
(Chinese or Spanish) with the 20 most frequent
words from the aligned English topic and two ran-
dom English topics. The annotators are asked to
select the most related English topic among the
three; the one with the most votes is considered
the aligned topic. We count how often the model?s
alignments agree.
LDA may learn comparable topics in different
languages but gives no explicit alignments. We
create alignments by classifying each LDA topic
by language using the KL-divergence between the
topic?s words distribution and a word distribution
for the English/foreign language inferred from the
monolingual documents. Language is assigned to
a topic by taking the minimum KL. For Weibo
data, this was not effective since the vocabularies
of each language are highly unbalanced. Instead,
677
20/40 30/60 40/80 50/100 60/120 70/140
# Topics
8000
8500
9000
9500
10000
P
e
r
p
l
e
x
i
t
y
LDA
LDA-bg
csLDA-bg with LID
csLDA-bg
csLDA
20/40 30/60 40/80 50/100 60/120 70/140
# Topics
18000
20000
22000
24000
26000
28000
30000
P
e
r
p
l
e
x
i
t
y
LDA
LDA-bg
csLDA-bg
csLDA
T =60/120 Olympics Weibo
En Es CS En Cn CS
LDA 11.32 9.44 6.97 29.19 23.06 11.69
LDA-bg 11.35 9.51 6.79 40.87 27.56 10.91
csLDA 8.72 7.94 6.17 18.20 17.31 12.72
csLDA-bg 8.72 7.73 6.04 18.25 17.74 12.46
csLDA-bg 8.73 7.93 4.91 - - -
with LID
Figure 3: Plots show perplexity for different T (Olympics left, Weibo right). Perplexity in the table are
in magnitude of 1? 10
3
.
Football Basketball
English Spanish English Spanish
mexico mucho game espa?na
brazil argentina basketball baloncesto
soccer m?exico year basketball
vs brasil finals bronce
womens ganar?a gonna china
football tri nba final
mens yahel castillo obama rusia
final delpo lebron espa?nola
Social Media Transportation
English Chinese English Chinese
twitter ??? car ??
bitly ?? drive ??
facebook ?? road ??
check ?? line ??
use ?? train ???
blog ?? harry ??
free pm ?? ??
post ?? bus ??
Figure 4: Examples of aligned topics from Olympics (left) and Weibo (right).
we manually labeled the topics by language. We
then pair topics across languages using the cosine
similarity of their co-occurrence statistics in code-
switched documents. Topic pairs with similarity
above t are considered aligned topics. We also
used a threshold p (?3.2) to select aligned topics
in csLDA. To ensure a fair comparison, we select
the same number of aligned topics for LDA and
csLDA.
3
. We used the best performing setting:
csLDA T =60, LDA T =120, which produced 12
alignments from Olympics and 28 from Weibo.
Using Mechanical Turk we collected multiple
judgements per alignment. For Spanish, we re-
moved workers who disagreed with the majority
more than 50% of the time (83 deletions), leav-
ing 6.5 annotations for each alignment (85.47%
inter-annotator agreement.) For Chinese, since
quality of general Chinese turkers is low (Pavlick
et al, 2014) we invited specific workers and
obtained 9.3 annotations per alignment (78.72%
inter-annotator agreement.) For Olympics, LDA
alignments matched the judgements 25% of the
time, while csLDA matched 50% of the time.
While csLDA found 12 alignments and LDA 29,
the 12 topics evaluated from both models show
that csLDA?s alignments are higher quality. For
the Weibo data, LDA matched judgements 71.4%,
while csLDA matched 75%. Both obtained high
3
We used thresholds p = 0.2 and t = 0.0001. We limited
the model with more alignments to match the one with less.
quality alignments ? likely due both to the fact
that the code-switched data is curated to find trans-
lations and we hand labeled topic language ? but
csLDA found many more alignments: 60 as com-
pared to 28. These results confirm our automated
results: csLDA finds higher quality topics that
span both languages.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research (JMLR), 3:993?1022.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-graber, and David M Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in neural information processing systems,
pages 288?296.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS.
Mona Diab and Ankit Kamboj. 2011. Feasibility of
leveraging crowd sourcing for the creation of a large
scale annotated resource for Hindi English code
switched data: A pilot annotation. In Proceedings
of the 9th Workshop on Asian Language Resources,
pages 36?40, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model
678
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1277?1287. Asso-
ciation for Computational Linguistics.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In NAACL.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of COLING 2012: Posters, pages 287?296,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings of
the First Workshop on Social Media Analytics, pages
80?88. ACM.
Jagadeesh Jagarlamudi and Hal Daum?e. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. Advances in Information Retrieval,
pages 444?456.
Aravind K Joshi. 1982. Processing of sentences
with intra-sentential code-switching. In Proceed-
ings of the 9th Conference on Computational lin-
guistics (COLING), pages 145?150.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In NAACL.
Ying Li and Pascale Fung. 2012. Code-switch lan-
guage model with inversion constraints for mixed
language speech recognition. In Proceedings of
COLING 2012, pages 1671?1680, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL
Anthology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. To-
ward web-scale analysis of codeswitching. In An-
nual Meeting of the Linguistic Society of America.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting
on Association for Computational Linguistics, ACL
?13. Association for Computational Linguistics.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2-Volume 2, pages
880?889. Association for Computational Linguis-
tics.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics, 2(Feb):79?92.
Daniel Ramage, Susan T Dumais, and Daniel J
Liebling. 2010. Characterizing microblogs with
topic models. In ICWSM.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th conference on Uncertainty in artificial intelli-
gence, pages 487?494. AUAI Press.
David Sankofl. 1998. The production of code-mixed
discourse. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 1, pages 8?21, Montreal,
Quebec, Canada, August. Association for Computa-
tional Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to
predict code-switching points. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 973?981, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-Speech
tagging for English-Spanish code-switched text. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
1051?1060, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Hanna M Wallach, David M Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter.
In NIPS, volume 22, pages 1973?1981.
Liang-Chih Yu, Wei-Cheng He, and Wei-Nan Chien.
2012. A language modeling approach to identify-
ing code-switched sentences and words. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Confer-
ence on Chinese Language Processing, pages 3?8,
Tianjin, China, December. Association for Compu-
tational Linguistics.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing
He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.
2011. Comparing twitter and traditional media us-
ing topic models. In Advances in Information Re-
trieval, pages 338?349. Springer.
679
