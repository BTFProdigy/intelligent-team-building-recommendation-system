Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 298?301,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
  Eliciting interactional phenomena in human-human dialogues 
Joakim Gustafson 
 
KTH Speech Music & Hearing  
jocke@speech.kth.se 
    Miray Merkes 
 
    KTH Speech Music & Hearing  
   miray@kth.se 
Abstract 
In order to build a dialogue system that can inte-
ract with humans in the same way as humans in-
teract with each other, it is important to be able 
to collect conversational data. This paper intro-
duces a dialogue recording method where an 
eavesdropping human operator sends instruc-
tions to the participants in an ongoing human-
human task-oriented dialogue. The purpose of 
the instructions is to control the dialogue pro-
gression or to elicit interactional phenomena. 
The recordings were used to build a Swedish 
synthesis voice with disfluent diphones. 
1 Background 
Our research group have a long-standing interest in 
human conversational behaviour and a special in-
terest in its mimicry and evaluation in spoken dia-
logue systems (Edlund et al, 2008). In human-
human conversations both parties continuously and 
simultaneously contribute actively and interac-
tively to the conversation. Listeners actively con-
tribute by providing feedback during the other?s 
speech, and speakers continuously monitor the re-
actions to their utterances (Clark, 1996). If spoken 
dialogue systems are to achieve the responsiveness 
and flexibility found in human-human interaction, 
it is essential that they process information incre-
mentally and continuously rather than in turn sized 
chunks (Dohsaka & Shimazu, 1997, Skantze & 
Schlangen, 2009). These systems need to be able to 
stop speaking in different manners depending on 
whether it has finished what it planned to say or if 
it was interrupted mid-speech by the user. In order 
to be responsive, the system might also need to 
start talking before it has decided exactly what to 
say. In this case it has to be able to generate inter-
actional cues that restrain the user from start speak-
ing while the system plans the last part.  
To date very few spoken dialogues systems can 
generate crucial and commonly used interactional 
cues. Adell et al (2007) have developed a set of 
rules for synthesizing filled pauses and repetitions 
with PSOLA. Unit selection synthesizers are often 
used in dialogue systems, but a problem with these 
is that even though most databases have been care-
fully designed and read, they are not representative 
of ?speech in use? (Campbell & Mokhiari, 2003). 
There are examples of synthesizers that have been 
trained on speech in use, like Sundaram & Naraya-
nan (2003) that used a limited-domain dialogue 
corpus of transcribed human utterances as input for 
offline training of a machine learning system that 
could insert fillers and breathing at the appropriate 
places in new domain-related texts. However, these 
were synthesized with a unit selection voice that 
had been trained on lecture speech.  
When modelling talk-in-use it is important to 
study representative data. The problem with study-
ing real dialogues is that the interesting interac-
tional phenomena often are sparsely occurring and 
very context dependent. When conducting research 
on spontaneous speech you have the option to use 
controlled or uncontrolled conditions. Anderson et 
al., 1991) recorded unscripted conversations in a 
map task exercise that had been carefully designed 
to elicit interactional phenomena. When using con-
trolled conditions in a study you risk to manipulate 
the data, while in uncontrolled conditions there?s a 
risk that the conversation goes out of hand which 
leads to a lot of unnecessary material (Bock, 1996). 
Bock suggests a set of eliciting methods to be used 
when studying disfluent speech. If the goal is to 
study speech errors and interruptions, a situation 
with two competing humans is useful. If the goal is 
to study hesitations and self-interruptions, distract-
ing events can be used to disrupt the flow of 
speech.  
298
Say nothing at pauses Talk slowly 
The Wizard?s GUI 
Say nothing at pauses Talk slowly 
  The Shopkeeper?s GUI      The Customer?s GUI                                        
Figure 1. The GUIs used by the wizard and subjects. 
This paper presents a new method for elicitation of 
interactional phenomena, with the goal of reducing 
the amount of necessary dialogue recordings. In 
this method an eavesdropping human operator 
sends instructions two subjects as they engage in a 
task-oriented dialogue. The purpose of these in-
structions is either to control the dialogue progres-
sion or to elicit certain interactional phenomena. 
The recordings from two sessions were used to 
build a synthesis voice with disfluent diphones. In 
a small synthesis study on generation of disfluent 
conversational utterances this voice was compared 
with a commercial Swedish diphone voice based 
on read speech. The subjects rated the created 
voice as more natural than the commercial voice.  
2 Method 
A dialogue collection environment has been devel-
oped that allows a human operator (Wizard) to ea-
vesdrop an ongoing computer-mediated human-
human conversation. It also allows the Wizard to 
send instructions to the interlocutors during their 
conversation, see Figure 1. The purpose of the in-
structions is to control the progression of the task-
oriented dialogue and to elicit interactional pheno-
mena, e.g. interruptions and hesitations. The Wizard 
has access to graphical and textual instructions. 
Graphical instructions are pictures that are manipu-
lated or text labels that are changed. Textual instruc-
tions are scrolled in from the right at the bottom of 
the screen. They can be of three categories: Emo-
tional instructions that tell the receiver to act emo-
tional (e.g. act grumpy); Task-related instructions 
that require the receiver to initiate a certain sub-
tasks (e.g. buy a red car); and Dialogue flow related 
instructions that tell the receiver to change his way 
of speaking, (e.g. speak fast, do not pause). 
3 The pilot study 
The DEAL system is a speech-enabled computer 
game currently under development, that will be 
used for conversational training for second lan-
guage learners of Swedish (Hjalmarsson, 2008). In 
this system an embodied conversational character 
(ECA) acts as a shopkeeper in a flea trade-market 
and the user is a customer. The developed envi-
ronment was adapted to the DEAL domain, and in 
a pilot study two human subjects were instructed to 
act as shopkeeper and customer. They were given 
written persona descriptions and were then placed 
in separate rooms. They interacted via a three-party 
Skype call, which allowed the Wizard to eavesdrop 
their conversation. In order to get a situation that 
was similar to the DEAL system, the subjects saw 
an avatar with lip movements driven by, and in 
synchrony with, the other subjects? speech. In or-
der to achieve this, the SynFace system was used, 
which introduced a 200 ms delay in each direction 
(Beskow et al, 2004). Apart from the avatar the 
interfaces also contained pictures of objects cur-
rently for sale with accompanying prices, see Fig-
ure 1. At the bottom of the screen there was a black 
area where the subjects got the textual instructions 
from the Wizard.     
The eavesdropping Wizard was placed in a 
third room, with an interface that allowed her to 
control the current set of objects and prices on the 
subjects? screens. The Wizard interface also con-
tained an area for the textual instructions. In order 
to distort the dialogue flow some of the instruc-
tions involved sending instructions to both subjects 
at the same time. A main idea is to instruct one of 
the interlocutors to display a verbal behavior that 
will elicit interactional phenomena in the other di-
alogue partner's contributions. Table 1 shows some 
examples of the different types of textual instruc-
tions to the subjects and their intended effect on 
the shopkeeper party in an ongoing conversation. 
The Wizard interface also gave access to auto-
mated instructions that follows a pre-scripted ma-
nuscript in order to facilitate consistent instructions 
across different sessions. This also made it possible 
to transmit multiple successive instructions with 
high speed and a minimum risk of mistakes.  
299
 Shopkeeper 
reaction 
Graphical Emotional Task related Dialog flow related 
Hesitation Show an ambiguous 
picture (S) 
Be wining and talk about 
how unfair life is (S) 
Sell blue car (S) 
Buy red car (C)  
Talk slowly (S) 
Say nothing at pauses (C) 
Interruption Change picture in mid 
speech (S) 
Be a annoying customer (C) Tell your price (S) 
Tell your price (C)  
Speak without pauses (S) 
Try to speak all the time (C) 
Change of 
sub-task  
Show a picture (S) Discuss the advantages of a 
certain item (S) 
Sell the red car (S) Ask a lot of questions (C) 
Answer with questions (S) 
Table 1. Examples of instruction types and their intended reaction in the shopkeeper?s subsequent turn(s). The re-
ceiver of the instruction is indicated by S (Shopkeeper) and C (Customer).  
 
4 The effect of the Wizard?s instruction 
Two half-hour conversations were recorded where 
the same male subject (acting as shopkeeper) inte-
racted with two different female subjects (acting as 
customers). The audio recordings were synchro-
nized with the instructions that had been submitted 
by the Wizard during the conversation. The effects 
of the instructions were analyzed by inspecting 
both subjects? turns following an instruction from 
the Wizard. The analysis was focused on the dis-
ruptive effect of the instructions, and it showed 
that they often lead to turns that contained hesita-
tions, interruptions and pauses. The task-related 
instructions lead to disfluent speech in half of the 
succeeding turns, while the dialogue flow related 
instructions, the emotional instructions and the 
graphical instructions led to disfluent turns in two 
thirds of the cases. The analysis of the instructions? 
effect on the disfluency rates revealed that the ones  
that changed the task while the subjects talked 
were very efficient, e.g. changing the price while it 
was discussed. The effect on the disfluency rates 
was most substantial when contradictive instruc-
tions were given to both subjects at the same time.  
In order to get a baseline of disfluency rates in 
human-human dialogues in the current domain, the 
dialogue data was compared with data recorded in 
a previous DEAL recording. In this study 8 dialo-
gues were recorded where two subjects role-played 
as a shopkeeper and a customer, but without the 
controlling Wizard used in the present study 
(Hjalmarsson, 2008). In these recordings approx-
imately one third of the turns contained disfluent 
speech. This indicates that the disfluency rates 
found after the instructions in the current study are 
a higher than in the previous DEAL recording. Fi-
nally we analyzed the effect of the instructions on 
the dialogue progression. The instructions were 
very helpful in keeping the discussion going and 
the task oriented instructions provided useful guid-
ance to the subjects in their role-playing. 
5 A speech synthesis experiment 
In a second experiment the goal was to evaluate two 
methods for collecting conversational data for build-
ing a corpus-based conversational speech synthesiz-
er: collecting a controlled human-human role-
playing dialogue or a recording a human that reads a 
dialogue transcription with tags for interruptions and 
hesitations. In this experiment the recordings of the 
male subject that acted as shopkeeper were used. 20 
of his utterances that contained hesitations, inter-
ruptions and planned pauses were selected. New 
versions of these utterances were created, where 
the disruptions were removed. In order to verify 
that the disruptive sections could be synthesized in 
new places a set of test sentences were constructed 
that included their immediate contexts. Finally, 
new versions of the new test sentences were 
created, that had added tags for disruptions. All 
types of utterances were read by the original male 
speaker. Both the original dialogue recordings and 
the read utterances were phonetically transcribed 
and aligned in order to build a small diphone voice 
with the EXPROS tool (Gustafson & Edlund, 2008). 
This diphone voice contained fillers, truncated pho-
nemes and audible breathing.  
All types of utterances were re-synthesized with 
the newly created voice and with a Swedish com-
mercial diphone voice that was trained on clear 
read speech. While re-synthesizing the original 
recordings all prosodic features (pitch, duration 
and loudness) were kept. The main difference be-
tween the two voices was the voice quality: the 
commercial voice is trained on clear read speech, 
while the new voice was created from the dialogue 
recordings contains both reduced and truncated 
diphones.  
Secondly, a number of utterances were synthe-
sized, where disfluent sections were inserted into 
fluently read sentences. For both voices the disflu-
ent sections? original pitch, duration and loudness 
were kept. As in the previous case the main differ-
300
ence between the two cases is that the newly 
created also made use of its disfluent diphones. 
The disfluent sections were either taken from the 
original dialogue recordings or from the set of read 
sentences with tags for disfluencies. 
 
6 Preliminary synthesis evaluation 
16 subjects participated in a listening test, where 
they were told to focus on the disrupted parts of the 
utterances. They were instructed to indicate when 
they could detect the following disruptions: hesita-
tion, pause, interruption and correction. They were 
also asked to assess on a six-graded likert scale 
how natural these sounded and how easy it was to 
detect the disrupted parts. Results show that dis-
rupted utterances that were synthesized with the 
new voice were rated as natural in two thirds of the 
cases, while the ones that were generated with 
commercial synthesis voice, that lacked disfluent 
diphones, was rated as natural in half of the cases. 
Kruskal-Wallis rank sums were performed, and the 
interrupted utterances generated by new voice was 
significantly more natural than those generated 
with the commercial voice (p=0.001). When com-
paring how easy it was to detect the disrupted parts 
both versions are comparable (90% of them were 
easy to detect, with no significant difference).   
In order to analyze the difference between real 
and pretended disruptions, the subjects were asked 
to compare re-synthesis of the of disrupted dialo-
gue turns with corresponding read versions. They 
were asked to judge which of the two they thought 
contained a pretended disruption. When comparing 
re-synthesis of complete utterances from either of 
these types they were able to detect the version 
with pretended disruptions in 60% of the cases. In 
cases where the disfluent parts were moved to new 
fluently read sentences the users could not tell 
which version contained a pretended disruption. 
This is probably because they rated how the whole 
sentence sounded, rather than only the disrupted 
part. These differences were significant according 
to a chi-square test. Finally, the subjects? ability to 
identify the different types of disfluencies when 
synthesized by the two voices was compared. For 
both voices, about 80% of the hesitations and inter-
ruptions were correctly identified, while only 70% 
of the planned pauses were correctly identified. For 
both voices about 85% of the missed pauses were 
instead identified as hesitations or interruptions. 
For the new voice most of them were identified as 
hesitations, while they were mostly misinterpreted 
as interruptions for the commercial voice. The 
share of inserted interruptions is the only signifi-
cant identification difference between the two 
voices. This is not surprising since they both used 
the pitch, power and durations from the original 
human recordings, while only the new voice also 
had access to truncated diphones. 
This pilot study showed that the instructions 
from the Wizards were useful both to control the 
dialogue flow and to elicit interactional phenome-
na. Finally, the male participant reported that it 
was hard to pretend to be disfluent while reading 
dialogue transcripts where this was tagged.  
Acknowledgements 
This research is supported by MonAMI, an Integrated 
Project under the European Commission (IP-035147). 
References 
Adell, J., Bonafonte, A., & Escudero, D. (2007). Filled pauses 
in speech synthesis: towards conversational speech. In Proc. 
of Conference on Text, Speech and Dialogue( LNAI 07) 
Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G., 
Garrod, S., Isard, S., Kowtko, J., McAllister, J., Miller, J., 
Sotillo, C., Thompson, H., & Weinert, R. (1991). The 
HCRC Map Task corpus. Language and Speech, 34(4). 
Beskow, J., Karlsson, I., Kewley, J., & Salvi, G. (2004). 
SYNFACE - A talking head telephone for the hearing-
impaired. In Miesenberger, K., Klaus, J., Zagler, W., & 
Burger, D. (Eds.), Computers Helping People with Special 
Needs. Springer-Verlag. 
Bock, K. (1996). Language production: Methods and metho-
dologies. In Psychonomic Bulletin and Review.  
Campbell, N., & Mokhiari, P. (2003). Using a Non-
Spontaneous Speech Synthesiser as a Driver for a Spontane-
ous Speech Synthesiser. In Proceedings of ISCA & IEEE 
Workshop on Spontaneous Speech Processing and Recogni-
tion. Tokyo, Japan. 
Clark, H. H. (1996). Using language. Cambridge, UK: Cam-
bridge University Press. 
Dohsaka, K., & Shimazu, A. (1997). System architecture for 
spoken utterance production in collaborative dialogue. In 
Working Notes of IJCAI 1997 Workshop on Collaboration, 
Cooperation and Conflict in Dialogue Systems.  
Edlund, J., Gustafson, J., Heldner, M., & Hjalmarsson, A. 
(2008). Towards human-like spoken dialogue systems. 
Speech Communication, 50(8-9). 
Gustafson, J., & Edlund, J. (2008). expros: a toolkit for explo-
ratory experimentation with prosody in customized diphone 
voices. In Proceedings of PIT 2008.  
Hjalmarsson, A. (2008). Speaking without knowing what to 
say... or when to end. In Proceedings of SIGDial 2008. 
Skantze, G., & Schlangen, D. (2009). Incremental dialogue 
processing in a micro-domain. In Proceedings of EACL-09. 
Sundaram, S., & Narayanan, S. (2003). An empirical text 
transformation method for spontaneous speech synthesizers. 
In Proceedings of Interspeech 2003, Switzerland. 
301
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 310?313,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Attention and Interaction Control in  
a Human-Human-Computer Dialogue Setting 
 
Gabriel Skantze 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
gabriel@speech.kth.se 
Joakim Gustafson 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
jocke@speech.kth.se 
 
 
 
Abstract 
This paper presents a simple, yet effective 
model for managing attention and interaction 
control in multimodal spoken dialogue sys-
tems. The model allows the user to switch at-
tention between the system and other hu-
mans, and the system to stop and resume 
speaking. An evaluation in a tutoring setting 
shows that the user?s attention can be effec-
tively monitored using head pose tracking, 
and that this is a more reliable method than 
using push-to-talk.  
1 Introduction 
Most spoken dialogue systems are based on the 
assumption that there is a clear beginning and 
ending of the dialogue, during which the user 
pays attention to the system constantly. However, 
as the use of dialogue systems is extended to 
settings where several humans are involved, or 
where the user needs to attend to other things 
during the dialogue, this assumption is obviously 
too simplistic (Bohus & Horvitz, 2009). When it 
comes to interaction, a strict turn-taking protocol 
is often assumed, where user and system wait for 
their turn and deliver their contributions in whole 
utterance-sized chunks. If system utterances are 
interrupted, they are treated as either fully 
delivered or basically unsaid. 
This paper presents a simple, yet effective 
model for managing attention and interaction 
control in multimodal (face-to-face) spoken dia-
logue systems, which avoids these simplifying 
assumptions. We also present an evaluation in a 
tutoring setting where we explore the use of head 
tracking for monitoring user attention, and com-
pare it with a more traditional method: push-to-
talk.  
2 Monitoring user attention 
In multi-party dialogue settings, gaze has been 
identified as an effective cue to help disambi-
guate the addressee of a spoken utterance 
(Vertegaal et al, 2001).  When it comes to hu-
man-machine interaction, Maglio et al (2000) 
showed that users tend to look at speech-
controlled devices when talking to them, even if 
they do not have the manifestation of an embo-
died agent. Bakx et al (2003) investigated the 
use of head pose for identifying the addressee in 
a multi-party interaction between two humans 
and an information kiosk. The results indicate 
that head pose should be combined with acoustic 
and linguistic features such as utterances length. 
Facial orientation in combination with speech-
related features was investigated by Katzenmaier 
et al (2004) in a human-human-robot interaction, 
confirming that a combination of cues was most 
effective. A common finding in these studies is 
that if a user does not look at the system while 
talking he is most likely not addressing it. How-
ever, when the user looks at the system while 
speaking, there is a considerable probability that 
she is actually addressing a bystander. 
3 The MonAMI Reminder 
This study is part of the 6th framework IP project 
MonAMI1. The goal of the MonAMI project is to 
develop and evaluate services for elderly and 
disabled people. Based on interviews with poten-
tial users in the target group, we have developed 
the MonAMI Reminder, a multimodal spoken 
dialogue system which can assist elderly and dis-
abled people in organising and initiating their 
daily activities (Beskow et al, 2009). The dia-
logue system uses Google Calendar as a back-
bone to answer questions about events. However, 
                                                 
1 http://www.monami.info/ 
310
it can also take the initiative and give reminders 
to the user.  
The MonAMI Reminder is based on the HIG-
GINS platform (Skantze, 2007). The architecture 
is shown in Figure 1. A microphone and a cam-
era are used for system input (speech recognition 
and head tracking), and a speaker and a display 
are used for system output (an animated talking 
head). This is pretty much a standard dialogue 
system architecture, with some exceptions. Di-
alogue management is split into a Discourse 
Modeller and an Action Manager, which consults 
the discourse model and decides what to do next. 
There is also an Attention and Interaction Con-
troller (AIC), which will be discussed next.  
 
Figure 1. The system architecture in the MonAMI 
Reminder. 
4 Attention and interaction model 
The purpose of the AIC is to act as a low level 
monitor and controller of the system?s speaking 
and attentional behaviour. The AIC uses a state-
based model to track the attentional and interac-
tional state of the user and the system, shown in 
Figure 2. The states shown in the boxes can be 
regarded as the combined state of the system 
(columns) and the user (rows)2. Depending on 
the combined state, events from input and output 
components will have different effects. As can be 
seen in the figure, some combination of states 
cannot be realised, such as the system and user 
speaking at the same time (if the user speaks 
while the system is speaking, it will automati-
cally change to the state INTERRUPTED). Of 
course, the user might speak while the system is 
speaking without the system detecting this, but 
                                                 
2 This is somewhat similar to the ?engagement state? used 
in Bohus & Horvitz (2009). 
the model should be regarded from the system?s 
perspective, not from an observer. 
The user?s attention is monitored using a cam-
era and an off-the-shelf head tracking software. 
As the user starts to look at the system, the state 
changes from NONATTENTIVE to ATTENTIVE. 
When the user starts to speak, a UserStartSpeak 
event from the ASR will trigger a change to the 
LISTENING state. The Action Manager might 
then trigger a SystemResponse event (together 
with what should be said), causing a change into 
the SPEAKING state. Now, if the user would look 
away while the system is speaking, the system 
would enter the HOLDING state ? the system 
would pause and then resume when the user 
looks back. If the user starts to speak while the 
system is speaking, the controller will enter the 
INTERRUPTED state. The Action Manager might 
then either decide to answer the new request, 
resume speaking (e.g., if there was just a back-
channel or the confidence was too low), or abort 
speaking (e.g., if the user told the system to shut 
up).  
There is also a CALLING state, in which the 
system might try to grab the user?s attention. 
This is very important for the current application 
when the system needs to remind the user about 
something.  
4.1 Incremental multimodal speech  
synthesis 
The speech synthesiser used must be capable of 
reporting the timestamp of each word in the 
synthesised string. These are two reasons for this. 
First, it must be possible to resume speaking 
after returning from the states INTERRUPTED and 
HOLDING. Second, the AIC is responsible for 
reporting what has actually been said by the 
system back to the Discourse Modeller for 
continuous self monitoring (there is a direct 
feedback loop as can be seen in Figure 1). This 
way, the Discourse Modeller may relate what the 
system says to what the user says on a high 
resolution time scale (which is necessary for 
handling phenomena such as backchannels, as 
discussed in Skantze & Schlangen, 2009).  
Currently, the system may pause and resume 
speaking at any word boundary and there is no 
specific prosodic modelling of these events. The 
synthesis of interrupted speech is something that 
we will need to improve. 
 
 
 
 
GALATEA:  
Discourse Modeller 
ASR 
PICKERING:  
Semantic Parsing 
Multimodal Speech  
Synthesis 
Utterance  
Generation 
Google  
Calendar 
Action  
Manager 
Attention and Inter-
action Controller 
Display Microphone Camera 
Head 
Tracker 
Speaker 
311
An animated talking head is shown on a display, 
synchronised with the synthesised speech 
(Beskow, 2003). The head is making small con-
tinuous movements (recorded from real human 
head movements), giving it a more life-like ap-
pearance. The head pose and facial gestures are 
triggered by the different states and events in the 
AIC, as can be seen in Figure 3. Thus, when the 
user approaches the system and starts to look at it, 
the system will look up, giving a clear signal that 
it is now attending to the user and ready to listen. 
5 Evaluation 
In the evaluation, we not only wanted to check 
whether the AIC model worked, but also to un-
derstand whether user attention could be effec-
tively modelled using head tracking. Similarly to 
Oh et al (2002), we wanted to compare ?look-to-
talk? with ?push-to-talk?. To do this, we used a 
human-human-computer dialogue setting, where 
a tutor was explaining the system to a subject 
(shown in Figure 4). Thus, the subject needed to 
frequently switch between speaking to the tutor 
and the system. A second version of the system 
was also implemented where the head tracker 
was not used, but where the subject instead 
pushed a button to switch between the attentional 
states (a sort-of push-to-talk). The tutor first ex-
plained both versions of the system to the subject 
and let her try both. The tutor gave the subjects 
hints on how to express themselves, but avoided 
to remind them about how to control the atten-
tion of the system, as this was what we wanted to 
test. After the introduction, the tutor gave the 
subject a task where both of them were supposed 
to find a suitable slot in their calendars to plan a 
dinner or lunch together. The tutor used a paper 
calendar, while the subject used the MonAMI 
Reminder. At the end of the experiment, the tutor 
interviewed the subject about her experience of 
using the system. 7 subjects (4 women and 3 men) 
were used in the evaluation, 3 lab members and 4 
elderly persons in the target group (recruited by 
the Swedish Handicap Institute).  
There was no clear consensus on which ver-
sion of the system was the best. Most subjects 
liked the head tracking version better when it 
worked but were frustrated when the head 
tracker occasionally failed. They reported that a 
combined version would perhaps be the best, 
where head pose could be the main method for 
handling attention, but where a button or a verbal 
call for attention could be used as a fall-back. 
When looking at the interaction from an objec-
tive point of view, however, the head tracking 
NonAttentive
Attentive Speaking
Listening
UserStartLook
SystemInitiative
SystemResponse
SystemStopSpeak
UserStopLook
Holding
Timeout
Interrupted
UserStartSpeak SystemIgnore (resume)
SystemResponse (restart)SystemIgnore
Calling
PausingSpeakingAttending
Not attending
Attending
Speaking
Not attending
SystemInitiative
UserStartLook 
SystemResponse
UserStartSpeak
UserStopLookUserStartLook (resume)
SystemStopSpeak
System
User
SystemAbortSpeak
Figure 2. The attention and interaction model. Dashed lines indicate events coming from input modules. Solid
lines indicate events from output modules. Note that some events and transitions are not shown in the figure. 
NonAttentive Attentive Listening SystemIgnore 
Figure 3. Examples of facial animations triggered by
the different states and events shown in Figure 2.  
 
312
version was clearly more successful in terms of 
number of misdirected utterances. When talking 
to the system, the subjects always looked at the 
system in the head tracking condition and never 
forgot to activate it in the push-to-talk condition. 
However, on average 24.8% of all utterances 
addressed to the tutor in the push-to-talk condi-
tion were picked up by the system, since the user 
had forgotten to deactivate it. The number of ut-
terances addressed to the tutor while looking at 
the system in the head tracking condition was 
significantly lower, only 5.1% on average (paired 
t-test; p<0.05).   
These findings partly contradict findings from 
previous studies, where head pose has not been 
that successful as a sole indicator when the user 
is looking at the system, as discussed in section 2 
above. One explanation for this might be that the 
subjects were explicitly instructed about how the 
system worked. Another explanation is the clear 
feedback (and entrainment) that the agent?s head 
pose provided. 
Two of the elderly subjects had no previous 
computer experience. During pre-interviews they 
reported that they were intimidated by com-
puters, and that they got nervous just thinking 
about having to operate them. However, after 
only a short tutorial session with the spoken in-
terface, they were able to navigate through a 
computerized calendar in order to find two 
empty slots. We think that having a human tutor 
that guides the user through their first interac-
tions with this kind of system is very important. 
One of the tutor?s tasks is to explain why the sys-
tem fails to understand out-of-vocabulary ex-
pressions. By doing this, the users? trust in the 
system is increased and they become less con-
fused and frustrated. We are confident that moni-
toring and modelling the user?s attention is a key 
component of spoken dialogue systems that are 
to be used in tutoring settings.   
Acknowledgements 
This research is supported by MonAMI, an Integrated 
Project under the European Commission?s 6th Frame-
work Program (IP-035147), and the Swedish research 
council project GENDIAL (VR #2007-6431). 
References 
Bakx, I., van Turnhout, K., & Terken, J. (2003). Fa-
cial orientation during multi-party interaction with 
information kiosks. In Proceedings of the Interact 
2003. 
Beskow, J., Edlund, J., Granstr?m, B., Gustafson, J., 
Skantze, G., & Tobiasson, H. (2009). The 
MonAMI Reminder: a spoken dialogue system for 
face-to-face interaction. In Proceedings of Inter-
speech 2009. 
Beskow, J. (2003). Talking heads - Models and appli-
cations for multimodal speech synthesis. Doctoral 
dissertation, KTH, Department of Speech, Music 
and Hearing, Stockholm, Sweden. 
Bohus, D., & Horvitz, E. (2009). Open-World Dialog: 
Challenges, Directions, and Prototype. In Proceed-
ings of IJCAI'2009 Workshop on Knowledge and 
Reasoning in Practical Dialogue Systems. Pasade-
na, CA. 
Katzenmaier, M., Stiefelhagen, R., Schultz, T., Rogi-
na, I., & Waibel, A. (2004). Identifying the Ad-
dressee in Human-Human-Robot Interactions 
based on Head Pose and Speech. In Proceedings of 
ICMI 2004. 
Maglio, P. P., Matlock, T., Campbell, C. S., Zhai, S., 
& Smith, B. A. (2000). Gaze and speech in atten-
tive user interfaces. In Proceedings of ICMI 2000.  
Oh, A., Fox, H., Van Kleek, M., Adler, A., Gajos, K., 
Morency, L-P., & Darrell, T. (2002). Evaluating 
Look-to-Talk: A Gaze-Aware Interface in a Col-
laborative Environment. In Proceedings of CHI 
2002. 
Skantze, G., & Schlangen, D. (2009). Incremental 
dialogue processing in a micro-domain. In Pro-
ceedings of EACL-09. Athens, Greece. 
Skantze, G. (2007). Error Handling in Spoken Dia-
logue Systems ? Managing Uncertainty, Grounding 
and Miscommunication. Doctoral dissertation, 
KTH, Department of Speech, Music and Hearing, 
Stockholm, Sweden. 
Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt, 
A. (2001). Eye gaze patterns in conversations: 
there is more to conversational agents than meets 
the eyes. In Proceedings of ACM Conf. on Human 
Factors in Computing Systems.   
Figure 4. The human-human-computer dialogue set-
ting used in the evaluation. The tutor is sitting on the 
left side and the subject on the right side 
313
The NICE Fairy-tale Game System1 
 
 
Joakim Gustafson, Linda Bell, Johan Boye, Anders Lindstr?m and Mats Wir?n 
TeliaSonera AB, 12386 Farsta, Sweden 
firstname.lastname@teliasonera.com 
 
                                                          
1 The work described in this paper was supported by the EU/HLT funded project NICE (IST-2001-35293), www.niceproject.com 
Abstract 
This paper presents the NICE fairy-tale game 
system, in which adults and children can 
interact with various animated characters in a 
3D world. Computer games is an interesting 
application for spoken and multimodal 
dialogue systems. Moreover, for the 
development of future computer games, 
multimodal dialogue has the potential to 
greatly enrichen the user?s experience. In this 
paper, we also present some requirements that 
have to be fulfilled to successfully integrate 
spoken dialogue technology with  a computer 
game application. 
1 Introduction 
The goal of the NICE project is to allow users of all 
ages to interact with lifelike conversational characters 
in a fairy-tale world inspired by the Danish author     
H C Andersen. To make these characters convincing 
in a computer game scenario, they have to possess 
conversational skills as well as the ability to perform  
physical actions in an interactive 3D world.  
What primarily distinguishes the NICE fairy-tale 
game system from other spoken dialogue systems is 
that the human-computer dialogue takes place within 
the context of an interactive computer game. 
However, spoken and multimodal dialogue is not 
supposed to be just an ?add-on? to the game, but the 
user?s primary means of progression through the 
story. The rationale for this is the great potential for 
more natural interaction we see in making methods 
from multimodal dialogue systems available in 
controlling gameplay. Potentially, spoken and 
multimodal interaction will make it possible to create 
a more engaging and immersive experience, or even 
facilitate the development of new kinds of computer 
games.    
Secondly, what makes NICE differ from typical 
spoken dialogue systems is the attempt to move away 
from strictly task-oriented dialogue. Instead, the 
interaction with the characters is domain-oriented. 
This means that the dialogue concerns different 
subplots in the fairy-tales, but without a clear goal-
orientation and without other demands than it being 
entertaining to the user. Furthermore, social 
interaction plays an important role in the fairy-tale 
world where the game takes place. By engaging in 
socializing with the animated characters, the user will 
find out things necessary to overcome various 
obstacles and enable progression through the story.  
Thirdly, a feature that differentiates NICE from 
other systems is that the main target user group of the 
system is children and young users. Previous studies 
have indicated that children employ partly different 
strategies when interacting with dialogue systems 
than adults do, and that there are also differences 
between age groups. For instance, younger children 
use less overt politeness markers and verbalize their 
frustration more than older children do (Arunachalam 
et al 2001). It has also been shown that children?s 
user experience is improved if they can communicate 
with a system with a ?personality? and that they 
benefit from being able to choose from several input 
modalities (Narayanan and Potamianos 2002). 
Furthermore, since many young people have a lot of 
experience with computer games, the believability of 
the dialogue characters and natural expressions will 
be critical aspects for the system?s success.  
Thus, computer games provide an excellent 
application area for research in spoken dialogue 
technology, requiring an advance of the state-of-the-
art in several fronts. Perhaps more importantly, game 
players will have a lot to gain from a successful 
incorporation of spoken dialogue technology into 
computer games. Today?s computer games are 
limited by the user?s input options, which are often 
restricted to direct manipulation and simple 
commands. In the development of the next generation 
of computer games, we believe that multimodal 
dialogue has the potential to greatly enrichen the 
user?s experience. For instance, spoken interaction 
makes it possible to refer to past events and objects 
currently not visible on the screen. Social interaction, 
which is already part of popular games such as SIMS, 
can be improved with spoken dialogue. Furthermore, 
speech and multimodal interaction supports 
cooperative games, where the user and character 
works together in solving a mutual problem.  
2 Spoken dialogue systems 
Spoken dialogue systems have so far mostly been 
designed with an overall goal to carry out a specific 
task, e.g. accessing time table information or ordering 
tickets (e.g. Zue et al 1991; Aust et al 1995). With 
task-oriented systems, it is possible to build domain 
models that can be used to predefine the language 
models and dialogue rules. The existence of 
predefined tasks makes it rather straight-forward to 
evaluate the performance of the dialogue system.  
Recent developments have made it possible to 
modify and extend the goals of spoken dialogue 
systems. Explorative dialogues, in which users are 
encouraged to browse through information without 
pursuing a specific task, have been presented by 
(Cassell et al 1999; Bell et al 2001). These 
dialogues still contain tasks to be solved during the 
interaction, e.g. giving constraints or receiving 
information about objects. However, explorative 
dialogue systems cannot be evaluated using merely 
the number of turns between different user 
interactions. A user who continues speaking with the 
system for a long time may do so because she is 
finding a lot of interesting information.  
Yet another type of dialogue system aims to 
present its users with an engaging and entertaining 
experience, without the presence of an external 
predetermined task. Conversational kiosks, such as 
August (Gustafson and Bell 2000) and MACK 
(Cassell et al 2002), encourage users to engage in 
social dialogues with embodied characters. Such 
dialogues are amenable to handling by a correctly 
designed dialogue system, since they primarily bring 
up features from the shared context. 
3 Interactive storytelling   
Interactivity has been defined as ?a kind of drama 
where the audience can modify the course of the 
actions [?] thus having an active role? (Szilas 1999). 
In interactive scenarios, the user helps the story 
unfold and may affect its course depending on his or 
her active participation. It has been argued that 
interactive storytelling will change computer 
entertainment by introducing better narrative content 
and allowing users to interfere with the progression 
of the storyline (Cavazza et al 2002). However, 
Young (2001) suggests that the drama manager of the 
system should put a limit to the user?s actions by not 
allowing interference that violates the overall 
narrative plan. Most interactive games developed so 
far allow users to intervene in the storytelling by 
acting on physical objects on the screen using direct 
maniputation (Young 2001; Cavazza et al 2002). 
Moreover, some systems allow users to interact with 
characters by means of written text input (Mateas and 
Stern 2002). In addition, Cavazza et al (2002) 
explored using a speech interface that handled 
isolated utterances from the user.  
4 The NICE fairy-tale game scenario  
The overall goal of the project is to provide users 
with an immersive dialogue experience in a 3D fairy-
tale world, see Figure 1. To this end, we have chosen 
to make spoken and multimodal dialogue the user?s 
primary vehicle of progressing through the story. It is 
also by verbal and non-verbal communication that the 
user can gain access to the goals and desires of the 
fairy-tale characters. This will be critical as the 
characters will ask the users to help them in solving 
problems. These problems either relate to objects that 
have to be manipulated or information that has to be 
retrieved from other fairy-tale characters.  
 
Figure 1. Cloddy Hans in the fairy-tale world. 
The fairy-tale domain was chosen because of its 
classic themes and stereotypical characters, well-
known to most adults as well as children. Some of 
these familiar characters are shown in Figure 2. 
 
Figure 2. The fairy-tale characters. 
To facilitate the progression through the story, we 
introduce Cloddy Hans, the user?s faithful assistant.  
Cloddy Hans?s character is conveyed to the users in 
the following way: he is a bit slow to understand, or 
so it seems. He sometimes appears hard of hearing 
and only understands spoken utterances and graphical 
gestures at a rather simple level. Cloddy Hans does 
not take a lot of initiatives, but is honest and anxious 
to try to help the user. In spite of his limited 
intellectual and perceptual capabilities, he may 
sometimes provide important clues through sudden 
flashes of insight.  
The user can ask Cloddy Hans to manipulate objects 
by referring to them verbally and/or by using the 
mouse. To understand the reason for not allowing 
users to directly manipulate objects on the screen, we 
have to recall what distinguishes NICE from other 
games, namely, spoken multimodal dialogue. We 
thus want to ensure that multimodal dialogue is 
appreciated by the user not just as an ?add-on? but as 
the primary means of progressing in the game. Our 
key to achieving this is to deliberately limit the 
capabilities of the key actors ? the user and Cloddy 
Hans ? in such a way that they can succeed only by 
cooperating through spoken multimodal dialogue. In 
other words, the user is intelligent but cannot himself 
affect objects in the world; Cloddy Hans on the other 
hand is a bit slow but capable of physical action 
according to what he gets told (and he may 
occasionally also provide tips to the user). 
The fairy-tale game will start with an introductory 
dialogue, in which the user meets Cloddy Hans in H C 
Andersen?s fairy-tale laboratory, see Figure 3. The 
simple task the user and Cloddy have to solve 
together is to take fairy-tale objects from a shelf and 
put them in the appropriate slot in a fairy-tale 
machine. Each slot is labelled with a symbol, which 
denotes the type of object supposed to go there, but 
since Cloddy Hans is not very bright, he needs help 
understanding these labels.  
 
Figure 3. Cloddy Hans in the fairy-tale lab 
The initial scenario is a ?grounding game? set in the 
context of a narrow task. In other words, its real 
purpose is a training session in which the user and 
Cloddy Hans agree on what different objects can be 
used for and how they can be referred to. This 
process also lets the player find out (by trial-and-
error) how to adapt in order to make it easier for the 
system to understand him or her. Moreover, Cloddy 
Hans sometimes explicitly instructs the user. For 
example, one lesson might be that it is sometimes 
more efficient to use multimodal input instead of just 
spoken utterances.  
The subsequent game in the fairy-tale world 
depends on what objects have been chosen by the 
user in the initial scenario. The advantage of this is 
that the objects are already grounded; for example, a 
sack of gold will be visually recognized by the player 
and there is an already agreed way of referring to it. 
5 System characteristics  
The game scenario as presented in the preceding 
section puts a number of requirements on the system. 
The scenario involves several animated characters, 
each with its own intended distinct personality. These 
personalities must be made explicit for the game 
player, and manifest themselves on all levels: from 
the appearance of the characters, their gestures and 
voices, choice of words, to their long-term behavior 
and overall role in the fairy-tale world. Furthermore, 
the characters need to be responsive, and be able to 
engage in conversation which makes sense to the 
player of the game. 
On the surface level, then, we need to have 
beautifully crafted animated characters and 
environments (these have been designed by the 
computer-game company Liquid Media). Each 
character must have its own voice that conveys the 
nature of that character?s personality, and be able to 
use prosodic cues to signal mood and emotions. To 
this end, a unit-selection speech synthesizer has been 
developed. Cloddy Hans has been given a slow, deep 
voice that goes along with his intended dunce 
personality. His repertoire of gestures and his style of 
walking also amplifies the impression of a slow-
witted but friendly person.  
On the input side, we need to recognize 
continuous, unconstrained speech for users of all 
ages. Previous studies have shown that children?s 
speech is associated with elevated error rates 
(Potamianos et al 1997; Oviatt and Adams 2000), 
making it necessary for Scansoft to retrain the NICE 
recognizer?s acoustic models. In addition, we need to 
take into account the disfluent speech patterns that 
are likely to arise, most probably because the users 
are unused to the situation or distracted by the virtual 
environment. On the other hand, not all input needs 
to be adequately interpreted. Much of the socializing 
utterances from the user can be handled in a 
satisfactory way by using shallow methods. 
Furthermore, the interpretation of the goal oriented 
interactions is simplified by the fact that the system 
knows which objects are visible on the screen and, 
more importantly, since it already knows what 
problems the fairy-tale characters has asked the user 
to help them to solve. Finally, the user also has the 
possibility of referring to objects using a pointing 
device. The software for the interpretation of this 
graphical input has been developed by LIMSI.  
The above characteristics have led us to design the 
system?s interpretation of user input in the following 
way. The system is implemented as a set of event-
driven processes that communicate via message-
passing. The architecture is essentially an extension 
of the one described in (Bell et al 2001). This 
architecture allows, among other things, for highly 
flexible turn-taking. When the user speaks, the 
system first tries to categorize the utterance as either 
social (needing only shallow interpretation) or goal-
oriented (needing further analysis). 
Finally, the long-term behavior of a character is 
decided by its set of internal goals and rules. A goal 
is essentially a predicate (that can be either true or 
false) concerning of the state of the virtual world. For 
instance, a character may have a goal to acquire a 
certain object or visit a certain place. If a given goal 
is not fulfilled (the predicate is false), the character 
will try to fulfill it. To this end it will use its set of 
rules, that define actions and dialogue acts that are 
likely to contribute to reaching the goal. 
6  Evaluation issues  
Task-oriented spoken dialogue systems are usually 
evaluated in terms of objective and subjective 
features. Objective criteria include the technical 
robustness and core functionality of the system 
components as well as system performance measures 
such as task completion rate. Subjective usability 
evaluations estimate features like naturalness and 
quality of the interactions, as well as user satisfaction 
reported in post-experimental interviews. However, 
many of these measures are simply not relevant for 
entertainment-type applications, where user 
satisfaction increases rather than decreases with task 
completion time. It can even be difficult to define 
what the completion of the task would be. In practice, 
computer games are usually evaluated by 
professional game reviewers and by the users in 
terms of number of copies sold.  
In the evaluation of the NICE fairy-tale game sales 
figures will not be possible to use, and several of the 
traditional objective measures are less relevant due to 
the domain. Instead, subjective measures involving 
features like ?narrative progression?, ?character 
believability?, and ?entertainment value?, will be 
used. They will be obtained off-line, by interviewing 
the users after their interactions and asking them to 
fill out questionnaires. Users will be asked how they 
perceived the quality of the actual interaction, as well 
as the personality of the fairy-tale characters. Expert 
evaluators, who will be able to replay the user 
interactions and inspect the system logs, will also be 
employed. Examples of evaluation questions to the 
experts include: ?Do the characters display 
meaningful roles and believable personalities that 
contribute to the story??, ?Do they succeed in 
signaling their level of understanding?, ?To what 
extent is the user able to affect the plot?? 
In order to be able to replay the user interactions with 
the fairy-tale system, all communication between the 
system modules are logged with time stamps. This 
will be a valuable tool both in the iterative system 
development and for system evaluations. At present, 
we are in the process of collecting data with the 
introductory game scenario. The data collected will 
be used to develop the subsequent scenarios in the 
fairy-tale game.  
References 
Arunachalam, S., D. Gould, E. Andersen, D. Byrd and S. S. 
Narayanan. (2001). Politeness and frustration language 
in child-machine interactions. Proceedings of 
Eurospeech: 2675-2678. 
Aust, H., M. Oerder, F. Seide and V. Steinbiss (1995). The 
Philips automatic train timetable information system. 
Speech Communication 17(3-4): 249-262. 
Bell, L., J. Boye and J. Gustafson (2001). Real-time 
handling of fragmented utterances. Proc. NAACL 2001 
workshop on Adaptation in Dialogue Systems. 
Cassell, J., T. Bickmore, M. Billinghurst, L. Campbell, K. 
Chang, H. Vilhj?lmsson and H. Yan (1999). 
Embodiment in conversational interfaces: Rea. 
Proceedings of CHI: 520-527. 
Cassell, J., T. Stocky, T. Bickmore, Y. Gao, Y. Nakano, K. 
Ryokai, D. Tversky, C. Vaucelle and H. Vilhjlmsson 
(2002). MACK: Media lab Autonomous 
Conversational Kiosk. Imagina 02. Monte Carlo. 
Cavazza, M., F. Charles and S. J. Mead (2002). Character-
based interactive storytelling. IEEE Intelligent 
Systems, Special issue on AI in Interactive 
Entertainment: 17-24. 
Gustafson, J. and L. Bell (2000). Speech technology on 
trial - Experiences from the August system. Natural 
Language Engineering 6(3-4): 273-286. 
Mateas, M. and A. Stern (2002). Architecture, authorial 
idioms and early observations of the interactive drama 
Facade. Technical report CM-CS-02-198. 
Narayanan, S. and A. Potamianos (2002). Creating 
conversational interfaces for children. IEEE 
Transactions on Speech and Audio Proc. 10(2): 65-78. 
Oviatt, S. and B. Adams (2000). Designing and evaluating 
conversational interfaces with animated characters. 
Embodied Conversational Agents. J. Cassell, J. 
Sullivan, S. Prevost and E. Churchill. MIT Press. 
Potamianos, A., S. Narayanan and S. Lee (1997). 
Automatic speech recognition for children. 
Proceedings of Eurospeech. 5: 2371-2374. 
Szilas, N. (1999). Interactive drama on the computer: 
beyond linear narrative. AAAI 1999 Fall Symposium 
on Narrative Intelligence. 
Young, R. M. (2001). An Overview of the Mimesis 
Architecture: Integrating Intelligent Narrative Control 
into an Existing Gaming Environment. Working Notes 
of the AAAI Spring Symposium on Artificial 
Intelligence and Interactive Entertainment. 
Zue, V., J. Glass, D. Goodline, H. Leung, M. Phillips, J. 
Polifroni and S. Seneff (1991). Integration of speech 
recognition and natural language processing in the 
MIT voyager system. Proc. ICASSP'91. Toronto. 
Proceedings of the SIGDIAL 2013 Conference, pages 366?368,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
The Map Task Dialogue System:  
A Test-bed for Modelling Human-Like Dialogue 
 
Raveesh Meena Gabriel Skantze Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
raveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.se  
  
 
  
 
Abstract 
The demonstrator presents a test-bed for 
collecting data on human?computer dia-
logue: a fully automated dialogue system 
that can perform Map Task with a user. 
In a first step, we have used the test-bed 
to collect human?computer Map Task di-
alogue data, and have trained various da-
ta-driven models on it for detecting feed-
back response locations in the user?s 
speech. One of the trained models has 
been tested in user interactions and was 
perceived better in comparison to a sys-
tem using a random model. The demon-
strator will exhibit three versions of the 
Map Task dialogue system?each using a 
different trained data-driven model of 
Response Location Detection.  
1 Introduction 
A common procedure in modelling human-like 
dialogue systems is to collect data on human?
human dialogue and then train models that pre-
dict the behaviour of the interlocutors. However, 
we think that it might be problematic to use a 
corpus of human?human dialogue as a basis for 
implementing dialogue system components. One 
problem is the interactive nature of the task. If 
the system produces a slightly different behav-
iour than what was found in the original data, 
this would likely result in a different behaviour 
in the interlocutor. Another problem is that hu-
mans are likely to behave differently towards a 
system as compared to another human (even if a 
more human-like behaviour is being modelled). 
Yet another problem is that much dialogue be-
haviour is optional and therefore makes the actu-
al behaviour hard to use as a gold standard. 
 
Figure 1: The Map Task system user interface 
To improve current systems, we need both a 
better understanding of the phenomena of human 
interaction, better computational models and bet-
ter data to build these models. An alternative ap-
proach that has proven to be useful is to train 
models on human?computer dialogue data col-
lected through Wizard-of-Oz studies (Dahlb?ck 
et al, 1993). However, the methodology might 
be hard to use when the issue under investigation 
is time-critical behaviour such as back-channels.  
A third alternative is to use a boot-strapping 
procedure, where more and more advanced (or 
human-like) versions of the system are built iter-
atively. After each iteration, users interact with 
the system and data is collected. This data is then 
used to train/improve data-driven models of in-
teraction in the system. A problem here, howev-
er, is how to build the first iteration of the sys-
tem, since many components, e.g., Automatic 
Speech Recognition (ASR), need some data to be 
useful at all.  
In this demonstration we present a test-bed for 
collecting data on time-critical human?computer 
dialogue phenomena: a fully automated dialogue 
system that can perform the Map Task with a 
366
user (Skantze, 2012). In a first step, following 
the boot-strapping procedure, we collected hu-
man?computer Map Task dialogue data using 
this test-bed and then trained various data-driven 
models on this data for detecting feedback re-
sponse locations in user?s speech. A trained 
model has been implemented and evaluated in 
interaction with users?in the same environment 
used for collecting the data (Meena et al, in 
press). The demonstrator will exhibit three ver-
sions of the Map Task dialogue system?each 
using a different trained data-driven model of 
Response Location Detection (RLD). 
2 The Map Task Dialogue System 
Map Task is a common experimental paradigm 
for studying human?human dialogue. In our set-
up, the user (the information giver) is given the 
task of describing a route on a map to the system 
(the information follower). The choice of Map 
Task is motivated partly because the system may 
allow the user to keep the initiative during the 
whole dialogue, and thus only produce responses 
that are not intended to take the initiative, most 
often some kind of feedback. Thus, the system 
might be described as an attentive listener.  
The basic components of the system can be 
seen in Figure 2. Dashed lines indicate compo-
nents that were not part of the first iteration of 
the system (used for data collection), but which 
have been used in the second iteration of the sys-
tem that uses a model trained on the collected 
data. To make the human?computer Map Task 
dialogue feasible without any full speech under-
standing we have implemented a trick: the user is 
presented with a map on a screen (see Figure 1) 
and instructed to move the mouse cursor along 
the route as it is being described. The user is told 
that this is for logging purposes, but the real rea-
son for this is that the system tracks the mouse 
position and thus knows what the user is current-
ly talking about. It is thereby possible to produce 
a coherent system behaviour without any speech 
recognition at all, only basic speech detection. 
This often results in a very realistic interaction1.  
The system uses a simple energy-based speech 
detector to chunk the user?s speech into inter-
pausal units (IPUs), that is, periods of speech that 
contain no sequence of silence longer than 200 
ms. Such a short threshold allows the system to 
give backchannels (seemingly) while the user is 
                                                 
1 An example video can be seen at 
http://www.youtube.com/watch?v=MzL-B9pVbOE. 
speaking or take the turn with barely any gap. 
The end of an IPU is a candidate for the RLD 
model to identify whether it is a Response Loca-
tion (RL). 
 
Prosodic 
analysis
Dialogue 
manager
Map
Window
Speech 
detector
Response
Location
Detector
Contextual
features
Prosodic
features
IPUs Response
Location
Mouse movements
Speech 
synthesizer
Response
ASR Syntactic features
 
Figure 2: The basic components of the system. 
Each time the RLD detects a RL, the dialogue 
manager produces a response, depending on the 
current state of the dialogue and the position of 
the mouse cursor. Table 1 shows an example 
Map Task dialogue illustrating the different types 
of responses the system could produce. The dia-
logue manager always starts with an Introduction 
and ends with an Ending, once the mouse cursor 
has reached the destination. Between these, it 
selects from the other responses (Reprise Frag-
ment, Clarification and Backchannel), partly 
randomly, but also depending on the length of 
the last user turn (longer turn triggers Repeat or 
Restart) and the current mouse location (e.g., a 
Guess if mouse hovers over the same area for 
some time). 
Table 1: An example Map Task dialogue between a 
user (U) and system (S) [User utterances with ASR 
error(s) are indicated with a *]  
S Introduction Could you help me to find my way to 
the bus stop? 
U  start from the department store 
S Backchannel yeah 
U  and eh 
S Guess Should I start by going west? 
U  yeah do that 
U  then you will get to a meadow and 
when you get to the meadow 
*you get to a meadow until you get 
to the meadow 
S Repeat Eh, could you repeat that? 
U  you go straight and you see a mead-
ow on your right side 
S Clarification  
Request  
A green field? 
U  ehm yeah a field 
S Backchannel mhm 
U  pass the meadow and turn right so 
you are going north  
*the meadow and turn right to your 
going north 
S Backchannel okay 
U  you get to a restaurant 
367
S Reprise 
Fragment 
A restaurant 
U  then you have to turn right and that 
will take you to 
*you have turned right and at will 
take you turn 
S Restart Eh, I think I lost you at the restau-
rant, could you take that again? 
?   
U  at the junction go south and then 
you will get to the bus stop 
S Ending Okay, thanks a lot. 
3 Data-driven models of RLD 
Using the system described in the previous 
section a corpus of 50 human?computer Map 
Task dialogue was collected and used to train a 
data-driven model of RLD. Since we didn?t have 
a sophisticated model of RLD during the first 
iteration a na?ve model was used. This model 
would wait for a random period between 0 and 
800 ms after an IPU ended. If no new IPUs were 
initiated during this period, a RL was detected. 
Each IPU in the corpus was then manually la-
belled as either Hold (a response would be inap-
propriate) or Respond (a response is expected) 
type. On this data various models were trained 
on online extractable features?covering syntax, 
context and prosody. Table 2 illustrates the per-
formance of the various models. Going a step 
further, model #6 was deployed in the Map Task 
dialogue system (with an ASR component) and 
evaluated in user interactions. The result sug-
gests that the trained model provide for smooth 
turn-transitions in contrast to the Random model 
(Meena et al, in press). 
Table 2: Performance of various models of RLD 
[NB: Na?ve Bayes; SVM: Support Vector Machine; 
Models with * will be exhibited in the demonstration] 
# RLD model % accuracy (on ASR results) 
1* Random 50.79% majority class baseline 
2 Prosody 64.5% (SVM learner) 
3 Context 64.8% (SVM learner) 
4* 
Prosody 
+ Context 
69.1% (SVM learner) 
5 Syntax 81.1% (NB learner) 
6* 
Syntax 
+ Prosody  
+ Context 
82.0 % (NB learner) 
4 Future applications 
The Map Task test-bed presented here has the 
potential for modelling other human-like conver-
sational behaviour in dialogue systems: 
Clarification strategies: by deploying explicit 
(did you mean turn right?) and implicit (a reprise 
such as turn right) or elliptical (?right??) clarifi-
cation forms in the grounding process one could 
investigate the efficiency and effectively of these 
human-like clarification strategies.  
User utterance completion: It has been sug-
gested that completion of user utterances by a 
dialogue system would result in human-like con-
versational interactions. However, completing 
user?s utterance at every opportunity may not be 
the best strategy (DeVault et al, 2009). The pre-
sented system could be used to explore when it is 
appropriate to do so. We have observed in our 
data that the system dialogue acts Guess (cf. Ta-
ble 1) and Reprise often helped the dialogue pro-
ceed further ? by completing user utterances ? 
when the user had difficulty describing a land-
mark on a route. 
Visual cues: the system could be integrated in 
a robotic head, such as Furhat (Al Moubayed et 
al., 2013), and visual cues from the user could be 
used for improving the current model of RLD. 
This could be used further to explore the use of 
extra-linguistic system behaviours, such as head 
nods and facial gestures, as feedback responses. 
Acknowledgement 
This work is supported by the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237) 
References 
Al Moubayed, S., Skantze, G., & Beskow, J. (2013). 
The Furhat Back-Projected Humanoid Head - Lip 
reading, Gaze and Multiparty Interaction. Interna-
tional Journal of Humanoid Robotics, 10(1). 
Dahlb?ck, N., J?nsson, A., & Ahrenberg, L. (1993). 
Wizard of Oz studies ?  why and how. In Proceed-
ings from the 1993 International Workshop on In-
telligent User Interfaces (pp. 193-200).  
DeVault, D., Sagae, K., & Traum, D. (2009). Can I 
Finish? Learning When to Respond to Incremental 
Interpretation Results in Interactive Dialogue. In 
Proceedings of SIGdial (pp. 11-20). London, UK. 
Meena, R., Skantze, G., & Gustafson, J. (in press). A 
Data-driven Model for Timing Feedback in a Map 
Task Dialogue System. To be published in 14th 
Annual Meeting of the Special Interest Group on 
Discourse and Dialogue - SIGdial. Metz, France. 
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
368
Proceedings of the SIGDIAL 2013 Conference, pages 375?383,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
 
A Data-driven Model for Timing Feedback 
in a Map Task Dialogue System 
 
Raveesh Meena Gabriel Skantze Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
raveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.se 
 
  
 
Abstract 
We present a data-driven model for de-
tecting suitable response locations in the 
user?s speech. The model has been 
trained on human?machine dialogue data 
and implemented and tested in a spoken 
dialogue system that can perform the 
Map Task with users. To our knowledge, 
this is the first example of a dialogue sys-
tem that uses automatically extracted 
syntactic, prosodic and contextual fea-
tures for online detection of response lo-
cations. A subjective evaluation of the 
dialogue system suggests that interac-
tions with a system using our trained 
model were perceived significantly better 
than those with a system using a model 
that made decisions at random. 
1 Introduction 
Traditionally, dialogue systems have rested on a 
very simple model for turn-taking, where the sys-
tem uses a fixed silence threshold to detect the 
end of the user?s utterance, after which the sys-
tem responds. However, this model does not cap-
ture human-human dialogue very accurately; 
sometimes a speaker just hesitates and no turn-
change is intended, sometimes the turn changes 
after barely any silence (Sacks et al, 1974). 
Therefore, such models can result in systems that 
interrupt the user or are perceived as unrespon-
sive. Related to the problem of turn-taking is that 
of backchannels (Yngve, 1970).  Backchannel 
feedback ? short acknowledgements such as uh-
huh or mm-hm ? are used by human interlocutors 
to signal continued attention to the speaker, 
without claiming the floor. If a dialogue system 
should be able to manage smooth turn-taking and 
back-channelling, it must be able to first identify 
suitable locations in the user?s speech to do so.  
Duncan (1972) found that human interlocutors 
continuously monitor several cues, such as con-
tent, syntax, intonation, paralanguage, and body 
motion, in parallel to manage turn-taking. Simi-
lar observations have been made in various other 
studies investigating the turn-taking and back-
channelling phenomena in human conversations. 
Ward (1996) has suggested that a low pitch re-
gion is a good cue that backchannel feedback is 
appropriate. On the other hand, Koiso et al 
(1998) have argued that both syntactic and pro-
sodic features make significant contributions in 
identifying turn-taking and back-channelling rel-
evant places. Cathcart et al (2003) have shown 
that syntax in combination with pause duration is 
a strong predictor for backchannel continuers.  
Gravano & Hirschberg (2009) observed that the 
likelihood of occurrence of a backchannel in-
creases with the number of syntactic and prosod-
ic cues conjointly displayed by the speaker. 
However, there is a general lack of studies on 
how such models could be used online in dia-
logue systems and to what extent that would im-
prove the interaction. There are two main prob-
lems in doing so. First, the data used in the stud-
ies mentioned above are from human?human 
dialogue and it is not obvious to what extent the 
models derived from such data transfers to hu-
man?machine dialogue. Second, many of the 
features used were manually extracted. This is 
especially true for the transcription of utterances, 
but several studies also rely on manually anno-
tated prosodic features.  
In this paper, we present a data-driven model 
of what we call Response Location Detection 
(RLD), which is fully online. Thus, it only relies 
375
on automatically extractable features?covering 
syntax, prosody and context. The model has been 
trained on human?machine dialogue data and has 
been implemented in a dialogue system that is in 
turn evaluated with users. The setting is that of a 
Map Task, where the user describes the route and 
the system may respond with for example 
acknowledgements and clarification requests.  
2 Background 
Two influential theories that have examined the 
turn-taking mechanism in human conversations 
are the signal-based mechanism of Duncan 
(1972) and the rule-based mechanism proposed 
by Sacks (1974). According to Duncan, ?the 
turn-taking mechanism is mediated through sig-
nals composed of clear-cut behavioural cues, 
considered to be perceived as discrete?. Duncan 
identified six discrete behavioural cues that a 
speaker may use to signal the intent to yield the 
turn. These behavioural cues are: (i) any devia-
tion from the sustained intermediate pitch level; 
(ii) drawl on the final syllable of a terminal 
clause; (iii) termination of any hand gesticulation 
or the relaxation of tensed hand position?during 
a turn; (iv) a stereotyped expression with trailing 
off effect; (v) a drop in pitch and/or loudness; 
and (vi) completion of a grammatical clause. Ac-
cording to the rule-based mechanism of Sacks 
(1974) turn-taking is regulated by applying rules 
(e.g. ?one party at a time?) at Transition-
Relevance Places (TRPs)?possible completion 
points of basic units of turns, in order to mini-
mize gaps and overlaps. The basic units of turns 
(or turn-constructional units) include sentential, 
clausal, phrasal, and lexical constructions. 
Duncan (1972) also suggested that speakers 
may display behavioural cues either singly or 
together, and when displayed together they may 
occur either simultaneously or in tight sequence. 
In his analysis, he found that the likelihood that a 
listener attempts to take the turn is higher when 
the cues are conjointly displayed across the vari-
ous modalities.  
While these theories have offered a function-
based account of turn-taking, another line of re-
search has delved into corpora-based techniques 
to build models for detecting turn-transition and 
feedback relevant places in speaker utterances.  
Ward (1996) suggested that a 110 millisecond 
(ms) region of low pitch is a fairly good predic-
tor for back-channel feedback in casual conver-
sational interactions. He also argued that more 
obvious factors, such as utterance end, rising in-
tonation, and specific lexical items, account for 
less than they seem to. He contended that proso-
dy alone is sometimes enough to tell you what to 
say and when to say. 
In their analysis of turn-taking and backchan-
nels based on prosodic and syntactic features, in 
Japanese Map Task dialogs, Koiso et al (1998) 
observed that some part-of-speech (POS) fea-
tures are strong syntactic cues for turn-change, 
and some others are strongly associated with no 
turn-change. Using manually extracted prosodic 
features for their analysis, they observed that 
falling and rising F0 patterns are related to 
changes of turn, and flat, flat-fall and rise-fall 
patterns are indications of the speaker continuing 
to speak. Extending their analysis to backchan-
nels, they asserted that syntactic features, such as 
filled pauses, alone might be sufficient to dis-
criminate when back-channelling is inappropri-
ate, whereas presence of backchannels is always 
preceded by certain prosodic patterns. 
Cathcart et al (2003) presented a shallow 
model for predicting the location of backchannel 
continuers in the HCRC Map Task Corpus 
(Anderson et al, 1991). They explored features 
such as POS, word count in the preceding speak-
er turn, and silence pause duration in their mod-
els. A model based on silence pause only insert-
ed a backchannel in every speaker pause longer 
than 900 ms and performed better than a word 
model that predicted a backchannel every sev-
enth word. A tri-gram POS model predicted that 
nouns and pronouns before a pause are the two 
most important cues for predicting backchannel 
continuers. The combination of the tri-gram POS 
model and pause duration model offered a five-
fold improvement over the others. 
Gravano & Hirschberg (2009) investigated 
whether backchannel-inviting cues differ from 
turn-yielding cues. They examined a number of 
acoustic features and lexical cues in the speaker 
utterances preceding smooth turn-changes, back-
channels, and holds. They have identified six 
measureable events that are strong predictors of a 
backchannel at the end of an inter-pausal unit: (i) 
a final rising intonation; (ii) a higher intensity 
level; (iii) a higher pitch level; (iv) a final POS 
bi-gram equal to ?DT NN?, ?JJ NN?, or ?NN 
NN?; (v) lower values of noise-to-harmonic rati-
os; and (vi) a longer IPU duration. They also ob-
served that the likelihood of a backchannel in-
creases in quadratic fashion with the number of 
cues conjointly displayed by the speaker. 
When it comes to using these features for 
making turn-taking decisions in dialogue sys-
376
tems, there is however, very little related work. 
One notable exception is Raux & Eskenazi 
(2008) who presented an algorithm for dynami-
cally setting endpointing silence thresholds based 
on features from discourse, semantics, prosody, 
timing, and speaker characteristics. The model 
was also applied and evaluated in the Let?s Go 
dialogue system for bus timetable information. 
However, that model only predicted the end-
pointing threshold based on the previous interac-
tion up to the last system utterance, it did not 
base the decision on the current user utterance to 
which the system response is to be made. 
In this paper, we train a model for online Re-
sponse Location Detection that makes a decision 
whether to respond at every point where a very 
short silence (200 ms) is detected. The model is 
trained on human?machine dialogue data taken 
from a first set of interactions with a system that 
used a very na?ve policy for Response Location 
Detection. The trained model is then applied to 
the same system, which has allowed us to evalu-
ate the model online in interaction with users.  
3 A Map Task dialogue system 
In a previous study, we presented a fully auto-
mated spoken dialogue system that can perform 
the Map Task with a user (Skantze, 2012). Map 
Task is a common experimental paradigm for 
studying human-human dialogue, where one sub-
ject (the information giver) is given the task of 
describing a route on a map to another subject 
(the information follower). In our case, the user 
acts as the giver and the system as the follower. 
The choice of Map Task is motivated partly be-
cause the system may allow the user to keep the 
initiative during the whole dialogue, and thus 
only produce responses that are not intended to 
take the initiative, most often some kind of feed-
back. Thus, the system might be described as an 
attentive listener.  
Implementing a Map Task dialogue system 
with full speech understanding would indeed be 
a challenging task, given the state-of-the-art in 
automatic recognition of conversational speech. 
In order to make the task feasible, we have im-
plemented a trick: the user is presented with a 
map on a screen (see Figure 1) and instructed to 
move the mouse cursor along the route as it is 
being described. The user is told that this is for 
logging purposes, but the real reason for this is 
that the system tracks the mouse position and 
thus knows what the user is currently talking 
about. It is thereby possible to produce a coher-
ent system behaviour without any speech recog-
nition at all, only basic speech detection. This 
often results in a very realistic interaction, as 
compared to what users are typically used to 
when interacting with dialogue systems?in our 
experiments, several users first thought that there 
was a hidden operator behind it1.  
 
 
Figure 1: The user interface, showing the map. 
The basic components of the system can be 
seen in Figure 2. Dashed lines indicate compo-
nents that were not part of the first iteration of 
the system (used for data collection), but which 
have been used in the model presented and eval-
uated here. The system uses a simple energy-
based speech detector to chunk the user?s speech 
into inter-pausal units (IPUs), that is, periods of 
speech that contain no sequence of silence longer 
than 200 ms. Such a short threshold allows the 
system to give backchannels (seemingly) while 
the user is speaking or take the turn with barely 
any gap. Similar to Gravano & Hirschberg 
(2009) and Koiso et al (1998), we define the end 
of an IPU as a candidate for the Response Loca-
tion Detection model to identify as a Response 
Location (RL). We use the term turn to refer to a 
sequence of IPUs which do not have any re-
sponses between them. 
 
 
Figure 2: The basic components of the system. 
                                                 
1 An example video can be seen at 
http://www.youtube.com/watch?v=MzL-B9pVbOE. 
Prosodic 
analysis
Dialogue 
manager
Map
Windo
Speech 
det ctor
Response
Location
Detector
Contextual
features
Prosodic
features
IPUs Response
Location
Mouse movements
Speech 
synthesizer
Response
ASR Syntactic features
377
Each time the RLD model detected a RL, the 
dialogue manager produced a Response, depend-
ing on the current state of the dialogue and the 
position of the mouse cursor. Table 1 shows the 
different types of responses the system could 
produce. The dialogue manager always started 
with an Introduction and ended with an Ending, 
once the mouse cursor had reached the destina-
tion. Between these, it selected from the other 
responses, partly randomly, but also depending 
on the length of the last user turn and the current 
mouse location. Longer turns often led to Restart 
or Repetition Requests, thus discouraging longer 
sequences of speech that did not invite the sys-
tem to respond. If the system detected that the 
mouse had been at the same place over a longer 
time, it pushed the task forward by making a 
Guess response. We also wanted to explore other 
kinds of feedback than just backchannels, and 
therefore added short Reprise Fragments and 
Clarification Requests (see for example Skantze 
(2007) for a discussion on these).  
Table 1: Different responses from the system 
Introduction ?Could you help me to find my way to 
the train station?? 
Backchannel ?Yeah?, ?Mhm?, ?Okay?, ?Uhu? 
Reprise  
Fragment  
?A station, yeah? 
Clarification  
Request  
?A station?? 
Restart ?Eh, I think I lost you at the hotel, how 
should I continue from there?? 
Repetition  
Request  
?Sorry, could you take that again?? 
Guess ?Should I continue above the church?? 
Ending ?Okay, thanks a lot.? 
 
A na?ve version of the system was used to col-
lect data. Since we initially did not have any so-
phisticated model of RLD, it was simply set to 
wait for a random period between 0 and 800 ms 
after an IPU ended. If no new IPUs were initiated 
during this period, a RL was detected, resulting 
in random response delays between 200 and 
1000 ms. Ten subjects participated in the data 
collection. Each subject did 5 consecutive tasks 
on 5 different maps, resulting in a total of 50 dia-
logues. 
Each IPU in the corpus was manually annotat-
ed into three categories: Hold (a response would 
be inappropriate), Respond (a response is ex-
pected) and Optional (a response would not be 
inappropriate, but it is perfectly fine not to re-
spond). Two human-annotators labelled the cor-
pus separately. For all the three categories the 
kappa score was 0.68, which is substantial 
agreement (Landis & Koch, 1977). Since only 
2.1% of all the IPUs in the corpus were identified 
for category Optional, we excluded them from 
the corpus and focused on the Respond and Hold 
categories only. The data-set contains 2272 IPUs 
in total; the majority of which belong to the class 
Respond (50.79%), which we take as our majori-
ty class baseline. Since the two annotators agreed 
on 87.20% of the cases, this can be regarded as 
an approximate upper limit for the performance 
expected from a model trained on this data. 
In (Skantze, 2012), we used this collected data 
to build an offline model of RLD that was 
trained on prosodic and contextual features. In 
this paper, we extend this work in three ways. 
First, we bring in Automatic Speech Recognition 
(ASR) for adding syntactic features to the model. 
Second, the model is implemented as a module 
in the dialogue system so that it can extract the 
prosodic features online. Third, we evaluate the 
performance of our RLD model against a base-
line system that makes a random choice, in a dia-
logue system interacting with users.  
In contrast to some related work (e.g. Koiso et 
al., 1998), we do not discriminate between loca-
tions for backchannels and turn-changes. Instead, 
we propose a general model for response loca-
tion detection. The reason for this is that the sys-
tem mostly plays the role of an attentive listener 
that produces utterances that are not intended to 
take the initiative or claim the floor, but only to 
provide different types of feedback (cf. Table 1). 
Thus, suitable response locations will be where 
the user invites the system to give feedback, re-
gardless of whether the feedback is simply an 
acknowledgement that encourages the system to 
continue, or a clarification request. Moreover, it 
is not clear whether the acknowledgements the 
system produces in this domain should really be 
classified as backchannels, since they do not only 
signal continued attention, but also that some 
action has been performed (cf. Clark, 1996). In-
deed, none of the annotators felt the need to mark 
relevant response locations within IPUs.  
4 A data-driven model for response lo-
cation detection 
The human?machine Map Task corpus described 
in the previous section was used for training a 
new model of RLD. We describe below how we 
extracted prosodic, syntactic and contextual fea-
tures from the IPUs. We test the contribution of 
these feature categories?individually as well as 
378
in combination, in classifying a given IPU as 
either Respond or Hold type. For this we explore 
the Na?ve Bayes (NB) and Support Vector Ma-
chine (SVM) algorithms in the WEKA toolkit 
(Hall et al, 2009). All results presented here are 
based on 10-fold cross-validation. 
4.1 Prosodic features 
Pitch and intensity (sampled at 10 ms) for each 
IPU were extracted using ESPS in 
Wavesurfer/Snack (Sj?lander & Beskow, 2000). 
The values were transformed to log scale and z-
normalized for each user. The final 200 ms 
voiced region was then identified for each IPU. 
For this region, the mean pitch, slope of the 
pitch (using linear regression)?in combination 
with the correlation coefficient r for the regres-
sion line, were used as features. In addition to 
these, we also used the duration of the voiced 
region as a feature. The last 500 ms of each IPU 
were used to obtain the mean intensity (also z-
normalised). Table 2 illustrates the power of pro-
sodic features, individually as well as collective-
ly (last row), in classifying an IPU as either Re-
spond or Hold type. Except for mean intensity all 
other features individually provide an improve-
ment over the baseline. The best accuracy, 
64.5%, was obtained by the SVM algorithm us-
ing all the prosodic features. This should be 
compared against the baseline of 50.79%. 
Table 2: Percentage accuracy of prosodic features 
in detecting response locations 
 Algorithm 
Feature(s) NB  SVM  
Mean pitch 60.3 62.7 
Pitch slope 59.0 57.8 
Duration 58.1 55.6 
Mean intensity 50.3 52.2 
Prosody (all combined) 63.3 64.5 
4.2 Syntactic features 
As lexico-syntactic features, we use the word 
form and part-of-speech tag of the last two 
words in an IPU. All the IPUs in the Map Task 
corpus were manually transcribed. To obtain the 
part-of-speech tag we used the LBJ toolkit 
(Rizzolo & Roth, 2010). Column three in Table 3 
illustrates the discriminatory power of syntactic 
features?extracted from the manual transcrip-
tion of the IPUs. Using the last two words and 
their POS tags, the Na?ve Bayes learner achieves 
the best accuracy of 83.6% (cf. row 7). While 
POS tag is a generic feature that would enable 
the model to generalize, using word form as a 
feature has the advantage that some words, such 
as yeah, are strong cues for predicting the Re-
spond class, whereas pause fillers, such as ehm, 
are strong predictors of the Hold class. 
Table 3: Percentage accuracy of syntactic features 
in detecting response locations 
  
Manual  
transcriptions 
ASR  
results 
# Feature(s) NB SVM NB SVM  
1 Last word (Lw) 82.5 83.9 80.8 80.9 
2 
Last word part-of-
speech (Lw-POS)  
79.4 79.5 74.5 74.6 
3 
Second last word 
(2ndLw) 
68.1 67.7 67.1 67.0 
4 
Second last word 
Part-of-speech 
(2ndLw-POS) 
66.9 66.5 65.8 66.1 
5 Lw + 2ndLw 82.3 81.5 80.8 80.6 
6 
Lw-POS 
+ 2ndLw-POS 
80.3 80.5 75.4 74.87 
7 
Lw + 2ndLw 
+ Lw-POS 
+ 2ndLw-POS 
83.6 81.7 79.7 79.7 
8 
Last word diction-
ary (Lw-Dict) 
83.4 83.4 78.0 78.0 
9 
Lw-Dict 
+ 2ndLw-Dict 
81.2 82.6 76.1 77.7 
10 
Lw + 2ndLw 
+ Lw-Conf 
+ 2ndLw-Conf  
82.3 81.5 81.1 80.5 
 
An RLD model for online predictions requires 
that the syntactic features are extracted from the 
output of a speech recogniser. Since speech 
recognition is prone to errors, an RLD model 
trained on manual transcriptions alone would not 
be robust when making predictions in noisy data. 
Therefore we train our RLD model on actual 
speech recognised results. To achieve this, we 
did an 80-20 split of the Map Task corpus into 
training and test sets respectively. The transcrip-
tions of IPUs in the training set were used to 
train the language model of the Nuance 9 ASR 
system. The audio recordings of the IPUs in the 
test set were then recognised by the trained ASR 
system. After performing five iterations of split-
ting, training and testing, we had obtained the 
speech recognised results for all the IPUs in the 
Map Task corpus. The mean word error rate for 
the five iterations was 17.22% (SD = 3.8%).  
Column four in Table 3 illustrates the corre-
sponding performances of the RLD model 
trained on syntactic features extracted from the 
best speech recognized hypotheses for the IPUs. 
With the introduction of a word error rate of 
17.22%, the performances of all the models us-
379
ing only POS tag feature decline. The perfor-
mances are bound to decline further with in-
crease in ASR errors. This is because the POS 
tagger itself uses the left context to make POS 
tag predictions. With the introduction of errors in 
the left context, the tagger?s accuracy is affected, 
which in turn affects the accuracy of the RLD 
models. However, this decline is not significant 
for models that use word form as a feature. This 
suggests that using context independent lexico-
syntactic features would still offer better perfor-
mance for an online model of RLD. We therefore 
also created a word class dictionary, which gen-
eralises the words into domain-specific classes in 
a simple way (much like a class-based n-gram 
model). Row 9 in Table 3 illustrates that using a 
dictionary instead of POS tag (cf. row 6) im-
proves the performance of the online model. We 
have also explored the use of word-level confi-
dence scores (Conf) from the ASR as another 
feature that could be used to reinforce a learning 
algorithm?s confidence in trusting the recognised 
words (cf. row 10 in Table 3).  
The best accuracy, 81.1%, for the online mod-
el of RLD is achieved by the Na?ve Bayes algo-
rithm using the features word form and confi-
dence score, for last two words in an IPU. 
4.3 Contextual features 
We have explored three discourse context fea-
tures: turn and IPU length (in words and se-
conds) and last system dialogue act. Dialogue 
act history information have been shown to be 
vital for predicting a listener response when the 
speaker has just responded to the listener?s clari-
fication request (Koiso et al (1998); Cathcart et 
al. 2003; Gravano & Hirschberg (2009); Skantze, 
2012). To verify if this rule holds in our corpus, 
we extracted turn length and dialogue act labels 
for the IPUs, and trained a J48 decision tree 
learner. The decision tree achieved an accuracy 
of 65.7%. One of the rules learned by the deci-
sion tree is: if the last system dialogue act is 
Clarification or Guess (cf. Table 1), and the turn 
word count is less than equal to 1, then Respond. 
In other words, if the system had previously 
sought a clarification, and the user has responded 
with a yes/no utterance, then a system response 
is expected. A more general rule in the decision 
tree suggests that: if the last system dialogue act 
was a Restart or Repetition Request, and if the 
turn word count is more than 4 then Respond 
otherwise Hold. In other words, the system 
should wait until it gets some amount of infor-
mation from the user.  
Table 4 illustrates the power of these contex-
tual features in discriminating IPUs, using the 
NB and the SVM algorithms. All the features 
individually provide improvement over the base-
line of 50.79%. The best accuracy, 64.8%, is 
achieved by the SVM learner using the features 
last system dialogue act and turn word count. 
Table 4: Percentage accuracy of contextual features 
in detecting response locations 
 
Manual 
transcriptions 
ASR  
results 
Features NB  SVM  NB  SVM  
Last system dialogue act 54.1 54.1 54.1 54.1 
Turn word count 61.8 61.9 61.5 62.9 
Turn length in seconds 58.4 58.8 58.4 58.8 
IPU word count 58.4 58.2 58.1 59.3 
IPU length in seconds 57.3 61.2 57.3 61.2 
Last system dialogue act 
+ Turn word count 
59.9 64.5 60.4 64.8 
 
4.4 Combined model 
Table 5 illustrates the performances of the RLD 
model using various feature category combina-
tions. It could be argued that the discriminatory 
power of prosodic and contextual feature catego-
ries is comparable. A model combining prosodic 
and contextual features offers an improvement 
over their individual performances. Using the 
three feature categories in combination, the Na-
?ve Bayes learner provided the best accuracy: 
84.6% (on transcriptions) and 82.0% (on ASR 
output). These figures are significantly better 
than the majority class baseline of 50.79% and 
approach the expected upper limit of 87.20% on 
the performance.  
Table 5: Percentage accuracy of combined models  
 
Manual  
transcriptions 
ASR  
results 
Feature categories NB SVM NB SVM 
Prosody  63.3 64.5 63.3 64.5 
Context  59.9 64.5 60.4 64.8 
Syntax  82.3 81.5 81.1 80.5 
Prosody + Context 67.7 70.2 67.5 69.1 
Prosody + Context 
+ Syntax 
84.6 77.2 82.0 77.1 
  
Table 6 illustrates that the Na?ve Bayes model 
for Response Location Detection trained on 
combined syntactic, prosodic and contextual fea-
tures, offers better precision (fraction of correct 
decisions in all model decisions) and recall (frac-
tion of all relevant decisions correctly made) in 
comparison to the SVM model. 
380
Table 6: Precision and Recall scores of the NB and 
the SVM learners trained on combined prosodic, con-
textual and syntactic features. 
Prediction class 
Precision (in %) Recall (in %) 
NB  SVM  NB  SVM  
Respond 81.0  73.0 87.0 84.0 
Hold 85.0 81.0 78.0 68.0 
 
5 User evaluation 
In order to evaluate the usefulness of the com-
bined model, we have performed a user evalua-
tion where we test the trained model in the Map 
Task dialogue system that was used to collect the 
corpus (cf. section 3). A version of the dialogue 
system was created that uses a Random model, 
which makes a random choice between Respond 
and Hold. The Random model thus approximates 
our majority class baseline (50.79% for Re-
spond). Another version of the system used the 
Trained model ? our data-driven model ? to 
make the decision. For both models, if the deci-
sion was a Hold, the system waited 1.5 seconds 
and then responded anyway if no more speech 
was detected from the user. 
We hypothesize that since the Random model 
makes random choices, it is likely to produce 
false-positive responses (resulting in overlap in 
interaction) as well as false-negative responses 
(resulting in gap/delayed response) in equal pro-
portion. The Trained model on the other hand 
would produce fewer overlaps and gaps.  
In order to evaluate the models, 8 subjects (2 
female, 6 male) were asked to perform the Map 
Task with the two systems. Each subject per-
formed five dialogues (which included 1 trial and 
2 tests) with each version of the system. This 
resulted in 16 test dialogues each for the two sys-
tems. The trial session was used to allow the us-
ers to familiarize themselves with the dialogue 
system. Also, the audio recording of the users? 
speech from this session was used to normalize 
the user pitch and intensity for the online prosod-
ic extraction. The order in which the systems and 
maps were presented to the subjects was varied 
over the subjects to avoid any ordering effect in 
the analysis.  
The 32 dialogues from the user evaluation 
were, on average, 1.7 min long (SD = 0.5 min). 
The duration of the interactions with the Random 
and the Trained model were not significantly 
different. A total of 557 IPUs were classified by 
the Random model whereas the Trained model 
classified 544 IPUs. While the Trained model 
classified 57.7% of the IPUs as Respond type the 
Random model classified only 48.29% of the 
total IPUs as Respond type, suggesting that the 
Random model was somewhat quieter.  
It turned out that it was very hard for the sub-
jects to perform the Map Task and at the same 
time make a valid subjective comparison be-
tween the two versions of the system, as we had 
initially intended. Therefore, we instead con-
ducted another subjective evaluation to compare 
the two systems. We asked subjects to listen to 
the interactions and press a key whenever a sys-
tem response was either lacking or inappropriate. 
The subjects were asked not to consider how the 
system actually responded, only evaluate the tim-
ing of the response. 
Eight users participated in this subjective 
judgment task. Although five of these were from 
the same set of users who had performed the 
Map Task, none of them got to judge their own 
interactions. The judges listened to the Map Task 
interactions in the same order as the users had 
interacted, including the trial session. Whereas it 
had been hard for the subjects who participated 
in the dialogues to characterize the two versions 
of the system, almost all of the judges could 
clearly tell the two versions apart. They stated 
that the Trained system provided for a smooth 
flow of dialogue. The timing of the IPUs was 
aligned with the timing of the judges? key-
presses in order to measure the numbers of IPUs 
that had been given inappropriate response deci-
sions. The results show that for the Random 
model, 26.75% of the RLD decisions were per-
ceived as inappropriate, whereas only 11.39% of 
the RLD decisions for the Trained model were 
perceived inappropriate. A two-tailed two-
sample t-test for difference in mean of the frac-
tion of inappropriate instances (key-press count 
divided by IPU count) for Random and Trained 
model show a clear significant difference (t = 
4.66, dF = 30, p < 0.001). 
We have not yet analysed whether judges pe-
nalized false-positives or false-negatives to a 
larger extent, this is left to future work. Howev-
er, some judges informed us that they did not 
penalize delayed response (false-negative), as the 
system eventually responded after a delay. In the 
context of a system trying to follow a route de-
scription, such delays could sometimes be ex-
pected and wouldn?t be unnatural. For other 
types of interactions (such as story-telling), such 
delays may on the other hand be perceived as 
unresponsive. Thus, the balance between false-
positives and false-negatives might need to be 
tuned depending on the topic of the conversation.  
381
6 Conclusion  
We have presented a data-driven model for de-
tecting response locations in the user?s speech. 
The model has been trained on human?machine 
dialogue data and has been integrated and tested 
in a spoken dialogue system that can perform the 
Map Task with users. To our knowledge, this is 
the first example of a dialogue system that uses 
automatically extracted syntactic, prosodic and 
contextual features for making online detection 
of response locations. The models presented in 
earlier works have used only prosody (Ward, 
1996), or combinations of syntax and prosody 
(Koiso et al, 1998), syntax and context (Cathcart 
et al, 2003), prosody and context (Skantze, 
2012), or prosody, context and semantics (Raux 
& Eskenazi (2008). Furthermore, we have evalu-
ated the usefulness of our model by performing a 
user evaluation of a dialogue system interacting 
with users. None of the earlier models have been 
tested in user evaluations. 
The significant improvement of the model 
gained by adding lexico-syntactic features such 
as word form and part-of-speech tag corroborates 
with earlier observations about the contribution 
of syntax in predicting response location (Koiso 
et al, 1998; Cathcart et al, 2003; Gravano & 
Hirschberg, 2009). While POS tag alone is a 
strong generic feature for making predictions in 
offline models its contribution to decision mak-
ing in online models is reduced due to speech 
recognition errors. This is because the POS tag-
ger itself uses the left context to make predic-
tions, and is not typically trained to handle noisy 
input. We have shown that using only the word 
form or a dictionary offers a better performance 
despite speech recognition errors. However, this 
of course results in a more domain-dependent 
model. 
Koiso et al, (1998), have shown that prosodic 
features contribute almost as strongly to response 
location prediction as the syntactic features. We 
do not find such results with our model. This 
difference could be partly attributed to inter-
speaker variation in the human?machine Map 
Task corpus used for training the models. All the 
users who participated in the corpus collection 
were non-native speakers of English. Also, our 
algorithm for extracting prosodic features is not 
as powerful as the manual extraction scheme 
used in (Koiso et al, 1998). Although prosodic 
and contextual features do not seem to improve 
the performance very much when syntactic fea-
tures are available, they are clearly useful when 
no ASR is available (70.2% as compared to the 
baseline of 50.79%).  
The subjective evaluation indicates that the in-
teractions with a system using our trained model 
were perceived as smoother (more accurate re-
sponses) as compared to a system using a model 
that makes a random choice between Respond 
and Hold. 
7 Future work 
Coordination problems in turn-transition and re-
sponsiveness have been identified as important 
short-comings of turn-taking models in current 
dialogue systems (Ward et al, 2005). In continu-
ation of the current evaluation exercise, we 
would next evaluate our Trained model?on an 
objective scale, in terms of its responsiveness 
and smoothness in turn-taking and back-
channels. An objective measure is the proportion 
of judge key-presses coinciding with false-
positive and false-negative model decisions. We 
argue that in comparison to the Random model 
our Trained model produces (i) fewer instances 
of false-negatives (gap/delayed response) and 
therefore has a faster response time, and (ii) few-
er instances of false-positives (overlap) and thus 
provides for smooth turn-transitions.  
We have so far explored syntactic, prosodic 
and contextual features for predicting response 
location. An immediate extension to our model 
would be to bring semantic features in the model. 
In Meena et al (2012) we have presented a data-
driven method for semantic interpretation of ver-
bal route descriptions into conceptual route 
graphs?a semantic representation that captures 
the semantics of the way human structure infor-
mation in route descriptions. Another possible 
extension is to situate the interaction in a face-to-
face Map Task between a human and a robot and 
add features from other modalities such as gaze. 
In a future version of the system, we do not 
only want to determine when to give responses 
but also what to respond. In order to do this, the 
system will need to extract the semantic concepts 
of the route directions (as described above) and 
utilize the confidence scores from the spoken 
language understanding component in order to 
select between different forms of clarification 
requests and acknowledgements.  
Acknowledgments 
This work is supported by the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237).  
382
References 
Anderson, A., Bader, M., Bard, E., Boyle, E., 
Doherty, G., Garrod, S., Isard, S., Kowtko, J., 
McAllister, J., Miller, J., Sotillo, C., Thompson, 
H., & Weinert, R. (1991). The HCRC Map Task 
corpus. Language and Speech, 34(4), 351-366. 
Cathcart, N., Carletta, J., & Klein, E. (2003). A shal-
low model of backchannel continuers in spoken di-
alogue. In 10th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics. Budapest. 
Clark, H. H. (1996). Using language. Cambridge, 
UK: Cambridge University Press. 
Duncan, S. (1972). Some Signals and Rules for Tak-
ing Speaking Turns in Conversations. Journal of 
Personality and Social Psychology, 23(2), 283-
292. 
Gravano, A., & Hirschberg, J. (2009). Backchannel-
inviting cues in task-oriented dialogue. In Proceed-
ings of Interspeech 2009 (pp. 1019-1022). Bright-
on, U.K. 
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P., & Witten, I. H. (2009). The WEKA 
Data Mining Software: An Update. SIGKDD Ex-
plorations, 11(1). 
Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., & 
Den, Y. (1998). An analysis of turn-taking and 
backchannels based on prosodic and syntactic fea-
tures in Japanese Map Task dialogs. Language and 
Speech, 41, 295-321. 
Landis, J., & Koch, G. (1977). The measurement of 
observer agreement for categorical data. Biomet-
rics, 33(1), 159-174. 
Meena, R., Skantze, G., & Gustafson, J. (2012). A 
Data-driven Approach to Understanding Spoken 
Route Directions in Human-Robot Dialogue. In 
Proceedings of Interspeech. Portland, OR, US. 
Raux, A., & Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a 
spoken dialogue system. In Proceedings of SIGdial 
2008. Columbus, OH, USA. 
Rizzolo, N., & Roth, D. (2010). Learning Based Java 
for Rapid Development of NLP Systems. Lan-
guage Resources and Evaluation. 
Sacks, H., Schegloff, E., & Jefferson, G. (1974). A 
simplest systematics for the organization of turn-
taking for conversation. Language, 50, 696-735. 
Sj?lander, K., & Beskow, J. (2000). WaveSurfer - an 
open source speech tool. In Yuan, B., Huang, T., & 
Tang, X. (Eds.), Proceedings of ICSLP 2000, 6th 
Intl Conf on Spoken Language Processing (pp. 
464-467). Beijing. 
Skantze, G. (2007). Error Handling in Spoken Dia-
logue Systems - Managing Uncertainty, Grounding 
and Miscommunication. Doctoral dissertation, 
KTH, Department of Speech, Music and Hearing. 
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
Ward, N., Rivera, A., Ward, K., & Novick, D. (2005). 
Root causes of lost time and user stress in a simple 
dialog system. In Proceedings of Interspeech 2005. 
Lisbon, Portugal. 
Ward, N. (1996). Using prosodic clues to decide when 
to produce backchannel utterances. In Proceedings 
of the fourth International Conference on Spoken 
Language Processing (pp. 1728-1731). Philadelph-
ia, USA. 
Yngve, V. H. (1970). On getting a word in edgewise. 
In Papers from the sixth regional meeting of the 
Chicago Linguistic Society (pp. 567-578). Chicago. 
 
383
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 73?77,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Human pause and resume behaviours for  
unobtrusive humanlike in-car spoken dialogue systems 
Jens Edlund 
KTH Speech, Music and Hearing 
Stockholm 
Sweden 
edlund@speech.kth.se 
Fredrik Edelstam 
KTH Speech, Music and Hearing 
Stockholm 
Sweden 
freede41@kth.se 
Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm 
Sweden 
jocke@speech.kth.se 
 
Abstract 
This paper presents a first, largely qualitative 
analysis of a set of human-human dialogues 
recorded specifically to provide insights in how 
humans handle pauses and resumptions in 
situations where the speakers cannot see each 
other, but have to rely on the acoustic signal alone. 
The work presented is part of a larger effort to find 
unobtrusive human dialogue behaviours that can be 
mimicked and implemented  in-car spoken 
dialogue systems within in the EU project Get 
Home Safe, a collaboration between KTH, DFKI, 
Nuance, IBM and Daimler aiming to find ways of 
driver interaction that minimizes safety issues,. The 
analysis reveals several human temporal, 
semantic/pragmatic, and structural behaviours that 
are good candidates for inclusion in spoken 
dialogue systems. 
1 Introduction 
In-car spoken dialogue systems face specific 
challenges that are of little or no relevance for 
systems designed for other environments. The 
two most striking of these are (1) the very strong 
focus on safety in the driving situation and (2) 
the fact that the person who speaks to the system 
? its user, in other words the driver in the 
majority of cases ? does so in an environment 
that may change quite drastically from the 
beginning of an interaction to its completion. The 
most straightforward source for this change is the 
fact that the car (and the user) moves through the 
environment while the dialogue progresses. The 
dynamic and mobile nature of the surrounding 
traffic adds to the complexity. Generally 
speaking, safety is the key concern when 
designing spoken dialogue systems for in-car use. 
While poor performance in spoken dialogue 
systems can clearly be a nuisance to a driver, the 
promise of using properly designed spoken 
dialogue instead of other interfaces is increased 
safety. This promise is based in the nature of 
speech: it does not require the driver to divert the 
use hands and eyes from the driving, and it is a 
mode of communication that most are quite used 
to and comfortable with, so should not induce 
great amounts of cognitive load. 
We present a corpus consisting of a set of 
human-human dialogues recorded specifically to 
provide insights in how humans handle 
interruptions - how they pause and resume 
speaking - in situations where the speakers 
cannot see each other, but have to rely on the 
acoustic signal alone, and a preliminary analysis 
of these which reveals several candidates for 
inclusion in in-car spoken dialogue systems. 
Finally, we discuss how these can be 
implemented and how a selection of them are 
included in the Get Home Safe experiment 
implementation. 
2 Background and related work 
In a government-commissioned survey from 
2011, the Swedish National Road and Transport 
Research Institute reviews several hundred 
research publications on traffic safety and the use 
of mobile phones and other communication 
devices [Kircher et al., 2011]. Amongst the most 
striking findings: although there is a broad 
consensus that visual-manual interactions (e.g. 
using social media or texting) with 
communication devices impair driving 
performance, bans have not had any measurable 
effects in terms of lowered accident rates or 
insurance claims. Ban compliance statistics show 
73
that bans have an effect on driver behaviour the 
first year, after which drivers return to their 
former habits. With bans being virtually 
ineffective, solutions must be sought elsewhere. 
Allowing drivers to manage more tasks using 
speech, which does not occupy hands and eyes, 
would decrease the time spent in visual-manual 
interaction while driving, provided that the 
drivers can be persuaded to use the systems.  
Clearly, the systems must work well - a large 
proportion of errors may well put the driver at 
risk (e.g. Kun et al., 2007). It is also unlikely that 
drivers can be persuaded to use systems that do 
not work well. But using hand-free and eyes-free 
controls may not suffice. Kircher et al. (2011) 
notes that there is virtually no evidence that 
hands-free telephony is less risky than hand-held 
use, suggesting that the conversations in 
themselves may be a risk factor. Speaking to a 
person who is present in the car and who shares 
the driver?s situation, however, is much safer 
(Peissner et al., 2011), suggesting that a system 
that is perceived as and behaves like a co-present 
human is a sensible aim. In the EU project Get 
Home Safe, of which this research is a part, we 
call such systems humanlike proactive systems. 
Where a traditional spoken dialogue system 
bases its decisions largely on (1) whether it has 
something to say, (2) what the user has just said, 
and (3) whether the user is speaking or is silent, a 
humanlike proactive system will also consider (4) 
the (traffic) situation, (5) the user?s (driver's) 
estimated attention, and (6) the urgency of the 
task at hand, much like a passenger might.  
This paper focusses on two broad types of 
proactive humanlike behaviours: user controlled 
pacing, referring to the ability to pause at the 
whim of the user in the middle of a conversation, 
or even an utterance, and then resume the 
conversation; and situation sensitive speech, the 
ability to allow the situation to affect the manner 
in which the system speaks. We are searching for 
behaviours that people use when interrupted, 
either by their interlocutor or by some event in 
their environment, and when they resume the 
original dialogue again. We are specifically 
interested in behaviours that can be implemented 
in the Get Home Safe architecture without major 
changes to existing applications. The architecture 
allows a central manager to instruct applications 
to stop where they are and maintain their inner 
state until instructed to either exit or continue 
where they were.  
The task has been approached by others, albeit 
in different manners. Villing (2010) presents an 
analysis of interruptions and resumptions in 
human-human in-vehicle dialogues, as well as 
implications for future in-car dialogue systems, 
and Yang et al. (2011) used human-human multi-
tasking dialogues that involved a poker game as 
the main task, and a picture game as an 
interrupting real-time task. 
3 Method 
Our goal is to collect and analyse data that will 
provide an insight to how a human speaker deals 
with interruptions in in-car dialogue (our target 
setting) and to find relevant behaviours that can 
be successfully mimicked in an in-car human-
computer environment. The question can be 
subdivided: How does a human speaker stop 
speaking when faced with an (possible) 
interruption? How does a human speaker resume 
speaking after such an event? Which of these 
behaviours are plausible candidates for inclusion 
in a spoken dialogue system? 
3.1 Data Collection 
Setting. Collecting data from a real driving 
situation is time consuming, not to say dangerous 
when adding a secondary task. We have instead 
opted to simulate the key elements of interest in 
our dialogue recording studio ? a safe recording 
environment consisting of several physically 
distinct locations that are interconnected with 
low and constant latency audio and video. The 
interlocutors were placed in different rooms, and 
communicated through pairs of wireless close-
range microphones and loudspeakers. 
Subjects. The purpose of this data collection is 
not for example training a recognizer, but the 
generation of a consistent set of candidate 
74
behaviours for implementation in a spoken 
dialogue system ? one that contains behaviours 
that could all plausibly be used by the same 
speaker. To achieve this, we consistently use the 
same single male speaker in the role as the 
system (?speaker?, hereafter) for all recordings. 
For the user role (?listener?, hereafter), a 
balanced variety of speakers were used: two sets 
of 8 listeners, both balanced for gender, were 
used. None of the listeners had any previous 
knowledge of this research. All listeners were 
rewarded with one cinema ticket. They were told 
that those who performed the task best would 
earn a second ticket, and the top performers from 
each setup received a second ticket after the 
recordings were completed. 
Task. The data collection was designed as a dual 
task experiment. The main task for the speaker 
was to read three short informative texts about 
each of three cities (Paris, Stockholm, and 
Tokyo), arranged so that the first is quite general, 
the second more specific, and the third deals with 
a quite narrow detail with some connection to the 
city. This task is equivalent to what one might 
expect from a tourist information system. For the 
listener, the main task is to listen to the city 
information. The listener is motivated by the 
knowledge that the reading of each segment - 
that is each of the nine informative texts - is 
followed by three questions on the content of the 
text. Their performance in answering these 
questions and in completing the secondary task 
counted towards the extra movie ticket. The 
secondary task was designed as follows. At 
irregular, random intervals, a clearly visible 
coloured circle would appear, either in front of 
the speaker or the listener. When this happened, 
the speaker was under obligation to stop the 
narration and instead read a sequence of eight 
digits from a list. The listener must then to repeat 
the digit sequence back to the speaker, after 
which the speaker could resume the narration.  
Conditions. We considered two characteristics 
of in-car interruptions that we assumed would 
have an effect on how humans react to the 
interruption and to how they resume speaking 
after it: the source of an interruption can be 
either internal or external in an in-car dialogue 
(our target setting); and the duration and content 
of an interruption varies, they can be brief or 
even the result of a mistake, or they can be long 
and contentful. The condition mapping to the 
first of these characteristics was designed such 
that the coloured circle signalling an interruption 
was presented randomly to either the speaker, 
mapping to en external event visible to the 
system but not the driver, or to the listener, 
mapping to an interruption from the driver to the 
system (the listener had to speak up to inform the 
speaker that the circle was present). The second 
condition was designed such that in one set of 
eight dialogues, the coloured circle would start 
out yellow, and as soon as the speaker became 
silent, it would randomly either disappear 
(causing only a short interruption with light or no 
content, corresponding to e.g. a false alarm) or 
turn red, in which case the sequence of digits 
would be read and repeated (a contentful 
interruption). In the other set of eight recordings, 
the circle always went straight to red, and always 
caused digits to be read and repeated. 
3.2 Analysis 
Each channel of each recording was segmented 
into silence delimited speech segments 
automatically, and these were transcribed using 
Nuance Dragon Dictate. The transcriptions were 
then corrected by a human annotator, and 
labelled for interruptions and resumptions. In this 
initial analysis, we looked at temporal statistics 
(e.g. the durations between interruption from the 
listener and silence from the speaker), 
semantics/pragmatics (e.g. lexical choices, 
insertions, repetitions) and syntax (e.g. where in 
an utterance resumption begins).  
4 Results 
A categorical difference was found in the 
distribution of speaker response times (from the 
onset of a listener interruption to the offset of 
speaker speech) depending on whether the 
interruption occurred in the middle of a phrase or 
close to the end of the phrase. In the first case, 
the vast majority of the response times are 
75
distributed between 300 and 700 ms, with a clear 
mode around 400 ms. Only a fraction of response 
times are slower than 700 ms, and none except 
one is faster than 300 ms. Phrase final 
interruptions show an almost flat response time 
distribution, with only a very weak mode around 
500 ms, and a large proportion with response 
times longer than 700 ms.  
For lexical/pragmatic choices, we find a 
categorical variation for the insertion of 
vocalizations we somewhat lazily term filled 
pauses (e.g. "eh", "em") and what we equally 
lazily term lexical cue phrases (e.g. "right", "ok") 
before resumption. The existence of such 
insertions, as well as the choice of vocalization, 
is straightforwardly dependant on the 
contentfulness of the interruption. For short 
interruptions of light content, filled pauses are 
nearly never inserted before resumption. Lexical 
cue phrases are inserted, but rarely. In the typical 
case, the speaker goes straight back to the 
informational text. For long, contentful 
interruptions, resumption is initiated by an 
insertion in an overwhelming majority of cases. 
If the insertion consists of one vocalization only, 
this is nearly always a filled pause. If more than 
one vocalization is present, then lexical cue 
phrases occur frequently, but overall, lexical cue 
phrases are no more common here than in the 
case of the short interruptions.  
In the case of structural comparisons, the one 
clear distinction we found has to do with what, if 
any, material is repeated at resumption, a 
characteristic that varies strongly with the type of 
interruption. For long interruptions, in every 
instance but a handful, the speaker either repeats 
the entire utterance in which the interruption 
occurs, or - in the few cases where an 
interruption occurred just as an utterance came to 
an end - with the next utterance. For short 
interruptions, resumptions also start most 
regularly from either the start of the current 
utterance or from the start of the next one. 
However, starts from the beginning or end of the 
current phrase, word, or even part of word are 
also frequent.  
5 Discussion 
We think that the three main findings presented 
in the results are all good candidates for 
implementation. The different distributions of 
response times suggest that if an interruption 
occurs centrally, in the midst of a production, the 
speaker stops as fast as possible - the distribution 
is largely consistent with reaction time 
distributions. Towards the end of phrases, the 
distribution is flat and quite different to what one 
would expect if reaction time was the main 
governing factor. The larger proportion of long 
response times suggests that when the speaker is 
close to the end of a phrase, finishing the phrase 
first might be preferable to stopping as soon as 
reaction permits. From an implementation 
perspective, this is quite encouraging. In order to 
create a behaviour consistent with this, we need 
to halt system speech with a reaction time of 
around 3-500ms. If possible (i.e. if the system 
knows how much time remains of its production), 
we may instead complete the utterance if less 
than, say, 700ms remains.  
Seemingly, short light content interruptions 
need no specific signalling of resumption. If such 
signalling is made, it is in the form of a lexical 
cue phrase, such as "ok" or "right". Resumptions 
following longer, contentful interruptions are 
routinely initiated by a filled pause. This may be 
solely due to the speaker's need to find the 
correct place in the script to start over, but it is 
noteworthy that instead of doing this in silence, 
the speaker opts to vocalize. For implementation, 
resumptions following contentful subdialogues 
should start with a filled pause and perhaps a 
lexical cue phrase.  
The straightforward interpretation of the third 
finding is that in the case of short interruptions, 
both speaker and listener have the point of 
interruption in fresh memory, and need no 
reminder, while long interruptions require the 
speaker to help the listener out by recapitulating 
what was last said. In the latter case, the system 
can simply start over with its last utterance 
(provided that it produces its synthesis on a 
granularity of at least utterance level).  
76
Acknowledgments 
This work was funded by the GetHomeSafe  (EU 
7th Framework STREP project  288667). 
References  
Kircher, K., Patten, C., & Ahlstr?m, C. (2011). 
Mobile telephones and other communication devices 
and their impact on traffic safety: a review of the 
literature. Technical Report VTI 729A, Stockholm. 
Kun, A., Paek, T., & Medenica, Z. (2007). The effect 
of speech interface accuracy on driving performance. 
In Proc. of Interspeech 2007. Antwerp, Belgium. 
Peissner, M., Doebler, V., & Metze, F. (2011). Can 
voice interaction help reducing the level of distraction 
and prevent accidents? Meta-Study on Driver 
Distraction and Voice Interaction. Technical Report, 
Fraunhofer, Germany and CMU, USA, Aachen, 
Germany. 
Villing, J. (2010). Now, where was I? Resumption 
strategies for an in-vehicle dialogue system. In The 
48th Annual Meeting of the Association for 
Computational Linguistics (pp. 798-805). Sweden. 
Yang, F., Heeman, P. A., & Kun, A. L. (2011). An 
investigation of interruptions and resumptions in 
multi-tasking dialogues. Computational inguistics, 
27(1), 75-104. 
 
77
Proceedings of the SIGDIAL 2014 Conference, pages 2?11,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Crowdsourcing Street-level Geographic Information Using a 
Spoken Dialogue System 
 
 
 Raveesh Meena Johan Boye Gabriel Skantze Joakim Gustafson 
KTH Royal Institute of Technology 
School of Computer Science and Communication  
Stockholm, Sweden 
{raveesh, jboye}@csc.kth.se, {gabriel, jocke}@speech.kth.se 
 
  
 
Abstract 
We present a technique for crowd-
sourcing street-level geographic infor-
mation using spoken natural language. In 
particular, we are interested in obtaining 
first-person-view information about what 
can be seen from different positions in 
the city. This information can then for 
example be used for pedestrian routing 
services. The approach has been tested in 
the lab using a fully implemented spoken 
dialogue system, and has shown promis-
ing results. 
1 Introduction 
Crowdsourcing is increasingly being used in 
speech processing for tasks such as speech data 
acquisition, transcription/labeling, and assess-
ment of speech technology, e.g. spoken dialogue 
systems (Parent & Eskenazi, 2011). However, 
we are not aware of any attempts where a dia-
logue system is the vehicle for crowdsourcing 
rather than the object of study, that is, where a 
spoken dialogue system is used to collect infor-
mation from a large body of users.  A task where 
such crowdsourcing dialogue systems would be 
useful is to populate geographic databases. While 
there are now open databases with geographic 
information, such as OpenStreetMap (Haklay & 
Weber, 2008), these are typically intended for 
map drawing, and therefore lack detailed street-
level information about city landmarks, such as 
colors and height of buildings, ornamentations, 
facade materials, balconies, conspicuous signs, 
etc. Such information could for example be very 
useful for pedestrian navigation (Tom & Denis, 
2003; Ross et al., 2004). With the current grow-
ing usage of smartphones, we might envisage a 
community of users using their phones to con-
tribute information to geographic databases, an-
notating cities to a great level of detail, using 
multi-modal method including speech. The key 
reason for using speech for map annotation is 
convenience; it is easy to talk into a mobile 
phone while walking down the street, so a user 
with a little experience will not be slowed down 
by the activity of interacting with a database. 
This way, useful information could be obtained 
that is really hard to add offline, sitting in front 
of one?s PC using a map interface, things like: 
Can you see X from this point? Is there a big 
sign over the entrance of the restaurant? What 
color is the building on your right? 
Another advantage of using a spoken dialogue 
system is that the users could be asked to freely 
describe objects they consider important in their 
current view. In this way, the system could learn 
new objects not anticipated by the system de-
signers, and their associated properties.   
In this paper we present a proof-of-concept 
study of how a spoken dialogue system could be 
used to enrich geographic databases by 
crowdsourcing. To our knowledge, this is the 
first attempt at using spoken dialogue systems 
for crowdsourcing in this way. In Section 2, we 
elaborate on the need of spoken dialogue systems 
for crowdsourcing geographic information. In 
Section 3 we describe the dialogue system im-
plementation. Section 4 presents our in-lab 
crowdsourcing experiment. We present an analy-
sis of crowd-sourced data in Section 5, and dis-
cuss directions for future work in Section 6. 
2 The pedestrian routing domain 
Routing systems have been around quite some 
time for car navigation, but systems for pedestri-
2
an routing are relatively new and are still in their 
nascent stage (Bartie & Mackaness, 2006; Krug 
et al., 2003; Janarthanam et al., 2012; Boye et al., 
2014). In the case of pedestrian navigation, it is 
preferable for way-finding systems to base their 
instructions on landmarks, by which we under-
stand distinctive objects in the city environment. 
Studies have shown that the inclusion of land-
marks into system-generated instructions for a 
pedestrian raises the user?s confidence in the sys-
tem, compared to only left-right instructions 
(Tom & Denis, 2003; Ross et al., 2004).  
Basing routing instructions on landmarks 
means that the routing system would, for exam-
ple, generate an instruction ?Go towards the red 
brick building? (where, in this case, ?the red 
brick building? is the landmark), rather than 
?Turn slightly left here? or ?Go north 200 me-
ters?. This strategy for providing instructions 
places certain requirements on the geographic 
database: It has to include many landmarks and 
many details about them as well, so that the sys-
tem can generate clear and un-ambiguous in-
structions. However, the information contained 
in current databases is still both sparse and 
coarse-grained in many cases.  
Our starting point is a pedestrian routing sys-
tem we designed and implemented, using the 
landmark-based approach to instruction-giving 
(Boye et al., 2014). The system performs visibil-
ity calculations whenever the pedestrian ap-
proaches a waypoint, in order to compute the set 
of landmarks that are visible for the user from his 
current position. OpenStreetMap (Haklay & We-
ber, 2008) is used as the data source. Figure 1 
shows a typical situation in pedestrian routing 
session. The blue dot indicates the user?s position 
and the blue arrow her direction. Figure 2 shows 
the same situation in a first-person perspective. 
The system can now compute the set of visible 
landmarks, such as buildings and traffic lights, 
along with distances and angles to those land-
marks. The angle to a building is given as an in-
terval in degrees relative to the direction of the 
user (e.g. 90? left to 30? left). This is exemplified 
in Figure 1, where four different buildings are in 
view (with field of view marked with numbers 
1?4). Landmarks that are not buildings are con-
sidered to be a single point, and hence the rela-
tive angle can be given as a single number. 
When comparing the map with the street view 
picture, it becomes obvious that the ?SEB? bank 
office is very hard to see and probably not very 
suitable to use as a landmark in route descrip-
tions. On the other hand, the database does not 
contain the fact that the building has six stories 
and a fa?ade made of yellow bricks, something 
that would be easily recognizable for the pedes-
trian. This is not due to any shortcoming of the 
OpenStreetMap database; it just goes to show 
that the database has been constructed with map 
drawing in mind, rather than pedestrian routing. 
There are also some other notable omissions in 
the database; e.g. the shop on the corner, visible 
right in front of the user, is not present in the da-
tabase. Since OpenStreetMap is crowd-sourced, 
there is no guarantee as to which information 
will be present in the database, and which will 
not. This also highlights the limitation of existing 
approaches to crowd-sourcing geographic infor-
mation: Some useful information is difficult to 
add off-line, using a map interface on a PC. On 
the other hand, it would be a straightforward 
matter given the kind of crowd-sourcing spoken 
dialogue system we present next. 
 
 
 
Figure 1: A pedestrian routing scenario 
  
 
 
Figure 2: The visual scene corresponding to the 
pedestrian routing scenario in Figure 1 
3 A dialogue system for crowd-sourcing 
To verify the potential of the ideas discussed 
above, we implemented a spoken dialogue sys-
tem that can engage in spoken conversation with 
3
users and learn details about landmarks in visual 
scenes (such as Figure 2). To identify the kind of 
details in a visual scene that the system could 
potentially ask the users, we first conducted a 
preliminary informal crowd-sourcing dialogue: 
one person (the receiver), was instructed to seek 
information that could be useful for pedestrian 
navigation from the other person (the giver).  
The receiver only had access to information 
available in the maps from OpenStreetMap, as in 
Figure 1, but without any marking of field of 
views, whereas the giver only had access to the 
corresponding visual scene (as in Figure 2). In-
teraction data from eight such dialogues (from 
four participants, and four different visual 
scenes) suggested that in a city environment, 
buildings are prominent landmarks and much of 
the interaction involves their properties such as 
color, number of stories, color of roof, signs or 
ornamentations on buildings, whether it has 
shops, etc. Seeking further details on mentioned 
signs, shops, and entities (whether mapped or 
unmapped) proved to be a useful strategy to ob-
tain information. We also noted that asking for 
open-ended questions, such as ?Is there anything 
else in this scene that I should be aware of?? 
towards the end has the potential of revealing 
unknown landmarks and details in the map.  
Obtaining specific details about known objects 
from the user corresponds to slot-filling in a dia-
logue system, where the dialogue system seeks a 
value for a certain slot (= attribute). By engaging 
in an open-ended interaction the system could 
also obtain general details to identify new slot-
value pairs. Although slots could be in some cas-
es be multi-valued (e.g., a building could have 
both color red and yellow), we have here made 
the simplifying assumption that they are single 
valued. Since users may not always be able to 
specify values for slots we treat no-value as a 
valid slot-value for all type of slots.  
We also wanted the system to automatically 
learn the most reliable values for the slots, over 
several interactions. As the system interacts with 
new users, it is likely that the system will obtain 
a range of values for certain slots. The variability 
of the answers could appear for various reasons: 
users may have differences in perception about 
slot-values such as colors, some users might 
misunderstand what building is being talked 
about, and errors in speech recognition might 
result in the wrong slot values. Some of these 
values may therefore be in agreement with those 
given by other users, while some may differ 
slightly or be in complete contradiction. Thus the 
system should be able to keep a record of all the 
various slot-values obtained (including the dis-
puted ones), identify slot-values that need to be 
clarified, and engage in a dialogue with users for 
clarification. 
In view of these requirements, we have de-
signed our crowd-sourcing dialogue system to be 
able to (1) take and retain initiative during the 
interactions for slot-filling, (2) behave as a re-
sponsive listener when engaging in open-ended 
dialogue, and (3) ask wh? and yes?no questions 
for seeking and clarifying slot-values, respective-
ly. Thus when performing the slot-filling task, 
the system mainly asks questions, acknowledges, 
or clarifies the concepts learned for the slot-
values. Apart from requesting repetitions, the 
user cannot ask any questions or by other means 
take the initiative. A summary of all the attrib-
utes and corresponding system prompts is pre-
sented in Appendix A. 
The top half of Figure 3 illustrates the key 
components of the dialogue system. The Dia-
logue Manager queries the Scene Manager (SM) 
for slots to be filled or slot-values to be clarified, 
engages in dialogue with users to learn/clarify 
slot-values, and informs the SM about the values 
obtained for these slots. The SM manages a list 
of scenes and the predefined slots ? for each type 
of landmark in visual scenes ? that need to be 
filled, maintains a record of slot-values obtained 
from all the users, and identifies slot-values with 
majority vote as the current reliable slot-value. 
To achieve these objectives, the scene manager 
uses an XML representation of visual scenes. In 
this representation, landmarks (e.g., buildings, 
junctions, etc.) ? automatically acquired through 
the OpenStreetMap database and the visibility 
computations mentioned in Section 2  ? are 
stored as scene-objects (cf. Figure 4). 
 
 
 
Figure 3: Dialogue system architecture 
 
The Dialogue Manager (DM) uses scene-
object attributes, such as type, angle or interval 
of a building, to generate referential expressions, 
such as ?Do you see a building on the far left?? 
4
or ?Do you see a shop on the left?? to draw the 
users? attention to the intended landmark in the 
scene. During the course of interaction, the Sce-
ne Manager (SM) extends scene-objects with a 
set of predefined attributes (= slots) that we iden-
tified in the preliminary study, along with their 
various slot-values (cf. Figure 5). For each slot, 
the SM keeps a record of slot-values obtained 
through wh? questions as well as the ones dis-
puted by the users in yes?no questions (cf. ob-
tained and disputed tags in the XML), and 
uses their tally to identify the slot-value in major-
ity. The system assumes this slot-value (or one of 
them in case of a tie) as its best estimate of a 
slot-value pair, which it could clarify with anoth-
er user using a yes?no query. During the slot-
filling mode the DM switches to open-ended in-
teraction mode to seek general details (using 
prompts such as ?Could you describe it/them??), 
if the user suggests/agrees that there are signs 
on/at a scene-object, or a building has shops or 
restaurants. Once all the slots for all the scene-
objects in a visual scene have been queried, the 
DM once again switches to the open-ended inter-
action mode and queries the users whether there 
are any other relevant signs or landmarks that the 
system may have missed and should be aware of. 
On completion of the open-ended queries the SM 
selects the next visual scene, and the DM engag-
es in a new dialogue.  
 
<scene xmlns="cityCS.scene" name=" view7.jpg" lat="59.34501" 
lon="18.0614" fovl="-60" fovr="60" bearing="320" dist="100"> 
    <scene-object> 
        <id>35274588</id> <type>building</type> 
        <from>-60</from> <end>-39</end> 
    </scene-object> 
    <scene-object> 
        <id>538907080</id> <type>shop</type> 
        <distance>34.82</distance> 
        <angle>-39</angle> <bearing>281</bearing> 
    </scene-object> 
    <scene-object> 
        <id>280604</id> <type>building</type> 
        <from>-38</from> <end>6</end> 
    </scene-object> 
    <scene-object> 
        <id>193906</id> <type>traffic_signals</type> 
        <distance>40.77</distance> 
        <angle>-14</angle> <bearing>306</bearing> 
    </scene-object> 
    ... 
</scene> 
Figure 4: XML representation of visual scenes 
 
For speech recognition and semantic interpre-
tation the system uses a context-free grammar 
with semantic tags (SRGS1), tailored for the do-
main. The output of semantic interpretation is a 
concept. If the concept type matches the type of 
the slot, the dialogue manager informs the scene 
manager about the obtained slot-value. If the 
                                                 
1 http://www.w3.org/TR/speech-grammar/ 
concept type is inappropriate the DM queries the 
user once more (albeit using different utterance 
forms). If still no appropriate concept is learned 
the DM requests the SM for the next slot and 
proceeds with the dialogue. For speech synthesis, 
we use the CereVoice system developed by 
CereProc2. The dialogue system has been imple-
mented using the IrisTK framework (Skantze & 
Al Moubayed, 2012). 
 
<scene-object> 
    <id>35274588</id> <type>building</type> 
    <from>-60</from> <end>-39</end> 
    <slot slotName="VISIBLE">?    </slot> 
    <slot slotName="COLOR"> 
     <obtained> 
       <value slotValue="Green"> 
         <userlist> 
           <usrDtls uid="u01" asrCnf="0.06" qType="WH"/> 
         </userlist> 
       </value> 
       <value slotValue="no-value"> 
         <userlist> 
           <usrDtls uid="u02" asrCnf="0.46" qType ="WH"/> 
         </userlist> 
       </value> 
       <value slotValue="Gray"> 
         <userlist> 
           <usrDtls uid="u03" asrCnf="0.19" qType ="WH"/> 
         </userlist> 
       </value> 
     </obtained> 
     <disputed> 
       <value slotValue="Green"> 
         <userlist> 
           <usrDtls uid="u02" asrCnf="0.92" qType ="YN"/> 
         </userlist> 
       </value> 
     </disputed> 
    </slot> 
    <slot slotName="STORIES">?    </slot> 
    <slot slotName="ROOF_COLOR">?    </slot> 
    ? 
</scene-object> 
 
Figure 5: Every slot-value is recorded  
 
In contrast to the slot-filling mode, when en-
gaging in an open-ended interaction, the system 
leaves the initiative to the user and behaves as a 
responsive listener. That is, the system only pro-
duces feedback responses, such as backchannels 
(e.g., okay, mh-hmm, uh-huh), repetition requests 
for longer speaker turns (e.g., could you repeat 
that?), or continuation prompts such as ?any-
thing else?? until the user is finished speaking. 
Unless the system recognized an explicit closing 
statement from the user (e.g., ?I can?t?), the sys-
tem encourages the user to continue the descrip-
tions for 2 to 4 turns (chosen randomly). 
To detect appropriate locations in users? 
speech where the system should give feedback 
response, the system uses a trained data-driven 
model (Meena et al., 2013). When the voice ac-
tivity detector detects a silence of 200 ms in us-
ers? speech, the model uses prosodic, contextual 
and lexico-syntactic features from the preceding 
speech segment to decide whether the system 
                                                 
2 https://www.cereproc.com/ 
5
should produce a feedback response. The lower 
half of Figure 3 shows the additional components 
of the dialogue system used in open-ended inter-
action mode. In this mode, the ASR system uses 
a language model that is trained on interactions 
from a related domain (verbal route descrip-
tions), in parallel to the SRGS grammar.  
4 In-lab crowd-sourcing experiment  
Nine visual scenes (wide-angle pictures in first-
person perspective and taken in Stockholm city, 
cf. Figure 2) were used for the task of 
crowdsourcing. Fifteen human participants (4 
females and 11 males) participated in the 
crowdsourcing exercise. All participants either 
studied or worked at the School of Computer 
Science and Communication, KTH, Stockholm. 
Participants were placed in front of a computer 
display and were told that the system will engage 
them in a spoken conversation to seek or clarify 
details about landmarks and other objects in vis-
ual scenes. They were told that the details would 
be used for pedestrian routing and therefore they 
are free to choose and specify details (in open-
ended questions) that they thought would be use-
ful when giving route instructions to another per-
son. 
Each participant did the nine visual scenes in 
the same order, with a 1 minute pause between 
each of them. The first visual scene was used as 
a trial in order to familiarize participants with the 
interaction scenario. For this reason, the trial in-
teraction was specifically designed to engage the 
participants in both wh? and yes?no type ques-
tions. We also wanted to investigate whether the 
use of wh? or yes?no questions alone has any 
impact on the interactions. For the remaining 
eight interactions, the dialogue system therefore 
alternatively used wh? and yes?no questions 
throughout the whole dialogue. The order of que-
ry types was switched for each successive partic-
ipant. Thus visual scenes presented with wh? 
queries to a participant pi were presented to par-
ticipant pi+1 with only yes?no queries, and vice-
versa. After each interaction participants were 
asked (1) whether the system appeared to be fa-
miliar with the visual scene; and (2) whether the 
interactions went smoothly. 
An example of a conversation with wh?type 
queries is presented in Table 1. The second col-
umn indicates the various types of utterances 
(dialogue acts) used by the system during this 
interaction. Note that during the slot-filling 
mode, the system has the initiative, whereas in 
the open-ended queries (cf. rows 15?22 in Table 
1) the system gives the initiative to the partici-
pant and only produces feedback responses. Ta-
ble 2 presents a conversation with yes?no queries 
following the conversation in Table 1. In a con-
versation with only yes?no  queries, a wh? query 
may be presented if a slot has been observed 
with no-value (cf. rows 9?10 in Table 1) or when 
the participant disputes the system?s current best 
estimate (cf. rows 6?9 in Table 2).  
 
S Introduction Could you help me with some de-
tails about this view? 
P  Yes 
S wh? query Do you see a building on the far 
left? 
P  Yes 
S Backchannel Mh-hmm 
S wh? query What color does the building have? 
P  Cream color [Green color] 
S Reprise Green 
S wh? query How many floors is this building? 
P  I don?t know [I do no] 
 ?  
S wh? query Are there any important signs in 
this scene that I may have missed? 
P  No 
S Backchannel Uh-huh 
S wh? query Is there anything peculiar in this 
scene that I should be familiar with 
P  There is a bike painted on street 
just in front of me 
[ is the right pink university in front 
of may ] 
S Backchannel Anything else? 
P  Yeah its white paint [Yes light pink] 
S Backchannel Mh-hmm 
P  That?s it [that that] 
S Backchannel Ehm 
P  I am done [okay] 
S Ending Okay, thanks 
 
Table 1: An example interaction with wh? que-
ries for the visual scene in Figure 2. S: system, P: 
participant (here, u01 from Figure 5). ASR re-
sults are shown within square brackets. 
 
S Introduction Could you help me with some details 
about this view? 
P  Yes 
S yes?no  
query 
Is the building on the far left visible to 
you? 
P  Yes 
S Backchannel Mh-hmm 
S yes?no  
query 
Does this building have green color? 
P  No 
S Backchannel Uh-huh 
S wh? query But than what color is the building? 
6
P  white [light] 
S Repeat Sorry, what color did you say? 
P  white [light] 
S wh? query How many stories is this building? 
P  I don?t know [I do rental] 
S Backchannel Ehm 
 ?  
S wh? query Are there any important signs in this 
scene that I may have missed? 
P  Ehm no [nine no] 
S Backchannel Ehm 
S yes?no  
query 
I am informed that there are other 
important landmarks or structures in 
this scene that I should be aware of, 
is it true? 
P  No 
S Backchannel uh-huh 
S Ending Okay, thanks 
 
Table 2: An example interaction with yes?no 
queries corresponding to the visual scene in Fig-
ure 2. S: system, P: participant (here u02 from 
Figure 5). ASR results are shown within square 
brackets. 
5 Data analysis 
We analyzed the data (15 8 interactions) col-
lected from the experiment along the following 
tracks: first, we compare the majority value of 
the slots to the ground truth as given by a human 
annotator; second, we explore how the ground 
truth of slot-values could be estimated automati-
cally; third, we also analyzed the instances where 
the participants disputed the system?s current 
estimate of slot-values; and fourth, we examined 
the post-experimental questionnaires.  
5.1 Rate of learning slot-values 
A total of 197 slots were learned in the exper-
iment. We analyzed how many slot-values had 
been correctly retrieved after 1, 2? 15 users. In 
Figure 6, the curve ?Majority? illustrates the 
fraction of slot-values correctly learned with 
each new user, under the assumption that the 
slot-values with majority votes ? from all the 15 
users ? constitute the ground truth. Thus after 
interacting with the first user the system had ob-
tained 67.0% of slot-values correctly (according 
to the majority) and 96.4% of slot-values after 
interacting with the first six users. Another eight 
users, or fourteen in total, were required to learn 
all the slot-values correctly. The progression 
curve thus provides an estimate of how many 
users are required to achieve a specific percent-
age of slot-values correctly if majority is to be 
considered the ground truth. The curve ?Not-in-
Majority? indicates the number of slot with val-
ues that were not in the majority. Thus after in-
teracting with the first user 20.8% of slot-values 
the system had obtained were not in majority and 
could be treated as incorrect. Note that the curves 
Majority and Not-in-Majority do not sum up to 
100%, this is because we consider no-value as a 
valid slot-value, and treat the slot as unfilled. For 
example, 12.2% of the slots remained unfilled 
after interacting with the first user.  
 
 
 
Figure 6: Rate of learning slot-values with two differ-
ent estimates of ground truth 
 
We also investigated how close the majority is 
to the actual truth. A human annotator (one of the 
coauthors) labeled all the obtained slot-values as 
either sensible or insensible, based on the com-
bined knowledge from the corresponding maps, 
the visual scenes, and the set of obtained values. 
Thus a slot could have many sensible values. For 
example, various parts of a building could be 
painted in different colors. The progression 
curves ?Sensible? and ?Insensible? in Figure 6 
illustrate the fraction of total slots for which the 
learned values were actually correct and incor-
rect, respectively. While the curve for sensible 
values follows the same pattern as the progres-
sion curve for majority as the estimate of ground 
truth, the percent of slot-values that were actually 
correct is always lower than the majority as 
ground truth, and it never reached 100%. The 
constant gap between the two curves suggests 
that some slot-values learned by the majority 
were not actually the ground truth. What led the 
majority into giving incorrect slot-values is left 
as a topic for future work. 
As mentioned earlier, much of the slot-filling 
interaction involved buildings and their proper-
ties. Figure 7 illustrates that sensible values for 
most slots, pertaining to whether a building is 
visible, whether it is residential, whether it has 
shops, and the color of roof were obtained by 
interacting with only few participants. In con-
trast, properties such as color of the building and 
7
number of stories required many more partici-
pants. This could be attributed to the fact that 
participants may have differences in perception 
about slot-values. As regards to whether there are 
signs on buildings, we observed that the recall is 
relatively low. This is largely due to lack of 
common ground among participants about what 
could be considered a sign. Our intentions with 
designing this prompt was to retrieve any peculi-
ar detail on the building that is easy to locate: for 
us a sign suggesting a name of restaurant is as 
useful as the knowledge that the building has 
blue sunshade on the windows. Some partici-
pants understood this while other didn?t. 
 
 
 
Figure 7: Learning rate of various slots for land-
mark type building  
5.2 Estimated ground truth of slot-values 
The 15 subjects in the in-lab experiment were all 
asked for the same information. In a real applica-
tion, however, we want the system to only ask 
for slots for which it has insufficient or conflict-
ing information. If the ground truth of a certain 
slot-value pair can be estimated with a certainty 
exceeding some threshold (given the quality re-
quirements of the database, say 0.8), the system 
can consider the matter settled, and need not ask 
about that slot again. We therefore want to esti-
mate the ground truth of slot-values along with a 
certainty measure. To this end, we use the 
CityCrowdSource Trust software package 
(Dickens & Lupu, 2014), which is based on the 
probabilistic approach for supervised learning 
when we have multiple annotators providing la-
bels (possibly noisy) but no absolute gold stand-
ard, presented in Raykar et al. (2009). 
Using this approach, a question concerning the 
color of a building, say with ID 24, (e.g. ?What 
color is the building??) would be translated into 
several binary predicates COLOR_Red(24), 
COLOR_Brown(24), COLOR_Orange(24), etc. 
The justification for this binary encoding is that 
the different color values are not mutually exclu-
sive: A building might of course have more than 
one color, and in many cases more than one color 
name might be appropriate even though the 
building has only one dominating color (e.g. to 
describe the color either as ?brown? and ?red? 
might be acceptable to most people). Figure 8 
shows the incremental estimates for different 
colors for a certain building (OpenStreetMap ID 
163966736) after 1, 2? 15 subjects had been 
asked. The answer from the first subject was er-
roneously recognized as ?pink?. The next 9 sub-
jects all referred to the building as ?brown?. 
Among the final subjects, 3 subjects referred to 
building as ?red?, and 2 subjects as ?brown?. The 
final truth estimates are 0.98 for ?brown?, 0.002 
for ?red?, and 0.00005 for ?pink?. The diagram 
shows that if the certainty threshold is set to 0.8, 
the value ?brown? would have been established 
already after 4 subjects. 
 
 
 
Figure 8: Probabilities of different estimated ground 
truth values for the color of a certain building 
5.3 Disputed slot-values 
We also examined all system questions of 
yes?no type that received negative answers, i.e. 
instances where the participants disputed the sys-
tem?s current best estimate (based on majority 
vote) of a slot-value. Among the 95 such in-
stances, the system?s current best estimate was 
actually insensible only on 43 occasions. In 30 of 
these instances the participants provided a recti-
fied slot-value that was sensible. For the remain-
ing 13 instances the new slot-values proposed by 
the participant were actually insensible. There 
were 52 instances of false disputations, i.e. the 
system?s current estimate of a slot-value was 
sensible, but the participants disputed it. 6 of the-
se occurrences were due to errors in speech 
recognition, but for the remaining 46 occasions, 
error in grounding the intended landmark (15), 
users? perception of slot-values (3), and ambigui-
ty in what the annotator terms as sensible slot-
values (28), (e.g. whether there are signs on a 
building (as discussed in Section 5.1)) were iden-
8
tified as the main reasons. This suggests that 
slots (i.e. attributes) that are often disputed may 
not be easily understood by users. 
5.4 Post-experimental questionnaire 
As described above, the participants filled in a 
questionnaire after each interaction. They were 
asked to rate the system?s familiarity with the 
visual scene based on the questions asked. A 
Mann?Whitney U test suggests that participants? 
perception of the system?s familiarity with the 
visual scene was significantly higher for interac-
tions with yes?no queries than interactions with 
wh? queries (U=1769.5, p= 0.007). This result 
has implications for the design choice for sys-
tems that provide as well as ask for information 
from users. For example, a pedestrian routing 
system can already be used to offer routing in-
structions as well as crowdsourcing information. 
The system is more likely to give an impression 
of familiarity with the surrounding, to the user, 
by asking yes?no type questions than wh?
questions. This may influence a user?s confi-
dence or trust in using the routing system.  
Since yes?no questions expect a ?yes? or 
?no? in response, we therefore hypothesized that 
interactions with yes?no questions would be per-
ceived smoother in comparison to interactions 
with wh? questions. However, a Mann?Whitney 
U test suggests that the participants perceived no 
significant difference between the two interac-
tion types (U=1529.0, p= 0.248). Feedback 
comments from participants suggest that abrupt 
ending of open-ended interactions by the system 
(due to the simplistic model of detecting whether 
the user has anything more to say) gave users an 
impression that the system is not allowing them 
to speak. 
6 Discussion and future work 
We have presented a proof-of-concept study on 
using a spoken dialogue system for crowd-
sourcing street-level geographic information. To 
our knowledge, this is the first attempt at using 
spoken dialogue systems for crowdsourcing in 
this way. The system is fully automatic, in the 
sense that it (i) starts with minimal details ? ob-
tained from OpenStreetMap ? about a visual sce-
ne, (ii) prompts users with wh? questions to ob-
tain values for a predefined set of attributes; and 
(iii) assumes attribute-values with majority vote 
as its beliefs, and engages in yes?no questions 
with new participants to confirm them. In a data 
collection experiment, we have observed that 
after interacting with only 6 human participants 
the system acquires more than 80% of the slots 
with actually sensible values. 
We have also shown that the majority vote (as 
perceived by the system) could also be incorrect. 
To mitigate this, we have explored the use of the 
CityCrowdSource Trust software package 
(Dickens & Lupu, 2014) for obtaining the proba-
bilistic estimate of the ground truth of slot-values 
in a real crowd-sourcing system. However, it is 
important not only to consider the ground truth 
probabilities per se, but also on how many con-
tributing users the estimate is based and the qual-
ity of information obtained. We will explore the-
se two issues in future work. 
We have observed that through open-ended 
prompts, the system could potentially collect a 
large amount of details about the visual scenes. 
Since we did not use any automatic interpretation 
of these answers, we transcribed key concepts in 
participants? speech in order to obtain an esti-
mate of this. However, it is not obvious how to 
quantify the number of concepts. For example, 
we have learned that in Figure 2, at the junction 
ahead, there is: a traffic-sign, a speed-limit sign, 
a sign with yellow color, a sign with red color, a 
sign with red boarder, a sign that is round, a sign 
with some text, the text says 50. These are details 
obtained in pieces from various participants. 
Looking at Figure 2 one can see that these pieces 
when put together refer to the speed-limit sign 
mounted on the traffic-signal at the junction. 
How to assimilate these pieces together into a 
unified concept is a task that we have left for fu-
ture work. 
Acknowledgement 
We would like to thank the participants of the in-
lab crowd-sourcing experiment. This work is 
supported by the EIT KIC project 
?CityCrowdSource?, and the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237).  
Reference 
Bartie, P. J., & Mackaness, W. A. (2006). Develop-
ment of a Speech-Based Augmented Reality Sys-
tem to Support Exploration of Cityscape. Transac-
tions in GIS, 10(1), 63-86. 
Boye, J., Fredriksson, M., G?tze, J., Gustafson, J., & 
K?nigsmann, J. (2014). Walk This Way: Spatial 
Grounding for City Exploration. In Mariani, J., 
Rosset, S., Garnier-Rizet, M., & Devillers, L. 
9
(Eds.), Natural Interaction with Robots, Knowbots 
and Smartphones (pp. 59-67). Springer New York. 
Dickens, L., & Lupu, E. (2014). Trust service final 
deliverable report. Technical Report, Imperial Col-
lege, UK. 
Haklay, M., & Weber, P. (2008). OpenStreetMap: 
User-Generated Street Maps. IEEE Pervasive 
Computing, 7(4), 12-18. 
Janarthanam, S., Lemon, O., Liu, X., Bartie, P., 
Mackaness, W., Dalmas, T., & Goetze, J. (2012). 
Integrating Location, Visibility, and Question-
Answering in a Spoken Dialogue System for Pe-
destrian City Exploration. In Proceedings of the 
13th Annual Meeting of the Special Interest Group 
on Discourse and Dialogue (pp. 134-136). Seoul, 
South Korea: Association for Computational Lin-
guistics. 
Krug, K., Mountain, D., & Phan, D. (2003). Webpark: 
Location-based services for mobile users in pro-
tected areas.. GeoInformatics, 26-29. 
Parent, G., & Eskenazi, M. (2011). Speaking to the 
Crowd: Looking at Past Achievements in Using 
Crowdsourcing for Speech and Predicting Future 
Challenges. In INTERSPEECH (pp. 3037-3040). 
ISCA. 
Raykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Flor-
in, C., Valadez, G. H., Bogoni, L., & Moy, L. 
(2009). Supervised Learning from Multiple Ex-
perts: Whom to Trust when Everyone Lies a Bit. In 
Proceedings of the 26th Annual International Con-
ference on Machine Learning (pp. 889-896). New 
York, NY, USA: ACM. 
Ross, T., May, A., & Thompson, S. (2004). The Use 
of Landmarks in Pedestrian Navigation Instructions 
and the Effects of Context. In Brewster, S., & Dun-
lop, M. (Eds.), Mobile Human-Computer Interac-
tion - MobileHCI 2004 (pp. 300-304). Springer 
Berlin Heidelberg. 
Skantze, G., & Al Moubayed, S. (2012). IrisTK: a 
statechart-based toolkit for multi-party face-to-face 
interaction. In Proceedings of ICMI. Santa Monica, 
CA. 
Tom, A., & Denis, M. (2003). Referring to Landmark 
or Street Information in Route Directions: What 
Difference Does It Make?. In Kuhn, W., Worboys, 
M., & Timpf, S. (Eds.), Spatial Information Theo-
ry. Foundations of Geographic Information Sci-
ence (pp. 362-374). Springer Berlin Heidelberg. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
10
Appendix A 
The table below lists slots (= landmark attributes) and the corresponding wh? and yes?no system questions. For 
attributes marked with * the dialogue manager switches to open-ended interaction mode. 
 
Slot (=attribute ) System wh? questions System yes?no questions 
Visible: whether a particular 
landmark is visible from this 
view. 
? Do you see a building on the far left? 
? Do you see another building in front of 
you? 
? Is there a junction on the right? 
? Do you see a traffic-signal ahead? 
? Is the building on the far right visible to 
you? 
? I think there is another building in front of 
you, do you see it? 
? Can you see the junction on the right? 
? Are you able to see the traffic-signal 
ahead? 
Color of the building 
? What color does the building have? 
? What color is the building? 
? I think this building is red in color, what do 
you think? 
? Does this building have red color? 
Size of the building (in num-
ber of stories) 
? How many floors do you think are 
there in this building 
? How many stories is this building 
? I think there are six floors in this building, 
what do you think? 
? Is this building six storied? 
Color of the building?s roof 
? What color does the roof of this build-
ing have? 
? What color is the roof of this building? 
? I think the roof of this building is orange in 
color, what do you think? 
? Do you think that the roof of this building 
is orange? 
Signs or ornamentation on the 
building 
? Do you see any signs or decorations 
on this building? 
? I think there is a sign or some decoration 
on this building, do you see it? 
? There may be a sign or a name on this 
building, do you see it? 
Shops or restaurants in the 
building 
? Are there any shops or restaurants in 
this building? 
? I am informed that there are some shops or 
restaurants in this building, is it true? 
? I think there are some shops or restaurants 
in this building, what do you think? 
Signs at landmarks 
? Are there any important signs at the 
junction/crossing? 
? I believe there is a sign at this junc-
tion/crossing, do you see it? 
? Do you see the sign at this junc-
tion/crossing? 
*Description of sign  
? Could you describe this sign? 
? What does this sign look like? 
? Does the sign say something? 
? Could you describe this sign? 
? What does this sign look like? 
? Does the sign say something? 
*Signs in the visual scene 
 
? Are there any important signs in this 
scene that I may have missed? 
? Have I missed any relevant signs in 
this scene? 
? There are some important signs in this 
scene that could be useful for my 
knowledge, am I right? 
? I am informed that there are some signs in 
this scene that are relevant for me, is it 
true? 
*Landmarks in the visual sce-
ne 
 
? Are there any other important build-
ings or relevant structures in this scene 
that I should be aware of? 
? Is there anything particular in this 
scene that I should be familiar with? 
? Have I missed any relevant buildings 
or landmarks in this scene? 
? I am informed that there are some im-
portant landmarks or structures in this sce-
ne that I should be aware of, is it true? 
? I have been told that there are some other 
things in this scene that I are relevant for 
me, is it true? 
? I believe I have missed some relevant 
landmarks in this scene, am I right? 
*Description of unknown 
landmarks e.g. shop, restau-
rant, building, etc. 
? Could you describe it? 
? Could you describe them? 
? How do they look like? 
? Could you describe it? 
? Could you describe them? 
? How do they look like? 
 
11
