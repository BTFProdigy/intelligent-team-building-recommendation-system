Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1083?1091,
Beijing, August 2010
Chinese CCGbank:
extracting CCG derivations from the Penn Chinese Treebank
Daniel Tse and James R. Curran
School of Information Technologies
University of Sydney
{dtse6695,james}@it.usyd.edu.au
Abstract
Automated conversion has allowed the de-
velopment of wide-coverage corpora for a
variety of grammar formalisms without the
expense of manual annotation. Analysing
new languages also tests formalisms, ex-
posing their strengths and weaknesses.
We present Chinese CCGbank, a 760,000
word corpus annotated with Combinatory
Categorial Grammar (CCG) derivations, in-
duced automatically from the Penn Chi-
nese Treebank (PCTB). We design parsimo-
nious CCG analyses for a range of Chinese
syntactic constructions, and transform the
PCTB trees to produce them. Our process
yields a corpus of 27,759 derivations, cov-
ering 98.1% of the PCTB.
1 Introduction
An annotated corpus is typically used to develop
statistical parsers for a given formalism and lan-
guage. An alternative to the enormous cost  of
hand-annotating a corpus for a specific formalism
is to convert from an existing corpus.
The Penn Treebank (PTB; Marcus et al, 1994)
has been converted to HPSG (Miyao et al, 2004),
LFG (Cahill  et al,  2002), LTAG (Xia, 1999), and
CCG (Hockenmaier, 2003). Dependency corpora,
e.g. the German Tiger corpus, have also been con-
verted (Hockenmaier, 2006). The Penn Chinese
Treebank (PCTB; Xue et al, 2005) provides analy-
ses for 770,000 words of Chinese. Existing PCTB
conversions have targeted TAG (Chen et al, 2005)
and LFG (Burke and Lam, 2004; Guo et al, 2007).
We present Chinese CCGbank, a Chinese cor-
pus of CCG derivations automatically induced from
the PCTB. Combinatory Categorial Grammar (CCG;
Steedman, 2000) is a lexicalised grammar formal-
ism offering a unified account of local and non-
local dependencies. We harness the facilities of
CCG to provide analyses of Chinese syntax includ-
ing topicalisation, pro-drop, zero copula, extrac-
tion, and the? ba- and? bei-constructions.
Pushing the boundaries of formalisms by sub-
jecting them to unfamiliar syntax also tests their
universality claims. The freer word order of Turk-
ish (Hoffman, 1996) and the complex morphology
of Korean (Cha et al, 2002) led to the development
of extensions to the CCG formalism.
We present our analysis of Chinese syntax un-
der CCG, and provide an algorithm, modelled af-
ter Hockenmaier and Steedman (2007), to incre-
mentally transform PCTB trees into CCG derivations.
The algorithm assigns CCG categories which di-
rectly encode head and subcategorisation informa-
tion. Instances of Chinese syntax demanding spe-
cial analysis, such as extraction, pro-drop or topi-
calisation, are pin-pointed and given elegant anal-
yses which exploit the expressivity of CCG.
Our conversion yields CCG analyses for 27,759
PCTB trees (98.1%). Coverage on lexical items,
evaluated by 10-fold cross-validation, is 94.46%
(by token) and 73.38% (by type).
We present  the  first CCG analysis  of  Chinese
syntax and obtain a wide-coverage CCG corpus of
Chinese. Highly efficient statistical parsing using
a CCGbank has recently been demonstrated for
English (Clark and Curran, 2007). Our Chinese
CCGbank will enable the development of similarly
efficient wide-coverage CCG parsers for Chinese.
2 Combinatory Categorial Grammar
CCG (Steedman,  2000) is  a  lexicalised grammar
formalism, with a transparent syntax-semantics in-
terface, a flexible view of constituency enabling
concise accounts of various phenomena, and a con-
sistent account of local/non-local dependencies.
It consists of categories, which encode the type
and number of arguments taken by lexical items,
and combinators, which govern the possible inter-
actions between categories.
1083
? ? ?? ? ?? ? ? ?
that MW movie I already see EXP SFP
(N/N)/M M N NP (S\NP)/(S\NP) (S[dcl]\NP)/NP (S\NP)\(S\NP) S\S
> <B?
N/N (S[dcl]\NP)/NP
>
N
>T >B
NP S/(S\NP) (S[dcl]\NP)/NP
T
top
>B
S/(S/NP) S[dcl]/NP
>
S[dcl]
<
S[dcl]
Figure 1: Chinese CCG derivation: ?That movie, I?ve already seen.?
A CCG grammar defines atomic categories, e.g.
NP and S, which may be recursively constructed
into complex categories, e.g. N/N and S\NP.1
Figure 1 shows how combinators govern the inter-
action of categories for lexical items, while slashes
specify argument directionality.
The combinators allow us to reduce lexical am-
biguity, by preserving a word?s canonical category
even when displaced from its canonical position.
This facility is a strength of CCG, but elevates its
generative power to mild context-sensitivity.
Some combinators may be disabled in a given
language ? the multi-modal CCG (Baldridge, 2002)
allows these distinctions to be lexically specified.
Introducing non-CCG rules decrease categorial
ambiguity at the expense of deviating from the for-
malism. Hockenmaier and Steedman (2002) show
that these greatly improve lexical coverage. Their
analysis of English employs non-CCG rules to co-
erce a verb phrase headed by a participle (category
S[ng]\NP) to a post-nominal modifier:
S[ng]\NP?? NP\NP (1)
This frees verbs from having to possess a dis-
tinct category in each position, thus trading off lex-
ical ambiguity for derivational ambiguity. Honni-
bal and Curran (2009) extended CCG with hat cat-
egories, enabling the lexical specification of these
unary type-change rules.
Hockenmaier and Steedman (2002, 2007) de-
veloped CCGbank, the first wide-coverage English
CCG corpus, by converting 1.2 million words from
the Wall Street Journal section of the PTB. CCG-
bank has made possible the development of wide-
coverage statistical parsers for CCG in English, no-
tably C&C (Clark and Curran, 2007).
1
Abbreviations in this paper: The directionless slash |
stands for one of {/,\}. We also use the verbal category ab-
breviations VP? S\NP and TV? (S\NP)/NP.
3 Penn Chinese Treebank
Xue  et al  (2005)  developed  the  Penn  Chinese
Treebank (PCTB), the first syntactically annotated
corpus for Chinese. The corpus includes newswire
text, magazine articles, and transcribed speech.
2
Xue et al  establishes several principles for a
more disciplined and consistent style of annota-
tion compared to the original PTB.  These princi-
ples include complement/adjunct marking: allow-
ing the recovery of predicate-argument structure;
limited semantic role marking: the annotation of
modifier phrases with semantic roles; covert ar-
gument marking: the retention of traces of argu-
ments deleted through pro-drop; and NP internal
structure: bracketing of NP structure where the in-
tended interpretation is clear.
The one  relation  per  bracketing principle
unambiguously  encodes  a  grammatical  relation
(chiefly, predication, adjunction, or complementa-
tion) through the configuration of a node and its
children. Xue et al developed this principle to as-
sist conversions from the PTB, e.g. Hockenmaier
(2003), in resolving argument/adjunct distinctions.
PCTB derivations  are  pre-segmented, pre-
tokenised, and POS tagged. Owing to the dearth
of  morphology in  Chinese, the  concept  of part
of speech is more fluid than that of English ? the
word ?? bijiao ?compare? might  be  glossed
as a verb, adjective, adverb, or noun depending
on  its  context. Noun/verb  mis-taggings  are  a
frequent error case for PCFG parsing on PCTB data,
compounded in Chinese by the lack of function
words  and  morphology  (Levy  and  Manning,
2003). This ambiguity is better handled by the
adaptive multitagging approach used by Clark and
Curran (2007) for CCG supertagging, in which each
lexical item is tagged with a set of CCG categories.
We present our CCG analysis of Chinese syntax
below, followed by our conversion algorithm.
2
We use the Penn Chinese Treebank 6.0 (LDC2007T36).
1084
4 The syntax of Chinese
4.1 Basic clause structure
Chinese is typologically SVO, with some OV el-
ements  (relative  clauses, adjunct  PPs  and noun
modifiers precede their heads). Numbers and de-
terminers may not modify nouns directly; a mea-
sure word must intervene.
The  category  structure  of  the  grammar  may
be inferred directly from headedness information.
Heads subcategorise for the type, number and di-
rectionality of their arguments, while adjuncts re-
ceive modifier categories of the form X | X.
(2) ?
I
NP
?
at
(VP/VP)/NP
??
supermarket
NP
?
buy
VP/NP
?
PERF
VP\VP
?
one
(N/N)/M
?
box:MW
M
??
eggs
N
I bought a box of eggs at the supermarket.
4.2 Topicalisation
In topic-prominent languages, the topic refers to
information which the speaker assumes is known
by the listener. In Mandarin, topicalisation mani-
fests as left-dislocation of the topic phrase (Li and
Thompson, 1989). We distinguish gap and non-
gap topicalisation depending on whether the topic
is co-referent with a gap in the sentence.
3
For gapped topicalisation (cf. Figure 1), we
adopt the Steedman (1987) topicalisation analysis:
T ? S/(S/T ) for parametrically licensed T (3)
For non-gap topicalisation (Example 5), we use
a variation of the analysis described in Hocken-
maier and Steedman (2005), which treats the topi-
calised constituent as a sentential modifier. Under
this analysis, the determiner in a topicalisedNP re-
ceives (S/S)/N instead of its canonical category
NP/N. Instead, we propose a unary rule:
T ? S/S for topicalisation candidate T (4)
This delays the coercion to sentential modifier type
(i.e. NP? S/S) until after the NP has been con-
solidated, allowing the words under the topicalised
NP to preserve their canonical categories.
3
Non-gap topicalisation is also known as the double sub-
ject construction (Li and Thompson, 1989).
(5) (As for) trade, it has developed rapidly.
?? ?? ? ?
trade development very fast
NP NP VP/VP VP
T >T >
S/S S/(S\NP) S\NP
>
S
>
S
Topicalisation  is  far  less  marked  in  Chinese
than in English, and the structure of topicalised
constituents  is  potentially  quite  complex. The
additional  categorial  ambiguity  in  Hockenmaier
and Steedman (2005) compounds the data sparsity
problem, leading us to prefer the unary rule.
4.3 Pro-drop
Since Chinese exhibits radical pro-drop (Neele-
man and Szendro?i, 2007), in which the viability of
the pro-drop is not conditioned on the verb, the cat-
egorial ambiguity resulting from providing an ad-
ditional argument-dropped category for every verb
is prohibitive.
Rather than engendering sparsity on verbal cate-
gories, we prefer derivational ambiguity by choos-
ing the unary rule analysis S[dcl] | NP? S[dcl] to
capture Chinese pro-drop.
4.4 Zero copula
Although the Chinese copula ? shi is obligatory
when equating NPs, it may be omitted when equat-
ing an NP and a QP or PP (Tiee and Lance, 1986).
4
(6) ?
NP
3SG
??
VP/VP
this-year
??
(S\NP)/M
18
?
M
years-old
She is 18 this year.
A solution  involving  a  binary  rule
NP QP? S[dcl] is  not  properly  headed, and
thus  violates  the  Principle  of  Lexical  Head
Government  (Steedman,  2000). Conversely, a
solution  where, for  example, ?? ?18? would
have to receive the category (S[dcl]\NP)/M in-
stead of its canonical category QP/M would lead
to  both  data  sparsity  and  over-generation, with
VP modifiers  becoming able  to  modify  the  QP
directly. Tentatively, we ignore the data sparsity
consequences, and  have ?? ?18? receive  the
category (S[dcl]\NP)/M in this context.
4
The copula is ungrammatical in predication on an adjec-
tival verb, such as?? ?happy?. However, we analyse such
words as verbs proper, with category S[dcl]\NP.
1085
4.5 ? ba- and? bei-constructions
? bei and? ba introduce a family of passive-like
constructions in Chinese. Although superficially
similar, the resulting constructions exhibit distinct
syntax, as our CCG analysis reflects and clarifies.
In the? bei-construction, the patient argument
of a verb moves to subject position, while the agent
either becomes the complement of a particle? bei
(the long passive), or disappears (the short pas-
sive; Yip and Rimmington, 1997). Although the
two constructions are superficially similar (appar-
ently differing only by the deletion of the agent
NP), they behave differently in more complex con-
texts (Huang et al, 2008).
The long passive occurs with or without an ob-
ject gap (deleted by identity with the subject of
the matrix verb). We analyse this construction by
assigning ? bei a category which permutes the
surface positions of the agent and patient. Co-
indexation  of  heads  allows  us  to  express  long-
distance dependencies.
Bei receives ((S\NP
y
)/((S\NP
x
)/NP
y
))/NP
x
in  the  gapped  case  (cf.  Example 7)  and
((S\NP)/(S\NP
x
))/NP
x
in the non-gapped case.
(7) Zhangsan was beaten by Lisi.
?? ? ?? ??
Z. BEI L. beat-PERF
NP (VP/TV )/NP
y
NP TV
>(S\NP
x
)/((S\NP
y
)/NP
x
)
>
S\NP
x
<
S
Short  passives also occur with or  without  an
object gap, receiving (S\NP
x
)/((S\NP)/NP
x
) in
the gapped case and (S\NP)\(S\NP) in the non-
gapped case. Our analysis agrees with Huang et al
(2008)?s observation that short-bei is isomorphic
to English tough-movement: our short-bei cate-
gory is the same as Hockenmaier and Steedman
(2005)?s category for English tough-adjectives.
In the ? ba construction, a direct object be-
comes the complement of the morpheme ? ba,
and  gains  semantics  related  to  ?being  affected,
dealt with, or disposed of? (Huang et al, 2008). As
for? bei, we distinguish two variants depending
on whether the object is deleted under coreference
with the complement of ? ba.
Ba receives ((S\NP
y
)/((S\NP
y
)/NP
x
))/NP
x
in  the  gapped  case  (cf.  Example 8), and
((S\NP
y
)/(S\NP
y
))/NP in the non-gapped case.
As Levy and Manning (2003) suggest, we re-
shape the PCTB analysis of the ba-construction so
Tag Headedness Example
VSB head-final ?? ?? ?plan [then] build?
VRD right-adjunction ? ? ?cook done?
VCP head-initial ?? ? ?confirm as?
VCD appositive ?? ?? ?invest [&] build-factory?
VNV special ? ? ? ?go [or] not go?
VPT special ? ? ? ?leave able away?
Table 1: Verb compounds in PCTB
that ba subcategorises for its NP and VP, rather
than subcategorising for an IP sibling, which al-
lows the NP to undergo extraction.
(8) The criminals were arrested by the police.
?? ? ?? ???
police BA criminal arrest-PERF
NP (VP/TV )/NP NP TV
>(S\NP
y
)/((S\NP
y
)/NP
x
)
<
S\NP
y
<
S
4.6 Verbal compounding
Verbs resulting from compounding strategies are
tagged and internally bracketed. Table 1 lists the
types distinguished by the PCTB, and the headed-
ness we assign to compounds of each type.
Modifier-head compounds (PCTB tag VSB) ex-
hibit clear head-final semantics, with the first verb
V1 causally or temporally precedingV2. Verb coor-
dination compounds (VCD) project multiple heads,
like ordinary lexical coordination.
In a resultative compound (VRD), the result or
direction ofV1 is indicated byV2, which we treat as
a post-verbal modifier. The V-not-V construction
(VNV) forms a yes/no question where V1 = V2. In
the V-bu/de-V or potential verb construction (VPT),
a disyllabic verbV =V1V2 receives the infix? de
or? bu with the meaning can/cannot V . In both
these cases, it is the infixed particle? de or? bu
which collects its arguments on either side.
4.7 Extraction
In the Chinese relative clause construction, the par-
ticle ? de links a sentence with a subject or ob-
ject gap with a NP to which that gap co-refers,
in an analysis similar to the English construction
described by Hockenmaier and Steedman (2005),
mediated by the relative pronoun that.
As in the English object extraction case, forward
type-raising on the subject argument, and forward
composition into the verbal category allows us to
obtain the correct object gap category S/NP.
1086
4.8 Right node raising
Two coordinated verbs may share one or more con-
tiguous arguments under right node raising. This
analysis follows directly from the CCG definition of
coordination, requiring no new lexical categories.
(9) Scholars have formulated and are releasing
the documents.
?? ?? ? ?? ??
scholar formulate and release document
NP VP/NP con j VP/NP NP
????
(VP/NP)[con j]
?????
VP/NP
>
S\NP
<
S
4.9 Apposition
Apposition is the juxtaposition of two phrases re-
ferring to the same entity. Unlike noun modifica-
tion, no clear modification relationship holds be-
tween the two phrases. The direct juxtaposition
rules out Hockenmaier?s (2003) analysis where a
delimiting comma mediates the apposition. Chi-
nese also allows full sentence/NP apposition:
(10) (??
(users
??
waste
?)
S
water)
S
??
NP
incident
NP
incidents of users wasting water
This gives rise to the Chinese apposition binary
rules NP NP? NP and S[dcl] NP? NP.
5 The translation pipeline
5.1 Tagging
Each PCTB internal node structurally encodes a con-
figuration, which lets us distinguish head-initial
and head-final complementation from adjunction
and predication (Xue et al, 2000).
The tagging mechanism annotates the PCTB tag
of each internal node with a marker, which pre-
serves this headedness information, even after the
nodes are re-structured in the binarisation phase.
Hockenmaier?s  (2003)  conversion  algorithm
uses the Magerman (1994) head-finding heuristics,
a potential source of noise. Fortunately, the PCTB
encodes gold standard headedness data.
The  tagging  algorithm  is  straightforward: if
a  node  and  its  children  unify  with  one  of  the
schemata below, then the markers (e.g. :l or :n)
are attached to its children. The markers l and r
indicate complements left, or right of the head h;
adjuncts are marked with a.
Head-initial, -final complementation
XP
ZP:r . . .YP:rX:h
XP
X:hZP:l. . . YP:l
Adjunction, predication
XP
XP:hZP:a. . . YP:a
IP
YP:hXP-SBJ:l
Topicalisation (gap and non-gap)
IP
YP:rXP-SBJ:lZP-TPC(-i):T(t)
Coordination
XP
XP:c{CC,PU})+(XP:c({CC,PU})
Others identify nodes with special syntax, such
as topicalisation (t/T), apposition (A) or coordina-
tion (c), for special treatment in following phases.
NP internal structure
To speed annotation, NP internal structure is often
left underspecified in PCTB (Xue et al, 2005), as in
the Penn Treebank. As a result, 68% of non-trace
NPs in PCTB have only a flat bracketing.
We assume that the internal structure of flat NPs
is right-branching and head-final (Li and Thomp-
son, 1989), following Hockenmaier and Steedman
(2005), who assume this structure for English. A
re-analysis of PCTB, like Vadas and Curran (2007)
for the PTB, could restore this structure, and allow
our conversion algorithm to yield the correct CCG
analysis with no further modifications.
To obtain this default analysis, each node under
NP internal structure receives the marker n, except
the the final node, the head, which receives N.
5.2 Binarisation
CCG combinators take at most two categories, in-
ducing binary derivation trees. As such, PCTB trees
must be re-shaped to accommodate a CCG analysis.
Our markers control the shape of the binarised
structure: head-initial complementation yields a
left-branching tree, while head-final complemen-
tation, adjunction, predication, coordination, and
NP internal  structure  all  yield  right-branching
trees. Following Hockenmaier (2003), sentence-
final punctuation is attached high.
Although  the  distinction  between  word-level
tags (such as NN, VA) and phrasal tags (such as NP,
VP, LCP) enables the configurational encoding of
grammatical relations, it leaves a large number of
1087
VP ? VV,VE,VA,VRD ADJP ? JJ
ADVP ? AD, CS CLP ? M
LCP ? LC DP ? DT, OD
LST ? OD INTJ ? IJ
FLR ? any node PP ? P
Figure 2: Pruned unary projections
unary projections. While an intransitive verb (e.g.
?? ?sleep?) would carry the verbal PCTB tag VV,
and a transitive verb combined with its object (e.g.
???? ?ate dinner?) is annotated as VP, under
CCG?s freer concept of constituency, both receive
the category S\NP.
Pruning the unary projections in Fig. 2 prevents
spurious category labellings in the next phase.
5.3 Labelling
We label each node of the binarised tree with CCG
categories, respecting the headedness information
encoded in the markers.
Atomic categories
The chosen mapping from PCTB tags to categories
defines the atomic category set for the grammar.
The richer representation in CCG categories permits
some constituents to be expressed using a smaller
set of atoms (e.g. an adjective is simply a noun
modifier ? N/N). Despite their critical importance
in controlling the degree of under-/over-generation
in the corpus, little guidance exists as to the selec-
tion of atomic categories in a CCG grammar. We
observed the following principles:
Modifier proliferation: when  two  classes  of
words can be modified by the same class of modi-
fiers, they should receive a single category;
Over-generation: the atom set should not over-
generalise to accept ungrammatical examples;
Efficiency: the representation may be motivated
by the needs of applications such as parsers.
Table 2 shows the eight atomic categories cho-
sen for our corpus. Two of these categories: LCP
(localisers) andM (measure words) have variously
been argued to  be  special  sub-classes  of  nouns
(Huang et al, 2008). However, based on our over-
generation criterion, we decided to represent these
as atomic categories.
We  adopt  the  bare/non-bare  noun  distinction
from Hockenmaier and Steedman (2007) on pars-
ing efficiency grounds. Although they roughly
correspond to English PPs, the distributional dif-
ferences between PPs, LCPs and QPs justify their
LCP Localiser phrase PP Prepositional phrase
M Measure word QP Quantifier phrase
N Bare noun S Sentence
NP Noun phrase conj Conjunction word
Table 2: Chinese CCGbank atomic category set
inclusion as atoms in Chinese. Future work in
training a wide-coverage parser on Chinese CCG-
bank will evaluate the impact of these choices.
Labelling algorithm
We developed a recursive algorithm which applies
one of  several  labelling functions  based on the
markers on a node and its children.
The algorithm proceeds top-down and assigns
a CCG category to every node. The markers on a
node?s children are matched against the schema
of Table 3, applying the categories of the match-
ing schema to the children. The algorithm is then
called recursively on each child. If the algorithm
is called on an unlabelled node, the mapping from
PCTB tags is used to assign a CCG category.
Predication
C
C\LL
Left  absorp-
tion
C
Cp
Adjunction
C
CC/C:a
Right
absorption
C
pC
Right
adjunction
C
C\C:aC Coordination
C
C[conj]C:c
Head-initial
C
RC/R:h
Partial
coordination
C[conj]
C:cconj
Head-final
C
C\L:hL Apposition
NP
NPXP:A
Table 3: Category labelling schemata
Left-  and  right-absorption  are  non-CCG rules
which functionally ignore punctuation, assuming
that they project no dependencies and combine to
yield the same category as their non-punctuation
sibling (Hockenmaier and Steedman, 2007). In the
schema, p represents a PCTB punctuation POS tag.
NPs  receive  a  head-final  bracketing  (by  our
right-branching assumption), respecting NP inter-
nal structure where provided by PCTB:
N
N
?? struct.
N
?? org.
N/N
N/N
?? bank
N/N
?? China
(N/N)/(N/N)
1088
6 Post-processing
A number of cases remain which are either not
covered by the general translation algorithm, or
otherwise could be improved in a post-processing
step. The primary disharmony at this stage is the
presence of traces, the  empty categories  which
the PCTB annotation style uses to mark the canoni-
cal position of extraposed or deleted constituents.
19,781 PCTB derivations (69.9%) contain a trace.
Since CCG aims  to  provide  a  transparent  inter-
face between surface string syntax and semantics,
traces are expressly disallowed (Steedman, 2000).
Hence, we eliminate traces from the annotation, by
devising alternate analyses in terms of categories
and combinatory rules.
Subject/object extraction
8966 PCTB derivations (31.7%) contain a subject
extraction, while 3237 (11.4%) contain an object
extraction. Figure 3 shows the canonical represen-
tation of subject extraction in the PCTB annotation
style. The PCTB annotation follows the X
?
analysis
of the relative clause construction as described by
Wu (2004), which we transform into an equivalent,
trace-free CCG analysis.
NP (N)
??
NP document
CP (N/N)
CP (N/N)
?
DEC
IP (S[dcl])
VP (S[dcl]\NP)
??
NP market
??
VV std.ize
NP-SBJ (NP)
*T*-i
WHNP-i
*OP*
Figure 3: ?the document which standardises the
market?
First, the Spec trace, WHNP-i, coindexed with
the extracted argument(s), is deleted. Next, the
extracted argument(s) with matching indices are
deleted, and category structure is adjusted to gen-
erate the correct gap category.
Modifier categories
Under our analysis, aspect particles such as ? le
(perfective) and ? guo (experiential) are verbal
post-modifiers, corresponding to right adjunction
in Table 3. Accordingly, an aspect particle fol-
lowing a transitive verb VP/NP will receive the
modifier category (VP/NP)\(VP/NP). Under this
analysis, every verbal category gives rise to one
possible modifier category for each aspect particle,
leading to detrimental categorial ambiguity.
However, the  generalised  backward  crossed
composition  combinator  (Steedman,  2000)  lets
aspect  particles  retain  their  canonical  category
(S\NP)\(S\NP) regardless of the arity of the verb
they modify.
Transformations
The PCTB annotation style posits traces to account
for  gapping, control/raising, argument  sharing,
pro-drop and topicalisation. To effect the parsimo-
nious CCG analyses of Section 4, structural trans-
formations on the original PCTB trees are necessary
to accommodate the new analyses.
We  developed  a tgrep-like  language  which
identifies instances of Chinese constructions, such
as right node raising and pro-drop, whose PCTB an-
notation posits traces. The local trees are then re-
shaped to accommodate trace-free CCG analyses.
7 Evaluation
This  section  explores  the  coverage  characteris-
tics  of  Chinese  CCGbank, in  comparison  with
the English and German CCGbanks generated by
Hockenmaier. Our analysis follows Hockenmaier
(2006) in establishing coverage as the metric re-
flecting how well the target corpus has accounted
for constructions in the source corpus.
7.1 Corpus coverage
The Chinese CCGbank conversion algorithm com-
pletes  for  28,227  of  the  28,295  (99.76%) PCTB
trees. Annotation noise, and rare but legitimate
syntax, such as ellipsis, account for the coverage
lost in this phase. Following Hockenmaier and
Steedman (2005), we adjust the PCTB annotation
only for systematic tagging errors that lead to cat-
egory mis-assignments, maintaining as far as pos-
sible the PCTB bracketing.
269  derivations  (0.95%)  contain  unresolved
traces, resulting from annotation noise and rare
constructions (such as ellipsis) not currently han-
dled by our translation algorithm. In 468 (1.66%)
derivations, residues of PCTB tags not eliminated by
the translation algorithm generate malformed cate-
gories outside the allowed set (Table 2). Excluding
these cases, our conversion algorithm results in a
corpus of 27,759 (98.1%) valid derivations.
7.2 Category set
The Chinese CCGbank category set is compared
against existing CCG corpora derived from similar
automatic corpus conversions, to determine how
1089
well we have generalised over syntactic phenom-
ena in the source corpus.
A total of 1197 categories appear in the final
corpus, of which 329 occur at least ten times, and
478 are attested only once. By comparison, En-
glish CCGbank, contains 1286 categories, 425 of
which occur at least ten times, and 440 only once,
while German CCGbank has a category inventory
of 2506 categories, with 1018 attested only once.
5
7.3 Lexicon coverage
Lexical  item coverage  establishes  the  extent  to
which data sparsity due to unseen words is prob-
lematic in the source corpus, and hence in any cor-
pus derived from it. Hockenmaier and Steedman
(2001) showed that formalisms with rich tagsets,
such as CCG, are particularly sensitive to this spar-
sity ? while a lexical item may be attested in the
training data, it may lack the necessary category.
We divided the  27,759 valid  derivations  into
ten contiguous sections, performing ten-fold cross-
validation  to  determine  the  coverage  of  lexical
items and CCG categories in the resulting corpus.
Average coverage on lexical items is 73.38%,
while average coverage on categories is 88.13%.
94.46% of token types from the held-out set are
found in the training set. These figures compare to
86.7% lexical coverage (by type) and 92% (by to-
ken) in German CCGbank (Hockenmaier, 2006).
Although lexical coverage by token is comparable
to the German corpus, we observe a marked differ-
ence in coverage by type.
To explain this, we examine the most frequent
POS tags among the missing tokens. These are NN
(common nouns; 16,552 tokens), NR (proper noun;
8458), VV (verb; 6879), CD (numeral; 1814) and JJ
(adjective; 1257). The 100 most frequent missing
tokens across the ten folds comprise 48 NR tokens,
46 NR, 3 NT (temporal nouns), 2 JJ (adjectives) and
one VA (verbal adjective). Personal names are also
not tokenised into surnames and forenames in the
PCTB, increasing unseen NR tokens.
The  missing VVs  (verbs)  include  1342 four-
character compounds, fossilised idiomatic expres-
sions which are considered atomic verbs in the
PCTB annotation. Another  source  of  verb  spar-
sity stems from the PCTB analysis of verbal infix-
ation. Given a polysyllabic verb (e.g. ?? leave-
away ?leave?), we  can  add  the  adverbial  infix
5
All German verbs having at least two categories to ac-
count for German verbal syntax contributes to the greater size
of the category set (Hockenmaier, 2006).
? not to form a potential verb??? leave-not-
away ?unable to leave?. In the PCTB annotation,
however, this results in lexical items for the two
cleaved parts, even though? leave can no longer
stand alone as a verb in modern Chinese. In this
case, a morphologically decomposed representa-
tion which does not split the lexical item could mit-
igate against this sparsity. Alternatively, candidate
verbs for this construction could have the first verb
fragment subcategorise for the second.
8 Conclusion
We have developed the first analysis of Chinese
with Combinatory Categorial Grammar, crafting
novel CCG analyses for a range of constructions in-
cluding topicalisation, pro-drop, zero copula, verb
compounding, and the  long-range dependencies
resulting from the? ba- and? bei-constructions.
We have presented an elegant and economical
account of Chinese syntax that exploits the power
of CCG combinatory rules, supporting Steedman?s
claim to its language-independence.
We have designed a conversion algorithm to ex-
tract this analysis from an existing treebank, avoid-
ing the massive cost of hand re-annotation, creat-
ing a corpus of 27,759 CCG derivations, covering
98.1% of the PCTB. The corpus will be publicly re-
leased, together with the converter, providing the
tools to create CCGbanks in new languages.
At release, Chinese CCGbank will include gold-
standard head co-indexation data, as required for
the training and evaluation of head-driven depen-
dency parsers. Co-indexation analyses, like those
provided for the ? ba- and ? bei-constructions,
will be extended to all categories.
Future refinements which could be brought to
bear  on  Chinese  CCGbank include  the  integra-
tion of PropBank data into CCGbank (Honnibal
and Curran, 2007; Boxwell and White, 2008) us-
ing Chinese PropBank (Xue, 2008). The hat cat-
egories of Honnibal and Curran (2009) may bet-
ter  handle  form/function  discrepancies  such  as
the Chinese zero copula construction, leading to
cleaner, more general analyses.
We  have  presented  a  wide-coverage  Chinese
corpus which exploits the strengths of CCG to anal-
yse a range of challenging Chinese constructions.
We are now ready to develop rich NLP tools, includ-
ing efficient, wide-coverage CCG parsers, to ad-
dress the ever-increasing volumes of Chinese text
now available.
1090
Acknowledgements
James Curran was  supported  by Australian  Re-
search Council (ARC) Discovery grant DP1097291
and  the  Capital  Markets  Cooperative  Research
Centre.
References
Jason Baldridge. 2002. Lexically Specified Derivational Con-
trol in Combinatory Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Stephen Boxwell and Michael White. 2008. Projecting Prop-
bank roles onto the CCGbank. Proceedings of LREC 2008.
Michael Burke and Olivia Lam. 2004. Treebank-based ac-
quisition of a Chinese lexical-functional grammar. In Pro-
ceedings of the 18th Pacific Asia Conference on Language,
Information and Computation, pages 161?172.
Aoife Cahill, Mairead McCarthy, Josef van Genabith, and
Andy  Way.  2002. Automatic  annotation  of  the  Penn
Treebank with LFG F-structure information. In LREC
2002 Workshop on Linguistic Knowledge Acquisition and
Representation-Bootstrapping Annotated Language Data,
pages 8?15.
Jeongwon Cha, Geunbae Lee, and Jonghyeok Lee. 2002. Ko-
rean combinatory categorial grammar and statistical pars-
ing. Computers and the Humanities, 36(4):431?453.
John  Chen, Srinivas  Bangalore, and  K. Vijay-Shanker.
2005. Automated extraction of  Tree-Adjoining Gram-
mars  from treebanks. Natural  Language Engineering,
12(03):251?299.
Stephen Clark and James R. Curran. 2007. Wide-Coverage
Efficient  Statistical  Parsing  with  CCG and  Log-Linear
Models. In Computational Linguistics, volume 33, pages
493?552.
Yuqing Guo, Josef van Genabith, and Haifeng Wang. 2007.
Treebank-based acquisition of LFG resources for Chinese.
In Proceedings of LFG07 Conference, pages 214?232.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Julia Hockenmaier. 2006. Creating a CCGbank and a wide-
coverage CCG lexicon for German. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the ACL, pages
505?512. Morristown, NJ, USA.
Julia Hockenmaier and Mark Steedman. 2001. Generative
models for statistical  parsing with combinatory catego-
rial grammar. In ACL ?02: Proceedings of the 40th An-
nual Meeting on Association for Computational Linguis-
tics, pages 335?342. Association for Computational Lin-
guistics, Morristown, NJ, USA.
Julia Hockenmaier and Mark Steedman. 2002. Acquiring
compact lexicalized grammars from a cleaner treebank. In
Proceedings of the Third International Conference on Lan-
guage Resources and Evaluation, pages 1974?1981.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank:
Users? manual. Technical report, MS-CIS-05-09, Com-
puter and Information Science, University of Pennsylva-
nia.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
Corpus of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computational Lin-
guistics, 33(3):355?396.
Beryl Hoffman. 1996. The computational analysis of the syn-
tax and interpretation of free word order in Turkish. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Matthew Honnibal and James R. Curran. 2007. Improving
the complement/adjunct distinction in CCGbank. In Pro-
ceedings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING-07), pages
210?217.
Matthew Honnibal and James R. Curran. 2009. Fully Lex-
icalising CCGbank with Hat Categories. In Proceedings
of the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1212?1221.
C.-T. James Huang, Y.-H. Audrey Li, and Yafei Li. 2008. The
syntax of Chinese. Cambridge University Press.
Roger Levy and Christopher Manning. 2003. Is it harder to
parse Chinese, or the Chinese Treebank? In Annual Meet-
ing of the Association for Computational Linguistics, vol-
ume 1, pages 439?446. Morristown, NJ, USA.
Charles N. Li and Sandra A. Thompson. 1989.Mandarin Chi-
nese: A functional reference grammar. University of Cal-
ifornia Press.
David M. Magerman. 1994. Natural language parsing as sta-
tistical pattern recognition. Ph.D. thesis, Stanford Univer-
sity.
Mitchell P.  Marcus, Beatrice  Santorini, and  Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-Oriented Grammar Development for Acquiring a
Head-Driven Phrase Structure Grammar from the Penn
Treebank. pages 684?693.
Ad Neeleman and Kriszta Szendro?i. 2007. Radical pro drop
and  the  morphology  of  pronouns. Linguistic  Inquiry,
38(4):671?714.
Mark Steedman.  1987. Combinatory  grammars  and par-
asitic  gaps. Natural  Language  &  Linguistic  Theory,
5(3):403?439.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
Cambridge, MA, USA.
Henry H.Y. Tiee and Donald M. Lance. 1986. A reference
grammar of Chinese sentences with exercises. University
of Arizona Press.
David Vadas and James R. Curran. 2007. Adding noun phrase
structure to the Penn Treebank. In Association for Com-
putational Linguistics, volume 45, page 240.
Xiu-Zhi Zoe Wu. 2004. Grammaticalization and language
change in Chinese: A formal view. Routledge.
Fei  Xia.  1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of Natural Language
Processing Pacific Rim Symposium ?99, pages 398?403.
Nianwen Xue. 2008. Labeling chinese predicates with seman-
tic roles. Computational Linguistics, 34(2):225?255.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase structure an-
notation of a large corpus. Natural Language Engineering,
11(02):207?238.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch.
2000. The Bracketing Guidelines for the Penn Chinese
Treebank (3.0). IRCS Report 00-08, University of Penn-
sylvania.
Po Ching Yip and Don Rimmington. 1997. Chinese: An es-
sential grammar. Routledge.
1091
Coling 2010: Poster Volume, pages 725?729,
Beijing, August 2010
Collective Semantic Role Labeling on Open News Corpus  
by Leveraging Redundancy 
 
 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
5School of Information Technologies 
The University of Sydney 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
dtse6695@it.usyd.edu.au 
zyxiong@cqu.edu.cn 
 
  
Abstract 
We propose a novel MLN-based method 
that collectively conducts SRL on 
groups of news sentences. Our method is 
built upon a baseline SRL, which uses 
no parsers and leverages redundancy. 
We evaluate our method on a manually 
labeled news corpus and demonstrate 
that news redundancy significantly im-
proves the performance of the baseline, 
e.g., it improves the F-score from 
64.13% to 67.66%.  * 
1 Introduction 
Semantic Role Labeling (SRL, M?rquez, 2009) 
is generally understood as the task of identifying 
the arguments of a given predicate and assigning 
them semantic labels describing the roles they 
play. For example, given a sentence The luxury 
auto maker sold 1,214 cars., the goal is to iden-
tify the arguments of sold and produce the fol-
lowing output: [A0 The luxury auto maker] [V 
sold] [A1 1,214 cars]. Here A0 represents the 
seller, and A1 represents the things sold (CoNLL 
2008 shared task, Surdeanu et al, 2008). 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
Gildea and Jurafsky (2002) first tackled SRL 
as an independent task, which is divided into 
several sub-tasks such as argument identifica-
tion, argument classification, global inference, 
etc. Some researchers (Xue and Palmer, 2004; 
Koomen et al, 2005; Cohn and Blunsom, 2005; 
Punyakanok et al, 2008; Toutanova et al, 2005; 
Toutanova et al, 2008) used a pipelined ap-
proach to attack the task. Some others resolved 
the sub-tasks simultaneously. For example, some 
work (Musillo and Merlo, 2006; Merlo and Mu-
sillo, 2008) integrated syntactic parsing and SRL 
into a single model, and another (Riedel and 
Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009) 
jointly handled all sub-tasks using Markov Log-
ic Networks (MLN, Richardson and Domingos, 
2005). 
All the above methods conduct sentence level 
SRL, and rely on parsers. Parsers have showed 
great effects on SRL performance. For example, 
Xue and Palmer (2004) reported that SRL per-
formance dropped more than 10% when they 
used syntactic features from an automatic parser 
instead of the gold standard parsing trees. Even 
worse, parsers are not robust and cannot always 
analyze any input, due to the fact that some in-
puts are not in the language described by the 
parser?s formal grammar, or adequately repre-
sented within the parser?s training data. 
725
We propose a novel MLN-based method that 
collectively conducts SRL on groups of news 
sentences to leverage the content redundancy in 
news. To isolate the negative effect of noise 
from parsers and thus focus on the study of the 
contribution of redundancy to SRL, we use no 
parsers in our approach. We built a baseline SRL, 
which depends on no parsers, and use the MLN 
framework to exploit  redundancy. Our intuition 
is that SRL on one sentence can help that on 
other differently phrased sentences with similar 
meaning. For example, consider the following 
sentence from a news article: 
A suicide bomber blew himself up Sunday in 
market in Pakistan's northwest crowded with 
shoppers ahead of a Muslim holiday, killing 
12 people, including a mayor who once sup-
ported but had turned against the Taliban, of-
ficials said. 
The state-of-art MLN-based system (Meza-Ruiz 
and Riedel, 2009), hereafter referred to as 
MLNBS for brevity, incorrectly labels northwest 
instead of bomber as A0 of killing. Now consider 
another sentence from another news article: 
Police in northwestern Pakistan say that a su-
icide bomber has killed at least 13 people and 
wounded dozens of others. 
Here MLNBS correctly identify bomber as A0 
of killing. When more sentences are observed 
where bomber as A0 of killing is correctly identi-
fied, we will be more confident that bomber 
should be labeled as A0 of killing, and that 
northwest should not be the A0 of killing accord-
ing to the constraint that one predicate has at 
most one A0. 
We manually construct a news corpus to 
evaluate our method. In the corpus, semantic 
role information is annotated and sentences with 
similar meanings are grouped together. Experi-
mental results show that news redundancy can 
significantly improve the performance of the 
baseline system. 
Our contributions can be summarized as fol-
lows: 
1. We present a novel method that conducts 
SRL on a set of sentences collectively, in-
stead of on a single sentence, by extend-
ing MLNBS to leverage redundancy. 
2. We show redundancy can significantly 
improve the performance of the baseline 
system, indicating a promising research 
direction towards open SRL. 
In the next section, we introduce news sen-
tence extraction and clustering. In Section 3, we 
describe our collective inference method. In Sec-
tion 4, we show our experimental results. Finally, 
in Section 5 we conclude our paper with a dis-
cussion of future work. 
2 Extraction and Clustering of News 
Sentences 
To construct a corpus to evaluate our method, 
we extract sentences from clustered news arti-
cles returned by news search engines such as 
Bing and Google, and divide them into groups 
so that sentences in a group have similar mean-
ing. 
News articles in the same cluster are supposed 
to report the same event. Thus we first group 
sentences according to the news cluster they 
come from. Then we split sentences in the same 
cluster into several groups according to the simi-
larity of meaning. We assume that two sentences 
are more similar in meaning if they share more 
synonymous proper nouns and verbs. The syno-
nyms of verbs, like plod and trudge, are mainly 
extracted from the Microsoft Encarta Diction-
ary1, and the proper nouns thesaurus, containing 
synonyms such as U.S. and the United States, is 
manually compiled. 
As examples, below are two sentence groups 
which are extracted from a news cluster describ-
ing Hurricane Ida. 
Group 1: 
? Hurricane Ida, the first Atlantic hurri-
cane to target the U.S. this year, plod-
ded yesterday toward the Gulf Coast? 
? Hurricane Ida trudged toward the Gulf 
Coast? 
? ? 
Group 2: 
? It could make landfall as early as Tues-
day morning, although it was forecast to 
weaken further. 
                                                 
1
http://uk.encarta.msn.com/encnet/features/dictionary/dictio
naryhome.aspx 
726
? Authorities said Ida could make landfall 
as early as Tuesday morning, although 
it was forecast to weaken by then. 
? ? 
3 Collective Inference Based on MLN 
Our method includes two core components: a 
baseline system that conducts SRL on every sen-
tence; and a collective inference system that ac-
cepts as input a group of sentences with prelimi-
nary SRL information provided by the baseline. 
We build the baseline by removing formulas 
involving syntactic parsing information from 
MLNBS (while keeping other rules) and retrain-
ing the system using the tool and scripts provid-
ed by Riedel and Meza-Ruiz (2008) on the man-
ually annotated news corpus described in Sec-
tion 4. 
A collective inference system is constructed 
to leverage redundancy in the SRL information 
from the baseline.  
We first redefine the predicate role and treat it 
as observed: 
predicate role: Int x Int x Int x Role; 
role has four parameters: the first one stands for 
the number of sentence in the input, which is 
necessary to distinguish the sentences in a group; 
the other three are taken from the arguments of 
the role predicate defined by Riedel and Meza-
Ruiz (2008), which denote the positions of the 
predicate and the argument in the sentence and 
the role of the argument, respectively. If the 
predication holds, it returns 1, otherwise 0.  
A hidden predicate final_role is defined to 
present the final output, which has the same pa-
rameters as the predicate role: 
predicate final_role: Int x Int x Int x Role; 
We introduce the following formula, which 
directly passes the semantic role from the base-
line to the final output: 
role(s, p, a, +r)=> final_role (s, p, a, +r)    (1) 
Here s is the sentence number in a group; p and 
a denote the positions of the predicate and ar-
gument in s, respectively; r stands for the role of 
the argument; the ?+? before the variable r indi-
cates that different r has different weight. 
Then we define another formula for collective 
inference: 
s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2, 
p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,
a2,a_lemma)^role(s2,p2,a2,+r)=>final_role 
(s1,p1,a1,+r)                                                 (2) 
Here p_lemma(a_lemma) stands for the lemma 
of the predicate(argument), which is obtained 
from the lemma dictionary. This dictionary is 
extracted from the dataset of CoNLL 2008 
shared task and is normalized using synonym 
dictionary described in Section 2; lemma is an 
observed predicate that states whether or not the 
word has the lemma. 
Formula 2 encodes our basic ideas about col-
lective SRL: given several sentences expressing 
similar meaning, if one sentence has a predicate 
p with an argument a of role r, the other sen-
tences would be likely to have a predicate p? 
with an argument a? of role r, where p? and a? 
are the same or synonymous with p and a, re-
spectively, as illustrated by the example in Sec-
tion 1. 
Besides, we also apply structural constraints 
(Riedel and Meza-Ruiz, 2008) to final_role. 
To learn parameters of the collective infer-
ence system, we use  thebeast (Riedel and Meza-
Ruiz, 2008),  which is an open Markov Logic 
Engine, and train it on manually annotated news 
corpus described in Section 4. 
4 Experiments 
To train and test the collective inference system, 
we extract 1000 sentences from news clusters, 
and group them into 200 clusters using the 
method described in Section 2. For every sen-
tence, POS tagging is conducted with the 
OpenNLP toolkit (Jason Baldridge et al, 2009), 
lemma of each word is obtained through the 
normalized lemma dictionary described in Sec-
tion 3, and SRL is manually labeled. To reduce 
human labeling efforts, we retrain our baseline 
on the WSJ corpus of CoNLL 2008 shared task 
and run it on our news corpus, and then edit the 
SRL outputs by hand. 
We implement the collective inference system 
with the thebeast toolkit. Precision, recall, and 
F-score are used as evaluation metrics.  In both 
training and evaluation, we follow the CoNLL 
2008 shared task and regard only heads of 
phrases as arguments. 
727
Table 1 shows the averaged 10-fold cross val-
idation results of our systems and the baseline, 
where the third and second line report the results 
of using and not using Formula 1 in our collec-
tive inference system, respectively. 
 
Systems Pre. (%) Rec. (%) F-score (%) 
Baseline 69.87 59.26 64.13 
CI-1 62.99 72.96 67.61 
CI 67.01 68.33 67.66 
Table 1. Averaged 10-fold cross validation re-
sults (Pre.: precision; Rec.: recall). 
Experimental results show that the two collec-
tive inference engines (CI-1 and CI) perform 
significantly better than the baseline in terms of 
the recall and F-score, though a little worse in 
the precision. We observe that predicate-
argument relationships in sentences with com-
plex syntax are usually not recognized by the 
baseline, but some of them are correctly identi-
fied by the collective inference systems. This, 
we guess, explains in large part the difference in 
performance. For instance, consider the follow-
ing sentences in a group, where order and tell 
are synonyms: 
? Colombia said on Sunday it will appeal 
to the U.N. Security Council and the 
OAS after Hugo Chavez, the fiery leftist 
president of neighboring Venezuela, or-
dered his army to prepare for war in or-
der to assure peace. 
? President Hugo Chavez ordered Vene-
zuela's military to prepare for a possible 
armed conflict with Colombia, saying 
yesterday that his country's soldiers 
should be ready if the U.S. tries to pro-
voke a war between the South American 
neighbors. 
? Venezuelan President Hugo Chavez told 
his military and civil militias to prepare 
for a possible war with Colombia as ten-
sions mount over an agreement giving 
U.S. troops access to Colombian mili-
tary bases. 
The baseline cannot label (ordered, Chavez, A0) 
for the first sentence, partially owing to the syn-
tactic complexity of the sentence, but can identi-
fy the relationship for the second and third sen-
tence. In contrast, the collective inference sys-
tems can identify Chavez in the first sentence as 
A0 of order because of its occurrence in the oth-
er sentences of the same group. 
As Table 1 shows, the CI system achieves the 
highest F-score (67.66%), and a higher precision 
than the CI-1 system, indicating the effective-
ness of Formula 1. Consider the above three sen-
tences. CI-1 mislabels (ordered, Venezuela, A1) 
for the first sentence because the baseline labels 
it for the second sentence. In contrast, CI does 
not label it for the first sentence because the 
baseline does not and (ordered, Venezuela, A1) 
rarely occurs in the outputs of the baseline for 
this sentence group. 
We also find cases where the collective infer-
ence systems do not but should help. For exam-
ple, consider the following group of sentences: 
? A Brazilian university expelled a woman 
who was heckled by hundreds of fellow 
students when she wore a short, pink 
dress to class, taking out newspaper ads 
Sunday to publicly accuse her of immo-
rality.  
? The university also published newspaper 
ads accusing the student, Geisy Arruda, 
of immorality. 
The baseline has identified (published, univer-
sity, A0) for the second sentence. But neither 
the baseline nor our method labels (taking, uni-
versity, A0) for the first one.  This happens be-
cause publish is not considered as a synonym 
of take, and thus (published, university, A0) in 
the second provides no evidence for (taking, 
university, A0) in the first. We plan to develop 
a context based synonym detection component 
to address this issue in the future. 
5 Conclusions and Future Work 
We present a novel MLN-based method that col-
lectively conducts SRL on groups of sentences. 
To help build training and test corpora, we de-
sign a method to collect news sentences and to 
divide them into groups so that sentences of sim-
ilar meaning fall into the same cluster. Experi-
mental results on a manually labeled news cor-
pus show that collective inference, which lever-
ages redundancy, can effectively improve the 
performance of the baseline. 
728
In the future, we plan to evaluate our method 
on larger news corpora, and to extend our meth-
od to other genres of corpora, such as tweets. 
 
References  
Baldridge, Jason, Tom Morton, and Gann. 2009. 
OpenNLP, http://opennlp.sourceforge.net/ 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labelling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Journal of Computa-
tional Linguistics, 28(3):245?288. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses 
using Markov Logic. Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the ACL, pages: 155-
163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and 
inference in semantic role labeling. Journal of 
Computational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. 
Collective semantic role labelling with Markov 
Logic. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 193-197. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
 
729
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 295?304,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The Challenges of Parsing Chinese with Combinatory Categorial Grammar
Daniel Tse and James R. Curran
School of Information Technologies
University of Sydney
Australia
{dtse6695,james}@it.usyd.edu.au
Abstract
We apply Combinatory Categorial Grammar
to wide-coverage parsing in Chinese with the
new Chinese CCGbank, bringing a formalism
capable of transparently recovering non-local
dependencies to a language in which they are
particularly frequent.
We  train  two  state-of-the-art  English ???
parsers: the parser of Petrov and Klein (P&K),
and the Clark and Curran (C&C) parser, uncov-
ering a surprising performance gap between
them not observed in English ? 72.73 (P&K)
and 67.09 (C&C) F -score on ???? 6.
We explore the challenges of  Chinese ???
parsing through three novel  ideas: develop-
ing corpus variants rather than treating the cor-
pus as fixed; controlling noun/verb and other
??? ambiguities; and quantifying the impact
of constructions like pro-drop.
1 Introduction
Automatic corpus conversions from the Penn Tree-
bank (Marcus et al, 1994) have driven research in
lexicalised grammar formalisms, such as ???? (Xia,
1999), ???? (Miyao et al, 2004) and ??? (Hock-
enmaier and Steedman, 2007), producing the lexical
resources key to wide-coverage statistical parsing.
The Chinese Penn Treebank (????;  Xue et al,
2005) has filled a comparable niche, enabling the
development of a Chinese ???? (Xia et al, 2000),
a wide-coverage ???? parser (Yu et al, 2011), and
recently Chinese CCGbank (Tse and Curran, 2010),
a 750 000-word corpus of Combinatory Categorial
Grammar (???; Steedman, 2000) derivations.
We train two ??? parsers, Clark and Curran (C&C;
2007), and the Petrov and Klein (P&K; 2007) ????
parser, on Chinese CCGbank. We follow Fowler and
Penn (2010), who treat the English CCGbank (Hock-
enmaier and Steedman, 2007) grammar as a ??? and
train and evaluate the P&K parser directly on it.
We obtain the first Chinese ??? parsing results:
F -scores  of  72.73 (P&K) and 67.09 (C&C)  on la-
belled dependencies computed over the ???? 6 test
set. While the state-of-the-art in Chinese syntactic
parsing has always lagged behind English, this large
gap is surprising, given that Fowler and Penn (2010)
found only a small margin separated the two parsers
on English CCGbank (86.0 versus 85.8).
Levy and Manning (2003) established that prop-
erties of Chinese such as noun/verb ambiguity con-
tribute to the difficulty of Chinese parsing. We focus
on two factors within our control: annotation deci-
sions and parser architecture.
Existing research has varied parsers whilst keep-
ing the corpus fixed. We vary the corpus whilst keep-
ing the parsers fixed by exploring multiple design
choices for particular constructions. By exploiting
the fully automatic CCGbank extraction process, we
can immediately implement these choices and assess
their impact on parsing performance.
Secondly, we contrast the performance of C&C,
with its tagging/parsing pipeline, with P&K, a parser
which performs joint tagging and parsing, and estab-
lish that P&K is less sensitive to the greater lexical
category ambiguity in Chinese CCGbank.
We demonstrate that Chinese ??? parsing is very
difficult, and propose novel techniques for identify-
ing where the challenges lie.
295
? ??? ? trap ? ?? ?? princess ? I ??? rescued
(S[dcl]\NP)/((S[dcl]\NP)/NP) (S[dcl]\NP)/NP (N/N)\(S[dcl]/NP) N NP (S[dcl]\NP)/NP
? ?T
S[dcl]\NP S/(S\NP)
? ?B
N/N S[dcl]/NP
?
N ?? NP
T
S/(S/NP)
?
S[dcl]
Figure 1: 3 types of non-local dependencies in 6 words: ?(As for) the trapped princess, I rescued (her).?
2 Background
Bikel and Chiang (2000) developed the first ????
parser, demonstrating  that  Chinese  was  similar
enough to English for techniques such as a Collins-
style head-driven parser or ??? to succeed. Later
???? parsers  used  Tree  Insertion  Grammar  (Chi-
ang  and  Bikel,  2002), ????s  (Levy  and  Man-
ning, 2003), the Collins models (Bikel, 2004) and
transition-based discriminative models (Wang et al,
2006; Zhang and Clark, 2009; Huang et al, 2009).
These systems also established the relative difficulty
of parsing Chinese and English; while ????????
scores over 92% are possible for English (McClosky
et al, 2006), systems for Chinese have achieved only
87% (Zhang and Clark, 2009) on the same metric.
Non-local dependencies (???s) are lexical depen-
dencies which hold over unbounded distances. Guo
et al (2007) observed that despite the importance of
???s for correct semantic interpretation, and the fact
that Chinese syntax generates more ???s than En-
glish, few parsers in Chinese are equipped to recover
the traces which mark ???s. For instance, extrac-
tion, a common ??? type, occurs more frequently in
???? sentences (38%) compared to ??? (17%).
A more satisfying approach is to use a grammar
formalism, such as ??? (Steedman, 2000), which
generates them inherently, enabling a unified parsing
model over local and non-local dependencies. This
approach is taken in the C&C parser (Clark and Cur-
ran, 2007), which can directly and transparently re-
cover ???s in English (Rimell et al, 2009).
Chinese CCGbank (Tse and Curran, 2010) demon-
strates that a parsimonious account of Chinese syn-
tax with ??? is  possible. Many familiar  objects
of Chinese syntax which generate ???s, including
the ? ba/? bei constructions, topicalisation and
extraction receive natural ??? analyses in Chinese
(a) Derivational
.
.
.S/S
..NP
..NP/N
.
..N
(b) Lexical
.
.
.S/S
..(S/S)/N
.
..N
Figure 2: Two types of ambiguity
CCGbank. Figure 1 shows the CCGbank analysis
of passivisation, topicalisation and extraction, creat-
ing ???s between?? princess and each of? ???,
? trap and?? rescue respectively.
We take two state-of-the-art parsers and train them
to establish the difficulty of parsing Chinese with
???. The first is the Clark and Curran (C&C; 2007)
parser, which uses supertagging (Clark and Curran,
2004), a local, linear-time tagging technique which
drastically  prunes  the  space  of  lexical  categories
which the polynomial-time parsing algorithm later
considers. The second is the coarse-to-fine parser
of Petrov and Klein (2007) which iteratively refines
its grammar by splitting production rules to uncover
latent distinctions. Fowler and Penn (2010) demon-
strate that the English CCGbank grammar is strongly
context-free, allowing them to treat it as a ??? and
train the Petrov and Klein (2007) parser directly.
2.1 Derivational vs. lexical ambiguity
The designer of a CCGbank must frequently choose
between derivational and lexical ambiguity (Hock-
enmaier, 2003; Tse and Curran, 2010). Derivational
ambiguity analyses special constructions through ar-
bitrary label-rewriting phrase structure rules, while
lexical ambiguity assigns additional categories to lex-
ical items for when they participate in special con-
structions.
296
Derivational and lexical ambiguity often arise in
??? because  of  the form-function  distinction ?
when the syntactic form of a constituent does not co-
incide with its semantic function (Honnibal, 2010).
For instance, in English, topicalisation causes an NP
to appear in clause-initial position, fulfilling the func-
tion of a sentential pre-modifier while maintaining
the form of an NP. Figure 2 shows two distinct ???
analyses which yield the same dependency edges.
Derivational ambiguity increases the parser search
space, while lexical ambiguity enlarges the tag set,
and hence the complexity of the supertagging task.
3 Three versions of Chinese CCGbank
We extract three versions of Chinese CCGbank to ex-
plore the trade-off between lexical and derivational
ambiguity, training both parsers on each corpus to
determine the impact of the annotation changes. Our
hypothesis is that the scarcity of training data in Chi-
nese means that derivational ambiguity results in bet-
ter coverage and accuracy, at the cost of increasing
time and space requirements of the resulting parser.
3.1 The lexical category LC (localiser)
In the following sentences, the words in bold have of-
ten been analysed as belonging to a lexical category
localiser (Chao, 1968; Li and Thompson, 1989).
(1) a. ??
house
??
inside:??
the inside of the house/inside the house
b. ?
big
?
tree
??
beside:??
(the area) beside the big tree
Localisers, like English prepositions, identify a (tem-
poral, spatial, etc.) extent of their complement. How-
ever, the combination Noun + Localiser is ambigu-
ous between noun function (the inside of the house)
and modifier function (inside the house).
We consider two possibilities to represent localis-
ers in ???, which trade derivational for lexical am-
biguity. In (2-a), a direct ??? transfer of the ????
analysis, the preposition? at expects arguments of
type LCP. In (2-b),? at now expects only NP argu-
ments, and the unary promotion LCP ? NP allows
LCP-form constituents to function as NPs.
(2) a. ? at ?? room ? in:??
PP/LCP NP LCP\NP
?
LCP
?
PP
b. ? at ?? room ? in:??
PP/NP NP LCP\NP
?
LCP ? NP
?
PP
The analysis in (2-a) exhibits greater lexical ambigu-
ity, with the lexical item? at carrying at least two
categories, PP/NP and PP/LCP, while (2-b) trades
off derivational for lexical ambiguity: the unary pro-
motion LCP ? NP becomes necessary, but ? at
no longer needs the category PP/LCP.
The base release of Chinese CCGbank, corpus A,
like (2-a), makes the distinction between categories
LCP and NP. However, in corpus B, we test the im-
pact of applying (2-b), in which the unary promotion
LCP ? NP is available.
3.2 The bare/non-bare NP distinction
The most frequent unary rule in English CCGbank,
occurring in over 91% of sentences, is the promotion
from bare to non-bare nouns: N ? NP. Hocken-
maier (2003) explains that the rule accounts for the
form-function distinction in determiner-less English
nouns  which nevertheless  have  definite  reference,
while preventing certain over-generations (e.g.
?the
the car). The N-NP distinction also separates adjec-
tives and noun modifiers (category N/N), from pre-
determiners (category NP/NP)  (Hockenmaier  and
Steedman, 2005), a distinction also made in Chinese.
While Chinese has strategies to mark definite or in-
definite reference, they are not obligatory, and a bare
noun is referentially ambiguous, calling into ques-
tion whether the distinction is justified in ???:
(3) a. ?
dog
?
very
??
clever
Dogs are clever.
b. ?
???
??
see
?
dog
I saw a dog/dogs.
c. ?
dog
??
run-away
?
???
The dog/dogs ran away.
297
The fact that the Chinese determiner is not necessar-
ily a maximal projection of the noun ? in other words,
the determiner does not ?close off? a level of NP ?
also argues against importing the English analysis.
In contrast, the English CCGbank determiner cate-
gory NP/N reflects the fact that determiners ?close
off? NP? further modification by noun modifiers is
blocked after combining with a determiner.
(4) ???
Republican Party
?
this
??
act
this action by the Republican Party
To test its impact on Chinese parsing, we create a
version of Chinese CCGbank (corpus C) which neu-
tralises the distinction. This eliminates the atomic
category N, as well as the promotion rule N ? NP.
4 Experiments
While a standard split of ???? 5 exists, as defined
by Zhang and Clark (2008), we are not aware of a
consistently used split for ???? 6. We present a
new split in Table 1 which adds data from the ???
broadcast section of ???? 6, maintaining the same
train/dev/test set proportions as the ???? 5 split.
We train C&C using the hybrid model, the best-
performing model for English, which extracts fea-
tures from the dependency structure (Clark and Cur-
ran, 2007). We use ? = ?0.055, 0.01, 0.05, 0.1? dur-
ing training with a Gaussian smoothing parameter
? = 2.4 (optimised on the corpus A dev set). We
use ? = ?0.15, 0.075, 0.03, 0.01, 0.005, 0.001? dur-
ing parsing, with the maximum number of supercats
(chart entries) set to 5,000,000, reflecting the greater
supertagging ambiguity of Chinese parsing.
The P&K parser is used ?off-the-shelf? and trained
with its default parameters, only varying the number
of split-merge iterations and enabling the Chinese-
specific lexicon features. The P&K parser involves
no explicit ??? tagging step, as the (super)tags cor-
respond directly to non-terminals in a ???.
Fowler  and  Penn  (2010)  use  the C&C tool
generate to convert P&K output to the C&C evalu-
ation dependency format. generate critically does
not depend on the C&C parsing model, permitting a
fair comparison of the parsers? output.
???? 5 +???? 6 #sents
Train 1?815, 1001?1136 2000?2980 22033
Test 816?885, 1137?1147 3030?3145 2758
Dev 900?931, 1148?1151 2981?3029 1101
Table 1: ???? 5 and 6 dev/train/test splits
4.1 Evaluation
Carroll  et al  (1998) argued against ???????? in
favour of a dependency-based evaluation. Rimell
et al  (2009)  focus  on  evaluating ??? recovery,
proposing a dependency-based evaluation and a ??
mapping procedure for inter-parser comparison.
Since the P&K parser plus generate produce de-
pendencies in the same format as C&C, we can use
the standard Clark and Curran (2007) dependency-
based evaluation from the ??? literature: labelledF -
score (LF ) over dependency tuples, as used for ???
parser evaluation in English. Critically, this metric
is also ???-sensitive. We also report labelled sen-
tence accuracy (Lsa), the proportion of sentences for
which the parser returned all and only the gold stan-
dard dependencies. Supertagger accuracy compares
leaf categories against the gold standard (stag).
For C&C, we report on two configurations: ????,
evaluated using gold standard ??? tags; and ????,
with automatic ??? tags provided by the C&C tagger
(Curran and Clark, 2003). For P&K, we vary the num-
ber of split-merge iterations from one to six (follow-
ing Fowler and Penn (2010), the k-iterations model
is called I-k). Because the P&K parser does not use
??? tags, the most appropriate comparison is against
the ???? configuration of C&C. For C&C, we use the
average of the logarithm of the chart size (logC) as
a measure of ambiguity, that is, the number of alter-
native analyses the parser must choose between.
Following Fowler and Penn (2010), we perform
two sets of experiments: one evaluated over all sen-
tences in a section, and another evaluated only over
sentences for which both parsers successfully parse
and generate dependencies.
We define the size of a ??? grammar as the num-
ber of categories it contains. The size of a grammar
affects the difficulty of the supertagging task (as the
size of a grammar is the size of the supertag set). We
also consider the number of categories of each shape,
as defined in Table 2. Decomposing the category in-
298
Shape Pattern
V (predicate-like) (S[dcl]\NP)$
M (modifier) X|X
P (preposition-like) (X|X)|Y
N (noun-like) N or NP
O (all others)
Table 2: Shapes of categories
model LF Lsa % stag cov logC
A
I-3 68.97 13.45 83.64 95.7 -
I-6 71.67 15.70 85.00 96.4 -
???? 75.45 16.70 89.43 99.4 14.55
???? 66.32 12.81 83.88 98.6 14.69
B
I-3 69.75 14.15 84.07 96.0 -
I-5 71.40 14.83 84.97 96.4 -
???? 75.41 16.67 89.50 99.6 14.74
???? 66.24 12.61 83.95 98.7 14.75
C
I-3 70.22 16.49 84.37 96.5 -
I-5 72.74 18.59 85.61 96.5 -
???? 76.73 20.56 89.66 99.5 13.58
???? 66.95 14.62 83.90 99.2 13.86
Table 3: Dev set evaluation for P&K and C&C
ventory into shapes demonstrates how changes to the
corpus annotation affect the distribution of types of
category. Finally, we calculate the average number
of tags per lexical item (Avg. Tags/Word), as a metric
of the degree of lexical ambiguity in each corpus.
5 Results
Table 3 shows the performance of P&K and C&C on
the three dev sets, and Table 4 only over sentences
parsed by both parsers. (A is the base release, B
includes the unary rule LCP ? NP, and C also
collapses the N-NP distinction.) For P&K on corpus
A, F -score and supertagger accuracy increase mono-
tonically as further split-merge iterations refine the
model. P&K on B and C overfits at 6 iterations, con-
sistent with Fowler and Penn?s findings for English.
The ?9% drop in F -score between the ???? and
???? figures shows that C&C is highly sensitive to
??? tagging accuracy (92.56% on the dev set, com-
pared to 96.82% on English). Considering Table 4,
each best P&K model outperforms the corresponding
???? model by 3-5%. However, while P&K is sub-
stantially better without gold-standard information,
gold ??? tags allow C&C to outperform P&K, again
model LF Lsa % stag cov
A
I-6 71.74 15.87 85.29 100.0
???? 67.50 15.36 84.52 100.0
B
I-5 71.40 14.97 85.26 100.0
???? 67.72 14.97 84.68 100.0
C
I-5 72.84 18.69 86.04 100.0
???? 68.43 16.17 84.57 100.0
Table 4: Dev set evaluation for P&K and C&C on
???? 6 sentences parsed by both parsers
model LF Lsa % stag cov logC
C
I-5 72.73 20.28 85.43 97.1 -
???? 76.89 22.90 89.63 99.1 14.53
???? 67.09 15.28 83.95 98.7 14.89
Table 5: Test set evaluation for P&K and C&C
demonstrating the impact of incorrect ??? tags.
Supertagging  and  parsing  accuracy  are  not  en-
tirely correlated between the parsers ? in corpora A
and B, ???? supertagging is comparable or better
than I-3, but F -score is substantially worse.
Comparing A and B in  Table 3, C&C receives
small increases in supertagger accuracy and cover-
age, but  parsing performance remains  largely  un-
changed; P&K performance degrades slightly. On
both parsers,C yields the best results out of the three
corpora, with LF gains of 1.07 (P&K), 1.28 (????)
and 0.63 (????) over the base Chinese CCGbank.
We select C for our remaining parser experiments.
Both C&C?s ???? and ???? results show higher
coverage than P&K (a combination of parse failures
in P&K itself, and in generate). Since F -score is
only computed over successful parses, it is possible
that P&K is avoiding harder sentences. In Table 4,
evaluated only over sentences parsed by both parsers
shows that as expected, C&C gains more (1.15%)
than P&K on the common sentences.
Table 5 shows that the behaviour of both parsers
on the test section is consistent with the dev section.
corpus
Avg. Grammar size
tags/word all f ? 10
A 1.84 1177 324
B 1.83 1084 303
C 1.79 964 274
Table 6: Corpus statistics
299
corpus V P M N O Total
A 791 158 56 2 170 1177
B 712 149 55 2 166 1084
C 670 119 41 1 133 964
Table 7: Grammar size, categorised by shape
5.1 Corpus ambiguity
To understand why corpus C is superior for parsing,
we compare the ambiguity and sparsity characteris-
tics of the three corpora. Examining logC, the aver-
age log-chart size (Table 3) shows that the corpus B
changes (the addition of the unary rule LCP ? NP)
increase ambiguity, while the additional corpus C
changes (eliminating the N-NP distinction, resulting
in the removal of the unary rule N ? NP) have the
net effect of reducing ambiguity.
Table 6 shows that the changes reduce the size
of the lexicon, thus reducing the average number of
tags each word can potentially receive, and therefore
the difficulty of the supertagging task. This, in part,
contributes to the reduced logC values in Table 3.
While the size of the lexicon is reduced in B, the cor-
responding logC figure in Table 3 increases slightly,
because of the additional unary rule.
Table 7 breaks  down the  size  of  each  lexicon
according to category shape. Introducing the rule
LCP ? NP reduces the number of V-shaped cat-
egories by 10%, while not substantially affecting the
quantity of other category shapes, because the sub-
categorisation frames which previously referred to
LCP are no longer necessary. Eliminating the N-NP
distinction, however, reduces the number of P and
M-shaped categories by over 20%, as the distinction
is no longer made between attachment at N and NP.
6 Error analysis
The  well-known noun/verb  ambiguity  in  Chinese
(where, e.g., ???? ?design-build? is both a ver-
bal compound ?design and build? and a noun com-
pound ?design  and  construction?)  greatly  affects
parsing accuracy (Levy and Manning, 2003).
However, little work has quantified the impact of
noun/verb ambiguity on parsing, and for that mat-
ter, the  impact  of  other  frequent  confusion types.
To quantify C&C?s sensitivity to ??? tagging errors,
Confusion LF ?LF stag cov
Base (????) 76.73 89.66 99.50
NR ?? NN 76.72 -0.01 89.64 99.37
JJ ?? NN 76.60 -0.12 89.57 99.37
DEC ?? DEG 75.10 -1.50 89.07 98.83
VV ?? NN 73.35 -1.75 87.68 98.74
All (????) 66.95 83.90 99.20
Table 8: Corrupting C&C gold ??? tags piecemeal on
???? 6 dev set of corpus C. ?LF is the change in
LF when each additional confusion type is allowed.
which we saw in Table 3, we perform an experiment
where we corrupt the gold ??? tags, by gradually re-
introducing automatic ??? errors on a cumulative ba-
sis, one confusion type at a time.
The notation X ?? Y indicates that the ??? tags X
and Y are frequently confused with each other by the
??? tagger. For example, VV ?? NN represents the
problematic noun/verb ambiguity, allowing the in-
clusion of noun/verb confusion errors.
Table 8 shows  that  while  the  confusion  types
NR ?? NN and JJ ?? NN have no impact on the evalua-
tion, the confusions DEC ?? DEG and VV ?? NN, intro-
duced one at a time, cause reductions in F -score of
1.50 and 1.75% respectively. This is expected; Chi-
nese CCGbank does not distinguish between noun
modifiers  (NN) and  adjectives  (JJ).  On  the  other
hand, the critical noun/verb ambiguity, and the con-
fusion between DEC/DEG (two senses of the particle
? de) adversely impact F -score. We performed an
experiment with C&C to merge DEC and DEG into a
single tag, but found that this increased category am-
biguity without improving accuracy.
The VV ?? NN confusion  is  particularly  damag-
ing to the ??? labelled dependency evaluation, be-
cause verbs generate a large number of dependencies.
While Fowler and Penn (2010) report a gap of 6.31%
between C&C?s labelled and unlabelled F -score on
the development set in English, we observe a gap of
10.35% for Chinese.
Table 10 breaks down the 8,414 false positives
generated  by C&C on  the  dev  set, according  to
whether the head of each dependency was incorrectly
???-tagged and/or supertagged. The top-left  cell
shows that despite the correct ??? and supertag, C&C
makes a large number of pure attachment location er-
rors. The vast majority of false positives, though, are
300
C&C ???? P&K I-5
category ???? dependency functionLF freq LF freq
0.78 4204 0.78 3106 NP/NP noun modifier attachment
0.73 2173 0.81 1765 (S[dcl]\NP)/NP transitive object
0.65 1717 0.72 1459 (S[dcl]\NP)/NP transitive subject
0.68 870 0.74 643 (S[dcl]\NP)/(S[dcl]\NP) control/raising S complement
0.70 862 0.67 697 S[dcl]\NP intransitive subject
0.60 670 0.69 499 (S[dcl]\NP)/(S[dcl]\NP) ? control/raising subject
0.55 626 0.54 412 (NP/NP)/(NP/NP) noun modifier modifier attachment
0.57 370 0.68 321 (NP/NP)\(S[dcl]\NP) subject extraction S complement
0.59 343 0.70 314 (NP/NP)\(S[dcl]\NP) ? subject extraction modifier attachment
0.59 110 0.69 84 (NP/NP)\(S[dcl]/NP) object extraction S complement
0.63 106 0.75 86 (NP/NP)\(S[dcl]/NP) ? object extraction modifier attachment
Table 9: Accuracy per dependency, for selected dependency types
correct ??? incorrect ???
correct stag 2307 (27.42%) 51 (0.61% )
incorrect stag 4493 (53.40%) 1563 (18.58%)
Table 10: Analysis of the 8,414 false positive depen-
dencies from C&C on ???? 6 dev set
caused by supertagging errors (the bottom row), but
most of these are not a result of incorrect ??? tags,
demonstrating that supertagging and parsing are dif-
ficult even with correct ??? tags.
The  sensitivity  of C&C to  tagging  errors, and
the higher performance of the P&K parser, which
does not directly use ??? tags, calls into question
whether ??? tagging yields a net gain in a language
where distinctions such as the noun/verb ambiguity
are often difficult to resolve using local tagging ap-
proaches. The approach of Auli and Lopez (2011),
which achieves superior results in English ??? pars-
ing with a joint supertagging/parsing model, may be
promising in light of the performance difference be-
tween P&K and C&C.
6.1 Non-local dependencies
Table 9 shows how well the best models of each
parser recovered selected local and non-local depen-
dencies. The slot represented by each row appears
in boldface. While C&C and P&K perform similarly
recovering NP-internal structure, the ability of P&K
to recover verbal arguments, unbounded long-range
dependencies such as subject and object extraction,
and bounded long-range dependencies such as con-
trol/raising constructions, is superior.
The C&C ???? parser appears to be biased to-
wards generating far  more of  the frequent  depen-
dency types, yet does not typically have a higher re-
call for these dependency types than P&K.
6.2 Pro-drop and its impact on ??? parsing
One of the most common types of unary rules in
Chinese  CCGbank, occurring  in  36% of  Chinese
CCGbank sentences, is  the subject  pro-drop  rule
S[dcl]\NP ? S[dcl], which accounts for the op-
tional absence of the subject pronoun of a verb for
pragmatic reasons where the referent can be recov-
ered from the discourse (Li and Thompson, 1989).
The subject pro-drop rule is problematic in Chi-
nese parsing because its left hand side, S[dcl]\NP, is
a very common category, and also because several
syntactic distinctions in Chinese CCGbank hinge on
the difference between S[dcl]\NP and S[dcl].
The latter point is illustrated by two of the senses
of ? de, the Chinese subordinating particle. Two
categories  which ? de receives  in  the  grammar
are (NP/NP)\(S[dcl]\NP) (introducing  a  relative
clause) and (NP/NP)\S[dcl] (in the construction S de
NP). Because subject pro-drop promotes any unsatu-
rated S[dcl]\NP to S[dcl], whenever the supertagger
returns both of the above categories for the lexical
item? de, the parser must consider two alternative
analyses which yield different dependencies:
(5) a. ti
ti
??
come out
?
??
??i
questioni
the questions which arise
301
English Chinese
PTB/PCTB-based 92.1% (McClosky et al, 2006) 86.8% (Zhang and Clark, 2009)
CCGbank-based
86.0% (Fowler and Penn, 2010) 72.7% (this work)
85.8% (Clark and Curran, 2007) 67.1% (this work)
Table 11: Summary of Chinese parsing approaches
model LF Lsa % stag cov logC
C
???? 74.99 7.42 89.36 98.6 18.35
(76.73 20.56 89.66 99.5 13.58)
???? 65.42 4.82 83.73 97.9 18.67
(66.95 14.62 83.90 99.2 13.86)
I-5 70.67 8.62 84.99 93.8 -
(72.74 18.59 85.61 96.5 -)
Table 12: Dev set evaluation for C&C over pro-drop
sentences only (and over full set in parentheses)
b. pro??
pro come out
?
??
??
question
the question of (him, her) coming out
38.1% of sentences in the development set contain
at least one instance of pro-drop. The evaluation
over only these sentences is given in Table 12. This
restricted  evaluation  shows  that  while  we  cannot
conclude that pro-drop is the causative factor, sen-
tences with pro-drop are much more difficult for both
parsers to analyse correctly, although the drops in F -
score and supertagging accuracy are largest for P&K.
Critically, the fact that supertagging performance
on these more difficult sentences is reasonably com-
parable with performance on the full set suggests
that the bottleneck is in the parser rather than the
supertagger. One measure of the complexity of pro-
drop sentences is the substantial increase in the logC
value of these sentences. This suggests that a key to
bringing parser performance on Chinese in line with
English lies in reining in the ambiguity caused by
very productive unary rules such as pro-drop.
7 Conclusion
Using Chinese CCGbank (Tse and Curran, 2010), we
have trained and evaluated the first ??? parsers for
Chinese in the literature: the Clark and Curran (C&C;
2007)  and Petrov and Klein  (P&K; 2007)  parsers.
The P&K parser substantially outperformed (72.73)
C&C with automatic ??? tags (67.09).
Table 11 summarises  the  best  performance  of
parsers on ??? and CCGbank, for English and Chi-
nese. We observe a drop in performance between En-
glish and Chinese ??? parsers which is much larger
than, but consistent with, ??? parsers. To close this
gap, future research in Chinese parsing should be in-
formed by quantifying the aspects of Chinese which
account most for the deficit.
We start by using corpus conversion to compare
different  linguistic  representation  choices, rather
than  for  generating  a  single  immutable  resource.
This can also be exploited to develop syntactic cor-
pora parameterised for particular applications. We
found that  collapsing categorial  distinctions  moti-
vated by theory can yield less ambiguous corpora,
and hence, more accurate parsers. We have also
taken a novel approach to investigating the impact
of noun/verb and other ??? ambiguities on parsing.
The large gap between Chinese C&C and P&K is
surprising, given that Fowler and Penn (2010) found
only a small gap for English. We found that C&C
is very sensitive to ??? tagging performance, which
leads to its inferior performance given automatically
assigned ??? tags. This suggests that joint supertag-
ging/parsing approaches, as performed by P&K, are
more suitable for Chinese. Finally, we have shown
that pro-drop is correlated with poor performance
on both parsers, suggesting an avenue to closing the
Chinese-English parsing gap.
While developing the first wide-coverage Chinese
??? parsers, we have shed light on the nature of the
Chinese-English parsing gap, and identified new and
significant challenges for ??? parsing.
Acknowledgements
We thank our anonymous reviewers for their insight-
ful and detailed feedback. James R. Curran was sup-
ported by Australian Research Council (???) Dis-
covery grant DP1097291 and the Capital Markets
Cooperative Research Centre.
302
References
Michael  Auli  and Adam Lopez.  2011. A comparison
of loopy belief propagation and dual decomposition
for integrated ccg supertagging and parsing. In 49th
Annual Meeting of the Association for Computational
Linguistics, pages 470?480. Association for Computa-
tional Linguistics.
DanielM. Bikel. 2004. On the parameter space of genera-
tive lexicalized statistical parsing models. Ph.D. thesis,
Citeseer.
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the Chinese Treebank. In
Second workshop on Chinese language processing, vol-
ume 12, pages 1?6. Morristown, NJ, USA.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In Pro-
ceedings of the 1st International Conference on Lan-
guage Resources and Evaluation, pages 447?454.
Yuen-Ren Chao. 1968. A grammar of spoken Chinese.
University of California Press.
David Chiang and Daniel M. Bikel. 2002. Recovering la-
tent information in treebanks. In Proceedings of the
19th international  conference on Computational  lin-
guistics, volume 1, pages 1?7. Association for Compu-
tational Linguistics.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proceedings of the 20th international conference on
Computational Linguistics. Association for Computa-
tional Linguistics.
Stephen  Clark  and  James R.  Curran.  2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. In Computational Linguistics, vol-
ume 33, pages 493?552.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and smoothing for maximum entropy taggers. In
Proceedings of the 10th Meeting of the EACL, pages
91?98. Budapest, Hungary.
Timothy A.D. Fowler and Gerald Penn.  2010. Accu-
rate context-free parsing with combinatory categorial
grammar. Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
335?344.
Yuqing Guo, Haifeng  Wang, and  Josef  Van Genabith.
2007. Recovering non-local dependencies for Chinese.
In EMNLP/CoNLL, pages 257?266.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank:
Users? manual. Technical report, MS-CIS-05-09, Com-
puter and Information Science, University of Pennsyl-
vania.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank:
A Corpus of CCG Derivations and Dependency Struc-
tures Extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
Matthew Honnibal. 2010. Hat Categories: Represent-
ing Form and Function Simultaneously in Combinatory
Categorial Grammar. Ph.D. thesis, University of Syd-
ney.
Liang  Huang, Wenbin  Jiang, and  Qun  Liu.  2009.
Bilingually-constrained  (monolingual)  shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical  Methods  in  Natural  Language  Process-
ing, volume 3, pages  1222?1231.  Association  for
Computational Linguistics.
Roger Levy and Christopher Manning. 2003. Is it harder
to parse Chinese, or the Chinese Treebank? In Annual
Meeting of the Association for Computational Linguis-
tics, volume 1, pages 439?446. Morristown, NJ, USA.
Charles N. Li and Sandra A. Thompson. 1989. Mandarin
Chinese: A functional reference grammar. University
of California Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the main conference on Human Language Tech-
nology Conference of the North American Chapter of
the Association of  Computational Linguistics, pages
152?159. Association for Computational Linguistics.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi  Tsujii.
2004. Corpus-Oriented Grammar Development for Ac-
quiring a Head-Driven Phrase Structure Grammar from
the Penn Treebank. pages 684?693.
Slav Petrov and Dan Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL HLT
2007, pages 404?411.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evaluation.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2-
Volume 2, pages 813?821. Association for Computa-
tional Linguistics.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
Cambridge, MA, USA.
303
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the Penn Chi-
nese Treebank. Proceedings of the 23rd International
Conference on Computational  Linguistics  (COLING
2010), pages 1083?1091.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006.
A fast, accurate deterministic parser for Chinese. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages  425?432.  Association  for  Computational  Lin-
guistics.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of  Natural Lan-
guage Processing Pacific Rim Symposium ?99, pages
398?403.
Fei Xia, Chung-hye Han, Martha Palmer, and Aravind
Joshi. 2000. Comparing lexicalized treebank grammars
extracted from Chinese, Korean, and English corpora.
In Proceedings of the second workshop on Chinese lan-
guage processing: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics, volume 12, pages 52?59. Association for
Computational Linguistics.
Nianwen  Xue, Fei  Xia, Fu-Dong  Chiou, and  Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(02):207?238.
Kun  Yu, Yusuke  Miyao, Takuya  Matsuzaki, Xiangli
Wang, and Jun?ichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In 12th International
Conference on Parsing Technologies, page 48.
Yue Zhang and Stephen Clark.  2008. A tale  of  two
parsers: investigating  and  combining  graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
562?571. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global discrim-
inative model. In Proceedings of the 11th International
Conference on Parsing Technologies, pages 162?171.
Association for Computational Linguistics.
304
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98?103,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Examination of Challenges in Chinese Parsing
Jonathan K. Kummerfeld
?
Daniel Tse
?
James R. Curran
?
Dan Klein
?
?
Computer Science Division
?
School of Information Technology
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,klein}@cs.berkeley.edu {dtse6695,james}@it.usyd.edu.au
Abstract
Aspects of Chinese syntax result in a dis-
tinctive mix of parsing challenges. How-
ever, the contribution of individual sources
of error to overall difficulty is not well un-
derstood. We conduct  a  comprehensive
automatic analysis of error types made by
Chinese parsers, covering a broad range of
error types for large sets of sentences, en-
abling the first empirical ranking of Chi-
nese error types by their performance im-
pact. We also investigate which error types
are resolved by using gold part-of-speech
tags, showing that improving Chinese tag-
ging  only  addresses  certain  error  types,
leaving substantial outstanding challenges.
1 Introduction
A decade of Chinese parsing research, enabled
by the Penn Chinese Treebank (PCTB; Xue et al,
2005), has seen Chinese parsing performance im-
prove from 76.7 F1 (Bikel and Chiang, 2000) to
84.1 F1 (Qian and Liu, 2012). While recent ad-
vances have focused on understanding and reduc-
ing the errors that occur in segmentation and part-
of-speech tagging (Qian and Liu, 2012; Jiang et al,
2009; Forst and Fang, 2009), a range of substantial
issues remain that are purely syntactic.
Early work by Levy and Manning (2003) pre-
sented modifications to a parser motivated by a
manual investigation of parsing errors. They noted
substantial differences between Chinese and En-
glish parsing, attributing some of the differences to
treebank annotation decisions and others to mean-
ingful differences in syntax. Based on this analysis
they considered how to modify their parser to cap-
ture the information necessary to model the syn-
tax within the PCTB. However, their manual ana-
lysis was limited in scope, covering only part of
the parser output, and was unable to characterize
the relative impact of the issues they uncovered.
This paper presents a more comprehensive ana-
lysis of errors in Chinese parsing, building on the
technique presented in Kummerfeld et al (2012),
which characterized the error behavior of English
parsers by quantifying how often they make er-
rors such as PP attachment and coordination scope.
To accommodate error classes that are absent in
English, we  augment  the  system  to  recognize
Chinese-specific parse errors.
1
We use the modi-
fied system to show the relative impact of different
error types across a range of Chinese parsers.
To understand the impact of tagging errors on
different  error  types, we  performed  a  part-of-
speech ablation experiment, in  which particular
confusions are introduced in isolation. By analyz-
ing the distribution of errors in the system output
with and without gold part-of-speech tags, we are
able to isolate and quantify the error types that can
be resolved by improvements in tagging accuracy.
Our analysis shows that improvements in tag-
ging accuracy can only address a subset of the chal-
lenges of Chinese syntax. Further improvement in
Chinese parsing performance will require research
addressing other challenges, in particular, deter-
mining coordination scope.
2 Background
The closest previous work is the detailed manual
analysis performed by Levy and Manning (2003).
While their focus was on issues faced by their fac-
tored PCFG parser (Klein and Manning, 2003b),
the error types they identified are general issues
presented by Chinese syntax in the PCTB. They
presented several Chinese error types that are rare
or absent in English, including noun/verb ambigu-
ity, NP-internal structure and coordination ambi-
guity due to pro-drop, suggesting that closing the
English-Chinese parsing gap demands techniques
1
The system described  in  this  paper  is  available  from
http://code.google.com/p/berkeley-parser-analyser/
98
beyond those currently used for English. How-
ever, as noted in their final section, their manual
analysis of parse errors in 100 sentences only cov-
ered a portion of a single parser?s output, limiting
the conclusions they could reach regarding the dis-
tribution of errors in Chinese parsing.
2.1 Automatic Error Analysis
Our  analysis  builds  on  Kummerfeld  et al
(2012), which presented a system that automati-
cally classifies English parse errors using a two
stage process. First, the system finds the shortest
path from the system output to the gold annota-
tions, where each step in the path is a tree transfor-
mation, fixing at least one bracket error. Second,
each transformation step is classified into one of
several error types.
When directly applied to Chinese parser output,
the system placed over 27% of the errors in the
catch-all ?Other? type. Many of these errors clearly
fall into one of a small set of error types, motivat-
ing an adaptation to Chinese syntax.
3 Adapting error analysis to Chinese
To adapt the Kummerfeld et al (2012) system to
Chinese, we developed a new version of the second
stage of the system, which assigns an error cate-
gory to each tree transformation step.
To characterize the errors the original system
placed in the ?Other? category, we looked through
one  hundred  sentences, identifying  error  types
generated by Chinese syntax that the existing sys-
tem did not account for. With these observations
we were able to implement new rules to catch the
previously missed cases, leading to the set shown
in Table 1. To ensure the accuracy of our classifica-
tions, we alternated between refining the classifica-
tion code and looking at affected classifications to
identify issues. We also periodically changed the
sentences from the development set we manually
checked, to avoid over-fitting.
Where necessary, we also expanded the infor-
mation available during classification. For exam-
ple, we use the structure of the final gold standard
tree when classifying errors that are a byproduct of
sense disambiguation errors.
4 Chinese parsing errors
Table 1 presents the errors made by the Berkeley
parser. Below we describe the error types that are
Error Type Brackets % of total
NP-internal* 6019 22.70%
Coordination 2781 10.49%
Verb taking wrong args* 2310 8.71%
Unary 2262 8.53%
Modifier Attachment 1900 7.17%
One Word Span 1560 5.88%
Different label 1418 5.35%
Unary A-over-A 1208 4.56%
Wrong sense/bad attach* 1018 3.84%
Noun boundary error* 685 2.58%
VP Attachment 626 2.36%
Clause Attachment 542 2.04%
PP Attachment 514 1.94%
Split Verb Compound* 232 0.88%
Scope error* 143 0.54%
NP Attachment 109 0.41%
Other 3186 12.02%
Table 1: Errors made when parsing Chinese. Values are the
number of bracket errors attributed to that error type. The
values shown are for the Berkeley parser, evaluated on the
development set. * indicates error types that were added or
substantially changed as part of this work.
either new in this analysis, have had their definition
altered, or have an interesting distribution.
2
In all of our results we follow Kummerfeld et al
(2012), presenting the number of bracket errors
(missing or extra)  attributed to each error type.
Bracket counts are more informative than a direct
count of each error type, because the impact on
EVALB F-score varies between errors, e.g. a sin-
gle attachment error can cause 20 bracket errors,
while a unary error causes only one.
NP-internal. (Figure 1a). Unlike  the  Penn
Treebank (Marcus et al, 1993), the PCTB anno-
tates some NP-internal structure. We assign this
error type when a transformation involves words
whose parts of speech in the gold tree are one of:
CC, CD, DEG, ETC, JJ, NN, NR, NT and OD.
We investigated the errors that fall into the NP-
internal category and found that 49% of the errors
involved the creation or deletion of a single pre-
termianl phrasal bracket. These errors arise when
a parser proposes a tree in which POS tags (for in-
stance, JJ or NN) occur as siblings of phrasal tags
(such as NP), a configuration used by the PCTB
bracketing guidelines to indicate complementation
as opposed to adjunction (Xue et al, 2005).
2
For an explanation of the English error types, see Kum-
merfeld et al (2012).
99
Verb taking wrong args. (Figure 1b). This
error type arises when a verb (e.g.?? reverse)
is  hypothesized  to  take  an  incorrect  argument
(?? Bush instead of ?? position). Note that
this also covers some of the errors that Kummer-
feld  et al (2012) classified  as  NP Attachment,
changing the distribution for that type.
Unary. For mis-application of unary rules we
separate out instances in which the two brackets in
the production have the the same label (A-over-A).
This cases is created when traces are eliminated, a
standard step in evaluation. More than a third of
unary errors made by the Berkeley parser are of the
A-over-A type. This can be attributed to two fac-
tors: (i) the PCTB annotates non-local dependen-
cies using traces, and (ii) Chinese syntax generates
more traces than English syntax (Guo et al, 2007).
However, for parsers that do not return traces they
are a benign error.
Modifier attachment. (Figure 1c). Incorrect
modifier scope caused by modifier phrase attach-
ment level. This is less frequent in Chinese than
in English: while English VP modifiers occur in
pre- and post-verbal positions, Chinese only al-
lows pre-verbal modification.
Wrong sense/bad attach. (Figure 1d). This ap-
plies when the head word of a phrase receives the
wrong POS, leading to an attachment error. This
error type is common in Chinese because of POS
fluidity, e.g. the well-known Chinese verb/noun
ambiguity often causes mis-attachments that are
classified as this error type.
In  Figure 1d, the  word ?? invest has
both  noun  and  verb  senses. While  the  gold
standard  interpretation  is  the  relative  clause
firms that Macau invests in, the parser returned an
NP interpretation Macau investment firms.
Noun boundary error. In this error type, a span
is moved to a position where the POS tags of its
new siblings all belong to the list of NP-internal
structure tags which we identified above, reflecting
the inclusion of additional material into an NP.
Split  verb  compound. The  PCTB annota-
tions recognize several Chinese verb compound-
ing strategies, such as  the serial  verb construc-
tion (???? plan [and] build) and the resulta-
tive construction (?? cook [until] done), which
join a bare verb to another lexical item. We in-
troduce an error type specific to Chinese, in which
such verb compounds are split, with the two halves
of the compound placed in different phrases.
..NP
.
.NN .
??
coach
.
.NN .
??
soccer
.
.NN .
??
nat'l
.NP
.
.NP
.
.NP.NN
.
.NP.NN
.
.NP.NN
(a) NP-internal structure errors
..VP
.
.NP
.
.NP .
??
position
.
.DNP
.
.DEG .?
.
.NP .
??
Bush
.
.VV .
??
reverse
.CP
.
.IP
.
.VP
.
.VV
.
.NP
.
.DEC
.
.NP
(b) Verb taking wrong arguments
..VP
.
.VP .
????
win gold
.
.QP
.
.QP .
???
3rd time
.
.ADVP .
??
in a row
.VP
.
.ADVP
.
.QP
.
.QP
.VP
(c) Modifier attachment ambiguity
..CP
.
.NP .
??
firm
.
.IP
.
.VP .
??
invest
.
.NP .
??
Macau
.NP
.
.NP
.
.NP
.
.NP
.NP
(d) Sense confusion
Figure 1: Prominent error types in Chinese parsing. The left
tree is the gold structure; the right is the parser hypothesis.
Scope error. These are cases in which a new
span must be added to more closely bind a modifier
phrase (ADVP, ADJP, and PP).
PP attachment. This error type is rare in Chi-
nese, as adjunct PPs are pre-verbal. It does oc-
cur near coordinated VPs, where ambiguity arises
about  which of  the conjuncts  the PP has scope
over. Whether this particular case is PP attachment
or coordination is debatable; we follow Kummer-
feld et al (2012) and label it PP attachment.
4.1 Chinese-English comparison
It is difficult to directly compare error analysis
results for Chinese and English parsing because
of substantial changes in the classification method,
and differences in treebank annotations.
As described in the previous section, the set of
error categories considered for Chinese is very dif-
ferent to the set of categories for English. Even
for some of the categories that were not substan-
tially changed, errors may be classified differently
because of cross-over between categories between
100
NP Verb Mod. 1-Word Diff Wrong Noun VP Clause PP
System F1 Int. Coord Args Unary Attach Span Label Sense Edge Attach Attach Attach Other
Best 1.54 1.25 1.01 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87
Berk-G 86.8
Berk-2 81.8
Berk-1 81.1
ZPAR 78.1
Bikel 76.1
Stan-F 76.0
Stan-P 70.0
Worst 3.94 1.75 1.73 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11
Table 2: Error breakdown for the development set of PCTB 6. The area filled in for each bar indicates the average number of
bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in
the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with
two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel),
the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P).
two categories (e.g. between Verb taking wrong
args and NP Attachment).
Differences in treebank annotations also present
a challenge for cross-language error comparison.
The  most  common  error  type  in  Chinese, NP-
internal structure, is rare in the results of Kummer-
feld et al (2012), but the datasets are not compara-
ble because the PTB has very limited NP-internal
structure annotated. Further characterization of the
impact of annotation differences on errors is be-
yond the scope of this paper.
Three conclusions that can be made are that (i)
coordination is a major issue in both languages,
(ii) PP attachment is a much greater problem in
English, and  (iii)  a  higher  frequency  of  trace-
generating syntax in Chinese compared to English
poses substantial challenges.
5 Cross-parser analysis
The previous section described the error types
and their distribution for a single Chinese parser.
Here we confirm that these are general trends, by
showing that the same pattern is observed for sev-
eral  different  parsers  on  the  PCTB 6 dev  set.
3
We include results  for  a  transition-based parser
(ZPAR; Zhang  and  Clark, 2009), a  split-merge
PCFG parser (Petrov et al, 2006; Petrov and Klein,
2007; Petrov, 2010), a lexicalized parser (Bikel
and Chiang, 2000), and a factored PCFG and de-
pendency parser (Levy and Manning, 2003; Klein
and Manning, 2003a,b).
4
Comparing the two Stanford parsers in Table 2,
the factored model provides clear improvements
3
We use the standard data split suggested by the PCTB 6
file manifest. As a result, our results differ from those previ-
ously reported on other splits. All analysis is on the dev set,
to avoid revealing specific information about the test set.
4
These parsers represent a variety of parsing methods,
though exclude some recently developed parsers that are not
publicly available (Qian and Liu, 2012; Xiong et al, 2005).
on  sense  disambiguation, but  performs  slightly
worse on coordination.
The Berkeley product parser we include uses
only two grammars because we found, in contrast
to the English results (Petrov, 2010), that further
grammars provided limited benefits. Comparing
the performance with the standard Berkeley parser
it seems that the diversity in the grammars only as-
sists certain error types, with most of the improve-
ment  occurring in  four  of  the categories, while
there is no improvement, or a slight decrease, in
five categories.
6 Tagging Error Impact
The challenge of accurate POS tagging in Chi-
nese has been a major part of several recent papers
(Qian and Liu, 2012; Jiang et al, 2009; Forst and
Fang, 2009). The Berk-G row of Table 2 shows
the performance of the Berkeley parser when given
gold POS tags.
5
While the F1 improvement is un-
surprising, for the first time we can clearly show
that the gains are only in a subset of the error types.
In particular, tagging improvement will not help
for two of the most significant challenges: coordi-
nation scope errors, and verb argument selection.
To see which tagging confusions contribute to
which error reductions, we adapt the POS ablation
approach of Tse and Curran (2012). We consider
the POS tag pairs shown in Table 3. To isolate the
effects of each confusion we start from the gold
tags and introduce the output of the Stanford tag-
ger whenever it returns one of the two tags being
considered.
6
We then feed these ?semi-gold? tags
5
We used the Berkeley parser as it was the best of the
parsers we considered. Note that the Berkeley parser occa-
sionally prunes all of the parses that use the gold POS tags,
and so returns the best available alternative. This leads to a
POS accuracy of 99.35%, which is still well above the parser?s
standard POS accuracy of 93.66%.
6
We introduce errors to gold tags, rather than removing er-
101
Confused tags Errors ? F1
VV NN 1055 -2.72
DEC DEG 526 -1.72
JJ NN 297 -0.57
NR NN 320 -0.05
Table 3: The most frequently confused POS tag pairs. Each
? F1 is relative to Berk-G.
to the Berkeley parser, and run the fine-grained er-
ror analysis on its output.
VV/NN. This confusion has been consistently
shown to be a major contributor to parsing errors
(Levy and Manning, 2003; Tse and Curran, 2012;
Qian and Liu, 2012), and we find a drop of over 2.7
F1 when the output of the tagger is introduced. We
found that while most error types have contribu-
tions from a range of POS confusions, verb/noun
confusion was responsible for virtually all of the
noun boundary errors corrected by using gold tags.
DEG/DEC. This confusion between the rela-
tivizer and subordinator senses of the particle ?
de is the primary source of improvements on mod-
ifier attachment when using gold tags.
NR/NN and JJ/NN. Despite  their  frequency,
these confusions have little effect on parsing per-
formance. Even within the NP-internal error type
their impact is limited, and almost all of the errors
do not change the logical form.
7 Conclusion
We have  quantified  the  relative  impacts  of  a
comprehensive set of error types in Chinese pars-
ing. Our analysis has also shown that while im-
provements in Chinese POS tagging can make a
substantial difference for some error types, it will
not address two high-frequency error types: in-
correct verb argument attachment and coordina-
tion scope. The frequency of these two error types
is also unimproved by the use of products of la-
tent variable grammars. These observations sug-
gest that resolving the core challenges of Chinese
parsing will require new developments that suit the
distinctive properties of Chinese syntax.
Acknowledgments
We extend our thanks to Yue Zhang for helping
us train new ZPAR models. We would also like
to thank the anonymous reviewers for their help-
ful suggestions. This research was supported by
a General Sir John Monash Fellowship to the first
rors from automatic tags, isolating the effect of a single con-
fusion by eliminating interaction between tagging decisions.
author, the Capital Markets CRC under ARC Dis-
covery grant DP1097291, and the NSF under grant
0643742.
References
Daniel M. Bikel and David Chiang. 2000. Two
Statistical Parsing Models Applied to the Chi-
nese Treebank. In Proceedings of the Second
Chinese Language Processing Workshop, pages
1?6. Hong Kong, China.
Martin Forst and Ji Fang. 2009. TBL-improved
non-deterministic  segmentation  and  POS tag-
ging for a Chinese parser. In Proceedings of the
12th Conference of the European Chapter of the
ACL, pages 264?272. Athens, Greece.
Yuqing Guo, Haifeng Wang, and Josef van Gen-
abith. 2007. Recovering Non-Local Dependen-
cies for Chinese. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 257?266. Prague, Czech Republic.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging
? A Case Study. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
volume 1, pages 522?530. Suntec, Singapore.
Dan Klein and Christopher D. Manning. 2003a.
Accurate Unlexicalized Parsing. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 423?430.
Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b.
Fast Exact Inference with a Factored Model for
Natural Language Parsing. In Advances in Neu-
ral Information Processing Systems 15, pages
3?10. MIT Press, Cambridge, MA.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser Show-
down at the Wall Street Corral: An Empirical
Investigation of Error Types in Parser Output.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 1048?1059. Jeju Island, South
Korea.
102
Roger Levy and Christopher Manning. 2003. Is
it harder to parse Chinese, or the Chinese Tree-
bank? In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguis-
tics, pages 439?446. Sapporo, Japan.
Mitchell P.  Marcus, Beatrice  Santorini, and
Mary Ann  Marcinkiewicz.  1993. Building
a  Large  Annotated  Corpus  of  English: The
Penn  Treebank. Computational  Linguistics,
19(2):313?330.
Slav Petrov. 2010. Products of Random Latent
Variable Grammars. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational  Linguistics, pages  19?27.  Los
Angeles, California.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein.  2006. Learning  Accurate, Com-
pact, and Interpretable Tree Annotation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual
Meeting of the Association for Computational
Linguistics, pages 433?440. Sydney, Australia.
Slav Petrov and Dan Klein. 2007. Improved In-
ference for Unlexicalized Parsing. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics; Proceed-
ings of the Main Conference, pages 404?411.
Rochester, New York, USA.
Xian Qian and Yang Liu.  2012. Joint Chinese
word segmentation, POS tagging and parsing.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 501?511. Jeju Island, Korea.
Daniel  Tse  and  James R.  Curran.  2012. The
Challenges of Parsing Chinese with Combina-
tory Categorial Grammar. In Proceedings of the
2012 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, pages
295?304. Montre?al, Canada.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun
Lin, and Yueliang Qian. 2005. Parsing the Penn
Chinese Treebank with semantic knowledge. In
Proceedings of  the Second international  joint
conference  on  Natural  Language  Processing,
pages 70?81. Jeju Island, Korea.
Nianwen  Xue, Fei  Xia, Fu-Dong  Chiou, and
Martha  Palmer.  2005. The  Penn  Chinese
TreeBank: Phrase  structure  annotation  of  a
large corpus. Natural Language Engineering,
11(2):207?238.
Yue Zhang and Stephen Clark. 2009. Transition-
Based Parsing of the Chinese Treebank using a
Global Discriminative Model. In Proceedings
of the 11th International Conference on Parsing
Technologies (IWPT?09), pages 162?171. Paris,
France.
103
